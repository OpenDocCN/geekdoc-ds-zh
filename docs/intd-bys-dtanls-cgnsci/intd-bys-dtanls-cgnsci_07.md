# 第二章 贝叶斯数据分析简介

> 原文：[`bruno.nicenboim.me/bayescogsci/ch-introBDA.html`](https://bruno.nicenboim.me/bayescogsci/ch-introBDA.html)

在我们开始使用贝叶斯定理分析实际数据集之前，理解贝叶斯定理在涉及二项似然的最简单情况下的应用非常重要。这个简单情况之所以重要，是因为它包含了贝叶斯数据分析方法的本质，并且它允许我们仅使用笔和纸就分析出感兴趣参数的后验分布。这个简单情况还帮助我们理解一个关键点：参数的后验分布是先验和似然之间的折衷。这个重要的洞察将在本书剩余部分讨论的现实数据分析场景中发挥核心作用。

## 2.1 贝叶斯定理

回顾贝叶斯定理：当 \(A\) 和 \(B\) 是可观察的离散事件（例如“下雨了”或“街道湿了”）时，我们可以将规则表述如下：

\[\begin{equation} P(A\mid B) = \frac{P(B\mid A) P(A)}{P(B)} \tag{2.1} \end{equation}\]

给定数据向量 \(\boldsymbol{y}\)，贝叶斯定理允许我们计算出感兴趣参数的后验分布，我们可以将其表示为参数向量 \(\boldsymbol{\Theta}\)。这种计算是通过将方程 (2.1) 重新表述为 (2.2) 来实现的。这里的不同之处在于贝叶斯定理是以概率分布的形式书写的。在这里，\(p(\cdot)\) 是概率密度函数（连续情况）或概率质量函数（离散情况）。

\[\begin{equation} p(\boldsymbol{\Theta}|\boldsymbol{y}) = \cfrac{ p(\boldsymbol{y}|\boldsymbol{\Theta}) \times p(\boldsymbol{\Theta}) }{p(\boldsymbol{y})} \tag{2.2} \end{equation}\]

上述陈述可以用以下文字表述：

\[\begin{equation} \hbox{后验} = \frac{\hbox{似然} \times \hbox{先验}}{\hbox{边缘似然}} \end{equation}\]

这里使用的术语具有以下含义。以下我们将通过示例详细说明每个要点。

+   *后验*，\(p(\boldsymbol{\Theta}|\boldsymbol{y})\)，是在给定数据的情况下参数的概率分布。

+   *似然*，\(p(\boldsymbol{y}|\boldsymbol{\Theta})\)，在第一章中有描述：它是 \(\boldsymbol{\Theta}\) 的函数，表示为 PMF（离散情况）或 PDF（连续情况）。

+   *先验*，\(p(\boldsymbol{\Theta})\)，是在看到数据之前参数的初始概率分布。

+   在第一章中介绍了 *边缘似然*，\(p(\boldsymbol{y})\)，它将后验分布标准化，以确保分布曲线下的面积总和为 1，也就是说，它确保后验是一个有效的概率分布。

一个例子可以阐明所有这些术语。

## 2.2 使用贝叶斯定理推导后验：一个分析例子

回想我们之前的完形填空概率例子。受试者被展示类似以下句子：

“下雨了。我要拿……”

假设有 100 个受试者被要求完成这个句子。如果 100 个受试者中有 80 个用“雨伞”来完成这个句子，那么根据前文给出的上下文，估计的完形填空概率或可预测性（即\(\frac{80}{100}=0.8\)）。这是产生这个单词概率的最大似然估计；我们将用参数名称上的“帽子”来表示这个估计：\(\hat \theta=0.8\)。在频率主义范式下，\(\hat \theta=0.8\)是对自然界中未知点值\(\theta\)的估计。

这里一个关键点是，我们之前从数据中估计出的比例 0.8 可能从一个数据集变化到另一个数据集，估计的变异性将受样本大小（试验次数）的影响。例如，假设\(\theta\)参数的真实值实际上是 0.8，如果我们反复进行上述实验，每次 10 次试验，我们将在估计的比例中得到一些变异性。让我们通过进行 100 次模拟实验并计算重复抽样的估计均值变异性来检查这一点：

```r
estimated_means <-  rbinom(100, size = 10, prob = 0.8) /  10
sd(estimated_means)
```

```r
## [1] 0.118
```

（模拟）实验的重复运行是估计比例（如上`sd(estimated)`命令所示）变异性（唯一）的根本原因；参数\(\theta=0.80\)本身在这里是不变的（我们反复估计这个点值）。

然而，现在考虑一个不同的、激进的想法：如果我们把\(\theta\)看作一个随机变量呢？也就是说，现在假设\(\theta\)有一个与之相关的概率密度函数（PDF）。这个 PDF 将现在代表我们对\(\theta\)可能值的信念，即使在我们看到任何数据之前。例如，如果在实验开始时，我们相信 0 到 1 之间所有可能的值都是等可能的，我们可以通过声明\(\theta \sim \mathit{Uniform}(0,1)\)来表示这种信念。这里激进的新想法是我们现在有一种方法来表示我们对参数可能值的先验信念或知识。

现在，如果我们反复运行我们的模拟实验，参数估计的变异性将有两个来源：数据和与\(\theta\)相关的不确定性。

```r
theta <-  runif(100, min = 0, max = 1)
estimated_means <-  rbinom(100, size = 10, prob = theta) /  10
sd(estimated_means)
```

```r
## [1] 0.322
```

现在更高的标准差来自于与\(\theta\)参数相关的不确定性。为了看到这一点，假设一个“更紧”的 PDF，比如\(\theta \sim \mathit{Uniform}(0.3,0.8)\)，那么估计的均值的变化性会再次减小，但不会像当我们假设\(\theta\)是一个点值时那样小：

```r
theta <-  runif(100, min = 0.3, max = 0.8)
estimated_means <-  rbinom(n=100, size = 10, prob = theta) /  10
sd(estimated_means)
```

```r
## [1] 0.214
```

换句话说，与参数\(\theta\)相关的确定性越大，数据中的变异性就越大。

参数估计的贝叶斯方法与标准频率主义假设有根本的不同，频率主义假设认为 \(\theta\) 的真实、未知的值是某个点值。在贝叶斯方法中，\(\theta\) 是一个随机变量，与它相关联的概率密度/质量函数。这个 PDF 被称为先验分布，代表我们对该参数可能值的先验信念或先验知识。一旦我们获得数据，这些数据就会用来修改我们对该分布的先验信念；这个参数更新的概率密度函数被称为后验分布。以下各节将详细阐述这些概念。

### 2.2.1 选择似然函数

在我们上面设定的假设下，响应遵循二项分布，因此 PMF 可以写成以下形式。

\[\begin{equation} p(k|n,\theta) = \binom{n} {k} \theta^k (1-\theta)^{n-k} \tag{2.3} \end{equation}\]

其中 \(k\) 表示“雨伞”被给出作为答案的次数，\(n\) 是试验次数。在我们的例子中，\(k\) 可以是 \(0\) 到 \(100\) 之间的任何整数。

在我们进行的特定实验中，如果我们收集了 \(100\) 个数据点 (\(n=100\))，并且结果 \(k = 80\)，那么这些数据现在是一个固定量。在上述 PMF 中现在唯一的变量是 \(\theta\)：

\[\begin{equation} p(k=80 | n= 100, \theta) = \binom{100}{80} \theta^{80} (1-\theta)^{20} \end{equation}\]

上述函数现在是一个关于 \(\theta\) 值的连续函数，其可能值范围从 \(0\) 到 \(1\)。将其与二项分布的 PMF 进行比较，二项分布将 \(\theta\) 视为固定值，并在我们可能观察到的 \(n+1\) 个可能的离散值 \(k\) 上定义一个离散分布（可能的成功次数）。

回想一下，概率质量函数（PMF）和似然函数是从不同角度看待的同一函数。这两个函数之间的唯一区别在于，我们考虑什么是不变的，什么是在变化的。PMF 将数据视为随实验而变化，而 \(\theta\) 是固定的，而似然函数将收集到的数据视为固定的，而参数 \(\theta\) 是变化的。

现在，我们将注意力转回到我们的主要目标上，即使用贝叶斯定理，找出给定我们的数据 \(\theta\) 的后验分布：\(p(\theta|n,k)\)。为了使用贝叶斯定理来计算这个后验分布，我们需要在参数 \(\theta\) 上定义一个先验分布。这样做时，我们明确地表达了对 \(\theta\) 可能值的先验不确定性。

### 2.2.2 为 \(\theta\) 选择先验

对于二项分布中 \(\theta\) 的先验选择，我们需要假设参数 \(\theta\) 是一个随机变量，其概率密度函数（PDF）的范围位于 [0,1] 内，这是 \(\theta\) 可以变化的范围（这是因为 \(\theta\) 代表一个概率）。贝塔分布，作为一个连续随机变量的 PDF，通常被用作表示概率的参数的先验。选择这个分布的一个原因是它的 PDF 范围在 \([0,1]\) 区间内。另一个原因是这使得贝叶斯规则的计算变得非常简单。

贝塔分布具有以下 PDF。

\[\begin{equation} p(\theta|a,b)= \frac{1}{B(a,b)} \theta^{a - 1} (1-\theta)^{b-1} \tag{2.4} \end{equation}\]

项 \(B(a,b)\) 展开为 \(\int_0¹ \theta^{a-1}(1-\theta)^{b-1}\, \mathrm{d}\theta\)，是一个归一化常数，确保曲线下方的面积总和为 1。在某些教科书中，你可能会看到带有归一化常数 \(\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\) 的贝塔分布 PDF（表达式 \(\Gamma(n)\) 定义为 (n-1)!）：\[p(\theta|a,b)= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \theta^{a - 1} (1-\theta)^{b-1}\] 这两个关于贝塔分布的陈述是相同的，因为 \(B(a,b)\) 可以证明等于 \(\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}\)（参见 Ross 2002）。

贝塔分布的参数 \(a\) 和 \(b\) 可以解释为我们对成功概率的先验信念的表达；\(a\) 代表“成功”的数量，在我们的例子中，是“雨伞”答案的数量，而 \(b\) 代表失败的数量，即不是“雨伞”的答案。图 2.1 展示了不同 \(a\) 和 \(b\) 值下的不同贝塔分布形状。

![不同参数下的贝塔分布示例](img/ba978b1d5f101a9c1ca2fa12dc31fa33.png)

图 2.1：不同参数下的贝塔分布示例。

正如我们在第一章中看到的二项分布和正态分布一样，可以解析地推导出贝塔分布的期望和方差公式。这些是：

\[\begin{equation} \operatorname{E}[X] = \frac{a}{a+b} \quad \operatorname{Var}(X)=\frac {a \times b }{(a + b )^{2}(a + b +1)} \tag{2.5} \end{equation}\]

例如，选择 \(a=4\) 和 \(b=4\) 意味着“雨伞”这个答案与其他答案一样可能，但我们对此相对不确定。我们可以通过计算我们 95% 确定参数值所在的区域来表达我们的不确定性；这就是 *95% 置信区间*。为此，我们将使用 R 中的 `qbeta()` 函数；在 R 中，参数 \(a\) 和 \(b\) 分别称为 `shape1` 和 `shape2`。

```r
qbeta(c(0.025, 0.975), shape1 = 4, shape2 = 4)
```

```r
## [1] 0.184 0.816
```

上文选择的可信区间是一个等尾区间：低于下限和高于上限的面积相同（在上述情况下为 \(0.025\)）。可以定义其他区间；例如，在只有一个峰（一个峰值；单峰分布）的分布中，可以选择使用包含峰值的狭窄区间。这被称为最高后验密度区间（HDI）。在偏斜的后验分布中，等尾可信区间和 HDI 不会相同，因为 HDI 将有不等的尾概率。一些作者，如 Kruschke (2014)，更喜欢报告 HDI。我们将在这本书中使用等尾区间，因为这仅仅是 Stan 和 `brms` 的标准输出。

如果我们选择 \(a=10\) 和 \(b=10\)，我们仍然假设先验地，答案“雨伞”与其他答案一样可能，但现在我们对这个均值的先验不确定性更低，如下面的 95% 可信区间所示。

```r
qbeta(c(0.025, 0.975), shape1 = 10, shape2 = 10)
```

```r
## [1] 0.289 0.711
```

在图 2.1 中，我们也可以从图形上看到这两个例子中的不确定性差异。

我们应该选择哪个先验？在真实的数据分析问题中，先验的选择将取决于我们希望带入分析中的先验知识（参见在线章节 E）。如果我们没有太多先验信息，我们可以使用 \(a=b=1\)；这给我们一个均匀先验（即，\(\mathit{Uniform}(0,1)\)）。这种先验有各种名称，例如 *平坦、非信息先验* 或 *无信息先验*。相比之下，如果我们有很多先验知识以及/或强烈的信念（例如，基于特定理论的预测或先验数据），认为 \(\theta\) 有一个特定的合理值范围，我们可以使用一组不同的 \(a, b\) 值来反映我们对参数的信念。一般来说，参数 \(a\) 和 \(b\) 的值越大，分布的分布范围就越窄；即，我们对参数均值的不确定性就越低。

我们将在在线章节 E 中详细讨论先验指定。目前，只是为了说明，我们选择 \(a=4\) 和 \(b=4\) 作为贝塔先验的值。然后，我们的 \(\theta\) 先验是以下贝塔概率密度函数：

\[\begin{equation} p(\theta) = \frac{1}{B(4,4)} \theta^{3} (1-\theta)^{3} \end{equation}\]

在选择了一个似然函数，并在 \(\theta\) 上定义了一个先验之后，我们准备进行我们的第一次贝叶斯分析，以推导出 \(\theta\) 的后验分布。

### 2.2.3 使用贝叶斯定理计算后验 \(p(\theta|n,k)\)

在指定了似然函数和先验之后，我们现在将使用贝叶斯定理来计算 \(p(\theta|n,k)\)。使用贝叶斯定理简单地将我们上面定义的似然函数和先验代入到我们之前看到的方程中：

\[\begin{equation} \hbox{Posterior} = \frac{\hbox{Likelihood} \times \hbox{Prior}}{\hbox{Marginal Likelihood}} \end{equation}\]

将似然和先验的项代入此方程：

\[\begin{equation} p(\theta|n=100,k=80) = \frac{\left[\binom{100}{80} \theta^{80} \times (1-\theta)^{20}\right] \times \left[\frac{1}{B(4,4)} \times \theta^{3} (1-\theta)^{3}\right]}{p(k=80)} \tag{2.6} \end{equation}\]

其中 \(p(k=80)\) 是 \(\int_{0}¹ p(k=80|n=100,\theta) p(\theta)\, \mathrm{d}\theta\)。这个项一旦成功次数 \(k\) 已知，将是一个常数；这是我们在第一章中遇到的边缘似然。事实上，一旦 \(k\) 已知，上述方程中还有几个常数；它们是常数，因为它们都不依赖于感兴趣的参数，\(\theta\)。我们可以将它们全部收集在一起：

\[\begin{equation} p(\theta|n=100,k=80) = \left[ \frac{\binom{100}{80}}{B(4,4)\times p(k=80)} \right] [\theta^{80} (1-\theta)^{20} \times \theta^{3} (1-\theta)^{3}] \tag{2.7} \end{equation}\]

方程中第一个括号内的项，\(\frac{\binom{100}{80}}{B(4,4)\times p(k=80)}\)，是所有常数收集在一起，是我们之前看到的归一化常数；它使得后验分布 \(p(\theta|n=100,k=80)\) 的总和为 1。由于它是一个常数，我们现在可以忽略它，并专注于方程中的其他两个项。因为我们忽略了常数，所以现在我们将说后验是右侧的比例。

\[\begin{equation} p(\theta|n=100,k=80) \propto [\theta^{80} (1-\theta)^{20} \times \theta^{3} (1-\theta)^{3} ] \tag{2.8} \end{equation}\]

写出上述方程的一种常见方式是：

\[\begin{equation} \hbox{Posterior} \propto \hbox{Likelihood} \times \hbox{Prior} \end{equation}\]

现在解决右侧的问题只是简单地相加指数！在这个例子中，计算后验实际上归结为对指数进行简单的加法运算。

\[\begin{equation} p(\theta|n=100,k=80) \propto [\theta^{80+3} (1-\theta)^{20+3}] = \theta^{83} (1-\theta)^{23} \tag{2.9} \end{equation}\]

右侧的表达式对应于参数 \(a=84\) 和 \(b=24\) 的贝塔分布。如果我们重新写右侧的表达式，使其代表贝塔概率密度函数（PDF）的核心部分（参见方程 (2.4)），这一点就会变得明显。唯一缺少的是归一化常数，它会使曲线下的面积总和为 1。

\[\begin{equation} \theta^{83} (1-\theta)^{23} = \theta^{84-1} (1-\theta)^{24-1} \end{equation}\]

任何概率密度函数（PDF）或概率质量函数（PMF）的核心部分被称为该分布的核心。没有归一化常数，曲线下的面积将不会总和为 1。让我们来验证这一点：

```r
PostFun <-  function(theta) {
 theta⁸⁴ *  (1 -  theta)²⁴
}
(AUC <-  integrate(PostFun, lower = 0, upper = 1)$value)
```

```r
## [1] 1.42e-26
```

因此，曲线下的面积（AUC）不是 \(1\) ——我们上面计算的后验分布不是一个合适的概率分布。我们上面所做的是计算以下积分：

\[\begin{equation} \int_{0}^{1} \theta^{84} (1-\theta)^{24} \end{equation}\]

我们可以使用这个积分来确定归一化常数。基本上，我们想知道常数 k 是什么，使得曲线下的面积总和为 1：

\[\begin{equation} k \int_{0}^{1} \theta^{84} (1-\theta)^{24} = 1 \end{equation}\]

我们知道 \(\int_{0}^{1} \theta^{84} (1-\theta)^{24}\) 是什么；我们刚刚计算了那个值（在上述 R 代码中称为 `AUC`）。因此，归一化常数是：

\[\begin{equation} k = \frac{1}{\int_{0}^{1} \theta^{84} (1-\theta)^{24}} = \frac{1}{AUC} \end{equation}\]

因此，要将核 \(\theta^{84} (1-\theta)^{24}\) 转换为合适的概率分布，只需要包含一个归一化常数，根据贝塔分布的定义（方程 (2.4)），这个常数将是 \(B(84,24)\)。这个项实际上是我们上面计算过的积分。

因此，我们得到的是给定数据的 \(\theta\) 分布，表示为 PDF：

\[\begin{equation} p(\theta|n=100,k=80) = \frac{1}{B(84,24)} \theta^{84-1} (1-\theta)^{24-1} \end{equation}\]

现在，这个函数的总和为 1：

```r
PostFun <-  function(theta) {
 theta⁸⁴ *  (1 -  theta)²⁴ /  AUC
}
integrate(PostFun, lower = 0, upper = 1)$value
```

```r
## [1] 1
```

### 2.2.4 程序总结

总结一下，我们从一个数据集（\(n=100, k=80\)）和一个二项式似然开始，将其与先验概率密度函数 \(\theta \sim \mathit{Beta}(4,4)\) 相乘，得到后验 \(p(\theta|n,k) \sim \mathit{Beta}(84,24)\)。在执行乘法时忽略了常数；我们说我们计算了后验 *到比例*。最后，我们展示了在这个简单例子中，如何通过包含一个比例常数将后验缩放为概率分布。

上述例子是一个 *共轭* 分析的案例：参数的后验具有与先验相同的格式（属于同一族概率分布）。上述似然和先验的组合称为贝塔二项式共轭情况。还有其他几种似然和先验的组合，它们的后验具有与先验 PDF 相同的 PDF；一些例子将在练习中出现。

形式上，共轭性定义为以下内容：给定似然 \(p(y| \theta)\)，如果先验 \(p(\theta)\) 导致的后验 \(p(\theta|y)\) 与 \(p(\theta)\) 具有相同的格式，那么我们称 \(p(\theta)\) 为共轭先验。

对于贝塔二项式共轭情况，我们可以推导出似然、先验和后验之间一个非常一般的关系。给定比例上的二项式似然（忽略常数）\(\theta^k (1-\theta)^{n-k}\)，以及先验，也到比例，\(\theta^{a-1} (1-\theta)^{b-1}\)，它们的乘积将是：

\[\begin{equation} \theta^k (1-\theta)^{n-k} \theta^{a-1} (1-\theta)^{b-1} = \theta^{a+k-1} (1-\theta)^{b+n-k-1} \end{equation}\]

因此，给定一个 \(\mathit{Binomial}(n,k|\theta)\) 似然函数和一个 \(\mathit{Beta}(a,b)\) 先验分布，后验分布将是 \(\mathit{Beta}(a+k,b+n-k)\)。

### 2.2.5 可视化先验分布、似然函数和后验分布

在上面的例子中，我们确定了后验分布是一个参数为 \(a = 84\) 和 \(b = 24\) 的贝塔分布。我们在图 2.2 中并排可视化似然函数、先验分布和后验分布。

![在贝塔二项式共轭示例中（缩放后的）似然函数、先验分布和后验分布。似然函数被缩放以积分等于 1，以便更容易与先验分布和后验分布进行比较。](img/39deb282fe143607e31431e44c6a63e0.png)

图 2.2：在贝塔二项式共轭示例中（缩放后的）似然函数、先验分布和后验分布。似然函数被缩放以积分等于 1，以便更容易与先验分布和后验分布进行比较。

我们可以像上面那样通过图形总结后验分布，或者通过计算均值和方差来总结。均值给出了在句子中产生“雨伞”的概率的估计（给定模型，即给定似然函数和先验）：

\[\begin{equation} \operatorname{E}[\hat\theta] = \frac{84}{84+24}=0.78 \tag{2.10} \end{equation}\]

\[\begin{equation} \operatorname{var}[\hat\theta]=\frac {84 \times 24 }{(84+24 )^{2}(84+24 +1)}= 0.0016 \tag{2.11} \end{equation}\]

我们还可以显示 95%的置信区间，即给定数据和模型，我们 95%确信 \(\theta\) 落在这个范围内的范围。

```r
qbeta(c(0.025, 0.975), shape1 = 84, shape2 = 24)
```

```r
## [1] 0.695 0.851
```

通常，我们会通过显示参数（或参数）的后验分布以及上述汇总统计信息（均值、标准差或方差、95%置信区间）来总结贝叶斯分析的结果。您将在后面的例子中看到许多此类汇总的例子。

### 2.2.6 后验分布是先验分布和似然函数之间的折中

回想一下前面的章节，贝塔分布中的 \(a\) 和 \(b\) 参数决定了 \(\theta\) 参数上的先验分布的形状。仅为了说明，让我们考虑四个不同的贝塔先验，它们反映了关于 \(\theta\) 的先验确定性逐渐增加。

+   \(\mathit{Beta}(a=2,b=2)\)

+   \(\mathit{Beta}(a=3,b=3)\)

+   \(\mathit{Beta}(a=6,b=6)\)

+   \(\mathit{Beta}(a=21,b=21)\)

这些先验中的每一个都反映了对 \(\theta=0.5\) 的信念，但具有不同程度的（不）确定性。根据我们上面为贝塔二项式情况开发的通用公式，我们只需将似然函数和先验分布插入即可得到后验分布：

\[\begin{equation} p(\theta | n,k) \propto p(k |n,\theta) p(\theta) \end{equation}\]

对应的四个后验分布将是：

\[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{2-1}(1-\theta)^{2-1}] = \theta^{82-1} (1-\theta)^{22-1} \end{equation}\]

\[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{3-1}(1-\theta)^{3-1}] = \theta^{83-1} (1-\theta)^{23-1} \end{equation}\]

\[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{6-1}(1-\theta)^{6-1}] = \theta^{86-1} (1-\theta)^{26-1} \end{equation}\]

\[\begin{equation} p(\theta\mid k,n) \propto [\theta^{80} (1-\theta)^{20}] [\theta^{21-1}(1-\theta)^{21-1}] = \theta^{101-1} (1-\theta)^{41-1} \end{equation}\]

我们可以可视化这些先验、似然和后验的三元组；见图 2.3。

![在先验不确定性不同的情况下，beta-二项式共轭示例中的（缩放）似然、先验和后验。似然已缩放以积分到 1，以便更容易比较。](img/e01b7219966e6918cc8688ecd2aa8646.png)

图 2.3：在先验不确定性不同的情况下，beta-二项式共轭示例中的（缩放）似然、先验和后验。似然已缩放以积分到 1，以便更容易比较。

给定一些数据和似然函数，先验越紧，后验就越倾向于先验。一般来说，我们可以就似然-先验-后验关系说以下内容：

+   参数的后验分布是先验和似然性之间的折衷。

+   对于给定的一组数据，先验的确定性越大，后验就会受到先验均值的影响越大。

+   相反，对于给定的一组数据，先验的不确定性越大，后验就会受到似然性的更大影响。

如果我们将样本量（这里指试验次数）从 100 增加到，比如说，1000000，另一个重要的观察结果就会出现。假设我们在这里仍然得到样本均值为 0.8，因此 k=800000。现在，后验均值将几乎完全受到样本均值的影响。这是因为，在我们上面计算的后验的一般形式\(\mathit{Beta}(a+k,b+n-k)\)中，\(n\)和\(k\)相对于\(a\)、\(b\)值变得非常大，并在确定后验均值时占主导地位。

每当我们进行贝叶斯分析时，检查你感兴趣估计的参数是否对先验指定敏感是一个好的做法。这种调查被称为*敏感性分析*。在本书的后面部分，我们将看到许多在现实数据分析设置中的敏感性分析的例子。

### 2.2.7 使用先验知识进行增量知识获取

在上述例子中，我们使用了一个人工示例，其中我们要求 \(100\) 个受试者完成本章开头显示的句子，然后我们计算了他们产生“雨伞”与其他词语作为后续的次数。给定 \(80\) 个“雨伞”实例，并使用 \(\mathit{Beta}(4,4)\) 先验，我们推导出后验为 \(\mathit{Beta}(84,24)\)。现在我们可以使用这个后验作为下一项研究的先验。假设我们进行第二次实验，再次有 \(100\) 个受试者，这次有 \(60\) 个产生了“雨伞”。现在我们可以使用我们新的先验（\(\mathit{Beta}(84,24)\)）来获得更新的后验。我们有 \(a=84, b=24, n=100, k=60\)。这给我们带来了后验：\(\mathit{Beta}(a+k, b+n-k) = \mathit{Beta}(84+60, 24+100-60)=\mathit{Beta}(144, 64)\)。

现在，如果我们把从两个实验中获得的所有数据合并起来，那么我们就会有 \(n=200, k=140\) 的数据。假设我们保持初始先验 \(a=4, b=4\)。那么，我们的后验将是 \(\mathit{Beta}(4+140, 4+200-140)=\mathit{Beta}(144, 64)\)。这正是我们在首次分析前 \(100\) 个受试者的数据时得到的后验，推导出后验，然后将其作为下一个 \(100\) 个受试者数据的先验。

这个玩具示例说明了认知科学中一个具有重要实际意义的重要观点。通过使用先前研究的信息并推导出后验，然后使用该后验作为下一项研究的先验，可以逐步获取关于研究问题的信息。有关如何从先前研究中汇总信息的心理语言学实例，请参阅 Jäger, Engelmann, 和 Vasishth (2017) 以及 Nicenboim, Roettger, 和 Vasishth (2018)。Vasishth 和 Engelmann (2022) 说明了如何将先前研究或研究集合的后验用作先验来计算新数据的后验。这种方法使我们能够从前人的工作中获取的信息中获益。

## 2.3 摘要

在本章中，我们学习了如何在二项似然的具体情况下使用贝叶斯定理，以及在似然函数中对 \(\theta\) 参数使用贝塔先验。在任何贝叶斯分析中，我们的目标都将遵循我们在简单示例中采取的路径：选择一个合适的似然函数，为似然函数中涉及的所有参数选择先验，并使用这个模型（即似然函数和先验）来推导每个参数的后验分布。然后，我们根据参数的后验分布对研究问题进行推断。

在本章讨论的例子中，贝叶斯分析很简单。这是因为我们考虑了 beta-二项分布的简单共轭情况。在现实的数据分析环境中，我们的似然函数将非常复杂，并且将涉及许多参数。乘以似然函数和先验分布将变得数学上困难或不可能。对于这种情况，我们使用计算方法从参数的后验分布中获取样本。

## 2.4 进一步阅读

关于共轭贝叶斯分析的易于理解的介绍有 Lynch (2007) 和 Lunn 等人 (2012)。关于共轭分析的更深入讨论可以在 Lee (2012)，Carlin 和 Louis (2008)，Christensen 等人 (2011)，O’Hagan 和 Forster (2004) 以及 Bernardo 和 Smith (2009) 的作品中找到。

### 参考文献

Bernardo, José M., 和 Adrian F. M. Smith. 2009\. *贝叶斯理论*. 第 405 卷. 约翰·威利与 Sons.

Carlin, Bradley P., 和 Thomas A Louis. 2008\. *贝叶斯数据分析方法*. CRC 压力出版社。

Christensen, Ronald, Wesley Johnson, Adam Branscum, 和 Timothy Hanson. 2011\. “贝叶斯思想和数据分析。” CRC 压力出版社。

Jäger, Lena A., Felix Engelmann, 和 Shravan Vasishth. 2017\. “句子理解中的基于相似性的干扰：文献综述和贝叶斯元分析。” *记忆与语言杂志* 94: 316–39\. [`doi.org/https://doi.org/10.1016/j.jml.2017.01.004`](https://doi.org/https://doi.org/10.1016/j.jml.2017.01.004).

Kruschke, John K. 2014\. *贝叶斯数据分析教程：使用 R、JAGS 和 Stan*. 学术出版社。

Lee, Peter M. 2012\. *贝叶斯统计学：导论*. 约翰·威利与 Sons.

Lunn, David J., Chris Jackson, David J. Spiegelhalter, Nichola G. Best, 和 Andrew Thomas. 2012\. *BUGS 书：贝叶斯分析的实用导论*. 第 98 卷. CRC 压力出版社。

Lynch, Scott Michael. 2007\. *应用贝叶斯统计和社会科学家估计导论*. 纽约，纽约：斯普林格。

Nicenboim, Bruno, Timo B. Roettger, 和 Shravan Vasishth. 2018\. “使用元分析进行证据综合：德国不完全中和的案例。” *语音学杂志* 70: 39–55\. [`doi.org/https://doi.org/10.1016/j.wocn.2018.06.001`](https://doi.org/https://doi.org/10.1016/j.wocn.2018.06.001).

O’Hagan, Anthony, 和 Jonathan J. Forster. 2004\. “肯德尔高级统计学理论，第 2B 卷：贝叶斯推理。” 威利。

Ross, Sheldon. 2002\. *概率论入门*. Pearson 教育。

Vasishth, Shravan, 和 Felix Engelmann. 2022\. *句子理解作为认知过程：计算方法*. 英国剑桥：剑桥大学出版社。 [`books.google.de/books?id=6KZKzgEACAAJ`](https://books.google.de/books?id=6KZKzgEACAAJ).
