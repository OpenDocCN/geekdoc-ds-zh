地图 > 问题定义 > 数据准备 > 数据探索 > 建模 > 评估 > 部署

# 模型评估

模型评估是模型开发过程中的一个重要部分。它有助于找到最能代表我们数据的模型，以及选择的模型在未来的工作效果如何。在数据科学中，使用训练数据来评估模型性能是不可接受的，因为这很容易生成过于乐观和过拟合的模型。在数据科学中有两种评估模型的方法，分别是留出法和交叉验证。为了避免过拟合，这两种方法都使用一个测试集（模型未见过的）来评估模型性能。**留出法**在这种方法中，通常大型数据集会被*随机*分成三个子集：

1.  **训练集**是用于构建预测模型的数据集的子集。

1.  **验证集**是用于评估在训练阶段构建的模型性能的数据集的子集。它为微调模型参数和选择表现最佳的模型提供了一个测试平台。并非所有建模算法都需要验证集。

1.  **测试集**或未见过的示例是用于评估模型未来性能的数据集的子集。如果模型对训练集的拟合要比对测试集的拟合好得多，那么过拟合可能是原因。

**交叉验证**当只有有限的数据可用时，为了获得模型性能的无偏估计，我们使用*k*折交叉验证。在*k*折交叉验证中，我们将数据分成*k*个大小相等的子集。我们构建模型*k*次，每次将一个子集从训练中排除并将其用作测试集。如果*k*等于样本大小，则称为“留一法”。模型评估可以分为两个部分：

+   分类评估

+   回归评估
