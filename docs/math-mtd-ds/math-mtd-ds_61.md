# 8. 神经网络、反向传播和随机梯度下降#

> 原文：[`mmids-textbook.github.io/chap08_nn/00_intro/roch-mmids-nn-intro.html`](https://mmids-textbook.github.io/chap08_nn/00_intro/roch-mmids-nn-intro.html)

我们以现代人工智能（AI）的基本数学构建块介绍结束本书。我们首先推导出链式法则的推广，并对自动微分进行了简要概述。然后，我们在递归函数的背景下描述了反向传播，实现了随机梯度下降（SGD），并将这些方法应用于深度神经网络（特别是多层感知器）。以下是本章主要部分的更详细概述。

*“背景：雅可比矩阵、链式法则以及自动微分简介”* 本节介绍了雅可比矩阵，它将导数的概念推广到多个变量的向量值函数，以及在此设置中组合可微函数的广义链式法则。它还涵盖了有用的矩阵代数，特别是 Hadamard 和 Kronecker 乘积。最后，本节简要介绍了自动微分，这是一种强大的技术，可以有效地计算导数，是现代机器学习的关键，并使用 PyTorch 库说明了其应用。

*“AI 构建块 1：反向传播”* 本节在多层递归函数的背景下发展了自动微分的数学基础，这些函数是具有层特定参数的复合序列。它解释了如何系统地应用链式法则来计算这些函数的梯度。本节对比了两种计算梯度的方法：前向模式和反向模式（也称为反向传播）。虽然前向模式以递归方式同时计算函数及其梯度，但反向模式通常更有效，特别是对于具有许多参数但输出数量较少的函数。反向模式通过本质上计算矩阵-向量乘积而不是矩阵-矩阵乘积来实现这种效率，这使得它特别适合机器学习应用。

*“人工智能构建块 2：随机梯度下降”* 本节讨论了随机梯度下降（SGD），这是一种流行的优化算法，用于训练机器学习模型，尤其是在处理大数据集的场景中。它是梯度下降的一种变体，其中，不是在整个数据集上计算梯度，而是使用随机选择的数据点子集（单个样本或小批量）来估计梯度。SGD 背后的关键思想是，虽然每次更新可能并不完全与真实梯度对齐，但更新的期望方向仍然是下降最快的方向，从而随着时间的推移实现收敛。这种方法在处理大规模数据集时提供了计算上的优势，因为它避免了在每次迭代中计算完整梯度的昂贵计算。本节提供了应用 SGD 与反向传播相结合的详细示例。此外，它还涵盖了使用 PyTorch 的使用，展示了如何处理数据集、构建模型以及使用小批量 SGD 执行优化任务。

*“人工智能构建块 3：神经网络”* 本节介绍了神经网络，特别是关注多层感知器（MLP）架构。它解释了 MLP 的每一层由一个仿射映射后跟一个非线性激活函数组成。在分类任务的设置中，输出层使用 softmax 函数来生成可能的类别的概率分布；用于训练 MLP 的损失函数是交叉熵损失。然后本节详细说明了如何使用链式法则和 Kronecker 积的性质，计算一个小型 MLP 中损失函数相对于权重的梯度。它将这种梯度计算推广到具有任意层数的 MLP。最后，本节展示了如何在 PyTorch 中使用 Fashion-MNIST 数据集实现神经网络的训练。
