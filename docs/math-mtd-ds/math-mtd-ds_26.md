# 4.奇异值分解

> 原文：[`mmids-textbook.github.io/chap04_svd/00_intro/roch-mmids-svd-intro.html`](https://mmids-textbook.github.io/chap04_svd/00_intro/roch-mmids-svd-intro.html)

在本章中，我们介绍了奇异值分解（SVD），这是线性代数中的一个基本概念。它与谱分解相关，但存在于任何矩阵中，包括矩形矩阵。奇异值分解在数据科学中有许多应用，包括主成分分析、低秩逼近、伪逆等。以下是本章主要部分的更详细概述。

*“背景：矩阵秩和谱分解的回顾”* 这一部分涵盖了两个关键的线性代数概念：矩阵秩和谱定理。矩阵的秩定义为它的列空间或行空间的维度。讨论了秩的性质，包括与零空间维度相关的秩-零度定理。谱定理表明，一个实对称矩阵有一个正交归一的特征向量基，具有实特征值，从而可以对矩阵进行谱分解。该部分还根据特征值来描述半正定矩阵和正定矩阵。

*“逼近子空间和奇异值分解（SVD）”* 这一部分介绍了奇异值分解（SVD）作为一种矩阵分解方法，可用于找到一组数据点的最佳低维逼近子空间。该部分展示了通过寻找最佳一维子空间，然后是第一个子空间正交的最佳一维子空间，以此类推，可以贪婪地解决这个问题。然后它正式定义了奇异值分解并证明了对于任何矩阵其存在性。它还讨论了奇异值分解与矩阵的谱分解之间的关系。该部分还讨论了奇异值分解满足的一些重要性质和关系。

*“幂迭代”* 这一部分讨论了用于计算矩阵（截断）奇异值分解的幂迭代方法。幂迭代的关键引理本质上表明，重复乘以 $A^T A$ 与一个随机向量将收敛到 $A$ 的右上奇异向量 $\mathbf{v}_1$。该部分还涵盖了如何使用收敛的 $\mathbf{v}_1$ 来计算相应的奇异值和左奇异向量。

*“应用：主成分分析”* 本节讨论主成分分析（PCA）作为一种降维技术。它解释说 PCA 找到特征的一组线性组合，称为主成分，这些主成分能够捕捉数据中的最大方差。第一个主成分 $t_{i1} = \sum_{j=1}^p \phi_{j1} x_{ij}$ 通过求解 $\max \left\{ \frac{1}{n-1} \|X\boldsymbol{\phi}_1\|² : \|\boldsymbol{\phi}_1\|² = 1\right\}$ 得到，其中 $X$ 是中心化的数据矩阵。后续的主成分通过施加额外的约束得到，特别是与先前组件的不相关性。本节建立了 PCA 与奇异值分解（SVD）之间的联系。

*“奇异值分解的进一步应用：低秩逼近和岭回归”* 本节首先定义了矩阵范数，特别是诱导的 2-范数，并将它们与奇异值联系起来。然后，本节考虑了低秩矩阵逼近，强调了 Eckart-Young 定理，该定理指出，在 Frobenius 范数和诱导的 2-范数下，矩阵的最佳低秩逼近是通过截断 SVD 实现的。最后，本节讨论了岭回归，这是一种正则化技术，通过平衡数据拟合与最小化解的范数来解决过定系统中的多重共线性问题。
