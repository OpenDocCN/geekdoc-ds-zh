# 3. 优化理论与算法

> 原文：[`mmids-textbook.github.io/chap03_opt/00_intro/roch-mmids-opt-intro.html`](https://mmids-textbook.github.io/chap03_opt/00_intro/roch-mmids-opt-intro.html)

在本章中，我们转向优化理论与算法，这是现代数据科学和人工智能的核心。我们推导了基本的最优性条件，包括在凸性存在的情况下。然后，我们介绍了一种基本的优化算法——梯度下降法，并详细分析了其在不同假设下的收敛性理论。我们通过一个重要的监督学习问题来阐述这些概念。以下是本章主要部分的更详细概述。

*“背景：多变量可微函数的回顾”* 本节回顾了多变量可微函数的基本概念。梯度被定义为偏导数向量，它推广了单变量函数导数的概念。本节还介绍了 Hessian 矩阵，它包含二阶偏导数。本节展示了多变量函数的链式法则，并用于证明均值定理的多变量版本。最后，本节提供了计算各种函数（包括仿射函数和二次函数）的梯度和 Hessian 矩阵的示例。

*“最优性条件”* 本节专注于推导无约束连续优化问题的最优性条件。它引入了全局最小值点的概念，即函数在其整个定义域上达到最小值的点。鉴于寻找全局最小值点的困难，本节还定义了局部最小值点，即函数在某个邻域内取最小值的点。本节介绍了基于梯度的第一阶条件和基于 Hessian 矩阵的第二阶条件。本节解释说，虽然梯度在一点为零的第一阶必要条件对于局部最小值点来说并不充分，但涉及满足第一阶条件的点的 Hessian 矩阵正定性的第二阶充分条件保证了严格的局部最小值点。本节最后将讨论扩展到具有等式约束的优化问题，介绍了拉格朗日乘数法。

*“凸性”* 本节介绍了凸性的概念，这在优化问题中起着至关重要的作用。它定义了凸集和函数，并提供了每个的例子和性质。如果一个集合中的任意两点之间的线段完全位于集合内，则该集合是凸的。凸函数的定义类似，但增加了额外的要求，即线段上任意点的函数值小于或等于函数在端点值的加权平均值。本节接着建立了凸性与优化之间的联系，表明对于凸函数，任何局部最小值也是全局最小值。它还基于梯度和海森矩阵提出了凸性的条件。本节最后介绍了强凸性的概念，这保证了全局最小值的存在和唯一性，并提供了如最小二乘目标函数等强凸函数的例子。

*“梯度下降及其收敛性分析”* 本节讨论了梯度下降，这是一种用于寻找连续可微函数最小值的数值优化方法。该方法迭代地沿着负梯度的方向移动，已被证明是最陡下降方向。在关于被最小化函数的不同假设下分析了梯度下降的收敛性。对于光滑函数，当使用适当的步长时，梯度下降会产生一个目标值递减且梯度在极限中消失的点的序列。当函数既是光滑的又是强凸的时，可以获得更快的收敛速度，函数值以指数速度收敛到全局最小值。本节还包括了示例和 Python 实现来阐述这些概念。

*“应用：逻辑回归”* 该部分将梯度下降法应用于逻辑回归，这是一种基于输入特征建模二元结果概率的方法。它推导出逻辑回归目标函数的梯度和海森矩阵，证明目标函数是凸的和光滑的，并提供了实现逻辑回归梯度下降的 Python 代码。该方法应用于示例数据集。
