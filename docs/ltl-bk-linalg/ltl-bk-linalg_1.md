# 图书

> 原文：[`little-book-of.github.io/linear-algebra/books/en-US/book.html`](https://little-book-of.github.io/linear-algebra/books/en-US/book.html)

### 过度调谐

*柔和的开启，邀请进入向量和空间的世界，每一步都开始一段旅程。*

**1. 几何的曙光**

```py
Lines cross in silence,
planes awaken with order,
numbers sketch the world.
```

**2. 学习的邀请**

```py
Steps begin with dots,
arrows stretch into new paths,
the journey unfolds.
```

**3. 光与影**

```py
Shadows fall on grids,
hidden shapes emerge in form,
clarity takes root.
```

**4. 结构的种子**

```py
One point, then a line,
spaces blossom out from rules,
infinity grows.
```

**5. 代数的低语**

```py
Silent rules of space,
woven threads of thought align,
order sings through time.
```

**6. 欢迎初学者**

```py
Empty page awaits,
axes cross like guiding hands,
first steps find their place.
```

**7. 永恒之路**

```py
From vectors to stars,
equations trace destiny,
patterns guide our sight.
```

## 第一章 向量、标量和几何

#### 开篇

```py
Arrows in the air,
directions whisper softly—
the plane comes alive.
```

### 1. 标量、向量和坐标系

当我们开始学习线性代数时，一切从最简单的构建块开始：标量和向量。标量只是一个单一的数字，比如 3，-7，或者π。它只携带大小，没有方向。标量是我们用来计数、测量长度或调整其他对象大小和形状的东西。相比之下，向量是一组有序的数字。你可以想象它是一个指向空间中某个地方的箭头，或者简单地想象一个像（2, 5）在二维或（1, -3, 4）在三维中的列表。标量衡量“多少”，而向量衡量“多少”和“哪个方向”。

#### 坐标系

要谈论向量，我们需要一个坐标系。想象在一张纸上放置两条垂直的轴：x 轴（从左到右）和 y 轴（从上到下）。纸上的每个点都可以用两个数字来描述：沿着 x 轴有多远，沿着 y 轴有多远。这对数字是一个二维向量。添加一个从页面向上指的 z 轴，你就有了三维空间。每个坐标系都给我们提供了一种用数值描述向量的方法，尽管底层“空间”是相同的。

#### 可视化标量和向量

+   标量就像尺子上一个单独的刻度。

+   向量就像一个从原点（0, 0, …）开始并以其分量定义的点结束的箭头。例如，二维中的向量（3, 4）从原点指向 x 轴上 3 个单位，y 轴上 4 个单位。

#### 为什么从这里开始？

理解标量和向量之间的区别是线性代数中其他一切的基础。每个概念——矩阵、线性变换、特征值——最终都归结为我们如何操作向量和用标量缩放它们。没有这种区分，这个主题的其他部分将没有根基。

#### 为什么这很重要

几乎所有科学和工程领域都依赖于这个概念。物理学使用向量表示速度、加速度和力。计算机图形学使用它们来表示点、颜色和变换。数据科学将整个数据集视为高维向量。通过早期掌握标量和向量，你就能解锁现代科学和技术所使用的语言。

#### 尝试自己动手

1.  在一张纸上画一个 x 轴和 y 轴。画出向量（2, 3）。

1.  现在画出向量（-1, 4）。比较它们的方向和长度。

1.  思考：这两个向量中哪一个指向“更向上”？哪一个“更长”？

这些简单的实验已经给你提供了在线性代数中反复进行的操作的直觉。

### 2\. 向量符号、分量和箭头

线性代数为我们提供了强大的描述和操作向量的方法，但在我们能够对它们进行任何操作之前，我们需要一个精确的符号系统。符号不仅仅是装饰性的——它告诉我们如何清晰、明确地阅读、书写和思考向量。在本节中，我们将探讨向量是如何书写的，它们的分量是如何表示的，以及我们如何将它们作为箭头直观地解释。

#### 写向量

向量通常用粗体小写字母表示（如 $\mathbf{v}, \mathbf{w}, \mathbf{x}$）

或者用箭头标示（如 $\vec{v}$）。

例如，向量 $\mathbf{v} = (2, 5)$ 与 $\vec{v} = (2, 5)$ 相同。

风格取决于上下文：数学家通常使用粗体，物理学家通常使用箭头。

在手写笔记中，人们有时会在向量下划线（例如，$\underline{v}$）以避免与标量混淆。

重要的是要一眼就能区分向量和标量。

#### 向量的分量

二维空间中的向量有两个分量，表示为 $(x, y)$。

在三维空间中，它有三个分量：$(x, y, z)$。

更一般地，一个 $n$ 维向量有 $n$ 个分量：$(v_1, v_2, \ldots, v_n)$。

每个分量告诉我们向量在坐标系统的一个轴上延伸多远。

例如：

+   $\mathbf{v} = (3, 4)$ 表示向量在 $x$ 轴上延伸 3 个单位，在 $y$ 轴上延伸 4 个单位。

+   $\mathbf{w} = (-2, 0, 5)$ 表示向量在 $x$ 轴上延伸 $-2$ 个单位，在 $y$ 轴上延伸 $0$ 个单位，在 $z$ 轴上延伸 5 个单位。

我们通常将向量 $\mathbf{v}$ 的第 $i$ 个分量称为 $v_i$。

因此，对于 $\mathbf{v} = (3, 4, 5)$，我们有 $v_1 = 3$，$v_2 = 4$，$v_3 = 5$。

#### 列向量与行向量

向量可以用两种常见的方式书写：

+   作为行向量：$(v_1, v_2, v_3)$

+   作为列向量：

    $$ \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} $$

这两种表示都代表同一个抽象对象。

行向量便于快速书写，而列向量在开始与矩阵相乘时是必不可少的，因为维度必须对齐。

#### 向量作为箭头

最直观地想象向量的方式是将其视为箭头：

+   它从原点（0, 0, …）开始。

+   它的终点由其分量给出。

例如，二维空间中的向量 $(2, 3)$ 被绘制成从 $(0, 0)$ 到 $(2, 3)$ 的箭头。箭头既有方向（它指向哪里）也有大小（它的长度）。这幅几何图使得抽象的代数操作更容易理解。

#### 位置向量与自由向量

向量有两种常见的解释：

1.  位置向量——一个从原点指向空间中特定点的向量。例如：（2, 3）是点（2, 3）的位置向量。

1.  自由向量——一个具有长度和方向的箭头，但与特定的起点无关。例如，长度为 5，指向东北方向的箭头可以画在任何地方，但它仍然代表同一个向量。

在线性代数中，我们通常将向量视为自由向量，因为它们的意义不依赖于它们被画在哪里。

#### 示例：读取一个向量

假设 $\mathbf{u} = (–3, 2)$。

+   第一个分量（-3）意味着沿着 x 轴向左移动 3 个单位。

+   第二个分量（2）意味着沿着 y 轴向上移动 2 个单位。所以箭头指向点（-3, 2）。即使没有图表，分量也能确切地告诉我们箭头会是什么样子。

#### 为什么这很重要

清晰的符号是线性代数的骨架。没有它，方程很快就会变得难以阅读，关于方向和大小的直觉也会丢失。我们写向量的方式决定了我们如何容易地将代数（数字和符号）与几何（箭头和空间）联系起来。这种双重视角——符号和视觉——是线性代数强大和实用的原因。

#### 尝试自己来做

1.  写下向量（4, –1）。在坐标纸上画出它。

1.  将相同的向量重写为列向量。

1.  将向量（4, –1）通过将其起点移动到（2, 3）而不是原点来平移。注意箭头看起来是一样的——它只是从别处开始。

1.  对于一个更具挑战性的任务：画出三维向量（2, –1, 3）。即使你无法在三维空间中完美地绘制，也尽量展示每个分量沿 x、y 和 z 轴的情况。

通过练习符号和箭头图，你将能够熟练地在抽象符号和具体可视化之间切换。这种技能将使线性代数中的每一个后续概念都更加直观。

### 3. 向量加法和数乘

一旦我们学会了如何用分量和箭头来描述向量，下一步就是学习如何将它们组合起来。两个基本操作构成了线性代数的骨架：向量的加法和向量的数乘（标量乘法）。这两个动作虽然简单，但产生了我们后来构建的所有其他内容。有了它们，我们可以描述运动、力、数据变换等等。

#### 坐标系中的向量加法

假设我们在二维空间中有两个向量：

$\mathbf{u} = (u_1, u_2), \quad \mathbf{v} = (v_1, v_2)$.

它们的和定义为：

$$ \mathbf{u} + \mathbf{v} = (u_1 + v_1, \; u_2 + v_2). $$

用话来说，你加对应分量。

这在更高维的情况下也适用：

$$ (u_1, u_2, \ldots, u_n) + (v_1, v_2, \ldots, v_n) = (u_1 + v_1, \; u_2 + v_2, \; \ldots, \; u_n + v_n). $$

示例：$$ (2, 3) + (-1, 4) = (2 - 1, \; 3 + 4) = (1, 7). $$

#### 向量加法作为几何

几何图像更加直观。如果你画出向量 $\mathbf{u}$ 作为一条箭头，然后将向量 $\mathbf{v}$ 的尾端放在 $\mathbf{u}$ 的箭头处，从 $\mathbf{u}$ 的起点到 $\mathbf{v}$ 的箭头末端的箭头就是 $\mathbf{u} + \mathbf{v}$。这被称为首尾相接法则。平行四边形法则是一种另一种可视化方法：将 $\mathbf{u}$ 和 $\mathbf{v}$ 尾尾相接，形成一个平行四边形，对角线就是它们的和。

示例：u = (3, 1), v = (2, 2)。从原点画出这两个向量。它们的和 (5, 3) 正好是它们所围成的平行四边形的对角线。

#### 坐标中的标量乘法

标量可以拉伸或缩短向量。

如果 $\mathbf{u} = (u_1, u_2, \ldots, u_n)$ 且 $c$ 是一个标量，那么：

$$ c \cdot \mathbf{u} = (c \cdot u_1, \; c \cdot u_2, \; \ldots, \; c \cdot u_n). $$

示例：

$$ 2 \cdot (3, 4) = (6, 8). $$

$$ (-1) \cdot (3, 4) = (-3, -4). $$

乘以正标量会拉伸或压缩箭头，同时保持方向不变。乘以负标量会将箭头翻转，使其指向相反方向。

#### 标量乘法作为几何

想象向量 (1, 2)。在坐标纸上画出它：它向右移动 1，向上移动 2。现在将它乘以 2：(2, 4)。箭头指向同一个方向，但长度是原来的两倍。将它除以 2：(0.5, 1)。方向相同，但更短。取它的相反数：(–1, –2)。现在箭头指向相反方向。

这个几何图形解释了为什么我们称这些数为“标量”：它们可以缩放向量。

#### 结合两者：线性组合

向量加法和数乘不仅仅是单独的技巧——它们结合形成了线性代数的心脏：线性组合。

向量 $u$ 和 $v$ 的线性组合是任何形式为

$a \cdot u + b \cdot v$，其中 $a$ 和 $b$ 是标量。

示例：

如果 $u = (1, 0)$ 和 $v = (0, 1)$，那么

$3 \cdot u + 2 \cdot v = (3, 2)$。

这显示了如何通过缩放和添加这两个基本向量来到达网格上的任何一点。

这就是构建空间的核心。

#### 代数性质

向量加法和数乘遵循与数字算术相似的规则：

+   交换律：$u + v = v + u$

+   结合律：$(u + v) + w = u + (v + w)$

+   对标量的分配律：$c \cdot (u + v) = c \cdot u + c \cdot v$

+   对数字的分配律：$(a + b) \cdot u = a \cdot u + b \cdot u$

这些规则并非微不足道的簿记——它们保证了线性代数的可预测性，

这就是为什么它作为科学语言发挥作用的原因。

#### 为什么这很重要

只用这两个运算——加法和缩放——你就可以描述直线、平面以及整个空间。任何通过组合影响而增长的系统，如物理学、经济学或机器学习，都是建立在这些简单规则之上的。稍后，当我们定义矩阵乘法、点积和特征值时，它们都归结为重复添加和缩放向量的模式。

#### 尝试自己来做

1.  将 (2, 3) 和 (–1, 4) 相加。在坐标纸上画出结果。

1.  将向量 (1, –2) 乘以 3，然后加上 (0, 5)。最终向量是什么？

1.  对于一个更深的挑战：设 u = (1, 2) 和 v = (2, –1)。对于 –2 和 2 之间的整数 a, b，画出形式为 a·u + b·v 的所有向量。注意你创建的点阵——这就是这两个向量的张成。

这种简单的练习展示了如何通过加法和缩放组合两个基本向量，从而生成一个整个结构化的空间，这是线性代数真正力量的第一瞥。

### 4\. 线性组合和生成空间

在学会了加向量并缩放它们之后，自然而然的下一个问题就是：*我们能从这两个操作中构建什么？* 答案是线性组合的概念，这直接引出了线性代数中最基本的思想之一：向量集的生成空间。这些思想不仅告诉我们单个向量能做什么，还告诉我们向量组如何塑造整个空间。

#### 什么是线性组合？

线性组合是由标量乘以向量然后相加得到的任何向量。

形式上，给定向量 $v_1, v_2, \ldots, v_k$ 和标量 $a_1, a_2, \ldots, a_k$，线性组合看起来像：

$$ a_1 \cdot v_1 + a_2 \cdot v_2 + \cdots + a_k \cdot v_k. $$

这不过是重复加法和缩放，但这个想法很强大，因为它描述了向量如何组合生成新的向量。

示例：

设 $u = (1, 0)$ 和 $v = (0, 1)$。那么任何线性组合 $a \cdot u + b \cdot v = (a, b)$。

这表明二维平面上的每个点都可以表示为这两个简单向量的线性组合。

#### 几何意义

线性组合涉及混合方向和大小。每个向量就像一个“方向成分”，而标量控制你使用每种成分的多少。

+   有一个向量：你只能到达通过原点的单一直线上的点。

+   在二维中，有两个非平行向量：你可以到达平面上的每个点。

+   在三维中，有三个非共面的向量：你可以到达整个三维空间。

这个进展表明，线性组合的力量不仅取决于向量本身，还取决于它们之间的关系。

#### 向量集的生成空间

向量集的生成空间是它们所有可能的线性组合的集合。

它回答了问题：“*这些向量生成什么空间？*”

符号：

$$ \text{Span}\{v_1, v_2, \ldots, v_k\} = \{a_1 v_1 + a_2 v_2 + \cdots + a_k v_k \;|\; a_i \in \mathbb{R}\}. $$

示例：

+   $\text{Span}\{(1, 0)\}$ = 所有 $(1, 0)$ 的倍数，即 $x$ 轴。

+   $\text{Span}\{(1, 0), (0, 1)\}$ = 整个 $\mathbb{R}²$，整个平面。

+   $\text{Span}\{(1, 2), (2, 4)\}$ = 只是通过 $(1, 2)$ 的直线，因为第二个向量是第一个向量的倍数。

因此，生成空间在很大程度上取决于向量是否添加了新的方向，或者只是重复了已经存在的内容。

#### 平行和非独立向量

如果向量指向相同或相反的方向（一个是另一个的标量倍数），那么它们的生成空间只是一个直线。它们没有增加任何新的空间覆盖。但如果它们指向不同的方向，它们就会开辟新的维度。这引出了线性独立性的关键思想，我们将在后面探讨：如果没有任何一个向量是其他向量的线性组合，则向量是独立的。

#### 在不同维度中可视化生成空间

+   在二维中：

    +   一个向量可以覆盖一条直线。

    +   两个独立向量可以覆盖整个平面。

+   在三维中：

    +   一个向量可以覆盖一条直线。

    +   两个线性无关的向量张成一个平面。

    +   三个线性无关的向量张成整个三维空间。

+   在更高维中：相同的模式继续。一组 k 个线性无关的向量在较大空间内张成一个 k 维子空间。

#### 代数性质

+   向量的张成总是包括零向量，因为你可以选择所有标量 = 0。

+   张成始终是一个子空间，这意味着它在加法和数乘下是封闭的。如果你在张成中添加两个向量，结果仍然在张成中。

+   当你添加新的线性无关向量时，张成会增长，但如果新向量只是旧向量的组合，则不会增长。

#### 为什么它很重要

线性组合和张成是线性代数中几乎所有其他内容的基础：

+   它定义了向量独立或依赖的含义。

+   它们是解决线性系统（解通常描述为张成）的基础。

+   它们解释了向量空间中维度是如何出现的。

+   它们是主成分分析等实际方法的基础，其中数据被投影到几个重要向量的张成上。

简而言之，张成告诉我们一组向量的“范围”，而线性组合是探索这种范围的方式。

#### 尝试自己操作

1.  考虑向量 (1, 0) 和 (0, 1)。写下三种不同的线性组合并绘制它们。你注意到什么形状？

1.  尝试向量 (1, 2) 和 (2, 4)。写下三种不同的线性组合。绘制它们。与上一个案例有什么不同？

1.  在三维空间中，考虑 (1, 0, 0) 和 (0, 1, 0)。描述它们的张成。加上 (0, 0, 1)。张成如何变化？

1.  挑战：选择向量 (1, 2, 3) 和 (4, 5, 6)。它们张成的是平面还是整个三维空间？你怎么判断？

通过实验简单的例子，你会清楚地看到张成的概念如何捕捉向量组合的丰富性或局限性。

### 5. 长度（范数）和距离

到目前为止，向量是有方向和分量的箭头。为了更有意义地比较它们，我们需要谈论它们有多长以及它们有多远。这些概念通过向量的范数（其长度）和向量之间的距离来形式化。这些概念将分量代数和空间几何联系在一起。

#### 向量的长度（范数）

向量的范数衡量其大小，或者说箭头有多长。

对于 $n$-维空间中的向量 $v = (v_1, v_2, \ldots, v_n)$，其范数定义为：

$$ \|v\| = \sqrt{v_1² + v_2² + \cdots + v_n²}. $$

这个公式直接来自勾股定理：斜边的长度等于两腿平方和的平方根。

在二维中，这是原点到一点的熟悉距离公式。

例子：

+   对于 $v = (3, 4)$：

    $$ \|v\| = \sqrt{3² + 4²} = \sqrt{9 + 16} = 5. $$

+   对于 $w = (1, -2, 2)$：

    $$ \|w\| = \sqrt{1² + (-2)² + 2²} = \sqrt{1 + 4 + 4} = \sqrt{9} = 3. $$

#### 单位向量

单位向量是一个长度正好为 1 的向量。

这些很重要，因为它们捕捉了方向而没有缩放。

要从任何非零向量创建一个单位向量，除以其范数：

$$ u = \frac{v}{\|v\|}. $$

例子：

对于 $v = (3, 4)$，单位向量是

$$ u = \left(\tfrac{3}{5}, \tfrac{4}{5}\right). $$

它与 $(3, 4)$ 方向相同，但长度为 1。

单位向量就像纯方向。

它们在投影、定义坐标系和归一化数据方面特别有用。

#### 向量之间的距离

两个向量 $u$ 和 $v$ 之间的距离定义为它们差值的长度：

$$ \text{dist}(u, v) = \|u - v\|. $$

例子：

令 $u = (2, 1)$ 和 $v = (5, 5)$。然后

$$ u - v = (-3, -4). $$

它的范数是

$$ \sqrt{(-3)² + (-4)²} = \sqrt{9 + 16} = 5. $$

因此，距离是 5。这符合我们的直觉：点 $(2, 1)$ 和 $(5, 5)$ 之间的直线距离。

#### 几何解释

+   范数告诉你一个点到原点的距离。

+   距离告诉你两个点之间的距离。

这两个都是使用相同的公式——平方和的平方根——但在稍微不同的上下文中应用。

#### 不同类型的范数

上述公式定义了欧几里得范数（或 $\ell_2$ 范数），这是最常见的一种。

但在线性代数中，其他范数也很有用：

+   $\ell_1$ 范数：

    $$ \|v\|_1 = |v_1| + |v_2| + \cdots + |v_n| $$

    （绝对值之和）。

+   $\ell_\infty$ 范数：

    $$ \|v\|_\infty = \max(|v_1|, |v_2|, \ldots, |v_n|) $$

    （最大分量）。

这些范数改变了“长度”和“距离”的几何形状。例如，在 $\ell_1$ 范数中，单位圆的形状像钻石；在 $\ell_\infty$ 范数中，它看起来像正方形。

#### 代数性质

范数和距离满足使它们成为一致度量的关键性质：

+   非负性：$\|v\| \geq 0$，并且只有当 $v = 0$ 时，$\|v\| = 0$。

+   同质性：$\|c \cdot v\| = |c| \, \|v\|$（缩放影响长度可预测）。

+   三角不等式：$\|u + v\| \leq \|u\| + \|v\|$（最短路径是直接路径）。

+   对称性（对于距离）：$\text{dist}(u, v) = \text{dist}(v, u)$。

正是这些性质使得范数和距离成为数学中稳健的工具。

#### 为什么这很重要

理解长度和距离是向更高维几何迈出的第一步。这些概念：

+   允许我们定量地比较向量。

+   构成角度、正交性和投影等概念的基础。

+   为优化问题（例如，“找到最近的向量”）奠定基础，这在机器学习中至关重要。

+   定义空间的几何形状，这取决于你使用的范数。

#### 尝试自己操作

1.  计算向量 (6, 8) 的范数。然后除以范数以找到其单位向量。

1.  找到点 (1, 1, 1) 和 (4, 5, 6) 之间的距离。

1.  比较点 (0, 0) 和 (3, 4) 之间的欧几里得距离和曼哈顿（$\ell_1$）距离。如果你沿着城市网格行走，哪一个更符合你的直觉？

1.  挑战：对于向量 u = (2, –1, 3) 和 v = (–2, 0, 1)，计算 $\|u – v\|$。然后解释这个距离在几何上的意义。

通过解决这些例子，你会看到范数和距离如何使抽象向量感觉像日常生活中可以测量的点和箭头一样真实。

### 6\. 点积（代数和几何视角）

点积是线性代数中最基本的运算之一。它看起来像是一个简单的公式，但它解锁了测量角度、检测正交性、将一个向量投影到另一个向量以及计算物理学中的能量或功的能力。理解它需要看到代数视图（分量上的公式）和几何视图（比较方向的方法）。

#### 代数定义

对于相同维度的两个向量，$u = (u_1, u_2, \ldots, u_n)$ 和 $v = (v_1, v_2, \ldots, v_n)$，点积定义为：

$$ u \cdot v = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n. $$

这只是将对应分量相乘并求和的结果。

示例：

+   $(2, 3) \cdot (4, 5) = (2 \times 4) + (3 \times 5) = 8 + 15 = 23$

+   $(1, -2, 3) \cdot (0, 4, -1) = (1 \times 0) + (-2 \times 4) + (3 \times -1) = 0 - 8 - 3 = -11$

注意到点积始终是一个标量，而不是一个向量。

#### 几何定义

点积也可以用向量长度和角度来定义：

$$ u \cdot v = \|u\| \, \|v\| \cos(\theta), $$

其中 $\theta$ 是 $u$ 和 $v$ 之间的角度（$0^\circ \leq \theta \leq 180^\circ$）。

这个公式告诉我们：

+   如果角度是锐角（小于 $90^\circ$），$\cos(\theta) > 0$，所以点积是正的。

+   如果角度是直角（正好 $90^\circ$），$\cos(\theta) = 0$，所以点积是 0。

+   如果角度是钝角（大于 $90^\circ$），$\cos(\theta) < 0$，所以点积是负的。

因此，点积的符号编码了方向对齐。

#### 连接两个定义

乍一看，代数乘积的和和几何长度-角度公式似乎无关。但它们是等价的。为了了解为什么，考虑将余弦定理应用于由 u、v 和 u – v 形成的三角形。展开两边将直接导致两个公式之间的等价性。这种双重解释是点积如此强大的原因：它既是计算规则也是几何测量。

#### 正交性

如果两个向量的点积为零，则这两个向量（垂直）是正交的：

$$ u \cdot v = 0 \;\;\Longleftrightarrow\;\; \theta = 90^\circ. $$

这为我们提供了一种不需要画图就可以检查垂直性的代数方法。

示例：

$(2, 1) \cdot (-1, 2) = (2 \times -1) + (1 \times 2) = -2 + 2 = 0$,

因此，这两个向量是正交的。

#### 投影

点积也提供了一种将一个向量投影到另一个向量的方法。

$u$ 在 $v$ 上的标量投影是：

$$ \text{proj}_{\text{标量}}(u \text{ 投影到 } v) = \frac{u \cdot v}{\|v\|}. $$

然后向量投影是：

$$ \text{proj}_{\text{向量}}(u \text{ 投影到 } v) = \frac{u \cdot v}{\|v\|²} \, v. $$

这使我们能够将向量分解为“平行”和“垂直”分量，这在几何学、物理学和数据分析中至关重要。

#### 例子

1.  计算 $u = (3, 4)$ 和 $v = (4, 3)$。

    +   点积：$(3 \times 4) + (4 \times 3) = 12 + 12 = 24$。

    +   范数：$\|u\| = 5$，$\|v\| = 5$。

    +   $\cos(\theta) = \tfrac{24}{5 \times 5} = \tfrac{24}{25} \approx 0.96$，所以 $\theta \approx 16^\circ$。

        这些向量几乎平行。

1.  计算 $u = (1, 2, -1)$ 和 $v = (2, -1, 1)$。

    +   点积：$(1 \times 2) + (2 \times -1) + (-1 \times 1) = 2 - 2 - 1 = -1$。

    +   范数：$\|u\| = \sqrt{6}$，$\|v\| = \sqrt{6}$。

    +   $\cos(\theta) = \tfrac{-1}{\sqrt{6} \times \sqrt{6}} = -\tfrac{1}{6}$，所以 $\theta \approx 99.6^\circ$。

        略显晦涩。

#### 物理解释

在物理学中，点积计算功：

$$ \text{功} = \text{力} \cdot \text{位移} = \|\text{力}\| \, \|\text{位移}\| \cos(\theta). $$

只有力的运动方向的分量起作用。如果你在试图水平移动一个盒子时垂直向下推，点积为零：在运动方向上没有做功。

#### 代数性质

+   交换律：$u \cdot v = v \cdot u$

+   分配律：$u \cdot (v + w) = u \cdot v + u \cdot w$

+   标量兼容性：$(c \cdot u) \cdot v = c \,(u \cdot v)$

+   非负性：$v \cdot v = \|v\|² \geq 0$

这些保证了点积的行为一致，并与向量空间的结构相匹配。

#### 它的重要性

点积是代数和几何之间的第一座桥梁。它：

+   在高维空间中定义角度和正交性。

+   权威地投影和分解，这是最小二乘法、回归和数据拟合的基础。

+   在物理学中表现为能量、功率和功。

+   是许多机器学习方法的核（例如，高维空间中的相似度度量）。

没有点积，线性代数将缺乏将数字与几何和意义联系起来的方法。

#### 尝试自己动手做

1.  计算 (2, –1) · (–3, 4)。然后找出它们之间的角度。

1.  检查 (1, 2, 3) 和 (2, 4, 6) 是否正交。点积告诉你什么？

1.  找出 (3, 1) 在 (1, 2) 上的投影。画出原始向量、投影和垂直分量。

1.  用物理术语来说：假设一个 10 N 的力以 60°的角度作用于运动方向，位移是 5 m。做了多少功？

这些练习揭示了点积的双重力量：作为计算公式和作为几何解释工具。

### 7. 向量之间的角度和余弦

定义了点积后，我们现在可以测量向量之间的角度。在日常生活中，角度告诉我们两条线或方向之间的关系——它们是否指向同一方向，是否垂直，或者是否相反。在线性代数中，点积和余弦函数为我们提供了一种精确、可推广的方法来定义任何维度上的角度，而不仅仅是 2D 或 3D。本节探讨了我们是如何计算、解释和应用向量角度的。

#### 向量之间角度的定义

对于两个非零向量 $u$ 和 $v$，它们之间的角度 $\theta$ 定义为：

$$ \cos(\theta) = \frac{u \cdot v}{\|u\| \, \|v\|}. $$

这个公式直接来自点积的几何定义。

重新排列给出：

$$ \theta = \arccos\!\left(\frac{u \cdot v}{\|u\| \, \|v\|}\right). $$

关键点：

+   $\theta$ 总是在 $0^\circ$ 和 $180^\circ$（或 $0$ 和 $\pi$ 弧度）之间。

+   分母通过除以长度的乘积来归一化点积，因此结果是无量纲的，并且始终介于 $-1$ 和 $1$ 之间。

+   余弦值直接编码了对齐：正的、零的或负的。

#### 余弦值的解释

余弦告诉我们方向关系：

+   $\cos(\theta) = 1 \;\;\Rightarrow\;\; \theta = 0^\circ$ → 向量指向完全相同的方向。

+   $\cos(\theta) = 0 \;\;\Rightarrow\;\; \theta = 90^\circ$ → 向量是正交的（垂直的）。

+   $\cos(\theta) = -1 \;\;\Rightarrow\;\; \theta = 180^\circ$ → 向量指向完全相反的方向。

+   $\cos(\theta) > 0$ → 锐角 → 向量指向更“在一起”而不是分开。

+   $\cos(\theta) < 0$ → 钝角 → 向量指向更“相对”。

因此，余弦将几何对齐压缩成一个单一的数字。

#### 示例

1.  $u = (1, 0), \; v = (0, 1)$

    +   点积：$1 \times 0 + 0 \times 1 = 0$

    +   范数：$1$ 和 $1$

    +   $\cos(\theta) = 0 \;\Rightarrow\; \theta = 90^\circ$

        向量是垂直的，正如预期的那样。

1.  $u = (2, 3), \; v = (4, 6)$

    +   点积：$(2 \times 4) + (3 \times 6) = 8 + 18 = 26$

    +   范数：$\sqrt{2² + 3²} = \sqrt{13}$，和 $\sqrt{4² + 6²} = \sqrt{52} = 2\sqrt{13}$

    +   $\cos(\theta) = \tfrac{26}{\sqrt{13} \cdot 2\sqrt{13}} = \tfrac{26}{26} = 1$

    +   $\theta = 0^\circ$

        这些向量是倍数，因此它们完美对齐。

1.  $u = (1, 1), \; v = (-1, 1)$

    +   点积：$(1 \times -1) + (1 \times 1) = -1 + 1 = 0$

    +   $\cos(\theta) = 0 \;\Rightarrow\; \theta = 90^\circ$

        向量是垂直的，形成正方形的对角线。

#### 高维角度

公式的美妙之处在于它在任何维度上都能工作。

即使在 $\mathbb{R}^{100}$ 或更高维中，我们也可以仅使用两个向量的点积和范数来定义它们之间的角度。

虽然我们无法直接在高度维度中可视化几何形状，但余弦公式仍然捕捉了两个方向的对齐程度：

$$ \cos(\theta) = \frac{u \cdot v}{\|u\| \, \|v\|}. $$

这在机器学习中至关重要，因为数据通常存在于非常高的维空间中。

#### 余弦相似度

两个向量之间角度的余弦通常称为余弦相似度。它在数据分析机器学习中广泛使用，以衡量两个数据向量在大小无关的情况下有多相似。

+   在文本挖掘中，文档被转换为词频向量。余弦相似度衡量两个文档在主题上的“接近程度”，无论它们的长度如何。

+   在推荐系统中，余弦相似度比较用户偏好向量以建议相似的用户或项目。

这展示了如何一个几何概念远远超出纯数学的范畴。

#### 重新审视正交性

角度公式加强了正交性的特殊作用。

如果 $\cos(\theta) = 0$，那么 $u \cdot v = 0$。

这意味着点积不仅计算长度，还直接测试正交性。

这个代数捷径比手动检查几何直角要容易得多。

#### 角度和投影

角度与投影紧密相关。

向量 $u$ 投影到 $v$ 的长度是 $\|u\|\cos(\theta)$。

如果角度很小，则投影很大——$u$ 的大部分位于 $v$ 的方向上。

如果角度接近 $90^\circ$，则投影会缩小到零。

因此，余弦在方向之间充当缩放因子。

#### 为什么这很重要

向量之间的角度提供：

+   一种将几何推广到 2D/3D 以外的通用方法。

+   高维数据中相似度的度量。

+   正交性、投影和空间分解的基础。

+   优化工具：例如，在梯度下降中，梯度与步进方向之间的角度决定了我们减少误差的有效性。

如果没有测量角度的能力，我们就无法将代数操作与几何直觉或实际应用联系起来。

#### 尝试自己操作

1.  计算向量 (2, 1) 和 (1, –1) 之间的角度。解释结果。

1.  找到两个在三维空间中形成 60° 角的向量。使用余弦公式进行验证。

1.  考虑机器学习模型中“猫”和“狗”的词向量。为什么余弦相似度可能比欧几里得距离更好地衡量相似度？

1.  挑战：在 $\mathbb{R}³$ 中，找到一个与 (1, 2, 3) 和 (3, 2, 1) 都正交的向量。它与每个向量形成什么角度？

通过实验这些问题，你会看到角度如何在线性代数中提供代数公式和几何意义之间的缺失环节。

### 8. 投影和分解

在前面的章节中，我们看到了点积如何衡量对齐以及余弦公式如何给出向量之间的角度。下一步自然的步骤是使用这些工具将一个向量投影到另一个向量上。投影是将一个向量“影子”投射到另一个向量的方向上的方法，将向量分解成有意义的部分：一个沿着给定方向，一个垂直于它。这是分解的本质，它在线性代数、几何、物理和数据科学中无处不在。

#### 标量投影

向量 $u$ 投影到向量 $v$ 的标量投影衡量 $u$ 在 $v$ 方向上的分量。它由以下公式给出：

$$ \text{proj}_{\text{scalar}}(u \text{ onto } v) = \frac{u \cdot v}{\|v\|}. $$

+   如果这个值是正的，$u$ 有一个分量指向与 $v$ 相同的方向。

+   如果它是负的，$u$ 部分指向相反方向。

+   如果它是零，$u$ 完全垂直于 $v$。

示例：

$u = (3, 4)$，$v = (1, 0)$。

点积：$(3 \times 1 + 4 \times 0) = 3$.

$\|v\| = 1$.

因此，标量投影是 $3$。这告诉我们 $u$ 在 $x$ 轴上有一个长度为 $3$ 的“影子”。

#### 向量投影

向量投影给出了与这个标量量相对应的 $v$ 方向上的实际箭头：

$$ \text{proj}_{\text{vector}}(u \text{ onto } v) = \frac{u \cdot v}{\|v\|²} \, v. $$

这个公式将 $v$ 标准化为单位向量，然后通过标量投影进行缩放。

结果是一个新向量，沿着 $v$ 方向，精确地捕捉了 $u$ 的“平行”部分。

示例：

$u = (3, 4)$，$v = (1, 2)$

+   点积：$3 \times 1 + 4 \times 2 = 3 + 8 = 11$

+   $v$ 的范数的平方：$(1² + 2²) = 5$

+   系数：$11 / 5 = 2.2$

+   投影向量：$2.2 \cdot (1, 2) = (2.2, 4.4)$

所以 $(3, 4)$ 在 $(1, 2)$ 方向上的部分是 $(2.2, 4.4)$。

#### 垂直分量

一旦我们有了投影，我们可以通过减法简单地找到垂直分量（通常称为拒绝）：

$$ u_{\perp} = u - \text{proj}_{\text{vector}}(u \text{ onto } v). $$

这给出了 $u$ 中完全垂直于 $v$ 的部分。

示例继续：

$u_{\perp} = (3, 4) - (2.2, 4.4) = (0.8, -0.4)$

检查：

$(0.8, -0.4) \cdot (1, 2) = 0.8 \times 1 + (-0.4) \times 2 = 0.8 - 0.8 = 0$.

的确，正交。

#### 几何图像

投影就像从一个向量垂直地投射到另一个向量上。想象一下垂直于 $v$ 的光线：$u$ 在 $v$ 张成的线上的影子就是投影。这种可视化解释了为什么投影自然地将向量分成两部分：

+   平行部分：沿着 $v$ 的线。

+   垂直部分：垂直于 $v$，形成一个直角。

这两个部分结合起来可以精确地重建原始向量。

#### 向量的分解

每个向量 $u$ 都可以相对于另一个向量 $v$ 分解成两个部分：

$$ u = \text{proj}_{\text{vector}}(u \text{ onto } v) + \big(u - \text{proj}_{\text{vector}}(u \text{ onto } v)\big). $$

这种分解是唯一的，并且具有几何意义。

它推广到子空间：我们可以投影到整个平面或更高维的跨度，将向量分成“子空间内”的部分和“垂直于子空间”的部分。

#### 应用

1.  物理（功和力）：功是力在位移上的投影。只有力的运动方向上的部分才贡献。例如：推着雪橇侧向移动会浪费力气——侧向分量投影为零。

1.  几何和工程：投影在 CAD（计算机辅助设计）中用于将 3D 对象展平到 2D 表面，如蓝图或阴影。

1.  计算机图形学：将 3D 场景渲染到 2D 屏幕上本质上是一个投影过程。

1.  数据科学：将高维数据投影到低维子空间（如 PCA 中的前两个主成分）可以使模式可见，同时尽可能保留信息。

1.  信号处理：将信号分解为正弦波和余弦波的投影是傅里叶分析的基础，它为音频、图像和视频压缩提供动力。

#### 代数性质

+   投影是线性的：proj(u + w) = proj(u) + proj(w)。

+   垂直部分始终垂直于投影方向。

+   分解是唯一的：没有其他平行和垂直向量对可以重构 $u$。

+   投影算子到单位向量 $\hat{v}$ 满足：proj(u) = ($\hat{v}$ $\hat{v}^\mathrm{T}$)u，这显示了投影如何以矩阵形式表示。

#### 为什么它很重要

投影不仅仅是几何技巧；它是许多高级主题的核心：

+   最小二乘回归是找到数据向量在预测向量张量上的投影。

+   像格拉姆-施密特和 QR 分解这样的正交分解依赖于投影来构建正交基。

+   优化方法通常涉及将猜测投影回可行集。

+   机器学习不断使用投影来降低维度、比较向量和对齐特征。

没有投影，我们无法干净地分离方向上的影响或以结构化的方式降低复杂性。

#### 尝试自己来做

1.  将 (2, 3) 投影到 (1, 0) 上。垂直分量看起来像什么？

1.  将 (3, 1) 投影到 (2, 2) 上。验证垂直部分是正交的。

1.  将 (5, 5, 0) 相对于 (1, 0, 0) 分解为平行和垂直部分。

1.  挑战：写出投影到 (1, 2) 的投影矩阵。将其应用于 (3, 4)。它是否与公式匹配？

通过这些练习，你会发现投影不仅仅是运算——它是我们分解、解释和简化向量和空间的一个透镜。

### 9. 柯西-施瓦茨不等式和三角不等式

线性代数不仅涉及向量的运算——它还涉及理解它们之间的基本关系。在这方面，最重要的两个结果是柯西-施瓦茨不等式和三角不等式。这些是向量空间的基础，因为它们为长度、角度和内积建立了精确的界限。没有它们，线性代数的几何就会崩溃。

#### 柯西-施瓦茨不等式

对于 $\mathbb{R}^n$ 中的任意两个向量 $u$ 和 $v$，柯西-施瓦茨不等式表明：

$$ |u \cdot v| \leq \|u\| \, \|v\|. $$

这意味着两个向量点积的绝对值总是小于或等于它们长度的乘积。

如果且仅当 $u$ 和 $v$ 线性相关（即一个是另一个的标量倍数）时，等式成立。

##### 为什么它是真的

回忆一下点积的几何公式：

$$ u \cdot v = \|u\| \, \|v\| \cos(\theta). $$

由于 $-1 \leq \cos(\theta) \leq 1$，点积的模不能超过 $\|u\| \, \|v\|$。

这正是不等式。

##### 示例

设 $u = (3, 4)$ 和 $v = (-4, 3)$。

+   点积：$(3 \times -4) + (4 \times 3) = -12 + 12 = 0$

+   范数：$\|u\| = 5$，$\|v\| = 5$

+   范数乘积：$25$

+   $|u \cdot v| = 0 \leq 25$，这满足不等式

由于它们不是倍数关系——它们是垂直的，所以等号不成立。

##### 直觉

这个不等式告诉我们，两个向量永远不会比它们长度的乘积“重叠”得更强。如果它们完全对齐，重叠最大（等号成立）。如果它们垂直，重叠为零。

想象一下：“一个向量在另一个向量上的影子永远不会比这个向量本身更长。”

#### 三角不等式

对于任何向量 $u$ 和 $v$，三角不等式表明：

$$ \|u + v\| \leq \|u\| + \|v\|. $$

这反映了几何事实：在三角形中，任何一边的长度都不会超过其他两边长度之和。

##### 示例

令 $u = (1, 2)$ 和 $v = (3, 4)$。

+   $\|u + v\| = \|(4, 6)\| = \sqrt{16 + 36} = \sqrt{52} \approx 7.21$

+   $\|u\| + \|v\| = \sqrt{5} + 5 \approx 2.24 + 5 = 7.24$

事实上，$7.21 \leq 7.24$，在这个例子中非常接近。

##### 等号情况

当向量指向完全相同的方向（或是有非负系数的标量倍数）时，三角不等式变为等号。例如，(1, 1) 和 (2, 2) 产生等号，因为将它们相加得到的向量长度等于它们长度之和。

#### 扩展

+   这些不等式在所有内积空间中都成立，而不仅仅是 $\mathbb{R}^n$。这意味着它们适用于函数、序列以及更抽象的数学对象。

+   在希尔伯特空间（无限维推广）中，它们仍然同样重要。

#### 为什么它们很重要

1.  它们保证了点积和范数具有良好的行为和几何意义。

1.  它们确保范数满足距离度量的要求：非负性、对称性和三角不等式。

1.  它们支撑着投影、正交性和最小二乘方法的正确性。

1.  它们在证明算法收敛性、误差界限和数值线性代数中的稳定性方面至关重要。

没有这些不等式，我们就无法相信向量空间的几何行为是一致的。

#### 试试看

1.  验证 Cauchy–Schwarz 不等式对于 (2, –1, 3) 和 (–1, 4, 0) 是否成立。计算两边。

1.  尝试对 (–3, 4) 和 (5, –12) 使用三角不等式。等号是否成立？

1.  找到两个向量，使得 Cauchy–Schwarz 不等式成立。解释原因。

1.  挑战：仅使用勾股定理和代数证明 $\mathbb{R}²$ 中的三角不等式，而不依赖于点积。

通过解决这些问题，你会明白这些不等式为什么不是抽象的奇思妙想，而是线性代数几何结构上的粘合剂。

### 10. $\mathbb{R}²$ 和 $\mathbb{R}³$ 中的正交归一集

到目前为止，我们已经讨论了向量、它们的长度、角度以及如何将一个向量投影到另一个向量上。这些想法的自然结果是正交归一集的概念。这些是向量集合，不仅正交（相互垂直），而且归一化（每个向量的长度为 1）。正交归一集构成了线性代数中最干净、最有效的坐标系。它们是具有直角尺子和完美校准到单位长度的数学等价物。

#### 正交和归一化

让我们将“正交归一”这个词分成两部分：

+   正交：如果两个向量 $u$ 和 $v$ 的点积 $u \cdot v = 0$，则这两个向量是正交的。

    在 $\mathbb{R}²$ 中，这意味着这些向量在直角处相交。

    在 $\mathbb{R}³$ 中，这意味着它们形成垂直方向。

+   归一化：如果向量 $v$ 的长度为 $1$，即 $\|v\| = 1$，则该向量是归一化的。

    这样的向量被称为单位向量。

当我们结合这两个条件时，我们得到正交归一向量：既相互垂直又具有单位长度的向量。

#### $\mathbb{R}²$ 中的正交归一集

在二维空间中，一个正交归一集通常由两个向量组成。

一个经典的例子是：

$e_1 = (1, 0), \quad e_2 = (0, 1)$

+   点积：$e_1 \cdot e_2 = (1 \times 0 + 0 \times 1) = 0 \;\;\Rightarrow\;\;$ 正交

+   长度：$\|e_1\| = 1$, $\|e_2\| = 1 \;\;\Rightarrow\;\;$ 归一化

因此，$\{e_1, e_2\}$ 是一个正交归一集。

实际上，这是 $\mathbb{R}²$ 的标准基。

任何向量 $(x, y)$ 都可以写成 $x e_1 + y e_2$。

这是最简单的坐标系。

#### $\mathbb{R}³$ 中的正交归一集

在三维空间中，一个正交归一集通常有三个向量。

标准基是：

$e_1 = (1, 0, 0), \quad e_2 = (0, 1, 0), \quad e_3 = (0, 0, 1)$

+   每一对的点积为零，因此它们是正交的

+   每个向量的长度为 $1$，因此它们是归一化的

+   一起，它们构成了整个 $\mathbb{R}³$

几何上，它们对应于三维空间中的 $x$、$y$ 和 $z$ 轴。

任何向量 $(x, y, z)$ 都可以写成线性组合 $x e_1 + y e_2 + z e_3$。

#### 超越标准基

标准基不是唯一的正交归一集。例如：

$u = \left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right), \quad v = \left(-\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right)$

+   点积：$(\tfrac{1}{\sqrt{2}})(-\tfrac{1}{\sqrt{2}}) + (\tfrac{1}{\sqrt{2}})(\tfrac{1}{\sqrt{2}}) = -\tfrac{1}{2} + \tfrac{1}{2} = 0$

+   长度：$\sqrt{(\tfrac{1}{\sqrt{2}})² + (\tfrac{1}{\sqrt{2}})²} = \sqrt{\tfrac{1}{2} + \tfrac{1}{2}} = 1$

因此，$\{u, v\}$ 在 $\mathbb{R}²$ 中也是正交归一的。

这些向量相对于标准轴旋转了 $45^\circ$。

类似地，在 $\mathbb{R}³$ 中，只要满足垂直性和单位长度的条件，你可以构建旋转的正交归一集（例如对角线上的单位向量），

#### 正交归一集的性质

1.  简化坐标：如果 $\{v_1, \ldots, v_k\}$ 是一个正交归一集，那么对于它们张成的任意向量 $u$，系数容易计算：

    $$ c_i = u \cdot v_i $$

    这比解方程组简单得多。

1.  毕达哥拉斯定理的推广：如果向量是正交归一的，那么它们的和的平方长度等于它们系数平方的和。

    例如，如果 $u = a v_1 + b v_2$，那么

    $$ \|u\|² = a² + b² $$

1.  投影简单：将向量投影到正交归一集上很简单——只需计算点积。

1.  矩阵变得很棒：当向量形成矩阵的列时，正交归一性使得该矩阵成为一个正交矩阵，它具有特殊性质：其转置等于其逆，并且它保持长度和角度。

#### 在 $\mathbb{R}²$ 和 $\mathbb{R}³$ 中的重要性

+   在几何学中，正交归一基对应于坐标轴。

+   在物理学中，它们代表独立的运动或力的方向。

+   在计算机图形学中，正交归一集定义了相机轴和对象旋转。

+   在工程学中，他们简化了应力、应变和旋转分析。

虽然 $\mathbb{R}²$ 和 $\mathbb{R}³$ 相对简单，但同样的思想自然地扩展到更高维，在那里可视化是不可能的，但代数是相同的。

#### 为什么正交归一集很重要

正交归一是线性代数中构建基的金标准：

+   它使计算变得快速简单。

+   它确保了计算的数值稳定性（在算法和模拟中很重要）。

+   它是关键分解（如 QR 分解、奇异值分解（SVD）和谱定理）的基础。

+   它提供了思考空间的最佳方式：正交、独立的、缩放到单位长度的方向。

每当可能时，数学家和工程师更喜欢正交归一基而不是任意基。

#### 尝试自己动手做

1.  验证 $(3/5, 4/5)$ 和 $(-4/5, 3/5)$ 在 $\mathbb{R}²$ 中形成一个正交归一集。

1.  在 $\mathbb{R}³$ 中构造三个不是标准基的正交归一向量。提示：从 $(1/\sqrt{2}, 1/\sqrt{2}, 0)$ 开始，构建垂直向量。

1.  对于 $u = (2, 1)$，计算其相对于正交归一集 $\{(1/\sqrt{2}, 1/\sqrt{2}), (-1/\sqrt{2}, 1/\sqrt{2})\}$ 的坐标。

1.  挑战：证明如果 $\{v_1, \ldots, v_k\}$ 是正交归一的，那么这些向量作为列的矩阵是正交的，即 $Q^TQ = I$。

通过这些练习，你会看到正交归一集如何使线性代数的各个方面——从投影到分解——变得更简单、更干净、更强大。

#### 结束语

```py
Lengths, angles revealed,
projections trace hidden lines,
clarity takes shape.
```

## 第二章 矩阵和基本运算

#### 开场

```py
Rows and columns meet,
woven grids of silent rules,
machines of order.
```

### 11. 矩阵作为表格和作为机器

我们旅程的下一阶段是从向量到矩阵的过渡。矩阵可能看起来只是一个数字的矩形数组，但在线性代数中，它扮演着两个截然不同且同等重要的角色：

1.  作为数字表格，以紧凑的形式存储数据、系数或几何模式。

1.  作为一种将向量转换为其他向量的机器，捕捉线性变换的本质。

这两种观点都是有效的，学会在这两者之间切换对于建立直觉至关重要。

#### 矩阵作为表格

在最基本层面上，矩阵是由行和列排列成的一组数字。

+   一个 $2 \times 2$ 矩阵有 2 行和 2 列：

    $$ A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} $$

+   一个 $3 \times 2$ 矩阵有 3 行和 2 列：

    $$ B = \begin{bmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \\ b_{31} & b_{32} \end{bmatrix} $$

每一个条目 $a_{ij}$ 或 $b_{ij}$ 都告诉我们第 i 行和第 j 列的数字。矩阵的行可以表示约束、方程或观察结果；列可以表示特征、变量或方向。

在这种意义上，矩阵是数据容器，高效地组织信息。这就是为什么矩阵出现在电子表格、统计学、计算机图形学和科学计算中。

#### 矩阵作为机器

矩阵的更深层次的理解是它是一个从向量到向量的函数。如果 x 是一个列向量，那么乘以 A·x 会产生一个新的向量。

例如：

$$ A = \begin{bmatrix} 2 & 0 \\ 1 & 3 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} 4 \\ 5 \end{bmatrix}. $$

乘法：

$$ A\mathbf{x} = \begin{bmatrix} 2×4 + 0×5 \\ 1×4 + 3×5 \end{bmatrix} = \begin{bmatrix} 8 \\ 19 \end{bmatrix}. $$

在这里，矩阵作为一个机器，它接受输入（4, 5）并输出（8, 19）。A 的“机器规则”编码在 A 的行中。

#### 矩阵乘法的列视图

另一种理解方式：乘以 A·x 等同于对 A 的列进行线性组合。

如果

$$ A = \begin{bmatrix} a_1 & a_2 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}, $$

然后：

$$ A\mathbf{x} = x_1 a_1 + x_2 a_2. $$

因此，向量 x 告诉机器“每种列的混合量”。这种列的观点至关重要——它将矩阵与之前看到的跨度、维度和基的概念联系起来。

#### 表格与机器的对偶性

+   作为表格，矩阵是一个静态对象：按行和列书写的数字。

+   作为机器，相同的数字成为转换向量的指令。

这种对偶性不仅仅是概念上的——它是理解为什么线性代数如此强大的关键。一旦将数据集存储为表格，就可以将其解释为一种变换。同样，一旦理解了变换，就可以将其编码为表格。

#### 实践中的例子

1.  物理：应力-应变矩阵是一张系数表。但它也作为一个机器，将施加的力转换为变形。

1.  计算机图形学：一个二维旋转矩阵是一个旋转向量的机器，但它可以存储在一个简单的 2×2 表格中。

1.  经济学：投入产出模型使用矩阵作为生产系数表。将它们应用于需求向量将它们转换为资源需求。

#### 几何直觉

每个 2×2 或 3×3 矩阵对应于平面或空间中的某个线性变换。例如：

+   缩放：$\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}$ 将长度加倍。

+   反射：$\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}$ 在 x 轴上翻转。

+   旋转：$\begin{bmatrix} \cos θ & -\sin θ \\ \sin θ & \cos θ \end{bmatrix}$ 将向量旋转 θ。

这些不仅仅是数字表——它们是精确且可重复使用的机器。

#### 为什么这很重要

本节为所有矩阵理论奠定了基础：

+   将矩阵视为表有助于数据解释和组织。

+   将矩阵视为机器有助于理解线性变换、特征值和分解。

+   最重要的是，学会在两种视角之间切换，使线性代数既具体又抽象——将计算与几何联系起来。

#### 尝试自己操作

1.  写一个 2×3 矩阵并识别其行和列。它们在现实世界的数据集中可能代表什么？

1.  将 $\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$ 乘以 $\begin{bmatrix} 2 \\ –1 \end{bmatrix}$。使用行和列视图解释结果。

1.  构造一个矩阵，它沿 x 轴将向量缩放 2 倍，并沿 y 轴反射它们。在 (1, 1) 上测试它。

1.  挑战：展示相同的 3×3 旋转矩阵如何被视为余弦/正弦的数据表，以及作为一个将输入向量转换的机器。

通过掌握这两种视角，你会看到矩阵不仅仅是数字，而是动态对象，它们编码并执行变换。

### 12. 矩阵形状、索引和块视图

矩阵有多种形状和大小，我们标记它们的条目方式很重要。本节是关于学习如何仔细阅读和编写矩阵，如何处理行和列，以及如何使用块结构简化问题。这些看似简单但重要的想法使我们能够以精确和高效的方式操作大型系统。

#### 矩阵形状

矩阵的形状由其行数和列数给出：

+   一个 m×n 矩阵有 m 行和 n 列。

+   行水平运行，列垂直运行。

+   方阵有 m = n；矩形矩阵有 m ≠ n。

示例：

+   一个 2×3 矩阵：

    $$ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} $$

+   一个 3×2 矩阵：

    $$ \begin{bmatrix} 7 & 8 \\ 9 & 10 \\ 11 & 12 \end{bmatrix} $$

形状很重要，因为它决定了某些操作（如乘法）是否可行。

#### 索引：条目语言

矩阵中的每个元素都有两个索引：一个用于其行，一个用于其列。

+   $a_{ij}$ = 第 i 行，第 j 列的条目。

+   第一个索引始终指代行，第二个指代列。

例如，在

$$ A = \begin{bmatrix} 1 & 4 & 7 \\ 2 & 5 & 8 \\ 3 & 6 & 9 \end{bmatrix}, $$

我们有：

+   $a_{11} = 1$，$a_{23} = 8$，$a_{32} = 6$。

索引是矩阵语言的语法。没有它，我们无法指定位置或清楚地写出公式。

#### 行和列作为向量

矩阵的每一行和每一列本身就是一个向量。

+   第 i 行写作 $A_{i,*}$。

+   第 j 列写作 $A_{*,j}$。

示例：从上面的矩阵中，

+   第一行： (1, 4, 7)。

+   第二列： (4, 5, 6)。

这种对偶性非常强大：行通常表示约束或方程，而列表示方向或特征。稍后，当我们解释矩阵-向量乘积时，我们会看到乘以 A·x 意味着组合列，而乘以 yᵀ·A 意味着组合行。

#### 子矩阵

有时我们只想矩阵的一部分。子矩阵是通过选择某些行和列形成的。

示例：从

$$ B = \begin{bmatrix} 2 & 4 & 6 \\ 1 & 3 & 5 \\ 7 & 8 & 9 \end{bmatrix}, $$

前两行和最后两列的子矩阵是：

$$ \begin{bmatrix} 4 & 6 \\ 3 & 5 \end{bmatrix}. $$

子矩阵允许我们放大并隔离问题的部分。

#### 块矩阵：分而治之

大矩阵通常可以分解为块，这些块是较小的子矩阵，排列在内部。这就像将电子表格划分为象限一样。

例如：

$$ C = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}, $$

其中每个 $A_{ij}$ 本身也是一个更小的矩阵。

这种结构在以下方面很有用：

+   计算：算法通常处理块而不是单个元素。

+   理论：许多证明和分解依赖于将矩阵视为块（例如，LU、QR、Schur 分解）。

+   应用：将数据表划分为逻辑部分。

示例：将 4×4 矩阵拆分为四个 2×2 块有助于我们将其视为“矩阵的矩阵”。

#### 特殊形状

一些矩阵形状非常常见，值得命名：

+   行向量：1×n 矩阵。

+   列向量：n×1 矩阵。

+   对角矩阵：对角线上的非零项。

+   单位矩阵：对角线上的 1 的正方形对角矩阵。

+   零矩阵：所有项都是 0。

识别这些形状可以节省时间并澄清推理。

#### 为什么这很重要

仔细关注矩阵形状、索引和块视图可以确保：

1.  精度：我们可以明确地描述位置。

1.  结构意识：识别模式（对角线、三角形、块）导致更有效的计算。

1.  可扩展性：块划分是现代数值线性代数库的基础，其中矩阵太大，无法逐个处理。

1.  几何：行和列作为向量将矩阵结构连接到张量、基和维度。

这些基本工具使我们为乘法、变换和分解做好准备。

#### 尝试自己操作

1.  写一个 3×4 矩阵，并标记第 2 行第 3 列的元素。

1.  从你选择的 4×4 矩阵的角落提取一个 2×2 的子矩阵。

1.  将一个 6×6 矩阵分解为四个 3×3 块。你将如何紧凑地表示它？

1.  挑战：给定

    $$ D = \begin{bmatrix} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 10 & 11 & 12 \end{bmatrix}, $$

    写成一个带有左上角 2×2 块、右上角 2×2 块和底部行 1×4 块的块矩阵。

通过练习形状、索引和块，你将培养出将矩阵视为不仅仅是数字的原始网格，而是准备好进行更深入的代数和几何洞察的结构化对象的能力。

### 13. 矩阵加法和标量乘法

在探索矩阵-向量乘法和矩阵-矩阵乘法之前，理解我们可以用矩阵执行的最简单操作——加法和标量乘法——是至关重要的。这些操作扩展了我们为向量学到的规则，但现在应用于整个数字网格。虽然简单，但它们是更复杂代数操作的基础，并有助于建立矩阵作为向量空间元素的观念。

#### 矩阵加法：逐项相加

如果两个矩阵 $A$ 和 $B$ 形状相同（行数和列数相同），我们可以通过添加对应的项来将它们相加。

形式上：如果

$$ A = [a_{ij}], \quad B = [b_{ij}], $$

那么

$$ A + B = [a_{ij} + b_{ij}]. $$

示例：

$$ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} + \begin{bmatrix} 7 & 8 & 9 \\ 10 & 11 & 12 \end{bmatrix} = \begin{bmatrix} 8 & 10 & 12 \\ 14 & 16 & 18 \end{bmatrix}. $$

关键点：只有当矩阵形状相同时，加法才有定义。一个 2×3 矩阵不能加到一个 3×2 矩阵上。

#### 标量乘法：缩放每个元素

标量乘以矩阵的每个元素。

形式上：对于标量 $c$ 和矩阵 $A = [a_{ij}]$，

$$ cA = [c \cdot a_{ij}]. $$

示例：

$$ 3 \cdot \begin{bmatrix} 2 & -1 \\ 0 & 4 \end{bmatrix} = \begin{bmatrix} 6 & -3 \\ 0 & 12 \end{bmatrix}. $$

这反映了向量缩放：通过常数因子拉伸或缩小整个矩阵。

#### 加法和标量乘法的性质

这两个操作满足熟悉的代数性质，使得所有 m×n 矩阵的集合成为一个向量空间：

1.  交换律：$A + B = B + A$。

1.  结合律：$(A + B) + C = A + (B + C)$。

1.  加法单位元：$A + 0 = A$，其中 0 是零矩阵。

1.  加法逆元：对于每个 $A$，存在 $-A$ 使得 $A + (-A) = 0$。

1.  分配律：$c(A + B) = cA + cB$。

1.  兼容性：$(c + d)A = cA + dA$。

1.  标量结合律：$(cd)A = c(dA)$。

1.  单位标量：$1A = A$。

这些保证了使用矩阵的感觉就像使用数字和向量一样，只是在更高层次上。

#### 矩阵算术作为表格操作

从表格视图来看，加法和标量乘法只是简单的簿记：将形状相同的两个表格排成一行，逐项相加；将整个表格乘以一个常数。

示例：想象两个月度支出电子表格。将它们相加可以得到合并的总数。乘以 12 将月度表格转换为年度估计。

#### 矩阵算术作为机器操作

从机器视图来看，这些操作调整线性变换的行为：

+   添加矩阵对应于将它们应用于向量时的效果相加。

+   缩放矩阵会缩放变换的效果。

示例：设 $A$ 轻微旋转向量，$B$ 拉伸向量。矩阵 $A + B$ 表示应用这两种影响的同时变换。缩放 2 倍将变换的效果加倍。

#### 特殊情况：零和单位

+   零矩阵：所有项都是 0。将其加到任何矩阵上都不会改变任何东西。

+   标量乘以单位矩阵：当应用时，$cI$ 将每个向量缩放为 $c$。例如，$2I$ 将每个向量的长度加倍。

这些在矩阵算术中充当中性或缩放元素。

#### 几何直觉

1.  在 $\mathbb{R}²$ 或 $\mathbb{R}³$ 中，添加变换矩阵就像叠加几何效果一样：例如，一个矩阵进行剪切，另一个进行旋转，它们的和混合了两种效果。

1.  缩放变换会使其作用更强或更弱。将剪切加倍会使它更加明显。

这表明甚至在乘法之前，加法和缩放已经具有几何意义。

#### 为什么这很重要

虽然简单，但这些操作：

+   将矩阵定义为向量空间的元素。

+   为矩阵的线性组合打下基础，这在特征值问题、优化和控制理论中至关重要。

+   使模块化问题解决成为可能：将大变换分解成更小的部分，然后重新组合它们。

+   在实践中无处不在，从组合数据集到缩放变换。

没有加法和标量乘法，我们就不能系统地作为代数对象处理矩阵。

#### 尝试自己动手做

1.  加

$$ \begin{bmatrix} 2 & 0 \\ 1 & 3 \end{bmatrix} \quad \text{和} \quad \begin{bmatrix} -2 & 5 \\ 4 & -3 \end{bmatrix}. $$

1.  乘

$$ \begin{bmatrix} 1 & -1 & 2 \\ 0 & 3 & 4 \end{bmatrix} $$

乘以 -2。

1.  用显式的 2×2 矩阵证明 $(A + B) + C = A + (B + C)$。

1.  挑战：构造两个 3×3 矩阵 $A$ 和 $B$，使得 $A + B = 0$。这对你关于 $B$ 的了解有什么启示？

通过练习这些基础知识，你会发现即使是最基本的矩阵操作也已经为更深入的结果，如矩阵乘法、变换和分解，建立了代数基础。

### 14. 矩阵-向量乘积（列的线性组合）

我们现在到达了线性代数中最重要的操作之一：矩阵-向量乘积。这个操作取一个矩阵 $A$ 和一个向量 $x$，并产生一个新的向量。虽然计算很简单，但其解释很深刻：它可以看作是组合行，组合列，或者应用线性变换。这是将矩阵与向量空间的几何学联系起来的操作。

#### 代数规则

假设 $A$ 是一个 $m \times n$ 的矩阵，$x$ 是 $\mathbb{R}^n$ 中的一个向量。乘积 $A\mathbf{x}$ 是一个 $\mathbb{R}^m$ 中的向量，定义为：

$$ A = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22} & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots & a_{mn} \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}. $$

然后：

$$ A\mathbf{x} = \begin{bmatrix} a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n \\ a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \\ \vdots \\ a_{m1}x_1 + a_{m2}x_2 + \cdots + a_{mn}x_n \end{bmatrix}. $$

输出的每个元素都是 $A$ 的某一行与向量 $x$ 的点积。

#### 行视图：点积

从行的角度来看，$A\mathbf{x}$ 是逐行计算的：

+   取 $A$ 的每一行。

+   与 $x$ 进行点积。

+   那个结果成为输出中的一个条目。

示例：

$$ A = \begin{bmatrix} 2 & 1 \\ 3 & 4 \\ -1 & 2 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} 5 \\ -1 \end{bmatrix}. $$

+   第一行与 $x$ 的点积：$2(5) + 1(-1) = 9$。

+   第二行与 $x$ 的点积：$3(5) + 4(-1) = 11$。

+   第三行与 $x$ 的点积：$(-1)(5) + 2(-1) = -7$。

所以：

$$ A\mathbf{x} = \begin{bmatrix} 9 \\ 11 \\ -7 \end{bmatrix}. $$

#### 列视图：线性组合

从列的角度来看，$A\mathbf{x}$ 是 A 的列的线性组合。

如果

$$ A = \begin{bmatrix} | & | & & | \\ a_1 & a_2 & \cdots & a_n \\ | & | & & | \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}, $$

然后：

$$ A\mathbf{x} = x_1 a_1 + x_2 a_2 + \cdots + x_n a_n. $$

那就是将 $A$ 的每一列乘以 $x$ 中相应的项，然后将它们相加。

这种解释直接与张量的概念相关：所有向量 $A\mathbf{x}$（随着 $x$ 的变化）的集合正好是 $A$ 的列的张量。

#### 机器视图：线性变换

机器视图将一切联系在一起：将向量乘以矩阵意味着应用由矩阵表示的线性变换。

+   如果 $A$ 是一个 2×2 旋转矩阵，那么 $A\mathbf{x}$ 将向量 $x$ 旋转。

+   如果 $A$ 是一个缩放矩阵，那么 $A\mathbf{x}$ 会拉伸或缩小 $x$。

+   如果 $A$ 是一个投影矩阵，那么 $A\mathbf{x}$ 将 $x$ 投影到一条线或一个平面上。

因此，代数定义编码了几何和功能意义。

#### 几何作用的例子

1.  缩放：

$$ A = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}. $$

然后 $A\mathbf{x}$ 将任何向量 $x$ 的长度加倍。

1.  反射：

$$ A = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}. $$

这将向量翻转到 x 轴的另一侧。

1.  绕 θ 旋转：

$$ A = \begin{bmatrix} \cosθ & -\sinθ \\ \sinθ & \cosθ \end{bmatrix}. $$

这在平面上将向量逆时针旋转 θ。

#### 为什么这很重要

矩阵-向量乘积是线性代数中一切的基础：

1.  它定义了矩阵的作用为线性映射。

1.  它直接与张量和维度（列生成所有可能的输出）相关联。

1.  它是解决线性系统、特征值问题和分解的基础。

1.  它是应用数学中计算的引擎，从计算机图形学到机器学习（例如，神经网络计算数十亿个矩阵-向量乘积）。

#### 尝试自己操作

1.  计算

$$ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} \begin{bmatrix} 2 \\ 0 \\ 1 \end{bmatrix}. $$

1.  将上述乘积的结果表示为矩阵列的线性组合。

1.  构造一个 2×2 矩阵，使其能够将向量反射到直线 $y = x$ 上。在 (1, 0) 和 (0, 1) 上测试它。

1.  挑战：对于一个 3×3 矩阵，证明所有可能的 $A\mathbf{x}$（随着 $x$ 的变化）的集合正好是 $A$ 的列空间。

通过掌握矩阵-向量乘法的计算规则和解释，你将获得线性代数中最重要的洞察：矩阵不仅仅是表格——它们是变换空间的引擎。

### 15. 矩阵-矩阵乘法（线性步骤的组合）

理解了矩阵如何作用于向量后，下一步自然是要了解一个矩阵如何作用于另一个矩阵。这引出了矩阵-矩阵乘法，这是一种将两个矩阵组合成单个新矩阵的规则。虽然一开始看起来算术很复杂，但背后的思想是优雅的：两个矩阵的乘法表示两个线性变换的组合。

#### 代数规则

假设 $A$ 是一个 $m \times n$ 的矩阵，$B$ 是一个 $n \times p$ 的矩阵。它们的乘积 $C = AB$ 是一个 $m \times p$ 的矩阵，定义为：

$$ c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}. $$

那就是每个 $C$ 的元素是 $A$ 的第 i 行与 $B$ 的第 j 列的点积。

#### 示例：2×3 乘以 3×2

$$ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}, \quad B = \begin{bmatrix} 7 & 8 \\ 9 & 10 \\ 11 & 12 \end{bmatrix}. $$

积：$C = AB$ 将是 2×2。

+   $c_{11} = 1\cdot 7 + 2\cdot 9 + 3\cdot 11 = 58$。

+   $c_{12} = 1\cdot 8 + 2\cdot 10 + 3\cdot 12 = 64$。

+   $c_{21} = 4\cdot 7 + 5\cdot 9 + 6\cdot 11 = 139$。

+   $c_{22} = 4\cdot 8 + 5\cdot 10 + 6\cdot 12 = 154$。

所以：

$$ C = \begin{bmatrix} 58 & 64 \\ 139 & 154 \end{bmatrix}. $$

#### 列视图：列的线性组合

从列视角来看，$AB$ 是通过将 $A$ 应用到 $B$ 的每一列来计算的。

如果 $B = [b_1 \; b_2 \; \cdots \; b_p]$，那么：

$$ AB = [A b_1 \; A b_2 \; \cdots \; A b_p]. $$

那就是将 $A$ 乘以 $B$ 的每一列。这通常是思考乘法最简单的方式。

#### 行视图：行的线性组合

从行视角来看，$AB$ 的每一行是通过使用 $A$ 的某一行中的系数来组合 $B$ 的行形成的。这种双重视图虽然不常见，但同样有用，尤其是在证明和算法中。

#### 机器视图：变换的组合

最重要的解释是机器视图：矩阵相乘对应于变换的组合。

+   如果 $A$ 将 $\mathbb{R}^n \to \mathbb{R}^m$，而 $B$ 将 $\mathbb{R}^p \to \mathbb{R}^n$，那么 $AB$ 将 $\mathbb{R}^p \to \mathbb{R}^m$。

+   用话来说：先做 $B$，再做 $A$。

示例：

+   让 $B$ 将向量旋转 90°。

+   让 $A$ 将向量缩放 2 倍。

+   然后 $AB$ 先旋转再缩放——这两个步骤组合成一个单一的变换。

#### 几何示例

1.  缩放后旋转：

$$ A = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}, \quad B = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}. $$

然后 $AB$ 在旋转向量 90° 后将向量缩放 2 倍。

1.  投影后反射：如果 $B$ 投影到 x 轴上，而 $A$ 反射 y 轴，那么 $AB$ 表示“先投影后反射”。

#### 矩阵乘法的性质

1.  结合律：$(AB)C = A(BC)$。

1.  分配律：$A(B + C) = AB + AC$.

1.  非交换性：一般来说，$AB \neq BA$。顺序很重要！

1.  标识：$AI = IA = A$.

这些性质突出了虽然乘法是有结构的，但它不是对称的。顺序编码了变换中的操作顺序。

#### 为什么这很重要

矩阵乘法是线性代数的核心，因为：

1.  它将函数组合编码为代数形式。

1.  它提供了一种方法来捕捉单个矩阵中的多个变换。

1.  它支撑着计算机图形学、机器人技术、统计学和机器学习中的算法。

1.  它揭示了更深层次的结构，如交换律失败，这反映了现实世界的操作顺序。

几乎所有线性代数的应用——解方程、计算特征值、训练神经网络——都依赖于高效的矩阵乘法。

#### 尝试自己来做

1.  计算

$$ \begin{bmatrix} 1 & 0 \\ 2 & 3 \end{bmatrix} \begin{bmatrix} 4 & 5 \\ 6 & 7 \end{bmatrix}. $$

1.  证明对于以下矩阵 $AB \neq BA$：

$$ A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}, \quad B = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}. $$

1.  构造两个 2×2 矩阵，使得 $AB = BA$。为什么这里会发生交换律？

1.  挑战：如果 $A$ 是一个投影矩阵，$B$ 是一个旋转矩阵，计算 $AB$ 和 $BA$。它们是否代表相同的几何操作？

通过这些视角，矩阵-矩阵乘法从机械公式转变为组合线性步骤的语言——每个乘积都讲述着“这样做，然后那样做”的故事。

### 16. 单位矩阵、逆矩阵和转置

在加法、标量乘法和矩阵乘法已经到位的情况下，我们现在介绍三个特殊操作和对象，它们构成了矩阵代数的骨干：单位矩阵、矩阵的逆和矩阵的转置。每个都捕捉到一个基本原理——中立性、可逆性和对称性——并且它们共同提供了使线性代数如此强大的代数结构。

#### 单位矩阵

单位矩阵是乘法中数字 1 的矩阵等价物。

+   定义：单位矩阵 $I_n$ 是一个 $n \times n$ 的矩阵，对角线上的元素为 1，其余位置为 0。

示例（3×3）：

$$ I_3 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}. $$

+   性质：对于任何 $n \times n$ 的矩阵 $A$，

    $$ AI_n = I_nA = A. $$

+   机器视角：$I$ 不做任何事情——它将每个向量映射到自身。

#### 矩阵的逆

逆矩阵是数字倒数在矩阵中的等价物。

+   定义：对于一个方阵 $A$，其逆矩阵 $A^{-1}$ 是这样一个矩阵，使得

    $$ AA^{-1} = A^{-1}A = I. $$

+   并非所有矩阵都有逆矩阵。一个矩阵是可逆的，当且仅当它是方阵且其行列式不为零。

示例：

$$ A = \begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix}, \quad A^{-1} = \begin{bmatrix} 1 & -1 \\ -1 & 2 \end{bmatrix}. $$

检查：

$$ AA^{-1} = \begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 1 & -1 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I. $$

+   机器视角：应用 $A$ 会变换一个向量。应用 $A^{-1}$ 会撤销这种变换，恢复原始输入。

#### 不可逆矩阵

一些矩阵不能求逆，这些被称为奇异矩阵。

+   例如：

    $$ B = \begin{bmatrix} 2 & 4 \\ 1 & 2 \end{bmatrix}. $$

    在这里，第二列是第一列的倍数。这种变换将向量压缩成一条线，丢失了信息——因此不能逆转。

这将可逆性与几何联系起来：一个压缩维度的变换不能撤销。

#### 矩阵的转置

转置反映了矩阵关于其对角线的对称性。

+   定义：对于 $A = [a_{ij}]$,

    $$ A^T = [a_{ji}]. $$

+   用文字来说：行变成列，列变成行。

例如：

$$ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}, \quad A^T = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}. $$

+   性质：

    +   $(A^T)^T = A$.

    +   $(A + B)^T = A^T + B^T$.

    +   $(cA)^T = cA^T$.

    +   $(AB)^T = B^T A^T$（注意顺序的颠倒！）。

#### 对称矩阵和正交矩阵

从转置中产生了两个重要的类别：

+   对称矩阵：$A = A^T$。例如：

    $$ \begin{bmatrix} 2 & 3 \\ 3 & 5 \end{bmatrix}. $$

    这些具有美丽的性质：实特征值和正交特征向量。

+   正交矩阵：$Q^TQ = I$。它们的列形成一个正交归一集，并且它们代表纯旋转/反射。

#### 为什么这很重要

1.  标准矩阵保证了乘法的中性元素。

1.  逆矩阵提供了一种通过 $\mathbf{x} = A^{-1}\mathbf{b}$ 来解方程 $A\mathbf{x} = \mathbf{b}$ 的方法。

1.  转置将矩阵与几何、内积和对称性联系起来。

1.  一起，它们构成了深入主题的代数基础：行列式、特征值、分解和数值方法。

没有这些工具，矩阵代数就会缺乏结构和可逆性。

#### 尝试自己来做

1.  计算矩阵的转置

$$ \begin{bmatrix} 1 & 0 & 2 \\ -3 & 4 & 5 \end{bmatrix}. $$

1.  验证 $(AB)^T = B^TA^T$ 对于

$$ A = \begin{bmatrix} 1 & 2 \\ 0 & 3 \end{bmatrix}, \quad B = \begin{bmatrix} 4 & 0 \\ 5 & 6 \end{bmatrix}. $$

1.  求以下矩阵的逆

$$ \begin{bmatrix} 3 & 2 \\ 1 & 1 \end{bmatrix}. $$

1.  挑战：证明如果 $Q$ 是正交的，那么 $Q^{-1} = Q^T$。从几何上解释为“旋转可以通过转置来撤销”。

通过这些练习，你会看到身份、逆和转置如何锚定线性代数的结构，为每次计算提供中立性、可逆性和对称性。

### 17. 对称矩阵、对角矩阵、三角矩阵和置换矩阵

并非所有矩阵都是相同的——一些具有特殊的形状或模式，赋予它们独特的属性。这些结构化矩阵是线性代数的工作马——它们简化了计算，揭示了几何形状，并构成了算法的构建块。在本节中，我们研究四个特别重要的类别：对称矩阵、对角矩阵、三角矩阵和置换矩阵。

#### 对称矩阵

一个矩阵如果等于其转置，则称为对称矩阵：

$$ A = A^T. $$

示例：

$$ \begin{bmatrix} 2 & 3 & 4 \\ 3 & 5 & 6 \\ 4 & 6 & 9 \end{bmatrix}. $$

+   几何意义：对称矩阵代表没有“手性”的线性变换。它们在物理学中经常出现（能量、协方差、刚度）。

+   代数事实：对称矩阵具有实特征值和一个正交的特征向量基。这一性质是谱定理的基础，而谱定理是线性代数的支柱之一。

#### 对角矩阵

一个矩阵如果所有非对角线元素都是零，则称为对角矩阵。

$$ D = \begin{bmatrix} d_1 & 0 & 0 \\ 0 & d_2 & 0 \\ 0 & 0 & d_3 \end{bmatrix}. $$

+   乘以 $D$ 会分别缩放每个坐标。

+   使用对角线的计算非常快：

    +   加法：加上对角线元素。

    +   乘法：乘以对角线元素。

    +   逆运算：求每个对角线元素的逆（如果非零）。

示例：

$$ \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 2x \\ 3y \end{bmatrix}. $$

这就是为什么对角化如此有价值：将一个一般矩阵转换为对角矩阵可以简化一切。

#### 三角矩阵

一个矩阵如果所有主对角线以下的元素都是零，则称为上三角矩阵；如果所有主对角线以上的元素都是零，则称为下三角矩阵。

+   上三角矩阵示例：

    $$ \begin{bmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & 6 \end{bmatrix}. $$

+   下三角矩阵示例：

    $$ \begin{bmatrix} 7 & 0 & 0 \\ 8 & 9 & 0 \\ 10 & 11 & 12 \end{bmatrix}. $$

它们的重要性：

+   矩阵的行列式等于对角线元素的乘积。

+   通过代入（正向或反向）容易求解系统。

+   每个方阵都可以分解为三角矩阵（LU 分解）。

#### 置换矩阵

通过对单位矩阵的行（或列）进行置换得到置换矩阵。

示例：

$$ P = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}. $$

乘以 $P$：

+   在左侧，置换矩阵的行。

+   在右侧，置换矩阵的列。

置换矩阵在消元法中的主元策略中应用，确保在求解系统时数值稳定性。它们也是正交的：$P^{-1} = P^T$。

#### 两者之间的联系

+   对角矩阵是三角矩阵的特殊情况（上三角和下三角）。

+   对称矩阵在正交变换下通常变为对角矩阵。

+   置换矩阵有助于重新排列三角矩阵或对角矩阵，而不会破坏其结构。

一起，这些类别表明结构导致简单——许多计算算法利用这些模式以实现速度和稳定性。

#### 它的重要性

1.  对称矩阵保证了稳定的可解释的特征分解。

1.  对角矩阵使计算变得容易。

1.  三角矩阵是消元和分解方法的基础。

1.  排列矩阵在重新排序的同时保持结构，这对于算法至关重要。

几乎所有数值线性代数的高级方法都依赖于将一般矩阵简化为这些结构化形式之一。

#### 试试你自己

1.  验证

$$ \begin{bmatrix} 1 & 2 \\ 2 & 5 \end{bmatrix} $$

是对称的。找到它的转置。

1.  计算行列式

$$ \begin{bmatrix} 3 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 5 \end{bmatrix}. $$

1.  解

$$ \begin{bmatrix} 2 & 3 & 1 \\ 0 & 5 & 2 \\ 0 & 0 & 4 \end{bmatrix} \mathbf{x} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} $$

使用回代法。

1.  构造一个交换第一行和最后一行的 4×4 排列矩阵。将其应用于你选择的 4×1 向量。

通过探索这四个结构化的家族，你会发现并非所有矩阵都是杂乱的——许多矩阵在其排列中隐藏着秩序，而利用这种秩序是理论理解和高效计算的关键。

### 18. 迹与基本矩阵性质

到目前为止，我们已经研究了形状、乘法规则和特殊矩阵类。在本节中，我们介绍一个简单但非常强大的量：矩阵的迹。同时，我们回顾了一组基本的矩阵性质，这些性质提供了捷径、不变量和关于矩阵行为的洞察。

#### 迹的定义

对于一个大小为 $n \times n$ 的方阵 $A = [a_{ij}]$，迹是对角元素的求和：

$$ \text{tr}(A) = a_{11} + a_{22} + \cdots + a_{nn}. $$

示例：

$$ A = \begin{bmatrix} 2 & 5 & 7 \\ 0 & 3 & 1 \\ 4 & 6 & 8 \end{bmatrix}, \quad \text{tr}(A) = 2 + 3 + 8 = 13. $$

迹提取一个单一的数字来总结矩阵的“对角内容”。

#### 迹的性质

迹是线性的，并且与乘法和转置很好地相互作用：

1.  线性：

    +   $\text{tr}(A + B) = \text{tr}(A) + \text{tr}(B)$。

    +   $\text{tr}(cA) = c \cdot \text{tr}(A)$。

1.  循环性质：

    +   $\text{tr}(AB) = \text{tr}(BA)$，只要乘积是定义好的。

    +   更一般地，$\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)$。

    +   但在一般情况下，$\text{tr}(AB) \neq \text{tr}(A)\text{tr}(B)$。

1.  转置不变性：

    +   $\text{tr}(A^T) = \text{tr}(A)$。

1.  相似不变性：

    +   如果 $B = P^{-1}AP$，那么 $\text{tr}(B) = \text{tr}(A)$。

    +   这意味着迹是一个相似不变量，只依赖于线性变换，而不依赖于基。

#### 迹与特征值

最重要联系之一是迹与特征值之间的关系：

$$ \text{tr}(A) = \lambda_1 + \lambda_2 + \cdots + \lambda_n, $$

其中 $\lambda_i$ 是 $A$ 的特征值（计重数）。

这将简单的对角和与矩阵的深层谱性质联系起来。

示例：

$$ A = \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix}, \quad \text{tr}(A) = 4, \quad \lambda_1 = 1, \; \lambda_2 = 3, \quad \lambda_1 + \lambda_2 = 4. $$

#### 其他基本矩阵性质

除了迹之外，这里还有一些重要的代数事实，每个线性代数的学生都必须知道：

1.  行列式与迹的比较：

    +   对于 $2 \times 2$ 矩阵，$A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$，$\text{tr}(A) = a + d$，$\det(A) = ad - bc$。

    +   一起，迹和行列式编码了特征值：$x² - \text{tr}(A)x + \det(A) = 0$ 的根。

1.  范数和内积：

    +   弗罗贝尼乌斯范数是通过迹来定义的：$\|A\|_F = \sqrt{\text{tr}(A^TA)}$。

1.  正交不变性：

    +   对于任何正交矩阵 $Q$，$\text{tr}(Q^TAQ) = \text{tr}(A)$。

#### 几何与实际意义

+   变换的迹可以看作是其沿坐标轴作用的和。

+   在物理学中，应力张量的迹测量压力。

+   在概率论中，协方差矩阵的迹是系统的总方差。

+   在统计学和机器学习中，迹经常被用作衡量模型整体“大小”或复杂性的度量。

#### 它的重要性

迹看似简单但极其强大：

1.  它直接与特征值相关联，形成从原始矩阵项到谱理论之间的桥梁。

1.  它在相似性下是不变的，这使得它成为一个可靠的变换度量，与基无关。

1.  它出现在优化、物理学、统计学和量子力学中。

1.  它简化了计算：线性代数中的许多证明都归结为迹的性质。

#### 试试看

1.  计算以下矩阵的迹

$$ \begin{bmatrix} 4 & 2 & 0 \\ -1 & 3 & 5 \\ 7 & 6 & 1 \end{bmatrix}. $$

1.  验证 $\text{tr}(AB) = \text{tr}(BA)$ 对于

$$ A = \begin{bmatrix} 1 & 2 \\ 0 & 3 \end{bmatrix}, \quad B = \begin{bmatrix} 4 & 0 \\ 5 & 6 \end{bmatrix}. $$

1.  对于 $2 \times 2$ 矩阵

$$ \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}, $$

计算其特征值并检查它们的和是否等于迹。

1.  挑战：证明具有协方差矩阵 $\Sigma$ 的数据集的总方差等于 $\text{tr}(\Sigma)$。

掌握迹及其性质将为您准备下一个飞跃：理解矩阵如何与体积、方向和行列式相互作用。

### 19. 仿射变换和齐次坐标

到目前为止，矩阵已被用来描述线性变换：缩放、旋转、反射、投影。但现实世界的几何往往不仅仅涉及线性效应——它还包括平移（位移）。纯线性映射不能移动原点，因此为了处理平移（以及与旋转、缩放和剪切组合的平移），我们扩展了我们的工具集到仿射变换。使这一切得以实现的秘密武器是齐次坐标的概念。

#### 什么是仿射变换？

仿射变换是任何形式的映射：

$$ f(\mathbf{x}) = A\mathbf{x} + \mathbf{b}, $$

其中 $A$ 是一个矩阵（线性部分），而 $\mathbf{b}$ 是一个向量（平移部分）。

+   $A$ 处理缩放、旋转、反射、剪切或投影。

+   $\mathbf{b}$ 将所有内容平移一个常数量。

2D 中的示例：

1.  旋转 90°然后向右平移 2 个单位。

1.  垂直拉伸 3 倍并向上平移 1 个单位。

仿射映射保持平行线和沿线的距离比，但不一定是角度或长度。

#### 为什么仅线性映射还不够

如果我们只在 2D 中使用 2×2 矩阵或在 3D 中使用 3×3 矩阵，则原点始终保持固定。这是一个限制：现实世界的运动（如将形状从一个地方移动到另一个地方）也需要移动原点。为了统一捕捉线性和平移效应，我们需要一个巧妙的技巧。

#### 同次坐标

技巧是添加一个额外的坐标。

+   在 2D 中，一个点 $(x, y)$ 变为 $(x, y, 1)$。

+   在 3D 中，一个点 $(x, y, z)$ 变为 $(x, y, z, 1)$。

这种新的表示称为同次坐标。它允许我们将平移折叠到矩阵乘法中。

#### 仿射变换作为同次形式的矩阵

在 2D：

$$ \begin{bmatrix} a & b & t_x \\ c & d & t_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ 1 \end{bmatrix} = \begin{bmatrix} ax + by + t_x \\ cx + dy + t_y \\ 1 \end{bmatrix}. $$

这里，

+   2×2 块 $\begin{bmatrix} a & b \\ c & d \end{bmatrix}$ 是线性部分。

+   最后的列 $\begin{bmatrix} t_x \\ t_y \end{bmatrix}$ 是平移。

因此，通过一个统一的矩阵，我们可以处理线性变换和平移。

#### 2D 中的示例

1.  平移（2, 3）：

$$ \begin{bmatrix} 1 & 0 & 2 \\ 0 & 1 & 3 \\ 0 & 0 & 1 \end{bmatrix}. $$

1.  在 x 方向上缩放 2 倍，在 y 方向上缩放 3 倍，然后平移（-1, 4）：

$$ \begin{bmatrix} 2 & 0 & -1 \\ 0 & 3 & 4 \\ 0 & 0 & 1 \end{bmatrix}. $$

1.  旋转 90°并向右平移 5 个单位：

$$ \begin{bmatrix} 0 & -1 & 5 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}. $$

#### 3D 中的同次坐标

在 3D 中，仿射变换使用 4×4 矩阵。左上角的 3×3 块处理旋转、缩放或剪切；最后一列编码平移。

示例：平移（2, –1, 4）：

$$ \begin{bmatrix} 1 & 0 & 0 & 2 \\ 0 & 1 & 0 & -1 \\ 0 & 0 & 1 & 4 \\ 0 & 0 & 0 & 1 \end{bmatrix}. $$

这种公式在计算机图形学和机器人技术中是通用的。

#### 为什么它很重要

1.  统一表示：使用同次坐标，我们可以将平移视为矩阵，从而实现所有变换的一致矩阵乘法。

1.  实用性：这种方法是 3D 图形管线、动画、CAD、机器人和计算机视觉的基础。

1.  可组合性：多个仿射变换可以通过乘法组合成一个单一的统一同次矩阵。

1.  几何保持：仿射映射保持直线和平行性，这在工程和设计中至关重要。

#### 尝试自己操作

1.  写出沿 x 轴反射并向上平移 3 个单位的同次矩阵。将其应用于 $(2, 1)$。

1.  构造一个绕 z 轴旋转 90°并沿（1，2，0）平移的 4×4 齐次矩阵。

1.  证明在 2D 中乘以两个 3×3 齐次矩阵产生另一个有效的仿射变换。

1.  挑战：通过将一个一般的仿射矩阵应用于两条平行线并检查它们的斜率，证明仿射映射保持平行线。

掌握仿射变换和齐次坐标将纯线性代数与现实世界几何之间的差距弥合，为你提供了计算机图形学、机器人和空间建模背后的数学基础。

### 20. 使用矩阵进行计算（成本计数和简单加速）

到目前为止，我们已经研究了矩阵是什么以及它们代表什么。但在实践中，使用矩阵也意味着思考计算——操作需要多少工作量，算法如何加速，以及为什么结构很重要。本节介绍了矩阵运算中的计算成本的基本思想，提高效率的简单策略，以及为什么这些考虑在现代应用中至关重要。

#### 计算操作：成本模型

测量矩阵运算成本的最简单方法是计算基本算术运算（加法和乘法）。

+   矩阵-向量乘积：对于一个 $m \times n$ 矩阵和一个 $n \times 1$ 向量：

    +   每个输出项都需要 $n$ 次乘法和 $n-1$ 次加法。

    +   总成本 ≈ $2mn$ 次操作。

+   矩阵-矩阵乘积：对于一个 $m \times n$ 矩阵乘以一个 $n \times p$ 矩阵：

    +   每个的 $mp$ 个条目都需要 $n$ 次乘法和 $n-1$ 次加法。

    +   总成本 ≈ $2mnp$ 次操作。

+   高斯消元（解 $Ax=b$）：对于一个 $n \times n$ 系统：

    +   大约 $\tfrac{2}{3}n³$ 次操作。

这些计数显示了成本如何随着维度的增加而迅速增长。将 $n$ 加倍会使消元的工作量增加 8 倍。

#### 为什么成本计数很重要

1.  可扩展性：小问题（2×2 或 3×3）是微不足道的，但现代数据集涉及数百万行的矩阵。了解成本是至关重要的。

1.  可行性：对于非常大的矩阵，一些精确算法变得不可能。因此使用近似方法。

1.  优化：工程师和科学家设计专门的算法，通过利用结构（稀疏性、对称性、三角形式）来降低成本。

#### 利用结构进行简单的加速

+   对角矩阵：乘以对角矩阵的成本仅为 $n$ 次操作（缩放每个分量）。

+   三角矩阵：解三角系统只需要 $\tfrac{1}{2}n²$ 次操作（替换），远比一般消元法便宜。

+   稀疏矩阵：如果大多数条目为零，我们跳过乘以零的操作。对于大型稀疏系统，成本与非零项的数量成比例，而不是 $n²$。

+   分块矩阵：将矩阵分解成块允许算法重用优化的小矩阵例程（在 BLAS 库中很常见）。

#### 内存考虑

成本不仅仅是算术运算：存储也很重要。

+   一个密集的 $n \times n$ 矩阵需要 $n²$ 个内存条目。

+   稀疏存储格式（如 CSR、COO）仅记录非零条目及其位置，节省大量空间。

+   在大型计算中，内存访问速度可以主导算术成本。

#### 并行性和硬件

现代计算利用硬件提高速度：

+   向量化（SIMD）：同时执行许多乘法。

+   并行化：将工作分配到多个 CPU 核心。

+   GPU：专门用于大规模并行矩阵-向量矩阵-矩阵运算（在深度学习中至关重要）。

这就是为什么线性代数库（BLAS、LAPACK、cuBLAS）是不可或缺的：它们从硬件中榨取性能。

#### 为什么这很重要

1.  效率：理解成本让我们能够选择适合工作的正确算法。

1.  算法设计：结构化矩阵（对角、稀疏、正交）使计算更快更稳定。

1.  应用：每个使用矩阵的领域——图形、优化、统计学、AI——都依赖于高效的计算。

1.  基础：像 LU/QR/SVD 分解这样的后续主题是由平衡成本和稳定性来激发的。

#### 试试你自己

1.  计算乘以 1000×500 矩阵和 500×200 矩阵所需的操作数。与乘以 1000×1000 密集矩阵的向量比较。

1.  展示如何解决 3×3 三角系统比高斯消元更快。计算确切的乘法和加法次数。

1.  构造一个只有 7 个非零条目的 5×5 稀疏矩阵。估计它与一个密集 5×5 矩阵相乘的成本。

1.  挑战：假设你需要存储一个 1,000,000×1,000,000 的密集矩阵。如果每个条目是 8 字节，估计它需要多少内存（以字节为单位）。它能否放在笔记本电脑上？为什么稀疏格式能拯救这一天？

通过学习计算成本和利用结构，你不仅能够抽象地理解矩阵，而且能够在现实世界的大规模问题中有效地使用它们。这种理论与实践之间的平衡是现代线性代数的核心。

#### 结束语

```py
Patterns intertwine,
transformations gently fold,
structure in the square.
```

## 第三章\. 线性系统和消元

### 21\. 从方程到矩阵

线性代数通常从方程组开始——未知数通过线性关系连接的集合。虽然这些系统可以直接使用代入法或消元法来解决，但当变量很多时，它们会很快变得混乱。线性代数的关键洞察是，所有线性方程组都可以通过矩阵和向量紧凑地捕捉。本节解释了如何从用文字和符号写出的方程转换为推动计算的矩阵形式。

#### 一个简单的例子

考虑这个由两个未知数的两个方程组成的系统：

$$ \begin{cases} 2x + y = 5 \\ 3x - y = 4 \end{cases} $$

初看，这似乎只是代数：两个方程，两个未知数。但请注意结构：每个方程是变量的倍数之和，等于一个常数。这种模式——未知数的线性组合等于一个结果——正是矩阵所捕捉的。

#### 以系数表形式书写

从系统中提取每个变量的系数：

+   第一个方程：$x$ 的系数是 $2$，$y$ 的系数是 $1$。

+   第二个方程：$x$ 的系数是 $3$，$y$ 的系数是 $-1$。

将这些系数排列成一个矩形数组：

$$ A = \begin{bmatrix} 2 & 1 \\ 3 & -1 \end{bmatrix}. $$

这个矩阵 $A$ 被称为系数矩阵。

接下来，将未知数写成向量：

$$ \mathbf{x} = \begin{bmatrix} x \\ y \end{bmatrix}. $$

最后，将右侧的常数写成另一个向量：

$$ \mathbf{b} = \begin{bmatrix} 5 \\ 4 \end{bmatrix}. $$

现在可以将整个系统写在一行中：

$$ A\mathbf{x} = \mathbf{b}. $$

#### 为什么这很强大

这种紧凑的形式没有隐藏任何信息；它与原始方程等价。但它给我们带来了巨大的优势：

1.  清晰度：我们清楚地看到结构——系统是“矩阵乘以向量等于向量”。

1.  可扩展性：无论我们有 2 个方程还是 2000 个方程，相同的符号都适用。

1.  工具：矩阵运算的所有机制（消元、逆、分解）现在都可用。

1.  几何：矩阵方程 $A\mathbf{x} = \mathbf{b}$ 的意思是：将 $A$ 的列（由 $x$ 的项进行缩放）组合起来，得到 $b$。

#### 一个更大的例子

三个未知数的三个方程组：

$$ \begin{cases} x + 2y - z = 2 \\ 2x - y + 3z = 1 \\ 3x + y + 2z = 4 \end{cases} $$

+   系数矩阵：

    $$ A = \begin{bmatrix} 1 & 2 & -1 \\ 2 & -1 & 3 \\ 3 & 1 & 2 \end{bmatrix}. $$

+   未知向量：

    $$ \mathbf{x} = \begin{bmatrix} x \\ y \\ z \end{bmatrix}. $$

+   常数向量：

    $$ \mathbf{b} = \begin{bmatrix} 2 \\ 1 \\ 4 \end{bmatrix}. $$

矩阵形式：

$$ A\mathbf{x} = \mathbf{b}. $$

这个单一方程在一个对象中捕捉了三个方程和三个未知数。

#### 行与列的视角

+   行视角：$A$ 的每一行与 $x$ 点积得到一个方程。

+   列视角：整个系统意味着 $b$ 是 $A$ 的列的线性组合。

对于之前的 2×2 情况：

$$ A\mathbf{x} = \begin{bmatrix} 2 & 1 \\ 3 & -1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = x \begin{bmatrix} 2 \\ 3 \end{bmatrix} + y \begin{bmatrix} 1 \\ -1 \end{bmatrix}. $$

因此，解这个系统意味着找到标量 $x, y$，它们将 $A$ 的列组合起来达到 $\mathbf{b}$。

#### 增广矩阵形式

有时我们想要进一步节省空间。我们可以在增广矩阵中将系数和常数并排放置：

$$ [A | \mathbf{b}] = \begin{bmatrix} 2 & 1 & | & 5 \\ 3 & -1 & | & 4 \end{bmatrix}. $$

这种形式对于消元法特别有用，在那里我们可以在每一步不写变量地操作行。

#### 为什么这很重要

这一步——将方程重写为矩阵形式——是线性代数的大门。一旦你能做到这一点，你就不再把方程组看作是纸上孤立的线条，而是作为一个可以研究的一般对象。它打开了通往：

+   高斯消元法，

+   行列式，

+   行列式，

+   特征值，

+   优化方法。

每一个主要思想都源于这种紧凑的表示。

#### 试试你自己

1.  写出系统

    $$ \begin{cases} 4x - y = 7 \\ -2x + 3y = 5 \end{cases} $$

    以矩阵形式。

1.  对于系统

    $$ \begin{cases} x + y + z = 6 \\ 2x - y + z = 3 \\ x - y - z = -2 \end{cases} $$

    构建系数矩阵、未知向量常数向量。

1.  表达上述系统的增广矩阵。

1.  挑战：从列视角解释这个系统。将 $(6, 3, -2)$ 表示为系数矩阵列的线性组合在几何上意味着什么？

通过练习这些重写，你会发现线性代数不是关于玩弄许多方程——它是关于在一个紧凑的方程中看到结构。这一步将散乱的方程转化为矩阵的语言，真正的力量从这里开始。

### 22\. 行操作

一旦将线性方程组表示为矩阵，下一步就是将这个矩阵简化为一个形式，使得解变得清晰。这个简化的主要工具是初等行操作集合。这些操作允许我们以系统的方式操纵矩阵的行，同时保持相应方程组的解集。

#### 三种类型的行操作

恰好有三种合法的行操作，每种操作都有明确的代数意义：

1.  行交换 ($R_i \leftrightarrow R_j$)：交换两行。这相当于重新排列一个系统中的方程。由于方程的顺序不会改变解，这个操作始终是有效的。

    示例：

    $$ \begin{bmatrix} 2 & 1 & | & 5 \\ 3 & -1 & | & 4 \end{bmatrix} \quad \longrightarrow \quad \begin{bmatrix} 3 & -1 & | & 4 \\ 2 & 1 & | & 5 \end{bmatrix}. $$

1.  行缩放 ($R_i \to cR_i, \; c \neq 0$)：将一个行中的所有项乘以一个非零常数。这就像将方程的两边乘以同一个数，这不会改变它的真值。

    示例：

    $$ \begin{bmatrix} 2 & 1 & | & 5 \\ 3 & -1 & | & 4 \end{bmatrix} \quad \longrightarrow \quad \begin{bmatrix} 1 & \tfrac{1}{2} & | & \tfrac{5}{2} \\ 3 & -1 & | & 4 \end{bmatrix}. $$

1.  行替换 ($R_i \to R_i + cR_j$)：将一个行的倍数加到另一个行上。这相当于用一个方程和另一个方程的线性组合替换一个方程，这是一个基本的消元步骤。

    示例：

    $$ \begin{bmatrix} 2 & 1 & | & 5 \\ 3 & -1 & | & 4 \end{bmatrix} \quad \overset{R_2 \to R_2 - \tfrac{3}{2}R_1}{\longrightarrow} \quad \begin{bmatrix} 2 & 1 & | & 5 \\ 0 & -\tfrac{5}{2} & | & -\tfrac{7}{2} \end{bmatrix}. $$

#### 为什么这些是唯一允许的操作

这三种操作是消元的支柱，因为它们不会改变系统的解集。每个操作都相当于应用一个可逆变换：

+   行交换是可逆的（交换回来）。

+   行缩放 $c$ 可以通过缩放 $1/c$ 来撤销。

+   行替换可以通过添加相反的倍数来撤销。

因此，每个操作都是可逆的，变换后的系统始终与原始系统等价。

#### 行操作作为矩阵

每个初等行操作本身可以通过左乘一个称为初等矩阵的特殊矩阵来表示。

例如：

+   在一个 2×2 系统中，通过以下方式交换第 1 行和第 2 行：

    $$ E = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}. $$

+   在一个 2×2 系统中，通过将第 1 行乘以 3 来实现缩放。

    $$ E = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix}. $$

这种观点对于后续的因子分解方法，如 LU 分解，至关重要，其中消去被表示为初等矩阵的乘积。

#### 步骤示例

系统：

$$ \begin{cases} x + 2y = 4 \\ 3x + 4y = 10 \end{cases} $$

增广矩阵：

$$ \begin{bmatrix} 1 & 2 & | & 4 \\ 3 & 4 & | & 10 \end{bmatrix}. $$

1.  消去第一个主元下的$3x$：$R_2 \to R_2 - 3R_1$。

    $$ \begin{bmatrix} 1 & 2 & | & 4 \\ 0 & -2 & | & -2 \end{bmatrix}. $$

1.  缩放第 2 行：$R_2 \to -\tfrac{1}{2}R_2$。

    $$ \begin{bmatrix} 1 & 2 & | & 4 \\ 0 & 1 & | & 1 \end{bmatrix}. $$

1.  消去主元以上的元素：$R_1 \to R_1 - 2R_2$。

    $$ \begin{bmatrix} 1 & 0 & | & 2 \\ 0 & 1 & | & 1 \end{bmatrix}. $$

解：$x = 2, \; y = 1$。

#### 行操作的几何

行操作不会改变解空间：

+   交换行重新排列方程，但保持相同的线或平面。

+   缩放行重新缩放方程，但保持它们的几何集不变。

+   添加行对应于合并约束，但共享的交集（解集）被保留。

因此，行操作就像“重塑系统”的同时保持交集完整。

#### 为什么这很重要

行操作是用手或计算机解决线性系统所必需的基本步骤。它们包括：

1.  使消去系统化。

1.  在简化结构的同时保留解集。

1.  为行阶梯形、秩和因子分解打下基础。

1.  提供计算机在高斯消元中自动化的机械步骤。

#### 试试看

1.  应用行操作以简化

    $$ \begin{bmatrix} 2 & 1 & | & 7 \\ 1 & -1 & | & 1 \end{bmatrix} $$

    到一个解显然的形式。

1.  明确地说明为什么交换系统中的两个方程不会改变其解。

1.  在一个 3×3 系统中，构造“将第 1 行减去 2 倍的第 3 行”的初等矩阵。

1.  挑战：证明任何初等行操作都对应于乘以一个可逆矩阵。

掌握这些操作为你提供了下一阶段的机械和概念基础：系统地简化矩阵到行阶梯形。

### 23. 行阶梯形和简化行阶梯形

在引入行操作后，自然会问：*我们通过执行这些操作试图达到什么目的？* 答案是将矩阵转换成一个标准化的、简化的形式，其中对应方程组的解可以直接读出。在线性代数中，这两种标准化形式是核心：行阶梯形（REF）和简化行阶梯形（RREF）。

#### 行阶梯形（REF）

如果一个矩阵满足以下条件，则该矩阵处于行阶梯形：

1.  所有非零行都位于所有零行之上。

1.  在每个非零行中，第一个非零项（称为首项或主元）位于其上方行的首项的右侧。

1.  所有位于主元下方的项都是零。

REF 的示例：

$$ \begin{bmatrix} 1 & 2 & 3 & | & 4 \\ 0 & 1 & -1 & | & 2 \\ 0 & 0 & 5 & | & -3 \\ 0 & 0 & 0 & | & 0 \end{bmatrix}. $$

这里，主元是第一行第一列的第一个 1，第二行的 1 和第三行的 5。每个主元都在其上方主元的右侧，并且主元下方的所有项都是零。

#### 简化行阶梯形（RREF）

如果一个矩阵除了满足 REF 的规则外，还满足以下条件，则该矩阵处于简化行阶梯形：

1.  每个主元都等于 1。

1.  每个主元是其列中唯一的非零项（主元上方的和下方的所有项都是零）。

RREF 的示例：

$$ \begin{bmatrix} 1 & 0 & 0 & | & 3 \\ 0 & 1 & 0 & | & -2 \\ 0 & 0 & 1 & | & 1 \end{bmatrix}. $$

这种形式简化到如此程度，以至于可以直接读取解：这里，$x=3$，$y=-2$，$z=1$。

#### REF 与 RREF 之间的关系

+   REF 更容易达到——只需要消除主元下方的项。

+   RREF 需要进一步简化——清除主元上方的项，并将主元缩放到 1。

+   每个矩阵都可以被简化为 REF（可能有多种版本），但 RREF 是唯一的：无论你如何进行，如果你完全执行所有行操作，最终都会得到相同的 RREF。

#### 示例：逐步到 RREF

系统：

$$ \begin{cases} x + 2y + z = 4 \\ 2x + 5y + z = 7 \\ 3x + 6y + 2z = 10 \end{cases} $$

增广矩阵：

$$ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 2 & 5 & 1 & | & 7 \\ 3 & 6 & 2 & | & 10 \end{bmatrix}. $$

1.  消除第一个主元（第一行第一列的 1）下方的项：

    +   $R_2 \to R_2 - 2R_1$

    +   $R_3 \to R_3 - 3R_1$

    $$ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 0 & 1 & -1 & | & -1 \\ 0 & 0 & -1 & | & -2 \end{bmatrix}. $$

    现在已经处于 REF 状态。

1.  缩放主元并消除其上方的项：

    +   $R_3 \to -R_3$ 以使主元 1。

    +   $R_2 \to R_2 + R_3$。

    +   $R_1 \to R_1 - R_2 - R_3$

    最终：

    $$ \begin{bmatrix} 1 & 0 & 0 & | & 2 \\ 0 & 1 & 0 & | & 1 \\ 0 & 0 & 1 & | & 2 \end{bmatrix}. $$

解：$x=2, y=1, z=2$。

#### REF 和 RREF 的几何

+   行阶梯形（REF）对应于逐步简化系统，使其成为“三角形”形式，以便可以依次求解变量。

+   简化行阶梯形（RREF）对应于一个完全解耦的系统——每个变量都是孤立的，其值或自由变量关系明确可见。

#### 重要性

1.  REF 是高斯消元法的基础，这是求解系统的主力算法。

1.  RREF 提供了完整的清晰度：解集的唯一表示，揭示了自由变量和主变量。

1.  RREF 是计算机代数系统、符号求解器和教育工具中算法的基础。

1.  理解这些形式有助于建立对秩、零空间和解决方案结构的直觉。

#### 试试你自己

1.  减少

    $$ \begin{bmatrix} 2 & 4 & | & 6 \\ 1 & 3 & | & 5 \end{bmatrix} $$

    到 REF，然后到 RREF。

1.  求 RREF

    $$ \begin{bmatrix} 1 & 1 & 1 & | & 3 \\ 2 & 3 & 4 & | & 8 \\ 1 & 2 & 3 & | & 5 \end{bmatrix}. $$

1.  解释为什么不同的消元序列可以导致不同的行阶梯形，但具有相同的简化行阶梯形。

1.  挑战：通过系统地考虑行操作的影响，证明每个矩阵都有一个唯一的简化行阶梯形。

达到行阶梯形和简化行阶梯形，将混乱的系统转化为结构化的系统，将代数混乱转化为通向解的有序路径。

### 24. 主元、自由变量和首项

当将矩阵简化为行阶梯形或简化行阶梯形时，矩阵中的某些位置具有特殊的重要性。这些是主元——每行的首项非零项。围绕它们，线性系统的整个解结构被组织起来。理解主元、它们所锚定的变量以及非主元列带来的自由度对于系统地求解线性方程至关重要。

#### 什么是主元？

在行阶梯形中，主元是每行从左到右的第一个非零项。在简化行阶梯形中，每个主元被设置为正好为 1。

示例：

$$ \begin{bmatrix} 1 & 2 & 0 & | & 5 \\ 0 & 1 & 3 & | & -2 \\ 0 & 0 & 0 & | & 0 \end{bmatrix} $$

+   第一行中的主元：第一列的 1。

+   第二行中的主元：第二列的 1。

+   第三列没有主元。

有主元的列是主元列。没有主元的列对应于自由变量。

#### 主元变量与自由变量

+   主元变量：与主元列对齐的变量。它们由方程确定。

+   自由变量：与非主元列对齐的变量。它们不受约束，可以取任意值。

示例：

$$ \begin{bmatrix} 1 & 0 & 2 & | & 3 \\ 0 & 1 & -1 & | & 4 \end{bmatrix}. $$

这对应于：

$$ x_1 + 2x_3 = 3, \quad x_2 - x_3 = 4. $$

这里：

+   $x_1$ 和 $x_2$ 是主元变量（来自主元列 1 和 2）。

+   $x_3$ 是一个自由变量。

因此，$x_1$ 和 $x_2$ 依赖于 $x_3$：

$$ x_1 = 3 - 2x_3, \quad x_2 = 4 + x_3, \quad x_3 \text{自由}. $$

解集是无限的，由 $x_3$ 的自由度描述。

#### 几何意义

+   主元变量代表“固定”的坐标。

+   自由变量对应于解可以无限延伸的方向。

在二维空间中：

+   如果有一个主元变量和一个自由变量，解构成一条线。在三维空间中：

+   两个主元，一个自由变量→解构成一条线。

+   一个主元，两个自由变量→解构成一个平面。

因此，自由变量的数量决定了解集的维度。

#### 秩和自由变量

主元列的数量等于矩阵的秩。

如果系数矩阵 $A$ 是 $m \times n$：

+   行秩 = 主元的数量。

+   自由变量的数量 = $n - \text{rank}(A)$。

这就是秩-零度联系的实际应用：

$$ \text{变量的数量} = \text{rank} + \text{nullity}. $$

#### 逐步示例

系统：

$$ \begin{cases} x + 2y + z = 4 \\ 2x + 5y + z = 7 \end{cases} $$

增广矩阵：

$$ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 2 & 5 & 1 & | & 7 \end{bmatrix}. $$

化简：

+   $R_2 \to R_2 - 2R_1$ →

    $$ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 0 & 1 & -1 & | & -1 \end{bmatrix}. $$

现在：

+   主元列：1 和 2 → 变量 $x, y$。

+   自由列：3 → 变量 $z$。

解：

$$ x = 4 - 2y - z, \quad y = -1 + z, \quad z \text{ free}. $$

代入：

$$ (x, y, z) = (6 - 3z, \; -1 + z, \; z). $$

解在 3D 空间中形成一条线，由 $z$ 参数化。

#### 为什么领导者很重要

在简化行阶梯形式（RREF）中，每个主元都被缩放为 1，这使得隔离主元变量变得容易。没有主元，方程可能仍然正确，但更难解释。

例如：

$$ \begin{bmatrix} 2 & 0 & | & 6 \\ 0 & -3 & | & 9 \end{bmatrix} $$

变为

$$ \begin{bmatrix} 1 & 0 & | & 3 \\ 0 & 1 & | & -3 \end{bmatrix}. $$

解立即可见：$x=3, y=-3$。

#### 为什么它很重要

1.  识别主元可以显示哪些变量是确定的，哪些是自由的。

1.  主元的数量定义了秩，这是线性代数中的一个核心概念。

1.  自由变量决定了系统是否有唯一解、有无穷多解或无解。

1.  简化行阶梯形式（RREF）中的主元立即为解集提供了透明度。

#### 尝试自己

1.  化简

    $$ \begin{bmatrix} 1 & 3 & 1 & | & 5 \\ 2 & 6 & 2 & | & 10 \end{bmatrix} $$

    并识别主元和自由变量。

1.  对于该系统

    $$ x + y + z = 2, \quad 2x + 3y + 5z = 7, $$

    写出简化行阶梯形式（RREF）并用自由变量表示解。

1.  计算一个 3×5 矩阵的两个主元列的秩和自由变量的数量。

1.  挑战：证明如果主元的数量等于变量的数量，则该系统要么无解，要么有唯一解，但永远不会有无穷多解。

理解主元和自由变量是分类解集的关键：唯一解、无穷多解或无解。这种分类是解决线性系统问题的关键。

### 25. 解决一致系统

如果一个线性方程组至少有一个解，则称为一致。一致性是在处理系统时首先要检查的性质，因为在考虑唯一性或参数化之前，我们必须知道是否存在解。本节解释了如何识别一致系统，如何使用行简化法解决它们，以及如何用主元和自由变量的术语描述它们的解。

#### 一致性意味着什么

给定一个系统 $A\mathbf{x} = \mathbf{b}$：

+   一致：至少有一个解 $\mathbf{x}$ 满足该系统。

+   不一致：不存在解。

一致性取决于向量 $\mathbf{b}$ 和 $A$ 的列空间之间的关系：

$$ \mathbf{b} \in \text{Col}(A) \quad \iff \quad \text{system is consistent}. $$

如果 $\mathbf{b}$ 不能被表示为 $A$ 的列的线性组合，则该系统无解。

#### 使用行简化法检查一致性

为了测试一致性，将增广矩阵 $[A | \mathbf{b}]$ 化简为行阶梯形式。

+   如果你找到一个形式的行：

    $$ [0 \;\; 0 \;\; \dots \;\; 0 \;|\; c], \quad c \neq 0, $$

    则该系统是不一致的（矛盾：$0 = c$）。

+   如果没有出现此类矛盾，则系统是一致的。

#### 示例 1：具有唯一解的一致系统

系统：

$$ \begin{cases} x + y = 2 \\ x - y = 0 \end{cases} $$

增广矩阵：

$$ \begin{bmatrix} 1 & 1 & | & 2 \\ 1 & -1 & | & 0 \end{bmatrix}. $$

行简化：

+   $R_2 \to R_2 - R_1$:

    $$ \begin{bmatrix} 1 & 1 & | & 2 \\ 0 & -2 & | & -2 \end{bmatrix}. $$

+   $R_2 \to -\tfrac{1}{2}R_2$:

    $$ \begin{bmatrix} 1 & 1 & | & 2 \\ 0 & 1 & | & 1 \end{bmatrix}. $$

+   $R_1 \to R_1 - R_2$:

    $$ \begin{bmatrix} 1 & 0 & | & 1 \\ 0 & 1 & | & 1 \end{bmatrix}. $$

解：$x = 1, \; y = 1$。唯一解。

#### 示例 2：具有无限多解的一致系统

系统：

$$ \begin{cases} x + y + z = 3 \\ 2x + 2y + 2z = 6 \end{cases} $$

增广矩阵：

$$ \begin{bmatrix} 1 & 1 & 1 & | & 3 \\ 2 & 2 & 2 & | & 6 \end{bmatrix}. $$

行简化：

+   $R_2 \to R_2 - 2R_1$:

    $$ \begin{bmatrix} 1 & 1 & 1 & | & 3 \\ 0 & 0 & 0 & | & 0 \end{bmatrix}. $$

没有矛盾，所以一致。解：

$$ x = 3 - y - z, \quad y \text{ 自由}, \quad z \text{ 自由}. $$

解集是 $\mathbb{R}³$ 中的一个平面。

#### 示例 3：不一致系统（用于对比）

系统：

$$ \begin{cases} x + y = 1 \\ x + y = 2 \end{cases} $$

增广矩阵：

$$ \begin{bmatrix} 1 & 1 & | & 1 \\ 1 & 1 & | & 2 \end{bmatrix}. $$

行简化：

+   $R_2 \to R_2 - R_1$:

    $$ \begin{bmatrix} 1 & 1 & | & 1 \\ 0 & 0 & | & 1 \end{bmatrix}. $$

矛盾：$0 = 1$。不一致，无解。

#### 一致性的几何解释

+   在二维空间中：

    +   两条直线在一点相交 → 一致，唯一解。

    +   两条直线重合 → 一致，无限多解。

    +   两条平行且不同的直线 → 不一致，无解。

+   在三维空间中：

    +   三个平面在一点相交 → 唯一解。

    +   平面沿直线相交或重合 → 无限多解。

    +   平面无法相交（如三角形的“缝隙”）→ 无解。

#### 主元结构和解

+   唯一解：每个变量都是主元变量（没有自由变量）。

+   无限多解：至少存在一个自由变量，但没有矛盾。

+   无解：增广矩阵中出现矛盾行。

#### 为什么这很重要

1.  一致性是解决系统问题的第一个检查点。

1.  将系统分类为唯一、无限或无解，是线性代数的基础。

1.  理解一致性将代数（行操作）与几何（直线、平面、超平面的交点）联系起来。

1.  这些思想可以扩展：在数据科学和工程中，检查方程是否一致等价于询问模型是否拟合观察数据。

#### 尝试自己操作

1.  简化增广矩阵

    $$ \begin{bmatrix} 1 & 2 & 1 & | & 5 \\ 2 & 4 & 2 & | & 10 \\ 3 & 6 & 3 & | & 15 \end{bmatrix} $$

    并确定系统是否一致。

1.  将系统分类为具有唯一、无限或无解：

    $$ \begin{cases} x + y + z = 2 \\ x - y + z = 0 \\ 2x + 0y + 2z = 3 \end{cases} $$

1.  几何上解释当增广矩阵有矛盾行时意味着什么。

1.  挑战：通过代数证明，如果 $\mathbf{b}$ 位于 $A$ 的列的线性组合中，则系统一致。

一致系统标志着代数规则与几何现实之间的平衡点：它们是方程与空间和谐相遇的地方。

### 26. 检测不一致性

并非每个线性方程组都有解。有些是不一致的，这意味着方程相互矛盾，没有向量 $\mathbf{x}$ 可以同时满足它们。早期检测这种不一致性至关重要：它节省了试图解决不可能系统的无用功，并揭示了重要的几何和代数属性。

#### 不一致性在代数上的表现

考虑以下系统：

$$ \begin{cases} x + y = 1 \\ x + y = 3 \end{cases} $$

显然，这两个方程不可能同时成立。在增广矩阵形式中：

$$ \begin{bmatrix} 1 & 1 & | & 1 \\ 1 & 1 & | & 3 \end{bmatrix}. $$

行简化给出：

$$ \begin{bmatrix} 1 & 1 & | & 1 \\ 0 & 0 & | & 2 \end{bmatrix}. $$

底行表明 $0 = 2$，这是一个矛盾。这是不一致性的标志：系数部分中有一行零，增广部分中有一个非零常数。

#### 检测的一般规则

一个系统 $A\mathbf{x} = \mathbf{b}$ 不一致，如果在行简化后，增广矩阵包含如下形式的行：

$$ [0 \;\; 0 \;\; \dots \;\; 0 \;|\; c], \quad c \neq 0. $$

这表明所有变量从方程中消失，留下一个不可能的陈述，如 $0 = c$。

#### 示例 1：二维空间中的平行线

$$ \begin{cases} x + y = 2 \\ 2x + 2y = 5 \end{cases} $$

增广矩阵：

$$ \begin{bmatrix} 1 & 1 & | & 2 \\ 2 & 2 & | & 5 \end{bmatrix}. $$

行简化：

+   $R_2 \to R_2 - 2R_1$:

$$ \begin{bmatrix} 1 & 1 & | & 2 \\ 0 & 0 & | & 1 \end{bmatrix}. $$

矛盾：无解。从几何上看，这两个方程是永远不会相交的平行线。

#### 示例 2：三维空间中的矛盾平面

$$ \begin{cases} x + y + z = 1 \\ 2x + 2y + 2z = 2 \\ x + y + z = 3 \end{cases} $$

前两个方程已经冲突：同一个平面方程被迫等于两个不同的常数。

增广矩阵简化为：

$$ \begin{bmatrix} 1 & 1 & 1 & | & 1 \\ 0 & 0 & 0 & | & 0 \\ 0 & 0 & 0 & | & 2 \end{bmatrix}. $$

矛盾：无解。这些“平面”未能相交。

#### 不一致性的几何

+   在二维空间中：不一致系统对应于具有不同截距的平行线。

+   在三维空间中：它们对应于平行但偏移的平面，或者以留下“间隙”（没有共享交点）的方式排列的平面。

+   在更高维数中：不一致性意味着目标向量 $\mathbf{b}$ 位于 $A$ 的列空间之外。

#### 一致性检验的秩测试

检测不一致性的另一种方法是使用秩。

+   令 $\text{rank}(A)$ 为系数矩阵中的主元数量。

+   令 $\text{rank}([A|\mathbf{b}])$ 为增广矩阵中的主元数量。

规则：

+   如果 $\text{rank}(A) = \text{rank}([A|\mathbf{b}])$，则该系统是一致的。

+   如果 $\text{rank}(A) < \text{rank}([A|\mathbf{b}])$，则该系统是不一致的。

这个秩条件是基本的，并且在任何维度上都适用。

#### 为什么这很重要

1.  不一致性揭示了现实问题（物理学、工程学、统计学）中的过度确定或矛盾数据。

1.  通过行简化或秩的检测快速发现不一致性可以节省计算。

1.  它将几何（不相交的空间）与代数（矛盾行）联系起来。

1.  这为最小二乘法铺平了道路，在最小二乘法中，不一致的系统被近似而不是精确求解。

#### 尝试自己解决

1.  减少增广矩阵

$$ \begin{bmatrix} 1 & -1 & | & 2 \\ 2 & -2 & | & 5 \end{bmatrix} $$

并决定系统是否一致。

1.  从几何上说明为什么该系统

$$ x + y = 0, \quad x + y = 1 $$

是不一致的。

1.  使用秩测试来检查系统的一致性。

$$ \begin{cases} x + y + z = 2 \\ 2x + 2y + 2z = 4 \\ 3x + 3y + 3z = 5 \end{cases} $$

1.  挑战：使用列空间的概念解释为什么 $\text{rank}(A) < \text{rank}([A|\mathbf{b}])$ 意味着不一致。

发现不一致性不仅仅是发现矛盾——它将代数、几何和线性变换联系起来，精确地表明系统不可能组合在一起。

### 27. 手动高斯消元

高斯消元是通过行操作系统地解决线性方程组的过程，目的是将系统转换为行阶梯形式 (REF)，然后使用回代找到解。这种方法是线性代数计算的基础，也是大多数计算机算法解决线性系统的基础。

#### 核心思想

1.  将系统表示为增广矩阵。

1.  使用行操作逐步消除变量，从左到右，从上到下进行。

1.  当矩阵处于行最简形式 (REF) 时停止。

1.  通过回代求解三角系统。

#### 逐步配方

假设我们有一个 $n$ 个方程和 $n$ 个未知数的系统。

1.  在第一列选择一个主元（一个非零项）。如果需要，交换行以将一个非零项带到顶部。

1.  通过从下到上减去主元行的倍数来消去主元以下的元素，使得主元以下的全部项变为零。

1.  移到下一行和下一列，选择下一个主元，并重复消元。

1.  继续进行，直到所有主元都呈阶梯形式 (REF)。

1.  从底部行开始，使用回代求解未知数。

#### 示例 1：一个 2×2 系统

系统：

$$ \begin{cases} x + 2y = 5 \\ 3x + 4y = 11 \end{cases} $$

增广矩阵：

$$ \begin{bmatrix} 1 & 2 & | & 5 \\ 3 & 4 & | & 11 \end{bmatrix}. $$

1.  在 (1,1) 处进行主元操作 = 1。

1.  消去以下：$R_2 \to R_2 - 3R_1$。

    $$ \begin{bmatrix} 1 & 2 & | & 5 \\ 0 & -2 & | & -4 \end{bmatrix}. $$

1.  回代：从第 2 行：$-2y = -4 \implies y = 2$。代入第 1 行：$x + 2(2) = 5 \implies x = 1$。

解：$(x, y) = (1, 2)$。

#### 示例 2：一个 3×3 系统

系统：

$$ \begin{cases} x + y + z = 6 \\ 2x + 3y + z = 14 \\ x - y + 2z = 2 \end{cases} $$

增广矩阵：

$$ \begin{bmatrix} 1 & 1 & 1 & | & 6 \\ 2 & 3 & 1 & | & 14 \\ 1 & -1 & 2 & | & 2 \end{bmatrix}. $$

第 1 步：在(1,1)处进行旋转。消除以下：

+   $R_2 \to R_2 - 2R_1$.

+   $R_3 \to R_3 - R_1$.

$$ \begin{bmatrix} 1 & 1 & 1 & | & 6 \\ 0 & 1 & -1 & | & 2 \\ 0 & -2 & 1 & | & -4 \end{bmatrix}. $$

第 2 步：在(2,2)处进行旋转。消除以下：$R_3 \to R_3 + 2R_2$.

$$ \begin{bmatrix} 1 & 1 & 1 & | & 6 \\ 0 & 1 & -1 & | & 2 \\ 0 & 0 & -1 & | & 0 \end{bmatrix}. $$

第 3 步：在(3,3)处进行旋转。缩放行：$R_3 \to -R_3$.

$$ \begin{bmatrix} 1 & 1 & 1 & | & 6 \\ 0 & 1 & -1 & | & 2 \\ 0 & 0 & 1 & | & 0 \end{bmatrix}. $$

回代：

+   从第 3 行：$z = 0$.

+   从第 2 行：$y - z = 2 \implies y = 2$.

+   从第 1 行：$x + y + z = 6 \implies x = 4$.

解：$(x, y, z) = (4, 2, 0)$.

#### 为什么高斯消元法总是有效

+   每一步都减少了低阶方程中的变量数。

+   旋转确保稳定性（交换行以避免除以零）。

+   算法要么产生一个三角系统（可以通过代入求解），要么揭示不一致（矛盾行）。

#### 几何解释

+   消元对应于逐步限制解集：

    +   第一个方程 → $\mathbb{R}³$中的一个平面。

    +   添加第二个方程 → 交点变为一条线。

    +   添加第三个方程 → 交点变为一个点（唯一解）或消失（不一致）。

#### 它的重要性

1.  高斯消元法是手动和计算机求解系统的基础。

1.  它揭示了系统是否一致，以及解是否唯一或无限。

1.  它是 LU 分解、QR 分解和数值求解等高级方法的基础。

1.  它显示了代数（行操作）和几何（子空间的交集）之间的相互作用。

#### 试试看

1.  求解该系统

    $$ \begin{cases} 2x + y = 7 \\ 4x + 3y = 15 \end{cases} $$

    使用高斯消元法。

1.  减少

    $$ \begin{bmatrix} 1 & 2 & -1 & | & 3 \\ 3 & 8 & 1 & | & 12 \\ 2 & 6 & 3 & | & 11 \end{bmatrix} $$

    转换为 REF 并求解。

1.  练习一个具有无限多解的系统：

    $$ x + y + z = 4, \quad 2x + 2y + 2z = 8. $$

1.  挑战：解释为什么高斯消元法对于一个$n \times n$的系统，最多在$n$个主元步骤中终止。

高斯消元法将许多方程的复杂性转化为有序的过程，逐步使解的隐藏结构变得可见。

### 28. 回代和解集

一旦高斯消元将系统简化为行阶梯形（REF），下一步就是实际求解未知数。这个过程称为回代：我们从底部方程开始（涉及最少的变量）并逐步向上工作，逐步求解。回代是将结构化的三角系统转换为显式解的过程。

#### 行阶梯形结构

一个在行最简形式（REF）的系统看起来是这样的：

$$ \begin{bmatrix} - & * & * & * & | & * \\ 0 & * & * & * & | & * \\ 0 & 0 & * & * & | & * \\ 0 & 0 & 0 & * & | & * \end{bmatrix} $$

+   每一行对应一个比上一行变量少的方程。

+   底部方程只有一个或两个变量。

+   这种三角形形式使得“从下往上”求解成为可能。

#### 逐步示例：唯一解

消元后的系统：

$$ \begin{bmatrix} 1 & 2 & -1 & | & 3 \\ 0 & 1 & 2 & | & 4 \\ 0 & 0 & 1 & | & 2 \end{bmatrix}. $$

这对应于：

$$ \begin{cases} x + 2y - z = 3 \\ y + 2z = 4 \\ z = 2 \end{cases} $$

1.  从最后一个方程：$z = 2$。

1.  代入第二个方程：$y + 2(2) = 4 \implies y = 0$。

1.  代入第一个方程：$x + 2(0) - 2 = 3 \implies x = 5$。

解：$(x, y, z) = (5, 0, 2)$。

#### 带有自由变量的无限解

并非所有系统都能简化为唯一解。如果有自由变量（非主元列），回代将主元变量用自由变量表示。

示例：

$$ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 0 & 1 & -1 & | & 1 \\ 0 & 0 & 0 & | & 0 \end{bmatrix}. $$

方程：

$$ \begin{cases} x + 2y + z = 4 \\ y - z = 1 \end{cases} $$

1.  从第二行：$y = 1 + z$。

1.  从第一行：$x + 2(1 + z) + z = 4 \implies x = 2 - 3z$。

解集：

$$ (x, y, z) = (2 - 3t, \; 1 + t, \; t), \quad t \in \mathbb{R}. $$

这里 $z = t$ 是自由变量。解在 3D 中形成一个线。

#### 一般解结构

对于一个一致的系统：

1.  唯一解 → 每个变量都是主元变量（没有自由变量）。

1.  无穷多个解 → 一些自由变量仍然存在。解集由这些变量参数化，形成一个线、平面或更高维的子空间。

1.  无解 → 之前发现矛盾，因此回代不可能。

#### 几何意义

+   唯一解 → 直线/平面的单个交点。

+   无限解 → 重叠的子空间（例如，两个平面在一条线上相交）。

+   回代描述了这个交点的确切形状。

#### 示例：参数向量形式

对于上面的无限解示例：

$$ (x, y, z) = (2, 1, 0) + t(-3, 1, 1). $$

这将解集表示为一个基点加上一个方向向量，使几何意义清晰。

#### 为什么它很重要

1.  回代将行阶梯形转换为具体答案。

1.  它区分了唯一解与无限解。

1.  它为小系统提供了一种手动的系统化方法，并为大型系统的计算机算法奠定了基础。

1.  它揭示了解集的结构——是点、线、平面还是更高维的对象。

#### 尝试自己解决

1.  通过回代求解：

$$ \begin{bmatrix} 1 & -1 & 2 & | & 3 \\ 0 & 1 & 3 & | & 5 \\ 0 & 0 & 1 & | & 2 \end{bmatrix}. $$

1.  简化并求解：

$$ x + y + z = 2, \quad 2x + 2y + 2z = 4. $$

1.  将上述系统的解集表示为参数向量形式。

1.  挑战：对于一个有两个自由变量的 4×4 系统，解释为什么解集在$\mathbb{R}⁴$中形成一个平面。

回代完成消元过程，将三角结构转换为显式解，并展示了代数和几何如何在解集的分类中相遇。

### 29. 秩及其第一含义

秩的概念是线性代数的核心。它将解系统的代数、子空间的几何和矩阵的结构连接成一个统一的思想。秩衡量矩阵中独立信息的数量：有多少行或列携带独特的方向而不是重复或组合其他行或列。

#### 秩的定义

矩阵 $A$ 的秩是其行阶梯形式中的主元数量。等价地，它是：

+   列空间的维度（独立列的数量）。

+   行空间的维度（独立行的数量）。

所有这些定义是一致的。

#### 第一次遇到秩：主元计数

当用高斯消元法求解系统时：

+   每个主元对应一个确定的变量。

+   主元的数量等于秩。

+   自由变量的数量 = 总变量数 - 秩。

示例：

$$ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 0 & 1 & -1 & | & 2 \\ 0 & 0 & 0 & | & 0 \end{bmatrix}. $$

这里，有 2 个主元。所以：

+   秩 = 2。

+   总共有 3 个变量，有 1 个自由变量。

#### 从独立性的角度看待秩

一组向量线性无关，如果其中没有一个可以表示为其他向量的组合。

+   矩阵的秩告诉我们它有多少个独立的行或列。

+   如果某些列是其他列的组合，它们不会增加秩。

示例：

$$ \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\ 3 & 6 & 9 \end{bmatrix}. $$

在这里，每一行都是第一行的倍数。秩 = 1，因为只有一个独立的行/列方向存在。

#### 秩与系统的解

考虑 $A\mathbf{x} = \mathbf{b}$。

+   如果 $\text{rank}(A) = \text{rank}([A|\mathbf{b}])$，则系统是一致的。

+   如果不是，则不一致。

+   如果秩等于变量的数量，则系统有唯一解。

+   如果秩小于变量的数量，则有无穷多个解。

因此，秩对解集进行分类。

#### 秩与几何

秩告诉我们由行或列生成的子空间的维度。

+   秩 1：所有信息都沿一条线。

+   秩 2：位于一个平面内。

+   秩 3：填充 3D 空间。

示例：

+   在 $\mathbb{R}³$ 中，秩为 2 的矩阵的列通过原点形成一个平面。

+   矩阵的秩为 1 时，所有列都在一条直线上。

#### 秩与行对列视图

这是一个显著的事实，即独立行的数量等于独立列的数量。乍一看，这并不明显，但它始终是正确的。因此，我们可以通过行或列来定义秩——这没有区别。

#### 为什么这很重要

1.  秩是代数和几何之间的桥梁：主元 ↔ 维度。

1.  它对方程组的解进行分类。

1.  它衡量数据中的冗余（在统计学、机器学习、信号处理中很重要）。

1.  这为高级概念如零空间、秩-零定理和奇异值分解铺平了道路。

#### 试试你自己

1.  找到

    $$ \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 5 \\ 3 & 6 & 8 \end{bmatrix}. $$

1.  解这个系统

    $$ x + y + z = 2, \quad 2x + 2y + 2z = 4, $$

    并确定系数矩阵的秩。

1.  在 $\mathbb{R}³$ 中，秩为 2 的 3×3 矩阵的几何意义是什么？

1.  挑战：通过考虑矩阵的行阶梯形来证明行秩总是等于列秩。

行秩是线性代数中第一个真正统一的观念：它告诉我们矩阵包含多少独立结构，并为理解空间、维度和变换奠定了基础。

### 30. LU 分解

高斯消元不仅解系统，还揭示了更深层次的结构：许多矩阵可以被分解成更简单的部分。其中最有用的是 LU 分解，其中矩阵 $A$ 被写成下三角矩阵 $L$ 和上三角矩阵 $U$ 的乘积。这种分解以紧凑的形式捕捉了所有消元步骤，并允许高效地解系统。

#### 什么是 LU 分解？

如果 $A$ 是一个 $n \times n$ 矩阵，那么

$$ A = LU, $$

其中：

+   $L$ 是下三角矩阵（对角线以下的元素可能不为零，对角线元素 = 1）。

+   $U$ 是上三角矩阵（对角线以上的元素可能不为零）。

这意味着：

+   $U$ 存储了消元的结果（三角系统）。

+   $L$ 记录了消元过程中使用的乘数。

#### 示例：2×2 情况

取

$$ A = \begin{bmatrix} 2 & 3 \\ 4 & 7 \end{bmatrix}. $$

消元：$R_2 \to R_2 - 2R_1$.

+   乘数 = 2（用于消除元素 4）。

+   结果 $U$：

    $$ U = \begin{bmatrix} 2 & 3 \\ 0 & 1 \end{bmatrix}. $$

+   $L$:

    $$ L = \begin{bmatrix} 1 & 0 \\ 2 & 1 \end{bmatrix}. $$

检查：

$$ LU = \begin{bmatrix} 1 & 0 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} 2 & 3 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 3 \\ 4 & 7 \end{bmatrix} = A. $$

#### 示例：3×3 情况

$$ A = \begin{bmatrix} 2 & 1 & 1 \\ 4 & -6 & 0 \\ -2 & 7 & 2 \end{bmatrix}. $$

第 1 步：消去基下的元素（第 1 行）。

+   乘数 $m_{21} = 4/2 = 2$。

+   乘数 $m_{31} = -2/2 = -1$。

第 2 步：消去第 2 列的基下元素。

+   替换后，收集乘数和主元。

结果：

$$ L = \begin{bmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix}, \quad U = \begin{bmatrix} 2 & 1 & 1 \\ 0 & -8 & -2 \\ 0 & 0 & 1 \end{bmatrix}. $$

因此 $A = LU$.

#### 使用 LU 解系统

假设 $Ax = b$。如果 $A = LU$：

1.  通过前向代入解 $Ly = b$（因为 $L$ 是下三角矩阵）。

1.  通过回代解 $Ux = y$（因为 $U$ 是上三角矩阵）。

这个两步过程比每次从头开始消元要快得多，尤其是当解决具有相同 $A$ 但不同 $b$ 的多个系统时。

#### 交换和排列

有时消元需要行交换（以避免除以零或不稳定）。然后分解写成：

$$ PA = LU, $$

其中 $P$ 是一个置换矩阵，记录了行交换。这是数值计算中使用的实际形式。

#### $LU$ 分解的应用

1.  高效求解：多个右端 $Ax = b$。计算一次 $LU$，然后对每个 $b$ 重复使用。

1.  行列式：$\det(A) = \det(L)\det(U)$。由于 $L$ 的对角线为 1，这简化为 $U$ 对角线的乘积。

1.  矩阵逆：通过求解 $Ax = e_i$ 对每一列 $e_i$，我们可以使用 $LU$ 高效地计算 $A^{-1}$。

1.  数值方法：$LU$ 在科学计算、工程模拟和优化中处于核心地位。

#### 几何意义

$LU$ 分解将消元过程分解为：

+   $L$：剪切变换（添加行的倍数）。

+   $U$：缩放和对齐到三角形形式。

一起，它们代表与 $A$ 相同的线性变换，但分解成更简单的构建块。

#### 为什么它很重要

1.  $LU$ 分解将消元压缩成一个可重用的格式。

1.  它是数值线性代数的基础，几乎在每一个求解器中都被使用。

1.  它将计算（有效的算法）与理论（变换的分解）联系起来。

1.  它介绍了这样一个更广泛的概念：矩阵可以被分解成简单、可解释的部分。

#### 尝试自己操作

1.  因子

    $$ A = \begin{bmatrix} 1 & 2 \\ 3 & 8 \end{bmatrix} $$

    分解为 $LU$。

1.  求解

    $$ \begin{bmatrix} 2 & 1 \\ 6 & 3 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 5 \\ 15 \end{bmatrix} $$

    使用 $LU$ 分解。

1.  计算 $\det(A)$。

    $$ A = \begin{bmatrix} 2 & 1 & 1 \\ 4 & -6 & 0 \\ -2 & 7 & 2 \end{bmatrix} $$

    通过使用其 $LU$ 分解。

1.  挑战：证明如果 $A$ 是可逆的，那么它有一个 $LU$ 分解（可能经过行交换）。

$LU$ 分解将消元组织成一个强大的工具：紧凑、高效，并且与线性代数的理论和实践紧密相连。

#### 结束语

```py
Paths diverge or merge,
pivots mark the way forward,
truth distilled in rows.
```

## 第四章：向量空间与子空间

#### 开场

```py
Endless skies expand,
spaces within spaces grow,
freedom takes its shape.
```

### 31. 向量空间的公理

到目前为止，我们一直在处理 $\mathbb{R}²$、$\mathbb{R}³$ 和更高维的欧几里得空间中的向量。但线性代数的真正力量来自于从坐标中抽象出来。向量空间不局限于物理空间中的箭头——它是任何满足某些规则的对象集合，只要它们的行为像向量。这些规则被称为向量空间的公理。

#### 向量空间的概念

向量空间是一个带有两个运算的集合 $V$：

1.  向量加法：将 $V$ 中的两个向量组合起来得到 $V$ 中的另一个向量。

1.  数乘：将 $V$ 中的向量乘以一个标量（来自域的一个数，通常是 $\mathbb{R}$ 或 $\mathbb{C}$）。

奇迹在于，只要某些规则（公理）成立，$V$ 中的对象就可以被视为向量。它们不需要是箭头或坐标列表——它们可以是多项式、函数、矩阵或序列。

#### 八个公理

设 $u, v, w \in V$（向量）和 $a, b \in \mathbb{R}$（标量）。公理如下：

1.  加法封闭性：$u + v \in V$.

1.  加法的交换律：$u + v = v + u$.

1.  加法的结合律：$(u + v) + w = u + (v + w)$.

1.  加法单位元的存在：存在一个零向量 $0 \in V$，使得 $v + 0 = v$。

1.  加法逆元的存在：对于每个 $v$，存在 $-v$ 使得 $v + (-v) = 0$。

1.  标量乘法封闭性：$a v \in V$.

1.  标量乘法对向量加法的分配律：$a(u + v) = au + av$.

1.  标量乘法对标量加法的分配律：$(a + b)v = av + bv$。

1.  标量乘法的结合律：$a(bv) = (ab)v$.

1.  乘法单位元的存在：$1 \cdot v = v$.

（有时这些被列为一共有八个，其中一些被分组在一起，但本质是相同的。）

#### 向量空间示例

1.  欧几里得空间：具有标准加法和标量乘法的 $\mathbb{R}^n$。

1.  多项式：所有实系数多项式的集合，$\mathbb{R}[x]$。

1.  函数：所有在 $[0,1]$ 上的连续函数的集合，具有函数加法和标量乘法。

1.  矩阵：所有 $m \times n$ 实数矩阵的集合。

1.  序列：所有无限实数序列 $(a_1, a_2, \dots)$ 的集合。

所有这些都满足向量空间的公理。

#### 非示例

1.  自然数集 $\mathbb{N}$ 不是一个向量空间（没有加法逆元）。

1.  正实数集 $\mathbb{R}^+$ 不是一个向量空间（不封闭于负数的标量乘法）。

1.  恰好二次多项式的集合不是一个向量空间（不封闭于加法：$x² + x² = 2x²$ 仍然是二次，但 $x² - x² = 0$，这是零次，不允许）。

这些例子说明了公理的重要性：没有它们，结构就会崩溃。

#### 零向量

每个向量空间都必须包含一个零向量。这是不可选择的。它是加法中的“不做任何事情”的元素。在 $\mathbb{R}^n$ 中，这是 $(0,0,\dots,0)$。在多项式中，它是零多项式。在函数空间中，它是函数 $f(x) = 0$。

#### 加法逆元

对于每个向量 $v$，我们要求 $-v$。这确保了方程 $u+v=w$ 总是可以重新排列为 $u=w-v$。没有加法逆元，解线性方程就不会工作。

#### 标量和域

标量来自域：通常是实数 $\mathbb{R}$ 或复数 $\mathbb{C}$。标量的选择很重要：

+   在 $\mathbb{R}$ 上，多项式空间与在 $\mathbb{C}$ 上的不同。

+   在有限域（如模 $p$ 的整数）中，向量空间存在于离散数学和编码理论中。

#### 几何解释

+   公理保证了向量可以以可预测的方式相加和缩放。

+   封闭性确保空间是“自包含”的。

+   加法逆元确保对称性：每个方向都可以反转。

+   分配律确保了缩放和加法之间的一致性。

这些规则共同使得向量空间成为稳定可靠的数学对象。

#### 为什么它很重要

1.  向量空间将许多数学领域统一在一个框架下。

1.  它们将 $\mathbb{R}^n$ 推广到函数、多项式以及更多。

1.  公理保证了所有线性代数的工具——张成、基、维度、线性映射——都适用。

1.  识别隐藏的向量空间是高级数学和物理学中的一个重要步骤。

#### 试试你自己

1.  验证所有 2×2 矩阵的集合在矩阵加法和数乘下是一个向量空间。

1.  证明次数最多为 3 的多项式的集合是一个向量空间，但次数正好为 3 的多项式的集合不是。

1.  检查所有偶函数 $f(-x) = f(x)$ 的集合是否是一个向量空间。

1.  挑战：考虑 $[0,1]$ 上所有可微函数 $f$ 的集合。证明这个集合在通常操作下形成一个向量空间。

向量空间的公理为线性代数的其余部分提供了基础。所有随后的内容——子空间、独立性、基、维度——都自然地从这个正式框架中生长出来。

### 32. 子空间、列空间和零空间

一旦向量空间的概念确立，下一步就是识别存在于更大向量空间内部的较小向量空间。这些被称为子空间。子空间在线性代数中处于核心地位，因为它们揭示了矩阵和线性系统的内部结构。两个特殊的子空间——列空间和零空间——扮演着特别重要的角色。

#### 什么是子空间？

向量空间 $V$ 的子空间 $W$ 是 $V$ 的一个子集，在该子集下，它本身也是一个向量空间。要成为子空间，$W$ 必须满足：

1.  零向量 $0$ 在 $W$ 中。

1.  如果 $u, v \in W$，那么 $u+v \in W$（加法封闭）。

1.  如果 $u \in W$ 且 $c$ 是一个标量，那么 $cu \in W$（数乘封闭）。

就这样，不需要进一步检查所有十个向量空间公理，因为那些是从 $V$ 继承的。

#### 子空间的简单例子

+   在 $\mathbb{R}³$ 中：

    +   通过原点的直线是一个一维子空间。

    +   通过原点的平面是一个二维子空间。

    +   整个空间本身也是一个子空间。

    +   平凡的子空间 $\{0\}$ 只包含零向量。

+   在多项式空间中：

    +   所有次数 ≤ 3 的多项式构成一个子空间。

    +   所有常数项为零的多项式构成一个子空间。

+   在函数空间中：

    +   $[0,1]$ 上所有连续函数构成 $[0,1]$ 上所有函数的子空间。

    +   所有线性微分方程的解构成一个子空间。

#### 矩阵的列空间

给定一个矩阵 $A$，列空间是它所有列的线性组合的集合。形式上，

$$ C(A) = \{ A\mathbf{x} : \mathbf{x} \in \mathbb{R}^n \}. $$

+   如果 $A$ 是 $m \times n$ 的，则列空间位于 $\mathbb{R}^m$ 内。

+   它代表由 $A$ 定义的线性变换的所有可能输出。

+   它的维度等于 $A$ 的秩。

示例：

$$ A = \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 3 & 6 \end{bmatrix}. $$

第二列只是第一列的两倍。所以列空间是 $\begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}$ 的所有倍数，这是一条在 $\mathbb{R}³$ 中的直线。秩 = 1。

#### 矩阵的零空间

矩阵 $A$ 的零空间（或核）是所有满足 $\mathbf{x}$ 的向量的集合，

$$ A\mathbf{x} = 0. $$

+   如果 $A$ 是 $m \times n$，它生活在 $\mathbb{R}^n$ 中。

+   它代表了在变换下坍缩为零的“看不见”的方向。

+   它的维度是 $A$ 的零度。

示例：

$$ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}. $$

解 $A\mathbf{x} = 0$。这产生一个由一个向量张成的零空间，意味着它是在 $\mathbb{R}³$ 中通过原点的直线。

#### 列空间与零空间

+   列空间：描述输出（$y$ 值可以到达）。

+   零空间：描述隐藏的输入（消失的方向）。

一起，它们捕捉了矩阵的全部行为。

#### 几何解释

+   在 $\mathbb{R}³$ 中，列空间可以是三维空间内的一个平面或一条直线。

+   零空间与行空间正交（在精确的意义上），我们将在后面研究。

+   理解这两个空间可以全面了解矩阵如何变换向量。

#### 为什么这很重要

1.  子空间是线性代数的自然栖息地：几乎所有事情都发生在它们内部。

1.  列空间解释了哪些系统 $Ax=b$ 是可解的。

1.  零空间解释了为什么某些系统有多个解（自由变量）。

1.  这些思想可以扩展到高级主题，如特征向量、奇异值分解和微分方程。

#### 试试你自己

1.  证明集合 $\{(x,y,0) : x,y \in \mathbb{R}\}$ 是 $\mathbb{R}³$ 的一个子空间。

1.  对于

    $$ A = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 0 & 0 \\ 1 & 2 & 3 \end{bmatrix}, $$

    找出列空间及其维度。

1.  对于相同的 $A$，计算零空间及其维度。

1.  挑战：证明矩阵 $A$ 的零空间始终是 $\mathbb{R}^n$ 的一个子空间。

子空间——特别是列空间和零空间——是每个矩阵内部隐藏几何学的第一印象，展示了哪些方向幸存，哪些消失。

### 33. 张和生成集

张的概念捕捉了从旧向量构建新向量的最简单和最强大的方式：通过取线性组合。张不仅是一组散点，而是一组结构化、完整的所有给定向量组合的集合。理解张直接导致基、维度和子空间结构的概念。

#### 张的 定义

给定向量 $v_1, v_2, \dots, v_k \in V$，这些向量的张量是

$$ \text{span}\{v_1, v_2, \dots, v_k\} = \{a_1 v_1 + a_2 v_2 + \dots + a_k v_k : a_i \in \mathbb{R}\}. $$

+   张是所有可能的向量线性组合的集合。

+   它始终是一个子空间。

+   给定的向量称为生成集。

#### 简单示例

1.  在 $\mathbb{R}²$ 中：

    +   $(1,0)$ 的张量 = x 轴的所有倍数（一条直线）。

    +   $(1,0)$ 和 $(0,1)$ 的张量 = 整个平面 $\mathbb{R}²$。

    +   $(1,0)$ 和 $(2,0)$ 的张量 = 仍然是 x 轴，因为第二个向量是冗余的。

1.  在 $\mathbb{R}³$ 中：

    +   单个向量的张量 = 一条线。

    +   两个线性无关向量的张量 = 通过原点的平面。

    +   三个独立向量的张量 = 整个空间 $\mathbb{R}³$。

#### 张量作为覆盖

+   如果你把向量看作“方向”，张量就是你可以通过这些方向行走所能到达的一切，允许任何步长（标量）。

+   如果你只有一个方向，你可以在一条线上来回走。

+   有两个独立的方向，你可以扫出一个平面。

+   在 3D 中有三个独立的方向，你可以移动到任何地方。

#### 生成集

如果一组向量的张量等于子空间，那么这组向量是该子空间的生成集（或张量集）。

+   示例：$\{(1,0), (0,1)\}$ 生成 $\mathbb{R}²$。

+   示例：$\{(1,0,0), (0,1,0), (0,0,1)\}$ 生成 $\mathbb{R}³$。

+   示例：矩阵的列生成其列空间。

不同的生成集可以张量出相同的空间。有些可能是冗余的，有些可能是极小的。后来，基的概念细化了这个想法。

#### 张量集中的冗余

+   如果一个向量是其他向量的线性组合，它不会扩大张量。

+   示例：在 $\mathbb{R}²$ 中，$\{(1,0), (0,1), (1,1)\}$ 张量出与 $\{(1,0), (0,1)\}$ 相同的空间。

+   消除冗余导致更有效的生成集。

#### 张量与线性系统

考虑系统 $Ax=b$。

+   问题“是否有解？”等价于“$b$ 是否在 $A$ 的列的张量中？”

+   因此，张量提供了求解的几何语言。

#### 它的重要性

1.  张量是定义由向量生成的子空间的基础。

1.  它直接与线性方程的求解相关。

1.  它引入了冗余的概念，为基和独立性做准备。

1.  它自然地推广到函数空间和抽象向量空间。

#### 尝试自己操作

1.  在 $\mathbb{R}²$ 中找到 $\{(1,2), (2,4)\}$ 的张量。

1.  证明向量 $(1,0,1), (0,1,1), (1,1,2)$ 只在 $\mathbb{R}³$ 中张量出一个平面。

1.  判断 $(1,2,3)$ 是否在 $(1,0,1)$ 和 $(0,1,2)$ 的张量中。

1.  挑战：证明所有多项式 $\{1, x, x², \dots\}$ 的集合张量出所有多项式的空间。

张量的概念改变了我们的视角：我们不再关注单个向量，而是看到它们生成的整个可能性景观。

### 34. 线性独立性与依赖性

在引入张量和生成集之后，自然会提出这样的问题：*张量集中的向量何时是真正必要的，何时是冗余的？* 这导致了线性独立性的想法。它是区分本质向量（那些添加新方向的向量）和依赖向量（那些可以用其他向量表示的向量）的精确方法。

#### 线性独立性的定义

如果一组向量 $\{v_1, v_2, \dots, v_k\}$ 的唯一解是

$$ a_1 v_1 + a_2 v_2 + \dots + a_k v_k = 0 $$

是

$$ a_1 = a_2 = \dots = a_k = 0. $$

如果存在非平凡解（某些 $a_i \neq 0$），则向量线性相关。

#### 直觉

+   独立向量指向真正不同的方向。

+   相关向量重叠：至少有一个可以由其他向量构建。

+   从张成的角度来看：移除一个相关向量不会缩小张成空间，因为它没有添加新的方向。

#### $\mathbb{R}²$ 中的简单例子

1.  $(1,0)$ 和 $(0,1)$ 是独立的。

    +   方程 $a(1,0) + b(0,1) = (0,0)$ 使得 $a = b = 0$。

1.  $(1,0)$ 和 $(2,0)$ 是相关的。

    +   方程 $2(1,0) - (2,0) = (0,0)$ 显示了相关性。

1.  在 $\mathbb{R}²$ 中，任何 3 个向量集都是相关的，因为空间的维度是 2。

#### $\mathbb{R}³$ 中的例子

1.  $(1,0,0), (0,1,0), (0,0,1)$ 是独立的。

1.  $(1,2,3), (2,4,6)$ 是相关的，因为第二个是第一个的两倍。

1.  $(1,0,1), (0,1,1), (1,1,2)$ 是相关的：第三个是前两个的和。

#### 使用矩阵检测独立性

将向量作为列放入矩阵中。进行行简化：

+   如果每一列都有一个主元 → 集合是独立的。

+   如果某些列是自由的 → 集合是相关的。

示例：

$$ \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & 0 \end{bmatrix}. $$

这里第三列没有主元 → 第三个向量依赖于前两个。

#### 与维度的关系

+   在 $\mathbb{R}^n$ 中，最多存在 $n$ 个独立向量。

+   如果你有的超过 $n$，则线性相关是保证的。

+   向量空间的一个基是一个简单的最大独立集，它可以覆盖整个空间。

#### 几何解释

+   独立向量 = 不同方向。

+   相关向量 = 一个向量位于其他向量的张成空间中。

+   在 2D 中：两个独立向量可以覆盖整个平面。

+   在 3D 中：三个独立向量可以覆盖整个空间。

#### 为什么这很重要

1.  独立性确保生成集是最小和高效的。

1.  它确定一个向量系统是否是基。

1.  它直接与秩相关：秩 = 独立列（或行）的数量。

1.  在几何、数据压缩和机器学习中至关重要——在这些领域中必须识别并消除冗余。

#### 试试你自己

1.  测试 $(1,2)$ 和 $(2,4)$ 是否独立。

1.  向量 $(1,0,0), (0,1,0), (1,1,0)$ 在 $\mathbb{R}³$ 中是独立的吗？

1.  将向量 $(1,0,1), (0,1,1), (1,1,2)$ 放入矩阵中，并进行行简化以检查独立性。

1.  挑战：证明 $\mathbb{R}^n$ 中任意 $n+1$ 个向量集都是线性相关的。

线性相关性是将基本方向与冗余方向区分开来的工具。它是定义基、计算维度和理解所有向量空间结构的关键。

### 35. 基和坐标

生成和线性无关的概念在基的强大思想中结合在一起。基给出了生成整个向量空间所需的最小构建块集合，没有冗余。一旦选择了基，空间中的每个向量都可以通过其坐标（一个数字列表）唯一地描述。

#### 什么是基？

向量空间$V$的基是满足两个性质的向量集合$\{v_1, v_2, \dots, v_k\}$：

1.  生成性质：$\text{span}\{v_1, \dots, v_k\} = V$。

1.  独立性质：向量是线性无关的。

简而言之：基是一个没有冗余的生成集。

#### 示例：标准基

1.  在$\mathbb{R}²$中，标准基是$\{(1,0), (0,1)\}$。

1.  在$\mathbb{R}³$中，标准基是$\{(1,0,0), (0,1,0), (0,0,1)\}$。

1.  在$\mathbb{R}^n$中，标准基是由单位向量组成的集合，每个向量在一个位置上有 1，其他位置为 0。

这些被称为标准基，因为它们是描述坐标的默认方式。

#### 坐标唯一性

关于基的一个重要事实是它们提供了向量的唯一表示。

+   给定一个基$\{v_1, \dots, v_k\}$，任何向量$x \in V$都可以唯一地表示为：

    $$ x = a_1 v_1 + a_2 v_2 + \dots + a_k v_k. $$

+   系数$(a_1, a_2, \dots, a_k)$是$x$相对于该基的坐标。

这种唯一性将基与任意生成集区分开来，在生成集中冗余允许有多个表示。

#### $\mathbb{R}²$中的示例

设基为$\{(1,0), (0,1)\}$。

+   向量$(3,5) = 3(1,0) + 5(0,1)$。

+   相对于这个基的坐标：$(3,5)$。

如果我们切换到另一个基，坐标会改变，尽管向量本身并没有改变。

#### 非标准基的示例

在$\mathbb{R}²$中，基是$\{(1,1), (1,-1)\}$。找出$x = (2,0)$的坐标。

解方程$a(1,1) + b(1,-1) = (2,0)$。这给出系统：

$$ a + b = 2, \quad a - b = 0. $$

所以$a=1, b=1$。相对于这个基的坐标：$(1,1)$。

注意：坐标取决于基的选择。

#### 函数空间基

1.  对于次数≤2 的多项式：基为$\{1, x, x²\}$。

    +   示例：$2 + 3x + 5x²$的坐标是$(2,3,5)$。

1.  对于$[0,1]$上的连续函数，一个可能的基是无限集合$\{1, x, x², \dots\}$。

这表明基并不局限于几何向量。

#### 维度

基中向量的数量是向量空间的维度。

+   $\mathbb{R}²$的维度是 2。

+   $\mathbb{R}³$的维度是 3。

+   次数≤3 的多项式空间维度是 4。

维度告诉我们空间中存在多少个独立的方向。

#### 基变换

+   从一个基切换到另一个基就像翻译语言一样。

+   同一个向量根据你使用的“词典”（基）看起来不同。

+   基变换矩阵允许在坐标系之间进行系统性的转换。

#### 几何解释

+   基就像在一个空间中设置坐标轴。

+   在 2D 中，两个独立的向量定义了一个网格。

+   在 3D 中，三个独立的向量定义了一个完整的坐标系。

+   不同的基 = 在同一空间上叠加的不同网格。

#### 为什么这很重要

1.  基础提供了向量空间最简单可能的描述。

1.  它们允许我们为向量分配唯一的坐标。

1.  它们将空间的抽象结构与具体的数值表示联系起来。

1.  这一概念几乎涵盖了线性代数的所有内容：维度、变换、特征向量等等。

#### 试试你自己

1.  证明 $\{(1,2), (3,4)\}$ 是 $\mathbb{R}²$ 的一个基。

1.  用基 $\{(1,1), (1,-1)\}$ 表示 $(4,5)$。

1.  证明 $\mathbb{R}³$ 的任何基都不能超过 3 个向量。

1.  挑战：证明集合 $\{1, \cos x, \sin x\}$ 是所有 $1, \cos x, \sin x$ 线性组合的空间的一个基。

基础是向量空间最简洁、最优雅的基石，通过提供有限个独立的构建块，将无限转化为可管理的。

### 36. 维度

维度是线性代数中最深刻和最具统一性的思想之一。它给出了一个单一的数字，捕捉了向量空间的“大小”或“容量”：它有多少个独立的方向。与日常几何中的长度、宽度或高度不同，线性代数中的维度适用于任何类型的空间——几何的、代数的，甚至是函数空间。

#### 定义

向量空间 $V$ 的维度是任何基中向量的数量。

+   由于向量空间的所有基都具有相同数量的元素，因此维度是良好定义的。

+   如果 $\dim V = n$，那么：

    +   $V$ 中任何超过 $n$ 个向量的集合都是相关的。

    +   每组恰好 $n$ 个独立的向量构成一个基。

#### 熟悉空间中的例子

1.  $\dim(\mathbb{R}²) = 2$。

    +   基础：$(1,0), (0,1)$。

    +   两个方向可以覆盖整个平面。

1.  $\dim(\mathbb{R}³) = 3$。

    +   基础：$(1,0,0), (0,1,0), (0,0,1)$。

    +   三个独立的方向可以构成 3D 空间。

1.  所有次数 ≤ 2 的多项式的集合具有维度 3。

    +   基础：$\{1, x, x²\}$。

1.  所有 $m \times n$ 矩阵的空间具有维度 $mn$。

    +   每个元素都是独立的，标准基由只有一个 1 和其余为 0 的矩阵组成。

#### 有限维与无限维

+   有限维空间：$\mathbb{R}^n$，次数 ≤ $k$ 的多项式。

+   无限维空间：

    +   所有多项式的空间（没有次数限制）。

    +   所有连续函数的空间。

    +   这些不能由有限个向量的集合生成。

#### 维度和子空间

+   任何 $\mathbb{R}^n$ 的子空间维度 ≤ $n$。

+   通过原点的 $\mathbb{R}³$ 中的直线：维度 1。

+   通过原点的 $\mathbb{R}³$ 中的平面：维度 2。

+   整个空间：维度 3。

+   平凡的子空间 $\{0\}$：维度 0。

#### 维度和方程组系统

当求解 $A\mathbf{x} = \mathbf{b}$：

+   列空间的维度 = 秩 = 输出中的独立方向的数量。

+   零空间的维度 = 自由变量的数量。

+   根据秩-零度定理：

    $$ \dim(\text{column space}) + \dim(\text{null space}) = \text{number of variables}. $$

#### 几何意义

+   维度计算描述一个向量所需的坐标的最小数量。

+   在 $\mathbb{R}²$ 中，你需要 2 个数字。

+   在 $\mathbb{R}³$ 中，你需要 3 个数字。

+   在三次或以下的多项式空间中，你需要 4 个系数。

因此，维度 = 坐标列表的长度。

#### 实际中检查维度

1.  将候选向量作为矩阵的列放置。

1.  将其化简为阶梯形。

1.  计算主元。那个数字 = 这些向量的张成空间的维度。

#### 它为什么重要

1.  维度是向量空间的最基本度量。

1.  它告诉我们空间“大小”或“复杂度”。

1.  它设定了绝对限制：在 $\mathbb{R}^n$ 中，不超过 $n$ 个独立的向量存在。

1.  它是坐标系、基和变换的基础。

1.  它将几何（线、面、体积）与代数（解、方程、矩阵）联系起来。

#### 试试看自己来做

1.  $(1,2,3)$，$(2,4,6)$，$(0,0,0)$ 张成的空间的维度是多少？

1.  找出由 $x+y+z=0$ 定义的 $\mathbb{R}³$ 子空间的维度。

1.  证明所有 $2 \times 2$ 对称矩阵的集合具有维度 3。

1.  挑战：证明次数 ≤ $k$ 的多项式空间具有维度 $k+1$。

维度是线性代数的测量尺：它告诉我们描述整个空间需要多少个独立的信息块。

### 37. 行秩-零空间定理

行秩-零空间定理是线性代数的一个核心结果。它给出了矩阵两个基本方面的精确平衡：其列空间的维度（行秩）和其零空间的维度（零空间维数）。它表明，无论矩阵看起来多么复杂，其“可见”输出和“隐藏”的零方向之间的信息分布总是遵循严格的规律。

#### 定理的陈述

设 $A$ 为一个 $m \times n$ 矩阵（映射 $\mathbb{R}^n \to \mathbb{R}^m$)：

$$ \text{rank}(A) + \text{nullity}(A) = n $$

其中：

+   $\text{rank}(A)$ = $A$ 的列空间的维度。

+   $\text{nullity}(A)$ = $A$ 的零空间的维度。

+   $n$ = $A$ 的列数，即变量的数量。

#### 直觉

将候选向量作为矩阵的列放置：

+   行秩衡量了多少个独立的输出方向得以保留。

+   零空间维数衡量了多少个输入方向“丢失”（映射到零）。

+   定理表明：总输入 = 有用方向（行秩）+ 浪费方向（零空间维数）。

这确保了没有神秘消失的东西——每个输入方向都被考虑在内。

#### 示例 1：满秩

$$ A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ \end{bmatrix}. $$

+   行秩 = 2（两个独立列）。

+   零空间 = $\{0\}$，所以零空间维数 = 0。

+   行秩 + 零空间 = 2 = 变量的数量。

#### 示例 2：相关列

$$ A = \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 3 & 6 \\ \end{bmatrix}. $$

+   第二列是第一列的倍数。行秩 = 1。

+   零空间包含所有 $y = -2x$ 的向量 $(x,y)$。零空间维数 = 1。

+   秩 + 零度 = 1 + 1 = 2 = 变量数。

#### 示例 3：更大的系统

$$ A = \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \end{bmatrix}. $$

+   列向量：$(1,0), (0,1), (1,1)$.

+   只有两个独立列 → 秩 = 2.

+   零空间：解 $x + z = 0, y + z = 0 \Rightarrow (x,y,z) = (-t,-t,t)$. 零度 = 1.

+   秩 + 零度 = 2 + 1 = 3 = 变量数。

#### 证明草图（概念性）

1.  将 $A$ 简化为阶梯形。

1.  主元对应于独立列 → 个数 = 行秩。

1.  自由变量对应于零空间方向 → 个数 = 零度。

1.  每一列要么是主元列，要么对应一个自由变量，所以：

    $$ \text{秩} + \text{零度} = \text{列数}. $$

#### 几何意义

+   在 $\mathbb{R}³$ 中，如果一个变换将所有向量折叠到一个平面上（秩 = 2），那么一个方向将完全消失（零度 = 1）。

+   在 $\mathbb{R}⁴$ 中，如果一个矩阵的秩为 2，那么它的零空间维度为 2，意味着一半的输入方向消失。

该定理保证了“存活”和“消失”方向的几何总是保持一致的总和。

#### 应用

1.  解方程组 $Ax = b$:

    +   秩决定了解的一致性和结构。

    +   零度说明了解中存在多少自由参数。

1.  数据压缩：秩识别独立特征；零度显示冗余。

1.  计算机图形学：秩-零度解释了 3D 坐标如何折叠成 2D 图像：深度维度丢失了一个。

1.  机器学习：秩表示数据集中包含的真实信息量；零度表示增加新信息的自由度。

#### 为什么它很重要

1.  秩-零度定理将秩和零性的抽象概念结合成一个单一、优雅的公式。

1.  它确保了维度的守恒：没有信息神奇地出现或消失。

1.  它在理解系统解、子空间维度和线性变换结构方面至关重要。

1.  它为代数、拓扑和微分方程中的更深入结果奠定了基础。

#### 试试看

1.  验证秩-零度

    $$ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}. $$

1.  对于一个秩为 3 的 $4 \times 5$ 矩阵，它的零度是多少？

1.  在 $\mathbb{R}³$ 中，假设一个矩阵将整个空间映射到一条直线上。它的秩和零度是多少？

1.  挑战：严格证明行空间和零空间是正交补，并利用这一点再次推导秩-零度。

秩-零度定理是线性代数中的平衡法则：每个输入维度都被考虑在内，要么作为存活方向（秩），要么作为消失方向（零度）。

### 38. 基于基的坐标

一旦选择了一个向量空间的基，该空间中的每个向量都可以唯一地用基来描述。这些描述被称为坐标。坐标将抽象向量转换为具体的数字列表，使计算成为可能。改变基会改变坐标，但底层的向量保持不变。

#### 核心思想

给定一个向量空间 $V$ 和一个基 $B = \{v_1, v_2, \dots, v_n\}$，每个向量 $x \in V$ 可以唯一地表示为：

$$ x = a_1 v_1 + a_2 v_2 + \dots + a_n v_n. $$

系数 $(a_1, a_2, \dots, a_n)$ 是 $x$ 相对于基 $B$ 的坐标。

这种表示是唯一的，因为基向量是独立的。

#### $\mathbb{R}²$ 中的例子

1.  标准基：$B = \{(1,0), (0,1)\}$。

    +   向量 $x = (3,5)$。

    +   相对于 $B$ 的坐标：$(3,5)$。

1.  非标准基：$B = \{(1,1), (1,-1)\}$。

    +   将 $x = (3,5)$ 写成 $a(1,1) + b(1,-1)$。

    +   解：

        $$ a+b = 3, \quad a-b = 5. $$

        加法：$2a = 8 \implies a = 4$。减法：$2b = -2 \implies b = -1$。

    +   相对于这个基的坐标：$(4, -1)$。

同一个向量根据所选的基看起来不同。

#### $\mathbb{R}³$ 中的例子

令 $B = \{(1,0,0), (1,1,0), (1,1,1)\}$。找出向量 $x = (2,3,4)$ 的坐标。

解 $a(1,0,0) + b(1,1,0) + c(1,1,1) = (2,3,4)$。这给出系统：

$$ a+b+c = 2, \quad b+c = 3, \quad c = 4. $$

从 $c=4$，我们得到 $b+c=3 \implies b=-1$。然后 $a+b+c=2 \implies a-1+4=2 \implies a=-1$。坐标：$(-1, -1, 4)$。

#### 矩阵表示法

如果 $B = \{v_1, \dots, v_n\}$，形成基矩阵

$$ P = [v_1 \ v_2 \ \dots \ v_n]. $$

然后，对于向量 $x$，其坐标向量 $[x]_B$ 满足

$$ P [x]_B = x. $$

因此，

$$ [x]_B = P^{-1}x. $$

这表明坐标变换仅仅是矩阵乘法。

#### 改变坐标

假设一个向量相对于基 $B$ 的坐标为 $[x]_B$。如果我们切换到另一个基 $C$，我们使用基变换矩阵来转换坐标：

$$ [x]_C = (P_C^{-1} P_B) [x]_B. $$

这个过程在计算机图形学、机器人和数据变换中是基本的。

#### 几何意义

+   基定义了一个坐标系：空间中的轴。

+   坐标是相对于这些轴的向量的“地址”。

+   改变基就像旋转或拉伸网格：地址改变了，但点没有改变。

#### 为什么这很重要

1.  坐标使抽象向量可计算。

1.  它们使我们能够以数值形式表示函数、多项式和几何对象。

1.  改变基可以简化问题——例如，对角化使得矩阵易于分析。

1.  它们将抽象（空间、基）与具体（数字、矩阵）联系起来。

#### 试试你自己

1.  以基 $\{(1,1),(1,-1)\}$ 相对于 $x=(4,2)$ 表示。

1.  找出向量 $x=(2,1,3)$ 相对于基 $\{(1,0,1),(0,1,1),(1,1,0)\}$ 的坐标。

1.  如果基 $B$ 是标准基且基 $C=\{(1,1),(1,-1)\}$，计算从 $B$ 到 $C$ 的基变换矩阵。

1.  挑战：证明如果 $P$ 是可逆的，其列向量构成一个基，并解释为什么这保证了坐标的唯一性。

相对于基的坐标是几何与代数之间的桥梁：它们将抽象空间转化为数值系统，其中计算、推理和变换变得系统化和精确。

### 39. 基变换矩阵

每个向量空间都允许选择多个基，每个基都提供了描述相同向量的不同方式。从一个基移动到另一个基的过程称为基变换。为了系统地执行这种变换，我们使用基变换矩阵。这个矩阵充当坐标系统之间的翻译者：它将一个基相对于向量的坐标转换为另一个基的坐标。

#### 为什么变换基？

1.  计算的简单性：某些问题在特定的基下更容易解决。例如，对矩阵进行对角化可以更容易地将其提升到幂。

1.  几何学：不同的基可以表示旋转或缩放的坐标系。

1.  应用：在物理学、计算机图形学、机器人和数据科学中，变换基相当于切换视角或参考系。

#### 基本设置

设 $V$ 是一个具有两个基的向量空间：

+   $B = \{b_1, b_2, \dots, b_n\}$

+   $C = \{c_1, c_2, \dots, c_n\}$

假设一个向量 $x \in V$ 相对于 $B$ 的坐标是 $[x]_B$，相对于 $C$ 的坐标是 $[x]_C$。

我们想要一个矩阵 $P_{B \to C}$ 使得：

$$ [x]_C = P_{B \to C} [x]_B. $$

这个矩阵 $P_{B \to C}$ 是从 $B$ 到 $C$ 的基变换矩阵。

#### 构造基变换矩阵

1.  用基 $C$ 表示基 $B$ 中的每个向量。

1.  将这些坐标向量作为矩阵的列。

1.  结果矩阵将坐标从 $B$ 转换为 $C$。

以矩阵形式：

$$ P_{B \to C} = \big[ [b_1]_C \ [b_2]_C \ \dots \ [b_n]_C \big]. $$

#### $\mathbb{R}²$ 中的例子

让

+   $B = \{(1,0), (0,1)\}$（标准基）。

+   $C = \{(1,1), (1,-1)\}$.

要构建 $P_{B \to C}$：

+   用 $C$ 来表示 $B$ 中的每个向量。

解：

$$ (1,0) = a(1,1) + b(1,-1). $$

这给出系统：

$$ a+b=1, \quad a-b=0. $$

解答：$a=\tfrac{1}{2}, b=\tfrac{1}{2}$。所以 $(1,0) = \tfrac{1}{2}(1,1) + \tfrac{1}{2}(1,-1)$。

接下来：

$$ (0,1) = a(1,1) + b(1,-1). $$

系统：

$$ a+b=0, \quad a-b=1. $$

解答：$a=\tfrac{1}{2}, b=-\tfrac{1}{2}$。

因此：

$$ P_{B \to C} = \begin{bmatrix} \tfrac{1}{2} & \tfrac{1}{2} \\ \tfrac{1}{2} & -\tfrac{1}{2} \end{bmatrix}. $$

因此，对于任何向量 $x$，

$$ [x]_C = P_{B \to C}[x]_B. $$

#### 逆基变换

如果 $P_{B \to C}$ 是从 $B$ 到 $C$ 的基变换矩阵，那么它的逆是相反方向的基变换矩阵：

$$ P_{C \to B} = (P_{B \to C})^{-1}. $$

这是有意义的：在语言之间来回翻译应该会取消自身。

#### 一般公式与基矩阵

让

$$ P_B = [b_1 \ b_2 \ \dots \ b_n], \quad P_C = [c_1 \ c_2 \ \dots \ c_n], $$

列为标准坐标中写的基向量的矩阵。

然后从 $B$ 到 $C$ 的基变换矩阵是：

$$ P_{B \to C} = P_C^{-1} P_B. $$

这个公式非常有用，因为它将问题简化为矩阵乘法。

#### 几何解释

+   变换基就像旋转或拉伸坐标系的网格线。

+   向量本身（空间中的点）不会移动。改变的是其在新网格中的描述。

+   换基矩阵是这些描述之间的翻译工具。

#### 应用

1.  对角化：将矩阵表示为其特征向量的基中的形式，使其对角化，简化分析。

1.  计算机图形学：改变摄像机视角需要换基矩阵。

1.  机器人技术：坐标变换连接机器人臂、关节和工作空间框架。

1.  数据科学：PCA 找到一个新的基（主成分），其中数据更容易分析。

#### 为什么它很重要

1.  提供了一种通用的方法在基之间转换坐标。

1.  使抽象变换具体化并可计算。

1.  是对角化、Jordan 形和谱定理的基础。

1.  将代数操作与几何和现实世界的参考框架联系起来。

#### 尝试自己操作

1.  计算从标准基到 $\{(2,1),(1,1)\}$ 在 $\mathbb{R}²$ 中的换基矩阵。

1.  找到从基 $\{(1,0,0),(0,1,0),(0,0,1)\}$ 到 $\{(1,1,0),(0,1,1),(1,0,1)\}$ 在 $\mathbb{R}³$ 中的换基矩阵。

1.  证明应用 $P_{B \to C}$ 然后应用 $P_{C \to B}$ 会返回原始坐标。

1.  挑战：从坐标的定义推导出公式 $P_{B \to C} = P_C^{-1} P_B$。

换基矩阵为我们提供了切换视角的精确机制。它们确保尽管基发生变化，向量保持不变，计算保持一致。

### 40. 仿射子空间

到目前为止，向量空间和子空间总是通过原点。但在许多实际情况下，我们处理的是这些空间的平移版本：不通过原点的平面，偏离零向量的直线，或线性方程的解集（常数不为零）。这些结构被称为仿射子空间。它们通过允许“远离原点的平移”扩展了子空间的概念。

#### 定义

向量空间 $V$ 的仿射子空间是形如的集合

$$ x_0 + W = \{x_0 + w : w \in W\}, $$

其中：

+   $x_0 \in V$ 是一个固定向量（“基点”或“锚点”），

+   $W \subseteq V$ 是一个线性子空间。

因此，仿射子空间简单地是一个通过向量的平移的子空间。

#### $\mathbb{R}²$ 中的例子

1.  通过原点的直线：$\text{span}\{(1,2)\}$。这是一个子空间。

1.  不通过原点的直线：$(3,1) + \text{span}\{(1,2)\}$。这是一个仿射子空间。

1.  整个平面：$\mathbb{R}²$，它既是子空间也是仿射子空间。

#### $\mathbb{R}³$ 中的例子

1.  通过原点的平面：$\text{span}\{(1,0,0),(0,1,0)\}$。

1.  不通过原点的平面：$(2,3,4) + \text{span}\{(1,0,0),(0,1,0)\}$。

1.  平行于 z 轴但通过 $(1,1,5)$ 的直线：$(1,1,5) + \text{span}\{(0,0,1)\}$。

#### 与线性系统的关系

次线性子空间自然地作为线性方程的解集出现。

1.  同质系统：$Ax = 0$。

    +   解集是一个子空间（零空间）。

1.  非齐次系统：$Ax = b$ 且 $b \neq 0$。

    +   解集是仿射的。

    +   如果 $x_p$ 是一个特解，那么通解是：

        $$ x = x_p + N(A), $$

        其中 $N(A)$ 是零空间。

因此，解方程的几何学自然地引导到仿射子空间。

#### 仿射维度

仿射子空间的维度定义为它的方向子空间 $W$ 的维度。

+   一个点：维度为 0 的仿射子空间。

+   一条线：维度 1。

+   一个平面：维度 2。

+   更高阶的类比在 $\mathbb{R}^n$ 中继续。

#### 子空间与仿射子空间之间的区别

+   子空间总是包含原点。

+   仿射子空间可能通过也可能不通过原点。

+   每个子空间都是一个仿射子空间（基点 $x_0 = 0$）。

#### 几何直觉

将仿射子空间想象为“平坦的薄片”漂浮在空间中：

+   通过原点的线是中心绑着的绳子。

+   与它平行但偏移的线是同一根绳子移到一边。

+   仿射子空间保持形状和方向，但不保持位置。

#### 应用

1.  线性方程：通解是仿射子空间。

1.  优化：线性规划中的可行区域是仿射子空间（与不等式相交）。

1.  计算机图形学：仿射变换将仿射子空间映射到仿射子空间，保持直线性和平行性。

1.  机器学习：仿射决策边界（如超平面）将数据分为类别。

#### 为什么它很重要

1.  仿射子空间推广了子空间，使线性代数更加灵活。

1.  它们允许我们描述不包含原点的解集。

1.  它们为仿射几何、计算机图形学和优化提供了几何基础。

1.  它们作为从纯线性代数到应用建模的桥梁。

#### 试试看

1.  证明以下方程组的解集

    $$ x+y+z=1 $$

    是 $\mathbb{R}³$ 的一个仿射子空间。确定其维度。

1.  找到以下方程的通解

    $$ x+2y=3 $$

    并将其描述为仿射子空间。

1.  证明两个仿射子空间的交集要么是空的，要么是另一个仿射子空间。

1.  挑战：证明每个仿射子空间可以唯一地写成 $x_0 + W$ 的形式，其中 $W$ 是一个子空间。

仿射子空间是大多数现实世界线性问题的自然环境：它们结合了子空间的严格结构和平移的自由度，捕捉方向和位置。

#### 结尾

```py
Each basis a song,
dimension counts melodies,
the space breathes its form.
```

## 第五章\. 线性变换与结构

#### 开篇

```py
Maps preserve the line,
reflections ripple outward,
motion kept in frame.
```

### 41\. 线性变换

线性变换是线性代数的核心。它是连接两个向量空间并尊重其线性结构（加法和数乘）的规则：线性变换让我们研究向量如何移动、拉伸、旋转、投影或反射。它们赋予了线性代数动态的力量，是抽象理论与具体应用之间的桥梁。

#### 定义

如果函数 $T: V \to W$ 在向量空间之间，对于所有 $u, v \in V$ 和标量 $a, b \in \mathbb{R}$（或另一个域），

$$ T(au + bv) = aT(u) + bT(v). $$

这个单一条件编码了两个规则：

1.  可加性：$T(u+v) = T(u) + T(v)$。

1.  齐次性：$T(av) = aT(v)$。

如果两者都满足，则该变换是线性的。

#### 线性变换的例子

1.  缩放：$\mathbb{R}$ 中的 $T(x) = 3x$。每个数都拉伸了三倍。

1.  平面中的旋转：$T(x,y) = (x\cos\theta - y\sin\theta, \, x\sin\theta + y\cos\theta)$。

1.  投影：将 $(x,y,z)$ 投影到 $xy$ 平面：$T(x,y,z) = (x,y,0)$。

1.  微分：在多项式空间中，$T(p(x)) = p'(x)$。

1.  积分：在连续函数上，$T(f)(x) = \int_0^x f(t) \, dt$。

所有这些都是线性的，因为它们保留了加法和缩放。

#### 非示例

+   $T(x) = x²$ 不是线性的，因为 $(x+y)² \neq x² + y²$。

+   $T(x,y) = (x+1, y)$ 不是线性的，因为它失败了齐次性：缩放不保留“+1”。

非线性规则破坏了向量空间的结构。

#### 矩阵表示

从 $\mathbb{R}^n$ 到 $\mathbb{R}^m$ 的每个线性变换都可以用一个矩阵表示。

如果 $T: \mathbb{R}^n \to \mathbb{R}^m$，那么存在一个 $m \times n$ 的矩阵 $A$，使得：

$$ T(x) = Ax. $$

$A$ 的列仅仅是 $T(e_1), T(e_2), \dots, T(e_n)$，其中 $e_i$ 是标准基向量。

示例：设 $T(x,y) = (2x+y, x-y)$。

+   $T(e_1) = T(1,0) = (2,1)$。

+   $T(e_2) = T(0,1) = (1,-1)$。所以

$$ A = \begin{bmatrix} 2 & 1 \\ 1 & -1 \end{bmatrix}. $$

然后 $T(x,y) = A \begin{bmatrix} x \\ y \end{bmatrix}$。

#### 线性变换的性质

1.  零向量的像始终为零：$T(0) = 0$。

1.  通过原点的线的像是又一条线（或塌缩为一个点）。

1.  线性变换的合成是线性的。

1.  每个线性变换都保留了子空间的结构。

#### 核和像（预览）

对于 $T: V \to W$：

+   核（或零空间）是所有映射到零的向量：$\ker T = \{v \in V : T(v) = 0\}$。

+   像素（或范围）是所有可以实现的输出：$\text{im}(T) = \{T(v) : v \in V\}$。秩-零度定理适用于此处：

$$ \dim(\ker T) + \dim(\text{im}(T)) = \dim(V). $$

#### 几何解释

线性变换重塑空间：

+   缩放在单一方向上均匀拉伸空间。

+   旋转在保持长度的同时旋转空间。

+   投影将空间映射到低维。

+   反射将空间沿一条线或一个平面翻转。

关键特征：直线保持直线，原点保持固定。

#### 应用

1.  计算机图形学：缩放、旋转、将 3D 对象投影到 2D 屏幕上。

1.  机器人学：关节坐标和工作空间位置之间的变换。

1.  数据科学：线性映射表示降维和特征提取。

1.  微分方程：解通常涉及作用于函数空间的线性算子。

1.  机器学习：神经网络中的权重矩阵是线性变换的堆叠，其中穿插着非线性。

#### 为什么这很重要

1.  线性变换将矩阵推广到任何向量空间。

1.  它们统一了几何、代数和应用，在单一概念下。

1.  它们为研究特征值、特征向量和分解提供了自然框架。

1.  它们模拟了无数现实世界的过程：物理的、计算的、抽象的。

#### 试试你自己

1.  证明 $T(x,y,z) = (x+2y, z, x-y+z)$ 是线性的。

1.  找出将 $\mathbb{R}²$ 中的向量关于直线 $y=x$ 反射的变换的矩阵表示。

1.  解释为什么 $T(x,y) = (x²,y)$ 不是线性的。

1.  挑战：对于多项式度 ≤ 3 的微分算子 $D: P_3 \to P_2$，找到其在定义域的基 $\{1,x,x²,x³\}$ 和值域的基 $\{1,x,x²\}$ 下的矩阵。

线性变换是线性代数的语言。它们捕捉了任何空间中对称性、运动和结构的本质，使它们在理论和实践中都不可或缺。

### 42\. 线性映射的矩阵表示

每个线性变换都可以具体地表示为一个矩阵。这是数学中最有力的桥梁之一：它将抽象的函数规则转换为可以计算、操作和可视化的数字数组。

#### 从抽象规则到具体数字

假设 $T: V \to W$ 是两个有限维向量空间之间的线性变换。为了将 $T$ 表示为矩阵，我们首先选择基：

+   对于定义域 $V$，$B = \{v_1, v_2, \dots, v_n\}$。

+   对于值域 $W$，$C = \{w_1, w_2, \dots, w_m\}$。

对于每个基向量 $v_j$，计算 $T(v_j)$。每个像 $T(v_j)$ 是 $W$ 中的一个向量，因此它可以写成基 $C$ 的组合：

$$ T(v_j) = a_{1j}w_1 + a_{2j}w_2 + \dots + a_{mj}w_m. $$

系数 $(a_{1j}, a_{2j}, \dots, a_{mj})$ 成为表示 $T$ 的矩阵的第 $j$ 列。

因此，$T$ 相对于基 $B$ 和 $C$ 的矩阵是

$$ [T]_{B \to C} = \begin{bmatrix} a_{11} & a_{12} & \dots & a_{1n} \\ a_{21} & a_{22} & \dots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \dots & a_{mn} \end{bmatrix}. $$

这保证了对于任何在 $B$ 的坐标下的向量 $x$，

$$ [T(x)]_C = [T]_{B \to C}[x]_B. $$

#### 标准基情况

当 $B$ 和 $C$ 都是标准基时，过程简化为：

+   取 $T(e_1), T(e_2), \dots, T(e_n)$。

+   将它们作为矩阵的列放置。

那个矩阵直接表示 $T$。

示例：设 $T(x,y) = (2x+y, x-y)$。

+   $T(e_1) = (2,1)$。

+   $T(e_2) = (1,-1)$。

因此，标准矩阵是

$$ A = \begin{bmatrix} 2 & 1 \\ 1 & -1 \end{bmatrix}. $$

对于任何向量 $\begin{bmatrix} x \\ y \end{bmatrix}$，

$$ T(x,y) = A \begin{bmatrix} x \\ y \end{bmatrix}. $$

#### 多种视角

+   列作为图像：每一列显示一个基向量去向的位置。

+   行视图：每一行编码了如何计算输出坐标的一种方法。

+   运算符视角：矩阵像一个机器：输入向量 → 乘法 → 输出向量。

#### 几何洞察

矩阵重塑空间。在 $\mathbb{R}²$ 中：

+   第一列显示了 x 轴的去向。

+   第二列显示了 y 轴的去向。整个网格由这两个像确定。

在 $\mathbb{R}³$ 中，三个列向量是单位坐标方向的像，定义了整个空间如何扭曲、旋转或压缩。

#### 应用

1.  计算机图形学：旋转、缩放和投影由小矩阵表示。

1.  机器人技术：关节和工作空间之间的坐标变换依赖于变换矩阵。

1.  数据科学：PCA 等线性映射通过将数据投影到低维度的矩阵来实现。

1.  物理：旋转、加速和应力张量等线性算子是矩阵表示。

#### 为什么它很重要

1.  矩阵是计算工具：我们可以对它们进行加法、乘法和求逆。

1.  他们让我们可以使用高斯消元法、LU/QR/SVD 等算法来研究变换。

1.  他们将抽象的向量空间理论与实际操作中的数值计算联系起来。

1.  它们通过检查列和行就能立即揭示变换的结构。

#### 试试看

1.  找出变换 $T(x,y,z) = (x+2y, y+z, x+z)$ 在标准基下的矩阵。

1.  计算矩阵 $T: \mathbb{R}² \to \mathbb{R}²$，其中 $T(x,y) = (x-y, x+y)$。

1.  使用 $\mathbb{R}²$ 的基 $B=\{(1,1), (1,-1)\}$，找到 $T(x,y) = (2x, y)$ 相对于 $B$ 的矩阵。

1.  挑战：证明矩阵乘法对应于变换的复合，即 $[S \circ T] = [S][T]$。

矩阵表示是线性变换的实际形式，将优雅的定义转化为我们可以计算、可视化和在科学和工程中应用的东西。

### 43. 核与像

每个线性变换都隐藏着两个基本结构：向量的集合，这些向量会塌缩为零，以及所有可能的输出集合。这些被称为核和像。它们是线性映射的 DNA，揭示了其内部结构、优势和局限性。

#### 核

线性变换 $T: V \to W$ 的核（或零空间）定义为：

$$ \ker(T) = \{ v \in V : T(v) = 0 \}. $$

+   它是变换将所有向量发送到零向量的集合。

+   它衡量在变换下“丢失”了多少信息。

+   核始终是定义域 $V$ 的子空间。

例子：

1.  对于 $T: \mathbb{R}² \to \mathbb{R}²$，$T(x,y) = (x,0)$。

    +   核：所有形式为 $(0,y)$ 的向量。这是 y 轴。

1.  对于 $T: \mathbb{R}³ \to \mathbb{R}²$，$T(x,y,z) = (x,y)$。

    +   核：所有形式为 $(0,0,z)$ 的向量。这是 z 轴。

核告诉我们哪些方向在 $T$ 下消失。

#### 像素

线性变换的像（或值域）定义为：

$$ \text{im}(T) = \{ T(v) : v \in V \}. $$

+   这是通过应用 $T$ 可以实际达到的所有向量的集合。

+   它描述了变换的“输出空间”。

+   像总是目标空间 $W$ 的子空间。

例子：

1.  对于 $T(x,y) = (x,0)$：

    +   像：所有形式为 $(a,0)$ 的向量。这是 x 轴。

1.  对于 $T(x,y,z) = (x+y, y+z)$：

    +   像：整个 $\mathbb{R}²$。任何向量 $(u,v)$ 都可以通过解 $(x,y,z)$ 的方程来实现。

#### 核和像一起

这两个子空间反映了 $T$ 的两个方面：

+   核衡量了维度的坍缩。

+   像衡量了保留和传递的方向。

一个中心结果是秩-零度定理：

$$ \dim(\ker T) + \dim(\text{im }T) = \dim(V). $$

+   $\dim(\ker T)$ 是零度。

+   $\dim(\text{im }T)$ 是秩。

这个定理保证了完美的平衡：域分解为丢失的方向（核）和活跃的方向（像）。

#### 矩阵视图

对于矩阵 $A$，线性映射是 $T(x) = Ax$。

+   核是 $Ax = 0$ 的解集。

+   像是 $A$ 的列空间。

例子：

$$ A = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 1 \end{bmatrix}. $$

+   像：列的生成空间

$$ \text{im}(A) = \text{span}\{ (1,0), (2,1), (3,1) \}. $$

+   核：求解

$$ \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}. $$

这导致了像 $x=-y-2z$ 这样的解。因此，核是一维的，像二维的，域（三维）分解为 $1+2=3$。

#### 几何直觉

+   核是无形方向的集合，就像在投影中消失的阴影。

+   像是可以出现的所有阴影的集合。

+   它们共同描述了投影、展平、拉伸或坍缩。

例子：将 $\mathbb{R}³$ 投影到 xy 平面：

+   核：z 轴（所有点坍缩到零高度）。

+   像：整个 xy 平面（所有可能的阴影）。

#### 应用

1.  解方程：核描述了 $Ax=0$ 的所有解。像描述了哪些右端项 $b$ 使得 $Ax=b$ 可解。

1.  数据科学：零度对应于冗余特征；秩对应于有用的独立特征。

1.  物理：在力学中，对称性通常形成变换的核，而可观测量形成像。

1.  控制理论：核和像决定了系统的可控性和可观测性。

#### 为什么它很重要

1.  核和像将变换分类为可逆或不可逆。

1.  它们为描述维度变化提供了精确的语言。

1.  它们是秩、零度和可逆性的基础。

1.  它们将矩阵推广得远远超出了：到多项式、函数、算子和微分方程。

#### 尝试自己来做

1.  计算 $T(x,y,z) = (x+y, y+z)$ 的核和像。

1.  对于投影 $T(x,y,z) = (x,y,0)$，识别核和像。

1.  证明如果核是平凡的 ($\{0\}$)，那么变换是单射的。

1.  挑战：通过举例证明 $3\times 3$ 矩阵的秩-零度定理。

核和像是通过线性变换被理解的双焦点。一个告诉我们什么消失了，另一个告诉我们什么留下了。共同地，它们给出了变换本质的最清晰图景。

### 44. 可逆性和同构

线性变换有多种形式：一些将空间折叠到较低维度，一些将其拉伸，而一个特殊的群可以完美地保留所有信息。这些特殊变换是可逆的，意味着它们可以被精确地反转。当两个向量空间通过这种变换相关联时，我们说它们是同构的——结构上相同，即使表面上看起来不同。

#### 线性变换的可逆性

一个线性变换 $T: V \to W$ 是可逆的，如果存在另一个线性变换 $S: W \to V$，使得：

$$ S \circ T = I_V \quad \text{and} \quad T \circ S = I_W, $$

其中 $I_V$ 和 $I_W$ 是 $V$ 和 $W$ 上的恒等映射。

+   $S$ 被称为 $T$ 的逆。

+   如果这样的逆存在，$T$ 是一个双射：既是单射（注入的）也是满射（溢出的）。

+   在有限维空间中，这相当于说 $T$ 被一个可逆矩阵表示。

#### 可逆矩阵

一个 $n \times n$ 的矩阵 $A$ 是可逆的，如果存在另一个 $n \times n$ 的矩阵 $A^{-1}$，使得：

$$ AA^{-1} = A^{-1}A = I. $$

可逆性的特征：

1.  $A$ 是可逆的 ⇔ $\det(A) \neq 0$。

1.  ⇔ $A$ 的列向量线性无关。

1.  ⇔ $A$ 的列向量张成 $\mathbb{R}^n$。

1.  ⇔ $A$ 的秩是 $n$。

1.  ⇔ 系统 $Ax=b$ 对于每个 $b$ 都有唯一解。

所有这些属性都是相互关联的：可逆性意味着在变换向量时不会丢失信息。

#### 向量空间同构

两个向量空间 $V$ 和 $W$ 是同构的，如果存在一个双射线性变换 $T: V \to W$。

+   这意味着 $V$ 和 $W$ 在结构上是“相同”的，尽管它们可能看起来不同。

+   对于有限维空间：

    $$ V \cong W \quad \text{if and only if} \quad \dim(V) = \dim(W). $$

+   例子：$\mathbb{R}²$ 和所有度数 ≤ 1 的多项式的集合是同构的，因为它们都有维度 2。

#### 可逆性的例子

1.  平面中的旋转：每个旋转矩阵都有一个逆（旋转相反的角度）。

    $$ R(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}, \quad R(\theta)^{-1} = R(-\theta). $$

1.  通过非零因子缩放：$T(x) = ax$，其中 $a \neq 0$。逆是 $T^{-1}(x) = \tfrac{1}{a}x$。

1.  投影到一条直线上：不可逆的，因为深度丢失。核是非平凡的。

1.  多项式（度数 ≤ n）上的微分：不可逆的，因为常数项在核中消失。

1.  指数函数上的微分：可逆的：其逆是积分（加上常数）。

#### 几何解释

+   可逆变换保持维度：不会发生展平或折叠。

+   它们可以旋转、剪切、拉伸或反射，但每个输入向量都可以唯一恢复。

+   行列式告诉变换的“体积缩放”：可逆性要求这个体积不会缩放到零。

#### 应用

1.  计算机图形学：可逆矩阵允许平滑变换，不会丢失任何信息。不可逆映射（如投影）从 3D 世界创建 2D 渲染。

1.  密码学：加密系统依赖于可逆线性映射进行编码/解码。

1.  机器人技术：关节和工作空间坐标之间的转换通常必须是可逆的，以便进行精确控制。

1.  数据科学：PCA 通常减少维度（不可逆），但白化变换在所选子空间内是可逆的。

1.  物理：坐标变化（例如，伽利略或洛伦兹变换）是可逆的，确保物理定律保持一致。

#### 为什么这很重要

1.  可逆映射保留向量空间的整个结构。

1.  它们对向量空间进行分类：如果两个具有相同的维度，它们通过同构在本质上相同。

1.  它们允许可逆建模，这在物理学、密码学和计算中是必不可少的。

1.  它们突出了无损变换（可逆）和有损变换（不可逆）之间微妙的平衡。

#### 试试你自己

1.  通过计算其行列式和逆来证明矩阵 $\begin{bmatrix} 2 & 1 \\ 3 & 2 \end{bmatrix}$ 是可逆的。

1.  证明在 $\mathbb{R}²$ 中投影到 x 轴是不可逆的。识别其核。

1.  构造 $\mathbb{R}³$ 和度 ≤ 2 的多项式空间之间的显式同构。

1.  挑战：证明如果 $T$ 是一个同构，那么它将基映射到基。

可逆性和同构是“线性规则”通往等价伟大思想的门户。它们允许我们用数学的精确性来说明，当两个空间在结构上真正相同——不同的衣服，相同的骨架时。

### 45. 合成、幂和迭代

线性变换不是孤立的操作——它们可以组合、重复和分层，以构建更复杂的效果。这使我们想到了合成、变换的幂和迭代的概念。这些概念构成了线性动力学、算法以及许多现实世界系统的骨架，在这些系统中，重复的行为累积成令人惊讶的结果。

#### 线性变换的合成

如果 $T: U \to V$ 和 $S: V \to W$ 是线性变换，那么它们的合成是另一个变换

$$ S \circ T : U \to W, \quad (S \circ T)(u) = S(T(u)). $$

+   合成是结合律的：$(R \circ S) \circ T = R \circ (S \circ T)$。

+   合成是线性的：合成两个线性映射的结果仍然是线性的。

+   从矩阵的角度来看，如果 $T(x) = Ax$ 和 $S(x) = Bx$，那么

    $$ (S \circ T)(x) = B(Ax) = (BA)x. $$

    注意顺序很重要：合成对应于矩阵乘法。

示例：

1.  $T(x,y) = (x+2y, y)$。

1.  $S(x,y) = (2x, x-y)$。那么 $(S \circ T)(x,y) = S(x+2y,y) = (2(x+2y), (x+2y)-y) = (2x+4y, x+y)$。矩阵乘法确认了相同的结果。

#### 变换的幂

如果 $T: V \to V$，我们可以反复应用它：

$$ T² = T \circ T, \quad T³ = T \circ T \circ T, \quad \dots $$

+   这些被称为 $T$ 的幂。

+   如果 $T(x) = Ax$，那么 $T^k(x) = A^k x$。

+   变换的幂捕捉重复的过程，如复利、人口增长或迭代算法。

示例：令 $T(x,y) = (2x, 3y)$。那么

$$ T^n(x,y) = (2^n x, 3^n y). $$

每次迭代都会放大不同方向上的缩放。

#### 迭代和动力系统

迭代意味着反复应用相同的变换来研究长期行为：

$$ x_{k+1} = T(x_k), \quad x_0 \text{ given}. $$

+   这创建了一个离散动力系统。

+   根据 $T$，向量可能增长、缩小、振荡或稳定。

示例 1（马尔可夫链）：如果 $T$ 是一个随机矩阵，迭代描述了随时间变化的概率演变。最终，系统可能收敛到一个稳态分布。

示例 2（人口模型）：如果 $T$ 描述了亚种群之间的相互作用，迭代模拟了世代更替。特征值决定了种群是爆炸、稳定还是消失。

示例 3（计算机图形学）：重复的仿射变换可以创建类似 Sierpinski 三角形的分形。

#### 稳定性和特征值

$T^n(x)$ 的行为在很大程度上取决于变换的特征值。

+   如果 $|\lambda| < 1$，重复应用会缩小该方向上的向量到零。

+   如果 $|\lambda| > 1$，重复应用会导致指数增长。

+   如果 $|\lambda| = 1$，向量旋转或振荡而不改变长度。

这种幂和特征值之间的联系是数值分析和物理学中许多算法的基础。

#### 几何解释

+   组合 = 几何动作的链式（先旋转再反射，先缩放再剪切）。

+   幂 = 重复应用相同的动作（旋转 90° 四次 = 单位矩阵）。

+   迭代 = 探索在重复变换下向量的“轨道”。

#### 应用

1.  搜索引擎：PageRank 通过迭代线性变换直到稳定来计算。

1.  经济学：投入产出模型迭代以预测行业的长期均衡。

1.  物理学：量子态的时间演化是通过重复应用单位算子来建模的。

1.  数值方法：迭代求解器（如幂迭代）近似特征向量。

1.  计算机图形学：迭代函数系统生成自相似的分形。

#### 为什么这很重要

1.  组合统一了矩阵乘法和变换链。

1.  幂揭示了指数增长、衰减和振荡。

1.  迭代是数学、科学和工程中建模动态过程的核心。

1.  特征值的联系使这些想法成为稳定性分析的基础。

#### 尝试自己动手做

1.  令 $T(x,y) = (x+y, y)$。计算 $T²(x,y)$ 和 $T³(x,y)$。当 $n \to \infty$ 时会发生什么？

1.  考虑在 $\mathbb{R}²$ 中旋转 90°。证明 $T⁴ = I$。

1.  对于矩阵 $A = \begin{bmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{bmatrix}$，迭代 $A^n$。任意向量会发生什么？

1.  挑战：证明如果 $A$ 可以对角化为 $A = PDP^{-1}$，那么 $A^n = PD^nP^{-1}$。利用这一点来分析长期行为。

组合、幂和迭代将线性代数从静态方程带入到随时间变化的过程世界。它们解释了小而重复的步骤如何塑造长期结果——无论是稳定系统、放大信号还是创造无限复杂性。

### 46. 相似性与共轭

在线性代数中，不同的矩阵可以表示相同的底层变换，当它们在不同的坐标系中书写时。这种关系由相似性的概念所捕捉。如果通过一个可逆的基变换矩阵的共轭得到另一个矩阵，则两个矩阵是相似的。这一概念是理解规范形式、特征值分解和线性算子深层结构的关键。

#### 相似性的定义

如果存在一个可逆矩阵 $P$ 使得 $A$ 和 $B$ 是 $n \times n$ 的矩阵，则称它们是相似的：

$$ B = P^{-1}AP. $$

+   在这里，$P$ 代表基变换。

+   $A$ 和 $B$ 描述了相同的线性变换，但相对于不同的基来表达。

#### 共轭作为基变换

假设 $T: V \to V$ 是一个线性变换，$A$ 是其在基 $B$ 中的矩阵。如果我们切换到新的基 $C$，矩阵变为 $B$。转换是：

$$ B = P^{-1}AP, $$

其中 $P$ 是从基 $B$ 到基 $C$ 的基变换矩阵。

这表明相似性不仅仅是代数巧合——它是几何的：算子是相同的，但我们的视角（基）已经改变。

#### 相似性下保持的性质

如果 $A$ 和 $B$ 是相似的，它们共享许多关键属性：

1.  行列式：$\det(A) = \det(B)$。

1.  轨迹：$\text{tr}(A) = \text{tr}(B)$。

1.  行列式：$\text{rank}(A) = \text{rank}(B)$。

1.  特征值：相同的特征值集合（具有重数）。

1.  特征多项式：相同。

1.  最小多项式：相同。

这些不变量定义了线性算子的“骨架”，不受坐标变化的影响。

#### 例子

1.  平面中的旋转：旋转 90° 的矩阵是

    $$ A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}. $$

    在另一个基中，旋转可能表示为一个看起来更复杂的矩阵，但所有这样的矩阵都与 $A$ 相似。

1.  对角化：如果矩阵 $A$ 与对角矩阵 $D$ 相似，则矩阵 $A$ 是可对角化的。也就是说，

    $$ A = PDP^{-1}. $$

    在这里，相似性将 $A$ 简化为其最简形式。

1.  切变变换：切变矩阵 $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$ 不可对角化，但它可能与一个约当块相似。

#### 几何解释

+   相似性表明：两个矩阵可能看起来不同，但它们是从不同坐标系看到的“相同”的变换。

+   共轭是重新标记坐标的数学行为。

+   想象改变你的摄像机角度：场景没有改变，只是视角改变了。

#### 应用

1.  对角化：将矩阵简化为对角形式（如果可能）使用相似性。这简化了幂、指数和迭代分析。

1.  约当标准形：每个方阵都与一个约当形相似，给出了完整的结构分类。

1.  量子力学：状态空间上的算子经常改变表示，但相似性保证了谱的不变性。

1.  控制理论：规范形简化了系统稳定性和可控性的分析。

1.  数值方法：特征值算法依赖于重复的相似性变换（例如，QR 算法）。

#### 为什么这很重要

1.  相似性揭示了线性算子的真实身份，与坐标无关。

1.  它允许简化：许多问题在合适的基下变得更容易。

1.  它保持不变量，为我们提供了分类和比较算子的工具。

1.  它将抽象代数与几何、物理和工程中的具体计算联系起来。

#### 试试看

1.  证明 $\begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}$ 与 $\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}$ 相似。为什么或为什么不？

1.  计算 $P^{-1}AP$，其中 $A = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}$ 和 $P = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$。解释结果。

1.  证明如果两个矩阵是相似的，它们必须具有相同的迹。

1.  挑战：证明如果 $A$ 和 $B$ 是相似的，那么对于所有整数 $k \geq 0$，$A^k$ 和 $B^k$ 也是相似的。

相似性和共轭将线性代数从单纯的计算提升到结构理解。它们告诉我们当两个看似不同的矩阵只是同一基本变换的不同“面”时。

### 47. 投影和反射

在线性代数的许多变换中，投影和反射因其几何清晰性和实际重要性而脱颖而出。这些操作以简单但强大的方式重塑向量，它们是统计学、优化、图形和物理学中算法的构建块。

#### 投影：将向量展平到子空间

投影是一种线性变换，它将向量投影到子空间上，就像投射影子一样。

形式上，如果 $W$ 是 $V$ 的一个子空间，那么向量 $v$ 在 $W$ 上的投影是唯一的向量 $w \in W$，它最接近 $v$。

在 $\mathbb{R}²$ 中：将 $(x,y)$ 投影到 x 轴上得到 $(x,0)$。

##### 正交投影公式

假设 $u$ 是一个非零向量。$v$ 在由 $u$ 张成的直线上的投影是：

$$ \text{proj}_u(v) = \frac{v \cdot u}{u \cdot u} u. $$

这个公式在任意维度上都适用。它使用点积来衡量 $v$ 在 $u$ 方向上的指向程度。

例子：将 $(2,3)$ 投影到 $u=(1,1)$：

$$ \text{proj}_u(2,3) = \frac{(2,3)\cdot(1,1)}{(1,1)\cdot(1,1)} (1,1) = \frac{5}{2}(1,1) = (2.5,2.5). $$

向量 $(2,3)$ 沿着直线分为 $(2.5,2.5)$ 和垂直于它的 $(-0.5,0.5)$。

##### 投影矩阵

对于单位向量 $u$：

$$ P = uu^T $$

是投影矩阵，其投影到 $u$ 的张成空间。

对于具有矩阵 $Q$ 中正交基列的一般子空间：

$$ P = QQ^T $$

将任何向量投影到该子空间上。

属性：

1.  $P² = P$（幂等的）。

1.  $P^T = P$（对称的，对于正交投影）。

#### 反射：跨越子空间翻转

反射将向量翻转到一条线或一个平面上。从几何上看，就像一面镜子。

通过单位向量 $u$ 张成的直线上的反射：

$$ R(v) = 2\text{proj}_u(v) - v. $$

矩阵形式：

$$ R = 2uu^T - I. $$

示例：将 $(2,3)$ 反射到直线 $y=x$ 上。给定 $u=(1/\sqrt{2},1/\sqrt{2})$：

$$ R = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}. $$

因此，反射交换坐标：$(2,3) \mapsto (3,2)$。

#### 几何洞察

+   投影通过移除与子空间正交的分量来缩短向量。

+   反射保持长度，但相对于子空间翻转方向。

+   投影关于逼近（“最近点”），反射关于对称性。

#### 应用

1.  统计学与机器学习：最小二乘回归是将数据投影到预测变量张成空间上的过程。

1.  计算机图形学：投影将 3D 场景转换为 2D 屏幕图像。反射模拟镜子和光滑表面。

1.  优化：投影通过将猜测值带回到可行区域来强制实施约束。

1.  物理：反射描述波的行为、光学和粒子相互作用。

1.  数值方法：投影算子是迭代算法（如 Krylov 子空间方法）的关键。

#### 为什么它很重要

1.  投影捕捉了逼近的本质：保留适合的部分，丢弃不适合的部分。

1.  反射体现了对称性和不变性，这是几何学和物理学的关键。

1.  两者都是线性变换，具有优雅的矩阵表示。

1.  它们容易与其他变换结合，使它们在计算中变得多才多艺。

#### 尝试自己动手

1.  找到由 $(3,4)$ 张成的线的投影矩阵。验证它是幂等的。

1.  计算向量 $(1,2)$ 在 x 轴上的反射。

1.  证明反射矩阵是正交的 ($R^T R = I$)。

1.  挑战：对于具有正交基 $Q$ 的子空间 $W$，推导反射矩阵 $R = 2QQ^T - I$。

投影和反射是线性变换体现几何思想的纯粹例子之一。一个进行逼近，另一个进行对称化——但两者都通过线性代数的视角揭示了空间的深层结构。

### 48. 旋转和剪切

线性变换可以以惊人的不同方式扭曲、旋转和变形空间。两个最基本示例是旋转——在旋转向量时保持长度和角度，以及剪切——相对于另一个部分滑动空间的一部分，扭曲形状但通常保持面积。这两个变换构成了线性代数的几何核心，并且在图形学、物理学和工程中是必不可少的。

#### 平面中的旋转

$\mathbb{R}²$ 中的旋转角度 $\theta$ 定义为：

$$ R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}. $$

对于任意向量 $(x,y)$:

$$ R_\theta \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} x\cos\theta - y\sin\theta \\ x\sin\theta + y\cos\theta \end{bmatrix}. $$

属性：

1.  保持长度：$\|R_\theta v\| = \|v\|$。

1.  保持角度：点积不变。

1.  行列式 = $+1$，因此它保持方向和面积。

1.  逆变换: $R_\theta^{-1} = R_{-\theta}$.

示例：90° 旋转：

$$ R_{90^\circ} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}, \quad (1,0) \mapsto (0,1). $$

#### 三维旋转

在 $\mathbb{R}³$ 中的旋转是围绕一个轴发生的。例如，围绕 z 轴旋转角度 $\theta$:

$$ R_z(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta & 0 \\ \sin\theta & \cos\theta & 0 \\ 0 & 0 & 1 \end{bmatrix}. $$

+   保持 z 轴固定。

+   类似于 2D 旋转旋转 xy 平面。

3D 中的通用旋转由行列式为 +1 的正交矩阵描述，形成群 $SO(3)$。

#### 剪切变换

剪切滑动一个坐标方向，同时保持另一个方向固定，扭曲形状。

在 $\mathbb{R}²$:

$$ S = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix} \quad \text{或} \quad \begin{bmatrix} 1 & 0 \\ k & 1 \end{bmatrix}. $$

+   第一种形式根据 y 坐标滑动 x 坐标。

+   第二种形式根据 x 坐标滑动 y 坐标。

示例：

$$ S = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}, \quad (x,y) \mapsto (x+y, y). $$

+   正方形变成平行四边形。

+   如果 $\det(S) = 1$，则面积保持不变。

在 $\mathbb{R}³$: 剪切扭曲体积，同时保持面的平行性。

#### 几何比较

+   旋转：精确保持大小和形状，只改变方向。圆仍然保持为圆。

+   剪切：扭曲形状但通常保持面积（在 2D）或体积（在 3D）。圆变成椭圆或斜形。

旋转和剪切一起可以生成大量不同的线性扭曲。

#### 应用

1.  计算机图形学：旋转定位对象；剪切模拟透视。

1.  工程：剪切应力变形材料；旋转模拟刚体运动。

1.  机器人技术：旋转定义手臂方向；剪切近似局部变形。

1.  物理：旋转是空间的对称性；剪切出现在流体流动和弹性中。

1.  数据科学：剪切表示保持体积但扭曲分布的变量变化。

#### 为什么它很重要

1.  旋转表示纯粹的对称——无扭曲，只是重新定位。

1.  剪切展示了几何如何被扭曲，同时保持体积或面积。

1.  两者都是构建块：$\mathbb{R}²$ 中的任何可逆矩阵都可以分解为旋转、剪切和缩放。

1.  它们连接代数和几何，为抽象矩阵赋予视觉意义。

#### 试试你自己

1.  将 $(1,0)$ 旋转 60° 并显式计算结果。

1.  将剪切 $S=\begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}$ 应用到顶点为 $(0,0),(1,0),(0,1),(1,1)$ 的正方形上。结果是什么形状？

1.  证明旋转矩阵是正交的 ($R^TR=I$)。

1.  挑战：证明任何面积保持的 $2\times2$ 矩阵（行列式为 1）都可以分解为旋转和剪切的乘积。

旋转和剪切突出了线性代数的两个互补方面：对称与扭曲。它们共同展示了变换如何在保持结构完整的同时，要么保留空间的本质，要么将其弯曲成新的形状。

### 49. 级数和算子视角

线性变换或矩阵的级数是其力量的最重要的度量之一。它捕捉了变换保留多少独立方向，它从输入到输出携带了多少信息，以及其作用在空间上的“满”程度。将级数不仅仅视为一个数字，而是视为操作符的描述，给我们一个更清晰的变换真正做什么的图景。

#### 级数的定义

对于表示线性变换 $T: V \to W$ 的矩阵 $A$：

$$ \text{rank}(A) = \dim(\text{im}(A)) = \dim(\text{im}(T)). $$

即，级数是像（或列空间）的维度。它计算线性无关列的最大数量。

#### 基本性质

1.  $\text{rank}(A) \leq \min(m,n)$ 对于一个 $m \times n$ 矩阵。

1.  $\text{rank}(A) = \text{rank}(A^T)$。

1.  级数等于行简化形式中的主元列数。

1.  级数直接与零度通过级数-零度定理相联系：

    $$ \text{rank}(A) + \text{nullity}(A) = n. $$

#### 算子视角

而不是专注于行和列，想象级数作为衡量域有多少部分忠实传递到陪域的度量。

+   如果级数等于满 ($n$)，则变换是单射的：没有内容塌缩。

+   如果级数等于陪域的维度 ($m$)，则变换是满射的：每个目标向量都可以到达。

+   如果级数较小，变换会压缩空间：域的部分“不可见”并塌缩到核中。

示例 1（投影）：从 $\mathbb{R}³$ 投影到 xy 平面具有级数 2。它消除了 z 方向但保留了两个独立方向。

示例 2（旋转）：$\mathbb{R}²$ 中的旋转具有级数 2。没有方向丢失。

示例 3（零映射）：将所有内容映射到零的变换的级数为 0。

#### 几何意义

+   级数 = 保留的独立方向的数量。

+   一阶变换将整个空间映射到一条单线上。

+   在 $\mathbb{R}³$ 中，二阶级数将空间映射到平面。

+   排序满映射将空间映射到其整个维度而不发生塌陷。

视觉上：秩描述了图像的“维度厚度”。

#### 排序和矩阵分解

排序揭示隐藏的结构：

1.  LU 分解：排序列数决定了非零主元的数量。

1.  QR 分解：秩控制正交方向的数量。

1.  SVD（奇异值分解）：非零奇异值的数量等于秩。

SVD 特别提供了一个几何算子视图：每个非零奇异值对应一个保留的维度，而零表示塌陷的方向。

#### 排序的应用

1.  数据压缩：低秩逼近减少存储（例如，使用 SVD 的图像压缩）。

1.  统计学：设计矩阵的秩决定了回归系数的可识别性。

1.  机器学习：权重矩阵的秩控制模型的表达能力。

1.  控制理论：秩条件确保系统的可控性和可观测性。

1.  网络分析：邻接矩阵或拉普拉斯矩阵的秩反映了图的连通性。

#### 排序不足

如果一个变换的秩小于满秩，它是秩不足的。这意味着：

+   一些方向丢失（核非平凡）。

+   一些输出无法到达（像小于陪域）。

+   方程 $Ax=b$ 可能是不一致的或欠确定的。

在数值线性代数中，秩不足的检测和处理至关重要，不良条件可能隐藏在几乎相关的列中。

#### 为什么这很重要

1.  排序衡量变换的真实维度效应。

1.  它区分了全强度算子和那些折叠信息算子。

1.  它将行空间、列空间、图像和核连接到一个数字下。

1.  它支撑着回归、分解和降维算法。

#### 尝试自己操作

1.  找到 $\begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \end{bmatrix}$ 的秩。为什么它小于 2？

1.  在 $\mathbb{R}³$ 中几何地描述秩为 1 的变换的像。

1.  对于一个 $5 \times 5$ 的对角矩阵，其对角元素为 $(2,0,3,0,5)$，计算秩和零度。

1.  挑战：证明对于任何矩阵 $A$，秩等于 $A$ 的非零奇异值的数量。

排序不仅告诉我们有多少独立向量在变换中幸存，而且还告诉我们算子真正保留了多少结构。它是抽象线性映射和其实际功能之间的桥梁。

### 50. 块矩阵和块映射

随着问题规模的增大，矩阵变得很大，难以逐元素管理。一种强大的策略是将矩阵组织成块子矩阵，就像马赛克中的瓷砖一样组合在一起。这允许我们将大型变换视为较小、更易于理解的变换的组合。块矩阵保留结构，简化计算，并揭示对变换在子空间上作用的深入见解。

#### 什么是块矩阵？

块矩阵将矩阵划分为矩形子矩阵。每个块本身也是一个矩阵，整个矩阵可以使用块规则进行操作。

例子：一个$4 \times 4$矩阵被分成四个$2 \times 2$块：

$$ A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}, $$

其中每个$A_{ij}$是$2 \times 2$。

我们不是以 16 个条目为思考单位，而是与 4 个块一起工作。

#### 块映射作为线性变换

假设$V = V_1 \oplus V_2$被分解成两个子空间。线性映射$T: V \to V$可以用它对每个分量的作用来描述。相对于这种分解，$T$的矩阵具有块形式：

$$ [T] = \begin{bmatrix} T_{11} & T_{12} \\ T_{21} & T_{22} \end{bmatrix}. $$

+   $T_{11}$：$V_1$如何映射到自身。

+   $T_{12}$：$V_2$对$V_1$的贡献。

+   $T_{21}$：$V_1$对$V_2$的贡献。

+   $T_{22}$：$V_2$如何映射到自身。

这种分解突出了子空间在变换下的相互作用。

#### 块矩阵运算

块矩阵遵循与普通矩阵相同的规则，但操作是按块进行的。

加法：

$$ \begin{bmatrix} A & B \\ C & D \end{bmatrix} + \begin{bmatrix} E & F \\ G & H \end{bmatrix} = \begin{bmatrix} A+E & B+F \\ C+G & D+H \end{bmatrix}. $$

乘法：

$$ \begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} E & F \\ G & H \end{bmatrix} = \begin{bmatrix} AE+BG & AF+BH \\ CE+DG & CF+DH \end{bmatrix}. $$

这些公式看起来像普通的乘法，但每个项本身是子矩阵的乘积。

#### 特殊块结构

1.  块对角矩阵：

    $$ \begin{bmatrix} A & 0 \\ 0 & D \end{bmatrix}. $$

    对子空间独立操作，没有混合。

1.  块上三角：

    $$ \begin{bmatrix} A & B \\ 0 & D \end{bmatrix}. $$

    子空间$V_1$影响$V_2$，但反之则不然。

1.  块对称：如果整体矩阵是对称的，那么某些块关系也是对称的：$A^T=A, D^T=D, B^T=C$。

这些结构在分解和迭代算法中自然出现。

#### 块矩阵逆

一些块矩阵可以使用特殊公式求逆。对于

$$ M = \begin{bmatrix} A & B \\ C & D \end{bmatrix}, $$

如果$A$是可逆的，逆可以用 Schur 补表达：

$$ M^{-1} = \begin{bmatrix} A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1} \\ -(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1} \end{bmatrix}. $$

这个公式在统计学、优化和数值分析中处于核心地位。

#### 几何解释

+   块对角矩阵像两个独立变换并排操作。

+   块三角矩阵显示了“层次”：一个子空间影响另一个，但反之则不然。

+   这种分解反映了系统如何被分解成更小的相互作用部分。

#### 应用

1.  数值线性代数：块操作优化了大型稀疏矩阵的计算。

1.  控制理论：状态空间模型自然地以块形式表达。

1.  统计学：分割协方差矩阵依赖于块求逆公式。

1.  机器学习：神经网络层变换，通常结构化为块以提高效率。

1.  并行计算：块分解将大型矩阵问题分布到处理器上。

#### 为什么这很重要

1.  块矩阵将大问题转化为可管理的较小问题。

1.  它们反映了系统自然分解为相互作用的部件。

1.  它们明确地展示了子空间交互的几何结构。

1.  它们提供了高效的算法，特别是对于大规模科学计算。

#### 尝试自己操作

1.  将两个以$2 \times 2$块矩阵形式写的$4 \times 4$矩阵相乘，并确认块乘法规则。

1.  使用块形式写出$\mathbb{R}⁴$中的二维子空间的投影矩阵。

1.  计算 Schur 补

    $$ \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}. $$

1.  挑战：证明块三角矩阵的行列式等于其对角块行列式的乘积。

块矩阵和块映射展示了如何组织复杂性。我们不再淹没在成千上万的条目中，我们看到结构、交互和层次——揭示如何从简单的线性部分构建大型系统。

#### 结束语

```py
Shadows twist and turn,
kernels hide and images flow,
form remains within.
```

## 第六章. 行列式和体积

#### 开场

```py
Areas unfold,
parallels stretch into waves,
scale whispers in signs.
```

### 51. 面积、体积和符号尺度因子

行列式通常感觉像是一个抽象公式，直到我们看到它们的几何意义：它们在二维中测量面积，在三维中测量体积，在更高维度中，测量变换形状的一般“大小”。更重要的是，行列式编码了方向是否保持或翻转，赋予它们“符号”解释。这种视角将行列式从代数上的好奇心转变为几何工具。

#### 空间的变换和缩放

考虑一个由方阵$A$表示的线性变换$T: \mathbb{R}^n \to \mathbb{R}^n$。当$A$作用于向量时，它重塑空间：它拉伸、压缩、旋转、反射或扭曲区域。

+   如果你将$A$应用于$\mathbb{R}²$中的单位正方形，图像是一个平行四边形。

+   如果你将$A$应用于$\mathbb{R}³$中的单位立方体，图像是一个平行六面体。

+   通常，$A$的行列式告诉我们形状的度量（面积、体积、超体积）是如何变化的。

#### 行列式作为符号尺度因子

+   $|\det(A)|$ = 面积（二维）、体积（三维）或 n 维内容的尺度因子。

+   如果$\det(A) = 0$，变换将空间折叠到较低维度，将所有体积压平。

+   如果$\det(A) > 0$，空间的方向被保持。

+   如果$\det(A) < 0$，方向被翻转（就像在镜子中的反射）。

因此，行列式不仅仅是数字——它们同时携带大小和符号，告诉我们关于大小和手性的信息。

#### 二维情况：平行四边形的面积

取两个列向量$u,v \in \mathbb{R}²$。将它们作为矩阵的列：

$$ A = \begin{bmatrix} u & v \end{bmatrix}. $$

行列式的绝对值给出了由 $u$ 和 $v$ 张成的平行四边形的面积：

$$ \text{面积} = |\det(A)|. $$

示例：

$$ A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}. $$

然后 $\det(A) = (2)(3) - (1)(1) = 5$。单位正方形映射到面积为 5 的平行四边形。

#### 3D 情况：平行六面体的体积

对于三个向量 $u,v,w \in \mathbb{R}³$，形成一个矩阵

$$ A = \begin{bmatrix} u & v & w \end{bmatrix}. $$

然后行列式给出了平行六面体的体积：

$$ \text{体积} = |\det(A)|. $$

从几何上看，这是标量三重积：

$$ \det(A) = u \cdot (v \times w). $$

示例：

$$ A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{bmatrix}, \quad \det(A) = 6. $$

因此，单位立方体被拉伸成一个体积为 6 的盒子。

#### 方向和有符号测度

行列式不仅测量大小，还检测方向：

+   在二维中，翻转 x 轴和 y 轴会改变行列式的符号。

+   在 3D 中，交换两个向量会改变“手性”（右手定则变为左手定则）。

这解释了为什么行列式可以是负数：它们标记了反转方向的变换。

#### 高维

在 $\mathbb{R}^n$ 中，行列式扩展了同样的想法。一个单位超立方体（边长为 1）被变换成一个 n 维平行六面体，其体积由 $|\det(A)|$ 给出。

尽管我们无法在 3D 以上进行可视化，但这个概念可以平滑地推广：行列式编码了一个 n 维对象拉伸或压缩的程度。

#### 应用

1.  几何学：直接从向量计算面积、体积和方向。

1.  计算机图形学：行列式检测变换是否保持或翻转方向，这在渲染中很有用。

1.  物理：行列式描述了积分中坐标变化的雅可比矩阵，调整体积元素。

1.  工程：行列式量化材料中的变形和应力（应变张量）。

1.  数据科学：协方差矩阵的行列式编码了不确定性椭圆体的“体积”。

#### 为什么它很重要

1.  行列式将代数（公式）与几何（形状）联系起来。

1.  它们解释了为什么某些变换会丢失信息：$\det=0$。

1.  它们保持方向，这对于一致的物理定律和几何学至关重要。

1.  它们为我们准备了像雅可比矩阵、特征值和体积保持映射这样的高级工具。

#### 试试看

1.  计算由 $(1,2)$ 和 $(3,1)$ 张成的平行四边形的面积。

1.  找出由向量 $(1,0,0),(0,1,0),(1,1,1)$ 定义的平行六面体的体积。

1.  证明交换矩阵的两列会翻转行列式的符号，但保持绝对值不变。

1.  挑战：解释为什么 $\det(A)$ 在变量变化下给出积分的缩放因子。

行列式最初是代数公式，但它们的真正意义在于几何：它们测量线性变换如何缩放、压缩或翻转空间本身。

### 52. 通过线性规则求行列式

行列式不仅仅是一个神秘的公式——它是由几个简单规则构建的函数，这些规则唯一地确定了其行为。这些规则，通常被称为行列式公理，使我们能够将行列式视为与线性代数兼容的“带符号体积”的唯一度量。理解这些规则提供了清晰的见解：我们不是记住展开公式，而是看到行列式为什么会这样表现。

#### 设置

考虑一个 $A \in \mathbb{R}^{n \times n}$ 的方阵。将 $A$ 视为 $n$ 个列向量的列表：

$$ A = \begin{bmatrix} a_1 & a_2 & \cdots & a_n \end{bmatrix}. $$

行列式是一个函数 $\det: \mathbb{R}^{n \times n} \to \mathbb{R}$，它将一个单一的数字分配给 $A$。从几何上看，它给出了由 $(a_1, \dots, a_n)$ 张成的平行六面体的带符号体积。从代数上看，它遵循三个关键规则。

#### 规则 1：每列的线性

如果你用一个标量 $c$ 缩放一列，行列式也会按 $c$ 缩放。

$$ \det(a_1, \dots, c a_j, \dots, a_n) = c \cdot \det(a_1, \dots, a_j, \dots, a_n). $$

如果你用一个和替换一列，行列式会分解：

$$ \det(a_1, \dots, (b+c), \dots, a_n) = \det(a_1, \dots, b, \dots, a_n) + \det(a_1, \dots, c, \dots, a_n). $$

这种线性意味着行列式在缩放和加法方面表现出可预测的行为。

#### 规则 2：交替性质

如果两列相同，行列式为零：

$$ \det(\dots, a_i, \dots, a_i, \dots) = 0. $$

从几何上看是有意义的：如果两个生成向量相同，它们会将体积压缩为零。

同样地：如果你交换两列，行列式的符号会翻转：

$$ \det(\dots, a_i, \dots, a_j, \dots) = -\det(\dots, a_j, \dots, a_i, \dots). $$

#### 规则 3：归一化

单位矩阵的行列式为 1：

$$ \det(I_n) = 1. $$

这为函数提供了锚点：单位立方体的体积为 1，具有正方向。

#### 结果：唯一性

这三个规则（线性、交替、归一化）唯一地定义了行列式。任何满足这些条件的函数都必须是行列式。这使得它不再是任意公式，而是线性结构的自然结果。

#### 小案例：显式公式

+   2×2 矩阵：

    $$ \det \begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc. $$

    这个公式直接来源于规则：列的线性以及交换列时的交替符号。

+   3×3 矩阵：展开公式：

    $$ \det \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix} = aei + bfg + cdh - ceg - bdi - afh. $$

这看起来很复杂，但它来自于系统地应用规则来分解体积。

#### 规则的几何解释

1.  线性：拉伸平行四边形或平行六面体的一边会按比例缩放面积或体积。

1.  交替：如果两边折叠到同一方向，面积/体积就会消失。交换两边会翻转方向。

1.  归一化：单位立方体的尺寸定义为 1。

这些规则与几何直觉完全一致。

#### 高维推广

在 $\mathbb{R}^n$ 中，行列式测量有向超体积。例如，在 4D 中，行列式给出平行体的“4-体积”。虽然无法想象，但相同的规则适用。

#### 应用

1.  定义面积和体积：行列式提供了从坐标计算几何尺寸的通用公式。

1.  雅可比行列式：在多重积分变量变化时用于微积分。

1.  方向检测：在几何或物理中，变换是否保持手性。

1.  计算机图形学：确保多边形和网格的一致方向。

#### 为什么这很重要

行列式不是任意的。一旦我们要求一个在列上线性、交错和归一化的函数，它们就会自然出现。这解释了为什么有这么多不同的公式和性质一致：它们都是同一基本定义的影子。

#### 尝试自己操作

1.  证明将一列乘以 3 会将行列式乘以 3。

1.  计算矩阵 $\begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}$ 的行列式，并解释为什么它是零。

1.  在矩阵 $\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}$ 中交换两列，并确认行列式的符号发生变化。

1.  挑战：仅使用三个规则推导出 $2 \times 2$ 行列式公式。

行列式是代数和几何之间独特的桥梁，它源于几个简单但强大的规则。

### 53. 行列式和行操作

计算行列式最实用的方法之一是使用行操作，这与高斯消元法中使用的工具相同。行列式与这些操作以非常结构化的方式进行交互。通过理解这些规则，我们可以系统地计算行列式，而无需求助于冗长的展开公式。

#### 行操作回顾

有三种基本的行操作：

1.  行交换（R$_i \leftrightarrow$ R$_j$）- 交换两行。

1.  行缩放（c·R$_i$）- 将一行乘以一个标量 $c$。

1.  行替换（R$_i$ + c·R$_j$）- 用自身加上另一行的倍数替换一行。

由于行列式是根据行（或列）的线性性和交错性定义的，每个操作都有一个明确的效果。

#### 规则 1：行交换会改变符号

如果你交换两行，行列式的符号会改变：

$$ \det(A \text{ with } R_i \leftrightarrow R_j) = -\det(A). $$

原因：交换两个生成向量会翻转方向。在 2D 中，交换基向量会将平行四边形沿对角线翻转，反转手性。

#### 规则 2：行缩放会乘行列式

如果你将一行乘以一个标量 $c$：

$$ \det(A \text{ with } cR_i) = c \cdot \det(A). $$

原因：平行四边形的一边缩放会乘以其面积；立方体的一维缩放会乘以其体积。

#### 规则 3：行替换不会改变行列式

如果你用自身加上另一行的倍数替换一行：

$$ \det(A \text{ with } R_i \to R_i + cR_j) = \det(A). $$

原因：将一个基向量的倍数加到另一个基向量上不会改变所张成的体积。平行四边形或平行六面体被剪切，但其面积或体积保持不变。

#### 为什么这些规则可以一起工作

这三个规则与行列式公理完美一致：

+   交替 → 行交换改变符号。

+   线性 → 缩放乘以标量。

+   归一化 → 行替换保持量度。

因此，行操作为计算行列式提供了一个完整的框架。

#### 使用消元法计算行列式

要计算 $\det(A)$：

1.  执行高斯消元法将 $A$ 化为上三角矩阵 $U$。

1.  跟踪行交换和缩放如何影响行列式。

1.  利用三角矩阵的行列式是其对角线元素的乘积这一事实。

示例：

$$ A = \begin{bmatrix} 2 & 1 & 3 \\ 4 & 1 & 7 \\ -2 & 5 & 1 \end{bmatrix}. $$

+   第 1 步：$R_2 \to R_2 - 2R_1$，$R_3 \to R_3 + R_1$。行列式不变。

+   第 2 步：出现上三角形式：

    $$ U = \begin{bmatrix} 2 & 1 & 3 \\ 0 & -1 & 1 \\ 0 & 0 & -5 \end{bmatrix}. $$

+   第 3 步：行列式是对角线的乘积：$\det(A) = 2 \cdot (-1) \cdot (-5) = 10.$

高效、清晰，没有繁琐的余子式展开。

#### 几何视图

+   行交换：翻转体积的定向。

+   行缩放：拉伸或压缩体积的一个维度。

+   行替换：在不改变体积大小的情况下滑动体积的面。

这种几何推理加强了为什么规则是自然的。

#### 应用

1.  高效计算：大行列式（LU 分解）的算法基于行操作。

1.  数值分析：行列式规则有助于检测稳定性和奇异性。

1.  几何：多边形的定向测试依赖于行交换规则。

1.  理论结果：许多行列式恒等式直接从行操作行为推导而来。

#### 为什么这很重要

+   行列式将代数与几何联系起来，但计算需要高效的方法。

+   行操作提供了一个实用的工具包：它们是实际行列式计算的骨架。

+   理解这些规则可以解释为什么像 LU 分解这样的算法工作得如此之好。

#### 试试看

1.  使用消元法计算 $\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}$ 的行列式。

1.  验证将 $R_2 \to R_2 + 3R_1$ 替换后行列式不变。

1.  检查如果将行按严格递增顺序重新排序，会发生多少符号翻转。

1.  挑战：证明消元法结合这些规则总是导致三角乘积公式。

行列式不应该通过蛮力展开；行操作将问题转化为一系列清晰的步骤，将代数效率与几何直觉联系起来。

### 54. 三角矩阵和对角线乘积

在所有类型的矩阵中，三角矩阵因其简单性而突出。这些矩阵的每个元素要么在主对角线之上，要么在主对角线之下都是零。它们特别重要的原因是它们的行列式几乎可以立即计算出来：三角矩阵的行列式就是其对角线元素的乘积。这一性质不仅计算方便，还揭示了行列式、行操作和线性代数结构之间的深层联系。

#### 三角矩阵定义

如果一个方阵的主对角线以下的所有项都是零，则称为上三角矩阵；如果主对角线以上的所有项都是零，则称为下三角矩阵。

+   上三角矩阵示例：

    $$ U = \begin{bmatrix} 2 & 5 & -1 \\ 0 & 3 & 4 \\ 0 & 0 & 7 \end{bmatrix}. $$

+   下三角矩阵示例：

    $$ L = \begin{bmatrix} 4 & 0 & 0 \\ -2 & 5 & 0 \\ 1 & 3 & 6 \end{bmatrix}. $$

两者都具备一个关键特征：“对角线一侧的所有项都消失。”

#### 行列式规则

对于任何三角矩阵，

$$ \det(T) = \prod_{i=1}^n t_{ii}, $$

其中 $t_{ii}$ 是对角线元素。

因此，对于上面的上三角矩阵 $U$，

$$ \det(U) = 2 \times 3 \times 7 = 42. $$

#### 为什么这有效

行列式是多元线性且交替的。当你展开它（例如，通过余子式展开）时，只有一项乘积在展开中存活：即恰好选择对角线项的乘积。

+   如果你尝试在行中选取一个非对角线元素，由于三角形的形状，你最终会得到一个零元素。

+   唯一存活的项是对角线的乘积，符号为 $+1$。

这优雅的推理解释了为什么这个规则普遍适用。

#### 与行操作的关系

回忆：消元法将任何方阵简化为上三角形式。一旦变为三角形式，行列式就简单地是主对角线元素的乘积，并考虑行交换和缩放。

因此，三角矩阵不仅简单，它们还是行列式计算消元算法的最终目标。

#### 几何意义

在几何术语中：

+   三角矩阵表示一种变换，其中每个坐标方向只依赖于自身和之前的坐标。

+   矩阵的行列式等于每个轴上缩放乘积的乘积。

+   示例：在 3D 中，将 x 缩放 2 倍，y 缩放 3 倍，z 缩放 7 倍，体积缩放为 $2 \cdot 3 \cdot 7 = 42$。

即使在上面的元素中存在剪切，行列式也会忽略它——它只关心纯对角线缩放。

#### 应用

1.  高效计算：LU 分解将行列式简化为对角线乘积形式。

1.  理论证明：许多行列式恒等式简化为三角情况。

1.  数值稳定性：三角矩阵在计算中表现良好，对数值线性代数中的算法至关重要。

1.  特征值：对于三角矩阵，特征值正好是对角线元素；因此行列式 = 特征值的乘积。

1.  计算机图形学：三角形式简化了几何变换。

#### 为什么这很重要

1.  提供了在特殊情况下计算行列式的最快方法。

1.  作为通用行列式算法的计算基础。

1.  将行列式直接与特征值和缩放因子联系起来。

1.  阐述了消元如何将复杂性转化为简单性。

#### 试试看

1.  计算以下行列式

    $$ \begin{bmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & 6 \end{bmatrix}. $$

    （检查：它应该等于 $1 \cdot 4 \cdot 6$）。

1.  验证对角线元素为 $(2, -1, 5)$ 的下三角矩阵的行列式为 $-10$。

1.  解释为什么对角线为零的上三角矩阵的行列式必须为 0。

1.  挑战：证明每个方阵都可以通过消元步骤将行列式追踪到三角形式。

三角情况揭示了确定性的核心：对角缩放因子的乘积，去除了所有额外的噪声。这是确定性的最简单透镜，通过它确定性的本质变得透明。

### 55. 确定性的乘法性质：$\det(AB) = \det(A)\det(B)$

关于确定性的最令人瞩目且有用的一个事实是，它们在矩阵乘积中相乘。对于相同大小的两个方阵，

$$ \det(AB) = \det(A) \cdot \det(B). $$

这一性质是基本的：它将代数（矩阵乘法）与几何（缩放体积）联系起来，对于数学、物理和工程中的证明、计算和应用至关重要。

#### 用语言陈述

+   如果你首先应用线性变换 $B$，然后应用 $A$，空间的总缩放是它们各自缩放的乘积。

+   确定性跟踪的正是这一点：线性变换下的带符号体积变化。

#### 几何直观

将 $\det(A)$ 视为 $A$ 改变体积的带符号缩放因子。

1.  应用 $B$：一个单位立方体变成了某个体积为 $|\det(B)|$ 的平行六面体。

1.  应用 $A$：新的平行六面体再次按 $|\det(A)|$ 缩放。

1.  总效果：体积按 $|\det(A)| \times |\det(B)|$ 缩放。

方向翻转也是一致的：如果两者都翻转（负行列式），则总方向保持不变（正乘积）。

#### 代数推理

证明可以从多个角度进行：

1.  行操作和消元：

    +   $A$ 和 $B$ 可以分解为初等矩阵（行交换、缩放、替换）。

    +   确定性对于每个操作都表现出可预测的行为。

    +   由于两边在初等运算上是一致的，且行列式是可乘的，因此恒等式在一般情况下成立。

1.  抽象描述：

    +   确定性是唯一的多线性交替函数，在恒等变换下归一化。

    +   线性映射的复合保持这一性质，因此乘法性随之而来。

#### 小案例

+   2×2 矩阵：

    $$ A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}, \quad B = \begin{bmatrix} e & f \\ g & h \end{bmatrix}. $$

    计算 $AB$，然后 $\det(AB)$。展开后，你会发现：$\det(AB) = (ad - bc)(eh - fg) = \det(A)\det(B)$。

+   3×3 矩阵：直接计算很麻烦，但这个性质仍然成立，并且与消元证明一致。

#### 关键后果

1.  幂的行列式：

    $$ \det(A^k) = (\det(A))^k. $$

    几何意义：应用相同的变换 $k$ 次会重复乘以体积尺度。

1.  逆矩阵：如果 $A$ 是可逆的，

    $$ \det(A^{-1}) = \frac{1}{\det(A)}. $$

1.  特征值：由于 $\det(A)$ 是特征值的乘积，这一性质与特征值在矩阵乘法下相乘的事实相匹配（通过特征多项式考虑）。

#### 高维中的几何意义

+   如果 $B$ 通过 3 缩放空间并翻转（行列式 = –3），而 $A$ 通过 2 缩放但不翻转（行列式 = 2），那么 $AB$ 通过 –6 缩放，这与规则一致。

+   行列式封装了大小（体积缩放）和符号（方向）。乘法性确保这些组合正确。

#### 应用

1.  微积分中的变量替换：雅可比行列式遵循这个乘法规则，确保变换能够一致地组合。

1.  群论：$\det$ 定义了一个从一般线性群 $GL_n$ 到乘法下非零实数的群同态。

1.  数值分析：行列式的乘法性是 LU 分解方法的基础。

1.  物理：在力学和相对论中，体积元素在连续变换下保持一致性。

1.  密码学和编码理论：模算术中的行列式依赖于这个乘法性质以保持结构。

#### 为什么这很重要

+   保证一致性：行列式与我们对缩放的直觉相匹配。

+   简化计算：通过乘以较小的部分可以获得因式分解的行列式。

+   提供理论结构：$\det$ 是一个同态，将线性代数嵌入到标量代数中。

#### 尝试自己

1.  验证 $\det(AB) = \det(A)\det(B)$ 对于

    $$ A = \begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix}, \quad B = \begin{bmatrix} 1 & 4 \\ 0 & -2 \end{bmatrix}. $$

1.  使用乘法规则证明 $\det(A^{-1}) = 1/\det(A)$。

1.  证明如果 $\det(A)=0$，则对于任何 $B$，$\det(AB)=0$。解释为什么这在几何上是有意义的。

1.  挑战：使用行操作，明确地展示乘法性如何从初等矩阵的性质中产生。

规则 $\det(AB) = \det(A)\det(B)$ 将行列式从神秘的计算转变为线性变换组合的自然且一致的度量。

### 56. 可逆性和零行列式

行列式不仅仅是一个几何尺度因子——它是检验矩阵是否可逆的终极测试。一个 $A \in \mathbb{R}^{n \times n}$ 的方阵有逆矩阵当且仅当其行列式不为零。当行列式为零时，矩阵将空间折叠到更低维度，丢失了任何变换都无法恢复的信息。

#### 判别准则

$$ A \text{ invertible } \iff \det(A) \neq 0. $$

+   如果 $\det(A) \neq 0$，变换会拉伸或压缩空间，但永远不会扁平化。每个输出都对应于一个唯一的输入，因此 $A^{-1}$ 存在。

+   如果 $\det(A) = 0$，某些方向会被压缩到低维。信息被破坏，因此不存在逆矩阵。

#### 几何意义

1.  在 2D 中：

    +   非零行列式意味着单位正方形被映射到具有非零面积的平行四边形。

    +   零行列式意味着平方矩阵塌缩成线段或一个点。

1.  在 3D 中：

    +   非零行列式 → 单位立方体变为具有体积的 3D 长方体。

    +   零行列式 → 立方体塌缩成薄片或线；3D 体积丢失。

1.  在高维中：

    +   非零行列式保持 n 维体积。

    +   零行列式会塌缩维度，破坏可逆性。

#### 代数意义

+   行列式是特征值的乘积：

    $$ \det(A) = \lambda_1 \lambda_2 \cdots \lambda_n. $$

    如果任何特征值为零，则 $\det(A) = 0$ 且矩阵是奇异的（不可逆的）。

+   同样，零行列式意味着矩阵有线性相关的列或行。这种相关性意味着冗余：不是所有方向都是独立的，因此映射不能是一对一。

#### 与线性系统的联系

+   如果 $\det(A) \neq 0$：

    +   对于每个 $b$，系统 $Ax = b$ 都有唯一解。

    +   逆矩阵 $A^{-1}$ 存在，并满足 $x = A^{-1}b$。

+   如果 $\det(A) = 0$：

    +   要么无解（不一致的方程组）要么有无穷多个解（相关方程）。

    +   映射 $x \mapsto Ax$ 不能逆转。

#### 例子：可逆与奇异

$$ A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad \det(A) = 5 \neq 0. $$

可逆的。

$$ B = \begin{bmatrix} 2 & 4 \\ 1 & 2 \end{bmatrix}, \quad \det(B) = 0. $$

不可逆，因为第二列是第一列的两倍。

#### 应用

1.  解系统：基于逆的方法依赖于非零行列式。

1.  数值方法：检测接近奇异性会警告不稳定解。

1.  几何：奇异矩阵对应于退化形状（扁平化、塌缩）。

1.  物理：在力学和相对论中，可逆性确保变换可以逆转。

1.  计算机图形学：不可逆变换会压缩维度，破坏渲染管线。

#### 为什么这很重要

+   行列式提供了一个单一的标量测试来检验可逆性。

+   这将几何（体积塌缩）、代数（线性相关）和分析（系统的可解性）联系起来。

+   零/非零的划分是线性代数中最尖锐和最重要的之一。

#### 尝试自己操作

1.  判断

    $$ \begin{bmatrix} 1 & 2 \\ 3 & 6 \end{bmatrix} $$

    是否可逆。从几何和代数两方面进行解释。

1.  对于

    $$ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 0 \end{bmatrix}, $$

    计算行列式并描述几何变换。

1.  挑战：证明如果 $\det(A)=0$，则 $A$ 的行（或列）线性相关。

行列式充当最终的是或否测试：非零表示满维可逆变换；零表示崩溃和不可逆。

### 57. 代数余子式展开

虽然消元给出了计算行列式的实用方法，但代数余子式展开（也称为拉普拉斯展开）提供了一个适用于所有方阵的递归定义。它用较小 $(n-1) \times (n-1)$ 矩阵的行列式来表示 $n \times n$ 矩阵的行列式。这种方法揭示了行列式的内部结构，并在理论与计算之间架起了一座桥梁。

#### 代数余子式和代数余子式

+   元素 $a_{ij}$ 的代数余子式 $M_{ij}$ 是从 $A$ 中删除第 $i$ 行和第 $j$ 列得到的子矩阵的行列式。

+   代数余子式 $C_{ij}$ 添加一个符号因子：

    $$ C_{ij} = (-1)^{i+j} M_{ij}. $$

因此，每个元素通过其代数余子式对行列式做出贡献，正负符号以棋盘式图案交替排列：

$$ \begin{bmatrix} + & - & + & - & \cdots \\ - & + & - & + & \cdots \\ + & - & + & - & \cdots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{bmatrix}. $$

#### 展开公式

对于任何行 $i$：

$$ \det(A) = \sum_{j=1}^n a_{ij} C_{ij}. $$

或者对于任何列 $j$：

$$ \det(A) = \sum_{i=1}^n a_{ij} C_{ij}. $$

即，行列式可以通过沿任意行或列展开来计算。

#### 示例：3×3 情况

让

$$ A = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix}. $$

沿第一行展开：

$$ \det(A) = a \cdot \det \begin{bmatrix} e & f \\ h & i \end{bmatrix} - b \cdot \det \begin{bmatrix} d & f \\ g & i \end{bmatrix} + c \cdot \det \begin{bmatrix} d & e \\ g & h \end{bmatrix}. $$

简化每个 2×2 行列式：

$$ = a(ei - fh) - b(di - fg) + c(dh - eg). $$

这与 3×3 行列式的熟悉展开公式相匹配。

#### 为什么它有效

代数余子式展开直接源于行列式的多线性规则和交替规则：

+   每一行和每一列只对每个项贡献一个元素。

+   符号交替，因为交换行/列会反转方向。

+   递归展开将问题规模减小，直到达到 2×2 行列式，那里的公式很简单。

#### 计算复杂性

+   对于 $n=2$，展开是直接的。

+   对于 $n=3$，展开是可管理的。

+   对于大的 $n$，展开非常低效：通过代数余子式计算 $\det(A)$ 需要 $O(n!)$ 次操作。

这就是为什么在实践中，人们更倾向于使用消元或 LU 分解。代数余子式展开最适合理论、证明和小矩阵。

#### 几何解释

每个代数余子式对应于排除一个方向（行/列），测量剩余子平行体的体积。交替符号跟踪方向。因此，行列式是所有条目沿所选行或列的贡献的加权组合。

#### 应用

1.  理论证明：代数余子式展开是许多行列式恒等式的基础。

1.  伴随矩阵：余子式形成伴随矩阵，用于矩阵逆的显式公式。

1.  特征值：特征多项式使用余子式展开。

1.  几何：余子式描述了高维形状面的带符号体积。

#### 为什么这很重要

+   余子式展开连接了不同维度的行列式。

+   它提供了一个与行操作无关的通用定义。

+   它解释了行列式如何与体积、方向和代数规则保持一致。

#### 尝试自己操作

1.  展开行列式。

    $$ \begin{bmatrix} 2 & 1 & 3 \\ 0 & -1 & 4 \\ 1 & 2 & 0 \end{bmatrix} $$

    沿第一行展开。

1.  通过沿第二列展开来计算相同的行列式。验证结果是否一致。

1.  证明沿不同行展开给出相同的行列式。

1.  挑战：通过归纳法证明余子式展开对所有 $n \times n$ 矩阵都有效。

余子式展开不是最快的方法，但它揭示了行列式的递归结构，并解释了为什么它们具有丰富的代数和几何意义。

### 58. 排列和行列式的符号

每个行列式公式背后都隐藏着一个隐藏的结构：排列。行列式可以表示为对所有可能的从矩阵的每一行和每一列中选择一个元素的所有方式的加权求和。每个选择的权重由排列的符号确定。这种观点揭示了行列式编码方向的原因，以及为什么它们的公式在正负项之间交替。

#### 排列定义

令 $S_n$ 表示 $n$ 个元素的排列的集合。每个排列 $\sigma \in S_n$ 重新排列数字 $\{1, 2, \ldots, n\}$。

$n \times n$ 矩阵 $A = [a_{ij}]$ 的行列式定义为：

$$ \det(A) = \sum_{\sigma \in S_n} \text{sgn}(\sigma) \prod_{i=1}^n a_{i, \sigma(i)}. $$

+   每个乘积 $\prod_{i=1}^n a_{i, \sigma(i)}$ 根据排列 $\sigma$ 从每一行和每一列中选取一个元素。

+   因子 $\text{sgn}(\sigma)$ 为 $+1$ 如果 $\sigma$ 是偶排列（通过偶数个交换实现），如果是奇排列则为 $-1$。

#### 排列为何出现

行列式需要：

1.  每行线性。

1.  交替性质（行交换翻转符号）。

1.  归一化（$\det(I)=1$）。

当你通过多线性展开时，所有可能的从每一行和每一列中选择一个元素的组合都会出现。交替规则强制使具有重复列的项消失，只留下排列。每个排列的符号强制执行方向翻转。

#### 示例：2×2 情况

$$ A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}. $$

在 $S_2$ 中有两个排列：

+   标识 $(1,2)$：符号 $+1$，贡献 $a \cdot d$。

+   交换 $(2,1)$：符号 $-1$，贡献 $-bc$。

因此，

$$ \det(A) = ad - bc. $$

#### 示例：3×3 情况

$$ A = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix}. $$

有 $3! = 6$ 个排列：

1.  $(1,2,3)$：偶数，$+aei$。

1.  $(1,3,2)$：奇数，$-afh$。

1.  $(2,1,3)$：奇排列，$-bdi$。

1.  $(2,3,1)$：偶排列，$+bfg$。

1.  $(3,1,2)$：偶排列，$+cdh$。

1.  $(3,2,1)$：奇排列，$-ceg$。

因此，

$$ \det(A) = aei + bfg + cdh - ceg - bdi - afh. $$

这正是余子式展开的结果，但现在被解释为排列和。

#### 符号的几何意义

+   交换排列对应于基向量的一致方向。

+   奇排列对应于翻转的方向。

+   行列式交替符号是因为翻转轴改变了手性。

#### 计数增长

+   对于 $n=4$，有 $4! = 24$ 项。

+   对于 $n=5$，$5! = 120$ 项。

+   通常，$n!$ 项使得这个公式对于大矩阵来说不实用。

+   尽管如此，它给出了行列式的最深刻定义，所有其他规则都由此而来。

#### 应用

1.  抽象代数：行列式的定义通过排列在任意域上成立。

1.  组合数学：行列式编码排列上的带符号和，与永真值相关联。

1.  理论证明：许多行列式性质，如可乘性，从排列定义中清晰地出现。

1.  莱布尼茨公式：显式但实际应用不实用的计算公式。

1.  高级数学：行列式在线性代数和微分几何中推广到交替多线性形式。

#### 为什么它很重要

+   提供了行列式的最基本定义。

+   自然地解释公式中的交替符号。

+   连接代数、几何和组合数学。

+   展示了方向如何从行/列排列中产生。

#### 试试你自己

1.  在 3×3 行列式展开中写出所有 6 项，并验证每个排列的符号。

1.  使用排列定义计算 $\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}$ 的行列式。

1.  证明如果两列相等，所有排列项都会抵消，给出 $\det(A)=0$。

1.  挑战：证明交换两行会改变每个排列项的符号，翻转总行列式的符号。

行列式可能看起来像代数谜题，但排列公式揭示了它们的真正本质：对所有可能的行与列匹配方式的总和，符号记录了方向是否保持或反转。

### 59. 克莱姆法则

克莱姆法则是一种使用行列式解决线性方程组的经典方法。虽然由于效率低下而很少用于大规模计算，但它提供了关于行列式、可逆性和线性系统之间关系的深刻理论见解。它展示了行列式不仅编码了体积缩放，而且还编码了方程的精确解。

#### 设置

考虑一个具有 $n$ 个未知数的 $n$ 个线性方程组：

$$ Ax = b, $$

其中 $A$ 是一个可逆的 $n \times n$ 矩阵，$x$ 是未知数向量，$b$ 是右侧向量。

克莱姆法则指出：

$$ x_i = \frac{\det(A_i)}{\det(A)}, $$

其中 $A_i$ 是矩阵 $A$，其第 $i$ 列被 $b$ 替换。

#### 示例：2×2 情况

解：

$$ \begin{cases} 2x + y = 5 \\ x + 3y = 7 \end{cases} $$

矩阵形式：

$$ A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad b = \begin{bmatrix} 5 \\ 7 \end{bmatrix}. $$

$A$ 的行列式：

$$ \det(A) = 2\cdot 3 - 1\cdot 1 = 5. $$

+   对于 $x_1$: 将第一列替换为 $b$:

$$ A_1 = \begin{bmatrix} 5 & 1 \\ 7 & 3 \end{bmatrix}, \quad \det(A_1) = 15 - 7 = 8. $$

所以 $x_1 = 8/5$。

+   对于 $x_2$: 将第二列替换为 $b$:

$$ A_2 = \begin{bmatrix} 2 & 5 \\ 1 & 7 \end{bmatrix}, \quad \det(A_2) = 14 - 5 = 9. $$

所以 $x_2 = 9/5$。

解：$(x,y) = (8/5, 9/5)$。

#### 为什么它有效

由于 $A$ 是可逆的，

$$ x = A^{-1}b. $$

但要记住逆矩阵的公式：

$$ A^{-1} = \frac{1}{\det(A)} \text{adj}(A), $$

其中 $\text{adj}(A)$ 是伴随（余子式矩阵的转置）。当我们乘以 $\text{adj}(A)b$ 时，每个分量自然成为一个行列式，其中一个列被 $b$ 替换。这正是克莱姆法则。

#### 几何解释

+   分母 $\det(A)$ 表示由 $A$ 的列所张成的平行六面体的体积。

+   分子 $\det(A_i)$ 表示当第 $i$ 列被 $b$ 替换时的体积。

+   这个比值说明了体积贡献中有多少是与第 $i$ 个方向对齐的，从而给出了解的坐标。

#### 效率和局限性

+   对于小的 $n$ 来说很好：对于 2×2 或 3×3 系统来说很优雅。

+   对于大的 $n$ 来说不高效：需要计算 $n+1$ 个行列式，如果通过余子式展开，每个行列式的复杂度都是阶乘。

+   数值不稳定性：行列式可能对舍入误差敏感。

+   在实践中，高斯消元法或 LU 分解法更为优越。

#### 应用

1.  理论证明：为小系统建立了解的唯一性。

1.  几何：将解与平行六面体的体积比联系起来。

1.  符号代数：对于推导闭式表达式很有用。

1.  控制理论：有时用于可控性/可观测性的证明。

#### 为什么它很重要

+   提供了一个将行列式与线性系统的解联系起来的清晰公式。

+   证明了行列式不仅仅是体积测量的力量。

+   作为代数解与几何解释之间的概念桥梁。

#### 尝试自己操作

1.  使用克莱姆法则解方程组 $\begin{cases} x + 2y = 3 \\ 4x + 5y = 6 \end{cases}$。

1.  对于矩阵 $\begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 5 & 6 & 0 \end{bmatrix}$ 的 3×3 系统来说，使用克莱姆法则计算 $x_1$。

1.  验证当 $\det(A)=0$ 时，克莱姆法则失效，这与系统要么不一致要么有无穷多解的事实相匹配。

1.  挑战：从伴随矩阵公式推导出克莱姆法则。

克莱姆法则不是一个计算上的工作马，但它优雅地将行列式、可逆性和线性系统的解联系在一起——展示了几何、代数和计算如何在简洁的公式中相遇。

### 60. 实际计算行列式

行列式具有深刻的含义，但在实际计算中，你选择的方法至关重要。对于小矩阵，如余子式展开或克莱姆法则等公式是可管理的。然而，对于较大的系统，这些直接方法很快就会变得低效。实际计算依赖于利用结构的系统算法，特别是消元和矩阵分解。

#### 小矩阵（$n \leq 3$）

+   2×2 情况：

    $$ \det \begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc. $$

+   3×3 情况：可以通过余子式展开或使用“萨鲁斯法则”：

    $$ \det \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix} = aei + bfg + cdh - ceg - bdi - afh. $$

这些公式很紧凑，但并不适用于 $3 \times 3$ 以外的情形。

#### 大矩阵：消元和 LU 分解

对于 $n > 3$，实际方法围绕高斯消元展开。

1.  行简化：

    +   使用行操作将 $A$ 简化为上三角矩阵 $U$。

    +   跟踪操作：

        +   行交换 → 行列式的符号翻转。

        +   行缩放 → 行列式乘以缩放因子。

        +   行替换 → 无影响。

    +   一旦成为三角矩阵，就计算行列式为对角线元素的乘积。

1.  LU 分解：

    +   表示 $A = LU$，其中 $L$ 是下三角矩阵，$U$ 是上三角矩阵。

    +   然后 $\det(A) = \det(L)\det(U)$。

    +   由于 $L$ 的对角线上有 1，所以 $\det(L)=1$，因此行列式只是 $U$ 对角线的乘积。

这种方法将复杂度降低到 $O(n³)$，比余子式展开的阶乘增长要高效得多。

#### 数值考虑

+   浮点稳定性：行列式可能非常大或非常小，导致计算机溢出或下溢。

+   旋转：在实际操作中，部分旋转确保了消元过程中的稳定性。

+   条件数：如果一个矩阵几乎奇异（$\det(A)$ 接近 0），计算出的行列式可能非常不准确。

由于这些原因，在数值线性代数中，行列式很少直接计算；相反，使用 LU 或 QR 分解的性质。

#### 通过特征值计算行列式

由于行列式等于特征值的乘积，

$$ \det(A) = \lambda_1 \lambda_2 \cdots \lambda_n, $$

它可以通过找到特征值（通过 QR 迭代或其他方法数值计算）来计算。当特征值已经需要时，这种方法很有用，但仅仅为了计算行列式而计算特征值通常比消元更昂贵。

#### 特殊矩阵

+   对角或三角矩阵：行列式是对角线的乘积-最快的情况。

+   块对角矩阵：行列式是块行列式的乘积。

+   稀疏矩阵：利用结构-只有非零模式很重要。

+   正交矩阵：行列式始终为 $+1$ 或 $-1$。

#### 应用

1.  系统求解：行列式测试可逆性，但实际求解使用消元。

1.  计算机图形学：行列式检测方向翻转（对渲染很有用）。

1.  优化：海森矩阵的行列式表示曲率和稳定性。

1.  统计学：协方差矩阵的行列式测量不确定性体积。

1.  物理：行列式出现在积分变量变化的雅可比矩阵中。

#### 为什么这很重要

+   行列式提供了矩阵的全局属性，但计算必须高效。

+   直接展开是优雅的，但不实用。

+   基于消元的方法平衡了理论、速度和可靠性，构成了现代计算线性代数的核心。

#### 尝试自己操作

1.  使用消元法计算 $\begin{bmatrix} 2 & 1 & 3 \\ 4 & 1 & 7 \\ -2 & 5 & 1 \end{bmatrix}$ 的行列式，以确认对角线乘积法。

1.  对于一个具有条目 $(2, 3, -1, 5)$ 的对角矩阵，验证行列式仅仅是它们的乘积。

1.  使用 LU 分解计算你选择的 $3 \times 3$ 矩阵的行列式。

1.  挑战：证明通过 LU 分解计算行列式只需要 $O(n³)$ 次操作，而余子式展开需要 $O(n!)$。

行列式是核心的，但在实践中，最好使用系统算法来处理，其中三角形形式和分解可以快速而可靠地揭示答案。

#### 结尾

```py
Flatness or fullness,
determinants quietly weigh
depth in every move.
```

## 第七章\. 特征值、特征向量和动力学

#### 开场

```py
Stillness in motion,
directions that never fade,
time reveals its core.
```

### 61\. 特征值和特征向量

在线性代数的所有概念中，很少有像特征值和特征向量那样核心和强大的。它们揭示了线性变换隐藏的“作用轴”——在空间中变换以最简单方式行为的方向。不是混合和旋转一切，特征向量在方向上保持不变，仅按其对应的特征值进行缩放。

#### 核心思想

设 $A$ 是一个 $n \times n$ 矩阵。如果非零向量 $v \in \mathbb{R}^n$ 是 $A$ 的特征向量，则

$$ Av = \lambda v, $$

对于某些标量 $\lambda \in \mathbb{R}$（或 $\mathbb{C}$）。标量 $\lambda$ 是与 $v$ 对应的特征值。

+   特征向量：变换保持的特殊方向。

+   特征值：特征向量被拉伸或压缩的因子。

如果 $\lambda > 1$，特征向量会被拉伸。如果 $0 < \lambda < 1$，它会被压缩。如果 $\lambda < 0$，它的方向会被翻转并按比例缩放。如果 $\lambda = 0$，向量会被压扁为零。

#### 为什么它们很重要

特征值和特征向量描述了变换的内禀结构：

+   它们给出了矩阵作用最简单的方向。

+   他们总结了重复应用的长远行为（例如，$A$ 的幂）。

+   它们将代数、几何以及物理学、数据科学和工程中的应用联系起来。

#### 示例：一个简单的 2D 情况

让

$$ A = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}. $$

+   将 $A$ 应用到 $(1,0)$ 上：

    $$ A \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 2 \\ 0 \end{bmatrix} = 2 \begin{bmatrix} 1 \\ 0 \end{bmatrix}. $$

    因此 $(1,0)$ 是一个特征值为 $2$ 的特征向量。

+   将 $A$ 应用到 $(0,1)$ 上：

    $$ A \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 3 \end{bmatrix} = 3 \begin{bmatrix} 0 \\ 1 \end{bmatrix}. $$

    所以 $(0,1)$ 是一个特征向量，其特征值为 $3$。

在这种情况下，特征向量与坐标轴对齐，特征值是对角线元素。

#### 一般情况：特征值方程

要找到特征值，我们解

$$ Av = \lambda v \quad \Leftrightarrow \quad (A - \lambda I)v = 0. $$

对于非平凡 $v$，矩阵 $(A - \lambda I)$ 必须是奇异的：

$$ \det(A - \lambda I) = 0. $$

这个行列式展开成特征多项式，其根是特征值。特征向量来自解相应的零空间。

#### 几何解释

+   特征向量是不变方向。当你应用 $A$ 时，向量可能会拉伸或翻转，但它不会绕其线旋转。

+   特征值是缩放因子。它们描述了沿着那个不变方向发生了多少拉伸、收缩或翻转。

例如：

+   在二维空间中，一个特征向量可能是一条通过原点的直线，其中变换作用类似于拉伸。

+   在三维空间中，剪切平面通常沿着不变轴有特征向量。

#### 动力学和重复应用

特征值之所以如此重要，一个原因是它们描述了重复的变换：

$$ A^k v = \lambda^k v. $$

如果你反复将 $A$ 应用到一个特征向量上，结果是可以预测的：只需乘以 $\lambda^k$。这解释了动力系统中的稳定性、人口模型中的增长和马尔可夫链中的收敛。

+   如果 $|\lambda| < 1$，重复应用将向量缩小到零。

+   如果 $|\lambda| > 1$，向量无界增长。

+   如果 $\lambda = 1$，向量保持相同的长度（尽管如果 $\lambda = -1$，方向可能会翻转）。

#### 应用

1.  物理：分子的振动、量子能级和共振都依赖于特征值/特征向量。

1.  数据科学：主成分分析（PCA）找到协方差矩阵的特征向量以检测方差的关键方向。

1.  马尔可夫链：稳态概率对应于特征值为 1 的特征向量。

1.  微分方程：特征值简化了线性常微分方程组。

1.  计算机图形学：可以使用特征分解来分析旋转和缩放等变换。

#### 为什么它很重要

+   特征值和特征向量将复杂的变换简化为其最简单的组成部分。

+   它们统一了代数（特征多项式的根）、几何（不变方向）和应用（稳定性、共振、方差）。

+   它们是特征化、奇异值分解和谱分析的基础，这些在现代应用数学中占主导地位。

#### 试试看自己动手做

1.  计算矩阵 $\begin{bmatrix} 4 & 2 \\ 1 & 3 \end{bmatrix}$ 的特征值和特征向量。

1.  对于 $A = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}$，找出它的特征值。（提示：它们是复数。）

1.  随机取一个 $2 \times 2$ 矩阵，检查其特征向量是否与坐标轴对齐。

1.  挑战：证明对应于不同特征值的特征向量是线性无关的。

特征值和特征向量是矩阵的“指纹”：它们捕捉了变换的基本行为，引导我们理解无数学科中的稳定性、动力学和结构。

### 62\. 特征多项式

为了揭示矩阵的特征值，我们使用一个中心工具：特征多项式。这个多项式编码了矩阵与其特征值之间的关系。多项式的根正是特征值，使其成为谱分析的代数门户。

#### 定义

对于一个 $A \in \mathbb{R}^{n \times n}$ 的方阵，特征多项式定义为

$$ p_A(\lambda) = \det(A - \lambda I). $$

+   $I$ 是与 $A$ 同大小的单位矩阵。

+   多项式 $p_A(\lambda)$ 的次数为 $n$。

+   $A$ 的特征值正好是 $p_A(\lambda)$ 的根。

#### 为什么这有效

特征值方程是

$$ Av = \lambda v \quad \iff \quad (A - \lambda I)v = 0. $$

对于非平凡 $v$，矩阵 $A - \lambda I$ 必须是奇异的：

$$ \det(A - \lambda I) = 0. $$

因此，特征值正是使得行列式为零的标量 $\lambda$。

#### 示例：2×2 情况

让

$$ A = \begin{bmatrix} 4 & 2 \\ 1 & 3 \end{bmatrix}. $$

计算：

$$ p_A(\lambda) = \det \begin{bmatrix} 4-\lambda & 2 \\ 1 & 3-\lambda \end{bmatrix}. $$

展开：

$$ p_A(\lambda) = (4-\lambda)(3-\lambda) - 2. $$

$$ = \lambda² - 7\lambda + 10. $$

根是 $\lambda = 5$ 和 $\lambda = 2$。这些是 $A$ 的特征值。

#### 示例：3×3 情况

对于

$$ B = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 3 & 4 \\ 0 & 4 & 9 \end{bmatrix}, $$

$$ p_B(\lambda) = \det \begin{bmatrix} 2-\lambda & 0 & 0 \\ 0 & 3-\lambda & 4 \\ 0 & 4 & 9-\lambda \end{bmatrix}. $$

展开：

$$ p_B(\lambda) = (2-\lambda)\big[(3-\lambda)(9-\lambda) - 16\big]. $$

$$ = (2-\lambda)(\lambda² - 12\lambda + 11). $$

根：$\lambda = 2, 1, 11$。

#### 特征多项式的性质

1.  次数：总是次数 $n$。

1.  首项：$(-1)^n \lambda^n$。

1.  常数项：$\det(A)$。

1.  $\lambda^{n-1}$ 的系数：$-\text{tr}(A)$，其中 $\text{tr}(A)$ 是迹（对角线元素之和）。

因此：

$$ p_A(\lambda) = (-1)^n \lambda^n + (\text{tr}(A))(-1)^{n-1}\lambda^{n-1} + \cdots + \det(A). $$

这将迹、行列式和特征值在一个多项式中联系起来。

#### 几何意义

+   特征多项式的根告诉我们沿不变方向的缩放因子。

+   在二维中：多项式编码了面积缩放（$\det(A)$）和总拉伸（$\text{tr}(A)$）。

+   在高维中：它将 $A$ 的复杂性压缩成一个方程，其解揭示了谱。

#### 应用

1.  特征值计算：对角化和谱理论的基础。

1.  控制理论：系统的稳定性取决于特征值（特征多项式的根）。

1.  微分方程：特征多项式描述了自然频率和振荡模式。

1.  图论：邻接矩阵的特征多项式编码了图的拓扑性质。

1.  量子力学：量子系统的能级来自求解算子的特征多项式。

#### 为什么这很重要

+   提供了一种系统、代数的方法来寻找特征值。

+   将迹和行列式与更深的谱性质联系起来。

+   桥接线性代数、多项式理论和几何学。

+   为现代计算方法如 QR 迭代奠定了基础。

#### 尝试自己动手做

1.  计算矩阵 $\begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}$ 的特征多项式。找出其特征值。

1.  验证特征值的乘积等于行列式。

1.  验证特征值的和等于迹。

1.  挑战：证明对于任何相同大小的 $A, B$，$p_{AB}(\lambda) = p_{BA}(\lambda)$。

特征多项式将矩阵提炼成一个单一的代数对象，其根揭示了变换的基本动力学。

### 63. 代数重数与几何重数

在研究特征值时，仅仅找到特征多项式的根是不够的。每个特征值可以出现多次，这种“重数”可以以两种不同但相关的方式理解：代数重数（它作为根出现的次数）和几何重数（其特征空间的维度）。这两个重数捕捉了特征值的代数和几何丰富性。

#### 代数重数

特征值 $\lambda$ 的代数重数（AM）是它在特征多项式 $p_A(\lambda)$ 中作为根出现的次数。

+   如果 $(\lambda - \lambda_0)^k$ 能整除 $p_A(\lambda)$，那么 $\lambda_0$ 的代数重数是 $k$。

+   所有代数重数的和等于矩阵的大小（$n$）。

示例：如果

$$ p_A(\lambda) = (\lambda-2)³(\lambda+1)², $$

然后，特征值 $\lambda=2$ 的代数重数为 3，而 $\lambda=-1$ 的代数重数为 2。

#### 几何重数

特征值 $\lambda$ 的几何重数（GM）是对应于 $\lambda$ 的特征空间的维度：

$$ \text{GM}(\lambda) = \dim(\ker(A - \lambda I)). $$

+   这计算了与 $\lambda$ 对应的线性无关特征向量的数量。

+   总是满足：

    $$ 1 \leq \text{GM}(\lambda) \leq \text{AM}(\lambda). $$

示例：如果

$$ A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}, $$

然后 $p_A(\lambda) = (\lambda-2)²$.

+   $\lambda=2$ 的代数重数是 2。

+   解 $(A-2I)v=0$:

    $$ \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} v = 0 \quad \Rightarrow \quad v = \begin{bmatrix} 1 \\ 0 \end{bmatrix}. $$

    只有 1 个独立的特征向量。

+   $\lambda=2$ 的几何重数是 1。

#### 两者之间的关系

+   总是：$\text{GM}(\lambda) \leq \text{AM}(\lambda)$。

+   如果对所有特征值都相等，则矩阵可对角化。

+   如果对于某些特征值 GM < AM，则矩阵是缺陷的，这意味着它不能对角化，尽管它可能仍然有约当标准形。

#### 几何意义

+   AM 衡量特征值在多项式中的“编码”强度。

+   GM 衡量特征值的特征空间提供的几何自由度。

+   如果 AM > GM，特征值“想要”比空间允许的更多独立方向。

将 AM 视为特征向量的*理论需求*，GM 视为*实际供应*。

#### 示例：对角化与非对角化

1.  对角化情况：

    $$ B = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}. $$

    +   $p_B(\lambda) = (\lambda-2)²$.

    +   对于特征值 2，AM = 2。

    +   GM = 2，因为特征空间是整个$\mathbb{R}²$。

    +   有足够的特征向量进行对角化。

1.  缺陷情况：前面的例子

    $$ A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} $$

    如果 AM = 2，GM = 1。

    +   缺少足够的特征向量。

    +   不能对角化。

#### 应用

1.  对角化：只有当所有特征值的 GM = AM 时才可能。

1.  约当形：缺陷矩阵需要约当块，由 AM 和 GM 之间的差距控制。

1.  微分方程：解的形式取决于重数；重复的特征值需要广义解。

1.  稳定性分析：重数揭示了动力系统中的简并性。

1.  量子力学：特征值的简并性（AM 与 GM）编码物理对称性。

#### 为什么这很重要

+   重数将代数根与几何结构分开。

+   他们决定是否可以进行对角化。

+   它们揭示了具有重复特征值的系统中的隐藏约束。

+   它们是像约当标准形和广义特征向量这样的高级概念的基础。

#### 试试你自己

1.  求$\begin{bmatrix} 3 & 1 \\ 0 & 3 \end{bmatrix}$的 AM 和 GM。

1.  求$\begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix}$的 AM 和 GM。与第一种情况比较。

1.  证明 AM 总是等于特征多项式的根的重数。

1.  挑战：证明对于任何特征值，GM ≥ 1。

代数重数和几何重数一起讲述整个故事：代数告诉我们特征值出现多少次，而几何告诉我们它在向量空间中实际占据了多少空间。

### 64. 对角化

对角化是线性代数中最强大的思想之一。它将一个复杂的矩阵，在可能的情况下，重新写成一种简单形式，其中其作用完全透明。对角矩阵很容易理解：它只是通过一个固定的因子拉伸或压缩每个坐标轴。如果我们能将一个矩阵转换成对角矩阵，那么许多计算——如计算幂或指数——几乎变得微不足道。

#### 核心概念

如果存在可逆矩阵$P$和对角矩阵$D$，使得$A \in \mathbb{R}^{n \times n}$是可对角化的，则

$$ A = P D P^{-1}. $$

+   $D$的对角元素是$A$的特征值。

+   $P$的列是相应的特征向量。

用话来说：$A$ 可以在由其特征向量构成的坐标系中“重写”，其中其作用简化为沿着独立方向进行简单的缩放。

#### 为什么对角化很重要

1.  简化计算：

    +   计算幂：

        $$ A^k = P D^k P^{-1}, \quad D^k \text{ 是容易计算的}. $$

    +   矩阵指数：

        $$ e^A = P e^D P^{-1}. $$

        在解微分方程中至关重要。

1.  澄清动力学：

    +   迭代过程的长远行为直接取决于特征值。

    +   从 $D$ 中可以读出稳定系统与不稳定系统。

1.  揭示结构：

    +   告诉我们系统是否可以通过独立模式来理解。

    +   将代数结构与几何联系起来。

#### 对角化的条件

一个矩阵 $A$ 是可对角化的，当且仅当它有足够的线性无关的特征向量来构成 $\mathbb{R}^n$ 的基。

+   等价地：对于每个特征值，几何重数等于代数重数。

+   不同的特征值保证了可对角化，因为它们的特征向量是线性无关的。

#### 示例：可对角化情况

令

$$ A = \begin{bmatrix} 4 & 0 \\ 1 & 3 \end{bmatrix}. $$

+   特征多项式：

    $$ p_A(\lambda) = (4-\lambda)(3-\lambda). $$

    特征值：$\lambda_1=4, \lambda_2=3$。

+   特征向量：

    +   对于 $\lambda=4$：$(1,1)^T$。

    +   对于 $\lambda=3$：$(0,1)^T$。

+   构建 $P = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}$，$D = \begin{bmatrix} 4 & 0 \\ 0 & 3 \end{bmatrix}$。

+   然后 $A = P D P^{-1}$。

现在，计算 $A^{10}$ 是容易的：只需计算 $D^{10}$ 并进行共轭。

#### 示例：缺陷（不可对角化）情况

$$ B = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}. $$

+   特征多项式：$(\lambda - 2)²$。

+   AM of eigenvalue 2 is 2, but GM = 1 (only one eigenvector).

+   不可对角化。需要使用约当形式。

#### 几何意义

对角化意味着我们可以旋转到一个由特征向量构成的基中，在这个基中，变换的作用很简单：每个轴按其特征值进行缩放。

+   想象一个房间，地板在一个方向上的拉伸比另一个方向上的拉伸要大。在正确的坐标系（与特征向量对齐）中，拉伸是纯粹沿着轴进行的。

+   没有对角化，拉伸会混合方向，更难以描述。

#### 应用

1.  微分方程：解线性常微分方程系统依赖于对角化或约当形式。

1.  马尔可夫链：通过对角化分析转移矩阵以研究稳态。

1.  量子力学：算子被对角化以揭示可测状态。

1.  PCA（主成分分析）：协方差矩阵被对角化以提取独立方差方向。

1.  计算机图形学：对角化简化了旋转缩放变换。

#### 为什么它很重要

对角化将复杂性转化为简单性。它揭示了矩阵的基本作用：沿着首选轴进行缩放。没有它，理解或计算重复变换将是难以处理的。

#### 试试你自己

1.  对角化

    $$ C = \begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}. $$

    使用 $P D⁵ P^{-1}$ 计算 $C⁵$。

1.  解释原因

    $$ \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} $$

    不能对角化。

1.  挑战：证明任何对称实矩阵都可以用正交基对角化。

对角化就像找到矩阵的自然“语言”：一旦我们听其本地的基，一切就变得清晰、优雅和简单。

### 65. 矩阵的幂

一旦我们了解了对角化，它最强大的后果之一就是能够高效地计算矩阵的幂。通常，反复乘以一个矩阵是昂贵且混乱的。但如果一个矩阵可以被对角化，它的幂就几乎变得容易计算。这在理解动力系统的长期行为、马尔可夫链和迭代算法中至关重要。

#### 一般原则

如果一个矩阵 $A$ 可对角化，那么

$$ A = P D P^{-1}, $$

其中 $D$ 是对角矩阵且 $P$ 是可逆的。

然后，对于任何正整数 $k$：

$$ A^k = (P D P^{-1})^k = P D^k P^{-1}. $$

因为 $P^{-1}P = I$，乘积中的中间项相互抵消。

+   计算 $D^k$ 很简单：只需将每个对角元素提高到 $k$ 次幂。

+   因此，特征值控制矩阵幂的增长或衰减。

#### 示例：一个简单的对角化情况

让

$$ D = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}. $$

然后

$$ D^k = \begin{bmatrix} 2^k & 0 \\ 0 & 3^k \end{bmatrix}. $$

每个特征值都独立地被提高到 $k$ 次幂。

#### 示例：使用对角化

考虑

$$ A = \begin{bmatrix} 4 & 0 \\ 1 & 3 \end{bmatrix}. $$

从之前，我们知道它对角化为

$$ A = P D P^{-1}, \quad D = \begin{bmatrix} 4 & 0 \\ 0 & 3 \end{bmatrix}. $$

所以，

$$ A^k = P \begin{bmatrix} 4^k & 0 \\ 0 & 3^k \end{bmatrix} P^{-1}. $$

我们不是将 $A$ 乘以自身 $k$ 次，而是对特征值进行指数运算。

#### 长期行为

特征值精确地揭示了 $k \to \infty$ 时发生的情况。

+   如果所有特征值满足 $|\lambda| < 1$，那么 $A^k \to 0$。

+   如果某些特征值满足 $|\lambda| > 1$，那么 $A^k$ 将沿着那些特征向量方向发散。

+   如果 $|\lambda| = 1$，行为取决于具体结构：它可能振荡、稳定或保持有界。

这解释了递归系统和迭代算法中的稳定性。

#### 特殊情况：马尔可夫链

在概率论中，马尔可夫链的转移矩阵具有小于或等于 1 的特征值。

+   最大的特征值总是 $\lambda = 1$。

+   随着转移矩阵的幂增长，链收敛到与 $\lambda = 1$ 相关的特征向量，表示稳态分布。

因此，$A^k$ 描述了链的长期行为。

#### 不可对角化矩阵

如果一个矩阵不可对角化，事情变得更为复杂。这样的矩阵需要约当标准形，其中块可以导致类似 $k \lambda^{k-1}$ 的项。

示例：

$$ B = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}. $$

然后

$$ B^k = \begin{bmatrix} 2^k & k 2^{k-1} \\ 0 & 2^k \end{bmatrix}. $$

非对角元素的存在引入了 $k$ 的线性增长，除了指数缩放。

#### 几何意义

+   $A$ 的幂次对应于线性变换的重复应用。

+   特征值决定了方向是扩展、收缩还是保持稳定。

+   特征向量标记了重复作用最简单的描述轴。

想象拉伸橡皮膜：每次拉伸后，橡皮膜与主导特征向量的对齐越来越强。

#### 应用

1.  动态系统：人口模型、经济增长和迭代算法都依赖于矩阵的幂次。

1.  马尔可夫链：幂次揭示了平衡行为和混合速率。

1.  微分方程：离散时间模型使用矩阵幂来描述状态演变。

1.  计算机图形学：重复变换可以通过特征值进行分析。

1.  机器学习：迭代求解器（如具有线性更新的梯度下降）的收敛性取决于谱半径。

#### 为什么这很重要

矩阵幂是稳定性分析、渐近行为和收敛性的基础。对角化将这种暴力乘法转化为深入的结构化理解。

#### 试试你自己

1.  计算 $\begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}$ 的 $A⁵$。

1.  对于 $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$，计算 $A^k$。当 $k \to \infty$ 时会发生什么？

1.  探索当最大特征值的绝对值 < 1、= 1 和 > 1 时，$A^k$ 发生了什么。

1.  挑战：证明如果一个可对角化的矩阵有 $|\lambda_i| < 1$ 的特征值，那么 $\lim_{k \to \infty} A^k = 0$。

矩阵的幂次揭示了重复的故事：变换在反复应用时如何演变。它们将线性代数与时间、增长和展开的每个系统的稳定性联系起来。

### 66. 实数与复数谱

并非所有特征值都是实数。即使在处理实矩阵时，特征值也可能以复数的形式出现。理解特征值何时是实数，何时是复数，以及这在几何上意味着什么，对于掌握线性变换的完整行为至关重要。

#### 复数上的特征值

每个方阵 $A \in \mathbb{R}^{n \times n}$ 至少有一个复数特征值。这是由代数基本定理保证的，该定理表明每个多项式（如特征多项式）在 $\mathbb{C}$ 中都有根。

+   如果 $p_A(\lambda)$ 只有实根，则所有特征值都是实数。

+   如果 $p_A(\lambda)$ 有没有实根的二次因子，则特征值以复共轭对的形式出现。

#### 为什么会出现复数

考虑一个二维旋转矩阵：

$$ R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{bmatrix}. $$

特征多项式是

$$ p(\lambda) = \lambda² - 2\cos\theta \lambda + 1. $$

特征值是

$$ \lambda = \cos\theta \pm i \sin\theta = e^{\pm i\theta}. $$

+   除非 $\theta = 0, \pi$，这些特征值不是实数。

+   从几何上看，这是有意义的：纯旋转没有不变实方向。相反，特征值是单位模长的复数，编码旋转角度。

#### 实数与复数场景

1.  对称实矩阵：

    +   所有特征值都是实数。

    +   特征向量构成一个正交基。

    +   示例：$\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}$ 的特征值为 $3, 1$。

1.  一般实矩阵：

    +   特征值可能是复数。

    +   如果是复数，它们总是成共轭对出现：如果 $\lambda = a+bi$，那么 $\overline{\lambda} = a-bi$ 也是一个特征值。

1.  非对称矩阵：

    +   纯虚数特征值。

    +   示例：$\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$ 的特征值为 $\pm i$。

#### 复数特征值的几何意义

+   如果特征值是实数，变换沿着实方向缩放。

+   如果特征值是复数，变换涉及旋转和缩放的组合。

对于 $\lambda = re^{i\theta}$:

+   $r = |\lambda|$ 控制扩张或收缩。

+   $\theta$ 控制旋转。

因此，一个复数特征值代表一个螺旋：在旋转的同时拉伸或收缩。

#### 示例：螺旋动力学

矩阵

$$ A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} $$

将向量旋转 90°。

+   特征值：$\pm i$。

+   模量 = 1，角度 = $\pi/2$。

+   解释：每一步都是 90° 的旋转，没有缩放。

如果我们改变到

$$ B = \begin{bmatrix} 0.8 & -0.6 \\ 0.6 & 0.8 \end{bmatrix}, $$

特征值是模数小于 1 的复数。

+   解释：旋转结合收缩 → 向原点螺旋。

#### 应用

1.  微分方程：复数特征值产生带有正弦和余弦项的振荡解。

1.  物理：振动和波现象依赖于复数特征值来模拟周期性行为。

1.  控制系统：稳定性需要检查复平面中特征值的幅度。

1.  计算机图形学：旋转和螺旋运动自然可以用复数谱来描述。

1.  信号处理：傅里叶变换依赖于卷积算子的复数特征结构。

#### 为什么这很重要

+   实数特征值描述纯拉伸或压缩。

+   复数特征值描述结合旋转和缩放。

+   一起，它们提供了在实数和复数空间中矩阵行为的完整图景。

+   如果不考虑复数特征值，我们将错过整个类别的变换，如旋转和振荡。

#### 试试看

1.  找出 $\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}$ 的特征值。从几何上解释。

1.  对于旋转 45°，找出 $\begin{bmatrix} \cos\frac{\pi}{4} & -\sin\frac{\pi}{4} \\ \sin\frac{\pi}{4} & \cos\frac{\pi}{4} \end{bmatrix}$ 的特征值。证明它们是 $e^{\pm i\pi/4}$。

1.  检查 $\begin{bmatrix} 2 & -5 \\ 1 & -2 \end{bmatrix}$ 的特征值。它们是实数还是复数？

1.  挑战：证明奇数次多项式总是至少有一个实根。将此与奇维实矩阵的特征值联系起来。

复数谱将线性代数的理解扩展到振荡、旋转和螺旋的完整丰富性，其中仅数字是不够的——几何和复分析融合以揭示真相。

### 67. 病态矩阵和约当形式（一瞥）

并非每个矩阵都可以完全简化为对角形式。一些矩阵，尽管有重复的特征值，但缺乏足够的独立特征向量来覆盖整个空间。这些被称为病态矩阵。理解它们需要引入约当标准形，这是对角化的推广，可以处理这些棘手的情况。

#### 病态矩阵

一个 $A \in \mathbb{R}^{n \times n}$ 的方阵被称为病态的，如果：

+   它有一个代数重数（AM）严格大于其几何重数（GM）的特征值 $\lambda$。

+   相当于，$A$ 没有足够的线性无关的特征向量来形成 $\mathbb{R}^n$ 的一个完整基。

示例：

$$ A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}. $$

+   特征多项式：$(\lambda - 2)²$，所以 AM = 2。

+   解 $(A - 2I)v = 0$：

    $$ \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}v = 0 \quad \Rightarrow \quad v = \begin{bmatrix} 1 \\ 0 \end{bmatrix}. $$

    只有一个独立特征向量 → GM = 1。

+   由于 GM < AM，这个矩阵是病态的。

病态矩阵不能对角化。

#### 为什么存在病态矩阵

对角化需要一个独立特征向量对应于每个特征值副本。但有时矩阵“合并”了这些方向，产生的特征向量比预期的要少。

+   想象一下，乐谱上（AM）写有多个音符，但可用的乐器（GM）较少。

+   矩阵“想要”更多的独立方向，但其零空间的几何形状阻止了这一点。

#### 约当标准形（直观）

虽然病态矩阵不能对角化，但它们仍然可以放入一个称为约当标准形（JCF）的近似对角形式：

$$ J = P^{-1} A P, $$

其中 $J$ 由约当块组成：

$$ J_k(\lambda) = \begin{bmatrix} \lambda & 1 & 0 & \cdots & 0 \\ 0 & \lambda & 1 & \cdots & 0 \\ 0 & 0 & \lambda & \cdots & 0 \\ \vdots & \vdots & \vdots & \ddots & 1 \\ 0 & 0 & 0 & \cdots & \lambda \end{bmatrix}. $$

每个块对应于一个特征值 $\lambda$，超对角线上的 1 表示缺乏独立特征向量。

+   如果每个块都是 $1 \times 1$，则矩阵可对角化。

+   如果出现较大的块，则矩阵是病态的。

#### 示例：大小为 2 的约当块

早期的病态示例

$$ A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} $$

有约当形式

$$ J = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}. $$

注意它已经处于约当形式：一个大小为 2 的块对应于特征值 2。

#### 约当块的幂

一个关键属性是幂的行为。对于

$$ J = \begin{bmatrix} \lambda & 1 \\ 0 & \lambda \end{bmatrix}, $$

$$ J^k = \begin{bmatrix} \lambda^k & k\lambda^{k-1} \\ 0 & \lambda^k \end{bmatrix}. $$

+   与对角矩阵不同，$k$ 中会出现额外的多项式项。

+   这解释了为什么病态矩阵会产生类似于 $k \lambda^{k-1}$ 的增长行为。

#### 几何意义

+   特征向量描述不变线。

+   当特征向量不足时，约当形编码了广义特征向量的链。

+   每个链捕捉了矩阵如何将稍微偏离不变线的向量进行变换，通过约当块将它们推向相互连接的方向。

因此，虽然可对角化矩阵将空间分解成整洁的独立方向，但病态矩阵将某些方向纠缠在一起，迫使它们进入链。

#### 应用

1.  微分方程：约当块决定了解中出现额外的多项式因子（如 $t e^{\lambda t}$）。

1.  马尔可夫链：非对角化转移矩阵会导致收敛到稳态的速度变慢。

1.  数值分析：如果系统矩阵是病态的，算法可能会失败或变慢。

1.  控制理论：稳定性不仅取决于特征值，还取决于矩阵是否可对角化。

1.  量子力学：简并特征值需要约当分析来完全描述状态。

#### 为什么这很重要

+   对角化并不总是可能的，病态矩阵是例外。

+   约当形是通用的后备方案：每个方阵都有一个，它泛化了对角化。

+   它引入了广义特征向量，扩展了谱理论的范围。

#### 尝试自己操作

1.  验证 $\begin{bmatrix} 3 & 1 \\ 0 & 3 \end{bmatrix}$ 是病态的。找到它的约当形。

1.  证明对于大小为 3 的约当块，

    $$ J^k = \lambda^k I + k \lambda^{k-1} N + \frac{k(k-1)}{2}\lambda^{k-2} N², $$

    其中 $N$ 是幂零部分（对角线以上的 1 组成的矩阵）。

1.  比较具有相同特征值的可对角化矩阵和病态矩阵的 $A^k$ 的行为。

1.  挑战：证明每个方阵在复数域上都有一个约当形。

病态矩阵和约当形表明，即使特征向量“不足”，我们仍然可以施加结构，捕捉线性变换在其最基本构建块中的行为。

### 68. 稳定性和谱半径

当一个矩阵通过迭代、递归或动力系统反复应用时，其长期行为不是由单个元素控制的，而是由其特征值控制的。这里的关键指标是谱半径，它告诉我们重复应用会导致收敛、振荡还是发散。

#### 谱半径

矩阵 $A$ 的谱半径定义为

$$ \rho(A) = \max \{ |\lambda| : \lambda \text{ 是 } A \text{ 的特征值} \}. $$

+   它是所有特征值中的最大绝对值。

+   如果 $|\lambda| > 1$，特征值会导致其特征向量上的指数增长。

+   如果 $|\lambda| < 1$，它会导致指数衰减。

+   如果 $|\lambda| = 1$，行为取决于特征值是简单的还是缺陷的。

#### 迭代系统中的稳定性

考虑一个递归过程：

$$ x_{k+1} = A x_k. $$

+   如果 $\rho(A) < 1$，那么 $A^k \to 0$ 当 $k \to \infty$。所有轨迹都会收敛到原点。

+   如果 $\rho(A) > 1$，那么 $A^k$ 沿着主导特征向量无界增长。

+   如果 $\rho(A) = 1$，轨迹既不会消失也不会发散，但可能会振荡或停滞。

#### 示例：具有小谱半径的收敛

$$ A = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.8 \end{bmatrix}. $$

+   特征值：$0.5, 0.8$。

+   $\rho(A) = 0.8 < 1$.

+   $A^k$ 的幂将向量缩小到零 → 稳定系统。

#### 示例：具有大谱半径的发散

$$ B = \begin{bmatrix} 2 & 0 \\ 0 & 0.5 \end{bmatrix}. $$

+   特征值：$2, 0.5$。

+   $\rho(B) = 2 > 1$.

+   $B^k$ 的幂沿着特征向量 $(1,0)$ 爆炸。

#### 示例：具有复特征值的振荡

$$ C = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}. $$

+   特征值：$\pm i$，两者模数均为 1。

+   $\rho(C) = 1$.

+   系统是中性稳定的：向量永远旋转而不缩小或增大。

#### 超越简单稳定性：缺陷情况

如果一个矩阵有 $|\lambda|=1$ 的特征值且是缺陷的，$A^k$ 中会出现额外的 $k$ 次多项式项，即使 $\rho(A)=1$，也会导致缓慢发散。

示例：

$$ D = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}. $$

+   特征值：$\lambda=1$（算术平均数=2，几何平均数=1）。

+   $\rho(D)=1$.

+   幂次与 $k$ 线性增长：

    $$ D^k = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}. $$

+   尽管谱半径等于 1，系统仍然是不稳定的。

#### 几何意义

谱半径衡量变换的主导模式：

+   想象拉伸和旋转一块橡皮片。经过多次重复，橡皮片会与对应于最大特征值的方向对齐。

+   如果拉伸小于 1，一切都会缩小。

+   如果大于 1，一切都会膨胀。

+   如果恰好为 1，系统处于稳定性的边缘。

#### 应用

1.  数值方法：迭代求解器的收敛性（例如，雅可比法、高斯-赛德尔法）取决于谱半径小于 1。

1.  马尔可夫链：如果最大特征值 = 1 且其他特征值模数小于 1，则存在长期分布。

1.  控制理论：通过单位圆内的特征值（$|\lambda| < 1$）判断系统稳定性。

1.  经济学：如果谱半径小于 1，投入产出模型才会保持有界。

1.  流行病学：基本再生数 $R_0$ 实际上是下一代矩阵的谱半径。

#### 为什么这很重要

+   谱半径将矩阵的整个谱压缩成一个单一的稳定性标准。

+   它预测迭代过程的命运，从经济增长到疾病传播。

+   它在线性系统中划清了衰减、平衡和爆炸之间的界限。

#### 试试看

1.  计算矩阵 $\begin{bmatrix} 0.6 & 0.3 \\ 0.1 & 0.8 \end{bmatrix}$ 的谱半径。系统会收敛吗？

1.  证明对于任何矩阵范数 $\|\cdot\|$，

    $$ \rho(A) \leq \|A\|. $$

    （提示：使用 Gel'fand 公式。）

1.  对于 $\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}$，解释为什么即使 $\rho=1$，它也会发散。

1.  挑战：证明 Gel'fand 公式：

    $$ \rho(A) = \lim_{k\to\infty} \|A^k\|^{1/k}. $$

特征半径是线性动力学的指南针：它指向稳定性、振荡或发散，引导我们在重复变换塑造未来的各个学科中前行。

### 69. 马尔可夫链与稳态

马尔可夫链是概率和统计学中应用特征值最直接和最美丽的方法之一。它们描述了逐步演变的系统，其中下一个状态只取决于当前状态，而不是过去。稳态的数学——这种链的长期行为——建立在转移矩阵的特征值和特征向量上。

#### 转移矩阵

马尔可夫链由一个 $P \in \mathbb{R}^{n \times n}$ 转移矩阵定义，具有以下性质：

1.  所有项都是非负的：$p_{ij} \geq 0$。

1.  每一行之和为 1：$\sum_j p_{ij} = 1$。

如果链在时间 $k$ 处于状态 $i$，那么 $p_{ij}$ 是在时间 $k+1$ 转移到状态 $j$ 的概率。

#### 状态的演变

如果在时间 $k$ 的概率分布是一个行向量 $\pi^{(k)}$，那么

$$ \pi^{(k+1)} = \pi^{(k)} P. $$

经过 $k$ 步后：

$$ \pi^{(k)} = \pi^{(0)} P^k. $$

因此，理解长期行为需要分析 $P^k$。

#### 转移矩阵的特征值结构

+   每个转移矩阵 $P$ 都有特征值 $\lambda = 1$。

+   所有其他特征值满足 $|\lambda| \leq 1$。

+   如果链是不可约的（所有状态都是互通的）并且是非周期的（没有循环锁定），那么：

    +   $\lambda=1$ 是一个简单特征值（AM=GM=1）。

    +   所有其他特征值的绝对值严格小于 1。

这确保了收敛到一个唯一的稳态。

#### 稳态作为特征向量

稳态分布 $\pi$ 满足：

$$ \pi = \pi P. $$

这相当于：

$$ \pi^T \text{ 是 } P^T \text{ 的右特征向量，特征值为 } 1. $$

+   稳态向量位于特征值 1 的特征空间中。

+   由于概率之和必须为 1，归一化给出了一个唯一的稳态。

#### 示例：二状态马尔可夫链

$$ P = \begin{bmatrix} 0.7 & 0.3 \\ 0.4 & 0.6 \end{bmatrix}. $$

+   特征值：解 $\det(P-\lambda I) = 0$。

    $$ \lambda_1 = 1, \quad \lambda_2 = 0.3. $$

+   通过 $\pi = \pi P$ 找到稳态：

    $$ \pi = \bigg(\frac{4}{7}, \frac{3}{7}\bigg). $$

+   当 $k \to \infty$ 时，任何初始分布 $\pi^{(0)}$ 都会收敛到这个稳态。

#### 示例：图上的随机游走

考虑一个简单的图：一条线上的 3 个节点，其中每个节点以相等的概率传递给邻居。

转移矩阵：

$$ P = \begin{bmatrix} 0 & 1 & 0 \\ 0.5 & 0 & 0.5 \\ 0 & 1 & 0 \end{bmatrix}. $$

+   特征值：$\{1, 0, -1\}$。

+   稳态对应于特征值 1。

+   经过许多步骤后，分布收敛到 $(0.25, 0.5, 0.25)$。

#### 几何意义

+   特征值 1：概率的固定“方向”，在转换下不改变。

+   特征值小于 1 的幅度：瞬态模式随 $k \to \infty$ 消失。

+   主特征向量（稳态）就像系统的“重心”。

因此 $P$ 的幂次过滤掉了除了特征值为 1 的特征向量之外的所有向量。

#### 应用

1.  Google PageRank：稳态特征向量对网页进行排序。

1.  经济学：投入产出模型像马尔可夫链一样演变。

1.  流行病学：疾病的传播可以建模为马尔可夫过程。

1.  机器学习：隐马尔可夫模型（HMMs）是语音识别和生物信息学的基石。

1.  排队论：客户到达和服务根据马尔可夫动力学演变。

#### 为什么这很重要

+   稳态的概念展示了随机性如何导致可预测性。

+   特征值解释了收敛发生的原因以及收敛的速度。

+   线性代数与概率之间的联系提供了特征向量最清晰的实际应用之一。

#### 尝试自己来做

1.  对于

    $$ P = \begin{bmatrix} 0.9 & 0.1 \\ 0.5 & 0.5 \end{bmatrix}, $$

    计算其特征值和稳态。

1.  证明对于任何转移矩阵，最大的特征值总是 1。

1.  证明如果一个链是不可约的且非周期的，那么稳态是唯一的。

1.  挑战：构建一个具有周期（周期性）的 3 状态转移矩阵，并说明为什么它不会收敛到稳态分布，直到受到扰动。

马尔可夫链和稳态是概率和线性代数的交汇点：随机性，当乘以多次时，被特征值 1 的平静持久性所驯服。

### 70. 线性微分系统

许多自然和工程过程随时间连续演变。当这些过程可以表示为线性关系时，它们会导致线性微分方程组。这类系统的分析几乎完全依赖于特征值和特征向量，它们决定了解的行为：它们是振荡、衰减、增长还是稳定。

#### 一般设置

考虑一个一阶线性微分方程组：

$$ \frac{d}{dt}x(t) = A x(t), $$

其中：

+   $x(t) \in \mathbb{R}^n$ 是时间 $t$ 的状态向量。

+   $A \in \mathbb{R}^{n \times n}$ 是一个常数系数矩阵。

任务是求解 $x(t)$，给定初始状态 $x(0)$。

#### 矩阵指数

正式解为：

$$ x(t) = e^{At} x(0), $$

其中 $e^{At}$ 是矩阵指数，定义为：

$$ e^{At} = I + At + \frac{(At)²}{2!} + \frac{(At)³}{3!} + \cdots. $$

但我们如何在实践中计算 $e^{At}$ 呢？答案来自对角化和约当形式。

#### 情况 1：可对角化矩阵

如果 $A$ 是可对角化的：

$$ A = P D P^{-1}, \quad D = \text{diag}(\lambda_1, \ldots, \lambda_n). $$

然后：

$$ e^{At} = P e^{Dt} P^{-1}, \quad e^{Dt} = \text{diag}(e^{\lambda_1 t}, \ldots, e^{\lambda_n t}). $$

因此解为：

$$ x(t) = P \begin{bmatrix} e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n t} \end{bmatrix} P^{-1} x(0). $$

每个特征值 $\lambda_i$ 决定了其特征向量方向上的时间行为。

#### 情况 2：非对角化矩阵

如果 $A$ 是不可对角化的，使用其约当形式 $J = P^{-1}AP$：

$$ e^{At} = P e^{Jt} P^{-1}. $$

对于大小为 2 的约当块：

$$ J = \begin{bmatrix} \lambda & 1 \\ 0 & \lambda \end{bmatrix}, \quad e^{Jt} = e^{\lambda t} \begin{bmatrix} 1 & t \\ 0 & 1 \end{bmatrix}. $$

$t$ 中的多项式项出现，乘以指数部分。这解释了为什么重复的特征值和不足的特征向量会产生带有额外多项式因子的解。

#### 实特征值与复特征值

+   实特征值：解沿着特征向量方向呈指数增长或衰减。

    +   如果 $\lambda < 0$：指数衰减 → 稳定性。

    +   如果 $\lambda > 0$：指数增长 → 不稳定。

+   复特征值：$\lambda = a \pm bi$。解涉及振荡：

    $$ e^{(a+bi)t} = e^{at}(\cos(bt) + i \sin(bt)). $$

    +   如果 $a < 0$：衰减振荡。

    +   如果 $a > 0$：增长振荡。

    +   如果 $a = 0$：纯振荡，中性稳定。

#### 示例 1：实特征值

$$ A = \begin{bmatrix} -2 & 0 \\ 0 & -3 \end{bmatrix}. $$

特征值：$-2, -3$。解：

$$ x(t) = \begin{bmatrix} c_1 e^{-2t} \\ c_2 e^{-3t} \end{bmatrix}. $$

两个项都衰减 → 在原点处的稳定平衡。

#### 示例 2：复特征值

$$ A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}. $$

特征值：$\pm i$。解：

$$ x(t) = c_1 \begin{bmatrix} \cos t \\ \sin t \end{bmatrix} + c_2 \begin{bmatrix} -\sin t \\ \cos t \end{bmatrix}. $$

纯振荡 → 以原点为中心的圆周运动。

#### 示例 3：混合稳定性

$$ A = \begin{bmatrix} 1 & 0 \\ 0 & -2 \end{bmatrix}. $$

特征值：$1, -2$。解：

$$ x(t) = \begin{bmatrix} c_1 e^t \\ c_2 e^{-2t} \end{bmatrix}. $$

一个方向增长，一个衰减 → 整体不稳定，因为一个方向上的发散占主导地位。

#### 几何意义

+   特征向量形成系统的“流动轴”。

+   特征值决定沿着这些轴的流动是螺旋、增长还是收缩。

+   系统的相图——平面上的轨迹——由这种相互作用决定。

例如：

+   负特征值 → 轨迹汇聚到原点。

+   正特征值 → 轨迹向外排斥。

+   复特征值 → 螺旋或圆。

#### 应用

1.  控制理论：系统稳定性分析需要将特征值放置在左半平面。

1.  物理：振动、量子振荡和衰减过程都遵循特征值规则。

1.  生物学：种群模型根据线性微分方程演化。

1.  经济学：市场的线性模型根据特征值收敛或发散。

1.  神经科学：神经元的放电动力学可以建模为线性常微分方程系统。

#### 为什么它很重要

+   线性微分系统将线性代数与实际动力学联系起来。

+   特征值不仅决定数字，还决定随时间的行为：增长、衰减、振荡或平衡。

+   它们为分析非线性系统提供了基础，这些系统通常通过围绕平衡点进行线性化来研究。

#### 尝试自己动手做

1.  解 $\frac{dx}{dt} = \begin{bmatrix} -1 & 2 \\ -2 & -1 \end{bmatrix}x$。解释解的含义。

1.  对于 $A = \begin{bmatrix} 0 & -2 \\ 2 & 0 \end{bmatrix}$，计算特征值并描述运动。

1.  验证当 $A$ 可对角化时，$e^{At} = P e^{Dt} P^{-1}$ 是否成立。

1.  挑战：证明如果 $A$ 的所有特征值的实部都是负的，那么对于任何初始条件，$\lim_{t \to \infty} x(t) = 0$。

线性微分系统展示了特征值如何控制模型中时间的流动。它们解释了为什么某些过程会消亡，某些过程会振荡，而某些过程会无界增长——为无数现实世界现象背后的数学骨架提供了解释。

#### 结束

```py
Spectra guide the flow,
growth and decay intertwining,
future sings through roots.
```

## 第八章. 正交性、最小二乘法和 QR

#### 开场

```py
Perpendiculars,
meeting without crossing paths,
balance in silence.
```

### 71. 点积之外的内积

点积是大多数学生遇到的第一个内积。在 $\mathbb{R}^n$ 中，它被定义为

$$ \langle x, y \rangle = x \cdot y = \sum_{i=1}^n x_i y_i, $$

它提供了一种测量长度、角度和正交性的方法。但点积只是更广泛概念的一个特例。内积推广了点积，将其几何直觉扩展到更抽象的向量空间。

#### 内积的定义

实向量空间 $V$ 上的内积是一个函数

$$ \langle \cdot, \cdot \rangle : V \times V \to \mathbb{R} $$

满足以下公理对所有 $x,y,z \in V$ 和标量 $\alpha \in \mathbb{R}$ 成立：

1.  正定性：$\langle x, x \rangle \geq 0$，且 $\langle x, x \rangle = 0 \iff x=0$。

1.  对称性：$\langle x, y \rangle = \langle y, x \rangle$。

1.  第一参数的线性性：$\langle \alpha x + y, z \rangle = \alpha \langle x, z \rangle + \langle y, z \rangle$。

在复向量空间中，对称条件变为共轭对称：$\langle x, y \rangle = \overline{\langle y, x \rangle}$。

#### 从内积得到的范数和角度

一旦定义了内积，我们立即得到：

+   范数（长度）：$\|x\| = \sqrt{\langle x, x \rangle}$。

+   距离：$d(x,y) = \|x-y\|$。

+   向量之间的角度：$\cos \theta = \frac{\langle x, y \rangle}{\|x\|\|y\|}$。

因此，内积将 $\mathbb{R}^n$ 的熟悉几何推广到更广泛的环境中。

#### 点积之外的例子

1.  加权内积（在 $\mathbb{R}^n$ 中）：

    $$ \langle x, y \rangle_W = x^T W y, $$

    其中 $W$ 是一个对称正定矩阵。

    +   在这里，长度和角度取决于 $W$ 中编码的权重。

    +   当某些维度比其他维度更重要时（例如，加权最小二乘法）很有用。

1.  函数空间（连续内积）：在 $V = C[a,b]$，$[a,b]$ 上连续函数的空间：

    $$ \langle f, g \rangle = \int_a^b f(t) g(t) \, dt. $$

    +   长度：$\|f\| = \sqrt{\int_a^b f(t)² dt}$。

    +   正交性：如果 $f$ 和 $g$ 的积分积分为零，则 $f$ 和 $g$ 是正交的。

    +   这个内积是傅里叶级数的基础。

1.  复内积（在 $\mathbb{C}^n$ 中）:

    $$ \langle x, y \rangle = \sum_{i=1}^n x_i \overline{y_i}. $$

    +   共轭确保了正定性。

    +   对量子力学至关重要，其中状态是复希尔伯特空间中的向量。

1.  多项式空间：对于 $[-1,1]$ 上的多项式：

    $$ \langle p, q \rangle = \int_{-1}¹ p(x) q(x) \, dx. $$

    +   导致正交多项式（勒让德，切比雪夫），在逼近理论中至关重要。

#### 几何解释

+   内积重塑几何。我们不再用欧几里得度量来测量长度和角度，而是用所选内积诱导的度量来测量。

+   不同的内积在相同的向量空间上创建不同的几何。

示例：加权内积将圆扭曲成椭圆，改变了哪些向量被视为“正交”。

#### 应用

1.  信号处理：信号之间的相关性是一个内积。正交性意味着两个信号携带独立的信息。

1.  傅里叶分析：傅里叶系数来自与正弦和余弦函数的内积。

1.  机器学习：核方法将内积推广到无限维空间。

1.  量子力学：概率是复内积的平方模。

1.  优化：加权最小二乘问题使用加权内积。

#### 为什么这很重要

+   内积将几何推广到新的上下文：加权空间、函数、多项式、量子态。

+   它们为在 $\mathbb{R}^n$ 以外的空间中定义正交性、投影和正交基提供了基础。

+   它们统一了纯数学、物理学、工程学和计算机科学中的思想。

#### 尝试自己动手做

1.  证明如果 $W$ 是正定的，则加权内积 $\langle x, y \rangle_W = x^T W y$ 满足内积公理。

1.  计算 $\langle f, g \rangle = \int_0^\pi \sin(t)\cos(t)\, dt$。$f=\sin$ 和 $g=\cos$ 是否正交？

1.  在 $\mathbb{C}²$ 中，验证 $\langle (1,i), (i,1) \rangle = 0$。这在几何上意味着什么？

1.  挑战：证明每个内积都诱导一个范数，并且不同的内积可以在同一空间上导致不同的几何。

点积只是开始。内积提供了将几何扩展到加权空间、连续函数和无限维度的语言，从而改变了我们在数学和科学中衡量相似性、距离和结构的方式。

### 72. 正交性和正交基

正交性是线性代数中最强大的思想之一。它将欧几里得空间中熟悉的垂直概念推广到具有内积的抽象向量空间。当正交性与归一化（使向量具有单位长度）结合时，我们获得正交基，这简化了计算，阐明了几何，并支撑了许多算法。

#### 正交性

如果两个向量 $x, y \in V$ 正交，则

$$ \langle x, y \rangle = 0. $$

+   在 $\mathbb{R}²$ 或 $\mathbb{R}³$ 中，这意味着向量是垂直的。

+   在函数空间中，这意味着它们乘积的积分为零。

+   在信号处理中，这意味着信号是独立且不重叠的。

正交性在点积几何下捕捉了“无重叠”或“独立性”的概念。

#### 正交向量的性质

1.  如果 $x \perp y$，那么 $\|x+y\|² = \|x\|² + \|y\|²$（毕达哥拉斯定理的推广）。

1.  正交性是对称的：如果 $x \perp y$，那么 $y \perp x$。

1.  任何一组相互正交的非零向量都是自动线性无关的。

这个最后性质是关键的：正交性保证了独立性。

#### 正交集

正交集是一组向量 $\{u_1, \dots, u_k\}$，使得

$$ \langle u_i, u_j \rangle = \begin{cases} 1 & \text{if } i=j, \\ 0 & \text{if } i \neq j. \end{cases} $$

+   每个向量都有单位长度。

+   不同的向量是相互正交的。

这种结构使得使用坐标进行计算尽可能简单。

#### 正交基

向量空间的一个基 $\{u_1, \dots, u_n\}$ 是正交的，如果它作为一个集合是正交的。

+   任何向量 $x \in V$ 都可以写成

    $$ x = \sum_{i=1}^n \langle x, u_i \rangle u_i. $$

+   系数仅仅是内积，无需解方程组。

这就是为什么正交基是最方便的：它们使表示和投影变得轻而易举。

#### 例子

1.  $\mathbb{R}^n$ 中的标准基：$\{e_1, e_2, \dots, e_n\}$，其中 $e_i$ 在第 $i$ 个坐标上为 1，其他地方为 0。

    +   在标准点积下正交。

1.  傅里叶基：在 $[0,2\pi]$ 上的函数 $\{\sin(nx), \cos(nx)\}$ 在内积 $\langle f,g\rangle = \int_0^{2\pi} f(x)g(x)dx$ 下是正交的。

    +   这个基将信号分解为纯频率。

1.  多项式基：勒让德多项式 $P_n(x)$ 在 $[-1,1]$ 上相对于 $\langle f,g\rangle = \int_{-1}¹ f(x)g(x)\,dx$ 是正交的。

#### 几何意义

正交性将空间分割成独立的“方向”。

+   正交基就像完美对齐的坐标轴。

+   任何向量可以唯一地分解为沿着这些轴的独立贡献之和。

+   距离和角度得到保留，使得几何形状变得透明。

#### 应用

1.  信号处理：将信号分解为正交频率分量。

1.  机器学习：主成分形成捕获方差方向的正交基。

1.  数值方法：正交基提高了数值稳定性。

1.  量子力学：如果状态代表互斥的结果，则状态是正交的。

1.  计算机图形学：旋转由具有正交列的正交矩阵表示。

#### 为什么它很重要

+   正交性提供独立性；正交性提供归一化。

+   一起使计算、分解和投影变得干净且高效。

+   它们是傅里叶分析、主成分分析和无数现代算法的基础。

#### 尝试自己动手做

1.  证明 $\{(1,0,0), (0,1,0), (0,0,1)\}$ 是 $\mathbb{R}³$ 的一个正交基。

1.  检查 $\{(1,1,0), (1,-1,0), (0,0,1)\}$ 在点积下是否正交归一。如果不是，则归一化它。

1.  计算向量 $x=(3,4)$ 在基 $\{(1,0), (0,1)\}$ 和旋转正交基 $\{(1/\sqrt{2}, 1/\sqrt{2}), (-1/\sqrt{2}, 1/\sqrt{2})\}$ 中的系数。

1.  挑战：证明在任何有限维内积空间中，总存在一个正交基（提示：格拉姆-施密特）。

正交性和正交基是线性代数的骨架：它们将混乱的问题转化为优雅的分解，为我们描述向量、信号和数据提供了最简洁的语言。

### 73. 格拉姆-施密特过程

格拉姆-施密特过程是将任何线性无关向量集转化为正交基的系统方法。这个过程是代数与几何之间最优雅的桥梁之一：它将任意向量转化为相互垂直的向量，同时保持张成。

#### 它解决的问题

给定一个内积空间中的一组线性无关向量 $\{v_1, v_2, \dots, v_n\}$：

+   它们张成某个子空间 $W$。

+   但它们不一定正交或归一化。

目标：为 $W$ 构造一个正交基 $\{u_1, u_2, \dots, u_n\}$。

#### 格拉姆-施密特算法

1.  从第一个向量开始：

    $$ u_1 = \frac{v_1}{\|v_1\|}. $$

1.  对于第二个向量，减去在 $u_1$ 上的投影：

    $$ w_2 = v_2 - \langle v_2, u_1 \rangle u_1, \quad u_2 = \frac{w_2}{\|w_2\|}. $$

1.  对于第三个向量，减去在 $u_1$ 和 $u_2$ 上的投影：

    $$ w_3 = v_3 - \langle v_3, u_1 \rangle u_1 - \langle v_3, u_2 \rangle u_2, \quad u_3 = \frac{w_3}{\|w_3\|}. $$

1.  递归地继续：

    $$ w_k = v_k - \sum_{j=1}^{k-1} \langle v_k, u_j \rangle u_j, \quad u_k = \frac{w_k}{\|w_k\|}. $$

在每一步，$w_k$ 都被制成与所有之前的 $u_j$ 正交，然后归一化以形成 $u_k$。

#### $\mathbb{R}²$ 中的例子

从 $v_1 = (1,1)$，$v_2 = (1,0)$ 开始。

1.  首先归一化第一个向量：

    $$ u_1 = \frac{(1,1)}{\sqrt{2}} = \left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right). $$

1.  减去 $v_2$ 在 $u_1$ 上的投影：

    $$ w_2 = (1,0) - \left(\tfrac{1}{\sqrt{2}}\cdot1 + \tfrac{1}{\sqrt{2}}\cdot0\right)\left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right). $$

    $$ = (1,0) - \tfrac{1}{\sqrt{2}}\left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right). $$

    $$ = (1,0) - (0.5,0.5) = (0.5,-0.5). $$

1.  归一化：

    $$ u_2 = \frac{(0.5,-0.5)}{\sqrt{0.5²+(-0.5)²}} = \frac{(0.5,-0.5)}{\sqrt{0.5}} = \left(\tfrac{1}{\sqrt{2}}, -\tfrac{1}{\sqrt{2}}\right). $$

最终正交基：

$$ u_1 = \left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right), \quad u_2 = \left(\tfrac{1}{\sqrt{2}}, -\tfrac{1}{\sqrt{2}}\right). $$

#### 几何直觉

+   每一步都消除了先前选择的方向上的“重叠”。

+   想象在原始向量的张成空间内建立新的正交坐标轴。

+   结果就像将原始集合旋转和缩放到一个完美的正交系统中。

#### 数值稳定性

+   经典的 Gram–Schmidt 在计算机计算中可能受到舍入误差的影响。

+   一种数值稳定的替代方法是改进的 Gram–Schmidt（MGS），它重新排序了投影步骤以减少正交性的损失。

+   在实践中，QR 分解算法通常实现 MGS 或 Householder 反射。

#### 应用

1.  QR 分解：Gram–Schmidt 提供了基础：$A = QR$，其中 $Q$ 是正交的，$R$ 是上三角的。

1.  数据压缩：从 Gram–Schmidt 得到的正交基导致有效的表示。

1.  信号处理：确保独立的频率或波分量。

1.  机器学习：用于特征正交化和降维。

1.  物理：可以使用 Gram–Schmidt 从任意状态构建量子力学中的正交状态。

#### 为什么这很重要

+   Gram–Schmidt 保证了任何独立集都可以重塑为正交基。

+   它是 QR 分解、最小二乘和数值偏微分方程求解器等计算方法的基础。

+   它使投影、坐标和正交性变得明确且易于管理。

#### 尝试自己操作

1.  将 $(1,0,1)$，$(1,1,0)$，$(0,1,1)$ 应用到 $\mathbb{R}³$ 中。验证正交性。

1.  证明正交基的生成空间等于原始向量的生成空间。

1.  使用 Gram–Schmidt 方法找到多项式 $\{1,x,x²\}$ 在 $[-1,1]$ 上的正交基，内积为 $\langle f,g\rangle = \int_{-1}¹ f(x)g(x)\,dx$。

1.  挑战：证明 Gram–Schmidt 方法总是适用于线性无关的集合，但如果集合是相关的，则方法会失败。

Gram–Schmidt 过程是正交性的算法核心：它将混乱和冗余重塑为干净、垂直的构建块，用于我们研究的空间。

### 74. 投影到子空间

投影是正交性的自然扩展：它们描述了如何以最自然的方式将向量“降”到子空间中，最小化距离。理解投影对于解决最小二乘问题、分解向量和以更简单、更低维度的结构解释数据至关重要。

#### 投影到向量

从最简单的情况开始：将向量 $x$ 投影到非零向量 $u$ 上。

1.  投影是 $x$ 在 $u$ 方向上的分量。

1.  公式：

    $$ \text{proj}_u(x) = \frac{\langle x, u \rangle}{\langle u, u \rangle} u. $$

    如果 $u$ 是归一化的（$\|u\|=1$），这可以简化为

    $$ \text{proj}_u(x) = \langle x, u \rangle u. $$

几何上，这是从 $x$ 到由 $u$ 张成的直线的垂足。

#### 投影到正交基

假设我们有一个子空间 $W$ 的正交基 $\{u_1, u_2, \dots, u_k\}$。那么 $x$ 投影到 $W$ 上是：

$$ \text{proj}_W(x) = \sum_{i=1}^k \langle x, u_i \rangle u_i. $$

这个公式非常强大：

+   每个系数$\langle x, u_i \rangle$捕捉了$x$与$u_i$对齐的程度。

+   总和重构了$x$在$W$内的最佳逼近。

#### 投影矩阵

当在坐标系中工作时，投影可以用矩阵表示。

+   如果$U$是一个具有正交列$\{u_1, \dots, u_k\}$的$n \times k$矩阵，那么

    $$ P = UU^T $$

    是$W$上的投影矩阵。

$P$的性质：

1.  幂等性：$P² = P$.

1.  对称性：$P^T = P$.

1.  最佳逼近：对于任何$x$，$\|x - Px\|$是最小的。

#### 投影和正交补

如果$W$是$V$的子空间，那么每个向量$x \in V$都可以唯一地分解为

$$ x = \text{proj}_W(x) + \text{proj}_{W^\perp}(x), $$

其中$W^\perp$是$W$的正交补。

这种分解是正交分解定理。它说：空间相对于子空间干净地分为“内部”和“外部”部分。

#### $\mathbb{R}²$中的例子

设$u = (2,1)$，并将$x = (3,4)$投影到$\{u\}$的张成空间上。

1.  计算内积：$\langle x,u\rangle = 3\cdot 2 + 4\cdot 1 = 10$.

1.  计算范数的平方：$\langle u,u\rangle = 2² + 1² = 5$.

1.  投影：

    $$ \text{proj}_u(x) = \frac{10}{5}(2,1) = 2(2,1) = (4,2). $$

1.  正交误差：

    $$ x - \text{proj}_u(x) = (3,4) - (4,2) = (-1,2). $$

注意：$(4,2)$位于通过$u$的直线上，而误差向量$(-1,2)$与$u$正交。

#### 应用

1.  最小二乘回归：回归线是数据在预测变量张成的子空间上的投影。

1.  维度降低：主成分分析（PCA）将数据投影到最高特征向量的子空间。

1.  计算机图形学：3D 对象被投影到 2D 屏幕上。

1.  数值方法：当不存在精确解时，投影可以近似求解方程。

1.  物理：通过力和速度的投影来计算功和能量。

#### 为什么这很重要

+   投影是逼近的本质：它们给出所选子空间内向量的“最佳可能”版本。

+   它们形式化了独立性：误差向量始终与子空间正交。

+   它们为统计学、机器学习和数值计算提供了几何直觉。

#### 试试看

1.  计算$x = (2,3,4)$在$u = (1,1,1)$上的投影。

1.  验证残差$x - \text{proj}_u(x)$与$u$正交。

1.  在$\mathbb{R}³$中，写出由$\{(1,0,0),(0,1,0)\}$张成的子空间的投影矩阵。

1.  挑战：证明投影矩阵是幂等的且对称的。

投影将向量空间分解成干净的部分：什么是“在”子空间“内部”的，什么是“外部”的。这个简单而深刻的思想贯穿于几何、数据分析物理学。

### 75. 正交分解定理

线性代数的一个基石是正交分解定理，它表明内积空间中的每个向量都可以唯一地分为两部分：一部分位于子空间内，另一部分位于其正交补中。这为我们提供了清晰的组织信息、分离影响和简化计算的方法。

#### 定理陈述

设 $V$ 为一个内积空间，$W$ 为 $V$ 的一个子空间。那么对于 $V$ 中的每个向量 $x$，存在唯一的向量 $w \in W$ 和 $z \in W^\perp$，使得

$$ x = w + z. $$

这里：

+   $w = \text{proj}_W(x)$，$x$ 在 $W$ 上的投影。

+   $z = x - \text{proj}_W(x)$，正交分量。

这种分解是唯一的：没有其他来自 $W$ 和 $W^\perp$ 的向量对加起来等于 $x$。

#### $\mathbb{R}²$ 中的例子

将 $W$ 取为由 $u = (1,2)$ 张成的直线。对于 $x = (4,1)$：

1.  投影：

    $$ \text{proj}_u(x) = \frac{\langle x,u \rangle}{\langle u,u \rangle} u. $$

    计算：$\langle x,u\rangle = 4\cdot 1 + 1\cdot 2 = 6$，且 $\langle u,u\rangle = 1²+2²=5$。所以

    $$ \text{proj}_u(x) = \frac{6}{5}(1,2) = \left(\tfrac{6}{5}, \tfrac{12}{5}\right). $$

1.  正交分量：

    $$ z = x - \text{proj}_u(x) = (4,1) - \left(\tfrac{6}{5}, \tfrac{12}{5}\right) = \left(\tfrac{14}{5}, -\tfrac{7}{5}\right). $$

1.  验证：$\langle u, z\rangle = 1\cdot \tfrac{14}{5} + 2\cdot (-\tfrac{7}{5}) = 0$。因此，$z \in W^\perp$。

所以我们有

$$ x = \underbrace{\left(\tfrac{6}{5}, \tfrac{12}{5}\right)}_{\in W} + \underbrace{\left(\tfrac{14}{5}, -\tfrac{7}{5}\right)}_{\in W^\perp}. $$

#### 几何意义

+   这种分解将 $x$ 分解为其“子空间内”的部分和其“子空间外”的部分。

+   $w$ 是 $W$ 中离 $x$ 最近的点。

+   $z$ 是剩余的“误差”，始终垂直于 $W$。

几何上，从 $x$ 到子空间的最近路径总是正交的。

#### 正交补

+   正交补 $W^\perp$ 包含了与 $W$ 中每个向量都正交的所有向量。

+   维度关系：

    $$ \dim(W) + \dim(W^\perp) = \dim(V). $$

+   一起，$W$ 和 $W^\perp$ 将空间 $V$ 划分。

#### 投影矩阵和分解

如果 $P$ 是投影矩阵到 $W$：

$$ x = Px + (I-P)x, $$

其中 $Px \in W$ 且 $(I-P)x \in W^\perp$.

这种公式在数值线性代数中经常被使用。

#### 应用

1.  最小二乘近似：最佳拟合解是投影；误差在于正交补中。

1.  傅里叶分析：任何信号都分解为沿正交基函数的分量之和加上残差。

1.  统计学：回归将数据分解为解释方差（在预测者的子空间中）和残差方差（正交）。

1.  工程：将力相对于表面的平行和垂直分量分开。

1.  计算机图形学：将运动分解为屏幕平面投影和深度（正交方向）。

#### 为什么这很重要

+   正交分解提供了清晰性：每个向量相对于所选子空间分解为“相关”和“无关”部分。

+   它为最小二乘法、回归和信号逼近提供了基础。

+   它确保了向量计算中的唯一性、稳定性和可解释性。

#### 尝试自己动手

1.  在 $\mathbb{R}³$ 中，将 $x = (1,2,3)$ 分解为在 $(1,0,0)$ 的张成及其正交补中的分量。

1.  证明如果 $W$ 由 $(1,1,0)$ 和 $(0,1,1)$ 张成，那么 $\mathbb{R}³$ 中的任意向量可以唯一地分解为 $W$ 和 $W^\perp$。

1.  写下 $W = \text{span}\{(1,0,0),(0,1,0)\}$ 在 $\mathbb{R}³$ 中的投影矩阵 $P$。验证 $I-P$ 投影到 $W^\perp$。

1.  挑战：使用投影矩阵和 $P² = P$ 的事实证明正交分解定理。

正交分解定理保证了每个向量在所选子空间中找到其最接近的近似，并有一个完全正交的余项——这是一种优雅的结构，使得在无数领域中的分析和计算成为可能。

### 76. 正交投影和最小二乘

线性代数中最深刻的联系之一是正交投影和最小二乘法。当方程没有精确解时，最小二乘法找到“最佳近似”解。其背后的理论完全是几何的：最佳解是向量在子空间上的投影。

#### 设置：过定系统

考虑一个方程组 $Ax = b$，其中

+   $A$ 是一个 $m \times n$ 矩阵，其中 $m > n$（方程多于未知数）。

+   $b \in \mathbb{R}^m$ 可能不在 $A$ 的列空间中。

这意味着：

+   可能没有精确解。

+   相反，我们希望 $x$ 使得 $Ax$ 尽可能接近 $b$。

#### 最小二乘问题

最小二乘解最小化误差：

$$ \min_x \|Ax - b\|². $$

这里：

+   $Ax$ 是 $b$ 在 $A$ 的列空间上的投影。

+   误差向量 $b - Ax$ 与列空间正交。

这正是将正交分解定理应用于 $b$。

#### 正则方程的推导

我们希望 $r = b - Ax$ 与 $A$ 的每一列正交：

$$ A^T (b - Ax) = 0. $$

重新排列：

$$ A^T A x = A^T b. $$

这个系统被称为正则方程。其解 $x$ 给出最小二乘近似。

#### 最小二乘中的投影矩阵

$b$ 在 $\text{Col}(A)$ 上的投影是

$$ \hat{b} = A(A^T A)^{-1} A^T b, $$

假设 $A^T A$ 是可逆的。

这里，

+   $P = A(A^T A)^{-1} A^T$ 是投影到 $A$ 的列空间的投影矩阵。

+   拟合向量是 $\hat{b} = Pb$。

+   剩余 $r = b - \hat{b}$ 位于 $\text{Col}(A)$ 的正交补中。

#### 示例

假设 $A = \begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}$，$b = \begin{bmatrix}2 \\ 2 \\ 4\end{bmatrix}$。

+   $A$ 的列空间：$(1,2,3)$ 的张成。

+   投影公式：

    $$ \hat{b} = \frac{\langle b, A \rangle}{\langle A, A \rangle} A. $$

+   计算：$\langle b,A\rangle = 2\cdot1+2\cdot2+4\cdot3 = 18$。 $\langle A,A\rangle = 1²+2²+3²=14$。

+   投影：

    $$ \hat{b} = \frac{18}{14}(1,2,3) = \left(\tfrac{9}{7}, \tfrac{18}{7}, \tfrac{27}{7}\right). $$

+   残差：

    $$ r = b - \hat{b} = \left(\tfrac{5}{7}, -\tfrac{4}{7}, \tfrac{1}{7}\right). $$

检查：$\langle r,A\rangle = 0$，因此它是正交的。

#### 几何意义

+   最小二乘解是 $\text{Col}(A)$ 中离 $b$ 最近的点。

+   误差向量与子空间正交。

+   这就像从 $b$ 到子空间 $\text{Col}(A)$ 投下的垂线。

#### 应用

1.  统计学：线性回归使用最小二乘法拟合数据模型。

1.  工程：曲线拟合、系统识别和校准。

1.  计算机图形学：最佳拟合变换（例如，Procrustes 分析）。

1.  机器学习：线性模型的优化（在转向非线性方法之前）。

1.  数值方法：求解不一致的方程组。

#### 为什么它很重要

+   正交投影解释了为什么最小二乘法给出了最佳近似。

+   它们揭示了回归背后的几何：数据被投影到模型空间。

+   它们将线性代数与统计学、优化和应用科学联系起来。

#### 尝试自己动手

1.  求解 $\min_x \|Ax-b\|$ 对于 $A = \begin{bmatrix}1 & 1 \\ 1 & 2 \\ 1 & 3\end{bmatrix}$， $b=(1,2,2)^T$。解释结果。

1.  推导这个系统的投影矩阵 $P$。

1.  证明残差与 $A$ 的每一列正交。

1.  挑战：证明在所有可能的近似 $Ax$ 中，如果且仅当 $A^T A$ 可逆时，最小二乘解是唯一的。

正交投影将过定方程的混乱、不可解的世界转化为最佳可能的近似。最小二乘法不仅仅是一个代数技巧——它是高维空间中“接近”的几何本质。

### 77. QR 分解

QR 分解是将矩阵分解为正交部分和三角部分的过程。它直接源于正交性和 Gram-Schmidt 过程，并在数值线性代数中扮演着核心角色，提供了一种稳定且有效的方法来解决系统、计算最小二乘解和分析矩阵。

#### 定义

对于一个具有线性无关列的实 $m \times n$ 矩阵 $A$：

$$ A = QR, $$

其中：

+   $Q$ 是一个具有正交列的 $m \times n$ 矩阵 ($Q^T Q = I$)。

+   $R$ 是一个 $n \times n$ 的上三角矩阵。

如果我们要求 $R$ 具有正的对角元素，这种分解是唯一的。

#### 与 Gram-Schmidt 的联系

将 Gram-Schmidt 过程应用于 $A$ 的列产生 $Q$ 的正交列。在正交化步骤中使用的系数自然形成 $R$ 的条目。

+   $A$ 的每一列都可以表示为 $Q$ 的正交列的组合。

+   这个表达式的系数填充了三角形矩阵 $R$。

#### 示例

让

$$ A = \begin{bmatrix} 1 & 1 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}. $$

1.  对列应用 Gram–Schmidt：

    +   $v_1 = (1,1,0)^T$，归一化：

        $$ u_1 = \frac{1}{\sqrt{2}}(1,1,0)^T. $$

    +   从 $v_2=(1,0,1)^T$ 中减去投影：

        $$ w_2 = v_2 - \langle v_2,u_1\rangle u_1. $$

        计算 $\langle v_2,u_1\rangle = \tfrac{1}{\sqrt{2}}(1+0+0)=\tfrac{1}{\sqrt{2}}$. 因此

        $$ w_2 = (1,0,1)^T - \tfrac{1}{\sqrt{2}}(1,1,0)^T = \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)^T. $$

        归一化：

        $$ u_2 = \frac{1}{\sqrt{1.5}} \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)^T. $$

1.  构造 $Q = [u_1, u_2]$。

1.  计算 $R = Q^T A$。

结果是 $A = QR$，其中 $Q$ 是正交的，$R$ 是三角形的。

#### 几何意义

+   $Q$ 表示一个正交的基变换——旋转和反射保持长度和角度。

+   $R$ 在新的正交坐标系中编码缩放和剪切。

+   一起，它们展示了 $A$ 如何变换空间：首先旋转到一个干净的基础，然后应用三角变形。

#### 应用

1.  最小二乘法：我们不是求解 $A^T A x = A^T b$，而是使用 $QR$：

    $$ Ax = b \quad \Rightarrow \quad QRx = b. $$

    乘以 $Q^T$：

    $$ Rx = Q^T b. $$

    由于 $R$ 是三角形的，求解 $x$ 是高效且数值稳定的。

1.  特征值算法：QR 算法迭代应用 QR 分解来逼近特征值。

1.  数值稳定性：与求解正则方程相比，正交变换最小化了数值误差。

1.  机器学习：许多算法（例如，线性回归、PCA）使用 QR 分解以提高效率和稳定性。

1.  计算机图形学：正交因子保持形状；三角因子简化变换。

#### 为什么这很重要

+   QR 分解将理论（Gram–Schmidt 正交化）与计算（矩阵分解）联系起来。

+   它避免了正则方程的陷阱，提高了数值稳定性。

+   它支撑着统计学、工程学和计算机科学中的算法。

#### 尝试自己操作

1.  计算以下矩阵的 QR 分解

    $$ A = \begin{bmatrix}1 & 2 \\ 2 & 3 \\ 4 & 5\end{bmatrix}. $$

1.  验证 $Q^T Q = I$ 且 $R$ 是上三角的。

1.  使用 QR 求解最小二乘问题 $Ax \approx b$，其中 $b=(1,1,1)^T$。

1.  挑战：证明如果 $A$ 是平方正交矩阵，那么 $R=I$ 且 $Q=A$。

QR 分解将求解最小二乘法的繁琐过程转化为一个干净、几何的过程——在求解之前旋转到更好的坐标系中。它是线性代数工具箱中最强大的工具之一。

### 78. 正交矩阵

正交矩阵是行和列都形成正交集的方阵。它们是几何中刚体运动的代数对应物：保持长度、角度和方向（除了反射）的变换。

#### 定义

如果一个方阵 $Q \in \mathbb{R}^{n \times n}$ 是正交的，那么

$$ Q^T Q = QQ^T = I. $$

这意味着：

+   $Q$ 的列也是正交归一的。

+   $Q$ 的行也是正交归一的。

#### 属性

1.  逆等于转置：

    $$ Q^{-1} = Q^T. $$

    这使得正交矩阵特别容易求逆。

1.  规范保持：对于任何向量 $x$，

    $$ \|Qx\| = \|x\|. $$

    正交变换永远不会拉伸或收缩向量。

1.  内积保持：

    $$ \langle Qx, Qy \rangle = \langle x, y \rangle. $$

    角度被保留。

1.  行列式：$\det(Q) = \pm 1$.

    +   如果 $\det(Q) = 1$，则 $Q$ 是一个旋转。

    +   如果 $\det(Q) = -1$，则 $Q$ 是旋转和反射的组合。

#### 例子

1.  2D 旋转矩阵：

    $$ Q = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{bmatrix}. $$

    通过角度 $\theta$ 旋转向量。

1.  2D 反射矩阵：沿 $x$ 轴的反射：

    $$ Q = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}. $$

1.  置换矩阵：交换坐标是正交的，因为它保持了长度。例如在 3D 中：

    $$ Q = \begin{bmatrix}0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1\end{bmatrix}. $$

#### 几何意义

正交矩阵代表等距变换：保持物体形状的变换。

+   他们可以旋转、反射或置换轴。

+   他们从不扭曲长度或角度。

这就是为什么在计算机图形学中，正交矩阵模拟纯旋转和反射而不进行缩放。

#### 应用

1.  计算机图形学：3D 模型的旋转使用正交矩阵以避免扭曲。

1.  数值线性代数：正交变换在数值上是稳定的，广泛应用于 QR 分解和特征值算法。

1.  数据压缩：正交变换，如傅里叶变换和余弦变换，可以保持能量。

1.  信号处理：正交滤波器将信号分离成独立成分。

1.  物理：正交矩阵描述了刚体动力学中的旋转。

#### 为什么这很重要

+   正交矩阵是稳定算法的构建块。

+   他们描述了物理和计算系统中的对称性、结构和不变性。

+   它们是保持几何精确性的最简单和最强大的变换类。

#### 试试看

1.  验证以下

    $$ Q = \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix} $$

    是正交的。它代表了什么几何变换？

1.  证明正交矩阵的行列式必须是 $\pm 1$。

1.  证明两个正交矩阵相乘得到另一个正交矩阵。

1.  挑战：证明正交矩阵的特征值位于复单位圆上（即 $|\lambda|=1$）。

正交矩阵捕捉了对称性的本质：精确保持结构的变换。它们位于几何、物理和计算的内核。

### 79. 傅里叶视角

傅里叶视角是线性代数中最深刻的联系之一：复杂信号、数据或函数可以被分解为更简单正交波的叠加。我们不是以原始形式（时间、空间或坐标）描述信息，而是用频率来表示它。这种视角重塑了我们在数学、物理和工程中分析、压缩和理解信息的方式。

#### 傅里叶级数：基本思想

假设我们有一个在 $[-\pi, \pi]$ 上定义的周期函数 $f(x)$。傅里叶级数将 $f(x)$ 表示为：

$$ f(x) = a_0 + \sum_{n=1}^\infty \left( a_n \cos(nx) + b_n \sin(nx) \right). $$

+   系数 $a_n, b_n$ 是通过与正弦和余弦函数的内积找到的。

+   每个正弦和余弦函数在内积下与其他函数正交

    $$ \langle f, g \rangle = \int_{-\pi}^\pi f(x) g(x) \, dx. $$

因此，傅里叶级数不过是将一个函数展开在三角函数的正交基上。

#### 傅里叶变换：从时间到频率

对于非周期信号，傅里叶变换推广了这个展开。对于一个函数 $f(t)$，

$$ \hat{f}(\omega) = \int_{-\infty}^\infty f(t) e^{-i \omega t} dt $$

将其转换到频域。逆变换从其频率重建 $f(t)$。

这又是一个内积观点：指数函数 $e^{i \omega t}$ 在 $\mathbb{R}$ 上作为正交基函数。

#### 波的正交性

三角函数 $\{\cos(nx), \sin(nx)\}$ 和复指数 $\{e^{i\omega t}\}$ 形成正交族。

+   两个不同的正弦波在整个周期内内积为零。

+   同样，不同频率的指数是正交的。

这就像 $\mathbb{R}^n$ 中的正交向量，只是这里的空间是无限维的。

#### 离散傅里叶变换 (DFT)

在计算设置中，我们不处理无限积分，而是处理有限数据。DFT 将 $n$ 维向量 $x = (x_0, \dots, x_{n-1})$ 表示为正交复指数的线性组合：

$$ X_k = \sum_{j=0}^{n-1} x_j e^{-2\pi i jk / n}, \quad k=0,\dots,n-1. $$

这只是一个基变换：从标准基（时域）到傅里叶基（频域）。

快速傅里叶变换 (FFT) 以 $O(n \log n)$ 的时间复杂度计算，使得傅里叶分析在规模上变得实用。

#### 几何意义

+   在时域中，数据表示为原始值的序列。

+   在频域中，数据表示为正交波幅。

+   傅里叶观点只是旋转到一个新的正交坐标系，就像对矩阵进行对角化或改变基一样。

#### 应用

1.  信号处理：过滤掉不需要的噪声相当于去除高频分量。

1.  图像压缩：JPEG 使用类似傅里叶的变换（余弦变换）来压缩图像。

1.  数据分析：识别时间序列中的周期性和周期模式。

1.  物理：量子态在位置和动量基中表示，通过傅里叶变换相互联系。

1.  偏微分方程：通过移动到频域简化了解，其中导数变为乘数。

#### 为什么它很重要

+   傅里叶方法将困难的问题转化为简单的问题：卷积变为乘法，微分变为缩放。

+   它们为分析周期性、振荡和波动现象提供了一个通用语言。

+   它们本质上都是线性代数：特殊基的正交展开。

#### 试试看

1.  计算 $f(x) = x$ 在 $[-\pi,\pi]$ 上的傅里叶级数系数。

1.  对于序列 $(1,0,0,0)$，计算 4 点 DFT 并解释结果。

1.  证明 $\int_{-\pi}^\pi \sin(mx)\cos(nx) dx = 0$。

1.  挑战：证明傅里叶基 $\{e^{i2\pi k t}\}_{k=0}^{n-1}$ 在 $\mathbb{C}^n$ 中是正交归一的。

傅里叶视角揭示，无论信号多么复杂，都可以看作是简单正交波的组合。它是几何、代数和分析的完美结合，也是现代数学中最重要的思想之一。

### 80. 多项式和多重特征最小二乘

当扩展到拟合多项式或同时处理多个特征时，最小二乘问题变得特别强大。我们不仅可以拟合数据点上的单一直线，还可以拟合更高次度的曲线或更高维度的表面。这些推广是回归、数据分析和科学建模的核心。

#### 从线性到多项式

最简单的最小二乘模型是一条直线：

$$ y \approx \beta_0 + \beta_1 x. $$

但许多关系是非线性的。多项式最小二乘将模型推广为：

$$ y \approx \beta_0 + \beta_1 x + \beta_2 x² + \dots + \beta_d x^d. $$

在这里，每个 $x$ 的幂被视为一个新的特征。问题简化为设计矩阵上的普通最小二乘：

$$ A = \begin{bmatrix} 1 & x_1 & x_1² & \dots & x_1^d \\ 1 & x_2 & x_2² & \dots & x_2^d \\ \vdots & \vdots & \vdots & & \vdots \\ 1 & x_n & x_n² & \dots & x_n^d \end{bmatrix}, \quad \beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_d\end{bmatrix}. $$

最小二乘解最小化 $\|A\beta - y\|$。

#### 多重特征

当数据涉及多个预测因子时，我们将模型扩展为：

$$ y \approx \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p. $$

以矩阵形式：

$$ y \approx A\beta, $$

其中 $A$ 是设计矩阵，其列对应于特征（包括一个截距的列）。

最小二乘解仍然由正规方程给出：

$$ \hat{\beta} = (A^T A)^{-1} A^T y, $$

或者通过 QR 或 SVD 分解得到更稳定的结果。

#### 示例：多项式拟合

假设我们有一些数据点 $(1,1), (2,2.2), (3,2.9), (4,4.1)$。拟合二次模型 $y \approx \beta_0 + \beta_1 x + \beta_2 x²$：

1.  构建设计矩阵：

    $$ A = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 4 \\ 1 & 3 & 9 \\ 1 & 4 & 16 \end{bmatrix}. $$

1.  求解最小二乘问题 $\min \|A\beta - y\|$。

1.  结果给出了系数 $\beta_0, \beta_1, \beta_2$，它们最好地近似曲线。

同样的过程适用于更高次的多项式或多个特征。

#### 几何意义

+   在多项式最小二乘中，特征空间扩展：数据不再位于一条线上的点，而是在更高维的特征空间 $(1, x, x², \dots, x^d)$ 中。

+   在多特征最小二乘法中，$A$ 的列空间涵盖了所有可能的特征线性组合。

+   最小二乘解将观察到的输出向量 $y$ 投影到这个子空间。

因此，无论是多项式还是多特征，几何结构是相同的：投影到模型空间。

#### 实际挑战

1.  过拟合：高阶多项式拟合噪声，而不仅仅是信号。

1.  多重共线性：特征可能相关，使得 $A^T A$ 几乎奇异。

1.  规模化：具有不同量级的特征应该进行归一化。

1.  正则化：添加惩罚项（岭回归或 LASSO）可以稳定解。

#### 应用

1.  统计学中的回归：将线性回归扩展到处理多个预测因子或多项式项。

1.  机器学习：特征工程的基础扩展（在神经网络之前，这是标准）。

1.  工程：用于校准、建模和预测的曲线拟合。

1.  经济学：具有许多变量的预测模型（通货膨胀、利率、支出）。

1.  物理学和化学：使用多项式回归来模拟实验数据。

#### 为什么它很重要

+   多项式最小二乘法捕捉数据中的曲率。

+   多特征最小二乘法允许多个预测因子解释结果。

+   这两种推广都将线性代数转化为科学和社会中的实用建模工具。

#### 尝试自己操作

1.  通过点 $(0,1), (1,2), (2,5), (3,10)$ 拟合二次曲线。与直线拟合进行比较。

1.  构建一个多特征设计矩阵，用于根据学习时间、睡眠和先前成绩预测考试成绩。

1.  证明多项式回归只是变换特征上的线性回归。

1.  挑战：推导多项式最小二乘法中的偏差-方差权衡——为什么高阶数会增加方差。

多项式和多特征最小二乘法将线性代数的范围从直线扩展到复杂模式，为我们提供了一个通用的数据关系建模框架。

#### 结束语

```py
Closest lines are drawn,
errors fall away to rest,
angles guard the truth.
```

## 第九章. SVD、PCA 和条件数

### 开场

```py
Closest lines are drawn,
errors fall away to rest,
angles guard the truth.
```

### 81. 奇异值和 SVD

单值分解（SVD）是线性代数中最强大的工具之一。它推广了特征分解，适用于所有矩形矩阵（而不仅仅是方阵），并为几何、计算和数据分析提供了深刻的见解。在其核心，SVD 告诉我们每个矩阵都可以分解为三个部分：旋转/反射、缩放和再次旋转/反射。

#### SVD 的定义

对于任何实 $m \times n$ 矩阵 $A$，其 SVD 为：

$$ A = U \Sigma V^T, $$

其中：

+   $U$ 是一个 $m \times m$ 的正交矩阵（列 = 左奇异向量）。

+   $\Sigma$ 是一个 $m \times n$ 的对角矩阵，具有非负条目 $\sigma_1 \geq \sigma_2 \geq \dots \geq 0$（奇异值）。

+   $V$ 是一个 $n \times n$ 的正交矩阵（列 = 右奇异向量）。

即使 $A$ 是矩形的或不可对角化的，这种分解总是存在的。

#### 几何意义

SVD 描述了 $A$ 如何变换空间：

1.  第一次旋转/反射：乘以$V^T$以将坐标旋转或反射到正确的奇异向量基。

1.  缩放：乘以$\Sigma$，通过奇异值拉伸/收缩每个轴。

1.  第二次旋转/反射：乘以$U$以重新定位到输出空间。

因此，$A$的作用是先旋转，然后缩放，最后再旋转。

#### 奇异值

+   奇异值$\sigma_i$是$A^T A$的特征值的平方根。

+   它们衡量$A$在特定方向上拉伸空间的程度。

+   最大的奇异值$\sigma_1$是$A$的算子范数：最大拉伸因子。

+   如果某些奇异值为零，它们对应于由$A$压缩的方向。

#### $\mathbb{R}²$中的例子

让

$$ A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}. $$

1.  计算$A^T A = \begin{bmatrix} 9 & 3 \\ 3 & 5 \end{bmatrix}$。

1.  找到它的特征值：$\lambda_1, \lambda_2 = 10 \pm \sqrt{10}$。

1.  奇异值：$\sigma_i = \sqrt{\lambda_i}$。

1.  对应的特征向量形成右奇异向量$V$。

1.  左奇异向量$U$通过$U = AV/\Sigma$获得。

这种分解揭示了$A$如何将圆重新塑造成椭圆。

#### 链接到特征值分解

+   特征值分解仅适用于方阵和可对角化的矩阵。

+   SVD 适用于所有矩阵，无论是方阵还是矩形阵，可对角化或不可对角化。

+   我们得到的是奇异值（总是实数且非负），而不是特征值（可能是复数或负数）。

+   特征向量可能在完整基中不存在；奇异向量总是形成正交归一基。

#### 应用

1.  数据压缩：截断小的奇异值以近似具有较少维度的矩阵（用于 JPEG）。

1.  主成分分析（PCA）：对中心化数据应用奇异值分解（SVD）可以找到主成分，即最大方差的方向。

1.  最小二乘问题：SVD 提供稳定的解，即使对于病态或奇异系统也是如此。

1.  噪声滤波：丢弃小的奇异值以去除信号和图像中的噪声。

1.  数值稳定性：SVD 有助于诊断条件数——解对输入误差的敏感性。

#### 为什么它很重要

+   SVD 是线性代数的“瑞士军刀”：多功能、始终适用且解释丰富。

+   它提供了几何、代数和计算上的清晰性。

+   它对于机器学习、统计学、工程和物理学中的现代应用是必不可少的。

#### 试试看

1.  计算以下矩阵的 SVD

    $$ A = \begin{bmatrix}1 & 0 \\ 0 & 2 \\ 0 & 0\end{bmatrix}. $$

    解释缩放和旋转。

1.  证明对于任何向量$x$，$\|Ax\| \leq \sigma_1 \|x\|$。

1.  使用 SVD 来近似矩阵

    $$ \begin{bmatrix}1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1\end{bmatrix} $$

    级数为 1。

1.  挑战：证明$A$的 Frobenius 范数是它的奇异值平方和的平方根。

奇异值分解是通用的：每个矩阵都可以分解为旋转和缩放，揭示其结构，并在数学和应用科学中启用强大的技术。

### 82. SVD 的几何学

奇异值分解（SVD）不仅仅是一个代数分解——它具有精确的几何意义。它精确地解释了任何线性变换如何重塑空间：拉伸、旋转、压缩，以及可能折叠维度。理解这种几何学将 SVD 从一个正式的工具转变为矩阵如何工作的直观图景。

#### 单位球面的变换

取输入空间中的单位球面（或在 2D 中的圆）。当我们应用矩阵 $A$：

+   球面被变换成椭球。

+   这个椭球的轴对应于右奇异向量 $v_i$。

+   轴的长度是奇异值 $\sigma_i$。

+   输出空间中轴的方向是左奇异向量 $u_i$。

因此，SVD 告诉我们：

$$ A v_i = \sigma_i u_i. $$

每个矩阵都将正交基方向映射到由奇异值缩放的正交椭球轴。

#### 步步几何

分解 $A = U \Sigma V^T$ 可以从几何上理解：

1.  通过 $V^T$ 旋转/反射：将输入坐标与 $A$ 的“主方向”对齐。

1.  通过 $\Sigma$ 缩放：根据其奇异值拉伸或压缩每个轴。某些奇异值可能为零，使维度扁平化。

1.  通过 $U$ 旋转/反射：将缩放后的轴重新定向到输出空间。

这个过程是通用的：无论矩阵看起来多么不规则，它总是通过旋转 → 缩放 → 旋转来重塑空间。

#### 2D 示例

取

$$ A = \begin{bmatrix}3 & 1 \\ 0 & 2\end{bmatrix}. $$

+   在 $\mathbb{R}²$ 中的圆被映射成椭圆。

+   椭圆的主轴和副轴与 $A$ 的右奇异向量对齐。

+   它们的长度等于奇异值。

+   椭圆本身随后根据左奇异向量在输出平面中定向。

这使得 SVD 成为可视化 $A$ “扭曲”几何的完美工具。

#### 拉伸和秩

+   如果所有奇异值都是正的，椭球具有完整维度（没有折叠）。

+   如果某些奇异值为零，$A$ 沿某些方向将球面压扁，降低秩。

+   $A$ 的秩等于非零奇异值的数量。

因此，秩不足的矩阵实际上将空间压扁到较低维度。

#### 距离和能量保持

+   最大的奇异值 $\sigma_1$ 是 $A$ 可以拉伸向量的程度。

+   最小的非零奇异值 $\sigma_r$（其中 $r = \text{rank}(A)$）衡量矩阵压缩的程度。

+   条件数 $\kappa(A) = \sigma_1 / \sigma_r$ 衡量扭曲：小值意味着接近球形的拉伸，大值意味着极端的拉伸。

#### 几何应用

1.  数据压缩：仅保留最大的奇异值可以保留变化的“主轴”。

1.  PCA：数据沿着最大方差的正交轴进行分析（奇异向量）。

1.  数值分析：SVD 的几何性质说明了为什么病态系统放大错误——因为某些方向几乎被压平。

1.  信号处理：椭圆畸变对应于滤除某些频率分量。

1.  机器学习：降维本质上是在最大的奇异方向上投影数据。

#### 为什么这很重要

+   SVD 将代数方程转化为几何图像。

+   它揭示了矩阵如何扭曲空间，提供了抽象操作背后的直觉。

+   通过解释椭圆、奇异值和正交向量，我们为数据、物理和计算中的问题获得了视觉清晰度。

#### 尝试自己操作

1.  在 $\mathbb{R}²$ 中绘制单位圆，应用该矩阵

    $$ A = \begin{bmatrix}2 & 0 \\ 1 & 3\end{bmatrix}, $$

    并绘制结果椭圆。识别其轴和长度。

1.  通过数值验证 $Av_i = \sigma_i u_i$ 对于计算出的奇异向量和奇异值。

1.  对于秩为 1 的矩阵，绘制单位圆如何塌缩成线段。

1.  挑战：证明在 $A$ 下拉伸最大的向量集合恰好是第一组右奇异向量。

SVD 的几何性质为我们提供了一个通用的透镜：每个线性变换都是空间的可控畸变，由正交旋转和方向缩放构建。

### 83. 与特征分解的关系

单值分解（SVD）通常被介绍为全新的概念，但实际上它与特征分解紧密相连。事实上，奇异值和奇异向量是从由 $A$ 构造的某些对称矩阵的特征分解中出现的。理解这种联系可以说明为什么 SVD 总是存在，为什么奇异值是非负的，以及它是如何将特征分析推广到所有矩阵，甚至是矩形矩阵的。

#### 特征分解回顾

对于一个方阵 $M \in \mathbb{R}^{n \times n}$，特征分解是：

$$ M = X \Lambda X^{-1}, $$

其中 $\Lambda$ 是特征值的对角矩阵，$X$ 的列是特征向量。

然而：

+   并非所有矩阵都是可对角化的。

+   特征值可能是复数。

+   矩阵没有特征值。

这就是 SVD 提供通用框架的地方。

#### 从 $A^T A$ 到奇异值

对于任何 $m \times n$ 矩阵 $A$：

1.  考虑对称、正半定矩阵 $A^T A \in \mathbb{R}^{n \times n}$。

    +   对称性确保所有特征值都是实数。

    +   正半定确保它们是非负的。

1.  $A^T A$ 的特征值是 $A$ 的奇异值的平方：

    $$ \lambda_i(A^T A) = \sigma_i². $$

1.  $A^T A$ 的特征向量是右奇异向量 $v_i$。

1.  同样，对于 $AA^T$，特征值是相同的 $\sigma_i²$，而特征向量是左奇异向量 $u_i$。

因此：

$$ Av_i = \sigma_i u_i, \quad A^T u_i = \sigma_i v_i. $$

这对关系将特征分解和 SVD 联系在一起。

#### 为什么特征分解不足以

+   特征分解需要方阵。奇异值分解适用于矩形矩阵。

+   特征值可以是负数或复数；奇异值总是实数且非负。

+   特征向量可能不是完整的基；奇异向量总是形成正交归一基。

简而言之，SVD 提供了特征分解所缺乏的鲁棒性。

#### 示例

让

$$ A = \begin{bmatrix}3 & 0 \\ 4 & 0 \\ 0 & 5\end{bmatrix}. $$

1.  计算 $A^T A = \begin{bmatrix}25 & 0 \\ 0 & 25\end{bmatrix}$。

    +   特征值：$25, 25$。

    +   奇异值：$\sigma_1 = \sigma_2 = 5$。

1.  右奇异向量是 $A^T A$ 的特征向量。在这里，它们形成标准基。

1.  左奇异向量来自 $Av_i / \sigma_i$。

因此，SVD 的几何结构完全编码在 $A^T A$ 和 $AA^T$ 的特征分析中。

#### 几何图像

+   $A^T A$ 的特征向量描述了 $A$ 在输入空间中拉伸而不混合方向的方向。

+   $AA^T$ 的特征向量描述了输出空间中的对应方向。

+   奇异值告诉我们拉伸的程度。

因此，奇异值分解本质上是一种伪装的特征分解——但应用于正确的对称伴随矩阵。

#### 连接的应用

1.  PCA：数据协方差矩阵 $X^T X$ 使用特征分解，但 PCA 直接使用奇异值分解实现。

1.  数值方法：奇异值分解的算法依赖于 $A^T A$ 的特征分析。

1.  稳定性分析：这种关系确保奇异值是可靠的条件度量。

1.  信号处理：信号中的功率（方差）由协方差矩阵的特征值解释，这些特征值与奇异值相关。

1.  机器学习：核主成分分析（PCA）和相关方法依赖于这个联系来处理非线性特征。

#### 为什么它很重要

+   奇异值分解解释了每个矩阵变换，用正交基和缩放来表示。

+   它与特征分解的关系确保 SVD 不是一个外来的工具，而是一种推广。

+   特征视图展示了奇异值分解为什么总是存在，以及奇异值为什么总是实数且非负。

#### 尝试自己操作

1.  证明如果 $v$ 是 $A^T A$ 的特征向量，其特征值为 $\lambda$，那么 $Av$ 要么是零，要么是 $A$ 的左奇异向量，其奇异值为 $\sqrt{\lambda}$。

1.  对于矩阵

    $$ A = \begin{bmatrix}1 & 2 \\ 2 & 1\end{bmatrix}, $$

    计算特征分解和奇异值分解。比较结果。

1.  证明 $A^T A$ 和 $AA^T$ 总是共享相同的非零特征值。

1.  挑战：解释为什么 $A^T A$ 的正交对角化足以保证 $A$ 的完整奇异值分解的存在。

奇异值分解与特征分解的关系统一了线性代数中最深奥的两个思想：每个矩阵变换都是基于特征几何，拉伸成一种总是存在且总是有意义的形态。

### 84. 低秩近似（最佳小型模型）

数据分析、科学计算和机器学习中的一个核心观点是，许多数据集或矩阵在原始形式下比实际需要的要复杂得多。大部分看似的复杂性隐藏了冗余、噪声或低维模式。低秩逼近是将一个大而复杂的矩阵压缩成更小、更简单的版本的过程，同时保留最重要的信息。这一概念基于奇异值分解（SVD），是降维、推荐系统和现代人工智能的核心。

#### 一般问题

假设我们有一个矩阵 $A \in \mathbb{R}^{m \times n}$，可能表示：

+   一个图像，行表示像素，列表示颜色通道。

+   一个评分表，行表示用户，列表示电影。

+   一个词嵌入矩阵，行表示单词，列表示特征。

通常，$A$ 非常大但高度结构化。问题是：

*我们能否找到一个更小的矩阵 $B$，其秩为 $k$（其中 $k \ll \min(m, n)$），并且能够很好地逼近 $A$？*

#### 秩和复杂性

矩阵的秩是它编码的独立方向的数量。高秩意味着复杂性；低秩意味着冗余。

+   一个秩-1 矩阵可以写成两个向量的外积：$uv^T$。

+   一个秩-$k$矩阵是 $k$ 个这样的外积之和。

+   限制秩控制了逼近可以捕捉的结构量。

#### SVD 解法

SVD 提供了一种自然的分解：

$$ A = U \Sigma V^T, $$

其中奇异值 $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r$ 衡量重要性。

要用秩 $k$ 近似 $A$：

$$ A_k = U_k \Sigma_k V_k^T, $$

其中我们只保留前 $k$ 个奇异值和向量。

这不仅仅是一个启发式方法：这是 Eckart–Young 定理：

> 在所有秩-$k$矩阵中，$A_k$ 最小化误差 $\|A - B\|$（在 Frobenius 范数和谱范数中）。

因此，SVD 提供了*最佳可能的*低秩逼近。

#### 几何直觉

+   每个奇异值 $\sigma_i$ 衡量 $A$ 在奇异向量 $v_i$ 方向上的拉伸程度。

+   保留前 $k$ 个奇异值意味着保留最重要的拉伸并忽略较弱的方向。

+   逼近捕捉了 $A$ 的“本质”同时丢弃了小的、噪声的或冗余的影响。

#### 例子

1.  图像：灰度图像可以存储为像素强度的矩阵。使用 SVD，可以通过仅保留最大的奇异值来压缩它：

+   $k = 10$：模糊但可识别的图像。

+   $k = 50$：图像更加清晰，但存储成本远低于完整图像。

+   $k = 200$：几乎与原始图像无法区分。

这是一种实用的图像压缩：数字更少，感知相同。

1.  推荐系统考虑一个用户-电影评分矩阵。尽管它可能很大，但真正的模式（类型偏好、流行趋势）存在于一个低维子空间中。秩-$k$逼近捕捉这些模式，通过填充结构来预测缺失的评分。

1.  自然语言处理（NLP）词嵌入通常来自共现矩阵。通过奇异值分解（SVD）的低秩近似提取语义结构，使得像“king”（国王）、“queen”（王后）和“crown”（王冠）这样的词能够聚集在一起。

#### 错误和权衡

+   错误衰减：如果奇异值迅速下降，小的$k$给出很好的近似。如果它们缓慢衰减，则需要更多的项。

+   能量保持：平方奇异值$\sigma_i²$代表捕获的方差。保留前$k$项保留了大部分“能量”。

+   平衡：太低的秩=过度简化（结构损失）。太高的秩=没有压缩。

#### 实际计算

对于非常大的矩阵，完整的奇异值分解（SVD）是昂贵的（对于$m \geq n$，$O(mn²)$）。替代方案包括：

+   截断 SVD 算法（Lanczos、随机方法）。

+   只计算前$k$个奇异值的迭代方法。

+   逐步方法，随着新数据的到来更新低秩模型。

这些在现代数据科学中至关重要，因为数据集通常有数百万个条目。

#### 类比

+   音乐播放列表：想象一个包含数百首歌曲的播放列表，但其中大多数都是基于几个主题的变体。低秩近似就像只保留核心旋律，而丢弃重复的 riff。

+   照片压缩：只保留最亮和最重要的光线笔触，而忽略微弱和不相关的细节。

+   书籍摘要：不是阅读全文，而是阅读关键情节点。这就是低秩近似。

#### 为什么它很重要

+   揭示了高维数据中的隐藏结构。

+   减少了存储和计算成本。

+   在保留信号的同时过滤噪声。

+   为 PCA、推荐系统和降维提供了基础。

#### 试试看

1.  取一个小的$5 \times 5$随机矩阵。计算其 SVD。构建最佳秩-1 近似。与原始矩阵比较。

1.  下载一个灰度图像（例如，$256 \times 256$）。用 10、50 和 100 个奇异值重建它。进行视觉比较。

1.  证明 Eckart-Young 定理对于谱范数：为什么没有其他秩-$k$近似能比截断 SVD 做得更好？

1.  对于具有许多特征的数据集，计算 PCA 并解释为什么它等同于寻找低秩近似。

低秩近似展示了线性代数如何捕捉复杂性的本质：大部分重要的东西都存在于少数几个维度中。艺术在于找到并有效地使用它们。

### 85. 主成分分析（方差和方向）

主成分分析（PCA）是统计学、数据分析以及机器学习中最广泛使用的技术之一。它提供了一种在尽可能保留重要信息的同时降低数据集维度的方法。核心洞察是数据在某个方向上的变化往往比其他方向更强，通过关注这些方向，我们可以用更少的维度、更少的噪声和更高的可解释性来总结数据集。

#### 基本问题

假设我们在高维空间中有数据点，例如 $x_1, x_2, \dots, x_m \in \mathbb{R}^n$。每个点可能是：

+   将人脸图像展平成数千个像素。

+   一个客户在数百个产品上的购物历史。

+   涉及数千个基因的基因表达谱。

直接存储和处理所有特征是昂贵的，并且许多特征可能是冗余或相关的。PCA 询问：

*我们能否用一组更小的方向重新表达这些数据，这些方向能够捕捉到最大的变异性？*

#### 方差作为信息

主成分分析（PCA）的指导原则是方差。

+   方差衡量数据在某个方向上的分散程度。

+   高方差方向捕捉有意义的结构（例如，不同的面部表情，主要消费习惯）。

+   低方差方向通常对应于噪声或不重要的波动。

因此，PCA 寻找数据方差最大的方向（称为主成分）。

#### 中心化和协方差

首先，我们通过减去均值向量来对数据进行中心化：

$$ X_c = X - \mathbf{1}\mu^T, $$

其中 $\mu$ 是所有数据点的平均值。

那么协方差矩阵是：

$$ C = \frac{1}{m} X_c^T X_c. $$

+   对角线条目衡量每个特征的变化量。

+   对角线外的条目衡量特征如何一起变化。

找到主成分等价于找到这个协方差矩阵的特征向量。

#### 特征视图

1.  $C$ 的特征向量是方向（主成分）。

1.  相应的特征值告诉我们每个成分沿每个方向有多少方差。

1.  将特征值从大到小排序，给出了从最有信息到最少信息的方向。

如果我们保留前 $k$ 个特征向量，我们将数据投影到一个 $k$ 维子空间中，该子空间保留了大部分方差。

#### SVD 视图

另一个视角使用奇异值分解（SVD）：

$$ X_c = U \Sigma V^T. $$

+   $V$ 的列是主方向。

+   奇异值平方 ($\sigma_i²$) 对应于协方差矩阵的特征值。

+   投影到 $V$ 的前 $k$ 列给出降维表示。

这使得 PCA 和 SVD 本质上是一样的计算。

#### 简单示例

想象我们测量了 1000 个人的身高和体重。绘制它们显示了一个强烈的关联：更高的人通常更重。点云沿着对角线延伸。

+   PCA 的第一个成分是这条对角线：最大方差的方向。

+   第二个分量是垂直的，捕捉到更小的差异（例如身高相同但体重略有不同的人）。

+   只保留第一个成分可以将两个特征减少到一个，同时保留大部分信息。

#### 几何图像

+   PCA 旋转坐标系，使得轴与最大方差的方向对齐。

+   投影到前 $k$ 个成分将数据展平到低维空间，就像将倾斜的薄饼展平到最宽的平面上。

#### 应用

1.  数据压缩：仅保留主要成分（例如，压缩图像）以减少存储。

1.  噪声减少：小方差方向通常对应于测量噪声；丢弃它们会产生更干净的数据。

1.  可视化：将数据减少到 2D 或 3D 进行散点图有助于我们看到簇和模式。

1.  机器学习中的预处理：许多模型在 PCA 转换后的数据上训练更快，泛化更好。

1.  基因组和生物学：PCA 在数千个基因中找到变化的主要轴。

1.  金融：PCA 将股票的相关运动总结为几个主要的“因子”。

#### 权衡和限制

+   可解释性：主成分是原始特征的线性组合，有时难以用简单术语解释。

+   线性：PCA 仅捕捉线性关系；非线性方法（如核 PCA、t-SNE 或 UMAP）可能更适合曲线流形。

+   缩放：特征必须被正确归一化；否则，PCA 可能会过度强调具有大原始方差的单位。

+   全局方法：PCA 捕捉整体方差，而不是局部结构（例如，数据中的小簇）。

#### 数学保证

PCA 具有最优性保证：

+   在所有$k$维线性子空间中，PCA 子空间最小化重建误差（数据与其投影之间的平方欧几里得距离）。

+   这实际上是之前看到的低秩近似定理，应用于协方差矩阵。

#### 为什么它很重要

PCA 展示了线性代数如何将原始数据转化为洞察。通过关注方差，它提供了一种原则性的方法来过滤噪声、压缩信息并揭示隐藏的模式。它简单、计算效率高，是基础性的——几乎每个现代数据管道都使用 PCA，无论是显式还是隐式。

#### 尝试自己操作

1.  取一个具有两个相关特征的数据集（如身高和体重）。计算协方差矩阵、特征向量和投影到第一个成分。可视化前后情况。

1.  对于存储为矩阵的灰度图像，将其展平为向量并应用 PCA。需要多少个成分才能以 90%的精度重建它？

1.  在著名的 Iris 数据集（4 个特征）上使用 PCA。使用前两个成分在 2D 中绘制数据。注意在这个减少的空间中物种是如何分开的。

1.  证明第一个主成分是最大化$\|X_c v\|²$的单位向量$v$。

PCA 将复杂性提炼为清晰：它不仅告诉我们数据在哪里，而且告诉我们数据*真正去向何方*。

### 86.伪逆（摩尔-彭罗斯）和求解病态系统

在线性代数中，矩阵的逆是一个强大的工具：如果 $A$ 是可逆的，那么解 $Ax = b$ 就像 $x = A^{-1}b$ 一样简单。但当 $A$ 不是方阵，或者不可逆时会发生什么？在实践中，这是常态：许多问题涉及矩形矩阵（方程多于未知数，或者未知数多于方程），或者奇异方阵。Moore-Penrose 伪逆，通常表示为 $A^+$，将逆矩阵的概念推广到所有矩阵，提供了一种在普通求逆失败时找到解或最佳近似的方法。

#### 为什么普通逆失败

+   非方阵：如果 $A$ 是 $m \times n$ 且 $m \neq n$，则不存在标准逆。

+   奇异矩阵：即使 $A$ 是方阵，如果 $\det(A) = 0$，则它没有逆。

+   不良定问题：在现实世界的数据中，精确解可能不存在（不一致的系统）或者可能不唯一（欠定系统）。

尽管存在这些障碍，我们仍然想要一种系统的方法来解决或近似 $Ax = b$。

#### 伪逆的定义

Moore-Penrose 伪逆 $A^+$ 被定义为满足四个性质的唯一矩阵：

1.  $AA^+A = A$。

1.  $A^+AA^+ = A^+$。

1.  $(AA^+)^T = AA^+$。

1.  $(A^+A)^T = A^+A$.

这些条件确保 $A^+$ 在最广泛的一致意义上充当“逆”。

#### 使用奇异值分解构造伪逆

给定 $A$ 的奇异值分解：

$$ A = U \Sigma V^T, $$

其中 $\Sigma$ 是对角矩阵，具有奇异值 $\sigma_1, \dots, \sigma_r$，伪逆为：

$$ A^+ = V \Sigma^+ U^T, $$

其中 $\Sigma^+$ 是通过求非零奇异值的逆并转置矩阵形成的。具体来说：

+   如果 $\sigma_i \neq 0$，则将其替换为 $1/\sigma_i$。

+   如果 $\sigma_i = 0$，则将其保留为 0。

这个定义适用于所有矩阵，无论是方阵还是矩形。

#### 使用 $A^+$ 解线性系统

1.  超定系统（$m > n$，方程多于未知数）:

    +   通常不存在精确解。

    +   伪逆给出最小二乘解：

        $$ x = A^+ b, $$

        该矩阵最小化 $\|Ax - b\|$。

1.  欠定系统（$m < n$，未知数多于方程）:

    +   存在无穷多个解。

    +   伪逆选择具有最小范数的解：

        $$ x = A^+ b, $$

        该解在所有解中使 $\|x\|$ 最小。

1.  方阵但奇异系统：

    +   存在一些解，但不是唯一的。

    +   伪逆再次选择最小范数解。

#### 示例 1：超定

假设我们想要求解：

$$ \begin{bmatrix}1 & 1 \\ 1 & -1 \\ 1 & 0\end{bmatrix} x = \begin{bmatrix}2 \\ 0 \\ 1\end{bmatrix}. $$

这个 $3 \times 2$ 系统没有精确解。使用伪逆，我们得到同时最适合所有三个方程的最小二乘解。

#### 示例 2：欠定

对于

$$ \begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0\end{bmatrix} x = \begin{bmatrix}3 \\ 4\end{bmatrix}, $$

该系统有无穷多个解，因为 $x_3$ 是自由的。伪逆给出：

$$ x = \begin{bmatrix}3 \\ 4 \\ 0\end{bmatrix}, $$

选择具有最小范数的解。

#### 几何解释

+   伪逆的作用类似于投影到子空间。

+   对于超定系统，它将 $b$ 投影到 $A$ 的列空间，然后找到最近的 $x$。

+   对于欠定系统，它选择解空间中离原点最近的点。

因此 $A^+$ 在这种情况下体现了“最佳可能逆”的原则。

#### 应用

1.  最小二乘回归：通过 $A^+$ 求解 $\min_x \|Ax - b\|²$。

1.  信号处理：从不完整或含噪声的数据中重建信号。

1.  控制理论：当精确控制不可能时设计输入。

1.  机器学习：使用不可逆设计矩阵训练模型。

1.  统计学：计算协方差矩阵的广义逆。

#### 局限性

+   对非常小的奇异值敏感：可能发生数值不稳定性。

+   在噪声环境中，正则化（如岭回归）通常更受欢迎。

+   对于非常大的矩阵，计算成本很高，尽管截断 SVD 可以有所帮助。

#### 为什么它很重要

伪逆是一个统一的概念：它用一个公式处理不一致、欠定或奇异问题。它确保我们始终有一个原则性的答案，即使经典代数说“无解”或“无穷多解”。在现实数据分析中，几乎每个问题在某种程度上都是病态的，这使得伪逆成为现代应用线性代数的实用基石。

#### 试试看

1.  通过手算 SVD 计算一个简单的 $2 \times 2$ 奇异矩阵的伪逆。

1.  使用 $A^+$ 解一个超定系统（$3 \times 2$）和一个欠定系统（$2 \times 3$）。与直观预期进行比较。

1.  探索当奇异值非常小时的数值情况。尝试截断它们——这涉及到正则化。

Moore-Penrose 伪逆表明，即使线性系统“损坏”，线性代数仍然提供了一种系统性的前进方式。

### 87. 条件与敏感性（错误如何放大）

线性代数不仅关于精确解——它还关于当数据受到扰动时这些解的稳定性。在现实世界的应用中，每个数据集都包含噪声：物理实验中的测量误差、金融计算中的舍入误差或数值软件中的浮点精度限制。条件性是研究问题解对输入微小变化敏感性的研究。一个条件良好的问题对扰动反应温和；一个条件不良的问题会放大错误。

#### 基本思想

假设我们解这个线性系统：

$$ Ax = b. $$

现在想象我们稍微改变 $b$ 为 $b + \delta b$。新的解是 $x + \delta x$。

+   如果 $\|\delta x\|$ 大约与 $\|\delta b\|$ 相同大小，则问题是有良好条件的。

+   如果 $\|\delta x\|$ 非常大，则问题是不良条件的。

条件度量这个放大因子。

#### 条件数

中心工具是矩阵 $A$ 的条件数：

$$ \kappa(A) = \|A\| \cdot \|A^{-1}\|, $$

其中 $\|\cdot\|$ 是矩阵范数（通常是 2-范数）。

+   如果 $\kappa(A)$ 接近 1，则问题条件数好。

+   如果 $\kappa(A)$ 很大（例如，$10⁶$ 或更高），则问题条件数不好。

解释：

+   $\kappa(A)$ 估计了解相对于数据相对误差的最大相对误差。

+   在实际应用中，如果 $\kappa(A)$ 太大，$b$ 的每一个有效数字在 $x$ 中都可能丢失。

#### 奇异值和条件数

2-范数下的条件数可以用奇异值表示：

$$ \kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}, $$

$\sigma_{\max}$ 和 $\sigma_{\min}$ 分别是矩阵 $A$ 的最大和最小奇异值。

+   如果最小的奇异值与最大的奇异值相比非常小，$A$ 在某些方向上几乎会崩溃，使得求逆不稳定。

+   这解释了为什么在数值计算中几乎奇异的矩阵如此有问题。

#### 示例 1：一个稳定的系统

$$ A = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix}. $$

在这里，$\sigma_{\max} = 3, \sigma_{\min} = 2$。因此 $\kappa(A) = 3/2 = 1.5$。条件数很好：输入的小变化会产生输出的微小变化。

#### 示例 2：一个条件数不好的系统

$$ A = \begin{bmatrix}1 & 1 \\ 1 & 1.0001\end{bmatrix}. $$

系数的行列式非常小，因此系统几乎奇异。

+   一个奇异值大约是 2.0。

+   另一个大约是 0.0001。

+   条件数：$\kappa(A) \approx 20000$。

这意味着 $b$ 的微小变化也可能导致 $x$ 的剧烈变化。

#### 几何直觉

一个矩阵将单位球体变换成椭圆。

+   椭圆的最长轴 = $\sigma_{\max}$。

+   最短的轴 = $\sigma_{\min}$。

+   $\sigma_{\max} / \sigma_{\min}$ 的比率显示了变换拉伸的程度。

如果椭圆几乎平坦，与短轴对齐的方向几乎消失，恢复它们是非常不稳定的。

#### 为什么条件数在计算中很重要

1.  数值精度：计算机以有限的精度存储数字（浮点数）。条件数不好的系统会放大舍入误差，导致结果不可靠。

1.  回归：在统计学中，高度相关的特征会使设计矩阵条件数不好，导致系数估计不稳定。

1.  机器学习：条件数不好会导致训练不稳定，梯度爆炸或消失。

1.  工程：基于条件数不好的模型的控制系统可能对测量误差非常敏感。

#### 处理条件数不好的技术

+   正则化：添加一个惩罚项，如岭回归 ($\lambda I$)，以稳定求逆。

+   截断奇异值分解：忽略仅放大噪声的微小奇异值。

+   缩放和预条件：重新缩放数据或乘以一个精心选择的矩阵以改善条件数。

+   避免显式求逆：使用分解（LU，QR，SVD）而不是计算 $A^{-1}$。

#### 与先前主题的联系

+   伪逆：当奇异值接近零时，条件数不好变得明显，使得 $A^+$ 不稳定。

+   低秩近似：截断小的奇异值既压缩数据又改善条件数。

+   PCA：丢弃低方差成分本质上是一个条件改进步骤。

#### 为什么它很重要

条件数将抽象代数与数值现实联系起来。线性代数承诺有解，但条件数告诉我们这些解是否可信。没有它，人们可能会误将噪声视为信号，或者在看起来很好的计算中失去所有精度。

#### 尝试自己操作

1.  计算矩阵 $\begin{bmatrix}1 & 1 \\ 1 & 1.0001\end{bmatrix}$ 的条件数。对于几个略有不同的 $b$，求解 $Ax = b$。观察解如何剧烈波动。

1.  考虑一个具有几乎线性相关特征的样本数据集。计算其协方差矩阵的条件数。将其与回归系数的不稳定性联系起来。

1.  模拟数值误差：向病态系统添加大小为 $10^{-6}$ 的随机噪声，并观察解的误差。

1.  证明 $\kappa(A) \geq 1$ 总是成立。

条件数揭示了问题的潜在脆弱性。当代数说“解存在”时，它警告我们，但计算却在低声说“不要相信它”。

### 88. 矩阵范数和奇异值（正确度量大小）

在线性代数中，我们经常需要度量矩阵的“大小”。对于向量来说，这是直截了当的：长度（范数）告诉我们向量有多大。但对于矩阵来说，问题更为微妙：我们是按矩阵的元素来度量大小，还是按矩阵拉伸向量的程度，或者按某种不变性质来度量？不同的上下文需要不同的答案，而矩阵范数（与奇异值紧密相关）提供了这样做的框架。

#### 为什么要度量矩阵的大小？

1.  稳定性：了解矩阵可能放大多少误差。

1.  条件数：最大拉伸与最小拉伸的比率。

1.  优化：许多算法最小化某些矩阵范数。

1.  数据分析：范数衡量数据的复杂度或能量。

没有范数，我们无法比较矩阵，分析敏感性，或判断近似质量。

#### 从向量范数到矩阵范数

定义矩阵范数的一个自然方法是询问：*这个矩阵将向量拉伸多少？*

形式上，对于给定的向量范数 $\|\cdot\|$：

$$ \|A\| = \max_{x \neq 0} \frac{\|Ax\|}{\|x\|}. $$

这被称为诱导矩阵范数。

#### 2-范数和奇异值

当我们使用欧几里得范数 ($\|x\|_2$) 对向量进行度量时，诱导的矩阵范数变为：

$$ \|A\|_2 = \sigma_{\max}(A), $$

$A$ 的最大奇异值。

+   这意味着 2-范数衡量的是*最大拉伸因子*。

+   几何上：$A$ 将单位球映射成椭圆；$\|A\|_2$ 是椭圆最长轴的长度。

这个链接使奇异值成为矩阵大小的自然语言。

#### 其他常见范数

1.  弗罗贝尼乌斯范数

$$ \|A\|_F = \sqrt{\sum_{i,j} |a_{ij}|²}. $$

+   等价于将所有元素堆叠成一个大向量时的欧几里得长度。

+   也可以表示为：

    $$ \|A\|_F² = \sum_i \sigma_i². $$

+   通常在数据科学和机器学习中使用，因为它易于计算且可微分。

1.  1-范数

$$ \|A\|_1 = \max_j \sum_i |a_{ij}|, $$

最大绝对列和。

1.  无穷范数

$$ \|A\|_\infty = \max_i \sum_j |a_{ij}|, $$

最大绝对行和。

这两种范数在数值分析中计算成本低，非常有用。

1.  核范数（迹范数）

$$ \|A\|_* = \sum_i \sigma_i, $$

奇异值的总和。

+   在低秩近似和机器学习（矩阵补全、推荐系统）中非常重要。

#### 奇异值作为统一线索

+   谱范数（2-范数）：最大奇异值。

+   弗罗贝尼乌斯范数：平方奇异值之和的平方根。

+   核范数：奇异值的总和。

因此，范数捕捉了总结奇异值的不同方式：最大值、总和或能量。

#### 示例：小矩阵

取

$$ A = \begin{bmatrix}3 & 4 \\ 0 & 0\end{bmatrix}. $$

+   奇异值：$\sigma_1 = 5, \sigma_2 = 0$。

+   $\|A\|_2 = 5$.

+   $\|A\|_F = \sqrt{3² + 4²} = 5$.

+   $\|A\|_* = 5$.

在这里，不同的范数是一致的，但通常它们突出了矩阵的不同方面。

#### 几何直觉

+   2-范数：“$A$ 能将向量拉伸到多长？”

+   弗罗贝尼乌斯范数：“所有项中的总能量是多少？”

+   1-范数/∞-范数：“最重的列或行负载是多少？”

+   核范数：“$A$ 有多少总拉伸能力？”

每个都是一个透镜，提供了不同的视角。

#### 应用

1.  数值稳定性：条件数 $\kappa(A) = \sigma_{\max}/\sigma_{\min}$ 使用谱范数。

1.  机器学习：核范数用于矩阵补全（Netflix 奖）。

1.  图像压缩：弗罗贝尼乌斯范数衡量重建误差。

1.  控制理论：1-范数和∞-范数限制系统响应。

1.  优化：范数作为惩罚或约束，塑造解。

#### 为什么这很重要

矩阵范数提供了比较、近似和控制矩阵的语言。奇异值确保这种语言不是任意的，而是基于几何的。它们共同解释了矩阵如何扭曲空间，误差如何增长，以及我们如何测量复杂性。

#### 尝试自己操作

1.  对于 $A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}$，计算 $\|A\|_1$、$\|A\|_\infty$、$\|A\|_F$ 和 $\|A\|_2$（最后使用奇异值分解），进行比较。

1.  证明 $\|A\|_F² = \sum \sigma_i²$。

1.  证明 $\|A\|_2 \leq \|A\|_F \leq \|A\|_*$。从几何上进行解释。

1.  考虑一个秩为 1 的矩阵 $uv^T$。它的范数是什么？哪些是相等的？

矩阵范数和奇异值是线性代数的测量尺——它们告诉我们矩阵的大小，以及它的作用、稳定性以及脆弱性。

### 89. 正则化（岭回归/蒂科诺夫正则化以驯服不稳定性）

在求解线性系统或回归问题时，不稳定性通常是由于系统条件不良：数据中的微小误差会导致解的巨大波动。正则化是通过故意修改问题来添加稳定性的策略，牺牲精确性以换取鲁棒性。两种最常见的方法——岭回归和 Tikhonov 正则化——体现了这一原则。

#### 不稳定性的问题

考虑最小二乘问题：

$$ \min_x \|Ax - b\|_2². $$

如果 $A$ 的列几乎线性相关，或者如果 $\sigma_{\min}(A)$ 非常小，那么：

+   解是不稳定的。

+   系数 $x$ 的幅度可能会爆炸。

+   预测随着 $b$ 的微小变化而剧烈变化。

正则化通过修改目标函数，使得解更倾向于稳定性而非精确性。

#### 岭回归 / Tikhonov 正则化

修改后的问题是：

$$ \min_x \big( \|Ax - b\|_2² + \lambda \|x\|_2² \big), $$

其中 $\lambda > 0$ 是正则化参数。

+   第一项强制数据拟合。

+   第二项惩罚大系数，从而阻止不稳定的解。

在统计学中，这被称为岭回归，在数值分析中称为 Tikhonov 正则化。

#### 闭式解

扩展目标函数并求导得到：

$$ x_\lambda = (A^T A + \lambda I)^{-1} A^T b. $$

关键点：

+   添加的 $\lambda I$ 使得矩阵可逆，即使 $A^T A$ 是奇异的。

+   随着 $\lambda \to 0$，解趋近于普通最小二乘解。

+   随着 $\lambda \to \infty$，解趋向于 0。

#### SVD 视角

如果 $A = U \Sigma V^T$，则最小二乘解为：

$$ x = \sum_i \frac{u_i^T b}{\sigma_i} v_i. $$

如果 $\sigma_i$ 非常小，则 $\frac{1}{\sigma_i}$ 项会导致不稳定性。

使用正则化：

$$ x_\lambda = \sum_i \frac{\sigma_i}{\sigma_i² + \lambda} (u_i^T b) v_i. $$

+   小的奇异值（不稳定的方向）被抑制。

+   大的奇异值（稳定的方向）大多被保留。

这解释了为什么岭回归可以稳定解：它抑制了噪声放大的方向。

#### 几何解释

+   未正则化的问题在 $A$ 的列空间中精确地拟合 $b$。

+   正则化将解推向原点，缩小系数。

+   几何上，可行域（由 $Ax$ 形成的椭球体）与来自 $\|x\|_2$ 的球约束相交。解位于这两个形状平衡的位置。

#### 扩展

1.  Lasso ($\ell_1$ 正则化)：将 $\|x\|_2²$ 替换为 $\|x\|_1$，鼓励稀疏解。

1.  弹性网：结合岭回归和 Lasso 惩罚。

1.  一般 Tikhonov：使用 $\|Lx\|_2²$ 与某些矩阵 $L$，定制惩罚（例如，信号处理中的平滑）。

1.  贝叶斯视角：岭回归对应于对系数施加高斯先验。

#### 应用

+   机器学习：在回归和分类中防止过拟合。

+   信号处理：在重建信号时抑制噪声。

+   图像重建：稳定逆问题，如去模糊。

+   数值偏微分方程：向解添加平滑性约束。

+   计量经济学和金融学：控制高度相关变量的不稳定性。

#### 为什么这很重要

正则化将脆弱问题转化为可靠问题。它承认噪声和有限精度的现实，而不是追求不可能的精确性，而是提供可用、稳定的答案。在现代数据驱动领域，几乎每个大规模模型都依赖于正则化来实现鲁棒性。

#### 试试你自己

1.  求解系统 $Ax = b$ 其中

    $$ A = \begin{bmatrix}1 & 1 \\ 1 & 1.0001\end{bmatrix}, \quad b = \begin{bmatrix}2 \\ 2\end{bmatrix}. $$

    比较 $\lambda = 0.01, 1, 10$ 时的非正则化最小二乘解与岭正则化解。

1.  使用奇异值分解（SVD），展示如何缩小小奇异值系数。

1.  在具有许多相关特征的回归中，随着 $\lambda$ 的变化计算系数路径。观察它们如何稳定。

1.  探索图像去噪：将岭正则化应用于模糊/噪声图像重建问题。

正则化展示了线性代数在实践中的智慧：有时最好的解决方案不是精确的，而是稳定的。

### 90. 揭示秩的 QR 和实用诊断（秩的真正含义）

秩——矩阵中独立方向的数目——是线性代数中的核心。它告诉我们关于系统的可解性、特征冗余和数据维度。但在实践中，计算秩并不像计算主元或检查行列式那样简单。现实世界的数据是噪声的、几乎相关的或高维的。揭示秩的 QR（RRQR）分解和相关诊断提供了揭示秩和结构的稳定、实用工具。

#### 为什么秩很重要

+   线性系统：秩决定了系统是否有唯一解、无限多个解或无解。

+   数据科学：秩衡量内在维度，指导降维。

+   数值：小的奇异值使有效秩模糊——精确秩与数值秩分离。

因此，我们需要可靠的算法来决定矩阵中“有多少方向是重要的”。

#### 精确秩与数值秩的比较

+   精确秩：在精确算术中定义。如果一列不能表示为其他列的线性组合，则该列是独立的。

+   数值秩：在浮点计算中，微小的奇异值不可信。阈值 $\epsilon$ 决定了何时将它们视为零。

例如，如果 $A$ 的最小奇异值为 $10^{-12}$，并且计算是在双精度（$\sim 10^{-16}$）下进行的，我们可能会认为有效秩小于满秩。

#### QR 分解回顾

基本 QR 分解将矩阵 $A \in \mathbb{R}^{m \times n}$ 表示为：

$$ A = QR, $$

其中：

+   $Q$ 是正交的（$Q^T Q = I$），保持长度。

+   $R$ 是上三角矩阵，包含了 $A$ 的“本质”。

QR 是稳定的、快速的，并且是许多算法的基础。

#### 揭示秩的 QR（RRQR）

RRQR 是具有列置换的 QR 的增强：

$$ A P = Q R, $$

其中 $P$ 是一个排列矩阵，它重新排列列。

+   行交换确保最大的独立方向首先出现。

+   $R$ 的对角线条目指示哪些列是显著的。

+   对角线上的小值表示相关（或几乎相关）的方向。

实际上，RRQR 通过检查 $R$ 的对角线衰减来近似秩。

#### 比较 RRQR 和 SVD

+   SVD：确定秩的黄金标准；奇异值给出每个方向的精确缩放。

+   RRQR：更快、更便宜；当近似秩足够时足够使用。

+   折衷：SVD 更准确，RRQR 更高效。

根据精度和成本的平衡，两者都得到使用。

#### 示例

让

$$ A = \begin{bmatrix}1 & 1 & 1 \\ 1 & 1.0001 & 2 \\ 1 & 2 & 3\end{bmatrix}. $$

+   精确算术：秩 = 3。

+   数值上：第二列几乎依赖于第一列。SVD 显示一个接近零的奇异值。

+   带有行交换的 RRQR 通过揭示 $R$ 中的微小对角线来识别近似相关性。

因此，RRQR “揭示”了有效秩，而不必完全计算奇异值分解（SVD）。

#### 秩的实用诊断

1.  条件数：高条件数表明接近秩亏。

1.  RRQR 中的 $R$ 的对角线：监控列的独立性。

1.  SVD 中的奇异值：最可靠的指标，但成本高昂。

1.  系数/余子式：在理论上有用，但在实践中不稳定。

#### 应用

+   数据压缩：通过识别有效秩允许截断。

+   回归：通过检查设计矩阵的秩来检测多重共线性。

+   控制系统：秩测试稳定性和可控性。

+   机器学习：降维管道（例如，PCA）从秩估计开始。

+   信号处理：从混合物中识别潜在源的数量。

#### 为什么这很重要

理论上秩很简单，但在实践中难以捉摸。RRQR 和相关诊断架起了精确数学与噪声数据之间的桥梁。它们允许从业者有稳定性和信心地说：“这些是真正重要的独立方向的数量。”

#### 试试看

1.  在一个小的 $5 \times 5$ 近似相关矩阵上实现带列交换的 RRQR。比较估计的秩与 SVD。

1.  探索 $R$ 的对角元素与数值秩之间的关系。

1.  构建一个包含 100 个特征的数据库，其中 95 个是随机噪声，但 5 个是线性组合。使用 RRQR 来检测冗余。

1.  证明列交换不会改变 $A$ 的列空间，只会改变其数值稳定性。

秩揭示 QR 显示线性代数不仅关于精确公式，也关于实用诊断——知道两个方向是否真正不同，以及它们是否本质上相同。

#### 结束语

```py
Noise reduced to still,
singular values unfold space,
essence shines within.
```

## 第十章 应用和计算

#### 开场

```py
Worlds in numbers bloom,
graphs and data interlace,
algebra takes flight.
```

### 91. 2D/3D 几何管道（相机、旋转和变换）

线性代数是现代图形、机器人和计算机视觉的默默支柱。每次屏幕上渲染图像、相机捕捉场景或机器人臂在空间中移动时，一系列矩阵乘法都在将点从一个坐标系转换到另一个坐标系。这些几何管道将 3D 现实映射到 2D 表示，确保对象出现在正确的位置、方向和比例。

#### 坐标系的几何

3D 空间中的一个点表示为一个列向量：

$$ p = \begin{bmatrix} x \\ y \\ z \end{bmatrix}. $$

但计算机通常将此扩展到齐次坐标，将点嵌入到 4D 中：

$$ p_h = \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}. $$

额外的坐标允许将平移表示为矩阵乘法，保持整个管道的一致性：每一步只是乘以一个矩阵。

#### 2D 和 3D 中的变换

+   翻译通过 $(t_x, t_y, t_z)$ 移动一个点。

    $$ T = \begin{bmatrix} 1 & 0 & 0 & t_x \\ 0 & 1 & 0 & t_y \\ 0 & 0 & 1 & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}. $$

+   缩放沿每个轴扩展或缩小空间。

    $$ S = \begin{bmatrix} s_x & 0 & 0 & 0 \\ 0 & s_y & 0 & 0 \\ 0 & 0 & s_z & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}. $$

+   在 3D 中，围绕 z 轴的旋转是：

    $$ R_z(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta & 0 & 0 \\ \sin\theta & \cos\theta & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}. $$

    x 轴和 y 轴周围的旋转也有类似的形式。

每个变换都是线性的（或仿射的），将它们链接起来只是乘以矩阵。

#### 相机管道

将 3D 对象渲染成 2D 图像遵循一系列步骤，每个步骤都是一个矩阵乘法：

1.  模型变换将对象从其局部坐标移动到世界坐标。

1.  视图变换将相机置于原点并使其轴与世界对齐，实际上改变了视点。

1.  投影变换将 3D 点投影到 2D。有两种类型：

    +   正交投影：平行投影，无透视。

    +   透视：远处的物体看起来更小，更接近人类的视觉。

    透视投影的例子：

    $$ P = \begin{bmatrix} f & 0 & 0 & 0 \\ 0 & f & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix}, $$

    其中 $f$ 是焦距。

1.  视口变换将归一化的 2D 坐标映射到屏幕像素。

从对象到图像的这个序列是几何管道。

#### 示例：渲染一个立方体

+   从局部坐标中的立方体顶点（$[-1,1]³$）开始。

+   应用一个缩放矩阵来拉伸它。

+   应用一个旋转矩阵使其倾斜。

+   应用一个平移矩阵将其移动到场景中。

+   应用一个投影矩阵将其平铺到屏幕上。

每一步都是线性代数，最终图像是连续乘以多个矩阵的结果。

#### 机器人连接

机器人臂使用类似的管道：每个关节贡献一个旋转或平移，编码为一个矩阵。通过乘以它们，我们得到正向运动学——给定关节角度的手的位置和方向。

#### 为什么这很重要

几何管道统一了图形、机器人和视觉。它们展示了线性代数如何为视频游戏、动画、模拟甚至自动驾驶汽车中的日常视觉提供动力。没有矩阵乘法的这种一致性，管理变换的复杂性将是不可管理的。

#### 尝试自己操作

1.  写下旋转正方形 45°、缩放 2 倍以及平移$(3, 1)$的矩阵序列。将它们相乘以获得组合变换。

1.  在三维中构建一个立方体，并手动模拟一个透视投影到一个顶点。

1.  对于一个简单的双关节机器人臂，用旋转矩阵表示每个关节并计算最终手的位置。

1.  证明仿射变换的组合在乘法下是封闭的——为什么这使管道成为可能？

几何管道是抽象线性代数和有形的视觉和机械系统之间的桥梁。它们是数学如何成为运动、光线和图像的方式。

### 92. 计算机图形学和机器人学（齐次技巧的实际应用）

线性代数不仅仅停留在粉笔板上——它驱动着计算机图形学和机器人的引擎。这两个领域都需要描述和操作空间中的对象，通常需要在多个坐标系之间转换。齐次坐标技巧——增加一个额外的维度——使得这一点变得优雅：平移、缩放和旋转都适合于矩阵乘法的单一框架。这种一致性允许高效的计算和一致的管道。

#### 齐次坐标回顾

在二维中，一个点 $(x, y)$ 变为 $[x, y, 1]^T$。在三维中，一个点 $(x, y, z)$ 变为 $[x, y, z, 1]^T$。

为什么要添加额外的 1？因为这样，平移——在更高维嵌入中通常不是线性的——变成了线性。每个仿射变换（旋转、缩放、剪切、反射和平移）都只是通过一个齐次矩阵的单次乘法。

示例：

$$ T = \begin{bmatrix} 1 & 0 & 0 & t_x \\ 0 & 1 & 0 & t_y \\ 0 & 0 & 1 & t_z \\ 0 & 0 & 0 & 1 \end{bmatrix}, \quad p_h' = T p_h. $$

这个技巧使得管道模块化：只需按顺序乘以矩阵。

#### 计算机图形学管道

图形引擎（如 OpenGL 或 DirectX）完全依赖于齐次变换：

1.  模型矩阵：将对象放入场景中。

    +   示例：将汽车旋转 90°并向前平移 10 个单位。

1.  视图矩阵：定位虚拟相机。

    +   等价于将世界移动，使相机位于原点。

1.  投影矩阵：将 3D 点投影到 2D。

    +   透视投影会使远处的物体缩小，正交投影则不会。

1.  视口矩阵：将归一化的二维坐标转换为屏幕像素。

你在视频游戏中看到的每一个像素都通过了这个矩阵栈。

#### 机器人管道

在机器人学中，同样的原则适用：

+   带有关节的机器人臂被建模为刚体变换的链。

+   每个关节都贡献一个旋转或平移矩阵。

+   将它们相乘给出机器人末端执行器（手、工具或夹具）的最终姿态。

这被称为正向运动学。

示例：一个具有两个关节的 2D 机器人臂：

$$ p = R(\theta_1) T(l_1) R(\theta_2) T(l_2) \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}. $$

这里 $R(\theta_i)$ 是旋转矩阵，$T(l_i)$ 是沿臂长度的平移。将它们相乘给出手的位姿。

#### 图形和机器人共有的挑战

1.  精度：数值舍入误差可能会累积；稳定的算法至关重要。

1.  速度：两个领域都要求实时计算——图形为每秒 60 帧，机器人为毫秒级反应时间。

1.  层次：图形中的对象可能是嵌套的（例如，汽车的轮子相对于汽车旋转），就像机器人关节一样。齐次变换自然地处理这些层次。

1.  逆问题：图形使用逆变换进行相机移动；机器人使用它们进行逆运动学（找到关节角度以到达一个点）。

#### 为什么齐次技巧如此强大

+   一致性：一个系统（矩阵乘法）处理所有变换。

+   效率：硬件（GPU、控制器）可以直接优化矩阵运算。

+   可扩展性：在 2D、3D 或更高维度中工作相同。

+   可组合性：长管道只是矩阵的乘积，避免了特殊情况。

#### 应用

+   图形：渲染引擎、VR/AR、CAD 软件、动作捕捉。

+   机器人技术：机械臂、无人机、自动驾驶汽车、类人机器人。

+   交叉：仿真平台使用相同的数学来测试机器人和渲染虚拟环境。

#### 试试看

1.  建立一个 2D 变换管道：旋转一个三角形，将其平移，并将其投影到屏幕空间。写下最终的变换矩阵。

1.  建模一个简单的双关节机器人臂。使用齐次矩阵推导正向运动学。

1.  实现相机变换：放置一个立方体在 $(0,0,5)$，将相机移动到 $(0,0,0)$，并计算其 2D 屏幕投影。

1.  证明直接组合旋转和平移与将它们嵌入到齐次矩阵中并相乘是等价的。

齐次坐标是图形和机器人共享相同数学 DNA 的隐藏秘密。它们统一了我们移动像素、机器和虚拟世界的方式。

### 93. 图、邻接和拉普拉斯矩阵（通过矩阵的网络）

线性代数提供了一种强大的语言来研究图——由边连接的节点网络。从社交网络到电路，从互联网的结构到生物途径，图无处不在。矩阵为图提供了数值形式，使得可以使用代数技术分析其结构。

#### 图形基础回顾

+   一个图 $G = (V, E)$ 有一个顶点集 $V$（节点）和边集 $E$（连接）。

+   图可以是无向的或定向的，带权重的或无权重的。

+   许多图属性——连通性、流、集群——可以通过矩阵进行研究。

#### 邻接矩阵

对于具有 $n$ 个顶点的图，邻接矩阵 $A \in \mathbb{R}^{n \times n}$ 编码了连接：

$$ A_{ij} = \begin{cases} w_{ij}, & \text{如果存在从节点 $i$ 到节点 $j$ 的边} \\ 0, & \text{否则} \end{cases} $$

+   **无权图**：条目是 0 或 1。

+   **加权图**：条目是边权重（距离、成本、容量）。

+   **无向图**：$A$ 是对称的。

+   **有向图**：$A$ 可能是不对称的。

**邻接矩阵是图的代数指纹**。

#### **邻接矩阵的幂**

矩阵 $(A^k)_{ij}$ 计算从节点 $i$ 到节点 $j$ 的长度为 $k$ 的路径数量。

+   $A²$ 告诉我们存在多少两步连接。

+   这个属性用于检测路径、聚类和网络流等算法中。

#### **度矩阵**

顶点的度数是连接到它的边的数量（或在加权图中的权重之和）。

**度矩阵 $D$** 是对角矩阵：

$$ D_{ii} = \sum_j A_{ij}. $$

这个矩阵衡量每个节点“连接”的程度。

#### **图拉普拉斯矩阵**

组合拉普拉斯矩阵定义为：

$$ L = D - A. $$

**关键属性**：

+   $L$ 是对称的（对于无向图）。

+   每行的和为零。

+   最小的特征值总是 0，特征向量为 $[1, 1, \dots, 1]^T$。

拉普拉斯矩阵编码了连通性：如果图分为 $k$ 个连通分量，则 $L$ 有 exactly $k$ 个零特征值。

#### **归一化拉普拉斯矩阵**

两种常见的归一化版本是：

$$ L_{sym} = D^{-1/2} L D^{-1/2}, \quad L_{rw} = D^{-1} L. $$

这些将拉普拉斯矩阵缩放以应用于谱聚类等应用。

#### **谱图理论**

$A$ 或 $L$ 的特征值和特征向量揭示了结构：

+   **代数连通性**：$L$ 的第二小的特征值衡量图连接的好坏。

+   **谱聚类**：$L$ 的特征向量将图划分为社区。

+   **随机游走**：转移概率与 $D^{-1}A$ 相关。

#### 示例：一个简单图

考虑一个有 3 个节点的三角形图，每个节点都与其他两个节点相连。

$$ A = \begin{bmatrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix}, \quad D = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{bmatrix}, \quad L = \begin{bmatrix} 2 & -1 & -1 \\ -1 & 2 & -1 \\ -1 & -1 & 2 \end{bmatrix}. $$

+   $L$ 的特征值：$0, 3, 3$。

+   单个零特征值确认图是连通的。

#### **应用**

1.  **社区检测**：谱聚类在社交或生物网络中找到自然的划分。

1.  **图绘制**：$L$ 的特征向量提供视觉嵌入图的坐标。

1.  **随机游走与 PageRank**：从邻接矩阵定义的转移矩阵定义重要性分数。

1.  物理学：拉普拉斯矩阵出现在扩散和振动问题的离散版本中。

1.  **机器学习**：图神经网络（GNNs）使用拉普拉斯矩阵在图结构中传播信号。

#### **为什么它很重要**

**图和矩阵是同一枚硬币的两面：一个是组合的，一个是代数的**。通过将网络转换为矩阵，线性代数使我们能够访问特征值、范数和分解的全套工具箱，从而对连通性、流量和结构有深刻的见解。

#### 尝试自己操作

1.  计算正方形图（循环中的 4 个节点）的邻接矩阵、度矩阵和拉普拉斯矩阵。找到 $L$ 的特征值。

1.  证明拉普拉斯算子始终至少有一个零特征值。

1.  证明如果一个图有 $k$ 个组件，那么零作为特征值的重数恰好是 $k$。

1.  对于图上的随机游走，推导出转移矩阵 $P = D^{-1}A$。解释其特征向量。

图形展示了线性代数如何超越几何和数据表——它成为从分子到巨型城市的网络通用语言。

### 94. 数据预处理作为线性运算（中心化、白化、缩放）

在训练任何复杂模型之前，原始数据必须进行预处理。令人惊讶的是，最常见的预处理步骤——中心化、缩放、白化——不过是伪装成线性代数操作的步骤。以这种方式理解它们不仅阐明了为什么它们有效，而且还展示了它们如何与更广泛的概念如协方差、特征值和奇异值分解相联系。

#### 预处理本质

大多数数据集都存储为矩阵：行对应于样本（观测值），列对应于特征（变量）。例如，在一个包含 1,000 人的身高、体重和年龄记录的数据集中，我们将有一个 $1000 \times 3$ 的矩阵。线性代数使我们能够系统地重塑、缩放和旋转这个矩阵，以便为下游分析做好准备。

#### 中心化：平移原点

中心化意味着从每列（特征）的均值中减去该列的所有条目。

$$ X_{centered} = X - \mathbf{1}\mu^T $$

+   这里 $X$ 是数据矩阵，$\mu$ 是列均值的向量，$\mathbf{1}$ 是一列全为 1 的列。

+   影响：将数据集移动，使每个特征具有零均值。

+   几何视角：将点云平移，使其“质心”位于原点。

+   为什么重要：协方差和相关性公式假设数据是均值中心化的；否则，交叉项会偏斜。

示例：如果人们的平均身高为 170 厘米，则从每个身高中减去 170。在中心化后，“身高 = 0”对应于平均身高的人。

#### 缩放：归一化变异性

原始特征可能具有不同的单位或量级（例如，体重以公斤为单位，收入以千美元为单位）。为了公平比较，我们进行缩放：

$$ X_{scaled} = X D^{-1} $$

其中 $D$ 是特征标准差的对角矩阵。

+   每个特征现在具有方差 1。

+   几何视角：重新缩放轴，使所有维度具有相等的“分布”。

+   在机器学习中很常见：确保梯度下降不会不成比例地关注具有大原始值的特征。

示例：如果体重在 60 公斤 ± 15 的范围内变化，除以 15 使其分布与身高（±10 厘米）的分布相当。

#### 白化：去除相关性

即使在中心化和缩放之后，特征仍然可能保持相关性（例如，身高和体重）。白化将数据转换，使特征成为与单位方差不相关的。

+   令 $\Sigma = \frac{1}{n} X^T X$ 为中心化数据的协方差矩阵。

+   执行特征值分解：$\Sigma = Q \Lambda Q^T$。

+   白化变换：

$$ X_{white} = X Q \Lambda^{-1/2} Q^T $$

结果：

1.  $X_{white}$ 的协方差矩阵是单位矩阵。

1.  每个新特征都是旧特征的旋转组合，没有冗余。

几何视角：白化“球形化”数据云，将椭圆变为完美的圆。

#### 协方差矩阵作为关键角色

协方差矩阵本身自然地来源于预处理：

$$ \Sigma = \frac{1}{n} X^T X \quad \text{(如果 $X$ 已中心化)。} $$

+   对角线元素：特征的方差。

+   非对角线元素：协方差，衡量线性关系。

+   预处理操作（中心化、缩放、白化）旨在重塑数据，使 $\Sigma$ 更易于解释，并使学习算法更稳定。

#### 与 PCA 的联系

+   在 PCA 之前需要中心化，否则第一个成分只是指向均值。

+   缩放确保 PCA 不会过度重视方差大的特征。

+   白化与 PCA 本身密切相关：PCA 对协方差进行对角化，而白化则通过将特征值缩放为 1 进一步前进。

因此，PCA 可以被视为预处理管道加上分析步骤。

#### 实际工作流程

1.  中心化和缩放（标准化）：许多算法（如逻辑回归或 SVM）的默认设置。

1.  白化：常用于信号处理（例如，去除音频或图像中的相关性）。

1.  深度学习中的批量归一化：在训练过程中逐层应用中心化和缩放的变体。

1.  图像处理中的白化：确保像像素强度这样的特征去相关，提高压缩和识别。

#### 工作示例

假设我们有三个特征：身高、体重和年龄。

1.  原始数据：

    +   平均身高 = 170 cm，平均体重 = 65 kg，平均年龄 = 35 岁。

    +   方差差异很大：年龄变化较小，体重变化较大。

1.  在中心化后：

    +   每个特征的均值都是零。

    +   现在平均身高的人在该特征中的值为 0。

1.  在缩放后：

    +   所有特征都具有单位方差。

    +   算法可以平等地处理年龄和体重。

1.  白化后：

    +   身高和体重的相关性消失。

    +   特征成为特征空间中的正交方向。

#### 为什么它很重要

没有预处理，模型可能会被规模、单位或相关性误导。预处理使特征可比较、平衡且独立——这对于依赖于几何（距离、角度、内积）的算法是关键条件。

从本质上讲，预处理是混乱的、现实世界数据到线性代数期望的清洁结构的桥梁。

#### 尝试自己操作

1.  对于小数据集，在中心化前后计算协方差矩阵。有什么变化？

1.  缩放数据集，使每个特征具有单位方差。检查新的协方差。

1.  通过特征值分解进行白化，并验证协方差矩阵变为单位矩阵。

1.  在白化前后在二维空间中绘制数据点。注意椭圆如何变成圆。

通过线性代数进行预处理表明，准备数据不仅仅是家务管理——它是对问题几何形状的根本重塑。

### 95. 线性回归和分类（从模型到矩阵）

线性代数为数据科学和应用统计学中最广泛使用的两种工具提供了基础：线性回归（预测连续结果）和线性分类（分离类别）。这两个问题都归结为以矩阵形式表示数据，然后应用线性运算来估计参数。

#### 回归设置

假设我们想要从数据矩阵 $X \in \mathbb{R}^{n \times p}$ 中收集的特征预测输出 $y \in \mathbb{R}^n$，其中：

+   $n$ = 观察数（样本数）。

+   $p$ = 特征数（变量数）。

我们假设一个线性模型：

$$ y \approx X\beta, $$

其中 $\beta \in \mathbb{R}^p$ 是系数向量（权重）。$\beta$ 的每个条目告诉我们其特征对预测的贡献程度。

#### 正规方程

我们希望最小化平方误差：

$$ \min_\beta \|y - X\beta\|². $$

求导得到正规方程：

$$ X^T X \beta = X^T y. $$

+   如果 $X^T X$ 是可逆的：

$$ \hat{\beta} = (X^T X)^{-1} X^T y. $$

+   如果不可逆（多重共线性，特征过多），我们通过奇异值分解（SVD）使用伪逆：

$$ \hat{\beta} = X^+ y. $$

#### 几何解释

+   $X\beta$ 是 $y$ 在 $X$ 的列空间上的投影。

+   剩余 $r = y - X\hat{\beta}$ 与 $X$ 的所有列正交。

+   这种“最佳拟合”特性是为什么回归是一个投影问题。

#### 线性模型分类

有时我们不想预测连续输出，而是想分离类别（例如，垃圾邮件与非垃圾邮件）。

+   线性分类器：根据线性函数的符号做出决定。

$$ \hat{y} = \text{sign}(w^T x + b). $$

+   几何视图：$w$ 定义了特征空间中的超平面。一侧的点被标记为正，另一侧为负。

+   与回归的关系：逻辑回归用对数似然损失代替平方误差，但仍然通过迭代线性代数方法求解权重。

#### 多类扩展

+   对于 $k$ 个类别，我们使用权重矩阵 $W \in \mathbb{R}^{p \times k}$。

+   预测：

$$ \hat{y} = \arg \max_j (XW)_{ij}. $$

+   每个类别都有一个 $W$ 的列，分类器选择得分最高的列。

#### 示例：预测房价

+   特征：大小、房间数量、距离市中心距离。

+   目标：价格。

+   $X$ = 特征矩阵，$y$ = 价格向量。

+   回归求解系数，以显示每个因素对价格的影响程度。

如果我们切换到分类（预测“昂贵”与“便宜”），我们将价格视为标签，并求解分离两个类别的超平面。

#### 计算方面

+   直接求解正规方程：$O(p³)$（矩阵求逆）。

+   QR 分解：数值上更稳定。

+   SVD：当 $X$ 条件不良或秩亏时最佳。

+   现代库：利用稀疏性或使用基于梯度的方法处理大数据集。

#### 与其他主题的联系

+   最小二乘法（第八章）：回归是典型的最小二乘问题。

+   SVD（第九章）：当列相关时，伪逆给出回归。

+   正则化（第九章）：岭回归添加一个惩罚 $\lambda \|\beta\|²$ 以提高稳定性。

+   分类（第十章）：构成了支持向量机和神经网络等更复杂模型的基础。

#### 它的重要性

线性回归和分类展示了线性代数与实际决策之间的直接联系。它们结合了几何（投影、超平面）、代数（解系统）和计算（分解）。尽管它们很简单，但仍然是不可或缺的：它们是可解释的、快速的，并且通常与更复杂的模型具有竞争力。

#### 尝试自己操作

1.  给定三个特征和五个样本，构建 $X$ 和 $y$。使用正规方程求解 $\beta$。

1.  证明残差与 $X$ 的所有列正交。

1.  在 2D 中写下分离两个点簇的线性分类器。绘制分离的超平面。

1.  探索当两个特征高度相关（共线性）时会发生什么。使用伪逆来恢复一个稳定的解。

线性回归和分类证明了线性代数不仅仅是抽象的——它是实际预测的引擎。

### 96. 实践中的 PCA（降维工作流程）

主成分分析（PCA）是应用线性代数中最广泛使用的工具之一。在其核心，PCA 识别数据变化最大的方向（主成分），然后以这些方向重新表达数据。在实践中，PCA 不仅仅是一个数学上的好奇心——它是一个完整的降低维度、去噪和从高维数据集中提取模式的工作流程。

#### 动机

现代数据集通常拥有数千甚至数百万个特征：

+   图像：每个像素是一个特征。

+   基因组学：每个基因表达水平是一个特征。

+   文本：词汇表中的每个词成为一个维度。

在如此高维度的空间中工作（从计算上）是昂贵的且脆弱的（噪声累积）。PCA 提供了一种系统的方法，将特征空间降低到更小的维度集合，同时仍然捕捉到大部分的变异性。

#### 第 1 步：组织数据

我们从一个数据矩阵 $X \in \mathbb{R}^{n \times p}$ 开始：

+   $n$：样本数（观测数）。

+   $p$：特征数（变量数）。

每行是一个样本；每列是一个特征。

定中心是预处理的第一步：减去每列的平均值，使得数据集具有零均值。这确保 PCA 描述的是方差，而不是由偏移造成的偏差。

$$ X_{centered} = X - \mathbf{1}\mu^T $$

#### 第 2 步：协方差矩阵

接下来，计算协方差矩阵：

$$ \Sigma = \frac{1}{n} X_{centered}^T X_{centered}. $$

+   对角线元素：每个特征的方差。

+   非对角线元素：特征如何共变。

$\Sigma$ 的结构决定了数据中最大变化的方向。

#### 第 3 步：特征分解或 SVD

两种等效方法：

1.  特征分解：解 $\Sigma v = \lambda v$。

    +   特征向量 $v$ 是主成分。

    +   特征值 $\lambda$ 测量沿这些方向的变化。

1.  奇异值分解（SVD）：直接分解居中的数据矩阵：

    $$ X_{centered} = U \Sigma V^T. $$

    +   $V$ 的列 = 主方向。

    +   奇异值平方对应于方差。

在实践中，SVD 由于数值稳定性和效率而被优先考虑，尤其是在 $p$ 非常大的时候。

#### 第 4 步：选择成分数量

我们按顺序排列特征值 $\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_p$。

+   解释方差比：

    $$ \text{EVR}(k) = \frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^p \lambda_i}. $$

+   我们选择 $k$ 使得 EVR 超过某个阈值（例如，90-95%）。

+   这在降维和信息保留之间取得了平衡。

图形上，特征值图显示了特征值，我们寻找添加额外成分增加方差很少的“肘点”。

#### 第 5 步：投影数据

一旦我们选择了 $k$ 个成分，我们就将它们投影到这些成分上：

$$ X_{PCA} = X_{centered} V_k, $$

其中 $V_k$ 包含前 $k$ 个特征向量。

结果：

+   $X_{PCA} \in \mathbb{R}^{n \times k}$。

+   现在，每一行都是原始样本的 $k$ 维表示。

#### 工作示例：人脸图像

假设我们有一个灰度图像数据集，每个图像 $100 \times 100$ 像素 ($p = 10,000$)。

1.  将每个像素值居中。

1.  计算所有图像的协方差。

1.  找到特征向量 = 特征面。这些是像“眼镜”、“嘴型”或“光照方向”这样的特征模式。

1.  保留前 50 个成分。现在每个面都由一个 50 维向量而不是 10,000 维向量表示。

这大大减少了存储并加快了识别速度，同时保留了关键特征。

#### 实际考虑因素

+   标准化：如果特征具有不同的尺度（例如，年龄以年为单位与收入以千为单位），在 PCA 之前必须对它们进行缩放。

+   计算捷径：对于非常大的 $p$，直接在 $X$ 上通过截断 SVD 计算 PCA 通常更快。

+   噪声过滤：小的特征值通常对应于噪声；截断它们可以去除数据集中的噪声。

+   可解释性：主成分是特征的线性组合。有时这些组合是可解释的，有时不是。

#### 与其他概念的联系

+   白化（第九十四章）：PCA 后跟缩放特征值到 1 是白化。

+   SVD（第九章）：PCA 实质上是 SVD 的应用。

+   回归（第九十五章）：PCA 可以在回归之前使用，以减少预测变量之间的共线性（PCA 回归）。

+   机器学习流程：PCA 通常在聚类、分类或神经网络之前使用。

#### 为什么这很重要

PCA 将原始、难以处理的数据转换为紧凑的形式，同时不丢失基本结构。它使得可视化（高维数据的 2D/3D 图）成为可能，加快学习速度，并减少噪声。许多突破——从人脸识别到基因表达分析——都依赖于 PCA 作为预处理的第一步。

#### 试试你自己

1.  取一个具有 3 个特征的样本数据集。手动计算协方差、特征值和特征向量。

1.  将数据投影到前两个主成分上并绘图。与原始的 3D 散点图进行比较。

1.  下载一个图像数据集并应用 PCA 进行压缩。用 10、50、100 个成分重建图像。观察压缩与保真度之间的权衡。

1.  计算解释方差比并决定保留多少个成分。

主成分分析（PCA）是原始数据和有意义的表示之间的桥梁：它在降低复杂性的同时锐化模式。它展示了线性代数如何揭示高维混沌中的隐藏秩序。

### 97. 推荐系统和低秩模型（填充缺失项）

推荐系统——如 Netflix、Amazon 或 Spotify 所使用的——建立在这样一个原则之上：偏好可以通过隐藏在大型稀疏数据中的低维结构来捕捉。线性代数为我们提供了揭示和利用这些结构的工具，特别是通过低秩模型。

#### 偏好矩阵

我们从一个用户-物品矩阵 $R \in \mathbb{R}^{m \times n}$ 开始：

+   行代表用户。

+   列代表物品（电影、书籍、歌曲）。

+   条目 $R_{ij}$ 存储评分（例如 1-5 星）或交互（点击、购买）。

在实践中，大多数条目都是缺失的——用户只对一小部分物品进行评分。核心挑战：预测缺失项。

#### 为什么是低秩结构？

尽管其规模庞大，$R$ 通常接近低秩近似：

$$ R \approx U V^T $$

+   $U \in \mathbb{R}^{m \times k}$：用户因子。

+   $V \in \mathbb{R}^{n \times k}$：物品因子。

+   $k \ll \min(m, n)$。

在这里，每个用户和每个物品都在一个共享的潜在特征空间中表示。

+   例子：对于电影，潜在维度可能捕捉“动作与浪漫”、“旧与新”或“主流与独立”。

+   在这个空间中，用户的偏好向量与物品的特征向量相互作用以生成预测评分。

这种分解解释了相关性：如果你喜欢电影 A 和 B，而电影 C 具有相似的潜在特征，系统会预测你也会喜欢 C。

#### 奇异值分解（SVD）方法

如果 $R$ 是完整的（没有缺失项），我们可以计算 SVD：

$$ R = U \Sigma V^T. $$

+   保留最大的 $k$ 个奇异值以形成秩-$k$ 近似。

+   这捕捉了用户偏好的主导模式。

+   几何视角：将庞大的数据云投影到一个较小的 $k$ 维子空间，其中结构更清晰。

但真实数据是不完整的。这导致了矩阵补全问题。

#### 矩阵补全

矩阵补全试图通过假设低秩来推断 $R$ 的缺失项。优化问题是：

$$ \min_{X} \ \text{rank}(X) \quad \text{s.t. } X_{ij} = R_{ij} \text{ for observed entries}. $$

由于最小化秩是 NP-hard，实际算法通常最小化核范数（奇异值之和）或使用交替最小化：

+   随机初始化 $U, V$。

+   逐次求解一个，同时固定另一个。

+   收敛到一个适合观察评分的低秩分解。

#### 交替最小二乘（ALS）

ALS 是一种标准方法：

1.  固定 $V$，对 $U$ 进行最小二乘求解。

1.  固定 $U$，对 $V$ 进行最小二乘求解。

1.  重复直到收敛。

每个子问题是直接的线性回归，可以使用正常方程或 QR 分解求解。

#### 随机梯度下降（SGD）

另一种方法：将每个观察到的评分视为一个训练样本。通过最小化平方误差来更新潜在向量：

$$ \ell = (R_{ij} - u_i^T v_j)². $$

逐次调整用户向量 $u_i$ 和项目向量 $v_j$ 沿着梯度。这种方法在大数据集上扩展良好，因此在实践中很常见。

#### 正则化

为了防止过拟合：

$$ \ell = (R_{ij} - u_i^T v_j)² + \lambda (\|u_i\|² + \|v_j\|²). $$

+   正则化缩小因子，抑制极端值。

+   几何上，它将潜在向量保持在特征空间中一个合理的球体内。

#### 冷启动问题

+   新用户：没有评分，$u_i$ 是未知的。解决方案：使用人口统计特征或要求提供一些初始评分。

+   新项目：同样，项目需要辅助信息（元数据、标签）来生成初始潜在向量。

这就是混合模型将矩阵分解与基于内容的特征结合起来的地方。

#### 示例：电影评分

想象有 1,000 个用户和 5,000 部电影。

+   原始 $R$ 矩阵有 500 万个条目，但每个用户只对大约 50 部电影进行了评分。

+   矩阵补全，当 $k = 20$ 时，恢复密集近似。

+   每个用户由 20 个潜在的“口味”因子表示；每部电影由 20 个潜在的“主题”因子表示。

+   预测：用户向量和电影向量的点积。

#### 不仅仅是评分：隐式反馈

在实践中，系统通常缺乏明确的评分。相反，它们使用：

+   浏览、点击、购买、跳过。

+   这些信号是间接的但很丰富。

+   分解可以通过将交互视为加权观察来处理。

#### 与其他线性代数工具的联系

+   SVD（第九章）：分解方法的基础。

+   伪逆（第九章）：在解决 ALS 中的小回归子问题时很有用。

+   条件化（第九章）：分解稳定性取决于良好缩放的潜在因子。

+   PCA（第九十六章）：PCA 实质上是一种低秩近似，因此 PCA 和推荐系统共享相同的数学。

#### 为什么它很重要

推荐系统使现代互联网个性化。每个播放列表建议、书籍推荐或广告定位都是由隐藏在巨大稀疏矩阵中的线性代数驱动的。低秩建模展示了即使是不完整、有噪声的数据也可以被利用来揭示偏好和行为模式。

#### 试试看自己动手。

1.  考虑一个带有缺失项的小用户-物品矩阵。通过 SVD 进行秩 2 近似来填补空缺。

1.  实现 ALS 的一步：固定电影因子，并使用最小二乘法更新用户因子。

1.  比较有和没有正则化的预测。注意正则化如何稳定结果。

1.  探索冷启动问题：模拟一个新用户并尝试从最少的数据中预测偏好。

低秩模型揭示了一个强大的真理：在人类选择的巨大多样性背后，隐藏着令人惊讶的少数基本模式——而线性代数是揭示这些模式的关键。

### 98. PageRank 和随机游走（使用特征向量进行排名）

PageRank，曾经是谷歌搜索引擎主导地位的算法，是线性代数和特征向量如何衡量网络中重要性的一个引人注目的例子。在其核心，它将网络建模为一个图，并提出了一个简单的问题：如果你永远随机冲浪网络，你将最常访问哪些页面？

#### 网络作为图

+   每个网页是一个节点。

+   每个超链接都是一个有向边。

+   邻接矩阵 $A$ 编码了哪些页面链接到哪些页面：

$$ A_{ij} = 1 \quad \text{if page $j$ links to page $i$}. $$

为什么是列而不是行？因为链接是从源到目的地的，当分析列满秩转移矩阵时，PageRank 自然出现。

#### 转移矩阵

为了模拟随机冲浪，我们定义一个列满秩矩阵 $P$:

$$ P_{ij} = \frac{1}{\text{outdeg}(j)} \quad \text{if $j \to i$}. $$

+   每一列的和为 1。

+   $P_{ij}$ 是从页面 $j$ 移动到页面 $i$ 的概率。

+   这定义了一个马尔可夫链：一个随机过程，其中下一个状态只依赖于当前状态。

如果用户在页面 $j$ 上，他们将随机均匀选择一个出站链接。

#### 随机游走解释

想象一个网络冲浪者根据 $P$ 移动页面。经过许多步骤后，每个页面花费的时间比例收敛到一个稳态分布向量 $\pi$:

$$ \pi = P \pi. $$

这是一个特征向量方程：$\pi$ 是 $P$ 的特征值为 1 的平稳特征向量。

+   $\pi_i$ 是在页面 $i$ 上长期存在的概率。

+   $\pi_i$ 越高意味着重要性越大。

#### PageRank 调整：传送

纯随机游走存在问题：

1.  死胡同：没有出站链接的页面会困住冲浪者。

1.  蜘蛛陷阱：只有相互链接的页面组会积累概率质量。

解决方案：添加一个传送机制：

+   以概率 $\alpha$（例如 0.85），跟随一个链接。

+   以概率 $1-\alpha$ 跳转到随机页面。

这定义了 PageRank 矩阵：

$$ M = \alpha P + (1-\alpha)\frac{1}{n} ee^T, $$

其中 $e$ 是全一向量。

+   $M$ 是随机的，不可约的，且非周期的。

+   根据 Perron-Frobenius 定理，它有一个唯一的平稳分布 $\pi$。

#### 解特征问题

PageRank 向量 $\pi$ 满足：

$$ M \pi = \pi. $$

+   直接通过特征分解计算 $\pi$ 对于数十亿页面来说是不切实际的。

+   而是使用幂迭代：重复地将向量乘以 $M$ 直到收敛。

这之所以有效，是因为最大的特征值是 1，并且该方法收敛到其特征向量。

#### 工作示例：一个小型网络

假设有 3 个带有链接的页面：

+   页面 1 → 页面 2

+   页面 2 → 页面 3

+   页面 3 → 页面 1 和页面 2

邻接矩阵（列=源）：

$$ A = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix}. $$

转移矩阵：

$$ P = \begin{bmatrix} 0 & 0 & 1/2 \\ 1 & 0 & 1/2 \\ 0 & 1 & 0 \end{bmatrix}. $$

使用 teleportation ($\alpha=0.85$)，我们形成 $M$。幂迭代快速收敛到 $\pi = [0.37, 0.34, 0.29]^T$。页面 1 排名最高。

#### 超越网络

尽管起源于搜索引擎，PageRank 的数学应用范围广泛：

+   社交网络：根据其连接对有影响力用户进行排名。

+   引用网络：根据它们被引用的方式对科学论文进行排名。

+   生物学：在蛋白质-蛋白质相互作用网络中识别关键蛋白质。

+   推荐系统：通过链接结构对产品或电影进行排名。

在每种情况下，重要性不是由节点有多少连接来定义的，而是由指向它的节点的重要性来定义的。

#### 计算挑战

+   规模：数十亿个页面意味着 $M$ 无法完全存储；稀疏矩阵技术是必不可少的。

+   收敛性：幂迭代可能需要数百步；预条件化和并行化可以加速它。

+   个性化：不是使用均匀的 teleportation，而是调整概率以偏向用户兴趣。

#### 为什么这很重要

PageRank 阐述了深刻的原则：重要性源于连通性。线性代数通过识别转移矩阵的主特征向量来捕捉这一点。这一思想——通过平稳分布对网络中的节点进行排名——已经改变了搜索引擎、社交媒体和科学本身。

#### 试试看

1.  构建一个 4 页面的网页图，并使用 $\alpha = 0.85$ 手动计算其 PageRank。

1.  在 Python 或 MATLAB 中实现小型邻接矩阵的幂迭代。

1.  将 PageRank 与简单的度数计数进行比较。注意 PageRank 如何更重地奖励来自重要节点的链接。

1.  修改 teleportation 以偏向页面子集（个性化 PageRank）。观察排名如何变化。

PageRank 不仅是计算机科学历史上的一个里程碑——它也是特征向量如何从局部结构中捕获全局重要性的一个活生生的例子。

### 99. 数值线性代数基础（浮点数，BLAS/LAPACK）

理论上的线性代数是精确的：数字的行为像实数，操作是确定性的，结果是精确的。在实践中，计算是在计算机上进行的，其中数字以有限精度表示，算法必须平衡速度、准确性和稳定性。这个交集——数值线性代数——使得线性代数在现代规模上可用。

#### 浮点表示

实数无法在数字机器上精确存储。相反，它们使用 IEEE 754 浮点标准进行近似。

+   浮点数存储如下：

    $$ x = \pm (1.m_1 m_2 m_3 \dots) \times 2^e $$

    其中 $m$ 是尾数，$e$ 是指数。

+   单精度（float32）：32 位 → ~7 位十进制精度。

+   双精度（float64）：64 位 → ~16 位十进制精度。

+   计算机 epsilon ($\epsilon$)：1 和下一个可表示数字之间的最小间隔。对于双精度，$\epsilon \approx 2.22 \times 10^{-16}$。

含义：像几乎相等的数字相减这样的操作会导致灾难性的舍入误差，其中有效数字消失。

#### 问题的条件

一个线性代数问题可能在数学上很好地设定，但仍然在数值上困难。

+   矩阵 $A$ 的条件数：

    $$ \kappa(A) = \|A\| \cdot \|A^{-1}\| $$

+   如果 $\kappa(A)$ 很大，小的输入误差会导致大的输出误差。

+   例子：求解 $Ax = b$。对于病态的 $A$，即使算法完美，计算出的解也可能是不稳定的。

几何直觉：病态矩阵不均匀地拉伸向量，因此方向上的微小扰动在求逆时会被放大。

#### 算法的稳定性

+   如果一个算法能够控制有限精度下的误差增长，则它是数值稳定的。

+   带部分选主的高斯消元是稳定的；没有选主，它可能会失败。

+   正交分解（QR，SVD）通常比消元法更稳定。

数值分析专注于设计算法，保证在机器 epsilon 的几倍内保证精度。

#### 直接方法与迭代方法

1.  直接方法：在有限步内求解（例如，高斯消元、LU 分解、正定系统的 Cholesky 分解）。

    +   对于小/中等问题可靠。

    +   复杂度 ~ $O(n³)$。

1.  迭代方法：生成一系列近似值（例如，雅可比、高斯-赛德尔、共轭梯度）。

    +   对于非常大的稀疏系统很有用。

    +   每次迭代的复杂度 ~ $O(n²)$ 或更少，通常利用稀疏性。

#### 矩阵分解在计算中的应用

许多算法依赖于一次分解矩阵，然后重用它：

+   LU 分解：对于求解多个右端项非常有效。

+   QR 分解：最小二乘法的稳定方法。

+   SVD：对于病态问题，是金标准，尽管成本高昂。

这些分解将重复操作简化为结构化、缓存友好的步骤。

#### 稀疏与稠密计算

+   稠密矩阵：大多数条目非零。使用像 BLAS 和 LAPACK 这样的稠密线性代数包。

+   稀疏矩阵：大多数条目为零。只存储非零项，使用专门的算法以避免不必要的计算。

大规模问题（例如，有限元模拟、网络图）之所以可行，仅仅是因为稀疏方法。

#### BLAS 和 LAPACK：标准库

+   BLAS（基本线性代数子程序）：定义了向量和矩阵操作的核心（点积、矩阵-向量、矩阵-矩阵乘法）。优化的 BLAS 实现利用缓存、SIMD 和多核并行性。

+   LAPACK（线性代数包）：建立在 BLAS 之上，提供求解系统、特征值问题、SVD 等算法。LAPACK 是许多科学计算环境（MATLAB、NumPy、Julia）的骨干。

+   MKL、OpenBLAS、cuBLAS：针对 Intel CPU、开源系统或 NVIDIA GPU 优化的供应商特定实现。

这些库使得代码在分钟内运行和毫秒内运行之间的区别。

#### 浮点数陷阱

1.  累积舍入误差：相加大量不同量级的数字可能会丢弃小的贡献。

1.  正交性丢失：重复的 Gram-Schmidt 正交化而没有重新正交化可能会在数值上漂移。

1.  溢出/下溢：极端的大/小数字超出了可表示的范围。

1.  NaNs 和 Infs：除以零或无效操作会传播错误。

缓解措施：使用数值稳定的算法、缩放输入并检查条件数。

#### 并行和 GPU 计算

现代数值线性代数依赖于并行性：

+   GPU 通过数千个核心加速密集线性代数（cuBLAS、cuSOLVER）。

+   分布式库（ScaLAPACK、PETSc、Trilinos）允许在集群上解决包含数十亿未知数的问题。

+   混合精度方法：在 float32 甚至 float16 中计算，然后在 float64 中细化，平衡速度和精度。

#### 真实世界的应用

+   工程仿真：结构力学、流体动力学依赖于稀疏求解器。

+   机器学习：训练深度网络依赖于优化的 BLAS 进行矩阵乘法。

+   金融：风险模型使用分解协方差矩阵解决巨大的回归问题。

+   大数据：降维（PCA、SVD）需要大规模、稳定的算法。

#### 为什么这很重要

实践中的线性代数不仅仅是关于定理：它关乎将抽象模型转化为在不完美的硬件上可靠运行的计算。数值线性代数提供了确保结果既快速又可靠的必要工具箱——浮点数理解、条件分析、稳定算法和优化库。

#### 试试看

1.  计算几乎奇异的矩阵的条件数（例如，$\begin{bmatrix} 1 & 1 \\ 1 & 1.0001 \end{bmatrix}$）并求解 $Ax=b$。比较单精度和双精度下的结果。

1.  实现带和不带主元的高斯消元法。比较病态矩阵的错误。

1.  使用 NumPy 和 OpenBLAS 来计时大型矩阵乘法；与原始 Python 实现进行比较。

1.  探索迭代求解器：实现稀疏对称正定系统的共轭梯度法。

数值线性代数是数学优雅和计算现实之间的桥梁。它教导我们，在计算机上解方程不仅仅是关于方程——它是关于使它们生动起来的算法、表示和硬件。

### 100. 终极问题集和下一步（精通之路） 

你现在已经走过了线性代数的主要里程碑：向量、矩阵、系统、变换、行列式、特征值、正交性、奇异值分解，以及数据和网络的应用。旅程并没有结束。本节被设计为一个顶点，一个将事物联系在一起并展示你如何继续练习、探索和深化理解的方法。把它看作是你的“下一步”地图。

#### 练习基础知识直到它们变得自然

线性代数一开始可能看起来很复杂，但最简单的练习能建立持久的信心。尝试通过消元法手动解决几个方程组，并注意如何通过主元揭示解决方案存在或不存在的地方。写下一个小矩阵并练习将其乘以一个向量。这可能会感觉机械，但这是你的直觉变得敏锐的方式：每次你将数字通过规则推来推去，你都在学习代数如何重塑空间。

即使是一个单一的概念，比如点积，也能教会你很多。在平面上取两个短向量，计算它们的点积，然后将其与它们之间角度的余弦值进行比较。看到代数与几何相匹配是使线性代数生动起来的原因。

#### 超越计算：理解结构

一旦你对这些机制感到舒适，尝试反思更大的结构。一组向量作为子空间意味着什么？你能判断通过原点的直线是否是一个子空间吗？关于偏离原点的直线呢？这就是你看到的规则和公理开始引导你的推理的地方。

尝试不同的基和坐标：为平面选择两个不同的基，看看一个点根据你使用的“尺子”看起来如何不同。写出基变换矩阵并检查它是否以你预期的方式变换坐标。这些练习表明，线性代数不仅仅是关于数字——它是关于视角的。

#### 在更大的问题中结合思想

当不同的想法碰撞时，真正的乐趣才到来。假设你有一些噪声数据，比如应该沿直线分布的一组散点。尝试使用最小二乘法拟合一条直线。你实际上正在将数据投影到一个子空间上。或者，考虑一个小的马尔可夫链，比如围绕三个或四个节点的随机游走，并计算其长期分布。这个稳定状态是一个隐藏的特征向量。这些综合问题展示了你所学主题之间的联系。

项目使这一点更加生动。例如：

+   在计算机图形学中，编写简单的代码，使用矩阵旋转或反射形状。

+   在网络中，使用拉普拉斯矩阵来识别朋友社交图中的聚类。

+   在推荐系统中，分解小的用户-项目表来预测缺失的评分。

这些不是抽象的谜题——它们展示了线性代数在现实世界中的工作方式。

#### 展望未来：线性代数能引领你走向何方

到现在为止，你知道线性代数不是一个孤立的学科；它是一个基础。下一步取决于你的兴趣。

如果你喜欢计算，数值线性代数是自然的扩展。它深入探讨了浮点数在真实机器上的行为，如何控制舍入误差，以及为什么某些算法比其他算法更稳定。你会了解到为什么带主元的高斯消元法是安全的，而不带主元则可能失败，以及为什么 QR 和 SVD 在敏感应用中被信任。

如果你喜欢抽象，那么抽象线性代数为你打开了大门。在这里，你将超越 $\mathbb{R}^n$ 进入一般的向量空间：多项式作为向量，函数作为向量，对偶空间，最终是张量积。这些思想推动了现代数学和物理学的许多部分。

如果你喜欢数据，统计学和机器学习是自然的路径。协方差矩阵、主成分分析、回归和神经网络都建立在线性代数的基础上。深入理解它们需要你练习的计算和构建的几何洞察力。

如果你好奇心指向科学，线性代数无处不在：在量子力学中，状态是向量，算子是矩阵；在工程中，振动和控制系统依赖于特征值；在计算机图形学中，每个旋转和投影都是一个线性变换。

#### 为什么这个总结性项目很重要

这最后一步与其说是新的定理，不如说是视角。你现在解决的问题——无论是小练习还是大项目——都训练你看到结构，而不仅仅是数字。路线图是开放的，因为线性代数本身也是开放的：一旦你学会了通过它的视角看世界，你就会发现它在无处不在，从网络中的模式到算法的行为，再到空间的几何。

#### 试试你自己

1.  选择一个你关心的数据集——可能是体育比分、你听的歌曲，或者支出记录。将其组织成一个矩阵。计算一些简单的东西：平均值（中心化）、回归线，甚至可能连主成分分析。看看你发现了什么结构。

1.  编写一个短程序，使用消元法求解方程组。在行为良好和几乎奇异的矩阵上测试它。注意稳定性是如何变化的。

1.  绘制一个二维散点图，并用最小二乘法拟合一条线。绘制残差。从几何上讲，残差与线正交意味着什么？

1.  尝试不用公式向朋友解释特征值——只用图片和故事。教学会使它变得真实。

线性代数既是工具也是思维方式。你现在已经足够独立，但道路仍在继续——进入更深的数学，进入实用的计算，以及每天依赖这些思想的科学。这个总结性项目是一个邀请：继续练习，继续探索，让线性代数的结构锐化你看待世界的方式。

#### 结束语

```py
From lines to the stars,
each problem bends, transforms, grows—
paths extend ahead.
```

### 尾声

*一个平静的结尾，其中教训沉淀，代数的音乐在最后一页之后继续回响。*

**1. 平静的反思**

```py
Lessons intertwining,
the book rests, but vectors stretch—
silence holds their song.
```

**2. 无尽之旅**

```py
One map now complete,
yet beyond each line and plane
new horizons call.
```

**3. 结构与增长**

```py
Roots beneath the ground,
branches weaving endless skies,
algebra takes flight.
```

**4. 学习后的光明**

```py
Numbers fade to light,
patterns linger in the mind,
paths remain open.
```

**5. 永恒的运动**

```py
Stillness finds its place,
transformations carry on,
movement without end.
```

**6. 感恩与结束**

```py
Steps of thought complete,
spaces carved with gentle care,
thank you, wandering mind.
```

**7. 未来的回声**

```py
From shadows to form,
each question births new echoes—
the journey goes on.
```

**8. 超越的地平线**

```py
The book closes here,
yet the lines refuse to end,
they stretch toward the stars.
```
