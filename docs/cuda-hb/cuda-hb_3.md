# 第一部分

## 第一章 背景

有很多文章描述了 GPU 在计算领域的革命。我对此一直很感兴趣，因为我很早就参与其中。在 1990 年代中期，我在微软担任 Direct3D 的开发负责人，当时 Intel 和 AMD 正在推出第一个多媒体指令集来加速浮点计算。Intel 曾尝试（但未成功）通过与微软合作推出使用其 MMX 指令集的光栅化器，来阻止 3D 光栅化的时钟周期从他们的 CPU 迁移出去。我知道那项努力注定会失败，因为我们发现，运行在尚未发布的 Pentium 2 处理器上的 MMX 光栅化器，速度只有市面上可用的简易 S3 Virge GX 光栅化器的一半。

对于 Direct3D 6.0，我们与 CPU 厂商合作，将他们的代码集成到我们的几何管线中，以便开发者能够透明地从厂商优化的代码路径中受益，这些代码路径使用了 Intel 和 AMD 的新指令集。游戏开发者积极接受了新的几何管线，但这并未阻止时钟周期从 CPU 迁移到 GPU 的趋势，因为新的指令集被用来生成供 GPU 硬件几何管线使用的顶点数据。

大约在这个时候，GPU 上的晶体管数量超越了 CPU 上的晶体管数量。这个交叉点出现在 1997 年至 1998 年，当时奔腾 2 和 NVIDIA RIVA TNT 的晶体管数量都约为 800 万。此后，Geforce 256（1500 万晶体管）、Geforce 2（2800 万晶体管）和 Geforce 3（6300 万晶体管）上的晶体管数量都超过了当时的 CPU。此外，两者之间的架构差异也逐渐显现：大部分 CPU 的芯片区域被用于缓存，而大部分 GPU 的芯片区域则用于逻辑运算。英特尔能够以微乎其微的面积成本添加显著的新指令集扩展（如 MMX、SSE、SSE2 等），因为它们的 CPU 主要由缓存组成。GPU 则设计用于并行吞吐处理；它们的小缓存更多是为了带宽聚合，而非减少延迟。

虽然像 ATI 和 NVIDIA 这样的公司在构建更快且功能越来越强大的 GPU，但 CPU 厂商依然在推动时钟频率的提升，随着摩尔定律的推动，晶体管预算和时钟速度都在不断增加。第一代奔腾处理器（大约 1993 年）时钟频率为 60MHz，而支持 MMX 技术的奔腾处理器（大约 1997 年）时钟频率为 200MHz。到了十年末，时钟频率已经超过了 1,000MHz。但不久之后，计算机历史上发生了一件重要事件：摩尔定律遇到了瓶颈。晶体管将继续缩小，但时钟频率无法再继续提升。

这一事件并不意外。英特尔的帕特·基尔辛格在 2001 年 IEEE 固态电路会议上发表了主题演讲，并表示，如果芯片继续沿着当前的设计路径发展，到十年末，它们的温度将像核反应堆一样高，而到 2015 年，它们的温度将像太阳表面一样高。未来，性能将必须依赖于“同时多线程”（SMT），这可能通过将多个 CPU 核心集成到单个芯片上来实现。事实上，这正是 CPU 厂商所做的；今天，几乎不可能找到配备单核 CPU 的台式电脑。但由摩尔定律所带来的几十年的“免费 ride”，通过提升时钟频率，使得应用程序能够在软件开发者几乎不需要做任何努力的情况下变得更快，这一时期已经结束。多核 CPU 需要多线程应用程序。只有那些能够从并行计算中受益的应用程序，才能期待从拥有更多核心的 CPU 中获得性能提升。

GPU（图形处理单元）在摩尔定律的这一新趋势中占据了有利位置。虽然没有考虑并行性的 CPU 应用程序需要进行大量重构（如果它们能够变成并行的话），但图形应用程序已经以利用独立像素之间固有并行性的方式进行设计。对于 GPU 来说，通过增加执行核心的数量来提高性能是一个自然的进步。事实上，GPU 设计师倾向于选择更多的核心，而非更强大的核心。他们摒弃了 CPU 制造商理所当然采用的策略，比如最大化时钟频率（GPU 从未，也仍然不会，在接近晶体管制造极限的时钟频率下运行）、投机执行、分支预测和存储转发。而且，为了防止这个日益强大的处理器变得受限于 I/O，GPU 设计师集成了内存控制器，并与 DRAM 制造商合作，启用了远超 CPU 可用带宽的带宽。

但是，这种丰富的计算能力对非图形开发者来说难以利用。一些敢于冒险的人使用图形 API，如 Direct3D 和 OpenGL，将图形硬件转用于执行非图形计算。*GPGPU*（通用 GPU 编程）这一术语便是用来描述这种做法的，但在 CUDA 出现之前，GPU 的计算潜力大多未被开发。Ian Buck，他在斯坦福大学的 Brook 项目使 GPGPU 应用开发变得更加简化，后来加入了 NVIDIA 并领导了新一套开发工具的开发，这些工具能够让非图形应用程序更容易地为 GPU 编写。最终的结果便是 CUDA：NVIDIA 的专有工具链，允许 C 语言程序员通过几个易于使用的语言扩展来为 GPU 编写并行代码。

自 2007 年推出以来，CUDA 一直受到广泛欢迎。成千上万篇使用该技术的学术论文已经被撰写出来。它已经被应用于各种商业软件包，从 Adobe 的 CS5 到 Manifold 的 GIS（地理信息系统）。对于合适的工作负载，支持 CUDA 的 GPU 比当代 CPU 快 5 到 400 倍。这些加速的来源各不相同。有时 GPU 更快是因为它们拥有更多的核心；有时是因为它们有更高的内存带宽；有时则是因为应用程序可以利用 CPU 中没有的 GPU 专用硬件，比如纹理硬件或可以执行快速超越运算的 SFU 单元。并非所有应用程序都可以用 CUDA 实现。事实上，并非所有*并行*应用程序都能用 CUDA 实现。但它已被应用于比其他任何 GPU 计算技术更多样化的应用程序。我希望这本书能帮助成熟的 CUDA 开发者充分发挥 CUDA 的优势。

### 1.1\. 我们的方法

CUDA 是一个很难写的主题。即使不考虑操作系统（Windows、Linux、MacOS）、平台（Tesla 和 Fermi、集成与独立 GPU、多 GPU）、CPU/GPU 并发性以及 CUDA 特定的考虑（例如，必须在使用 CUDA 运行时和驱动程序 API 之间做出选择），并行编程本身就已经非常复杂。当你再加上如何最佳构建 CUDA 内核的复杂性时，这可能会显得让人不知所措。

为了以一种可管理的方式呈现这种复杂性，大多数话题都会从不同的角度多次解释。*纹理映射硬件做什么？* 是与 *如何编写执行纹理映射的内核？* 不同的问题。本书在不同的章节中分别解决这两个问题。异步内存复制操作可以在几种不同的背景下解释：软件抽象之间的交互（例如，参与的主机内存必须被固定）、不同的硬件实现、API 对该特性的支持以及优化策略。读者有时可能希望查阅索引，阅读给定主题的所有不同呈现方式。

优化指南就像建议专栏：很多时候，指导意见没有足够的背景信息，无法有意义地应用，而且它们常常互相矛盾。这个观察并不是贬义的；它只是问题复杂性的一个表现。自从至少 20 年前可以对 CPU 优化做出一刀切的概括以来，GPU 的编程变得更加复杂，因此，期望 CUDA 优化建议能够简单化是不现实的。

此外，GPU 计算如此新颖，以至于 GPU 架构师，更不用说开发者，仍在学习如何最好地编程。对于 CUDA 开发者来说，最终的裁定标准通常是性能，而性能通常是通过墙上时钟时间来衡量的！关于网格和块大小、如何以及何时使用共享内存、每个线程计算多少结果以及占用率对性能的影响等建议，应该通过实施不同的方法并测量每种方法的性能来进行经验验证。

### 1.2\. 代码

开发人员希望 CUDA 代码既能起到说明作用，又不至于成为玩具；有用，但不需要深入探讨一个偏远的技术主题；性能高，但不会掩盖实现者从初步移植到最终版本的过程。为此，本书提供了三种类型的代码示例，旨在解决这些考虑因素：微基准、微演示和优化历程。

#### 1.2.1\. 微基准

微基准旨在说明非常具体的 CUDA 问题的性能影响，例如不合并的内存事务如何降低设备内存带宽，或者 WDDM 驱动程序执行内核调用所需的时间。它们设计为可以独立编译，并且对于许多已经实现过自己微基准的 CUDA 程序员来说会显得非常熟悉。从某种意义上说，我编写了一套微基准，以避免其他人做同样的事情。

#### 1.2.2\. 微演示

微演示是小型应用程序，旨在阐明硬件或软件行为的具体问题。与微基准类似，它们是小型且自包含的，但它们突出的不是性能问题，而是功能性问题。例如，关于纹理章节包括了微演示，展示了如何从 1D 设备内存纹理、如何执行 float→int 转换、不同的纹理寻址模式如何工作，以及纹理执行的线性插值如何受到 9 位权重的影响。

和微基准一样，这些微演示是以开发者可能希望编写它们的精神来提供的，或者至少希望它们可用。我编写了这些代码，以便你无需亲自编写！

#### 1.2.3\. 优化历程

许多关于 CUDA 的论文将其结果呈现为*既成事实*，可能会附带一些关于在最终方案确定之前，探索不同方法之间权衡的旁注。作者通常面临篇幅限制和截止日期的压力，这使得他们难以呈现对其工作的更全面阐述。

对于 CUDA 启用的数据并行编程中的一些核心主题，本书包括了*优化历程*，其精神来源于 Mark Harris 的“优化 CUDA 中的并行归约”演示，带领读者逐步体验七种越来越复杂的实现，以提升性能。^(1) 我们选择用这种方式探讨的主题包括归约、并行前缀和（“扫描”）以及 N 体问题。

1. [`bit.ly/Z2q37x`](http://bit.ly/Z2q37x)

### 1.3\. 管理事项

#### 1.3.1\. 开源

本书附带的源代码可在[www.cudahandbook.com](http://www.cudahandbook.com)获取，它是开源的，并且使用 2 条款 BSD 许可证进行版权保护。^(2)

2. [www.opensource.org/licenses/bsd-license.php](http://www.opensource.org/licenses/bsd-license.php)

#### 1.3.2\. CUDA 手册库（Chlib）

位于源代码的`chLib/`目录中的 CUDA 手册库包含一个可移植的库，支持计时、线程、驱动程序 API 工具等。它们在附录 A 中有更详细的描述。

#### 1.3.3\. 编码风格

不谈括号位置的争论，本书中最引人注目的代码特性是基于`goto`的错误处理机制。执行多个资源分配（或其他可能失败的操作，并且失败应该传递给调用者）的函数，结构上采用初始化/错误检查/清理模式，这与 Linux 内核代码中常用的模式类似。

当出现故障时，所有清理操作将在函数末尾由相同的代码块执行。重要的是，在函数开始时将资源初始化为保证无效的值，以便清理代码知道哪些资源必须释放。如果资源分配或其他函数失败，代码会执行`goto`跳转到清理代码。`chError.h`，在第 A.6 节中描述，定义了 CUDA 运行时和驱动程序 API 的错误处理宏，这些宏实现了这种习惯用法。

#### 1.3.4. CUDA SDK

SDK 是所有 CUDA 开发者的共享体验，因此我们假设你已经安装了 CUDA SDK，并且能够使用它构建 CUDA 程序。SDK 还包括 GLUT（GL 实用库），它是一个方便的库，可以使 OpenGL 应用程序通过相同的代码库针对多种操作系统。GLUT 旨在构建演示质量的应用程序，而非生产质量的应用程序，但它完全满足我们的需求。

### 1.4. 路线图

第一部分中的剩余章节提供了 CUDA 硬件和软件的架构概述。

• 第二章详细介绍了 CUDA 硬件平台和 GPU 本身。

• 第三章同样讨论了 CUDA 软件架构。

• 第四章涵盖了 CUDA 软件环境，包括 CUDA 软件工具和亚马逊 EC2 环境的描述。

在第二部分中，第五章到第十章深入讨论了 CUDA 编程模型的各个方面。

• 第五章讨论了内存，包括设备内存、常量内存、共享内存和纹理内存。

• 第六章介绍了流和事件——用于 CPU 与 GPU 之间、GPU 硬件单元（如复制引擎与流多处理器之间）或离散 GPU 之间进行“粗粒度”并行的机制。

• 第七章涵盖了内核执行，包括 SM 3.5 和 CUDA 5.0 中新引入的动态并行性特性。

• 第八章涵盖了流式多处理器的各个方面。

• 第九章涵盖了多 GPU 应用，包括点对点操作和非常并行的操作，以 N-body 为例。

• 第十章涵盖了 CUDA 纹理处理的各个方面。

最后，在第 III 部分，第十一章到第十五章讨论了各种针对 CUDA 的应用。

• 第十一章描述了带宽受限的流式工作负载，例如向量-向量乘法。

• 第十二章和第十三章描述了归约和并行前缀和（即扫描），这两者是并行编程中的重要构建模块。

• 第十四章描述了 N-body，这是一个重要的应用类别，具有高计算密度，从 GPU 计算中获得了特别的好处。

• 第十五章深入探讨了一种图像处理操作，称为*归一化互相关*，用于特征提取。第十五章展示了本书中唯一使用纹理和共享内存结合来提供最佳性能的代码。

## 第二章 硬件架构

本章提供了关于 CUDA 平台的更详细描述，从系统级别到 GPU 内部的功能单元。第一部分讨论了构建 CUDA 系统的多种方式。第二部分讨论了地址空间以及 CUDA 的内存模型如何在硬件和软件中实现。第三部分讨论了 CPU/GPU 交互，特别关注如何将命令提交给 GPU，以及如何进行 CPU/GPU 同步。最后，本章以对 GPU 本身的高级描述结束：功能单元，如拷贝引擎和流式多处理器，并展示了三代支持 CUDA 的硬件中不同类型流式多处理器的框图。

### 2.1. CPU 配置

本节描述了多种 CPU/GPU 架构，并对 CUDA 开发者如何以不同方式编程系统进行了评论。我们研究了各种 CPU 配置、集成 GPU 和多 GPU 配置。我们从图 2.1 开始。

![图像](img/02fig01.jpg)

*图 2.1* CPU/GPU 架构简化图。

在图 2.1 中省略了一个重要元素，即连接 CPU 和外部世界的“芯片组”或“核心逻辑”。系统的每一位输入和输出，包括磁盘和网络控制器、键盘和鼠标、USB 设备，当然还有 GPU，都会经过芯片组。直到最近，芯片组被划分为连接大多数外设的“南桥”（southbridge）和包含图形总线（加速图形端口，直到 PCI Express [PCIe] 总线取而代之）以及连接到 CPU 内存的内存控制器（“前端总线”）的“北桥”（northbridge）。

1。为了简化起见，本节中的所有图示均省略了南桥部分。

在 PCI Express 2.0 中，每个“通道”理论上可以提供约 500MB/s 的带宽，给定外设的通道数可以是 1、4、8 或 16。GPU 是平台上带宽需求最大的外设，因此它们通常设计为插入 16 通道的 PCIe 插槽中。考虑到数据包开销，虽然这种连接的带宽为 8G/s，但实际可用带宽大约为 6G/s。^(2)

2. PCI 3.0 提供的带宽大约是 PCIe 2.0 的两倍。

#### 2.1.1\. 前端总线

图 2.2 在原始简化图中增加了北桥及其内存控制器。为完整起见，图 2.2 还展示了 GPU 的集成内存控制器，它是在与 CPU 内存控制器完全不同的约束条件下设计的。GPU 必须支持所谓的*同步*客户端，例如视频显示器，它们的带宽需求是固定的且不可协商。GPU 的内存控制器还特别考虑了 GPU 对极高延迟容忍度和巨大的内存带宽需求的设计。截止目前，顶级 GPU 通常提供超过 100G/s 的本地 GPU 内存带宽。GPU 内存控制器总是与 GPU 集成，因此在本章其余图示中未显示。

![图片](img/02fig02.jpg)

*图 2.2* CPU/GPU 架构—北桥。

#### 2.1.2\. 对称多处理器

图 2.3 展示了一个传统北桥配置的多 CPU 系统^(3)。在多核处理器出现之前，应用程序必须使用多个线程才能充分利用多个 CPU 的额外计算能力。北桥必须确保每个 CPU 都能看到相同的一致内存视图，尽管每个 CPU 和北桥本身都包含缓存。由于这些所谓的“对称多处理器”（SMP）系统共享到 CPU 内存的公共路径，内存访问表现出相对一致的性能。

3。由于一些即将阐明的原因，我们提供图 2.3 更多是为了历史参考，而不是因为有具有这种配置的 CUDA 计算机。

![图片](img/02fig03.jpg)

*图 2.3* 多个 CPU（SMP 配置）。

#### 2.1.3\. 非统一内存访问

从 AMD 的 Opteron 和 Intel 的 Nehalem (i7) 处理器开始，北桥中的内存控制器被直接集成到 CPU 中，如图 2.4 所示。这一架构变更提高了 CPU 的内存性能。

![图片](img/02fig04.jpg)

*图 2.4* 集成内存控制器的 CPU。

对于开发者，图 2.4 中的系统与我们已经讨论的系统仅有细微的差别。对于包含多个 CPU 的系统，如图 2.5 所示，情况变得更加有趣。

![图片](img/02fig05.jpg)

*图 2.5* 多个 CPU（NUMA）。

对于具有多个 CPU 的机器配置^(4)，这一架构意味着每个 CPU 都拥有自己的内存带宽池。与此同时，由于多线程操作系统和应用程序依赖于先前的 CPU 和北桥配置强制执行的缓存一致性，Opteron 和 Nehalem 架构分别引入了 HyperTransport (HT) 和 QuickPath Interconnect (QPI)。

4。在这种系统上，CPU 也可能被称为“节点”或“插槽”。

HT 和 QPI 是点对点互连，它们将 CPU 连接到其他 CPU，或将 CPU 连接到 I/O 中心。在包含 HT/QPI 的系统中，任何 CPU 都可以访问任何内存位置，但访问“本地”内存位置的速度要快得多，这些本地内存的物理地址直接连接到 CPU。非本地访问通过使用 HT/QPI 来窥探其他 CPU 的缓存，逐出任何缓存的请求数据副本，并将数据传送到执行内存请求的 CPU。一般而言，这些 CPU 上巨大的片上缓存缓解了这些非本地内存访问的成本；请求的 CPU 可以将数据保留在自己的缓存层级中，直到另一个 CPU 请求该内存。

为了帮助开发者绕过这些性能陷阱，Windows 和 Linux 引入了 API，使应用程序能够将其分配引导到特定的 CPU，并设置 CPU “线程亲和性”，以便操作系统将线程调度到 CPU 上，使它们的大部分或所有内存访问都是本地的。

一个有决心的程序员可以利用这些 API 编写特殊代码，暴露 NUMA 性能的漏洞，但更常见（且隐蔽！）的症状是由于“虚假共享”导致的性能下降，其中在不同 CPU 上运行的两个线程通过访问位于同一缓存行的内存位置，导致大量 HT/QPI 事务。因此，NUMA API 必须谨慎使用：尽管它们为程序员提供了提高性能的工具，但也可能使开发者很容易给自己带来性能问题。

减轻非本地内存访问性能影响的一种方法是启用*内存交错*，即物理内存在所有 CPU 之间按缓存行边界均匀划分。^(5) 对于 CUDA 来说，这种方法在按照 图 2.5 所示的系统中效果良好，该系统在 NUMA 配置下有多个 CPU，并通过共享 I/O 集线器与 GPU(s) 相连。然而，由于 PCI Express 带宽常常成为整体应用性能的瓶颈，许多系统拥有独立的 I/O 集线器来服务多个 PCI Express 总线，如 图 2.6 所示。

5。一个愤世嫉俗者可能会说，这让所有的内存访问“同样糟糕”。

![Image](img/02fig06.jpg)

*图 2.6* 多核 CPU（NUMA 配置），多个总线。

为了在这种“亲和性”系统上良好运行，CUDA 应用程序必须小心使用 NUMA API，将内存分配和线程亲和性与连接到给定 GPU 的 PCI Express 总线匹配。否则，GPU 发起的内存拷贝将是非本地的，并且内存事务需要通过 HT/QPI 互连多“跳”一次。由于 GPU 对带宽的需求非常大，这些 DMA 操作会减少 HT/QPI 执行其主要任务的能力。与虚假共享相比，非本地 GPU 内存拷贝对性能的影响是 CUDA 应用程序更为显著的性能风险。

#### 2.1.4\. PCI Express 集成

英特尔的 Sandy Bridge 类处理器通过将 I/O 集线器集成到 CPU 中，迈出了系统集成的又一步，如 图 2.7 所示。单个 Sandy Bridge CPU 可拥有最多 40 条 PCI Express 带宽（记住，一个 GPU 最多使用 16 条带宽，因此 40 条足够支持两个以上的全尺寸 GPU）。

![Image](img/02fig07.jpg)

*图 2.7* 集成 PCI Express 的多核 CPU。

对于 CUDA 开发者而言，PCI Express 集成带来了坏消息和好消息。坏消息是 PCI Express 流量总是具有亲和性。设计者无法像图 2.5 中的系统那样，使用单一 I/O 中心来服务多个 CPU；所有多 CPU 系统都类似于图 2.6。因此，关联到不同 CPU 的 GPU 无法执行点对点操作。好消息是 CPU 缓存可以参与 PCI Express 总线流量：CPU 可以从缓存中服务 DMA 读取请求，GPU 的写入操作会被提交到 CPU 缓存。

### 2.2\. 集成 GPU

这里，*集成*的意思是“集成到芯片组中”。如图 2.8 所示，之前仅属于 CPU 的内存池现在与集成到芯片组中的 GPU 共享。NVIDIA 具有 CUDA 能力的芯片组示例包括 MCP79（用于笔记本电脑和上网本）和 MCP89。MCP89 是 NVIDIA 将制造的最后一款也是最强大的 CUDA 能力 x86 芯片组；除了集成 L3 缓存外，它还拥有 MCP7x 芯片组的 3 倍 SM 数。

![Image](img/02fig08.jpg)

*图 2.8* 集成 GPU。

CUDA 的映射固定内存 API 在集成 GPU 上具有特殊意义。这些 API 将主机内存分配映射到 CUDA 内核的地址空间，以便它们可以直接访问，这也被称为“零拷贝”，因为内存是共享的，不需要通过总线进行复制。实际上，对于传输受限的工作负载，集成 GPU 的性能可以超越更大的独立 GPU。

“写合并”内存分配对集成 GPU 也有重要意义；CPU 对该内存的缓存窥探会被禁止，这在 GPU 访问内存时能够提高性能。当然，如果 CPU 从该内存读取数据，则会出现通常的 WC 内存性能惩罚。

集成 GPU 与离散 GPU 并不互相排斥；MCP7x 和 MCP89 芯片组提供了 PCI Express 连接（图 2.9）。在这样的系统上，CUDA 更倾向于在离散 GPU 上运行，因为大多数 CUDA 应用程序都是以离散 GPU 为设计目标的。例如，设计为在单一 GPU 上运行的 CUDA 应用程序将自动选择离散 GPU。

![Image](img/02fig09.jpg)

*图 2.9* 集成 GPU 与离散 GPU。

CUDA 应用程序可以通过检查`cudaDeviceProp.integrated`或传递`CU_DEVICE_ATTRIBUTE_INTEGRATED`给`cuDeviceGetAttribute()`来查询 GPU 是否是集成的。

对于 CUDA 来说，集成 GPU 并不罕见；数百万台计算机都配备了集成的、支持 CUDA 的 GPU，但它们有些像是一种新奇事物，在几年后它们将成为过时的产物，因为 NVIDIA 已退出了 x86 芯片组业务。也就是说，NVIDIA 已经宣布计划推出将支持 CUDA 的 GPU 与 ARM CPU 集成的系统级芯片（SOC），可以肯定的是，零拷贝优化将在这些系统上表现良好。

### 2.3 多个 GPU

本节探讨了如何在系统中安装多个 GPU 以及这对 CUDA 开发者的影响。为了本讨论的目的，我们将省略 GPU 内存的内容。假设每个 GPU 都连接到其专用内存。

大约在 2004 年，NVIDIA 推出了“SLI”（可扩展链接接口）技术，使多个 GPU 能够并行工作以提供更高的图形性能。通过可以容纳多个 GPU 板卡的主板，最终用户可以通过在系统中安装两个 GPU 来几乎翻倍其图形性能（见图 2.10）。默认情况下，NVIDIA 驱动程序会将这些板卡配置为像单个高速 GPU 一样工作，以加速图形 API，如 Direct3D 和 OpenGL。打算使用 CUDA 的最终用户必须在 Windows 的显示控制面板中显式启用它。

![Image](img/02fig10.jpg)

*图 2.10* 多插槽中的 GPU。

也可以构建包含多个 GPU 的 GPU 板卡（见图 2.11）。这类板卡的示例包括 GeForce 9800GX2（双 G92）、GeForce GTX 295（双 GT200）、GeForce GTX 590（双 GF110）和 GeForce GTX 690（双 GK104）。这些板卡上的 GPU 之间唯一的共享资源是桥接芯片，它使得两个芯片能够通过 PCI Express 进行通信。它们不共享内存资源；每个 GPU 都有一个集成的内存控制器，能够为连接到该 GPU 的内存提供全带宽性能。板卡上的 GPU 可以通过点对点内存复制（peer-to-peer memcpy）进行通信，使用桥接芯片绕过主 PCIe 架构。此外，如果它们是 Fermi 架构或更新的 GPU，每个 GPU 可以将属于另一个 GPU 的内存映射到其全局地址空间中。

![Image](img/02fig11.jpg)

*图 2.11* 多 GPU 板卡。

SLI 是 NVIDIA 的一项技术，使多个 GPU（通常在同一块板上，如图 2.11 所示）看起来像一个更快的单一 GPU。当图形应用程序下载纹理或其他数据时，NVIDIA 图形驱动程序会将数据广播到两个 GPU；大多数渲染命令也会被广播，并进行小的更改以使每个 GPU 能够渲染其输出缓冲区的一部分。由于 SLI 使多个 GPU 看起来像一个单一的 GPU，并且由于 CUDA 应用程序无法像图形应用程序那样透明加速，因此 CUDA 开发人员通常应禁用 SLI。

该板设计超额订阅了可用于 GPU 的 PCI Express 带宽。由于板上的两个 GPU 仅可用一个 PCI Express 插槽的带宽，因此传输受限工作负载的性能可能会受到影响。如果有多个 PCI Express 插槽可用，最终用户可以安装多个双 GPU 板。图 2.12 显示了一台配备四个 GPU 的机器。

![图像](img/02fig12.jpg)

*图 2.12* 多个插槽中的多 GPU 板。

如果有多个 PCI Express I/O 集线器，如图 2.6 中的系统，NUMA 系统的放置和线程亲和性考虑同样适用于这些板，就像它们适用于插入该配置的单 GPU 板一样。

如果芯片组、主板、操作系统和驱动程序软件能够支持，系统中可以安装更多的 GPU。安特卫普大学的研究人员在将四个 GeForce 9800GX2 插入一台桌面计算机时，构建了一个名为 FASTRA 的 8-GPU 系统，引起了轰动。基于双 PCI Express 芯片组构建的类似系统将如图 2.13 所示。

![图像](img/02fig13.jpg)

*图 2.13* 多 GPU 板，多个 I/O 集线器。

作为附注，点对点内存访问（即其他 GPU 设备内存的映射，而不是 memcpy）无法跨越 I/O 集线器，或者在集成 PCI Express 的 CPU（如 Sandy Bridge）的情况下，无法跨越插槽。

### 2.4. CUDA 中的地址空间

每个初学 CUDA 的程序员都知道，CPU 和 GPU 的地址空间是分开的。CPU 无法读取或写入 GPU 的设备内存，反之，GPU 也无法读取或写入 CPU 的内存。因此，应用程序必须显式地将数据复制到 GPU 内存并从 GPU 内存中复制数据，以便进行处理。

现实情况要复杂一些，随着 CUDA 增加了映射固定内存和点对点访问等新功能，情况变得更加复杂。本节将从基本原理出发，详细描述 CUDA 中地址空间的工作原理。

#### 2.4.1. 虚拟寻址：简史

虚拟地址空间是一种普遍且成功的抽象，绝大多数程序员每天都在使用并受益于它们，却从未意识到它们的存在。它们是最初的洞察的延伸，即将连续的数字分配给计算机中的内存位置是有用的。标准的计量单位是*字节*，例如，一台具有 64K 内存的计算机，其内存位置是 0..65535。用于指定内存位置的 16 位值称为*地址*，而计算地址并对相应内存位置进行操作的过程统称为*寻址*。

早期计算机执行*物理寻址*。它们会计算一个内存位置，然后读取或写入相应的内存位置，如图 2.14 所示。随着软件变得越来越复杂，以及托管多个用户或运行多个任务的计算机变得越来越普遍，显然允许任何程序读取或写入任何物理内存位置是不可接受的；在机器上运行的软件可能会通过写入错误的内存位置致命地损坏其他软件。除了稳健性问题，还有安全性问题：软件可以通过读取它不“拥有”的内存位置来监视其他软件。

![图像](img/02fig14.jpg)

*图 2.14* 简单的 16 位地址空间。

因此，现代计算机实现了*虚拟地址空间*。每个程序（操作系统设计者称之为*进程*）获得一个类似于图 2.14 的内存视图，但每个进程都有自己的地址空间。它们不能在没有操作系统特别许可的情况下读取或写入属于其他进程的内存。机器指令指定一个*虚拟地址*，该地址通过执行一系列查找操作在操作系统设置的表中转换为物理地址。

在大多数系统中，虚拟地址空间被划分为*页*，这些是至少为 4096 字节的寻址单位。硬件不是直接从地址引用物理内存，而是查找一个*页表项*（PTE），该项指定页面内存所在的物理地址。

从图 2.15 可以清楚地看到，虚拟地址映射使得一个连续的虚拟地址空间能够映射到物理内存中的不连续页面。此外，当一个应用程序尝试读取或写入一个其页面尚未映射到物理内存的内存位置时，硬件会发出一个故障信号，操作系统必须处理该故障。

![图片](img/02fig15.jpg)

*图 2.15* 虚拟地址空间。

顺便提一下：实际上，没有硬件实现单级页表，如图 2.15 所示。至少，地址被拆分成两个索引：一个索引用于“页目录”中的页表，另一个索引用于通过第一个索引选择的页表。分层设计减少了页表所需的内存量，并使得不活动的页表可以被标记为非驻留并交换到磁盘，就像不活动的页面一样。

除了物理内存位置，PTE 还包含权限位，硬件在进行地址转换时可以验证这些权限。例如，操作系统可以将页面设置为只读，如果应用程序尝试写入该页面，硬件会发出故障信号。

操作系统利用虚拟内存硬件实现许多功能。

• *懒惰分配*：可以通过预留没有物理内存支持的 PTE 来“分配”大量内存。如果请求内存的应用程序访问了其中的某个页面，操作系统会在那时通过找到一页物理内存来解决故障。

• *按需分页*：内存可以被复制到磁盘，并且该页面标记为非驻留。如果内存再次被引用，硬件会发出“页面故障”信号，操作系统通过将数据复制到物理页面、修正页面表项（PTE）以指向该页面并恢复执行来解决故障。

• *写时复制*：可以通过创建第二组 PTE（页表项），这些 PTE 映射到相同的物理页面，然后将两组 PTE 标记为只读，从而“复制”虚拟内存。如果硬件检测到尝试写入这些页面之一的行为，操作系统可以将其复制到另一个物理页面，重新标记两组 PTE 为可写，并恢复执行。如果应用程序只对“复制”的页面中的一小部分进行写操作，写时复制将带来显著的性能提升。

• *映射文件 I/O*：文件可以映射到地址空间，并且通过访问文件可以解决页面错误。对于执行文件随机访问的应用程序，将内存管理委托给操作系统中高度优化的 VMM 代码可能是有利的，特别是因为它与大容量存储驱动程序紧密耦合。

重要的是要理解，地址转换会在 CPU 执行的*每一个*内存访问中进行。为了使这个操作快速，CPU 包含了特殊的硬件：称为转换旁路缓冲区（TLB）的缓存，用于存储最近翻译的地址范围，以及“页面遍历器”，它通过读取页表来解决 TLB 中的缓存缺失。^(6) 现代 CPU 还包括对“统一地址空间”的硬件支持，其中多个 CPU 可以通过 AMD 的 HT（超传输）和 Intel 的快速路径互连（QPI）高效地访问彼此的内存。由于这些硬件设施使得 CPU 可以使用统一的地址空间访问系统中的任何内存位置，本节将不管系统中有多少个 CPU，均称其为“CPU”及“CPU 地址空间”。

6。可以编写程序（针对 CPU 和 CUDA），暴露 TLB 的大小和结构以及/或页面遍历器的内存开销，只要它们在短时间内跨越足够的内存。

* * *

侧边栏：内核模式与用户模式

关于 CPU 内存管理的最后一点是，操作系统代码必须使用内存保护，以防止应用程序破坏操作系统自身的数据结构——例如，控制地址转换的页表。为了辅助此内存保护，操作系统有一个“特权”执行模式，当它执行关键系统功能时使用此模式。为了管理低级硬件资源，如页表，或编程硬件寄存器，例如磁盘、网络控制器或 CUDA GPU 的外围设备，CPU 必须在*内核模式*下运行。应用程序代码使用的无特权执行模式被称为*用户模式*。^(7) 除了操作系统提供者编写的代码外，控制硬件外围设备的低级驱动程序代码也运行在内核模式下。由于内核模式代码中的错误可能导致系统稳定性或安全问题，因此内核模式代码的质量标准更高。此外，许多操作系统服务，如映射文件 I/O 或其他内存管理功能，在内核模式下不可用。

7. x86 特定的*内核模式*和*用户模式*术语分别是“Ring 0”和“Ring 3”。

为了确保系统的稳定性和安全性，用户模式和内核模式之间的接口受到严格监管。用户模式代码必须在内存中设置一个数据结构，并进行一个特殊的系统调用来验证内存和请求。这个从用户模式到内核模式的转换被称为*内核抽象*。内核抽象代价高昂，有时 CUDA 开发人员必须考虑其成本。

用户模式驱动程序与 CUDA 硬件的每次交互都由内核模式代码调解。这通常意味着需要为它分配资源——不仅仅是内存，还有硬件资源，例如用户模式驱动程序用于向硬件提交工作任务的硬件寄存器。

CUDA 驱动程序的大部分运行在用户模式下。例如，为了分配页面锁定的系统内存（例如，使用`cudaHostAlloc()`函数），CUDA 应用程序调用用户模式的 CUDA 驱动程序，该驱动程序向内核模式的 CUDA 驱动程序发出请求，并执行内核转换。内核模式的 CUDA 驱动程序使用低级操作系统服务（例如，它可能调用系统服务来映射 GPU 硬件寄存器）和硬件特定代码（例如，编程 GPU 的内存管理硬件）来满足请求。

* * *

#### 2.4.2\. 不连续地址空间

在 GPU 上，CUDA 也使用虚拟地址空间，尽管硬件不像 CPU 那样支持丰富的功能集。GPU 确实执行内存保护，因此 CUDA 程序无法意外读取或损坏其他 CUDA 程序的内存，也无法访问内核模式驱动程序尚未为其映射的内存。但是，GPU 不支持按需分页，因此 CUDA 分配的每个虚拟内存字节必须由一个物理内存字节支持。此外，按需分页是操作系统用来实现上述大多数功能的底层硬件机制。

由于每个 GPU 都有自己的内存和地址转换硬件，CUDA 地址空间与运行 CUDA 应用程序主机代码的 CPU 地址空间是分开的。图 2.16 显示了 CUDA 1.0 版本的地址空间架构，即在映射固定内存可用之前。CPU 和 GPU 各自有自己的地址空间，并使用各自的页表进行映射。两个设备通过显式的 memcpy 命令交换数据。GPU 可以分配*固定内存*——GPU 为 DMA 映射的页面锁定内存——但固定内存仅仅是加速 DMA；它并不使 CUDA 内核能够访问主机内存。^(8)

8。在 32 位操作系统上，支持 CUDA 的 GPU 可以在一个 40 位的地址空间中映射固定内存，以便在 CUDA 内核使用的地址空间之外执行 memcpy 操作。

![Image](img/02fig16.jpg)

*图 2.16* 不同的地址空间。

CUDA 驱动程序跟踪固定内存范围，并自动加速引用这些内存的 memcpy 操作。异步 memcpy 调用需要固定内存范围，以确保操作系统在执行 memcpy 操作之前不会取消映射或移动物理内存。

并非所有 CUDA 应用程序都能分配它们希望通过 CUDA 处理的主机内存。例如，某个大型扩展应用程序的 CUDA 感知插件可能希望在非 CUDA 感知代码分配的主机内存上进行操作。为了适应这种用例，CUDA 4.0 增加了注册现有主机地址范围的功能，这会将虚拟地址范围页锁定，映射到 GPU 上，并将该地址范围添加到跟踪数据结构中，以便 CUDA 知道它已被固定。然后，可以将内存传递给异步 memcpy 调用，或以它由 CUDA 分配的方式进行处理。

#### 2.4.3\. 映射固定内存

CUDA 2.2 增加了一个名为 *映射固定内存* 的特性，如 图 2.17 所示。映射固定内存是页锁定的主机内存，已经映射到 CUDA 地址空间，在这里 CUDA 内核可以直接读取或写入它。CPU 和 GPU 的页表都被更新，使得 CPU 和 GPU 都有指向同一主机内存缓冲区的地址范围。由于地址空间不同，因此必须使用 `cuMemHostGetDevicePointer()/cudaHostGetDevicePointer()` 查询 GPU 指针指向的缓冲区。^(9)

9. 对于多 GPU 配置，CUDA 2.2 还添加了一个名为“可移植”固定内存的功能，导致分配被映射到每个 GPU 的地址空间。但是不能保证`cu(da)HostGetDevicePointer()`会为不同的 GPU 返回相同的值！

![Image](img/02fig17.jpg)

*图 2.17* 映射的固定内存。

#### 2.4.4\. 可移植固定内存

CUDA 2.2 还启用了一个名为*可移植固定内存*的功能，如图 2.18 所示。使固定内存“可移植”会导致 CUDA 驱动程序将其映射到系统中*所有*GPU，而不仅仅是当前上下文的那个 GPU。为 CPU 和系统中的每个 GPU 创建了一组独立的页表条目，使得相应的设备能够将虚拟地址转换为底层物理内存。主机内存范围也会被添加到每个活动 CUDA 上下文的跟踪机制中，因此每个 GPU 都能将可移植分配识别为固定内存。

![Image](img/02fig18.jpg)

*图 2.18* 可移植的映射固定内存。

图 2.18 可能代表了开发人员对于多个地址空间的容忍度的极限。在这里，一个 2 个 GPU 的系统为一个分配提供了 3 个地址；一个 4 个 GPU 的系统则会提供 5 个地址。尽管 CUDA 有快速的 API 可以查找给定的 CPU 地址范围并返回相应的 GPU 地址范围，但在一个 N 个 GPU 的系统上，拥有 N+1 个地址，且这些地址都指向同一个分配，至少可以说是非常不便的。

#### 2.4.5\. 统一寻址

对于 32 位 CUDA GPU，需要多个地址空间，因为它们只能映射 2³²=4GiB 的地址空间；由于一些高端 GPU 拥有最多 4GiB 的设备内存，它们很难完全映射所有设备内存，同时还要映射任何固定内存，更不用说使用与 CPU 相同的地址空间了。但是，在具有 Fermi 或更高版本 GPU 的 64 位操作系统上，可以实现更简化的抽象。

CUDA 4.0 增加了一个叫做*统一虚拟寻址*（UVA）的功能，如图 2.19 所示。当 UVA 生效时，CUDA 从同一虚拟地址空间为 CPU 和 GPU 分配内存。CUDA 驱动程序通过其初始化例程从 CPU 地址空间进行大规模虚拟分配（这些分配不由物理内存支持），然后将 GPU 的分配映射到这些地址范围。由于 x64 CPU 支持 48 位虚拟地址空间^10，而 CUDA GPU 仅支持 40 位，因此使用 UVA 的应用程序应确保 CUDA 尽早初始化，以防 CPU 代码使用 CUDA 所需的虚拟地址。

10。48 位虚拟地址空间 = 256TB。未来的 x64 CPU 将支持更大的地址空间。

![图片](img/02fig19.jpg)

*图 2.19* 统一虚拟寻址（UVA）。

对于映射的固定内存分配，GPU 和 CPU 的指针是相同的。对于其他类型的内存分配，CUDA 可以从地址推断出给定分配所执行的设备。因此，线性内存拷贝函数（如指定了方向的`cudaMemcpy()`、`cuMemcpyHtoD()`、`cuMemcpyDtoH()`等）已被简化的`cuMemcpy()`和`cudaMemcpy()`函数替代，这些简化版函数不再需要指定内存方向。

UVA 在支持 UVA 的系统上自动启用。在撰写本文时，UVA 在 64 位 Linux、64 位 MacOS 和 64 位 Windows 上使用 TCC 驱动程序时启用；WDDM 驱动程序尚不支持 UVA。当 UVA 生效时，CUDA 执行的所有固定分配既是映射的，也是可移植的。请注意，对于使用`cuMemRegisterHost()`固定的系统内存，设备指针仍需使用`cu(da)HostGetDevicePointer()`查询。即使 UVA 生效，CPU 仍无法访问设备内存。此外，默认情况下，GPU 不能访问彼此的内存。

#### 2.4.6\. 同伴对等映射

在我们探索 CUDA 虚拟内存抽象的最后阶段，我们讨论了设备内存的点对点映射，如图 2.20 所示。点对点映射使得 Fermi 类 GPU 可以读取或写入另一个 Fermi 类 GPU 上的内存。点对点映射仅在支持 UVA 的平台上受支持，并且只适用于连接到同一 I/O 集线器的 GPU。由于在使用点对点时 UVA 始终处于启用状态，不同设备的地址范围不会重叠，驱动程序（及运行时）可以从指针值推断出拥有设备。

![图片](img/02fig20.jpg)

*图 2.20* 点对点。

点对点内存寻址是不对称的；请注意，图 2.20 显示了一个不对称的映射，其中 GPU 1 的分配对 GPU 0 可见，但反之则不可见。为了让 GPU 互相查看彼此的内存，每个 GPU 必须显式地映射对方的内存。管理点对点映射的 API 函数在第 9.2 节中进行了讨论。

### 2.5. CPU/GPU 交互

本节描述了 CPU 与 GPU 交互的关键要素。

• *固定主机内存：* CPU 内存，GPU 可以直接访问

• *命令缓冲区：* CUDA 驱动程序写入并由 GPU 读取的缓冲区，用于控制其执行

• *CPU/GPU 同步：* CPU 如何追踪 GPU 的进度

本节从硬件层面描述这些功能，仅在必要时引用 API，以帮助读者理解它们与 CUDA 开发的关系。为了简化，本文使用了图 2.1 中的 CPU/GPU 模型，暂时忽略了多 CPU 或多 GPU 编程的复杂性。

#### 2.5.1. 固定主机内存和命令缓冲区

出于显而易见的原因，CPU 和 GPU 各自最擅长访问自己的内存，但 GPU 可以通过直接内存访问（DMA）直接访问被页面锁定的 CPU 内存。页面锁定是操作系统提供的一种功能，用于使硬件外设能够直接访问 CPU 内存，从而避免了不必要的拷贝。“锁定”的页面已被操作系统标记为不可驱逐，因此设备驱动程序可以编程让这些外设使用页面的物理地址直接访问内存。CPU 仍然可以访问相关内存，但内存不能移动或换出到磁盘。

由于 GPU 是与 CPU 分开的设备，直接内存访问还使 GPU 可以独立于 CPU 执行，且与 CPU 执行并行地读写 CPU 内存。在 CPU 和 GPU 之间必须小心同步，以避免竞态条件，但对于那些能够在 GPU 处理时有效利用 CPU 时钟周期的应用程序，并行执行的性能优势可能非常显著。

图 2.21 展示了一个由 GPU 映射的“固定”缓冲区，以便直接访问。CUDA 程序员对固定缓冲区很熟悉，因为 CUDA 一直提供通过 `cudaMallocHost()` 等 API 分配固定内存的能力。但在背后，这种缓冲区的主要应用之一是向 GPU 提交命令。CPU 将命令写入一个 GPU 可消费的“命令缓冲区”，GPU 同时读取并执行之前写入的命令。图 2.22 展示了 CPU 和 GPU 如何共享这个缓冲区。这个示意图被简化了，因为命令可能有几百字节长，且缓冲区足够大，能够容纳几千个这样的命令。缓冲区的“前沿”由 CPU 构建，尚未准备好供 GPU 读取；缓冲区的“尾端”正被 GPU 读取。中间的命令在 GPU 准备好时可以处理。

11. 重要说明：在这个上下文中，GPU 的“映射”涉及设置硬件表格，指向 CPU 内存的物理地址。内存可能已被映射到可以由 CUDA 内核访问的地址空间，也可能没有。

![图片](img/02fig21.jpg)

*图 2.21* 固定缓冲区。

![图片](img/02fig22.jpg)

*图 2.22* CPU/GPU 命令缓冲区。

通常，CUDA 驱动程序会重用命令缓冲区内存，因为一旦 GPU 完成处理一个命令，该内存就可以再次被 CPU 写入。图 2.23 展示了 CPU 如何“循环”命令缓冲区。

![图片](img/02fig23.jpg)

*图 2.23* 命令缓冲区循环。

由于启动一个 CUDA 内核需要几千个 CPU 时钟周期，因此 CPU/GPU 并发的一个关键使用场景是，在 GPU 正在处理时，CPU 只是准备更多的 GPU 命令。那些未能平衡以使 CPU 和 GPU 都保持繁忙的应用程序可能会变得“CPU 绑定”或“GPU 绑定”，分别如图 2.24 和 2.25 所示。在 CPU 绑定的应用程序中，GPU 准备好并等待处理下一个命令；在 GPU 绑定的应用程序中，CPU 已经完全填充了命令缓冲区，必须等待 GPU 才能写入下一个 GPU 命令。一些应用程序天生就是 CPU 绑定或 GPU 绑定的，因此 CPU 和 GPU 绑定并不一定表示应用程序结构上有根本问题。然而，知道应用程序是 CPU 绑定还是 GPU 绑定，有助于突出性能提升的机会。

![Image](img/02fig24.jpg)

*图 2.24* GPU 绑定的应用程序。

![Image](img/02fig25.jpg)

*图 2.25* CPU 绑定的应用程序。

#### 2.5.2\. CPU/GPU 并发

前一节介绍了 CUDA 系统中最粗粒度的并行性：*CPU/GPU 并发*。所有的 CUDA 内核启动都是异步的：CPU 通过将命令写入命令缓冲区请求启动，然后返回而不检查 GPU 的进度。内存复制也可以选择异步进行，这样可以实现 CPU/GPU 并发，并可能使内存复制与内核处理并行执行。

##### 阿姆达尔定律

当 CUDA 程序编写正确时，CPU 和 GPU 可以完全并行工作，可能将性能提升一倍。受 CPU 或 GPU 限制的程序不会从 CPU/GPU 并发中获益太多，因为即使另一个设备并行工作，CPU 或 GPU 也会限制性能。这个模糊的观察可以通过 *阿姆达尔定律* 来具体化，这一定律首次由吉恩·阿姆达尔（Gene Amdahl）在 1967 年的论文中提出。^12 阿姆达尔定律通常总结如下。

12. [`bit.ly/13UqBm0`](http://bit.ly/13UqBm0)

![Image](img/036equ01.jpg)

其中 r[s] + r[p] = 1，r[s] 表示顺序部分的比例。当考察诸如 CPU/GPU 并发等小规模性能机会时，这种公式显得有些笨拙。将方程式重新排列如下

![Image](img/036equ02.jpg)

清晰地显示，当 r[p] = 1 时，速度提升为 *N*x。如果有一个 CPU 和一个 GPU（*N* = 2），那么完全并发下的最大加速是 2x；对于平衡负载（如视频转码），CPU 可以执行顺序操作（如变长解码），并与 GPU 执行并行操作（如像素处理）并行进行，这几乎可以实现。但是对于更多受 CPU 或 GPU 限制的应用来说，这种并发形式提供的好处有限。

阿姆达尔的论文旨在警示那些相信并行性能够解决性能问题的读者，我们在本书的其他部分也会引用它，讨论 GPU 内部并发、多 GPU 并发以及从 CUDA 核心迁移中能获得的加速效果。然而，了解哪些并发形式不会为特定应用带来任何好处，反而可能赋予开发者一种权力，让他们可以将时间用于探索其他提高性能的途径。

##### 错误处理

CPU/GPU 并发也对错误处理产生影响。如果 CPU 启动了十几个内核，而其中一个导致了内存故障，CPU 直到执行了 CPU/GPU 同步（在下一节中描述）后，才能发现故障。开发者可以通过调用`cudaThreadSynchronize()`或`cuCtxSynchronize()`手动执行 CPU/GPU 同步，其他函数如`cudaFree()`或`cuMemFree()`也可能会作为副作用触发 CPU/GPU 同步。*CUDA C 编程指南*通过列出可能引起 CPU/GPU 同步的函数来提到这一行为：“请注意，此函数可能还会返回来自之前异步启动的错误代码。”

根据当前的 CUDA 实现，如果发生故障，就无法知道是哪个内核引起了故障。对于调试代码，如果通过同步很难隔离故障，开发者可以设置 `CUDA_LAUNCH_BLOCKING` 环境变量，强制所有启动都同步进行。

##### CPU/GPU 同步

尽管 CUDA 使用的大多数 GPU 命令涉及执行内存复制或内核启动，但一个重要的命令子类帮助 CUDA 驱动程序跟踪 GPU 在处理命令缓冲区中的进度。由于应用程序无法知道给定的 CUDA 内核可能运行多长时间，GPU 本身必须向 CPU 报告进度。图 2.26 展示了驱动程序和 GPU 用来跟踪进度的命令缓冲区和“同步位置”（也位于固定主机内存中）。驱动程序维护一个单调递增的整数值（“进度值”），每个主要的 GPU 操作后都会有一个命令将新的进度值写入共享的同步位置。在图 2.26 中，进度值是 3，直到 GPU 执行完命令并将值 4 写入同步位置。

![Image](img/02fig26.jpg)

*图 2.26* 共享同步值—之前。

CUDA 以隐式和显式两种方式暴露这些硬件能力。上下文范围内的同步调用，如`cuCtxSynchronize()`或`cudaThreadSynchronize()`，仅仅检查请求 GPU 的最后同步值，并等待同步位置达到该值。例如，如果 CPU 在图 2.27 中写入的命令 8 后面跟着`cuCtxSynchronize()`或`cudaThreadSynchronize()`，驱动程序将等待共享同步值大于或等于 8。

![图像](img/02fig27.jpg)

*图 2.27* 共享同步值—之后。

CUDA 事件更明确地暴露了这些硬件能力。`cuEvent-Record()`将命令排入队列，以将新的同步值写入共享同步位置，而`cuEventQuery()`和`cuEventSynchronize()`分别检查和等待事件的同步值。

CUDA 早期版本仅仅轮询共享同步位置，反复读取内存直到满足等待条件，但这种方法成本高昂，并且仅在应用程序不需要长时间等待时效果良好（即，在退出之前同步位置不需要被读取多次，因为等待条件已经满足）。对于大多数应用程序，基于中断的方案（CUDA 称之为“阻塞同步”）更好，因为它们允许 CPU 挂起等待线程，直到 GPU 发出中断信号。驱动程序将 GPU 中断映射到特定平台的线程同步原语，例如 Win32 事件或 Linux 信号，这可以在应用程序开始等待时，如果等待条件不成立，则用于挂起 CPU 线程。

应用程序可以通过向 `cuCtxCreate()` 指定 `CU_CTX_BLOCKING_SYNC` 或向 `cudaSetDeviceFlags()` 指定 `cudaDeviceBlockingSync` 强制执行上下文范围的同步为阻塞模式。然而，最好使用阻塞的 CUDA 事件（向 `cuEventCreate()` 指定 `CU_EVENT_BLOCKING_SYNC` 或向 `cudaEventCreate()` 指定 `cudaEventBlockingSync`），因为它们更加细粒度，并能与任何类型的 CUDA 上下文无缝互操作。

聪明的读者可能会担心，CPU 和 GPU 在不使用原子操作或其他同步原语的情况下读取和写入这个共享内存位置。但由于 CPU 只是读取共享位置，因此竞争条件不是问题。最糟糕的情况是 CPU 读取到一个“过时”的值，这会导致它等待比正常情况稍长的时间。

##### 事件与时间戳

主机接口具有一个内建的高分辨率定时器，并且它可以在写入 32 位同步值的同时写入时间戳。CUDA 利用这个硬件设施来实现 CUDA 事件中的异步计时功能。

#### 2.5.3. 主机接口与 GPU 内部同步

GPU 可能包含多个引擎，以启用并发的内核处理和内存复制。在这种情况下，驱动程序将写入命令，并将这些命令分配给不同的引擎，这些引擎并发运行。每个引擎都有自己的命令缓冲区和共享同步值，并且如图 2.26 和 2.27 所述，跟踪引擎的进度。图 2.28 展示了这种情况，其中两个复制引擎和一个计算引擎并行工作。主机接口负责读取命令并将其分配给适当的引擎。在图 2.28 中，已将主机→设备的内存复制（memcpy）和两个依赖操作——内核启动和设备→主机的内存复制——提交给硬件。在 CUDA 编程抽象中，这些操作位于同一流中。流就像 CPU 线程，内核启动是在 memcpy 之后提交给该流的，因此 CUDA 驱动程序必须在命令流中插入用于 GPU 内同步的命令，以便主机接口进行处理。

![图片](img/02fig28.jpg)

*图 2.28* GPU 内部同步。

如图 2.28 所示，主机接口在协调流的所需同步方面起着核心作用。例如，当内核必须等到所需的 memcpy 完成后才能启动时，DMA 单元可以停止向给定引擎发送命令，直到共享的同步位置达到某个值。这种操作类似于 CPU/GPU 同步，但 GPU 是在自身内部同步不同的引擎。

在该硬件机制上层的软件抽象是 CUDA 流。CUDA 流类似于 CPU 线程，因为流内的操作是顺序的，并且需要多个流才能实现并发。由于命令缓冲区在不同的引擎之间共享，因此应用程序必须在不同的流中“软件流水化”其请求。因此，代替

每个流

Memcpy 设备←主机

启动内核

Memcpy 主机←设备

它们必须实现

遍历每个流

Memcpy 设备←主机

遍历每个流

启动内核

遍历每个流

Memcpy 主机←设备

没有软件管道化的情况下，DMA 引擎通过同步引擎来保持每个流的顺序执行模型，从而“打破并发性”。

##### Kepler 上的多个 DMA 引擎

NVIDIA 最新的 Kepler 系列硬件为每个引擎实现了一个 DMA 单元，避免了应用程序需要对其流操作进行软件管道化。

#### 2.5.4. 跨 GPU 同步

由于图 2.26 到 2.28 中的同步位置位于主机内存中，因此系统中的任何 GPU 都可以访问它。因此，在 CUDA 4.0 中，NVIDIA 通过`cudaStreamWaitEvent()`和`cuStreamWaitEvent()`以跨 GPU 同步的形式增加了这一功能。这些 API 调用使驱动程序在当前 GPU 的命令缓冲区中插入等待主机接口的命令，从而使 GPU 等待，直到给定事件的同步值被写入。从 CUDA 4.0 开始，事件可能会由进行等待操作的同一 GPU 或其他 GPU 发出信号。流从能够在单个 GPU 上同步硬件单元之间的执行，提升到能够同步 GPU 之间的执行。

### 2.6. GPU 架构

三种不同的 GPU 架构可以运行 CUDA。

• Tesla 硬件于 2006 年首次亮相，搭载 GeForce 8800 GTX（G80）。

• Fermi 硬件于 2010 年首次亮相，搭载 GeForce GTX 480（GF100）。

• Kepler 硬件于 2012 年首次亮相，搭载 GeForce GTX 680（GK104）。

GF100/GK104 命名法指的是实现 GPU 的 ASIC。GK104 和 GF100 中的 “K” 和 “F” 分别指代 Kepler 和 Fermi 架构。

Tesla 和 Fermi 系列继承了 NVIDIA 的传统：首先发布巨大的高端旗舰芯片，这些芯片在基准测试中取得胜利。这些芯片价格昂贵，因为 NVIDIA 的制造成本与制造 ASIC 所需的晶体管数量（因此与芯片面积）密切相关。首批大型“胜利”芯片之后，紧接着发布更小的芯片：中端为半尺寸，低端为四分之一尺寸，依此类推。

与传统不同，NVIDIA 的第一款 Kepler 架构芯片面向中端市场；他们的“胜利”芯片在第一批 Kepler 架构芯片上市几个月后才发货。GK104 拥有 35 亿个晶体管，而 GK110 拥有 71 亿个晶体管。

#### 2.6.1\. 概述

CUDA 对 GPU 的简化视图包括以下内容。

• 一个连接 GPU 与 PCI Express 总线的主机接口

• 0 到 2 个复制引擎

• 一个连接 GPU 与其设备内存的 DRAM 接口

• 一些 TPC 或 GPC（纹理处理集群或图形处理集群），每个集群包含缓存和一定数量的流式多处理器（SMs）

本章末尾引用的架构论文提供了有关 CUDA 支持 GPU 功能的完整故事，包括诸如抗锯齿渲染支持等图形特定功能。

##### 主机接口

主机接口实现了上一节描述的功能。它读取 GPU 命令（如 memcpy 和内核启动命令）并将其分派给适当的硬件单元，同时还实现了 CPU 和 GPU 之间、GPU 上不同引擎之间以及不同 GPU 之间的同步机制。在 CUDA 中，主机接口的功能主要通过 Stream 和 Event API 暴露（见第六章）。

##### 复制引擎

复制引擎可以在 SM（流处理器）进行计算时执行主机↔设备内存传输。最早的 CUDA 硬件没有任何复制引擎；后来的硬件版本包括了一个可以传输线性设备内存（但不能传输 CUDA 数组）的复制引擎，而最新的 CUDA 硬件则包括了最多两个复制引擎，这些引擎可以在饱和 PCI Express 总线的同时进行 CUDA 数组与线性内存之间的转换。^(13)

13。超过两个复制引擎没有太大意义，因为每个引擎只能饱和 PCI Express 的一个方向。

##### DRAM 接口

GPU 范围的 DRAM 接口，支持超过 100 GB/s 的带宽，包含了合并内存请求的硬件。更近的 CUDA 硬件具有更复杂的 DRAM 接口。最早的（SM 1.x）硬件具有繁琐的合并要求，需要地址是连续的，并且按 64 字节、128 字节或 256 字节对齐（具体取决于操作数的大小）。从 SM 1.2（GT200 或 GeForce GTX 280）开始，地址可以根据局部性进行合并，而不管地址对齐情况。Fermi 架构的硬件（SM 2.0 及更高版本）具有写回 L2 缓存，它提供了 SM 1.2 合并硬件的优点，并且在数据重用时进一步提高了性能。

##### TPC 和 GPC

TPC 和 GPC 是介于完整 GPU 和执行 CUDA 计算的流处理器之间的硬件单元。Tesla 架构的硬件将 SM 分组为“TPC”（纹理处理集群），其中包含纹理硬件支持（特别是纹理缓存）和两个或三个流处理器，如下所述。Fermi 架构的硬件将 SM 分组为“GPC”（图形处理集群），每个集群包含一个光栅单元和四个 SM。

大多数情况下，CUDA 开发人员不需要关注 TPC 或 GPC，因为流处理器是计算硬件中最重要的抽象单元。

###### 对比特斯拉与费米

第一代支持 CUDA 的 GPU 代号为特斯拉，第二代为费米。这些在开发过程中是保密的代号，但 NVIDIA 决定将它们用作外部产品名称，以描述第一代和第二代支持 CUDA 的 GPU。更令人困惑的是，NVIDIA 还使用“特斯拉”这一名称来描述用于构建 CUDA 计算机集群的服务器级显卡。^(14) 为了区分高价的服务器级特斯拉显卡和架构系列，本书将架构系列称为“特斯拉级硬件”、“费米级硬件”和“凯普勒级硬件”。

14。当然，当选择“特斯拉”这个品牌名称时，费米级硬件尚不存在。市场部门告诉我们工程师，这只是一个巧合，架构代号和品牌名称都叫“特斯拉”！

* * *

特斯拉级硬件与费米级硬件之间的所有差异同样适用于凯普勒。

* * *

早期的特斯拉级硬件在运行执行未合并内存事务的代码时，会面临严重的性能惩罚（最高可达 6 倍）。而从 GeForce GTX 280 开始，后续的特斯拉级硬件将未合并事务的惩罚降低到了约 2 倍。特斯拉级硬件还配备了性能计数器，允许开发者衡量有多少内存事务是未合并的。

特斯拉级硬件只包含 24 位整数乘法器，因此开发者必须使用像`__mul24()`这样的内建函数来获得最佳性能。完整的 32 位乘法（即 CUDA 中的原生操作符*）是通过一小段指令序列模拟实现的。

Tesla 级硬件将共享内存初始化为零，而 Fermi 级硬件则将其保持为未初始化。对于使用驱动 API 的应用程序，这一行为变化的一个微妙副作用是，使用`cuParamSeti()`在 64 位平台上传递指针参数的应用程序，在 Fermi 上无法正常工作。由于在 Tesla 级硬件上，参数是通过共享内存传递的，因此参数的未初始化的上半部分将成为 64 位指针的最重要的 32 位。

双精度支持是在 GT200 上的 SM 1.3 中引入的，这是 Tesla 系列的第二代“win”芯片。^(15) 当时，这个功能被认为是试探性的，因此它以一种节省面积的方式实现，可以根据 NVIDIA 需要的双精度与单精度性能比率进行增减（在 GT200 的情况下，这个比率是 1:8）。Fermi 则将双精度支持更加紧密地集成，并且提供更高的性能。^(16) 最后，对于图形应用程序来说，Tesla 级硬件是第一款支持 DirectX 10 的硬件。

15。事实上，SM 1.2 和 SM 1.3 之间唯一的区别是 SM 1.3 支持双精度。

16。在 SM 3.x 中，NVIDIA 将双精度浮点性能与 SM 版本解耦，因此 GK104 的双精度性能较差，而 GK110 的双精度性能非常优秀。

Fermi 级硬件比 Tesla 级硬件更强大。它支持 64 位寻址；增加了 L1 和 L2 缓存硬件；增加了完整的 32 位整数乘法指令，并新增了专门支持 Scan 原语的新指令；增加了表面加载/存储操作，使得 CUDA 内核可以在不使用纹理硬件的情况下读取和写入 CUDA 数组；它是第一款配备多个复制引擎的 GPU 系列；并且它改善了对 C++代码的支持，例如虚函数。

Fermi 类硬件不包括用于跟踪未合并内存事务的性能计数器。此外，由于它不包括 24 位乘法器，因此在运行使用 24 位乘法内建函数的代码时，Fermi 类硬件可能会出现小的性能损失。在 Fermi 上，使用*操作符进行乘法是快速路径。

对于图形应用程序，Fermi 类硬件可以运行 DirectX 11\. 表 2.1 总结了 Tesla 类和 Fermi 类硬件之间的差异。

![图片](img/02tab01.jpg)

*表 2.1* Tesla 类和 Fermi 类硬件之间的差异

##### 纹理处理细节

Tesla 类和 Fermi 类硬件之间的一个微妙区别是，在 Tesla 类硬件上，执行纹理处理的指令会用输出覆盖输入寄存器向量。而在 Fermi 类硬件上，输入和输出寄存器向量可以是不同的。因此，Tesla 类硬件可能需要额外的指令将纹理坐标移动到输入寄存器中，在那里它们将被覆盖。

Tesla 和 Fermi 类硬件之间的另一个微妙区别是，当从 1D CUDA 数组进行纹理处理时，Fermi 类硬件使用 2D 纹理进行此功能的仿真，并且第二个坐标始终设置为 0.0\。由于这种仿真只需要额外的一个寄存器和很少的额外指令，因此只有极少数应用程序会注意到这个差异。

#### 2.6.2\. 流式多处理器

GPU 的工作马是流式多处理器（SM）。正如上一节所提到的，SM 1.x 硬件中的每个 TPC 包含 2 或 3 个 SM，而 SM 2.x 硬件中的每个 GPC 包含 4 个 SM。第一个支持 CUDA 的 GPU——G80 或 GeForce 8800 GTX，包含 8 个 TPC；每个 TPC 有 2 个 SM，总共有 16 个 SM。下一个大规模支持 CUDA 的 GPU——GT200 或 GeForce GTX 280，将每个 TPC 的 SM 数量增加到 3，并包含 10 个 TPC，总共 30 个 SM。

CUDA GPU 中的 SM 数量可能从 2 个到几十个不等，每个 SM 包含

• 执行单元，执行 32 位整数以及单精度和双精度浮点运算

• 特殊功能单元（SFUs），用于计算 log/exp、sin/cos 和 rcp/rsqrt 的单精度近似值

• 一个 warp 调度器，用于协调指令调度到执行单元

• 一个常量缓存，用于将数据广播到 SMs（流处理器）

• 共享内存，用于线程间的数据交换

• 专用硬件用于纹理映射

图 2.29 显示了特斯拉架构的流处理器（SM 1.x）。它包含 8 个流处理器，支持 32 位整数和单精度浮点运算。第一代 CUDA 硬件完全不支持双精度，但从 GT200 开始，SMs 可能包括一个双精度浮点单元。^(17)

17。GT200 增加了一些指令以及双精度（如共享内存原子操作），因此没有双精度的 GT200 指令集为 SM 1.2，支持双精度的是 SM 1.3。

![Image](img/02fig29.jpg)

*图 2.29* 流处理器 1.x。

图 2.30 显示了 Fermi 架构的流处理器（SM 2.0）。与特斯拉架构硬件将双精度浮点支持单独实现不同，每个 Fermi 架构 SM 都完全支持双精度。双精度指令的执行速度比单精度慢，但由于其性能比特斯拉架构硬件的 8:1 比例更有利，因此总体双精度性能要高得多。^(18)

18。对于 Kepler 架构硬件，NVIDIA 可以根据 GPU 的目标市场调整浮点性能。

![Image](img/02fig30.jpg)

*图 2.30* SM 2.0（Fermi）。

图 2.31 显示了更新后的 Fermi 类流处理器（SM 2.1），例如可在 GF104 芯片中找到。为了提高性能，NVIDIA 选择将每个 SM 的流处理器数量增加到 48。SFU 到 SM 的比率从 1:8 增加到 1:6。

![Image](img/02fig31.jpg)

*图 2.31* SM 2.1 (Fermi)。

图 2.32 显示了最新的（截至本文写作时）流处理器设计，出现在 NVIDIA 最新的 Kepler 类硬件中。这个设计与前几代的差异如此之大，以至于 NVIDIA 称之为“SMX”（下一代 SM）。核心数量增加了 6 倍，达到了 192，每个 SMX 的规模也远大于前代 GPU 中类似的 SM。最大的 Fermi GPU，GF110，拥有约 30 亿个晶体管，包含 16 个 SM；而 GK104 拥有 35 亿个晶体管，性能大幅提升，但仅有 8 个 SMX。出于面积节省和功率效率的考虑，NVIDIA 大幅增加了每个 SM 的资源，唯一显著的例外是共享内存/L1 缓存。像 Fermi 的 SM 一样，每个 Kepler SMX 拥有 64K 缓存，可以分为 48K L1/16K 共享或 48K 共享/16K L1。对于 CUDA 开发人员的主要影响是，在 Kepler 架构上，相比于以前的架构，开发人员有更多的动力将数据保存在寄存器中（而不是 L1 缓存或共享内存中）。

![Image](img/02fig32.jpg)

*图 2.32* SM 3.0 (SMX)。

### 2.7. 进一步阅读

NVIDIA 在其网站上发布了描述 Fermi 和 Kepler 架构的白皮书。以下白皮书描述了 Fermi 架构。

NVIDIA GeForce GPU 下一代架构 [www.nvidia.com/object/GTX_400_architecture.html](http://www.nvidia.com/object/GTX_400_architecture.html)

以下白皮书描述了 Kepler 架构及其在 NVIDIA GeForce GTX 680 (GK104) 中的实现。

[www.geforce.com/Active/en_US/en_US/pdf/GeForce-GTX-680-Whitepaper-FINAL.pdf](http://www.geforce.com/Active/en_US/en_US/pdf/GeForce-GTX-680-Whitepaper-FINAL.pdf)

NVIDIA 工程师也发布了几篇架构论文，详细描述了各种支持 CUDA 的 GPU。

Lindholm, E., J. Nickolls, S. Oberman 和 J. Montrym. NVIDIA Tesla: 一个统一的图形与计算架构. *IEEE Micro* 28 (2), 2008 年 3 月–4 月，第 39–55 页。

Wittenbrink, C., E. Kilgariff 和 A. Prabhu. Fermi GF100 GPU 架构. *IEEE Micro* 31 (2), 2011 年 3 月–4 月，第 50–59 页。

Wong 等人使用 CUDA 开发了微基准测试，并阐明了 Tesla 系列硬件架构的某些方面。

Wong, H., M. Papadopoulou, M. Sadooghi-Alvandi 和 A. Moshovos. 通过微基准测试揭示 GPU 微架构的奥秘. 2010 年 IEEE 系统与软件性能分析国际研讨会 (IPSASS)，2010 年 3 月 28–30 日，第 235–246 页。

## 第三章 软件架构

本章概述了 CUDA 软件架构。第二章概述了硬件平台以及硬件如何与 CUDA 交互，我们将从描述 CUDA 支持的软件平台和操作环境开始本章内容。接下来，简要介绍 CUDA 中的每个软件抽象，从设备和上下文到模块和内核，再到内存。本节可能会在描述某些软件抽象如何由硬件支持时，提到第二章。最后，我们将花一些时间对比 CUDA 运行时和驱动 API，并研究 CUDA 源代码如何被翻译成在 GPU 上运行的微码。请记住，本章仅为概述。大多数涉及的主题将在后续章节中有更详细的描述。

### 3.1 软件层

图 3.1 显示了 CUDA 应用程序中不同的软件层，从应用程序本身到操作 GPU 硬件的 CUDA 驱动程序。除内核模式驱动程序外，所有软件都在目标操作系统的非特权用户模式下运行。在现代多任务操作系统的安全模型下，用户模式被认为是“非可信的”，硬件和操作系统软件必须采取措施，严格地将应用程序相互隔离。在 CUDA 中，这意味着一个 CUDA 程序分配的主机和设备内存不能被其他 CUDA 程序访问。唯一的例外是当这些程序特意请求内存共享时，这必须由内核模式驱动程序提供。

![Image](img/03fig01.jpg)

*图 3.1* CUDA 中的软件层。

CUDA 库，例如 cuBLAS，是建立在 CUDA 运行时或驱动 API 之上的。CUDA 运行时是 CUDA 集成的 C++/GPU 工具链所针对的库。当 `nvcc` 编译器将 `.cu` 文件分为主机部分和设备部分时，主机部分包含自动生成的对 CUDA 运行时的调用，以便执行诸如通过 `nvcc` 的特殊三重尖括号 `<<< >>>` 语法调用的内核启动等操作。

CUDA 驱动 API 由 CUDA 的用户模式驱动直接导出，是 CUDA 应用程序可用的最低级 API。驱动 API 调用用户模式驱动，后者可能会进一步调用内核模式驱动来执行诸如内存分配等操作。驱动 API 和 CUDA 运行时中的函数通常分别以 `cu*()` 和 `cuda*()` 开头。许多函数，例如 `cudaEventElapsedTime(),` 本质上是相同的，唯一的区别在于前缀。

#### 3.1.1\. CUDA 运行时与驱动

CUDA 运行时（通常缩写为 CUDART）是 CUDA 语言集成功能使用的库。每个 CUDA 工具链版本都有其特定的 CUDA 运行时版本，使用该工具链构建的程序将自动链接到对应版本的运行时。除非路径中有正确版本的 CUDART，否则程序将无法正常运行。

CUDA 驱动程序设计为向后兼容，支持所有针对其版本的 CUDA 编写的程序或更早版本的程序。它导出了一个低级的“驱动程序 API”（在 `cuda.h` 中），使开发人员能够精细管理资源和初始化的时机。可以通过调用 `cuDriverGetVersion()` 查询驱动程序版本。

CUresult CUDAAPI cuDriverGetVersion(int *driverVersion);

此函数返回一个十进制值，表示驱动程序支持的 CUDA 版本——例如，CUDA 3.1 对应 3010，CUDA 5.0 对应 5000。

表 3.1 总结了与 `cuDriverGetVersion()` 返回的版本号相对应的特性。对于 CUDA 运行时应用程序，这些信息由 `cudaDeviceProp` 结构中的 `major` 和 `minor` 成员提供，如第 3.2.2 节所述。

![图片](img/03tab01.jpg)

*表 3.1* CUDA 驱动程序特性

CUDA 运行时要求安装的驱动程序版本大于或等于运行时支持的 CUDA 版本。如果驱动程序版本低于运行时版本，CUDA 应用程序将因错误 `cudaErrorInsufficientDriver`（35）而无法初始化。CUDA 5.0 引入了 *设备运行时*，这是 CUDA 运行时的一个子集，可以从 CUDA 内核中调用。设备运行时的详细描述见第七章。

#### 3.1.2 驱动程序模型

除了 Windows Vista 及其之后的 Windows 版本外，所有运行 CUDA 的操作系统——Linux、MacOS 和 Windows XP——都通过*用户模式客户端驱动程序*访问硬件。这些驱动程序绕过了现代操作系统普遍要求的硬件资源必须由内核代码操作的要求。现代硬件，如 GPU，可以通过将某些硬件寄存器（如用于将工作提交给硬件的硬件寄存器）映射到用户模式来绕过这一要求。由于操作系统不信任用户模式代码，硬件必须包含防止恶意写入用户模式硬件寄存器的保护措施。目标是防止用户模式代码触发硬件使用其直接内存访问（DMA）设施读取或写入不应该访问的内存（例如操作系统的内核代码！）。

硬件设计师通过在用户模式软件可以访问的命令流中引入间接层来防止内存损坏，这样 DMA 操作只能在之前由内核代码验证和映射的内存上启动；反过来，驱动开发人员必须仔细验证他们的内核代码，以确保它只提供对应该开放的内存的访问。最终结果是一个驱动程序，它能够通过将工作提交给硬件来以最佳效率运行，而无需承受内核切换的开销。

许多操作，如内存分配，仍然需要内核模式切换，因为编辑 GPU 的页表只能在内核模式下进行。在这种情况下，用户模式驱动程序可以采取措施减少内核模式切换的次数——例如，CUDA 内存分配器尝试从内存池中满足内存分配请求。

##### 统一虚拟地址

统一虚拟地址（UVA），详细描述在第 2.4.5 节中，适用于 64 位 Linux、64 位 XPDDM 和 MacOS。在这些平台上，UVA 是透明提供的。截至目前，UVA 在 WDDM 上不可用。

##### Windows 显示驱动程序模型

对于 Windows Vista，微软引入了一种新的桌面呈现模型，其中屏幕输出是在后缓冲区合成并进行页面翻转，类似于视频游戏。新的“Windows 桌面管理器”（WDM）比以前的 Windows 更广泛地使用了 GPU，因此微软决定最好在呈现模型的基础上修改 GPU 驱动程序模型。最终形成的 Windows 显示驱动程序模型（WDDM）现已成为 Windows Vista 及后续版本的默认驱动程序模型。XPDDM 这一术语用于指代在早期版本的 Windows 上用于 GPU 的驱动程序模型。^(1)

1. Tesla 显卡（支持 CUDA 的显卡，但没有显示输出）可以在 Windows 上使用 XPDDM，称为 Tesla 计算集群（TCC）驱动程序，并可以通过 `nvidia-smi` 工具进行切换。

就 CUDA 而言，这是 WDDM 所做的两个主要更改。

**1.** WDDM 不允许硬件寄存器映射到用户模式。硬件命令——甚至是启动 DMA 操作的命令——必须由内核代码调用。用户→内核的切换对于用户模式驱动程序来说成本过高，因此用户模式驱动程序会缓冲命令，以便稍后提交。

**2.** 由于 WDDM 的设计目的是允许多个应用程序同时使用 GPU，并且 GPU 不支持按需分页，因此 WDDM 包括了一些功能，以便在“内存对象”基础上模拟分页。对于图形应用程序，内存对象可能是渲染目标、Z 缓冲区或纹理；对于 CUDA，内存对象包括全局内存和 CUDA 数组。由于驱动程序必须在每次内核调用前设置对 CUDA 数组的访问权限，CUDA 数组可以被 WDDM 交换。对于全局内存，它位于线性地址空间中（可以存储指针），每个 CUDA 上下文的内存对象必须是常驻的，才能启动 CUDA 内核。

由于上述第 1 点，WDDM 的主要影响是，CUDA 请求的工作（例如内核启动或异步 memcpy 操作）通常不会立即提交给硬件。

强制提交待处理工作的常用惯用语是查询 NULL 流：`cudaStreamQuery(0)`或`cuStreamQuery(NULL)`。如果没有待处理工作，这些调用会迅速返回。如果有任何待处理工作，它将被提交，并且由于调用是异步的，执行可能会在硬件完成处理之前就返回给调用者。在非 WDDM 平台上，查询 NULL 流总是很快的。

由于上述第 2 点，WDDM 的主要影响是 CUDA 对内存分配的控制变得不那么具体。在用户模式客户端驱动程序中，成功的内存分配意味着内存已经被分配，并且不再可供其他操作系统客户端（例如可能正在运行的游戏或其他 CUDA 应用程序）使用。在 WDDM 上，如果有多个应用程序在同一 GPU 上竞争时间，Windows 可以并且会交换内存对象，以使每个应用程序都能运行。Windows 操作系统会尽可能高效地进行此操作，但就像所有分页一样，*永远*不发生分页的速度要比*有时*发生分页要快得多。

###### 超时检测与恢复

由于 Windows 使用 GPU 与用户交互，因此计算应用程序不应占用过多的 GPU 时间非常重要。在 WDDM 下，Windows 会强制执行超时（默认值为 2 秒），如果超时发生，将出现一个对话框，显示“显示驱动程序停止响应并已恢复”，并且显示驱动程序会重新启动。如果发生这种情况，CUDA 上下文中的所有工作都会丢失。详情见 [`bit.ly/16mG0dX`](http://bit.ly/16mG0dX)。

###### Tesla 计算集群驱动程序

对于不需要 WDDM 的计算应用程序，NVIDIA 提供了 Tesla 计算集群（TCC）驱动程序，该驱动程序仅适用于 Tesla 系列的显卡。TCC 驱动程序是用户模式客户端驱动程序，因此它不需要内核转换来提交工作到硬件。可以使用 `nvidia-smi` 工具启用和禁用 TCC 驱动程序。

#### 3.1.3\. `nvcc`、PTX 和微代码

`nvcc` 是 CUDA 开发人员用来将源代码转化为可运行的 CUDA 应用程序的编译驱动程序。它可以执行多种功能，从像编译、链接和执行一个示例程序（这是本书中许多示例程序推荐的用法）这样复杂的操作，到简单的编译仅针对 GPU 的 `.cu` 文件。

图 3.2 展示了使用 `nvcc` 分别进行 CUDA 运行时和驱动程序 API 应用程序的两种推荐工作流程。对于大于最简单大小的应用程序，`nvcc` 最好严格用于编译 CUDA 代码并将 CUDA 功能封装到可以从其他工具调用的代码中。这是由于 `nvcc` 的局限性。

• `nvcc` 仅与特定的编译器集兼容。许多 CUDA 开发人员从未注意到这一点，因为他们选择的编译器恰好在支持的编译器集合中。但在生产软件开发中，CUDA 代码的数量通常比其他代码少得多，因此是否支持 CUDA 可能不是决定使用哪种编译器的主要因素。

• `nvcc` 对编译环境进行的更改可能与大部分应用程序的构建环境不兼容。

• `nvcc` 会“污染”命名空间，加入非标准的内建类型（例如，`int2`）和内建函数名称（例如，`__popc()`）。只有在 CUDA 的较新版本中，这些内建符号才变得可选，并且可以通过包含相应的 `sm_*_intrinsics.h` 头文件来使用。

![Image](img/03fig02.jpg)

*图 3.2* `nvcc` 工作流。

对于 CUDA 运行时应用程序，`nvcc` 将 GPU 代码嵌入到输出可执行文件的字符串字面量中。如果指定了 `--fatbin` 选项，执行文件会自动加载适合目标 GPU 的微代码，或者如果没有可用的微代码，驱动程序会自动将 PTX 编译成微代码。

##### `nvcc` 和 PTX

PTX（“并行线程执行”）是已编译 GPU 代码的中间表示，可以被编译成原生 GPU 微代码。它是使 CUDA 应用程序“面向未来”的机制，能够适应 NVIDIA 的指令集创新——只要给定的 CUDA 内核的 PTX 可用，CUDA 驱动程序就可以将其翻译成适用于运行该应用程序的 GPU 的微代码（即使该 GPU 在编写代码时尚不可用）。

PTX 可以被编译成 GPU 微代码，既可以是“离线”编译，也可以是“在线”编译。离线编译指的是为未来将由某台计算机执行的软件进行构建。例如，图 3.2 展示了 CUDA 编译过程中的离线部分。在线编译，也称为“即时”编译，是指为运行应用程序的计算机编译中间代码（如 PTX），以便立即执行。

`nvcc` 可以通过调用 PTX 汇编器 `ptxas` 离线编译 PTX，`ptxas` 会将 PTX 编译为特定 GPU 版本的本地微代码。生成的微代码会被输出为 CUDA 二进制文件，称为“cubin”（发音类似于“Cuban”）。可以使用 `cuobjdump --dump-sass;` 来反汇编 cubin 文件；这将输出针对 GPU 特定微代码的 SASS 助记符。^(2)

2. 检查 SASS 代码是推动优化的关键策略。

PTX 也可以通过 CUDA 驱动程序在线编译（JIT）。当运行使用`--fatbin`选项构建的 CUDART 应用程序时（这是默认选项），在线编译会自动发生。每个内核的 `.cubin` 和 PTX 表示形式都包含在可执行文件中，如果该文件在不支持任何 `.cubin` 表示的硬件上运行，驱动程序将编译 PTX 版本。驱动程序会将这些编译后的内核缓存到磁盘上，因为编译 PTX 可能会消耗较长时间。

最后，PTX 可以在运行时生成，并通过调用 `cuModuleLoadEx()` 由驱动程序显式编译。驱动程序 API 不会自动化任何嵌入或加载 GPU 微代码的操作。可以将 `.cubin` 和 `.ptx` 文件传递给 `cuModuleLoadEx()`；如果 `.cubin` 不适合目标 GPU 架构，则会返回错误。对于驱动程序 API 开发人员来说，一个合理的策略是编译并嵌入 PTX，且他们应始终使用 `cuModuleLoadEx()` 将其 JIT 编译到 GPU 上，依赖于驱动程序来缓存已编译的微代码。

### 3.2\. 设备与初始化

设备对应物理 GPU。当 CUDA 被初始化时（无论是通过显式调用驱动程序 API 的 `cuInit()` 函数，还是通过隐式调用 CUDA 运行时函数），CUDA 驱动程序会枚举可用的设备，并创建一个包含其名称和不可变能力（如设备内存大小和最大时钟频率）的全局数据结构。

对于某些平台，NVIDIA 提供了一种工具，可以设置与特定设备相关的策略。`nvidia-smi` 工具设置与给定 GPU 相关的策略。例如，`nvidia-smi` 可以用来启用和禁用给定 GPU 上的 ECC（错误纠正）。`nvidia-smi` 还可以用来控制在给定设备上可以创建的 CUDA 上下文的数量。这些是可能的模式。

• 默认：设备上可以创建多个 CUDA 上下文。

• “独占”模式：设备上只能创建一个 CUDA 上下文。

• “禁止”：设备上不能创建 CUDA 上下文。

如果设备已被枚举，但无法在该设备上创建上下文，则很可能该设备处于“禁止”模式或“独占”模式，且另一个 CUDA 上下文已在该设备上创建。

#### 3.2.1\. 设备数量

应用程序可以通过调用 `cuDeviceGetCount()` 或 `cudaGetDeviceCount()` 来发现可用的 CUDA 设备数量。然后，可以通过索引 [0..*DeviceCount*-1] 引用设备。驱动程序 API 要求应用程序调用 `cuDeviceGet()` 将设备索引映射到设备句柄 (`CUdevice`)。

#### 3.2.2\. 设备属性

驱动程序 API 应用程序可以通过调用 `cuDeviceGetName()` 查询设备的名称，并通过调用 `cuDeviceTotalMem()` 查询设备的总内存量。设备的主要和次要计算能力（即 SM 版本，如第一个 Fermi 支持的 GPU 为 2.0）可以通过调用 `cuDeviceComputeCapability()` 查询。

CUDA 运行时应用程序可以调用 `cudaGetDeviceProperties()`，该函数将返回一个包含设备名称和属性的结构。 表 3.2 给出了 `cudaDeviceProp` 成员的描述，这是 `cudaGetDeviceProperties()` 返回的结构。

![Image](img/03tab02.jpg)![Image](img/03tab02a.jpg)![Image](img/03tab02b.jpg)![Image](img/03tab02c.jpg)

*表 3.2.* `cudaDeviceProp` 成员

驱动程序 API 用于查询设备属性的函数 `cuDeviceGetAttribute()`，可以一次返回一个属性，具体取决于 `CUdevice_attribute` 参数。在 CUDA 5.0 中，CUDA 运行时以 `cudaDeviceGetAttribute()` 的形式添加了相同的功能，推测是因为基于结构的接口在设备上运行过于繁琐。

#### 3.2.3\. 当没有 CUDA 时

CUDA 运行时可以在不能运行 CUDA 或没有安装 CUDA 的机器上运行；如果 `cudaGetDeviceCount()` 返回 `cudaSuccess` 且设备数量不为零，则说明 CUDA 可用。

当使用驱动程序 API 时，直接链接 `nvcuda.dll`（Windows）或 `libcuda.so`（Linux）的可执行文件将无法加载，除非驱动程序二进制文件可用。对于需要 CUDA 的驱动程序 API 应用程序，尝试启动一个直接链接到驱动程序的应用程序将导致错误，如图 3.3 所示。

![Image](img/03fig03.jpg)

*图 3.3* 当没有 CUDA 时的错误（Windows）。

对于那些必须在有或没有 CUDA 的情况下运行的应用程序，CUDA SDK 提供了一组头文件和一个 C 源文件，包装了驱动程序 API，使应用程序可以在不需要操作系统发出异常信号的情况下检查是否有 CUDA。这些文件位于 `dynlink` 子目录 `<SDKRoot>/C/common/inc/dynlink`，可以替代核心 CUDA 文件。它们插入了一组中间函数，如果 CUDA 可用，会延迟加载 CUDA 库。

作为示例，我们比较两个使用驱动程序 API 来初始化 CUDA 并写入系统中每个设备名称的程序。清单 3.1 给出了 `init_hardcoded.cpp`，这是一个可以使用以下命令行编译为 CUDA SDK 的文件。

nvcc –oinit_hardcoded –I ../chLib init_hardcoded.cpp -lcuda

使用 `nvcc` 编译一个不包含任何 GPU 代码的 C++ 文件只是一个方便的方式来引入 CUDA 头文件。开头的 `–oinit_hardcoded` 指定了输出可执行文件的根名称。结尾的 `-lcuda` 使得 `nvcc` 链接驱动 API 的库；没有它，构建将因为链接错误而失败。这个程序硬链接了 CUDA 驱动 API，因此在没有安装 CUDA 的系统上会失败。

*Listing 3.1.* 初始化（硬编码）。

点击此处查看代码图像

* * *

/*

* init_hardcoded.cpp

*

*/

#include <stdio.h>

#include <cuda.h>

#include <chError.h>

int

main()

{

CUresult status;

int numDevices;

CUDA_CHECK( cuInit( 0 ) );

CUDA_CHECK( cuDeviceGetCount( &numDevices ) );

printf( "%d 个设备被检测到:\n", numDevices );

for ( int i = 0; i < numDevices; i++ ) {

char szName[256];

CUdevice device;

CUDA_CHECK( cuDeviceGet( &device, i ) );

CUDA_CHECK( cuDeviceGetName( szName, 255, device ) );

printf( "\t%s\n", szName );

}

return 0;

错误：

fprintf( stderr, "CUDA 错误代码: 0x%x\n", status );

return 1;

}

* * *

Listing 3.2 给出了一个在没有 CUDA 的系统上能够运行的程序。正如你所看到的，源代码除了几行代码外完全相同。

#include <cuda.h>

被替换为

点击此处查看代码图像

#include "cuda_drvapi_dynlink.c"

#include "dynlink/cuda_drvapi_dynlink.h"

并且 `cuInit()` 调用已被更改为指定 CUDA 版本。

CUDA_CHECK( cuInit(0) );

被替换为

CUDA_CHECK( cuInit( 0, 4010 ) );

在这里，传递 `4010` 作为第二个参数请求 CUDA 4.1，如果系统不支持该功能级别，函数将会失败。

请注意，你可以单独编译并链接`cuda_drvapi_dynlink.c`到应用程序中，而不是将其`#include`到单一源文件中。头文件和 C 文件共同作用，将一组包装函数附加到驱动 API 上。头文件使用预处理器将驱动 API 函数重命名为在`cuda_drvapi_dynlink.h`中声明的包装函数（例如，对`cuCtxCreate()`的调用变成对`tcuCtxCreate()`的调用）。在支持 CUDA 的系统上，驱动 DLL 会动态加载，包装函数会在初始化过程中调用从驱动 DLL 获取的函数指针。在不支持 CUDA 的系统上，或者如果驱动不支持请求的 CUDA 版本，初始化函数将返回错误。

*清单 3.2.* 初始化（dynlink）。

点击此处查看代码图像

* * *

/*

* init_dynlink.cpp

*

*/

#include <stdio.h>

#include "dynlink/cuda_drvapi_dynlink.h"

#include <chError.h>

int

main()

{

CUresult status;

int numDevices;

CUDA_CHECK( cuInit( 0, 4010 ) );

CUDA_CHECK( cuDeviceGetCount( &numDevices ) );

printf( "%d 个设备检测到：\n", numDevices );

for ( int i = 0; i < numDevices; i++ ) {

char szName[256];

CUdevice device;

CUDA_CHECK( cuDeviceGet( &device, i ) );

CUDA_CHECK( cuDeviceGetName( szName, 255, device ) );

printf( "\t%s\n", szName );

}

return 0;

错误：

fprintf( stderr, "CUDA 错误代码：0x%x\n", status );

return 1;

}

* * *

##### 仅 CUDA DLLs

对于 Windows 开发者，另一种构建可以在不支持 CUDA 的系统上运行的 CUDA 应用程序的方法如下。

**1.** 将 CUDA 特定的代码移到 DLL 中。

**2.** 显式调用`LoadLibrary()`加载 DLL。

**3.** 将`LoadLibrary()`调用包含在`__try/__except`语句中，以便在 CUDA 不存在时捕获异常。

### 3.3\. 上下文

上下文类似于 CPU 上的进程。除少数例外，它们是管理 CUDA 中所有其他对象生命周期的容器，包括以下内容。

• 所有内存分配（包括线性设备内存、主机内存和 CUDA 数组）

• 模块

• CUDA 流

• CUDA 事件

• 纹理和表面引用

• 使用本地内存的内核的设备内存

• 用于调试、分析和同步的内部资源

• 用于分页内存拷贝（memcpy）的固定暂存缓冲区

CUDA 运行时不提供对 CUDA 上下文的直接访问。它通过*延迟初始化*进行上下文创建。每个 CUDART 库调用或内核调用都会检查是否存在当前 CUDA 上下文，如果需要，创建 CUDA 上下文（使用之前由如`cudaSetDevice()`、`cudaSetDeviceFlags()`、`cudaGLSetGLDevice()`等调用设置的状态）。

许多应用程序更喜欢显式控制此延迟初始化的时机。为了强制 CUDART 在没有其他副作用的情况下进行初始化，可以调用

cudaFree(0);

CUDA 运行时应用程序可以通过驱动程序 API 访问当前上下文栈（如下所述）。

对于在驱动程序 API 中指定每个上下文状态的函数，CUDA 运行时将上下文和设备合并在一起。代替`cuCtxSynchronize()`，CUDA 运行时使用`cudaDeviceSynchronize()`；代替`cuCtxSetCacheConfig()`，CUDA 运行时使用`cudaDeviceSetCacheConfig()`。

#### 当前上下文

替代当前上下文栈，CUDA 运行时提供了`cudaSetDevice()`函数，用于为调用线程设置当前上下文。一个设备可以同时对多个 CPU 线程为当前上下文。^(3)

3。早期版本的 CUDA 禁止多个线程同时使用一个上下文，因为驱动程序不是线程安全的。现在，驱动程序实现了所需的同步——即使应用程序调用了同步函数，如`cudaDeviceSynchronize()`。

#### 3.3.1\. 生命周期和作用域

与 CUDA 上下文相关的所有资源在上下文被销毁时都会被销毁。除少数例外外，为特定 CUDA 上下文创建的资源不能与其他任何 CUDA 上下文一起使用。此限制不仅适用于内存，还适用于像 CUDA 流和 CUDA 事件这样的对象。

#### 3.3.2\. 资源预分配

CUDA 尽量避免“懒加载”现象，即按需分配资源以避免由于资源不足导致操作失败。例如，分页内存拷贝无法因内存不足而失败，因为执行分页内存拷贝所需的固定暂存缓冲区是在上下文创建时分配的。如果 CUDA 无法分配这些缓冲区，则上下文创建将失败。

在某些孤立的情况下，CUDA 并不会为特定操作预分配所有可能需要的资源。为了启动内核所需的局部内存的大小可能非常大，因此 CUDA 不会预分配所需的最大理论内存。结果，如果内核启动所需的局部内存超过 CUDA 为上下文分配的默认内存，则内核启动可能会失败。

#### 3.3.3\. 地址空间

除了那些在上下文销毁时自动被销毁（“清理”）的对象之外，上下文所体现的关键抽象是其*地址空间*：它可以用来分配线性设备内存或映射固定主机内存的虚拟内存地址的私有集合。这些地址对于每个上下文是唯一的。不同上下文的相同地址可能有效也可能无效，并且除非采取特别的措施，否则肯定不会指向相同的内存位置。CUDA 上下文的地址空间与 CUDA 主机代码使用的 CPU 地址空间是分开且独立的。事实上，与共享内存的多 CPU 系统不同，多个 GPU 配置中的 CUDA 上下文并不共享地址空间。当 UVA（统一虚拟地址）生效时，CPU 和 GPU(s) 共享相同的地址空间，即任何给定的分配在进程内有唯一的地址，但 CPU 和 GPU 之间只能在特殊情况下相互读写内存，例如映射的固定内存（见第 5.1.3 节）或点对点内存（见第 9.2.2 节）。

#### 3.3.4 当前上下文栈

大多数 CUDA 入口点不接受上下文参数。相反，它们操作的是“当前上下文”，该上下文存储在 CPU 线程的线程局部存储（TLS）句柄中。在驱动程序 API 中，每个 CPU 线程都有一个当前上下文的栈；创建上下文时会将新的上下文推入栈中。

当前上下文栈有三个主要应用。

• 单线程应用程序可以驱动多个 GPU 上下文。

• 库可以创建并管理自己的 CUDA 上下文，而不干扰调用者的 CUDA 上下文。

• 库可以对调用到 CUDA 感知库的 CPU 线程保持无关态。

当前上下文堆栈用于 CUDA 的最初动机是使单线程 CUDA 应用程序能够驱动多个 CUDA 上下文。在创建并初始化每个 CUDA 上下文后，应用程序可以将其从当前上下文堆栈中弹出，使其成为一个“浮动”上下文。由于在任何时候只有一个 CUDA 上下文可以对 CPU 线程是当前的，因此单线程 CUDA 应用程序通过轮流推送和弹出上下文来驱动多个上下文，保持所有上下文中除了一个之外的都是“浮动”状态。

在大多数驱动架构中，推送和弹出 CUDA 上下文的开销足够小，以至于单线程应用程序可以保持多个 GPU 处于活跃状态。在 WDDM（Windows 显示驱动程序模型）驱动程序中，该驱动程序仅在 Windows Vista 及之后版本上运行，只有在没有 GPU 命令待处理的情况下，弹出当前上下文才会很快。如果有命令待处理，驱动程序将产生一个内核函数调用来提交命令，然后再弹出 CUDA 上下文。^(4)

4。这种开销并非驱动程序 API 或当前上下文堆栈所独有。在 WDDM 中，当命令待处理时，调用 `cudaSetDevice()` 来切换设备也会导致内核函数调用。

当前上下文栈的另一个好处是能够通过不同的 CPU 线程来驱动给定的 CUDA 上下文。使用驱动 API 的应用程序可以通过调用`cuCtxPopCurrent()`弹出上下文，然后在另一个线程中调用`cuCtxPushCurrent()`将 CUDA 上下文“迁移”到其他 CPU 线程。库可以利用此功能创建 CUDA 上下文，而无需调用方的知识或参与。例如，一个 CUDA 感知的插件库可以在初始化时创建自己的 CUDA 上下文，然后弹出它并保持悬浮状态，只有在主应用程序调用时才使用。悬浮上下文使得库完全不关心哪个 CPU 线程用于调用它。当以这种方式使用时，CUDA 上下文强制执行的封装既是福音也是祸根。一方面，悬浮上下文的内存不会被第三方 CUDA 核心的无效写入污染，但另一方面，库只能操作它自己分配的 CUDA 资源。

##### 附加和分离上下文

直到 CUDA 4.0，每个 CUDA 上下文在创建时都会设置一个“使用计数”为 1。`cuCtxAttach()`和`cuCtxDetach()`函数分别增加和减少使用计数。^(5) 使用计数旨在使库能够“附加”到由应用程序创建的 CUDA 上下文中，这样应用程序和它的库可以通过应用程序创建的 CUDA 上下文进行互操作。^(6)

5。直到 CUDA 2.2 中添加了 `cuCtxDestroy()` 函数之前，CUDA 上下文是通过调用 `cuCtxDetach()` 销毁的。

6。回顾起来，让 NVIDIA 将引用计数交给更高层的软件层，而不是驱动 API，可能会更明智。

如果在首次调用 CUDART 时，CUDA 上下文已经是当前的，它会附加现有的 CUDA 上下文，而不是创建一个新的。CUDA 运行时不提供访问上下文使用计数的功能。从 CUDA 4.0 开始，使用计数已被弃用，且`cuCtxAttach()/cuCtxDetach()`不再有任何副作用。

#### 3.3.5\. 上下文状态

`cuCtxSetLimit()`和`cuCtxGetLimit()`函数配置与 CPU 类似的功能限制：内核中的`malloc()`和`printf()`。`cuCtxSetCacheConfig()`指定在启动内核时使用的首选缓存配置（是分配 16K 还是 48K 的共享内存和 L1 缓存）。这只是一个提示，因为任何使用超过 16K 共享内存的内核都需要 48K 共享内存配置。此外，内核特定状态（`cuFuncSetCacheConfig()`）可以覆盖上下文状态。这些状态具有上下文作用域（换句话说，它们不会为每个内核启动单独指定），因为它们更改起来成本较高。

### 3.4\. 模块和函数

模块是代码和相关数据的集合，它们一起加载，类似于 Windows 上的 DLL 或 Linux 上的 DSO。像 CUDA 上下文一样，CUDA 运行时并不显式支持模块；它们仅在 CUDA 驱动 API 中可用。^(7)

7。如果 CUDA 增加了从源代码进行 JIT 编译的功能（如同 OpenCL 所能），NVIDIA 可能会考虑将模块暴露给 CUDA 运行时。

CUDA 没有类似于目标文件的中间结构，无法合成到 CUDA 模块中。相反，`nvcc`直接生成可以作为 CUDA 模块加载的文件。

• 目标特定 SM 版本的`.cubin`文件

• 可以由驱动程序编译到硬件上的`.ptx`文件

这些数据无需以这些文件的形式发送给最终用户。CUDA 包括加载模块为 NULL 终止字符串的 API，这些字符串可以嵌入到可执行资源中或其他地方。^(8)

8。`cuModuleLoadDataEx()`函数在第 4.2 节中有详细描述。

一旦 CUDA 模块被加载，应用程序就可以查询其中包含的资源。

• 全局变量

• 函数（内核）

• 纹理引用

一个重要提示：所有这些资源在模块加载时创建，因此查询函数不会因为资源不足而失败。

与上下文一样，CUDA 运行时隐藏了模块的存在和管理。所有模块在 CUDART 初始化时一起加载。对于包含大量 GPU 代码的应用程序，能够通过显式加载和卸载模块来管理驻留性，是使用驱动程序 API 而非 CUDA 运行时的主要原因之一。

模块通过调用`nvcc`构建，具体可以生成不同类型的模块，这取决于命令行参数，详细信息见表 3.3。由于 cubins 已针对特定 GPU 架构进行编译，它们不需要“即时编译”，加载速度更快。但它们既不向后兼容（例如，针对 SM 2.x 编译的 cubins 无法在 SM 1.x 架构上运行），也不向前兼容（例如，针对 SM 2.x 架构编译的 cubins 无法在 SM 3.x 架构上运行）。因此，只有具有*先验*知识的应用程序（即已知目标 GPU 架构及其 cubin 版本）才能在不嵌入同一模块的 PTX 版本作为备份的情况下使用 cubins。

![Image](img/03tab03.jpg)

*表 3.3* nvcc 模块类型

PTX 是作为驱动程序即时编译源的中间语言。由于这种编译可能需要相当长的时间，驱动程序会保存已编译的模块，并在硬件和驱动程序未发生变化的情况下重用它们。如果驱动程序或硬件发生变化，所有 PTX 模块必须重新编译。

使用 fatbin 时，CUDA 运行时会自动选择合适的 cubin（如果可用），否则会编译 PTX。不同版本的 cubin 作为字符串嵌入在由`nvcc`生成的主机 C++ 代码中。使用驱动程序 API 的应用程序可以更细粒度地控制模块。例如，它们可以作为资源嵌入到可执行文件中、加密，或在运行时生成，但如果可用的 cubin 必须显式地使用，而如果没有 cubin 则需要编译 PTX。

一旦模块被加载，应用程序可以查询其中包含的资源：全局变量、函数（内核）和纹理引用。有一个重要的注意事项：所有这些资源在模块加载时就已创建，因此查询函数（在表 3.4 中总结）不会因为资源不足而失败。

![Image](img/03tab04.jpg)

*表 3.4* 模块查询函数

### 3.5\. 内核（函数）

内核通过 `.cu` 文件中的 `__global__` 关键字来标识。在使用 CUDA 运行时时，它们可以通过三重尖括号 `<<< >>>` 语法进行内联调用。第七章详细描述了如何调用内核及其在 GPU 上的执行过程。

模块的 GPU 可执行代码以内核的形式存在，这些内核可以通过 CUDA 运行时的语言集成功能（`<<< >>>`语法）或驱动程序 API 中的`cuLaunchKernel()`函数进行调用。在撰写本文时，CUDA 不进行任何动态驻留管理的可执行代码管理。当模块加载时，*所有*内核都会被加载到设备内存中。

一旦模块加载完成，可以通过`cuModuleGetFunction()`查询内核；可以通过`cuFuncGetAttribute()`查询内核的属性；并且可以通过`cuLaunchKernel()`启动内核。`cuLaunchKernel()`使得一大堆 API 接口变得过时：像`cuFuncSetBlockShape()`这样的函数用于指定下次启动给定内核时的块大小；像`cuParamSetv()`这样的函数用于指定下次启动给定内核时传递的参数；而`cuLaunch()`、`cuLaunchGrid()`和`cuLaunchGridAsync()`则使用之前设置的状态启动内核。这些 API 效率较低，因为设置内核启动需要进行多次调用，而且像块大小这样的参数最好是和启动内核的请求一起原子性地指定。

`cuFuncGetAttribute()`函数可以用于查询函数的特定属性，例如

• 每个块的最大线程数

• 静态分配的共享内存的大小

• 用户分配的常量内存的大小

• 每个函数使用的本地内存量

• 每个线程使用的寄存器数量

• 函数编译时使用的虚拟（PTX）和二进制架构版本

使用驱动 API 时，通常建议使用`extern "C"`来抑制 C++的默认名称改编行为。否则，必须指定名称改编后的名称给`cuModuleGetFunction()`。

##### CUDA 运行时

当使用 CUDA 运行时构建的可执行文件被加载时，它们会在主机内存中创建描述 CUDA 资源的全局数据结构，以便在创建 CUDA 设备时分配这些资源。一旦 CUDA 设备初始化完成，这些全局数据就会一次性创建所有 CUDA 资源。由于这些全局数据由 CUDA 运行时在进程范围内共享，因此无法使用 CUDA 运行时逐步加载和卸载 CUDA 模块。

由于 CUDA 运行时与 C++语言的集成方式，内核和符号应该通过名称指定（即，不使用字符串字面量）传递给诸如`cudaFuncGetAttributes()`和`cudaMemcpyToSymbol()`等 API 函数。

##### 缓存配置

在 Fermi 架构中，流式多处理器具有可以分配为 16K 共享/48K L1 缓存或 48K 共享/16K L1 缓存的 L1 缓存。^(9) 最初，CUDA 允许在每个内核的基础上指定缓存配置，使用 CUDA 运行时中的`cudaFuncSetCacheConfig()`或驱动程序 API 中的`cuFuncSetCacheConfig()`。后来，这一状态被移到了更全局的位置：`cuCtxSetCacheConfig()/cudaDeviceSetCacheConfig()`指定默认的缓存配置。

9. SM 3.x 增加了将缓存在 L1 和共享内存之间均匀分配（32K/32K）的功能。

### 3.6\. 设备内存

设备内存（或线性设备内存）位于 CUDA 地址空间中，可以通过正常的 C/C++指针和数组解引用操作由 CUDA 内核访问。大多数 GPU 都有一个专门的设备内存池，它直接连接到 GPU 并由集成的内存控制器访问。

CUDA 硬件不支持按需分页，因此所有内存分配都由实际的物理内存支持。与 CPU 应用程序不同，后者可以分配比系统中物理内存更多的虚拟内存，而当物理内存耗尽时，CUDA 的内存分配功能会失败。有关如何分配、释放和访问设备内存的详细信息，请参见第 5.2 节。

##### CUDA 运行时

CUDA 运行时应用程序可以通过调用 `cudaGetDeviceProperties()` 并检查 `cudaDeviceProp::totalGlobalMem` 来查询给定设备上可用的设备内存总量。`cudaMalloc()` 和 `cudaFree()` 分别用于分配和释放设备内存。`cudaMallocPitch()` 分配带有间距的内存；`cudaFree()` 可用于释放它。`cudaMalloc3D()` 执行带有间距的 3D 内存分配。

##### 驱动程序 API

驱动程序 API 应用程序可以通过调用 `cuDeviceTotalMem()` 查询给定设备上可用的设备内存总量。或者，可以使用 `cuMemGetInfo()` 函数查询可用的设备内存量以及总量。`cuMemGetInfo()` 只能在 CUDA 上下文当前与 CPU 线程时调用。`cuMemAlloc()` 和 `cuMemFree()` 分别用于分配和释放设备内存。`cuMemAllocPitch()` 分配带有间距的内存；`cuMemFree()` 可用于释放它。

### 3.7\. 流和事件

在 CUDA 中，添加了流和事件，以便主机↔设备内存复制可以与内核执行并发进行。CUDA 的后续版本扩展了流的功能，以支持在同一 GPU 上并发执行多个内核，并支持多个 GPU 之间的并发执行。

CUDA 流用于以粗粒度管理执行单元之间的并发。

• GPU 和 CPU

• 在 SM 处理时可以执行 DMA 的复制引擎

• 流处理多处理器 (SM)

• 旨在并发运行的内核

• 同时执行的独立 GPU

在给定流中请求的操作是顺序执行的。从某种意义上说，CUDA 流就像 CPU 线程，因为在 CUDA 流中的操作是按顺序执行的。

#### 3.7.1\. 软件流水线

因为 GPU 中只有一个 DMA 引擎为各种粗粒度硬件资源提供服务，所以应用程序必须对多个流执行的操作进行“软件流水线”处理。否则，DMA 引擎会通过在不同引擎之间强制同步来“破坏并发性”。如何通过软件流水线充分利用 CUDA 流的详细描述，请参见第 6.5 节，更多示例见第十一章。

Kepler 架构减少了对流式操作进行软件流水线处理的需求，并且通过 NVIDIA 的 Hyper-Q 技术（首次在 SM 3.5 中可用），几乎完全消除了对软件流水线的需求。

#### 3.7.2\. 流回调

CUDA 5.0 引入了另一种 CPU/GPU 同步机制，补充了现有机制，该机制使得 CPU 线程能够等待直到流空闲或事件已记录。流回调是由应用程序提供的函数，向 CUDA 注册，稍后当流到达调用`cuStreamAddCallback()`的点时，CUDA 会调用这些函数。

在流回调期间，流的执行会暂停，因此出于性能考虑，开发者应小心确保其他流在回调期间可用来处理任务。

#### 3.7.3\. NULL 流

任何异步的 memcpy 函数都可以将 NULL 作为流参数进行调用，memcpy 将在 GPU 上所有前序操作完成后才会启动；实际上，NULL 流是 GPU 上所有引擎的合并。此外，所有流式 memcpy 函数都是*异步的*，可能在 memcpy 操作执行之前就将控制权返回给应用程序。NULL 流对于在不需要通过多个流来促进 GPU 内部并发的应用程序中促进 CPU/GPU 并发非常有用。一旦通过 NULL 流启动了流式操作，应用程序必须使用同步函数，如`cuCtxSynchronize()`或`cudaThreadSynchronize()`，以确保操作在继续之前已完成。但应用程序可以在执行同步之前请求许多此类操作。例如，应用程序可以执行

• 一个异步的主机→设备 memcpy

• 一个或多个内核启动

• 一个异步的设备→主机 memcpy

然后再与上下文同步。`cuCtxSynchronize()`或`cudaThreadSynchronize()`调用会在 GPU 执行完最后一个请求的操作后返回。这个惯用法在执行较小的 memcpy 或启动不会长时间运行的内核时特别有用。CUDA 驱动程序需要宝贵的 CPU 时间来向 GPU 写入命令，通过将 CPU 执行与 GPU 的命令处理重叠，可以提高性能。

*注意：* 即使在 CUDA 1.0 中，内核启动也是异步的；如果没有明确指定流，则默认使用 NULL 流。

#### 3.7.4. 事件

CUDA 事件提供了另一种同步机制。它与 CUDA 流一起引入，"记录" CUDA 事件是一种让应用程序跟踪 CUDA 流内进度的方法。所有 CUDA 事件通过在所有前置操作完成后，写入共享同步内存位置来工作。^(10) *查询* CUDA 事件会让驱动程序检查该内存位置，并报告事件是否已经记录；*同步* CUDA 事件会让驱动程序等待，直到事件被记录。

10。指定 NULL 流给 `cuEventRecord()` 或 `cudaEventRecord()` 表示事件将在 GPU 处理完*所有*前置操作后才会被记录。

可选地，CUDA 事件还可以写入从硬件中的高分辨率计时器得出的时间戳。基于事件的计时比基于 CPU 的计时更加可靠，尤其是对于较小的操作，因为它不受一些无关事件（如页面错误或网络流量）的干扰，而这些无关事件可能会影响 CPU 的墙钟时间。墙钟时间是确定性的，因为它们更接近最终用户看到的时间，因此 CUDA 事件最适合用于产品开发中的性能调优。^(11)

11。此外，不能用于某些其他操作的 CUDA 事件，如果需要计时功能，则不能使用；CUDA 的更新版本允许开发者选择不使用计时功能，以便使 CUDA 事件用于其他用途，例如设备间同步。

使用 CUDA 事件进行计时时，最好与 NULL 流结合使用。这个经验法则的动机与 RDTSC（读取时间戳计数器）在 CPU 上作为序列化指令的原因类似：正如 CPU 是一种超标量处理器，可以同时执行多个指令，GPU 也可以同时在多个流上操作。如果没有明确的序列化，计时操作可能会无意中包括不应该计时的操作，或排除本应计时的操作。与 RDTSC 一样，诀窍是通过足够的工作将 CUDA 事件记录括起来，这样执行计时本身的开销就可以忽略不计。

CUDA 事件可以选择性地导致硬件信号中断，使得驱动程序能够执行所谓的“阻塞”等待。阻塞等待会暂停等待的 CPU 线程，在驱动程序等待 GPU 时节省 CPU 时钟周期和电力。在阻塞等待出现之前，CUDA 开发者常常抱怨 CUDA 驱动程序通过轮询内存位置使得整个 CPU 核心都在等待 GPU。与此同时，由于处理中断的开销，阻塞等待可能会花费更长的时间，因此对延迟敏感的应用可能仍然希望使用默认的轮询行为。

### 3.8\. 主机内存

“主机”内存是 CPU 内存——我们曾经在使用 `malloc()/free()` 和 `new[]/delete[]` 管理内存多年，直到有人听说 CUDA。在所有运行 CUDA 的操作系统中，主机内存是*虚拟化*的；硬件实施的内存保护机制用于防止 CPU 进程在没有特殊规定的情况下读取或写入彼此的内存。^(12) 内存的“页面”，通常为 4K 或 8K 大小，可以在不改变其虚拟地址的情况下进行重新定位；尤其是，它们可以交换到磁盘，从而有效地使计算机拥有比物理内存更多的虚拟内存。当页面被标记为“非驻留”时，尝试访问该页面将触发“页面错误”信号，操作系统会根据这一信号查找一个空闲的物理页面，将数据从磁盘复制到该页面，并继续执行，虚拟页面指向新的物理位置。

12。促进进程间共享的 API 示例包括 Windows 上的 `MapViewOfFile()` 或 Linux 上的 `mmap()`。

管理虚拟内存的操作系统组件称为“虚拟内存管理器”（VMM）。VMM 除了监控内存活动，还利用启发式方法决定何时将页面“逐出”到磁盘，并解决逐出页面被引用时发生的页面错误。

VMM 提供服务给硬件驱动程序，便于硬件直接访问主机内存。在现代计算机中，许多外设，包括磁盘控制器、网络控制器和 GPU，都可以使用一种被称为“直接内存访问”（DMA）的技术来读写主机内存。DMA 提供了两大性能优势：它避免了数据复制^(13)，并且使硬件能够与 CPU 并行工作。第三个优势是，硬件通过 DMA 可能实现更好的总线性能。

13。对于使用 GPU 的开发人员来说，这种额外的复制操作更为明显，因为其目标外设的带宽消耗远高于磁盘或网络控制器等常规设备。无论外设类型如何，如果没有 DMA，驱动程序必须使用 CPU 来复制数据到特殊硬件缓冲区或从中复制数据。

为了便于 DMA 操作，操作系统的虚拟内存管理器（VMM）提供了一项名为“页锁定”的服务。被页锁定的内存已被 VMM 标记为不可逐出的，因此其物理地址无法更改。一旦内存被页锁定，驱动程序可以将其 DMA 硬件编程为引用该内存的物理地址。这个硬件设置与页锁定本身是分开的独立操作。因为页锁定使得操作系统无法将底层物理内存用于其他用途，过度的页锁定可能会对性能产生不利影响。

非页锁定的内存被称为“可分页”内存。被页锁定的内存有时被称为“固定”内存，因为其物理地址不能被操作系统更改（它已经被固定在该位置）。

#### 3.8.1. 固定主机内存

“固定”主机内存是通过 CUDA 的函数`cuMemHostAlloc() / cudaHostAlloc()`分配的。这块内存被页锁定，并由当前的 CUDA 上下文设置为 DMA 操作。^(14)

14。CUDA 开发人员常常会问页锁定内存和 CUDA 的“固定”内存是否有区别。确实有！通过 CUDA 分配或注册的固定内存是为了 GPU(s) 的直接访问而映射的；普通的页锁定内存则不是。

CUDA 跟踪以这种方式分配的内存范围，并自动加速引用固定内存的 memcpy 操作。异步 memcpy 操作仅适用于固定内存。应用程序可以使用`cuMemHostGetFlags()`函数来确定给定的主机内存地址范围是否为固定内存。

在操作系统文档的上下文中，*页面锁定*和*固定*是同义词，但对于 CUDA 来说，可以将“固定”内存视为已被页面锁定并为硬件访问而映射的主机内存。“页面锁定”仅指操作系统机制，用于将主机内存页面标记为不可驱逐。

#### 3.8.2\. 可移植固定内存

*可移植*固定内存在页面锁定后被映射到所有 CUDA 上下文中。执行此操作的底层机制较为复杂：当执行可移植固定分配时，它会在返回之前被映射到所有 CUDA 上下文中。此外，每当创建 CUDA 上下文时，所有可移植固定内存分配会在返回之前映射到新的 CUDA 上下文中。对于可移植内存分配或上下文创建，如果无法执行这些映射操作，将导致分配或上下文创建失败。幸运的是，从 CUDA 4.0 开始，如果启用了 UVA（统一虚拟寻址），*所有*固定分配都是可移植的。

#### 3.8.3\. 映射固定内存

*映射*的固定内存被映射到 CUDA 上下文的地址空间中，因此内核可以读取或写入该内存。默认情况下，固定内存不会被映射到 CUDA 地址空间，因此不会被内核的无效写操作破坏。对于集成 GPU，映射的固定内存实现了“零复制”：由于主机（CPU）和设备（GPU）共享相同的内存池，它们可以在不需要显式复制的情况下交换数据。

对于离散 GPU，映射的固定内存使得主机内存可以被内核直接读取或写入。对于少量数据，这可以消除显式内存复制命令的开销。映射的固定内存对于写操作尤其有利，因为无需覆盖延迟。从 CUDA 4.0 开始，如果启用了 UVA（统一虚拟寻址），*所有*固定分配都被映射。

#### 3.8.4\. 主机内存注册

由于开发人员（特别是库开发人员）并不总是能分配他们想要访问的内存，CUDA 4.0 添加了将现有虚拟地址范围“注册”供 CUDA 使用的功能。`cuMemHostRegister()/cudaHostRegister()` 函数接受一个虚拟地址范围并对其进行页面锁定和映射，以供当前 GPU 使用（或者如果指定了 `CU_MEMHOSTREGISTER_PORTABLE` 或 `cudaHostRegisterPortable`，则对所有 GPU 生效）。主机内存注册与 UVA（统一虚拟地址）之间有一种特殊关系，即任何符合注册条件的地址范围，在初始化 CUDA 驱动时，不能包含在用于 UVA 目的的虚拟地址范围内。

### 3.9\. CUDA 数组和纹理

CUDA 数组从与设备内存相同的物理内存池中分配，但它们具有针对二维和三维局部性优化的不可见布局。图形驱动程序使用这些布局来存储纹理；通过将索引与地址分离，硬件可以对二维或三维元素块进行操作，而不是一维行。对于表现出稀疏访问模式的应用程序，特别是具有维度局部性模式的应用程序（例如计算机视觉应用），CUDA 数组是一个明显的优势。对于具有规律访问模式的应用程序，特别是那些几乎没有重用或其重用可以通过应用程序在共享内存中显式管理的应用程序，设备指针显然是更好的选择。

一些应用程序，例如图像处理应用程序，处于一个灰色区域，其中设备指针和 CUDA 数组之间的选择并不明显。在其他条件相同的情况下，设备内存可能优于 CUDA 数组，但可以使用以下考虑因素来帮助决策。

• CUDA 数组不会占用 CUDA 地址空间。

• 在 WDDM 驱动程序（Windows Vista 及更高版本）上，系统可以自动管理 CUDA 数组的驻留。

• CUDA 数组只能驻留在设备内存中，GPU 可以在传输数据通过总线时在两种表示方式之间进行转换。对于某些应用，保持主机内存中的步幅表示和设备内存中的 CUDA 数组表示是最好的方法。

#### 3.9.1\. 纹理引用

*纹理引用* 是 CUDA 用来设置纹理硬件以“解释”底层内存内容的对象。^(15) 这种间接访问方式的存在部分原因是，因为允许多个纹理引用指向相同的内存，但具有不同的属性。

15. 在 CUDA 3.2 之前，纹理引用是唯一可以从 CUDA 数组读取数据的方式，除了显式的 memcpy。如今，表面引用不仅可以用于从 CUDA 数组读取数据，还可以写入它们。

纹理引用的属性可能是 *不可变的* —— 即，在编译时指定，并且无法改变，否则会导致应用程序行为不正确；或者是 *可变的* —— 即，应用程序可以改变纹理的行为，而编译器对此不可见（表 3.5）。例如，纹理的维度（1D、2D 或 3D）是不可变的，因为编译器必须知道它才能传递正确数量的输入参数并发出正确的机器指令。相反，过滤和寻址模式是可变的，因为它们会隐式地改变应用程序的行为，而编译器对此毫不知情或参与。

![图片](img/03tab05.jpg)

*表 3.5* 可变和不可变纹理属性

CUDA 运行时（语言集成）和 CUDA 驱动程序 API 对纹理引用的处理方式大不相同。在这两种情况下，纹理引用是通过调用名为 `texture` 的模板来声明的。

texture<Type, Dimension, ReadMode> Name;

其中 *Type* 是内存中被纹理读取的元素类型，*Dimension* 是纹理的维度（1、2 或 3），*ReadMode* 指定在纹理引用读取时，整数值纹理类型是否应转换为归一化浮点值。

纹理引用必须先*绑定*到底层内存，才能使用。硬件在从 CUDA 数组读取纹理时会进行更好的优化，但在以下情况下，应用程序通过设备内存读取纹理会带来好处。

• 它列举了纹理缓存，作为带宽聚合器。

• 它使得应用程序可以绕过合并限制。

• 它避免了在读取最好通过设备内存写入的内存时产生多余的复制。例如，视频编解码器可能希望将帧输出到设备内存，但通过纹理从中读取。

一旦纹理引用绑定到底层内存，CUDA 核心可以通过调用 `tex* 内建函数` 来读取内存，例如 表 3.6 中给出的 `tex1D()`。

![图片](img/03tab06.jpg)

*表 3.6* 纹理内建函数

*注意:* 通过全局加载/存储或表面加载/存储执行的纹理读写之间没有一致性保证。因此，CUDA 核心必须小心避免从同时被其他方式访问的内存中读取纹理。

##### CUDA 运行时

要将内存绑定到纹理，应用程序必须调用 表 3.7 中的某个函数。CUDA 运行时应用程序可以通过直接赋值结构成员来修改纹理引用的可变属性。

点击这里查看代码图片

texture<float, 1, cudaReadModeElementType> tex1;

...

tex1.filterMode = cudaFilterModeLinear; // 启用线性过滤

tex1.normalized = true; // 纹理坐标将被归一化

![图片](img/03tab07.jpg)

*表 3.7* 将设备内存绑定到纹理的函数

对这些结构成员的赋值会立即生效；无需重新绑定纹理。

##### 驱动程序 API

使用驱动程序 API 时，由于 CPU 代码和 GPU 代码之间有更严格的分隔，任何在 CUDA 模块中声明的纹理引用都必须通过`cuModuleGetTexRef()`查询，该函数返回一个`CUtexref`。与 CUDA 运行时不同，纹理引用必须使用*所有*正确的属性进行初始化——包括可变和不可变属性——因为编译器不会将纹理引用的不可变属性编码到 CUDA 模块中。表 3.8 总结了可以用于将纹理引用绑定到内存的驱动程序 API 函数。

![Image](img/03tab08.jpg)

*表 3.8* 驱动程序 API 函数用于将内存绑定到纹理

#### 3.9.2\. 表面引用

*表面引用*是 CUDA 的一个较新功能，Tesla 级别的 GPU 不支持，它使得 CUDA 核心能够通过表面加载/存储内在函数读写 CUDA 数组。它们的主要目的是使 CUDA 核心能够直接写入 CUDA 数组。在表面加载/存储功能出现之前，内核必须先写入设备内存，然后执行设备→数组的内存拷贝（memcpy）操作，将输出复制并转换成 CUDA 数组。

与纹理引用相比，纹理引用可以根据设置的方式从输入坐标到输出格式进行转换，而表面引用则暴露了一个简单的按位接口，用于读取和写入 CUDA 数组的内容。

你可能会想，为什么 CUDA 没有实现直接对 CUDA 数组操作的表面加载/存储内在函数（就像 OpenCL 做的那样）。原因是为了未来能够支持更复杂的表面加载/存储操作，这些操作能够以更精巧的方式转换为底层表示，例如使样本能够通过分数坐标“撒布”到 CUDA 数组中，或者与抗锯齿图形表面进行交互。目前，CUDA 开发者将不得不在软件中实现此类操作。

### 3.10\. 图形互操作性

图形互操作性（或称为“图形互操作”）系列函数使得 CUDA 能够读写属于 OpenGL 或 Direct3D API 的内存。如果应用程序通过主机内存共享数据能够达到可接受的性能，那么就不需要这些 API 了。但由于本地内存带宽可超过 140GB/s，而 PCI Express 带宽在实际情况中很少超过 6GB/s，因此在可能的情况下，为应用程序提供将数据保留在 GPU 上的机会是非常重要的。通过使用图形互操作 API，CUDA 内核可以将数据写入图像和纹理，然后将其融入到 OpenGL 或 Direct3D 执行的图形输出中。

由于图形驱动和 CUDA 驱动必须在后台协同工作以实现互操作性，应用程序必须尽早表明其进行图形互操作的意图。特别是，必须通过调用特殊的上下文创建 API，如`cuD3D10CtxCreate()`或`cudaGLSetDevice()`，通知 CUDA 上下文它将与给定的 API 进行互操作。

驱动之间的协同工作还促使图形 API 和 CUDA 之间的资源共享分为两个步骤进行。

**1.** *注册：* 可能是一个昂贵的操作，表明开发者打算共享资源给底层驱动，这可能促使驱动移动和/或锁定相关资源。

**2.** *映射：* 预期会频繁发生的轻量级操作。

在早期版本的 CUDA 中，所有四个图形 API（OpenGL、Direct3D 9、Direct3D 10 和 Direct3D 11）与图形互操作性的 API 是严格分开的。例如，针对 Direct3D 9 互操作性，以下函数需要配合使用。

• `cuD3D9RegisterResource()/cudaD3D9RegisterResource()`

• `cuD3D9MapResources()/cudaD3D9MapResources()`

• `cuD3D9UnmapResources()/cudaD3D9UnmapResources()`

• `cuD3D9UnregisterResource()/cudaD3D9UnregisterResource()`

由于底层硬件能力相同，无论使用何种 API 访问，它们中的许多功能在 CUDA 3.2 中被合并。注册函数仍然是特定于 API 的，因为它们需要特定于 API 的绑定，但映射、取消映射和注销资源的函数已被统一。CUDA 3.2 中与上述功能对应的 API 如下。

• `cuD3D9RegisterResource()/cudaD3D9RegisterResource()`

• `cuGraphicsMapResources()/cudaGraphicsMapResources()`

• `cuGraphicsUnmapResources()/cudaGraphicsUnmapResources()`

• `cuGraphicsUnregisterResource()/cudaGraphicsUnregisterResource()`

Direct3D 10 的互操作性 API 相同，不同之处在于开发人员必须使用 `cuD3D10RegisterResource()/cudaD3D10RegisterResource()`，而不是 `cuD3D9*` 变体。

CUDA 3.2 还增加了通过 CUDA 数组访问图形 API 中的纹理的功能。在 Direct3D 中，纹理只是另一种类型的“资源”，可以通过 `IDirect3DResource9 *`（或 `IDirect3DResource10 *` 等）进行引用。在 OpenGL 中，提供了一个单独的函数 `cuGraphicsGLRegisterImage()`。

### 3.11. CUDA 运行时和 CUDA 驱动程序 API

CUDA 运行时（“CUDART”）简化了语言集成，使得 CUDA 编程变得非常容易。通过自动处理诸如初始化上下文、加载模块等任务，特别是通过允许内联调用内核与其他 C++ 代码，CUDART 让开发人员专注于快速使代码运行。一些 CUDA 抽象，如 CUDA 模块，不能通过 CUDART 访问。

相比之下，驱动程序 API 暴露了所有 CUDA 抽象，并允许开发人员根据应用程序的需要对其进行操作。驱动程序 API 不提供任何性能提升，而是为需要的应用程序（如具有插件架构的大规模商业应用程序）提供显式的资源管理。

* * *

驱动程序 API 的速度与 CUDA 运行时没有显著差异。如果你想提升 CUDA 应用程序的性能，可以考虑其他方法。

* * *

大多数 CUDA 特性同时适用于 CUDART 和驱动程序 API，但有一些特性是某一方特有的。表 3.9 总结了它们的区别。

![Image](img/03tab09.jpg)![Image](img/03tab09a.jpg)

*表 3.9.* CUDA 运行时与驱动程序 API 特性对比

在这两个 API 之间，像 memcpy 这样的操作功能上通常是相同的，但接口可能会有很大不同。流 API 几乎是相同的。

![Image](img/089tab01.jpg)

事件 API 之间有一些小差异，CUDART 提供了一个单独的 `cudaEventCreateWithFlags()` 函数，如果开发者希望指定标志字（这是创建阻塞事件所需的）。

![Image](img/089tab02.jpg)

memcpy 函数是接口差异最大的一类，尽管它们的底层功能是相同的。CUDA 支持三种内存变体——主机内存、设备内存和 CUDA 数组——它们是参与内存类型的排列组合，并且支持 1D、2D 或 3D 的 memcpy。因此，memcpy 函数必须包含一个庞大的不同函数家族，或者包含少数几个支持多种 memcpy 类型的函数。

CUDA 中最简单的 memcpy 函数用于在主机内存和设备内存之间复制数据，但即便是这些函数的接口也有所不同：CUDART 使用 `void *` 来表示主机和设备指针的类型，并通过一个带方向参数的单一 memcpy 函数，而驱动程序 API 使用 `void *` 来表示主机内存，`CUdeviceptr` 来表示设备内存，并为不同的 memcpy 方向提供三个独立的函数（`cuMemcpyHtoD()`、`cuMemcpyDtoH()` 和 `cuMemcpyDtoD()`）。下面是 CUDART 和驱动程序 API 在三种主机↔设备 memcpy 变体下的等效表示。

![Image](img/090tab01.jpg)

对于 2D 和 3D 内存复制，驱动程序 API 实现了一些函数，这些函数接受描述符结构并支持所有排列的内存复制，包括低维内存复制。例如，如果需要，可以使用`cuMemcpy3D()`执行 1D 主机→设备内存复制，而不是使用`cuMemcpyHtoD()`。

点击这里查看代码图像

CUDA_MEMCPY3D cp = {0};

cp.dstMemoryType = CU_MEMORYTYPE_DEVICE;

cp.dstDevice = dptr;

cp.srcMemoryType = CU_MEMORYTYPE_HOST;

cp.srcHost = host;

cp.WidthInBytes = bytes;

cp.Height = cp.Depth = 1;

status = cuMemcpy3D( &cp );

CUDART 使用描述符结构的组合来处理更复杂的内存复制（例如，`cudaMemcpy3D()`），同时使用不同的函数来覆盖不同的内存类型。与`cuMemcpy3D()`类似，CUDART 的`cudaMemcpy3D()`函数接受一个描述符结构，可以描述任何排列的内存复制，包括跨维度的内存复制（例如，从 2D CUDA 数组的行进行 1D 复制，或将 2D CUDA 数组复制到 3D CUDA 数组的切片）。其描述符结构略有不同，因为它嵌入了其他结构；两个 API 的 3D 内存复制结构在表 3.10 中进行了并排比较。

![图片](img/03tab10.jpg)

*表 3.10* 3D 内存复制结构

两种 3D 内存复制函数的用法相似。它们设计为零初始化，开发人员设置给定操作所需的成员。例如，执行主机→3D 数组复制可以如下进行。

![图片](img/091tab01.jpg)

对于覆盖整个 CUDA 数组的 3D 复制，源和目标偏移量由第一行设置为 0，不必再次引用。与函数的参数不同，代码只需引用复制所需的参数，如果程序必须执行多个类似的复制（例如，填充多个 CUDA 数组或设备内存区域），则描述符结构可以重用。

## 第四章 软件环境

本章概述了 CUDA 开发工具和可以承载 CUDA 应用程序的软件环境。章节内容专门讲解了 NVIDIA 工具包中的各种工具。

• `nvcc:` CUDA 编译驱动程序

• `ptxas:` PTX 汇编器

• `cuobjdump:` CUDA 对象文件转储工具

• `nvidia-smi:` NVIDIA 系统管理接口

第 4.5 节介绍了亚马逊的 EC2（弹性计算云）服务，以及如何使用它通过互联网访问支持 GPU 的服务器。本章更多的是作为参考，而非教程。在本书的第三部分中给出了示例用法。

### 4.1. `nvcc`—CUDA 编译驱动程序

`nvcc`是 CUDA 开发人员用来将源代码转换为功能性 CUDA 应用程序的编译驱动程序。它可以执行多种功能，从简单的针对 GPU 的`.cu`文件编译，到复杂的编译、链接和执行示例程序（这是本书中许多示例程序推荐的用法）。

作为一个编译驱动程序，`nvcc`除了设置构建环境并启动本地工具（例如系统上安装的 C 编译器）和 CUDA 特定的命令行工具（如`ptxas`）外，不做其他事情来构建 CUDA 代码。它实现了许多合理的默认行为，这些行为可以通过命令行选项来覆盖；其确切行为取决于主命令行选项请求的“编译轨迹”。

表 4.1 列出了`nvcc`支持的文件扩展名及其默认行为（*注意：*某些中间文件类型，例如包含由 CUDA 前端生成的主机代码的`.i/.ii`文件，在这里没有列出）。表 4.2 列出了编译阶段选项及相应的编译路径。表 4.3 列出了影响环境的`nvcc`选项，例如包含目录的路径。表 4.4 列出了影响输出的`nvcc`选项，例如是否包含调试信息。表 4.5 列出了“传递”选项，这些选项使`nvcc`能够将选项传递给它调用的工具，例如`ptxas`。表 4.6 列出了不容易分类的`nvcc`选项，例如`--keep`选项，它指示`nvcc`不要删除它创建的临时文件。

![Image](img/04tab01.jpg)

*表 4.1* `nvcc` 输入文件的扩展名

![Image](img/04tab02.jpg)

*表 4.2* 编译路径

![Image](img/04tab03.jpg)![Image](img/04tab03a.jpg)

*表 4.3* `nvcc` 选项（环境）

![Image](img/04tab04.jpg)

*表 4.4* 指定编译器/链接器行为的选项

![Image](img/04tab05.jpg)

*表 4.5* `nvcc` 传递选项

![Image](img/04tab06.jpg)![Image](img/04tab06a.jpg)

*表 4.6* 各种 `nvcc` 选项

表 4.7 列出了与代码生成相关的`nvcc`选项。`--gpu-architecture`和`--gpu-code`选项尤其令人困惑。前者控制要编译的*虚拟*GPU 架构（即要发出的 PTX 版本），而后者控制要编译的*实际*GPU 架构（即要发出的 SM 微码版本）。`--gpu-code`选项必须指定的 SM 版本至少与`--gpu-architecture`指定的版本相同。

![Image](img/04tab07.jpg)![Image](img/04tab07a.jpg)

*表 4.7.* `nvcc` 编译选项

`--export-dir` 选项指定一个目录，所有设备代码镜像将被复制到该目录。它作为设备代码的存储库，CUDA 驱动在应用程序运行时可以检查该存储库（此时该目录应位于 `CUDA_DEVCODE_PATH` 环境变量中）。该存储库可以是一个目录，也可以是一个 ZIP 文件。无论哪种情况，CUDA 都会保持目录结构，以便 CUDA 驱动进行代码查找。如果指定了文件名但该文件不存在，则会在该位置创建一个目录结构（而不是 ZIP 文件）。

### 4.2. `ptxas`—PTX 汇编器

`ptxas`，这个将 PTX 编译成特定 GPU 微码的工具，在 CUDA 生态系统中占据着独特的地位，因为 NVIDIA 将它同时提供在离线工具中（开发者将其编译进应用程序）和作为驱动的一部分，从而支持所谓的“在线”或“即时” (JIT) 编译（即在运行时进行编译）。

在离线编译时，如果通过 `--gpu-code` 命令行选项指定了任何实际的 GPU 架构，`ptxas` 通常由 `nvcc` 调用。在这种情况下，可以通过 `nvcc` 的 `-Xptxas` 命令行选项将命令行选项（总结见 表 4.8）传递给 `ptxas`。

![Image](img/04tab08.jpg)![Image](img/04tab08a.jpg)![Image](img/04tab08b.jpg)

*表 4.8.* `ptxas` 命令行选项

开发者还可以通过调用 `cuModuleLoadDataEx()` 动态加载 PTX 代码，如下所示。

CUresult cuModuleLoadDataEx (

CUmodule *module，

const void *image，

unsigned int numOptions，

CUjit_option *options，

void **optionValues

);

`cuModuleLoadDataEx()` 接受一个指针图像并将相应的模块加载到当前上下文中。该指针可以通过映射 `cubin`、`PTX` 或 `fatbin` 文件来获取，或者通过将 `cubin`、`PTX` 或 `fatbin` 文件作为以 NULL 终止的文本字符串传递，亦可通过将 `cubin` 或 `fatbin` 对象纳入可执行资源并使用操作系统调用（如 Windows 的 `FindResource()`）来获取指针。选项通过数组形式传递，任何相应的参数通过 `optionValues` 传递。选项的总数由 `numOptions` 指定。任何输出将通过 `optionValues` 返回。支持的选项见 表 4.9。

![图片](img/04tab09.jpg)![图片](img/04tab09a.jpg)

*表 4.9.* `cuModuleLoadDataEx()` 选项

### 4.3. `cuobjdump`

`cuobjdump` 是一个工具，可用于检查 CUDA 生成的二进制文件。特别是，它对于检查 `nvcc` 生成的微码非常有用。指定 `--cubin` 参数给 `nvcc` 以生成 `.cubin` 文件，然后使用

cuobjdump --dump-sass <filename.cubin>

从 `.cubin` 文件中转储反汇编的微码。`cuobjdump` 的完整命令行选项列表见 表 4.10。

![图片](img/04tab10.jpg)![图片](img/04tab10a.jpg)

*表 4.10* `cuobjdump` 命令行选项

### 4.4. `nvidia-smi`

`nvidia-smi`，即 NVIDIA 系统管理接口，用于管理 Tesla 系列 NVIDIA GPU 板卡所在的环境。它可以报告 GPU 状态并控制 GPU 执行的各个方面，例如是否启用了 ECC 以及在给定 GPU 上可以创建多少个 CUDA 上下文。

当调用`nvidia-smi`并使用`-–help (-h)`选项时，它会生成一个使用说明，其中除了简要描述其功能和命令行选项外，还会列出支持的产品。特斯拉（Tesla）和奎达（Quadro）品牌的 GPU 完全支持，而 GeForce 品牌的 GPU 则获得有限支持。

许多受`nvidia-smi`支持的 GPU 板卡包含多个 GPU；`nvidia-smi`将这些板卡称为*单元*。某些操作，例如切换 LED（发光二极管）的状态，只能在单元级别进行。

`nvidia-smi`有几种操作模式。如果未给出其他命令行参数，它会列出可用 GPU 的摘要，可以通过表 4.11 中的命令行选项进一步精细化。否则，其他可用的命令行选项包括以下内容。

• 列表：`--list-gpus (-L)`选项显示可用 GPU 及其 UUID 的列表。用于细化列表的其他选项在表 4.11 中进行了总结。

![图片](img/04tab11.jpg)

*表 4.11* `nvidia-smi` 列表选项

• 查询：`--query (-q)`选项显示 GPU 或单元信息。用于细化查询的其他选项在表 4.12 中进行了总结。

![图片](img/04tab12.jpg)

*表 4.12* `nvidia-smi` 查询选项

• 文档类型定义（DTD）：`--dtd`选项生成`nvidia-smi`的 XML 格式输出的文档类型定义。`--filename (-f)`选项可选择指定输出文件；`--unit (-u)`选项则会将 GPU 板卡（而非 GPU）的 DTD 写入。

• 设备修改：可以使用表 4.13 中列出的选项设置 GPU 的持久状态，例如是否启用 ECC（错误校正）。

![图片](img/04tab13.jpg)![图片](img/04tab13a.jpg)

*表 4.13* `nvidia-smi` 设备修改选项

• 单元修改：`--toggle-led` 选项（`-t`）可以设置为 `0/GREEN` 或 `1/AMBER`。`--id (-i)` 选项可用于指定特定单元。

### 4.5\. 亚马逊 Web 服务

亚马逊 Web 服务是“基础设施即服务”（IAAS）云计算服务的领先供应商。它们的 Web 服务使客户能够分配存储、将数据传输进出其数据中心，并在其数据中心运行服务器。客户按需支付费用：每字节存储费用、每字节传输费用或每小时服务器实例费用。一方面，客户可以在无需投资自有基础设施的情况下访问几乎无限的计算资源，另一方面，他们只需为所使用的资源付费。由于灵活性以及云计算能够应对快速增长的需求（比如一个独立游戏开发者的游戏“爆火”），云计算正迅速普及。

AWS 的所有功能和使用方法的完整描述超出了本书的范围。在这里，我们介绍一些重要功能，供那些有兴趣试用支持 CUDA 的虚拟机的人参考。

• S3（简单存储服务）对象可以上传和下载。

• EC2（弹性计算云）实例可以启动、重启和终止。

• EBS（弹性块存储）卷可以创建、复制、附加到 EC2 实例以及销毁。

• 它具有安全组功能，类似于 EC2 实例的防火墙。

• 它具有密钥对功能，用于身份验证。

亚马逊 Web 服务的所有功能都可以通过 AWS 管理控制台访问，网址为 aws.amazon.com。AWS 管理控制台可以执行许多未在上面列出的任务，但本书中我们只需要使用前面提到的几个操作。

#### 4.5.1\. 命令行工具

AWS 命令行工具可以从 `[`aws.amazon.com/developertools`](http://aws.amazon.com/developertools)` 下载。查找“Amazon EC2 API Tools”。这些工具可以在 Linux 机器上开箱即用；Windows 用户可以安装 Cygwin。安装后，你可以使用 `ec2-run-instances` 等命令来启动 EC2 实例，使用 `ec2-describe-instances` 来列出正在运行的实例，或者使用 `ec2-terminate-instances` 来终止一组实例。管理控制台中能完成的操作，也可以通过命令行工具来完成。

#### 4.5.2\. EC2 与虚拟化

EC2，即“弹性计算云”，是 AWS 家族中的一员，允许客户“租用”一个支持 CUDA 的服务器，并仅为服务器实际使用的时间付费。这些虚拟计算机，看起来像独立服务器，被称为 *实例*。客户可以使用 EC2 的 Web 服务根据对计算资源的需求启动、重启或终止实例。

EC2 的一种支持技术是 *虚拟化*，它使得单台服务器能够同时托管多个“虚拟”操作系统。EC2 集群中的一台服务器可能会托管多个客户的运行实例，从而提高规模经济性并降低成本。不同的实例类型具有不同的特性和定价。它们可能拥有不同数量的 RAM、CPU 性能、^(1) 本地存储和 I/O 性能，按需定价可能从每小时 $0.085 到每小时 $2.40 不等。截止目前，支持 CUDA 的 `cg1.4xlarge` 实例类型每小时 $2.10，具有以下特点。

1。CPU 能力以 EC2 计算单元（ECU）来衡量。截止目前，某个实例的 ECUs 范围从 1（`m1.small` 实例类型）到 88.5（`cc2.8xlarge` 实例类型）。

• 23 GB 的 RAM

• 33.5 ECUs（两个四核 Intel Xeon X5570 “Nehalem” CPU）

• 1690 GB 的实例存储

• 64 位平台

由于 `cg1.4xlarge` 是“集群”实例系列的成员，因此在给定服务器上只能运行一个实例；此外，它连接到比其他 EC2 实例类型更高带宽的网络，以支持并行计算的集群计算。

#### 4.5.3\. 密钥对

访问 EC2 实例通过 *密钥对* 来实现。这个术语指的是公钥密码学中的核心概念：身份验证是通过私钥（仅限授权人员使用）和可以自由共享的公钥来进行的。

当创建密钥对时，私钥会以 `.pem` 文件的形式下载。创建密钥对后，需要仔细保管 `.pem` 文件的原因有两个：首先，任何拥有 `.pem` 文件的人都可以使用它来访问你的 EC2 计算资源；其次，无法重新获得私钥的副本！亚马逊并不提供密钥保存服务，因此一旦下载了私钥，它就由你负责保管。

示例 4.1 给出了一个 `.pem` 文件的示例。该格式非常方便，因为它包含锚点行（`“BEGIN RSA PRIVATE KEY”/ “END RSA PRIVATE KEY”`），并且是“7 位清洁”的（即仅使用 ASCII 文本字符），因此可以通过电子邮件发送、复制粘贴到文本字段中、附加到 `~/.ssh/authorized_keys` 文件中以启用免密登录，或者出版到书籍中。创建 EC2 实例时指定的密钥对名称对应的私钥文件用于访问该实例。要更具体地了解 `.pem` 文件如何用于访问 EC2 实例，请参见下面关于 Linux 和 Windows 的部分。

*示例 4.1.* `.pem` 文件示例。

点击这里查看代码图片

* * *

-----BEGIN RSA PRIVATE KEY-----

MIIEowIBAAKCAQEA2mHaXk9tTZqN7ZiUWoxhcSHjVCbHmn1SKamXqOKdLDfmqducvVkAlB1cjIz/

NcwIHk0TxbnEPEDyPPHg8RYGya34evswzBUCOIcilbVIpVCyaTyzo4k0WKPW8znXJzQpxr/OHzzu

tAvlq95HGoBobuGM5kaDSlkugOmTUXFKxZ4ZN1rm2kUo21N2m9jrkDDq4qTMFxuYW0H0AXeHOfNF

ImroUCN2udTWOjpdgIPCgYEzz3Cssd9QIzDyadw+wbkTYq7eeqTNKULs4/gmLIAw+EXKE2/seyBL

leQeK11j1TFhDCjYRfghp0ecv4UnpAtiO6nNzod7aTAR1bXqJXbSqwIDAQABAoIBAAh2umvlUCst

zkpjG3zW6//ifFkKl7nZGZIbzJDzF3xbPklfBZghFvCmoquf21ROcBIckqObK4vaSIksJrexTtoK

MBM0IRQHzGo8co6y0/n0QrXpcFzqOGknEHGk0D3ou6XEUUzMo8O+okwi9UaFq4aAn2FdYkFDa5X7

d4Y0id1WzPcVurOSrnFNkWl4GRu+pluD2bmSmb7RUxQWGbP7bf98EyhpdugOdO7R3yOCcdaaGg0L

hdTlwJ3jCP9dmnk7NqApRzkv7R1sXzOnU2v3b9+WpF0g6wCeM2eUuK1IY3BPl0Pg+Q4xU0jpRSr0

vLDt8fUcIdH4PXTKua1NxsBA1uECgYEA72wC3BmL7HMIgf33yvK+/yA1z6AsAvIIAlCHJOi9sihT

XF6dnfaJ6d12oCj1RUqG9e9Y3cW1YjgcdqQBk5F8M6bPuIfzOctM/urd1ryWZ3ddSxgBaLEO1h4c

3/cQWGGvaMPpDSAihs2d/CnnlVoQGiQrlWxDGzIHzu8RRV43fKcCgYEA6YDkj6kzlx4cuQwwsPVb

IfdtP6WrHe+Ro724ka3Ry+4xFPcarXj5yl5/aPHNpdPPCfR+uYNjBiTD90w+duV8LtBxJoF+i/lt

Mui4116xXMBaMGQfFMS0u2+z3aZI8MXZF8gGDIrI9VVfpDCi2RNKaT7KhfraZ8VzZsdAqDO8Zl0C

gYEAvVq3iEvMFl2ERQsPhzslQ7G93U/Yfxvcqbf2qoJIRTcPduZ90gjCWmwE/fZmxT6ELs31grBz

HBM0r8BWXteZW2B6uH8NJpBbfOFUQhk0+u+0oUeDFcGy8jUusRM9oijgCgOntfHMXMESSfT6a2yn

f4VL0wmkqUWQV2FMT4iMadECgYATFUGYrA9XTlKynNht3d9wyzPWe8ecTrPsWdj3rujybaj9OaSo

gLaJX2eyP/C6mLDW83BX4PD6045ga46/UMnxWX+l0fdxoRTXkEVq9IYyOlYklkoj/F944gwlFS3o

34J6exJjfAQoaK3EUWU9sGHocAVFJdcrm+tufuI93NyM0QKBgB+koBIkJG8u0f19oW1dhUWERsuo

poXZ9Kh/GvJ9u5DUwv6F+hCGRotdBFhjuwKNTbutdzElxDMNHKoy/rhiqgcneMUmyHh/F0U4sOWl

XqqMD2QfKXBAU0ttviPbsmm0dbjzTTd3FO1qx2K90T3u9GEUdWYqMxOyZjUoLyNr+Tar

-----END RSA PRIVATE KEY-----

* * *

#### 4.5.4\. 可用区（AZs）和区域

AWS 通过在不同的可用区（AZ）中提供服务，这些可用区彼此严格隔离，旨在防止某个可用区的故障影响其他可用区。对于 CUDA 开发人员来说，主要需要考虑的是，实例、EBS 卷和其他资源必须位于同一可用区才能进行互操作。

#### 4.5.5\. S3

S3（简单存储服务）旨在提供一种可靠的方式来存储数据以供以后检索。其“对象”（基本上是文件）以分层布局存储，层次结构的顶部是“桶”，并可以选择性地插入“文件夹”。

除了存储和检索（分别为“PUT”和“GET”）之外，S3 还包括以下功能。

• 权限控制：默认情况下，S3 对象仅对 S3 账户的所有者可访问，但可以将其设为公开，或者将权限授予特定的 AWS 账户。

• 对象可以被加密。

• 可以为 S3 对象关联元数据，例如文本文件的语言或该对象是否为图像。

• 日志记录：对 S3 对象执行的操作可以记录日志（日志作为更多的 S3 对象存储）。

• 减少冗余：S3 对象可以存储在较低的可靠性因子下，从而降低成本。

• 通知：可以设置自动通知，例如，当检测到某个减少冗余对象丢失时，可以通知客户。

• 对象生命周期管理：可以安排对象在指定时间后自动删除。

许多其他 AWS 服务将 S3 用作持久数据存储；例如，AMI 和 EBS 卷的快照存储在 S3 中。

#### 4.5.6\. EBS

EBS（弹性块存储）是基于网络的存储，可以分配、附加并与运行中的实例分离。AWS 客户还可以对 EBS 卷进行“快照”，为新的 EBS 卷创建模板。

EC2 实例通常具有一个包含操作系统和驱动程序软件的根 EBS 卷。如果需要更多存储，您可以创建并附加一个 EBS 卷，并在客户操作系统中挂载它。^(2)

2. 如果您希望 EBS 卷对从派生 AMI 启动的实例可用，您可能需要更改操作系统配置；请参阅本章中的“Linux 在 EC2 上”或“Windows 在 EC2 上”部分。

#### 4.5.7\. AMI

亚马逊机器映像（AMIs）是对 EC2 实例启动后“外观”的描述，包括操作系统和附加 EBS 卷的数量及内容。大多数 EC2 客户从亚马逊提供的“标准” AMI 开始，修改到满意为止，然后拍摄 AMI 的快照，以便可以使用相同的设置启动更多实例。

当实例启动时，EC2 将花费几分钟来调配请求的资源并启动虚拟机。一旦实例运行，您可以查询其 IP 地址，并使用在实例启动时指定的私钥通过互联网访问它。

实例的外部 IP 地址被纳入 DNS 名称中。例如，一个 `cg1.4xlarge` 实例可能被命名为

ec2-70-17-16-35.compute-1.amazonaws.com

该机器的外部 IP 地址是 `70.17.16.35.`^(3)

3. IP 地址已被更改以保护无辜者。

EC2 实例还具有可用于集群内部通信的内部 IP 地址。例如，如果您启动一个需要使用消息传递接口（MPI）等软件进行通信的实例集群，请使用内部 IP 地址。

#### 4.5.8\. Linux 在 EC2 上

EC2 支持多种不同版本的 Linux 操作系统，其中包括一个由 Red Hat 派生的亚马逊品牌版本（“Amazon Linux”）。一旦实例启动，可以通过`ssh`使用启动实例时所用的密钥对进行访问。使用上面的 IP 地址和 Listing 4.1 中的`Example.pem`文件，我们可以输入

ssh –i Example.pem ec2-user@70.17.16.35

（根用户名随 Linux 版本不同而不同：对于 Amazon Linux，根用户名是`ec2-user`，而 CentOS 使用`root`，Ubuntu 使用`ubuntu`。）

登录后，机器完全属于你了！你可以添加用户并设置他们的密码，设置 SSH 以实现无密码登录，安装所需的软件（如 CUDA 工具链），附加更多 EBS 卷，并设置临时磁盘。然后，你可以拍摄一个 AMI 快照，以便启动更多与你已经设置的实例完全相同的实例。

##### EBS

EBS（弹性块存储）卷易于创建，可以从空白卷开始，也可以通过创建快照的实时副本来生成。一旦创建，EBS 卷可以附加到实例上，并作为一个设备显示（如`/dev/sdf`，或者在较新的 Linux 内核中，显示为`/dev/xvdf`）。当 EBS 卷首次附加时，它只是一个原始块存储设备，必须使用类似`mkfs.ext3`的命令格式化后才能使用。格式化后，驱动器可以挂载到某个目录。

mount <设备> <目录>

最后，如果你想拍摄一个 AMI 快照，并希望该驱动器在使用衍生 AMI 启动的实例上可见，需编辑`/etc/fstab`以包含该卷。在创建 EBS 卷并附加到运行中的实例时，确保将其创建在与实例相同的可用区（例如 us-east-1b）。

##### 临时存储

许多 EC2 实例类型，包括 `cg1.4xlarge`，都有本地硬盘与之关联。这些磁盘在可用时，严格用于临时本地存储；与 EBS 或 S3 不同，未使用擦除编码或其他技术来提高磁盘的可靠性。为了强调这种降低的可靠性，这些磁盘被称为 *临时存储*。

要使临时磁盘可用，请在 ec2-run-instances 中指定 “-b” 选项——例如，

点击这里查看代码图片

ec2-run-instances –t cg1.4xlarge –k nwiltEC2 –b /dev/sdb=ephemeral0

/dev/sdc=ephemeral1

与 EBS 卷一样，临时磁盘必须先格式化（例如，mkfs.ext3）并挂载后才能使用，而且它们必须在 `fstab` 中有条目，才能在实例重启时重新出现。

##### 用户数据

用户数据可以在启动时或实例运行时指定（在这种情况下，实例必须重启）。然后，用户数据可以通过以下路径查询：

[`169.254.169.254/latest/user-data`](http://169.254.169.254/latest/user-data)

#### 4.5.9\. Windows 在 EC2 上

Windows 实例的访问方式与 Linux 实例略有不同。一旦启动，客户必须使用他们的私钥文件来获取 EC2 实例管理员账户的密码。你可以指定你的 `.pem` 文件，或者将其内容复制粘贴到 AWS 管理控制台中（如图 4.1 所示）。

![Image](img/04fig01.jpg)

*图 4.1* AWS Windows 密码获取。

默认情况下，这种密码生成行为仅适用于 AWS 提供的“原版”AMI。如果你对这些 AMI 执行“快照”操作，它们将保留快照时机器上的任何密码。要创建一个新的 Windows AMI，使其在启动时生成随机密码，可以运行“EC2 配置服务”工具（在开始菜单中可用），点击“打包”标签，然后点击名为“运行 Sysprep 并立即关机”的按钮（图 4.2）。点击该按钮后，任何基于此 AMI 创建的实例都会生成一个随机密码，就像原版 Windows AMI 一样。

![Image](img/04fig02.jpg)

*图 4.2* EC2 上 Windows 的 Sysprep。

##### 临时存储

为了使临时存储可以被 Windows 实例使用，必须在 `ec2-run-instances` 命令中指定 –b 选项，如下所示。

点击这里查看代码图片

ec2-run-instances –t cg1.4xlarge –k nwiltEC2 –b /dev/sdb=ephemeral0

/dev/sdc=ephemeral1 查询用户数据

##### 用户数据

用户数据可以在启动时或实例运行时指定（此时实例必须重新启动）。然后，可以在

[`169.254.169.254/latest/user-data`](http://169.254.169.254/latest/user-data)
