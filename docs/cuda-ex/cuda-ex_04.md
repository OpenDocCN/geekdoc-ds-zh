## 第一章：为什么选择 CUDA？为什么是现在？

曾几何时，在不太遥远的过去，并行计算被视为一种“异国情调”的追求，通常被视为计算机科学中的一种专业领域。这种看法近年来发生了深刻变化。计算世界已经发生了转变，现在并行计算不仅仅是一个深奥的领域，几乎每个有抱负的程序员都*需要*学习并行编程，才能在计算机科学领域发挥最大的效能。也许你拿起这本书时，对于并行编程在今天计算世界中的重要性以及它在未来几年将扮演的日益重要的角色心存疑虑。本章将回顾近年来支撑我们编写的软件的硬件发展趋势。通过这一过程，我们希望说服你，並行计算的革命*已经*发生，而且通过学习 CUDA C，你将能够为包含中央处理单元和图形处理单元的异构平台编写高性能应用程序。

### 1.1 章节目标

在本章中，你将完成以下内容：

• 你将了解并行计算日益重要的作用。

• 你将学习 GPU 计算和 CUDA 的简要历史。

• 你将了解一些成功应用 CUDA C 的案例。

### 1.2 并行处理的时代

近年来，计算行业的广泛转向并行计算已经成为热议话题。到 2010 年，几乎所有的消费者计算机都将配备多核中央处理器。从双核低端上网本到 8 核和 16 核工作站计算机，並行计算将不再仅限于异国情调的超级计算机或大型机。此外，像手机和便携式音乐播放器这样的电子设备，已开始融入并行计算能力，旨在提供远超其前身的功能。

软件开发人员将越来越需要应对各种并行计算平台和技术，以为日益复杂的用户群体提供新颖且丰富的体验。命令提示符逐渐被淘汰；多线程图形界面成为主流。仅能打电话的手机已经过时；能够同时播放音乐、浏览网页并提供 GPS 服务的手机才是未来。

#### 1.2.1 中央处理单元

在过去的 30 年中，提高消费计算设备性能的一个重要方法就是提高处理器时钟的运行速度。从 20 世纪 80 年代初期的第一批个人电脑开始，消费级中央处理单元（CPU）内部时钟大约在 1MHz 左右。大约 30 年后，大多数桌面处理器的时钟速度在 1GHz 到 4GHz 之间，比最初个人电脑的时钟速度快了近 1000 倍。尽管提高 CPU 时钟速度显然不是唯一提升计算性能的方法，但它一直是提高性能的可靠途径。

然而，近年来，制造商被迫寻找替代传统计算能力提升来源的方法。由于集成电路制造的各种基本限制，依赖处理器时钟速度不断上升来提取额外的处理能力已不再可行。由于功耗和热量限制，以及晶体管尺寸逼近物理极限，研究人员和制造商开始寻求其他解决方案。

在消费计算领域之外，超级计算机数十年来通过类似的方式实现了巨大的性能提升。超级计算机中使用的处理器性能已飞跃式上升，类似于个人电脑 CPU 的提升。然而，除了单个处理器性能的显著提高外，超级计算机制造商还通过稳步增加*处理器数量*，实现了巨大的性能飞跃。最快的超级计算机往往拥有成千上万甚至数十万的处理核心并行工作，这并不罕见。

在寻求个人电脑额外处理能力的过程中，超级计算机的提升提出了一个很好的问题：与其仅仅提高单个处理核心的性能，为什么不在个人电脑中放入多个处理核心呢？通过这种方式，个人电脑可以在不需要继续提高处理器时钟速度的情况下，持续提升性能。

2005 年，面对日益竞争激烈的市场和有限的选择，领先的 CPU 制造商开始提供具有两个计算核心的处理器，而非一个。随后几年，他们跟进发布了三核、四核、六核和八核的中央处理单元。这一趋势有时被称为*多核革命*，标志着消费计算市场演变中的一次巨大转折。

今天，购买一台仅含一个计算核心的桌面计算机已经相对困难。即使是低端、低功耗的中央处理器（CPU）也通常配备两个或更多核心。领先的 CPU 制造商已经宣布了 12 核和 16 核 CPU 的计划，进一步确认了并行计算已经成为现实。

### 1.3 GPU 计算的崛起

与中央处理器传统的数据处理管道相比，在图形处理单元（GPU）上执行通用计算是一个全新的概念。实际上，GPU 本身相较于整个计算领域来说是相对较新的技术。然而，在图形处理器上进行计算的想法并不像你想象的那样新。

#### 1.3.1 GPU 的简史

我们已经研究过中央处理器在时钟速度和核心数量方面的演变。与此同时，图形处理的状态经历了剧烈的变革。在 1980 年代末和 1990 年代初，图形驱动操作系统如微软 Windows 的流行促成了新型处理器市场的诞生。在 1990 年代初，用户开始为个人计算机购买 2D 显示加速器。这些显示加速器提供了硬件支持的位图操作，帮助提高图形操作系统的显示效果和可用性。

在同一时期，在专业计算领域，一家公司名为 Silicon Graphics 的公司在 1980 年代致力于推广三维图形在多个市场中的应用，包括政府和国防领域、科学和技术可视化，以及提供创造惊艳电影特效的工具。1992 年，Silicon Graphics 通过发布 OpenGL 库向硬件开放了编程接口。Silicon Graphics 希望 OpenGL 成为一种标准化、平台独立的 3D 图形应用编写方法。正如并行处理和 CPU 一样，这些技术最终也会进入消费级应用中，只是时间问题。

到了 1990 年代中期，消费者应用中使用 3D 图形的需求迅速上升，为两个相当重要的发展奠定了基础。首先，像《Doom》、《Duke Nukem 3D》和《Quake》这样的沉浸式第一人称游戏的发布，激发了人们创造越来越逼真的 PC 游戏 3D 环境的热情。尽管 3D 图形最终会进入几乎所有计算机游戏中，但新兴的第一人称射击游戏类型的流行显著加速了 3D 图形在消费计算中的普及。与此同时，像 NVIDIA、ATI Technologies 和 3dfx Interactive 等公司开始发布足够负担得起的图形加速器，引起了广泛关注。这些发展巩固了 3D 图形作为一种将在未来多年占据重要地位的技术。

NVIDIA 的 GeForce 256 的发布进一步推动了消费级图形硬件的能力。首次，变换和光照计算可以直接在图形处理器上进行，从而增强了更具视觉吸引力的应用的潜力。由于变换和光照已经是 OpenGL 图形管线的核心部分，GeForce 256 标志着图形管线越来越多地直接在图形处理器上实现的自然进程的开始。

从并行计算的角度来看，NVIDIA 于 2001 年发布的 GeForce 3 系列无疑代表了 GPU 技术的最重要突破。GeForce 3 系列是计算行业首个实现微软当时新发布的 DirectX 8.0 标准的芯片。该标准要求符合标准的硬件包含可编程的顶点着色和可编程的像素着色阶段。开发人员首次可以控制 GPU 上执行的具体计算。

#### 1.3.2 早期 GPU 计算

拥有可编程管线的 GPU 发布吸引了许多研究人员探索将图形硬件用于超越 OpenGL 或 DirectX 渲染的可能性。GPU 计算的早期方法极为复杂。因为像 OpenGL 和 DirectX 这样的标准图形 API 仍然是与 GPU 交互的唯一方式，所以任何试图在 GPU 上执行任意计算的尝试，仍然会受到在图形 API 内编程的限制。因此，研究人员通过尝试使他们的问题在 GPU 看来像传统渲染那样，探索通过图形 API 进行通用计算。

本质上，2000 年代初的 GPU 设计用于通过可编程算术单元（称为*像素着色器*）为屏幕上的每个像素生成颜色。一般来说，像素着色器使用其在屏幕上的`(x,y)`位置以及一些附加信息，通过计算最终颜色来结合各种输入。这些附加信息可能是输入颜色、纹理坐标或其他在着色器运行时传递给它的属性。但由于对输入颜色和纹理的算术操作完全由程序员控制，研究人员观察到，这些输入的“颜色”实际上可以是*任何*数据。

因此，如果输入实际上是表示除颜色以外的数字数据，程序员可以编写像素着色器来对这些数据进行任意计算。结果将作为最终像素的“颜色”返回给 GPU，尽管这些颜色只是程序员指示 GPU 对其输入执行计算的结果。研究人员可以读取这些数据，而 GPU 则对此一无所知。从本质上讲，GPU 被通过让这些任务看起来像是标准的渲染任务而被“欺骗”来执行非渲染任务。这种技巧非常巧妙，但也相当复杂。

由于 GPU 具有较高的算术吞吐量，这些实验的初步结果为 GPU 计算的光明未来带来了希望。然而，编程模型仍然过于限制，无法形成足够的开发者群体。由于程序只能从少数几个输入颜色和少数几个纹理单元中接收输入数据，因此存在严格的资源限制。程序员在内存中写入结果的方式和位置也存在严重限制，因此需要向任意内存位置写入数据（散射）的算法无法在 GPU 上运行。此外，几乎无法预测特定 GPU 如何处理浮点数据，甚至它是否能处理浮点数据，因此大多数科学计算无法使用 GPU。最后，当程序不可避免地计算出错误结果、无法终止或直接挂起机器时，几乎没有有效的调试方法来调试 GPU 上执行的任何代码。

就在这些限制已经足够严峻的情况下，任何*仍然*希望使用 GPU 进行通用计算的人员，都需要学习 OpenGL 或 DirectX，因为这仍然是与 GPU 交互的唯一方式。这不仅意味着需要将数据存储在图形纹理中，并通过调用 OpenGL 或 DirectX 函数来执行计算，还意味着必须使用专门的图形编程语言来编写计算，这些语言被称为*着色语言*。要求研究人员在应对严重的资源和编程限制的同时，还要学习计算机图形学和着色语言，然后才能尝试利用 GPU 的计算能力，这对于广泛接受而言，显然是一个过于庞大的障碍。

### 1.4 CUDA

直到 GeForce 3 系列发布五年后，GPU 计算才准备好迎接主流市场。2006 年 11 月，NVIDIA 发布了业界首款 DirectX 10 GPU——GeForce 8800 GTX。GeForce 8800 GTX 还是第一款采用 NVIDIA CUDA 架构的 GPU。该架构包括几个全新的组件，专门用于 GPU 计算，旨在缓解以前的图形处理器无法真正用于通用计算的多种限制。

#### 1.4.1 什么是 CUDA 架构？

与以往将计算资源划分为顶点着色器和像素着色器的架构不同，CUDA 架构包括了一个统一的着色器流水线，允许每个算术逻辑单元（ALU）都可以被程序调度，用于执行通用计算任务。由于 NVIDIA 打算将这代图形处理器用于通用计算，这些 ALU 被设计为符合 IEEE 单精度浮点运算要求，并采用了为通用计算而非专门针对图形设计的指令集。此外，GPU 上的执行单元允许对内存进行任意的读写访问，并可以访问一个由软件管理的缓存，称为*共享内存*。所有这些 CUDA 架构的特性都是为了创建一款在计算方面表现优异的 GPU，同时也能很好地完成传统的图形任务。

#### 1.4.2 使用 CUDA 架构

然而，NVIDIA 为了向消费者提供既能进行计算又能进行图形处理的产品，其努力并不能仅仅停留在生产包含 CUDA 架构的硬件上。无论 NVIDIA 在其芯片上增加了多少功能来促进计算，仍然没有办法在不使用 OpenGL 或 DirectX 的情况下访问这些功能。这不仅要求用户继续将计算任务伪装成图形问题，而且还需要继续使用图形导向的着色语言（如 OpenGL 的 GLSL 或 Microsoft 的 HLSL）来编写计算任务。

为了能够覆盖尽可能多的开发者，NVIDIA 在行业标准 C 语言的基础上添加了一小部分关键字，以便利用 CUDA 架构的一些特殊功能。在 GeForce 8800 GTX 发布几个月后，NVIDIA 公开了这门语言的编译器——CUDA C。由此，CUDA C 成为了 GPU 公司专门为促进 GPU 上的通用计算而设计的首个语言。

除了创建一种用于编写 GPU 代码的语言外，NVIDIA 还提供了一个专门的硬件驱动程序，以充分利用 CUDA 架构强大的计算能力。用户不再需要了解 OpenGL 或 DirectX 图形编程接口，也不需要强迫自己的问题看起来像计算机图形任务。

### 1.5 CUDA 的应用

自 2007 年初首次亮相以来，多个行业和应用通过选择使用 CUDA C 构建应用程序获得了巨大的成功。这些好处通常包括相较于之前的最先进实现，性能提升了几个数量级。此外，运行在 NVIDIA 图形处理器上的应用程序，相较于仅基于传统中央处理技术的实现，享有更高的每美元性能和每瓦性能。以下是一些人们将 CUDA C 和 CUDA 架构成功应用的例子。

#### 1.5.1 医学成像

过去 20 年里，受到乳腺癌悲剧影响的人数大幅增加。在许多人不懈努力的推动下，近年来乳腺癌预防和治疗的意识与研究也有所提升。最终，每一例乳腺癌都应该在足够早的阶段被发现，以防止辐射和化疗带来的剧烈副作用、手术留下的永久性痕迹以及那些未能响应治疗的病例所带来的致命后果。因此，研究人员都渴望找到快速、准确、且微创的方式来识别乳腺癌的早期迹象。

乳腺 X 线摄影是目前早期检测乳腺癌的最佳技术之一，但它也存在一些显著的局限性。需要拍摄两张或更多的影像，并且这些影像需要经过熟练的医生开发和读取，才能识别潜在的肿瘤。此外，这种 X 光检查会带来多次辐射病人胸部的风险。在仔细研究后，医生通常需要进一步进行更为具体的成像检查——甚至活检——以排除癌症的可能性。这些假阳性结果会导致昂贵的后续检查，并给病人带来不必要的压力，直到得出最终结论。

超声成像比 X 光成像更安全，因此医生常常将其与乳腺 X 线摄影联合使用，以辅助乳腺癌的护理和诊断。但传统的乳腺超声检查也有其局限性。因此，TechniScan Medical Systems 应运而生。TechniScan 开发了一种有前景的三维超声成像方法，但其解决方案未能投入实践的原因非常简单：计算限制。简而言之，将采集到的超声数据转化为三维图像所需的计算被认为过于耗时且昂贵，难以用于实际应用。

NVIDIA 基于 CUDA 架构的首个 GPU 以及其 CUDA C 编程语言的推出，为 TechniScan 提供了一个平台，使其能够将创始人的梦想变为现实。顾名思义，Svara 超声影像系统利用超声波对患者胸部进行成像。TechniScan Svara 系统依赖两颗 NVIDIA Tesla C1060 处理器来处理 15 分钟扫描生成的 35GB 数据。得益于 Tesla C1060 的强大计算能力，医生可以在 20 分钟内操作出女性乳房的高度详细、三维图像。TechniScan 预计从 2010 年开始，Svara 系统将广泛部署。

#### 1.5.2 计算流体力学

多年来，高效转子和叶片的设计一直是一种近乎神秘的技术。这些设备周围空气和流体的复杂运动无法通过简单的公式有效建模，因此准确的仿真往往在计算上过于昂贵，不具备现实性。只有世界上最大型的超级计算机才能提供与开发和验证设计所需的复杂数值模型相匹配的计算资源。由于很少有人能够使用这样的计算机，这类机器的设计创新继续停滞不前。

剑桥大学，继查尔斯·巴贝奇开创的伟大传统之后，一直致力于先进并行计算的积极研究。来自“多核小组”的格雷厄姆·普兰博士和托比亚斯·布兰德维克博士正确地识别了 NVIDIA CUDA 架构在加速计算流体力学方面的潜力，达到了前所未有的水平。他们最初的研究表明，GPU 驱动的个人工作站能够提供可接受的性能水平。随后，使用小型 GPU 集群轻松超越了他们更为昂贵的超级计算机，进一步确认了他们的猜想，即 NVIDIA GPU 的能力与他们想要解决的问题高度契合。

对于剑桥的研究人员来说，CUDA C 提供的巨大性能提升不仅仅是对其超级计算资源的简单、渐进性的增强。大量低成本 GPU 计算资源的可用性使得剑桥的研究人员能够进行快速实验。在几秒钟内获得实验结果简化了研究人员依赖的反馈过程，从而帮助他们实现突破性进展。因此，GPU 集群的使用从根本上改变了他们进行研究的方式。近乎交互式的仿真为以前受限的研究领域释放了新的创新和创意机会。

#### 1.5.3 环境科学

对环境友好型消费品需求的增加是全球经济快速工业化的自然结果。对气候变化的日益关注、燃料价格的不断上涨以及空气和水中污染物水平的不断增加，使得这种工业化进步的副作用变得尤为明显。洗涤剂和清洁剂长期以来一直是日常使用中最必要但也可能带来灾难性后果的消费品。因此，许多科学家开始探索在不降低其效能的情况下减少此类洗涤剂环境影响的方法。然而，要在不付出代价的情况下获得一些好处，可不是一件容易的事。

清洁剂的关键成分被称为*表面活性剂*。表面活性剂分子决定了洗涤剂和洗发水的清洁能力和质地，但它们通常被认为是清洁产品中最具环境破坏性的成分。这些分子附着在污垢上，然后与水混合，使得表面活性剂能够与污垢一起被冲洗掉。传统上，衡量新表面活性剂的清洁价值需要大量的实验室测试，涉及到多种材料和杂质的组合。这一过程，毫不意外，通常非常缓慢且昂贵。

天普大学一直在与行业领导者宝洁公司合作，利用分子模拟来研究表面活性剂与污垢、水和其他材料的相互作用。计算机模拟的引入不仅加速了传统实验室方法，还将测试的范围扩展到了多种环境条件的变体，远远超过了过去在实际测试中能覆盖的范围。天普大学的研究人员使用了由美国能源部阿梅斯实验室编写的 GPU 加速高度优化面向对象的多粒子动力学（HOOMD）模拟软件。通过将模拟分布到两块 NVIDIA Tesla GPU 上，他们成功地达到了与 Cray XT3 的 128 个 CPU 核心和 IBM BlueGene/L 机器的 1024 个 CPU 的相当性能。通过增加 Tesla GPU 的数量，他们的解决方案已经能够以比以往平台快 16 倍的性能模拟表面活性剂的相互作用。由于 NVIDIA 的 CUDA 技术将完成此类综合模拟的时间从几周缩短至几小时，未来几年应会大幅提高那些既高效又能减少环境影响的产品的出现。

### 1.6 章节回顾

计算行业正处于并行计算革命的边缘，而 NVIDIA 的 CUDA C 迄今为止已经成为并行计算领域最成功的语言之一。在本书的学习过程中，我们将帮助你学习如何用 CUDA C 编写自己的代码。我们将帮助你了解 NVIDIA 为 GPU 计算开发的 C 语言扩展和应用程序接口。你*不需要*了解 OpenGL 或 DirectX，也不需要具备计算机图形学方面的背景。

我们不会涉及 C 语言编程的基础内容，因此我们不推荐完全没有计算机编程经验的人阅读本书。对并行编程有一些了解会有所帮助，尽管我们并不*期望*你有并行编程经验。本书中将会解释你需要理解的与并行编程相关的术语或概念。事实上，有时候你可能会发现，传统的并行编程知识会让你对 GPU 计算产生错误的假设。所以，实际上，具备一定的 C 或 C++编程经验是完成本书学习的唯一前提。

在下一章，我们将帮助你为 GPU 计算配置机器，确保你拥有开始所需的硬件和软件组件。之后，你就可以开始动手实践 CUDA C 了。如果你已经有一些 CUDA C 的经验，或者确定你的系统已经正确配置，可以直接跳到第三章。
