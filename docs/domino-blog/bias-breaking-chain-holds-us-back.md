# 偏见:打破束缚我们的锁链

> 原文：<https://www.dominodatalab.com/blog/bias-breaking-chain-holds-us-back>

***演讲人简历**:[Vivienne Ming](https://www.linkedin.com/in/vivienneming/)博士被 Inc .杂志提名为十大科技关注女性之一，她是一位理论神经科学家、企业家和作家。她共同创立了她的第五家公司 Socos Labs，这是一家探索人类潜力未来的独立智库。Ming 博士创建了 Socos 实验室，将她与其他创造性专家的不同工作结合起来，扩大他们对全球政策问题的影响，无论是在公司内部还是在我们的社区。此前，Vivienne 是加州大学伯克利分校雷德伍德理论神经科学中心的访问学者，从事认知神经假体的研究。在空闲时间，Vivienne 发明了人工智能系统来帮助治疗她患有糖尿病的儿子，提前几周预测躁郁症患者的躁狂发作，并让孤儿难民与大家庭成员团聚。她是许多公司和非营利组织的董事会成员，包括 StartOut、Palm Center、Cornerstone Capital、Platypus Institute、Shiftgig、野象小姐资本和 SmartStones。明博士还经常谈到她对商业中的包容性和性别的人工智能驱动的研究。对于放松，她是一个妻子和两个孩子的母亲。*

***精选博文摘要:**[Vivienne Ming 博士在最近一次 Domino MeetUp 上的](https://twitter.com/neuraltheory)演讲深入探讨了偏见及其影响，包括算法、模型、企业和人类的潜在责任。明博士的证据包括为多个创业公司筹集的第一手知识，她在 Gild 担任首席科学家期间完成的数据分析，以及引用数据、经济、招聘和教育领域的研究。这篇博客文章提供了演讲的文本和视频片段。完整视频可供[观看](https://dominodatalab.wistia.com/medias/rjrdlfu6bh)。如果您有兴趣查看 Domino 过去活动的其他内容，请查看[数据科学弹出播放列表](https://dominodatalab.wistia.com/projects/pxqx1u53sr)。如果你有兴趣亲自参加一个活动，那么考虑一下即将到来的 [Rev](https://rev.dominodatalab.com?utm_source=blog&utm_medium=post&utm_campaign=) 。*

## 研究、实验和发现:科学的核心

研究、实验和发现是所有类型科学的核心，包括数据科学。Ming 博士以“做大量丰富数据工作的力量之一，是这整个范围-我的意思是，在这个世界上没有什么不是入门”开始了演讲。虽然 Ming 博士在整个演讲中提供了详细的见解和证据，指出了丰富数据工作的潜力，但这篇博客文章重点关注性别、姓名和种族人口统计数据中的偏见的影响和责任。它还涵盖了偏见不仅仅是一个数据或算法问题，它是一个人的问题。解决偏见的第一步是承认它的存在。

## 你看到变色龙了吗？偏见的根源

我们每个人都有偏见，并根据这些偏见做出评估。明博士用约翰内斯·斯托特的变色龙指出“偏见的根源是根本的，不可避免的”。许多人看到这个图像时，就像看到了变色龙。然而，[变色龙形象由两个全身涂满颜料的人组成](https://www.youtube.com/watch?v=3xWW4sTp4wA),被巧妙地放置成变色龙的样子。在下面的视频剪辑中，明博士指出

“我无法制造一个不带偏见的人工智能。世界上没有不偏不倚的老鼠。从一个非常基本的意义上来说，这些系统正在根据它们的不确定性做出决策，唯一合理的方法就是根据给定的数据尽力而为。问题是，当你拒绝承认我们的偏见有问题，并实际采取行动时。我们有大量的证据表明存在一个严重的问题，而且这个问题持续存在，不仅仅是小问题。但正如我稍后将要谈到的，它阻碍了我们从一个转变的世界中走出来，一个我认为任何人都可以自私地庆祝的世界。”

![Bias: Breaking the Chain that Holds Us Back video thumbnail](img/a80360b7e1e307b06256167835c8cfb2.png)

## 偏见就像拍在头上(或链条上)的东西，让我们止步不前

虽然历史上充满了偏见不被认为是一个问题的时刻，但也有人们解决社会强化的性别偏见的时刻。女性使用男性笔名[写史诗小说](https://en.wikipedia.org/wiki/George_Eliot)、[打仗](https://en.wikipedia.org/wiki/Cathay_Williams)、[赢得柔道冠军](https://www.nytimes.com/2009/11/23/sports/olympics/23kanokogi.html?_r=0)、[跑马拉松](https://kathrineswitzer.com/about-kathrine/1967-boston-marathon-the-real-story/)，甚至如明博士所指出的，在上世纪 60 年代创建了一家名为自由程序员的全女性软件公司。在会见中，明博士指出，女爵士夏羽“史蒂夫”雪莉的 TedTalk，“[为什么雄心勃勃的女人有平头？帮助她分析了两种截然不同的初创企业融资经历，这两种经历都是基于性别偏见。](https://www.ted.com/talks/dame_stephanie_shirley_why_do_ambitious_women_have_flat_heads/transcript)

在 Ming 博士与他人共同创建她目前的教育技术公司并获得学历证书之前，她从大学辍学，创办了一家电影公司。当...的时候

“我们创办了这家公司，有趣的是，尽管我们一无所有，也没有任何人应该投资的东西——我们没有一个剧本。我们没有天赋。从字面上看，我们甚至没有天赋。我们没有经验。我们一无所有。我们实际上是在打了几个电话之后，筹集了你在科技行业可能称之为种子资金的资金。”

然而，第二次筹集资金更加困难，因为她现在的公司，尽管有更多的学术，技术和商业证书。在一次与一个有 5 个合伙人的小公司的资金会议上，Ming 博士转述了最后一个合伙人是如何说的“‘你应该为你所建立的感到骄傲’”。当时，我想，哦，天哪，至少这些人中有一个是站在我们这边的。事实上，当我们离开房间时，他真的拍了拍我的头，这似乎有点奇怪。”这促使明博士考虑如何

“我的凭证第二次被转换了。没有人质疑我们的技术。他们很喜欢。他们质疑我们是否知道如何经营企业。人们喜欢的产品本身，而不是电影。第二次，一切都会变得简单得多。除了我能看到的唯一真正的区别是，第一次我是男人，第二次我是女人。”

这使明博士得出结论，并理解斯蒂芬妮·雪莉所说的雄心勃勃的女人有着平头是什么意思。明博士转述说

“从那以后，作为一名企业家，我明白了，一旦感觉他们在和他们最喜欢的侄女打交道，而不是和作为商人的我打交道，我就知道，我知道他们根本没有把我当回事。世界上所有的博士都不重要，我过去在其他公司的所有成功都不重要。你就是我的一切。我学到的是，要提前想好。不要浪费时间和精力，准备向那些根本无法理解你的人推销，当然，在很多情况下，这就是你所能得到的一切。"

明博士还指出，性别偏见在她变性前后所工作的组织中也有所体现。她指出，当她变性后开始工作时，

那是最后一天有人问我数学问题，这有点好笑。我碰巧也有心理学博士学位。但不知怎么一天到下一天，我都没忘记怎么做收敛证明。我没有忘记发明算法意味着什么。然而，人们就是这样处理的，那些以前就知道的人。你看，看到一个人换了一种皮肤，这种变化是多么强大。”

这种经历类似于 Dame Shirley 的经历，她为了在 20 世纪 60 年代创办一家价值数十亿美元的软件公司，“开始挑战当时的惯例，甚至在我的业务发展信件中把我的名字从“Stephanie”改为“Steve”，以便在任何人意识到他是一个“她”之前通过这扇门”。雪莉夫人颠覆了偏见，因为作为女性，她不能在股票交易所工作，不能驾驶公共汽车，或者“事实上，没有我丈夫的允许，我不能开银行账户”。然而，尽管有偏见，雪莉女士说

*“谁会想到超音速协和号黑匣子飞行记录器的编程会是一群在自己家里工作的女人做的”……后来，当它成为一家价值超过 30 亿美元的公司，我让 70 名员工成为百万富翁时，他们会说，“[干得好，史蒂夫](https://www.ted.com/talks/dame_stephanie_shirley_why_do_ambitious_women_have_flat_heads/transcript)！”*

虽然现在不再是 20 世纪 60 年代，但偏见的影响和责任仍然存在。然而，我们在数据科学领域能够访问数据，就偏见进行公开对话，这是避免数据科学项目和分析中的不准确性、培训数据责任和模型责任的第一步。如果在 2018 年，人们基于这样的假设来建立和训练模型，即拥有 XY 染色体的人类缺乏编码能力，因为他们只审查和使用了 20 世纪 60 年代雪莉女士公司的数据，会怎么样？请想一想，这就是发生在雪莉女士、明博士和许多其他人身上的事情。偏见的影响和责任有现实世界的后果。意识到这种偏见，然后解决它，推动行业向前发展，打破阻碍研究、数据科学和我们的链条。

## 说出我的名字:偏见被揭露

当明博士是 Gild 的首席科学家时，一名记者打电话给她，询问关于何塞·萨莫拉的报道。这也导致了明博士对她即将出版的书的研究。“与众不同的代价”，明博士在会上讲述了一些趣闻轶事(见视频剪辑)，并且[也为英国《金融时报》撰写了关于这项研究的文章](https://www.ft.com/content/1929cd86-3eb6-11e6-8716-a4a71e8140b0):

为了计算与众不同的税收，我利用了 Gild 收集的 1.22 亿份职业资料数据集，Gild 是一家专门从事招聘和人力资源技术的公司，我在该公司担任首席科学家。从这些数据中，我能够通过检查真实的个人来比较特定人群的职业轨迹。例如，我们的数据集有 151，604 人叫“乔”，103，011 人叫“何塞”。在只选择软件开发人员后，我们仍然分别有 7105 人和 4896 人，他们都是以写代码为生的人。通过分析他们的职业轨迹，我发现，与没有任何学位的乔相比，何塞通常需要硕士或更高的学位，才能在相同的工作质量下获得同等的晋升机会。与众不同的税收很大程度上是隐性的。人们不需要恶意行为就能被征收。这意味着 José需要额外的六年教育以及教育所需的所有学费和机会成本。这是与众不同的税，对何塞来说，这种税一生要花 50 万到 100 万美元。([《金融时报》](https://www.ft.com/content/1929cd86-3eb6-11e6-8716-a4a71e8140b0) )

![Bias: Breaking the Chain that Holds Us Back video thumbnail](img/025c6055f8a1d408c85e7756f030f477.png)

虽然这个特殊的例子侧重于种族导向的人口统计偏见，但在 meetup 的讨论中，Ming 博士引用了相当多的关于姓名偏见的研究。如果 Domino Data Science 博客的读者手头没有她引用的一些研究，那么已经发布的关于名字偏见的[研究样本](https://www.theatlantic.com/business/archive/2014/07/who-wins-in-the-name-game/374912/)包括:[暗示男性性别的名字](http://www.abajournal.com/files/NamesNLaw.pdf)、[、欧洲“听起来高贵”的姓氏](http://journals.sagepub.com/doi/abs/10.1177/0956797613494851)、[被认为是“容易发音的名字](https://ppw.kuleuven.be/okp/_pdf/Laham2012TNPEW.pdf)，这也暗示了[组织如何选择他们的名字](http://pages.stern.nyu.edu/~aalter/tribes.pdf)。然而，明博士并没有将讨论局限于性别和命名方面的偏见，她还深入探讨了人口统计偏见如何影响图像分类，尤其是种族方面的偏见。

## 图像分类中的偏见:错过 Uhura 和不解锁你的 iPhone X

在 Ming 博士担任 Gild 首席数据科学家之前，她能够看到 Paul Viola 的人脸识别[算法](https://en.wikipedia.org/wiki/Viola%E2%80%93Jones_object_detection_framework)演示。在那个演示中，她注意到算法没有检测到[乌胡拉](https://en.wikipedia.org/wiki/Uhura)。Viola 表示这是一个问题，将会得到解决。几年后，当明博士担任 Gild 的首席科学家时，她转述了她是如何接到“【华尔街日报】打来的电话”的，于是谷歌的人脸识别系统就把一对黑人夫妇标记为大猩猩。艾是种族主义者吗？我说，‘嗯，和我们其他人一样。就看你怎么养了。"

作为背景背景，2015 年，谷歌发布了一款新的照片应用，一名软件开发人员发现，[应用将两个有色人种标记为“大猩猩”](https://www.theguardian.com/technology/2015/jul/01/google-sorry-racist-auto-tag-photo-app)和 [Yonatan Zunger](https://www.linkedin.com/in/yonatanzunger/) 当时是谷歌[社交网站的首席架构师](https://twitter.com/yonatanzunger/status/615585375487045632)。自从 Yonatan Zunger 离开谷歌后，他就提供了关于偏见的坦诚评论。然后，在 2018 年 1 月[,《连线》对 2015 年的活动进行了后续报道](https://www.wired.com/story/when-it-comes-to-gorillas-google-photos-remains-blind/)。在文章中，《连线》测试了谷歌照片，发现大猩猩、黑猩猩、黑猩猩和猴子的标签“在 2015 年事件后被从搜索和图像标签中删除”。这一点得到了谷歌的证实。《连线》杂志还进行了一项测试，通过搜索“非洲裔美国人”、“黑人男性”、“黑人女性”或“黑人”，评估人们的观点，结果是“一幅放牧羚羊的图像”(在搜索“非洲裔美国人”时)，以及“人们的黑白图像，正确地按性别分类，但没有按种族过滤”。这指出了解决机器学习和模型中的偏见所涉及的持续挑战。偏见也有超出社会公正的含义。

正如 Ming 博士在下面的 meetup 视频剪辑中指出的那样，iPhone X 中还内置了面部识别功能。面部识别功能在识别全球有色人种的面部方面具有潜在的挑战。然而，尽管如此，明博士指出“但你必须认识到，这些都不是算法问题。这些都是人类的问题。”人类做出决策来构建算法、构建模型、训练模型，并推出包含具有广泛影响的偏见的产品。

![Bias: Breaking the Chain that Holds Us Back video thumbnail](img/64829c882bdbed7ede794fbe05008789.png)

## 结论

通过偏见将责任引入算法或模型不仅仅是一个数据或算法问题，而是一个人的问题。理解它是一个问题是解决它的第一步。在最近的多米诺会议上，明博士讲述了

*“AI 是一个了不起的工具，但它只是一个工具。它永远不会为你解决你的问题。你必须解决它们。尤其是在我的工作中，只有混乱的人类问题，只有混乱的人类解决方案。机器学习的神奇之处在于，一旦我们发现了其中的一些问题，我们实际上可以用它来帮助尽可能多的人，使这种方法具有成本效益，将解决方案推广到每个人。但如果你认为某个深度神经网络会以某种方式神奇地找出你想要雇用的人，而你一开始没有雇用合适的人，那么你认为那个数据集中发生了什么？”*

Domino 不断地策划和放大想法、观点和研究，以促进加速数据科学工作的讨论。Ming 博士在最近的 Domino MeetUp 上的演讲的完整视频可以在获得。另外，Ming 博士还在 Berkeley Institute of Data Science 发表了一篇关于“[使用机器学习驱动的应用程序最大限度地发挥人类潜力”](https://www.youtube.com/watch?v=h1A8gJwjkU8)的技术演讲。如果您对这些讲座的类似内容感兴趣，请随时访问[多米诺数据科学弹出播放列表](https://dominodatalab.wistia.com/projects/pxqx1u53sr)或参加即将到来的 [Rev](https://rev.dominodatalab.com/?utm_source=blog&utm_medium=post&utm_campaign=) 。