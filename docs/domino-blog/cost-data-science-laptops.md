# 在笔记本电脑上进行数据科学研究的成本

> 原文：<https://www.dominodatalab.com/blog/cost-data-science-laptops>

数据科学流程的核心是资源密集型的建模和验证任务。在这些任务中，数据科学家将尝试并丢弃数千个临时模型，以找到最佳配置。即使对于小数据集，这也可能需要几个小时来处理。

因此，依赖笔记本电脑或部门服务器处理能力的数据科学家必须在快速处理时间和模型复杂性之间做出选择。

无论哪种情况，性能和收入都会受到影响:

*   模型复杂性降低会导致模型不太精确，从而影响收入。
*   增加的处理时间意味着运行更少的实验，这限制了创新，因此影响了收入。

## 不太精确的模型的成本

以流失预测为例，这是几乎所有组织都有的事情。下面的分析表明，即使是 5000 个客户的小数据集，简单模型和复杂模型之间的准确度也有 10%的差异。**模型准确性 10%的微小差异会因客户流失而导致 28，750 美元的收入损失。**

我们的分析基于来自 UCI 知识库的流失数据集。该数据集包含 5，000 名电信客户的列表，每个客户的属性包括帐户长度和客户服务呼叫次数，以及客户是否有过交易。我们假设干预成本为 50 美元，成功率为 60%，每个客户的损失为 500 美元。

我们训练了 3 个模型来预测 1，000 个帐户的测试集上的客户流失。我们使用笔记本电脑质量的硬件，就像许多组织中的数据科学家一样。

| 模型 | 错过的搅拌 | 不必要的外联 | 费用 |
| --- | --- | --- | --- |
| GLMNET | One hundred and twenty | Seventeen | $36,850 |
| 马恩岛 | Thirty-eight | Fourteen | $12,100 |
| H2O 组合体 | Twenty-four | Eighteen | $8,100 |

### 使用 R’s glm net 的线性模型

这是数据科学家在硬件限制下可能训练的模型类型的代表。

结果:

*   测试集的总体准确率为 86%
*   120 名不明身份的顾客
*   17 例不必要的接触不太可能流失的客户
*   由于车型表现不佳造成 36，850 美元的损失

### 使用 R 的 GBM 的 GBM 模型

该模型利用了比线性模型更先进的算法，在预测客户流失方面提高了 8%。

结果:

*   总体准确率 94%
*   38 名不明身份的顾客
*   14 次不必要的外联
*   由于车型表现不佳，损失 12，100 美元

### 尖端堆叠 H2O 系综

这是建模技术的前沿。这是科学家*希望*做的那种建模数据，但受到硬件条件的限制。

该模型利用梯度提升机器、随机森林和深度学习神经网络来提供集合预测。它提供了最高的性能和最大的成本节约。

结果:

*   总体准确率为 95.8%
*   24 名不明身份的顾客
*   18 次不必要的外联
*   由于车型表现不佳，损失 8100 美元

尖端模型和简单模型之间的差异是 10%的精确度，这相当于 27，850 美元。请记住，这是一个只有 5000 名客户的小数据集，使用的是保守估计。在大型组织中，不太精确的模型的成本很容易达到几十万甚至几百万。

为什么不总是使用最好的模型呢？答案在于训练次数和有限处理能力的成本。

## 受限处理能力的成本

高性能模型需要更多的处理能力，在标准笔记本电脑上，训练这些模型可能需要*小时*。以下是我们分析中每个模型的训练时间:

| 模型 | 培训时间(笔记本电脑) | 错过的搅拌 | 不必要的外联 | 费用 |
| --- | --- | --- | --- | --- |
| GLMNET | 43 秒 | One hundred and twenty | Seventeen | $36,850 |
| 马恩岛 | 828 秒 | Thirty-eight | Fourteen | $12,100 |
| H2O 组合体 | 小时 | Twenty-four | Eighteen | $8,100 |

**在笔记本电脑等受限硬件上工作的数据科学家不太可能尝试高性能模型，因为他们要花半天时间才能得到结果。**这还没有考虑验证每个模型的结果所需的额外时间。

如果他们为了得到更精确的模型而选择等待几个小时，他们就没有时间去做其他可能会得到更好结果的实验了。这种机会成本导致创新缓慢或停滞，并且无法对组织产生重大影响。

这是一组可怕的选择，然而许多数据科学家每天都处于这种境地。

只要数据科学家被迫在受限的机器上工作，如笔记本电脑或自我管理的部门服务器，组织就将继续亏损和失去竞争优势。

## 另一个选择:云

解决方案是让数据科学家在云硬件上运行实验。

下表显示了在云中运行[时，我们的分析中每个模型的训练时间，证明了在不牺牲时间的情况下开发准确的模型是可能的。](https://www.dominodatalab.com/cloud-data-science/?utm_source=blog&utm_medium=post&utm_campaign=)

| 模型 | 培训时间(笔记本电脑) | 培训时间(云) | 错过的搅拌 | 不必要的外联 | 费用 |
| --- | --- | --- | --- | --- | --- |
| GLMNET | 43 秒 | 9 秒 | One hundred and twenty | Seventeen | $36,850 |
| 马恩岛 | 828 秒 | 27 秒 | Thirty-eight | Fourteen | $12,100 |
| H2O 组合体 | 小时 | 71 秒 | Twenty-four | Eighteen | $8,100 |

尖端的 H2O 模型——在笔记本电脑上需要几个小时——在一台 [AWS X1 实例](//blog.dominodatalab.com/high-performance-computing-with-amazons-x1-instance/)上只需一分多钟，成本约为 39 美分。这为组织节省了 27，850 美元，并让数据科学家有很多时间来尝试其他模型和实验。

## 结论

让数据科学家在笔记本电脑上工作的成本非常高。即使在处理小型数据集时，数据科学家也必须在开发精确模型和更快开发模型之间做出选择。这两种选择都会导致组织的收入损失。

云是数据科学团队的最佳家园。它使他们能够尝试更大胆的实验并使用尖端技术，从而为组织带来显著且可量化的投资回报。

让数据科学家访问按需和可扩展的云硬件，而不需要供应或维护云服务的最简单、最快速的方法是使用 Domino 等[数据科学平台](https://www.dominodatalab.com/?utm_source=blog&utm_medium=post&utm_campaign=)。