# 数据科学初学者的 10 个最佳机器学习算法

> 原文：<https://www.dataquest.io/blog/top-10-machine-learning-algorithms-for-beginners/>

June 26, 2019

自从*哈佛商业评论* [的文章](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century)将“数据科学家”命名为“21 世纪最性感的工作”以来，人们对机器学习的兴趣飙升。但如果你刚刚开始学习机器，可能会有点困难。这就是为什么我们重新启动我们非常受欢迎的关于初学者良好机器学习算法的帖子。

(这篇文章最初发表在 KDNuggets 上，名为[机器学习工程师需要知道的 10 种算法](https://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html)。已经用 perlesson 转贴了，最后一次更新是在 2019 年)。

这篇文章是针对初学者的。如果你在数据科学和机器学习方面有一些经验，你可能会对[这个更深入的关于用 Python](https://www.dataquest.io/blog/sci-kit-learn-tutorial/) 和`scikit-learn`进行机器学习的教程更感兴趣，或者对我们在这里开始的机器学习课程[更感兴趣。如果你还不清楚“数据科学”和“机器学习”之间的区别，这篇文章提供了一个很好的解释:](https://www.dataquest.io/course/machine-learning-fundamentals/)[机器学习和数据科学——是什么让它们不同？](https://www.netguru.com/blog/machine-learning-vs.-data-science-key-differences-and-similarities-you-should-know)

机器学习算法是可以从数据中学习并根据经验进行改进的程序，无需人工干预。学习任务可以包括学习将输入映射到输出的函数，学习未标记数据中的隐藏结构；或者“基于实例的学习”，其中通过将新实例(行)与存储在存储器中的来自训练数据的实例进行比较，为新实例产生类别标签。“基于实例的学习”并不从具体的实例中抽象出来。

## 机器学习算法的类型

有 3 种类型的机器学习(ML)算法:

#### 监督学习算法:

监督学习使用带标签的训练数据来学习将输入变量(X)变成输出变量(Y)的映射函数。换句话说，它在下面的等式中求解 *f* :

*Y = f (X)*

这允许我们在给定新输入时准确地生成输出。

我们将讨论两种类型的监督学习:分类和回归。

**分类**用于当输出变量为类别形式时，预测给定样本的结果。分类模型可能会查看输入数据，并尝试预测“生病”或“健康”等标签

**回归**用于当输出变量为实值形式时，预测给定样本的结果。例如，回归模型可以处理输入数据来预测降雨量、人的身高等。

我们在这篇博客中介绍的前 5 种算法——线性回归、逻辑回归、CART、朴素贝叶斯和 K 近邻(KNN)——都是监督学习的例子。

**集合**是另一种监督学习。这意味着结合多个单独较弱的机器学习模型的预测，以对新样本产生更准确的预测。本文的算法 9 和 10——用随机森林打包，用 XGBoost 增强——是集成技术的例子。

#### 无监督学习算法:

当我们只有输入变量(X)而没有相应的输出变量时，使用无监督学习模型。他们使用未标记的训练数据来建模数据的底层结构。

我们将讨论三种类型的无监督学习:

**关联**用于发现集合中项目同时出现的概率。它被广泛用于购物篮分析。例如，关联模型可能用于发现如果顾客购买面包，她/他有 80%的可能也购买鸡蛋。

**聚类**用于对样本进行分组，使得同一个聚类中的对象彼此之间的相似度高于另一个聚类中的对象。

**降维**用于减少数据集的变量数量，同时确保仍然传达重要信息。降维可以使用特征提取方法和特征选择方法来完成。特征选择选择原始变量的子集。特征提取执行从高维空间到低维空间的数据转换。示例:PCA 算法是一种特征提取方法。

我们在这里讨论的算法 6-8——Apriori、K-means、PCA——都是无监督学习的例子。

#### 强化学习:

强化学习是一种机器学习算法，它允许代理根据其当前状态，通过学习将最大化奖励的行为来决定最佳的下一步行动。

强化算法通常通过试错来学习最优动作。例如，想象一个视频游戏，玩家需要在特定的时间移动到特定的地方来获得分数。玩这种游戏的强化算法一开始会随机移动，但随着时间的推移，通过反复试验，它会知道何时何地需要移动游戏中的角色，以最大化其总点数。

## 量化机器学习算法的流行程度

的研究 [已经量化了 10 个最流行的数据挖掘算法，但是它们仍然依赖于调查回答的主观反应，通常是高级学术实践者。例如，在上面链接的研究中，被调查的人是 ACM KDD 创新奖、IEEE ICDM 研究贡献奖的获得者；06 年 KDD、06 年 ICDM 和 06 年 SDM 的项目委员会成员；以及 06 年 ICDM 的 145 名参加者。](https://dq-blog-files.s3.amazonaws.com/10Algorithms-08.pdf)

这篇文章中列出的 10 大算法是考虑到机器学习初学者而选择的。它们主要是我在孟买大学攻读计算机工程学士学位期间从“数据仓库和挖掘”(DWM)课程中学到的算法。我特别包括了最后两种算法(集成方法),因为它们经常被用来赢得游戏比赛。

## 事不宜迟，面向初学者的 10 大机器学习算法:

#### 1.线性回归

在机器学习中，我们有一组输入变量(x ),用于确定输出变量(y)。输入变量和输出变量之间存在关系。ML 的目标是量化这种关系。

![Linear-Regression](img/912967632063033689edeb6ed5b0aa37.png)

Figure 1: Linear Regression is represented as a line in the form of y = a + bx. [Source](https://bhagyeshvikani.blogspot.ca/2015/10/linear-regression.html)

在线性回归中，输入变量(x)和输出变量(y)之间的关系表示为 y = a + bx 形式的方程。因此，线性回归的目标是找出系数 a 和 b 的值。这里，a 是直线的截距，b 是直线的斜率。

图 1 显示了一个数据集的 x 和 y 值。目标是拟合一条最接近大多数点的线。这将减少数据点的 y 值和直线之间的距离(“误差”)。

#### 2.逻辑回归

线性回归预测是连续值(即以厘米为单位的降雨量)，逻辑回归预测是应用变换函数后的离散值(即学生是否及格)。

逻辑回归最适合二元分类:y = 0 或 1 的数据集，其中 1 表示默认类别。例如，在预测一个事件是否会发生时，只有两种可能性:它会发生(我们用 1 表示)或不会发生(0)。因此，如果我们预测一个病人是否生病，我们将使用数据集中的值`1`来标记生病的病人。

逻辑回归因其使用的变换函数而得名，称为逻辑函数 h(x)= 1/ (1 + ex)。这就形成了一个 S 形曲线。

在逻辑回归中，输出采用默认类别概率的形式(与直接产生输出的线性回归不同)。因为是概率，所以输出在 0-1 的范围内。因此，举例来说，如果我们试图预测患者是否生病，我们已经知道生病的患者被表示为`1`，因此如果我们的算法给患者分配 0.98 的分数，它认为该患者很可能生病。

此输出(y 值)是通过使用逻辑函数 h(x)= 1/ (1 + e^ -x)对 x 值进行对数变换而生成的。然后应用一个阈值将这个概率强制转换成二进制分类。

![Logistic-Function-machine-learning](img/bfe28ae210bf3207bc76ede7c1d38df9.png)

图 2:确定肿瘤是恶性还是良性的逻辑回归。如果概率 h(x)>= 0.5，则分类为恶性。[来源](https://athemathmo.github.io/2016/03/07/rusty-machine.html)

在图 2 中，为了确定肿瘤是否是恶性的，默认变量是 y = 1(肿瘤=恶性)。x 变量可以是肿瘤的度量，例如肿瘤的大小。如图所示，逻辑函数将数据集各种实例的 x 值转换到 0 到 1 的范围内。如果概率越过阈值 0.5(由水平线显示)，则肿瘤被分类为恶性。

logistic 回归方程*p(x)= e ^(B0+b1x)/(1+e(B0+b1x))*可以转化为 *ln(p(x) / 1-p(x)) = b0 + b1x* 。

逻辑回归的目标是使用训练数据来找到系数 b0 和 b1 的值，使得它将最小化预测结果和实际结果之间的误差。使用最大似然估计技术来估计这些系数。

#### 3.手推车

分类和回归树(CART)是决策树的一种实现。

分类和回归树的非终端节点是根节点和内部节点。终端节点是叶节点。每个非终端节点代表单个输入变量(x)和该变量上的分裂点；叶节点代表输出变量(y)。该模型如下用于进行预测:遍历树的分裂以到达叶节点，并输出叶节点处存在的值。

下图 3 中的决策树根据一个人的年龄和婚姻状况对他是会买跑车还是小型货车进行了分类。如果这个人已经超过 30 岁了，还没有结婚，我们就像下面这样遍历树:“超过 30 岁？”->是-> '结婚了吗？'->否。因此，模型输出一辆跑车。

![Decision-Tree-Diagram-machine-learning](img/849e218e2178a5bd1ba2c36c7c9f345a.png)

图 3:决策树的组成部分。[来源](https://www.hypertextbookshop.com/dataminingbook/public_version/contents/chapters/chapter001/section002/green/page001.html)

#### 4.朴素贝叶斯

为了计算一个事件发生的概率，假设另一个事件已经发生，我们使用贝叶斯定理。为了计算假设(h)为真的概率，给定我们的先验知识(d)，我们使用贝叶斯定理如下:

*P(h|d)= (P(d|h) P(h)) / P(d)*

其中:

*   P(h|d) =后验概率。假设 h 为真的概率，给定数据 d，其中 P(h|d)= P(d1| h) P(d2| h)…P(dn| h)
*   P(d|h) =可能性。假设假设 h 为真，数据 d 的概率。
*   P(h) =类别先验概率。假设 h 为真的概率(不考虑数据)
*   P(d) =预测值先验概率。数据的概率(不考虑假设)

这种算法被称为“天真的”，因为它假设所有的变量都是相互独立的，这是在现实世界的例子中做出的天真的假设。

![Naive-Bayes](img/bfc8236afe8bcef690a7ef80351a317c.png)

图 4:使用朴素贝叶斯通过变量“天气”来预测“播放”的状态。

以图 4 为例，如果天气=晴朗，结果会是什么？

给定变量 weather = 'sunny '的值，计算 P(yes|sunny)和 P(no|sunny)并选择概率较高的结果，以确定结果 play = 'yes '或' no'。

->P(是|晴)= (P(晴|是)* P(是))/ P(晴)= (3/9 * 9/14 ) / (5/14) = 0.60

-> P(no | sunny)=(P(sunny | no)* P(no))/P(sunny)=(2/5 * 5/14)/(5/14)= 0.40

因此，如果天气= '晴朗'，结果就是玩= '是'。

#### 5.KNN

K-最近邻算法使用整个数据集作为训练集，而不是将数据集分成训练集和测试集。

当一个新的数据实例需要一个结果时，KNN 算法会遍历整个数据集，以找到与新实例最近的 k 个实例，或者与新记录最相似的 k 个实例，然后输出结果的平均值(对于回归问题)或者模式(最频繁的类)以解决分类问题。k 值由用户指定。

实例之间的相似性使用诸如欧几里德距离和汉明距离之类的度量来计算。

## 无监督学习算法

#### 6.推测的

Apriori 算法用于事务数据库中挖掘频繁项目集，然后生成关联规则。它广泛用于市场购物篮分析，在这种分析中，人们检查数据库中频繁出现的产品组合。一般来说，我们把‘如果一个人购买了物品 X，那么他购买了物品 Y’的关联规则写成:X -> Y。

例如:如果一个人购买牛奶和糖，那么她可能会购买咖啡粉。这可以用关联规则的形式写成:{牛奶，糖} ->咖啡粉。关联规则是在跨越支持度和置信度的阈值之后生成的。

![Formulae-for-support](img/f8da5d4b9a4134d93182956212652f0a.png)

图 5:关联规则 X->Y 的支持度、置信度和提升度的公式。

支持度有助于减少在频繁项目集生成期间要考虑的候选项目集的数量。这项支持措施以先验原则为指导。先验原则指出，如果一个项目集是频繁的，那么它的所有子集也必须是频繁的。

#### 7.k 均值

K-means 是一种迭代算法，它将相似的数据分组到聚类中。它计算 k 个聚类的质心，并将一个数据点分配给质心和该数据点之间距离最小的那个聚类。

![k-means-algorithm](img/c316b0ae9d4cf9d1ced5194741a80d8e.png)

图 6:K 均值算法的步骤。[来源](https://hub.packtpub.com/clustering-and-other-unsupervised-learning-methods/)

它是这样工作的:

我们从选择 k 值开始，这里，假设 k = 3。然后，我们将每个数据点随机分配给 3 个集群中的任何一个。计算每个聚类的聚类质心。红色、蓝色和绿色的星星表示 3 个星团中每个星团的质心。

接下来，将每个点重新分配给最近的聚类质心。在上图中，上面的 5 个点被分配给了具有蓝色质心的簇。按照相同的步骤为包含红色和绿色质心的簇指定点。

然后，计算新簇的质心。旧的质心是灰色的星；新的质心是红色、绿色和蓝色的星。

最后，重复步骤 2-3，直到没有点从一个集群切换到另一个集群。一旦连续 2 步没有切换，退出 K-means 算法。

#### 8.污染控制局(Pollution Control Agency)

主成分分析(PCA)用于通过减少变量的数量来使数据易于探索和可视化。这是通过将数据中的最大方差捕获到一个新的坐标系中来实现的，该坐标系具有称为“主分量”的轴。

每个分量都是原始变量的线性组合，并且彼此正交。分量之间的正交性表示这些分量之间的相关性为零。

第一个主成分捕获数据中最大可变性的方向。第二个主成分捕获数据中的剩余方差，但具有与第一个成分不相关的变量。类似地，所有连续的主成分(PC3、PC4 等)捕获剩余的方差，同时与前一个成分不相关。

![PCA](img/f97099afc287a6c47a3196ffbe5b4c10.png)

图 7:3 个原始变量(基因)减少到 2 个新变量，称为主成分(PC 的)。[来源](http://www.nlpca.org/pca_principal_component_analysis.html)

## 集成学习技术:

集成意味着通过投票或平均来组合多个学习者(分类器)的结果以获得改进的结果。在分类期间使用投票，在回归期间使用平均。这个想法是全体学习者比单个学习者表现更好。

有 3 种类型的组合算法:打包、提升和堆叠。我们不打算在这里讨论“堆叠”，但是如果你想要一个详细的解释，这里有一个来自 Kaggle 的可靠介绍。

#### 9.用随机森林装袋

bagging 的第一步是用 Bootstrap 抽样方法创建的数据集创建多个模型。在 Bootstrap 采样中，每个生成的训练集都由原始数据集中的随机子样本组成。

这些训练集的大小都与原始数据集相同，但有些记录会重复多次，有些记录则根本不会出现。然后，整个原始数据集被用作测试集。因此，如果原始数据集的大小是 N，那么每个生成的训练集的大小也是 N，唯一记录的数量大约是(2N/3)；测试集的大小也是 n。

bagging 的第二步是通过对不同生成的训练集使用相同的算法来创建多个模型。

这就是随机森林的由来。与决策树不同，在决策树中，每个节点根据最小化错误的最佳特征进行分割，在随机森林中，我们选择随机选择的特征来构建最佳分割。随机性的原因是:即使使用 bagging，当决策树选择最佳特征进行分割时，它们最终会得到相似的结构和相关的预测。但是在特征的随机子集上分裂后的 bagging 意味着来自子树的预测之间更少的相关性。

在每个分割点要搜索的特征数量被指定为随机森林算法的一个参数。

因此，在具有随机森林的 bagging 中，使用记录的随机样本来构建每棵树，并且使用预测因子的随机样本来构建每个分裂。

#### 10.使用 AdaBoost 升压

Adaboost 代表自适应增压。Bagging 是一个并行集合，因为每个模型都是独立构建的。另一方面，boosting 是一个顺序集成，其中每个模型都是基于纠正前一个模型的错误分类而构建的。

Bagging 主要涉及“简单投票”，每个分类器投票以获得一个最终结果——一个由大多数平行模型决定的结果；提升涉及“加权投票”，其中每个分类器投票以获得由多数决定的最终结果——但顺序模型是通过给先前模型的错误分类实例分配更大的权重来建立的。

![Adaboost](img/b22b63479ee5dc91bfe451359725d988.png)

图 9:决策树的 Adaboost。[来源](https://sebastianraschka.com/faq/docs/bagging-boosting-rf.html)

在图 9 中，步骤 1、2、3 涉及一个称为决策树桩的弱学习器(一个 1 级决策树仅基于一个输入特征的值进行预测；其根直接连接到其叶的决策树)。

构建弱学习者的过程继续，直到已经构建了用户定义数量的弱学习者，或者直到在训练时没有进一步的改进。步骤 4 组合了前面模型的 3 个决策树桩(因此在决策树中有 3 个分割规则)。

首先，从一个决策树开始，对一个输入变量进行决策。

数据点的大小表明我们已经应用了相等的权重来将它们分类为圆形或三角形。decision stump 在上半部分生成了一条水平线来对这些点进行分类。我们可以看到，有两个圆被错误地预测为三角形。因此，我们将为这两个圆分配更高的权重，并应用另一个决策树桩。

其次，移动到另一个决策树树桩，对另一个输入变量做出决策。

我们观察到上一步中两个错误分类的圆的大小大于其余的点。现在，第二个决策树桩将尝试正确预测这两个圆。

由于分配了较高的权重，这两个圆已被左边的垂直线正确分类。但这导致了顶部三个圆圈的错误分类。因此，我们将为顶部的这三个圆圈分配更高的权重，并应用另一个决策树桩。

第三，训练另一个决策树树桩对另一个输入变量做出决策。

上一步中的三个错误分类的圆大于其余的数据点。现在，已经生成了一条向右的垂直线来对圆和三角形进行分类。

第四，结合决策树桩。

我们结合了前面 3 个模型中的分隔符，并观察到与任何单个弱学习者相比，该模型中的复杂规则能够正确分类数据点。

## 结论:

概括地说，我们已经介绍了数据科学中一些最重要的机器学习算法:

*   5 监督学习技术——线性回归、逻辑回归、CART、朴素贝叶斯、KNN。
*   3 无监督学习技术——Apriori，K-means，PCA。
*   2 集合技术——用随机森林打包，用 XGBoost 增强。

编者按:这篇文章最初发布在 [KDNuggets](https://www.kdnuggets.com/2017/10/top-10-machine-learning-algorithms-beginners.html?utm_source=dataquest&utm_medium=blog) 上，并被 perlesson 转载。作者[莉娜·肖](https://twitter.com/ReenaShawLegacy)是一名开发人员和数据科学记者。

## 获取免费的数据科学资源

免费注册获取我们的每周时事通讯，包括数据科学、 **Python** 、 **R** 和 **SQL** 资源链接。此外，您还可以访问我们免费的交互式[在线课程内容](/data-science-courses)！

[SIGN UP](https://app.dataquest.io/signup)