# 缓存行

> 原文：[`en.algorithmica.org/hpc/cpu-cache/cache-lines/`](https://en.algorithmica.org/hpc/cpu-cache/cache-lines/)

CPU 缓存系统中的数据传输基本单位不是单个比特和字节，而是**缓存行**。在大多数架构中，缓存行的尺寸是 64 字节，这意味着所有内存都被划分为 64 字节的块，并且无论你是否需要，每次当你请求（读取或写入）单个字节时，你也会获取其所有 63 个缓存行邻居。

为了证明这一点，我们在我们的增加循环中添加了一个“步长”参数。现在我们只触摸每$D$个元素：

```cpp
for (int i = 0; i < N; i += D)
    a[i]++; 
```

如果我们用$D=1$和$D=16$运行它，我们可以观察到一些有趣的现象：

![图片](img/3142c6c323ee109d005888386857b077.png)

性能是通过运行基准测试的总时间来归一化的，而不是通过增加的总元素数

随着问题规模的增加，两个循环的图表相遇，尽管一个循环的工作量是另一个的 1/16。这是因为，从缓存行的角度来看，我们在两个循环中获取了相同的内存，而步进循环只需要其中的 1/16 是不相关的。

当数组适合 L1 缓存时，步进版本的执行速度更快——虽然不是 16 倍，但只是两倍快。这是因为它只需要做一半的工作：它每处理 16 个元素只需要执行一个`inc DWORD PTR [rdx]`指令，而原始循环需要两个处理相同 16 个元素的 8 元素向量指令。这两个计算都受写入结果的瓶颈影响：Zen 2 每周期只能写入一个字——无论它是由一个整数还是八个组成的。

当我们将步长参数改为 8 时，图表变得一致，因为我们现在每 16 个元素也需要两个增加和两个写回：

![图片](img/254c995eba2d87ec3ed1d324103d9a0b.png)

我们可以使用这个效果来最小化我们的延迟基准测试中的缓存共享，以便更精确地测量它。我们需要对排列的索引进行**填充**，以便每个索引都位于自己的缓存行中：

```cpp
struct padded_int {
    int val;
    int padding[15];
};

padded_int q[N / 16];

// constructing a cycle from a random permutation
// ...

for (int i = 0; i < N / 16; i++)
    k = q[k].val; 
```

现在，每个索引在循环回来再次请求它的时候更有可能被踢出缓存：

![图片](img/0862c8d945214b8e8534c114937b9fa4.png)

在设计和分析内存绑定算法时，重要的实际经验教训是要计算访问的缓存行数量，而不仅仅是内存读取和写入的总数。

[←内存延迟](https://en.algorithmica.org/hpc/cpu-cache/latency/)[内存共享→](https://en.algorithmica.org/hpc/cpu-cache/sharing/)
