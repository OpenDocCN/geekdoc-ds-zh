# 舍入误差

> [原文链接](https://en.algorithmica.org/hpc/arithmetic/errors/)

硬件浮点数中舍入的方式非常简单：只有在操作结果无法精确表示时才会发生舍入，默认情况下舍入到最接近的可表示数字（在出现平局时优先选择以零结尾的数字）。

考虑以下代码片段：

```cpp
float x = 0;
for (int i = 0; i < (1 << 25); i++)
    x++;
printf("%f\n", x); 
```

而不是打印 $2^{25} = 33554432$（数学上应该是这个结果），它输出 $16777216 = 2^{24}$。为什么？

当我们反复增加一个浮点数 $x$ 时，最终会达到一个点，此时 $(x + 1)$ 被舍入回 $x$。第一个这样的数字是 $2^{24}$（尾数位加一），因为

$$ 2^{24} + 1 = 2^{24} \cdot 1.\underbrace{0\ldots0}_{\times 23} 1 $$

与 $2^{24}$ 和 $(2^{24} + 1)$ 的距离完全相同，但根据上述平局解决规则被舍入到 $2^{24}$。同时，低于该值的增加可以精确表示，因此最初不会发生舍入。

### [#](https://en.algorithmica.org/hpc/arithmetic/errors/#rounding-errors-and-operation-order)舍入误差和操作顺序

尽管在代数上是正确的，但浮点计算的最终结果可能取决于操作顺序。

例如，尽管加法和乘法在纯数学意义上是交换的和结合的，但它们的舍入误差却不是：当我们有三个浮点变量 $x$、$y$ 和 $z$ 时，$(x+y+z)$ 的结果取决于求和的顺序。同样的非交换性原则适用于大多数如果不是所有其他浮点操作。

编译器不允许产生不符合规范的结果，因此这个令人烦恼的细微差别禁用了涉及重新排列算术操作数的某些潜在优化。您可以通过在 GCC 和 Clang 中使用 `-ffast-math` 标志来禁用这种严格的合规性。如果我们添加它并重新编译上述代码片段，它将显著更快运行，并且意外地输出正确的结果，33554432（尽管您需要意识到编译器也可能选择一个不太精确的计算路径）。

### [#](https://en.algorithmica.org/hpc/arithmetic/errors/#rounding-modes)舍入模式

除了默认模式（也称为银行家舍入）之外，您还可以使用 4 种更多模式中的任意一种来设置其他舍入逻辑：

+   向最近舍入，在完美的平局情况下总是舍入“远离”零；

+   向上舍入（向 $+∞$；因此负数结果舍入到零）；

+   向下舍入（向 $-∞$；因此负数结果舍入远离零）；

+   向零舍入（二进制结果的截断）。

例如，如果你在运行上述循环之前调用`fesetround(FE_UPWARD)`，它输出的不是$2^{24}$，甚至不是$2^{25}$，而是$67108864 = 2^{26}$。这是因为当我们达到$2^{24}$时，$(x + 1)$开始四舍五入到下一个可表示的数字$(x + 2)$，并且我们用一半的时间达到$2^{25}$，之后，$(x + 1)$四舍五入到$(x+4)$，我们开始以四倍的速度前进。

使用替代舍入模式的用途之一是诊断数值不稳定性。如果一个算法在正无穷和负无穷之间切换舍入时结果有显著变化，这表明它容易受到舍入误差的影响。

这种测试通常比将所有计算切换到较低精度并检查结果变化是否过大要好，因为默认的舍入到最接近的政策在足够平均的情况下收敛到正确的“预期”值：一半的时间错误是向上舍入，另一半是向下舍入——因此，从统计上讲，它们相互抵消。

### [#](https://en.algorithmica.org/hpc/arithmetic/errors/#measuring-errors)测量误差

对于执行自然对数和平方根等复杂计算的硬件来说，期望这种保证似乎令人惊讶，但事实就是这样：你保证从所有操作中获得可能的最大精度。这使得分析舍入误差变得非常容易，我们将在下面看到。

测量计算误差有两种自然的方法：

+   创建硬件或符合规格的精确软件的工程师们关心的是*最后一位单位*（ulps），这是两个数字之间的距离，即有多少可表示的数字可以介于精确的实值和计算的实际结果之间。

+   从事数值算法研究的人们关心的是*相对精度*，即近似误差的绝对值除以真实答案：$|\frac{v-v’}{v}|$。

在任何情况下，分析错误的常用策略是假设最坏的情况并简单地对其进行限制。

如果你执行单个基本算术运算，那么最坏的情况是结果四舍五入到最接近的可表示数字，这意味着误差不超过 0.5 ulps。为了以相同的方式推理相对误差，我们可以定义一个称为*机器 epsilon*的数字，等于 1 与下一个可表示值之间的差（这应该等于 2 的负幂，等于为尾数分配的位数）。

这意味着如果在单个算术运算后你得到结果$x$，那么真实值在以下范围内

$$ [x \cdot (1-\epsilon),\; x \cdot (1 + \epsilon)] $$

在基于浮点计算结果进行离散的“是或否”决策时，记住错误的普遍性尤为重要。例如，以下是检查相等性的方法：

```cpp
const float eps = std::numeric_limits<float>::epsilon; // ~2^(-23)
bool eq(float a, float b) {
    return abs(a - b) <= eps;
} 
```

`eps` 的值应该取决于应用：上面提到的——`float` 的机器精度——对于不超过一次浮点运算来说是好的。

### [#](https://en.algorithmica.org/hpc/arithmetic/errors/#interval-arithmetic)区间算术

如果一个算法被称为 *数值稳定*，那么无论其误差的原因是什么，在计算过程中误差都不会增长得很大。这只能发生在问题本身是 *良态的*，这意味着如果输入数据只发生微小变化，解只会有微小变化。

在分析数值算法时，常常有用到实验物理学中使用的相同方法：我们不会处理未知实数值，而是处理它们可能存在的区间。

例如，考虑一系列连续的运算，我们将一个变量依次乘以任意实数：

```cpp
float x = 1;
for (int i = 0; i < n; i++)
    x *= a[i]; 
```

在第一次乘法之后，$x$ 相对于实数乘积的值被 $(1 + \epsilon)$ 所限制，并且每次额外的乘法之后，这个上限会乘以另一个 $(1 + \epsilon)$。通过归纳，经过 $n$ 次乘法后，计算出的值被 $(1 + \epsilon)^n = 1 + n \epsilon + O(\epsilon²)$ 和类似的下限所限制。

这意味着相对误差是 $O(n \epsilon)$，这还不错，因为通常 $n \ll \frac{1}{\epsilon}$。

例如，考虑一个数值 *不稳定* 的计算，考虑以下函数

$$ f(x, y) = x² - y² $$ 假设 $x > y$，该函数可以返回的最大值大约是 $$ x² \cdot (1 + \epsilon) - y² \cdot (1 - \epsilon) $$ 对应的绝对误差为 $$ x² \cdot (1 + \epsilon) - y² \cdot (1 - \epsilon) - (x² - y²) = (x² + y²) \cdot \epsilon $$ 因此相对误差为 $$ \frac{x² + y²}{x² - y²} \cdot \epsilon $$

如果 $x$ 和 $y$ 在大小上很接近，误差将是 $O(\epsilon \cdot |x|)$。

在直接计算中，减法“放大”了平方的误差。但可以通过使用以下公式来修复：

$$ f(x, y) = x² - y² = (x + y) \cdot (x - y) $$

在这个例子中，很容易证明误差被 $\epsilon \cdot |x - y|$ 所限制。它也更快，因为它只需要 2 次加法和 1 次乘法：比原始方法多一次快速加法和少一次慢速乘法。

### [#](https://en.algorithmica.org/hpc/arithmetic/errors/#kahan-summation)Kahan 求和

从前面的例子中，我们可以看到长链的运算不是问题，但添加和减去不同大小的数是问题。处理此类问题的通用方法是尝试用大数与大数相加，用小数与小数相加。

考虑标准的求和算法：

```cpp
float s = 0;
for (int i = 0; i < n; i++)
    s += a[i]; 
```

由于我们执行的是求和而不是乘法，其相对误差不再仅仅被 $O(\epsilon \cdot n)$ 所限制，而是严重依赖于输入。

在最荒谬的情况下，如果第一个值是 $2^{24}$，其他值都等于 $1$，那么无论 $n$ 如何，总和都将等于 $2^{24}$，这可以通过执行以下代码并观察它简单地打印出 $16777216 = 2^{24}$ 两次来验证：

```cpp
const int n = (1<<24);
printf("%d\n", n);

float s = n;
for (int i = 0; i < n; i++)
    s += 1.0;

printf("%f\n", s); 
```

这是因为 `float` 只有 23 位尾数位，所以 $2^{24} + 1$ 是第一个不能精确表示的整数，并且必须向下舍入，这发生在我们尝试将 $1$ 加到 $s = 2^{24}$ 的时候。误差确实是 $O(n \cdot \epsilon)$，但就绝对误差而言，不是相对误差：在上面的例子中，它是 $2$，如果最后一个数字恰好是 $-2^{24}$，它将无限增大。

显然，解决方案是切换到更大的类型，例如 `double`，但这并不是一个可扩展的方法。一个优雅的解决方案是将未添加的部分存储在一个单独的变量中，然后将其添加到下一个变量中：

```cpp
float s = 0, c = 0;
for (int i = 0; i < n; i++) {
    float y = a[i] - c; // c is zero on the first iteration
    float t = s + y;    // s may be big and y may be small, losing low-order bits of y
    c = (t - s) - y;    // (t - s) cancels high-order part of y
    s = t;
} 
```

这个技巧被称为 *Kahan 求和法*。它的相对误差被限制在 $2 \epsilon + O(n \epsilon²)$：第一项来自最后的求和，第二项是由于我们在每一步都处理小于 epsilon 的误差。

当然，一个更通用的方法，不仅适用于数组求和，是切换到更精确的数据类型，如 `double`，这也有效地平方了机器 epsilon。此外，它可以通过将两个 `double` 变量捆绑在一起来（某种程度上）进行扩展：一个用于存储值，另一个用于其不可表示的误差，以便它们代表值 $a+b$。这种方法被称为 double-double 运算，并且可以类似地推广来定义 quad-double 和更高精度的算术。[← IEEE 754 浮点数](https://en.algorithmica.org/hpc/arithmetic/ieee-754/)[牛顿法 →](https://en.algorithmica.org/hpc/arithmetic/newton/)
