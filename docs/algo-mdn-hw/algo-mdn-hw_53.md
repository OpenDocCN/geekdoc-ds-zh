# 内存带宽

> 原文：[`en.algorithmica.org/hpc/cpu-cache/bandwidth/`](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/)

在 CPU 寄存器和 RAM 之间的数据路径上，存在一个用于加速访问常用数据的缓存层次结构：靠近处理器的层级更快，但尺寸也更小。这里的“更快”适用于两个密切相关但不同的计时：

+   从读取或写入开始到数据到达之间的延迟（*延迟*）。

+   每单位时间内可以处理的内存操作数（*带宽*）。

对于许多算法，内存带宽是缓存系统最重要的特性。同时，它也是最容易被测量的，因此我们将从这里开始。

对于我们的实验，我们创建一个数组并对其迭代 $K$ 次，增加其值：

```cpp
int a[N];

for (int t = 0; t < K; t++)
    for (int i = 0; i < N; i++)
        a[i]++; 
```

改变 $N$ 并调整 $K$，使得访问的总数组单元格数大致保持不变，并以“每秒操作数”来表示总时间，我们得到如下所示的图表：

![](img/b704b1ff058f9b947bc80e669006da78.png)

点状垂直线表示缓存层的大小

你可以清楚地看到这个图表上的缓存大小：

+   当整个数组适合缓存的最底层时，程序的性能瓶颈是由 CPU 而不是 L1 缓存带宽造成的。随着数组变大，与循环第一次迭代相关的开销变得更小，性能更接近其理论最大值 16 GFLOPS。

+   但随后性能下降：首先当超过 L1 缓存时降至 12-13 GFLOPS，然后逐渐降至大约 2 GFLOPS，当它无法再适合 L3 缓存时。

这种情况对于许多轻量级循环来说是典型的。

### [#](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/#frequency-scaling)频率缩放

所有 CPU 缓存层都放置在处理器相同的微芯片上，因此带宽、延迟以及所有其他特性都随着时钟频率而缩放。另一方面，RAM 则生活在其自己的固定时钟上，其特性保持不变。我们可以通过重新运行带有超频的相同基准测试来观察这一点：

![](img/df8453e0bd574b085bccd579d6d178f2.png)

当比较算法实现时，这个细节就会发挥作用。当工作数据集适合缓存时，两种实现的相对性能可能因 CPU 时钟率的不同而不同，因为 RAM 不受其影响（而其他一切都会受到影响）。

因此，建议保持时钟频率固定，由于超频不够稳定，我们在这本书的大多数基准测试中都使用普通的 2GHz。

### [#](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/#directional-access)方向访问

这个递增循环在其执行过程中需要执行读取和写入：在每次迭代中，我们获取一个值，增加它，然后将其写回。在许多应用中，我们只需要做其中之一，所以让我们尝试测量单向带宽。

计算数组的和只需要内存读取：

```cpp
for (int i = 0; i < N; i++)
    s += a[i]; 
```

而清零数组（或用任何其他常量值填充它）只需要内存写入：

```cpp
for (int i = 0; i < N; i++)
    a[i] = 0; 
```

这两个循环都可以由编译器轻易向量化，第二个实际上被替换为`memset`，所以 CPU 也不是这里的瓶颈（除非数组适合 L1 缓存）。

![](img/7387e237a0c0c4a7b5759e210932988a.png)

单向和双向内存访问之所以会表现不同，是因为它们共享缓存和内存总线以及其他 CPU 设施。在 RAM 的情况下，这导致纯读取和同时读取和写入场景之间性能差异翻倍，因为内存控制器必须在单向内存总线上在两种模式之间切换，从而减半带宽。性能下降对于 L2 缓存来说不那么严重：这里的瓶颈不是缓存总线，所以递增循环只损失了大约 15%。

图表上有一个有趣的异常，即当数组达到 L3 缓存和 RAM 时，只写循环的表现与读写循环相同。这是因为 CPU 在每次访问时都会将数据移动到最高级别的缓存，无论是读取还是写入——这通常是一个很好的优化，因为在许多用例中我们很快就会需要它。当读取数据时，这并不是问题，因为数据无论如何都会通过缓存层次结构，但在写入时，这会导致在写入后立即调度另一个隐式读取——因此需要两倍的总线带宽。

### [#](https://en.algorithmica.org/hpc/cpu-cache/bandwidth/#bypassing-the-cache)绕过缓存

我们可以通过使用*非临时*内存访问来防止 CPU 预取我们刚刚写入的数据。为此，我们需要更直接地重新实现清零循环，而不依赖于编译器向量化。

忽略一些特殊情况，`memset`和自动向量化赋值循环在底层所做的只是使用 SIMD 指令移动 32 字节数据块：

```cpp
const __m256i zeros = _mm256_set1_epi32(0);

for (int i = 0; i + 7 < N; i += 8)
    _mm256_store_si256((__m256i*) &a[i], zeros); 
```

我们可以用一个*非临时*的替换通常的向量存储内建函数：

```cpp
const __m256i zeros = _mm256_set1_epi32(0);

for (int i = 0; i + 7 < N; i += 8)
    _mm256_stream_si256((__m256i*) &a[i], zeros); 
```

非临时内存读取或写入是一种告诉 CPU 我们将来不需要我们刚刚访问的数据的方法，因此在写入后不需要读取数据。

![](img/cc99a8910190218123f19011de90575f.png)

一方面，如果数组足够小，可以放入缓存，并且我们在短时间内实际访问它，这会有负面影响，因为我们必须从 RAM（或在这种情况下，我们必须将数据写入 RAM 而不是使用本地缓存版本）中完全读取它。另一方面，这防止了读取回写，并使我们能够更有效地使用内存总线。

事实上，在 RAM 的情况下，性能提升甚至超过 2 倍，并且比只读基准测试更快。这是因为：

+   存储控制器不必这样在读取和写入模式之间切换总线；

+   指令序列变得更简单，允许更多的挂起内存指令；

+   并且，最重要的是，存储控制器可以简单地“发射并忘记”非临时写入请求——而对于读取，它需要记住数据到达后要做什么（类似于网络软件中的连接句柄）。

理论上，这两个请求应该使用相同的带宽：一个读取请求发送地址并获取数据，一个非临时写入请求发送地址和数据，并获取无数据。不考虑方向，我们传输相同的数据，但读取周期会更长，因为它需要等待数据被检索。由于内存系统可以处理的并发请求数量存在实际限制[1]，这种读写周期延迟的差异也导致了它们带宽的差异。

此外，由于这些原因，单个 CPU 核心通常无法完全利用内存带宽。

同样的技术也适用于`memcpy`：它也仅使用 SIMD 加载/存储指令移动 32 字节块，并且可以类似地将其设置为非临时，对于大型数组，可以将吞吐量提高两倍。还有一个非临时加载指令（`_mm256_stream_load_si256`），当你想要*读取*而不污染缓存时使用（例如，当你不需要在调用`memcpy`之后的原始数组，但需要之前访问的一些数据时）。[← ../RAM & CPU Caches](https://en.algorithmica.org/hpc/cpu-cache/)[Memory Latency →](https://en.algorithmica.org/hpc/cpu-cache/latency/)
