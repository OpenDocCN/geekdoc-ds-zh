- en: The Book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://little-book-of.github.io/linear-algebra/books/en-US/book.html](https://little-book-of.github.io/linear-algebra/books/en-US/book.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Overtune
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*A soft opening, an invitation into the world of vectors and spaces, where
    each step begins a journey.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Geometry’s Dawn**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. Invitation to Learn**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. Light and Shadow**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**4\. The Seed of Structure**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**5\. Whisper of Algebra**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**6\. Beginner’s Welcome**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**7\. Eternal Path**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 1\. Vectors, scalars, and geometry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opening
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 1\. Scalars, Vectors, and Coordinate Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we begin learning linear algebra, everything starts with the simplest
    building blocks: scalars and vectors. A scalar is just a single number, like 3,
    –7, or π. It carries only magnitude and no direction. Scalars are what we use
    for counting, measuring length, or scaling other objects up and down. A vector,
    by contrast, is an ordered collection of numbers. You can picture it as an arrow
    pointing somewhere in space, or simply as a list like (2, 5) in 2D or (1, –3,
    4) in 3D. Where scalars measure “how much,” vectors measure both “how much” and
    “which way.”'
  prefs: []
  type: TYPE_NORMAL
- en: Coordinate Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To talk about vectors, we need a coordinate system. Imagine laying down two
    perpendicular axes on a sheet of paper: the x-axis (left to right) and the y-axis
    (up and down). Every point on the sheet can be described with two numbers: how
    far along the x-axis, and how far along the y-axis. This pair of numbers is a
    vector in 2D. Add a z-axis pointing up from the page, and you have 3D space. Each
    coordinate system gives us a way to describe vectors numerically, even though
    the underlying “space” is the same.'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Scalars vs. Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A scalar is like a single tick mark on a ruler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A vector is like an arrow that starts at the origin (0, 0, …) and ends at the
    point defined by its components. For example, the vector (3, 4) in 2D points from
    the origin to the point 3 units along the x-axis and 4 units along the y-axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Start Here?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Understanding the difference between scalars and vectors is the foundation for
    everything else in linear algebra. Every concept-matrices, linear transformations,
    eigenvalues-eventually reduces to how we manipulate vectors and scale them with
    scalars. Without this distinction, the rest of the subject would have no anchor.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Nearly every field of science and engineering depends on this idea. Physics
    uses vectors for velocity, acceleration, and force. Computer graphics uses them
    to represent points, colors, and transformations. Data science treats entire datasets
    as high-dimensional vectors. By mastering scalars and vectors early, you unlock
    the language in which modern science and technology are written.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Draw an x- and y-axis on a piece of paper. Plot the vector (2, 3).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now draw the vector (–1, 4). Compare their directions and lengths.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Think: which of these two vectors points “more upward”? Which is “longer”?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These simple experiments already give you intuition for the operations you’ll
    perform again and again in linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Vector Notation, Components, and Arrows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear algebra gives us powerful ways to describe and manipulate vectors, but
    before we can do anything with them, we need a precise notation system. Notation
    is not just cosmetic-it tells us how to read, write, and think about vectors clearly
    and unambiguously. In this section, we’ll explore how vectors are written, how
    their components are represented, and how we can interpret them visually as arrows.
  prefs: []
  type: TYPE_NORMAL
- en: Writing Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vectors are usually denoted by lowercase letters in bold (like \(\mathbf{v},
    \mathbf{w}, \mathbf{x}\))
  prefs: []
  type: TYPE_NORMAL
- en: or with an arrow overhead (like \(\vec{v}\)).
  prefs: []
  type: TYPE_NORMAL
- en: For instance, the vector \(\mathbf{v} = (2, 5)\) is the same as \(\vec{v} =
    (2, 5)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The style depends on context: mathematicians often use bold, physicists often
    use arrows.'
  prefs: []
  type: TYPE_NORMAL
- en: In handwritten notes, people sometimes underline vectors (e.g., \(\underline{v}\))
    to avoid confusion with scalars.
  prefs: []
  type: TYPE_NORMAL
- en: The important thing is to distinguish vectors from scalars at a glance.
  prefs: []
  type: TYPE_NORMAL
- en: Components of a Vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A vector in two dimensions has two components, written as \((x, y)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'In three dimensions, it has three components: \((x, y, z)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'More generally, an \(n\)-dimensional vector has \(n\) components: \((v_1, v_2,
    \ldots, v_n)\).'
  prefs: []
  type: TYPE_NORMAL
- en: Each component tells us how far the vector extends along one axis of the coordinate
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\mathbf{v} = (3, 4)\) means the vector extends 3 units along the \(x\)-axis
    and 4 units along the \(y\)-axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbf{w} = (-2, 0, 5)\) means the vector extends \(-2\) units along the
    \(x\)-axis, \(0\) along the \(y\)-axis, and 5 along the \(z\)-axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We often refer to the \(i\)-th component of a vector \(\mathbf{v}\) as \(v_i\).
  prefs: []
  type: TYPE_NORMAL
- en: So, for \(\mathbf{v} = (3, 4, 5)\), we have \(v_1 = 3\), \(v_2 = 4\), \(v_3
    = 5\).
  prefs: []
  type: TYPE_NORMAL
- en: Column vs. Row Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Vectors can be written in two common ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a row vector: \((v_1, v_2, v_3)\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As a column vector:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Both represent the same abstract object.
  prefs: []
  type: TYPE_NORMAL
- en: Row vectors are convenient for quick writing, while column vectors are essential
    when we start multiplying by matrices, because the dimensions must align.
  prefs: []
  type: TYPE_NORMAL
- en: Vectors as Arrows
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most intuitive way to picture a vector is as an arrow:'
  prefs: []
  type: TYPE_NORMAL
- en: It starts at the origin (0, 0, …).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It ends at the point given by its components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, the vector (2, 3) in 2D is drawn as an arrow from (0, 0) to (2,
    3). The arrow has both direction (where it points) and magnitude (its length).
    This geometric picture makes abstract algebraic manipulations much easier to grasp.
  prefs: []
  type: TYPE_NORMAL
- en: Position Vectors vs. Free Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two common interpretations of vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Position vector - a vector that points from the origin to a specific point
    in space. Example: (2, 3) is the position vector for the point (2, 3).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Free vector - an arrow with length and direction, but not tied to a specific
    starting point. For instance, an arrow of length 5 pointing northeast can be drawn
    anywhere, but it still represents the same vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In linear algebra, we often treat vectors as free vectors, because their meaning
    does not depend on where they are drawn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Reading a Vector'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose u = (–3, 2).
  prefs: []
  type: TYPE_NORMAL
- en: The first component (–3) means move 3 units left along the x-axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second component (2) means move 2 units up along the y-axis. So the arrow
    points to the point (–3, 2). Even without a diagram, the components tell us exactly
    what the arrow would look like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Clear notation is the backbone of linear algebra. Without it, equations quickly
    become unreadable, and intuition about direction and size is lost. The way we
    write vectors determines how easily we can connect the algebra (numbers and symbols)
    to the geometry (arrows and spaces). This dual perspective-symbolic and visual-is
    what makes linear algebra powerful and practical.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write down the vector (4, –1). Draw it on graph paper.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rewrite the same vector as a column vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Translate the vector (4, –1) by moving its starting point to (2, 3) instead
    of the origin. Notice that the arrow looks the same-it just starts elsewhere.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a harder challenge: draw the 3D vector (2, –1, 3). Even if you can’t draw
    perfectly in 3D, try to show each component along the x, y, and z axes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By practicing both the notation and the arrow picture, you’ll develop fluency
    in switching between abstract symbols and concrete visualizations. This skill
    will make every later concept in linear algebra far more intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Vector Addition and Scalar Multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we know how to describe vectors with components and arrows, the next step
    is to learn how to combine them. Two fundamental operations form the backbone
    of linear algebra: adding vectors together and scaling vectors with numbers (scalars).
    These two moves, though simple, generate everything else we’ll build later. With
    them, we can describe motion, forces, data transformations, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Vector Addition in Coordinates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we have two vectors in 2D:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\mathbf{u} = (u_1, u_2), \quad \mathbf{v} = (v_1, v_2)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Their sum is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{u} + \mathbf{v} = (u_1 + v_1, \; u_2 + v_2). \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, you add corresponding components.
  prefs: []
  type: TYPE_NORMAL
- en: 'This works in higher dimensions too:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (u_1, u_2, \ldots, u_n) + (v_1, v_2, \ldots, v_n) = (u_1 + v_1, \; u_2 +
    v_2, \; \ldots, \; u_n + v_n). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: \[ (2, 3) + (-1, 4) = (2 - 1, \; 3 + 4) = (1, 7). \]'
  prefs: []
  type: TYPE_NORMAL
- en: Vector Addition as Geometry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The geometric picture is even more illuminating. If you draw vector u as an
    arrow, then place the tail of v at the head of u, the arrow from the start of
    u to the head of v is u + v. This is called the tip-to-tail rule. The parallelogram
    rule is another visualization: place u and v tail-to-tail, form a parallelogram,
    and the diagonal is their sum.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: u = (3, 1), v = (2, 2). Draw both from the origin. Their sum (5, 3)
    is exactly the diagonal of the parallelogram they span.'
  prefs: []
  type: TYPE_NORMAL
- en: Scalar Multiplication in Coordinates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Scalars stretch or shrink vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'If \(\mathbf{u} = (u_1, u_2, \ldots, u_n)\) and \(c\) is a scalar, then:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ c \cdot \mathbf{u} = (c \cdot u_1, \; c \cdot u_2, \; \ldots, \; c \cdot
    u_n). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ 2 \cdot (3, 4) = (6, 8). \]
  prefs: []
  type: TYPE_NORMAL
- en: \[ (-1) \cdot (3, 4) = (-3, -4). \]
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying by a positive scalar stretches or compresses the arrow while keeping
    the direction the same. Multiplying by a negative scalar flips the arrow to point
    the opposite way.
  prefs: []
  type: TYPE_NORMAL
- en: Scalar Multiplication as Geometry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine the vector (1, 2). Draw it on graph paper: it goes right 1, up 2\.
    Now double it: (2, 4). The arrow points in the same direction but is twice as
    long. Halve it: (0.5, 1). It’s the same direction but shorter. Negate it: (–1,
    –2). Now the arrow points backward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This geometric picture explains why we call these numbers “scalars”: they scale
    the vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining Both: Linear Combinations'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Vector addition and scalar multiplication are not just separate tricks-they
    combine to form the heart of linear algebra: linear combinations.'
  prefs: []
  type: TYPE_NORMAL
- en: A linear combination of vectors \(u\) and \(v\) is any vector of the form
  prefs: []
  type: TYPE_NORMAL
- en: \(a \cdot u + b \cdot v\), where \(a\) and \(b\) are scalars.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: If \(u = (1, 0)\) and \(v = (0, 1)\), then
  prefs: []
  type: TYPE_NORMAL
- en: \(3 \cdot u + 2 \cdot v = (3, 2)\).
  prefs: []
  type: TYPE_NORMAL
- en: This shows how any point on the grid can be reached by scaling and adding these
    two basic vectors.
  prefs: []
  type: TYPE_NORMAL
- en: That’s the essence of constructing spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Algebraic Properties
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Vector addition and scalar multiplication obey rules that mirror arithmetic
    with numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Commutativity: \(u + v = v + u\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Associativity: \((u + v) + w = u + (v + w)\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distributivity over scalars: \(c \cdot (u + v) = c \cdot u + c \cdot v\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distributivity over numbers: \((a + b) \cdot u = a \cdot u + b \cdot u\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These rules are not trivial bookkeeping - they guarantee that linear algebra
    behaves predictably,
  prefs: []
  type: TYPE_NORMAL
- en: which is why it works as the language of science.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With only these two operations-addition and scaling-you can already describe
    lines, planes, and entire spaces. Any system that grows by combining influences,
    like physics, economics, or machine learning, is built on these simple rules.
    Later, when we define matrix multiplication, dot products, and eigenvalues, they
    all reduce to repeated patterns of adding and scaling vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Add (2, 3) and (–1, 4). Draw the result on graph paper.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply (1, –2) by 3, and then add (0, 5). What is the final vector?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a deeper challenge: Let u = (1, 2) and v = (2, –1). Sketch all vectors
    of the form a·u + b·v for integer values of a, b between –2 and 2\. Notice the
    grid of points you create-that’s the span of these two vectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This simple practice shows you how combining two basic vectors through addition
    and scaling generates a whole structured space, the first glimpse of linear algebra’s
    real power.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Linear Combinations and Span
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After learning to add vectors and scale them, the natural next question is:
    *what can we build from these two operations?* The answer is the concept of linear
    combinations, which leads directly to one of the most fundamental ideas in linear
    algebra: the span of a set of vectors. These ideas tell us not only what individual
    vectors can do, but how groups of vectors can shape entire spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Linear Combination?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A linear combination is any vector formed by multiplying vectors with scalars
    and then adding the results together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, given vectors \(v_1, v_2, \ldots, v_k\) and scalars \(a_1, a_2, \ldots,
    a_k\), a linear combination looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ a_1 \cdot v_1 + a_2 \cdot v_2 + \cdots + a_k \cdot v_k. \]
  prefs: []
  type: TYPE_NORMAL
- en: This is nothing more than repeated addition and scaling, but the idea is powerful
    because it describes how vectors combine to generate new ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: Let \(u = (1, 0)\) and \(v = (0, 1)\). Then any linear combination \(a \cdot
    u + b \cdot v = (a, b)\).
  prefs: []
  type: TYPE_NORMAL
- en: This shows that every point in the 2D plane can be expressed as a linear combination
    of these two simple vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Linear combinations are about mixing directions and magnitudes. Each vector
    acts like a “directional ingredient,” and the scalars control how much of each
    ingredient you use.
  prefs: []
  type: TYPE_NORMAL
- en: 'With one vector: You can only reach points on a single line through the origin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With two non-parallel vectors in 2D: You can reach every point in the plane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With three non-coplanar vectors in 3D: You can reach all of 3D space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This progression shows that the power of linear combinations depends not just
    on the vectors themselves but on how they relate to each other.
  prefs: []
  type: TYPE_NORMAL
- en: The Span of a Set of Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The span of a set of vectors is the collection of all possible linear combinations
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'It answers the question: *“What space do these vectors generate?”*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Span}\{v_1, v_2, \ldots, v_k\} = \{a_1 v_1 + a_2 v_2 + \cdots + a_k
    v_k \;|\; a_i \in \mathbb{R}\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\text{Span}\{(1, 0)\}\) = all multiples of \((1, 0)\), which is the \(x\)-axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\text{Span}\{(1, 0), (0, 1)\}\) = all of \(\mathbb{R}^2\), the entire plane.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\text{Span}\{(1, 2), (2, 4)\}\) = just the line through \((1, 2)\), because
    the second vector is a multiple of the first.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So the span depends heavily on whether the vectors add new directions or just
    duplicate what’s already there.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel and Independent Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If vectors point in the same or opposite directions (one is a scalar multiple
    of another), then their span is just a line. They don’t add any new coverage of
    space. But if they point in different directions, they open up new dimensions.
    This leads to the critical idea of linear independence, which we’ll explore later:
    vectors are independent if none of them is a linear combination of the others.'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Span in Different Dimensions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In 2D:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One vector spans a line.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Two independent vectors span the whole plane.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 3D:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One vector spans a line.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Two independent vectors span a plane.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Three independent vectors span all of 3D space.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In higher dimensions: The same pattern continues. A set of k independent vectors
    spans a k-dimensional subspace inside the larger space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algebraic Properties
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The span of vectors always includes the zero vector, because you can choose
    all scalars = 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The span is always a subspace, meaning it’s closed under addition and scalar
    multiplication. If you add two vectors in the span, the result stays in the span.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The span grows when you add new independent vectors, but not if the new vector
    is just a combination of the old ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linear combinations and span are the foundation for almost everything else
    in linear algebra:'
  prefs: []
  type: TYPE_NORMAL
- en: They define what it means for vectors to be independent or dependent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They form the basis for solving linear systems (solutions are often described
    as spans).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They explain how dimensions arise in vector spaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They underpin practical methods like principal component analysis, where data
    is projected onto the span of a few important vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, the span tells us the “reach” of a set of vectors, and linear combinations
    are the mechanism to explore that reach.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take vectors (1, 0) and (0, 1). Write down three different linear combinations
    and plot them. What shape do you notice?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try vectors (1, 2) and (2, 4). Write down three different linear combinations.
    Plot them. What’s different from the previous case?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In 3D, consider (1, 0, 0) and (0, 1, 0). Describe their span. Add (0, 0, 1).
    How does the span change?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Pick vectors (1, 2, 3) and (4, 5, 6). Do they span a plane or all
    of 3D space? How can you tell?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By experimenting with simple examples, you’ll see clearly how the idea of span
    captures the richness or limitations of combining vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Length (Norm) and Distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, vectors have been arrows with direction and components. To compare them
    more meaningfully, we need ways to talk about how long they are and how far apart
    they are. These notions are formalized through the norm of a vector (its length)
    and the distance between vectors. These concepts tie together the algebra of components
    and the geometry of space.
  prefs: []
  type: TYPE_NORMAL
- en: The Length (Norm) of a Vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The norm of a vector measures its magnitude, or how long the arrow is.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a vector \(v = (v_1, v_2, \ldots, v_n)\) in \(n\)-dimensional space, its
    norm is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|v\| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This formula comes directly from the Pythagorean theorem: the length of the
    hypotenuse equals the square root of the sum of squares of the legs.'
  prefs: []
  type: TYPE_NORMAL
- en: In 2D, this is the familiar distance formula between the origin and a point.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(v = (3, 4)\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \|v\| = \sqrt{3^2 + 4^2} = \sqrt{9 + 16} = 5. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For \(w = (1, -2, 2)\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \|w\| = \sqrt{1^2 + (-2)^2 + 2^2} = \sqrt{1 + 4 + 4} = \sqrt{9} = 3. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unit Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A unit vector is a vector whose length is exactly 1.
  prefs: []
  type: TYPE_NORMAL
- en: These are important because they capture direction without scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a unit vector from any nonzero vector, divide by its norm:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ u = \frac{v}{\|v\|}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: For \(v = (3, 4)\), the unit vector is
  prefs: []
  type: TYPE_NORMAL
- en: \[ u = \left(\tfrac{3}{5}, \tfrac{4}{5}\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: This points in the same direction as \((3, 4)\) but has length 1.
  prefs: []
  type: TYPE_NORMAL
- en: Unit vectors are like pure directions.
  prefs: []
  type: TYPE_NORMAL
- en: They’re especially useful for projections, defining coordinate systems, and
    normalizing data.
  prefs: []
  type: TYPE_NORMAL
- en: Distance Between Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The distance between two vectors \(u\) and \(v\) is defined as the length of
    their difference:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{dist}(u, v) = \|u - v\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: Let \(u = (2, 1)\) and \(v = (5, 5)\). Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ u - v = (-3, -4). \]
  prefs: []
  type: TYPE_NORMAL
- en: Its norm is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sqrt{(-3)^2 + (-4)^2} = \sqrt{9 + 16} = 5. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'So the distance is 5\. This matches our intuition: the straight-line distance
    between points \((2, 1)\) and \((5, 5)\).'
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The norm tells you how far a point is from the origin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distance tells you how far two points are from each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both are computed with the same formula-the square root of sums of squares-but
    applied in slightly different contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Different Kinds of Norms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The formula above defines the Euclidean norm (or \(\ell_2\) norm), the most
    common one.
  prefs: []
  type: TYPE_NORMAL
- en: 'But in linear algebra, other norms are also useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '\(\ell_1\) norm:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \|v\|_1 = |v_1| + |v_2| + \cdots + |v_n| \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (sum of absolute values).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '\(\ell_\infty\) norm:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \|v\|_\infty = \max(|v_1|, |v_2|, \ldots, |v_n|) \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (largest component).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These norms change the geometry of “length” and “distance.” For example, in
    the ℓ₁ norm, the unit circle is shaped like a diamond; in the ℓ∞ norm, it looks
    like a square.
  prefs: []
  type: TYPE_NORMAL
- en: Algebraic Properties
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Norms and distances satisfy critical properties that make them consistent measures:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-negativity: \(\|v\| \geq 0\), and \(\|v\| = 0\) only if \(v = 0\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Homogeneity: \(\|c \cdot v\| = |c| \, \|v\|\) (scaling affects length predictably).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Triangle inequality: \(\|u + v\| \leq \|u\| + \|v\|\) (the direct path is shortest).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Symmetry (for distance): \(\text{dist}(u, v) = \text{dist}(v, u)\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These properties are why norms and distances are robust tools across mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Understanding length and distance is the first step toward geometry in higher
    dimensions. These notions:'
  prefs: []
  type: TYPE_NORMAL
- en: Allow us to compare vectors quantitatively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Form the basis of concepts like angles, orthogonality, and projections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Underpin optimization problems (e.g., “find the closest vector” is central to
    machine learning).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the geometry of spaces, which changes dramatically depending on which
    norm you use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the norm of (6, 8). Then divide by the norm to find its unit vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the distance between (1, 1, 1) and (4, 5, 6).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare the Euclidean and Manhattan (ℓ₁) distances between (0, 0) and (3, 4).
    Which one matches your intuition if you were walking along a city grid?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: For vectors u = (2, –1, 3) and v = (–2, 0, 1), compute ‖u – v‖.
    Then explain what this distance means geometrically.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By working through these examples, you’ll see how norms and distances make abstract
    vectors feel as real as points and arrows you can measure in everyday life.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Dot Product (Algebraic and Geometric Views)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dot product is one of the most fundamental operations in linear algebra.
    It looks like a simple formula, but it unlocks the ability to measure angles,
    detect orthogonality, project one vector onto another, and compute energy or work
    in physics. Understanding it requires seeing both the algebraic view (a formula
    on components) and the geometric view (a way to compare directions).
  prefs: []
  type: TYPE_NORMAL
- en: Algebraic Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For two vectors of the same dimension, \(u = (u_1, u_2, \ldots, u_n)\) and
    \(v = (v_1, v_2, \ldots, v_n)\), the dot product is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ u \cdot v = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n. \]
  prefs: []
  type: TYPE_NORMAL
- en: This is simply multiplying corresponding components and summing the results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: \((2, 3) \cdot (4, 5) = (2 \times 4) + (3 \times 5) = 8 + 15 = 23\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \((1, -2, 3) \cdot (0, 4, -1) = (1 \times 0) + (-2 \times 4) + (3 \times -1)
    = 0 - 8 - 3 = -11\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice that the dot product is always a scalar, not a vector.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The dot product can also be defined in terms of vector length and angle:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ u \cdot v = \|u\| \, \|v\| \cos(\theta), \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\theta\) is the angle between \(u\) and \(v\) (\(0^\circ \leq \theta
    \leq 180^\circ\)).
  prefs: []
  type: TYPE_NORMAL
- en: 'This formula tells us:'
  prefs: []
  type: TYPE_NORMAL
- en: If the angle is acute (less than \(90^\circ\)), \(\cos(\theta) > 0\), so the
    dot product is positive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the angle is right (exactly \(90^\circ\)), \(\cos(\theta) = 0\), so the dot
    product is 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the angle is obtuse (greater than \(90^\circ\)), \(\cos(\theta) < 0\), so
    the dot product is negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, the sign of the dot product encodes directional alignment.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting the Two Definitions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'At first glance, the algebraic sum of products and the geometric length–angle
    formula seem unrelated. But they are equivalent. To see why, consider the law
    of cosines applied to a triangle formed by u, v, and u – v. Expanding both sides
    leads directly to the equivalence between the two formulas. This dual interpretation
    is what makes the dot product so powerful: it is both a computation rule and a
    geometric measurement.'
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two vectors are orthogonal (perpendicular) if and only if their dot product
    is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ u \cdot v = 0 \;\;\Longleftrightarrow\;\; \theta = 90^\circ. \]
  prefs: []
  type: TYPE_NORMAL
- en: This gives us an algebraic way to check for perpendicularity without drawing
    diagrams.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \((2, 1) \cdot (-1, 2) = (2 \times -1) + (1 \times 2) = -2 + 2 = 0\),
  prefs: []
  type: TYPE_NORMAL
- en: so the vectors are orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: Projections
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dot product also provides a way to project one vector onto another.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scalar projection of \(u\) onto \(v\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{proj}_{\text{scalar}}(u \text{ onto } v) = \frac{u \cdot v}{\|v\|}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The vector projection is then:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{proj}_{\text{vector}}(u \text{ onto } v) = \frac{u \cdot v}{\|v\|^2}
    \, v. \]
  prefs: []
  type: TYPE_NORMAL
- en: This allows us to decompose vectors into “parallel” and “perpendicular” components,
    which is central in geometry, physics, and data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute \(u = (3, 4)\) and \(v = (4, 3)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dot product: \((3 \times 4) + (4 \times 3) = 12 + 12 = 24\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Norms: \(\|u\| = 5\), \(\|v\| = 5\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\cos(\theta) = \tfrac{24}{5 \times 5} = \tfrac{24}{25} \approx 0.96\), so
    \(\theta \approx 16^\circ\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These vectors are nearly parallel.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute \(u = (1, 2, -1)\) and \(v = (2, -1, 1)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dot product: \((1 \times 2) + (2 \times -1) + (-1 \times 1) = 2 - 2 - 1 = -1\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Norms: \(\|u\| = \sqrt{6}\), \(\|v\| = \sqrt{6}\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\cos(\theta) = \tfrac{-1}{\sqrt{6} \times \sqrt{6}} = -\tfrac{1}{6}\), so
    \(\theta \approx 99.6^\circ\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Slightly obtuse.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Physical Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In physics, the dot product computes work:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Work} = \text{Force} \cdot \text{Displacement} = \|\text{Force}\| \,
    \|\text{Displacement}\| \cos(\theta). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Only the component of the force in the direction of motion contributes. If
    you push straight down on a box while trying to move it horizontally, the dot
    product is zero: no work is done in the direction of motion.'
  prefs: []
  type: TYPE_NORMAL
- en: Algebraic Properties
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Commutative: \(u \cdot v = v \cdot u\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distributive: \(u \cdot (v + w) = u \cdot v + u \cdot w\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scalar compatibility: \((c \cdot u) \cdot v = c \,(u \cdot v)\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Non-negativity: \(v \cdot v = \|v\|^2 \geq 0\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These guarantee that the dot product behaves consistently and meshes with the
    structure of vector spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The dot product is the first bridge between algebra and geometry. It:'
  prefs: []
  type: TYPE_NORMAL
- en: Defines angles and orthogonality in higher dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powers projections and decompositions, which underlie least squares, regression,
    and data fitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appears in physics as energy, power, and work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serves as the kernel of many machine learning methods (e.g., similarity measures
    in high-dimensional spaces).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without the dot product, linear algebra would lack a way to connect numbers
    with geometry and meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute (2, –1) · (–3, 4). Then find the angle between them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check if (1, 2, 3) and (2, 4, 6) are orthogonal. What does the dot product tell
    you?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the projection of (3, 1) onto (1, 2). Draw the original vector, the projection,
    and the perpendicular component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In physics terms: Suppose a 10 N force is applied at 60° to the direction of
    motion, and the displacement is 5 m. How much work is done?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These exercises reveal the dual power of the dot product: as a formula to compute
    and as a geometric tool to interpret.'
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Angles Between Vectors and Cosine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having defined the dot product, we are now ready to measure angles between vectors.
    In everyday life, angles tell us how two lines or directions relate-whether they
    point the same way, are perpendicular, or are opposed. In linear algebra, the
    dot product and cosine function give us a precise, generalizable way to define
    angles in any dimension, not just in 2D or 3D. This section explores how we compute,
    interpret, and apply vector angles.
  prefs: []
  type: TYPE_NORMAL
- en: The Definition of an Angle Between Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For two nonzero vectors \(u\) and \(v\), the angle \(\theta\) between them
    is defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \cos(\theta) = \frac{u \cdot v}{\|u\| \, \|v\|}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This formula comes directly from the geometric definition of the dot product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rearranging gives:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \theta = \arccos\!\left(\frac{u \cdot v}{\|u\| \, \|v\|}\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Key points:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\theta\) is always between \(0^\circ\) and \(180^\circ\) (or \(0\) and \(\pi\)
    radians).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The denominator normalizes the dot product by dividing by the product of lengths,
    so the result is dimensionless and always between \(-1\) and \(1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cosine value directly encodes alignment: positive, zero, or negative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpretation of Cosine Values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The cosine tells us about the directional relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\cos(\theta) = 1 \;\;\Rightarrow\;\; \theta = 0^\circ\) → vectors point in
    exactly the same direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\cos(\theta) = 0 \;\;\Rightarrow\;\; \theta = 90^\circ\) → vectors are orthogonal
    (perpendicular).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\cos(\theta) = -1 \;\;\Rightarrow\;\; \theta = 180^\circ\) → vectors point
    in exactly opposite directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\cos(\theta) > 0\) → acute angle → vectors point more “together” than apart.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\cos(\theta) < 0\) → obtuse angle → vectors point more “against” each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, the cosine compresses geometric alignment into a single number.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \(u = (1, 0), \; v = (0, 1)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dot product: \(1 \times 0 + 0 \times 1 = 0\)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Norms: \(1\) and \(1\)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\cos(\theta) = 0 \;\Rightarrow\; \theta = 90^\circ\)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The vectors are perpendicular, as expected.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: \(u = (2, 3), \; v = (4, 6)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dot product: \((2 \times 4) + (3 \times 6) = 8 + 18 = 26\)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Norms: \(\sqrt{2^2 + 3^2} = \sqrt{13}\), and \(\sqrt{4^2 + 6^2} = \sqrt{52}
    = 2\sqrt{13}\)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\cos(\theta) = \tfrac{26}{\sqrt{13} \cdot 2\sqrt{13}} = \tfrac{26}{26} = 1\)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\theta = 0^\circ\)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These vectors are multiples, so they align perfectly.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: \(u = (1, 1), \; v = (-1, 1)\)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dot product: \((1 \times -1) + (1 \times 1) = -1 + 1 = 0\)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\cos(\theta) = 0 \;\Rightarrow\; \theta = 90^\circ\)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The vectors are perpendicular, forming diagonals of a square.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Angles in Higher Dimensions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The beauty of the formula is that it works in any dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Even in \(\mathbb{R}^{100}\) or higher, we can define the angle between two
    vectors using only their dot product and norms.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we cannot visualize the geometry directly in high dimensions, the cosine
    formula still captures how aligned two directions are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \cos(\theta) = \frac{u \cdot v}{\|u\| \, \|v\|}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This is critical in machine learning, where data often lives in very high-dimensional
    spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Cosine Similarity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The cosine of the angle between two vectors is often called cosine similarity.
    It is widely used in data analysis and machine learning to measure how similar
    two data vectors are, independent of their magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: In text mining, documents are turned into word-frequency vectors. Cosine similarity
    measures how “close in topic” two documents are, regardless of length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In recommendation systems, cosine similarity compares user preference vectors
    to suggest similar users or items.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This demonstrates how a geometric concept extends far beyond pure math.
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonality Revisited
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The angle formula reinforces the special role of orthogonality.
  prefs: []
  type: TYPE_NORMAL
- en: If \(\cos(\theta) = 0\), then \(u \cdot v = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: This means the dot product not only computes length but also serves as a direct
    test for perpendicularity.
  prefs: []
  type: TYPE_NORMAL
- en: This algebraic shortcut is far easier than manually checking geometric right
    angles.
  prefs: []
  type: TYPE_NORMAL
- en: Angles and Projections
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Angles are closely tied to projections.
  prefs: []
  type: TYPE_NORMAL
- en: The length of the projection of \(u\) onto \(v\) is \(\|u\|\cos(\theta)\).
  prefs: []
  type: TYPE_NORMAL
- en: If the angle is small, the projection is large — most of \(u\) lies in the direction
    of \(v\).
  prefs: []
  type: TYPE_NORMAL
- en: If the angle is close to \(90^\circ\), the projection shrinks toward zero.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the cosine acts as a scaling factor between directions.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Angles between vectors provide:'
  prefs: []
  type: TYPE_NORMAL
- en: A way to generalize geometry beyond 2D/3D.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A measure of similarity in high-dimensional data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The foundation for orthogonality, projections, and decomposition of spaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A tool for optimization: in gradient descent, for example, the angle between
    the gradient and step direction determines how effectively we reduce error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without the ability to measure angles, we could not connect algebraic manipulations
    with geometric intuition or practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the angle between (2, 1) and (1, –1). Interpret the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find two vectors in 3D that form a 60° angle. Verify using the cosine formula.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider word vectors for “cat” and “dog” in a machine learning model. Why might
    cosine similarity be a better measure of similarity than Euclidean distance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: In \(\mathbb{R}^3\), find a vector orthogonal to both (1, 2, 3)
    and (3, 2, 1). What angle does it make with each of them?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By experimenting with these problems, you will see how angles provide the missing
    link between algebraic formulas and geometric meaning in linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Projections and Decompositions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In earlier sections, we saw how the dot product measures alignment and how
    the cosine formula gives us angles between vectors. The next natural step is to
    use these tools to project one vector onto another. Projection is a way to “shadow”
    one vector onto the direction of another, splitting vectors into meaningful parts:
    one along a given direction and one perpendicular to it. This is the essence of
    decomposition, and it is everywhere in linear algebra, geometry, physics, and
    data science.'
  prefs: []
  type: TYPE_NORMAL
- en: Scalar Projection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The scalar projection of a vector \(u\) onto a vector \(v\) measures how much
    of \(u\) lies in the direction of \(v\). It is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{proj}_{\text{scalar}}(u \text{ onto } v) = \frac{u \cdot v}{\|v\|}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: If this value is positive, \(u\) has a component pointing in the same direction
    as \(v\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is negative, \(u\) points partly in the opposite direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it is zero, \(u\) is completely perpendicular to \(v\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \(u = (3, 4)\), \(v = (1, 0)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Dot product: \((3 \times 1 + 4 \times 0) = 3\).'
  prefs: []
  type: TYPE_NORMAL
- en: \(\|v\| = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: So the scalar projection is \(3\). This tells us \(u\) has a “shadow” of length
    \(3\) on the \(x\)-axis.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Projection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The vector projection gives the actual arrow in the direction of \(v\) that
    corresponds to this scalar amount:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{proj}_{\text{vector}}(u \text{ onto } v) = \frac{u \cdot v}{\|v\|^2}
    \, v. \]
  prefs: []
  type: TYPE_NORMAL
- en: This formula normalizes \(v\) into a unit vector, then scales it by the scalar
    projection.
  prefs: []
  type: TYPE_NORMAL
- en: The result is a new vector lying along \(v\), capturing exactly the “parallel”
    part of \(u\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \(u = (3, 4)\), \(v = (1, 2)\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Dot product: \(3 \times 1 + 4 \times 2 = 3 + 8 = 11\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Norm squared of \(v\): \((1^2 + 2^2) = 5\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coefficient: \(11 / 5 = 2.2\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Projection vector: \(2.2 \cdot (1, 2) = (2.2, 4.4)\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So the part of \((3, 4)\) in the direction of \((1, 2)\) is \((2.2, 4.4)\).
  prefs: []
  type: TYPE_NORMAL
- en: Perpendicular Component
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once we have the projection, we can find the perpendicular component (often
    called the rejection) simply by subtracting:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ u_{\perp} = u - \text{proj}_{\text{vector}}(u \text{ onto } v). \]
  prefs: []
  type: TYPE_NORMAL
- en: This gives the part of \(u\) that is entirely orthogonal to \(v\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example continued:'
  prefs: []
  type: TYPE_NORMAL
- en: \(u_{\perp} = (3, 4) - (2.2, 4.4) = (0.8, -0.4)\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Check:'
  prefs: []
  type: TYPE_NORMAL
- en: \((0.8, -0.4) \cdot (1, 2) = 0.8 \times 1 + (-0.4) \times 2 = 0.8 - 0.8 = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Picture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Projection is like dropping a perpendicular from one vector onto another. Imagine
    shining a light perpendicular to v: the shadow of u on the line spanned by v is
    the projection. This visualization explains why projections split vectors naturally
    into two pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Parallel part: Along the line of v.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perpendicular part: Orthogonal to v, forming a right angle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these two parts reconstruct the original vector exactly.
  prefs: []
  type: TYPE_NORMAL
- en: Decomposition of Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Every vector \(u\) can be decomposed relative to another vector \(v\) into
    two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ u = \text{proj}_{\text{vector}}(u \text{ onto } v) + \big(u - \text{proj}_{\text{vector}}(u
    \text{ onto } v)\big). \]
  prefs: []
  type: TYPE_NORMAL
- en: This decomposition is unique and geometrically meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: 'It generalizes to subspaces: we can project onto entire planes or higher-dimensional
    spans, splitting a vector into a “within-subspace” part and a “perpendicular-to-subspace”
    part.'
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Physics (Work and Forces): Work is the projection of force onto displacement.
    Only the part of the force in the direction of motion contributes. Example: Pushing
    on a sled partly sideways wastes effort-the sideways component projects to zero.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geometry and Engineering: Projections are used in CAD (computer-aided design)
    to flatten 3D objects onto 2D surfaces, like blueprints or shadows.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Rendering 3D scenes onto a 2D screen is fundamentally a
    projection process.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Science: Projecting high-dimensional data onto a lower-dimensional subspace
    (like the first two principal components in PCA) makes patterns visible while
    preserving as much information as possible.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Signal Processing: Decomposition into projections onto sine and cosine waves
    forms the basis of Fourier analysis, which powers audio, image, and video compression.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Algebraic Properties
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Projections are linear: proj(u + w) = proj(u) + proj(w).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The perpendicular part is always orthogonal to the direction of projection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The decomposition is unique: no other pair of parallel and perpendicular vectors
    will reconstruct u.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The projection operator onto a unit vector v̂ satisfies: proj(u) = (v̂ v̂ᵀ)u,
    showing how projection can be expressed in matrix form.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Projection is not just a geometric trick; it is the core of many advanced topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Least squares regression is finding the projection of a data vector onto the
    span of predictor vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orthogonal decompositions like Gram–Schmidt and QR factorization rely on projections
    to build orthogonal bases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization methods often involve projecting guesses back onto feasible sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning uses projections constantly to reduce dimensions, compare vectors,
    and align features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without projection, we could not cleanly separate influence along directions
    or reduce complexity in structured ways.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Project (2, 3) onto (1, 0). What does the perpendicular component look like?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project (3, 1) onto (2, 2). Verify the perpendicular part is orthogonal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decompose (5, 5, 0) into parallel and perpendicular parts relative to (1, 0,
    0).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Write the projection matrix for projecting onto (1, 2). Apply it
    to (3, 4). Does it match the formula?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Through these exercises, you will see that projection is more than an operation-it
    is a lens through which we decompose, interpret, and simplify vectors and spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Cauchy–Schwarz and Triangle Inequalities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear algebra is not only about operations with vectors-it also involves understanding
    the fundamental relationships between them. Two of the most important results
    in this regard are the Cauchy–Schwarz inequality and the triangle inequality.
    These are cornerstones of vector spaces because they establish precise boundaries
    for lengths, angles, and inner products. Without them, the geometry of linear
    algebra would fall apart.
  prefs: []
  type: TYPE_NORMAL
- en: The Cauchy–Schwarz Inequality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For any two vectors \(u\) and \(v\) in \(\mathbb{R}^n\), the Cauchy–Schwarz
    inequality states:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ |u \cdot v| \leq \|u\| \, \|v\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: This means that the absolute value of the dot product of two vectors is always
    less than or equal to the product of their lengths.
  prefs: []
  type: TYPE_NORMAL
- en: Equality holds if and only if u and v are linearly dependent (i.e., one is a
    scalar multiple of the other).
  prefs: []
  type: TYPE_NORMAL
- en: Why It Is True
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Recall the geometric formula for the dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ u \cdot v = \|u\| \, \|v\| \cos(\theta). \]
  prefs: []
  type: TYPE_NORMAL
- en: Since \(-1 \leq \cos(\theta) \leq 1\), the magnitude of the dot product cannot
    exceed \(\|u\| \, \|v\|\).
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly the inequality.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let \(u = (3, 4)\) and \(v = (-4, 3)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Dot product: \((3 \times -4) + (4 \times 3) = -12 + 12 = 0\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Norms: \(\|u\| = 5\), \(\|v\| = 5\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Product of norms: \(25\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(|u \cdot v| = 0 \leq 25\), which satisfies the inequality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equality does not hold since they are not multiples - they are perpendicular.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The inequality tells us that two vectors can never “overlap” more strongly than
    the product of their magnitudes. If they align perfectly, the overlap is maximum
    (equality). If they’re perpendicular, the overlap is zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Think of it as: “the shadow of one vector on another can never be longer than
    the vector itself.”'
  prefs: []
  type: TYPE_NORMAL
- en: The Triangle Inequality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For any vectors \(u\) and \(v\), the triangle inequality states:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|u + v\| \leq \|u\| + \|v\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: This mirrors the geometric fact that in a triangle, any side is at most as long
    as the sum of the other two sides.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let \(u = (1, 2)\) and \(v = (3, 4)\).
  prefs: []
  type: TYPE_NORMAL
- en: \(\|u + v\| = \|(4, 6)\| = \sqrt{16 + 36} = \sqrt{52} \approx 7.21\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\|u\| + \|v\| = \sqrt{5} + 5 \approx 2.24 + 5 = 7.24\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indeed, \(7.21 \leq 7.24\), very close in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Equality Case
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The triangle inequality becomes equality when the vectors point in exactly the
    same direction (or are scalar multiples with nonnegative coefficients). For example,
    (1, 1) and (2, 2) produce equality because adding them gives a vector whose length
    equals the sum of their lengths.
  prefs: []
  type: TYPE_NORMAL
- en: Extensions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These inequalities hold in all inner product spaces, not just ℝⁿ. This means
    they apply to functions, sequences, and more abstract mathematical objects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Hilbert spaces (infinite-dimensional generalizations), they remain just as
    essential.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why They Matter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: They guarantee that the dot product and norm are well-behaved and geometrically
    meaningful.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'They ensure that the norm satisfies the requirements of a distance measure:
    nonnegativity, symmetry, and triangle inequality.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They underpin the validity of projections, orthogonality, and least squares
    methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They are essential in proving convergence of algorithms, error bounds, and stability
    in numerical linear algebra.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Without these inequalities, we could not trust that the geometry of vector spaces
    behaves consistently.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Verify Cauchy–Schwarz for (2, –1, 3) and (–1, 4, 0). Compute both sides.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try the triangle inequality for (–3, 4) and (5, –12). Does equality hold?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find two vectors where Cauchy–Schwarz is an equality. Explain why.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove the triangle inequality in \(\mathbb{R}^2\) using only the
    Pythagorean theorem and algebra, without relying on dot products.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Working through these problems will show you why these inequalities are not
    abstract curiosities but the structural glue of linear algebra’s geometry.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Orthonormal sets in \(\mathbb{R}^2\) and \(\mathbb{R}^3\)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up to now, we’ve discussed vectors, their lengths, angles, and how to project
    one onto another. A natural culmination of these ideas is the concept of orthonormal
    sets. These are collections of vectors that are not only orthogonal (mutually
    perpendicular) but also normalized (each of length 1). Orthonormal sets form the
    cleanest, most efficient coordinate systems in linear algebra. They are the mathematical
    equivalent of having rulers at right angles, perfectly calibrated to unit length.
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonal and Normalized
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s break the term “orthonormal” into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Orthogonal: Two vectors \(u\) and \(v\) are orthogonal if \(u \cdot v = 0\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^2\), this means the vectors meet at a right angle.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^3\), it means they form perpendicular directions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Normalized: A vector \(v\) is normalized if its length is \(1\), i.e., \(\|v\|
    = 1\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Such vectors are called unit vectors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When we combine both conditions, we get orthonormal vectors: vectors that are
    both perpendicular to each other and have unit length.'
  prefs: []
  type: TYPE_NORMAL
- en: Orthonormal Sets in \(\mathbb{R}^2\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In two dimensions, an orthonormal set typically consists of two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'A classic example is:'
  prefs: []
  type: TYPE_NORMAL
- en: \(e_1 = (1, 0), \quad e_2 = (0, 1)\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Dot product: \(e_1 \cdot e_2 = (1 \times 0 + 0 \times 1) = 0 \;\;\Rightarrow\;\;\)
    orthogonal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lengths: \(\|e_1\| = 1\), \(\|e_2\| = 1 \;\;\Rightarrow\;\;\) normalized'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, \(\{e_1, e_2\}\) is an orthonormal set.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, this is the standard basis for \(\mathbb{R}^2\).
  prefs: []
  type: TYPE_NORMAL
- en: Any vector \((x, y)\) can be written as \(x e_1 + y e_2\).
  prefs: []
  type: TYPE_NORMAL
- en: This is the simplest coordinate system.
  prefs: []
  type: TYPE_NORMAL
- en: Orthonormal Sets in \(\mathbb{R}^3\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In three dimensions, an orthonormal set usually has three vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard basis is:'
  prefs: []
  type: TYPE_NORMAL
- en: \(e_1 = (1, 0, 0), \quad e_2 = (0, 1, 0), \quad e_3 = (0, 0, 1)\)
  prefs: []
  type: TYPE_NORMAL
- en: Each pair has dot product zero, so they are orthogonal
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each has length \(1\), so they are normalized
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, they span all of \(\mathbb{R}^3\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometrically, they correspond to the \(x\)-, \(y\)-, and \(z\)-axes in 3D space.
  prefs: []
  type: TYPE_NORMAL
- en: Any vector \((x, y, z)\) can be written as a linear combination \(x e_1 + y
    e_2 + z e_3\).
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the Standard Basis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The standard basis is not the only orthonormal set. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: \(u = \left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right), \quad v = \left(-\tfrac{1}{\sqrt{2}},
    \tfrac{1}{\sqrt{2}}\right)\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Dot product: \((\tfrac{1}{\sqrt{2}})(-\tfrac{1}{\sqrt{2}}) + (\tfrac{1}{\sqrt{2}})(\tfrac{1}{\sqrt{2}})
    = -\tfrac{1}{2} + \tfrac{1}{2} = 0\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lengths: \(\sqrt{(\tfrac{1}{\sqrt{2}})^2 + (\tfrac{1}{\sqrt{2}})^2} = \sqrt{\tfrac{1}{2}
    + \tfrac{1}{2}} = 1\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So \(\{u, v\}\) is also orthonormal in \(\mathbb{R}^2\).
  prefs: []
  type: TYPE_NORMAL
- en: These vectors are rotated \(45^\circ\) relative to the standard axes.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in \(\mathbb{R}^3\), you can construct rotated orthonormal sets (such
    as unit vectors along diagonals), as long as the conditions of perpendicularity
    and unit length hold.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of Orthonormal Sets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Simplified coordinates: If \(\{v_1, \ldots, v_k\}\) is an orthonormal set,
    then for any vector \(u\) in their span, the coefficients are easy to compute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ c_i = u \cdot v_i \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is much simpler than solving systems of equations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Pythagorean theorem generalized: If vectors are orthonormal, the squared length
    of their sum is the sum of the squares of their coefficients.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For example, if \(u = a v_1 + b v_2\), then
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ \|u\|^2 = a^2 + b^2 \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Projection is easy: Projecting onto an orthonormal set is straightforward —
    just take dot products.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Matrices become nice: When vectors form the columns of a matrix, orthonormality
    makes that matrix an orthogonal matrix, which has special properties: its transpose
    equals its inverse, and it preserves lengths and angles.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Importance in \(\mathbb{R}^2\) and \(\mathbb{R}^3\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In geometry, orthonormal bases correspond to coordinate axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In physics, they represent independent directions of motion or force.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In computer graphics, orthonormal sets define camera axes and object rotations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In engineering, they simplify stress, strain, and rotation analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though \(\mathbb{R}^2\) and \(\mathbb{R}^3\) are relatively simple, the
    same ideas extend naturally to higher dimensions, where visualization is impossible
    but the algebra is identical.
  prefs: []
  type: TYPE_NORMAL
- en: Why Orthonormal Sets Matter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Orthonormality is the gold standard for building bases in linear algebra:'
  prefs: []
  type: TYPE_NORMAL
- en: It makes calculations fast and simple.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It ensures numerical stability in computations (important in algorithms and
    simulations).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It underpins key decompositions like QR factorization, singular value decomposition
    (SVD), and spectral theorems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It provides the cleanest way to think about space: orthogonal, independent
    directions scaled to unit length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever possible, mathematicians and engineers prefer orthonormal bases over
    arbitrary ones.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Verify that (3/5, 4/5) and (–4/5, 3/5) form an orthonormal set in \(\mathbb{R}^2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Construct three orthonormal vectors in \(\mathbb{R}^3\) that are not the standard
    basis. Hint: start with (1/√2, 1/√2, 0) and build perpendiculars.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For u = (2, 1), compute its coordinates relative to the orthonormal set {(1/√2,
    1/√2), (–1/√2, 1/√2)}.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that if {v₁, …, vₖ} is orthonormal, then the matrix with these
    as columns is orthogonal, i.e., QᵀQ = I.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Through these exercises, you will see how orthonormal sets make every aspect
    of linear algebra-from projections to decompositions-simpler, cleaner, and more
    powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 2\. Matrices and basic operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opening
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 11\. Matrices as Tables and as Machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The next stage in our journey is to move from vectors to matrices. A matrix
    may look like just a rectangular array of numbers, but in linear algebra it plays
    two distinct and equally important roles:'
  prefs: []
  type: TYPE_NORMAL
- en: As a table of numbers, storing data, coefficients, or geometric patterns in
    a compact form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a machine that transforms vectors into other vectors, capturing the essence
    of linear transformations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both views are valid, and learning to switch between them is crucial to building
    intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Matrices as Tables
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: At the most basic level, a matrix is a grid of numbers arranged into rows and
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'A \(2 \times 2\) matrix has 2 rows and 2 columns:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A \(3 \times 2\) matrix has 3 rows and 2 columns:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ B = \begin{bmatrix} b_{11} & b_{12} \\ b_{21} & b_{22} \\ b_{31} & b_{32}
    \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each entry \(a_{ij}\) or \(b_{ij}\) tells us the number in the i-th row and
    j-th column. The rows of a matrix can represent constraints, equations, or observations;
    the columns can represent features, variables, or directions.
  prefs: []
  type: TYPE_NORMAL
- en: In this sense, matrices are data containers, organizing information efficiently.
    That’s why matrices show up in spreadsheets, statistics, computer graphics, and
    scientific computing.
  prefs: []
  type: TYPE_NORMAL
- en: Matrices as Machines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The deeper view of a matrix is as a function from vectors to vectors. If x is
    a column vector, then multiplying A·x produces a new vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 0 \\ 1 & 3 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix}
    4 \\ 5 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplying:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A\mathbf{x} = \begin{bmatrix} 2×4 + 0×5 \\ 1×4 + 3×5 \end{bmatrix} = \begin{bmatrix}
    8 \\ 19 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here, the matrix is acting as a machine that takes input (4, 5) and outputs
    (8, 19). The “machine rules” are encoded in the rows of A.
  prefs: []
  type: TYPE_NORMAL
- en: Column View of Matrix Multiplication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another way to see it: multiplying A·x is the same as taking a linear combination
    of A’s columns.'
  prefs: []
  type: TYPE_NORMAL
- en: If
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} a_1 & a_2 \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix}
    x_1 \\ x_2 \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'then:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A\mathbf{x} = x_1 a_1 + x_2 a_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: So the vector x tells the machine “how much” of each column to mix together.
    This column view is critical-it connects matrices to span, dimension, and basis
    ideas we saw earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The Duality of Tables and Machines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As a table, a matrix is a static object: numbers written in rows and columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As a machine, the same numbers become instructions for transforming vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This duality is not just conceptual-it’s the key to understanding why linear
    algebra is so powerful. A dataset, once stored as a table, can be interpreted
    as a transformation. Likewise, a transformation, once understood, can be encoded
    as a table.
  prefs: []
  type: TYPE_NORMAL
- en: Examples in Practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Physics: A stress–strain matrix is a table of coefficients. But it also acts
    as a machine that transforms applied forces into deformations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: A 2D rotation matrix is a machine that spins vectors, but
    it can be stored in a simple 2×2 table.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Economics: Input–output models use matrices as tables of production coefficients.
    Applying them to demand vectors transforms them into resource requirements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Geometric Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Every 2×2 or 3×3 matrix corresponds to some linear transformation in the plane
    or space. Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scaling: \(\begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}\) doubles lengths.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reflection: \(\begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}\) flips across
    the x-axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rotation: \(\begin{bmatrix} \cos θ & -\sin θ \\ \sin θ & \cos θ \end{bmatrix}\)
    rotates vectors by θ.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are not just tables of numbers-they are precise, reusable machines.
  prefs: []
  type: TYPE_NORMAL
- en: Why This Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This section sets the stage for all matrix theory:'
  prefs: []
  type: TYPE_NORMAL
- en: Thinking of matrices as tables helps in data interpretation and organization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thinking of matrices as machines helps in understanding linear transformations,
    eigenvalues, and decompositions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most importantly, learning to switch between the two perspectives makes linear
    algebra both concrete and abstract-bridging computation with geometry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write a 2×3 matrix and identify its rows and columns. What might they represent
    in a real-world dataset?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiply \(\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}\) by \(\begin{bmatrix}
    2 \\ –1 \end{bmatrix}\). Interpret the result using both the row and column views.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a matrix that scales vectors by 2 along the x-axis and reflects them
    across the y-axis. Test it on (1, 1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show how the same 3×3 rotation matrix can be viewed as a data table
    of cosines/sines and as a machine that turns input vectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By mastering both perspectives, you’ll see matrices not just as numbers but
    as dynamic objects that encode and execute transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 12\. Matrix Shapes, Indexing, and Block Views
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Matrices come in many shapes and sizes, and the way we label their entries matters.
    This section is about learning how to read and write matrices carefully, how to
    work with rows and columns, and how to use block structure to simplify problems.
    These seemingly simple ideas are what allow us to manipulate large systems with
    precision and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Shapes of Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The shape of a matrix is given by its number of rows and columns:'
  prefs: []
  type: TYPE_NORMAL
- en: A m×n matrix has m rows and n columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rows run horizontally, columns run vertically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Square matrices have m = n; rectangular matrices have m ≠ n.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A 2×3 matrix:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A 3×2 matrix:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 7 & 8 \\ 9 & 10 \\ 11 & 12 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Shape matters because it determines whether certain operations (like multiplication)
    are possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indexing: The Language of Entries'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Each entry in a matrix has two indices: one for its row, one for its column.'
  prefs: []
  type: TYPE_NORMAL
- en: \(a_{ij}\) = entry in row i, column j.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first index always refers to the row, the second to the column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, in
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 4 & 7 \\ 2 & 5 & 8 \\ 3 & 6 & 9 \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \(a_{11} = 1\), \(a_{23} = 8\), \(a_{32} = 6\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indexing is the grammar of matrix language. Without it, we can’t specify positions
    or write formulas clearly.
  prefs: []
  type: TYPE_NORMAL
- en: Rows and Columns as Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Every row and every column of a matrix is itself a vector.
  prefs: []
  type: TYPE_NORMAL
- en: The i-th row is written as \(A_{i,*}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The j-th column is written as \(A_{*,j}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: From the matrix above,'
  prefs: []
  type: TYPE_NORMAL
- en: 'First row: (1, 4, 7).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second column: (4, 5, 6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This duality is powerful: rows often represent constraints or equations, while
    columns represent directions or features. Later, when we interpret matrix–vector
    products, we’ll see that multiplying A·x means combining columns, while multiplying
    yᵀ·A means combining rows.'
  prefs: []
  type: TYPE_NORMAL
- en: Submatrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes we want just part of a matrix. A submatrix is formed by selecting
    certain rows and columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: From'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = \begin{bmatrix} 2 & 4 & 6 \\ 1 & 3 & 5 \\ 7 & 8 & 9 \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'the submatrix of the first two rows and last two columns is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 4 & 6 \\ 3 & 5 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Submatrices allow us to zoom in and isolate parts of a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Block Matrices: Dividing to Conquer'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Large matrices can often be broken into blocks, which are smaller submatrices
    arranged inside. This is like dividing a spreadsheet into quadrants.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ C = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where each \(A_{ij}\) is itself a smaller matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'This structure is useful in:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Computation: Algorithms often process blocks instead of individual entries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Theory: Many proofs and factorizations rely on viewing a matrix in blocks (e.g.,
    LU, QR, Schur decomposition).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Applications: Partitioning data tables into logical sections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Splitting a 4×4 matrix into four 2×2 blocks helps us treat it as a
    “matrix of matrices.”'
  prefs: []
  type: TYPE_NORMAL
- en: Special Shapes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some shapes of matrices are so common they deserve names:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Row vector: 1×n matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column vector: n×1 matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Diagonal matrix: Nonzero entries only on the diagonal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Identity matrix: Square diagonal matrix with 1’s on the diagonal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zero matrix: All entries are 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing these shapes saves time and clarifies reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Careful attention to matrix shapes, indexing, and block views ensures:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision: We can describe positions unambiguously.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Structure awareness: Recognizing patterns (diagonal, triangular, block) leads
    to more efficient computations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scalability: Block partitioning is the foundation of modern numerical linear
    algebra libraries, where matrices are too large to handle entry by entry.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geometry: Rows and columns as vectors connect matrix structure to span, basis,
    and dimension.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These basic tools prepare us for multiplication, transformations, and factorization.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write a 3×4 matrix and label the entry in row 2, column 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract a 2×2 submatrix from the corners of a 4×4 matrix of your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Break a 6×6 matrix into four 3×3 blocks. How would you represent it compactly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Given'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ D = \begin{bmatrix} 1 & 2 & 3 & 4 \\ 5 & 6 & 7 & 8 \\ 9 & 10 & 11 & 12 \end{bmatrix},
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: write it as a block matrix with a 2×2 block in the top-left, a 2×2 block in
    the top-right, and a 1×4 block in the bottom row.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By practicing with shapes, indexing, and blocks, you’ll develop the ability
    to navigate matrices not just as raw grids of numbers but as structured objects
    ready for deeper algebraic and geometric insights.
  prefs: []
  type: TYPE_NORMAL
- en: 13\. Matrix Addition and Scalar Multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before exploring matrix–vector and matrix–matrix multiplication, it is essential
    to understand the simplest operations we can perform with matrices: addition and
    scalar multiplication. These operations extend the rules we learned for vectors,
    but now applied to entire grids of numbers. Although straightforward, they are
    the foundation for more complex algebraic manipulations and help establish the
    idea of matrices as elements of a vector space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix Addition: Entry by Entry'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If two matrices \(A\) and \(B\) have the same shape (same number of rows and
    columns), we can add them by adding corresponding entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally: If'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = [a_{ij}], \quad B = [b_{ij}], \]
  prefs: []
  type: TYPE_NORMAL
- en: then
  prefs: []
  type: TYPE_NORMAL
- en: \[ A + B = [a_{ij} + b_{ij}]. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} + \begin{bmatrix} 7
    & 8 & 9 \\ 10 & 11 & 12 \end{bmatrix} = \begin{bmatrix} 8 & 10 & 12 \\ 14 & 16
    & 18 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Key point: Addition is only defined if the matrices are the same shape. A 2×3
    matrix cannot be added to a 3×2 matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scalar Multiplication: Scaling Every Entry'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A scalar multiplies every entry of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally: For scalar \(c\) and matrix \(A = [a_{ij}]\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ cA = [c \cdot a_{ij}]. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ 3 \cdot \begin{bmatrix} 2 & -1 \\ 0 & 4 \end{bmatrix} = \begin{bmatrix} 6
    & -3 \\ 0 & 12 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This mirrors vector scaling: stretching or shrinking the whole matrix by a
    constant factor.'
  prefs: []
  type: TYPE_NORMAL
- en: Properties of Addition and Scalar Multiplication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'These two operations satisfy familiar algebraic properties that make the set
    of all m×n matrices into a vector space:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Commutativity: \(A + B = B + A\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Associativity: \((A + B) + C = A + (B + C)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Additive identity: \(A + 0 = A\), where 0 is the zero matrix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Additive inverse: For every \(A\), there exists \(-A\) such that \(A + (-A)
    = 0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distributivity: \(c(A + B) = cA + cB\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compatibility: \((c + d)A = cA + dA\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scalar associativity: \((cd)A = c(dA)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Unit scalar: \(1A = A\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These guarantee that working with matrices feels like working with numbers and
    vectors, only in a higher-level setting.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Arithmetic as Table Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the table view, addition and scalar multiplication are just simple bookkeeping:
    line up two tables of the same shape and add entry by entry; multiply the whole
    table by a constant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Imagine two spreadsheets of monthly expenses. Adding them gives combined
    totals. Multiplying by 12 converts a monthly table into a yearly estimate.'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Arithmetic as Machine Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the machine view, these operations adjust the behavior of linear transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding matrices corresponds to adding their effects when applied to vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling a matrix scales the effect of the transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Let \(A\) rotate vectors slightly, and \(B\) stretch vectors. The
    matrix \(A + B\) represents a transformation that applies both influences together.
    Scaling by 2 doubles the effect of the transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Case: Zero and Identity'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Zero matrix: All entries are 0\. Adding it to any matrix changes nothing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scalar multiples of the identity: \(cI\) scales every vector by c when applied.
    For example, \(2I\) doubles every vector’s length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These act as neutral or scaling elements in matrix arithmetic.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In \(\mathbb{R}^2\) or \(\mathbb{R}^3\), adding transformation matrices is
    like superimposing geometric effects: e.g., one matrix shears, another rotates,
    their sum mixes both.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scaling a transformation makes its action stronger or weaker. Doubling a shear
    makes it twice as pronounced.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This shows that even before multiplication, addition and scaling already have
    geometric meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Though simple, these operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Define matrices as elements of vector spaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lay the groundwork for linear combinations of matrices, critical in eigenvalue
    problems, optimization, and control theory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enable modular problem-solving: break big transformations into smaller ones
    and recombine them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appear everywhere in practice, from combining datasets to scaling transformations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without addition and scalar multiplication, we could not treat matrices systematically
    as algebraic objects.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Add
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 0 \\ 1 & 3 \end{bmatrix} \quad \text{and} \quad \begin{bmatrix}
    -2 & 5 \\ 4 & -3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Multiply
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & -1 & 2 \\ 0 & 3 & 4 \end{bmatrix} \]
  prefs: []
  type: TYPE_NORMAL
- en: by –2.
  prefs: []
  type: TYPE_NORMAL
- en: Show that (A + B) + C = A + (B + C) with explicit 2×2 matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Construct two 3×3 matrices A and B such that A + B = 0\. What does
    that tell you about B?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By practicing these fundamentals, you will see that even the most basic operations
    on matrices already build the algebraic backbone for deeper results like matrix
    multiplication, transformations, and factorization.
  prefs: []
  type: TYPE_NORMAL
- en: 14\. Matrix–Vector Product (Linear Combinations of Columns)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We now arrive at one of the most important operations in all of linear algebra:
    the matrix–vector product. This operation takes a matrix \(A\) and a vector x,
    and produces a new vector. While the computation is straightforward, its interpretations
    are deep: it can be seen as combining rows, as combining columns, or as applying
    a linear transformation. This is the operation that connects matrices to the geometry
    of vector spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: The Algebraic Rule
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose \(A\) is an \(m \times n\) matrix, and x is a vector in \(\mathbb{R}^n\).
    The product \(A\mathbf{x}\) is a vector in \(\mathbb{R}^m\), defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} a_{11} & a_{12} & \cdots & a_{1n} \\ a_{21} & a_{22}
    & \cdots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \cdots
    & a_{mn} \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots
    \\ x_n \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Then:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A\mathbf{x} = \begin{bmatrix} a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n
    \\ a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n \\ \vdots \\ a_{m1}x_1 + a_{m2}x_2
    + \cdots + a_{mn}x_n \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Each entry of the output is a dot product between one row of \(A\) and the vector
    x.
  prefs: []
  type: TYPE_NORMAL
- en: 'Row View: Dot Products'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From the row perspective, \(A\mathbf{x}\) is computed row by row:'
  prefs: []
  type: TYPE_NORMAL
- en: Take each row of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dot it with x.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That result becomes one entry of the output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 3 & 4 \\ -1 & 2 \end{bmatrix}, \quad \mathbf{x}
    = \begin{bmatrix} 5 \\ -1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'First row dot x: \(2(5) + 1(-1) = 9\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second row dot x: \(3(5) + 4(-1) = 11\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Third row dot x: \((-1)(5) + 2(-1) = -7\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A\mathbf{x} = \begin{bmatrix} 9 \\ 11 \\ -7 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Column View: Linear Combinations'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From the column perspective, \(A\mathbf{x}\) is a linear combination of the
    columns of A.
  prefs: []
  type: TYPE_NORMAL
- en: If
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} | & | & & | \\ a_1 & a_2 & \cdots & a_n \\ | & | & &
    | \end{bmatrix}, \quad \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n
    \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'then:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A\mathbf{x} = x_1 a_1 + x_2 a_2 + \cdots + x_n a_n. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'That is: multiply each column of \(A\) by the corresponding entry in x, then
    add them up.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This interpretation connects directly to the idea of span: the set of all vectors
    \(A\mathbf{x}\) as x varies is exactly the span of the columns of \(A\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Machine View: Linear Transformations'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The machine view ties everything together: multiplying a vector by a matrix
    means applying the linear transformation represented by the matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: If \(A\) is a 2×2 rotation matrix, then \(A\mathbf{x}\) rotates the vector x.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(A\) is a scaling matrix, then \(A\mathbf{x}\) stretches or shrinks x.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(A\) is a projection matrix, then \(A\mathbf{x}\) projects x onto a line
    or plane.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, the algebraic definition encodes geometric and functional meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Geometric Action
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scaling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then \(A\mathbf{x}\) doubles the length of any vector x.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reflection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This flips vectors across the x-axis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rotation by θ:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} \cosθ & -\sinθ \\ \sinθ & \cosθ \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This rotates vectors counterclockwise by θ in the plane.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The matrix–vector product is the building block of everything in linear algebra:'
  prefs: []
  type: TYPE_NORMAL
- en: It defines the action of a matrix as a linear map.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It connects directly to span and dimension (columns generate all possible outputs).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It underpins solving linear systems, eigenvalue problems, and decompositions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is the engine of computation in applied mathematics, from computer graphics
    to machine learning (e.g., neural networks compute billions of matrix–vector products).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix} \begin{bmatrix} 2 \\
    0 \\ 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Express the result of the above product as a linear combination of the columns
    of the matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a 2×2 matrix that reflects vectors across the line \(y = x\). Test
    it on (1, 0) and (0, 1).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: For a 3×3 matrix, show that the set of all possible \(A\mathbf{x}\)
    (as x varies) is exactly the column space of \(A\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By mastering both the computational rules and the interpretations of the matrix–vector
    product, you will gain the most important insight in linear algebra: matrices
    are not just tables-they are engines that transform space.'
  prefs: []
  type: TYPE_NORMAL
- en: 15\. Matrix–Matrix Product (Composition of Linear Steps)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having understood how a matrix acts on a vector, the next natural step is to
    understand how one matrix can act on another. This leads us to the matrix–matrix
    product, a rule for combining two matrices into a single new matrix. Though the
    arithmetic looks complicated at first, the underlying idea is elegant: multiplying
    two matrices represents composing two linear transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: The Algebraic Rule
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose \(A\) is an \(m \times n\) matrix and \(B\) is an \(n \times p\) matrix.
    Their product \(C = AB\) is an \(m \times p\) matrix defined by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ c_{ij} = \sum_{k=1}^n a_{ik} b_{kj}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'That is: each entry of \(C\) is the dot product of the i-th row of \(A\) with
    the j-th column of \(B\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: A 2×3 times a 3×2'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}, \quad B = \begin{bmatrix}
    7 & 8 \\ 9 & 10 \\ 11 & 12 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Product: \(C = AB\) will be 2×2.'
  prefs: []
  type: TYPE_NORMAL
- en: \(c_{11} = 1\cdot 7 + 2\cdot 9 + 3\cdot 11 = 58\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(c_{12} = 1\cdot 8 + 2\cdot 10 + 3\cdot 12 = 64\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(c_{21} = 4\cdot 7 + 5\cdot 9 + 6\cdot 11 = 139\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(c_{22} = 4\cdot 8 + 5\cdot 10 + 6\cdot 12 = 154\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ C = \begin{bmatrix} 58 & 64 \\ 139 & 154 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Column View: Linear Combinations of Columns'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From the column perspective, \(AB\) is computed by applying \(A\) to each column
    of \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: 'If \(B = [b_1 \; b_2 \; \cdots \; b_p]\), then:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ AB = [A b_1 \; A b_2 \; \cdots \; A b_p]. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'That is: multiply \(A\) by each column of \(B\). This is often the simplest
    way to think of the product.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Row View: Linear Combinations of Rows'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From the row perspective, each row of \(AB\) is formed by combining rows of
    \(B\) using coefficients from a row of \(A\). This dual view is less common but
    equally useful, especially in proofs and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Machine View: Composition of Transformations'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The most important interpretation is the machine view: multiplying matrices
    corresponds to composing transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: If \(A\) maps \(\mathbb{R}^n \to \mathbb{R}^m\) and \(B\) maps \(\mathbb{R}^p
    \to \mathbb{R}^n\), then \(AB\) maps \(\mathbb{R}^p \to \mathbb{R}^m\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In words: do \(B\) first, then \(A\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: Let \(B\) rotate vectors by 90°.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let \(A\) scale vectors by 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then \(AB\) rotates and then scales-both steps combined into a single transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scaling then rotation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}, \quad B = \begin{bmatrix}
    0 & -1 \\ 1 & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then \(AB\) scales vectors by 2 after rotating them 90°.
  prefs: []
  type: TYPE_NORMAL
- en: 'Projection then reflection: If \(B\) projects onto the x-axis and \(A\) reflects
    across the y-axis, then \(AB\) represents “project then reflect.”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Properties of Matrix Multiplication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Associative: \((AB)C = A(BC)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distributive: \(A(B + C) = AB + AC\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Not commutative: In general, \(AB \neq BA\). Order matters!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Identity: \(AI = IA = A\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These properties highlight that while multiplication is structured, it is not
    symmetric. The order encodes the order of operations in transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Matrix multiplication is the core of linear algebra because:'
  prefs: []
  type: TYPE_NORMAL
- en: It encodes function composition in algebraic form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It provides a way to capture multiple transformations in a single matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It underpins algorithms in computer graphics, robotics, statistics, and machine
    learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It reveals deeper structure, like commutativity failing, which reflects real-world
    order of operations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Almost every application of linear algebra-solving equations, computing eigenvalues,
    training neural networks-relies on efficient matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 \\ 2 & 3 \end{bmatrix} \begin{bmatrix} 4 & 5 \\ 6 &
    7 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Show that \(AB \neq BA\) for the matrices
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}, \quad B = \begin{bmatrix}
    0 & 0 \\ 1 & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Construct two 2×2 matrices where \(AB = BA\). Why does commutativity happen
    here?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: If \(A\) is a projection and \(B\) is a rotation, compute \(AB\)
    and \(BA\). Do they represent the same geometric operation?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Through these perspectives, the matrix–matrix product shifts from being a mechanical
    formula to being a language for combining linear steps-each product telling the
    story of “do this, then that.”
  prefs: []
  type: TYPE_NORMAL
- en: 16\. Identity, Inverse, and Transpose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With addition, scalar multiplication, and matrix multiplication in place, we
    now introduce three special operations and objects that form the backbone of matrix
    algebra: the identity matrix, the inverse of a matrix, and the transpose of a
    matrix. Each captures a fundamental principle-neutrality, reversibility, and symmetry-and
    together they provide the algebraic structure that makes linear algebra so powerful.'
  prefs: []
  type: TYPE_NORMAL
- en: The Identity Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The identity matrix is the matrix equivalent of the number 1 in multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition: The identity matrix \(I_n\) is the \(n \times n\) matrix with 1’s
    on the diagonal and 0’s everywhere else.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example (3×3):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ I_3 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Property: For any \(n \times n\) matrix \(A\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ AI_n = I_nA = A. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Machine view: \(I\) does nothing-it maps every vector to itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Inverse of a Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The inverse is the matrix equivalent of the reciprocal of a number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition: For a square matrix \(A\), its inverse \(A^{-1}\) is the matrix
    such that'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ AA^{-1} = A^{-1}A = I. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Not all matrices have inverses. A matrix is invertible if and only if it is
    square and its determinant is nonzero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix}, \quad A^{-1} = \begin{bmatrix}
    1 & -1 \\ -1 & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Check:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ AA^{-1} = \begin{bmatrix} 2 & 1 \\ 1 & 1 \end{bmatrix} \begin{bmatrix} 1
    & -1 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} =
    I. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine view: Applying \(A\) transforms a vector. Applying \(A^{-1}\) undoes
    that transformation, restoring the original input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-Invertible Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some matrices cannot be inverted. These are called singular.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ B = \begin{bmatrix} 2 & 4 \\ 1 & 2 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, the second column is a multiple of the first. The transformation squashes
    vectors into a line, losing information-so it cannot be reversed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This ties invertibility to geometry: a transformation that collapses dimensions
    cannot be undone.'
  prefs: []
  type: TYPE_NORMAL
- en: The Transpose of a Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The transpose reflects a matrix across its diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definition: For \(A = [a_{ij}]\),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A^T = [a_{ji}]. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In words: rows become columns, columns become rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}, \quad A^T = \begin{bmatrix}
    1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Properties:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \((A^T)^T = A\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \((A + B)^T = A^T + B^T\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \((cA)^T = cA^T\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \((AB)^T = B^T A^T\) (note the reversed order!).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Symmetric and Orthogonal Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two important classes emerge from the transpose:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Symmetric matrices: \(A = A^T\). Example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 3 \\ 3 & 5 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'These have beautiful properties: real eigenvalues and orthogonal eigenvectors.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Orthogonal matrices: \(Q^TQ = I\). Their columns form an orthonormal set, and
    they represent pure rotations/reflections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The identity guarantees a neutral element for multiplication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The inverse provides a way to solve equations \(A\mathbf{x} = \mathbf{b}\) via
    \(\mathbf{x} = A^{-1}\mathbf{b}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The transpose ties matrices to geometry, inner products, and symmetry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Together, they form the algebraic foundation for deeper topics: determinants,
    eigenvalues, factorizations, and numerical methods.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Without these tools, matrix algebra would lack structure and reversibility.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the transpose of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & 2 \\ -3 & 4 & 5 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Verify that \((AB)^T = B^TA^T\) for
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 \\ 0 & 3 \end{bmatrix}, \quad B = \begin{bmatrix}
    4 & 0 \\ 5 & 6 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Find the inverse of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 3 & 2 \\ 1 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge: Show that if \(Q\) is orthogonal, then \(Q^{-1} = Q^T\). Interpret
    this geometrically as saying “rotations can be undone by transposing.”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Through these exercises, you’ll see how identity, inverse, and transpose anchor
    the structure of linear algebra, providing neutrality, reversibility, and symmetry
    in every calculation.
  prefs: []
  type: TYPE_NORMAL
- en: 17\. Symmetric, Diagonal, Triangular, and Permutation Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Not all matrices are created equal-some have special shapes or patterns that
    give them unique properties. These structured matrices are the workhorses of linear
    algebra: they simplify computation, reveal geometry, and form the building blocks
    for algorithms. In this section, we study four especially important classes: symmetric,
    diagonal, triangular, and permutation matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A matrix is symmetric if it equals its transpose:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = A^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 3 & 4 \\ 3 & 5 & 6 \\ 4 & 6 & 9 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Geometric meaning: Symmetric matrices represent linear transformations that
    have no “handedness.” They often arise in physics (energy, covariance, stiffness).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Algebraic fact: Symmetric matrices have real eigenvalues and an orthonormal
    basis of eigenvectors. This property underpins the spectral theorem, one of the
    pillars of linear algebra.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diagonal Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A matrix is diagonal if all non-diagonal entries are zero.
  prefs: []
  type: TYPE_NORMAL
- en: \[ D = \begin{bmatrix} d_1 & 0 & 0 \\ 0 & d_2 & 0 \\ 0 & 0 & d_3 \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying by \(D\) scales each coordinate separately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computations with diagonals are lightning fast:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adding: add diagonal entries.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiplying: multiply diagonal entries.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inverting: invert each diagonal entry (if nonzero).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}
    = \begin{bmatrix} 2x \\ 3y \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This is why diagonalization is so valuable: turning a general matrix into a
    diagonal one simplifies everything.'
  prefs: []
  type: TYPE_NORMAL
- en: Triangular Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A matrix is upper triangular if all entries below the main diagonal are zero,
    and lower triangular if all entries above the diagonal are zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upper triangular example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & 6 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lower triangular example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 7 & 0 & 0 \\ 8 & 9 & 0 \\ 10 & 11 & 12 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Why they matter:'
  prefs: []
  type: TYPE_NORMAL
- en: Determinant = product of diagonal entries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to solve systems by substitution (forward or backward).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every square matrix can be factored into triangular matrices (LU decomposition).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Permutation Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A permutation matrix is obtained by permuting the rows (or columns) of an identity
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = \begin{bmatrix} 0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplying by \(P\):'
  prefs: []
  type: TYPE_NORMAL
- en: On the left, permutes the rows of a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the right, permutes the columns of a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Permutation matrices are used in pivoting strategies in elimination, ensuring
    numerical stability in solving systems. They are also orthogonal: \(P^{-1} = P^T\).'
  prefs: []
  type: TYPE_NORMAL
- en: Connections Between Them
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A diagonal matrix is a special case of triangular (both upper and lower).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Symmetric matrices often become diagonal under orthogonal transformations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Permutation matrices help reorder triangular or diagonal matrices without breaking
    their structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these classes show that structure leads to simplicity-many computational
    algorithms exploit these patterns for speed and stability.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Symmetric matrices guarantee stable and interpretable eigen-decompositions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diagonal matrices make computation effortless.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Triangular matrices are the backbone of elimination and factorization methods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Permutation matrices preserve structure while reordering, critical for algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Almost every advanced method in numerical linear algebra relies on reducing
    general matrices into one of these structured forms.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Verify that
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 \\ 2 & 5 \end{bmatrix} \]
  prefs: []
  type: TYPE_NORMAL
- en: is symmetric. Find its transpose.
  prefs: []
  type: TYPE_NORMAL
- en: Compute the determinant of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 3 & 0 & 0 \\ 0 & 4 & 0 \\ 0 & 0 & 5 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Solve
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 3 & 1 \\ 0 & 5 & 2 \\ 0 & 0 & 4 \end{bmatrix} \mathbf{x}
    = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} \]
  prefs: []
  type: TYPE_NORMAL
- en: using back substitution.
  prefs: []
  type: TYPE_NORMAL
- en: Construct a 4×4 permutation matrix that swaps the first and last rows. Apply
    it to a 4×1 vector of your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By exploring these four structured families, you’ll start to see that not all
    matrices are messy-many have order hidden in their arrangement, and exploiting
    that order is the key to both theoretical understanding and efficient computation.
  prefs: []
  type: TYPE_NORMAL
- en: 18\. Trace and Basic Matrix Properties
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far we have studied shapes, multiplication rules, and special classes of
    matrices. In this section we introduce a simple but surprisingly powerful quantity:
    the trace of a matrix. Along with it, we review a set of basic matrix properties
    that provide shortcuts, invariants, and insights into how matrices behave.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition of the Trace
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a square matrix \(A = [a_{ij}]\) of size \(n \times n\), the trace is the
    sum of the diagonal entries:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{tr}(A) = a_{11} + a_{22} + \cdots + a_{nn}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 5 & 7 \\ 0 & 3 & 1 \\ 4 & 6 & 8 \end{bmatrix}, \quad
    \text{tr}(A) = 2 + 3 + 8 = 13. \]
  prefs: []
  type: TYPE_NORMAL
- en: The trace extracts a single number summarizing the “diagonal content” of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of the Trace
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The trace is linear and interacts nicely with multiplication and transposition:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Linearity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\text{tr}(A + B) = \text{tr}(A) + \text{tr}(B)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\text{tr}(cA) = c \cdot \text{tr}(A)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cyclic Property:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\text{tr}(AB) = \text{tr}(BA)\), as long as the products are defined.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: More generally, \(\text{tr}(ABC) = \text{tr}(BCA) = \text{tr}(CAB)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: But in general, \(\text{tr}(AB) \neq \text{tr}(A)\text{tr}(B)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transpose Invariance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\text{tr}(A^T) = \text{tr}(A)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similarity Invariance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If \(B = P^{-1}AP\), then \(\text{tr}(B) = \text{tr}(A)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This means the trace is a similarity invariant, depending only on the linear
    transformation, not the basis.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Trace and Eigenvalues
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One of the most important connections is between the trace and eigenvalues:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{tr}(A) = \lambda_1 + \lambda_2 + \cdots + \lambda_n, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\lambda_i\) are the eigenvalues of \(A\) (counting multiplicity).
  prefs: []
  type: TYPE_NORMAL
- en: This links the simple diagonal sum to the deep spectral properties of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 0 \\ 0 & 3 \end{bmatrix}, \quad \text{tr}(A) = 4,
    \quad \lambda_1 = 1, \; \lambda_2 = 3, \quad \lambda_1 + \lambda_2 = 4. \]
  prefs: []
  type: TYPE_NORMAL
- en: Other Basic Matrix Properties
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Alongside the trace, here are some important algebraic facts that every student
    of linear algebra must know:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determinant vs. Trace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For 2×2 matrices, \(A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}\), \(\text{tr}(A)
    = a + d\), \(\det(A) = ad - bc\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Together, trace and determinant encode the eigenvalues: roots of \(x^2 - \text{tr}(A)x
    + \det(A) = 0\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Norms and Inner Products:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Frobenius norm is defined using the trace: \(\|A\|_F = \sqrt{\text{tr}(A^TA)}\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Orthogonal Invariance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For any orthogonal matrix \(Q\), \(\text{tr}(Q^TAQ) = \text{tr}(A)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric and Practical Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The trace of a transformation can be seen as the sum of its action along the
    coordinate axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In physics, the trace of the stress tensor measures pressure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In probability, the trace of a covariance matrix is the total variance of a
    system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In statistics and machine learning, the trace is often used as a measure of
    overall “size” or complexity of a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The trace is deceptively simple but incredibly powerful:'
  prefs: []
  type: TYPE_NORMAL
- en: It connects directly to eigenvalues, forming a bridge between raw matrix entries
    and spectral theory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is invariant under similarity, making it a reliable measure of a transformation
    independent of basis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It shows up in optimization, physics, statistics, and quantum mechanics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It simplifies computations: many proofs in linear algebra reduce to trace properties.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the trace of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 4 & 2 & 0 \\ -1 & 3 & 5 \\ 7 & 6 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Verify that \(\text{tr}(AB) = \text{tr}(BA)\) for
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 \\ 0 & 3 \end{bmatrix}, \quad B = \begin{bmatrix}
    4 & 0 \\ 5 & 6 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: For the 2×2 matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: compute its eigenvalues and check that their sum equals the trace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge: Show that the total variance of a dataset with covariance matrix
    \(\Sigma\) is equal to \(\text{tr}(\Sigma)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mastering the trace and its properties will prepare you for the next leap:
    understanding how matrices interact with volume, orientation, and determinants.'
  prefs: []
  type: TYPE_NORMAL
- en: 19\. Affine Transforms and Homogeneous Coordinates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Up to now, matrices have been used to describe linear transformations: scaling,
    rotating, reflecting, projecting. But real-world geometry often involves more
    than just linear effects-it includes translations (shifts) as well. A pure linear
    map cannot move the origin, so to handle translations (and combinations of them
    with rotations, scalings, and shears), we extend our toolkit to affine transformations.
    The secret weapon that makes this work is the idea of homogeneous coordinates.'
  prefs: []
  type: TYPE_NORMAL
- en: What is an Affine Transformation?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An affine transformation is any map of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{x}) = A\mathbf{x} + \mathbf{b}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(A\) is a matrix (linear part) and \(\mathbf{b}\) is a vector (translation
    part).
  prefs: []
  type: TYPE_NORMAL
- en: \(A\) handles scaling, rotation, reflection, shear, or projection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbf{b}\) shifts everything by a constant amount.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples in 2D:'
  prefs: []
  type: TYPE_NORMAL
- en: Rotate by 90° and then shift right by 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stretch vertically by 3 and shift upward by 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Affine maps preserve parallel lines and ratios of distances along lines, but
    not necessarily angles or lengths.
  prefs: []
  type: TYPE_NORMAL
- en: Why Linear Maps Alone Aren’t Enough
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If we only use a 2×2 matrix in 2D or 3×3 in 3D, the origin always stays fixed.
    That’s a limitation: real-world movements (like moving a shape from one place
    to another) require shifting the origin too. To capture both linear and translational
    effects uniformly, we need a clever trick.'
  prefs: []
  type: TYPE_NORMAL
- en: Homogeneous Coordinates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The trick is to add one extra coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: In 2D, a point \((x, y)\) becomes \((x, y, 1)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 3D, a point \((x, y, z)\) becomes \((x, y, z, 1)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This new representation is called homogeneous coordinates. It allows us to fold
    translations into matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Affine Transform as a Matrix in Homogeneous Form
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In 2D:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} a & b & t_x \\ c & d & t_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix}
    x \\ y \\ 1 \end{bmatrix} = \begin{bmatrix} ax + by + t_x \\ cx + dy + t_y \\
    1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here,
  prefs: []
  type: TYPE_NORMAL
- en: The 2×2 block \(\begin{bmatrix} a & b \\ c & d \end{bmatrix}\) is the linear
    part.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last column \(\begin{bmatrix} t_x \\ t_y \end{bmatrix}\) is the translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So with one unified matrix, we can handle both linear transformations and shifts.
  prefs: []
  type: TYPE_NORMAL
- en: Examples in 2D
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Translation by (2, 3):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & 2 \\ 0 & 1 & 3 \\ 0 & 0 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Scaling by 2 in x and 3 in y, then shifting by (–1, 4):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 0 & -1 \\ 0 & 3 & 4 \\ 0 & 0 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Rotation by 90° and shift right by 5:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 0 & -1 & 5 \\ 1 & 0 & 0 \\ 0 & 0 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Homogeneous Coordinates in 3D
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 3D, affine transformations use 4×4 matrices. The upper-left 3×3 block handles
    rotation, scaling, or shear; the last column encodes translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: translation by (2, –1, 4):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & 0 & 2 \\ 0 & 1 & 0 & -1 \\ 0 & 0 & 1 & 4 \\ 0 & 0
    & 0 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This formulation is universal in computer graphics and robotics.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unified representation: Using homogeneous coordinates, we can treat translations
    as matrices, enabling consistent matrix multiplication for all transformations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Practicality: This approach underpins 3D graphics pipelines, animation, CAD,
    robotics, and computer vision.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Composability: Multiple affine transformations can be combined into a single
    homogeneous matrix by multiplying them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geometry preserved: Affine maps preserve straight lines and parallelism, essential
    in engineering and design.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write the homogeneous matrix that reflects across the x-axis and then shifts
    up by 3\. Apply it to \((2, 1)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a 4×4 homogeneous matrix that rotates around the z-axis by 90° and
    translates by (1, 2, 0).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that multiplying two 3×3 homogeneous matrices in 2D yields another valid
    affine transform.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that affine maps preserve parallel lines by applying a general
    affine matrix to two parallel lines and checking their slopes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mastering affine transformations and homogeneous coordinates bridges the gap
    between pure linear algebra and real-world geometry, giving you the mathematical
    foundation behind computer graphics, robotics, and spatial modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 20\. Computing with Matrices (Cost Counts and Simple Speedups)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thus far, we have studied what matrices are and what they represent. But in
    practice, working with matrices also means thinking about computation-how much
    work operations take, how algorithms can be sped up, and why structure matters.
    This section introduces the basic ideas of computational cost in matrix operations,
    simple strategies for efficiency, and why these considerations are crucial in
    modern applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Counting Operations: The Cost Model'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The simplest way to measure the cost of a matrix operation is to count the basic
    arithmetic operations (additions and multiplications).
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix–vector product: For an \(m \times n\) matrix and an \(n \times 1\) vector:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the \(m\) output entries requires \(n\) multiplications and \(n-1\)
    additions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total cost ≈ \(2mn\) operations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Matrix–matrix product: For an \(m \times n\) matrix times an \(n \times p\)
    matrix:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of the \(mp\) entries requires \(n\) multiplications and \(n-1\) additions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Total cost ≈ \(2mnp\) operations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gaussian elimination (solving \(Ax=b\)): For an \(n \times n\) system:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roughly \(\tfrac{2}{3}n^3\) operations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These counts show how quickly costs grow with dimension. Doubling \(n\) makes
    the work 8 times larger for elimination.
  prefs: []
  type: TYPE_NORMAL
- en: Why Cost Counts Matter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scalability: Small problems (2×2 or 3×3) are trivial, but modern datasets involve
    matrices with millions of rows. Knowing the cost is essential.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Feasibility: Some exact algorithms become impossible for very large matrices.
    Approximation methods are used instead.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimization: Engineers and scientists design specialized algorithms to reduce
    costs by exploiting structure (sparsity, symmetry, triangular form).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Simple Speedups with Structure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Diagonal Matrices: Multiplying by a diagonal matrix costs only \(n\) operations
    (scale each component).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Triangular Matrices: Solving triangular systems requires only \(\tfrac{1}{2}n^2\)
    operations (substitution), far cheaper than general elimination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparse Matrices: If most entries are zero, we skip multiplications by zero.
    For large sparse systems, cost scales with the number of nonzeros, not \(n^2\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Block Matrices: Breaking matrices into blocks allows algorithms to reuse optimized
    small-matrix routines (common in BLAS libraries).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory Considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Cost is not only arithmetic: storage also matters.'
  prefs: []
  type: TYPE_NORMAL
- en: A dense \(n \times n\) matrix requires \(n^2\) entries of memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse storage formats (like CSR, COO) record only nonzero entries and their
    positions, saving massive space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory access speed can dominate arithmetic cost in large computations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelism and Hardware
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Modern computing leverages hardware for speed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Vectorization (SIMD): Perform many multiplications at once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Parallelization: Split work across many CPU cores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GPUs: Specialize in massive parallel matrix–vector and matrix–matrix operations
    (critical in deep learning).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is why linear algebra libraries (BLAS, LAPACK, cuBLAS) are indispensable:
    they squeeze performance from hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Efficiency: Understanding cost lets us choose the right algorithm for the job.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Algorithm design: Structured matrices (diagonal, sparse, orthogonal) make computations
    much faster and more stable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Applications: Every field that uses matrices-graphics, optimization, statistics,
    AI-relies on efficient computation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Foundations: Later topics like LU/QR/SVD factorization are motivated by balancing
    cost and stability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the number of operations required for multiplying a 1000×500 matrix
    with a 500×200 matrix. Compare with multiplying a 1000×1000 dense matrix by a
    vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show how solving a 3×3 triangular system is faster than Gaussian elimination.
    Count the exact multiplications and additions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a sparse 5×5 matrix with only 7 nonzero entries. Estimate the cost
    of multiplying it by a vector versus a dense 5×5 matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Suppose you need to store a 1,000,000×1,000,000 dense matrix. Estimate
    how much memory (in bytes) it would take if each entry is 8 bytes. Could it fit
    on a laptop? Why do sparse formats save the day?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By learning to count costs and exploit structure, you prepare yourself not only
    to understand matrices abstractly but also to use them effectively in real-world,
    large-scale problems. This balance between theory and computation is at the heart
    of modern linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 3\. Linear Systems and Elimination
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 21\. From Equations to Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear algebra often begins with systems of equations-collections of unknowns
    linked by linear relationships. While these systems can be solved directly using
    substitution or elimination, they quickly become messy when there are many variables.
    The key insight of linear algebra is that all systems of linear equations can
    be captured compactly by matrices and vectors. This section explains how we move
    from equations written out in words and symbols to the matrix form that powers
    computation.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider this system of two equations in two unknowns:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} 2x + y = 5 \\ 3x - y = 4 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'At first glance, this is just algebra: two equations, two unknowns. But notice
    the structure: each equation is a sum of multiples of the variables, set equal
    to a constant. This pattern-linear combinations of unknowns equal to a result-is
    exactly what matrices capture.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing in Coefficient Table Form
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Extract the coefficients of each variable from the system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First equation: coefficients are \(2\) for \(x\), \(1\) for \(y\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second equation: coefficients are \(3\) for \(x\), \(-1\) for \(y\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arrange these coefficients in a rectangular array:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 3 & -1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This matrix \(A\) is called the coefficient matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, write the unknowns as a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x} = \begin{bmatrix} x \\ y \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, write the right-hand side constants as another vector:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{b} = \begin{bmatrix} 5 \\ 4 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the entire system can be written in a single line:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A\mathbf{x} = \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Why This is Powerful
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This compact form hides no information; it is equivalent to the original equations.
    But it gives us enormous advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clarity: We see the structure clearly-the system is “matrix times vector equals
    vector.”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scalability: Whether we have 2 equations or 2000, the same notation applies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Tools: All the machinery of matrix operations (elimination, inverses, decompositions)
    now becomes available.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geometry: The matrix equation \(A\mathbf{x} = \mathbf{b}\) means: combine the
    columns of \(A\) (scaled by entries of x) to land on b.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Larger Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System of three equations in three unknowns:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + 2y - z = 2 \\ 2x - y + 3z = 1 \\ 3x + y + 2z = 4 \end{cases}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Coefficient matrix:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 & -1 \\ 2 & -1 & 3 \\ 3 & 1 & 2 \end{bmatrix}.
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Unknown vector:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \mathbf{x} = \begin{bmatrix} x \\ y \\ z \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Constant vector:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \mathbf{b} = \begin{bmatrix} 2 \\ 1 \\ 4 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A\mathbf{x} = \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This single equation captures three equations and three unknowns in one object.
  prefs: []
  type: TYPE_NORMAL
- en: Row vs. Column View
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Row view: Each row of \(A\) dotted with x gives one equation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column view: The entire system means b is a linear combination of the columns
    of \(A\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the 2×2 case earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A\mathbf{x} = \begin{bmatrix} 2 & 1 \\ 3 & -1 \end{bmatrix} \begin{bmatrix}
    x \\ y \end{bmatrix} = x \begin{bmatrix} 2 \\ 3 \end{bmatrix} + y \begin{bmatrix}
    1 \\ -1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So solving the system means finding scalars \(x, y\) that combine the columns
    of \(A\) to reach \(\mathbf{b}\).
  prefs: []
  type: TYPE_NORMAL
- en: Augmented Matrix Form
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sometimes we want to save space further. We can put the coefficients and constants
    side by side in an augmented matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ [A | \mathbf{b}] = \begin{bmatrix} 2 & 1 & | & 5 \\ 3 & -1 & | & 4 \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: This form is especially useful for elimination methods, where we manipulate
    rows without writing variables at each step.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This step-rewriting equations as matrix form-is the gateway into linear algebra.
    Once you can do it, you no longer think of systems of equations as isolated lines
    on paper, but as a unified object that can be studied with general tools. It opens
    the door to:'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian elimination,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rank and null space,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: determinants,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: eigenvalues,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: optimization methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every major idea flows from this compact representation.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write the system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{cases} 4x - y = 7 \\ -2x + 3y = 5 \end{cases} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: in matrix form.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + y + z = 6 \\ 2x - y + z = 3 \\ x - y - z = -2 \end{cases}
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: build the coefficient matrix, unknown vector, and constant vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Express the augmented matrix for the above system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Interpret the system in column view. What does it mean geometrically
    to express \((6, 3, -2)\) as a linear combination of the columns of the coefficient
    matrix?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By practicing these rewrites, you will see that linear algebra is not about
    juggling many equations-it is about seeing structure in one compact equation.
    This step transforms scattered equations into the language of matrices, where
    the real power begins.
  prefs: []
  type: TYPE_NORMAL
- en: 22\. Row Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once a system of linear equations has been expressed as a matrix, the next step
    is to simplify that matrix into a form where the solutions become clear. The main
    tool for this simplification is the set of elementary row operations. These operations
    allow us to manipulate the rows of a matrix in systematic ways that preserve the
    solution set of the corresponding system of equations.
  prefs: []
  type: TYPE_NORMAL
- en: The Three Types of Row Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are exactly three types of legal row operations, each with a clear algebraic
    meaning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Row Swapping (\(R_i \leftrightarrow R_j\)): Exchange two rows. This corresponds
    to reordering equations in a system. Since the order of equations doesn’t change
    the solutions, this operation is always valid.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 1 & | & 5 \\ 3 & -1 & | & 4 \end{bmatrix} \quad \longrightarrow
    \quad \begin{bmatrix} 3 & -1 & | & 4 \\ 2 & 1 & | & 5 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Row Scaling (\(R_i \to cR_i, \; c \neq 0\)): Multiply all entries in a row
    by a nonzero constant. This is like multiplying both sides of an equation by the
    same number, which doesn’t change its truth.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 1 & | & 5 \\ 3 & -1 & | & 4 \end{bmatrix} \quad \longrightarrow
    \quad \begin{bmatrix} 1 & \tfrac{1}{2} & | & \tfrac{5}{2} \\ 3 & -1 & | & 4 \end{bmatrix}.
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Row Replacement (\(R_i \to R_i + cR_j\)): Add a multiple of one row to another.
    This corresponds to replacing one equation with a linear combination of itself
    and another, a fundamental elimination step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 1 & | & 5 \\ 3 & -1 & | & 4 \end{bmatrix} \quad \overset{R_2
    \to R_2 - \tfrac{3}{2}R_1}{\longrightarrow} \quad \begin{bmatrix} 2 & 1 & | &
    5 \\ 0 & -\tfrac{5}{2} & | & -\tfrac{7}{2} \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why These Are the Only Allowed Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'These three operations are the backbone of elimination because they do not
    alter the solution set of the system. Each is equivalent to applying an invertible
    transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: Row swaps are reversible (swap back).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Row scalings by \(c\) can be undone by scaling by \(1/c\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Row replacements can be undone by adding the opposite multiple.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, each operation is invertible, and the transformed system is always equivalent
    to the original.
  prefs: []
  type: TYPE_NORMAL
- en: Row Operations as Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each elementary row operation can itself be represented by multiplying on the
    left with a special matrix called an elementary matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Swapping rows 1 and 2 in a 2×2 system is done by
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ E = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scaling row 1 by 3 in a 2×2 system is done by
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ E = \begin{bmatrix} 3 & 0 \\ 0 & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This perspective is crucial later for factorization methods like LU decomposition,
    where elimination is expressed as a product of elementary matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-Step Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + 2y = 4 \\ 3x + 4y = 10 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & | & 4 \\ 3 & 4 & | & 10 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eliminate the \(3x\) under the first pivot: \(R_2 \to R_2 - 3R_1\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & | & 4 \\ 0 & -2 & | & -2 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scale the second row: \(R_2 \to -\tfrac{1}{2}R_2\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & | & 4 \\ 0 & 1 & | & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Eliminate above the pivot: \(R_1 \to R_1 - 2R_2\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & | & 2 \\ 0 & 1 & | & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Solution: \(x = 2, \; y = 1\).'
  prefs: []
  type: TYPE_NORMAL
- en: Geometry of Row Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Row operations do not alter the solution space:'
  prefs: []
  type: TYPE_NORMAL
- en: Swapping rows reorders equations but keeps the same lines or planes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling rows rescales equations but leaves their geometric set unchanged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding rows corresponds to combining constraints, but the shared intersection
    (solution set) is preserved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, row operations act like “reshaping the system” while leaving the intersection
    intact.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Row operations are the essential moves in solving linear systems by hand or
    computer. They:'
  prefs: []
  type: TYPE_NORMAL
- en: Make elimination systematic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preserve solution sets while simplifying structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lay the groundwork for echelon forms, rank, and factorization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide the mechanical steps that computers automate in Gaussian elimination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Apply row operations to reduce
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 1 & | & 7 \\ 1 & -1 & | & 1 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: to a form where the solution is obvious.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Show explicitly why swapping two equations in a system doesn’t change its solutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct the elementary matrix for “add –2 times row 1 to row 3” in a 3×3 system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that any elementary row operation corresponds to multiplication
    by an invertible matrix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mastering these operations equips you with the mechanical and conceptual foundation
    for the next stage: systematically reducing matrices to row-echelon form.'
  prefs: []
  type: TYPE_NORMAL
- en: 23\. Row-Echelon and Reduced Row-Echelon Forms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After introducing row operations, the natural question is: *what are we trying
    to achieve by performing them?* The answer is to transform a matrix into a standardized,
    simplified form where the solutions to the corresponding system of equations can
    be read off directly. Two such standardized forms are central in linear algebra:
    row-echelon form (REF) and reduced row-echelon form (RREF).'
  prefs: []
  type: TYPE_NORMAL
- en: Row-Echelon Form (REF)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A matrix is in row-echelon form if:'
  prefs: []
  type: TYPE_NORMAL
- en: All nonzero rows are above any rows of all zeros.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In each nonzero row, the first nonzero entry (called the leading entry or pivot)
    is to the right of the leading entry of the row above it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All entries below a pivot are zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example of REF:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 3 & | & 4 \\ 0 & 1 & -1 & | & 2 \\ 0 & 0 & 5 & |
    & -3 \\ 0 & 0 & 0 & | & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here, the pivots are the first 1 in row 1, the 1 in row 2, and the 5 in row
    3\. Each pivot is to the right of the one above it, and all entries below pivots
    are zero.
  prefs: []
  type: TYPE_NORMAL
- en: Reduced Row-Echelon Form (RREF)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A matrix is in reduced row-echelon form if, in addition to the rules of REF:'
  prefs: []
  type: TYPE_NORMAL
- en: Each pivot is equal to 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each pivot is the only nonzero entry in its column (everything above and below
    pivots is zero).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example of RREF:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & 0 & | & 3 \\ 0 & 1 & 0 & | & -2 \\ 0 & 0 & 1 & |
    & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This form is so simplified that solutions can be read directly: here, \(x=3\),
    \(y=-2\), \(z=1\).'
  prefs: []
  type: TYPE_NORMAL
- en: Relationship Between REF and RREF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: REF is easier to reach-it only requires eliminating entries below pivots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RREF requires going further-clearing entries above pivots and scaling pivots
    to 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Every matrix can be reduced to REF (many possible versions), but RREF is unique:
    no matter how you proceed, if you carry out all row operations fully, you end
    with the same RREF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Step-by-Step to RREF'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + 2y + z = 4 \\ 2x + 5y + z = 7 \\ 3x + 6y + 2z = 10 \end{cases}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 2 & 5 & 1 & | & 7 \\ 3 & 6 & 2 & | &
    10 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eliminate below first pivot (the 1 in row 1, col 1):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(R_2 \to R_2 - 2R_1\)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(R_3 \to R_3 - 3R_1\)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 0 & 1 & -1 & | & -1 \\ 0 & 0 & -1 &
    | & -2 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is now in REF.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scale pivots and eliminate above them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(R_3 \to -R_3\) to make pivot 1.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(R_2 \to R_2 + R_3\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(R_1 \to R_1 - R_2 - R_3\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Final:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & 0 & | & 2 \\ 0 & 1 & 0 & | & 1 \\ 0 & 0 & 1 & | &
    2 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Solution: \(x=2, y=1, z=2\).'
  prefs: []
  type: TYPE_NORMAL
- en: Geometry of REF and RREF
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: REF corresponds to simplifying the system step by step, making it “triangular”
    so variables can be solved one after another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RREF corresponds to a system that is fully disentangled-each variable isolated,
    with its value or free-variable relationship explicitly visible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: REF is the foundation of Gaussian elimination, the workhorse algorithm for solving
    systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RREF gives complete clarity: unique representation of solution sets, revealing
    free and pivot variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RREF underlies algorithms in computer algebra systems, symbolic solvers, and
    educational tools.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding these forms builds intuition for rank, null space, and solution
    structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reduce
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 4 & | & 6 \\ 1 & 3 & | & 5 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: to REF, then RREF.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Find the RREF of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & 1 & | & 3 \\ 2 & 3 & 4 & | & 8 \\ 1 & 2 & 3 & | &
    5 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Explain why two different elimination sequences can lead to different REF but
    the same RREF.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that every matrix has a unique RREF by considering the effect
    of row operations systematically.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reaching row-echelon and reduced row-echelon forms transforms messy systems
    into structured ones, turning algebraic clutter into an organized path to solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 24\. Pivots, Free Variables, and Leading Ones
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When reducing a matrix to row-echelon or reduced row-echelon form, certain positions
    in the matrix take on a special importance. These are the pivots-the leading nonzero
    entries in each row. Around them, the entire solution structure of a linear system
    is organized. Understanding pivots, the variables they anchor, and the freedom
    that arises from non-pivot columns is essential to solving linear equations systematically.
  prefs: []
  type: TYPE_NORMAL
- en: What is a Pivot?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In row-echelon form, a pivot is the first nonzero entry in a row, moving from
    left to right. After scaling in reduced row-echelon form, each pivot is set to
    exactly 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 0 & | & 5 \\ 0 & 1 & 3 & | & -2 \\ 0 & 0 & 0 & |
    & 0 \end{bmatrix} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Pivot in row 1: the 1 in column 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pivot in row 2: the 1 in column 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Column 3 has no pivot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columns with pivots are pivot columns. Columns without pivots correspond to
    free variables.
  prefs: []
  type: TYPE_NORMAL
- en: Pivot Variables vs. Free Variables
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Pivot variables: Variables that align with pivot columns. They are determined
    by the equations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Free variables: Variables that align with non-pivot columns. They are unconstrained
    and can take arbitrary values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & 2 & | & 3 \\ 0 & 1 & -1 & | & 4 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This corresponds to:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_1 + 2x_3 = 3, \quad x_2 - x_3 = 4. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: \(x_1\) and \(x_2\) are pivot variables (from pivot columns 1 and 2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(x_3\) is a free variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, \(x_1\) and \(x_2\) depend on \(x_3\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_1 = 3 - 2x_3, \quad x_2 = 4 + x_3, \quad x_3 \text{ free}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The solution set is infinite, described by the freedom in \(x_3\).
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Pivot variables represent coordinates that are “pinned down.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Free variables correspond to directions along which the solution can extend
    infinitely.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 2D:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If there is one pivot variable and one free variable, solutions form a line.
    In 3D:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two pivots, one free → solutions form a line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One pivot, two free → solutions form a plane.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, the number of free variables determines the dimension of the solution
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Rank and Free Variables
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The number of pivot columns equals the rank of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the coefficient matrix \(A\) is \(m \times n\):'
  prefs: []
  type: TYPE_NORMAL
- en: Rank = number of pivots.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of free variables = \(n - \text{rank}(A)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the rank–nullity connection in action:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{number of variables} = \text{rank} + \text{nullity}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-Step Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + 2y + z = 4 \\ 2x + 5y + z = 7 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 2 & 5 & 1 & | & 7 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Reduce:'
  prefs: []
  type: TYPE_NORMAL
- en: \(R_2 \to R_2 - 2R_1\) →
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 0 & 1 & -1 & | & -1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Pivot columns: 1 and 2 → variables \(x, y\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Free column: 3 → variable \(z\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x = 4 - 2y - z, \quad y = -1 + z, \quad z \text{ free}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Substitute:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (x, y, z) = (6 - 3z, \; -1 + z, \; z). \]
  prefs: []
  type: TYPE_NORMAL
- en: Solutions form a line in 3D parameterized by \(z\).
  prefs: []
  type: TYPE_NORMAL
- en: Why Leading Ones Matter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In RREF, each pivot is scaled to 1, making it easy to isolate pivot variables.
    Without leading ones, equations may still be correct but harder to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 0 & | & 6 \\ 0 & -3 & | & 9 \end{bmatrix} \]
  prefs: []
  type: TYPE_NORMAL
- en: becomes
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & | & 3 \\ 0 & 1 & | & -3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The solutions are immediately visible: \(x=3, y=-3\).'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Identifying pivots shows which variables are determined and which are free.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The number of pivots defines rank, a central concept in linear algebra.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Free variables determine whether the system has a unique solution, infinitely
    many, or none.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leading ones in RREF give immediate transparency to the solution set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reduce
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 3 & 1 & | & 5 \\ 2 & 6 & 2 & | & 10 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: and identify pivot and free variables.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x + y + z = 2, \quad 2x + 3y + 5z = 7, \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: write the RREF and express the solution with free variables.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the rank and number of free variables of a 3×5 matrix with two pivot
    columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that if the number of pivots equals the number of variables,
    the system has either no solution or a unique solution, but never infinitely many.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Understanding pivots and free variables provides the key to classifying solution
    sets: unique, infinite, or none. This classification lies at the heart of solving
    linear systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 25\. Solving Consistent Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A system of linear equations is called consistent if it has at least one solution.
    Consistency is the first property to check when working with a system, because
    before worrying about uniqueness or parametrization, we must know whether a solution
    exists at all. This section explains how to recognize consistent systems, how
    to solve them using row-reduction, and how to describe their solutions in terms
    of pivots and free variables.
  prefs: []
  type: TYPE_NORMAL
- en: What Consistency Means
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given a system \(A\mathbf{x} = \mathbf{b}\):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consistent: At least one solution \(\mathbf{x}\) satisfies the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inconsistent: No solution exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consistency depends on the relationship between the vector \(\mathbf{b}\) and
    the column space of \(A\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{b} \in \text{Col}(A) \quad \iff \quad \text{system is consistent}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: If \(\mathbf{b}\) cannot be written as a linear combination of the columns of
    \(A\), the system has no solution.
  prefs: []
  type: TYPE_NORMAL
- en: Checking Consistency with Row Reduction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To test consistency, reduce the augmented matrix \([A | \mathbf{b}]\) to row-echelon
    form.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you find a row of the form:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ [0 \;\; 0 \;\; \dots \;\; 0 \;|\; c], \quad c \neq 0, \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'then the system is inconsistent (contradiction: 0 = c).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If no such contradiction appears, the system is consistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 1: Consistent System with Unique Solution'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + y = 2 \\ x - y = 0 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & | & 2 \\ 1 & -1 & | & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Row reduce:'
  prefs: []
  type: TYPE_NORMAL
- en: '\(R_2 \to R_2 - R_1\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & | & 2 \\ 0 & -2 & | & -2 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '\(R_2 \to -\tfrac{1}{2}R_2\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & | & 2 \\ 0 & 1 & | & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '\(R_1 \to R_1 - R_2\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & | & 1 \\ 0 & 1 & | & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Solution: \(x = 1, \; y = 1\). Unique solution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: Consistent System with Infinitely Many Solutions'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + y + z = 3 \\ 2x + 2y + 2z = 6 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & 1 & | & 3 \\ 2 & 2 & 2 & | & 6 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Row reduce:'
  prefs: []
  type: TYPE_NORMAL
- en: '\(R_2 \to R_2 - 2R_1\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & 1 & | & 3 \\ 0 & 0 & 0 & | & 0 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'No contradiction, so consistent. Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x = 3 - y - z, \quad y \text{ free}, \quad z \text{ free}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The solution set is a plane in \(\mathbb{R}^3\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 3: Inconsistent System (for contrast)'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + y = 1 \\ x + y = 2 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & | & 1 \\ 1 & 1 & | & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Row reduce:'
  prefs: []
  type: TYPE_NORMAL
- en: '\(R_2 \to R_2 - R_1\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & | & 1 \\ 0 & 0 & | & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Contradiction: \(0 = 1\). Inconsistent, no solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation of Consistency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In 2D:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two lines intersect at a point → consistent, unique solution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Two lines overlap → consistent, infinitely many solutions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Two lines are parallel and distinct → inconsistent, no solution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 3D:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three planes intersect at a point → unique solution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Planes intersect along a line or coincide → infinitely many solutions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Planes fail to meet (like a triangular “gap”) → no solution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Pivot Structure and Solutions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Unique solution: Every variable is a pivot variable (no free variables).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Infinitely many solutions: At least one free variable exists, but no contradiction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'No solution: Contradictory row appears in augmented matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consistency is the first checkpoint in solving systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The classification into unique, infinite, or none underpins all of linear algebra.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding consistency ties algebra (row operations) to geometry (intersections
    of lines, planes, hyperplanes).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These ideas scale: in data science and engineering, checking whether equations
    are consistent is equivalent to asking if a model fits observed data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reduce the augmented matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 1 & | & 5 \\ 2 & 4 & 2 & | & 10 \\ 3 & 6 & 3 & |
    & 15 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: and determine if the system is consistent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Classify the system as having unique, infinite, or no solutions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + y + z = 2 \\ x - y + z = 0 \\ 2x + 0y + 2z = 3 \end{cases}
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Explain geometrically what it means when the augmented matrix has a contradictory
    row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show algebraically that a system is consistent if and only if \(\mathbf{b}\)
    lies in the span of the columns of \(A\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Consistent systems mark the balance point between algebraic rules and geometric
    reality: they are where equations and space meet in harmony.'
  prefs: []
  type: TYPE_NORMAL
- en: 26\. Detecting Inconsistency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Not every system of linear equations has a solution. Some are inconsistent,
    meaning the equations contradict one another and no vector \(\mathbf{x}\) can
    satisfy them all at once. Detecting such inconsistency early is crucial: it saves
    wasted effort trying to solve an impossible system and reveals important geometric
    and algebraic properties.'
  prefs: []
  type: TYPE_NORMAL
- en: What Inconsistency Looks Like Algebraically
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider the system:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + y = 1 \\ x + y = 3 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, the two equations cannot both be true. In augmented matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & | & 1 \\ 1 & 1 & | & 3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Row reduction gives:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & | & 1 \\ 0 & 0 & | & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The bottom row says \(0 = 2\), a contradiction. This is the hallmark of inconsistency:
    a row of zeros in the coefficient part, with a nonzero constant in the augmented
    part.'
  prefs: []
  type: TYPE_NORMAL
- en: General Rule for Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A system \(A\mathbf{x} = \mathbf{b}\) is inconsistent if, after row reduction,
    the augmented matrix contains a row of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ [0 \;\; 0 \;\; \dots \;\; 0 \;|\; c], \quad c \neq 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: This indicates that all variables vanish from the equation, leaving an impossible
    statement like \(0 = c\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: Parallel Lines in 2D'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + y = 2 \\ 2x + 2y = 5 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & | & 2 \\ 2 & 2 & | & 5 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Row reduce:'
  prefs: []
  type: TYPE_NORMAL
- en: '\(R_2 \to R_2 - 2R_1\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & | & 2 \\ 0 & 0 & | & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Contradiction: no solution. Geometrically, the two equations are parallel lines
    that never intersect.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: Contradictory Planes in 3D'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + y + z = 1 \\ 2x + 2y + 2z = 2 \\ x + y + z = 3 \end{cases}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The first and third equations already conflict: the same plane equation is
    forced to equal two different constants.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented matrix reduces to:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & 1 & | & 1 \\ 0 & 0 & 0 & | & 0 \\ 0 & 0 & 0 & | &
    2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Contradiction: no solution. The “planes” fail to intersect in common.'
  prefs: []
  type: TYPE_NORMAL
- en: Geometry of Inconsistency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In 2D: Inconsistent systems correspond to parallel lines with different intercepts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 3D: They correspond to planes that are parallel but offset, or planes arranged
    in a way that leaves a “gap” (no shared intersection).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In higher dimensions: Inconsistency means the target vector \(\mathbf{b}\)
    lies outside the column space of \(A\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank Test for Consistency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another way to detect inconsistency is using ranks.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\text{rank}(A)\) be the number of pivots in the coefficient matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let \(\text{rank}([A|\mathbf{b}])\) be the number of pivots in the augmented
    matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rule:'
  prefs: []
  type: TYPE_NORMAL
- en: If \(\text{rank}(A) = \text{rank}([A|\mathbf{b}])\), the system is consistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\text{rank}(A) < \text{rank}([A|\mathbf{b}])\), the system is inconsistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This rank condition is fundamental and works in any dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inconsistency reveals overdetermined or contradictory data in real problems
    (physics, engineering, statistics).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The ability to detect inconsistency quickly through row reduction or rank saves
    computation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It connects geometry (non-intersecting spaces) with algebra (contradictory rows).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It prepares the way for least-squares methods, where inconsistent systems are
    approximated instead of solved exactly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reduce the augmented matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & -1 & | & 2 \\ 2 & -2 & | & 5 \end{bmatrix} \]
  prefs: []
  type: TYPE_NORMAL
- en: and decide if the system is consistent.
  prefs: []
  type: TYPE_NORMAL
- en: Show geometrically why the system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x + y = 0, \quad x + y = 1 \]
  prefs: []
  type: TYPE_NORMAL
- en: is inconsistent.
  prefs: []
  type: TYPE_NORMAL
- en: Use the rank test to check consistency of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + y + z = 2 \\ 2x + 2y + 2z = 4 \\ 3x + 3y + 3z = 5 \end{cases}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge: Explain why \(\text{rank}(A) < \text{rank}([A|\mathbf{b}])\) implies
    inconsistency, using the concept of the column space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detecting inconsistency is not just about spotting contradictions-it connects
    algebra, geometry, and linear transformations, showing exactly when a system cannot
    possibly fit together.
  prefs: []
  type: TYPE_NORMAL
- en: 27\. Gaussian Elimination by Hand
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gaussian elimination is the systematic procedure for solving systems of linear
    equations by using row operations to simplify the augmented matrix. The goal is
    to transform the system into row-echelon form (REF) and then use back substitution
    to find the solutions. This method is the backbone of linear algebra computations
    and is the foundation of most computer algorithms for solving linear systems.
  prefs: []
  type: TYPE_NORMAL
- en: The Big Idea
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Represent the system as an augmented matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use row operations to eliminate variables step by step, moving left to right,
    top to bottom.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop when the matrix is in REF.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solve the triangular system by back substitution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step-by-Step Recipe
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we have \(n\) equations with \(n\) unknowns.
  prefs: []
  type: TYPE_NORMAL
- en: Choose a pivot in the first column (a nonzero entry). If needed, swap rows to
    bring a nonzero entry to the top.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eliminate below the pivot by subtracting multiples of the pivot row from lower
    rows so that all entries below the pivot become zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move to the next row and next column, pick the next pivot, and repeat elimination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue until all pivots are in stair-step form (REF).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use back substitution to solve for the unknowns starting from the bottom row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example 1: A 2×2 System'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + 2y = 5 \\ 3x + 4y = 11 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & | & 5 \\ 3 & 4 & | & 11 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Pivot at (1,1) = 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Eliminate below: \(R_2 \to R_2 - 3R_1\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & | & 5 \\ 0 & -2 & | & -4 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Back substitution: From row 2: \(-2y = -4 \implies y = 2\). Substitute into
    row 1: \(x + 2(2) = 5 \implies x = 1\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Solution: \((x, y) = (1, 2)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: A 3×3 System'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + y + z = 6 \\ 2x + 3y + z = 14 \\ x - y + 2z = 2 \end{cases}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Augmented matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & 1 & | & 6 \\ 2 & 3 & 1 & | & 14 \\ 1 & -1 & 2 & |
    & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Pivot at (1,1). Eliminate below:'
  prefs: []
  type: TYPE_NORMAL
- en: \(R_2 \to R_2 - 2R_1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(R_3 \to R_3 - R_1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & 1 & | & 6 \\ 0 & 1 & -1 & | & 2 \\ 0 & -2 & 1 & |
    & -4 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Pivot at (2,2). Eliminate below: \(R_3 \to R_3 + 2R_2\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & 1 & | & 6 \\ 0 & 1 & -1 & | & 2 \\ 0 & 0 & -1 & |
    & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Pivot at (3,3). Scale row: \(R_3 \to -R_3\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 1 & 1 & | & 6 \\ 0 & 1 & -1 & | & 2 \\ 0 & 0 & 1 & |
    & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Back substitution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From row 3: \(z = 0\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From row 2: \(y - z = 2 \implies y = 2\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From row 1: \(x + y + z = 6 \implies x = 4\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solution: \((x, y, z) = (4, 2, 0)\).'
  prefs: []
  type: TYPE_NORMAL
- en: Why Gaussian Elimination Always Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each step reduces the number of variables in the lower equations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pivoting ensures stability (swap rows to avoid dividing by zero).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm either produces a triangular system (solvable by substitution)
    or reveals inconsistency (contradictory row).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Elimination corresponds to progressively restricting the solution set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First equation → a plane in \(\mathbb{R}^3\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Add second equation → intersection becomes a line.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Add third equation → intersection becomes a point (unique solution) or vanishes
    (inconsistent).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gaussian elimination is the foundation for solving systems by hand and by computer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It reveals whether a system is consistent and if solutions are unique or infinite.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is the starting point for advanced methods like LU decomposition, QR factorization,
    and numerical solvers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It shows the interplay between algebra (row operations) and geometry (intersections
    of subspaces).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Solve the system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{cases} 2x + y = 7 \\ 4x + 3y = 15 \end{cases} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: using Gaussian elimination.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Reduce
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & -1 & | & 3 \\ 3 & 8 & 1 & | & 12 \\ 2 & 6 & 3 & |
    & 11 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: to REF and solve.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Practice with a system that has infinitely many solutions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x + y + z = 4, \quad 2x + 2y + 2z = 8. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Challenge: Explain why Gaussian elimination always terminates in at most \(n\)
    pivot steps for an \(n \times n\) system.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Gaussian elimination transforms the complexity of many equations into an orderly
    process, making the hidden structure of solutions visible step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 28\. Back Substitution and Solution Sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once Gaussian elimination reduces a system to row-echelon form (REF), the next
    step is to actually solve for the unknowns. This process is called back substitution:
    we begin with the bottom equation (which involves the fewest variables) and work
    our way upward, solving step by step. Back substitution is what converts the structured
    triangular system into explicit solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: The Structure of Row-Echelon Form
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A system in REF looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} - & * & * & * & | & * \\ 0 & * & * & * & | & * \\ 0 & 0 &
    * & * & | & * \\ 0 & 0 & 0 & * & | & * \end{bmatrix} \]
  prefs: []
  type: TYPE_NORMAL
- en: Each row corresponds to an equation with fewer variables than the row above.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bottom equation has only one or two variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This triangular form makes it possible to solve “from the bottom up.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step-by-Step Example: Unique Solution'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System after elimination:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & -1 & | & 3 \\ 0 & 1 & 2 & | & 4 \\ 0 & 0 & 1 & |
    & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This corresponds to:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + 2y - z = 3 \\ y + 2z = 4 \\ z = 2 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'From the last equation: \(z = 2\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Substitute into the second: \(y + 2(2) = 4 \implies y = 0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Substitute into the first: \(x + 2(0) - 2 = 3 \implies x = 5\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Solution: \((x, y, z) = (5, 0, 2)\).'
  prefs: []
  type: TYPE_NORMAL
- en: Infinite Solutions with Free Variables
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Not all systems reduce to unique solutions. If there are free variables (non-pivot
    columns), back substitution expresses pivot variables in terms of free ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 0 & 1 & -1 & | & 1 \\ 0 & 0 & 0 & |
    & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Equations:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} x + 2y + z = 4 \\ y - z = 1 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'From row 2: \(y = 1 + z\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From row 1: \(x + 2(1 + z) + z = 4 \implies x = 2 - 3z\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Solution set:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (x, y, z) = (2 - 3t, \; 1 + t, \; t), \quad t \in \mathbb{R}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here \(z = t\) is the free variable. The solutions form a line in 3D.
  prefs: []
  type: TYPE_NORMAL
- en: General Solution Structure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a consistent system:'
  prefs: []
  type: TYPE_NORMAL
- en: Unique solution → every variable is a pivot variable (no free variables).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Infinitely many solutions → some free variables remain. The solution set is
    parametrized by these variables and forms a line, plane, or higher-dimensional
    subspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No solution → contradiction discovered earlier, so back substitution is impossible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unique solution → a single intersection point of lines/planes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Infinite solutions → overlapping subspaces (e.g., two planes intersecting in
    a line).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Back substitution describes the exact shape of this intersection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Parametric Vector Form'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the infinite-solution example above:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (x, y, z) = (2, 1, 0) + t(-3, 1, 1). \]
  prefs: []
  type: TYPE_NORMAL
- en: This expresses the solution set as a base point plus a direction vector, making
    the geometry clear.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Back substitution turns row-echelon form into concrete answers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It distinguishes unique vs. infinite solutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It provides a systematic method usable by hand for small systems and forms the
    basis of computer algorithms for large ones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It reveals the structure of solution sets-whether a point, line, plane, or higher-dimensional
    object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Solve by back substitution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & -1 & 2 & | & 3 \\ 0 & 1 & 3 & | & 5 \\ 0 & 0 & 1 & |
    & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Reduce and solve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x + y + z = 2, \quad 2x + 2y + 2z = 4. \]
  prefs: []
  type: TYPE_NORMAL
- en: Express the solution set of the above system in parametric vector form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: For a 4×4 system with two free variables, explain why the solution
    set forms a plane in \(\mathbb{R}^4\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Back substitution completes the elimination process, translating triangular
    structure into explicit solutions, and shows how algebra and geometry meet in
    the classification of solution sets.
  prefs: []
  type: TYPE_NORMAL
- en: 29\. Rank and Its First Meaning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The concept of rank lies at the heart of linear algebra. It connects the algebra
    of solving systems, the geometry of subspaces, and the structure of matrices into
    one unifying idea. Rank measures the amount of independent information in a matrix:
    how many rows or columns carry unique directions instead of being repetitions
    or combinations of others.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition of Rank
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The rank of a matrix \(A\) is the number of pivots in its row-echelon form.
    Equivalently, it is:'
  prefs: []
  type: TYPE_NORMAL
- en: The dimension of the column space (number of independent columns).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dimension of the row space (number of independent rows).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these definitions agree.
  prefs: []
  type: TYPE_NORMAL
- en: 'First Encounter with Rank: Pivot Counting'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When solving a system with Gaussian elimination:'
  prefs: []
  type: TYPE_NORMAL
- en: Every pivot corresponds to one determined variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of pivots = the rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of free variables = total variables – rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 1 & | & 4 \\ 0 & 1 & -1 & | & 2 \\ 0 & 0 & 0 & |
    & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, there are 2 pivots. So:'
  prefs: []
  type: TYPE_NORMAL
- en: Rank = 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With 3 variables total, there is 1 free variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank in Terms of Independence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A set of vectors is linearly independent if none can be expressed as a combination
    of the others.
  prefs: []
  type: TYPE_NORMAL
- en: The rank of a matrix tells us how many independent rows or columns it has.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If some columns are combinations of others, they do not increase the rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \\ 3 & 6 & 9 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here, each row is a multiple of the first. Rank = 1, since only one independent
    row/column direction exists.
  prefs: []
  type: TYPE_NORMAL
- en: Rank and Solutions of Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider \(A\mathbf{x} = \mathbf{b}\).
  prefs: []
  type: TYPE_NORMAL
- en: If \(\text{rank}(A) = \text{rank}([A|\mathbf{b}])\), the system is consistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If not, inconsistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If rank = number of variables, the system has a unique solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If rank < number of variables, there are infinitely many solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, rank classifies solution sets.
  prefs: []
  type: TYPE_NORMAL
- en: Rank and Geometry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rank tells us the dimension of the subspace spanned by rows or columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rank 1: all information lies along a line.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rank 2: lies in a plane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rank 3: fills 3D space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^3\), a matrix of rank 2 has columns spanning a plane through
    the origin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A matrix of rank 1 has all columns on a single line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank and Row vs. Column View
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is a remarkable fact that the number of independent rows = number of independent
    columns. This is not obvious at first glance, but it is always true. So we can
    define rank either by rows or by columns-it makes no difference.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rank is the bridge between algebra and geometry: pivots ↔︎ dimension.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It classifies solutions to systems of equations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It measures redundancy in data (important in statistics, machine learning, signal
    processing).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It prepares the way for advanced concepts like nullity, rank–nullity theorem,
    and singular value decomposition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Find the rank of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 5 \\ 3 & 6 & 8 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Solve the system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x + y + z = 2, \quad 2x + 2y + 2z = 4, \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: and identify the rank of the coefficient matrix.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^3\), what is the geometric meaning of a 3×3 matrix of rank 2?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that the row rank always equals the column rank by considering
    the echelon form of the matrix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rank is the first truly unifying concept in linear algebra: it tells us how
    much independent structure a matrix contains and sets the stage for understanding
    spaces, dimensions, and transformations.'
  prefs: []
  type: TYPE_NORMAL
- en: 30\. LU Factorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Gaussian elimination not only solves systems but also reveals a deeper structure:
    many matrices can be factored into simpler pieces. One of the most useful is the
    LU factorization, where a matrix \(A\) is written as the product of a lower-triangular
    matrix \(L\) and an upper-triangular matrix \(U\). This factorization captures
    all the elimination steps in a compact form and allows systems to be solved efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: What is LU Factorization?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If \(A\) is an \(n \times n\) matrix, then
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = LU, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(L\) is lower-triangular (entries below diagonal may be nonzero, diagonal entries
    = 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(U\) is upper-triangular (entries above diagonal may be nonzero).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This means:'
  prefs: []
  type: TYPE_NORMAL
- en: \(U\) stores the result of elimination (the triangular system).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(L\) records the multipliers used during elimination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: 2×2 Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 3 \\ 4 & 7 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Elimination: \(R_2 \to R_2 - 2R_1\).'
  prefs: []
  type: TYPE_NORMAL
- en: Multiplier = 2 (used to eliminate entry 4).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Resulting \(U\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ U = \begin{bmatrix} 2 & 3 \\ 0 & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '\(L\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ L = \begin{bmatrix} 1 & 0 \\ 2 & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ LU = \begin{bmatrix} 1 & 0 \\ 2 & 1 \end{bmatrix} \begin{bmatrix} 2 & 3 \\
    0 & 1 \end{bmatrix} = \begin{bmatrix} 2 & 3 \\ 4 & 7 \end{bmatrix} = A. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: 3×3 Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 & 1 \\ 4 & -6 & 0 \\ -2 & 7 & 2 \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Eliminate below pivot (row 1).'
  prefs: []
  type: TYPE_NORMAL
- en: Multiplier \(m_{21} = 4/2 = 2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiplier \(m_{31} = -2/2 = -1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Eliminate below pivot in column 2.'
  prefs: []
  type: TYPE_NORMAL
- en: After substitutions, multipliers and pivots are collected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Result:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L = \begin{bmatrix} 1 & 0 & 0 \\ 2 & 1 & 0 \\ -1 & -1 & 1 \end{bmatrix},
    \quad U = \begin{bmatrix} 2 & 1 & 1 \\ 0 & -8 & -2 \\ 0 & 0 & 1 \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus \(A = LU\).
  prefs: []
  type: TYPE_NORMAL
- en: Solving Systems with LU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose \(Ax = b\). If \(A = LU\):'
  prefs: []
  type: TYPE_NORMAL
- en: Solve \(Ly = b\) by forward substitution (since \(L\) is lower-triangular).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solve \(Ux = y\) by back substitution (since \(U\) is upper-triangular).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This two-step process is much faster than elimination from scratch each time,
    especially if solving multiple systems with the same \(A\) but different \(b\).
  prefs: []
  type: TYPE_NORMAL
- en: Pivoting and Permutations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sometimes elimination requires row swaps (to avoid division by zero or instability).
    Then factorization is written as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ PA = LU, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(P\) is a permutation matrix recording the row swaps. This is the practical
    form used in numerical computing.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of LU Factorization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Efficient solving: Multiple right-hand sides \(Ax = b\). Compute \(LU\) once,
    reuse for each \(b\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Determinants: \(\det(A) = \det(L)\det(U)\). Since diagonals of \(L\) are 1,
    this reduces to the product of the diagonal of \(U\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Matrix inverse: By solving \(Ax = e_i\) for each column \(e_i\), we can compute
    \(A^{-1}\) efficiently with LU.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical methods: LU is central in scientific computing, engineering simulations,
    and optimization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LU decomposition separates the elimination process into:'
  prefs: []
  type: TYPE_NORMAL
- en: '\(L\): shear transformations (adding multiples of rows).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\(U\): scaling and alignment into triangular form.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, they represent the same linear transformation as \(A\), but decomposed
    into simpler building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LU factorization compresses elimination into a reusable format.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is a cornerstone of numerical linear algebra and used in almost every solver.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It links computation (efficient algorithms) with theory (factorization of transformations).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It introduces the broader idea that matrices can be broken into simple, interpretable
    parts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Factor
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 \\ 3 & 8 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: into \(LU\).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Solve
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 1 \\ 6 & 3 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}
    = \begin{bmatrix} 5 \\ 15 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: using LU decomposition.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute \(\det(A)\) for
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 & 1 \\ 4 & -6 & 0 \\ -2 & 7 & 2 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: by using its LU factorization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Challenge: Prove that if \(A\) is invertible, then it has an LU factorization
    (possibly after row swaps).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LU factorization organizes elimination into a powerful tool: compact, efficient,
    and deeply tied to both the theory and practice of linear algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 4\. Vector spaces and subspaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opening
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 31\. Axioms of Vector Spaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up to now, we have worked with vectors in \(\mathbb{R}^2\), \(\mathbb{R}^3\),
    and higher-dimensional Euclidean spaces. But the true power of linear algebra
    comes from abstracting away from coordinates. A vector space is not tied to arrows
    in physical space-it is any collection of objects that behave like vectors, provided
    they satisfy certain rules. These rules are called the axioms of vector spaces.
  prefs: []
  type: TYPE_NORMAL
- en: The Idea of a Vector Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A vector space is a set \(V\) equipped with two operations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Vector addition: Combine two vectors in \(V\) to get another vector in \(V\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scalar multiplication: Multiply a vector in \(V\) by a scalar (a number from
    a field, usually \(\mathbb{R}\) or \(\mathbb{C}\)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The magic is that as long as certain rules (axioms) hold, the objects in \(V\)
    can be treated as vectors. They need not be arrows or coordinate lists-they could
    be polynomials, functions, matrices, or sequences.
  prefs: []
  type: TYPE_NORMAL
- en: The Eight Axioms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let \(u, v, w \in V\) (vectors) and \(a, b \in \mathbb{R}\) (scalars). The
    axioms are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Closure under addition: \(u + v \in V\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Commutativity of addition: \(u + v = v + u\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Associativity of addition: \((u + v) + w = u + (v + w)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Existence of additive identity: There exists a zero vector \(0 \in V\) such
    that \(v + 0 = v\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Existence of additive inverses: For every \(v\), there is \(-v\) such that
    \(v + (-v) = 0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Closure under scalar multiplication: \(a v \in V\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distributivity of scalar multiplication over vector addition: \(a(u + v) =
    au + av\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Distributivity of scalar multiplication over scalar addition: \((a + b)v =
    av + bv\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Associativity of scalar multiplication: \(a(bv) = (ab)v\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Existence of multiplicative identity: \(1 \cdot v = v\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (These are sometimes listed as eight, with some grouped together, but the essence
    is the same.)
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Vector Spaces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Euclidean spaces: \(\mathbb{R}^n\) with standard addition and scalar multiplication.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Polynomials: The set of all polynomials with real coefficients, \(\mathbb{R}[x]\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Functions: The set of all continuous functions on \([0,1]\), with addition
    of functions and scalar multiplication.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Matrices: The set of all \(m \times n\) matrices with real entries.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sequences: The set of all infinite real sequences \((a_1, a_2, \dots)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of these satisfy the vector space axioms.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The set of natural numbers \(\mathbb{N}\) is not a vector space (no additive
    inverses).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The set of positive real numbers \(\mathbb{R}^+\) is not a vector space (not
    closed under scalar multiplication with negative numbers).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The set of polynomials of degree exactly 2 is not a vector space (not closed
    under addition: \(x^2 + x^2 = 2x^2\) is still degree 2, but \(x^2 - x^2 = 0\),
    which is degree 0, not allowed).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These examples show why the axioms are essential: without them, the structure
    breaks.'
  prefs: []
  type: TYPE_NORMAL
- en: The Zero Vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Every vector space must contain a zero vector. This is not optional. It is the
    “do nothing” element for addition. In \(\mathbb{R}^n\), this is \((0,0,\dots,0)\).
    In polynomials, it is the zero polynomial. In function spaces, it is the function
    \(f(x) = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: Additive Inverses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For every vector \(v\), we require \(-v\). This ensures that equations like
    \(u+v=w\) can always be rearranged to \(u=w-v\). Without additive inverses, solving
    linear equations would not work.
  prefs: []
  type: TYPE_NORMAL
- en: Scalars and Fields
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scalars come from a field: usually the real numbers \(\mathbb{R}\) or the complex
    numbers \(\mathbb{C}\). The choice of scalars matters:'
  prefs: []
  type: TYPE_NORMAL
- en: Over \(\mathbb{R}\), a polynomial space is different from over \(\mathbb{C}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over finite fields (like integers modulo \(p\)), vector spaces exist in discrete
    mathematics and coding theory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The axioms guarantee that vectors can be added and scaled in predictable ways.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Closure ensures the space is “self-contained.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additive inverses ensure symmetry: every direction can be reversed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributivity ensures consistency between scaling and addition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these rules make vector spaces stable and reliable mathematical objects.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Vector spaces unify many areas of math under a single framework.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They generalize \(\mathbb{R}^n\) to functions, polynomials, and beyond.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The axioms guarantee that all the tools of linear algebra-span, basis, dimension,
    linear maps-apply.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recognizing vector spaces in disguise is a major step in advanced math and physics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Verify that the set of all 2×2 matrices is a vector space under matrix addition
    and scalar multiplication.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that the set of polynomials of degree at most 3 is a vector space, but
    the set of polynomials of degree exactly 3 is not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check whether the set of all even functions \(f(-x) = f(x)\) is a vector space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Consider the set of all differentiable functions \(f\) on \([0,1]\).
    Show that this set forms a vector space under the usual operations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The axioms of vector spaces provide the foundation on which the rest of linear
    algebra is built. Everything that follows-subspaces, independence, basis, dimension-grows
    naturally from this formal framework.
  prefs: []
  type: TYPE_NORMAL
- en: 32\. Subspaces, Column Space, and Null Space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the idea of a vector space is in place, the next step is to recognize smaller
    vector spaces that live inside bigger ones. These are called subspaces. Subspaces
    are central in linear algebra because they reveal the internal structure of matrices
    and linear systems. Two special subspaces-the column space and the null space-play
    particularly important roles.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Subspace?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A subspace \(W\) of a vector space \(V\) is a subset of \(V\) that is itself
    a vector space under the same operations. To qualify as a subspace, \(W\) must
    satisfy:'
  prefs: []
  type: TYPE_NORMAL
- en: The zero vector \(0\) is in \(W\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If \(u, v \in W\), then \(u+v \in W\) (closed under addition).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If \(u \in W\) and \(c\) is a scalar, then \(cu \in W\) (closed under scalar
    multiplication).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s it-no further checking of all ten vector space axioms is needed, because
    those are inherited from \(V\).
  prefs: []
  type: TYPE_NORMAL
- en: Simple Examples of Subspaces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In \(\mathbb{R}^3\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A line through the origin is a 1-dimensional subspace.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A plane through the origin is a 2-dimensional subspace.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The whole space itself is a subspace.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The trivial subspace \(\{0\}\) contains only the zero vector.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the space of polynomials:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All polynomials of degree ≤ 3 form a subspace.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All polynomials with zero constant term form a subspace.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In function spaces:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All continuous functions on \([0,1]\) form a subspace of all functions on \([0,1]\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All solutions to a linear differential equation form a subspace.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The Column Space of a Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given a matrix \(A\), the column space is the set of all linear combinations
    of its columns. Formally,
  prefs: []
  type: TYPE_NORMAL
- en: '\[ C(A) = \{ A\mathbf{x} : \mathbf{x} \in \mathbb{R}^n \}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: The column space lives inside \(\mathbb{R}^m\) if \(A\) is \(m \times n\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It represents all possible outputs of the linear transformation defined by \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its dimension is equal to the rank of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 3 & 6 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The second column is just twice the first. So the column space is all multiples
    of \(\begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}\), which is a line in \(\mathbb{R}^3\).
    Rank = 1.
  prefs: []
  type: TYPE_NORMAL
- en: The Null Space of a Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The null space (or kernel) of a matrix \(A\) is the set of all vectors \(\mathbf{x}\)
    such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ A\mathbf{x} = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: It lives in \(\mathbb{R}^n\) if \(A\) is \(m \times n\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It represents the “invisible” directions that collapse to zero under the transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its dimension is the nullity of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Solve \(A\mathbf{x} = 0\). This yields a null space spanned by one vector, meaning
    it is a line through the origin in \(\mathbb{R}^3\).
  prefs: []
  type: TYPE_NORMAL
- en: Column Space vs. Null Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Column space: describes outputs (\(y\)-values that can be reached).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Null space: describes hidden inputs (directions that vanish).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, they capture the full behavior of a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^3\), the column space could be a plane or a line inside 3D space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The null space is orthogonal (in a precise sense) to the row space, which we’ll
    study later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding both spaces gives a complete picture of how the matrix transforms
    vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Subspaces are the natural habitat of linear algebra: almost everything happens
    inside them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The column space explains what systems \(Ax=b\) are solvable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The null space explains why some systems have multiple solutions (free variables).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These ideas extend to advanced topics like eigenvectors, SVD, and differential
    equations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Show that the set \(\{(x,y,0) : x,y \in \mathbb{R}\}\) is a subspace of \(\mathbb{R}^3\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 0 & 0 \\ 1 & 2 & 3 \end{bmatrix}, \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: find the column space and its dimension.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the same \(A\), compute the null space and its dimension.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that the null space of \(A\) is always a subspace of \(\mathbb{R}^n\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subspaces-especially the column space and null space-are the first glimpse of
    the hidden geometry inside every matrix, showing us which directions survive and
    which vanish.
  prefs: []
  type: TYPE_NORMAL
- en: 33\. Span and Generating Sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The idea of a span captures the simplest and most powerful way to build new
    vectors from old ones: by taking linear combinations. A span is not just a set
    of scattered points but a structured, complete collection of all combinations
    of a given set of vectors. Understanding span leads directly to the concepts of
    bases, dimension, and the structure of subspaces.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition of Span
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given vectors \(v_1, v_2, \dots, v_k \in V\), the span of these vectors is
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \text{span}\{v_1, v_2, \dots, v_k\} = \{a_1 v_1 + a_2 v_2 + \dots + a_k
    v_k : a_i \in \mathbb{R}\}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: A span is the set of all possible linear combinations of the vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is always a subspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The given vectors are called a generating set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In \(\mathbb{R}^2\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Span of \((1,0)\) = all multiples of the x-axis (a line).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Span of \((1,0)\) and \((0,1)\) = the entire plane \(\mathbb{R}^2\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Span of \((1,0)\) and \((2,0)\) = still the x-axis, since the second vector
    is redundant.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In \(\mathbb{R}^3\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Span of a single vector = a line.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Span of two independent vectors = a plane through the origin.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Span of three independent vectors = the whole space \(\mathbb{R}^3\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Span as Coverage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you think of vectors as “directions,” the span is everything you can reach
    by walking in those directions, with any step lengths (scalars) allowed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you only have one direction, you can walk back and forth on a line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With two independent directions, you can sweep out a plane.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With three independent directions in 3D, you can move anywhere.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating Sets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A set of vectors is a generating set (or spanning set) for a subspace if their
    span equals that subspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: \(\{(1,0), (0,1)\}\) generates \(\mathbb{R}^2\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: \(\{(1,0,0), (0,1,0), (0,0,1)\}\) generates \(\mathbb{R}^3\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: The columns of a matrix generate its column space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different generating sets can span the same space. Some may be redundant, others
    minimal. Later, the concept of a basis refines this idea.
  prefs: []
  type: TYPE_NORMAL
- en: Redundancy in Spanning Sets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If one vector is a linear combination of others, it does not enlarge the span.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: In \(\mathbb{R}^2\), \(\{(1,0), (0,1), (1,1)\}\) spans the same space
    as \(\{(1,0), (0,1)\}\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating redundancy leads to a more efficient generating set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Span and Linear Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider the system \(Ax=b\).
  prefs: []
  type: TYPE_NORMAL
- en: The question “Is there a solution?” is equivalent to “Is \(b\) in the span of
    the columns of \(A\)?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, span provides the geometric language for solvability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Span is the foundation for defining subspaces generated by vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It connects directly to solvability of linear equations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It introduces the notion of redundancy, preparing for bases and independence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It generalizes naturally to function spaces and abstract vector spaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Find the span of \(\{(1,2), (2,4)\}\) in \(\mathbb{R}^2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that the vectors \((1,0,1), (0,1,1), (1,1,2)\) span only a plane in \(\mathbb{R}^3\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide whether \((1,2,3)\) is in the span of \((1,0,1)\) and \((0,1,2)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that the set of all polynomials \(\{1, x, x^2, \dots\}\) spans
    the space of all polynomials.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The concept of span transforms our perspective: instead of focusing on single
    vectors, we see the entire landscape of possibilities they generate.'
  prefs: []
  type: TYPE_NORMAL
- en: 34\. Linear Independence and Dependence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having introduced span and generating sets, the natural question arises: *when
    are the vectors in a spanning set truly necessary, and when are some redundant?*
    This leads to the idea of linear independence. It is the precise way to distinguish
    between essential vectors (those that add new directions) and dependent vectors
    (those that can be expressed in terms of others).'
  prefs: []
  type: TYPE_NORMAL
- en: Definition of Linear Independence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A set of vectors \(\{v_1, v_2, \dots, v_k\}\) is linearly independent if the
    only solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ a_1 v_1 + a_2 v_2 + \dots + a_k v_k = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: is
  prefs: []
  type: TYPE_NORMAL
- en: \[ a_1 = a_2 = \dots = a_k = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: If there exists a nontrivial solution (some \(a_i \neq 0\)), then the vectors
    are linearly dependent.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Independent vectors point in genuinely different directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dependent vectors overlap: at least one can be built from the others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In terms of span: removing a dependent vector does not shrink the span, because
    it adds no new direction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple Examples in \(\mathbb{R}^2\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \((1,0)\) and \((0,1)\) are independent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Equation \(a(1,0) + b(0,1) = (0,0)\) forces \(a = b = 0\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \((1,0)\) and \((2,0)\) are dependent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Equation \(2(1,0) - (2,0) = (0,0)\) shows dependence.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Any set of 3 vectors in \(\mathbb{R}^2\) is dependent, since the dimension of
    the space is 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examples in \(\mathbb{R}^3\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \((1,0,0), (0,1,0), (0,0,1)\) are independent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \((1,2,3), (2,4,6)\) are dependent, since the second is just 2× the first.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '\((1,0,1), (0,1,1), (1,1,2)\) are dependent: the third is the sum of the first
    two.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detecting Independence with Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Put the vectors as columns in a matrix. Perform row reduction:'
  prefs: []
  type: TYPE_NORMAL
- en: If every column has a pivot → the set is independent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If some column is free → the set is dependent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 0 & 0 & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here the third column has no pivot → the 3rd vector is dependent on the first
    two.
  prefs: []
  type: TYPE_NORMAL
- en: Relationship with Dimension
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^n\), at most \(n\) independent vectors exist.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you have more than \(n\), dependence is guaranteed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A basis of a vector space is simply a maximal independent set that spans the
    space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Independent vectors = different directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dependent vectors = one vector lies in the span of others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 2D: two independent vectors span the plane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 3D: three independent vectors span the space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Independence ensures a generating set is minimal and efficient.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It determines whether a system of vectors is a basis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It connects directly to rank: rank = number of independent columns (or rows).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is crucial in geometry, data compression, and machine learning-where redundancy
    must be identified and removed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Test whether \((1,2)\) and \((2,4)\) are independent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are the vectors \((1,0,0), (0,1,0), (1,1,0)\) independent in \(\mathbb{R}^3\)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Place the vectors \((1,0,1), (0,1,1), (1,1,2)\) into a matrix and row-reduce
    to check independence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that any set of \(n+1\) vectors in \(\mathbb{R}^n\) is linearly
    dependent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear independence is the tool that separates essential directions from redundant
    ones. It is the key to defining bases, counting dimensions, and understanding
    the structure of all vector spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 35\. Basis and Coordinates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The concepts of span and linear independence come together in the powerful idea
    of a basis. A basis gives us the minimal set of building blocks needed to generate
    an entire vector space, with no redundancy. Once a basis is chosen, every vector
    in the space can be described uniquely by a list of numbers called its coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Basis?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A basis of a vector space \(V\) is a set of vectors \(\{v_1, v_2, \dots, v_k\}\)
    that satisfies two properties:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spanning property: \(\text{span}\{v_1, \dots, v_k\} = V\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Independence property: The vectors are linearly independent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In short: a basis is a spanning set with no redundancy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Standard Bases'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^2\), the standard basis is \(\{(1,0), (0,1)\}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^3\), the standard basis is \(\{(1,0,0), (0,1,0), (0,0,1)\}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^n\), the standard basis is the collection of unit vectors, each
    with a 1 in one position and 0 elsewhere.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are called standard because they are the default way of describing coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: Uniqueness of Coordinates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the most important facts about bases is that they provide unique representations
    of vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a basis \(\{v_1, \dots, v_k\}\), any vector \(x \in V\) can be written
    uniquely as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ x = a_1 v_1 + a_2 v_2 + \dots + a_k v_k. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The coefficients \((a_1, a_2, \dots, a_k)\) are the coordinates of \(x\) relative
    to that basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This uniqueness distinguishes bases from arbitrary spanning sets, where redundancy
    allows multiple representations.
  prefs: []
  type: TYPE_NORMAL
- en: Example in \(\mathbb{R}^2\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let basis = \(\{(1,0), (0,1)\}\).
  prefs: []
  type: TYPE_NORMAL
- en: Vector \((3,5) = 3(1,0) + 5(0,1)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coordinates relative to this basis: \((3,5)\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we switch to a different basis, the coordinates change even though the vector
    itself does not.
  prefs: []
  type: TYPE_NORMAL
- en: Example with Non-Standard Basis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Basis = \(\{(1,1), (1,-1)\}\) in \(\mathbb{R}^2\). Find coordinates of \(x =
    (2,0)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Solve \(a(1,1) + b(1,-1) = (2,0)\). This gives system:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ a + b = 2, \quad a - b = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'So \(a=1, b=1\). Coordinates relative to this basis: \((1,1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice: coordinates depend on basis choice.'
  prefs: []
  type: TYPE_NORMAL
- en: Basis of Function Spaces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For polynomials of degree ≤ 2: basis = \(\{1, x, x^2\}\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example: \(2 + 3x + 5x^2\) has coordinates \((2,3,5)\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For continuous functions on \([0,1]\), one possible basis is the infinite set
    \(\{1, x, x^2, \dots\}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This shows bases are not restricted to geometric vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Dimension
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The number of vectors in a basis is the dimension of the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: \(\mathbb{R}^2\) has dimension 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbb{R}^3\) has dimension 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The space of polynomials of degree ≤ 3 has dimension 4.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimension tells us how many independent directions exist in the space.
  prefs: []
  type: TYPE_NORMAL
- en: Change of Basis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Switching from one basis to another is like translating between languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same vector looks different depending on which “dictionary” (basis) you
    use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Change-of-basis matrices allow systematic translation between coordinate systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A basis is like setting up coordinate axes in a space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 2D, two independent vectors define a grid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 3D, three independent vectors define a full coordinate system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different bases = different grids overlaying the same space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bases provide the simplest possible description of a vector space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They allow us to assign unique coordinates to vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They connect the abstract structure of a space with concrete numerical representations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The concept underlies almost all of linear algebra: dimension, transformations,
    eigenvectors, and more.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Show that \(\{(1,2), (3,4)\}\) is a basis of \(\mathbb{R}^2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Express \((4,5)\) in terms of basis \(\{(1,1), (1,-1)\}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that no basis of \(\mathbb{R}^3\) can have more than 3 vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that the set \(\{1, \cos x, \sin x\}\) is a basis for the space
    of all linear combinations of \(1, \cos x, \sin x\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A basis is the minimal, elegant foundation of a vector space, turning the infinite
    into the manageable by providing a finite set of independent building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 36\. Dimension
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dimension is one of the most profound and unifying ideas in linear algebra.
    It gives a single number that captures the “size” or “capacity” of a vector space:
    how many independent directions it has. Unlike length, width, or height in everyday
    geometry, dimension in linear algebra applies to spaces of any kind-geometric,
    algebraic, or even function spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dimension of a vector space \(V\) is the number of vectors in any basis
    of \(V\).
  prefs: []
  type: TYPE_NORMAL
- en: Since all bases of a vector space have the same number of elements, dimension
    is well-defined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If \(\dim V = n\), then:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every set of more than \(n\) vectors in \(V\) is dependent.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Every set of exactly \(n\) independent vectors forms a basis.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples in Familiar Spaces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \(\dim(\mathbb{R}^2) = 2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Basis: \((1,0), (0,1)\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Two directions cover the whole plane.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\dim(\mathbb{R}^3) = 3\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Basis: \((1,0,0), (0,1,0), (0,0,1)\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Three independent directions span 3D space.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The set of all polynomials of degree ≤ 2 has dimension 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Basis: \(\{1, x, x^2\}\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The space of all \(m \times n\) matrices has dimension \(mn\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each entry is independent, and the standard basis consists of matrices with
    a single 1 and the rest 0.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finite vs. Infinite Dimensions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finite-dimensional spaces: \(\mathbb{R}^n\), polynomials of degree ≤ \(k\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Infinite-dimensional spaces:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The space of all polynomials (no degree limit).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The space of all continuous functions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These cannot be spanned by a finite set of vectors.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimension and Subspaces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Any subspace of \(\mathbb{R}^n\) has dimension ≤ \(n\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A line through the origin in \(\mathbb{R}^3\): dimension 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A plane through the origin in \(\mathbb{R}^3\): dimension 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The whole space: dimension 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The trivial subspace \(\{0\}\): dimension 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimension and Systems of Equations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When solving \(A\mathbf{x} = \mathbf{b}\):'
  prefs: []
  type: TYPE_NORMAL
- en: The dimension of the column space = rank = number of independent directions
    in the outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dimension of the null space = number of free variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By the rank–nullity theorem:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \dim(\text{column space}) + \dim(\text{null space}) = \text{number of variables}.
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dimension counts the minimum number of coordinates needed to describe a vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^2\), you need 2 numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^3\), you need 3 numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the polynomial space of degree ≤ 3, you need 4 coefficients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, dimension = length of coordinate list.
  prefs: []
  type: TYPE_NORMAL
- en: Checking Dimension in Practice
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Place candidate vectors as columns of a matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Row reduce to echelon form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Count pivots. That number = dimension of the span of those vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dimension is the most fundamental measure of a vector space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It tells us how “large” or “complex” the space is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It sets absolute limits: in \(\mathbb{R}^n\), no more than \(n\) independent
    vectors exist.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It underlies coordinate systems, bases, and transformations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It bridges geometry (lines, planes, volumes) with algebra (solutions, equations,
    matrices).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: What is the dimension of the span of \((1,2,3)\), \((2,4,6)\), \((0,0,0)\)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the dimension of the subspace of \(\mathbb{R}^3\) defined by \(x+y+z=0\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that the set of all \(2 \times 2\) symmetric matrices has dimension 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that the space of polynomials of degree ≤ \(k\) has dimension
    \(k+1\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dimension is the measuring stick of linear algebra: it tells us how many independent
    pieces of information are needed to describe the whole space.'
  prefs: []
  type: TYPE_NORMAL
- en: 37\. Rank–Nullity Theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The rank–nullity theorem is one of the central results of linear algebra. It
    gives a precise balance between two fundamental aspects of a matrix: the dimension
    of its column space (rank) and the dimension of its null space (nullity). It shows
    that no matter how complicated a matrix looks, the distribution of information
    between its “visible” outputs and its “hidden” null directions always obeys a
    strict law.'
  prefs: []
  type: TYPE_NORMAL
- en: Statement of the Theorem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let \(A\) be an \(m \times n\) matrix (mapping \(\mathbb{R}^n \to \mathbb{R}^m\)):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{rank}(A) + \text{nullity}(A) = n \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: rank(A) = dimension of the column space of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: nullity(A) = dimension of the null space of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(n\) = number of columns of \(A\), i.e., the number of variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Think of a matrix as a machine that transforms input vectors into outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: Rank measures how many independent output directions survive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nullity measures how many input directions get “lost” (mapped to zero).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The theorem says: total inputs = useful directions (rank) + wasted directions
    (nullity).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This ensures nothing disappears mysteriously-every input direction is accounted
    for.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 1: Full Rank'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Rank = 2 (two independent columns).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Null space = \(\{0\}\), so nullity = 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank + nullity = 2 = number of variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 2: Dependent Columns'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 \\ 2 & 4 \\ 3 & 6 \\ \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Second column is a multiple of the first. Rank = 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Null space contains all vectors \((x,y)\) with \(y = -2x\). Nullity = 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank + nullity = 1 + 1 = 2 = number of variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 3: Larger System'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Columns: \((1,0), (0,1), (1,1)\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Only two independent columns → Rank = 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Null space: solve \(x + z = 0, y + z = 0 \Rightarrow (x,y,z) = (-t,-t,t)\).
    Nullity = 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank + nullity = 2 + 1 = 3 = number of variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proof Sketch (Conceptual)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Row reduce \(A\) to echelon form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pivots correspond to independent columns → count = rank.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Free variables correspond to null space directions → count = nullity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Each column is either a pivot column or corresponds to a free variable, so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \text{rank} + \text{nullity} = \text{number of columns}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^3\), if a transformation collapses all vectors onto a plane
    (rank = 2), then one direction disappears entirely (nullity = 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^4\), if a matrix has rank 2, then its null space has dimension
    2, meaning half the input directions vanish.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The theorem guarantees the geometry of “surviving” and “vanishing” directions
    always adds up consistently.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Solving systems \(Ax = b\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rank determines consistency and structure of solutions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Nullity tells how many free parameters exist in the solution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data compression: Rank identifies independent features; nullity shows redundancy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer graphics: Rank–nullity explains how 3D coordinates collapse into 2D
    images: one dimension of depth is lost.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine learning: Rank signals how much real information a dataset contains;
    nullity indicates degrees of freedom that add nothing new.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The rank–nullity theorem connects the abstract ideas of rank and nullity into
    a single, elegant formula.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It ensures conservation of dimension: no information magically appears or disappears.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is essential in understanding solutions of systems, dimensions of subspaces,
    and the structure of linear transformations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It prepares the ground for deeper results in algebra, topology, and differential
    equations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Verify rank–nullity for
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For a \(4 \times 5\) matrix of rank 3, what is its nullity?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^3\), suppose a matrix maps all of space onto a line. What are
    its rank and nullity?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove rigorously that the row space and null space are orthogonal
    complements, and use this to derive rank–nullity again.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The rank–nullity theorem is the law of balance in linear algebra: every input
    dimension is accounted for, either as a surviving direction (rank) or as one that
    vanishes (nullity).'
  prefs: []
  type: TYPE_NORMAL
- en: 38\. Coordinates Relative to a Basis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once a basis for a vector space is chosen, every vector in that space can be
    described uniquely in terms of the basis. These descriptions are called coordinates.
    Coordinates transform abstract vectors into concrete lists of numbers, making
    computation possible. Changing the basis changes the coordinates, but the underlying
    vector remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: The Core Idea
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given a vector space \(V\) and a basis \(B = \{v_1, v_2, \dots, v_n\}\), every
    vector \(x \in V\) can be written uniquely as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x = a_1 v_1 + a_2 v_2 + \dots + a_n v_n. \]
  prefs: []
  type: TYPE_NORMAL
- en: The coefficients \((a_1, a_2, \dots, a_n)\) are the coordinates of \(x\) with
    respect to the basis \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: This representation is unique because basis vectors are independent.
  prefs: []
  type: TYPE_NORMAL
- en: Example in \(\mathbb{R}^2\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Standard basis: \(B = \{(1,0), (0,1)\}\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vector \(x = (3,5)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coordinates relative to \(B\): \((3,5)\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Non-standard basis: \(B = \{(1,1), (1,-1)\}\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write \(x = (3,5)\) as \(a(1,1) + b(1,-1)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solve:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ a+b = 3, \quad a-b = 5. \]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Adding: \(2a = 8 \implies a = 4\). Subtracting: \(2b = -2 \implies b = -1\).'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Coordinates relative to this basis: \((4, -1)\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The same vector looks different depending on the chosen basis.
  prefs: []
  type: TYPE_NORMAL
- en: Example in \(\mathbb{R}^3\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let \(B = \{(1,0,0), (1,1,0), (1,1,1)\}\). Find coordinates of \(x = (2,3,4)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Solve \(a(1,0,0) + b(1,1,0) + c(1,1,1) = (2,3,4)\). This gives system:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ a+b+c = 2, \quad b+c = 3, \quad c = 4. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'From \(c=4\), we get \(b+c=3 \implies b=-1\). Then \(a+b+c=2 \implies a-1+4=2
    \implies a=-1\). Coordinates: \((-1, -1, 4)\).'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Formulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If \(B = \{v_1, \dots, v_n\}\), form the basis matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = [v_1 \ v_2 \ \dots \ v_n]. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then for a vector \(x\), its coordinate vector \([x]_B\) satisfies
  prefs: []
  type: TYPE_NORMAL
- en: \[ P [x]_B = x. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: \[ [x]_B = P^{-1}x. \]
  prefs: []
  type: TYPE_NORMAL
- en: This shows coordinate transformation is simply matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Changing Coordinates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose a vector has coordinates \([x]_B\) relative to basis \(B\). If we switch
    to another basis \(C\), we use a change-of-basis matrix to convert coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ [x]_C = (P_C^{-1} P_B) [x]_B. \]
  prefs: []
  type: TYPE_NORMAL
- en: This process is fundamental in computer graphics, robotics, and data transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A basis defines a coordinate system: axes in the space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coordinates are the “addresses” of vectors relative to those axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Changing basis is like rotating or stretching the grid: the address changes,
    but the point does not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Coordinates make abstract vectors computable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They allow us to represent functions, polynomials, and geometric objects numerically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Changing basis simplifies problems-e.g., diagonalization makes matrices easy
    to analyze.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They connect the abstract (spaces, bases) with the concrete (numbers, matrices).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Express \(x=(4,2)\) relative to basis \(\{(1,1),(1,-1)\}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find coordinates of \(x=(2,1,3)\) relative to basis \(\{(1,0,1),(0,1,1),(1,1,0)\}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If basis \(B\) is the standard basis and basis \(C=\{(1,1),(1,-1)\}\), compute
    the change-of-basis matrix from \(B\) to \(C\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that if \(P\) is invertible, its columns form a basis, and
    explain why this guarantees uniqueness of coordinates.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Coordinates relative to a basis are the bridge between geometry and algebra:
    they turn abstract spaces into numerical systems where computation, reasoning,
    and transformation become systematic and precise.'
  prefs: []
  type: TYPE_NORMAL
- en: 39\. Change-of-Basis Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every vector space allows multiple choices of basis, and each basis provides
    a different way of describing the same vectors. The process of moving from one
    basis to another is called a change of basis. To perform this change systematically,
    we use a change-of-basis matrix. This matrix acts as a translator between coordinate
    systems: it converts the coordinates of a vector relative to one basis into coordinates
    relative to another.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Change Bases?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Simplicity of computation: Some problems are easier in certain bases. For example,
    diagonalizing a matrix allows us to raise it to powers more easily.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geometry: Different bases can represent rotated or scaled coordinate systems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Applications: In physics, computer graphics, robotics, and data science, changing
    bases is equivalent to switching perspectives or reference frames.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Basic Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let \(V\) be a vector space with two bases:'
  prefs: []
  type: TYPE_NORMAL
- en: \(B = \{b_1, b_2, \dots, b_n\}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(C = \{c_1, c_2, \dots, c_n\}\)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose a vector \(x \in V\) has coordinates \([x]_B\) relative to \(B\), and
    \([x]_C\) relative to \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: 'We want a matrix \(P_{B \to C}\) such that:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ [x]_C = P_{B \to C} [x]_B. \]
  prefs: []
  type: TYPE_NORMAL
- en: This matrix \(P_{B \to C}\) is the change-of-basis matrix from \(B\) to \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the Change-of-Basis Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write each vector in the basis \(B\) in terms of the basis \(C\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Place these coordinate vectors as the columns of a matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting matrix converts coordinates from \(B\) to \(C\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_{B \to C} = \big[ [b_1]_C \ [b_2]_C \ \dots \ [b_n]_C \big]. \]
  prefs: []
  type: TYPE_NORMAL
- en: Example in \(\mathbb{R}^2\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \(B = \{(1,0), (0,1)\}\) (standard basis).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(C = \{(1,1), (1,-1)\}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To build \(P_{B \to C}\):'
  prefs: []
  type: TYPE_NORMAL
- en: Express each vector of \(B\) in terms of \(C\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solve:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (1,0) = a(1,1) + b(1,-1). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This gives system:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ a+b=1, \quad a-b=0. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution: \(a=\tfrac{1}{2}, b=\tfrac{1}{2}\). So \((1,0) = \tfrac{1}{2}(1,1)
    + \tfrac{1}{2}(1,-1)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (0,1) = a(1,1) + b(1,-1). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'System:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ a+b=0, \quad a-b=1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution: \(a=\tfrac{1}{2}, b=-\tfrac{1}{2}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_{B \to C} = \begin{bmatrix} \tfrac{1}{2} & \tfrac{1}{2} \\ \tfrac{1}{2}
    & -\tfrac{1}{2} \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So for any vector \(x\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ [x]_C = P_{B \to C}[x]_B. \]
  prefs: []
  type: TYPE_NORMAL
- en: Inverse Change of Basis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If \(P_{B \to C}\) is the change-of-basis matrix from \(B\) to \(C\), then
    its inverse is the change-of-basis matrix in the opposite direction:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_{C \to B} = (P_{B \to C})^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes sense: translating back and forth between languages should undo
    itself.'
  prefs: []
  type: TYPE_NORMAL
- en: General Formula with Basis Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_B = [b_1 \ b_2 \ \dots \ b_n], \quad P_C = [c_1 \ c_2 \ \dots \ c_n], \]
  prefs: []
  type: TYPE_NORMAL
- en: the matrices whose columns are basis vectors written in standard coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the change-of-basis matrix from \(B\) to \(C\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_{B \to C} = P_C^{-1} P_B. \]
  prefs: []
  type: TYPE_NORMAL
- en: This formula is extremely useful because it reduces the problem to matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Changing basis is like rotating or stretching the grid lines of a coordinate
    system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vector itself (the point in space) does not move. What changes is its description
    in terms of the new grid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The change-of-basis matrix is the tool that translates between these descriptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Diagonalization: Expressing a matrix in a basis of its eigenvectors makes it
    diagonal, simplifying analysis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer graphics: Changing camera viewpoints requires change-of-basis matrices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Robotics: Coordinate transformations connect robot arms, joints, and workspace
    frames.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data science: PCA finds a new basis (principal components) where data is easier
    to analyze.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Provides a universal method to translate coordinates between bases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Makes abstract transformations concrete and computable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Forms the backbone of diagonalization, Jordan form, and the spectral theorem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connects algebraic manipulations with geometry and real-world reference frames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the change-of-basis matrix from the standard basis to \(\{(2,1),(1,1)\}\)
    in \(\mathbb{R}^2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the change-of-basis matrix from basis \(\{(1,0,0),(0,1,0),(0,0,1)\}\) to
    \(\{(1,1,0),(0,1,1),(1,0,1)\}\) in \(\mathbb{R}^3\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that applying \(P_{B \to C}\) then \(P_{C \to B}\) returns the original
    coordinates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Derive the formula \(P_{B \to C} = P_C^{-1} P_B\) starting from
    the definition of coordinates.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change-of-basis matrices give us the precise mechanism for switching perspectives.
    They ensure that although bases change, vectors remain invariant, and computations
    remain consistent.
  prefs: []
  type: TYPE_NORMAL
- en: 40\. Affine Subspaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'So far, vector spaces and subspaces have always passed through the origin.
    But in many real-world situations, we deal with shifted versions of these spaces:
    planes not passing through the origin, lines offset from the zero vector, or solution
    sets to linear equations with nonzero constants. These structures are called affine
    subspaces. They extend the idea of subspaces by allowing “translation away from
    the origin.”'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An affine subspace of a vector space \(V\) is a set of the form
  prefs: []
  type: TYPE_NORMAL
- en: '\[ x_0 + W = \{x_0 + w : w \in W\}, \]'
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(x_0 \in V\) is a fixed vector (the “base point” or “anchor”),
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(W \subseteq V\) is a linear subspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, an affine subspace is simply a subspace shifted by a vector.
  prefs: []
  type: TYPE_NORMAL
- en: Examples in \(\mathbb{R}^2\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A line through the origin: \(\text{span}\{(1,2)\}\). This is a subspace.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A line not through the origin: \((3,1) + \text{span}\{(1,2)\}\). This is an
    affine subspace.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The entire plane: \(\mathbb{R}^2\), which is both a subspace and an affine
    subspace.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examples in \(\mathbb{R}^3\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Plane through the origin: \(\text{span}\{(1,0,0),(0,1,0)\}\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Plane not through the origin: \((2,3,4) + \text{span}\{(1,0,0),(0,1,0)\}\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Line parallel to the z-axis but passing through \((1,1,5)\): \((1,1,5) + \text{span}\{(0,0,1)\}\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Relation to Linear Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Affine subspaces naturally arise as solution sets of linear equations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Homogeneous system: \(Ax = 0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution set is a subspace (the null space).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Non-homogeneous system: \(Ax = b\) with \(b \neq 0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution set is affine.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If \(x_p\) is one particular solution, then the general solution is:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ x = x_p + N(A), \]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: where \(N(A)\) is the null space.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Thus, the geometry of solving equations leads naturally to affine subspaces.
  prefs: []
  type: TYPE_NORMAL
- en: Affine Dimension
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The dimension of an affine subspace is defined as the dimension of its direction
    subspace \(W\).
  prefs: []
  type: TYPE_NORMAL
- en: 'A point: affine subspace of dimension 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A line: dimension 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A plane: dimension 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher analogs continue in \(\mathbb{R}^n\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difference Between Subspaces and Affine Subspaces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Subspaces always contain the origin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affine subspaces may or may not pass through the origin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every subspace is an affine subspace (with base point \(x_0 = 0\)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Think of affine subspaces as “flat sheets” floating in space:'
  prefs: []
  type: TYPE_NORMAL
- en: A line through the origin is a rope tied at the center.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A line parallel to it but offset is the same rope moved to the side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affine subspaces preserve shape and direction, but not position.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linear equations: General solutions are affine subspaces.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimization: Feasible regions in linear programming are affine subspaces (intersected
    with inequalities).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer graphics: Affine transformations map affine subspaces to affine subspaces,
    preserving straightness and parallelism.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine learning: Affine decision boundaries (like hyperplanes) separate data
    into classes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Affine subspaces generalize subspaces, making linear algebra more flexible.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They allow us to describe solution sets that don’t include the origin.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They provide the geometric foundation for affine geometry, computer graphics,
    and optimization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They serve as the bridge from pure linear algebra to applied modeling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Show that the set of solutions to
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x+y+z=1 \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: is an affine subspace of \(\mathbb{R}^3\). Identify its dimension.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Find the general solution to
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x+2y=3 \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: and describe it as an affine subspace.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Prove that the intersection of two affine subspaces is either empty or another
    affine subspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that every affine subspace can be written uniquely as \(x_0
    + W\) with \(W\) a subspace.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Affine subspaces are the natural setting for most real-world linear problems:
    they combine the strict structure of subspaces with the freedom of translation,
    capturing both direction and position.'
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 5\. Linear Transformation and Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opening
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 41\. Linear Transformations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A linear transformation is the heart of linear algebra. It is the rule that
    connects two vector spaces in a way that respects their linear structure: addition
    and scalar multiplication. Instead of thinking of vectors as static objects, linear
    transformations let us study how vectors move, stretch, rotate, project, or reflect.
    They give linear algebra its dynamic power and are the bridge between abstract
    theory and concrete applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A function \(T: V \to W\) between vector spaces is called a linear transformation
    if for all \(u, v \in V\) and scalars \(a, b \in \mathbb{R}\) (or another field),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ T(au + bv) = aT(u) + bT(v). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This single condition encodes two rules:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additivity: \(T(u+v) = T(u) + T(v)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Homogeneity: \(T(av) = aT(v)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If both are satisfied, the transformation is linear.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of Linear Transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scaling: \(T(x) = 3x\) in \(\mathbb{R}\). Every number is stretched threefold.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rotation in the plane: \(T(x,y) = (x\cos\theta - y\sin\theta, \, x\sin\theta
    + y\cos\theta)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Projection: Projecting \((x,y,z)\) onto the \(xy\)-plane: \(T(x,y,z) = (x,y,0)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Differentiation: On the space of polynomials, \(T(p(x)) = p''(x)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Integration: On continuous functions, \(T(f)(x) = \int_0^x f(t) \, dt\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All these are linear because they preserve addition and scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \(T(x) = x^2\) is not linear, because \((x+y)^2 \neq x^2 + y^2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\(T(x,y) = (x+1, y)\) is not linear, because it fails homogeneity: scaling
    doesn’t preserve the “+1.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonlinear rules break the structure of vector spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Every linear transformation from \(\mathbb{R}^n\) to \(\mathbb{R}^m\) can be
    represented by a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'If \(T: \mathbb{R}^n \to \mathbb{R}^m\), then there exists an \(m \times n\)
    matrix \(A\) such that:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ T(x) = Ax. \]
  prefs: []
  type: TYPE_NORMAL
- en: The columns of \(A\) are simply \(T(e_1), T(e_2), \dots, T(e_n)\), where \(e_i\)
    are the standard basis vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Let \(T(x,y) = (2x+y, x-y)\).'
  prefs: []
  type: TYPE_NORMAL
- en: \(T(e_1) = T(1,0) = (2,1)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(T(e_2) = T(0,1) = (1,-1)\). So
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 1 & -1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then \(T(x,y) = A \begin{bmatrix} x \\ y \end{bmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: Properties of Linear Transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The image of the zero vector is always zero: \(T(0) = 0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The image of a line through the origin is again a line (or collapsed to a point).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Composition of linear transformations is linear.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Every linear transformation preserves the structure of subspaces.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kernel and Image (Preview)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For \(T: V \to W\):'
  prefs: []
  type: TYPE_NORMAL
- en: 'The kernel (or null space) is all vectors mapped to zero: \(\ker T = \{v \in
    V : T(v) = 0\}\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The image (or range) is all outputs that can be achieved: \(\text{im}(T) =
    \{T(v) : v \in V\}\). The rank–nullity theorem applies here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \dim(\ker T) + \dim(\text{im}(T)) = \dim(V). \]
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linear transformations reshape space:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling stretches space uniformly in one direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotation spins space while preserving lengths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projection flattens space onto lower dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflection flips space across a line or plane.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The key feature: straight lines remain straight, and the origin stays fixed.'
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Computer graphics: Scaling, rotating, projecting 3D objects onto 2D screens.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Robotics: Transformations between joint coordinates and workspace positions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data science: Linear mappings represent dimensionality reduction and feature
    extraction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Differential equations: Solutions often involve linear operators acting on
    function spaces.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine learning: Weight matrices in neural networks are stacked linear transformations,
    interspersed with nonlinearities.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Linear transformations generalize matrices to any vector space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They unify geometry, algebra, and applications under one concept.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They provide the natural framework for studying eigenvalues, eigenvectors, and
    decompositions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'They model countless real-world processes: physical, computational, and abstract.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prove that \(T(x,y,z) = (x+2y, z, x-y+z)\) is linear.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the matrix representation of the transformation that reflects vectors in
    \(\mathbb{R}^2\) across the line \(y=x\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show why \(T(x,y) = (x^2,y)\) is not linear.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: For the differentiation operator \(D: P_3 \to P_2\) on polynomials
    of degree ≤ 3, find its matrix relative to the basis \(\{1,x,x^2,x^3\}\) in the
    domain and \(\{1,x,x^2\}\) in the codomain.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear transformations are the language of linear algebra. They capture the
    essence of symmetry, motion, and structure in spaces of any kind, making them
    indispensable for both theory and practice.
  prefs: []
  type: TYPE_NORMAL
- en: 42\. Matrix Representation of a Linear Map
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every linear transformation can be expressed concretely as a matrix. This is
    one of the most powerful bridges in mathematics: it translates abstract functional
    rules into arrays of numbers that can be calculated, manipulated, and visualized.'
  prefs: []
  type: TYPE_NORMAL
- en: From Abstract Rule to Concrete Numbers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose \(T: V \to W\) is a linear transformation between two finite-dimensional
    vector spaces. To represent \(T\) as a matrix, we first select bases:'
  prefs: []
  type: TYPE_NORMAL
- en: \(B = \{v_1, v_2, \dots, v_n\}\) for the domain \(V\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(C = \{w_1, w_2, \dots, w_m\}\) for the codomain \(W\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each basis vector \(v_j\), compute \(T(v_j)\). Each image \(T(v_j)\) is
    a vector in \(W\), so it can be written as a combination of the basis \(C\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ T(v_j) = a_{1j}w_1 + a_{2j}w_2 + \dots + a_{mj}w_m. \]
  prefs: []
  type: TYPE_NORMAL
- en: The coefficients \((a_{1j}, a_{2j}, \dots, a_{mj})\) become the j-th column
    of the matrix representing \(T\).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the matrix of \(T\) relative to bases \(B\) and \(C\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ [T]_{B \to C} = \begin{bmatrix} a_{11} & a_{12} & \dots & a_{1n} \\ a_{21}
    & a_{22} & \dots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2}
    & \dots & a_{mn} \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This guarantees that for any vector \(x\) in coordinates relative to \(B\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ [T(x)]_C = [T]_{B \to C}[x]_B. \]
  prefs: []
  type: TYPE_NORMAL
- en: Standard Basis Case
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When both \(B\) and \(C\) are the standard bases, the process simplifies:'
  prefs: []
  type: TYPE_NORMAL
- en: Take \(T(e_1), T(e_2), \dots, T(e_n)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Place them as columns in a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That matrix directly represents \(T\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Let \(T(x,y) = (2x+y, x-y)\).'
  prefs: []
  type: TYPE_NORMAL
- en: \(T(e_1) = (2,1)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(T(e_2) = (1,-1)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So the standard matrix is
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 1 & -1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: For any vector \(\begin{bmatrix} x \\ y \end{bmatrix}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ T(x,y) = A \begin{bmatrix} x \\ y \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Perspectives
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Columns-as-images: Each column shows where a basis vector goes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Row view: Each row encodes how to compute one coordinate of the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Operator view: The matrix acts like a machine: input vector → multiply → output
    vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Insight
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Matrices reshape space. In \(\mathbb{R}^2\):'
  prefs: []
  type: TYPE_NORMAL
- en: The first column shows where the x-axis goes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second column shows where the y-axis goes. The entire grid is determined
    by these two images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^3\), the three columns are the images of the unit coordinate
    directions, defining how the whole space twists, rotates, or compresses.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Computer graphics: Rotations, scaling, and projections are represented by small
    matrices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Robotics: Coordinate changes between joints and workspaces rely on transformation
    matrices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data science: Linear maps such as PCA are implemented with matrices that project
    data into lower dimensions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Linear operators like rotations, boosts, and stress tensors are matrix
    representations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Matrices are computational tools: we can add, multiply, invert them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They let us use algorithms like Gaussian elimination, LU/QR/SVD to study transformations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They link abstract vector space theory to hands-on numerical calculation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They reveal the structure of transformations at a glance, just by inspecting
    columns and rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Find the matrix for the transformation \(T(x,y,z) = (x+2y, y+z, x+z)\) in the
    standard basis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the matrix of \(T: \mathbb{R}^2 \to \mathbb{R}^2\), where \(T(x,y)
    = (x-y, x+y)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the basis \(B=\{(1,1), (1,-1)\}\) for \(\mathbb{R}^2\), find the matrix
    of \(T(x,y) = (2x, y)\) relative to \(B\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that matrix multiplication corresponds to composition of transformations,
    i.e. \([S \circ T] = [S][T]\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Matrix representations are the practical form of linear transformations, turning
    elegant definitions into something we can compute, visualize, and apply across
    science and engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 43\. Kernel and Image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every linear transformation hides two essential structures: the set of vectors
    that collapse to zero, and the set of all possible outputs. These are called the
    kernel and the image. They are the DNA of a linear map, revealing its internal
    structure, its strengths, and its limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: The Kernel
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The kernel (or null space) of a linear transformation \(T: V \to W\) is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \ker(T) = \{ v \in V : T(v) = 0 \}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: It is the set of all vectors that the transformation sends to the zero vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It measures how much information is “lost” under the transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel is always a subspace of the domain \(V\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(T: \mathbb{R}^2 \to \mathbb{R}^2\), \(T(x,y) = (x,0)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kernel: all vectors of the form \((0,y)\). This is the y-axis.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For \(T: \mathbb{R}^3 \to \mathbb{R}^2\), \(T(x,y,z) = (x,y)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kernel: all vectors of the form \((0,0,z)\). This is the z-axis.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel tells us which directions in the domain vanish under \(T\).
  prefs: []
  type: TYPE_NORMAL
- en: The Image
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The image (or range) of a linear transformation is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \text{im}(T) = \{ T(v) : v \in V \}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: It is the set of all vectors that can actually be reached by applying \(T\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It describes the “output space” of the transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image is always a subspace of the codomain \(W\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(T(x,y) = (x,0)\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Image: all vectors of the form \((a,0)\). This is the x-axis.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For \(T(x,y,z) = (x+y, y+z)\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Image: all of \(\mathbb{R}^2\). Any vector \((u,v)\) can be achieved by solving
    equations for \((x,y,z)\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel and Image Together
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'These two subspaces reflect two aspects of \(T\):'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel measures the collapse in dimension.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image measures the preserved and transmitted directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A central result is the Rank–Nullity Theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \dim(\ker T) + \dim(\text{im }T) = \dim(V). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\dim(\ker T)\) is the nullity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\dim(\text{im }T)\) is the rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This theorem guarantees a perfect balance: the domain splits into lost directions
    (kernel) and active directions (image).'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix View
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For a matrix \(A\), the linear map is \(T(x) = Ax\).
  prefs: []
  type: TYPE_NORMAL
- en: The kernel is the solution set of \(Ax = 0\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image is the column space of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Image: span of the columns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \text{im}(A) = \text{span}\{ (1,0), (2,1), (3,1) \}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel: solve'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 1 \end{bmatrix} \begin{bmatrix} x \\
    y \\ z \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This leads to solutions like \(x=-y-2z\). So the kernel is 1-dimensional, the
    image is 2-dimensional, and the domain (3D) splits as \(1+2=3\).
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The kernel is the set of invisible directions, like shadows disappearing in
    projection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image is the set of all shadows that can appear.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together they describe projection, flattening, stretching, or collapsing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Projecting \(\mathbb{R}^3\) onto the xy-plane:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel: the z-axis (all points collapsed to zero height).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image: the entire xy-plane (all possible shadows).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Solving equations: Kernel describes all solutions to \(Ax=0\). Image describes
    what right-hand sides \(b\) make \(Ax=b\) solvable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data science: Nullity corresponds to redundant features; rank corresponds to
    useful independent features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: In mechanics, symmetries often form the kernel of a transformation,
    while observable quantities form the image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control theory: The kernel and image determine controllability and observability
    of systems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kernel and image classify transformations into invertible or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They give a precise language to describe dimension changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They are the foundation of rank, nullity, and invertibility.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'They generalize far beyond matrices: to polynomials, functions, operators,
    and differential equations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the kernel and image of \(T(x,y,z) = (x+y, y+z)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the projection \(T(x,y,z) = (x,y,0)\), identify kernel and image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that if the kernel is trivial (\(\{0\}\)), then the transformation is injective.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove the rank–nullity theorem for a \(3\times 3\) matrix by working
    through examples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The kernel and image are the twin lenses through which linear transformations
    are understood. One tells us what disappears, the other what remains. Together,
    they give the clearest picture of a transformation’s essence.
  prefs: []
  type: TYPE_NORMAL
- en: 44\. Invertibility and Isomorphisms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Linear transformations come in many forms: some collapse space into lower dimensions,
    others stretch it, and a special group preserves all information perfectly. These
    special transformations are invertible, meaning they can be reversed exactly.
    When two vector spaces are related by such a transformation, we say they are isomorphic-structurally
    identical, even if they look different on the surface.'
  prefs: []
  type: TYPE_NORMAL
- en: Invertibility of Linear Transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A linear transformation \(T: V \to W\) is invertible if there exists another
    linear transformation \(S: W \to V\) such that:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ S \circ T = I_V \quad \text{and} \quad T \circ S = I_W, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(I_V\) and \(I_W\) are identity maps on \(V\) and \(W\).
  prefs: []
  type: TYPE_NORMAL
- en: \(S\) is called the inverse of \(T\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If such an inverse exists, \(T\) is a bijection: both one-to-one (injective)
    and onto (surjective).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In finite-dimensional spaces, this is equivalent to saying that \(T\) is represented
    by an invertible matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Invertible Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An \(n \times n\) matrix \(A\) is invertible if there exists another \(n \times
    n\) matrix \(A^{-1}\) such that:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ AA^{-1} = A^{-1}A = I. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Characterizations of Invertibility:'
  prefs: []
  type: TYPE_NORMAL
- en: \(A\) is invertible ⇔ \(\det(A) \neq 0\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ⇔ Columns of \(A\) are linearly independent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ⇔ Columns of \(A\) span \(\mathbb{R}^n\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ⇔ Rank of \(A\) is \(n\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ⇔ The system \(Ax=b\) has exactly one solution for every \(b\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'All these properties tie together: invertibility means no information is lost
    when transforming vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: Isomorphisms of Vector Spaces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two vector spaces \(V\) and \(W\) are isomorphic if there exists a bijective
    linear transformation \(T: V \to W\).'
  prefs: []
  type: TYPE_NORMAL
- en: This means \(V\) and \(W\) are “the same” in structure, though they may look
    different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For finite-dimensional spaces:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ V \cong W \quad \text{if and only if} \quad \dim(V) = \dim(W). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Example: \(\mathbb{R}^2\) and the set of all polynomials of degree ≤ 1 are
    isomorphic, because both have dimension 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples of Invertibility
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rotation in the plane: Every rotation matrix has an inverse (rotation by the
    opposite angle).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ R(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta
    \end{bmatrix}, \quad R(\theta)^{-1} = R(-\theta). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Scaling by nonzero factor: \(T(x) = ax\) with \(a \neq 0\). Inverse is \(T^{-1}(x)
    = \tfrac{1}{a}x\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Projection onto a line: Not invertible, because depth is lost. The kernel is
    nontrivial.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Differentiation on polynomials of degree ≤ n: Not invertible, since constant
    terms vanish in the kernel.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Differentiation on exponential functions: Invertible: the inverse is integration
    (up to constants).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Invertible transformations preserve dimension: no flattening or collapsing
    occurs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They may rotate, shear, stretch, or reflect, but every input vector can be uniquely
    recovered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The determinant tells the “volume scaling” of the transformation: invertibility
    requires this volume not to collapse to zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Computer graphics: Invertible matrices allow smooth transformations where no
    information is lost. Non-invertible maps (like projections) create 2D renderings
    from 3D worlds.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cryptography: Encryption systems rely on invertible linear maps for encoding/decoding.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Robotics: Transformations between joint and workspace coordinates must often
    be invertible for precise control.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data science: PCA often reduces dimension (non-invertible), but whitening transformations
    are invertible within the chosen subspace.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Coordinate changes (e.g., Galilean or Lorentz transformations) are
    invertible, ensuring that physical laws remain consistent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Invertible maps preserve the entire structure of a vector space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'They classify vector spaces: if two have the same dimension, they are fundamentally
    the same via isomorphism.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They allow reversible modeling, essential in physics, cryptography, and computation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They highlight the delicate balance between lossless transformations (invertible)
    and lossy ones (non-invertible).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prove that the matrix \(\begin{bmatrix} 2 & 1 \\ 3 & 2 \end{bmatrix}\) is invertible
    by computing its determinant and its inverse.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that projection onto the x-axis in \(\mathbb{R}^2\) is not invertible.
    Identify its kernel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct an explicit isomorphism between \(\mathbb{R}^3\) and the space of
    polynomials of degree ≤ 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that if \(T\) is an isomorphism, then it maps bases to bases.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invertibility and isomorphism are the gateways from “linear rules” to the grand
    idea of equivalence. They allow us to say, with mathematical precision, when two
    spaces are truly the same in structure-different clothes, same skeleton.
  prefs: []
  type: TYPE_NORMAL
- en: 45\. Composition, Powers, and Iteration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear transformations are not isolated operations-they can be combined, repeated,
    and layered to build more complex effects. This leads us to the ideas of composition,
    powers of transformations, and iteration. These concepts form the backbone of
    linear dynamics, algorithms, and many real-world systems where repeated actions
    accumulate into surprising results.
  prefs: []
  type: TYPE_NORMAL
- en: Composition of Linear Transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If \(T: U \to V\) and \(S: V \to W\) are linear transformations, then their
    composition is another transformation'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ S \circ T : U \to W, \quad (S \circ T)(u) = S(T(u)). \]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Composition is associative: \((R \circ S) \circ T = R \circ (S \circ T)\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Composition is linear: the result of composing two linear maps is still linear.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of matrices, if \(T(x) = Ax\) and \(S(x) = Bx\), then
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ (S \circ T)(x) = B(Ax) = (BA)x. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Notice that the order matters: composition corresponds to matrix multiplication.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \(T(x,y) = (x+2y, y)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(S(x,y) = (2x, x-y)\). Then \((S \circ T)(x,y) = S(x+2y,y) = (2(x+2y), (x+2y)-y)
    = (2x+4y, x+y)\). Matrix multiplication confirms the same result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Powers of Transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If \(T: V \to V\), we can apply it repeatedly:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ T^2 = T \circ T, \quad T^3 = T \circ T \circ T, \quad \dots \]
  prefs: []
  type: TYPE_NORMAL
- en: These are called powers of \(T\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(T(x) = Ax\), then \(T^k(x) = A^k x\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powers of transformations capture repeated processes, like compounding interest,
    population growth, or iterative algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Let \(T(x,y) = (2x, 3y)\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ T^n(x,y) = (2^n x, 3^n y). \]
  prefs: []
  type: TYPE_NORMAL
- en: Each iteration amplifies the scaling along different directions.
  prefs: []
  type: TYPE_NORMAL
- en: Iteration and Dynamical Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Iteration means applying the same transformation repeatedly to study long-term
    behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_{k+1} = T(x_k), \quad x_0 \text{ given}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This creates a discrete dynamical system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on \(T\), vectors may grow, shrink, oscillate, or stabilize.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 1 (Markov Chains): If \(T\) is a stochastic matrix, iteration describes
    probability evolution over time. Eventually, the system may converge to a steady-state
    distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2 (Population Models): If \(T\) describes how sub-populations interact,
    iteration simulates generations. Eigenvalues dictate whether populations explode,
    stabilize, or vanish.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 3 (Computer Graphics): Repeated affine transformations create fractals
    like the Sierpinski triangle.'
  prefs: []
  type: TYPE_NORMAL
- en: Stability and Eigenvalues
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The behavior of \(T^n(x)\) depends heavily on eigenvalues of the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: If \(|\lambda| < 1\), repeated application shrinks vectors in that direction
    to zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(|\lambda| > 1\), repeated application causes exponential growth.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(|\lambda| = 1\), vectors rotate or oscillate without changing length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This link between powers and eigenvalues underpins many algorithms in numerical
    analysis and physics.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Composition = chaining geometric actions (rotate then reflect, scale then shear).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powers = applying the same action repeatedly (rotating 90° four times = identity).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iteration = exploring the “orbit” of a vector under repeated transformations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Search engines: PageRank is computed by iterating a linear transformation until
    it stabilizes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Economics: Input–output models iterate to predict long-term equilibrium of
    industries.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Time evolution of quantum states is modeled by repeated application
    of unitary operators.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical methods: Iterative solvers (like power iteration) approximate eigenvectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer graphics: Iterated function systems generate self-similar fractals.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Composition unifies matrix multiplication and transformation chaining.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Powers reveal exponential growth, decay, and oscillation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iteration is the core of modeling dynamic processes in mathematics, science,
    and engineering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The link to eigenvalues makes these ideas the foundation of stability analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let \(T(x,y) = (x+y, y)\). Compute \(T^2(x,y)\) and \(T^3(x,y)\). What happens
    as \(n \to \infty\)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider rotation by 90° in \(\mathbb{R}^2\). Show that \(T^4 = I\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For matrix \(A = \begin{bmatrix} 0.5 & 0.5 \\ 0.5 & 0.5 \end{bmatrix}\), iterate
    \(A^n\). What happens to arbitrary vectors?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that if \(A\) is diagonalizable as \(A = PDP^{-1}\), then
    \(A^n = PD^nP^{-1}\). Use this to analyze long-term behavior.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Composition, powers, and iteration take linear algebra beyond static equations
    into the world of processes over time. They explain how small, repeated steps
    shape long-term outcomes-whether stabilizing systems, amplifying signals, or creating
    infinite complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 46\. Similarity and Conjugation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In linear algebra, different matrices can represent the same underlying transformation
    when written in different coordinate systems. This relationship is captured by
    the idea of similarity. Two matrices are similar if one is obtained from the other
    by a conjugation with an invertible change-of-basis matrix. This concept is central
    to understanding canonical forms, eigenvalue decompositions, and the deep structure
    of linear operators.
  prefs: []
  type: TYPE_NORMAL
- en: Definition of Similarity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two \(n \times n\) matrices \(A\) and \(B\) are called similar if there exists
    an invertible matrix \(P\) such that:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = P^{-1}AP. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here, \(P\) represents a change of basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(A\) and \(B\) describe the same linear transformation, but expressed relative
    to different bases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conjugation as Change of Basis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose \(T: V \to V\) is a linear transformation and \(A\) is its matrix in
    basis \(B\). If we switch to a new basis \(C\), the matrix becomes \(B\). The
    conversion is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = P^{-1}AP, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(P\) is the change-of-basis matrix from basis \(B\) to basis \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: 'This shows that similarity is not just algebraic coincidence-it’s geometric:
    the operator is the same, but our perspective (basis) has changed.'
  prefs: []
  type: TYPE_NORMAL
- en: Properties Preserved Under Similarity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If \(A\) and \(B\) are similar, they share many key properties:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determinant: \(\det(A) = \det(B)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Trace: \(\text{tr}(A) = \text{tr}(B)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rank: \(\text{rank}(A) = \text{rank}(B)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Eigenvalues: Same set of eigenvalues (with multiplicity).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Characteristic polynomial: Identical.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Minimal polynomial: Identical.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These invariants define the “skeleton” of a linear operator, unaffected by coordinate
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rotation in the plane: The matrix for rotation by 90° is'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In another basis, the rotation might be represented by a more complicated-looking
    matrix, but all such matrices are similar to \(A\).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Diagonalization: A matrix \(A\) is diagonalizable if it is similar to a diagonal
    matrix \(D\). That is,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = PDP^{-1}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, similarity reduces \(A\) to its simplest form.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Shear transformation: A shear matrix \(\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\)
    is not diagonalizable, but it may be similar to a Jordan block.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Similarity says: two matrices may look different, but they are “the same” transformation
    seen from different coordinate systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conjugation is the mathematical act of relabeling coordinates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think of shifting your camera angle: the scene hasn’t changed, only the perspective
    has.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Diagonalization: Reducing a matrix to diagonal form (when possible) uses similarity.
    This simplifies powers, exponentials, and iterative analysis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jordan canonical form: Every square matrix is similar to a Jordan form, giving
    a complete structural classification.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantum mechanics: Operators on state spaces often change representation, but
    similarity guarantees invariance of spectra.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control theory: Canonical forms simplify analysis of system stability and controllability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical methods: Eigenvalue algorithms rely on repeated similarity transformations
    (e.g., QR algorithm).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similarity reveals the true identity of a linear operator, independent of coordinates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It allows simplification: many problems become easier in the right basis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It preserves invariants, giving us tools to classify and compare operators.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It connects abstract algebra with concrete computations in geometry, physics,
    and engineering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Show that \(\begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}\) is similar to \(\begin{bmatrix}
    2 & 0 \\ 0 & 2 \end{bmatrix}\). Why or why not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute \(P^{-1}AP\) for \(A = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}\)
    and \(P = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\). Interpret the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that if two matrices are similar, they must have the same trace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that if \(A\) and \(B\) are similar, then \(A^k\) and \(B^k\)
    are also similar for all integers \(k \geq 0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarity and conjugation elevate linear algebra from mere calculation to structural
    understanding. They tell us when two seemingly different matrices are just different
    “faces” of the same underlying transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 47\. Projections and Reflections
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Among the many transformations in linear algebra, two stand out for their geometric
    clarity and practical importance: projections and reflections. These operations
    reshape vectors in simple but powerful ways, and they form the building blocks
    of algorithms in statistics, optimization, graphics, and physics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Projection: Flattening onto a Subspace'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A projection is a linear transformation that takes a vector and drops it onto
    a subspace, like casting a shadow.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, if \(W\) is a subspace of \(V\), the projection of a vector \(v\)
    onto \(W\) is the unique vector \(w \in W\) that is closest to \(v\).
  prefs: []
  type: TYPE_NORMAL
- en: 'In \(\mathbb{R}^2\): projecting onto the x-axis takes \((x,y)\) and produces
    \((x,0)\).'
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonal Projection Formula
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Suppose \(u\) is a nonzero vector. The projection of \(v\) onto the line spanned
    by \(u\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{proj}_u(v) = \frac{v \cdot u}{u \cdot u} u. \]
  prefs: []
  type: TYPE_NORMAL
- en: This formula works in any dimension. It uses the dot product to measure how
    much of \(v\) points in the direction of \(u\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Project \((2,3)\) onto \(u=(1,1)\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{proj}_u(2,3) = \frac{(2,3)\cdot(1,1)}{(1,1)\cdot(1,1)} (1,1) = \frac{5}{2}(1,1)
    = (2.5,2.5). \]
  prefs: []
  type: TYPE_NORMAL
- en: The vector \((2,3)\) splits into \((2.5,2.5)\) along the line plus \((-0.5,0.5)\)
    orthogonal to it.
  prefs: []
  type: TYPE_NORMAL
- en: Projection Matrices
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For unit vector \(u\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = uu^T \]
  prefs: []
  type: TYPE_NORMAL
- en: is the projection matrix onto the span of \(u\).
  prefs: []
  type: TYPE_NORMAL
- en: 'For a general subspace with orthonormal basis columns in matrix \(Q\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = QQ^T \]
  prefs: []
  type: TYPE_NORMAL
- en: projects any vector onto that subspace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Properties:'
  prefs: []
  type: TYPE_NORMAL
- en: \(P^2 = P\) (idempotent).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(P^T = P\) (symmetric, for orthogonal projections).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reflection: Flipping Across a Subspace'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A reflection takes a vector and flips it across a line or plane. Geometrically,
    it’s like a mirror.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reflection across a line spanned by unit vector \(u\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R(v) = 2\text{proj}_u(v) - v. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R = 2uu^T - I. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Reflect \((2,3)\) across the line \(y=x\). With \(u=(1/\sqrt{2},1/\sqrt{2})\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R = \begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'So reflection swaps coordinates: \((2,3) \mapsto (3,2)\).'
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Insight
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Projection shortens vectors by removing components orthogonal to the subspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reflection preserves length but flips orientation relative to the subspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projection is about approximation (“closest point”), reflection is about symmetry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Statistics & Machine Learning: Least-squares regression is projection of data
    onto the span of predictor variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Projection transforms 3D scenes into 2D screen images. Reflections
    simulate mirrors and shiny surfaces.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimization: Projections enforce constraints by bringing guesses back into
    feasible regions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Reflections describe wave behavior, optics, and particle interactions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Methods: Projection operators are key to iterative algorithms (like
    Krylov subspace methods).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Projection captures the essence of approximation: keeping what fits, discarding
    what doesn’t.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reflection embodies symmetry and invariance, key to geometry and physics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both are linear, with elegant matrix representations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They combine easily with other transformations, making them versatile in computation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Find the projection matrix onto the line spanned by \((3,4)\). Verify it is
    idempotent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the reflection of \((1,2)\) across the x-axis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that reflection matrices are orthogonal (\(R^T R = I\)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: For subspace \(W\) with orthonormal basis \(Q\), derive the reflection
    matrix \(R = 2QQ^T - I\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Projections and reflections are two of the purest examples of how linear transformations
    embody geometric ideas. One approximates, the other symmetrizes-but both expose
    the deep structure of space through the lens of linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: 48\. Rotations and Shear
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear transformations can twist, turn, and distort space in strikingly different
    ways. Two of the most fundamental examples are rotations-which preserve lengths
    and angles while turning vectors-and shears-which slide one part of space relative
    to another, distorting shape while often preserving area. These two transformations
    form the geometric heart of linear algebra, and they are indispensable in graphics,
    physics, and engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Rotations in the Plane
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A rotation in \(\mathbb{R}^2\) by an angle \(\theta\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta
    \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For any vector \((x,y)\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R_\theta \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} x\cos\theta
    - y\sin\theta \\ x\sin\theta + y\cos\theta \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Properties:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Preserves lengths: \(\|R_\theta v\| = \|v\|\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Preserves angles: the dot product is unchanged.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determinant = \(+1\), so it preserves orientation and area.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inverse: \(R_\theta^{-1} = R_{-\theta}\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example: A 90° rotation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R_{90^\circ} = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}, \quad (1,0)
    \mapsto (0,1). \]
  prefs: []
  type: TYPE_NORMAL
- en: Rotations in Three Dimensions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rotations in \(\mathbb{R}^3\) occur around an axis. For example, rotation by
    angle \(\theta\) around the z-axis:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R_z(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta & 0 \\ \sin\theta
    & \cos\theta & 0 \\ 0 & 0 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Leaves the z-axis fixed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotates the xy-plane like a 2D rotation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General rotations in 3D are described by orthogonal matrices with determinant
    +1, forming the group \(SO(3)\).
  prefs: []
  type: TYPE_NORMAL
- en: Shear Transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A shear slides one coordinate direction while keeping another fixed, distorting
    shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In \(\mathbb{R}^2\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ S = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix} \quad \text{or} \quad \begin{bmatrix}
    1 & 0 \\ k & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The first form “slides” x-coordinates depending on y.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second form slides y-coordinates depending on x.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ S = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}, \quad (x,y) \mapsto (x+y,
    y). \]
  prefs: []
  type: TYPE_NORMAL
- en: Squares become parallelograms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Areas are preserved if \(\det(S) = 1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In \(\mathbb{R}^3\): shears distort volumes while preserving parallelism of
    faces.'
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Comparison
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rotation: Preserves size and shape exactly, only changes orientation. Circles
    remain circles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shear: Distorts shape but often preserves area (in 2D) or volume (in 3D). Circles
    become ellipses or slanted figures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, rotations and shears can generate a vast variety of linear distortions.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Computer Graphics: Rotations orient objects; shears simulate perspective.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Engineering: Shear stresses deform materials; rotations model rigid-body motion.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Robotics: Rotations define arm orientation; shears approximate local deformations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Rotations are symmetries of space; shears appear in fluid flows and
    elasticity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Science: Shears represent changes of variables that preserve volume but
    distort distributions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rotations model pure symmetry-no distortion, just reorientation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shears show how geometry can be distorted while preserving volume or area.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Both are building blocks: any invertible matrix in \(\mathbb{R}^2\) can be
    factored into rotations, shears, and scalings.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They bridge algebra and geometry, giving visual meaning to abstract matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rotate \((1,0)\) by 60° and compute the result explicitly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the shear \(S=\begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}\) to the square
    with vertices \((0,0),(1,0),(0,1),(1,1)\). What shape results?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that rotation matrices are orthogonal (\(R^TR=I\)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that any area-preserving \(2\times2\) matrix with determinant
    1 can be decomposed into a product of rotations and shears.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rotations and shears highlight two complementary sides of linear algebra: symmetry
    versus distortion. Together, they show how transformations can either preserve
    the essence of space or bend it into new shapes while keeping its structure intact.'
  prefs: []
  type: TYPE_NORMAL
- en: 49\. Rank and Operator Viewpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The rank of a linear transformation or matrix is one of the most important measures
    of its power. It captures how many independent directions a transformation preserves,
    how much information it carries from input to output, and how “full” its action
    on space is. Thinking of rank not just as a number, but as a description of an
    operator, gives us a clearer picture of what transformations really do.
  prefs: []
  type: TYPE_NORMAL
- en: Definition of Rank
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a matrix \(A\) representing a linear transformation \(T: V \to W\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{rank}(A) = \dim(\text{im}(A)) = \dim(\text{im}(T)). \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, the rank is the dimension of the image (or column space). It counts
    the maximum number of linearly independent columns.
  prefs: []
  type: TYPE_NORMAL
- en: Basic Properties
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \(\text{rank}(A) \leq \min(m,n)\) for an \(m \times n\) matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\text{rank}(A) = \text{rank}(A^T)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rank is equal to the number of pivot columns in row-reduced form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rank links directly with nullity via the rank–nullity theorem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \text{rank}(A) + \text{nullity}(A) = n. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Operator Perspective
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of focusing on rows and columns, imagine rank as a measure of how much
    of the domain is transmitted faithfully to the codomain.
  prefs: []
  type: TYPE_NORMAL
- en: 'If rank = full (\(n\)), the transformation is injective: nothing collapses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If rank = dimension of codomain (\(m\)), the transformation is surjective:
    every target vector can be reached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If rank is smaller, the transformation compresses space: parts of the domain
    are “invisible” and collapse into the kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 1 (Projection): Projection from \(\mathbb{R}^3\) onto the xy-plane
    has rank 2\. It annihilates the z-direction but preserves two independent directions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2 (Rotation): Rotation in \(\mathbb{R}^2\) has rank 2\. No directions
    are lost.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 3 (Zero map): The transformation sending everything to zero has rank
    0.'
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rank = number of independent directions preserved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rank-1 transformation maps all of space onto a single line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank-2 in \(\mathbb{R}^3\) maps space onto a plane.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank-full maps space onto its entire dimension without collapse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visually: rank describes the “dimensional thickness” of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: Rank and Matrix Factorizations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rank reveals hidden structure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'LU factorization: Rank determines the number of nonzero pivots.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'QR factorization: Rank controls the number of orthogonal directions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'SVD (Singular Value Decomposition): The number of nonzero singular values equals
    the rank.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'SVD in particular gives a geometric operator view: each nonzero singular value
    corresponds to a preserved dimension, while zeros indicate collapsed directions.'
  prefs: []
  type: TYPE_NORMAL
- en: Rank in Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data compression: Low-rank approximations reduce storage (e.g., image compression
    with SVD).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Statistics: Rank of the design matrix determines identifiability of regression
    coefficients.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine learning: Rank of weight matrices controls expressive power of models.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control theory: Rank conditions ensure controllability and observability of
    systems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Network analysis: Rank of adjacency or Laplacian matrices reflects connectivity
    of graphs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rank Deficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If a transformation has less than full rank, it is rank-deficient. This means:'
  prefs: []
  type: TYPE_NORMAL
- en: Some directions are lost (kernel nontrivial).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some outputs are unreachable (image smaller than codomain).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equations \(Ax=b\) may be inconsistent or underdetermined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting and handling rank deficiency is crucial in numerical linear algebra,
    where ill-conditioning can hide in nearly dependent columns.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Rank measures the true dimensional effect of a transformation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It distinguishes between full-strength operators and those that collapse information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It connects row space, column space, image, and kernel under one number.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It underpins algorithms for regression, decomposition, and dimensionality reduction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Find the rank of \(\begin{bmatrix} 1 & 2 & 3 \\ 2 & 4 & 6 \end{bmatrix}\). Why
    is it less than 2?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe geometrically the image of a rank-1 transformation in \(\mathbb{R}^3\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a \(5 \times 5\) diagonal matrix with diagonal entries \((2,0,3,0,5)\),
    compute rank and nullity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that for any matrix \(A\), the rank equals the number of nonzero
    singular values of \(A\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rank tells us not just how many independent vectors survive a transformation,
    but also how much structure the operator truly preserves. It is the bridge between
    abstract linear maps and their practical power.
  prefs: []
  type: TYPE_NORMAL
- en: 50\. Block Matrices and Block Maps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As problems grow in size, matrices become large and difficult to manage element
    by element. A powerful strategy is to organize matrices into blocks-submatrices
    grouped together like tiles in a mosaic. This allows us to treat large transformations
    as compositions of smaller, more understandable ones. Block matrices preserve
    structure, simplify computations, and reveal deep insights into how transformations
    act on subspaces.
  prefs: []
  type: TYPE_NORMAL
- en: What Are Block Matrices?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A block matrix partitions a matrix into rectangular submatrices. Each block
    is itself a matrix, and the entire matrix can be manipulated using block rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: a \(4 \times 4\) matrix divided into four \(2 \times 2\) blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where each \(A_{ij}\) is \(2 \times 2\).
  prefs: []
  type: TYPE_NORMAL
- en: Instead of thinking in terms of 16 entries, we work with 4 blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Block Maps as Linear Transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose \(V = V_1 \oplus V_2\) is decomposed into two subspaces. A linear map
    \(T: V \to V\) can be described in terms of how it acts on each component. Relative
    to this decomposition, the matrix of \(T\) has block form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ [T] = \begin{bmatrix} T_{11} & T_{12} \\ T_{21} & T_{22} \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '\(T_{11}\): how \(V_1\) maps into itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\(T_{12}\): how \(V_2\) contributes to \(V_1\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\(T_{21}\): how \(V_1\) contributes to \(V_2\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\(T_{22}\): how \(V_2\) maps into itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This decomposition highlights how subspaces interact under the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Block Matrix Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Block matrices obey the same rules as normal matrices, but operations are done
    block by block.
  prefs: []
  type: TYPE_NORMAL
- en: 'Addition:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} A & B \\ C & D \end{bmatrix} + \begin{bmatrix} E & F \\ G
    & H \end{bmatrix} = \begin{bmatrix} A+E & B+F \\ C+G & D+H \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} A & B \\ C & D \end{bmatrix} \begin{bmatrix} E & F \\ G &
    H \end{bmatrix} = \begin{bmatrix} AE+BG & AF+BH \\ CE+DG & CF+DH \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The formulas look like ordinary multiplication, but each term is itself a product
    of submatrices.
  prefs: []
  type: TYPE_NORMAL
- en: Special Block Structures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Block Diagonal Matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} A & 0 \\ 0 & D \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Independent actions on subspaces-no mixing between them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Block Upper Triangular:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} A & B \\ 0 & D \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Subspace \(V_1\) influences \(V_2\), but not vice versa.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Block Symmetric: If overall matrix is symmetric, so are certain block relationships:
    \(A^T=A, D^T=D, B^T=C\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These structures appear naturally in decomposition and iterative algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Block Matrix Inverses
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some block matrices can be inverted using special formulas. For
  prefs: []
  type: TYPE_NORMAL
- en: \[ M = \begin{bmatrix} A & B \\ C & D \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'if \(A\) is invertible, the inverse can be expressed using the Schur complement:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ M^{-1} = \begin{bmatrix} A^{-1} + A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1}
    \\ -(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1} \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This formula is central in statistics, optimization, and numerical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A block diagonal matrix acts like two independent transformations operating
    side by side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A block triangular matrix shows a “hierarchy”: one subspace influences the
    other but not the reverse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This decomposition mirrors how systems can be separated into smaller interacting
    parts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Numerical Linear Algebra: Block operations optimize computation on large sparse
    matrices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control Theory: State-space models are naturally expressed in block form.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Statistics: Partitioned covariance matrices rely on block inversion formulas.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Neural networks layer transformations, often structured into
    blocks for efficiency.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Parallel Computing: Block decomposition distributes large matrix problems across
    processors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Block matrices turn big problems into manageable smaller ones.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They reflect natural decompositions of systems into interacting parts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They make explicit the geometry of subspace interactions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They provide efficient algorithms, especially for large-scale scientific computing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multiply two \(4 \times 4\) matrices written as \(2 \times 2\) block matrices
    and confirm the block multiplication rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the projection matrix onto a 2D subspace in \(\mathbb{R}^4\) using block
    form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the Schur complement of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Challenge: Show that the determinant of a block triangular matrix equals the
    product of the determinants of its diagonal blocks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Block matrices and block maps show how complexity can be organized. Instead
    of drowning in thousands of entries, we see structure, interaction, and hierarchy-revealing
    how large systems can be built from simple linear pieces.
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 6\. Determinants and volume
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opening
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 51\. Areas, Volumes, and Signed Scale Factors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Determinants often feel like an abstract formula until we see their geometric
    meaning: they measure area in 2D, volume in 3D, and, in higher dimensions, the
    general “size” of a transformed shape. Even more, determinants encode whether
    orientation is preserved or flipped, giving them a “signed” interpretation. This
    perspective transforms determinants from algebraic curiosities into geometric
    tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformations and Scaling of Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a linear transformation \(T: \mathbb{R}^n \to \mathbb{R}^n\) represented
    by a square matrix \(A\). When \(A\) acts on vectors, it reshapes space: it stretches,
    compresses, rotates, reflects, or shears regions.'
  prefs: []
  type: TYPE_NORMAL
- en: If you apply \(A\) to a unit square in \(\mathbb{R}^2\), the image is a parallelogram.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you apply \(A\) to a unit cube in \(\mathbb{R}^3\), the image is a parallelepiped.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, the determinant of \(A\) tells us how the measure (area, volume,
    hyper-volume) of the shape has changed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determinant as Signed Scale Factor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \(|\det(A)|\) = the scale factor for areas (2D), volumes (3D), or n-dimensional
    content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\det(A) = 0\), the transformation collapses space into a lower dimension,
    flattening all volume away.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\det(A) > 0\), the orientation of space is preserved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\det(A) < 0\), the orientation is flipped (like a reflection in a mirror).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, determinants are not just numbers-they carry both magnitude and sign,
    telling us about size and handedness.
  prefs: []
  type: TYPE_NORMAL
- en: '2D Case: Area of Parallelogram'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Take two column vectors \(u,v \in \mathbb{R}^2\). Place them as columns in
    a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} u & v \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The absolute value of the determinant gives the area of the parallelogram spanned
    by \(u\) and \(v\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Area} = |\det(A)|. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then \(\det(A) = (2)(3) - (1)(1) = 5\). The unit square maps to a parallelogram
    of area 5.
  prefs: []
  type: TYPE_NORMAL
- en: '3D Case: Volume of Parallelepiped'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For three vectors \(u,v,w \in \mathbb{R}^3\), form a matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} u & v & w \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Then the absolute determinant gives the volume of the parallelepiped:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{Volume} = |\det(A)|. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Geometrically, this is the scalar triple product:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A) = u \cdot (v \times w). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 3 \end{bmatrix}, \quad
    \det(A) = 6. \]
  prefs: []
  type: TYPE_NORMAL
- en: So the unit cube is stretched into a box with volume 6.
  prefs: []
  type: TYPE_NORMAL
- en: Orientation and Signed Measure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Determinants do more than measure size-they also detect orientation:'
  prefs: []
  type: TYPE_NORMAL
- en: In 2D, flipping x and y axes changes the sign of the determinant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 3D, swapping two vectors changes the “handedness” (right-hand rule becomes
    left-hand rule).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This explains why determinants can be negative: they mark transformations that
    reverse orientation.'
  prefs: []
  type: TYPE_NORMAL
- en: Higher Dimensions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^n\), determinants extend the same idea. A unit hypercube (side
    length 1) is transformed into an n-dimensional parallelotope, whose volume is
    given by \(|\det(A)|\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Though we cannot visualize beyond 3D, the concept generalizes smoothly: determinants
    encode how much an n-dimensional object is stretched or collapsed.'
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Geometry: Computing areas, volumes, and orientation directly from vectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Determinants detect whether a transformation preserves or
    flips orientation, useful in rendering.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Determinants describe Jacobians for coordinate changes in integrals,
    adjusting volume elements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Engineering: Determinants quantify deformation and stress in materials (strain
    tensors).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Science: Determinants of covariance matrices encode “volume” of uncertainty
    ellipsoids.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Determinants connect algebra (formulas) to geometry (shapes).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'They explain why some transformations lose information: \(\det=0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They preserve orientation, key for consistent physical laws and geometry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They prepare us for advanced tools like Jacobians, eigenvalues, and volume-preserving
    maps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the area of the parallelogram spanned by \((1,2)\) and \((3,1)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the volume of the parallelepiped defined by vectors \((1,0,0),(0,1,0),(1,1,1)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that swapping two columns of a matrix flips the sign of the determinant
    but keeps absolute value unchanged.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Explain why \(\det(A)\) gives the scaling factor for integrals under
    change of variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Determinants begin as algebraic formulas, but their real meaning lies in geometry:
    they measure how linear transformations scale, compress, or flip space itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 52\. Determinant via Linear Rules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The determinant is not just a mysterious formula-it is a function built from
    a few simple rules that uniquely determine its behavior. These rules, often called
    determinant axioms, allow us to see the determinant as the only measure of “signed
    volume” compatible with linear algebra. Understanding these rules gives clarity:
    instead of memorizing expansion formulas, we see why determinants behave as they
    do.'
  prefs: []
  type: TYPE_NORMAL
- en: The Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Take a square matrix \(A \in \mathbb{R}^{n \times n}\). Think of \(A\) as a
    list of \(n\) column vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} a_1 & a_2 & \cdots & a_n \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The determinant is a function \(\det: \mathbb{R}^{n \times n} \to \mathbb{R}\)
    that assigns a single number to \(A\). Geometrically, it gives the signed volume
    of the parallelotope spanned by \((a_1, \dots, a_n)\). Algebraically, it follows
    three key rules.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rule 1: Linearity in Each Column'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you scale one column by a scalar \(c\), the determinant scales by \(c\).
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(a_1, \dots, c a_j, \dots, a_n) = c \cdot \det(a_1, \dots, a_j, \dots,
    a_n). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'If you replace a column with a sum, the determinant splits:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(a_1, \dots, (b+c), \dots, a_n) = \det(a_1, \dots, b, \dots, a_n) + \det(a_1,
    \dots, c, \dots, a_n). \]
  prefs: []
  type: TYPE_NORMAL
- en: This linearity means determinants behave predictably with respect to scaling
    and addition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rule 2: Alternating Property'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If two columns are the same, the determinant is zero:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(\dots, a_i, \dots, a_i, \dots) = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes sense geometrically: if two spanning vectors are identical, they
    collapse the volume to zero.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Equivalently: if you swap two columns, the determinant flips sign:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(\dots, a_i, \dots, a_j, \dots) = -\det(\dots, a_j, \dots, a_i, \dots).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Rule 3: Normalization'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The determinant of the identity matrix is 1:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(I_n) = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This anchors the function: the unit cube has volume 1, with positive orientation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequence: Uniqueness'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These three rules (linearity, alternating, normalization) uniquely define the
    determinant. Any function satisfying them must be the determinant. This makes
    it less of an arbitrary formula and more of a natural consequence of linear structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Small Cases: Explicit Formulas'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '2×2 matrices:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \det \begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This formula arises directly from the rules: linearity in columns and alternating
    sign when swapping them.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3×3 matrices: Expansion formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \det \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix} =
    aei + bfg + cdh - ceg - bdi - afh. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This looks complicated, but it comes from systematically applying the rules
    to break down the volume.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation of the Rules
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linearity: Stretching one side of a parallelogram or parallelepiped scales
    the area or volume.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternating: If two sides collapse into the same direction, the area/volume
    vanishes. Swapping sides flips orientation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Normalization: The unit cube has size 1 by definition.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Together, these mirror geometric intuition exactly.
  prefs: []
  type: TYPE_NORMAL
- en: Higher-Dimensional Generalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^n\), determinants measure oriented hyper-volume. For example,
    in 4D, determinants give the “4-volume” of a parallelotope. Though impossible
    to picture, the same rules apply.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Defining area and volume: Determinants provide a universal formula for computing
    geometric sizes from coordinates.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jacobian determinants: Used in calculus when changing variables in multiple
    integrals.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Orientation detection: Whether transformations preserve handedness in geometry
    or physics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer graphics: Ensuring consistent orientation of polygons and meshes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Determinants are not arbitrary. They arise naturally once we demand a function
    that is linear in columns, alternating, and normalized. This explains why so many
    different formulas and properties agree: they are all shadows of the same underlying
    definition.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Show that scaling one column by 3 multiplies the determinant by 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the determinant of \(\begin{bmatrix} 1 & 2 \\ 2 & 4 \end{bmatrix}\)
    and explain why it is zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Swap two columns in \(\begin{bmatrix} 1 & 2 \\ 3 & 4 \end{bmatrix}\) and confirm
    the determinant changes sign.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Use only the three rules to derive the \(2 \times 2\) determinant
    formula.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The determinant is the unique bridge between algebra and geometry, born from
    a handful of simple but powerful rules.
  prefs: []
  type: TYPE_NORMAL
- en: 53\. Determinant and Row Operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most practical ways to compute determinants is by using row operations,
    the same tools used in Gaussian elimination. Determinants interact with these
    operations in very structured ways. By understanding the rules, we can compute
    determinants systematically without resorting to long expansion formulas.
  prefs: []
  type: TYPE_NORMAL
- en: Row Operations Recap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are three elementary row operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Row Swap (R\(_i \leftrightarrow\) R\(_j\)) – exchange two rows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Row Scaling (c·R\(_i\)) – multiply a row by a scalar \(c\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Row Replacement (R\(_i\) + c·R\(_j\)) – replace one row with itself plus a multiple
    of another row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the determinant is defined in terms of linearity and alternation of rows
    (or columns), each operation has a clear effect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rule 1: Row Swap Changes Sign'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you swap two rows, the determinant changes sign:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A \text{ with } R_i \leftrightarrow R_j) = -\det(A). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Reason: Swapping two spanning vectors flips orientation. In 2D, swapping basis
    vectors flips a parallelogram across the diagonal, reversing handedness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rule 2: Row Scaling Multiplies Determinant'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you multiply a row by a scalar \(c\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A \text{ with } cR_i) = c \cdot \det(A). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Reason: Scaling one side of a parallelogram multiplies its area; scaling one
    dimension of a cube multiplies its volume.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rule 3: Row Replacement Leaves Determinant Unchanged'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you replace one row with itself plus a multiple of another row:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A \text{ with } R_i \to R_i + cR_j) = \det(A). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Reason: Adding a multiple of one spanning vector to another doesn’t change
    the spanned volume. The parallelogram or parallelepiped is sheared, but its area
    or volume remains the same.'
  prefs: []
  type: TYPE_NORMAL
- en: Why These Rules Work Together
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'These three rules align perfectly with the determinant axioms:'
  prefs: []
  type: TYPE_NORMAL
- en: Alternating → row swaps flip sign.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linearity → scaling multiplies by scalar.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization → row replacement preserves measure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, row operations provide a complete framework for computing determinants.
  prefs: []
  type: TYPE_NORMAL
- en: Computing Determinants with Elimination
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To compute \(\det(A)\):'
  prefs: []
  type: TYPE_NORMAL
- en: Perform Gaussian elimination to reduce \(A\) to an upper triangular matrix \(U\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Track how row swaps and scalings affect the determinant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the fact that the determinant of a triangular matrix is the product of its
    diagonal entries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 & 3 \\ 4 & 1 & 7 \\ -2 & 5 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: \(R_2 \to R_2 - 2R_1\), \(R_3 \to R_3 + R_1\). No determinant change.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Step 2: Upper triangular form emerges:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ U = \begin{bmatrix} 2 & 1 & 3 \\ 0 & -1 & 1 \\ 0 & 0 & -5 \end{bmatrix}.
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Step 3: Determinant is product of diagonals: \(\det(A) = 2 \cdot (-1) \cdot
    (-5) = 10.\)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficient, clear, and no messy cofactor expansions.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric View
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Row swap: Flips orientation of the volume.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Row scaling: Stretches or compresses one dimension of the volume.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Row replacement: Slides faces of the volume without changing its size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This geometric reasoning reinforces why the rules are natural.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Efficient computation: Algorithms for large determinants (LU decomposition)
    are based on row operations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical analysis: Determinant rules help detect stability and singularity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geometry: Orientation tests for polygons rely on row swap rules.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Theoretical results: Many determinant identities are derived directly from
    row operation behavior.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Determinants link algebra to geometry, but computation requires efficient methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Row operations give a hands-on toolkit: they’re the backbone of practical determinant
    computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding these rules explains why algorithms like LU factorization work
    so well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the determinant of \(\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8
    & 9 \end{bmatrix}\) using elimination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that replacing \(R_2 \to R_2 + 3R_1\) does not change the determinant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check how many sign flips occur if you reorder rows into strictly increasing
    order.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that elimination combined with these rules always leads to
    the triangular product formula.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determinants are not meant to be expanded by brute force; row operations transform
    the problem into a clear sequence of steps, connecting algebraic efficiency with
    geometric intuition.
  prefs: []
  type: TYPE_NORMAL
- en: 54\. Triangular Matrices and Product of Diagonals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Among all types of matrices, triangular matrices stand out for their simplicity.
    These are matrices where every entry either above or below the main diagonal is
    zero. What makes them especially important is that their determinants can be computed
    almost instantly: the determinant of a triangular matrix is simply the product
    of its diagonal entries. This property is not only computationally convenient,
    it also reveals deep connections between determinants, row operations, and structure
    in linear algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Triangular Matrices Defined
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A square matrix is called upper triangular if all entries below the main diagonal
    are zero, and lower triangular if all entries above the diagonal are zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Upper triangular example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ U = \begin{bmatrix} 2 & 5 & -1 \\ 0 & 3 & 4 \\ 0 & 0 & 7 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lower triangular example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ L = \begin{bmatrix} 4 & 0 & 0 \\ -2 & 5 & 0 \\ 1 & 3 & 6 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Both share the key feature: “everything off one side of the diagonal vanishes.”'
  prefs: []
  type: TYPE_NORMAL
- en: Determinant Rule
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For any triangular matrix,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(T) = \prod_{i=1}^n t_{ii}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(t_{ii}\) are the diagonal entries.
  prefs: []
  type: TYPE_NORMAL
- en: So for the upper triangular \(U\) above,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(U) = 2 \times 3 \times 7 = 42. \]
  prefs: []
  type: TYPE_NORMAL
- en: Why This Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The determinant is multilinear and alternating. When you expand it (e.g., via
    cofactor expansion), only one product of entries survives in the expansion: the
    one that picks exactly the diagonal terms.'
  prefs: []
  type: TYPE_NORMAL
- en: If you try to pick an off-diagonal entry in a row, you eventually get stuck
    with a zero entry because of the triangular shape.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only surviving term is the product of the diagonals, with sign \(+1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This elegant reasoning explains why the rule holds universally.
  prefs: []
  type: TYPE_NORMAL
- en: Connection to Row Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recall: elimination reduces any square matrix to an upper triangular form.
    Once triangular, the determinant is simply the product of the diagonals, adjusted
    for row swaps and scalings.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, triangular matrices are not just simple-they are the end goal of elimination
    algorithms for determinant computation.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In geometric terms:'
  prefs: []
  type: TYPE_NORMAL
- en: A triangular matrix represents a transformation where each coordinate direction
    depends only on itself and earlier coordinates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The determinant equals the product of scaling along each axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: In 3D, scaling x by 2, y by 3, and z by 7 gives a volume scaling of
    \(2 \cdot 3 \cdot 7 = 42\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if shear is present in the upper entries, the determinant ignores it-it
    only cares about the pure diagonal scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Efficient computation: LU decomposition reduces determinants to diagonal product
    form.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Theoretical proofs: Many determinant identities reduce to triangular cases.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical stability: Triangular matrices are well-behaved in computation, crucial
    for algorithms in numerical linear algebra.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Eigenvalues: For triangular matrices, eigenvalues are exactly the diagonal
    entries; thus determinant = product of eigenvalues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer graphics: Triangular forms simplify geometric transformations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Provides the fastest way to compute determinants in special cases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Serves as the computational foundation for general determinant algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connects determinants directly to eigenvalues and scaling factors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Illustrates how elimination transforms complexity into simplicity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the determinant of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 & 3 \\ 0 & 4 & 5 \\ 0 & 0 & 6 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '(Check: it should equal \(1 \cdot 4 \cdot 6\)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Verify that a lower triangular matrix with diagonal entries \((2, -1, 5)\) has
    determinant \(-10\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain why an upper triangular matrix with a zero on the diagonal must have
    determinant 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that every square matrix can be reduced to triangular form
    with determinant tracked by elimination steps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The triangular case reveals the heart of determinants: a product of diagonal
    scalings, stripped of all extra noise. It is the simplest lens through which determinants
    become transparent.'
  prefs: []
  type: TYPE_NORMAL
- en: '55\. The Multiplicative Property of Determinants: \(\det(AB) = \det(A)\det(B)\)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most remarkable and useful facts about determinants is that they
    multiply across matrix products. For two square matrices of the same size,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(AB) = \det(A) \cdot \det(B). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This property is fundamental: it connects algebra (matrix multiplication) with
    geometry (scaling volumes) and is essential for proofs, computations, and applications
    across mathematics, physics, and engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: The Statement in Words
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you first apply a linear transformation \(B\), and then apply \(A\), the
    total scaling of space is the product of their individual scalings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Determinants track exactly this: the signed volume change under linear transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Think of \(\det(A)\) as the signed scale factor by which \(A\) changes volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply \(B\): a unit cube becomes some parallelepiped with volume \(|\det(B)|\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Apply \(A\): the new parallelepiped scales again by \(|\det(A)|\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Total effect: volume scales by \(|\det(A)| \times |\det(B)|\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The orientation flips are also consistent: if both flip (negative determinants),
    the total orientation is preserved (positive product).'
  prefs: []
  type: TYPE_NORMAL
- en: Algebraic Reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The proof can be approached in multiple ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Row Operations and Elimination:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(A\) and \(B\) can be factored into elementary matrices (row swaps, scalings,
    replacements).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Determinants behave predictably for each operation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since both sides agree for elementary operations and determinant is multiplicative,
    the identity holds in general.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Abstract Characterization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determinants are the unique multilinear alternating functions normalized at
    the identity.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Composition of linear maps preserves this property, so multiplicativity follows.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Small Cases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '2×2 matrices:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}, \quad B = \begin{bmatrix}
    e & f \\ g & h \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute \(AB\), then \(\det(AB)\). After expansion, you find: \(\det(AB) =
    (ad - bc)(eh - fg) = \det(A)\det(B).\)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3×3 matrices: A direct computation is messy, but the property still holds and
    is consistent with elimination proofs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key Consequences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Determinant of a Power:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \det(A^k) = (\det(A))^k. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Geometric meaning: applying the same transformation \(k\) times multiplies
    volume scale repeatedly.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Inverse Matrix: If \(A\) is invertible,'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \det(A^{-1}) = \frac{1}{\det(A)}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Eigenvalues: Since \(\det(A)\) is the product of eigenvalues, this property
    matches the fact that eigenvalues multiply under matrix multiplication (when considered
    via characteristic polynomials).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Geometric Meaning in Higher Dimensions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If \(B\) scales space by 3 and flips it (det = –3), and \(A\) scales by 2 without
    flipping (det = 2), then \(AB\) scales by –6, consistent with the rule.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determinants encapsulate both magnitude (volume scaling) and sign (orientation).
    Multiplicativity ensures these combine correctly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Change of Variables in Calculus: The Jacobian determinant follows this multiplicative
    rule, ensuring transformations compose consistently.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Group Theory: \(\det\) defines a group homomorphism from the general linear
    group \(GL_n\) to the nonzero reals under multiplication.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Analysis: Determinant multiplicativity underlies LU decomposition
    methods.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: In mechanics and relativity, volume elements transform consistently
    under successive transformations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cryptography and Coding Theory: Determinants in modular arithmetic rely on
    this multiplicative property to preserve structure.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Guarantees consistency: determinants match our intuition about scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simplifies computation: determinants of factorizations can be obtained by multiplying
    smaller pieces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Provides theoretical structure: \(\det\) is a homomorphism, embedding linear
    algebra into the algebra of scalars.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Verify \(\det(AB) = \det(A)\det(B)\) for
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 0 & 3 \end{bmatrix}, \quad B = \begin{bmatrix}
    1 & 4 \\ 0 & -2 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Prove that \(\det(A^{-1}) = 1/\det(A)\) using the multiplicative rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that if \(\det(A)=0\), then \(\det(AB)=0\) for any \(B\). Explain why this
    makes sense geometrically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Using row operations, show explicitly how multiplicativity emerges
    from properties of elementary matrices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rule \(\det(AB) = \det(A)\det(B)\) transforms determinants from a mysterious
    calculation into a natural and consistent measure of how linear transformations
    combine.
  prefs: []
  type: TYPE_NORMAL
- en: 56\. Invertibility and Zero Determinant
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The determinant is more than a geometric scale factor-it is the ultimate test
    of whether a matrix is invertible. A square matrix \(A \in \mathbb{R}^{n \times
    n}\) has an inverse if and only if its determinant is nonzero. When the determinant
    vanishes, the matrix collapses space into a lower dimension, losing information
    that no transformation can undo.
  prefs: []
  type: TYPE_NORMAL
- en: The Criterion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A \text{ invertible } \iff \det(A) \neq 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: If \(\det(A) \neq 0\), the transformation stretches or shrinks space but never
    flattens it. Every output corresponds to exactly one input, so \(A^{-1}\) exists.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\det(A) = 0\), some directions are squashed into lower dimensions. Information
    is destroyed, so no inverse exists.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In 2D:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A nonzero determinant means the unit square is sent to a parallelogram with
    nonzero area.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A zero determinant means the square collapses into a line segment or a point.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 3D:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nonzero determinant → unit cube becomes a 3D parallelepiped with volume.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero determinant → cube flattens into a sheet or a line; 3D volume is lost.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Higher Dimensions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nonzero determinant preserves n-dimensional volume.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero determinant collapses dimension, destroying invertibility.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Algebraic Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The determinant is the product of eigenvalues:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \det(A) = \lambda_1 \lambda_2 \cdots \lambda_n. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If any eigenvalue is zero, then \(\det(A) = 0\) and the matrix is singular (not
    invertible).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Equivalently, a zero determinant means the matrix has linearly dependent columns
    or rows. This dependence implies redundancy: not all directions are independent,
    so the mapping cannot be one-to-one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connection with Linear Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If \(\det(A) \neq 0\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The system \(Ax = b\) has a unique solution for every \(b\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The inverse matrix \(A^{-1}\) exists and satisfies \(x = A^{-1}b\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If \(\det(A) = 0\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either no solutions (inconsistent system) or infinitely many solutions (dependent
    equations).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The mapping \(x \mapsto Ax\) cannot be reversed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Invertible vs. Singular'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad \det(A) = 5 \neq
    0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Invertible.
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = \begin{bmatrix} 2 & 4 \\ 1 & 2 \end{bmatrix}, \quad \det(B) = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Not invertible, since the second column is just twice the first.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Solving Systems: Inverse-based methods rely on nonzero determinants.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Methods: Detecting near-singularity warns of unstable solutions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geometry: A singular matrix corresponds to degenerate shapes (flattened, collapsed).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: In mechanics and relativity, invertibility ensures that transformations
    can be reversed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Non-invertible transformations crush dimensions, breaking
    rendering pipelines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Determinants provide a single scalar test for invertibility.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This connects geometry (volume collapse), algebra (linear dependence), and analysis
    (solvability of systems).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The zero/nonzero divide is one of the sharpest and most important in all of
    linear algebra.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Determine whether
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 2 \\ 3 & 6 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: is invertible. Explain both geometrically and algebraically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 0 \end{bmatrix}, \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: compute the determinant and describe the geometric transformation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Challenge: Show that if \(\det(A)=0\), the rows (or columns) of \(A\) are linearly
    dependent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The determinant acts as the ultimate yes-or-no test: nonzero means full-dimensional,
    reversible transformation; zero means collapse and irreversibility.'
  prefs: []
  type: TYPE_NORMAL
- en: 57\. Cofactor Expansion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While elimination gives a practical way to compute determinants, the cofactor
    expansion (also called Laplace expansion) offers a recursive definition that works
    for all square matrices. It expresses the determinant of an \(n \times n\) matrix
    in terms of determinants of smaller \((n-1) \times (n-1)\) matrices. This method
    reveals the internal structure of determinants and serves as a bridge between
    theory and computation.
  prefs: []
  type: TYPE_NORMAL
- en: Minors and Cofactors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The minor \(M_{ij}\) of an entry \(a_{ij}\) is the determinant of the submatrix
    obtained by deleting the \(i\)-th row and \(j\)-th column from \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The cofactor \(C_{ij}\) adds a sign factor:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ C_{ij} = (-1)^{i+j} M_{ij}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Thus each entry contributes to the determinant through its cofactor, with alternating
    signs arranged in a checkerboard pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} + & - & + & - & \cdots \\ - & + & - & + & \cdots \\ + & -
    & + & - & \cdots \\ \vdots & \vdots & \vdots & \vdots & \ddots \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The Expansion Formula
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For any row \(i\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A) = \sum_{j=1}^n a_{ij} C_{ij}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Or for any column \(j\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A) = \sum_{i=1}^n a_{ij} C_{ij}. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, the determinant can be computed by expanding along any row or column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: 3×3 Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Expanding along the first row:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A) = a \cdot \det \begin{bmatrix} e & f \\ h & i \end{bmatrix} - b \cdot
    \det \begin{bmatrix} d & f \\ g & i \end{bmatrix} + c \cdot \det \begin{bmatrix}
    d & e \\ g & h \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Simplify each 2×2 determinant:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ = a(ei - fh) - b(di - fg) + c(dh - eg). \]
  prefs: []
  type: TYPE_NORMAL
- en: This matches the familiar expansion formula for 3×3 determinants.
  prefs: []
  type: TYPE_NORMAL
- en: Why It Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Cofactor expansion follows directly from the multilinearity and alternating
    rules of determinants:'
  prefs: []
  type: TYPE_NORMAL
- en: Only one element per row and per column contributes to each term.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Signs alternate because swapping rows/columns reverses orientation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recursive expansion reduces the problem size until reaching 2×2 determinants,
    where the formula is simple.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computational Complexity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For \(n=2\), expansion is immediate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For \(n=3\), expansion is manageable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For large \(n\), expansion is very inefficient: computing \(\det(A)\) via cofactors
    requires \(O(n!)\) operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s why in practice, elimination or LU decomposition is preferred. Cofactor
    expansion is best for theory, proofs, and small matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each cofactor corresponds to excluding one direction (row/column), measuring
    the volume of the remaining sub-parallelotope. The alternating sign keeps track
    of orientation. Thus the determinant is a weighted combination of contributions
    from all entries along a chosen row or column.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Theoretical proofs: Cofactor expansion underlies many determinant identities.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Adjugate matrix: Cofactors form the adjugate used in the explicit formula for
    matrix inverses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Eigenvalues: Characteristic polynomials use cofactor expansion.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geometry: Cofactors describe signed volumes of faces of higher-dimensional
    shapes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cofactor expansion connects determinants across dimensions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a universal definition independent of row operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It explains why determinants behave consistently with volume, orientation, and
    algebraic rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Expand the determinant of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 1 & 3 \\ 0 & -1 & 4 \\ 1 & 2 & 0 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: along the first row.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the same determinant by expanding along the second column. Verify the
    result matches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that expanding along two different rows gives the same determinant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove by induction that cofactor expansion works for all \(n \times
    n\) matrices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cofactor expansion is not the fastest method, but it reveals the recursive structure
    of determinants and explains why they hold their rich algebraic and geometric
    meaning.
  prefs: []
  type: TYPE_NORMAL
- en: 58\. Permutations and the Sign of the Determinant
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Behind every determinant formula lies a hidden structure: permutations. Determinants
    can be expressed as a weighted sum over all possible ways of selecting one entry
    from each row and each column of a matrix. The weight for each selection is determined
    by the sign of the permutation used. This viewpoint reveals why determinants encode
    orientation and why their formulas alternate between positive and negative terms.'
  prefs: []
  type: TYPE_NORMAL
- en: The Permutation Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let \(S_n\) denote the set of all permutations of \(n\) elements. Each permutation
    \(\sigma \in S_n\) rearranges the numbers \(\{1, 2, \ldots, n\}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The determinant of an \(n \times n\) matrix \(A = [a_{ij}]\) is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A) = \sum_{\sigma \in S_n} \text{sgn}(\sigma) \prod_{i=1}^n a_{i, \sigma(i)}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Each product \(\prod_{i=1}^n a_{i, \sigma(i)}\) picks one entry from each row
    and each column, according to \(\sigma\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The factor \(\text{sgn}(\sigma)\) is \(+1\) if \(\sigma\) is an even permutation
    (achieved by an even number of swaps), and \(-1\) if it is odd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Permutations Appear
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A determinant requires:'
  prefs: []
  type: TYPE_NORMAL
- en: Linearity in each row.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alternating property (row swaps flip the sign).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Normalization (\(\det(I)=1\)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you expand by multilinearity, all possible combinations of choosing one
    entry per row and column arise. The alternating rule enforces that terms with
    repeated columns vanish, leaving only permutations. The sign of each permutation
    enforces the orientation flip.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: 2×2 Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two permutations in \(S_2\):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Identity \((1,2)\): sign \(+1\), contributes \(a \cdot d\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Swap \((2,1)\): sign \(-1\), contributes \(-bc\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A) = ad - bc. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: 3×3 Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'There are \(3! = 6\) permutations:'
  prefs: []
  type: TYPE_NORMAL
- en: '\((1,2,3)\): even, \(+aei\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '\((1,3,2)\): odd, \(-afh\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '\((2,1,3)\): odd, \(-bdi\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '\((2,3,1)\): even, \(+bfg\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '\((3,1,2)\): even, \(+cdh\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '\((3,2,1)\): odd, \(-ceg\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A) = aei + bfg + cdh - ceg - bdi - afh. \]
  prefs: []
  type: TYPE_NORMAL
- en: This is exactly the cofactor expansion result, but now explained as a permutation
    sum.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning of Signs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even permutations correspond to consistent orientation of basis vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Odd permutations correspond to flipped orientation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The determinant alternates signs because flipping axes reverses handedness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counting Growth
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For \(n=4\), there are \(4! = 24\) terms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For \(n=5\), \(5! = 120\) terms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In general, \(n!\) terms make this formula impractical for large matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Still, it gives the deepest definition of determinants, from which all other
    rules follow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Abstract algebra: Determinant definition via permutations works over any field.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Combinatorics: Determinants encode signed sums over permutations, connecting
    to permanents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Theoretical proofs: Many determinant properties, like multiplicativity, emerge
    cleanly from the permutation definition.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Leibniz formula: Explicit but impractical formula for computation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Advanced math: Determinants generalize to alternating multilinear forms in
    linear algebra and differential geometry.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Provides the most fundamental definition of determinants.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explains alternating signs in formulas naturally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridges algebra, geometry, and combinatorics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shows how orientation emerges from row/column arrangements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write out all 6 terms in the 3×3 determinant expansion and verify the sign of
    each permutation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the determinant of \(\begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8
    & 9 \end{bmatrix}\) using the permutation definition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that if two columns are equal, all permutation terms cancel, giving \(\det(A)=0\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that swapping two rows changes the sign of every permutation
    term, flipping the total determinant.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Determinants may look like algebraic puzzles, but the permutation formula reveals
    their true nature: a grand sum over all possible ways of matching rows to columns,
    with signs recording whether orientation is preserved or reversed.'
  prefs: []
  type: TYPE_NORMAL
- en: 59\. Cramer’s Rule
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cramer’s Rule is a classical method for solving systems of linear equations
    using determinants. While rarely used in large-scale computation due to inefficiency,
    it offers deep theoretical insights into the relationship between determinants,
    invertibility, and linear systems. It shows how the determinant of a matrix encodes
    not only volume scaling but also the exact solution to equations.
  prefs: []
  type: TYPE_NORMAL
- en: The Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a system of \(n\) linear equations with \(n\) unknowns:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Ax = b, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(A\) is an invertible \(n \times n\) matrix, \(x\) is the vector of unknowns,
    and \(b\) is the right-hand side vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cramer’s Rule states:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_i = \frac{\det(A_i)}{\det(A)}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(A_i\) is the matrix \(A\) with its \(i\)-th column replaced by \(b\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: 2×2 Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Solve:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{cases} 2x + y = 5 \\ x + 3y = 7 \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 1 & 3 \end{bmatrix}, \quad b = \begin{bmatrix}
    5 \\ 7 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Determinant of \(A\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A) = 2\cdot 3 - 1\cdot 1 = 5. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(x_1\): replace first column with \(b\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A_1 = \begin{bmatrix} 5 & 1 \\ 7 & 3 \end{bmatrix}, \quad \det(A_1) = 15
    - 7 = 8. \]
  prefs: []
  type: TYPE_NORMAL
- en: So \(x_1 = 8/5\).
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(x_2\): replace second column with \(b\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A_2 = \begin{bmatrix} 2 & 5 \\ 1 & 7 \end{bmatrix}, \quad \det(A_2) = 14
    - 5 = 9. \]
  prefs: []
  type: TYPE_NORMAL
- en: So \(x_2 = 9/5\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution: \((x,y) = (8/5, 9/5)\).'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since \(A\) is invertible,
  prefs: []
  type: TYPE_NORMAL
- en: \[ x = A^{-1}b. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'But recall the formula for the inverse:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{-1} = \frac{1}{\det(A)} \text{adj}(A), \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\text{adj}(A)\) is the adjugate (transpose of the cofactor matrix).
    When we multiply \(\text{adj}(A)b\), each component naturally becomes a determinant
    with one column replaced by \(b\). This is exactly Cramer’s Rule.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The denominator \(\det(A)\) represents the volume of the parallelotope spanned
    by the columns of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The numerator \(\det(A_i)\) represents the volume when the \(i\)-th column is
    replaced by \(b\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ratio tells how much of the volume contribution is aligned with the \(i\)-th
    direction, giving the solution coordinate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efficiency and Limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Good for small \(n\): Elegant for 2×2 or 3×3 systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inefficient for large \(n\): Requires computing \(n+1\) determinants, each
    with factorial complexity if done by cofactor expansion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Numerical instability: Determinants can be sensitive to rounding errors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, Gaussian elimination or LU decomposition is far superior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Theoretical proofs: Establishes uniqueness of solutions for small systems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geometry: Connects solutions to ratios of volumes of parallelotopes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Symbolic algebra: Useful for deriving closed-form expressions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control theory: Sometimes applied in proofs of controllability/observability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Provides a clear formula linking determinants and solutions of linear systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demonstrates the power of determinants as more than just volume measures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Acts as a conceptual bridge between algebraic solutions and geometric interpretations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Solve \(\begin{cases} x + 2y = 3 \\ 4x + 5y = 6 \end{cases}\) using Cramer’s
    Rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the 3×3 system with matrix \(\begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 5
    & 6 & 0 \end{bmatrix}\), compute \(x_1\) using Cramer’s Rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that when \(\det(A)=0\), Cramer’s Rule breaks down, matching the fact
    that the system is either inconsistent or has infinitely many solutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Derive Cramer’s Rule from the adjugate matrix formula.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cramer’s Rule is not a computational workhorse, but it elegantly ties together
    determinants, invertibility, and the solution of linear systems-showing how geometry,
    algebra, and computation meet in one neat formula.
  prefs: []
  type: TYPE_NORMAL
- en: 60\. Computing Determinants in Practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Determinants carry deep meaning, but when it comes to actual computation, the
    method you choose makes all the difference. For small matrices, formulas like
    cofactor expansion or Cramer’s Rule are manageable. For larger systems, however,
    these direct approaches quickly become inefficient. Practical computation relies
    on systematic algorithms that exploit structure-especially elimination and matrix
    factorizations.
  prefs: []
  type: TYPE_NORMAL
- en: Small Matrices (n ≤ 3)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '2×2 case:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \det \begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3×3 case: Either expand by cofactors or use the “rule of Sarrus”:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \det \begin{bmatrix} a & b & c \\ d & e & f \\ g & h & i \end{bmatrix} =
    aei + bfg + cdh - ceg - bdi - afh. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These formulas are compact, but do not generalize well beyond \(3 \times 3\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Matrices: Elimination and LU Decomposition'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For \(n > 3\), practical methods revolve around Gaussian elimination.
  prefs: []
  type: TYPE_NORMAL
- en: 'Row Reduction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce \(A\) to an upper triangular matrix \(U\) using row operations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keep track of operations:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Row swaps → flip sign of determinant.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Row scaling → multiply determinant by the scaling factor.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Row replacements → no effect.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once triangular, compute determinant as the product of diagonal entries.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LU Factorization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Express \(A = LU\), where \(L\) is lower triangular and \(U\) is upper triangular.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then \(\det(A) = \det(L)\det(U)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since \(L\) has 1s on its diagonal, \(\det(L)=1\), so the determinant is just
    the product of diagonals of \(U\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This approach reduces the complexity to \(O(n^3)\), far more efficient than
    the factorial growth of cofactor expansion.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical Considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Floating-Point Stability: Determinants can be very large or very small, leading
    to overflow or underflow in computers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pivoting: In practice, partial pivoting ensures stability during elimination.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Condition Number: If a matrix is nearly singular (\(\det(A)\) close to 0),
    computed determinants may be highly inaccurate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these reasons, in numerical linear algebra, determinants are rarely computed
    directly; instead, properties of LU or QR factorizations are used.
  prefs: []
  type: TYPE_NORMAL
- en: Determinant via Eigenvalues
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since the determinant equals the product of eigenvalues,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A) = \lambda_1 \lambda_2 \cdots \lambda_n, \]
  prefs: []
  type: TYPE_NORMAL
- en: it can be computed by finding eigenvalues (numerically via QR iteration or other
    methods). This is useful when eigenvalues are already needed, but computing them
    just for the determinant is often more expensive than elimination.
  prefs: []
  type: TYPE_NORMAL
- en: Special Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Diagonal or triangular matrices: Determinant is product of diagonals-fastest
    case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Block diagonal matrices: Determinant is the product of determinants of blocks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparse matrices: Exploit structure-only nonzero patterns matter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Orthogonal matrices: Determinant is always \(+1\) or \(-1\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'System solving: Determinants test invertibility, but actual solving uses elimination.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer graphics: Determinants detect orientation flips (useful for rendering).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimization: Determinants of Hessians signal curvature and stability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Statistics: Determinants of covariance matrices measure uncertainty volumes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Determinants appear in Jacobians for change of variables in integrals.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Determinants provide a global property of matrices, but computation must be
    efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Direct expansion is elegant but impractical.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elimination-based methods balance theory, speed, and reliability, forming the
    backbone of modern computational linear algebra.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the determinant of \(\begin{bmatrix} 2 & 1 & 3 \\ 4 & 1 & 7 \\ -2 &
    5 & 1 \end{bmatrix}\) using elimination, confirming the diagonal product method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a diagonal matrix with entries \((2, 3, -1, 5)\), verify that the determinant
    is simply their product.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use LU decomposition to compute the determinant of a \(3 \times 3\) matrix of
    your choice.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that determinant computation by LU requires only \(O(n^3)\)
    operations, while cofactor expansion requires \(O(n!)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determinants are central, but in practice they are best approached with systematic
    algorithms, where triangular forms and factorizations reveal the answer quickly
    and reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 7\. Eigenvalues, eigenvectors, and dynamics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opening
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 61\. Eigenvalues and Eigenvectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Among all the concepts in linear algebra, few are as central and powerful as
    eigenvalues and eigenvectors. They reveal the hidden “axes of action” of a linear
    transformation-directions in space where the transformation behaves in the simplest
    possible way. Instead of mixing and rotating everything, an eigenvector is left
    unchanged in direction, scaled only by its corresponding eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: The Core Idea
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let \(A\) be an \(n \times n\) matrix. A nonzero vector \(v \in \mathbb{R}^n\)
    is called an eigenvector of \(A\) if
  prefs: []
  type: TYPE_NORMAL
- en: \[ Av = \lambda v, \]
  prefs: []
  type: TYPE_NORMAL
- en: for some scalar \(\lambda \in \mathbb{R}\) (or \(\mathbb{C}\)). The scalar \(\lambda\)
    is the eigenvalue corresponding to \(v\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvector: A special direction that is preserved by the transformation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eigenvalue: The factor by which the eigenvector is stretched or compressed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\lambda > 1\), the eigenvector is stretched. If \(0 < \lambda < 1\), it
    is compressed. If \(\lambda < 0\), it is flipped in direction and scaled. If \(\lambda
    = 0\), the vector is flattened to zero.
  prefs: []
  type: TYPE_NORMAL
- en: Why They Matter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Eigenvalues and eigenvectors describe the intrinsic structure of a transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: They give preferred directions in which the action of the matrix is simplest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They summarize long-term behavior of repeated applications (e.g., powers of
    \(A\)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They connect algebra, geometry, and applications in physics, data science, and
    engineering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: A Simple 2D Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying \(A\) to \((1,0)\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 2 \\ 0 \end{bmatrix}
    = 2 \begin{bmatrix} 1 \\ 0 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So \((1,0)\) is an eigenvector with eigenvalue \(2\).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Applying \(A\) to \((0,1)\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 0 \\ 3 \end{bmatrix}
    = 3 \begin{bmatrix} 0 \\ 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So \((0,1)\) is an eigenvector with eigenvalue \(3\).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here the eigenvectors align with the coordinate axes, and the eigenvalues are
    the diagonal entries.
  prefs: []
  type: TYPE_NORMAL
- en: 'General Case: The Eigenvalue Equation'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To find eigenvalues, we solve
  prefs: []
  type: TYPE_NORMAL
- en: \[ Av = \lambda v \quad \Leftrightarrow \quad (A - \lambda I)v = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For nontrivial \(v\), the matrix \((A - \lambda I)\) must be singular:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A - \lambda I) = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: This determinant expands to the characteristic polynomial, whose roots are the
    eigenvalues. Eigenvectors come from solving the corresponding null spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Eigenvectors are invariant directions. When you apply \(A\), the vector may
    stretch or flip, but it does not rotate off its line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalues are scaling factors. They describe how much stretching, shrinking,
    or flipping happens along that invariant direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: In 2D, an eigenvector might be a line through the origin where the transformation
    acts as a stretch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In 3D, planes of shear often have eigenvectors along axes of invariance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamics and Repeated Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One reason eigenvalues are so important is that they describe repeated transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^k v = \lambda^k v. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'If you apply \(A\) repeatedly to an eigenvector, the result is predictable:
    just multiply by \(\lambda^k\). This explains stability in dynamical systems,
    growth in population models, and convergence in Markov chains.'
  prefs: []
  type: TYPE_NORMAL
- en: If \(|\lambda| < 1\), repeated applications shrink the vector to zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(|\lambda| > 1\), the vector grows without bound.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\lambda = 1\), the vector stays the same length (though direction may flip
    if \(\lambda=-1\)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Physics: Vibrations of molecules, quantum energy levels, and resonance all
    rely on eigenvalues/eigenvectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Science: Principal Component Analysis (PCA) finds eigenvectors of covariance
    matrices to detect key directions of variance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Markov Chains: Steady-state probabilities correspond to eigenvectors with eigenvalue
    1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Differential Equations: Eigenvalues simplify systems of linear ODEs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Transformations like rotations and scalings can be analyzed
    with eigen-decompositions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Eigenvalues and eigenvectors reduce complex transformations to their simplest
    components.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They unify algebra (roots of characteristic polynomials), geometry (invariant
    directions), and applications (stability, resonance, variance).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are the foundation for diagonalization, SVD, and spectral analysis, which
    dominate modern applied mathematics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the eigenvalues and eigenvectors of \(\begin{bmatrix} 4 & 2 \\ 1 & 3
    \end{bmatrix}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For \(A = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}\), find its eigenvalues.
    (Hint: they are complex.)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a random 2×2 matrix and check if its eigenvectors align with coordinate
    axes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that eigenvectors corresponding to distinct eigenvalues are
    linearly independent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Eigenvalues and eigenvectors are the “fingerprints” of a matrix: they capture
    the essential behavior of a transformation, guiding us to understand stability,
    dynamics, and structure across countless disciplines.'
  prefs: []
  type: TYPE_NORMAL
- en: 62\. The Characteristic Polynomial
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To uncover the eigenvalues of a matrix, we use a central tool: the characteristic
    polynomial. This polynomial encodes the relationship between a matrix and its
    eigenvalues. The roots of the polynomial are precisely the eigenvalues, making
    it the algebraic gateway to spectral analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For a square matrix \(A \in \mathbb{R}^{n \times n}\), the characteristic polynomial
    is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_A(\lambda) = \det(A - \lambda I). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(I\) is the identity matrix of the same size as \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The polynomial \(p_A(\lambda)\) has degree \(n\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The eigenvalues of \(A\) are exactly the roots of \(p_A(\lambda)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why This Works
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The eigenvalue equation is
  prefs: []
  type: TYPE_NORMAL
- en: \[ Av = \lambda v \quad \iff \quad (A - \lambda I)v = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For nontrivial \(v\), the matrix \(A - \lambda I\) must be singular:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \det(A - \lambda I) = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, eigenvalues are precisely the scalars \(\lambda\) for which the determinant
    vanishes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: 2×2 Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 4 & 2 \\ 1 & 3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_A(\lambda) = \det \begin{bmatrix} 4-\lambda & 2 \\ 1 & 3-\lambda \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Expanding:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_A(\lambda) = (4-\lambda)(3-\lambda) - 2. \]
  prefs: []
  type: TYPE_NORMAL
- en: \[ = \lambda^2 - 7\lambda + 10. \]
  prefs: []
  type: TYPE_NORMAL
- en: The roots are \(\lambda = 5\) and \(\lambda = 2\). These are the eigenvalues
    of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: 3×3 Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 3 & 4 \\ 0 & 4 & 9 \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_B(\lambda) = \det \begin{bmatrix} 2-\lambda & 0 & 0 \\ 0 & 3-\lambda &
    4 \\ 0 & 4 & 9-\lambda \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Expand:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_B(\lambda) = (2-\lambda)\big[(3-\lambda)(9-\lambda) - 16\big]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \[ = (2-\lambda)(\lambda^2 - 12\lambda + 11). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Roots: \(\lambda = 2, 1, 11\).'
  prefs: []
  type: TYPE_NORMAL
- en: Properties of the Characteristic Polynomial
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Degree: Always degree \(n\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Leading term: \((-1)^n \lambda^n\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Constant term: \(\det(A)\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Coefficient of \(\lambda^{n-1}\): \(-\text{tr}(A)\), where \(\text{tr}(A)\)
    is the trace (sum of diagonal entries).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_A(\lambda) = (-1)^n \lambda^n + (\text{tr}(A))(-1)^{n-1}\lambda^{n-1} +
    \cdots + \det(A). \]
  prefs: []
  type: TYPE_NORMAL
- en: This ties together trace, determinant, and eigenvalues in one polynomial.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The roots of the characteristic polynomial tell us scaling factors along invariant
    directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In 2D: the polynomial encodes area scaling (\(\det(A)\)) and total stretching
    (\(\text{tr}(A)\)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In higher dimensions: it condenses the complexity of \(A\) into a single equation
    whose solutions reveal the spectrum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Eigenvalue computation: Foundation for diagonalization and spectral theory.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control theory: Stability of systems depends on eigenvalues (roots of the characteristic
    polynomial).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Differential equations: Characteristic polynomials describe natural frequencies
    and modes of oscillation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Graph theory: The characteristic polynomial of an adjacency matrix encodes
    structural properties of the graph.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantum mechanics: Energy levels of quantum systems come from solving characteristic
    polynomials of operators.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Provides a systematic, algebraic way to find eigenvalues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connects trace and determinant to deeper spectral properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bridges linear algebra, polynomial theory, and geometry.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forms the foundation for modern computational methods like QR iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the characteristic polynomial of \(\begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}\).
    Find its eigenvalues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the product of eigenvalues equals the determinant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the sum of eigenvalues equals the trace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that \(p_{AB}(\lambda) = p_{BA}(\lambda)\) for any \(A, B\)
    of the same size.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The characteristic polynomial distills a matrix into a single algebraic object
    whose roots reveal the essential dynamics of the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 63\. Algebraic vs. Geometric Multiplicity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When studying eigenvalues, it’s not enough to just find the roots of the characteristic
    polynomial. Each eigenvalue can appear multiple times, and this “multiplicity”
    can be understood in two distinct but related ways: algebraic multiplicity (how
    many times it appears as a root) and geometric multiplicity (the dimension of
    its eigenspace). These two multiplicities capture both the algebraic and geometric
    richness of eigenvalues.'
  prefs: []
  type: TYPE_NORMAL
- en: Algebraic Multiplicity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The algebraic multiplicity (AM) of an eigenvalue \(\lambda\) is the number of
    times it appears as a root of the characteristic polynomial \(p_A(\lambda)\).
  prefs: []
  type: TYPE_NORMAL
- en: If \((\lambda - \lambda_0)^k\) divides \(p_A(\lambda)\), then the algebraic
    multiplicity of \(\lambda_0\) is \(k\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of all algebraic multiplicities equals the size of the matrix (\(n\)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: If'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_A(\lambda) = (\lambda-2)^3(\lambda+1)^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: then eigenvalue \(\lambda=2\) has AM = 3, and \(\lambda=-1\) has AM = 2.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Multiplicity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The geometric multiplicity (GM) of an eigenvalue \(\lambda\) is the dimension
    of the eigenspace corresponding to \(\lambda\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{GM}(\lambda) = \dim(\ker(A - \lambda I)). \]
  prefs: []
  type: TYPE_NORMAL
- en: This counts how many linearly independent eigenvectors correspond to \(\lambda\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Always satisfies:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ 1 \leq \text{GM}(\lambda) \leq \text{AM}(\lambda). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Example: If'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: then \(p_A(\lambda) = (\lambda-2)^2\).
  prefs: []
  type: TYPE_NORMAL
- en: AM of \(\lambda=2\) is 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solve \((A-2I)v=0\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} v = 0 \quad \Rightarrow \quad
    v = \begin{bmatrix} 1 \\ 0 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Only 1 independent eigenvector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: GM of \(\lambda=2\) is 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relationship Between the Two
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Always: \(\text{GM}(\lambda) \leq \text{AM}(\lambda)\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If they are equal for all eigenvalues, the matrix is diagonalizable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If GM < AM for some eigenvalue, the matrix is defective, meaning it cannot be
    diagonalized, though it may still have a Jordan canonical form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AM measures how strongly the eigenvalue is “encoded” in the polynomial.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GM measures how much geometric freedom the eigenvalue’s eigenspace provides.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If AM > GM, the eigenvalue “wants” more independent directions than the space
    allows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think of AM as the *theoretical demand* for eigenvectors, and GM as the *actual
    supply*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Diagonalizable vs. Defective'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Diagonalizable case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ B = \begin{bmatrix} 2 & 0 \\ 0 & 2 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \(p_B(\lambda) = (\lambda-2)^2\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: AM = 2 for eigenvalue 2.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: GM = 2, since the eigenspace is all of \(\mathbb{R}^2\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Enough eigenvectors to diagonalize.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Defective case: The earlier example'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: had AM = 2, GM = 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Not enough eigenvectors.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cannot be diagonalized.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Diagonalization: Only possible when GM = AM for all eigenvalues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Jordan form: Defective matrices require Jordan blocks, governed by the gap
    between AM and GM.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Differential equations: The solution form depends on multiplicity; repeated
    eigenvalues with fewer eigenvectors require generalized solutions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stability analysis: Multiplicities reveal degeneracies in dynamical systems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantum mechanics: Degeneracy of eigenvalues (AM vs. GM) encodes physical symmetry.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multiplicities separate algebraic roots from geometric structure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They decide whether diagonalization is possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They reveal hidden constraints in systems with repeated eigenvalues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They form the basis for advanced concepts like Jordan canonical form and generalized
    eigenvectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Find AM and GM for \(\begin{bmatrix} 3 & 1 \\ 0 & 3 \end{bmatrix}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find AM and GM for \(\begin{bmatrix} 3 & 0 \\ 0 & 3 \end{bmatrix}\). Compare
    with the first case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that AM always equals the multiplicity of a root of the characteristic
    polynomial.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that for any eigenvalue, GM ≥ 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Algebraic and geometric multiplicity together tell the full story: the algebra
    tells us how many times an eigenvalue appears, while the geometry tells us how
    much room it really occupies in the vector space.'
  prefs: []
  type: TYPE_NORMAL
- en: 64\. Diagonalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Diagonalization is one of the most powerful ideas in linear algebra. It takes
    a complicated matrix and, when possible, rewrites it in a simple form where its
    action is completely transparent. A diagonal matrix is easy to understand: it
    just stretches or compresses each coordinate axis by a fixed factor. If we can
    transform a matrix into a diagonal one, many calculations-like computing powers
    or exponentials-become almost trivial.'
  prefs: []
  type: TYPE_NORMAL
- en: The Core Concept
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A square matrix \(A \in \mathbb{R}^{n \times n}\) is diagonalizable if there
    exists an invertible matrix \(P\) and a diagonal matrix \(D\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = P D P^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The diagonal entries of \(D\) are the eigenvalues of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The columns of \(P\) are the corresponding eigenvectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In words: \(A\) can be “rewritten” in a coordinate system made of its eigenvectors,
    where its action reduces to simple scaling along independent directions.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Diagonalization Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Simplifies Computations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computing powers:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A^k = P D^k P^{-1}, \quad D^k \text{ is trivial to compute}. \]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Matrix exponential:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ e^A = P e^D P^{-1}. \]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Critical in solving differential equations.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Clarifies Dynamics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Long-term behavior of iterative processes depends directly on eigenvalues.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Stable vs. unstable systems can be read off from \(D\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reveals Structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tells us whether the system can be understood through independent modes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Connects algebraic structure with geometry.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Conditions for Diagonalization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A matrix \(A\) is diagonalizable if and only if it has enough linearly independent
    eigenvectors to form a basis for \(\mathbb{R}^n\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Equivalently: For each eigenvalue, geometric multiplicity = algebraic multiplicity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinct eigenvalues guarantee diagonalizability, since their eigenvectors are
    linearly independent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Diagonalizable Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 4 & 0 \\ 1 & 3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Characteristic polynomial:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ p_A(\lambda) = (4-\lambda)(3-\lambda). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Eigenvalues: \(\lambda_1=4, \lambda_2=3\).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Eigenvectors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For \(\lambda=4\): \((1,1)^T\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For \(\lambda=3\): \((0,1)^T\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Build \(P = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}\), \(D = \begin{bmatrix}
    4 & 0 \\ 0 & 3 \end{bmatrix}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then \(A = P D P^{-1}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, computing \(A^{10}\) is easy: just compute \(D^{10}\) and conjugate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Defective (Non-Diagonalizable) Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ B = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Characteristic polynomial: \((\lambda - 2)^2\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AM of eigenvalue 2 is 2, but GM = 1 (only one eigenvector).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not diagonalizable. Needs Jordan form instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Diagonalization means we can rotate into a basis of eigenvectors where the
    transformation acts simply: scale each axis by its eigenvalue.'
  prefs: []
  type: TYPE_NORMAL
- en: Think of a room where the floor stretches more in one direction than another.
    In the right coordinate system (aligned with eigenvectors), the stretch is purely
    along axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without diagonalization, stretching mixes directions and is harder to describe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Differential Equations: Solving systems of linear ODEs relies on diagonalization
    or Jordan form.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Markov Chains: Transition matrices are analyzed through diagonalization to
    study steady states.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantum Mechanics: Operators are diagonalized to reveal measurable states.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'PCA (Principal Component Analysis): A covariance matrix is diagonalized to
    extract independent variance directions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Diagonalization simplifies rotation-scaling transformations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Diagonalization transforms complexity into simplicity. It exposes the fundamental
    action of a matrix: scaling along preferred axes. Without it, understanding or
    computing repeated transformations would be intractable.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Diagonalize
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ C = \begin{bmatrix} 1 & 1 \\ 0 & 2 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute \(C^5\) using \(P D^5 P^{-1}\).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Show why
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: cannot be diagonalized.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Challenge: Prove that any symmetric real matrix is diagonalizable with an orthogonal
    basis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Diagonalization is like finding the natural “language” of a matrix: once we
    listen in its native basis, everything becomes clear, elegant, and simple.'
  prefs: []
  type: TYPE_NORMAL
- en: 65\. Powers of a Matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we know about diagonalization, one of its most powerful consequences is
    the ability to compute powers of a matrix efficiently. Normally, multiplying a
    matrix by itself repeatedly is expensive and messy. But if a matrix can be diagonalized,
    its powers become almost trivial to calculate. This is crucial in understanding
    long-term behavior of dynamical systems, Markov chains, and iterative algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The General Principle
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If a matrix \(A\) is diagonalizable, then
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = P D P^{-1}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(D\) is diagonal and \(P\) is invertible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then for any positive integer \(k\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^k = (P D P^{-1})^k = P D^k P^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(P^{-1}P = I\), the middle terms cancel out in the product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing \(D^k\) is simple: just raise each diagonal entry to the \(k\)-th
    power.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, eigenvalues control the growth or decay of powers of the matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: A Simple Diagonal Case'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ D = \begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ D^k = \begin{bmatrix} 2^k & 0 \\ 0 & 3^k \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Each eigenvalue is raised independently to the \(k\)-th power.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Using Diagonalization'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 4 & 0 \\ 1 & 3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: From before, we know it diagonalizes as
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = P D P^{-1}, \quad D = \begin{bmatrix} 4 & 0 \\ 0 & 3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So,
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^k = P \begin{bmatrix} 4^k & 0 \\ 0 & 3^k \end{bmatrix} P^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Instead of multiplying \(A\) by itself \(k\) times, we just exponentiate the
    eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: Long-Term Behavior
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Eigenvalues reveal exactly what happens as \(k \to \infty\).
  prefs: []
  type: TYPE_NORMAL
- en: If all eigenvalues satisfy \(|\lambda| < 1\), then \(A^k \to 0\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If some eigenvalues have \(|\lambda| > 1\), then \(A^k\) diverges along those
    eigenvector directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If \(|\lambda| = 1\), the behavior depends on the specific structure: it may
    oscillate, stabilize, or remain bounded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This explains stability in recursive systems and iterative algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Special Case: Markov Chains'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In probability, the transition matrix of a Markov chain has eigenvalues less
    than or equal to 1.
  prefs: []
  type: TYPE_NORMAL
- en: The largest eigenvalue is always \(\lambda = 1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As powers of the transition matrix grow, the chain converges to the eigenvector
    associated with \(\lambda = 1\), representing the stationary distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, \(A^k\) describes the long-run behavior of the chain.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Diagonalizable Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If a matrix is not diagonalizable, things become more complicated. Such matrices
    require the Jordan canonical form, where blocks can lead to terms like \(k \lambda^{k-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ B^k = \begin{bmatrix} 2^k & k 2^{k-1} \\ 0 & 2^k \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The presence of the off-diagonal entry introduces linear growth in \(k\), in
    addition to exponential scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Powers of \(A\) correspond to repeated application of the linear transformation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalues dictate whether directions expand, shrink, or remain steady.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The eigenvectors mark the axes along which the repeated action is simplest to
    describe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Think of stretching a rubber sheet: after each stretch, the sheet aligns more
    and more strongly with the dominant eigenvector.'
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Dynamical Systems: Population models, economic growth, and iterative algorithms
    all rely on powers of a matrix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Markov Chains: Powers reveal equilibrium behavior and mixing rates.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Differential Equations: Discrete-time models use matrix powers to describe
    state evolution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Repeated transformations can be analyzed via eigenvalues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Convergence of iterative solvers (like gradient descent with
    linear updates) depends on spectral radius.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Matrix powers are the foundation of stability analysis, asymptotic behavior,
    and convergence. Diagonalization turns this from a brute-force multiplication
    into a deep, structured understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute \(A^5\) for \(\begin{bmatrix} 2 & 0 \\ 0 & 3 \end{bmatrix}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For \(\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\), compute \(A^k\). What
    happens as \(k \to \infty\)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore what happens to \(A^k\) when the largest eigenvalue has absolute value
    < 1, = 1, and > 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that if a diagonalizable matrix has eigenvalues \(|\lambda_i|
    < 1\), then \(\lim_{k \to \infty} A^k = 0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Powers of a matrix reveal the story of repetition: how a transformation evolves
    when applied again and again. They connect linear algebra to time, growth, and
    stability in every system that unfolds step by step.'
  prefs: []
  type: TYPE_NORMAL
- en: 66\. Real vs. Complex Spectra
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not all eigenvalues are real numbers. Even when working with real matrices,
    eigenvalues can emerge as complex numbers. Understanding when eigenvalues are
    real, when they are complex, and what this means geometrically is critical for
    grasping the full behavior of linear transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvalues Over the Complex Numbers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Every square matrix \(A \in \mathbb{R}^{n \times n}\) has at least one eigenvalue
    in the complex numbers. This is guaranteed by the Fundamental Theorem of Algebra,
    which says every polynomial (like the characteristic polynomial) has roots in
    \(\mathbb{C}\).
  prefs: []
  type: TYPE_NORMAL
- en: If \(p_A(\lambda)\) has only real roots, all eigenvalues are real.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(p_A(\lambda)\) has quadratic factors with no real roots, then eigenvalues
    appear as complex conjugate pairs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Complex Numbers Appear
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a 2D rotation matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R_\theta = \begin{bmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta
    \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The characteristic polynomial is
  prefs: []
  type: TYPE_NORMAL
- en: \[ p(\lambda) = \lambda^2 - 2\cos\theta \lambda + 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda = \cos\theta \pm i \sin\theta = e^{\pm i\theta}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Unless \(\theta = 0, \pi\), these eigenvalues are not real.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geometrically, this makes sense: pure rotation has no invariant real direction.
    Instead, the eigenvalues are complex numbers of unit magnitude, encoding the rotation
    angle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real vs. Complex Scenarios
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Symmetric Real Matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All eigenvalues are real.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvectors form an orthogonal basis.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: \(\begin{bmatrix} 2 & 1 \\ 1 & 2 \end{bmatrix}\) has eigenvalues \(3,
    1\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'General Real Matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eigenvalues may be complex.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If complex, they always come in conjugate pairs: if \(\lambda = a+bi\), then
    \(\overline{\lambda} = a-bi\) is also an eigenvalue.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Skew-Symmetric Matrices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Purely imaginary eigenvalues.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: \(\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\) has eigenvalues
    \(\pm i\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Meaning of Complex Eigenvalues
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If eigenvalues are real, the transformation scales along real directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If eigenvalues are complex, the transformation involves a combination of rotation
    and scaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For \(\lambda = re^{i\theta}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \(r = |\lambda|\) controls expansion or contraction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\theta\) controls rotation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So a complex eigenvalue represents a spiral: stretching or shrinking while
    rotating.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Spiral Dynamics'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix} \]
  prefs: []
  type: TYPE_NORMAL
- en: rotates vectors by 90°.
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalues: \(\pm i\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Magnitude = 1, angle = \(\pi/2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpretation: every step is a rotation of 90°, with no scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we change to
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = \begin{bmatrix} 0.8 & -0.6 \\ 0.6 & 0.8 \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: the eigenvalues are complex with modulus < 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interpretation: rotation combined with shrinking → spiraling toward the origin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Differential Equations: Complex eigenvalues produce oscillatory solutions with
    sine and cosine terms.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Vibrations and wave phenomena rely on complex eigenvalues to model
    periodic behavior.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control Systems: Stability requires checking magnitudes of eigenvalues in the
    complex plane.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Rotations and spiral motions are naturally described by
    complex spectra.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Signal Processing: Fourier transforms rely on complex eigenstructures of convolution
    operators.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Real eigenvalues describe pure stretching or compression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex eigenvalues describe combined rotation and scaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, they provide a complete picture of matrix behavior in both real and
    complex spaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without considering complex eigenvalues, we miss entire classes of transformations,
    like rotation and oscillation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Find eigenvalues of \(\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}\). Interpret
    geometrically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For rotation by 45°, find eigenvalues of \(\begin{bmatrix} \cos\frac{\pi}{4}
    & -\sin\frac{\pi}{4} \\ \sin\frac{\pi}{4} & \cos\frac{\pi}{4} \end{bmatrix}\).
    Show that they are \(e^{\pm i\pi/4}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check eigenvalues of \(\begin{bmatrix} 2 & -5 \\ 1 & -2 \end{bmatrix}\). Are
    they real or complex?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that real polynomials of odd degree always have at least one
    real root. Connect this to eigenvalues of odd-dimensional real matrices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Complex spectra extend our understanding of linear algebra into the full richness
    of oscillations, rotations, and spirals, where numbers alone are not enough-geometry
    and complex analysis merge to reveal the truth.
  prefs: []
  type: TYPE_NORMAL
- en: 67\. Defective Matrices and Jordan Form (a Glimpse)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not every matrix can be simplified all the way into a diagonal form. Some matrices,
    while having repeated eigenvalues, do not have enough independent eigenvectors
    to span the entire space. These are called defective matrices. Understanding them
    requires introducing the Jordan canonical form, a generalization of diagonalization
    that handles these tricky cases.
  prefs: []
  type: TYPE_NORMAL
- en: Defective Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A square matrix \(A \in \mathbb{R}^{n \times n}\) is called defective if:'
  prefs: []
  type: TYPE_NORMAL
- en: It has an eigenvalue \(\lambda\) with algebraic multiplicity (AM) strictly larger
    than its geometric multiplicity (GM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Equivalently, \(A\) does not have enough linearly independent eigenvectors to
    form a full basis of \(\mathbb{R}^n\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Characteristic polynomial: \((\lambda - 2)^2\), so AM = 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Solving \((A - 2I)v = 0\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}v = 0 \quad \Rightarrow \quad
    v = \begin{bmatrix} 1 \\ 0 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Only one independent eigenvector → GM = 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since GM < AM, this matrix is defective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defective matrices cannot be diagonalized.
  prefs: []
  type: TYPE_NORMAL
- en: Why Defective Matrices Exist
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Diagonalization requires one independent eigenvector per eigenvalue copy. But
    sometimes the matrix “collapses” those directions together, producing fewer eigenvectors
    than expected.
  prefs: []
  type: TYPE_NORMAL
- en: Think of it like having multiple musical notes written in the score (AM), but
    fewer instruments available to play them (GM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The matrix “wants” more independent directions, but the geometry of its null
    spaces prevents that.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jordan Canonical Form (Intuition)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While defective matrices cannot be diagonalized, they can still be put into
    a nearly diagonal form called the Jordan canonical form (JCF):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = P^{-1} A P, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(J\) consists of Jordan blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_k(\lambda) = \begin{bmatrix} \lambda & 1 & 0 & \cdots & 0 \\ 0 & \lambda
    & 1 & \cdots & 0 \\ 0 & 0 & \lambda & \cdots & 0 \\ \vdots & \vdots & \vdots &
    \ddots & 1 \\ 0 & 0 & 0 & \cdots & \lambda \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Each block corresponds to one eigenvalue \(\lambda\), with 1s on the superdiagonal
    indicating the lack of independent eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: If every block is \(1 \times 1\), the matrix is diagonalizable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If larger blocks appear, the matrix is defective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Jordan Block of Size 2'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The earlier defective example
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix} \]
  prefs: []
  type: TYPE_NORMAL
- en: has Jordan form
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \begin{bmatrix} 2 & 1 \\ 0 & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice it is already in Jordan form: one block of size 2 for eigenvalue 2.'
  prefs: []
  type: TYPE_NORMAL
- en: Powers of Jordan Blocks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A key property is how powers behave. For
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \begin{bmatrix} \lambda & 1 \\ 0 & \lambda \end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: \[ J^k = \begin{bmatrix} \lambda^k & k\lambda^{k-1} \\ 0 & \lambda^k \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Unlike diagonal matrices, extra polynomial terms in \(k\) appear.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This explains why defective matrices produce behavior like growth proportional
    to \(k \lambda^{k-1}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Eigenvectors describe invariant lines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When there aren’t enough eigenvectors, Jordan form encodes chains of generalized
    eigenvectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each chain captures how the matrix transforms vectors slightly off the invariant
    line, nudging them along directions linked together by the Jordan block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So while a diagonalizable matrix decomposes space into neat independent directions,
    a defective matrix entangles some directions together, forcing them into chains.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Differential Equations: Jordan blocks determine the appearance of extra polynomial
    factors (like \(t e^{\lambda t}\)) in solutions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Markov Chains: Non-diagonalizable transition matrices produce slower convergence
    to steady states.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Analysis: Algorithms may fail or slow down if the system matrix is
    defective.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control Theory: Stability depends not just on eigenvalues, but on whether the
    matrix is diagonalizable.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantum Mechanics: Degenerate eigenvalues require Jordan analysis to fully
    describe states.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Diagonalization is not always possible, and defective matrices are the exceptions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jordan form is the universal fallback: every square matrix has one, and it
    generalizes diagonalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It introduces generalized eigenvectors, which extend the reach of spectral theory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Verify that \(\begin{bmatrix} 3 & 1 \\ 0 & 3 \end{bmatrix}\) is defective. Find
    its Jordan form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that for a Jordan block of size 3,
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ J^k = \lambda^k I + k \lambda^{k-1} N + \frac{k(k-1)}{2}\lambda^{k-2} N^2,
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where \(N\) is the nilpotent part (matrix with 1s above diagonal).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compare the behavior of \(A^k\) for a diagonalizable vs. a defective matrix
    with the same eigenvalues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that every square matrix has a Jordan form over the complex
    numbers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defective matrices and Jordan form show us that even when eigenvectors are “insufficient,”
    we can still impose structure, capturing how linear transformations behave in
    their most fundamental building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 68\. Stability and Spectral Radius
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a matrix is applied repeatedly-through iteration, recursion, or dynamical
    systems-its long-term behavior is governed not by individual entries, but by its
    eigenvalues. The key measure here is the spectral radius, which tells us whether
    repeated applications lead to convergence, oscillation, or divergence.
  prefs: []
  type: TYPE_NORMAL
- en: The Spectral Radius
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The spectral radius of a matrix \(A\) is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \rho(A) = \max \{ |\lambda| : \lambda \text{ is an eigenvalue of } A \}.
    \]'
  prefs: []
  type: TYPE_NORMAL
- en: It is the largest absolute value among all eigenvalues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(|\lambda| > 1\), the eigenvalue leads to exponential growth along its eigenvector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(|\lambda| < 1\), it leads to exponential decay.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(|\lambda| = 1\), behavior depends on whether the eigenvalue is simple or
    defective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stability in Iterative Systems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a recursive process:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_{k+1} = A x_k. \]
  prefs: []
  type: TYPE_NORMAL
- en: If \(\rho(A) < 1\), then \(A^k \to 0\) as \(k \to \infty\). All trajectories
    converge to the origin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\rho(A) > 1\), then \(A^k\) grows without bound along the dominant eigenvector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\rho(A) = 1\), trajectories neither vanish nor diverge but may oscillate
    or stagnate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Convergence with Small Spectral Radius'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 0.5 & 0 \\ 0 & 0.8 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalues: \(0.5, 0.8\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\rho(A) = 0.8 < 1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powers \(A^k\) shrink vectors to zero → stable system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Divergence with Large Spectral Radius'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ B = \begin{bmatrix} 2 & 0 \\ 0 & 0.5 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalues: \(2, 0.5\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\rho(B) = 2 > 1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powers \(B^k\) explode along the eigenvector \((1,0)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Oscillation with Complex Eigenvalues'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ C = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalues: \(\pm i\), both with modulus 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\rho(C) = 1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'System is neutrally stable: vectors rotate forever without shrinking or growing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beyond Simple Stability: Defective Cases'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If a matrix has eigenvalues with \(|\lambda|=1\) and is defective, extra polynomial
    terms in \(k\) appear in \(A^k\), leading to slow divergence even though \(\rho(A)=1\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ D = \begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalue: \(\lambda=1\) (AM=2, GM=1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\rho(D)=1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Powers grow linearly with \(k\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ D^k = \begin{bmatrix} 1 & k \\ 0 & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: System is unstable, despite spectral radius equal to 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The spectral radius measures the dominant mode of a transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine stretching and rotating a rubber sheet. After many repetitions, the
    sheet aligns with the direction corresponding to the largest eigenvalue.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the stretching is less than 1, everything shrinks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If greater than 1, everything expands.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If exactly 1, the system is balanced on the edge of stability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Numerical Methods: Convergence of iterative solvers (e.g., Jacobi, Gauss–Seidel)
    depends on spectral radius < 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Markov Chains: Long-term distributions exist if the largest eigenvalue = 1
    and others < 1 in magnitude.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control Theory: System stability is judged by eigenvalues inside the unit circle
    (\(|\lambda| < 1\)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Economics: Input-output models remain bounded only if spectral radius < 1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Epidemiology: Basic reproduction number \(R_0\) is essentially the spectral
    radius of a next-generation matrix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The spectral radius condenses the entire spectrum of a matrix into a single
    stability criterion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It predicts the fate of iterative processes, from financial growth to disease
    spread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It draws a sharp boundary between decay, balance, and explosion in linear systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the spectral radius of \(\begin{bmatrix} 0.6 & 0.3 \\ 0.1 & 0.8 \end{bmatrix}\).
    Does the system converge?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that for any matrix norm \(\|\cdot\|\),
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \rho(A) \leq \|A\|. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '(Hint: use Gelfand’s formula.)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For \(\begin{bmatrix} 1 & 1 \\ 0 & 1 \end{bmatrix}\), explain why it diverges
    even though \(\rho=1\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove Gelfand’s formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \rho(A) = \lim_{k\to\infty} \|A^k\|^{1/k}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The spectral radius is the compass of linear dynamics: it points to stability,
    oscillation, or divergence, guiding us across disciplines wherever repeated transformations
    shape the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 69\. Markov Chains and Steady States
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Markov chains are one of the most direct and beautiful applications of eigenvalues
    in probability and statistics. They describe systems that evolve step by step,
    where the next state depends only on the current one, not on the past. The mathematics
    of steady states-the long-term behavior of such chains-rests firmly on eigenvalues
    and eigenvectors of the transition matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Transition Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Markov chain is defined by a transition matrix \(P \in \mathbb{R}^{n \times
    n}\) with the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: 'All entries are nonnegative: \(p_{ij} \geq 0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Each row sums to 1: \(\sum_j p_{ij} = 1\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the chain is in state \(i\) at time \(k\), then \(p_{ij}\) is the probability
    of moving to state \(j\) at time \(k+1\).
  prefs: []
  type: TYPE_NORMAL
- en: Evolution of States
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If the probability distribution at time \(k\) is a row vector \(\pi^{(k)}\),
    then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi^{(k+1)} = \pi^{(k)} P. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'After \(k\) steps:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi^{(k)} = \pi^{(0)} P^k. \]
  prefs: []
  type: TYPE_NORMAL
- en: So understanding the long-term behavior requires analyzing \(P^k\).
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvalue Structure of Transition Matrices
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Every transition matrix \(P\) has eigenvalue \(\lambda = 1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All other eigenvalues satisfy \(|\lambda| \leq 1\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the chain is irreducible (all states communicate) and aperiodic (no cyclic
    locking), then:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\lambda=1\) is a simple eigenvalue (AM=GM=1).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All other eigenvalues have magnitude strictly less than 1.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This ensures convergence to a unique steady state.
  prefs: []
  type: TYPE_NORMAL
- en: Steady States as Eigenvectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A steady state distribution \(\pi\) satisfies:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi = \pi P. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi^T \text{ is a right eigenvector of } P^T \text{ with eigenvalue } 1.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The steady state vector lies in the eigenspace of eigenvalue 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since probabilities must sum to 1, normalization gives a unique steady state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: A 2-State Markov Chain'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ P = \begin{bmatrix} 0.7 & 0.3 \\ 0.4 & 0.6 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalues: solve \(\det(P-\lambda I) = 0\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \lambda_1 = 1, \quad \lambda_2 = 0.3. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The steady state is found from \(\pi = \pi P\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \pi = \bigg(\frac{4}{7}, \frac{3}{7}\bigg). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As \(k \to \infty\), any initial distribution \(\pi^{(0)}\) converges to this
    steady state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Random Walk on a Graph'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Take a simple graph: 3 nodes in a line, where each node passes to neighbors
    equally.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = \begin{bmatrix} 0 & 1 & 0 \\ 0.5 & 0 & 0.5 \\ 0 & 1 & 0 \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalues: \(\{1, 0, -1\}\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The steady state corresponds to eigenvalue 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After many steps, the distribution converges to \((0.25, 0.5, 0.25)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Eigenvalue 1: the fixed “direction” of probabilities that does not change under
    transitions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eigenvalues < 1 in magnitude: transient modes that vanish as \(k \to \infty\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dominant eigenvector (steady state) is like the “center of gravity” of the
    system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So powers of \(P\) filter out all but the eigenvector of eigenvalue 1.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Google PageRank: Steady state eigenvectors rank webpages.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Economics: Input-output models evolve like Markov chains.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Epidemiology: Spread of diseases can be modeled as Markov processes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Hidden Markov models (HMMs) underpin speech recognition and
    bioinformatics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Queuing Theory: Customer arrivals and service evolve according to Markov dynamics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The concept of steady states shows how randomness can lead to predictability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalues explain why convergence happens, and at what rate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The link between linear algebra and probability provides one of the clearest
    real-world uses of eigenvectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ P = \begin{bmatrix} 0.9 & 0.1 \\ 0.5 & 0.5 \end{bmatrix}, \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: compute its eigenvalues and steady state.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Show that for any transition matrix, the largest eigenvalue is always 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that if a chain is irreducible and aperiodic, the steady state is unique.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Construct a 3-state transition matrix with a cycle (periodic) and
    show why it doesn’t converge to a steady distribution until perturbed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Markov chains and steady states are the meeting point of probability and linear
    algebra: randomness, when multiplied many times, is tamed by the calm persistence
    of eigenvalue 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 70\. Linear Differential Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many natural and engineered processes evolve continuously over time. When these
    processes can be expressed as linear relationships, they lead to systems of linear
    differential equations. The analysis of such systems relies almost entirely on
    eigenvalues and eigenvectors, which determine the behavior of solutions: whether
    they oscillate, decay, grow, or stabilize.'
  prefs: []
  type: TYPE_NORMAL
- en: The General Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider a system of first-order linear differential equations:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{d}{dt}x(t) = A x(t), \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(x(t) \in \mathbb{R}^n\) is the state vector at time \(t\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(A \in \mathbb{R}^{n \times n}\) is a constant coefficient matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The task is to solve for \(x(t)\), given an initial state \(x(0)\).
  prefs: []
  type: TYPE_NORMAL
- en: The Matrix Exponential
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The formal solution is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x(t) = e^{At} x(0), \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(e^{At}\) is the matrix exponential defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ e^{At} = I + At + \frac{(At)^2}{2!} + \frac{(At)^3}{3!} + \cdots. \]
  prefs: []
  type: TYPE_NORMAL
- en: But how do we compute \(e^{At}\) in practice? The answer comes from diagonalization
    and Jordan form.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 1: Diagonalizable Matrices'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If \(A\) is diagonalizable:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = P D P^{-1}, \quad D = \text{diag}(\lambda_1, \ldots, \lambda_n). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Then:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ e^{At} = P e^{Dt} P^{-1}, \quad e^{Dt} = \text{diag}(e^{\lambda_1 t}, \ldots,
    e^{\lambda_n t}). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus the solution is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x(t) = P \begin{bmatrix} e^{\lambda_1 t} & & \\ & \ddots & \\ & & e^{\lambda_n
    t} \end{bmatrix} P^{-1} x(0). \]
  prefs: []
  type: TYPE_NORMAL
- en: Each eigenvalue \(\lambda_i\) dictates the time behavior along its eigenvector
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case 2: Non-Diagonalizable Matrices'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If \(A\) is defective, use its Jordan form \(J = P^{-1}AP\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ e^{At} = P e^{Jt} P^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For a Jordan block of size 2:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ J = \begin{bmatrix} \lambda & 1 \\ 0 & \lambda \end{bmatrix}, \quad e^{Jt}
    = e^{\lambda t} \begin{bmatrix} 1 & t \\ 0 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial terms in \(t\) appear, multiplying the exponential part. This explains
    why repeated eigenvalues with insufficient eigenvectors yield solutions with extra
    polynomial factors.
  prefs: []
  type: TYPE_NORMAL
- en: Real vs. Complex Eigenvalues
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Real eigenvalues: solutions grow or decay exponentially along eigenvector directions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If \(\lambda < 0\): exponential decay → stability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If \(\lambda > 0\): exponential growth → instability.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Complex eigenvalues: \(\lambda = a \pm bi\). Solutions involve oscillations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ e^{(a+bi)t} = e^{at}(\cos(bt) + i \sin(bt)). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If \(a < 0\): decaying oscillations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If \(a > 0\): growing oscillations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If \(a = 0\): pure oscillations, neutrally stable.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 1: Real Eigenvalues'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} -2 & 0 \\ 0 & -3 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalues: \(-2, -3\). Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x(t) = \begin{bmatrix} c_1 e^{-2t} \\ c_2 e^{-3t} \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Both terms decay → stable equilibrium at the origin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: Complex Eigenvalues'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalues: \(\pm i\). Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x(t) = c_1 \begin{bmatrix} \cos t \\ \sin t \end{bmatrix} + c_2 \begin{bmatrix}
    -\sin t \\ \cos t \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Pure oscillation → circular motion around the origin.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 3: Mixed Stability'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 0 \\ 0 & -2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalues: \(1, -2\). Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x(t) = \begin{bmatrix} c_1 e^t \\ c_2 e^{-2t} \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: One direction grows, one decays → unstable overall, since divergence in one
    direction dominates.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The eigenvectors form the “axes of flow” of the system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The eigenvalues determine whether the flow along those axes spirals, grows,
    or shrinks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The phase portrait of the system-trajectories in the plane-is shaped by this
    interplay.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Negative eigenvalues → trajectories funnel into the origin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positive eigenvalues → trajectories repel outward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complex eigenvalues → spirals or circles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Control theory: Stability analysis of systems requires eigenvalue placement
    in the left-half plane.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Vibrations, quantum oscillations, and decay processes all follow eigenvalue
    rules.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Biology: Population models evolve according to linear differential equations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Economics: Linear models of markets converge or diverge depending on eigenvalues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Neuroscience: Neural firing dynamics can be modeled as linear ODE systems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Linear differential systems bridge linear algebra with real-world dynamics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eigenvalues determine not just numbers, but behaviors over time: growth, decay,
    oscillation, or equilibrium.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They provide the foundation for analyzing nonlinear systems, which are often
    studied by linearizing around equilibrium points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Solve \(\frac{dx}{dt} = \begin{bmatrix} -1 & 2 \\ -2 & -1 \end{bmatrix}x\).
    Interpret the solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For \(A = \begin{bmatrix} 0 & -2 \\ 2 & 0 \end{bmatrix}\), compute eigenvalues
    and describe the motion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that \(e^{At} = P e^{Dt} P^{-1}\) works when \(A\) is diagonalizable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that if all eigenvalues of \(A\) have negative real parts,
    then \(\lim_{t \to \infty} x(t) = 0\) for any initial condition.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear differential systems show how eigenvalues control the flow of time itself
    in models. They explain why some processes die out, others oscillate, and others
    grow without bound-providing the mathematical skeleton behind countless real-world
    phenomena.
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 8\. Orthogonality, least squars, and QR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opening
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 71\. Inner Products Beyond Dot Product
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The dot product is the first inner product most students encounter. In \(\mathbb{R}^n\),
    it is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle x, y \rangle = x \cdot y = \sum_{i=1}^n x_i y_i, \]
  prefs: []
  type: TYPE_NORMAL
- en: and it provides a way to measure length, angle, and orthogonality. But the dot
    product is just one special case of a much broader concept. Inner products generalize
    the dot product, extending its geometric intuition to more abstract vector spaces.
  prefs: []
  type: TYPE_NORMAL
- en: Definition of an Inner Product
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An inner product on a real vector space \(V\) is a function
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \langle \cdot, \cdot \rangle : V \times V \to \mathbb{R} \]'
  prefs: []
  type: TYPE_NORMAL
- en: 'that satisfies the following axioms for all \(x,y,z \in V\) and scalar \(\alpha
    \in \mathbb{R}\):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Positivity: \(\langle x, x \rangle \geq 0\), and \(\langle x, x \rangle = 0
    \iff x=0\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Symmetry: \(\langle x, y \rangle = \langle y, x \rangle\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Linearity in the first argument: \(\langle \alpha x + y, z \rangle = \alpha
    \langle x, z \rangle + \langle y, z \rangle\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In complex vector spaces, the symmetry condition changes to conjugate symmetry:
    \(\langle x, y \rangle = \overline{\langle y, x \rangle}\).'
  prefs: []
  type: TYPE_NORMAL
- en: Norms and Angles from Inner Products
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once an inner product is defined, we immediately get:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Norm (length): \(\|x\| = \sqrt{\langle x, x \rangle}\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance: \(d(x,y) = \|x-y\|\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Angle between vectors: \(\cos \theta = \frac{\langle x, y \rangle}{\|x\|\|y\|}\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, inner products generalize the familiar geometry of \(\mathbb{R}^n\) to
    broader contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Examples Beyond the Dot Product
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Weighted Inner Product (in \(\mathbb{R}^n\)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \langle x, y \rangle_W = x^T W y, \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where \(W\) is a symmetric positive definite matrix.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, lengths and angles depend on the weights encoded in \(W\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Useful when some dimensions are more important than others (e.g., weighted least
    squares).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Function Spaces (continuous inner product): On \(V = C[a,b]\), the space of
    continuous functions on \([a,b]\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \langle f, g \rangle = \int_a^b f(t) g(t) \, dt. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Length: \(\|f\| = \sqrt{\int_a^b f(t)^2 dt}\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Orthogonality: \(f\) and \(g\) are orthogonal if their integral product is
    zero.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This inner product underpins Fourier series.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Complex Inner Product (in \(\mathbb{C}^n\)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \langle x, y \rangle = \sum_{i=1}^n x_i \overline{y_i}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Conjugation ensures positivity.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Critical for quantum mechanics, where states are vectors in complex Hilbert
    spaces.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Polynomial Spaces: For polynomials on \([-1,1]\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \langle p, q \rangle = \int_{-1}^1 p(x) q(x) \, dx. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Leads to orthogonal polynomials (Legendre, Chebyshev), fundamental in approximation
    theory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inner products reshape geometry. Instead of measuring lengths and angles with
    the Euclidean metric, we measure them with the metric induced by the chosen inner
    product.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different inner products create different geometries on the same vector space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: A weighted inner product distorts circles into ellipses, changing
    which vectors count as “orthogonal.”'
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Signal Processing: Correlation between signals is an inner product. Orthogonality
    means two signals carry independent information.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fourier Analysis: Fourier coefficients come from inner products with sine and
    cosine functions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Kernel methods generalize inner products to infinite-dimensional
    spaces.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantum Mechanics: Probabilities are squared magnitudes of complex inner products.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimization: Weighted least squares problems use weighted inner products.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Inner products generalize geometry to new contexts: weighted spaces, functions,
    polynomials, quantum states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They provide the foundation for defining orthogonality, projections, and orthonormal
    bases in spaces far beyond \(\mathbb{R}^n\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They unify ideas across pure mathematics, physics, engineering, and computer
    science.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Show that the weighted inner product \(\langle x, y \rangle_W = x^T W y\) satisfies
    the inner product axioms if \(W\) is positive definite.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute \(\langle f, g \rangle = \int_0^\pi \sin(t)\cos(t)\, dt\). Are \(f=\sin\)
    and \(g=\cos\) orthogonal?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In \(\mathbb{C}^2\), verify that \(\langle (1,i), (i,1) \rangle = 0\). What
    does this mean geometrically?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that every inner product induces a norm, and that different
    inner products can lead to different geometries on the same space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dot product is just the beginning. Inner products provide the language to
    extend geometry into weighted spaces, continuous functions, and infinite dimensions-transforming
    how we measure similarity, distance, and structure across mathematics and science.
  prefs: []
  type: TYPE_NORMAL
- en: 72\. Orthogonality and Orthonormal Bases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Orthogonality is one of the most powerful ideas in linear algebra. It generalizes
    the familiar concept of perpendicularity in Euclidean space to abstract vector
    spaces equipped with an inner product. When orthogonality is combined with normalization
    (making vectors have unit length), we obtain orthonormal bases, which simplify
    computations, clarify geometry, and underpin many algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Two vectors \(x, y \in V\) are orthogonal if
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle x, y \rangle = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^2\) or \(\mathbb{R}^3\), this means the vectors are perpendicular.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In function spaces, it means the integral of their product is zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In signal processing, it means the signals are independent and non-overlapping.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orthogonality captures the idea of “no overlap” or “independence” under the
    geometry of the inner product.
  prefs: []
  type: TYPE_NORMAL
- en: Properties of Orthogonal Vectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If \(x \perp y\), then \(\|x+y\|^2 = \|x\|^2 + \|y\|^2\) (Pythagoras’ theorem
    generalized).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Orthogonality is symmetric: if \(x \perp y\), then \(y \perp x\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Any set of mutually orthogonal nonzero vectors is automatically linearly independent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This last property is critical: orthogonality guarantees independence.'
  prefs: []
  type: TYPE_NORMAL
- en: Orthonormal Sets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An orthonormal set is a collection of vectors \(\{u_1, \dots, u_k\}\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle u_i, u_j \rangle = \begin{cases} 1 & \text{if } i=j, \\ 0 & \text{if
    } i \neq j. \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: Each vector has unit length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distinct vectors are mutually orthogonal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This structure makes computations with coordinates as simple as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Orthonormal Bases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A basis \(\{u_1, \dots, u_n\}\) for a vector space is orthonormal if it is orthonormal
    as a set.
  prefs: []
  type: TYPE_NORMAL
- en: Any vector \(x \in V\) can be written as
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ x = \sum_{i=1}^n \langle x, u_i \rangle u_i. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The coefficients are just inner products, no need to solve systems of equations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is why orthonormal bases are the most convenient: they make representation
    and projection effortless.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Standard Basis in \(\mathbb{R}^n\): \(\{e_1, e_2, \dots, e_n\}\), where \(e_i\)
    has 1 in the \(i\)-th coordinate and 0 elsewhere.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Orthonormal under the standard dot product.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fourier Basis: Functions \(\{\sin(nx), \cos(nx)\}\) on \([0,2\pi]\) are orthogonal
    under the inner product \(\langle f,g\rangle = \int_0^{2\pi} f(x)g(x)dx\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This basis decomposes signals into pure frequencies.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Polynomial Basis: Legendre polynomials \(P_n(x)\) are orthogonal on \([-1,1]\)
    with respect to \(\langle f,g\rangle = \int_{-1}^1 f(x)g(x)\,dx\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Orthogonality splits space into independent “directions.”
  prefs: []
  type: TYPE_NORMAL
- en: Orthonormal bases are like perfectly aligned coordinate axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any vector decomposes uniquely as a sum of independent contributions along these
    axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distances and angles are preserved, making the geometry transparent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Signal Processing: Decompose signals into orthogonal frequency components.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Principal components form an orthonormal basis capturing
    variance directions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Methods: Orthonormal bases improve numerical stability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Quantum Mechanics: States are orthogonal if they represent mutually exclusive
    outcomes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Rotations are represented by orthogonal matrices with orthonormal
    columns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Orthogonality provides independence; orthonormality provides normalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together they make computations, decompositions, and projections clean and efficient.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They underlie Fourier analysis, principal component analysis, and countless
    modern algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Show that \(\{(1,0,0), (0,1,0), (0,0,1)\}\) is an orthonormal basis of \(\mathbb{R}^3\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check whether \(\{(1,1,0), (1,-1,0), (0,0,1)\}\) is orthonormal under the dot
    product. If not, normalize it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the coefficients of \(x=(3,4)\) in the basis \(\{(1,0), (0,1)\}\) and
    in the rotated orthonormal basis \(\{(1/\sqrt{2}, 1/\sqrt{2}), (-1/\sqrt{2}, 1/\sqrt{2})\}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that in any finite-dimensional inner product space, an orthonormal
    basis always exists (hint: Gram–Schmidt).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Orthogonality and orthonormal bases are the backbone of linear algebra: they
    transform messy problems into elegant decompositions, giving us the cleanest possible
    language for describing vectors, signals, and data.'
  prefs: []
  type: TYPE_NORMAL
- en: 73\. Gram–Schmidt Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Gram–Schmidt process is a systematic method for turning any linearly independent
    set of vectors into an orthonormal basis. This process is one of the most elegant
    bridges between algebra and geometry: it takes arbitrary vectors and makes them
    mutually perpendicular, while preserving the span.'
  prefs: []
  type: TYPE_NORMAL
- en: The Problem It Solves
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given a set of linearly independent vectors \(\{v_1, v_2, \dots, v_n\}\) in
    an inner product space:'
  prefs: []
  type: TYPE_NORMAL
- en: They span some subspace \(W\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But they are not necessarily orthogonal or normalized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goal: Construct an orthonormal basis \(\{u_1, u_2, \dots, u_n\}\) for \(W\).'
  prefs: []
  type: TYPE_NORMAL
- en: The Gram–Schmidt Algorithm
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Start with the first vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ u_1 = \frac{v_1}{\|v_1\|}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the second vector, subtract the projection onto \(u_1\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ w_2 = v_2 - \langle v_2, u_1 \rangle u_1, \quad u_2 = \frac{w_2}{\|w_2\|}.
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the third vector, subtract projections onto both \(u_1\) and \(u_2\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ w_3 = v_3 - \langle v_3, u_1 \rangle u_1 - \langle v_3, u_2 \rangle u_2,
    \quad u_3 = \frac{w_3}{\|w_3\|}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Continue inductively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ w_k = v_k - \sum_{j=1}^{k-1} \langle v_k, u_j \rangle u_j, \quad u_k = \frac{w_k}{\|w_k\|}.
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At each step, \(w_k\) is made orthogonal to all previous \(u_j\), and then normalized
    to form \(u_k\).
  prefs: []
  type: TYPE_NORMAL
- en: Example in \(\mathbb{R}^2\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Start with \(v_1 = (1,1)\), \(v_2 = (1,0)\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Normalize first vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ u_1 = \frac{(1,1)}{\sqrt{2}} = \left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right).
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Subtract projection of \(v_2\) on \(u_1\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ w_2 = (1,0) - \left(\tfrac{1}{\sqrt{2}}\cdot1 + \tfrac{1}{\sqrt{2}}\cdot0\right)\left(\tfrac{1}{\sqrt{2}},
    \tfrac{1}{\sqrt{2}}\right). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ = (1,0) - \tfrac{1}{\sqrt{2}}\left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right).
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ = (1,0) - (0.5,0.5) = (0.5,-0.5). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Normalize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ u_2 = \frac{(0.5,-0.5)}{\sqrt{0.5^2+(-0.5)^2}} = \frac{(0.5,-0.5)}{\sqrt{0.5}}
    = \left(\tfrac{1}{\sqrt{2}}, -\tfrac{1}{\sqrt{2}}\right). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Final orthonormal basis:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ u_1 = \left(\tfrac{1}{\sqrt{2}}, \tfrac{1}{\sqrt{2}}\right), \quad u_2 =
    \left(\tfrac{1}{\sqrt{2}}, -\tfrac{1}{\sqrt{2}}\right). \]
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each step removes “overlap” with previously chosen directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think of it as building new perpendicular coordinate axes inside the span of
    the original vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result is like rotating and scaling the original set into a perfectly orthogonal
    system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerical Stability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Classical Gram–Schmidt can suffer from round-off errors in computer calculations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A numerically stable alternative is Modified Gram–Schmidt (MGS), which reorders
    the projection steps to reduce loss of orthogonality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, QR factorization algorithms often implement MGS or Householder
    reflections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'QR Factorization: Gram–Schmidt provides the foundation: \(A = QR\), where \(Q\)
    is orthogonal and \(R\) is upper triangular.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Compression: Orthonormal bases from Gram–Schmidt lead to efficient representations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Signal Processing: Ensures independent frequency or wave components.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Used in orthogonalization of features and dimensionality
    reduction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Orthogonal states in quantum mechanics can be constructed from arbitrary
    states using Gram–Schmidt.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gram–Schmidt guarantees that any independent set can be reshaped into an orthonormal
    basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It underlies computational methods like QR decomposition, least squares, and
    numerical PDE solvers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It makes projections, coordinates, and orthogonality explicit and manageable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Apply Gram–Schmidt to \((1,0,1)\), \((1,1,0)\), \((0,1,1)\) in \(\mathbb{R}^3\).
    Verify orthonormality.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that the span of the orthonormal basis equals the span of the original
    vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Gram–Schmidt to find an orthonormal basis for polynomials \(\{1,x,x^2\}\)
    on \([-1,1]\) with inner product \(\langle f,g\rangle = \int_{-1}^1 f(x)g(x)\,dx\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that Gram–Schmidt always works for linearly independent sets,
    but fails if the set is dependent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Gram–Schmidt process is the algorithmic heart of orthogonality: it takes
    the messy and redundant and reshapes it into clean, perpendicular building blocks
    for the spaces we study.'
  prefs: []
  type: TYPE_NORMAL
- en: 74\. Projections onto Subspaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Projections are a natural extension of orthogonality: they describe how to
    “drop” a vector onto a subspace in the most natural way, minimizing the distance.
    Understanding projections is crucial for solving least squares problems, decomposing
    vectors, and interpreting data in terms of simpler, lower-dimensional structures.'
  prefs: []
  type: TYPE_NORMAL
- en: Projection onto a Vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Start with the simplest case: projecting a vector \(x\) onto a nonzero vector
    \(u\).'
  prefs: []
  type: TYPE_NORMAL
- en: The projection is the component of \(x\) that lies in the direction of \(u\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \text{proj}_u(x) = \frac{\langle x, u \rangle}{\langle u, u \rangle} u. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If \(u\) is normalized (\(\|u\|=1\)), this simplifies to
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ \text{proj}_u(x) = \langle x, u \rangle u. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Geometrically, this is the foot of the perpendicular from \(x\) onto the line
    spanned by \(u\).
  prefs: []
  type: TYPE_NORMAL
- en: Projection onto an Orthonormal Basis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we have an orthonormal basis \(\{u_1, u_2, \dots, u_k\}\) for a subspace
    \(W\). Then the projection of \(x\) onto \(W\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \text{proj}_W(x) = \sum_{i=1}^k \langle x, u_i \rangle u_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This formula is powerful:'
  prefs: []
  type: TYPE_NORMAL
- en: Each coefficient \(\langle x, u_i \rangle\) captures how much of \(x\) aligns
    with \(u_i\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum reconstructs the best approximation of \(x\) inside \(W\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projection Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When working in coordinates, projections can be represented by matrices.
  prefs: []
  type: TYPE_NORMAL
- en: If \(U\) is the \(n \times k\) matrix with orthonormal columns \(\{u_1, \dots,
    u_k\}\), then
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ P = UU^T \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: is the projection matrix onto \(W\).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Properties of \(P\):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Idempotence: \(P^2 = P\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Symmetry: \(P^T = P\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Best approximation: For any \(x\), \(\|x - Px\|\) is minimized.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Projection and Orthogonal Complements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If \(W\) is a subspace of \(V\), then every vector \(x \in V\) can be decomposed
    uniquely as
  prefs: []
  type: TYPE_NORMAL
- en: \[ x = \text{proj}_W(x) + \text{proj}_{W^\perp}(x), \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(W^\perp\) is the orthogonal complement of \(W\).
  prefs: []
  type: TYPE_NORMAL
- en: 'This decomposition is the orthogonal decomposition theorem. It says: space
    splits cleanly into “in” and “out of” components relative to a subspace.'
  prefs: []
  type: TYPE_NORMAL
- en: Example in \(\mathbb{R}^2\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let \(u = (2,1)\), and project \(x = (3,4)\) onto span\(\{u\}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute inner product: \(\langle x,u\rangle = 3\cdot 2 + 4\cdot 1 = 10\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute norm squared: \(\langle u,u\rangle = 2^2 + 1^2 = 5\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Projection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \text{proj}_u(x) = \frac{10}{5}(2,1) = 2(2,1) = (4,2). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Orthogonal error:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ x - \text{proj}_u(x) = (3,4) - (4,2) = (-1,2). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Notice: \((4,2)\) lies on the line through \(u\), and the error vector \((-1,2)\)
    is orthogonal to \(u\).'
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Least Squares Regression: The regression line is the projection of data onto
    the subspace spanned by predictor variables.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dimensionality Reduction: Principal Component Analysis (PCA) projects data
    onto the subspace of top eigenvectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: 3D objects are projected onto 2D screens.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Methods: Projections solve equations approximately when exact solutions
    don’t exist.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Work and energy are computed via projections of forces and velocities.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Projections are the essence of approximation: they give the “best possible”
    version of a vector inside a chosen subspace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They formalize independence: the error vector is always orthogonal to the subspace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They provide geometric intuition for statistics, machine learning, and numerical
    computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the projection of \(x = (2,3,4)\) onto \(u = (1,1,1)\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Verify that the residual \(x - \text{proj}_u(x)\) is orthogonal to \(u\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write the projection matrix for the subspace spanned by \(\{(1,0,0),(0,1,0)\}\)
    in \(\mathbb{R}^3\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that projection matrices are idempotent and symmetric.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Projections turn vector spaces into cleanly split components: what lies “inside”
    a subspace and what lies “outside.” This idea, simple yet profound, runs through
    geometry, data analysis, and physics alike.'
  prefs: []
  type: TYPE_NORMAL
- en: 75\. Orthogonal Decomposition Theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the cornerstones of linear algebra is the orthogonal decomposition theorem,
    which states that every vector in an inner product space can be uniquely split
    into two parts: one lying inside a subspace and the other lying in its orthogonal
    complement. This gives us a clear way to organize information, separate influences,
    and simplify computations.'
  prefs: []
  type: TYPE_NORMAL
- en: Statement of the Theorem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let \(V\) be an inner product space and \(W\) a subspace of \(V\). Then for
    every vector \(x \in V\), there exist unique vectors \(w \in W\) and \(z \in W^\perp\)
    such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ x = w + z. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: \(w = \text{proj}_W(x)\), the projection of \(x\) onto \(W\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(z = x - \text{proj}_W(x)\), the orthogonal component.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This decomposition is unique: no other pair of vectors from \(W\) and \(W^\perp\)
    adds up to \(x\).'
  prefs: []
  type: TYPE_NORMAL
- en: Example in \(\mathbb{R}^2\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Take \(W\) to be the line spanned by \(u = (1,2)\). For \(x = (4,1)\):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Projection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \text{proj}_u(x) = \frac{\langle x,u \rangle}{\langle u,u \rangle} u. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute: \(\langle x,u\rangle = 4\cdot 1 + 1\cdot 2 = 6\), and \(\langle u,u\rangle
    = 1^2+2^2=5\). So'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ \text{proj}_u(x) = \frac{6}{5}(1,2) = \left(\tfrac{6}{5}, \tfrac{12}{5}\right).
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Orthogonal component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ z = x - \text{proj}_u(x) = (4,1) - \left(\tfrac{6}{5}, \tfrac{12}{5}\right)
    = \left(\tfrac{14}{5}, -\tfrac{7}{5}\right). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Verify: \(\langle u, z\rangle = 1\cdot \tfrac{14}{5} + 2\cdot (-\tfrac{7}{5})
    = 0\). Thus, \(z \in W^\perp\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ x = \underbrace{\left(\tfrac{6}{5}, \tfrac{12}{5}\right)}_{\in W} + \underbrace{\left(\tfrac{14}{5},
    -\tfrac{7}{5}\right)}_{\in W^\perp}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The decomposition splits \(x\) into its “in-subspace” part and its “out-of-subspace”
    part.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(w\) is the closest point in \(W\) to \(x\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(z\) is the leftover “error,” always perpendicular to \(W\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometrically, the shortest path from \(x\) to a subspace is always orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonal Complements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The orthogonal complement \(W^\perp\) contains all vectors orthogonal to every
    vector in \(W\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dimensional relationship:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \dim(W) + \dim(W^\perp) = \dim(V). \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Together, \(W\) and \(W^\perp\) partition the space \(V\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projection Matrices and Decomposition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If \(P\) is the projection matrix onto \(W\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x = Px + (I-P)x, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(Px \in W\) and \((I-P)x \in W^\perp\).
  prefs: []
  type: TYPE_NORMAL
- en: This formulation is used constantly in numerical linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Least Squares Approximation: The best-fit solution is the projection; the error
    lies in the orthogonal complement.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fourier Analysis: Any signal decomposes into a sum of components along orthogonal
    basis functions plus residuals.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Statistics: Regression decomposes data into explained variance (in the subspace
    of predictors) and residual variance (orthogonal).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Engineering: Splitting forces into parallel and perpendicular components relative
    to a surface.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Decomposing movement into screen-plane projection and depth
    (orthogonal direction).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Orthogonal decomposition gives clarity: every vector splits into “relevant”
    and “irrelevant” parts relative to a chosen subspace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides the foundation for least squares, regression, and signal approximation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It ensures uniqueness, stability, and interpretability in vector computations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In \(\mathbb{R}^3\), decompose \(x = (1,2,3)\) into components in span\((1,0,0)\)
    and its orthogonal complement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that if \(W\) is spanned by \((1,1,0)\) and \((0,1,1)\), then any vector
    in \(\mathbb{R}^3\) can be uniquely split into \(W\) and \(W^\perp\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write down the projection matrix \(P\) for \(W = \text{span}\{(1,0,0),(0,1,0)\}\)
    in \(\mathbb{R}^3\). Verify that \(I-P\) projects onto \(W^\perp\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove the orthogonal decomposition theorem using projection matrices
    and the fact that \(P^2 = P\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The orthogonal decomposition theorem guarantees that every vector finds its
    closest approximation in a chosen subspace and a perfectly perpendicular remainder-an
    elegant structure that makes analysis and computation possible in countless domains.
  prefs: []
  type: TYPE_NORMAL
- en: 76\. Orthogonal Projections and Least Squares
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the deepest connections in linear algebra is between orthogonal projections
    and the least squares method. When equations don’t have an exact solution, least
    squares finds the “best approximate” one. The theory behind it is entirely geometric:
    the best solution is the projection of a vector onto a subspace.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Setup: Overdetermined Systems'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Consider a system of equations \(Ax = b\), where
  prefs: []
  type: TYPE_NORMAL
- en: \(A\) is an \(m \times n\) matrix with \(m > n\) (more equations than unknowns).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(b \in \mathbb{R}^m\) may not lie in the column space of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This means:'
  prefs: []
  type: TYPE_NORMAL
- en: There may be no exact solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead, we want \(x\) that makes \(Ax\) as close as possible to \(b\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least Squares Problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The least squares solution minimizes the error:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_x \|Ax - b\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: \(Ax\) is the projection of \(b\) onto the column space of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The error vector \(b - Ax\) is orthogonal to the column space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is exactly the orthogonal decomposition theorem applied to \(b\).
  prefs: []
  type: TYPE_NORMAL
- en: Derivation of Normal Equations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We want \(r = b - Ax\) to be orthogonal to every column of \(A\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T (b - Ax) = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Rearranging:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A x = A^T b. \]
  prefs: []
  type: TYPE_NORMAL
- en: This system is called the normal equations. Its solution \(x\) gives the least
    squares approximation.
  prefs: []
  type: TYPE_NORMAL
- en: Projection Matrix in Least Squares
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The projection of \(b\) onto \(\text{Col}(A)\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{b} = A(A^T A)^{-1} A^T b, \]
  prefs: []
  type: TYPE_NORMAL
- en: assuming \(A^T A\) is invertible.
  prefs: []
  type: TYPE_NORMAL
- en: Here,
  prefs: []
  type: TYPE_NORMAL
- en: \(P = A(A^T A)^{-1} A^T\) is the projection matrix onto the column space of
    \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fitted vector is \(\hat{b} = Pb\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The residual \(r = b - \hat{b}\) lies in the orthogonal complement of \(\text{Col}(A)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose \(A = \begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}\), \(b = \begin{bmatrix}2
    \\ 2 \\ 4\end{bmatrix}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Column space of \(A\): span of \((1,2,3)\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Projection formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \hat{b} = \frac{\langle b, A \rangle}{\langle A, A \rangle} A. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compute: \(\langle b,A\rangle = 2\cdot1+2\cdot2+4\cdot3 = 18\). \(\langle A,A\rangle
    = 1^2+2^2+3^2=14\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Projection:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \hat{b} = \frac{18}{14}(1,2,3) = \left(\tfrac{9}{7}, \tfrac{18}{7}, \tfrac{27}{7}\right).
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Residual:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ r = b - \hat{b} = \left(\tfrac{5}{7}, -\tfrac{4}{7}, \tfrac{1}{7}\right).
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check: \(\langle r,A\rangle = 0\), so it’s orthogonal.'
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The least squares solution is the point in \(\text{Col}(A)\) closest to \(b\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The error vector is orthogonal to the subspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is like dropping a perpendicular from \(b\) to the subspace \(\text{Col}(A)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Statistics: Linear regression uses least squares to fit models to data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Engineering: Curve fitting, system identification, and calibration.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Best-fit transformations (e.g., Procrustes analysis).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Optimization of linear models (before moving to nonlinear
    methods).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Methods: Solving inconsistent systems of equations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Orthogonal projections explain why least squares gives the best approximation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They reveal the geometry behind regression: data is projected onto the model
    space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They connect linear algebra with statistics, optimization, and applied sciences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Solve \(\min_x \|Ax-b\|\) for \(A = \begin{bmatrix}1 & 1 \\ 1 & 2 \\ 1 & 3\end{bmatrix}\),
    \(b=(1,2,2)^T\). Interpret the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Derive the projection matrix \(P\) for this system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that the residual is orthogonal to each column of \(A\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that among all possible approximations \(Ax\), the least squares
    solution is unique if and only if \(A^T A\) is invertible.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Orthogonal projections turn the messy, unsolvable world of overdetermined equations
    into one of best possible approximations. Least squares is not just an algebraic
    trick-it is the geometric essence of “closeness” in higher-dimensional spaces.
  prefs: []
  type: TYPE_NORMAL
- en: 77\. QR Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: QR decomposition is a factorization of a matrix into an orthogonal part and
    a triangular part. It grows directly out of orthogonality and the Gram–Schmidt
    process, and it plays a central role in numerical linear algebra, providing a
    stable and efficient way to solve systems, compute least squares solutions, and
    analyze matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a real \(m \times n\) matrix \(A\) with linearly independent columns:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = QR, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(Q\) is an \(m \times n\) matrix with orthonormal columns (\(Q^T Q = I\)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(R\) is an \(n \times n\) upper triangular matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This decomposition is unique if we require \(R\) to have positive diagonal entries.
  prefs: []
  type: TYPE_NORMAL
- en: Connection to Gram–Schmidt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Gram–Schmidt process applied to the columns of \(A\) produces the orthonormal
    columns of \(Q\). The coefficients used during the orthogonalization steps naturally
    form the entries of \(R\).
  prefs: []
  type: TYPE_NORMAL
- en: Each column of \(A\) is expressed as a combination of the orthonormal columns
    of \(Q\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coefficients of this expression populate the triangular matrix \(R\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 1 \\ 1 & 0 \\ 0 & 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply Gram–Schmidt to the columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '\(v_1 = (1,1,0)^T\), normalize:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ u_1 = \frac{1}{\sqrt{2}}(1,1,0)^T. \]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Subtract projection from \(v_2=(1,0,1)^T\):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ w_2 = v_2 - \langle v_2,u_1\rangle u_1. \]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute \(\langle v_2,u_1\rangle = \tfrac{1}{\sqrt{2}}(1+0+0)=\tfrac{1}{\sqrt{2}}\).
    So
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ w_2 = (1,0,1)^T - \tfrac{1}{\sqrt{2}}(1,1,0)^T = \left(\tfrac{1}{2}, -\tfrac{1}{2},
    1\right)^T. \]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Normalize:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ u_2 = \frac{1}{\sqrt{1.5}} \left(\tfrac{1}{2}, -\tfrac{1}{2}, 1\right)^T.
    \]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Construct \(Q = [u_1, u_2]\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute \(R = Q^T A\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result is \(A = QR\), with \(Q\) orthonormal and \(R\) triangular.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \(Q\) represents an orthogonal change of basis-rotations and reflections that
    preserve length and angle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(R\) encodes scaling and shear in the new orthonormal coordinate system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Together, they show how \(A\) transforms space: first rotate into a clean basis,
    then apply triangular distortion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Least Squares: Instead of solving \(A^T A x = A^T b\), we use \(QR\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ Ax = b \quad \Rightarrow \quad QRx = b. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Multiply by \(Q^T\):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ Rx = Q^T b. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Since \(R\) is triangular, solving for \(x\) is efficient and numerically stable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Eigenvalue Algorithms: The QR algorithm iteratively applies QR factorizations
    to approximate eigenvalues.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Stability: Orthogonal transformations minimize numerical errors compared
    to solving normal equations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Many algorithms (e.g., linear regression, PCA) use QR decomposition
    for efficiency and stability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Computer Graphics: Orthogonal factors preserve shapes; triangular factors simplify
    transformations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: QR decomposition bridges theory (Gram–Schmidt orthogonalization) and computation
    (matrix factorization).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It avoids pitfalls of normal equations, improving numerical stability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It underpins algorithms across statistics, engineering, and computer science.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the QR decomposition of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}1 & 2 \\ 2 & 3 \\ 4 & 5\end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Verify that \(Q^T Q = I\) and \(R\) is upper triangular.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use QR to solve the least squares problem \(Ax \approx b\) with \(b=(1,1,1)^T\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Show that if \(A\) is square and orthogonal, then \(R=I\) and \(Q=A\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: QR decomposition turns the messy process of solving least squares into a clean,
    geometric procedure-rotating into a better coordinate system before solving. It
    is one of the most powerful tools in the linear algebra toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: 78\. Orthogonal Matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Orthogonal matrices are square matrices whose rows and columns form an orthonormal
    set. They are the algebraic counterpart of rigid motions in geometry: transformations
    that preserve lengths, angles, and orientation (except for reflections).'
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A square matrix \(Q \in \mathbb{R}^{n \times n}\) is orthogonal if
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q^T Q = QQ^T = I. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This means:'
  prefs: []
  type: TYPE_NORMAL
- en: The columns of \(Q\) are orthonormal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rows of \(Q\) are also orthonormal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Properties
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Inverse Equals Transpose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ Q^{-1} = Q^T. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This makes orthogonal matrices especially easy to invert.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Preservation of Norms: For any vector \(x\),'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \|Qx\| = \|x\|. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Orthogonal transformations never stretch or shrink vectors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Preservation of Inner Products:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \langle Qx, Qy \rangle = \langle x, y \rangle. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Angles are preserved.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Determinant: \(\det(Q) = \pm 1\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If \(\det(Q) = 1\), \(Q\) is a rotation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\det(Q) = -1\), \(Q\) is a reflection combined with rotation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '2D Rotation Matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ Q = \begin{bmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta
    \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Rotates vectors by angle \(\theta\).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2D Reflection Matrix: Reflection across the \(x\)-axis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ Q = \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Permutation Matrices: Swapping coordinates is orthogonal because it preserves
    lengths. Example in 3D:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ Q = \begin{bmatrix}0 & 1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 1\end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Orthogonal matrices represent isometries: transformations that preserve the
    shape of objects.'
  prefs: []
  type: TYPE_NORMAL
- en: They can rotate, reflect, or permute axes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They never distort lengths or angles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is why in computer graphics, orthogonal matrices model pure rotations and
    reflections without scaling.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Computer Graphics: Rotations of 3D models use orthogonal matrices to avoid
    distortion.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Linear Algebra: Orthogonal transformations are numerically stable,
    widely used in QR factorization and eigenvalue algorithms.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Compression: Orthogonal transforms like the Fourier and cosine transforms
    preserve energy.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Signal Processing: Orthogonal filters separate signals into independent components.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Orthogonal matrices describe rotations in rigid body dynamics.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Orthogonal matrices are the building blocks of stable algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They describe symmetry, structure, and invariance in physical and computational
    systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They serve as the simplest and most powerful class of transformations that preserve
    geometry exactly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Verify that
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ Q = \begin{bmatrix}0 & -1 \\ 1 & 0\end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: is orthogonal. What geometric transformation does it represent?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Prove that the determinant of an orthogonal matrix must be \(\pm 1\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that multiplying two orthogonal matrices gives another orthogonal matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that eigenvalues of orthogonal matrices lie on the complex
    unit circle (i.e., \(|\lambda|=1\)).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Orthogonal matrices capture the essence of symmetry: transformations that preserve
    structure exactly. They lie at the heart of geometry, physics, and computation.'
  prefs: []
  type: TYPE_NORMAL
- en: 79\. Fourier Viewpoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Fourier viewpoint is one of the most profound connections in linear algebra:
    the idea that complex signals, data, or functions can be decomposed into sums
    of simpler, orthogonal waves. Instead of describing information in its raw form
    (time, space, or coordinates), we express it in terms of frequencies. This perspective
    reshapes how we analyze, compress, and understand information across mathematics,
    physics, and engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fourier Series: The Basic Idea'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we have a periodic function \(f(x)\) defined on \([-\pi, \pi]\). The
    Fourier series expresses \(f(x)\) as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x) = a_0 + \sum_{n=1}^\infty \left( a_n \cos(nx) + b_n \sin(nx) \right).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: The coefficients \(a_n, b_n\) are found using inner products with sine and cosine
    functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each sine and cosine is orthogonal to the others under the inner product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \langle f, g \rangle = \int_{-\pi}^\pi f(x) g(x) \, dx. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Thus, Fourier series is nothing more than expanding a function in an orthonormal
    basis of trigonometric functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fourier Transform: From Time to Frequency'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For non-periodic signals, the Fourier transform generalizes this expansion.
    For a function \(f(t)\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{f}(\omega) = \int_{-\infty}^\infty f(t) e^{-i \omega t} dt \]
  prefs: []
  type: TYPE_NORMAL
- en: transforms it into frequency space. The inverse transform reconstructs \(f(t)\)
    from its frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is again an inner product viewpoint: the exponential functions \(e^{i
    \omega t}\) act as orthogonal basis functions on \(\mathbb{R}\).'
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonality of Waves
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The trigonometric functions \(\{\cos(nx), \sin(nx)\}\) and the complex exponentials
    \(\{e^{i\omega t}\}\) form orthogonal families.
  prefs: []
  type: TYPE_NORMAL
- en: Two different sine waves have zero inner product over a full period.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Likewise, exponentials with different frequencies are orthogonal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is exactly like orthogonal vectors in \(\mathbb{R}^n\), except here the
    space is infinite-dimensional.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete Fourier Transform (DFT)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In computational settings, we don’t work with infinite integrals but with finite
    data. The DFT expresses an \(n\)-dimensional vector \(x = (x_0, \dots, x_{n-1})\)
    as a linear combination of orthogonal complex exponentials:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_k = \sum_{j=0}^{n-1} x_j e^{-2\pi i jk / n}, \quad k=0,\dots,n-1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This is simply a change of basis: from the standard basis (time domain) to
    the Fourier basis (frequency domain).'
  prefs: []
  type: TYPE_NORMAL
- en: The Fast Fourier Transform (FFT) computes this in \(O(n \log n)\) time, making
    Fourier analysis practical at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the time domain, data is expressed as a sequence of raw values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the frequency domain, data is expressed as amplitudes of orthogonal waves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Fourier viewpoint is just a rotation into a new orthogonal coordinate system,
    exactly like diagonalizing a matrix or changing basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Signal Processing: Filtering unwanted noise corresponds to removing high-frequency
    components.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Image Compression: JPEG uses Fourier-like transforms (cosine transforms) to
    compress images.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data Analysis: Identifying cycles and periodic patterns in time series.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Quantum states are represented in both position and momentum bases,
    linked by Fourier transform.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Partial Differential Equations: Solutions are simplified by moving to frequency
    space, where derivatives become multipliers.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Fourier methods turn difficult problems into simpler ones: convolution becomes
    multiplication, differentiation becomes scaling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They provide a universal language for analyzing periodicity, oscillation, and
    wave phenomena.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They are linear algebra at heart: orthogonal expansions in special bases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the Fourier series coefficients for \(f(x) = x\) on \([-\pi,\pi]\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the sequence \((1,0,0,0)\), compute the 4-point DFT and interpret the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that \(\int_{-\pi}^\pi \sin(mx)\cos(nx) dx = 0\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that the Fourier basis \(\{e^{i2\pi k t}\}_{k=0}^{n-1}\) is
    orthonormal in \(\mathbb{C}^n\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Fourier viewpoint reveals that every signal, no matter how complex, can
    be seen as a combination of simple, orthogonal waves. It is a perfect marriage
    of geometry, algebra, and analysis, and one of the most important ideas in modern
    mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: 80\. Polynomial and Multifeature Least Squares
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Least squares problems become especially powerful when extended to fitting polynomials
    or handling multiple features at once. Instead of a single straight line through
    data, we can fit curves of higher degree or surfaces in higher dimensions. These
    generalizations lie at the heart of regression, data analysis, and scientific
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: From Line to Polynomial
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The simplest least squares model is a straight line:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y \approx \beta_0 + \beta_1 x. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'But many relationships are nonlinear. Polynomial least squares generalizes
    the model to:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y \approx \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_d x^d. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, each power of \(x\) is treated as a new feature. The problem reduces
    to ordinary least squares on the design matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & x_1 & x_1^2 & \dots & x_1^d \\ 1 & x_2 & x_2^2 &
    \dots & x_2^d \\ \vdots & \vdots & \vdots & & \vdots \\ 1 & x_n & x_n^2 & \dots
    & x_n^d \end{bmatrix}, \quad \beta = \begin{bmatrix}\beta_0 \\ \beta_1 \\ \vdots
    \\ \beta_d\end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The least squares solution minimizes \(\|A\beta - y\|\).
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When data involves several predictors, we extend the model to:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y \approx \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_p x_p. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'In matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y \approx A\beta, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(A\) is the design matrix with columns corresponding to features (including
    a column of ones for the intercept).
  prefs: []
  type: TYPE_NORMAL
- en: 'The least squares solution is still given by the normal equations:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\beta} = (A^T A)^{-1} A^T y, \]
  prefs: []
  type: TYPE_NORMAL
- en: or more stably by QR or SVD factorizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Polynomial Fit'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we have data points \((1,1), (2,2.2), (3,2.9), (4,4.1)\). Fitting a
    quadratic model \(y \approx \beta_0 + \beta_1 x + \beta_2 x^2\):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Construct design matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 1 & 1 & 1 \\ 1 & 2 & 4 \\ 1 & 3 & 9 \\ 1 & 4 & 16 \end{bmatrix}.
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Solve least squares problem \(\min \|A\beta - y\|\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The result gives coefficients \(\beta_0, \beta_1, \beta_2\) that best approximate
    the curve.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The same process works for higher-degree polynomials or multiple features.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In polynomial least squares, the feature space expands: instead of points on
    a line, data lives in a higher-dimensional feature space \((1, x, x^2, \dots,
    x^d)\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In multifeature least squares, the column space of \(A\) spans all possible
    linear combinations of features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The least squares solution projects the observed output vector \(y\) onto this
    subspace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, whether polynomial or multifeature, the geometry is the same: projection
    onto the model space.'
  prefs: []
  type: TYPE_NORMAL
- en: Practical Challenges
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Overfitting: Higher-degree polynomials fit noise, not just signal.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Multicollinearity: Features may be correlated, making \(A^T A\) nearly singular.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scaling: Features with different magnitudes should be normalized.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Regularization: Adding penalty terms (ridge or LASSO) stabilizes the solution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Regression in Statistics: Extending linear regression to handle multiple predictors
    or polynomial terms.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Basis expansion for feature engineering (before neural nets,
    this was the standard).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Engineering: Curve fitting for calibration, modeling, and prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Economics: Forecasting models with many variables (inflation, interest rates,
    spending).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics and Chemistry: Polynomial regression to model experimental data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Polynomial least squares captures curvature in data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multifeature least squares allows multiple predictors to explain outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both generalizations turn linear algebra into a practical modeling tool across
    science and society.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fit a quadratic curve through points \((0,1), (1,2), (2,5), (3,10)\). Compare
    to a straight-line fit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a multifeature design matrix for predicting exam scores based on hours
    studied, sleep, and prior grades.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that polynomial regression is just linear regression on transformed features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Derive the bias–variance tradeoff in polynomial least squares-why
    higher degrees increase variance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Polynomial and multifeature least squares extend the reach of linear algebra
    from straight lines to complex patterns, giving us a universal framework for modeling
    relationships in data.
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 9\. SVD, PCA, and conditioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opening
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 81\. Singular Values and SVD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Singular Value Decomposition (SVD) is one of the most powerful tools in
    linear algebra. It generalizes eigen-decomposition, works for all rectangular
    matrices (not just square ones), and provides deep insights into geometry, computation,
    and data analysis. At its core, the SVD tells us that every matrix can be factored
    into three pieces: rotations/reflections, scaling, and rotations/reflections again.'
  prefs: []
  type: TYPE_NORMAL
- en: Definition of SVD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For any real \(m \times n\) matrix \(A\), the SVD is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = U \Sigma V^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(U\) is an \(m \times m\) orthogonal matrix (columns = left singular vectors).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\Sigma\) is an \(m \times n\) diagonal matrix with nonnegative entries \(\sigma_1
    \geq \sigma_2 \geq \dots \geq 0\) (singular values).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(V\) is an \(n \times n\) orthogonal matrix (columns = right singular vectors).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if \(A\) is rectangular or not diagonalizable, this factorization always
    exists.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Meaning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The SVD describes how \(A\) transforms space:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First rotation/reflection: Multiply by \(V^T\) to rotate or reflect coordinates
    into the right singular vector basis.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scaling: Multiply by \(\Sigma\), stretching/shrinking each axis by a singular
    value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Second rotation/reflection: Multiply by \(U\) to reorient into the output space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, \(A\) acts as a rotation, followed by scaling, followed by another rotation.
  prefs: []
  type: TYPE_NORMAL
- en: Singular Values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The singular values \(\sigma_i\) are the square roots of the eigenvalues of
    \(A^T A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They measure how much \(A\) stretches space in particular directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The largest singular value \(\sigma_1\) is the operator norm of \(A\): the
    maximum stretch factor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If some singular values are zero, they correspond to directions collapsed by
    \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example in \(\mathbb{R}^2\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 3 & 1 \\ 0 & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Compute \(A^T A = \begin{bmatrix} 9 & 3 \\ 3 & 5 \end{bmatrix}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Find its eigenvalues: \(\lambda_1, \lambda_2 = 10 \pm \sqrt{10}\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Singular values: \(\sigma_i = \sqrt{\lambda_i}\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The corresponding eigenvectors form the right singular vectors \(V\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Left singular vectors \(U\) are obtained by \(U = AV/\Sigma\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This decomposition reveals how \(A\) reshapes circles into ellipses.
  prefs: []
  type: TYPE_NORMAL
- en: Links to Eigen-Decomposition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Eigen-decomposition works only for square, diagonalizable matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD works for all matrices, square or rectangular, diagonalizable or not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of eigenvalues (which may be complex or negative), we get singular values
    (always real and nonnegative).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvectors can fail to exist in a full basis; singular vectors always form
    orthonormal bases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data Compression: Truncate small singular values to approximate matrices with
    fewer dimensions (used in JPEG).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Principal Component Analysis (PCA): SVD on centered data finds principal components,
    directions of maximum variance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Least Squares Problems: SVD provides stable solutions, even for ill-conditioned
    or singular systems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Noise Filtering: Discard small singular values to remove noise in signals and
    images.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Stability: SVD helps diagnose conditioning-how sensitive solutions
    are to input errors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SVD is the “Swiss army knife” of linear algebra: versatile, always applicable,
    and rich in interpretation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides geometric, algebraic, and computational clarity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is indispensable for modern applications in machine learning, statistics,
    engineering, and physics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the SVD of
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}1 & 0 \\ 0 & 2 \\ 0 & 0\end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Interpret the scaling and rotations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Show that for any vector \(x\), \(\|Ax\| \leq \sigma_1 \|x\|\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use SVD to approximate the matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix}1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1\end{bmatrix} \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: with rank 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Challenge: Prove that the Frobenius norm of \(A\) is the square root of the
    sum of squares of its singular values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The singular value decomposition is universal: every matrix can be dissected
    into rotations and scalings, revealing its structure and enabling powerful techniques
    across mathematics and applied sciences.'
  prefs: []
  type: TYPE_NORMAL
- en: 82\. Geometry of SVD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Singular Value Decomposition (SVD) is not just an algebraic factorization-it
    has a precise geometric meaning. It explains exactly how any linear transformation
    reshapes space: stretching, rotating, compressing, and possibly collapsing dimensions.
    Understanding this geometry turns SVD from a formal tool into an intuitive picture
    of what matrices do.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformation of the Unit Sphere
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Take the unit sphere (or circle, in 2D) in the input space. When we apply a
    matrix \(A\):'
  prefs: []
  type: TYPE_NORMAL
- en: The sphere is transformed into an ellipsoid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The axes of this ellipsoid correspond to the right singular vectors \(v_i\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lengths of the axes are the singular values \(\sigma_i\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The directions of the axes in the output space are the left singular vectors
    \(u_i\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, SVD tells us:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A v_i = \sigma_i u_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: Every matrix maps orthogonal basis directions into orthogonal ellipsoid axes,
    scaled by singular values.
  prefs: []
  type: TYPE_NORMAL
- en: Step-by-Step Geometry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The decomposition \(A = U \Sigma V^T\) can be read geometrically:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rotate/reflect by \(V^T\): Align input coordinates with the “principal directions”
    of \(A\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scale by \(\Sigma\): Stretch or compress each axis by its singular value. Some
    singular values may be zero, flattening dimensions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Rotate/reflect by \(U\): Reorient the scaled axes into the output space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This process is universal: no matter how irregular a matrix seems, it always
    reshapes space by rotation → scaling → rotation.'
  prefs: []
  type: TYPE_NORMAL
- en: 2D Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}3 & 1 \\ 0 & 2\end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: A circle in \(\mathbb{R}^2\) is mapped into an ellipse.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ellipse’s major and minor axes align with the right singular vectors of
    \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Their lengths equal the singular values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ellipse itself is then oriented in the output plane according to the left
    singular vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This makes SVD the perfect tool for visualizing how \(A\) “distorts” geometry.
  prefs: []
  type: TYPE_NORMAL
- en: Stretching and Rank
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If all singular values are positive, the ellipsoid has full dimension (no collapse).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If some singular values are zero, \(A\) flattens the sphere along certain directions,
    lowering the rank.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rank of \(A\) equals the number of nonzero singular values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, rank-deficient matrices literally squash space into lower dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Distance and Energy Preservation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The largest singular value \(\sigma_1\) is how much \(A\) can stretch a vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The smallest nonzero singular value \(\sigma_r\) (where \(r = \text{rank}(A)\))
    measures how much the matrix compresses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The condition number \(\kappa(A) = \sigma_1 / \sigma_r\) measures distortion:
    small values mean nearly spherical stretching, large values mean extreme elongation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of the Geometry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data Compression: Keeping only the largest singular values keeps the “major
    axes” of variation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'PCA: Data is analyzed along orthogonal axes of greatest variance (singular
    vectors).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Analysis: Geometry of SVD shows why ill-conditioned systems amplify
    errors-because some directions are squashed almost flat.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Signal Processing: Elliptical distortions correspond to filtering out certain
    frequency components.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Dimensionality reduction is essentially projecting data onto
    the largest singular directions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SVD transforms algebraic equations into geometric pictures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It reveals exactly how matrices warp space, offering intuition behind abstract
    operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By interpreting ellipses, singular values, and orthogonal vectors, we gain visual
    clarity for problems in data, physics, and computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Draw the unit circle in \(\mathbb{R}^2\), apply the matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}2 & 0 \\ 1 & 3\end{bmatrix}, \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: and sketch the resulting ellipse. Identify its axes and lengths.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Verify numerically that \(Av_i = \sigma_i u_i\) for computed singular vectors
    and singular values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a rank-1 matrix, sketch how the unit circle collapses to a line segment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Prove that the set of vectors with maximum stretch under \(A\) are
    precisely the first right singular vectors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The geometry of SVD gives us a universal lens: every linear transformation
    is a controlled distortion of space, built from orthogonal rotations and directional
    scalings.'
  prefs: []
  type: TYPE_NORMAL
- en: 83\. Relation to Eigen-Decompositions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Singular Value Decomposition (SVD) is often introduced as something entirely
    new, but it is deeply tied to eigen-decomposition. In fact, singular values and
    singular vectors emerge from the eigen-decomposition of certain symmetric matrices
    constructed from \(A\). Understanding this connection shows why SVD always exists,
    why singular values are nonnegative, and how it generalizes eigen-analysis to
    all matrices, even rectangular ones.
  prefs: []
  type: TYPE_NORMAL
- en: Eigen-Decomposition Recap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a square matrix \(M \in \mathbb{R}^{n \times n}\), an eigen-decomposition
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ M = X \Lambda X^{-1}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Lambda\) is a diagonal matrix of eigenvalues and the columns of \(X\)
    are eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'However:'
  prefs: []
  type: TYPE_NORMAL
- en: Not all matrices are diagonalizable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalues may be complex.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rectangular matrices don’t have eigenvalues at all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is where SVD provides a universal framework.
  prefs: []
  type: TYPE_NORMAL
- en: From \(A^T A\) to Singular Values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For any \(m \times n\) matrix \(A\):'
  prefs: []
  type: TYPE_NORMAL
- en: Consider the symmetric, positive semidefinite matrix \(A^T A \in \mathbb{R}^{n
    \times n}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Symmetry ensures all eigenvalues are real.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Positive semidefiniteness ensures they are nonnegative.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The eigenvalues of \(A^T A\) are squares of the singular values of \(A\):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \lambda_i(A^T A) = \sigma_i^2. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The eigenvectors of \(A^T A\) are the right singular vectors \(v_i\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Similarly, for \(AA^T\), eigenvalues are the same \(\sigma_i^2\), and eigenvectors
    are the left singular vectors \(u_i\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Thus:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Av_i = \sigma_i u_i, \quad A^T u_i = \sigma_i v_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: This pair of relationships binds eigen-decomposition and SVD together.
  prefs: []
  type: TYPE_NORMAL
- en: Why Eigen-Decomposition Is Not Enough
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Eigen-decomposition requires a square matrix. SVD works for rectangular matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalues can be negative or complex; singular values are always real and
    nonnegative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvectors may not exist as a complete basis; singular vectors always form
    orthonormal bases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In short, SVD provides the robustness that eigen-decomposition lacks.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}3 & 0 \\ 4 & 0 \\ 0 & 5\end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Compute \(A^T A = \begin{bmatrix}25 & 0 \\ 0 & 25\end{bmatrix}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Eigenvalues: \(25, 25\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singular values: \(\sigma_1 = \sigma_2 = 5\).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Right singular vectors are eigenvectors of \(A^T A\). Here, they form the standard
    basis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Left singular vectors come from \(Av_i / \sigma_i\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So the geometry of SVD is fully encoded in eigen-analysis of \(A^T A\) and \(AA^T\).
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Picture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Eigenvectors of \(A^T A\) describe directions in input space where \(A\) stretches
    without mixing directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvectors of \(AA^T\) describe the corresponding directions in output space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular values tell us how much stretching occurs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, SVD is essentially eigen-decomposition in disguise-but applied to the
    right symmetric companions.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of the Connection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PCA: Data covariance matrix \(X^T X\) uses eigen-decomposition, but PCA is
    implemented with SVD directly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Numerical Methods: Algorithms for SVD rely on eigen-analysis of \(A^T A\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stability Analysis: The relationship ensures singular values are reliable measures
    of conditioning.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Signal Processing: Power in signals (variance) is explained by eigenvalues
    of covariance, which connect to singular values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Kernel PCA and related methods depend on this link to handle
    nonlinear features.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SVD explains every matrix transformation in terms of orthogonal bases and scalings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its relationship with eigen-decomposition ensures that SVD is not an alien tool,
    but a generalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The eigenview shows why SVD is guaranteed to exist and why singular values are
    always real and nonnegative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Prove that if \(v\) is an eigenvector of \(A^T A\) with eigenvalue \(\lambda\),
    then \(Av\) is either zero or a left singular vector of \(A\) with singular value
    \(\sqrt{\lambda}\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}1 & 2 \\ 2 & 1\end{bmatrix}, \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: compute both eigen-decomposition and SVD. Compare the results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Show that \(A^T A\) and \(AA^T\) always share the same nonzero eigenvalues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Challenge: Explain why an orthogonal diagonalization of \(A^T A\) is enough
    to guarantee existence of the full SVD of \(A\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The relationship between SVD and eigen-decomposition unifies two of linear
    algebra’s deepest ideas: every matrix transformation is built from eigen-geometry,
    stretched into a form that always exists and always makes sense.'
  prefs: []
  type: TYPE_NORMAL
- en: 84\. Low-Rank Approximation (Best Small Models)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A central idea in data analysis, scientific computing, and machine learning
    is that many datasets or matrices are far more complicated in raw form than they
    truly need to be. Much of the apparent complexity hides redundancy, noise, or
    low-dimensional patterns. Low-rank approximation is the process of compressing
    a large, complicated matrix into a smaller, simpler version that preserves the
    most important information. This concept, grounded in the Singular Value Decomposition
    (SVD), lies at the heart of dimensionality reduction, recommender systems, and
    modern AI.
  prefs: []
  type: TYPE_NORMAL
- en: The General Problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we have a matrix \(A \in \mathbb{R}^{m \times n}\), perhaps representing:'
  prefs: []
  type: TYPE_NORMAL
- en: An image, with rows as pixels and columns as color channels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ratings table, with rows as users and columns as movies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A word embedding matrix, with rows as words and columns as features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Often, \(A\) is very large but highly structured. The question is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Can we find a smaller matrix \(B\) of rank \(k\) (where \(k \ll \min(m, n)\))
    that approximates \(A\) well?*'
  prefs: []
  type: TYPE_NORMAL
- en: Rank and Complexity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The rank of a matrix is the number of independent directions it encodes. High
    rank means complexity; low rank means redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: 'A rank-1 matrix can be written as an outer product of two vectors: \(uv^T\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rank-\(k\) matrix is a sum of \(k\) such outer products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limiting rank controls how much structure we allow the approximation to capture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SVD Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The SVD provides a natural decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = U \Sigma V^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where singular values \(\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_r\) measure
    importance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To approximate \(A\) with rank \(k\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A_k = U_k \Sigma_k V_k^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we keep only the top \(k\) singular values and vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not just a heuristic: it is the Eckart–Young theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: Among all rank-\(k\) matrices, \(A_k\) minimizes the error \(\|A - B\|\) (both
    in Frobenius and spectral norm).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Thus, SVD provides the *best possible* low-rank approximation.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each singular value \(\sigma_i\) measures how strongly \(A\) stretches in the
    direction of singular vector \(v_i\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping the top \(k\) singular values means keeping the most important stretches
    and ignoring weaker directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The approximation captures the “essence” of \(A\) while discarding small, noisy,
    or redundant effects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Images A grayscale image can be stored as a matrix of pixel intensities. Using
    SVD, one can compress it by keeping only the largest singular values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '\(k = 10\): blurry but recognizable image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\(k = 50\): much sharper, yet storage cost is far less than full.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\(k = 200\): nearly indistinguishable from the original.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is practical image compression: fewer numbers, same perception.'
  prefs: []
  type: TYPE_NORMAL
- en: Recommender Systems Consider a user–movie rating matrix. Although it may be
    huge, the true patterns (genre preferences, popularity trends) live in a low-dimensional
    subspace. A rank-\(k\) approximation captures these patterns, predicting missing
    ratings by filling in the structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Natural Language Processing (NLP) Word embeddings often arise from co-occurrence
    matrices. Low-rank approximation via SVD extracts semantic structure, enabling
    words like “king,” “queen,” and “crown” to cluster together.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Error and Trade-Offs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Error decay: If singular values drop quickly, small \(k\) gives a great approximation.
    If they decay slowly, more terms are needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Energy preserved: The squared singular values \(\sigma_i^2\) represent variance
    captured. Keeping the first \(k\) terms preserves most of the “energy.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balance: Too low rank = oversimplification (loss of structure). Too high rank
    = no compression.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical Computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For very large matrices, full SVD is expensive (\(O(mn^2)\) for \(m \geq n\)).
    Alternatives include:'
  prefs: []
  type: TYPE_NORMAL
- en: Truncated SVD algorithms (Lanczos, randomized methods).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterative methods that compute only the top \(k\) singular values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incremental approaches that update low-rank models as new data arrives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are vital in modern data science, where datasets often have millions of
    entries.
  prefs: []
  type: TYPE_NORMAL
- en: Analogy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Music playlist: Imagine a playlist with hundreds of songs, but most are variations
    on a few themes. A low-rank approximation is like keeping only the core melodies
    while discarding repetitive riffs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Photograph compression: Keeping only the brightest and most important strokes
    of light, while ignoring faint and irrelevant details.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Book summary: Instead of the full text, you read the essential plot points.
    That’s low-rank approximation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Reveals hidden structure in high-dimensional data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduces storage and computational cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filters noise while preserving the signal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides the foundation for PCA, recommender systems, and dimensionality reduction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take a small \(5 \times 5\) random matrix. Compute its SVD. Construct the best
    rank-1 approximation. Compare to the original.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download a grayscale image (e.g., \(256 \times 256\)). Reconstruct it with 10,
    50, and 100 singular values. Visually compare.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prove the Eckart–Young theorem for the spectral norm: why can no other rank-\(k\)
    approximation do better than truncated SVD?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a dataset with many features, compute PCA and explain why it is equivalent
    to finding a low-rank approximation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Low-rank approximation shows how linear algebra captures the essence of complexity:
    most of what matters lives in a small number of dimensions. The art is in finding
    and using them effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 85\. Principal Component Analysis (Variance and Directions)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA) is one of the most widely used techniques
    in statistics, data analysis, and machine learning. It provides a method to reduce
    the dimensionality of a dataset while retaining as much important information
    as possible. The central insight is that data often varies more strongly in some
    directions than others, and by focusing on those directions we can summarize the
    dataset with fewer dimensions, less noise, and more interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Question
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we have data points in high-dimensional space, say \(x_1, x_2, \dots,
    x_m \in \mathbb{R}^n\). Each point might be:'
  prefs: []
  type: TYPE_NORMAL
- en: A face image flattened into thousands of pixels.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A customer’s shopping history across hundreds of products.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A gene expression profile across thousands of genes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Storing and working with all features directly is expensive, and many features
    may be redundant or correlated. PCA asks:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Can we re-express this data in a smaller set of directions that capture the
    most variability?*'
  prefs: []
  type: TYPE_NORMAL
- en: Variance as Information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The guiding principle of PCA is variance.
  prefs: []
  type: TYPE_NORMAL
- en: Variance measures how spread out the data is along a direction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High variance directions capture meaningful structure (e.g., different facial
    expressions, major spending habits).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low variance directions often correspond to noise or unimportant fluctuations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, PCA searches for the directions (called principal components) along which
    the variance of the data is maximized.
  prefs: []
  type: TYPE_NORMAL
- en: Centering and Covariance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To begin, we center the data by subtracting the mean vector:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_c = X - \mathbf{1}\mu^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mu\) is the average of all data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'The covariance matrix is then:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ C = \frac{1}{m} X_c^T X_c. \]
  prefs: []
  type: TYPE_NORMAL
- en: The diagonal entries measure variance of each feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Off-diagonal entries measure how features vary together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding principal components is equivalent to finding the eigenvectors of this
    covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: The Eigenview
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The eigenvectors of \(C\) are the directions (principal components).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The corresponding eigenvalues tell us how much variance lies along each component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sorting eigenvalues from largest to smallest gives the most informative to least
    informative directions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we keep the top \(k\) eigenvectors, we project data into a \(k\)-dimensional
    subspace that preserves most variance.
  prefs: []
  type: TYPE_NORMAL
- en: The SVD View
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another perspective uses the Singular Value Decomposition (SVD):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_c = U \Sigma V^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Columns of \(V\) are the principal directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular values squared (\(\sigma_i^2\)) correspond to eigenvalues of the covariance
    matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projecting onto the first \(k\) columns of \(V\) gives the reduced representation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This makes PCA and SVD essentially the same computation.
  prefs: []
  type: TYPE_NORMAL
- en: A Simple Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine we measure height and weight of 1000 people. Plotting them shows a
    strong correlation: taller people are often heavier. The cloud of points stretches
    along a diagonal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA’s first component is this diagonal line: the direction of maximum variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second component is perpendicular, capturing the much smaller differences
    (like people of equal height but slightly different weights).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping only the first component reduces two features into one while retaining
    most of the information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometric Picture
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PCA rotates the coordinate system so that axes align with directions of greatest
    variance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projecting onto the top \(k\) components flattens the data into a lower-dimensional
    space, like flattening a tilted pancake onto its broadest plane.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data Compression: Reduce storage by keeping only leading components (e.g.,
    compressing images).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Noise Reduction: Small-variance directions often correspond to measurement
    noise; discarding them yields cleaner data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Visualization: Reducing data to 2D or 3D for scatterplots helps us see clusters
    and patterns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Preprocessing in Machine Learning: Many models train faster and generalize
    better on PCA-transformed data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Genomics and Biology: PCA finds major axes of variation across thousands of
    genes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finance: PCA summarizes correlated movements of stocks into a few principal
    “factors.”'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Trade-Offs and Limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Interpretability: Principal components are linear combinations of original
    features, sometimes hard to explain in plain terms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linearity: PCA only captures linear relationships; nonlinear methods (like
    kernel PCA, t-SNE, or UMAP) may be better for curved manifolds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaling: Features must be normalized properly; otherwise, PCA might overemphasize
    units with large raw variance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Global Method: PCA captures overall variance, not local structures (e.g., small
    clusters within the data).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematical Guarantees
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PCA has an optimality guarantee:'
  prefs: []
  type: TYPE_NORMAL
- en: Among all \(k\)-dimensional linear subspaces, the PCA subspace minimizes the
    reconstruction error (squared Euclidean distance between data and its projection).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is essentially the low-rank approximation theorem seen earlier, applied
    to covariance matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PCA shows how linear algebra transforms raw data into insight. By focusing on
    variance, it provides a principled way to filter noise, compress information,
    and reveal hidden patterns. It is simple, computationally efficient, and foundational-almost
    every modern data pipeline uses PCA, explicitly or implicitly.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take a dataset of two correlated features (like height and weight). Compute
    the covariance matrix, eigenvectors, and project onto the first component. Visualize
    before and after.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a grayscale image stored as a matrix, flatten it into vectors and apply
    PCA. How many components are needed to reconstruct it with 90% accuracy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use PCA on the famous Iris dataset (4 features). Plot the data in 2D using the
    first two components. Notice how species separate in this reduced space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that the first principal component is the unit vector \(v\) that maximizes
    \(\|X_c v\|^2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'PCA distills complexity into clarity: it tells us not just where the data is,
    but where it *really goes*.'
  prefs: []
  type: TYPE_NORMAL
- en: 86\. Pseudoinverse (Moore–Penrose) and Solving Ill-Posed Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In linear algebra, the inverse of a matrix is a powerful tool: if \(A\) is
    invertible, then solving \(Ax = b\) is as simple as \(x = A^{-1}b\). But what
    happens when \(A\) is not square, or not invertible? In practice, this is the
    norm: many problems involve rectangular matrices (more equations than unknowns,
    or more unknowns than equations), or square matrices that are singular. The Moore–Penrose
    pseudoinverse, usually denoted \(A^+\), generalizes the idea of an inverse to
    all matrices, providing a systematic way to find solutions-or best approximations-when
    ordinary inversion fails.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Ordinary Inverses Fail
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Non-square matrices: If \(A\) is \(m \times n\) with \(m \neq n\), no standard
    inverse exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singular matrices: Even if \(A\) is square, if \(\det(A) = 0\), it has no inverse.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ill-posed problems: In real-world data, exact solutions may not exist (inconsistent
    systems) or may not be unique (underdetermined systems).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite these obstacles, we still want a systematic way to solve or approximate
    \(Ax = b\).
  prefs: []
  type: TYPE_NORMAL
- en: Definition of the Pseudoinverse
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Moore–Penrose pseudoinverse \(A^+\) is defined as the unique matrix that
    satisfies four properties:'
  prefs: []
  type: TYPE_NORMAL
- en: \(AA^+A = A\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(A^+AA^+ = A^+\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \((AA^+)^T = AA^+\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \((A^+A)^T = A^+A\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These conditions ensure \(A^+\) acts as an “inverse” in the broadest consistent
    sense.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing the Pseudoinverse with SVD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given the SVD of \(A\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = U \Sigma V^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(\Sigma\) is diagonal with singular values \(\sigma_1, \dots, \sigma_r\),
    the pseudoinverse is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^+ = V \Sigma^+ U^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(\Sigma^+\) is formed by inverting nonzero singular values and transposing
    the matrix. Specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: If \(\sigma_i \neq 0\), replace it with \(1/\sigma_i\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\sigma_i = 0\), leave it as 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This definition works for all matrices, square or rectangular.
  prefs: []
  type: TYPE_NORMAL
- en: Solving Linear Systems with \(A^+\)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Overdetermined systems (\(m > n\), more equations than unknowns):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Often no exact solution exists.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pseudoinverse gives the least-squares solution:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ x = A^+ b, \]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: which minimizes \(\|Ax - b\|\).
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Underdetermined systems (\(m < n\), more unknowns than equations):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Infinitely many solutions exist.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The pseudoinverse chooses the solution with the smallest norm:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ x = A^+ b, \]
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: which minimizes \(\|x\|\) among all solutions.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Square but singular systems:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some solutions exist, but not uniquely.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The pseudoinverse again picks the least-norm solution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 1: Overdetermined'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we want to solve:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix}1 & 1 \\ 1 & -1 \\ 1 & 0\end{bmatrix} x = \begin{bmatrix}2
    \\ 0 \\ 1\end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This \(3 \times 2\) system has no exact solution. Using the pseudoinverse, we
    obtain the least-squares solution that best fits all three equations simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: Underdetermined'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For
  prefs: []
  type: TYPE_NORMAL
- en: \[ \begin{bmatrix}1 & 0 & 0 \\ 0 & 1 & 0\end{bmatrix} x = \begin{bmatrix}3 \\
    4\end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'the system has infinitely many solutions because \(x_3\) is free. The pseudoinverse
    gives:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x = \begin{bmatrix}3 \\ 4 \\ 0\end{bmatrix}, \]
  prefs: []
  type: TYPE_NORMAL
- en: choosing the solution with minimum norm.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The pseudoinverse acts like projecting onto subspaces.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For overdetermined systems, it projects \(b\) onto the column space of \(A\),
    then finds the closest \(x\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For underdetermined systems, it picks the point in the solution space closest
    to the origin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So \(A^+\) embodies the principle of “best possible inverse” under the circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Least-Squares Regression: Solving \(\min_x \|Ax - b\|^2\) via \(A^+\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Signal Processing: Reconstructing signals from incomplete or noisy data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control Theory: Designing inputs when exact control is impossible.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Training models with non-invertible design matrices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Statistics: Computing generalized inverses of covariance matrices.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sensitive to very small singular values: numerical instability may occur.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization (like ridge regression) is often preferred in noisy settings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computationally expensive for very large matrices, though truncated SVD can
    help.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The pseudoinverse is a unifying idea: it handles inconsistent, underdetermined,
    or singular problems with one formula. It ensures we always have a principled
    answer, even when classical algebra says “no solution” or “infinitely many solutions.”
    In real data analysis, almost every problem is ill-posed to some degree, making
    the pseudoinverse a practical cornerstone of modern applied linear algebra.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the pseudoinverse of a simple \(2 \times 2\) singular matrix by hand
    using SVD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solve both an overdetermined (\(3 \times 2\)) and underdetermined (\(2 \times
    3\)) system using \(A^+\). Compare with intuitive expectations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore what happens numerically when singular values are very small. Try truncating
    them-this connects to regularization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Moore–Penrose pseudoinverse shows that even when linear systems are “broken,”
    linear algebra still provides a systematic way forward.
  prefs: []
  type: TYPE_NORMAL
- en: 87\. Conditioning and Sensitivity (How Errors Amplify)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Linear algebra is not only about exact solutions-it is also about how *stable*
    those solutions are when data is perturbed. In real-world applications, every
    dataset contains noise: measurement errors in physics experiments, rounding errors
    in financial computations, or floating-point precision limits in numerical software.
    Conditioning is the study of how sensitive the solution of a problem is to small
    changes in input. A well-conditioned problem reacts gently to perturbations; an
    ill-conditioned one amplifies errors dramatically.'
  prefs: []
  type: TYPE_NORMAL
- en: The Basic Idea
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we solve the linear system:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Ax = b. \]
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine we slightly change \(b\) to \(b + \delta b\). The new solution is
    \(x + \delta x\).
  prefs: []
  type: TYPE_NORMAL
- en: If \(\|\delta x\|\) is about the same size as \(\|\delta b\|\), the problem
    is well-conditioned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\|\delta x\|\) is much larger, the problem is ill-conditioned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conditioning measures this amplification factor.
  prefs: []
  type: TYPE_NORMAL
- en: Condition Number
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The central tool is the condition number of a matrix \(A\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa(A) = \|A\| \cdot \|A^{-1}\|, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\|\cdot\|\) is a matrix norm (often the 2-norm).
  prefs: []
  type: TYPE_NORMAL
- en: If \(\kappa(A)\) is close to 1, the problem is well-conditioned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If \(\kappa(A)\) is large (say, \(10^6\) or higher), the problem is ill-conditioned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpretation:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\kappa(A)\) estimates the maximum relative error in the solution compared
    to the relative error in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practical terms, every digit of accuracy in \(b\) may be lost in \(x\) if
    \(\kappa(A)\) is too large.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular Values and Conditioning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Condition number in 2-norm can be expressed using singular values:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \kappa(A) = \frac{\sigma_{\max}}{\sigma_{\min}}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\sigma_{\max}\) and \(\sigma_{\min}\) are the largest and smallest singular
    values of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: If the smallest singular value is tiny compared to the largest, \(A\) nearly
    collapses some directions, making inversion unstable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This explains why nearly singular matrices are so problematic in numerical computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example 1: A Stable System'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}2 & 0 \\ 0 & 3\end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, \(\sigma_{\max} = 3, \sigma_{\min} = 2\). So \(\kappa(A) = 3/2 = 1.5\).
    Very well-conditioned: small changes in input produce small changes in output.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2: An Ill-Conditioned System'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}1 & 1 \\ 1 & 1.0001\end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The determinant is very small, so the system is nearly singular.
  prefs: []
  type: TYPE_NORMAL
- en: One singular value is about 2.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other is about 0.0001.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Condition number: \(\kappa(A) \approx 20000\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means even tiny changes in \(b\) can wildly change \(x\).
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A matrix transforms a unit sphere into an ellipse.
  prefs: []
  type: TYPE_NORMAL
- en: The longest axis of the ellipse = \(\sigma_{\max}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shortest axis = \(\sigma_{\min}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ratio \(\sigma_{\max} / \sigma_{\min}\) shows how stretched the transformation
    is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the ellipse is nearly flat, directions aligned with the short axis almost
    vanish, and recovering them is highly unstable.
  prefs: []
  type: TYPE_NORMAL
- en: Why Conditioning Matters in Computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Numerical Precision: Computers store numbers with limited precision (floating-point).
    An ill-conditioned system magnifies rounding errors, leading to unreliable results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Regression: In statistics, highly correlated features make the design matrix
    ill-conditioned, destabilizing coefficient estimates.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Ill-conditioning leads to unstable training, exploding or
    vanishing gradients.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Engineering: Control systems based on ill-conditioned models may be hypersensitive
    to measurement errors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Techniques for Handling Ill-Conditioning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Regularization: Add a penalty term, like ridge regression (\(\lambda I\)),
    to stabilize inversion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Truncated SVD: Ignore tiny singular values that only amplify noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaling and Preconditioning: Rescale data or multiply by a well-chosen matrix
    to improve conditioning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Avoiding Explicit Inverses: Use factorizations (LU, QR, SVD) rather than computing
    \(A^{-1}\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connection to Previous Topics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Pseudoinverse: Ill-conditioning is visible when singular values approach zero,
    making \(A^+\) unstable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Low-rank approximation: Truncating small singular values both compresses data
    and improves conditioning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PCA: Discarding low-variance components is essentially a conditioning improvement
    step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Conditioning bridges abstract algebra and numerical reality. Linear algebra
    promises solutions, but conditioning tells us whether those solutions are trustworthy.
    Without it, one might misinterpret noise as signal, or lose all accuracy in computations
    that look fine on paper.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the condition number of \(\begin{bmatrix}1 & 1 \\ 1 & 1.0001\end{bmatrix}\).
    Solve for \(x\) in \(Ax = b\) for several slightly different \(b\). Watch how
    solutions swing dramatically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take a dataset with nearly collinear features. Compute the condition number
    of its covariance matrix. Relate this to instability in regression coefficients.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Simulate numerical errors: Add random noise of size \(10^{-6}\) to an ill-conditioned
    system and observe solution errors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that \(\kappa(A) \geq 1\) always holds.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Conditioning reveals the hidden fragility of problems. It warns us when algebra
    says “solution exists” but computation whispers “don’t trust it.”
  prefs: []
  type: TYPE_NORMAL
- en: 88\. Matrix Norms and Singular Values (Measuring Size Properly)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In linear algebra, we often need to measure the “size” of a matrix. For vectors,
    this is straightforward: the length (norm) tells us how big the vector is. But
    for matrices, the question is more subtle: do we measure size by entries, by how
    much the matrix stretches vectors, or by some invariant property? Different contexts
    demand different answers, and matrix norms-closely tied to singular values-provide
    the framework for doing so.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Measure the Size of a Matrix?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Stability: To know how much error a matrix might amplify.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Conditioning: The ratio of largest to smallest stretching.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimization: Many algorithms minimize some matrix norm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Data analysis: Norms measure complexity or energy of data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Without norms, we cannot compare matrices, analyze sensitivity, or judge approximation
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Norms from Vector Norms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A natural way to define a matrix norm is to ask: *How much does this matrix
    stretch vectors?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, for a given vector norm \(\|\cdot\|\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\| = \max_{x \neq 0} \frac{\|Ax\|}{\|x\|}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This is called the induced matrix norm.
  prefs: []
  type: TYPE_NORMAL
- en: The 2-Norm and Singular Values
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When we use the Euclidean norm (\(\|x\|_2\)) for vectors, the induced matrix
    norm becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|A\|_2 = \sigma_{\max}(A), \]
  prefs: []
  type: TYPE_NORMAL
- en: the largest singular value of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: This means the 2-norm measures the *maximum stretching factor*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geometrically: \(A\) maps the unit sphere into an ellipse; \(\|A\|_2\) is the
    length of the ellipse’s longest axis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This link makes singular values the natural language for matrix size.
  prefs: []
  type: TYPE_NORMAL
- en: Other Common Norms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Frobenius Norm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \|A\|_F = \sqrt{\sum_{i,j} |a_{ij}|^2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Equivalent to the Euclidean length of all entries stacked in one big vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Can also be expressed as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \|A\|_F^2 = \sum_i \sigma_i^2. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Often used in data science and machine learning because it is easy to compute
    and differentiable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1-Norm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \|A\|_1 = \max_j \sum_i |a_{ij}|, \]
  prefs: []
  type: TYPE_NORMAL
- en: the maximum absolute column sum.
  prefs: []
  type: TYPE_NORMAL
- en: Infinity Norm
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \|A\|_\infty = \max_i \sum_j |a_{ij}|, \]
  prefs: []
  type: TYPE_NORMAL
- en: the maximum absolute row sum.
  prefs: []
  type: TYPE_NORMAL
- en: Both are computationally cheap, useful in numerical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Nuclear Norm (Trace Norm)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \|A\|_* = \sum_i \sigma_i, \]
  prefs: []
  type: TYPE_NORMAL
- en: the sum of singular values.
  prefs: []
  type: TYPE_NORMAL
- en: Important in low-rank approximation and machine learning (matrix completion,
    recommender systems).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular Values as the Unifying Thread
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Spectral norm (2-norm): maximum singular value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frobenius norm: root of the sum of squared singular values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nuclear norm: sum of singular values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, norms capture different ways of summarizing singular values: maximum,
    sum, or energy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Small Matrix'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}3 & 4 \\ 0 & 0\end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Singular values: \(\sigma_1 = 5, \sigma_2 = 0\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\|A\|_2 = 5\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\|A\|_F = \sqrt{3^2 + 4^2} = 5\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\|A\|_* = 5\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, different norms coincide, but generally they highlight different aspects
    of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Intuition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '2-norm: “How much can \(A\) stretch a vector?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frobenius norm: “What is the overall energy in all entries?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '1-norm / ∞-norm: “What is the heaviest column or row load?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nuclear norm: “How much total stretching power does \(A\) have?”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each is a lens, giving a different perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Numerical Stability: Condition number \(\kappa(A) = \sigma_{\max}/\sigma_{\min}\)
    uses the spectral norm.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Nuclear norm is used for matrix completion (Netflix Prize).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Image Compression: Frobenius norm measures reconstruction error.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Control Theory: 1-norm and ∞-norm bound system responses.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Optimization: Norms serve as penalties or constraints, shaping solutions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Matrix norms provide the language to compare, approximate, and control matrices.
    Singular values ensure that this language is not arbitrary but grounded in geometry.
    Together, they explain how matrices distort space, how error grows, and how we
    can measure complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For \(A = \begin{bmatrix}1 & 2 \\ 3 & 4\end{bmatrix}\), compute \(\|A\|_1\),
    \(\|A\|_\infty\), \(\|A\|_F\), and \(\|A\|_2\) (using SVD for the last). Compare.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that \(\|A\|_F^2 = \sum \sigma_i^2\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that \(\|A\|_2 \leq \|A\|_F \leq \|A\|_*\). Interpret geometrically.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Consider a rank-1 matrix \(uv^T\). What are its norms? Which are equal?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Matrix norms and singular values are the measuring sticks of linear algebra-they
    tell us not just how big a matrix is, but how it acts, where it is stable, and
    when it is fragile.
  prefs: []
  type: TYPE_NORMAL
- en: 89\. Regularization (Ridge/Tikhonov to Tame Instability)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When solving linear systems or regression problems, instability often arises
    because the system is ill-conditioned: tiny errors in data lead to huge swings
    in the solution. Regularization is the strategy of *adding stability* by deliberately
    modifying the problem, sacrificing exactness for robustness. The two most common
    approaches-ridge regression and Tikhonov regularization-embody this principle.'
  prefs: []
  type: TYPE_NORMAL
- en: The Problem of Instability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider the least-squares problem:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_x \|Ax - b\|_2^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'If \(A\) has nearly dependent columns, or if \(\sigma_{\min}(A)\) is very small,
    then:'
  prefs: []
  type: TYPE_NORMAL
- en: Solutions are unstable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coefficients \(x\) can explode in magnitude.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictions vary wildly with small changes in \(b\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization modifies the objective so that the solution prefers stability
    over exactness.
  prefs: []
  type: TYPE_NORMAL
- en: Ridge / Tikhonov Regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The modified problem is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_x \big( \|Ax - b\|_2^2 + \lambda \|x\|_2^2 \big), \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\lambda > 0\) is the regularization parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The first term enforces data fit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second term penalizes large coefficients, discouraging unstable solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is called ridge regression in statistics and Tikhonov regularization in
    numerical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The Closed-Form Solution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Expanding the objective and differentiating gives:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_\lambda = (A^T A + \lambda I)^{-1} A^T b. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Key points:'
  prefs: []
  type: TYPE_NORMAL
- en: The added \(\lambda I\) makes the matrix invertible, even if \(A^T A\) is singular.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As \(\lambda \to 0\), the solution approaches the ordinary least-squares solution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As \(\lambda \to \infty\), the solution shrinks toward 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD View
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If \(A = U \Sigma V^T\), then the least-squares solution is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x = \sum_i \frac{u_i^T b}{\sigma_i} v_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: If \(\sigma_i\) is very small, the term \(\frac{1}{\sigma_i}\) causes instability.
  prefs: []
  type: TYPE_NORMAL
- en: 'With regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ x_\lambda = \sum_i \frac{\sigma_i}{\sigma_i^2 + \lambda} (u_i^T b) v_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: Small singular values (unstable directions) are suppressed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large singular values (stable directions) are mostly preserved.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This explains why ridge regression stabilizes solutions: it damps noise-amplifying
    directions.'
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The unregularized problem fits \(b\) exactly in the column space of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization tilts the solution toward the origin, shrinking coefficients.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometrically, the feasible region (ellipsoid from \(Ax\)) intersects with a
    ball constraint from \(\|x\|_2\). The solution is where these two shapes balance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Lasso (\(\ell_1\) regularization): Replaces \(\|x\|_2^2\) with \(\|x\|_1\),
    encouraging sparse solutions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Elastic Net: Combines ridge and lasso penalties.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'General Tikhonov: Uses \(\|Lx\|_2^2\) with some matrix \(L\), tailoring the
    penalty (e.g., smoothing in signal processing).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bayesian View: Ridge regression corresponds to placing a Gaussian prior on
    coefficients.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Machine Learning: Prevents overfitting in regression and classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Signal Processing: Suppresses noise when reconstructing signals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image Reconstruction: Stabilizes inverse problems like deblurring.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Numerical PDEs: Adds smoothness constraints to solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Econometrics and Finance: Controls instability from highly correlated variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regularization transforms fragile problems into reliable ones. It acknowledges
    the reality of noise and finite precision, and instead of chasing impossible exactness,
    it provides usable, stable answers. In modern data-driven fields, almost every
    large-scale model relies on regularization for robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Solve the system \(Ax = b\) where
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}1 & 1 \\ 1 & 1.0001\end{bmatrix}, \quad b = \begin{bmatrix}2
    \\ 2\end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compare the unregularized least-squares solution with ridge-regularized solutions
    for \(\lambda = 0.01, 1, 10\).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using the SVD, show how coefficients for small singular values are shrunk.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In regression with many correlated features, compute coefficient paths as \(\lambda\)
    varies. Observe how they stabilize.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Explore image denoising: apply ridge regularization to a blurred/noisy image
    reconstruction problem.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Regularization shows the wisdom of linear algebra in practice: sometimes the
    best solution is not the exact one, but the stable one.'
  prefs: []
  type: TYPE_NORMAL
- en: 90\. Rank-Revealing QR and Practical Diagnostics (What Rank Really Is)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rank-the number of independent directions in a matrix-is central to linear algebra.
    It tells us about solvability of systems, redundancy of features, and the dimensionality
    of data. But in practice, computing rank is not as simple as counting pivots or
    checking determinants. Real-world data is noisy, nearly dependent, or high-dimensional.
    Rank-revealing QR (RRQR) factorization and related diagnostics provide stable,
    practical tools for uncovering rank and structure.
  prefs: []
  type: TYPE_NORMAL
- en: Why Rank Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linear systems: Rank determines if a system has a unique solution, infinitely
    many, or none.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data science: Rank measures intrinsic dimensionality, guiding dimensionality
    reduction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Numerics: Small singular values make effective rank ambiguous-exact vs. numerical
    rank diverge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, we need reliable algorithms to decide “how many directions matter” in
    a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Exact Rank vs. Numerical Rank
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Exact rank: Defined over exact arithmetic. A column is independent if it cannot
    be expressed as a linear combination of others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Numerical rank: In floating-point computation, tiny singular values cannot
    be trusted. A threshold \(\epsilon\) determines when we treat them as zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if the smallest singular value of \(A\) is \(10^{-12}\), and computations
    are in double precision (\(\sim 10^{-16}\)), we might consider the effective rank
    smaller than full.
  prefs: []
  type: TYPE_NORMAL
- en: The QR Factorization Recap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The basic QR factorization expresses a matrix \(A \in \mathbb{R}^{m \times
    n}\) as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = QR, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(Q\) is orthogonal (\(Q^T Q = I\)), preserving lengths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(R\) is upper triangular, holding the “essence” of \(A\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QR is stable, fast, and forms the backbone of many algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Rank-Revealing QR (RRQR)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'RRQR is an enhancement of QR with column pivoting:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A P = Q R, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(P\) is a permutation matrix that reorders columns.
  prefs: []
  type: TYPE_NORMAL
- en: The pivoting ensures that the largest independent directions come first.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The diagonal entries of \(R\) indicate which columns are significant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small values on the diagonal signal dependent (or nearly dependent) directions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In practice, RRQR allows us to approximate rank by examining the decay of \(R\)’s
    diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing RRQR and SVD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SVD: Gold standard for determining rank; singular values give exact scaling
    of each direction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RRQR: Faster and cheaper; sufficient when approximate rank is enough.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trade-off: SVD is more accurate, RRQR is more efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both are used depending on the balance of precision and cost.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix}1 & 1 & 1 \\ 1 & 1.0001 & 2 \\ 1 & 2 & 3\end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Exact arithmetic: rank = 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Numerically: second column is nearly dependent on the first. SVD shows a singular
    value near zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RRQR with pivoting identifies the near-dependence by revealing a tiny diagonal
    in \(R\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, RRQR “reveals” effective rank without fully computing SVD.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Diagnostics for Rank
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Condition Number: A high condition number suggests near-rank-deficiency.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Diagonal of R in RRQR: Monitors independence of columns.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Singular Values in SVD: Most reliable indicator, but expensive.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Determinants/Minors: Useful in theory, unstable in practice.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Data Compression: Identifying effective rank allows truncation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regression: Detecting multicollinearity by examining rank of the design matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Control Systems: Rank tests stability and controllability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine Learning: Dimensionality reduction pipelines (e.g., PCA) start with
    rank estimation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Signal Processing: Identifying number of underlying sources from mixtures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rank is simple in theory, but elusive in practice. RRQR and related diagnostics
    bridge the gap between exact mathematics and noisy data. They allow practitioners
    to say, with stability and confidence: *this is how many independent directions
    really matter.*'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Implement RRQR with column pivoting on a small \(5 \times 5\) nearly dependent
    matrix. Compare estimated rank with SVD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the relationship between diagonal entries of \(R\) and numerical rank.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a dataset with 100 features, where 95 are random noise but 5 are linear
    combinations. Use RRQR to detect redundancy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that column pivoting does not change the column space of \(A\), only its
    numerical stability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Rank-revealing QR shows that linear algebra is not only about exact formulas
    but also about practical diagnostics-knowing when two directions are truly different
    and when they are essentially the same.
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Chapter 10\. Applications and computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Opening
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 91\. 2D/3D Geometry Pipelines (Cameras, Rotations, and Transforms)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear algebra is the silent backbone of modern graphics, robotics, and computer
    vision. Every time an image is rendered on a screen, a camera captures a scene,
    or a robot arm moves in space, a series of matrix multiplications are transforming
    points from one coordinate system to another. These geometry pipelines map 3D
    reality into 2D representations, ensuring that objects appear in the correct position,
    orientation, and scale.
  prefs: []
  type: TYPE_NORMAL
- en: The Geometry of Coordinates
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A point in 3D space is represented as a column vector:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p = \begin{bmatrix} x \\ y \\ z \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'But computers often extend this to homogeneous coordinates, embedding the point
    in 4D:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_h = \begin{bmatrix} x \\ y \\ z \\ 1 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The extra coordinate allows translations to be represented as matrix multiplications,
    keeping the entire pipeline consistent: every step is just multiplying by a matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformations in 2D and 3D
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Translation Moves a point by \((t_x, t_y, t_z)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ T = \begin{bmatrix} 1 & 0 & 0 & t_x \\ 0 & 1 & 0 & t_y \\ 0 & 0 & 1 & t_z
    \\ 0 & 0 & 0 & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Scaling Expands or shrinks space along each axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ S = \begin{bmatrix} s_x & 0 & 0 & 0 \\ 0 & s_y & 0 & 0 \\ 0 & 0 & s_z & 0
    \\ 0 & 0 & 0 & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Rotation In 3D, rotation around the z-axis is:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ R_z(\theta) = \begin{bmatrix} \cos\theta & -\sin\theta & 0 & 0 \\ \sin\theta
    & \cos\theta & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similar forms exist for rotations around the x- and y-axes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each transformation is linear (or affine), and chaining them is just multiplying
    matrices.
  prefs: []
  type: TYPE_NORMAL
- en: The Camera Pipeline
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Rendering a 3D object to a 2D image follows a sequence of steps, each one a
    matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: Model Transform Moves the object from its local coordinates into world coordinates.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: View Transform Puts the camera at the origin and aligns its axes with the world,
    effectively changing the point of view.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Projection Transform Projects 3D points into 2D. Two types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Orthographic: parallel projection, no perspective.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perspective: distant objects appear smaller, closer to human vision.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example of perspective projection:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: \[ P = \begin{bmatrix} f & 0 & 0 & 0 \\ 0 & f & 0 & 0 \\ 0 & 0 & 1 & 0 \end{bmatrix},
    \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where \(f\) is focal length.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Viewport Transform Maps normalized 2D coordinates to screen pixels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This sequence-from object to image-is the geometry pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Rendering a Cube'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Start with cube vertices in local coordinates (\([-1,1]^3\)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a scaling matrix to stretch it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a rotation matrix to tilt it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a translation matrix to move it into the scene.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply a projection matrix to flatten it onto the screen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every step is linear algebra, and the final picture is the result of multiplying
    many matrices in sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Robotics Connection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Robotic arms use similar pipelines: each joint contributes a rotation or translation,
    encoded as a matrix. By multiplying them, we get the forward kinematics-the position
    and orientation of the hand given the joint angles.'
  prefs: []
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Geometry pipelines unify graphics, robotics, and vision. They show how linear
    algebra powers the everyday visuals of video games, animations, simulations, and
    even self-driving cars. Without the consistency of matrix multiplication, the
    complexity of managing transformations would be unmanageable.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write down the sequence of matrices that rotate a square by 45°, scale it by
    2, and translate it by \((3, 1)\). Multiply them to get the combined transformation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Construct a cube in 3D and simulate a perspective projection by hand for one
    vertex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a simple 2-joint robotic arm, represent each joint with a rotation matrix
    and compute the final hand position.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that composing affine transformations is closed under multiplication-why
    does this make pipelines possible?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Geometry pipelines are the bridge between abstract linear algebra and tangible
    visual and mechanical systems. They are how math becomes movement, light, and
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 92\. Computer Graphics and Robotics (Homogeneous Tricks in Action)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Linear algebra doesn’t just stay on the chalkboard-it drives the engines of
    computer graphics and robotics. Both fields need to describe and manipulate objects
    in space, often moving between multiple coordinate systems. The homogeneous coordinate
    trick-adding one extra dimension-makes this elegant: translations, scalings, and
    rotations all fit into a single framework of matrix multiplication. This uniformity
    allows efficient computation and consistent pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: Homogeneous Coordinates Recap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 2D, a point \((x, y)\) becomes \([x, y, 1]^T\). In 3D, a point \((x, y, z)\)
    becomes \([x, y, z, 1]^T\).
  prefs: []
  type: TYPE_NORMAL
- en: Why add the extra 1? Because then translations-normally not linear-become linear
    in the higher-dimensional embedding. Every affine transformation (rotations, scalings,
    shears, reflections, and translations) is just a single multiplication by a homogeneous
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ T = \begin{bmatrix} 1 & 0 & 0 & t_x \\ 0 & 1 & 0 & t_y \\ 0 & 0 & 1 & t_z
    \\ 0 & 0 & 0 & 1 \end{bmatrix}, \quad p_h' = T p_h. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This trick makes pipelines modular: just multiply the matrices in order.'
  prefs: []
  type: TYPE_NORMAL
- en: Computer Graphics Pipelines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Graphics engines (like OpenGL or DirectX) rely entirely on homogeneous transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Model Matrix: Puts the object in the scene.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Example: Rotate a car 90° and translate it 10 units forward.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'View Matrix: Positions the virtual camera.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Equivalent to moving the world so the camera sits at the origin.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Projection Matrix: Projects 3D points to 2D.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perspective projection shrinks faraway objects, orthographic doesn’t.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Viewport Matrix: Converts normalized 2D coordinates into screen pixels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Every pixel you see in a video game has passed through this stack of matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Robotics Pipelines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In robotics, the same principle applies:'
  prefs: []
  type: TYPE_NORMAL
- en: A robot arm with joints is modeled as a chain of rigid-body transformations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each joint contributes a rotation or translation matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiplying them gives the final pose of the robot’s end-effector (hand, tool,
    or gripper).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is called forward kinematics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: A 2D robotic arm with two joints:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p = R(\theta_1) T(l_1) R(\theta_2) T(l_2) \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Here \(R(\theta_i)\) are rotation matrices and \(T(l_i)\) are translations along
    the arm length. Multiplying them gives the position of the hand.
  prefs: []
  type: TYPE_NORMAL
- en: Shared Challenges in Graphics and Robotics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Precision: Numerical round-off errors can accumulate; stable algorithms are
    critical.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Speed: Both fields demand real-time computation-60 frames per second for graphics,
    millisecond reaction times for robots.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hierarchy: Objects in graphics may be nested (a car’s wheel rotates relative
    to the car), just like robot joints. Homogeneous transforms naturally handle these
    hierarchies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Inverse Problems: Graphics uses inverse transforms for camera movement; robotics
    uses them for inverse kinematics (finding joint angles to reach a point).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why Homogeneous Tricks Are Powerful
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Uniformity: One system (matrix multiplication) handles all transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Efficiency: Hardware (GPUs, controllers) can optimize matrix operations directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scalability: Works the same in 2D, 3D, or higher.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Composability: Long pipelines are just products of matrices, avoiding special
    cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Graphics: Rendering engines, VR/AR, CAD software, motion capture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Robotics: Arm manipulators, drones, autonomous vehicles, humanoid robots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Crossover: Simulation platforms use the same math to test robots and render
    virtual environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Build a 2D transformation pipeline: rotate a triangle, translate it, and project
    it into screen space. Write down the final transformation matrix.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model a simple 2-joint robotic arm. Derive the forward kinematics using homogeneous
    matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement a camera transform: place a cube at \((0,0,5)\), move the camera
    to \((0,0,0)\), and compute its 2D screen projection.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that composing a rotation and translation directly is equivalent to embedding
    them into a homogeneous matrix and multiplying.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Homogeneous coordinates are the hidden secret that lets graphics and robots
    share the same mathematical DNA. They unify how we move pixels, machines, and
    virtual worlds.
  prefs: []
  type: TYPE_NORMAL
- en: 93\. Graphs, Adjacency, and Laplacians (Networks via Matrices)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Linear algebra provides a powerful language for studying graphs-networks of
    nodes connected by edges. From social networks to electrical circuits, from the
    internet’s structure to biological pathways, graphs appear everywhere. Matrices
    give graphs a numerical form, making it possible to analyze their structure using
    algebraic techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Graph Basics Recap
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A graph \(G = (V, E)\) has a set of vertices \(V\) (nodes) and edges \(E\) (connections).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphs may be undirected or directed, weighted or unweighted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many graph properties-connectivity, flow, clusters-can be studied through matrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Adjacency Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For a graph with \(n\) vertices, the adjacency matrix \(A \in \mathbb{R}^{n
    \times n}\) encodes connections:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A_{ij} = \begin{cases} w_{ij}, & \text{if there is an edge from node \(i\)
    to node \(j\)} \\ 0, & \text{otherwise} \end{cases} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Unweighted graphs: entries are 0 or 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weighted graphs: entries are edge weights (distances, costs, capacities).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Undirected graphs: \(A\) is symmetric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Directed graphs: \(A\) may be asymmetric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The adjacency matrix is the algebraic fingerprint of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: Powers of the Adjacency Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The entry \((A^k)_{ij}\) counts the number of walks of length \(k\) from node
    \(i\) to node \(j\).
  prefs: []
  type: TYPE_NORMAL
- en: \(A^2\) tells how many two-step connections exist.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This property is used in algorithms for detecting paths, clustering, and network
    flow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Degree Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The degree of a vertex is the number of edges connected to it (or the sum of
    weights in weighted graphs).
  prefs: []
  type: TYPE_NORMAL
- en: 'The degree matrix \(D\) is diagonal:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ D_{ii} = \sum_j A_{ij}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This matrix measures how “connected” each node is.
  prefs: []
  type: TYPE_NORMAL
- en: The Graph Laplacian
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The combinatorial Laplacian is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L = D - A. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Key properties:'
  prefs: []
  type: TYPE_NORMAL
- en: \(L\) is symmetric (for undirected graphs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row sums to zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The smallest eigenvalue is always 0, with eigenvector \([1, 1, \dots, 1]^T\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Laplacian encodes connectivity: if the graph splits into \(k\) connected
    components, then \(L\) has exactly \(k\) zero eigenvalues.'
  prefs: []
  type: TYPE_NORMAL
- en: Normalized Laplacians
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two common normalized versions are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{sym} = D^{-1/2} L D^{-1/2}, \quad L_{rw} = D^{-1} L. \]
  prefs: []
  type: TYPE_NORMAL
- en: These rescale the Laplacian for applications like spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Spectral Graph Theory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Eigenvalues and eigenvectors of \(A\) or \(L\) reveal structure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Algebraic connectivity: The second-smallest eigenvalue of \(L\) measures how
    well connected the graph is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spectral clustering: Eigenvectors of \(L\) partition graphs into communities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Random walks: Transition probabilities relate to \(D^{-1}A\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: A Simple Graph'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take a triangle graph with 3 nodes, each connected to the other two.
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0 \end{bmatrix}, \quad
    D = \begin{bmatrix} 2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2 \end{bmatrix}, \quad L
    = \begin{bmatrix} 2 & -1 & -1 \\ -1 & 2 & -1 \\ -1 & -1 & 2 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigenvalues of \(L\): \(0, 3, 3\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The single zero eigenvalue confirms the graph is connected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Community Detection: Spectral clustering finds natural divisions in social
    or biological networks.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Graph Drawing: Eigenvectors of \(L\) provide coordinates for visually embedding
    graphs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Random Walks & PageRank: Transition matrices from adjacency define importance
    scores.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Physics: Laplacians appear in discrete versions of diffusion and vibration
    problems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Machine Learning: Graph neural networks (GNNs) use Laplacians to propagate
    signals across graph structure.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Graphs and matrices are two sides of the same coin: one combinatorial, one
    algebraic. By turning a network into a matrix, linear algebra gives us access
    to the full toolbox of eigenvalues, norms, and factorizations, enabling deep insights
    into connectivity, flow, and structure.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute adjacency, degree, and Laplacian matrices for a square graph (4 nodes
    in a cycle). Find eigenvalues of \(L\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prove that the Laplacian always has at least one zero eigenvalue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that if a graph has \(k\) components, then the multiplicity of zero as
    an eigenvalue is exactly \(k\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a random walk on a graph, derive the transition matrix \(P = D^{-1}A\).
    Interpret its eigenvectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Graphs demonstrate how linear algebra stretches beyond geometry and data tables-it
    becomes a universal language for networks, from molecules to megacities.
  prefs: []
  type: TYPE_NORMAL
- en: 94\. Data Preprocessing as Linear Operations (Centering, Whitening, Scaling)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before any sophisticated model can be trained, raw data must be preprocessed.
    Surprisingly, many of the most common preprocessing steps-centering, scaling,
    whitening-are nothing more than linear algebra operations in disguise. Understanding
    them this way not only clarifies why they work, but also shows how they connect
    to broader concepts like covariance, eigenvalues, and singular value decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: The Nature of Preprocessing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Most datasets are stored as a matrix: rows correspond to samples (observations)
    and columns correspond to features (variables). For instance, in a dataset of
    1,000 people with height, weight, and age recorded, we’d have a \(1000 \times
    3\) matrix. Linear algebra allows us to systematically reshape, scale, and rotate
    this matrix to prepare it for downstream analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Centering: Shifting the Origin'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Centering means subtracting the mean of each column (feature) from all entries
    in that column.
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_{centered} = X - \mathbf{1}\mu^T \]
  prefs: []
  type: TYPE_NORMAL
- en: Here \(X\) is the data matrix, \(\mu\) is the vector of column means, and \(\mathbf{1}\)
    is a column of ones.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Effect: moves the dataset so that each feature has mean zero.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geometric view: translates the cloud of points so its “center of mass” sits
    at the origin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Why important: covariance and correlation formulas assume data are mean-centered;
    otherwise, cross-terms are skewed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: If people’s heights average 170 cm, subtract 170 from every height.
    After centering, “height = 0” corresponds to the average person.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scaling: Normalizing Variability'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Raw features can have different units or magnitudes (e.g., weight in kg, income
    in thousands of dollars). To compare them fairly, we scale:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_{scaled} = X D^{-1} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(D\) is a diagonal matrix of feature standard deviations.
  prefs: []
  type: TYPE_NORMAL
- en: Each feature now has variance 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geometric view: rescales axes so all dimensions have equal “spread.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Common in machine learning: ensures gradient descent does not disproportionately
    focus on features with large raw values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: If weight varies around 60 kg ± 15, dividing by 15 makes its spread
    comparable to that of height (±10 cm).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whitening: Removing Correlations'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Even after centering and scaling, features can remain correlated (e.g., height
    and weight). Whitening transforms the data so features become uncorrelated with
    unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\Sigma = \frac{1}{n} X^T X\) be the covariance matrix of centered data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform eigendecomposition: \(\Sigma = Q \Lambda Q^T\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whitening transform:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ X_{white} = X Q \Lambda^{-1/2} Q^T \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Result:'
  prefs: []
  type: TYPE_NORMAL
- en: The covariance matrix of \(X_{white}\) is the identity matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each new feature is a rotated combination of old features, with no redundancy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Geometric view: whitening “spheres” the data cloud, turning an ellipse into
    a perfect circle.'
  prefs: []
  type: TYPE_NORMAL
- en: Covariance Matrix as the Key Player
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The covariance matrix itself arises naturally from preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Sigma = \frac{1}{n} X^T X \quad \text{(if \(X\) is centered).} \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Diagonal entries: variances of features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Off-diagonal entries: covariances, measuring linear relationships.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing operations (centering, scaling, whitening) are designed to reshape
    data so \(\Sigma\) becomes easier to interpret and more stable for learning algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connections to PCA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Centering is required before PCA, otherwise the first component just points
    to the mean.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling ensures PCA does not overweight large-variance features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whitening is closely related to PCA itself: PCA diagonalizes the covariance,
    and whitening goes one step further by rescaling eigenvalues to unity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus, PCA can be seen as a preprocessing pipeline plus an analysis step.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Workflows
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Centering and Scaling (Standardization): The default for many algorithms like
    logistic regression or SVM.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Whitening: Often used in signal processing (e.g., removing correlations in
    audio or images).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Batch Normalization in Deep Learning: A variant of centering + scaling applied
    layer by layer during training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Whitening in Image Processing: Ensures features like pixel intensities are
    decorrelated, improving compression and recognition.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Worked Example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we have three features: height, weight, and age.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Raw data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mean height = 170 cm, mean weight = 65 kg, mean age = 35 years.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Variance differs widely: age varies less, weight more.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After centering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mean of each feature is zero.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A person of average height now has value 0 in that feature.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After scaling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All features have unit variance.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms can treat age and weight equally.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After whitening:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Correlation between height and weight disappears.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Features become orthogonal directions in feature space.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Without preprocessing, models may be misled by scale, units, or correlations.
    Preprocessing makes features comparable, balanced, and independent-a crucial condition
    for algorithms that rely on geometry (distances, angles, inner products).
  prefs: []
  type: TYPE_NORMAL
- en: In essence, preprocessing is the bridge from messy, real-world data to the clean
    structures linear algebra expects.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For a small dataset, compute the covariance matrix before and after centering.
    What changes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scale the dataset so each feature has unit variance. Check the new covariance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform whitening via eigendecomposition and verify the covariance matrix becomes
    the identity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the data points in 2D before and after whitening. Notice how an ellipse
    becomes a circle.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocessing through linear algebra shows that preparing data is not just housekeeping-it’s
    a fundamental reshaping of the problem’s geometry.
  prefs: []
  type: TYPE_NORMAL
- en: 95\. Linear Regression and Classification (From Model to Matrix)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Linear algebra provides the foundation for two of the most widely used tools
    in data science and applied statistics: linear regression (predicting continuous
    outcomes) and linear classification (separating categories). Both problems reduce
    to expressing data in matrix form and then applying linear operations to estimate
    parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: The Regression Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose we want to predict an output \(y \in \mathbb{R}^n\) from features collected
    in a data matrix \(X \in \mathbb{R}^{n \times p}\), where:'
  prefs: []
  type: TYPE_NORMAL
- en: \(n\) = number of observations (samples).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(p\) = number of features (variables).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We assume a linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ y \approx X\beta, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\beta \in \mathbb{R}^p\) is the vector of coefficients (weights). Each
    entry of \(\beta\) tells us how much its feature contributes to the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: The Normal Equations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We want to minimize the squared error:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_\beta \|y - X\beta\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Differentiating leads to the normal equations:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ X^T X \beta = X^T y. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'If \(X^T X\) is invertible:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \hat{\beta} = (X^T X)^{-1} X^T y. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'If not invertible (multicollinearity, too many features), we use the pseudoinverse
    via SVD:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \hat{\beta} = X^+ y. \]
  prefs: []
  type: TYPE_NORMAL
- en: Geometric Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: \(X\beta\) is the projection of \(y\) onto the column space of \(X\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The residual \(r = y - X\hat{\beta}\) is orthogonal to all columns of \(X\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This “closest fit” property is why regression is a projection problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification with Linear Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of predicting continuous outputs, sometimes we want to separate categories
    (e.g., spam vs. not spam).
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear classifier: decides based on the sign of a linear function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \hat{y} = \text{sign}(w^T x + b). \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Geometric view: \(w\) defines a hyperplane in feature space. Points on one
    side are labeled positive, on the other side negative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Relation to regression: logistic regression replaces squared error with a log-likelihood
    loss, but still solves for weights via iterative linear-algebraic methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiclass Extension
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For \(k\) classes, we use a weight matrix \(W \in \mathbb{R}^{p \times k}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prediction:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \hat{y} = \arg \max_j (XW)_{ij}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Each class has a column of \(W\), and the classifier picks the column with the
    largest score.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: Predicting House Prices'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Features: size, number of rooms, distance to city center.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Target: price.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(X\) = matrix of features, \(y\) = price vector.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression solves for coefficients showing how strongly each factor influences
    price.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we switch to classification (predicting “expensive” vs. “cheap”), we treat
    price as a label and solve for a hyperplane separating the two categories.
  prefs: []
  type: TYPE_NORMAL
- en: Computational Aspects
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Directly solving normal equations: \(O(p^3)\) (matrix inversion).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'QR factorization: numerically more stable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SVD: best when \(X\) is ill-conditioned or rank-deficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Modern libraries: exploit sparsity or use gradient-based methods for large
    datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connections to Other Topics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Least Squares (Chapter 8): Regression is the canonical least-squares problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SVD (Chapter 9): Pseudoinverse gives regression when columns are dependent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regularization (Chapter 9): Ridge regression adds a penalty \(\lambda \|\beta\|^2\)
    to improve stability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Classification (Chapter 10): Forms the foundation of more complex models like
    support vector machines and neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linear regression and classification show the direct link between linear algebra
    and real-world decisions. They combine geometry (projection, hyperplanes), algebra
    (solving systems), and computation (factorizations). Despite their simplicity,
    they remain indispensable: they are interpretable, fast, and often competitive
    with more complex models.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given three features and five samples, construct \(X\) and \(y\). Solve for
    \(\beta\) using the normal equations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Show that residuals are orthogonal to all columns of \(X\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write down a linear classifier separating two clusters of points in 2D. Sketch
    the separating hyperplane.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore what happens when two features are highly correlated (collinear). Use
    pseudoinverse to recover a stable solution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear regression and classification are proof that linear algebra is not just
    abstract-it is the engine of practical prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 96\. PCA in Practice (Dimensionality Reduction Workflow)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Principal Component Analysis (PCA) is one of the most widely used tools in applied
    linear algebra. At its heart, PCA identifies the directions (principal components)
    along which data varies the most, and then re-expresses the data in terms of those
    directions. In practice, PCA is not just a mathematical curiosity-it is a complete
    workflow for reducing dimensionality, denoising data, and extracting patterns
    from high-dimensional datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The Motivation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Modern datasets often have thousands or even millions of features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Images: each pixel is a feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Genomics: each gene expression level is a feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Text: each word in a vocabulary becomes a dimension.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working in such high dimensions is expensive (computationally) and fragile (noise
    accumulates). PCA provides a systematic way to reduce the feature space to a smaller
    set of dimensions that still captures most of the variability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Organizing the Data'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We start with a data matrix \(X \in \mathbb{R}^{n \times p}\):'
  prefs: []
  type: TYPE_NORMAL
- en: '\(n\): number of samples (observations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\(p\): number of features (variables).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row is a sample; each column is a feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Centering is the first preprocessing step: subtract the mean of each column
    so the dataset has mean zero. This ensures that PCA describes variance rather
    than being biased by offsets.'
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_{centered} = X - \mathbf{1}\mu^T \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Covariance Matrix'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, compute the covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \Sigma = \frac{1}{n} X_{centered}^T X_{centered}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Diagonal entries: variance of each feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Off-diagonal entries: how features co-vary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The structure of \(\Sigma\) determines the directions of maximal variation in
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Eigen-Decomposition or SVD'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Two equivalent approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Eigen-decomposition: Solve \(\Sigma v = \lambda v\).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eigenvectors \(v\) are the principal components.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Eigenvalues \(\lambda\) measure variance along those directions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singular Value Decomposition (SVD): Directly decompose the centered data matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ X_{centered} = U \Sigma V^T. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Columns of \(V\) = principal directions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Squared singular values correspond to variances.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD is preferred in practice for numerical stability and efficiency, especially
    when \(p\) is very large.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Choosing the Number of Components'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We order eigenvalues \(\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_p\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Explained variance ratio:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \text{EVR}(k) = \frac{\sum_{i=1}^k \lambda_i}{\sum_{i=1}^p \lambda_i}. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We choose \(k\) such that EVR exceeds some threshold (e.g., 90–95%).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This balances dimensionality reduction with information preservation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphically, a scree plot shows eigenvalues, and we look for the “elbow” point
    where additional components add little variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Projecting Data'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once we select \(k\) components, we project onto them:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_{PCA} = X_{centered} V_k, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(V_k\) contains the top \(k\) eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Result:'
  prefs: []
  type: TYPE_NORMAL
- en: \(X_{PCA} \in \mathbb{R}^{n \times k}\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row is now a \(k\)-dimensional representation of the original sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Worked Example: Face Images'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Suppose we have a dataset of grayscale images, each \(100 \times 100\) pixels
    (\(p = 10,000\)).
  prefs: []
  type: TYPE_NORMAL
- en: Center each pixel value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute covariance across all images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find eigenvectors = eigenfaces. These are characteristic patterns like “glasses,”
    “mouth shape,” or “lighting direction.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep top 50 components. Each face is now represented as a 50-dimensional vector
    instead of 10,000.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This drastically reduces storage and speeds up recognition while keeping key
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Practical Considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Standardization: If features have different scales (e.g., age in years vs. income
    in thousands), we must scale them before PCA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computational shortcuts: For very large \(p\), it’s often faster to compute
    PCA via truncated SVD on \(X\) directly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Noise filtering: Small eigenvalues often correspond to noise; truncating them
    denoises the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interpretability: Principal components are linear combinations of features.
    Sometimes these combinations are interpretable, sometimes not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connections to Other Concepts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Whitening (Chapter 94): PCA followed by scaling eigenvalues to 1 is whitening.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SVD (Chapter 9): PCA is essentially an application of SVD.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Regression (Chapter 95): PCA can be used before regression to reduce collinearity
    among predictors (PCA regression).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine learning pipelines: PCA is often used before clustering, classification,
    or neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PCA turns raw, unwieldy data into a compact form without losing essential structure.
    It enables visualization (2D/3D plots of high-dimensional data), faster learning,
    and noise reduction. Many breakthroughs-from face recognition to gene expression
    analysis-rely on PCA as the first preprocessing step.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take a dataset with 3 features. Manually compute covariance, eigenvalues, and
    eigenvectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project the data onto the first two principal components and plot. Compare to
    the original 3D scatter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download an image dataset and apply PCA to compress it. Reconstruct the images
    with 10, 50, 100 components. Observe the trade-off between compression and fidelity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute explained variance ratios and decide how many components to keep.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'PCA is the bridge between raw data and meaningful representation: it reduces
    complexity while sharpening patterns. It shows how linear algebra can reveal hidden
    order in high-dimensional chaos.'
  prefs: []
  type: TYPE_NORMAL
- en: 97\. Recommender Systems and Low-Rank Models (Fill the Missing Entries)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recommender systems-such as those used by Netflix, Amazon, or Spotify-are built
    on the principle that preferences can be captured by low-dimensional structures
    hidden inside large, sparse data. Linear algebra gives us the machinery to expose
    and exploit these structures, especially through low-rank models.
  prefs: []
  type: TYPE_NORMAL
- en: The Matrix of Preferences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We begin with a user–item matrix \(R \in \mathbb{R}^{m \times n}\):'
  prefs: []
  type: TYPE_NORMAL
- en: Rows represent users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columns represent items (movies, books, songs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entries \(R_{ij}\) store the rating (say 1–5 stars) or interaction (clicks,
    purchases).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, most entries are missing-users rate only a small subset of items.
    The central challenge: predict the missing entries.'
  prefs: []
  type: TYPE_NORMAL
- en: Why Low-Rank Structure?
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Despite its size, \(R\) often lies close to a low-rank approximation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R \approx U V^T \]
  prefs: []
  type: TYPE_NORMAL
- en: '\(U \in \mathbb{R}^{m \times k}\): user factors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '\(V \in \mathbb{R}^{n \times k}\): item factors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(k \ll \min(m, n)\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, each user and each item is represented in a shared latent feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: For movies, latent dimensions might capture “action vs. romance,”
    “old vs. new,” or “mainstream vs. indie.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A user’s preference vector in this space interacts with an item’s feature vector
    to generate a predicted rating.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This factorization explains correlations: if you liked Movie A and B, and Movie
    C shares similar latent features, the system predicts you’ll like C too.'
  prefs: []
  type: TYPE_NORMAL
- en: Singular Value Decomposition (SVD) Approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If \(R\) were complete (no missing entries), we could compute the SVD:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R = U \Sigma V^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Keep the top \(k\) singular values to form a rank-\(k\) approximation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This captures the dominant patterns in user preferences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geometric view: project the massive data cloud onto a smaller \(k\)-dimensional
    subspace where structure is clearer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But real data is incomplete. That leads to matrix completion problems.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix Completion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Matrix completion tries to infer missing entries of \(R\) by assuming low rank.
    The optimization problem is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{X} \ \text{rank}(X) \quad \text{s.t. } X_{ij} = R_{ij} \text{ for observed
    entries}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Since minimizing rank is NP-hard, practical algorithms instead minimize the
    nuclear norm (sum of singular values) or use alternating minimization:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize \(U, V\) randomly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iteratively solve for one while fixing the other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converge to a low-rank factorization that fits the observed ratings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternating Least Squares (ALS)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'ALS is a standard approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Fix \(V\), solve least squares for \(U\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fix \(U\), solve least squares for \(V\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat until convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each subproblem is straightforward linear regression, solvable with normal equations
    or QR decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent (SGD)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another approach: treat each observed rating as a training sample. Update latent
    vectors by minimizing squared error:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell = (R_{ij} - u_i^T v_j)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Iteratively adjust user vector \(u_i\) and item vector \(v_j\) along gradients.
    This scales well to huge datasets, making it common in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To prevent overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell = (R_{ij} - u_i^T v_j)^2 + \lambda (\|u_i\|^2 + \|v_j\|^2). \]
  prefs: []
  type: TYPE_NORMAL
- en: Regularization shrinks factors, discouraging extreme values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geometrically, it keeps latent vectors within a reasonable ball in feature space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cold Start Problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'New users: Without ratings, \(u_i\) is unknown. Solutions: use demographic
    features or ask for a few initial ratings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'New items: Similarly, items need side information (metadata, tags) to generate
    initial latent vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is where hybrid models combine matrix factorization with content-based
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Movie Ratings'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Imagine 1,000 users and 5,000 movies.
  prefs: []
  type: TYPE_NORMAL
- en: The raw \(R\) matrix has 5 million entries, but each user has rated only ~50
    movies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix completion with rank \(k = 20\) recovers a dense approximation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each user is represented by 20 latent “taste” factors; each movie by 20 latent
    “theme” factors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prediction: the dot product of user and movie vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beyond Ratings: Implicit Feedback'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In practice, systems often lack explicit ratings. Instead, they use:'
  prefs: []
  type: TYPE_NORMAL
- en: Views, clicks, purchases, skips.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These signals are indirect but abundant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Factorization can handle them by treating interactions as weighted observations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connections to Other Linear Algebra Tools
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SVD (Chapter 9): The backbone of factorization methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pseudoinverse (Chapter 9): Useful when solving small regression subproblems
    in ALS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Conditioning (Chapter 9): Factorization stability depends on well-scaled latent
    factors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PCA (Chapter 96): PCA is essentially a low-rank approximation, so PCA and recommenders
    share the same mathematics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recommender systems personalize the modern internet. Every playlist suggestion,
    book recommendation, or ad placement is powered by linear algebra hidden in a
    massive sparse matrix. Low-rank modeling shows how even incomplete, noisy data
    can be harnessed to reveal patterns of preference and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Take a small user–item matrix with missing entries. Apply rank-2 approximation
    via SVD to fill in gaps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Implement one step of ALS: fix movie factors and update user factors with least
    squares.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare predictions with and without regularization. Notice how regularization
    stabilizes results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Explore the cold-start problem: simulate a new user and try predicting preferences
    from minimal data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Low-rank models reveal a powerful truth: behind the enormous variety of human
    choices lies a surprisingly small set of underlying patterns-and linear algebra
    is the key to uncovering them.'
  prefs: []
  type: TYPE_NORMAL
- en: 98\. PageRank and Random Walks (Ranking with Eigenvectors)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PageRank, the algorithm that once powered Google’s search engine dominance,
    is a striking example of how linear algebra and eigenvectors can measure importance
    in a network. At its core, it models the web as a graph and asks a simple question:
    if you randomly surf the web forever, which pages will you visit most often?'
  prefs: []
  type: TYPE_NORMAL
- en: The Web as a Graph
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each web page is a node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each hyperlink is a directed edge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The adjacency matrix \(A\) encodes which pages link to which:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ A_{ij} = 1 \quad \text{if page \(j\) links to page \(i\)}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Why columns instead of rows? Because links flow from source to destination,
    and PageRank naturally arises when analyzing column-stochastic transition matrices.
  prefs: []
  type: TYPE_NORMAL
- en: Transition Matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To model random surfing, we define a column-stochastic matrix \(P\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P_{ij} = \frac{1}{\text{outdeg}(j)} \quad \text{if \(j \to i\)}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Each column sums to 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(P_{ij}\) is the probability of moving from page \(j\) to page \(i\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This defines a Markov chain: a random process where the next state depends
    only on the current one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a user is on page \(j\), they pick one outgoing link uniformly at random.
  prefs: []
  type: TYPE_NORMAL
- en: Random Walk Interpretation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Imagine a web surfer moving page by page according to \(P\). After many steps,
    the fraction of time spent on each page converges to a steady-state distribution
    vector \(\pi\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi = P \pi. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an eigenvector equation: \(\pi\) is the stationary eigenvector of \(P\)
    with eigenvalue 1.'
  prefs: []
  type: TYPE_NORMAL
- en: \(\pi_i\) is the long-run probability of being on page \(i\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A higher \(\pi_i\) means greater importance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The PageRank Adjustment: Teleportation'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The pure random walk has problems:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dead ends: Pages with no outgoing links trap the surfer.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Spider traps: Groups of pages linking only to each other hoard probability
    mass.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Solution: add a teleportation mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: With probability \(\alpha\) (say 0.85), follow a link.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With probability \(1-\alpha\), jump to a random page.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This defines the PageRank matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ M = \alpha P + (1-\alpha)\frac{1}{n} ee^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(e\) is the all-ones vector.
  prefs: []
  type: TYPE_NORMAL
- en: \(M\) is stochastic, irreducible, and aperiodic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the Perron–Frobenius theorem, it has a unique stationary distribution \(\pi\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving the Eigenproblem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The PageRank vector \(\pi\) satisfies:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ M \pi = \pi. \]
  prefs: []
  type: TYPE_NORMAL
- en: Computing \(\pi\) directly via eigen-decomposition is infeasible for billions
    of pages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Instead, use power iteration: repeatedly multiply a vector by \(M\) until convergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This works because the largest eigenvalue is 1, and the method converges to
    its eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: 'Worked Example: A Tiny Web'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose 3 pages with links:'
  prefs: []
  type: TYPE_NORMAL
- en: Page 1 → Page 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Page 2 → Page 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Page 3 → Page 1 and Page 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adjacency matrix (columns = source):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = \begin{bmatrix} 0 & 0 & 1 \\ 1 & 0 & 1 \\ 0 & 1 & 0 \end{bmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = \begin{bmatrix} 0 & 0 & 1/2 \\ 1 & 0 & 1/2 \\ 0 & 1 & 0 \end{bmatrix}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: With teleportation (\(\alpha=0.85\)), we form \(M\). Power iteration quickly
    converges to \(\pi = [0.37, 0.34, 0.29]^T\). Page 1 is ranked highest.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond the Web
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Although born in search engines, PageRank’s mathematics applies broadly:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Social networks: Rank influential users by their connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Citation networks: Rank scientific papers by how they are referenced.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Biology: Identify key proteins in protein–protein interaction networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recommendation systems: Rank products or movies via link structures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each case, importance is defined not by how many connections a node has,
    but by the importance of the nodes that point to it.
  prefs: []
  type: TYPE_NORMAL
- en: Computational Challenges
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Scale: Billions of pages mean \(M\) cannot be stored fully; sparse matrix techniques
    are essential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Convergence: Power iteration may take hundreds of steps; preconditioning and
    parallelization speed it up.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Personalization: Instead of uniform teleportation, adjust probabilities to
    bias toward user interests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'PageRank illustrates a deep principle: importance emerges from connectivity.
    Linear algebra captures this by identifying the dominant eigenvector of a transition
    matrix. This idea-ranking nodes in a network by stationary distributions-has transformed
    search engines, social media, and science itself.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Construct a 4-page web graph and compute its PageRank manually with \(\alpha
    = 0.85\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement power iteration in Python or MATLAB for a small adjacency matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare PageRank to simple degree counts. Notice how PageRank rewards links
    from important nodes more heavily.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modify teleportation to bias toward a subset of pages (personalized PageRank).
    Observe how rankings change.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PageRank is not only a milestone in computer science history-it is a living
    example of how eigenvectors can capture global importance from local structure.
  prefs: []
  type: TYPE_NORMAL
- en: 99\. Numerical Linear Algebra Essentials (Floating Point, BLAS/LAPACK)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Linear algebra in theory is exact: numbers behave like real numbers, operations
    are deterministic, and results are precise. In practice, computations are carried
    out on computers, where numbers are represented in finite precision and algorithms
    must balance speed, accuracy, and stability. This intersection-numerical linear
    algebra-is what makes linear algebra usable at modern scales.'
  prefs: []
  type: TYPE_NORMAL
- en: Floating-Point Representation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Real numbers cannot be stored exactly on a digital machine. Instead, they are
    approximated using the IEEE 754 floating-point standard.
  prefs: []
  type: TYPE_NORMAL
- en: 'A floating-point number is stored as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ x = \pm (1.m_1 m_2 m_3 \dots) \times 2^e \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: where \(m\) is the mantissa and \(e\) is the exponent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Single precision (float32): 32 bits → ~7 decimal digits of precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Double precision (float64): 64 bits → ~16 decimal digits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine epsilon (\(\epsilon\)): The smallest gap between 1 and the next representable
    number. For double precision, \(\epsilon \approx 2.22 \times 10^{-16}\).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Implication: operations like subtraction of nearly equal numbers cause catastrophic
    cancellation, where significant digits vanish.'
  prefs: []
  type: TYPE_NORMAL
- en: Conditioning of Problems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A linear algebra problem may be well-posed mathematically but still numerically
    difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'The condition number of a matrix \(A\):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \[ \kappa(A) = \|A\| \cdot \|A^{-1}\|. \]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If \(\kappa(A)\) is large, small input errors cause large output errors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: solving \(Ax = b\). With ill-conditioned \(A\), the computed solution
    may be unstable even with perfect algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geometric intuition: ill-conditioned matrices stretch vectors unevenly, so
    small perturbations in direction blow up under inversion.'
  prefs: []
  type: TYPE_NORMAL
- en: Stability of Algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An algorithm is numerically stable if it controls the growth of errors from
    finite precision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian elimination with partial pivoting is stable; without pivoting, it may
    fail catastrophically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orthogonal factorizations (QR, SVD) are usually more stable than elimination
    methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Numerical analysis focuses on designing algorithms that guarantee accuracy within
    a few multiples of machine epsilon.
  prefs: []
  type: TYPE_NORMAL
- en: Direct vs. Iterative Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Direct methods: Solve in a finite number of steps (e.g., Gaussian elimination,
    LU decomposition, Cholesky for positive definite systems).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reliable for small/medium problems.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Complexity ~ \(O(n^3)\).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iterative methods: Generate successive approximations (e.g., Jacobi, Gauss–Seidel,
    Conjugate Gradient).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Useful for very large, sparse systems.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Complexity per iteration ~ \(O(n^2)\) or less, often leveraging sparsity.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix Factorizations in Computation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many algorithms rely on factorizing a matrix once, then reusing it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'LU decomposition: Efficient for solving multiple right-hand sides.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'QR factorization: Stable approach for least squares.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SVD: Gold standard for ill-conditioned problems, though expensive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These factorizations reduce repeated operations into structured, cache-friendly
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse vs. Dense Computations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Dense matrices: Most entries are nonzero. Use dense linear algebra packages
    like BLAS and LAPACK.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sparse matrices: Most entries are zero. Store only nonzeros, use specialized
    algorithms to avoid wasted computation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-scale problems (e.g., finite element simulations, web graphs) are feasible
    only because of sparse methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'BLAS and LAPACK: Standard Libraries'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'BLAS (Basic Linear Algebra Subprograms): Defines kernels for vector and matrix
    operations (dot products, matrix–vector, matrix–matrix multiplication). Optimized
    BLAS implementations exploit cache, SIMD, and multi-core parallelism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LAPACK (Linear Algebra PACKage): Builds on BLAS to provide algorithms for solving
    systems, eigenvalue problems, SVD, etc. LAPACK is the backbone of many scientific
    computing environments (MATLAB, NumPy, Julia).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MKL, OpenBLAS, cuBLAS: Vendor-specific implementations optimized for Intel
    CPUs, open-source systems, or NVIDIA GPUs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These libraries make the difference between code that runs in minutes and code
    that runs in milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: Floating-Point Pitfalls
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Accumulated round-off: Summing numbers of vastly different magnitudes may discard
    small contributions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Loss of orthogonality: Repeated Gram–Schmidt orthogonalization without reorthogonalization
    may drift numerically.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Overflow/underflow: Extremely large/small numbers exceed representable range.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'NaNs and Infs: Divide-by-zero or invalid operations propagate errors.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mitigation: use numerically stable algorithms, scale inputs, and check condition
    numbers.'
  prefs: []
  type: TYPE_NORMAL
- en: Parallel and GPU Computing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Modern numerical linear algebra thrives on parallelism:'
  prefs: []
  type: TYPE_NORMAL
- en: GPUs accelerate dense linear algebra with thousands of cores (cuBLAS, cuSOLVER).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed libraries (ScaLAPACK, PETSc, Trilinos) allow solving problems with
    billions of unknowns across clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mixed precision methods: compute in float32 or even float16, then refine in
    float64, balancing speed and accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications in the Real World
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Engineering simulations: Structural mechanics, fluid dynamics rely on sparse
    solvers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine learning: Training deep networks depends on optimized BLAS for matrix
    multiplications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finance: Risk models solve huge regression problems with factorized covariance
    matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Big data: Dimensionality reduction (PCA, SVD) requires large-scale, stable
    algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why It Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linear algebra in practice is about more than theorems: it’s about turning
    abstract models into computations that run reliably on imperfect hardware. Numerical
    linear algebra provides the essential toolkit-floating-point understanding, conditioning
    analysis, stable algorithms, and optimized libraries-that ensures results are
    both fast and trustworthy.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compute the condition number of a nearly singular matrix (e.g., \(\begin{bmatrix}
    1 & 1 \\ 1 & 1.0001 \end{bmatrix}\)) and solve \(Ax=b\). Compare results in single
    vs. double precision.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement Gaussian elimination with and without pivoting. Compare errors for
    ill-conditioned matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use NumPy with OpenBLAS to time large matrix multiplications; compare against
    a naive Python implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Explore iterative solvers: implement Conjugate Gradient for a sparse symmetric
    positive definite system.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Numerical linear algebra is the bridge between mathematical elegance and computational
    reality. It teaches us that solving equations on a computer is not just about
    the equations-it’s about the algorithms, representations, and hardware that bring
    them to life.
  prefs: []
  type: TYPE_NORMAL
- en: 100\. Capstone Problem Sets and Next Steps (A Roadmap to Mastery)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You’ve now walked through the major landmarks of linear algebra: vectors, matrices,
    systems, transformations, determinants, eigenvalues, orthogonality, SVD, and applications
    to data and networks. The journey doesn’t end here. This last section is designed
    as a capstone, a way to tie things together and show you how to keep practicing,
    exploring, and deepening your understanding. Think of it as your “next steps”
    map.'
  prefs: []
  type: TYPE_NORMAL
- en: Practicing the Basics Until They Feel Natural
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linear algebra may seem heavy at first, but the simplest drills build lasting
    confidence. Try solving a few systems of equations by hand using elimination,
    and notice how pivoting reveals where solutions exist-or don’t. Write down a small
    matrix and practice multiplying it by a vector. This might feel mechanical, but
    it’s how your intuition sharpens: every time you push numbers through the rules,
    you’re learning how the algebra reshapes space.'
  prefs: []
  type: TYPE_NORMAL
- en: Even a single concept, like the dot product, can teach a lot. Take two short
    vectors in the plane, compute their dot product, and then compare it to the cosine
    of the angle between them. Seeing algebra match geometry is what makes linear
    algebra come alive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving Beyond Computation: Understanding Structures'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Once you’re comfortable with the mechanics, try reflecting on the bigger structures.
    What does it mean for a set of vectors to be a subspace? Can you tell whether
    a line through the origin is one? What about a line shifted off the origin? This
    is where the rules and axioms you’ve seen start to guide your reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiment with bases and coordinates: pick two different bases for the plane
    and see how a single point looks different depending on the “ruler” you’re using.
    Write out the change-of-basis matrix and check that it transforms coordinates
    the way you expect. These exercises show that linear algebra isn’t just about
    numbers-it’s about perspective.'
  prefs: []
  type: TYPE_NORMAL
- en: Bringing Ideas Together in Larger Problems
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The real joy comes when different ideas collide. Suppose you have noisy data,
    like a scatter of points that should lie along a line. Try fitting a line using
    least squares. What you’re really doing is projecting the data onto a subspace.
    Or take a small Markov chain, like a random walk around three or four nodes, and
    compute its long-term distribution. That steady state is an eigenvector in disguise.
    These integrative problems demonstrate how the topics you’ve studied connect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Projects make this even more vivid. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: In computer graphics, write simple code that rotates or reflects a shape using
    a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In networks, use the Laplacian to identify clusters in a social graph of friends.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In recommendation systems, factorize a small user–item table to predict missing
    ratings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These aren’t abstract puzzles-they show how linear algebra works in the real
    world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking Ahead: Where Linear Algebra Leads You'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By now you know that linear algebra is not an isolated subject; it’s a foundation.
    The next steps depend on your interests.
  prefs: []
  type: TYPE_NORMAL
- en: If you enjoy computation, numerical linear algebra is the natural extension.
    It digs into how floating-point numbers behave on real machines, how to control
    round-off errors, and why some algorithms are more stable than others. You’ll
    learn why Gaussian elimination with pivoting is safe while without pivoting it
    can fail, and why QR and SVD are trusted in sensitive applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'If abstraction intrigues you, then abstract linear algebra opens the door.
    Here you’ll move beyond \(\mathbb{R}^n\) into general vector spaces: polynomials
    as vectors, functions as vectors, dual spaces, and eventually tensor products.
    These ideas power much of modern mathematics and physics.'
  prefs: []
  type: TYPE_NORMAL
- en: If data excites you, statistics and machine learning are a natural path. Covariance
    matrices, principal component analysis, regression, and neural networks all rest
    on linear algebra. Understanding them deeply requires both the computation you’ve
    practiced and the geometric insights you’ve built.
  prefs: []
  type: TYPE_NORMAL
- en: 'And if your curiosity points toward the sciences, linear algebra is everywhere:
    in quantum mechanics, where states are vectors and operators are matrices; in
    engineering, where vibrations and control systems rely on eigenvalues; in computer
    graphics, where every rotation and projection is a linear transformation.'
  prefs: []
  type: TYPE_NORMAL
- en: Why This Capstone Matters
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This final step is less about new theorems and more about perspective. The
    problems you solve now-whether small drills or large projects-train you to see
    structure, not just numbers. The roadmap is open-ended, because linear algebra
    itself is open-ended: once you learn to see the world through its lens, you notice
    it everywhere, from the patterns in networks to the behavior of algorithms to
    the geometry of space.'
  prefs: []
  type: TYPE_NORMAL
- en: Try It Yourself
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Take a dataset you care about-maybe sports scores, songs you listen to, or
    spending records. Organize it as a matrix. Compute simple things: averages (centering),
    a regression line, maybe even principal components. See what structure you uncover.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a short program that solves systems of equations using elimination. Test
    it on well-behaved and nearly singular matrices. Notice how stability changes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a 2D scatterplot and fit a line with least squares. Plot the residuals.
    What does it mean geometrically that the residuals are orthogonal to the line?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try explaining eigenvalues to a friend without formulas-just pictures and stories.
    Teaching it will make it real.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Linear algebra is both a tool and a way of thinking. You now have enough to
    stand on your own, but the road continues forward-into deeper math, into practical
    computation, and into the sciences that rely on these ideas every day. This capstone
    is an invitation: keep practicing, keep exploring, and let the structures of linear
    algebra sharpen the way you see the world.'
  prefs: []
  type: TYPE_NORMAL
- en: Closing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Finale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*A quiet closing, where lessons settle and the music of algebra carries on
    beyond the final page.*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1\. Quiet Reflection**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**2\. Infinite Journey**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**3\. Structure and Growth**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**4\. Light After Study**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**5\. Eternal Motion**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '**6\. Gratitude and Closure**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '**7\. Future Echo**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '**8\. Horizon Beyond**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
