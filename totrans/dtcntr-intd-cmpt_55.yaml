- en: 17.5¬†Moravian Spanning Treesüîó
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ÂéüÊñáÔºö[https://dcic-world.org/2025-08-27/mst.html](https://dcic-world.org/2025-08-27/mst.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| ¬†¬†¬†¬†[17.5.1¬†The Problem](#%28part._.The_.Problem%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[17.5.2¬†A Greedy Solution](#%28part._.A_.Greedy_.Solution%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[17.5.3¬†Another Greedy Solution](#%28part._.Another_.Greedy_.Solution%29)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[17.5.4¬†A Third Solution](#%28part._.A_.Third_.Solution%29) |'
  prefs: []
  type: TYPE_TB
- en: '| ¬†¬†¬†¬†[17.5.5¬†Checking Component Connectedness](#%28part._union-find-functional%29)
    |'
  prefs: []
  type: TYPE_TB
- en: 'At the turn of the milennium, the US National Academy of Engineering surveyed
    its members to determine the ‚ÄúGreatest Engineering Achievements of the 20th Century‚Äù.
    The list contained the usual suspects: electronics, computers, the Internet, and
    so on. But a perhaps surprising idea topped the list: (rural) electrification.Read
    more about it [on their site](http://www.greatachievements.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: 17.5.1¬†The Problem[üîó](#(part._.The_.Problem) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To understand the history of national electrical grids, it helps to go back
    to [Moravia](http://en.wikipedia.org/wiki/Moravia) in the 1920s. Like many parts
    of the world, it was beginning to realize the benefits of electricity and intended
    to spread it around the region. A Moravian academia named Otakar Bor≈Øvka heard
    about the problem, and in a remarkable effort, described the problem abstractly,
    so that it could be understood without reference to Moravia or electrical networks.
    He modeled it as a problem about graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bor≈Øvka observed that at least initially, any solution to the problem of creating
    a network must have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The electrical network must reach all the towns intended to be covered by it.
    In graph terms, the solution must be spanning, meaning it must visit every node
    in the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redundancy is a valuable property in any network: that way, if one set of links
    goes down, there might be another way to get a payload to its destination. When
    starting out, however, redundancy may be too expensive, especially if it comes
    at the cost of not giving someone a payload at all. Thus, the initial solution
    was best set up without loops or even redundant paths. In graph terms, the solution
    had to be a tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the goal was to solve this problem for the least cost possible. In
    graph terms, the graph would be weighted, and the solution had to be a minimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus Bor≈Øvka defined the Moravian Spanning Tree (MST) problem.
  prefs: []
  type: TYPE_NORMAL
- en: 17.5.2¬†A Greedy Solution[üîó](#(part._.A_.Greedy_.Solution) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Bor≈Øvka had published his problem, and another Czech mathematician, [Vojtƒõch
    Jarn√≠k](http://en.wikipedia.org/wiki/Vojt%C4%9Bch_Jarn%C3%ADk), came across it.
    Jarn√≠k came up with a solution that should sound familiar:'
  prefs: []
  type: TYPE_NORMAL
- en: Begin with a solution consisting of a single node, chosen arbitrarily. For the
    graph consisting of this one node, this solution is clearly a minimum, spanning,
    and a tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of all the edges incident on nodes in the solution that connect to a node not
    already in the solution, pick the edge with the least weight.Note that we consider
    only the incident edges, not their weight added to the weight of the node to which
    they are incident.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add this edge to the solution. The claim is that for the new solution will be
    a tree (by construction), spanning (also by construction), and a minimum. The
    minimality follows by an argument similar to that used for Dijkstra‚Äôs Algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jarn√≠k had the misfortune of publishing this work in Czech in 1930, and it went
    largely ignored. It was rediscovered by others, most notably by R.C. Prim in 1957,
    and is now generally known as Prim‚Äôs Algorithm, though calling it Jarn√≠k‚Äôs Algorithm
    would attribute credit in the right place.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing this algorithm is pretty easy. At each point, we need to know the
    lightest edge incident on the current solution tree. Finding the lightest edge
    takes time linear in the number of these edges, but the very lightest one may
    create a cycle. We therefore need to efficiently check for whether adding an edge
    would create a cycle, a problem we will return to multiple times [[Checking Component
    Connectedness](#%28part._union-find-functional%29)]. Assuming we can do that effectively,
    we then want to add the lightest edge and iterate. Even given an efficient solution
    for checking cyclicity, this would seem to require an operation linear in the
    number of edges for each node. With better representations we can improve on this
    complexity, but let‚Äôs look at other ideas first.
  prefs: []
  type: TYPE_NORMAL
- en: 17.5.3¬†Another Greedy Solution[üîó](#(part._.Another_.Greedy_.Solution) "Link
    to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recall that Jarn√≠k presented his algorithm in 1930, when computers didn‚Äôt exist,
    and Prim his in 1957, when they were very much in their infancy. Programming computers
    to track heaps was a non-trivial problem, and many algorithms were implemented
    by hand, where keeping track of a complex data structure without making errors
    was harder still. There was need for a solution that was required less manual
    bookkeeping (literally speaking).
  prefs: []
  type: TYPE_NORMAL
- en: In 1956, [Joseph Kruskal](http://en.wikipedia.org/wiki/Joseph_Kruskal) presented
    such a solution. His idea was elegantly simple. The Jarn√≠k algorithm suffers from
    the problem that each time the tree grows, we have to revise the content of the
    heap, which is already a messy structure to track. Kruskal noted the following.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain a minimum solution, surely we want to include one of the edges of
    least weight in the graph. Because if not, we can take an otherwise minimal solution,
    add this edge, and remove one other edge; the graph would still be just as connected,
    but the overall weight would be no more and, if the removed edge were heavier,
    would be less.Note the careful wording: there may be many edges of the same least
    weight, so adding one of them may remove another, and therefore not produce a
    lighter tree; but the key point is that it certainly will not produce a heavier
    one. By the same argument we can add the next lightest edge, and the next lightest,
    and so on. The only time we cannot add the next lightest edge is when it would
    create a cycle (that problem again!).'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, Kruskal‚Äôs algorithm is utterly straightforward. We first sort all
    the edges, ordered by ascending weight. We then take each edge in ascending weight
    order and add it to the solution provided it will not create a cycle. When we
    have thus processed all the edges, we will have a solution that is a tree (by
    construction), spanning (because every connected vertex must be the endpoint of
    some edge), and of minimum weight (by the argument above). The complexity is that
    of sorting (which is \([e \rightarrow e \log e]\) where \(e\) is the size of the
    edge set. We then iterate over each element in \(e\), which takes time linear
    in the size of that set‚Äî<wbr>modulo the time to check for cycles. This algorithm
    is also easy to implement on paper, because we sort all the edges once, then keep
    checking them off in order, crossing out the ones that create cycles‚Äî<wbr>with
    no dynamic updating of the list needed.
  prefs: []
  type: TYPE_NORMAL
- en: 17.5.4¬†A Third Solution[üîó](#(part._.A_.Third_.Solution) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Both the Jarn√≠k and Kruskal solutions have one flaw: they require a centralized
    data structure (the priority heap, or the sorted list) to incrementally build
    the solution. As parallel computers became available, and graph problems grew
    large, computer scientists looked for solutions that could be implemented more
    efficiently in parallel‚Äî<wbr>which typically meant avoiding any centralized points
    of synchronization, such as these centralized data structures.'
  prefs: []
  type: TYPE_NORMAL
- en: In 1965, M. Sollin constructed an algorithm that met these needs beautifully.
    In this algorithm, instead of constructing a single solution, we grow multiple
    solution components (potentially in parallel if we so wish). Each node starts
    out as a solution component (as it was at the first step of Jarn√≠k‚Äôs Algorithm).
    Each node considers the edges incident to it, and picks the lightest one that
    connects to a different component (that problem again!). If such an edge can be
    found, the edge becomes part of the solution, and the two components combine to
    become a single component. The entire process repeats.
  prefs: []
  type: TYPE_NORMAL
- en: Because every node begins as part of the solution, this algorithm naturally
    spans. Because it checks for cycles and avoids them, it naturally forms a tree.Note
    that avoiding cycles yields a DAG and is not automatically guaranteed to yield
    a tree. We have been a bit lax about this difference throughout this section.
    Finally, minimality follows through similar reasoning as we used in the case of
    Jarn√≠k‚Äôs Algorithm, which we have essentially run in parallel, once from each
    node, until the parallel solution components join up to produce a global solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, maintaining the data for this algorithm by hand is a nightmare.
    Therefore, it would be no surprise that this algorithm was coined in the digital
    age. The real surprise, therefore, is that it was not: it was originally created
    by [Otakar Bor≈Øvka](http://en.wikipedia.org/wiki/Otakar_Bor%C5%AFvka) himself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bor≈Øvka, you see, had figured it all out. He‚Äôd not only understood the problem,
    he had:'
  prefs: []
  type: TYPE_NORMAL
- en: pinpointed the real problem lying underneath the electrification problem so
    it could be viewed in a context-independent way,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: created a descriptive language of graph theory to define it precisely, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: even solved the problem in addition to defining it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He‚Äôd just come up with a solution so complex to implement by hand that Jarn√≠k
    had in essence de-parallelized it so it could be done sequentially. And thus this
    algorithm lay unnoticed until it was reinvented ([several times, actually](http://en.wikipedia.org/wiki/Bor%C5%AFvka's_algorithm))
    by Sollin in time for parallel computing folks to notice a need for it. But now
    we can just call this Bor≈Øvka‚Äôs Algorithm, which is only fitting.
  prefs: []
  type: TYPE_NORMAL
- en: As you might have guessed by now, this problem is indeed called the MST in other
    textbooks, but ‚ÄúM‚Äù stands not for Moravia but for ‚ÄúMinimum‚Äù. But given Bor≈Øvka‚Äôs
    forgotten place in history, we prefer the more whimsical name.
  prefs: []
  type: TYPE_NORMAL
- en: 17.5.5¬†Checking Component Connectedness[üîó](#(part._union-find-functional) "Link
    to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we‚Äôve seen, we need to be able to efficiently tell whether two nodes are
    in the same component. One way to do this is to conduct a depth-first traversal
    (or breadth-first traversal) starting from the first node and checking whether
    we ever visit the second one. (Using one of these traversal strategies ensures
    that we terminate in the presence of loops.) Unfortunately, this takes a linear
    amount of time (in the size of the graph) for every pair of nodes‚Äî<wbr>and depending
    on the graph and choice of node, we might do this for every node in the graph
    on every edge addition! So we‚Äôd clearly like to do this better.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is helpful to reduce this problem from graph connectivity to a more general
    one: of disjoint-set structure (colloquially known as union-find for reasons that
    will soon be clear). If we think of each connected component as a set, then we‚Äôre
    asking whether two nodes are in the same set. But casting it as a set membership
    problem makes it applicable in several other applications as well.'
  prefs: []
  type: TYPE_NORMAL
- en: The setup is as follows. For arbitrary values, we want the ability to think
    of them as elements in a set. We are interested in two operations. One is obviously
    `union`, which merges two sets into one. The other would seem to be something
    like `is-in-same-set` that takes two elements and determines whether they‚Äôre in
    the same set. Over time, however, it has proven useful to instead define the operator
    `find` that, given an element, ‚Äúnames‚Äù the set (more on this in a moment) that
    the element belongs to. To check whether two elements are in the same set, we
    then have to get the ‚Äúset name‚Äù for each element, and check whether these names
    are the same. This certainly sounds more roundabout, but this means we have a
    primitive that may be useful in other contexts, and from which we can easily implement
    `is-in-same-set`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the question is, how do we name sets? The real question we should ask is,
    what operations do we care to perform on these names? All we care about is, given
    two names, they represent the same set precisely when the names are the same.
    Therefore, we could construct a new string, or number, or something else, but
    we have another option: simply pick some element of the set to represent it, i.e.,
    to serve as its name. Thus we will associate each set element with an indicator
    of the ‚Äúset name‚Äù for that element; if there isn‚Äôt one, then its name is itself
    (the `none` case of `parent`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will assume we have some equality predicate for checking when two elements
    are the same, which we do by comparing their value parts, ignoring their parent
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Do Now!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why do we check only the value parts?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will assume that for a given set, we always return the same representative
    element. (Otherwise, equality will fail even though we have the same set.) Thus:We‚Äôve
    used the name `fynd` because `find` is already defined to mean something else
    in Pyret. If you don‚Äôt like the misspelling, you‚Äôre welcome to use a longer name
    like `find-root`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'where `Sets` is the list of all elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'How do we find the representative element for a set? We first find it using
    `is-same-element`; when we do, we check the element‚Äôs `parent` field. If it is
    `none`, that means this very element names its set; this can happen either because
    the element is a singleton set (we‚Äôll initialize all elements with `none`), or
    it‚Äôs the name for some larger set. Either way, we‚Äôre done. Otherwise, we have
    to recursively find the parent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why is there a recursive call in the nested `cases`?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'What‚Äôs left is to implement `union`. For this, we find the representative elements
    of the two sets we‚Äôre trying to union; if they are the same, then the two sets
    are already in a union; otherwise, we have to update the data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To update, we arbitrarily choose one of the set names to be the name of the
    new compound set. We then have to update the parent of the other set‚Äôs name element
    to be this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some tests to illustrate this working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, this implementation suffers from two major problems:'
  prefs: []
  type: TYPE_NORMAL
- en: First, because we are performing functional updates, the value of the `parent`
    reference keeps ‚Äúchanging‚Äù, but these changes are not visible to older copies
    of the ‚Äúsame‚Äù value. An element from different stages of unioning has different
    parent references, even though it is arguably the same element throughout. This
    is a place where functional programming hurts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relatedly, the performance of this implementation is quite bad. `fynd` recursively
    traverses parents to find the set‚Äôs name, but the elements traversed are not updated
    to record this new name. We certainly could update them by reconstructing the
    set afresh each time, but that complicates the implementation and, as we will
    soon see, we can do much better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even worse, it may not even be correct!
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is it? Consider constructing `union`s that are not quite so skewed as above,
    and see whether you get the results you expect.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The bottom line is that pure functional programming is not a great fit with
    this problem. We need a better implementation strategy: [Union-Find](union-find.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 17.5.1¬†The Problem[üîó](#(part._.The_.Problem) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To understand the history of national electrical grids, it helps to go back
    to [Moravia](http://en.wikipedia.org/wiki/Moravia) in the 1920s. Like many parts
    of the world, it was beginning to realize the benefits of electricity and intended
    to spread it around the region. A Moravian academia named Otakar Bor≈Øvka heard
    about the problem, and in a remarkable effort, described the problem abstractly,
    so that it could be understood without reference to Moravia or electrical networks.
    He modeled it as a problem about graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bor≈Øvka observed that at least initially, any solution to the problem of creating
    a network must have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: The electrical network must reach all the towns intended to be covered by it.
    In graph terms, the solution must be spanning, meaning it must visit every node
    in the graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Redundancy is a valuable property in any network: that way, if one set of links
    goes down, there might be another way to get a payload to its destination. When
    starting out, however, redundancy may be too expensive, especially if it comes
    at the cost of not giving someone a payload at all. Thus, the initial solution
    was best set up without loops or even redundant paths. In graph terms, the solution
    had to be a tree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the goal was to solve this problem for the least cost possible. In
    graph terms, the graph would be weighted, and the solution had to be a minimum.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus Bor≈Øvka defined the Moravian Spanning Tree (MST) problem.
  prefs: []
  type: TYPE_NORMAL
- en: 17.5.2¬†A Greedy Solution[üîó](#(part._.A_.Greedy_.Solution) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Bor≈Øvka had published his problem, and another Czech mathematician, [Vojtƒõch
    Jarn√≠k](http://en.wikipedia.org/wiki/Vojt%C4%9Bch_Jarn%C3%ADk), came across it.
    Jarn√≠k came up with a solution that should sound familiar:'
  prefs: []
  type: TYPE_NORMAL
- en: Begin with a solution consisting of a single node, chosen arbitrarily. For the
    graph consisting of this one node, this solution is clearly a minimum, spanning,
    and a tree.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of all the edges incident on nodes in the solution that connect to a node not
    already in the solution, pick the edge with the least weight.Note that we consider
    only the incident edges, not their weight added to the weight of the node to which
    they are incident.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add this edge to the solution. The claim is that for the new solution will be
    a tree (by construction), spanning (also by construction), and a minimum. The
    minimality follows by an argument similar to that used for Dijkstra‚Äôs Algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jarn√≠k had the misfortune of publishing this work in Czech in 1930, and it went
    largely ignored. It was rediscovered by others, most notably by R.C. Prim in 1957,
    and is now generally known as Prim‚Äôs Algorithm, though calling it Jarn√≠k‚Äôs Algorithm
    would attribute credit in the right place.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing this algorithm is pretty easy. At each point, we need to know the
    lightest edge incident on the current solution tree. Finding the lightest edge
    takes time linear in the number of these edges, but the very lightest one may
    create a cycle. We therefore need to efficiently check for whether adding an edge
    would create a cycle, a problem we will return to multiple times [[Checking Component
    Connectedness](#%28part._union-find-functional%29)]. Assuming we can do that effectively,
    we then want to add the lightest edge and iterate. Even given an efficient solution
    for checking cyclicity, this would seem to require an operation linear in the
    number of edges for each node. With better representations we can improve on this
    complexity, but let‚Äôs look at other ideas first.
  prefs: []
  type: TYPE_NORMAL
- en: 17.5.3¬†Another Greedy Solution[üîó](#(part._.Another_.Greedy_.Solution) "Link
    to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recall that Jarn√≠k presented his algorithm in 1930, when computers didn‚Äôt exist,
    and Prim his in 1957, when they were very much in their infancy. Programming computers
    to track heaps was a non-trivial problem, and many algorithms were implemented
    by hand, where keeping track of a complex data structure without making errors
    was harder still. There was need for a solution that was required less manual
    bookkeeping (literally speaking).
  prefs: []
  type: TYPE_NORMAL
- en: In 1956, [Joseph Kruskal](http://en.wikipedia.org/wiki/Joseph_Kruskal) presented
    such a solution. His idea was elegantly simple. The Jarn√≠k algorithm suffers from
    the problem that each time the tree grows, we have to revise the content of the
    heap, which is already a messy structure to track. Kruskal noted the following.
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain a minimum solution, surely we want to include one of the edges of
    least weight in the graph. Because if not, we can take an otherwise minimal solution,
    add this edge, and remove one other edge; the graph would still be just as connected,
    but the overall weight would be no more and, if the removed edge were heavier,
    would be less.Note the careful wording: there may be many edges of the same least
    weight, so adding one of them may remove another, and therefore not produce a
    lighter tree; but the key point is that it certainly will not produce a heavier
    one. By the same argument we can add the next lightest edge, and the next lightest,
    and so on. The only time we cannot add the next lightest edge is when it would
    create a cycle (that problem again!).'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, Kruskal‚Äôs algorithm is utterly straightforward. We first sort all
    the edges, ordered by ascending weight. We then take each edge in ascending weight
    order and add it to the solution provided it will not create a cycle. When we
    have thus processed all the edges, we will have a solution that is a tree (by
    construction), spanning (because every connected vertex must be the endpoint of
    some edge), and of minimum weight (by the argument above). The complexity is that
    of sorting (which is \([e \rightarrow e \log e]\) where \(e\) is the size of the
    edge set. We then iterate over each element in \(e\), which takes time linear
    in the size of that set‚Äî<wbr>modulo the time to check for cycles. This algorithm
    is also easy to implement on paper, because we sort all the edges once, then keep
    checking them off in order, crossing out the ones that create cycles‚Äî<wbr>with
    no dynamic updating of the list needed.
  prefs: []
  type: TYPE_NORMAL
- en: 17.5.4¬†A Third Solution[üîó](#(part._.A_.Third_.Solution) "Link to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Both the Jarn√≠k and Kruskal solutions have one flaw: they require a centralized
    data structure (the priority heap, or the sorted list) to incrementally build
    the solution. As parallel computers became available, and graph problems grew
    large, computer scientists looked for solutions that could be implemented more
    efficiently in parallel‚Äî<wbr>which typically meant avoiding any centralized points
    of synchronization, such as these centralized data structures.'
  prefs: []
  type: TYPE_NORMAL
- en: In 1965, M. Sollin constructed an algorithm that met these needs beautifully.
    In this algorithm, instead of constructing a single solution, we grow multiple
    solution components (potentially in parallel if we so wish). Each node starts
    out as a solution component (as it was at the first step of Jarn√≠k‚Äôs Algorithm).
    Each node considers the edges incident to it, and picks the lightest one that
    connects to a different component (that problem again!). If such an edge can be
    found, the edge becomes part of the solution, and the two components combine to
    become a single component. The entire process repeats.
  prefs: []
  type: TYPE_NORMAL
- en: Because every node begins as part of the solution, this algorithm naturally
    spans. Because it checks for cycles and avoids them, it naturally forms a tree.Note
    that avoiding cycles yields a DAG and is not automatically guaranteed to yield
    a tree. We have been a bit lax about this difference throughout this section.
    Finally, minimality follows through similar reasoning as we used in the case of
    Jarn√≠k‚Äôs Algorithm, which we have essentially run in parallel, once from each
    node, until the parallel solution components join up to produce a global solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, maintaining the data for this algorithm by hand is a nightmare.
    Therefore, it would be no surprise that this algorithm was coined in the digital
    age. The real surprise, therefore, is that it was not: it was originally created
    by [Otakar Bor≈Øvka](http://en.wikipedia.org/wiki/Otakar_Bor%C5%AFvka) himself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bor≈Øvka, you see, had figured it all out. He‚Äôd not only understood the problem,
    he had:'
  prefs: []
  type: TYPE_NORMAL
- en: pinpointed the real problem lying underneath the electrification problem so
    it could be viewed in a context-independent way,
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: created a descriptive language of graph theory to define it precisely, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: even solved the problem in addition to defining it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He‚Äôd just come up with a solution so complex to implement by hand that Jarn√≠k
    had in essence de-parallelized it so it could be done sequentially. And thus this
    algorithm lay unnoticed until it was reinvented ([several times, actually](http://en.wikipedia.org/wiki/Bor%C5%AFvka's_algorithm))
    by Sollin in time for parallel computing folks to notice a need for it. But now
    we can just call this Bor≈Øvka‚Äôs Algorithm, which is only fitting.
  prefs: []
  type: TYPE_NORMAL
- en: As you might have guessed by now, this problem is indeed called the MST in other
    textbooks, but ‚ÄúM‚Äù stands not for Moravia but for ‚ÄúMinimum‚Äù. But given Bor≈Øvka‚Äôs
    forgotten place in history, we prefer the more whimsical name.
  prefs: []
  type: TYPE_NORMAL
- en: 17.5.5¬†Checking Component Connectedness[üîó](#(part._union-find-functional) "Link
    to here")
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As we‚Äôve seen, we need to be able to efficiently tell whether two nodes are
    in the same component. One way to do this is to conduct a depth-first traversal
    (or breadth-first traversal) starting from the first node and checking whether
    we ever visit the second one. (Using one of these traversal strategies ensures
    that we terminate in the presence of loops.) Unfortunately, this takes a linear
    amount of time (in the size of the graph) for every pair of nodes‚Äî<wbr>and depending
    on the graph and choice of node, we might do this for every node in the graph
    on every edge addition! So we‚Äôd clearly like to do this better.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is helpful to reduce this problem from graph connectivity to a more general
    one: of disjoint-set structure (colloquially known as union-find for reasons that
    will soon be clear). If we think of each connected component as a set, then we‚Äôre
    asking whether two nodes are in the same set. But casting it as a set membership
    problem makes it applicable in several other applications as well.'
  prefs: []
  type: TYPE_NORMAL
- en: The setup is as follows. For arbitrary values, we want the ability to think
    of them as elements in a set. We are interested in two operations. One is obviously
    `union`, which merges two sets into one. The other would seem to be something
    like `is-in-same-set` that takes two elements and determines whether they‚Äôre in
    the same set. Over time, however, it has proven useful to instead define the operator
    `find` that, given an element, ‚Äúnames‚Äù the set (more on this in a moment) that
    the element belongs to. To check whether two elements are in the same set, we
    then have to get the ‚Äúset name‚Äù for each element, and check whether these names
    are the same. This certainly sounds more roundabout, but this means we have a
    primitive that may be useful in other contexts, and from which we can easily implement
    `is-in-same-set`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the question is, how do we name sets? The real question we should ask is,
    what operations do we care to perform on these names? All we care about is, given
    two names, they represent the same set precisely when the names are the same.
    Therefore, we could construct a new string, or number, or something else, but
    we have another option: simply pick some element of the set to represent it, i.e.,
    to serve as its name. Thus we will associate each set element with an indicator
    of the ‚Äúset name‚Äù for that element; if there isn‚Äôt one, then its name is itself
    (the `none` case of `parent`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will assume we have some equality predicate for checking when two elements
    are the same, which we do by comparing their value parts, ignoring their parent
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Do Now!
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why do we check only the value parts?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will assume that for a given set, we always return the same representative
    element. (Otherwise, equality will fail even though we have the same set.) Thus:We‚Äôve
    used the name `fynd` because `find` is already defined to mean something else
    in Pyret. If you don‚Äôt like the misspelling, you‚Äôre welcome to use a longer name
    like `find-root`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'where `Sets` is the list of all elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'How do we find the representative element for a set? We first find it using
    `is-same-element`; when we do, we check the element‚Äôs `parent` field. If it is
    `none`, that means this very element names its set; this can happen either because
    the element is a singleton set (we‚Äôll initialize all elements with `none`), or
    it‚Äôs the name for some larger set. Either way, we‚Äôre done. Otherwise, we have
    to recursively find the parent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Why is there a recursive call in the nested `cases`?
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'What‚Äôs left is to implement `union`. For this, we find the representative elements
    of the two sets we‚Äôre trying to union; if they are the same, then the two sets
    are already in a union; otherwise, we have to update the data structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'To update, we arbitrarily choose one of the set names to be the name of the
    new compound set. We then have to update the parent of the other set‚Äôs name element
    to be this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some tests to illustrate this working:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, this implementation suffers from two major problems:'
  prefs: []
  type: TYPE_NORMAL
- en: First, because we are performing functional updates, the value of the `parent`
    reference keeps ‚Äúchanging‚Äù, but these changes are not visible to older copies
    of the ‚Äúsame‚Äù value. An element from different stages of unioning has different
    parent references, even though it is arguably the same element throughout. This
    is a place where functional programming hurts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relatedly, the performance of this implementation is quite bad. `fynd` recursively
    traverses parents to find the set‚Äôs name, but the elements traversed are not updated
    to record this new name. We certainly could update them by reconstructing the
    set afresh each time, but that complicates the implementation and, as we will
    soon see, we can do much better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even worse, it may not even be correct!
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Is it? Consider constructing `union`s that are not quite so skewed as above,
    and see whether you get the results you expect.
  prefs:
  - PREF_BQ
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The bottom line is that pure functional programming is not a great fit with
    this problem. We need a better implementation strategy: [Union-Find](union-find.html).'
  prefs: []
  type: TYPE_NORMAL
