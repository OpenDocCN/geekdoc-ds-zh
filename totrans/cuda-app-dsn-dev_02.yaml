- en: Chapter 1\. First Programs and How to Think in CUDAThe purpose of this chapter
    is to introduce the reader to CUDA (the parallel computing architecture developed
    by NVIDIA) and differentiate CUDA from programming conventional single and multicore
    processors. Example programs and instructions will show the reader how to compile
    and run programs as well as how to adapt them to their own purposes. The CUDA
    Thrust and runtime APIs (Application Programming Interface) will be used and discussed.
    Three rules of GPGPU programming will be introduced as well as Amdahl's law, Big-O
    notation, and the distinction between data-parallel and task-parallel programming.
    Some basic GPU debugging tools will be introduced, but for the most part NVIDIA
    has made debugging CUDA code identical to debugging any other C or C++ application.
    Where appropriate, references to introductory materials will be provided to help
    novice readers. At the end of this chapter, the reader will be able to write and
    debug massively parallel programs that concurrently utilize both a GPGPU and the
    host processor(s) within a single application that can handle a million threads
    of execution.**Keywords**CUDA, C++, Thrust, Runtime, API, debugging, Amdhal's
    law, Big-O notation, OpenMP, asynchronous, kernel, cuda-gdb, dddThe purpose of
    this chapter is to introduce the reader to CUDA (the parallel computing architecture
    developed by NVIDIA) and differentiate CUDA from programming conventional single
    and multicore processors. Example programs and instructions will show the reader
    how to compile and run programs as well as how to adapt them to their own purposes.
    The CUDA Thrust and runtime APIs (Application Programming Interface) will be used
    and discussed. Three rules of GPGPU programming will be introduced as well as
    Amdahl's law, Big-O notation, and the distinction between data-parallel and task-parallel
    programming. Some basic GPU debugging tools will be introduced, but for the most
    part NVIDIA has made debugging CUDA code identical to debugging any other C or
    C++ application. Where appropriate, references to introductory materials will
    be provided to help novice readers. At the end of this chapter, the reader will
    be able to write and debug massively parallel programs that concurrently utilize
    both a GPGPU and the host processor(s) within a single application that can handle
    a million threads of execution.At the end of the chapter, the reader will have
    a basic understanding of:■ How to create, build, and run CUDA applications.■ Criteria
    to decide which CUDA API to use.■ Amdahl's law and how it relates to GPU computing.■
    Three rules of high-performance GPU computing.■ Big-O notation and the impact
    of data transfers.■ The difference between task-parallel and data-parallel programming.■
    Some GPU-specific capabilities of the Linux, Mac, and Windows CUDA debuggers.■
    The CUDA memory checker and how it can find out-of-bounds and misaligned memory
    errors.
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第1章. 第一个程序以及如何在CUDA中思考 本章的目的是向读者介绍CUDA（由NVIDIA开发的并行计算架构），并将CUDA与编程传统的单核和多核处理器区分开来。示例程序和指令将向读者展示如何编译和运行程序，以及如何将其适应到自己的需求中。将使用并讨论CUDA
    Thrust和运行时API（应用程序编程接口）。本章将介绍GPGPU编程的三条规则，以及阿姆达尔定律、Big-O符号表示法，以及数据并行和任务并行编程的区别。还将介绍一些基本的GPU调试工具，但大多数情况下，NVIDIA已经将调试CUDA代码与调试任何其他C或C++应用程序保持一致。在适当的地方，将提供入门材料的参考，以帮助新手读者。章末，读者将能够编写和调试大规模并行程序，该程序可以在单个应用程序中并行利用GPGPU和主机处理器，并处理一百万个执行线程。**关键词**CUDA,
    C++, Thrust, 运行时, API, 调试, 阿姆达尔定律, Big-O符号表示法, OpenMP, 异步, 内核, cuda-gdb, ddd 本章的目的是向读者介绍CUDA（由NVIDIA开发的并行计算架构），并将CUDA与编程传统的单核和多核处理器区分开来。示例程序和指令将向读者展示如何编译和运行程序，以及如何将其适应到自己的需求中。将使用并讨论CUDA
    Thrust和运行时API（应用程序编程接口）。本章将介绍GPGPU编程的三条规则，以及阿姆达尔定律、Big-O符号表示法，以及数据并行和任务并行编程的区别。还将介绍一些基本的GPU调试工具，但大多数情况下，NVIDIA已经将调试CUDA代码与调试任何其他C或C++应用程序保持一致。在适当的地方，将提供入门材料的参考，以帮助新手读者。章末，读者将能够编写和调试大规模并行程序，该程序可以在单个应用程序中并行利用GPGPU和主机处理器，并处理一百万个执行线程。章末，读者将具备以下基本理解：■
    如何创建、构建和运行CUDA应用程序。■ 决定使用哪个CUDA API的标准。■ 阿姆达尔定律及其与GPU计算的关系。■ 高性能GPU计算的三条规则。■ Big-O符号表示法及数据传输的影响。■
    任务并行与数据并行编程的区别。■ Linux、Mac和Windows CUDA调试器的一些GPU特定功能。■ CUDA内存检查器及其如何发现越界和内存对齐错误。
- en: Source Code and Wiki
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 源代码与维基
- en: 'Source code for all the examples in this book can be downloaded from [http://booksite.mkp.com/9780123884268](http://booksite.mkp.com/9780123884268).
    A wiki (a website collaboratively developed by a community of users) is available
    to share information, make comments, and find teaching material; it can be reached
    at any of the following aliases on gpucomputing.net:■ My name: [http://gpucomputing.net/RobFarber](http://gpucomputing.net/RobFarber).■
    The title of this book as one word: [http://gpucomputing.net/CUDAapplicationdesignanddevelopment](http://gpucomputing.net/CUDAapplicationdesignanddevelopment).■
    The name of my series: [http://gpucomputing.net/supercomputingforthemasses](http://gpucomputing.net/supercomputingforthemasses).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中所有示例的源代码可以从[http://booksite.mkp.com/9780123884268](http://booksite.mkp.com/9780123884268)下载。一个wiki（由用户社区共同开发的网站）可供分享信息、发表评论以及查找教学材料；可以通过以下任意别名访问它，地址均在gpucomputing.net上：■
    我的名字：[http://gpucomputing.net/RobFarber](http://gpucomputing.net/RobFarber)。■
    本书标题作为一个单词：[http://gpucomputing.net/CUDAapplicationdesignanddevelopment](http://gpucomputing.net/CUDAapplicationdesignanddevelopment)。■
    我的系列名称：[http://gpucomputing.net/supercomputingforthemasses](http://gpucomputing.net/supercomputingforthemasses)。
- en: Distinguishing CUDA from Conventional Programming with a Simple Example
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用一个简单的例子区分CUDA和传统编程
- en: 'Programming a sequential processor requires writing a program that specifies
    each of the tasks needed to compute some result. See [Example 1.1](#tb0010), “seqSerial.cpp,
    a sequential C++ program”:`//seqSerial.cpp``#include <iostream>``#include <vector>``using
    namespace std;``int main()``{``const int N=50000;``// task 1: create the array``vector<int>
    a(N);``// task 2: fill the array``for(int i=0; i < N; i++) a[i]=i;``// task 3:
    calculate the sum of the array``int sumA=0;``for(int i=0; i < N; i++) sumA +=
    a[i];``// task 4: calculate the sum of 0 .. N−1``int sumCheck=0;``for(int i=0;
    i < N; i++) sumCheck += i;``// task 5: check the results agree``if(sumA == sumCheck)
    cout << "Test Succeeded!" << endl;``else {cerr << "Test FAILED!" << endl; return(1);}``return(0);``}`[Example
    1.1](#tb0010) performs five tasks:1\. It creates an integer array.2\. A **for**
    loop fills the array **a** with integers from 0 to **N**−1.3\. The sum of the
    integers in the array is computed.4\. A separate **for** loop computes the sum
    of the integers by an alternate method.5\. A comparison checks that the sequential
    and parallel results are the same and reports the success of the test.Notice that
    the processor runs each task consecutively one after the other. Inside of tasks
    2–4, the processor iterates through the loop starting with the first index. Once
    all the tasks have finished, the program exits. This is an example of a *single
    thread of execution*, which is illustrated in [Figure 1.1](#f0010) for task 2
    as a single thread fills the first three elements of array *a*.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 编写顺序处理器程序需要编写一个程序，指定计算某个结果所需的每个任务。参见[示例 1.1](#tb0010)，“seqSerial.cpp，一个顺序的C++程序”:`//seqSerial.cpp``#include
    <iostream>``#include <vector>``using namespace std;``int main()``{``const int
    N=50000;``// 任务 1：创建数组``vector<int> a(N);``// 任务 2：填充数组``for(int i=0; i < N; i++)
    a[i]=i;``// 任务 3：计算数组之和``int sumA=0;``for(int i=0; i < N; i++) sumA += a[i];``//
    任务 4：计算 0 .. N−1 的和``int sumCheck=0;``for(int i=0; i < N; i++) sumCheck += i;``//
    任务 5：检查结果是否一致``if(sumA == sumCheck) cout << "测试成功！" << endl;``else {cerr << "测试失败！"
    << endl; return(1);}``return(0);``}`[示例 1.1](#tb0010) 执行了五个任务：1. 它创建了一个整数数组。2.
    一个**for**循环将整数从0到**N**−1填充到数组**a**中。3. 计算数组中整数的和。4. 一个独立的**for**循环通过另一种方法计算整数之和。5.
    通过比较检查顺序结果和并行结果是否相同，并报告测试成功。请注意，处理器按顺序依次运行每个任务。在任务2到4中，处理器从第一个索引开始迭代循环。一旦所有任务完成，程序就退出。这是一个*单一执行线程*的示例，如[图
    1.1](#f0010)所示，在任务2中，单一线程填充数组*a*的前三个元素。
- en: '| ![B978012388426800001X/f01-01-9780123884268.jpg is missing](B978012388426800001X/f01-01-9780123884268.jpg)
    |'
  id: totrans-5
  prefs: []
  type: TYPE_TB
  zh: '| ![B978012388426800001X/f01-01-9780123884268.jpg 找不到图片](B978012388426800001X/f01-01-9780123884268.jpg)
    |'
- en: '| **Figure 1.1**A single thread of execution. |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| **图 1.1**单一执行线程。 |'
- en: 'This program can be compiled and executed with the following commands:■ Linux
    and Cygwin users ([Example 1.2](#tb0015), “Compiling with g++”):`g++ seqSerial.cpp
    –o seqSerial``./seqSerial`■ Utilizing the command-line interface for Microsoft
    Visual Studio users ([Example 1.3](#tb0020), “Compiling with the Visual Studio
    Command-Line Interface”):`cl.exe seqSerial.cpp –o seqSerial.exe``seqSerial.exe`■
    Of course, all CUDA users (Linux, Windows, MacOS, Cygwin) can utilize the NVIDIA
    **nvcc** compiler regardless of platform ([Example 1.4](#tb0025), “Compiling with
    nvcc”):`nvcc seqSerial.cpp –o seqSerial``./seqSerial`In all cases, the program
    will print “Test succeeded!”For comparison, let''s create and run our first CUDA
    program seqCuda*.cu*, in C++. (Note: CUDA supports both C and C++ programs. For
    simplicity, the following example was written in C++ using the Thrust data-parallel
    API as will be discussed in greater depth in this chapter.) CUDA programs utilize
    the *file extension* suffix “*.cu*” to indicate CUDA source code. See [Example
    1.5](#tb0030), “A Massively Parallel CUDA Code Using the Thrust API”:`//seqCuda.cu``#include
    <iostream>``using namespace std;``#include <thrust/reduce.h>``#include <thrust/sequence.h>``#include
    <thrust/host_vector.h>``#include <thrust/device_vector.h>``int main()``{``const
    int N=50000;``// task 1: create the array``thrust::device_vector<int> a(N);``//
    task 2: fill the array``**thrust::sequence(a.begin(), a.end(), 0);**``// task
    3: calculate the sum of the array``**int sumA= thrust::reduce(a.begin(),a.end(),
    0);**``// task 4: calculate the sum of 0 .. N−1``int sumCheck=0;``for(int i=0;
    i < N; i++) sumCheck += i;``// task 5: check the results agree``if(sumA == sumCheck)
    cout << "Test Succeeded!" << endl;``else { cerr << "Test FAILED!" << endl; return(1);}``return(0);``}`[Example
    1.5](#tb0030) is compiled with the NVIDIA **nvcc** compiler under Windows, Linux,
    and MacOS. If **nvcc** is not available on your system, download and install the
    free CUDA tools, driver, and SDK (Software Development Kit) from the NVIDIA CUDA
    Zone ([http://developer.nvidia.com](http://developer.nvidia.com)). See [Example
    1.6](#tb0035), “Compiling and Running the Example”:`nvcc seqCuda.cu –o seqCuda``./seqCuda`Again,
    running the program will print “Test succeeded!”Congratulations: you just created
    a CUDA application that uses 50,000 software threads of execution and ran it on
    a GPU! (The actual number of threads that run concurrently on the hardware depends
    on the capabilities of the GPGPU in your system.)Aside from a few calls to the
    CUDA Thrust API (prefaced by **thrust::** in this example), the CUDA code looks
    almost identical to the sequential C++ code. The highlighted lines in the example
    perform parallel operations.Unlike the single-threaded execution illustrated in
    [Figure 1.1](#f0010), the code in [Example 1.5](#tb0030) utilizes many threads
    to perform a large number of concurrent operations as is illustrated in [Figure
    1.2](#f0015) for task 2 when filling array **a**.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序可以通过以下命令进行编译和执行：■ Linux 和 Cygwin 用户（[示例 1.2](#tb0015)，“使用 g++ 编译”）：`g++
    seqSerial.cpp –o seqSerial``./seqSerial`■ 对于 Microsoft Visual Studio 用户，使用命令行接口（[示例
    1.3](#tb0020)，“使用 Visual Studio 命令行接口编译”）：`cl.exe seqSerial.cpp –o seqSerial.exe``seqSerial.exe`■
    当然，所有 CUDA 用户（Linux、Windows、MacOS、Cygwin）都可以使用 NVIDIA **nvcc** 编译器，无论平台如何（[示例
    1.4](#tb0025)，“使用 nvcc 编译”）：`nvcc seqSerial.cpp –o seqSerial``./seqSerial`在所有情况下，程序都会打印“测试成功！”为了进行比较，让我们创建并运行我们的第一个
    CUDA 程序 seqCuda*.cu*，使用 C++ 编写。（注意：CUDA 支持 C 和 C++ 程序。为了简单起见，以下示例使用 C++ 并结合 Thrust
    数据并行 API，如本章后续将深入讨论。）CUDA 程序使用 *文件扩展名* 后缀 “*.cu*” 来表示 CUDA 源代码。请参见 [示例 1.5](#tb0030)，“使用
    Thrust API 的大规模并行 CUDA 代码”：`//seqCuda.cu``#include <iostream>``using namespace
    std;``#include <thrust/reduce.h>``#include <thrust/sequence.h>``#include <thrust/host_vector.h>``#include
    <thrust/device_vector.h>``int main()``{``const int N=50000;``// 任务 1：创建数组``thrust::device_vector<int>
    a(N);``// 任务 2：填充数组``**thrust::sequence(a.begin(), a.end(), 0);**``// 任务 3：计算数组之和``**int
    sumA= thrust::reduce(a.begin(),a.end(), 0);**``// 任务 4：计算 0 .. N−1 的和``int sumCheck=0;``for(int
    i=0; i < N; i++) sumCheck += i;``// 任务 5：检查结果是否一致``if(sumA == sumCheck) cout <<
    "Test Succeeded!" << endl;``else { cerr << "Test FAILED!" << endl; return(1);}``return(0);``}`[示例
    1.5](#tb0030) 是在 Windows、Linux 和 MacOS 下使用 NVIDIA **nvcc** 编译器编译的。如果系统中没有 **nvcc**，请从
    NVIDIA CUDA Zone 下载并安装免费的 CUDA 工具、驱动程序和 SDK（软件开发工具包）（[http://developer.nvidia.com](http://developer.nvidia.com)）。请参见
    [示例 1.6](#tb0035)，“编译并运行示例”：`nvcc seqCuda.cu –o seqCuda``./seqCuda`再次运行程序会打印“测试成功！”恭喜你：你刚刚创建了一个
    CUDA 应用程序，使用了 50,000 个软件执行线程并在 GPU 上运行！（实际并发运行的线程数取决于你系统中 GPGPU 的能力。）除了几次对 CUDA
    Thrust API 的调用（在本示例中以 **thrust::** 开头），CUDA 代码几乎与顺序 C++ 代码完全相同。示例中突出显示的行执行了并行操作。与
    [图 1.1](#f0010) 中展示的单线程执行不同，[示例 1.5](#tb0030) 中的代码使用了多个线程来执行大量并发操作，如 [图 1.2](#f0015)
    中所示，任务 2 在填充数组 **a** 时的操作。
- en: '| ![B978012388426800001X/f01-02-9780123884268.jpg is missing](B978012388426800001X/f01-02-9780123884268.jpg)
    |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| ![B978012388426800001X/f01-02-9780123884268.jpg 找不到](B978012388426800001X/f01-02-9780123884268.jpg)
    |'
- en: '| **Figure 1.2**Parallel threads of execution. |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| **图 1.2** 执行的并行线程。 |'
- en: Choosing a CUDA API
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择一个CUDA API
- en: 'CUDA offers several APIs to use when programming. They are from highest to
    lowest level:1\. The data-parallel C++ Thrust API2\. The runtime API, which can
    be used in either C or C++3\. The driver API, which can be used with either C
    or C++Regardless of the API or mix of APIs used in an application, CUDA can be
    called from other high-level languages such as Python, Java, FORTRAN, and many
    others. The calling conventions and details necessary to correctly link vary with
    each language.Which API to use depends on the amount of control the developer
    wishes to exert over the GPU. Higher-level APIs like the C++ Thrust API are convenient,
    as they do more for the programmer, but they also make some decisions on behalf
    of the programmer. In general, Thrust has been shown to deliver high computational
    performance, generality, and convenience. It also makes code development quicker
    and can produce easier to read source code that many will argue is more maintainable.
    Without modification, programs written in Thrust will most certainly maintain
    or show improved performance as Thrust matures in future releases. Many Thrust
    methods like reduction perform significant work, which gives the Thrust API developers
    much freedom to incorporate features in the latest hardware that can improve performance.
    Thrust is an example of a well-designed API that is simple yet general and that
    has the ability to be adapted to improve performance as the technology evolves.A
    disadvantage of a high-level API like Thrust is that it can isolate the developer
    from the hardware and expose only a subset of the hardware capabilities. In some
    circumstances, the C++ interface can become too cumbersome or verbose. Scientific
    programmers in particular may feel that the clarity of simple loop structures
    can get lost in the C++ syntax.Use a high-level interface first and choose to
    drop down to a lower-level API when you think the additional programming effort
    will deliver greater performance or to make use of some lower-level capability
    needed to better support your application. The CUDA runtime in particular was
    designed to give the developer access to all the programmable features of the
    GPGPU with a few simple yet elegant and powerful syntactic additions to the C-language.
    As a result, CUDA runtime code can sometimes be the cleanest and easiest API to
    read; plus, it can be extremely efficient. An important aspect of the lowest-level
    driver interface is that it can provide very precise control over both queuing
    and data transfers.Expect code size to increase when using the lower-level interfaces,
    as the developer must make more API calls and/or specify more parameters for each
    call. In addition, the developer needs to check for runtime errors and version
    incompatibilities. In many cases when using low-level APIs, it is not unusual
    for more lines of the application code to be focused on the details of the API
    interface than on the actual work of the task.Happily, modern CUDA developers
    are not restricted to use just a single API in an application, which was not the
    case prior to the CUDA 3.2 release in 2010\. Modern versions of CUDA allow developers
    to use any of the three APIs in their applications whenever they choose. Thus,
    an initial code can be written in a high-level API such as Thrust and then refactored
    to use some special characteristic of the runtime or driver API.Let''s use this
    ability to mix various levels of API calls to highlight and make more explicit
    the parallel nature of the sequential fill task (task 2) from our previous examples.
    [Example 1.7](#tb0040), “Using the CUDA Runtime to Fill an Array with Sequential
    Integers,” also gives us a chance to introduce the CUDA runtime API:`//seqRuntime.cu``#include
    <iostream>``using namespace std;``#include <thrust/reduce.h>``#include <thrust/sequence.h>``#include
    <thrust/host_vector.h>``#include <thrust/device_vector.h>``**__global__ void fillKernel(int
    *a, int n)**``**{**``**int tid = blockIdx.x*blockDim.x + threadIdx.x;**``**if
    (tid < n) a[tid] = tid;**``**}**``**void fill(int* d_a, int n)**``**{**``**int
    nThreadsPerBlock= 512;**``**int nBlocks= n/nThreadsPerBlock + ((n%nThreadsPerBlock)?1:0);**``**fillKernel
    <<< nBlocks, nThreadsPerBlock >>> (d_a, n);**``**}**``int main()``{``const int
    N=50000;``// task 1: create the array``thrust::device_vector<int> a(N);``// task
    2: fill the array using the runtime``**fill(thrust::raw_pointer_cast(&a[0]),N);**``//
    task 3: calculate the sum of the array``int sumA= thrust::reduce(a.begin(),a.end(),
    0);``// task 4: calculate the sum of 0 .. N−1``int sumCheck=0;``for(int i=0; i
    < N; i++) sumCheck += i;``// task 5: check the results agree``if(sumA == sumCheck)
    cout << "Test Succeeded!" << endl;``else { cerr << "Test FAILED!" << endl; return(1);}``return(0);``}`The
    modified sections of the code are in bold. To minimize changes to the structure
    of **main()**, the call to **thrust::sequence()** is replaced by a call to a routine
    **fill()**, which is written using the runtime API. Because array **a** was allocated
    with **thrust::device_vector<>()**, a call to **thrust::raw_pointer_cast()** is
    required to get the actual location of the data on the GPU. The **fill()** subroutine
    uses a C-language calling convention (e.g., passing an int* and the length of
    the vector) to emphasize that CUDA is accessible using both C and C++. Astute
    readers will note that a better C++ programming practice would be to pass a reference
    to the Thrust device vector for a number of reasons, including: better type checking,
    as **fill()** could mistakenly be passed a pointer to an array in host memory;
    the number of elements in the vector **a** can be safely determined with **a.size()**
    to prevent the parameter **n** from being incorrectly specified; and a number
    of other reasons.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'CUDA 提供了多个 API 供编程时使用，它们按从高到低的层次排列：1\. 数据并行 C++ Thrust API 2\. 运行时 API，可以在
    C 或 C++ 中使用 3\. 驱动程序 API，可以在 C 或 C++ 中使用 无论在应用程序中使用哪种 API 或多种 API 的组合，CUDA 都可以从其他高级语言（如
    Python、Java、FORTRAN 等）中调用。每种语言所需的调用约定和细节有所不同。选择使用哪个 API 取决于开发人员希望对 GPU 的控制程度。像
    C++ Thrust API 这样的高级 API 非常方便，因为它们为程序员做了更多的工作，但它们也会为程序员做出一些决定。一般来说，Thrust 被证明能够提供高计算性能、通用性和便利性。它还可以加速代码开发，并生成更易于阅读的源代码，许多人认为它更易于维护。如果不进行修改，使用
    Thrust 编写的程序在未来的版本中很可能会保持或提高性能。许多 Thrust 方法，如 reduction，执行了大量的工作，这使得 Thrust API
    的开发者有更多的自由去结合最新硬件的特性，从而提升性能。Thrust 是一个设计良好的 API 示例，它简洁而通用，并具有适应技术进步以提高性能的能力。 像
    Thrust 这样的高级 API 的一个缺点是，它可能会将开发者与硬件隔离，只暴露硬件能力的子集。在某些情况下，C++ 接口可能会变得过于繁琐或冗长。尤其是科学程序员可能会觉得简单的循环结构的清晰度在
    C++ 语法中丢失了。首先使用高级接口，并在认为额外的编程工作可以带来更高的性能，或者需要使用某些低级功能来更好地支持应用程序时选择降级到低级 API。特别是
    CUDA 运行时被设计为通过一些简单而优雅、强大的 C 语言语法扩展，给开发者提供对 GPGPU 所有可编程特性的访问。因此，CUDA 运行时代码有时是最简洁、最容易阅读的
    API，此外，它也可以非常高效。最低级别的驱动接口的一个重要方面是，它可以提供对队列和数据传输的非常精确的控制。 当使用低级接口时，代码大小可能会增加，因为开发者需要进行更多的
    API 调用和/或为每次调用指定更多的参数。此外，开发者还需要检查运行时错误和版本不兼容。在许多情况下，使用低级 API 时，应用程序代码的更多行将专注于
    API 接口的细节，而不是任务的实际工作。 幸运的是，现代 CUDA 开发者不再仅限于在应用程序中使用单一的 API，这在 2010 年 CUDA 3.2
    发布之前并不适用。现代版本的 CUDA 允许开发者在其应用程序中随时使用三种 API 之一。因此，可以先使用高级 API（如 Thrust）编写初始代码，然后重构为使用运行时或驱动程序
    API 的某些特殊特性。 让我们利用这种混合使用不同级别 API 调用的能力，来突出并更明确地表达之前示例中顺序填充任务（任务 2）的并行特性。[示例 1.7](#tb0040)，“使用
    CUDA 运行时填充数组以生成顺序整数”，也给我们提供了一个机会来介绍 CUDA 运行时 API：`//seqRuntime.cu` `#include <iostream>`
    `using namespace std;` `#include <thrust/reduce.h>` `#include <thrust/sequence.h>`
    `#include <thrust/host_vector.h>` `#include <thrust/device_vector.h>` `**__global__
    void fillKernel(int *a, int n)**` `**{**` `**int tid = blockIdx.x*blockDim.x +
    threadIdx.x;**` `**if (tid < n) a[tid] = tid;**` `**}**` `**void fill(int* d_a,
    int n)**` `**{**` `**int nThreadsPerBlock= 512;**` `**int nBlocks= n/nThreadsPerBlock
    + ((n%nThreadsPerBlock)?1:0);**` `**fillKernel <<< nBlocks, nThreadsPerBlock >>>
    (d_a, n);**` `**}**` `**int main()**` `{` `const int N=50000;` `// 任务 1: 创建数组`
    `thrust::device_vector<int> a(N);` `// 任务 2: 使用运行时填充数组` `**fill(thrust::raw_pointer_cast(&a[0]),N);**`
    `// 任务 3: 计算数组的和` `int sumA= thrust::reduce(a.begin(),a.end(), 0);` `// 任务 4:
    计算 0 .. N−1 的和` `int sumCheck=0;` `for(int i=0; i < N; i++) sumCheck += i;` `//
    任务 5: 检查结果是否一致` `if(sumA == sumCheck) cout << "Test Succeeded!" << endl;` `else
    { cerr << "Test FAILED!" << endl; return(1);}` `return(0);` `}` 修改后的代码部分用粗体标出。为了最小化对**main()**结构的修改，**thrust::sequence()**的调用被替换为调用使用运行时
    API 编写的**fill()**例程。因为数组 **a** 是使用 **thrust::device_vector<>()** 分配的，所以需要调用 **thrust::raw_pointer_cast()**
    来获取数据在 GPU 上的实际位置。**fill()** 子例程使用 C 语言的调用约定（例如，传递 int* 和向量的长度），以强调 CUDA 可以通过
    C 和 C++ 两种方式访问。细心的读者会注意到，较好的 C++ 编程实践是传递 Thrust 设备向量的引用，原因有很多，包括：更好的类型检查，因为 **fill()**
    可能错误地传递了一个指向主机内存中数组的指针；可以安全地使用 **a.size()** 确定向量 **a** 的元素个数，以防止 **n** 参数被错误指定；以及其他一些原因。'
- en: Some Basic CUDA Concepts
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些基本的CUDA概念
- en: Before discussing the runtime version of the **fill()** subroutine, it is important
    to understand some basic CUDA concepts:■ **CUDA-enabled GPUs are separate devices
    that are installed in a host computer**. In most cases, GPGPUs connect to a host
    system via a high-speed interface like the PCIe (Peripheral Component Interconnect
    Express) bus.Two to four GPGPUs can be added to most workstations or cluster nodes.
    How many depends on the host system capabilities such as the number of PCIe slots
    plus the available space, power, and cooling within the box.Each GPGPU is a separate
    device that runs asynchronously to the host processor(s), which means that the
    host processor and all the GPGPUs can be busily performing calculations at the
    same time. The PCIe bus is used to transfer both data and commands between the
    devices. CUDA provides various data transfer mechanisms that include:■ Explicit
    data transfers with **cudaMemcpy()** (the most common runtime data transfer method).
    Those who use Thrust can perform an assignment to move data among vectors (see
    [Example 1.8](#tb0045), “Code Snippet Illustrating How to Move Data with Thrust”):`//Use
    thrust to move data from host to device``d_a = h_a;``//or from device to host``h_a
    = d_a;`■ Implicit transfers with mapped, pinned memory. This interface keeps a
    region of host memory synchronized with a region of GPU memory and does not require
    programmer intervention. For example, an application can load a data set on the
    host, map the memory to the GPU, and then use the data on the GPU as if it had
    been explicitly initialized with a copy operation. The use of mapped, pinned memory
    can increase the efficiency of a program because the transfers are asynchronous.
    Some low-power GPGPUs utilize host memory to save cost and power. On these devices,
    using mapped, pinned memory results in a *zero-copy* operation, as the GPU will
    access the data without any copy operation.At the very lowest level, the host
    and GPGPU hardware interact through a software component called a *device driver*.
    The device driver manages the hardware interface so the GPU and host system can
    interact to communicate, calculate, and display information. It also supports
    many operations, including mapped memory, buffering, queuing, and synchronization.
    The components of the CUDA software stack are illustrated in [Figure 1.3](#f0020).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论**fill()**子程序的运行时版本之前，了解一些基本的CUDA概念是非常重要的：■ **CUDA启用的GPU是安装在主机计算机中的独立设备**。在大多数情况下，GPGPU通过高速接口（如PCIe（外围组件互联快速通道）总线）连接到主机系统。大多数工作站或集群节点可以添加两到四个GPGPU，具体数量取决于主机系统的能力，如PCIe插槽数量、机箱内的可用空间、电源和冷却。每个GPGPU都是一个独立的设备，异步运行于主机处理器，这意味着主机处理器和所有GPGPU可以同时进行繁重的计算。PCIe总线用于在设备之间传输数据和命令。CUDA提供了多种数据传输机制，包括：■
    使用**cudaMemcpy()**的显式数据传输（最常见的运行时数据传输方法）。使用Thrust的用户可以通过分配操作将数据从一个向量移动到另一个向量（请参见[示例1.8](#tb0045)，“演示如何使用Thrust移动数据的代码片段”）：`//使用thrust将数据从主机移至设备``d_a
    = h_a;``//或从设备移至主机``h_a = d_a;`■ 使用映射、固定内存的隐式传输。此接口保持主机内存区域与GPU内存区域的同步，并且不需要程序员干预。例如，应用程序可以在主机上加载数据集，将内存映射到GPU，然后像显式初始化数据一样使用GPU上的数据。使用映射、固定内存可以提高程序的效率，因为传输是异步的。一些低功耗的GPGPU利用主机内存来节省成本和功耗。在这些设备上，使用映射、固定内存会导致*零拷贝*操作，因为GPU将直接访问数据，而无需任何拷贝操作。在最低级别，主机和GPGPU硬件通过一个名为*设备驱动程序*的软件组件进行交互。设备驱动程序管理硬件接口，使GPU和主机系统能够进行通信、计算和显示信息。它还支持许多操作，包括映射内存、缓冲、排队和同步。CUDA软件堆栈的组件在[图1.3](#f0020)中进行了说明。
- en: '| ![B978012388426800001X/f01-03-9780123884268.jpg is missing](B978012388426800001X/f01-03-9780123884268.jpg)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| ![B978012388426800001X/f01-03-9780123884268.jpg is missing](B978012388426800001X/f01-03-9780123884268.jpg)
    |'
- en: '| **Figure 1.3**Components of the software stack. |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| **图1.3** 软件堆栈的组件。 |'
- en: ■ **GPGPUs run in a memory space separate from the host processor**. Except
    for a few low-end devices, all GPGPUs have their own physical memory (e.g., RAM)
    that have been designed to deliver significantly higher memory bandwidth than
    traditional host memory. Many current GPGPU memory systems deliver approximately
    160–200 GB/s (gigabytes or billions of bytes per second) as compared to traditional
    host memory systems that can deliver 8–20 GB/s. Although CUDA 4.0 provides Unified
    Virtual Addressing (UVA), which joins the host and GPGPU memory into a single
    unified address space, do not forget that accessing memory on another device will
    require some transfer across the bus—even between GPGPUs. However, UVA is important
    because it provides software running on any device the ability to access the data
    on another device using just a pointer.■ **CUDA programs utilize *kernels*, which
    are subroutines callable from the host that execute on the CUDA device**. It is
    important to note that kernels are not functions, as they cannot return a value.
    Most applications that perform well on GPGPUs spend most of their time in one
    or a few computational routines. Turning these routines into CUDA kernels is a
    wonderful way to accelerate the application and utilize the computational power
    of a GPGPU. A kernel is defined with the **__global__** declaration specifier,
    which tells the compiler that the kernel is callable by the host processor.■ **Kernel
    calls are *asynchronous*, meaning that the host queues a kernel for execution
    only on the GPGPU and does not wait for it to finish but rather continues on to
    perform some other work**. At some later time, the kernel actually runs on the
    GPU. Due to this asynchronous calling mechanism, CUDA kernels cannot return a
    function value. For efficiency, a *pipeline* can be created by queuing a number
    of kernels to keep the GPGPU busy for as long as possible. Further, some form
    of synchronization is required so that the host can determine when the kernel
    or pipeline has completed. Two commonly used synchronization mechanisms are:■
    Explicitly calling **cudaThreadSynchronize()**, which acts as a *barrier* causing
    the host to stop and wait for all queued kernels to complete.■ Performing a blocking
    data transfer with **cudaMemcpy()** as **cudaThreadSynchronize()** is called inside
    **cudaMemcpy()**.■ **The basic unit of work on the GPU is a *thread***. It is
    important to understand from a software point of view that each thread is separate
    from every other thread. Every thread acts as if it has its own processor with
    separate registers and identity (e.g., location in a computational grid) that
    happens to run in a shared memory environment. The hardware defines the number
    of threads that are able to run concurrently. The onboard GPU hardware thread
    scheduler decides when a group of threads can run and has the ability to switch
    between threads so quickly that from a software point of view, thread switching
    and scheduling happen for free. Some simple yet elegant additions to the C language
    allow threads to communicate through the CUDA shared memory spaces and via atomic
    memory operations.A kernel should utilize many threads to perform the work defined
    in the kernel source code. This utilization is called *thread-level parallelism*,
    which is different than *instruction-level parallelism*, where parallelization
    occurs over processor instructions. [Figure 1.2](#f0015) illustrates the use of
    many threads in a parallel fill as opposed to the single-threaded fill operation
    illustrated in [Figure 1.1](#f0010).An *execution configuration* defines both
    the number of threads that will run the kernel plus their arrangement in a 1D,
    2D, or 3D computational grid. An execution configuration encloses the configuration
    information between triple angle brackets “<<<” and “>>>” that follow after the
    name of the kernel and before the parameter list enclosed between the parentheses.
    Aside from the execution configuration, queuing a kernel looks very similar to
    a subroutine call. [Example 1.7](#tb0040) demonstrates this syntax with the call
    to **fillKernel()** in subroutine **fill()**.■ **The largest shared region of
    memory on the GPU is called *global memory***. Measured in gigabytes of RAM, most
    application data resides in global memory. Global memory is subject to *coalescing*
    rules that combine multiple memory transactions into single large load or store
    operations that can attain the highest transfer rate to and from memory. In general,
    best performance occurs when memory accesses can be coalesced into 128 consecutive
    byte chunks. Other forms of onboard GPU programmer-accessible memory include *constant*,
    *cache*, *shared*, *local*, *texture*, and *register memory*, as discussed in
    [Chapter 5](B9780123884268000057.xhtml#B978-0-12-388426-8.00005-7).The latency
    in accessing global memory can be high, up to 600 times slower than accessing
    a register variable. CUDA programmers should note that although the bandwidth
    of global memory seems high, around 160–200 GB/s, it is slow compared to the teraflop
    performance capability that a GPU can deliver. For this reason, data reuse within
    the GPU is essential to achieving high performance.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ■ **GPGPU 运行在与主处理器分离的内存空间中**。除了一些低端设备外，所有的 GPGPU 都有自己的物理内存（例如 RAM），设计用于提供比传统主机内存显著更高的内存带宽。许多当前的
    GPGPU 内存系统提供大约 160–200 GB/s（每秒十亿字节或千兆字节）的传输速度，而传统的主机内存系统则提供 8–20 GB/s。尽管 CUDA
    4.0 提供了统一虚拟地址（UVA），将主机和 GPGPU 内存合并为单一统一的地址空间，但不要忘记，访问另一设备上的内存将需要通过总线进行一些传输，即使在
    GPGPU 之间也是如此。然而，UVA 很重要，因为它使任何设备上运行的软件能够仅通过指针访问另一设备上的数据。■ **CUDA 程序利用 *核函数*（kernels），这些核函数是可以从主机调用并在
    CUDA 设备上执行的子程序**。需要注意的是，核函数不是函数，因为它们不能返回值。在 GPGPU 上表现良好的大多数应用程序在一个或少数几个计算例程中花费大部分时间。将这些例程转换为
    CUDA 核函数是加速应用程序并利用 GPGPU 计算能力的绝佳方法。核函数使用 **__global__** 声明说明，告诉编译器该核函数可以被主处理器调用。■
    **核函数调用是 *异步* 的，这意味着主机只是将一个核函数排队以在 GPGPU 上执行，并不等待它完成，而是继续执行其他工作**。稍后，核函数实际在 GPU
    上运行。由于这种异步调用机制，CUDA 核函数无法返回函数值。为了效率，可以通过排队一些核函数来创建 *管道*，以尽可能使 GPGPU 保持忙碌。此外，需要某种形式的同步机制，以便主机确定核函数或管道何时完成。两种常用的同步机制包括：■
    显式调用 **cudaThreadSynchronize()**，它作为 *屏障*，导致主机停止并等待所有排队的核函数完成。■ 执行带有 **cudaThreadSynchronize()**
    的阻塞数据传输，就像在 **cudaMemcpy()** 中调用 **cudaThreadSynchronize()** 一样。■ **GPU 上的基本工作单位是
    *线程***。从软件角度来看，重要的是要理解每个线程都与其他线程分开。每个线程都像有自己的处理器一样，有单独的寄存器和标识（例如，在共享内存环境中的位置），硬件定义了能够同时运行的线程数。板载
    GPU 硬件线程调度器决定何时可以运行一组线程，并且具有在软件角度看，线程切换和调度似乎是免费的能力。一些简单而优雅的 C 语言补充允许线程通过 CUDA
    共享内存空间和原子内存操作进行通信。一个核函数应该利用许多线程执行核函数源代码中定义的工作。这种利用称为 *线程级并行性*，与 *指令级并行性* 不同，后者在处理器指令上进行并行化。[图
    1.2](#f0015) 说明了在并行填充中使用许多线程，与在 [图 1.1](#f0010) 中示例的单线程填充操作不同。*执行配置* 定义了将运行核函数的线程数以及它们在
    1D、2D 或 3D 计算网格中的排列。执行配置在名称后面的三角括号“<<<”和“>>>”之间的参数列表封闭配置信息。除了执行配置外，排队核函数看起来非常类似于子程序调用。[示例
    1.7](#tb0040) 展示了使用 **fill()** 子程序中的 **fillKernel()** 调用的语法。■ **GPU 上最大的共享内存区域称为
    *全局内存***。以 RAM 的千兆字节计量，大多数应用程序数据驻留在全局内存中。全局内存受到 *合并* 规则的影响，这些规则将多个内存事务合并为单个大型加载或存储操作，以实现与内存之间的最高传输速率。一般来说，当内存访问可以合并为
    128 个连续字节块时，可以获得最佳性能。其他可通过程序员访问的板载 GPU 存储器包括 *常量*、*缓存*、*共享*、*本地*、*纹理* 和 *寄存器内存*，如
    [第 5 章](B9780123884268000057.xhtml#B978-0-12-388426-8.00005-7) 所述。访问全局内存的延迟可能很高，比访问寄存器变量慢高达
    600 倍。CUDA 程序员应注意，尽管全局内存的带宽似乎很高，约为 160–200 GB/s，但与 GPU 可提供的 teraflop 性能能力相比，它的速度较慢。因此，在
    GPU 内重复使用数据对于实现高性能至关重要。
- en: Understanding Our First Runtime Kernel
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解我们的第一个运行时内核
- en: 'You now have the basic concepts needed to understand our first kernel.From
    the programmer''s point of view, execution starts on the host processor in **main()**
    of [Example 1.7](#tb0040). The constant integer **N** is initialized and the Thrust
    method **device_vector<int>** is used to allocate **N** integers on the GPGPU
    device. Execution then proceeds sequentially to the **fill()** subroutine. A simple
    calculation is performed to define the number of blocks, **nBlocks**, based on
    the 512 threads per block defined by **nThreadsPerBlock**. The idea is to provide
    enough threads at a granularity of **nThreadsPerBlock** to dedicate one thread
    to each location in the integer array **d_a**. By convention, device variables
    are frequently noted by a preceding “**d_**” before the variable name and host
    variables are denoted with an “**h_”** before the variable name.The CUDA kernel,
    **fillKernel()**, is then queued for execution on the GPGPU using the **nBlocks**
    and **nThreadsPerBlock** execution configuration. Both **d_a** and **n** are passed
    as parameters to the kernel. Host execution then proceeds to the return from the
    **fill()** subroutine. Note that **fillKernel()** must be preceded by the **__global__**
    declaration specifier or a compilation error will occur.At this point, two things
    are now happening:1\. The CUDA driver is informed that there is work for the device
    in a queue. Within a few microseconds (μsec or millionths of a second), the driver
    loads the executable on the GPGPU, defines the execution grid, passes the parameters,
    and—because the GPGPU does not have any other work to do—starts the kernel.2\.
    Meanwhile, the host processor continues its sequential execution to call **thrust::reduce()**
    to perform task 3\. The host performs the work defined by the Thrust templates
    and queues the GPU operations. Because **reduce()** is a blocking operation, the
    host has to wait for a result from the GPGPU before it can proceed.Once **fillKernel()**
    starts running on the GPU, each thread first calculates its particular thread
    ID (called **tid** in the code) based on the grid defined by the programmer. This
    example uses a simple 1D grid, so **tid** is calculated using three constant variables
    that are specific to each kernel and defined by the programmer via the execution
    configuration:■ **blockIdx.x**: This is the index of the block that the thread
    happens to be part of in the grid specified by the programmer. Because this is
    a 1D grid, only the *x* component is used; the *y* and *z* components are ignored.■
    **blockDim.x**: The dimension or number of threads in each block.■ **threadIdx.x**:
    The index within the block where this thread happens to be located.The value of
    **tid** is then checked to see if it is less than **n**, as there might be more
    threads than elements in the **a** array. If **tid** contains a valid index into
    the **a** array, then the element at index **tid** is set to the value of **tid**.
    If not, the thread waits at the end of the kernel until all the threads complete.
    This example was chosen to make it easy to see how the grid locations are converted
    into array indices.After all the threads finish, **fillKernel()** completes and
    returns control to the device driver so that it can start the next queued task.
    In this example, the GPU computes the reduction (the code of which is surprisingly
    complicated, as discussed in [Chapter 6](B9780123884268000069.xhtml#B978-0-12-388426-8.00006-9)),
    and the sum is returned to the host so that the application can finish sequentially
    processing tasks 4 and 5.From the preceding discussion, we can see that a CUDA
    kernel can be thought of in simple terms as a parallel form of the code snippet
    in [Example 1.9](#tb0050), “A Sequential Illustration of a Parallel CUDA Call”:`//
    setup blockIdx, blockDim, and threadIdx based on the execution``// configuration``for(int
    i=0; i < (nBlocks * nThreadsPerBlock); i++)``fillKernel(d_a, n);`Though Thrust
    relies on the runtime API, careful use of C++ templates allows a C++ *functor*
    (or *function object*, which is a C++ object that can be called as if it were
    a function) to be translated into a runtime kernel. It also defines the execution
    configuration and creates a kernel call for the programmer. This explains why
    [Example 1.5](#tb0030), which was written entirely in Thrust, does not require
    specification of an execution configuration or any of the other details required
    by the runtime API.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经掌握了理解我们第一个内核所需的基本概念。从程序员的角度来看，执行从**main()**中的[示例 1.7](#tb0040)开始。常量整数**N**被初始化，Thrust方法**device_vector<int>**用于在GPGPU设备上分配**N**个整数。执行随后顺序地进入**fill()**子例程。通过一个简单的计算，定义了基于每块512个线程（由**nThreadsPerBlock**定义）来计算块的数量**nBlocks**。其思路是提供足够的线程，以**nThreadsPerBlock**为粒度，确保为整数数组**d_a**中的每个位置分配一个线程。根据约定，设备变量通常会在变量名前加上“**d_**”，而主机变量则会加上“**h_**”。接下来，CUDA内核**fillKernel()**被排队执行，在GPGPU上使用**nBlocks**和**nThreadsPerBlock**的执行配置。**d_a**和**n**作为参数传递给内核。主机执行接着返回**fill()**子例程。注意，**fillKernel()**必须由**__global__**声明修饰符修饰，否则会发生编译错误。此时，两个事情正在发生：1.
    CUDA驱动程序被告知设备有任务排队。几微秒（μ秒，百万分之一秒）内，驱动程序将可执行文件加载到GPGPU上，定义执行网格，传递参数，并且因为GPGPU没有其他工作要做，启动内核。2.
    与此同时，主机处理器继续其顺序执行，调用**thrust::reduce()**来执行任务3。主机执行由Thrust模板定义的工作，并排队GPU操作。因为**reduce()**是一个阻塞操作，主机必须等待GPGPU的结果才能继续执行。一旦**fillKernel()**开始在GPU上运行，每个线程首先根据程序员定义的网格计算其特定的线程ID（在代码中称为**tid**）。此示例使用简单的1D网格，因此**tid**是使用三个常量变量计算的，这些常量变量对于每个内核是特定的，并且通过执行配置由程序员定义：■
    **blockIdx.x**：这是线程所在的块在程序员定义的网格中的索引。由于这是1D网格，因此只使用*x*分量；*y*和*z*分量被忽略。■ **blockDim.x**：每个块的维度或线程数。■
    **threadIdx.x**：该线程在块内的位置索引。然后检查**tid**的值，看看它是否小于**n**，因为可能有比**a**数组元素更多的线程。如果**tid**包含有效的**a**数组索引，则将索引为**tid**的元素值设置为**tid**。如果没有，线程将在内核的末尾等待，直到所有线程完成。选择此示例是为了方便观察网格位置如何转换为数组索引。所有线程完成后，**fillKernel()**完成并将控制权返回给设备驱动程序，以便它可以开始下一个排队任务。在此示例中，GPU计算了归约（其代码相当复杂，如在[第6章](B9780123884268000069.xhtml#B978-0-12-388426-8.00006-9)中讨论），并将结果返回给主机，以便应用程序能够顺序地完成任务4和5。从上述讨论可以看出，一个CUDA内核可以简单地理解为[示例
    1.9](#tb0050)中代码片段的并行形式，“一个并行CUDA调用的顺序示例”：`// 根据执行``// 配置设置blockIdx、blockDim和threadIdx``for(int
    i=0; i < (nBlocks * nThreadsPerBlock); i++)``fillKernel(d_a, n);`虽然Thrust依赖于运行时API，但仔细使用C++模板使得C++
    *函数对象*（或*函数对象*，这是一个可以像函数一样调用的C++对象）能够被转化为运行时内核。它还定义了执行配置并为程序员创建内核调用。这也解释了为什么[示例
    1.5](#tb0030)，完全使用Thrust编写，不需要指定执行配置或运行时API要求的其他细节。
- en: Three Rules of GPGPU Programming
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPGPU 编程的三条规则
- en: Observation has shown that there are three general rules to creating high-performance
    GPGPU programs:1\. Get the data on the GPGPU and keep it there.2\. Give the GPGPU
    enough work to do.3\. Focus on data reuse within the GPGPU to avoid memory bandwidth
    limitations.These rules make sense, given the bandwidth and latency limitations
    of the PCIe bus and GPGPU memory system as discussed in the following subsections.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 观察表明，创建高性能 GPGPU 程序有三个基本规则：1. 将数据放到 GPGPU 上并保持在那里；2. 给 GPGPU 足够的工作量；3. 专注于 GPGPU
    内的数据重用，以避免内存带宽限制。考虑到 PCIe 总线和 GPGPU 内存系统的带宽与延迟限制，这些规则是有道理的，以下小节将进一步讨论这些限制。
- en: 'Rule 1: Get the Data on the GPU and Keep It There'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规则 1：将数据放到 GPU 上并保持在那里
- en: GPGPUs are separate devices that are plugged into the PCI Express bus of the
    host computer. The PCIe bus is very slow compared to GPGPU memory system as can
    be seen by the 20-times difference highlighted in [Table 1.1](#t0010).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GPGPU 是独立的设备，插入主机计算机的 PCI Express 总线。与 GPGPU 内存系统相比，PCIe 总线非常慢，从[表 1.1](#t0010)中的
    20 倍差距可以看出。
- en: '**Table 1.1** PCIe vs. GPU Global Memory Bandwidth'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 1.1** PCIe 与 GPU 全局内存带宽对比'
- en: '|  | Bandwidth (GB/s) | Speedup over PCIe Bus |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  | 带宽 (GB/s) | 相较于 PCIe 总线的加速比 |'
- en: '| PCIe x16 v2.0 bus (one-way) | 8 | 1 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| PCIe x16 v2.0 总线（单向） | 8 | 1 |'
- en: '| GPU global memory | 160 to 200 | 20x to 28x |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| GPU 全局内存 | 160 至 200 | 20 倍至 28 倍 |'
- en: 'Rule 2: Give the GPGPU Enough Work to Do'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规则 2：给 GPGPU 足够的工作量
- en: The adage “watch what you ask for because you might get it” applies to GPGPU
    performance. Because CUDA-enabled GPUs can deliver teraflop performance, they
    are fast enough to complete small problems faster than the host processor can
    start kernels. To get a sense of the numbers, let's assume this overhead is 4
    μsec for a 1 teraflop GPU that takes 4 cycles to perform a floating-point operation.
    To keep this GPGPU busy, each kernel must perform roughly 1 million floating-point
    operations to avoid wasting cycles due to the kernel startup latency. If the kernel
    takes only 2 *μ*sec to complete, then 50 percent of the GPU cycles will be wasted.
    One caveat is that Fermi GPUs can run multiple small kernels at the same time
    on a single GPU.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 俗话说“当心你所要求的，因为你可能会得到它”，这句话适用于 GPGPU 性能。由于支持 CUDA 的 GPU 可以提供 teraFLOP 性能，它们的计算速度足以比主机处理器启动内核的速度更快地完成小型问题。为了更好地理解这些数据，假设对于一款
    1 teraFLOP 的 GPU，它需要 4 个周期来执行一次浮点操作，而启动这一过程的开销是 4 微秒。为了让 GPGPU 保持忙碌，每个内核必须执行大约
    100 万次浮点操作，以避免因内核启动延迟浪费周期。如果内核仅需要 2 微秒来完成，那么 GPU 周期的 50% 将被浪费。需要注意的是，Fermi GPU
    可以在单个 GPU 上同时运行多个小型内核。
- en: 'Rule 3: Focus on Data Reuse within the GPGPU to Avoid Memory Bandwidth Limitations'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规则 3：专注于 GPGPU 内的数据重用，以避免内存带宽限制
- en: All high-performance CUDA applications exploit internal resources on the GPU
    (such as registers, shared memory, and so on, discussed in [Chapter 5](B9780123884268000057.xhtml#B978-0-12-388426-8.00005-7))
    to bypass global memory bottlenecks. For example, multiplying a vector by a scale
    factor in global memory and assigning the result to a second vector also in global
    memory will be slow, as shown in [Example 1.10](#tb0055), “A Simple Vector Example”:`for(i=0;
    i < N; i++) c[i] = a * b[i];`Assuming the vectors require 4 bytes to store a single-precision
    32-bit floating-point value in each element, then the memory subsystem of a teraflop
    capable computer would need to provide at least 8 terabytes per second of memory
    bandwidth to run at peak performance—assuming the constant scale factor gets loaded
    into the GPGPU cache. Roughly speaking, such bandwidth is 40 to 50 times the capability
    of current GPU memory subsystems and around 400 times the bandwidth of a 20-GB/s
    commodity processor. Double-precision vectors that require 8 bytes of storage
    per vector element will double the bandwidth requirement. This example should
    make it clear that CUDA programmers must reuse as much data as possible to achieve
    high performance. Please note that data reuse is also important to attaining high
    performance on conventional processors as well.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所有高性能 CUDA 应用程序都利用 GPU 上的内部资源（例如寄存器、共享内存等，这些内容在[第 5 章](B9780123884268000057.xhtml#B978-0-12-388426-8.00005-7)中讨论）来绕过全局内存瓶颈。例如，在全局内存中将一个向量乘以一个尺度因子，并将结果分配给另一个同样在全局内存中的向量，将会很慢，如[示例
    1.10](#tb0055)中所示，“一个简单的向量示例”：`for(i=0; i < N; i++) c[i] = a * b[i];`假设向量每个元素存储一个单精度
    32 位浮点值需要 4 字节，那么一个具备一万亿次浮点运算能力的计算机的内存子系统需要提供至少每秒 8 TB 的内存带宽才能在峰值性能下运行——假设常量尺度因子已加载到
    GPGPU 缓存中。大致来说，这样的带宽是当前 GPU 内存子系统能力的 40 到 50 倍，是一台 20 GB/s 普通处理器带宽的 400 倍。需要 8
    字节存储的双精度向量将使带宽需求翻倍。这个例子应该能清楚地表明，CUDA 程序员必须尽可能多地重用数据以实现高性能。请注意，数据重用对于在传统处理器上获得高性能也同样重要。
- en: Big-O Considerations and Data Transfers
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Big-O 考虑和数据传输
- en: 'Big-O notation is a convenient way to describe how the size of the problem
    affects the consumption by an algorithm of some resource such as processor time
    or memory as a function of its input. In this way, computer scientists can describe
    the worst case (or, when specified, the average case behavior) as a function to
    compare algorithms regardless of architecture or clock rate. Some common growth
    rates are:■ O(1): These are constant time (or space) algorithms that always consume
    the same resources regardless of the size of the input set. Indexing a single
    element in a Thrust host vector does not vary in time with the size of the data
    set and thus exhibits O(1) runtime growth.■ O(*N*): Resource consumption with
    these algorithms grow linearly with the size of the input. This is a common runtime
    for algorithms that loop over a data set. However, the work inside the loop must
    require constant time.■ O(*N*²): Performance is directly proportional to the square
    of the size of the input data set. Algorithms that use nested loops over an input
    data set exhibit O(*N*²) runtime. Deeper nested iterations commonly show greater
    runtime (e.g., three nested loops result in O(*N*³), O(*N*⁴) when four loops are
    nested, and so on.There are many excellent texts on algorithm analysis that provide
    a more precise and comprehensive description of big-O notation. One popular text
    is *Introduction to Algorithms* (by Cormen, Leiserson, Rivest, and Stein; The
    MIT Press, 2009). There are also numerous resources on the Internet that discuss
    and teach big-O notation and algorithm analysis.Most computationally oriented
    scientists and programmers are familiar with the BLAS (the Basic Linear Algebra
    Subprograms) library. BLAS is the *de facto* programming interface for basic linear
    algebra. NVIDIA provides a GPGPU version of BLAS with their CUBLAS library. In
    fact, GPGPU computing is creating a resurgence of interest in new high-performance
    math libraries, such as the MAGMA project at the University of Tennessee Innovative
    Computing Laboratory ([http://icl.cs.utk.edu/magma/](http://icl.cs.utk.edu/magma/)).
    MAGMA in particular utilizes both the host and a GPU device to attain high performance
    on matrix operations. It is available for free download.BLAS is structured according
    to three different levels with increasing data and runtime requirements:■ Level-1:
    Vector-vector operations that require O(*N*) data and O(*N*) work. Examples include
    taking the inner product of two vectors or scaling a vector by a constant multiplier.■
    Level-2: Matrix-vector operations that require O(*N*²) data and O(*N*²) work.
    Examples include matrix-vector multiplication or a single right-hand-side triangular
    solve.■ Level-3 Matrix-vector operations that require O(*N*²) data and O(*N*³)
    work. Examples include dense matrix-matrix multiplication.[Table 1.2](#t0015)
    illustrates the amount of work that is performed by each BLAS level, assuming
    that *N* floating-point values are transferred from the host to the GPU. It does
    not take into account the time required to transfer the data back to the GPU.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '大O表示法是一种便捷的方式，用来描述问题的规模如何影响算法对某些资源（如处理器时间或内存）的消耗，作为输入的函数。通过这种方式，计算机科学家可以描述最坏情况（或在指定时，平均情况的行为），作为一个函数来比较算法，而不管架构或时钟频率。常见的增长率包括：  '
- en: '**Table 1.2** Work per Datum by BLAS Level'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 1.2** 按 BLAS 级别的每个数据的工作量'
- en: '| BLAS Level | Data | Work | Work per Datum |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| BLAS 级别 | 数据 | 工作量 | 每个数据的工作量 |'
- en: '| 1 | O(*N*) | O(*N*) | O(1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 1 | O(*N*) | O(*N*) | O(1) |'
- en: '| 2 | O(*N*²) | O(*N*²) | O(1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 2 | O(*N*²) | O(*N*²) | O(1) |'
- en: '| 3 | O(*N*²) | O(*N*³) | O(*N*) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 3 | O(*N*²) | O(*N*³) | O(*N*) |'
- en: '[Table 1.2](#t0015) tells us that level-3 BLAS operations should run efficiently
    on graphics processors because they perform O(*N*) work for every floating-point
    value transferred to the GPU. The same work-per-datum analysis applies to non-BLAS-related
    computational problems as well.To illustrate the cost of a level-1 BLAS operation,
    consider the overhead involved in moving data across the PCIe bus to calculate
    **cublasScal()** on the GPU and then return the vector to the host. The BLAS **Sscal()**
    method scales a vector by a constant value. CUBLAS includes a *thunking* interface
    for FORTRAN compatibility. It works by transferring data from the host to the
    GPU, performing the calculation, and then transferring the data back to the host
    from the GPU. Thunking is inefficient, as it requires moving 4*N* bytes of data
    (where *N* is the number of floats in the vector) twice across the PCIe bus to
    perform *N* multiplications, as shown in [Example 1.10](#tb0055). The best possible
    performance would be the transfer bandwidth divided by 8 (to account for two transfers
    of 4*N* bytes of data each way), as the time to perform the multiplication would
    be tiny compared to the data transfers. Such an application might achieve 1 Gflop
    (1 billion floating-point operations per second) floating-point performance assuming
    an 8-GB/s transfer rate between the host and GPU. [¹](#fn0010) Modest laptop processors
    and even some cell phones can exceed this calculation rate.¹Asynchronous data
    transfers can improve performance because the PCIe bus is full duplex, meaning
    that data can be transferred both to and from the host at the same time. At best,
    full-duplex asynchronous PCIe transfers would double the performance to two Gflop.This
    analysis applies to all programs, not just level-1 and level-2 BLAS calculations.
    Getting good performance requires keeping as much data as possible on the GPU.
    After that, attaining high performance requires performing as many calculations
    per datum like the level-3 BLAS operations. Creating a pipeline of many lower
    arithmetic density computations can help, but this will only increase performance
    when each operation can keep the GPU busy long enough to overcome the kernel startup
    latency. Alternatively, it is possible to increase performance—sometime significantly—by
    combining multiple low-density operations like BLAS level-2 and level-2 operation
    into a single functor or kernel.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 1.2](#t0015) 告诉我们，三级 BLAS 操作应该能在图形处理器上高效运行，因为它们每传输一个浮点值到 GPU 时，会执行 O(*N*)
    的工作量。相同的每个数据的工作量分析也适用于非 BLAS 相关的计算问题。为了说明一级 BLAS 操作的成本，考虑在 PCIe 总线上传输数据以计算 GPU
    上的 **cublasScal()**，然后将向量返回给主机的开销。BLAS **Sscal()** 方法通过常数值缩放一个向量。CUBLAS 包括一个*Thunking*
    接口，确保与 FORTRAN 兼容。它的工作方式是将数据从主机传输到 GPU，执行计算后再将数据从 GPU 传回主机。Thunking 是低效的，因为它需要将
    4*N* 字节的数据（其中 *N* 是向量中的浮点数数量）通过 PCIe 总线来回传输两次，以执行 *N* 次乘法，如 [示例 1.10](#tb0055)
    所示。最佳性能将是传输带宽除以 8（考虑到两次传输 4*N* 字节的数据），因为与数据传输相比，执行乘法所需的时间极短。假设主机和 GPU 之间的传输速率为
    8GB/s，应用程序可能达到 1 Gflop（每秒 10 亿次浮点运算）的浮点性能。[¹](#fn0010) 一些普通的笔记本处理器甚至某些手机的计算速率都能超过这一水平。¹异步数据传输可以提高性能，因为
    PCIe 总线是全双工的，意味着数据可以同时传输到主机和从主机传回。最好情况下，全双工异步 PCIe 传输将性能提高到两倍，达到 2 Gflop。这个分析适用于所有程序，而不仅仅是一级和二级
    BLAS 计算。要获得良好的性能，需要尽可能将数据保留在 GPU 上。之后，获得高性能需要像三级 BLAS 操作一样对每个数据执行尽可能多的计算。创建多个低算术密度计算的管道有助于提高性能，但只有当每个操作能保持
    GPU 足够长的时间以克服内核启动延迟时，这种方法才能增加性能。另一种方法是通过将多个低密度操作（如 BLAS 二级和二级操作）合并为一个函数或内核来提高性能，有时这种提高是显著的。'
- en: CUDA and Amdahl's Law
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA 和阿姆达尔定律
- en: Amdahl's law is named after computer architect Gene Amdahl. It is not really
    a law but rather an approximation that models the ideal speedup that can happen
    when serial programs are modified to run in parallel. For this approximation to
    be valid, it is necessary for the problem size to remain the same when parallelized.
    In other words, assume that a serial program is modified to run in parallel. Further,
    assume that the amount of work performed by the program does not change significantly
    in the parallel version of the code, which is not always true. Obviously, those
    portions of the application that do not parallelize will not run any faster, and
    the parallel sections can run much, much faster depending on the hardware capabilities.
    Thus, the expected speedup of the parallel code over the serial code when using
    *n* processors is dictated by the proportion of a program that can be made parallel,
    *P*, and the portion of that cannot be parallelized, (1 − *P*). This relationship
    is shown in [Equation 1.1](#fm0010), “Amdahl's law”.(1.1)![B978012388426800001X/si1.gif
    is missing](B978012388426800001X/si1.gif)Amdahl's law tells us that inventive
    CUDA developers have two concerns in parallelizing an application:1\. Express
    the parallel sections of code so that they run as fast as possible. Ideally, they
    should run *N* times faster when using *N* processors.2\. Utilize whatever techniques
    or inventiveness they have to minimize the (1 − *P*) serial time.Part of the beauty
    of CUDA lies in the natural way it encapsulates the parallelism of a program inside
    computational kernels. Applications that run well on GPGPU hardware tend to have
    a few computationally intensive sections of the code (e.g., hotspots) that consume
    most of the runtime. According to Amdahl's law, these are programs with kernels
    that can deliver good parallel speedups (*P* >> 1 − *P*).Observation has also
    shown that refactoring an application to use CUDA also tends to speed up performance
    on moderately parallel hardware such as multicore processors because CUDA allows
    developers to better see parallelism so that they can restructure the code to
    reduce the time spent in the serial (1 − *P*) sections. Of course, this assumes
    observation that the overhead consumed in transferring data to and from the GPGPU
    does not significantly affect application runtime.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 安达尔定律（Amdahl's law）得名于计算机架构师吉恩·安达尔（Gene Amdahl）。它实际上不是一个定律，而是一个近似模型，用于描述将串行程序修改为并行运行时可能实现的理想加速效果。为了使这个近似模型有效，问题规模在并行化时必须保持不变。换句话说，假设一个串行程序被修改为并行程序。此外，还假设在并行版本的代码中，程序所执行的工作量没有显著变化，但这并不总是成立的。显然，那些无法并行化的部分将不会变得更快，而并行部分可以根据硬件能力运行得更快。因此，使用*n*个处理器时，预计并行代码相对于串行代码的加速比由程序中可以并行化的部分*P*和不能并行化的部分（1 − *P*）的比例决定。这个关系在[方程式
    1.1](#fm0010)中展示，“安达尔定律”。(1.1)![B978012388426800001X/si1.gif is missing](B978012388426800001X/si1.gif)安达尔定律告诉我们，创新的CUDA开发者在并行化应用程序时有两个关注点：1.
    将代码的并行部分表达得尽可能快。理想情况下，当使用*N*个处理器时，应该实现*N*倍的速度提升。2. 运用任何技术或创造力，最小化（1 − *P*）串行时间。CUDA的一个重要优势就在于它以自然的方式将程序的并行性封装在计算内核中。那些在GPGPU硬件上运行良好的应用程序通常有一些计算密集型的代码部分（例如热点），这些部分占用了大部分的运行时间。根据安达尔定律，这些是具有能够提供良好并行加速的内核的程序（*P* >> 1 − *P*）。观察也表明，将应用程序重构为使用CUDA通常会在中等并行硬件（如多核处理器）上提高性能，因为CUDA让开发者能更好地识别并行性，从而可以重构代码，减少在串行部分（1 − *P*）上花费的时间。当然，这假设了将数据传输到GPGPU并从中传输的开销不会显著影响应用程序的运行时间。
- en: Data and Task Parallelism
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据与任务并行性
- en: The examples in this chapter have thus far demonstrated *data parallelism* or
    *loop-level parallelism* that parallelized data operations inside the **for**
    loops. *Task parallelism* is another form of parallelization that reduces the
    (1 − *P*) serial time by having multiple tasks executing concurrently. CUDA naturally
    supports task parallelism by running concurrent tasks on the host and GPU. As
    will be discussed in [Chapter 7](B9780123884268000070.xhtml#B978-0-12-388426-8.00007-0),
    CUDA provides other ways to exploit task parallelism within a GPU or across multiple
    GPUs.Even the simple GPU examples, like [Example 1.5](#tb0030) and [Example 1.7](#tb0040),
    can be modified to demonstrate task parallelism. These examples performed the
    following tasks:1\. Create an array.2\. Fill the array.3\. Calculate the sum of
    the array.4\. Calculate the sum of 0 ‥ **N**−1.5\. Check that the host and device
    results agree.The representative timeline in [Figure 1.4](#f0025) shows that the
    previous examples do not take advantage of the asynchronous kernel execution when
    task 2 is queued to run on the GPU.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例到目前为止展示了*数据并行性*或*循环级并行性*，它们通过在**for**循环内并行化数据操作。*任务并行性*是另一种并行化形式，它通过让多个任务同时执行来减少
    (1 − *P*) 的串行时间。CUDA 自然支持任务并行性，通过在主机和 GPU 上并行运行任务。正如在[第 7 章](B9780123884268000070.xhtml#B978-0-12-388426-8.00007-0)中将讨论的那样，CUDA
    提供了其他方式来在单个 GPU 或多个 GPU 之间利用任务并行性。即使是像[示例 1.5](#tb0030)和[示例 1.7](#tb0040)这样的简单
    GPU 示例，也可以修改为展示任务并行性。这些示例执行了以下任务：1\. 创建一个数组。2\. 填充数组。3\. 计算数组的总和。4\. 计算从 0 到 **N**−1
    的总和。5\. 检查主机和设备的结果是否一致。[图 1.4](#f0025) 中的代表性时间线显示，之前的示例并没有在任务 2 被排队到 GPU 上运行时利用异步内核执行。
- en: '| ![B978012388426800001X/f01-04-9780123884268.jpg is missing](B978012388426800001X/f01-04-9780123884268.jpg)
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ![B978012388426800001X/f01-04-9780123884268.jpg 缺失](B978012388426800001X/f01-04-9780123884268.jpg)
    |'
- en: '| **Figure 1.4**Sequential timeline. |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **图 1.4** 顺序时间线。 |'
- en: However, there is no reason why task 4 cannot be started after task 2 queues
    the kernel to run on the GPU. In other words, there is no *dependency* that forces
    task 4 to run after any other task, except that it must run before the check for
    correctness in task 5\. In other words, task 5 depends on the results from task
    4\. Similarly, task 2 must run sometime after task 1, which allocates the array
    on the GPU.Switching tasks 3 and 4 allows both the host and GPU to run in parallel,
    as shown in [Figure 1.5](#f0030), which demonstrates that it is possible to leverage
    CUDA asynchronous kernel execution to exploit both task and data parallelism in
    even this simple example. The benefit is a further reduction in application runtime.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，任务 4 完全可以在任务 2 将内核排队到 GPU 上运行后开始。换句话说，任务 4 并没有任何必须在其他任务之后执行的*依赖性*，唯一的要求是它必须在任务
    5 中进行正确性检查之前运行。换句话说，任务 5 依赖于任务 4 的结果。同样，任务 2 必须在任务 1 之后执行，任务 1 在 GPU 上分配数组。交换任务
    3 和任务 4 使得主机和 GPU 可以并行运行，如[图 1.5](#f0030)所示，这表明即便是这个简单的例子，也可以通过利用 CUDA 异步内核执行来实现任务和数据并行。这样做的好处是进一步减少了应用程序的运行时间。
- en: '| ![B978012388426800001X/f01-05-9780123884268.jpg is missing](B978012388426800001X/f01-05-9780123884268.jpg)
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ![B978012388426800001X/f01-05-9780123884268.jpg 缺失](B978012388426800001X/f01-05-9780123884268.jpg)
    |'
- en: '| **Figure 1.5**Asynchronous timeline. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **图 1.5** 异步时间线。 |'
- en: 'Hybrid Execution: Using Both CPU and GPU Resources'
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合执行：同时使用 CPU 和 GPU 资源
- en: 'The following example demonstrates a hybrid application that application runs
    simultaneously on both the CPU and GPU. Modern multicore processors are also parallel
    hardware that supports both data parallel and task parallel execution. OpenMP
    (Open Multi-Processing) is an easy way to utilize multithreaded execution on the
    host processor. The NVIDIA **nvcc** compiler supports OpenMP. Though this book
    is not about the OpenMP API, CUDA programmers need to know that they can exploit
    this host processor parallelism as well as the massive parallelism of the GPGPU.
    After all, the goal is to deliver the fastest application performance. It is also
    important to be fair when making benchmark comparisons between CPUs and GPUs by
    optimizing the application to achieve the highest performance on both systems
    to make the comparison as fair as possible.[Example 1.11](#tb0060), “An Asynchronous
    CPU/GPU Source Code” is the source code that switches the order of execution between
    task 3 and 4\. The OpenMP parallel for loop pragma was used in task 4 to exploit
    data parallelism on the host processor:`//seqAsync.cu``#include <iostream>``using
    namespace std;``#include <thrust/reduce.h>``#include <thrust/sequence.h>``#include
    <thrust/host_vector.h>``#include <thrust/device_vector.h>``int main()``{``const
    int N=50000;``// task 1: create the array``thrust::device_vector<int> a(N);``//
    task 2: fill the array``thrust::sequence(a.begin(), a.end(), 0);``// task 4: calculate
    the sum of 0 .. N−1``int sumCheck=0;``#pragma omp parallel for reduction(+ : sumCheck)``for(int
    i=0; i < N; i++) sumCheck += i;``// task 3: calculate the sum of the array``int
    sumA= thrust::reduce(a.begin(),a.end(), 0);``// task 5: check the results agree``if(sumA
    == sumCheck) cout << "Test Succeeded!" << endl;``else { cerr << "Test FAILED!"
    << endl; return(1);}``return(0);``}`To compile with OpenMP, add “**-Xcompiler
    -fopenmp**” to the **nvcc** command-line argument as shown in [Example 1.12](#tb0065)
    to compile [Example 1.11](#tb0060). Because we are timing the results, the “**−O3**”
    optimization flag was also utilized.`nvcc –O3 –Xcompiler –fopenmp seqAsync.cu
    - seqAsync`The output from `**seqAsync**` ([Example 1.13](#tb0070), “Results Showing
    a Successful Test”) shows that the sum is correctly calculated as it passes the
    golden test:`$ ./a.out``Test Succeeded!`'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '以下示例展示了一个混合应用程序，该应用程序同时在 CPU 和 GPU 上运行。现代多核处理器也是支持数据并行和任务并行执行的并行硬件。OpenMP（开放多处理）是一种便捷的方式，能够在主机处理器上利用多线程执行。NVIDIA的**nvcc**编译器支持OpenMP。尽管本书并不是关于OpenMP
    API的内容，但CUDA程序员需要知道，他们可以利用主机处理器的并行性以及GPGPU的巨大并行性。毕竟，目标是提供最快的应用程序性能。在进行CPU和GPU的基准测试比较时，优化应用程序以在两个系统上实现最高性能，从而使比较尽可能公平，也是非常重要的。[示例1.11](#tb0060)“异步CPU/GPU源代码”是切换任务3和任务4执行顺序的源代码。任务4中使用了OpenMP并行for循环指令，以便在主机处理器上利用数据并行：`//seqAsync.cu``#include
    <iostream>``using namespace std;``#include <thrust/reduce.h>``#include <thrust/sequence.h>``#include
    <thrust/host_vector.h>``#include <thrust/device_vector.h>``int main()``{``const
    int N=50000;``// 任务1：创建数组``thrust::device_vector<int> a(N);``// 任务2：填充数组``thrust::sequence(a.begin(),
    a.end(), 0);``// 任务4：计算0..N−1的和``int sumCheck=0;``#pragma omp parallel for reduction(+
    : sumCheck)``for(int i=0; i < N; i++) sumCheck += i;``// 任务3：计算数组的和``int sumA=
    thrust::reduce(a.begin(),a.end(), 0);``// 任务5：检查结果是否一致``if(sumA == sumCheck) cout
    << "Test Succeeded!" << endl;``else { cerr << "Test FAILED!" << endl; return(1);}``return(0);``}`为了使用OpenMP进行编译，请在**nvcc**命令行参数中添加“**-Xcompiler
    -fopenmp**”，如[示例1.12](#tb0065)所示，用于编译[示例1.11](#tb0060)。由于我们需要计时结果，因此还使用了“**−O3**”优化标志。`nvcc
    –O3 –Xcompiler –fopenmp seqAsync.cu - seqAsync`从`**seqAsync**`的输出（[示例1.13](#tb0070)，“显示成功测试的结果”）中可以看到，和被正确计算，且通过了黄金测试：`$
    ./a.out``Test Succeeded!`'
- en: Regression Testing and Accuracy
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归测试与准确性
- en: As always in programming, the most important metric for success is that the
    application produces the correct output. Regression testing is a critical part
    of this evaluation process that is unfortunately left out of most computer science
    books. Regression tests act as sanity checks that allow the programmer to use
    the computer to detect if some error has been introduced into the code. The computer
    is impartial and does not get tired. For these reasons, small amounts of code
    should be written and tested to ensure everything is in a known working state.
    Additional code can then be added along with more regression tests. This greatly
    simplifies debugging efforts as the errors generally occur in the smaller amounts
    of new code. The alternative is to write a large amount of code and hope that
    it will work. If there is an error, it is challenging to trace through many lines
    of code to find one or more problems. Sometimes you can get lucky with the latter
    approach. If you try it, track your time to see if there really was a savings
    over taking the time to use regression testing and incremental development.The
    examples in this chapter use a simple form of regression test, sometimes called
    a *golden test* or *smoke test*, which utilizes an alternative method to double-check
    the parallel calculation of the sum of the integers. Such simple tests work well
    for exact calculations, but become more challenging to interpret when using floating-point
    arithmetic due to the numerical errors introduced into a calculation when using
    floating-point. As GPGPU technology can perform several *trillion* floating-point
    operations per second, numerical errors can build up quickly. Still, using a computer
    to double-check the results beats staring at endless lines of numbers. For complex
    programs and algorithms, the regression test suite can take as much time to plan,
    be as complex to implement, and require more lines of code than the application
    code itself!While hard to justify to management, professors or to you (especially
    when working against a tight deadline), consider the cost of delivering an application
    that ***does not*** perform correctly. For example, the second startup I co-founded
    delivered an artificial learning system that facilitated the search for candidate
    drug leads using some of the technology discussed later in this book. The most
    difficult question our startup team had to answer was posed by the research executive
    committee of a major investor. That question was “how do we know that what you
    are doing on the computer reflects what really happens in the test tube?” The
    implied question was, “how do we know that you aren't playing some expensive computer
    game with our money?” The solution was to perform a double-blind test showing
    that our technology could predict the chemical activity of relevant compounds
    in real-world experiments using only on a very small set of measurements and no
    knowledge of the correct result. In other words, our team utilized a form of sanity
    checking to demonstrate the efficacy of our model and software. Without such a
    reality check, it likely that the company would not have received funding.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 和编程中的一贯做法一样，成功的最重要衡量标准是应用程序能够产生正确的输出。回归测试是这个评估过程中的关键部分，但不幸的是，大多数计算机科学书籍都没有涉及。回归测试充当了理智检查的作用，使程序员能够利用计算机检测代码中是否引入了某些错误。计算机是公正的，不会感到疲倦。因此，应该编写和测试少量代码，以确保一切处于已知的工作状态。然后可以添加更多的代码，并进行更多的回归测试。这大大简化了调试工作，因为错误通常发生在较少的新代码中。另一种方法是编写大量代码，并希望它能够正常工作。如果出现错误，追踪大量代码行以找到一个或多个问题将变得非常具有挑战性。有时，你可能会运气好，采用后一种方法。如果你尝试这种方法，可以记录下你的时间，看看是否真能节省时间，相比使用回归测试和增量开发。
    本章中的示例使用了一种简单的回归测试形式，有时称为*金标准测试*或*冒烟测试*，它利用一种替代方法来双重检查整数总和的并行计算。这种简单的测试在进行精确计算时效果很好，但在使用浮点运算时，由于浮点数计算中引入的数值误差，解读会变得更加困难。由于GPGPU技术每秒可以执行数万亿次浮点运算，数值误差会迅速积累。不过，使用计算机进行双重检查结果总比盯着无休止的数字行要好。对于复杂的程序和算法，回归测试套件的规划时间、实现复杂度以及所需的代码行数可能与应用程序代码本身一样多！尽管很难向管理层、教授或你自己（尤其是在紧迫的截止日期前）证明，但考虑到交付一个***不正确***运行的应用程序的成本。例如，我共同创办的第二家公司交付了一个人工学习系统，通过使用本书稍后讨论的某些技术，促进了候选药物的筛选。我们创业团队需要回答的最难的问题是一个主要投资者的研究执行委员会提出的。这个问题是：“我们怎么知道你们在计算机上做的事情与实验试管中的实际情况相符？”隐含的问题是：“我们怎么知道你们没有拿我们的钱在玩一些昂贵的电脑游戏？”解决方案是进行一次双盲测试，证明我们的技术可以在现实世界的实验中，仅使用一小部分测量数据，并且没有正确结果的知识，预测相关化合物的化学活性。换句话说，我们的团队利用了一种理智检查的方法来证明我们的模型和软件的有效性。如果没有这样的现实检查，可能公司就无法获得资金。
- en: Silent Errors
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 悄然错误
- en: 'A challenge with regression testing is that some errors can still silently
    pass through the test suite and make it to the application that the end-user sees.
    For example, Andy Regan at ICHEC (the Irish Center for High-End Computing) pointed
    out that increasing the value of **N** in the examples in this chapter can cause
    the sum to become so large that it cannot be contained in an integer. Some systems
    might catch the error and some systems will miss the error.Given that GPUs can
    perform a trillion arithmetic operations per second, it is important that CUDA
    programmers understand how quickly numerical errors can accumulate or computed
    values overflow their capacity. A quick introduction to this topic is my *Scientific
    Computing* article “Numerical Precision: How Much Is Enough?” ([Farber, 2009](B978012388426800015X.xhtml#ref42)),
    which is freely available on the Internet.Thrust provides the ability to perform
    a reduction where smaller data types are converted into a larger data type, *seqBig.cu*,
    demonstrates how to use a 64-bit unsigned integer to sum all the 32-bit integers
    in the vector. The actual values are printed at the end of the test. See [Example
    1.14](#tb0075), “A Thrust Program that Performs a Dual-Precision Reduction”:`//seqBig.cu``#include
    <iostream>``using namespace std;``#include <thrust/reduce.h>``#include <thrust/sequence.h>``#include
    <thrust/host_vector.h>``#include <thrust/device_vector.h>``int main()``{``const
    int N=1000000;``// task 1: create the array``thrust::device_vector<int> a(N);``//
    task 2: fill the array``thrust::sequence(a.begin(), a.end(), 0);``// task 3: calculate
    the sum of the array``unsigned long long sumA= thrust::reduce(a.begin(),a.end(),``(unsigned
    long long) 0, thrust::plus<unsigned long long>() );``// task 4: calculate the
    sum of 0 .. N−1``unsigned long long sumCheck=0;``for(int i=0; i < N; i++) sumCheck
    += i;``cerr << "host" << sumCheck << endl;``cerr << "device " << sumA << endl;``//
    task 5: check the results agree``if(sumA == sumCheck) cout << "Test Succeeded!"
    << endl;``else { cerr << "Test FAILED!" << endl; return(1);}``return(0);``}`The
    **nvcc** compiler has the ability to compile and run an application from a single
    command-line invocation. [Example 1.15](#tb0080), “Results Showing a Successful
    Test” shows the **nvcc** command line and the successful run of the *seqBig.cu*
    application:`$ nvcc seqBig.cu -run``host499999500000``device 499999500000``Test
    Succeeded!`Can you find any other silent errors that might occur as the size of
    **N** increases in these examples? (Hint: what limitations does **tid** have in
    the runtime example?)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 回归测试的一个挑战是，某些错误可能依然会悄无声息地通过测试套件，最终影响到终端用户看到的应用程序。例如，ICHEC（爱尔兰高性能计算中心）的Andy Regan指出，增加本章示例中的**N**值可能会导致总和变得过大，超出整数范围。一些系统可能会捕获该错误，而有些系统则会漏掉该错误。鉴于GPU每秒可以执行万亿次算术运算，CUDA程序员理解数值错误如何快速积累或计算值如何超出其容量变得尤为重要。有关这一主题的快速介绍，请参阅我的*科学计算*文章《数值精度：多少才够？》（[Farber,
    2009](B978012388426800015X.xhtml#ref42)），该文章可以在互联网上免费获取。Thrust提供了执行归约操作的能力，其中较小的数据类型会转换为较大的数据类型，*seqBig.cu*展示了如何使用64位无符号整数对向量中的所有32位整数进行求和。实际值将在测试结束时打印出来。请参见[示例1.14](#tb0075)，“执行双精度归约的Thrust程序”：`//seqBig.cu``#include
    <iostream>``using namespace std;``#include <thrust/reduce.h>``#include <thrust/sequence.h>``#include
    <thrust/host_vector.h>``#include <thrust/device_vector.h>``int main()``{``const
    int N=1000000;``// 任务1：创建数组``thrust::device_vector<int> a(N);``// 任务2：填充数组``thrust::sequence(a.begin(),
    a.end(), 0);``// 任务3：计算数组的和``unsigned long long sumA= thrust::reduce(a.begin(),a.end(),``(unsigned
    long long) 0, thrust::plus<unsigned long long>() );``// 任务4：计算0..N-1的和``unsigned
    long long sumCheck=0;``for(int i=0; i < N; i++) sumCheck += i;``cerr << "host"
    << sumCheck << endl;``cerr << "device " << sumA << endl;``// 任务5：检查结果是否一致``if(sumA
    == sumCheck) cout << "测试成功！" << endl;``else { cerr << "测试失败！" << endl; return(1);}``return(0);``}`**nvcc**编译器具有通过单个命令行调用编译并运行应用程序的能力。[示例1.15](#tb0080)，“显示成功测试的结果”展示了**nvcc**命令行和*seqBig.cu*应用程序的成功运行：`$
    nvcc seqBig.cu -run``host499999500000``device 499999500000``测试成功！`随着**N**的增大，你能发现其他可能发生的悄然错误吗？（提示：在运行时示例中，**tid**有什么限制？）
- en: Introduction to Debugging
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调试简介
- en: Debugging programs is a fact of life—especially when learning a new language.
    With a few simple additions, CUDA enables GPGPU computing, but simplicity of expression
    does not preclude programming errors. As when developing applications for any
    computer, finding bugs can be complicated.Following the same economy of change
    used to adapt C and C++, NVIDIA has extended several popular debugging tools to
    support GPU computing. These are tools that most Windows and UNIX developers are
    already proficient and comfortable using such as **gdb**, **ddd**, and Visual
    Studio. Those familiar with building, debugging, and profiling software under
    Windows, Mac, and UNIX should find the transition to CUDA straightforward. For
    the most part, NVIDIA has made debugging CUDA code identical to debugging any
    other C or C++ application. All CUDA tools are freely available on the NVIDIA
    website, including the professional edition of Parallel Nsight for Microsoft Visual
    Studio.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 调试程序是生活中的一部分——尤其是在学习一门新语言时。通过一些简单的补充，CUDA 实现了 GPGPU 计算，但表达的简洁性并不意味着没有编程错误。就像开发任何计算机应用程序时一样，找到
    bug 可能会很复杂。遵循与 C 和 C++ 适配相同的变化经济性，NVIDIA 扩展了几种流行的调试工具来支持 GPU 计算。这些工具是大多数 Windows
    和 UNIX 开发人员已经熟练并且使用自如的工具，如 **gdb**、**ddd** 和 Visual Studio。那些熟悉在 Windows、Mac 和
    UNIX 上构建、调试和分析软件的开发人员应该会发现过渡到 CUDA 是直接的。在大多数情况下，NVIDIA 已经将调试 CUDA 代码与调试任何其他 C
    或 C++ 应用程序保持一致。所有 CUDA 工具都可以在 NVIDIA 网站上免费获取，包括适用于 Microsoft Visual Studio 的 Parallel
    Nsight 专业版。
- en: UNIX Debugging
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: UNIX 调试
- en: NVIDIA's cuda-gdb Debugger
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NVIDIA 的 cuda-gdb 调试器
- en: 'To use the CUDA debugger, **cuda-gdb**, the source code must be compiled with
    the **-g -G** flags. The **-g** flag specifies that the host-side code is to be
    compiled for debugging. The **-G** flag specifies that the GPU side code is to
    be compiled for debugging (see [Example 1.16](#tb0085), “nvcc Command to Compile
    for cuda-gdb”):`nvcc −g −G seqRuntime.cu −o seqRuntime`Following is a list of
    some common commands, including the single-letter abbreviation and a brief description:■
    breakpoint (b): set a breakpoint to stop the program execution at selected locations
    in the code. The argument can either be a method name or line number.■ run (r):
    run the application within the debugger.■ next (n): move to the next line of code.■
    continue (c): continue to the next breakpoint or until the program completes.■
    backtrace (bt): shows contents of the stack including calling methods.■ thread:
    lists the current CPU thread.■ cuda thread: lists the current active GPU threads
    (if any).■ cuda kernel: lists the currently active GPU kernels and also allows
    switching “focus” to a given GPU thread.Use **cuda-gdb** to start the debugger,
    as shown in [Example 1.17](#tb0090), “cuda-gdb Startup”:`$ cuda-gdb seqRuntime``NVIDIA
    (R) CUDA Debugger``4.0 release``Portions Copyright (C) 2007-2011 NVIDIA Corporation``GNU
    gdb 6.6``Copyright (C) 2006 Free Software Foundation, Inc.``GDB is free software,
    covered by the GNU General Public License, and you are welcome to change it and/or
    distribute copies of it under certain conditions.``Type "show copying" to see
    the conditions.``There is absolutely no warranty for GDB. Type "show warranty"
    for details.``This GDB was configured as "x86_64-unknown-linux-gnu"…``Using host
    libthread_db library "/lib/libthread_db.so.1".`Use the “l” command to show the
    source for **fill**, as shown in [Example 1.18](#tb0095), “cuda-gdb List Source
    Code”:`(cuda-gdb) l fill``11int tid = blockIdx.x*blockDim.x + threadIdx.x;``12if
    (tid < n) a[tid] = tid;``13}``14``15void fill(int* d_a, int n)``16{``17int nThreadsPerBlock=
    512;``18int nBlocks= n/nThreadsPerBlock + ((n%nThreadsPerBlock)?1:0);``19``20fillKernel
    <<< nBlocks, nThreadsPerBlock >>> (d_a, n);`Set a breakpoint at line 12 and run
    the program. We see that a thrust kernel runs and then the breakpoint is hit in
    **fillkernel()**, as shown in [Example 1.19](#tb0100), “cuda-gdb Set a Breakpoint”:`(cuda-gdb)
    b 12``Breakpoint 1 at 0x401e30: file seqRuntime.cu, line 12.``(cuda-gdb) r``Starting
    program: /home/rmfarber/foo/ex1-3``[Thread debugging using libthread_db enabled]``[New
    process 3107]``[New Thread 139660862195488 (LWP 3107)]``[Context Create of context
    0x1ed03a0 on Device 0]``Breakpoint 1 at 0x20848a8: file seqRuntime.cu, line 12.``[Launch
    of CUDA Kernel 0 (thrust::detail::device::cuda::detail::launch_closure_by_value<thrust::detail::device::cuda::for_each_n_closure<thrust::device_ptr<unsigned
    long long>, unsigned int, thrust::detail::generate_functor<thrust::detail::fill_functor<unsigned
    long long> > > ><<<(28,1,1),(768,1,1)>>>) on Device 0]``[Launch of CUDA Kernel
    1 (fillKernel<<<(1954,1,1),(512,1,1)>>>) on Device 0]``[Switching focus to CUDA
    kernel 1, grid 2, block (0,0,0), thread (0,0,0), device 0, sm 0, warp 0, lane
    0]``Breakpoint 1, fillKernel<<<(1954,1,1),(512,1,1)>>> (a=0x200100000, n=50000)``at
    seqRuntime.cu:12``12if (tid < n) a[tid] = tid;`Print the value of the thread Id,
    **tid**, as in [Example 1.20](#tb0105), “cuda-gdb Print a Variable”:`(cuda-gdb)
    p tid``$1 = 0`Switch to thread 403 and print the value of **tid** again. The value
    of **tid** is correct, as shown in [Example 1.21](#tb0110), “cuda-gdb Change Thread
    and Print a Variable”:`(cuda-gdb) cuda thread(403)``[Switching focus to CUDA kernel
    1, grid 2, block (0,0,0), thread (403,0,0), device 0, sm 0, warp 12, lane 19]``12if
    (tid < n) a[tid] = tid;``(cuda-gdb) p tid``$2 = 403`Exit cuda-gdb ([Example 1.22](#tb0115),
    “cuda-gdb Exit”):`(cuda-gdb) quit``The program is running.Exit anyway? (y or n)
    y`This demonstrated only a minimal set of **cuda-gdb** capabilities. Much more
    information can be found in the manual “CUDA-GDB NVIDIA CUDA Debugger for Linux
    and Mac,” which is updated and provided with each release of the CUDA tools. Parts
    14 and 17 of my *Doctor Dobb''s Journal* tutorials discuss **cuda-gdb** techniques
    and capabilities in much greater depth. Also, any good **gdb** or **ddd** tutorial
    will also help novice readers learn the details of **cuda-gdb**.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '要使用 CUDA 调试器 **cuda-gdb**，源代码必须使用 **-g -G** 标志进行编译。**-g** 标志指定主机端代码进行调试编译。**-G**
    标志指定 GPU 端代码进行调试编译（参见 [示例 1.16](#tb0085)， “用于 cuda-gdb 编译的 nvcc 命令”）：`nvcc −g
    −G seqRuntime.cu −o seqRuntime` 以下是一些常见命令的列表，包括单字母缩写和简要描述：  '
- en: The CUDA Memory Checker
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUDA内存检查器
- en: 'Unfortunately, it is very easy to make a mistake when specifying the size of
    a dynamically allocated region of memory. In many cases, such errors are difficult
    to find. Programming with many threads compounds the problem because an error
    in thread usage can cause an out-of-bounds memory access. For example, neglecting
    to check whether **tid** is less than the size of the vector in the fill routine
    will cause an out-of-bounds memory access. This is a subtle bug because the number
    of threads utilized is specified in a different program location in the execution
    configuration. See [Example 1.23](#tb0120), “Modified Kernel to Cause an Out-of-Bounds
    Error”:`__global__ void fillKernel(int *a, int n)``{``int tid = blockIdx.x*blockDim.x
    + threadIdx.x;``// if(tid < n) // Removing this comparision introduces an out-of-bounds
    error``a[tid] = tid;``}`The CUDA tool suite provides a standalone memory check
    utility called **cuda-memcheck**. As seen in [Example 1.24](#tb0125), “Example
    Showing the Program Can Run with an Out-of-Bounds Error,” the program appears
    to run correctly:`$ cuda-memcheck badRuntime``Test Succeeded!`However **cuda-memcheck**
    correctly flags that there is an out-of-bounds error, as shown in [Example 1.25](#tb0130),
    “Out-of-Bounds Error Reported by cuda-memcheck”:`$ cuda-memcheck badRuntime``=========
    CUDA-MEMCHECK``Test Succeeded!``========= Invalid __global__ write of size 4``=========at
    0x000000e0 in badRuntime.cu:14:fillKernel``=========by thread (336,0,0) in block
    (97,0,0)``=========Address 0x200130d40 is out of bounds``=========``=========
    ERROR SUMMARY: 1 error`Notice that no special compilation flags were required
    to use **cuda-memcheck**.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在指定动态分配的内存区域大小时很容易犯错误。在许多情况下，这类错误很难被发现。使用多个线程进行编程会加剧这个问题，因为线程使用错误可能会导致越界内存访问。例如，在填充例程中忽略检查**tid**是否小于向量的大小，会导致越界内存访问。这是一个微妙的错误，因为使用的线程数是在执行配置的不同位置指定的。请参见[示例
    1.23](#tb0120)，“修改后的内核引起越界错误”：`__global__ void fillKernel(int *a, int n)``{``int
    tid = blockIdx.x*blockDim.x + threadIdx.x;``// if(tid < n) // 移除这个比较会引发越界错误``a[tid]
    = tid;``}`CUDA工具套件提供了一个独立的内存检查工具，叫做**cuda-memcheck**。如[示例 1.24](#tb0125)所示，“显示程序可以在发生越界错误时运行”的示例，程序似乎运行正常：`$
    cuda-memcheck badRuntime``Test Succeeded!`然而，**cuda-memcheck**正确地标记了存在越界错误，如[示例
    1.25](#tb0130)所示，“cuda-memcheck报告的越界错误”：`$ cuda-memcheck badRuntime``=========
    CUDA-MEMCHECK``Test Succeeded!``========= 无效的__global__写入，大小为4``========= 在 badRuntime.cu:14:fillKernel的地址0x000000e0处``=========
    由线程（336,0,0）在块（97,0,0）中引发``========= 地址0x200130d40越界``=========``========= 错误总结：1个错误`请注意，使用**cuda-memcheck**不需要任何特殊的编译标志。
- en: Use cuda-gdb with the UNIX ddd Interface
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用cuda-gdb与UNIX ddd接口
- en: 'The GNU **ddd** (Data Display Debugger) provides a visual interface for **cuda-gdb**
    (or **gdb**). Many people prefer a visual debugger to a plain-text interface.
    To use **cuda-gdb**, **ddd** must be told to use a different debugger with the
    “**-debugger”** command-line flag as shown in [Example 1.26](#tb0135), “Command
    to Start ddd with cuda-gdb”:`ddd –debugger cuda-gdb badRuntime`Break points can
    be set visually, and all **cuda-gdb** commands can be manually entered. The following
    screenshot shows how to find an out-of-bounds error by first specifying **set
    cuda memcheck on**. As will be demonstrated in [Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3),
    **ddd** also provides a useful machine instruction window to examine and step
    through the actual instructions the GPU is using. See [Example 1.27](#f0040),
    “Using cuda-gdb Inside ddd to Find the Out-of-Bounds Error”:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: GNU **ddd**（数据展示调试器）为**cuda-gdb**（或**gdb**）提供了一个可视化界面。许多人更喜欢可视化调试器而不是纯文本界面。要使用**cuda-gdb**，必须通过“**-debugger**”命令行标志告诉**ddd**使用不同的调试器，如[示例
    1.26](#tb0135)所示，“使用cuda-gdb启动ddd的命令”：`ddd –debugger cuda-gdb badRuntime`。可以通过可视化界面设置断点，所有**cuda-gdb**命令也可以手动输入。以下截图展示了如何通过首先指定**set
    cuda memcheck on**来查找越界错误。如[第3章](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)所示，**ddd**还提供了一个有用的机器指令窗口，用于检查和逐步调试GPU使用的实际指令。请参见[示例
    1.27](#f0040)，“在ddd中使用cuda-gdb查找越界错误”：
- en: '| ![B978012388426800001X/u01-01-9780123884268.jpg is missing](B978012388426800001X/u01-01-9780123884268.jpg)
    |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| ![B978012388426800001X/u01-01-9780123884268.jpg 缺失](B978012388426800001X/u01-01-9780123884268.jpg)
    |'
- en: Windows Debugging with Parallel Nsight
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Parallel Nsight进行Windows调试
- en: Parallel Nsight also provides a debugging experience familiar to Microsoft Visual
    Studio users yet includes powerful GPU features like thread-level debugging and
    the CUDA memory checker. It installs as a plug-in within Microsoft Visual Studio.
    Parallel insight also provides a number of features:■ Integrated into Visual Studio
    2008 SP1 or Visual Studio 2010.■ CUDA C/C++ debugging.■ DirectX 10/11 shader debugging.■
    DirectX 10/11 frame debugging.■ DirectX 10/11 frame profiling.■ CUDA kernel trace/profiling.■
    OpenCL kernel trace/profiling.■ DirectX 10/11 API & HW trace.■ Data breakpoints
    for CUDA C/C++ code.■ Analyzer/system trace.■ Tesla Compute Cluster (TCC) support.Parallel
    Nsight allows both debugging and analysis on the machine as well as on remote
    machines that can be located at a customer's site. The capabilities of Parallel
    Nsight vary with the hardware configuration seen in [Table 1.3](#t0020).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Parallel Nsight 还提供了一个调试体验，类似于 Microsoft Visual Studio 用户所熟悉的，但还包括了强大的 GPU 功能，如线程级调试和
    CUDA 内存检查器。它作为一个插件安装在 Microsoft Visual Studio 中。Parallel Nsight 还提供了多项功能：■ 集成到
    Visual Studio 2008 SP1 或 Visual Studio 2010。■ CUDA C/C++ 调试。■ DirectX 10/11 着色器调试。■
    DirectX 10/11 帧调试。■ DirectX 10/11 帧剖析。■ CUDA 内核追踪/剖析。■ OpenCL 内核追踪/剖析。■ DirectX
    10/11 API 和硬件追踪。■ CUDA C/C++ 代码的数据断点。■ 分析器/系统追踪。■ Tesla 计算集群 (TCC) 支持。Parallel
    Nsight 支持在本机和远程机器上进行调试和分析，远程机器可以位于客户站点。Parallel Nsight 的能力会根据硬件配置的不同而有所不同，如[表
    1.3](#t0020)所示。
- en: '**Table 1.3** Parallel Nsight Capabilities According to Machine Configuration'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 1.3** 根据机器配置的 Parallel Nsight 能力'
- en: '| Hardware Configuration | Single GPU System | Dual GPU System | Two Systems,
    Each with a GPU | Dual-GPU System SLI MultiOS |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 硬件配置 | 单 GPU 系统 | 双 GPU 系统 | 每个系统配备 GPU 的两台系统 | 双 GPU 系统 SLI 多操作系统 |'
- en: '| CUDA C/C++ parallel debugger |  | ☑ | ☑ | ☑ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| CUDA C/C++ 并行调试器 |  | ☑ | ☑ | ☑ |'
- en: '| Direct3D shader debugger |  |  | ☑ | ☑ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Direct3D 着色器调试器 |  |  | ☑ | ☑ |'
- en: '| Direct3D graphics inspector | ☑ | ☑ | ☑ | ☑ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Direct3D 图形检查器 | ☑ | ☑ | ☑ | ☑ |'
- en: '| Analyzer | ☑ | ☑ | ☑ | ☑ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 分析器 | ☑ | ☑ | ☑ | ☑ |'
- en: '[Figure 1.6](#f0035) shows Parallel Nsight stopped at a breakpoint on the GPU
    when running the **fillkernel()** kernel in *seqRuntime.cu*. The *locals* window
    shows the values of various variables on the GPU.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1.6](#f0035)显示了 Parallel Nsight 在运行 **fillkernel()** 内核时停在 GPU 上的断点，文件为
    *seqRuntime.cu*。*locals* 窗口显示了 GPU 上各种变量的值。'
- en: '| ![B978012388426800001X/f01-06-9780123884268.jpg is missing](B978012388426800001X/f01-06-9780123884268.jpg)
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| ![B978012388426800001X/f01-06-9780123884268.jpg 缺失](B978012388426800001X/f01-06-9780123884268.jpg)
    |'
- en: '| **Figure 1.6**A Parallel Nsight debug screen. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **图 1.6** Parallel Nsight 调试屏幕。 |'
- en: A screenshot does not convey the interactive nature of Parallel Nsight, as many
    of the fields on the screen are interactive and can be clicked to gain more information.
    Visual Studio users should find the Parallel Nsight comfortable, as it reflects
    and utilized the look and feel of other aspects of Visual Studio. Parallel Nsight
    is an extensive package that is growing and maturing quickly. The most current
    information, including videos and the user forums, can be found on the Parallel
    Nsight web portal at [http://www.nvidia.com/ParallelNsight](http://www.nvidia.com/ParallelNsight)
    as well as the help section in Visual Studio.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 截图无法传达 Parallel Nsight 的交互性，因为屏幕上的许多字段是交互式的，点击后可以获得更多信息。Visual Studio 用户应该会觉得
    Parallel Nsight 很熟悉，因为它采用并利用了 Visual Studio 其他方面的外观和感觉。Parallel Nsight 是一个庞大的软件包，正在快速发展和成熟。最新的信息，包括视频和用户论坛，可以通过
    Parallel Nsight 网络门户在[http://www.nvidia.com/ParallelNsight](http://www.nvidia.com/ParallelNsight)找到，也可以通过
    Visual Studio 的帮助部分获取。
- en: Summary
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter covered a number of important applied topics such as how to write,
    compile, and run CUDA applications on the GPGPU using both the runtime and Thrust
    APIs. With the tools and syntax discussed in this chapter, it is possible to write
    and experiment with CUDA using the basics of the Thrust and runtime APIs. Audacious
    readers may even attempt to write real applications, which is encouraged, as they
    can be refactored based on the contents of later chapters and feedback from the
    performance profiling tools. Remember, the goal is to become a proficient CUDA
    programmer and writing code is the fastest path to that goal. Just keep the three
    basic rules of GPGPU programming in mind:1\. Get the data on the GPGPU and keep
    it there.2\. Give the GPGPU enough work to do.3\. Focus on data reuse within the
    GPGPU to avoid memory bandwidth limitations.Understanding basic computer science
    concepts is also essential to achieving high performance and improving yourself
    as a computer scientist. Keep Amdahl's law in mind to minimize serial bottlenecks
    as well as exploiting both task and data parallelism. Always try to understand
    the big-O implications of the algorithms that you use and seek out alternative
    algorithms that exhibit better scaling behavior. Always attempt to combine operations
    to achieve the highest computational density on the GPU while minimizing slow
    PCIe bus transfers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了一些重要的应用主题，例如如何使用运行时和Thrust API在GPGPU上编写、编译和运行CUDA应用程序。通过本章讨论的工具和语法，您可以使用Thrust和运行时API的基础知识编写和实验CUDA程序。大胆的读者甚至可以尝试编写实际的应用程序，这是鼓励的，因为它们可以根据后续章节的内容和性能分析工具的反馈进行重构。请记住，目标是成为一名熟练的CUDA程序员，而编写代码是实现这一目标的最快途径。只需牢记GPGPU编程的三条基本规则：1\.
    将数据传输到GPGPU并保持在那里。2\. 给GPGPU足够的工作量。3\. 聚焦于在GPGPU内的数据重用，以避免内存带宽限制。理解计算机科学的基本概念对于实现高性能并提升自己作为计算机科学家的能力也是至关重要的。牢记Amdahl定律，尽量减少串行瓶颈，并利用任务和数据并行性。始终尝试理解您所使用的算法的大-O复杂度，并寻求具有更好扩展行为的替代算法。始终尝试将操作合并，以便在GPU上实现最高的计算密度，同时最小化缓慢的PCIe总线传输。
