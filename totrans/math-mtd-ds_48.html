<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>6.4. Modeling more complex dependencies 2: marginalizing out an unobserved variable#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>6.4. Modeling more complex dependencies 2: marginalizing out an unobserved variable#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap06_prob/04_em/roch-mmids-prob-em.html">https://mmids-textbook.github.io/chap06_prob/04_em/roch-mmids-prob-em.html</a></blockquote>

<p>In this section, we move on to the second technique for constructing joint distributions from simpler building blocks: marginalizing out an unobserved random variable.</p>
<section id="mixtures">
<h2><span class="section-number">6.4.1. </span>Mixtures<a class="headerlink" href="#mixtures" title="Link to this heading">#</a></h2>
<p>Mixtures<span class="math notranslate nohighlight">\(\idx{mixture}\xdi\)</span> are a natural way to define probability distributions. The basic idea is to consider a pair of random vectors <span class="math notranslate nohighlight">\((\bX,\bY)\)</span> and assume that <span class="math notranslate nohighlight">\(\bY\)</span> is unobserved. The effect on the observed vector <span class="math notranslate nohighlight">\(\bX\)</span> is that <span class="math notranslate nohighlight">\(\bY\)</span> is marginalized out. Indeed, by the law of total probability, for any <span class="math notranslate nohighlight">\(\bx \in \S_\bX\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_\bX(\bx)
&amp;= \P[\bX = \bx]\\
&amp;= \sum_{\by \in \S_\bY} \P[\bX=\bx|\bY=\by] \,\P[\bY=\by]\\
&amp;= \sum_{\by \in \S_\bY} p_{\bX|\bY}(\bx|\by) \,p_\bY(\by)
\end{align*}\]</div>
<p>where we used that the events <span class="math notranslate nohighlight">\(\{\bY=\by\}\)</span>, <span class="math notranslate nohighlight">\(\by \in \S_\bY\)</span>, form a partition of the probability space. We interpret this equation as defining <span class="math notranslate nohighlight">\(p_\bX(\bx)\)</span> as a convex combination – a mixture – of the distributions <span class="math notranslate nohighlight">\(p_{\bX|\bY}(\bx|\by)\)</span>, <span class="math notranslate nohighlight">\(\by \in \S_\bY\)</span>, with mixing weights <span class="math notranslate nohighlight">\(p_\bY(\by)\)</span>. In general, we need to specify the full conditional probability distribution (CPD): <span class="math notranslate nohighlight">\(p_{\bX|\bY}(\bx|\by), \forall \bx \in \S_{\bX}, \by \in \S_\bY\)</span>. But assuming that the mixing weights and/or CPD come from parametric families can help reduce the complexity of the model.</p>
<p>That can be represented in a digraph with a directed edge from a vertex for <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> to a vertex for <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. Further, we let the vertex for <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be shaded to indicate that it is observed, while the vertex for <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> is not shaded to indicate that it is not. Mathematically, that corresponds to applying the law of total probability as we did previously.</p>
<p><img alt="A mixture" src="../Images/d7c8f87d42f669398ac57f92c0a43388.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_mixture_networkx.png"/></p>
<p>In the parametric context, this gives rise to a fruitful approach to expanding distribution families. Suppose <span class="math notranslate nohighlight">\(\{p_{\btheta}:\btheta \in \Theta\}\)</span> is a parametric family of distributions. Let <span class="math notranslate nohighlight">\(K \geq 2\)</span>, <span class="math notranslate nohighlight">\(\btheta_1, \ldots, \btheta_K \in \Theta\)</span> and <span class="math notranslate nohighlight">\(\bpi = (\pi_1,\ldots,\pi_K) \in \Delta_K\)</span>. Suppose <span class="math notranslate nohighlight">\(Y \sim \mathrm{Cat}(\bpi)\)</span> and that the conditional distributions satisfy</p>
<div class="math notranslate nohighlight">
\[
p_{\bX|Y}(\bx|i)
= p_{\btheta_i}(\bx).
\]</div>
<p>We write this as <span class="math notranslate nohighlight">\(\bX|\{Y=i\} \sim p_{\btheta_i}\)</span>. Then we obtain the mixture model</p>
<div class="math notranslate nohighlight">
\[
p_{\bX}(\bx)
= \sum_{i=1}^K p_{\bX|Y}(\bx|i) \,p_Y(i)
= \sum_{i=1}^K \pi_i p_{\btheta_i}(\bx).
\]</div>
<p><strong>EXAMPLE:</strong> <strong>(Mixture of Multinomials)</strong> Let <span class="math notranslate nohighlight">\(n, m , K \geq 1\)</span>, <span class="math notranslate nohighlight">\(\bpi \in \Delta_K\)</span> and, for <span class="math notranslate nohighlight">\(i=1,\ldots,K\)</span>, <span class="math notranslate nohighlight">\(\mathbf{p}_i = (p_{i1},\ldots,p_{im}) \in \Delta_m\)</span>.
Suppose that <span class="math notranslate nohighlight">\(Y \sim \mathrm{Cat}(\bpi)\)</span> and that the conditional distributions are</p>
<div class="math notranslate nohighlight">
\[
\bX|\{Y=i\} \sim \mathrm{Mult}(n, \mathbf{p}_i).
\]</div>
<p>Then <span class="math notranslate nohighlight">\(\bX\)</span> is a mixture of multinomials. Its distribution is then</p>
<div class="math notranslate nohighlight">
\[
p_\bX(\bx)
= \sum_{i=1}^K \pi_i \frac{n!}{x_1!\cdots x_m!} \prod_{j=1}^m p_{ij}^{x_j}.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Next is an important continuous example.</p>
<p><strong>EXAMPLE:</strong> <strong>(Gaussian mixture model)</strong> <span class="math notranslate nohighlight">\(\idx{Gaussian mixture model}\xdi\)</span> For <span class="math notranslate nohighlight">\(i=1,\ldots,K\)</span>, let <span class="math notranslate nohighlight">\(\bmu_i\)</span> and <span class="math notranslate nohighlight">\(\bSigma_i\)</span> be the mean and covariance matrix of a multivariate Gaussian. Let <span class="math notranslate nohighlight">\(\bpi \in \Delta_K\)</span>. A Gaussian Mixture Model (GMM) is obtained as follows: take <span class="math notranslate nohighlight">\(Y \sim \mathrm{Cat}(\bpi)\)</span> and</p>
<div class="math notranslate nohighlight">
\[
\bX|\{Y=i\} \sim N_d(\bmu_i, \bSigma_i).
\]</div>
<p>Its probability density function (PDF) takes the form</p>
<div class="math notranslate nohighlight">
\[
f_\bX(\bx)
= \sum_{i=1}^K \pi_i \frac{1}{(2\pi)^{d/2} \,|\bSigma_i|^{1/2}}
\exp\left(-\frac{1}{2}(\mathbf{x} - \bmu_i)^T \bSigma_i^{-1} (\bx - \bmu_i)\right).
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We plot the density for means <span class="math notranslate nohighlight">\(\bmu_1 = (-2,-2)\)</span> and <span class="math notranslate nohighlight">\(\bmu_2 = (2,2)\)</span> and covariance matrices</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bSigma_1 = \begin{bmatrix} 1.0 &amp; 0 \\ 0 &amp; 1.0 \end{bmatrix}
\quad \text{and} \quad 
\bSigma_2 = \begin{bmatrix} \sigma_1^2 &amp; \rho \sigma_1 \sigma_2 \\ \rho \sigma_1 \sigma_2 &amp; \sigma_2^2 \end{bmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_1 = 1.5\)</span>, <span class="math notranslate nohighlight">\(\sigma_2 = 0.5\)</span> and <span class="math notranslate nohighlight">\(\rho = -0.75\)</span>. The mixing weights are <span class="math notranslate nohighlight">\(\pi_1 = 0.25\)</span> and <span class="math notranslate nohighlight">\(\pi_2 = 0.75\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>

<span class="k">def</span> <span class="nf">gmm2_pdf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">mean1</span><span class="p">,</span> <span class="n">cov1</span><span class="p">,</span> <span class="n">pi1</span><span class="p">,</span> <span class="n">mean2</span><span class="p">,</span> <span class="n">cov2</span><span class="p">,</span> <span class="n">pi2</span><span class="p">):</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">Z1</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span>
        <span class="n">xy</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean1</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span>
        <span class="n">xy</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean2</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">pi1</span> <span class="o">*</span> <span class="n">Z1</span> <span class="o">+</span> <span class="n">pi2</span> <span class="o">*</span> <span class="n">Z2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">start_point</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">stop_point</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">start_point</span><span class="p">,</span> <span class="n">stop_point</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">points</span><span class="p">)</span>

<span class="n">mean1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">])</span>
<span class="n">cov1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="n">pi1</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">mean2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="n">cov2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.5</span> <span class="o">**</span> <span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.75</span> <span class="o">*</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">],</span> 
                 <span class="p">[</span><span class="o">-</span><span class="mf">0.75</span> <span class="o">*</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">**</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="n">pi2</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">gmm2_pdf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">mean1</span><span class="p">,</span> <span class="n">cov1</span><span class="p">,</span> <span class="n">pi1</span><span class="p">,</span> <span class="n">mean2</span><span class="p">,</span> <span class="n">cov2</span><span class="p">,</span> <span class="n">pi2</span><span class="p">)</span>
<span class="n">mmids</span><span class="o">.</span><span class="n">make_surface_plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/d96dada4644d3611a7cfa6aeaa7c3378d8e8cca7a04c6ffbc22bff283cfaa596.png" src="../Images/810f47a1332c13f0595b65b28e44ab63.png" data-original-src="https://mmids-textbook.github.io/_images/d96dada4644d3611a7cfa6aeaa7c3378d8e8cca7a04c6ffbc22bff283cfaa596.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>In NumPy, as we have seen before, the module <a class="reference external" href="https://numpy.org/doc/stable/reference/random/index.html"><code class="docutils literal notranslate"><span class="pre">numpy.random</span></code></a> also provides a way to sample from mixture models by using <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.choice</span></code></a>.</p>
<p>For instance, we consider mixtures of multivariate Gaussians. We chage the notation slightly to track Python’s indexing. For <span class="math notranslate nohighlight">\(i=0,1\)</span>, we have a mean <span class="math notranslate nohighlight">\(\bmu_i \in \mathbb{R}^d\)</span> and a positive definite covariance matrix <span class="math notranslate nohighlight">\(\bSigma_i \in \mathbb{R}^{d \times d}\)</span>. We also have mixture weights <span class="math notranslate nohighlight">\(\phi_0, \phi_1 \in (0,1)\)</span> such that <span class="math notranslate nohighlight">\(\phi_0 + \phi_1 = 1\)</span>. Suppose we want to generate a total of <span class="math notranslate nohighlight">\(n\)</span> samples.</p>
<p>For each sample <span class="math notranslate nohighlight">\(j=1,\ldots, n\)</span>, independently from everything else:</p>
<ol class="arabic simple">
<li><p>We first pick a component <span class="math notranslate nohighlight">\(i \in \{0,1\}\)</span> at random according to the mixture weights, that is, <span class="math notranslate nohighlight">\(i=0\)</span> is chosen with probability <span class="math notranslate nohighlight">\(\phi_0\)</span> and <span class="math notranslate nohighlight">\(i=1\)</span> is chosen with probability <span class="math notranslate nohighlight">\(\phi_1\)</span>.</p></li>
<li><p>We generate a sample <span class="math notranslate nohighlight">\(\bX_j = (X_{j,1},\ldots,X_{j,d})\)</span> according to a multivariate Gaussian with mean <span class="math notranslate nohighlight">\(\bmu_i\)</span> and covariance <span class="math notranslate nohighlight">\(\bSigma_i\)</span>.</p></li>
</ol>
<p>This is straightforward to implement by using again <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.choice</span></code></a> to choose the component of each sample and <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.multivariate_normal</span></code></a> to generate multivariate Gaussians. For convenience, we will stack the means and covariances into one array with a new dimension. So, for instance, the covariance matrices will now be in a 3d-array, that is, an array with <span class="math notranslate nohighlight">\(3\)</span> indices. The first index corresponds to the component (here <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>).</p>
<p><strong>Figure:</strong> Three matrices (<a class="reference external" href="https://www.tensorflow.org/guide/tensor#basics">Source</a>)</p>
<p><img alt="Three matrices" src="../Images/e9a2eb3f0bbe5139202ee6636f55ede6.png" data-original-src="https://www.tensorflow.org/static/guide/images/tensor/3-axis_numpy.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>Figure:</strong> Three matrices stacked into a 3d-array (<a class="reference external" href="https://www.tensorflow.org/guide/tensor#basics">Source</a>)</p>
<p><img alt="Three matrices stacked into a tensor" src="../Images/a61cc745a3c58639bf7340b2af821416.png" data-original-src="https://www.tensorflow.org/static/guide/images/tensor/3-axis_front.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>The code is the following. It returns an <code class="docutils literal notranslate"><span class="pre">d</span></code> by <code class="docutils literal notranslate"><span class="pre">n</span></code> array <code class="docutils literal notranslate"><span class="pre">X</span></code>, where each row is a sample from a 2-component Gaussian mixture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">gmm2</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">phi0</span><span class="p">,</span> <span class="n">phi1</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="n">sigma0</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">):</span>
    
    <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">phi0</span><span class="p">,</span> <span class="n">phi1</span><span class="p">))</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">mu0</span><span class="p">,</span> <span class="n">mu1</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">sigma0</span><span class="p">,</span><span class="n">sigma1</span><span class="p">))</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
    <span class="n">component</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">phi</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span>
            <span class="n">mu</span><span class="p">[</span><span class="n">component</span><span class="p">[</span><span class="n">i</span><span class="p">],:],</span>
            <span class="n">sigma</span><span class="p">[</span><span class="n">component</span><span class="p">[</span><span class="n">i</span><span class="p">],:,:])</span>
    
    <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> Let us try it with following parameters. We first define the covariance matrices and show what happens when they are stacked into a 3d array (as is done within <code class="docutils literal notranslate"><span class="pre">gmm2</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">sigma0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]))</span> 
<span class="n">sigma0</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]))</span>
<span class="n">sigma1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">sigma0</span><span class="p">,</span><span class="n">sigma1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[4.25 3.75]
 [3.75 4.25]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">,:,:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[2. 0.]
 [0. 2.]]
</pre></div>
</div>
</div>
</div>
<p>Then we define the rest of the parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> 

<span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="mf">5.</span>
<span class="n">phi0</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">phi1</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">mu0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="o">-</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">gmm2</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">phi0</span><span class="p">,</span> <span class="n">phi1</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="n">sigma0</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'equal'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d22c61780e13a73ecc8364cd91c45d25951b009b2ea42174ad8643880d716bc6.png" src="../Images/22ec3b142976b6b20a0185e9e997fea9.png" data-original-src="https://mmids-textbook.github.io/_images/d22c61780e13a73ecc8364cd91c45d25951b009b2ea42174ad8643880d716bc6.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
<section id="example-mixtures-of-multivariate-bernoullis-and-the-em-algorithm">
<h2><span class="section-number">6.4.2. </span>Example: Mixtures of multivariate Bernoullis and the EM algorithm<a class="headerlink" href="#example-mixtures-of-multivariate-bernoullis-and-the-em-algorithm" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{C} = \{1, \ldots, K\}\)</span> be a collection of classes. Let <span class="math notranslate nohighlight">\(C\)</span> be a random variable taking values in <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> and, for <span class="math notranslate nohighlight">\(m=1, \ldots, M\)</span>, let <span class="math notranslate nohighlight">\(X_i\)</span> take values in <span class="math notranslate nohighlight">\(\{0,1\}\)</span>. Define <span class="math notranslate nohighlight">\(\pi_k = \P[C = k]\)</span> and <span class="math notranslate nohighlight">\(p_{k,m} = \P[X_m = 1|C = k]\)</span> for <span class="math notranslate nohighlight">\(m = 1,\ldots, M\)</span>. We denote by <span class="math notranslate nohighlight">\(\bX = (X_1, \ldots, X_M)\)</span> the corresponding vector of <span class="math notranslate nohighlight">\(X_i\)</span>’s and assume that the entries are conditionally independent given <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p>However, we assume this time that <span class="math notranslate nohighlight">\(C\)</span> itself is <em>not observed</em>. So the resulting joint distribution is the mixture</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[\bX = \bx]
&amp;= \sum_{k=1}^K \P[C = k, \bX = \bx]\\
&amp;= \sum_{k=1}^K \P[\bX = \bx|C=k] \,\P[C=k]\\
&amp;= \sum_{k=1}^K \pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}.
\end{align*}\]</div>
<p>Graphically, this is the same are the Naive Bayes model, except that <span class="math notranslate nohighlight">\(C\)</span> is not observed and therefore is not shaded.</p>
<p><img alt="Mixture of multivariate Bernoullis" src="../Images/c51352d9d6ab19d2f9e73f898c25b648.png" data-original-src="https://mmids-textbook.github.io/_images/dgm-em-networkx.png"/></p>
<p>This type of model is useful in particular for clustering tasks, where the <span class="math notranslate nohighlight">\(c_k\)</span>s can be thought of as different clusters. Similarly to what we did in the previous section, our goal is to infer the parameters from samples and then predict the class of an old or new sample given its features. The main – substantial – difference is that the true labels of the samples are not observed. As we will see, that complicates the task considerably.</p>
<p><strong>Model fitting</strong> We first fit the model from training data <span class="math notranslate nohighlight">\(\{\bx_i\}_{i=1}^n\)</span>. Recall that the corresponding class labels <span class="math notranslate nohighlight">\(c_i\)</span>s are not observed. In this type of model, they are referred to as hidden or latent variables and we will come back to their inference below.</p>
<p>We would like to use maximum likelihood estimation, that is, maximize the probability of observing the data</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i\})
= \prod_{i=1}^n \left( \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
\]</div>
<p>As usual, we assume that the samples are independent and identically distributed. Consider the negative log-likelihood (NLL)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L_n(\bpi, \{\bp_k\}; \{\bx_i\})
&amp;= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
\end{align*}\]</div>
<p>Already, we see that things are potentially more difficult than they were in the supervised (or fully observed) case. The NLL does not decompose into a sum of terms depending on different sets of parameters.</p>
<p>At this point, one could fall back on the field of optimization and use a gradient-based method to minimize the NLL. Indeed that is an option, although note that one must be careful to account for the constrained nature of the problem (i.e., the parameters sum to one). There is a vast array of constrained optimization techniques suited for this task.</p>
<p>Instead a more popular approach in this context, the EM algorithm, is based on the general principle of majorization-minimization, which we have encountered implicitly in the <span class="math notranslate nohighlight">\(k\)</span>-means algorithm and the convergence proof of gradient descent in the smooth case. We detail this important principle in the next subsection before returning to model fitting in mixtures.</p>
<p><strong>Majorization-minimization</strong> <span class="math notranslate nohighlight">\(\idx{majorization-minimization}\xdi\)</span> Here is a deceptively simple, yet powerful observation. Suppose we want to minimize a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span>. Finding a local minimum of <span class="math notranslate nohighlight">\(f\)</span> may not be easy. But imagine that for each <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> we have a surrogate function <span class="math notranslate nohighlight">\(U_{\mathbf{x}} : \mathbb{R}^d \to \mathbb{R}\)</span> that (1) dominates <span class="math notranslate nohighlight">\(f\)</span> in the following sense</p>
<div class="math notranslate nohighlight">
\[
U_\mathbf{x}(\mathbf{z}) \geq f(\mathbf{z}), \quad \forall \mathbf{z} \in \mathbb{R}^d 
\]</div>
<p>and (2) equals <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></p>
<div class="math notranslate nohighlight">
\[
U_\mathbf{x}(\mathbf{x}) = f(\mathbf{x}).
\]</div>
<p>We say that <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span> majorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Then we prove in the next lemma that <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span> can be used to make progress towards minimizing <span class="math notranslate nohighlight">\(f\)</span>, that is, find a point <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> such that <span class="math notranslate nohighlight">\(f(\mathbf{x}') \leq f(\mathbf{x})\)</span>. If in addition <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span> is easier to minimize than <span class="math notranslate nohighlight">\(f\)</span> itself, say because an explicit minimum can be computed, then this observation naturally leads to an iterative algorithm.</p>
<p><img alt="A majorizing function (with help from ChatGPT; inspired by Source)" src="../Images/c05a4e1add7058b646f07a53f61281a2.png" data-original-src="https://mmids-textbook.github.io/_images/majorize.png"/></p>
<p><strong>LEMMA</strong> <strong>(Majorization-Minimization)</strong> <span class="math notranslate nohighlight">\(\idx{majorization-minimization lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> and suppose <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> majorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> be a global minimum of <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}') \leq f(\mathbf{x}).
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Indeed</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}') \leq U_\mathbf{x}(\mathbf{x}') \leq U_{\mathbf{x}}(\mathbf{x}) = f(\mathbf{x}),
\]</div>
<p>where the first inequality follows from the domination property of <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span>, the second inequality follows from the fact that <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> is a global minimum  of <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span> and the equality follows from the fact that <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> equals <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We have already encountered this idea.</p>
<p><strong>EXAMPLE:</strong> <strong>(Minimizing a smooth function)</strong> Let <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> be <span class="math notranslate nohighlight">\(L\)</span>-smooth. By the <em>Quadratic Bound for Smooth Functions</em>, for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{z} \in \mathbb{R}^d\)</span> it holds that</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{z})
\leq U_{\mathbf{x}}(\mathbf{z})
:= f(\mathbf{x}) 
+ \nabla f(\mathbf{x})^T(\mathbf{z} - \mathbf{x})
+ \frac{L}{2} \|\mathbf{z} - \mathbf{x}\|^2.
\]</div>
<p>By showing that <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> is minimized at <span class="math notranslate nohighlight">\(\mathbf{z} = \mathbf{x} - (1/L)\nabla f(\mathbf{x})\)</span>, we previously obtained the descent guarantee</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x} - (1/L)\nabla f(\mathbf{x}))
\leq f(\mathbf{x}) - \frac{1}{2 L} \|\nabla f(\mathbf{x})\|^2
\]</div>
<p>for gradient descent, which played a central role in the analysis of its convergence<span class="math notranslate nohighlight">\(\idx{convergence analysis}\xdi\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(<span class="math notranslate nohighlight">\(k\)</span>-means)</strong> <span class="math notranslate nohighlight">\(\idx{Lloyd's algorithm}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{x}_1,\ldots,\mathbf{x}_n\)</span> be <span class="math notranslate nohighlight">\(n\)</span> vectors in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>. One way to formulate the <span class="math notranslate nohighlight">\(k\)</span>-means clustering problem is as the minimization of</p>
<div class="math notranslate nohighlight">
\[
f(\bmu_1,\ldots,\bmu_K)
= \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i - \bmu_j\|^2
\]</div>
<p>over the centers <span class="math notranslate nohighlight">\(\bmu_1,\ldots,\bmu_K\)</span>, where recall that <span class="math notranslate nohighlight">\([K] = \{1,\ldots,K\}\)</span>. For fixed <span class="math notranslate nohighlight">\(\bmu_1,\ldots,\bmu_K\)</span> and <span class="math notranslate nohighlight">\(\mathbf{m} = (\bmu_1,\ldots,\bmu_K)\)</span>, define</p>
<div class="math notranslate nohighlight">
\[
c_\mathbf{m}(i) \in \arg\min\left\{\|\mathbf{x}_i - \bmu_j\|^2 \ :\ j \in [K]\right\},
\quad i=1,\ldots,n
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) 
= \sum_{i=1}^n \|\mathbf{x}_i - \blambda_{c_\mathbf{m}(i)}\|^2
\]</div>
<p>for <span class="math notranslate nohighlight">\(\blambda_1,\ldots,\blambda_K \in \mathbb{R}^d\)</span>. That is, we fix the optimal cluster assignments under <span class="math notranslate nohighlight">\(\bmu_1,\ldots,\bmu_K\)</span> and then vary the centers.</p>
<p>We claim that <span class="math notranslate nohighlight">\(U_\mathbf{m}\)</span> is majorizing <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\bmu_1,\ldots,\bmu_K\)</span>. Indeed</p>
<div class="math notranslate nohighlight">
\[
f(\blambda_1,\ldots,\blambda_K)
= \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i - \blambda_j\|^2
\leq \sum_{i=1}^n \|\mathbf{x}_i - \blambda_{c_\mathbf{m}(i)}\|^2
= U_\mathbf{m}(\blambda_1,\ldots,\blambda_K)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
f(\bmu_1,\ldots,\bmu_K)
= \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i - \bmu_j\|^2
= \sum_{i=1}^n \|\mathbf{x}_i - \bmu_{c_\mathbf{m}(i)}\|^2
= U_\mathbf{m}(\bmu_1,\ldots,\bmu_K).
\]</div>
<p>Moreover <span class="math notranslate nohighlight">\(U_\mathbf{m}(\blambda_1,\ldots,\blambda_K)\)</span> is easy to minimize. We showed previously that the optimal representatives are</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\mu}_j' = \frac{1}{|C_j|} \sum_{i\in C_j} \mathbf{x}_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(C_j = \{i : c_\mathbf{m}(i) = j\}\)</span>.</p>
<p>The <em>Majorization-Minimization Lemma</em> implies that</p>
<div class="math notranslate nohighlight">
\[
f(\bmu_1',\ldots,\bmu_K')
\leq f(\bmu_1,\ldots,\bmu_K).
\]</div>
<p>This argument is equivalent to our previous analysis of the <span class="math notranslate nohighlight">\(k\)</span>-means algorithm. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The mixture of multivariate Bernoullis model assumes a fixed number of clusters. Ask your favorite AI chatbot to discuss Bayesian nonparametric extensions of this model, such as the Dirichlet process mixture model, which can automatically infer the number of clusters from the data. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><strong>EM algorithm</strong> The <a class="reference external" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-Maximization (EM) algorithm</a><span class="math notranslate nohighlight">\(\idx{EM algorithm}\xdi\)</span> is an instantiation of the majorization-minimization principle that applies widely to parameter estimation of mixtures. Here we focus on the mixture of multivariate Bernoullis.</p>
<p>Recall that the objective to be minimized is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L_n(\bpi, \{\bp_k\}; \{\bx_i\})
&amp;= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
\end{align*}\]</div>
<p>To simplify the notation and highlight the general idea, we let <span class="math notranslate nohighlight">\(\btheta = (\bpi, \{\bp_k\})\)</span>, denote by <span class="math notranslate nohighlight">\(\Theta\)</span> the set of allowed values for <span class="math notranslate nohighlight">\(\btheta\)</span>, and use <span class="math notranslate nohighlight">\(\P_{\btheta}\)</span> to indicate that probabilities are computed under the parameters <span class="math notranslate nohighlight">\(\btheta\)</span>. We also return to the description of the model in terms of the unobserved latent variables <span class="math notranslate nohighlight">\(\{C_i\}\)</span>. That is, we write the NLL as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L_n(\btheta)
&amp;= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \P_{\btheta}[\bX_i = \bx_i|C_i = k] \,\P_{\btheta}[C_i = k]\right)\\
&amp;= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \P_{\btheta}[\bX_i = \bx_i, C_i = k] \right).
\end{align*}\]</div>
<p>To derive a majorizing function, we use the convexity of the negative logarithm. Indeed</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial z}[- \log z]
= - \frac{1}{z}
\quad
\text{and}
\quad
\frac{\partial^2}{\partial^2 z}[- \log z]
= \frac{1}{z^2} &gt; 0,
\quad
\forall z &gt; 0.
\]</div>
<p>The first step of the construction is not obvious – it just works. For each <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>, we let <span class="math notranslate nohighlight">\(r_{k,i}^{\btheta}\)</span>, <span class="math notranslate nohighlight">\(k=1,\ldots,K\)</span>, be a strictly positive probability distribution over <span class="math notranslate nohighlight">\([K]\)</span>. In other words, it defines a convex combination for every <span class="math notranslate nohighlight">\(i\)</span>. Then we use convexity to obtain the upper bound</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L_n(\tilde\btheta)
&amp;= - \sum_{i=1}^n \log \left( \sum_{k=1}^K r_{k,i}^{\btheta} \frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
&amp;\leq - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right),
\end{align*}\]</div>
<p>which holds for any <span class="math notranslate nohighlight">\(\tilde\btheta = (\tilde\bpi, \{\tilde\bp_k\}) \in \Theta\)</span>.</p>
<p>We choose</p>
<div class="math notranslate nohighlight">
\[
r_{k,i}^{\btheta} = \P_{\btheta}[C_i = k|\bX_i = \bx_i]
\]</div>
<p>(which for the time being we assume is strictly positive) and we denote the right-hand side of the inequality by <span class="math notranslate nohighlight">\(Q_{n}(\tilde\btheta|\btheta)\)</span> (as a function of <span class="math notranslate nohighlight">\(\tilde\btheta\)</span>).</p>
<p>We make two observations.</p>
<p>1- <em>Dominating property</em>: For any <span class="math notranslate nohighlight">\(\tilde\btheta \in \Theta\)</span>, the inequality above implies immediately that <span class="math notranslate nohighlight">\(L_n(\tilde\btheta) \leq Q_n(\tilde\btheta|\btheta)\)</span>.</p>
<p>2- <em>Equality at <span class="math notranslate nohighlight">\(\btheta\)</span></em>: At <span class="math notranslate nohighlight">\(\tilde\btheta = \btheta\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Q_n(\btheta|\btheta) 
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\btheta}[C_i = k | \bX_i = \bx_i]
\P_{\btheta}[\bX_i = \bx_i]}{r_{k,i}^{\btheta}} \right)\\
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log
\P_{\btheta}[\bX_i = \bx_i]\\
&amp;= - \sum_{i=1}^n \log
\P_{\btheta}[\bX_i = \bx_i]\\
&amp;= L_n(\btheta).
\end{align*}\]</div>
<p>The two properties above show that <span class="math notranslate nohighlight">\(Q_n(\tilde\btheta|\btheta)\)</span>, as a function of <span class="math notranslate nohighlight">\(\tilde\btheta\)</span>, majorizes <span class="math notranslate nohighlight">\(L_n\)</span> at <span class="math notranslate nohighlight">\(\btheta\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(EM Guarantee)</strong> <span class="math notranslate nohighlight">\(\idx{EM guarantee}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\btheta^*\)</span> be a global minimizer of <span class="math notranslate nohighlight">\(Q_n(\tilde\btheta|\btheta)\)</span> as a function of <span class="math notranslate nohighlight">\(\tilde\btheta\)</span>, provided it exists. Then</p>
<div class="math notranslate nohighlight">
\[
L_n(\btheta^*) \leq L_n(\btheta).
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> The result follows directly from the <em>Majorization-Minimization Lemma</em>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>What have we gained from this? As we mentioned before, using the <em>Majorization-Minimization Lemma</em> makes sense if <span class="math notranslate nohighlight">\(Q_n\)</span> is easier to minimize than <span class="math notranslate nohighlight">\(L_n\)</span> itself. Let us see why that is the case here.</p>
<p><em>E Step:</em> The function <span class="math notranslate nohighlight">\(Q_n\)</span> naturally decomposes into two terms</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Q_n(\tilde\btheta|\btheta)
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]
+ \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log r_{k,i}^{\btheta}.
\end{align*}\]</div>
<p>Because <span class="math notranslate nohighlight">\(r_{k,i}^{\btheta}\)</span> depends on <span class="math notranslate nohighlight">\(\btheta\)</span> <em>but not <span class="math notranslate nohighlight">\(\tilde\btheta\)</span></em>, the second term is irrelevant to the opimization with respect to <span class="math notranslate nohighlight">\(\tilde\btheta\)</span>.</p>
<p>The first term above can be written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp; - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]\\
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\tilde{\pi}_{k} \prod_{m=1}^M \tilde{p}_{k, m}^{x_{i,m}} (1-\tilde{p}_{k,m})^{1-x_{i,m}}\right)\\
&amp;= - \sum_{k=1}^K \eta_k^{\btheta} \log \tilde{\pi}_k - \sum_{k=1}^K \sum_{m=1}^M [\eta_{k,m}^{\btheta} \log \tilde{p}_{k,m} + (\eta_k^{\btheta}-\eta_{k,m}^{\btheta}) \log (1-\tilde{p}_{k,m})],
\end{align*}\]</div>
<p>where we defined, for <span class="math notranslate nohighlight">\(k=1,\ldots,K\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\eta_{k,m}^{\btheta}
= \sum_{i=1}^n x_{i,m} r_{k,i}^{\btheta}
\quad
\text{and}
\quad
\eta_k^{\btheta} 
= \sum_{i=1}^n r_{k,i}^{\btheta}.
\]</div>
<p>Here comes the key observation: this last expression is essentially the same as the NLL for the fully observed Naive Bayes model, except that the terms <span class="math notranslate nohighlight">\(\mathbf{1}_{\{c_i = k\}}\)</span> are replaced by <span class="math notranslate nohighlight">\(r_{k,i}^{\btheta}\)</span>. If <span class="math notranslate nohighlight">\(\btheta\)</span> is our current estimate of the parameters, then the quantity <span class="math notranslate nohighlight">\(r_{k,i}^{\btheta} = \P_{\btheta}[C_i = k|\bX_i = \bx_i]\)</span> is our estimate – under the current parameter <span class="math notranslate nohighlight">\(\btheta\)</span> – of the probability that the sample <span class="math notranslate nohighlight">\(\bx_i\)</span> comes from cluster <span class="math notranslate nohighlight">\(k\)</span>. We have previously computed <span class="math notranslate nohighlight">\(r_{k,i}^{\btheta}\)</span> for prediction under the Naive Bayes model. We showed there that</p>
<div class="math notranslate nohighlight">
\[
r_{k,i}^{\btheta}
= \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_{i,m}} (1-p_{k,m})^{1-x_{i,m}}}
{\sum_{k'=1}^K \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_{i,m}} (1-p_{k',m})^{1-x_{i,m}}},
\]</div>
<p>which in this new context is referred to as the responsibility that cluster <span class="math notranslate nohighlight">\(k\)</span> takes for data point <span class="math notranslate nohighlight">\(i\)</span>. So we can interpret the expression above as follows: the variables <span class="math notranslate nohighlight">\(\mathbf{1}_{\{c_i = k\}}\)</span> are not observed here, but we have estimated their conditional probability distribution given the observed data <span class="math notranslate nohighlight">\(\{\bx_i\}\)</span>, and we are taking an expectation with respect to that distribution instead.</p>
<p>The “E” in E Step (and EM) stands for “expectation”, which refers to using a surrogate function that is essentially an expected NLL.</p>
<p><em>M Step:</em> In any case, from a practical point of view, minimizing <span class="math notranslate nohighlight">\(Q_n(\tilde\btheta|\btheta)\)</span> over <span class="math notranslate nohighlight">\(\tilde\btheta\)</span> turns out to be a variant of fitting a Naive Bayes model – and the upshot to all this is that there is a straightforward formula for that! Recall that this happens because, the NLL in the Naive Bayes model decomposes: it naturally breaks up into terms that depend on separate sets of parameters, each of which can be optimized with a closed-form expression. The same happens with <span class="math notranslate nohighlight">\(Q_n\)</span> as should be clear from the derivation.</p>
<p>Adapting our previous calculations for fitting a Naive Bayes model, we get that <span class="math notranslate nohighlight">\(Q_n(\tilde\btheta|\btheta)\)</span> is minimized at</p>
<div class="math notranslate nohighlight">
\[
\pi_k^* = \frac{\eta_k^{\btheta}}{n}
\quad
\text{and}
\quad 
p_{k,m}^* = \frac{\eta_{k,m}^{\btheta}}{\eta_k^{\btheta}}
\quad
\forall k \in [K], m \in [M].
\]</div>
<p>We used the fact that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sum_{k=1}^K \eta_k^{\btheta}
&amp;= \sum_{k=1}^K \sum_{i=1}^n r_{k,i}^{\btheta}\\
&amp;= \sum_{i=1}^n \sum_{k=1}^K \P_{\btheta}[C_i = k|\bX_i = \bx_i]\\
&amp;= \sum_{i=1}^n 1\\ 
&amp;= n,
\end{align*}\]</div>
<p>since the conditional probability <span class="math notranslate nohighlight">\(\P_{\btheta}[C_i = k|\bX_i = \bx_i]\)</span> adds up to one when summed over <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>The “M” in M Step (and EM) stands for maximization, which here turns into minimization because of the use of the NLL.</p>
<p>To summarize, the EM algorithm works as follows in this case. Assume we have data points <span class="math notranslate nohighlight">\(\{\bx_i\}_{i=1}^n\)</span>, that we have fixed <span class="math notranslate nohighlight">\(K\)</span> and that we have some initial parameter estimate <span class="math notranslate nohighlight">\(\btheta^0 = (\bpi^0, \{\bp_k^0\}) \in \Theta\)</span> with strictly positive <span class="math notranslate nohighlight">\(\pi_k^0\)</span>s and <span class="math notranslate nohighlight">\(p_{k,m}^0\)</span>s. For <span class="math notranslate nohighlight">\(t = 0,1,\ldots, T-1\)</span> we compute for all <span class="math notranslate nohighlight">\(i \in [n]\)</span>, <span class="math notranslate nohighlight">\(k \in [K]\)</span>, and <span class="math notranslate nohighlight">\(m \in [M]\)</span></p>
<div class="math notranslate nohighlight">
\[
r_{k,i}^t
= \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
{\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
\quad
\text{(E Step)}
\]</div>
<div class="math notranslate nohighlight">
\[
\eta_{k,m}^t
= \sum_{i=1}^n x_{i,m} r_{k,i}^t
\quad
\text{and}
\quad
\eta_k^t 
= \sum_{i=1}^n r_{k,i}^t,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\pi_k^{t+1} = \frac{\eta_k^t}{n}
\quad
\text{and}
\quad 
p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}.
\quad
\text{(M Step)}
\]</div>
<p>Provided <span class="math notranslate nohighlight">\(\sum_{i=1}^n x_{i,m} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(m\)</span>, the <span class="math notranslate nohighlight">\(\eta_{k,m}^t\)</span>s and <span class="math notranslate nohighlight">\(\eta_k^t\)</span>s remain positive for all <span class="math notranslate nohighlight">\(t\)</span> and the algorithm is well-defined. The <em>EM Guarantee</em> stipulates that the NLL cannot deteriorate, although note that it does not guarantee convergence to a global minimum.</p>
<p>We implement the EM algorithm for mixtures of multivariate Bernoullis. For this purpose, we adapt our previous Naive Bayes routines. We also allow for the possibility of using Laplace smoothing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">responsibility</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
   
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
    <span class="n">score_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
       
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pi_k</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span> 
                             <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:]))</span>
    <span class="n">r_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">score_k</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">score_k</span><span class="p">)))</span>
        
    <span class="k">return</span> <span class="n">r_k</span>

<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">eta_km</span><span class="p">,</span> <span class="n">eta_k</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>

    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">eta_k</span><span class="p">)</span>
    <span class="n">pi_k</span> <span class="o">=</span> <span class="p">(</span><span class="n">eta_k</span><span class="o">+</span><span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">eta</span><span class="o">+</span><span class="n">K</span><span class="o">*</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">p_km</span> <span class="o">=</span> <span class="p">(</span><span class="n">eta_km</span><span class="o">+</span><span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">eta_k</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">beta</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span>
</pre></div>
</div>
</div>
</div>
<p>We implement the E and M Step next.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">em_bern</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">pi_0</span><span class="p">,</span> <span class="n">p_0</span><span class="p">,</span> <span class="n">maxiters</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    
    <span class="n">n</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">pi_k</span> <span class="o">=</span> <span class="n">pi_0</span>
    <span class="n">p_km</span> <span class="o">=</span> <span class="n">p_0</span>
        
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiters</span><span class="p">):</span>
    
        <span class="c1"># E Step</span>
        <span class="n">r_ki</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">r_ki</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">responsibility</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:])</span>
        
        <span class="c1"># M Step     </span>
        <span class="n">eta_km</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">M</span><span class="p">))</span>
        <span class="n">eta_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">r_ki</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">eta_k</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
                <span class="n">eta_km</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="n">r_ki</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span> 
        <span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span>
            <span class="n">eta_km</span><span class="p">,</span> <span class="n">eta_k</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span>   
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We test the algorithm on a very simple dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="n">n</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">pi_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">/</span><span class="n">K</span>
<span class="n">p_0</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">M</span><span class="p">))</span>

<span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">em_bern</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">pi_0</span><span class="p">,</span> <span class="n">p_0</span><span class="p">,</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.66500949 0.33499051]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">p_km</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0.74982646 0.74982646 0.99800266]
 [0.00496739 0.00496739 0.25487292]]
</pre></div>
</div>
</div>
</div>
<p>We compute the probability that the vector <span class="math notranslate nohighlight">\((0, 0, 1)\)</span> is in each cluster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">responsibility</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.32947702 0.67052298]
</pre></div>
</div>
</div>
</div>
<p><strong>CHAT &amp; LEARN</strong> The EM algorithm can sometimes get stuck in local optima. Ask your favorite AI chatbot to discuss strategies for initializing the EM algorithm to avoid this issue, such as using multiple random restarts or using the k-means algorithm for initialization. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open In Colab</a>) <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
<section id="clustering-handwritten-digits">
<h2><span class="section-number">6.4.3. </span>Clustering handwritten digits<a class="headerlink" href="#clustering-handwritten-digits" title="Link to this heading">#</a></h2>
<p>To give a more involved example, we use the MNIST dataset.</p>
<p>Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">Wikipedia</a> again:</p>
<blockquote>
<div><p>The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by “re-mixing” the samples from NIST’s original datasets. The creators felt that since NIST’s training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST’s training dataset, while the other half of the training set and the other half of the test set were taken from NIST’s testing dataset.</p>
</div></blockquote>
<p><strong>Figure:</strong> MNIST sample images (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:MnistExamples.png">Source</a>)</p>
<p><img alt="MNIST sample images" src="../Images/4b9b7aff5e0fc5aab0dbfcb205c470d7.png" data-original-src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We load it from PyTorch. The data can be accessed with <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html"><code class="docutils literal notranslate"><span class="pre">torchvision.datasets.MNIST</span></code></a>. The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html"><code class="docutils literal notranslate"><span class="pre">squeeze()</span></code></a> below removes the color dimension in the image, which is grayscale. The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html"><code class="docutils literal notranslate"><span class="pre">numpy()</span></code></a> converts the PyTorch tensors into NumPy arrays. See <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a> for details on the data loading. We will say more about PyTorch in a later chapter.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                       <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">mnist</span><span class="p">),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>We turn the grayscale images into a black-and-white images by rounding the pixels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">imgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>There are two common ways to write a <span class="math notranslate nohighlight">\(2\)</span>. Let’s see if a mixture of multivariate Bernoullis can find them. We extract the images labelled <span class="math notranslate nohighlight">\(2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mask</span> <span class="o">=</span> <span class="n">labels</span> <span class="o">==</span> <span class="mi">2</span>
<span class="n">imgs2</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">labels2</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The first image is the following.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">imgs2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray_r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/09f5ba1d22597b26a8db0ef902985cfc9e10b9c5d6781e9341a9055390573fe8.png" src="../Images/e8eea00af5868fb166365334b2072575.png" data-original-src="https://mmids-textbook.github.io/_images/09f5ba1d22597b26a8db0ef902985cfc9e10b9c5d6781e9341a9055390573fe8.png"/>
</div>
</div>
<p>Next, we transform the images into vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="n">imgs2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">imgs2</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We run the algorithm with <span class="math notranslate nohighlight">\(2\)</span> clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">pi_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">/</span><span class="n">K</span>
<span class="n">p_0</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">M</span><span class="p">))</span>

<span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">em_bern</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">pi_0</span><span class="p">,</span> <span class="n">p_0</span><span class="p">,</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[nan nan]
</pre></div>
</div>
</div>
</div>
<p>Uh-oh. Something went wrong. We encountered a numerical issue, underflow, which we discussed briefly previously. To confirm this, we run the code again but ask Python to warn us about it using <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.seterr.html"><code class="docutils literal notranslate"><span class="pre">numpy.seterr</span></code></a>. (By default, warnings are turned off in the book, but they can be reactivated using <a class="reference external" href="https://docs.python.org/3/library/warnings.html#warnings.resetwarnings"><code class="docutils literal notranslate"><span class="pre">warnings.resetwarnings</span></code></a>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">warnings</span><span class="o">.</span><span class="n">resetwarnings</span><span class="p">()</span>
<span class="n">old_settings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">seterr</span><span class="p">(</span><span class="nb">all</span><span class="o">=</span><span class="s1">'warn'</span><span class="p">)</span>

<span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">em_bern</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">pi_0</span><span class="p">,</span> <span class="n">p_0</span><span class="p">,</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>/var/folders/k0/7k0fxl7j54q4k8dyqnrc6sz00000gr/T/ipykernel_84428/861379570.py:10: RuntimeWarning: underflow encountered in exp
  r_k = np.exp(-score_k)/(np.sum(np.exp(-score_k)))
/var/folders/k0/7k0fxl7j54q4k8dyqnrc6sz00000gr/T/ipykernel_84428/861379570.py:10: RuntimeWarning: invalid value encountered in divide
  r_k = np.exp(-score_k)/(np.sum(np.exp(-score_k)))
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>When we compute the responsibilities</p>
<div class="math notranslate nohighlight">
\[
r_{k,i}^t
= \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
{\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
\]</div>
<p>we first compute the negative logarithm of each term in the numerator as we did in the Naive Bayes case. But then we apply the function <span class="math notranslate nohighlight">\(e^{-x}\)</span>, because this time we are not simply computing an optimal score. When all scores are high, this last step may result in underflow, that is, produces numbers so small that they get rounded down to zero by NumPy. Then the ratio defining <code class="docutils literal notranslate"><span class="pre">r_k</span></code> is not well-defined.</p>
<p>To deal with this, we introduce a technique called the log-sum-exp trick<span class="math notranslate nohighlight">\(\idx{log-sum-exp trick}\xdi\)</span> (with some help from ChatGPT). Consider the computation of a function of <span class="math notranslate nohighlight">\(\mathbf{a} = (a_1, \ldots, a_n)\)</span> of the form</p>
<div class="math notranslate nohighlight">
\[
h(\mathbf{a}) = \log \left( \sum_{i=1}^{n} e^{-a_i} \right).
\]</div>
<p>When the <span class="math notranslate nohighlight">\(a_i\)</span> values are large positive numbers, the terms <span class="math notranslate nohighlight">\(e^{-a_i}\)</span> can be so small that they underflow to zero. To counter this, the log-sum-exp trick involves a shift to bring these terms into a more favorable numerical range.</p>
<p>It proceeds as follows:</p>
<ol class="arabic">
<li><p>Identify the minimum value <span class="math notranslate nohighlight">\(M\)</span> among the <span class="math notranslate nohighlight">\(a_i\)</span>s</p>
<div class="math notranslate nohighlight">
\[
   M = \min\{a_1, a_2, \ldots, a_n\}.
   \]</div>
</li>
<li><p>Subtract <span class="math notranslate nohighlight">\(M\)</span> from each <span class="math notranslate nohighlight">\(a_i\)</span> before exponentiation</p>
<div class="math notranslate nohighlight">
\[
   \log \left( \sum_{i=1}^{n} e^{-a_i} \right) 
   = \log \left( e^{-M} \sum_{i=1}^{n} e^{- (a_i - M)} \right).
   \]</div>
</li>
<li><p>Rewrite using log properties</p>
<div class="math notranslate nohighlight">
\[
   = -M + \log \left( \sum_{i=1}^{n} e^{-(a_i - M)} \right).
   \]</div>
</li>
</ol>
<p>Why does this help with underflow? By subtracting <span class="math notranslate nohighlight">\(M\)</span>, the smallest value in the set, from each <span class="math notranslate nohighlight">\(a_i\)</span>:
(i) the largest term in <span class="math notranslate nohighlight">\(\{e^{-(a_i - M)} :  i = 1,\ldots,n\}\)</span> becomes <span class="math notranslate nohighlight">\(e^0 = 1\)</span>; and (ii)
all other terms are between 0 and 1, as they are exponentiations of nonpositive numbers. This manipulation avoids terms underflowing to zero because even very large values, when shifted by <span class="math notranslate nohighlight">\(M\)</span>, are less likely to hit the underflow threshold.</p>
<p>Here is an example. Imagine you have <span class="math notranslate nohighlight">\(\mathbf{a} = (1000, 1001, 1002)\)</span>.</p>
<ul class="simple">
<li><p>Direct computation: <span class="math notranslate nohighlight">\(e^{-1000}\)</span>, <span class="math notranslate nohighlight">\(e^{-1001}\)</span>, and <span class="math notranslate nohighlight">\(e^{-1002}\)</span> might all underflow to zero.</p></li>
<li><p>With the log-sum-exp trick: Subtract <span class="math notranslate nohighlight">\(M = 1000\)</span>, leading to <span class="math notranslate nohighlight">\(e^{0}\)</span>, <span class="math notranslate nohighlight">\(e^{-1}\)</span>, and <span class="math notranslate nohighlight">\(e^{-2}\)</span>, all meaningful, non-zero results that accurately contribute to the sum.</p></li>
</ul>
<p>We implement in NumPy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">log_sum_exp_trick</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">min_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">min_val</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">a</span> <span class="o">+</span> <span class="n">min_val</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We try it on a simple example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1001</span><span class="p">,</span> <span class="mi">1002</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We first attempt a direct computation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>/var/folders/k0/7k0fxl7j54q4k8dyqnrc6sz00000gr/T/ipykernel_84428/214275762.py:1: RuntimeWarning: underflow encountered in exp
  np.log(np.sum(np.exp(-a)))
/var/folders/k0/7k0fxl7j54q4k8dyqnrc6sz00000gr/T/ipykernel_84428/214275762.py:1: RuntimeWarning: divide by zero encountered in log
  np.log(np.sum(np.exp(-a)))
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>-inf
</pre></div>
</div>
</div>
</div>
<p>Predictly, we get an underflow error and a useless output.</p>
<p>Next, we try the log-sum-exp trick.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">log_sum_exp_trick</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>-999.5923940355556
</pre></div>
</div>
</div>
</div>
<p>This time we get an output which seems reasonable, something slightly larger than <span class="math notranslate nohighlight">\(-1000\)</span> as expected (Why?).</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>After this long – but important! – parenthesis, we return to the EM algorithm. We modify it by implementing the log-sum-exp trick in the subroutine <code class="docutils literal notranslate"><span class="pre">responsibility</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">responsibility</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
   
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
    <span class="n">score_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
       
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pi_k</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span> 
                             <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:]))</span>
    <span class="n">r_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">score_k</span> <span class="o">-</span> <span class="n">log_sum_exp_trick</span><span class="p">(</span><span class="n">score_k</span><span class="p">))</span>
            
    <span class="k">return</span> <span class="n">r_k</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We go back to the MNIST example with only the 2s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">em_bern</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">pi_0</span><span class="p">,</span> <span class="n">p_0</span><span class="p">,</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here are the parameters for one cluster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/140b9ccf2e31df7febe808141c26d248ff25b7748bcf7be623f2bd60365c407b.png" src="../Images/4e3661e0a1a8051341921aba94079c7b.png" data-original-src="https://mmids-textbook.github.io/_images/140b9ccf2e31df7febe808141c26d248ff25b7748bcf7be623f2bd60365c407b.png"/>
</div>
</div>
<p>Here is the other one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/123b40b50b0c6ee77988c143d5de20e5dc96e914b6fdf3634b015b34143479ee.png" src="../Images/372533fd8fad14df01f1cf71749a0141.png" data-original-src="https://mmids-textbook.github.io/_images/123b40b50b0c6ee77988c143d5de20e5dc96e914b6fdf3634b015b34143479ee.png"/>
</div>
</div>
<p>Now that the model is trained, we compute the probability that an example image is in each cluster. We use the first image in the dataset that we plotted earlier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">responsibility</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>array([1.00000000e+00, 5.09357087e-17])
</pre></div>
</div>
</div>
</div>
<p>It indeed identifies the second cluster as significantly more likely.</p>
<p><strong>TRY IT!</strong> In the MNIST example, as we have seen, the probabilities involved are extremely small and the responsibilities are close to <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. Implement a variant of EM, called hard EM, which replaces responsibilities with the one-hot encoding of the largest responsibility. Test it on the MNIST example again. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open In Colab</a>)</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The mixture of multivariate Bernoullis model is a simple example of a latent variable model. Ask your favorite AI chatbot to discuss more complex latent variable models, such as the variational autoencoder or the Gaussian process latent variable model, and their applications in unsupervised learning. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> In the mixture of multivariate Bernoullis model, the joint distribution is given by:</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[C = k, \mathbf{X} = \mathbf{x}]\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X} = \mathbf{x}|C = k] \mathbb{P}[C = k]\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[\mathbf{X} = \mathbf{x}|C = k] \mathbb{P}[C = k]\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{\mathbf{x}} \mathbb{P}[C = k, \mathbf{X} = \mathbf{x}]\)</span></p>
<p><strong>2</strong> The majorization-minimization principle states that:</p>
<p>a) If <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> majorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then a global minimum <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> of <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> satisfies <span class="math notranslate nohighlight">\(f(\mathbf{x}') \geq f(\mathbf{x})\)</span>.</p>
<p>b) If <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> majorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then a global minimum <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> of <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> satisfies <span class="math notranslate nohighlight">\(f(\mathbf{x}') \leq f(\mathbf{x})\)</span>.</p>
<p>c) If <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> minorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then a global minimum <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> of <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> satisfies <span class="math notranslate nohighlight">\(f(\mathbf{x}') \geq f(\mathbf{x})\)</span>.</p>
<p>d) If <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> minorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then a global minimum <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> of <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> satisfies <span class="math notranslate nohighlight">\(f(\mathbf{x}') \leq f(\mathbf{x})\)</span>.</p>
<p><strong>3</strong> In the EM algorithm for mixtures of multivariate Bernoullis, the M-step involves:</p>
<p>a) Updating the parameters <span class="math notranslate nohighlight">\(\pi_k\)</span> and <span class="math notranslate nohighlight">\(p_{k,m}\)</span></p>
<p>b) Computing the responsibilities <span class="math notranslate nohighlight">\(r_{k,i}^t\)</span></p>
<p>c) Minimizing the negative log-likelihood</p>
<p>d) Applying the log-sum-exp trick</p>
<p><strong>4</strong> The mixture of multivariate Bernoullis model is represented by the following graphical model:</p>
<p>a)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"gray"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"white"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="s2">"X"</span><span class="p">)</span>
</pre></div>
</div>
<p>b)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"white"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"gray"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="s2">"X"</span><span class="p">)</span>
</pre></div>
</div>
<p>c)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"gray"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"gray"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="s2">"X"</span><span class="p">)</span>
</pre></div>
</div>
<p>d)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"white"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"white"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="s2">"X"</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>5</strong> In the context of clustering, what is the interpretation of the responsibilities computed in the E-step of the EM algorithm?</p>
<p>a) They represent the distance of each data point to the cluster centers.</p>
<p>b) They indicate the probability of each data point belonging to each cluster.</p>
<p>c) They determine the optimal number of clusters.</p>
<p>d) They are used to initialize the cluster centers in the M-step.</p>
<p>Answer for 1: b. Justification: The text states, “<span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[C = k, \mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X} = \mathbf{x}|C = k] \mathbb{P}[C = k]\)</span>.”</p>
<p>Answer for 2: b. Justification: “Let <span class="math notranslate nohighlight">\(f: \mathbb{R}^d \to \mathbb{R}\)</span> and suppose <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> majorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> be a global minimizer of <span class="math notranslate nohighlight">\(U_{\mathbf{x}}(\mathbf{z})\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, provided it exists. Then <span class="math notranslate nohighlight">\(f(\mathbf{x}') \leq f(\mathbf{x})\)</span>.”</p>
<p>Answer for 3: a. Justification: In the summary of the EM algorithm, the M-step is described as updating the parameters: “<span class="math notranslate nohighlight">\(\pi_k^{t+1} = \frac{\eta_k^t}{n}\)</span> and <span class="math notranslate nohighlight">\(p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}\)</span>,” which require the responsibilities.</p>
<p>Answer for 4: b. Justification: The text states, “Mathematically, that corresponds to applying the law of total probability as we did previously. Further, we let the vertex for <span class="math notranslate nohighlight">\(X\)</span> be shaded to indicate that it is observed, while the vertex for <span class="math notranslate nohighlight">\(Y\)</span> is not shaded to indicate that it is not.”</p>
<p>Answer for 5: b. Justification: The text refers to responsibilities as “our estimate – under the current parameter – of the probability that the sample comes from cluster <span class="math notranslate nohighlight">\(k\)</span>.”</p>
</section>
&#13;

<h2><span class="section-number">6.4.1. </span>Mixtures<a class="headerlink" href="#mixtures" title="Link to this heading">#</a></h2>
<p>Mixtures<span class="math notranslate nohighlight">\(\idx{mixture}\xdi\)</span> are a natural way to define probability distributions. The basic idea is to consider a pair of random vectors <span class="math notranslate nohighlight">\((\bX,\bY)\)</span> and assume that <span class="math notranslate nohighlight">\(\bY\)</span> is unobserved. The effect on the observed vector <span class="math notranslate nohighlight">\(\bX\)</span> is that <span class="math notranslate nohighlight">\(\bY\)</span> is marginalized out. Indeed, by the law of total probability, for any <span class="math notranslate nohighlight">\(\bx \in \S_\bX\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
p_\bX(\bx)
&amp;= \P[\bX = \bx]\\
&amp;= \sum_{\by \in \S_\bY} \P[\bX=\bx|\bY=\by] \,\P[\bY=\by]\\
&amp;= \sum_{\by \in \S_\bY} p_{\bX|\bY}(\bx|\by) \,p_\bY(\by)
\end{align*}\]</div>
<p>where we used that the events <span class="math notranslate nohighlight">\(\{\bY=\by\}\)</span>, <span class="math notranslate nohighlight">\(\by \in \S_\bY\)</span>, form a partition of the probability space. We interpret this equation as defining <span class="math notranslate nohighlight">\(p_\bX(\bx)\)</span> as a convex combination – a mixture – of the distributions <span class="math notranslate nohighlight">\(p_{\bX|\bY}(\bx|\by)\)</span>, <span class="math notranslate nohighlight">\(\by \in \S_\bY\)</span>, with mixing weights <span class="math notranslate nohighlight">\(p_\bY(\by)\)</span>. In general, we need to specify the full conditional probability distribution (CPD): <span class="math notranslate nohighlight">\(p_{\bX|\bY}(\bx|\by), \forall \bx \in \S_{\bX}, \by \in \S_\bY\)</span>. But assuming that the mixing weights and/or CPD come from parametric families can help reduce the complexity of the model.</p>
<p>That can be represented in a digraph with a directed edge from a vertex for <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> to a vertex for <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. Further, we let the vertex for <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be shaded to indicate that it is observed, while the vertex for <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> is not shaded to indicate that it is not. Mathematically, that corresponds to applying the law of total probability as we did previously.</p>
<p><img alt="A mixture" src="../Images/d7c8f87d42f669398ac57f92c0a43388.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_mixture_networkx.png"/></p>
<p>In the parametric context, this gives rise to a fruitful approach to expanding distribution families. Suppose <span class="math notranslate nohighlight">\(\{p_{\btheta}:\btheta \in \Theta\}\)</span> is a parametric family of distributions. Let <span class="math notranslate nohighlight">\(K \geq 2\)</span>, <span class="math notranslate nohighlight">\(\btheta_1, \ldots, \btheta_K \in \Theta\)</span> and <span class="math notranslate nohighlight">\(\bpi = (\pi_1,\ldots,\pi_K) \in \Delta_K\)</span>. Suppose <span class="math notranslate nohighlight">\(Y \sim \mathrm{Cat}(\bpi)\)</span> and that the conditional distributions satisfy</p>
<div class="math notranslate nohighlight">
\[
p_{\bX|Y}(\bx|i)
= p_{\btheta_i}(\bx).
\]</div>
<p>We write this as <span class="math notranslate nohighlight">\(\bX|\{Y=i\} \sim p_{\btheta_i}\)</span>. Then we obtain the mixture model</p>
<div class="math notranslate nohighlight">
\[
p_{\bX}(\bx)
= \sum_{i=1}^K p_{\bX|Y}(\bx|i) \,p_Y(i)
= \sum_{i=1}^K \pi_i p_{\btheta_i}(\bx).
\]</div>
<p><strong>EXAMPLE:</strong> <strong>(Mixture of Multinomials)</strong> Let <span class="math notranslate nohighlight">\(n, m , K \geq 1\)</span>, <span class="math notranslate nohighlight">\(\bpi \in \Delta_K\)</span> and, for <span class="math notranslate nohighlight">\(i=1,\ldots,K\)</span>, <span class="math notranslate nohighlight">\(\mathbf{p}_i = (p_{i1},\ldots,p_{im}) \in \Delta_m\)</span>.
Suppose that <span class="math notranslate nohighlight">\(Y \sim \mathrm{Cat}(\bpi)\)</span> and that the conditional distributions are</p>
<div class="math notranslate nohighlight">
\[
\bX|\{Y=i\} \sim \mathrm{Mult}(n, \mathbf{p}_i).
\]</div>
<p>Then <span class="math notranslate nohighlight">\(\bX\)</span> is a mixture of multinomials. Its distribution is then</p>
<div class="math notranslate nohighlight">
\[
p_\bX(\bx)
= \sum_{i=1}^K \pi_i \frac{n!}{x_1!\cdots x_m!} \prod_{j=1}^m p_{ij}^{x_j}.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Next is an important continuous example.</p>
<p><strong>EXAMPLE:</strong> <strong>(Gaussian mixture model)</strong> <span class="math notranslate nohighlight">\(\idx{Gaussian mixture model}\xdi\)</span> For <span class="math notranslate nohighlight">\(i=1,\ldots,K\)</span>, let <span class="math notranslate nohighlight">\(\bmu_i\)</span> and <span class="math notranslate nohighlight">\(\bSigma_i\)</span> be the mean and covariance matrix of a multivariate Gaussian. Let <span class="math notranslate nohighlight">\(\bpi \in \Delta_K\)</span>. A Gaussian Mixture Model (GMM) is obtained as follows: take <span class="math notranslate nohighlight">\(Y \sim \mathrm{Cat}(\bpi)\)</span> and</p>
<div class="math notranslate nohighlight">
\[
\bX|\{Y=i\} \sim N_d(\bmu_i, \bSigma_i).
\]</div>
<p>Its probability density function (PDF) takes the form</p>
<div class="math notranslate nohighlight">
\[
f_\bX(\bx)
= \sum_{i=1}^K \pi_i \frac{1}{(2\pi)^{d/2} \,|\bSigma_i|^{1/2}}
\exp\left(-\frac{1}{2}(\mathbf{x} - \bmu_i)^T \bSigma_i^{-1} (\bx - \bmu_i)\right).
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We plot the density for means <span class="math notranslate nohighlight">\(\bmu_1 = (-2,-2)\)</span> and <span class="math notranslate nohighlight">\(\bmu_2 = (2,2)\)</span> and covariance matrices</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bSigma_1 = \begin{bmatrix} 1.0 &amp; 0 \\ 0 &amp; 1.0 \end{bmatrix}
\quad \text{and} \quad 
\bSigma_2 = \begin{bmatrix} \sigma_1^2 &amp; \rho \sigma_1 \sigma_2 \\ \rho \sigma_1 \sigma_2 &amp; \sigma_2^2 \end{bmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_1 = 1.5\)</span>, <span class="math notranslate nohighlight">\(\sigma_2 = 0.5\)</span> and <span class="math notranslate nohighlight">\(\rho = -0.75\)</span>. The mixing weights are <span class="math notranslate nohighlight">\(\pi_1 = 0.25\)</span> and <span class="math notranslate nohighlight">\(\pi_2 = 0.75\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span>

<span class="k">def</span> <span class="nf">gmm2_pdf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">mean1</span><span class="p">,</span> <span class="n">cov1</span><span class="p">,</span> <span class="n">pi1</span><span class="p">,</span> <span class="n">mean2</span><span class="p">,</span> <span class="n">cov2</span><span class="p">,</span> <span class="n">pi2</span><span class="p">):</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">Z1</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span>
        <span class="n">xy</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean1</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span>
        <span class="n">xy</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">mean2</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov2</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 
    <span class="k">return</span> <span class="n">pi1</span> <span class="o">*</span> <span class="n">Z1</span> <span class="o">+</span> <span class="n">pi2</span> <span class="o">*</span> <span class="n">Z2</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">start_point</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">stop_point</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">start_point</span><span class="p">,</span> <span class="n">stop_point</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">points</span><span class="p">)</span>

<span class="n">mean1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">])</span>
<span class="n">cov1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="n">pi1</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">mean2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>
<span class="n">cov2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.5</span> <span class="o">**</span> <span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.75</span> <span class="o">*</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">],</span> 
                 <span class="p">[</span><span class="o">-</span><span class="mf">0.75</span> <span class="o">*</span> <span class="mf">1.5</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span> <span class="o">**</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="n">pi2</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">gmm2_pdf</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">mean1</span><span class="p">,</span> <span class="n">cov1</span><span class="p">,</span> <span class="n">pi1</span><span class="p">,</span> <span class="n">mean2</span><span class="p">,</span> <span class="n">cov2</span><span class="p">,</span> <span class="n">pi2</span><span class="p">)</span>
<span class="n">mmids</span><span class="o">.</span><span class="n">make_surface_plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/d96dada4644d3611a7cfa6aeaa7c3378d8e8cca7a04c6ffbc22bff283cfaa596.png" src="../Images/810f47a1332c13f0595b65b28e44ab63.png" data-original-src="https://mmids-textbook.github.io/_images/d96dada4644d3611a7cfa6aeaa7c3378d8e8cca7a04c6ffbc22bff283cfaa596.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>In NumPy, as we have seen before, the module <a class="reference external" href="https://numpy.org/doc/stable/reference/random/index.html"><code class="docutils literal notranslate"><span class="pre">numpy.random</span></code></a> also provides a way to sample from mixture models by using <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.choice</span></code></a>.</p>
<p>For instance, we consider mixtures of multivariate Gaussians. We chage the notation slightly to track Python’s indexing. For <span class="math notranslate nohighlight">\(i=0,1\)</span>, we have a mean <span class="math notranslate nohighlight">\(\bmu_i \in \mathbb{R}^d\)</span> and a positive definite covariance matrix <span class="math notranslate nohighlight">\(\bSigma_i \in \mathbb{R}^{d \times d}\)</span>. We also have mixture weights <span class="math notranslate nohighlight">\(\phi_0, \phi_1 \in (0,1)\)</span> such that <span class="math notranslate nohighlight">\(\phi_0 + \phi_1 = 1\)</span>. Suppose we want to generate a total of <span class="math notranslate nohighlight">\(n\)</span> samples.</p>
<p>For each sample <span class="math notranslate nohighlight">\(j=1,\ldots, n\)</span>, independently from everything else:</p>
<ol class="arabic simple">
<li><p>We first pick a component <span class="math notranslate nohighlight">\(i \in \{0,1\}\)</span> at random according to the mixture weights, that is, <span class="math notranslate nohighlight">\(i=0\)</span> is chosen with probability <span class="math notranslate nohighlight">\(\phi_0\)</span> and <span class="math notranslate nohighlight">\(i=1\)</span> is chosen with probability <span class="math notranslate nohighlight">\(\phi_1\)</span>.</p></li>
<li><p>We generate a sample <span class="math notranslate nohighlight">\(\bX_j = (X_{j,1},\ldots,X_{j,d})\)</span> according to a multivariate Gaussian with mean <span class="math notranslate nohighlight">\(\bmu_i\)</span> and covariance <span class="math notranslate nohighlight">\(\bSigma_i\)</span>.</p></li>
</ol>
<p>This is straightforward to implement by using again <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.choice</span></code></a> to choose the component of each sample and <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.multivariate_normal</span></code></a> to generate multivariate Gaussians. For convenience, we will stack the means and covariances into one array with a new dimension. So, for instance, the covariance matrices will now be in a 3d-array, that is, an array with <span class="math notranslate nohighlight">\(3\)</span> indices. The first index corresponds to the component (here <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>).</p>
<p><strong>Figure:</strong> Three matrices (<a class="reference external" href="https://www.tensorflow.org/guide/tensor#basics">Source</a>)</p>
<p><img alt="Three matrices" src="../Images/e9a2eb3f0bbe5139202ee6636f55ede6.png" data-original-src="https://www.tensorflow.org/static/guide/images/tensor/3-axis_numpy.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>Figure:</strong> Three matrices stacked into a 3d-array (<a class="reference external" href="https://www.tensorflow.org/guide/tensor#basics">Source</a>)</p>
<p><img alt="Three matrices stacked into a tensor" src="../Images/a61cc745a3c58639bf7340b2af821416.png" data-original-src="https://www.tensorflow.org/static/guide/images/tensor/3-axis_front.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>The code is the following. It returns an <code class="docutils literal notranslate"><span class="pre">d</span></code> by <code class="docutils literal notranslate"><span class="pre">n</span></code> array <code class="docutils literal notranslate"><span class="pre">X</span></code>, where each row is a sample from a 2-component Gaussian mixture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">gmm2</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">phi0</span><span class="p">,</span> <span class="n">phi1</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="n">sigma0</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">):</span>
    
    <span class="n">phi</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">phi0</span><span class="p">,</span> <span class="n">phi1</span><span class="p">))</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">mu0</span><span class="p">,</span> <span class="n">mu1</span><span class="p">))</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">sigma0</span><span class="p">,</span><span class="n">sigma1</span><span class="p">))</span>
    
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
    <span class="n">component</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">phi</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span>
            <span class="n">mu</span><span class="p">[</span><span class="n">component</span><span class="p">[</span><span class="n">i</span><span class="p">],:],</span>
            <span class="n">sigma</span><span class="p">[</span><span class="n">component</span><span class="p">[</span><span class="n">i</span><span class="p">],:,:])</span>
    
    <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> Let us try it with following parameters. We first define the covariance matrices and show what happens when they are stacked into a 3d array (as is done within <code class="docutils literal notranslate"><span class="pre">gmm2</span></code>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">d</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">sigma0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]))</span> 
<span class="n">sigma0</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]))</span>
<span class="n">sigma1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">sigma0</span><span class="p">,</span><span class="n">sigma1</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="mi">0</span><span class="p">,:,:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[4.25 3.75]
 [3.75 4.25]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">sigma</span><span class="p">[</span><span class="mi">1</span><span class="p">,:,:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[2. 0.]
 [0. 2.]]
</pre></div>
</div>
</div>
</div>
<p>Then we define the rest of the parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> 

<span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="mf">5.</span>
<span class="n">phi0</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">phi1</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">mu0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="o">-</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">gmm2</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">phi0</span><span class="p">,</span> <span class="n">phi1</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="n">sigma0</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="n">sigma1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">'equal'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d22c61780e13a73ecc8364cd91c45d25951b009b2ea42174ad8643880d716bc6.png" src="../Images/22ec3b142976b6b20a0185e9e997fea9.png" data-original-src="https://mmids-textbook.github.io/_images/d22c61780e13a73ecc8364cd91c45d25951b009b2ea42174ad8643880d716bc6.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
&#13;

<h2><span class="section-number">6.4.2. </span>Example: Mixtures of multivariate Bernoullis and the EM algorithm<a class="headerlink" href="#example-mixtures-of-multivariate-bernoullis-and-the-em-algorithm" title="Link to this heading">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{C} = \{1, \ldots, K\}\)</span> be a collection of classes. Let <span class="math notranslate nohighlight">\(C\)</span> be a random variable taking values in <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> and, for <span class="math notranslate nohighlight">\(m=1, \ldots, M\)</span>, let <span class="math notranslate nohighlight">\(X_i\)</span> take values in <span class="math notranslate nohighlight">\(\{0,1\}\)</span>. Define <span class="math notranslate nohighlight">\(\pi_k = \P[C = k]\)</span> and <span class="math notranslate nohighlight">\(p_{k,m} = \P[X_m = 1|C = k]\)</span> for <span class="math notranslate nohighlight">\(m = 1,\ldots, M\)</span>. We denote by <span class="math notranslate nohighlight">\(\bX = (X_1, \ldots, X_M)\)</span> the corresponding vector of <span class="math notranslate nohighlight">\(X_i\)</span>’s and assume that the entries are conditionally independent given <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p>However, we assume this time that <span class="math notranslate nohighlight">\(C\)</span> itself is <em>not observed</em>. So the resulting joint distribution is the mixture</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[\bX = \bx]
&amp;= \sum_{k=1}^K \P[C = k, \bX = \bx]\\
&amp;= \sum_{k=1}^K \P[\bX = \bx|C=k] \,\P[C=k]\\
&amp;= \sum_{k=1}^K \pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}.
\end{align*}\]</div>
<p>Graphically, this is the same are the Naive Bayes model, except that <span class="math notranslate nohighlight">\(C\)</span> is not observed and therefore is not shaded.</p>
<p><img alt="Mixture of multivariate Bernoullis" src="../Images/c51352d9d6ab19d2f9e73f898c25b648.png" data-original-src="https://mmids-textbook.github.io/_images/dgm-em-networkx.png"/></p>
<p>This type of model is useful in particular for clustering tasks, where the <span class="math notranslate nohighlight">\(c_k\)</span>s can be thought of as different clusters. Similarly to what we did in the previous section, our goal is to infer the parameters from samples and then predict the class of an old or new sample given its features. The main – substantial – difference is that the true labels of the samples are not observed. As we will see, that complicates the task considerably.</p>
<p><strong>Model fitting</strong> We first fit the model from training data <span class="math notranslate nohighlight">\(\{\bx_i\}_{i=1}^n\)</span>. Recall that the corresponding class labels <span class="math notranslate nohighlight">\(c_i\)</span>s are not observed. In this type of model, they are referred to as hidden or latent variables and we will come back to their inference below.</p>
<p>We would like to use maximum likelihood estimation, that is, maximize the probability of observing the data</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i\})
= \prod_{i=1}^n \left( \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
\]</div>
<p>As usual, we assume that the samples are independent and identically distributed. Consider the negative log-likelihood (NLL)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L_n(\bpi, \{\bp_k\}; \{\bx_i\})
&amp;= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
\end{align*}\]</div>
<p>Already, we see that things are potentially more difficult than they were in the supervised (or fully observed) case. The NLL does not decompose into a sum of terms depending on different sets of parameters.</p>
<p>At this point, one could fall back on the field of optimization and use a gradient-based method to minimize the NLL. Indeed that is an option, although note that one must be careful to account for the constrained nature of the problem (i.e., the parameters sum to one). There is a vast array of constrained optimization techniques suited for this task.</p>
<p>Instead a more popular approach in this context, the EM algorithm, is based on the general principle of majorization-minimization, which we have encountered implicitly in the <span class="math notranslate nohighlight">\(k\)</span>-means algorithm and the convergence proof of gradient descent in the smooth case. We detail this important principle in the next subsection before returning to model fitting in mixtures.</p>
<p><strong>Majorization-minimization</strong> <span class="math notranslate nohighlight">\(\idx{majorization-minimization}\xdi\)</span> Here is a deceptively simple, yet powerful observation. Suppose we want to minimize a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span>. Finding a local minimum of <span class="math notranslate nohighlight">\(f\)</span> may not be easy. But imagine that for each <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> we have a surrogate function <span class="math notranslate nohighlight">\(U_{\mathbf{x}} : \mathbb{R}^d \to \mathbb{R}\)</span> that (1) dominates <span class="math notranslate nohighlight">\(f\)</span> in the following sense</p>
<div class="math notranslate nohighlight">
\[
U_\mathbf{x}(\mathbf{z}) \geq f(\mathbf{z}), \quad \forall \mathbf{z} \in \mathbb{R}^d 
\]</div>
<p>and (2) equals <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></p>
<div class="math notranslate nohighlight">
\[
U_\mathbf{x}(\mathbf{x}) = f(\mathbf{x}).
\]</div>
<p>We say that <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span> majorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Then we prove in the next lemma that <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span> can be used to make progress towards minimizing <span class="math notranslate nohighlight">\(f\)</span>, that is, find a point <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> such that <span class="math notranslate nohighlight">\(f(\mathbf{x}') \leq f(\mathbf{x})\)</span>. If in addition <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span> is easier to minimize than <span class="math notranslate nohighlight">\(f\)</span> itself, say because an explicit minimum can be computed, then this observation naturally leads to an iterative algorithm.</p>
<p><img alt="A majorizing function (with help from ChatGPT; inspired by Source)" src="../Images/c05a4e1add7058b646f07a53f61281a2.png" data-original-src="https://mmids-textbook.github.io/_images/majorize.png"/></p>
<p><strong>LEMMA</strong> <strong>(Majorization-Minimization)</strong> <span class="math notranslate nohighlight">\(\idx{majorization-minimization lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> and suppose <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> majorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> be a global minimum of <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}') \leq f(\mathbf{x}).
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Indeed</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}') \leq U_\mathbf{x}(\mathbf{x}') \leq U_{\mathbf{x}}(\mathbf{x}) = f(\mathbf{x}),
\]</div>
<p>where the first inequality follows from the domination property of <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span>, the second inequality follows from the fact that <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> is a global minimum  of <span class="math notranslate nohighlight">\(U_\mathbf{x}\)</span> and the equality follows from the fact that <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> equals <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We have already encountered this idea.</p>
<p><strong>EXAMPLE:</strong> <strong>(Minimizing a smooth function)</strong> Let <span class="math notranslate nohighlight">\(f : \mathbb{R}^d \to \mathbb{R}\)</span> be <span class="math notranslate nohighlight">\(L\)</span>-smooth. By the <em>Quadratic Bound for Smooth Functions</em>, for all <span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{z} \in \mathbb{R}^d\)</span> it holds that</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{z})
\leq U_{\mathbf{x}}(\mathbf{z})
:= f(\mathbf{x}) 
+ \nabla f(\mathbf{x})^T(\mathbf{z} - \mathbf{x})
+ \frac{L}{2} \|\mathbf{z} - \mathbf{x}\|^2.
\]</div>
<p>By showing that <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> is minimized at <span class="math notranslate nohighlight">\(\mathbf{z} = \mathbf{x} - (1/L)\nabla f(\mathbf{x})\)</span>, we previously obtained the descent guarantee</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x} - (1/L)\nabla f(\mathbf{x}))
\leq f(\mathbf{x}) - \frac{1}{2 L} \|\nabla f(\mathbf{x})\|^2
\]</div>
<p>for gradient descent, which played a central role in the analysis of its convergence<span class="math notranslate nohighlight">\(\idx{convergence analysis}\xdi\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(<span class="math notranslate nohighlight">\(k\)</span>-means)</strong> <span class="math notranslate nohighlight">\(\idx{Lloyd's algorithm}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{x}_1,\ldots,\mathbf{x}_n\)</span> be <span class="math notranslate nohighlight">\(n\)</span> vectors in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>. One way to formulate the <span class="math notranslate nohighlight">\(k\)</span>-means clustering problem is as the minimization of</p>
<div class="math notranslate nohighlight">
\[
f(\bmu_1,\ldots,\bmu_K)
= \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i - \bmu_j\|^2
\]</div>
<p>over the centers <span class="math notranslate nohighlight">\(\bmu_1,\ldots,\bmu_K\)</span>, where recall that <span class="math notranslate nohighlight">\([K] = \{1,\ldots,K\}\)</span>. For fixed <span class="math notranslate nohighlight">\(\bmu_1,\ldots,\bmu_K\)</span> and <span class="math notranslate nohighlight">\(\mathbf{m} = (\bmu_1,\ldots,\bmu_K)\)</span>, define</p>
<div class="math notranslate nohighlight">
\[
c_\mathbf{m}(i) \in \arg\min\left\{\|\mathbf{x}_i - \bmu_j\|^2 \ :\ j \in [K]\right\},
\quad i=1,\ldots,n
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
U_\mathbf{m}(\blambda_1,\ldots,\blambda_K) 
= \sum_{i=1}^n \|\mathbf{x}_i - \blambda_{c_\mathbf{m}(i)}\|^2
\]</div>
<p>for <span class="math notranslate nohighlight">\(\blambda_1,\ldots,\blambda_K \in \mathbb{R}^d\)</span>. That is, we fix the optimal cluster assignments under <span class="math notranslate nohighlight">\(\bmu_1,\ldots,\bmu_K\)</span> and then vary the centers.</p>
<p>We claim that <span class="math notranslate nohighlight">\(U_\mathbf{m}\)</span> is majorizing <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\bmu_1,\ldots,\bmu_K\)</span>. Indeed</p>
<div class="math notranslate nohighlight">
\[
f(\blambda_1,\ldots,\blambda_K)
= \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i - \blambda_j\|^2
\leq \sum_{i=1}^n \|\mathbf{x}_i - \blambda_{c_\mathbf{m}(i)}\|^2
= U_\mathbf{m}(\blambda_1,\ldots,\blambda_K)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
f(\bmu_1,\ldots,\bmu_K)
= \sum_{i=1}^n \min_{j \in [K]} \|\mathbf{x}_i - \bmu_j\|^2
= \sum_{i=1}^n \|\mathbf{x}_i - \bmu_{c_\mathbf{m}(i)}\|^2
= U_\mathbf{m}(\bmu_1,\ldots,\bmu_K).
\]</div>
<p>Moreover <span class="math notranslate nohighlight">\(U_\mathbf{m}(\blambda_1,\ldots,\blambda_K)\)</span> is easy to minimize. We showed previously that the optimal representatives are</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\mu}_j' = \frac{1}{|C_j|} \sum_{i\in C_j} \mathbf{x}_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(C_j = \{i : c_\mathbf{m}(i) = j\}\)</span>.</p>
<p>The <em>Majorization-Minimization Lemma</em> implies that</p>
<div class="math notranslate nohighlight">
\[
f(\bmu_1',\ldots,\bmu_K')
\leq f(\bmu_1,\ldots,\bmu_K).
\]</div>
<p>This argument is equivalent to our previous analysis of the <span class="math notranslate nohighlight">\(k\)</span>-means algorithm. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The mixture of multivariate Bernoullis model assumes a fixed number of clusters. Ask your favorite AI chatbot to discuss Bayesian nonparametric extensions of this model, such as the Dirichlet process mixture model, which can automatically infer the number of clusters from the data. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><strong>EM algorithm</strong> The <a class="reference external" href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation-Maximization (EM) algorithm</a><span class="math notranslate nohighlight">\(\idx{EM algorithm}\xdi\)</span> is an instantiation of the majorization-minimization principle that applies widely to parameter estimation of mixtures. Here we focus on the mixture of multivariate Bernoullis.</p>
<p>Recall that the objective to be minimized is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L_n(\bpi, \{\bp_k\}; \{\bx_i\})
&amp;= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \pi_{k} \prod_{m=1}^M p_{k, m}^{x_{i,m}} (1-p_{k, m})^{1-x_{i,m}}\right).
\end{align*}\]</div>
<p>To simplify the notation and highlight the general idea, we let <span class="math notranslate nohighlight">\(\btheta = (\bpi, \{\bp_k\})\)</span>, denote by <span class="math notranslate nohighlight">\(\Theta\)</span> the set of allowed values for <span class="math notranslate nohighlight">\(\btheta\)</span>, and use <span class="math notranslate nohighlight">\(\P_{\btheta}\)</span> to indicate that probabilities are computed under the parameters <span class="math notranslate nohighlight">\(\btheta\)</span>. We also return to the description of the model in terms of the unobserved latent variables <span class="math notranslate nohighlight">\(\{C_i\}\)</span>. That is, we write the NLL as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L_n(\btheta)
&amp;= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \P_{\btheta}[\bX_i = \bx_i|C_i = k] \,\P_{\btheta}[C_i = k]\right)\\
&amp;= - \sum_{i=1}^n \log \left( \sum_{k=1}^K \P_{\btheta}[\bX_i = \bx_i, C_i = k] \right).
\end{align*}\]</div>
<p>To derive a majorizing function, we use the convexity of the negative logarithm. Indeed</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial z}[- \log z]
= - \frac{1}{z}
\quad
\text{and}
\quad
\frac{\partial^2}{\partial^2 z}[- \log z]
= \frac{1}{z^2} &gt; 0,
\quad
\forall z &gt; 0.
\]</div>
<p>The first step of the construction is not obvious – it just works. For each <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>, we let <span class="math notranslate nohighlight">\(r_{k,i}^{\btheta}\)</span>, <span class="math notranslate nohighlight">\(k=1,\ldots,K\)</span>, be a strictly positive probability distribution over <span class="math notranslate nohighlight">\([K]\)</span>. In other words, it defines a convex combination for every <span class="math notranslate nohighlight">\(i\)</span>. Then we use convexity to obtain the upper bound</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
L_n(\tilde\btheta)
&amp;= - \sum_{i=1}^n \log \left( \sum_{k=1}^K r_{k,i}^{\btheta} \frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
&amp;\leq - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right),
\end{align*}\]</div>
<p>which holds for any <span class="math notranslate nohighlight">\(\tilde\btheta = (\tilde\bpi, \{\tilde\bp_k\}) \in \Theta\)</span>.</p>
<p>We choose</p>
<div class="math notranslate nohighlight">
\[
r_{k,i}^{\btheta} = \P_{\btheta}[C_i = k|\bX_i = \bx_i]
\]</div>
<p>(which for the time being we assume is strictly positive) and we denote the right-hand side of the inequality by <span class="math notranslate nohighlight">\(Q_{n}(\tilde\btheta|\btheta)\)</span> (as a function of <span class="math notranslate nohighlight">\(\tilde\btheta\)</span>).</p>
<p>We make two observations.</p>
<p>1- <em>Dominating property</em>: For any <span class="math notranslate nohighlight">\(\tilde\btheta \in \Theta\)</span>, the inequality above implies immediately that <span class="math notranslate nohighlight">\(L_n(\tilde\btheta) \leq Q_n(\tilde\btheta|\btheta)\)</span>.</p>
<p>2- <em>Equality at <span class="math notranslate nohighlight">\(\btheta\)</span></em>: At <span class="math notranslate nohighlight">\(\tilde\btheta = \btheta\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Q_n(\btheta|\btheta) 
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\btheta}[C_i = k | \bX_i = \bx_i]
\P_{\btheta}[\bX_i = \bx_i]}{r_{k,i}^{\btheta}} \right)\\
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log
\P_{\btheta}[\bX_i = \bx_i]\\
&amp;= - \sum_{i=1}^n \log
\P_{\btheta}[\bX_i = \bx_i]\\
&amp;= L_n(\btheta).
\end{align*}\]</div>
<p>The two properties above show that <span class="math notranslate nohighlight">\(Q_n(\tilde\btheta|\btheta)\)</span>, as a function of <span class="math notranslate nohighlight">\(\tilde\btheta\)</span>, majorizes <span class="math notranslate nohighlight">\(L_n\)</span> at <span class="math notranslate nohighlight">\(\btheta\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(EM Guarantee)</strong> <span class="math notranslate nohighlight">\(\idx{EM guarantee}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\btheta^*\)</span> be a global minimizer of <span class="math notranslate nohighlight">\(Q_n(\tilde\btheta|\btheta)\)</span> as a function of <span class="math notranslate nohighlight">\(\tilde\btheta\)</span>, provided it exists. Then</p>
<div class="math notranslate nohighlight">
\[
L_n(\btheta^*) \leq L_n(\btheta).
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> The result follows directly from the <em>Majorization-Minimization Lemma</em>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>What have we gained from this? As we mentioned before, using the <em>Majorization-Minimization Lemma</em> makes sense if <span class="math notranslate nohighlight">\(Q_n\)</span> is easier to minimize than <span class="math notranslate nohighlight">\(L_n\)</span> itself. Let us see why that is the case here.</p>
<p><em>E Step:</em> The function <span class="math notranslate nohighlight">\(Q_n\)</span> naturally decomposes into two terms</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
Q_n(\tilde\btheta|\btheta)
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\frac{\P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]}{r_{k,i}^{\btheta}} \right)\\
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]
+ \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log r_{k,i}^{\btheta}.
\end{align*}\]</div>
<p>Because <span class="math notranslate nohighlight">\(r_{k,i}^{\btheta}\)</span> depends on <span class="math notranslate nohighlight">\(\btheta\)</span> <em>but not <span class="math notranslate nohighlight">\(\tilde\btheta\)</span></em>, the second term is irrelevant to the opimization with respect to <span class="math notranslate nohighlight">\(\tilde\btheta\)</span>.</p>
<p>The first term above can be written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp; - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \P_{\tilde\btheta}[\bX_i = \bx_i, C_i = k]\\
&amp;= - \sum_{i=1}^n \sum_{k=1}^K r_{k,i}^{\btheta} \log \left(\tilde{\pi}_{k} \prod_{m=1}^M \tilde{p}_{k, m}^{x_{i,m}} (1-\tilde{p}_{k,m})^{1-x_{i,m}}\right)\\
&amp;= - \sum_{k=1}^K \eta_k^{\btheta} \log \tilde{\pi}_k - \sum_{k=1}^K \sum_{m=1}^M [\eta_{k,m}^{\btheta} \log \tilde{p}_{k,m} + (\eta_k^{\btheta}-\eta_{k,m}^{\btheta}) \log (1-\tilde{p}_{k,m})],
\end{align*}\]</div>
<p>where we defined, for <span class="math notranslate nohighlight">\(k=1,\ldots,K\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\eta_{k,m}^{\btheta}
= \sum_{i=1}^n x_{i,m} r_{k,i}^{\btheta}
\quad
\text{and}
\quad
\eta_k^{\btheta} 
= \sum_{i=1}^n r_{k,i}^{\btheta}.
\]</div>
<p>Here comes the key observation: this last expression is essentially the same as the NLL for the fully observed Naive Bayes model, except that the terms <span class="math notranslate nohighlight">\(\mathbf{1}_{\{c_i = k\}}\)</span> are replaced by <span class="math notranslate nohighlight">\(r_{k,i}^{\btheta}\)</span>. If <span class="math notranslate nohighlight">\(\btheta\)</span> is our current estimate of the parameters, then the quantity <span class="math notranslate nohighlight">\(r_{k,i}^{\btheta} = \P_{\btheta}[C_i = k|\bX_i = \bx_i]\)</span> is our estimate – under the current parameter <span class="math notranslate nohighlight">\(\btheta\)</span> – of the probability that the sample <span class="math notranslate nohighlight">\(\bx_i\)</span> comes from cluster <span class="math notranslate nohighlight">\(k\)</span>. We have previously computed <span class="math notranslate nohighlight">\(r_{k,i}^{\btheta}\)</span> for prediction under the Naive Bayes model. We showed there that</p>
<div class="math notranslate nohighlight">
\[
r_{k,i}^{\btheta}
= \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_{i,m}} (1-p_{k,m})^{1-x_{i,m}}}
{\sum_{k'=1}^K \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_{i,m}} (1-p_{k',m})^{1-x_{i,m}}},
\]</div>
<p>which in this new context is referred to as the responsibility that cluster <span class="math notranslate nohighlight">\(k\)</span> takes for data point <span class="math notranslate nohighlight">\(i\)</span>. So we can interpret the expression above as follows: the variables <span class="math notranslate nohighlight">\(\mathbf{1}_{\{c_i = k\}}\)</span> are not observed here, but we have estimated their conditional probability distribution given the observed data <span class="math notranslate nohighlight">\(\{\bx_i\}\)</span>, and we are taking an expectation with respect to that distribution instead.</p>
<p>The “E” in E Step (and EM) stands for “expectation”, which refers to using a surrogate function that is essentially an expected NLL.</p>
<p><em>M Step:</em> In any case, from a practical point of view, minimizing <span class="math notranslate nohighlight">\(Q_n(\tilde\btheta|\btheta)\)</span> over <span class="math notranslate nohighlight">\(\tilde\btheta\)</span> turns out to be a variant of fitting a Naive Bayes model – and the upshot to all this is that there is a straightforward formula for that! Recall that this happens because, the NLL in the Naive Bayes model decomposes: it naturally breaks up into terms that depend on separate sets of parameters, each of which can be optimized with a closed-form expression. The same happens with <span class="math notranslate nohighlight">\(Q_n\)</span> as should be clear from the derivation.</p>
<p>Adapting our previous calculations for fitting a Naive Bayes model, we get that <span class="math notranslate nohighlight">\(Q_n(\tilde\btheta|\btheta)\)</span> is minimized at</p>
<div class="math notranslate nohighlight">
\[
\pi_k^* = \frac{\eta_k^{\btheta}}{n}
\quad
\text{and}
\quad 
p_{k,m}^* = \frac{\eta_{k,m}^{\btheta}}{\eta_k^{\btheta}}
\quad
\forall k \in [K], m \in [M].
\]</div>
<p>We used the fact that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sum_{k=1}^K \eta_k^{\btheta}
&amp;= \sum_{k=1}^K \sum_{i=1}^n r_{k,i}^{\btheta}\\
&amp;= \sum_{i=1}^n \sum_{k=1}^K \P_{\btheta}[C_i = k|\bX_i = \bx_i]\\
&amp;= \sum_{i=1}^n 1\\ 
&amp;= n,
\end{align*}\]</div>
<p>since the conditional probability <span class="math notranslate nohighlight">\(\P_{\btheta}[C_i = k|\bX_i = \bx_i]\)</span> adds up to one when summed over <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>The “M” in M Step (and EM) stands for maximization, which here turns into minimization because of the use of the NLL.</p>
<p>To summarize, the EM algorithm works as follows in this case. Assume we have data points <span class="math notranslate nohighlight">\(\{\bx_i\}_{i=1}^n\)</span>, that we have fixed <span class="math notranslate nohighlight">\(K\)</span> and that we have some initial parameter estimate <span class="math notranslate nohighlight">\(\btheta^0 = (\bpi^0, \{\bp_k^0\}) \in \Theta\)</span> with strictly positive <span class="math notranslate nohighlight">\(\pi_k^0\)</span>s and <span class="math notranslate nohighlight">\(p_{k,m}^0\)</span>s. For <span class="math notranslate nohighlight">\(t = 0,1,\ldots, T-1\)</span> we compute for all <span class="math notranslate nohighlight">\(i \in [n]\)</span>, <span class="math notranslate nohighlight">\(k \in [K]\)</span>, and <span class="math notranslate nohighlight">\(m \in [M]\)</span></p>
<div class="math notranslate nohighlight">
\[
r_{k,i}^t
= \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
{\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
\quad
\text{(E Step)}
\]</div>
<div class="math notranslate nohighlight">
\[
\eta_{k,m}^t
= \sum_{i=1}^n x_{i,m} r_{k,i}^t
\quad
\text{and}
\quad
\eta_k^t 
= \sum_{i=1}^n r_{k,i}^t,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\pi_k^{t+1} = \frac{\eta_k^t}{n}
\quad
\text{and}
\quad 
p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}.
\quad
\text{(M Step)}
\]</div>
<p>Provided <span class="math notranslate nohighlight">\(\sum_{i=1}^n x_{i,m} &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(m\)</span>, the <span class="math notranslate nohighlight">\(\eta_{k,m}^t\)</span>s and <span class="math notranslate nohighlight">\(\eta_k^t\)</span>s remain positive for all <span class="math notranslate nohighlight">\(t\)</span> and the algorithm is well-defined. The <em>EM Guarantee</em> stipulates that the NLL cannot deteriorate, although note that it does not guarantee convergence to a global minimum.</p>
<p>We implement the EM algorithm for mixtures of multivariate Bernoullis. For this purpose, we adapt our previous Naive Bayes routines. We also allow for the possibility of using Laplace smoothing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">responsibility</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
   
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
    <span class="n">score_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
       
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pi_k</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span> 
                             <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:]))</span>
    <span class="n">r_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">score_k</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">score_k</span><span class="p">)))</span>
        
    <span class="k">return</span> <span class="n">r_k</span>

<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">eta_km</span><span class="p">,</span> <span class="n">eta_k</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>

    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">eta_k</span><span class="p">)</span>
    <span class="n">pi_k</span> <span class="o">=</span> <span class="p">(</span><span class="n">eta_k</span><span class="o">+</span><span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">eta</span><span class="o">+</span><span class="n">K</span><span class="o">*</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">p_km</span> <span class="o">=</span> <span class="p">(</span><span class="n">eta_km</span><span class="o">+</span><span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">eta_k</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span><span class="o">+</span><span class="mi">2</span><span class="o">*</span><span class="n">beta</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span>
</pre></div>
</div>
</div>
</div>
<p>We implement the E and M Step next.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">em_bern</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">pi_0</span><span class="p">,</span> <span class="n">p_0</span><span class="p">,</span> <span class="n">maxiters</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    
    <span class="n">n</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">pi_k</span> <span class="o">=</span> <span class="n">pi_0</span>
    <span class="n">p_km</span> <span class="o">=</span> <span class="n">p_0</span>
        
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">maxiters</span><span class="p">):</span>
    
        <span class="c1"># E Step</span>
        <span class="n">r_ki</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">n</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">r_ki</span><span class="p">[:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">responsibility</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:])</span>
        
        <span class="c1"># M Step     </span>
        <span class="n">eta_km</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">M</span><span class="p">))</span>
        <span class="n">eta_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">r_ki</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">eta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">eta_k</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
                <span class="n">eta_km</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="n">r_ki</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span> 
        <span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span>
            <span class="n">eta_km</span><span class="p">,</span> <span class="n">eta_k</span><span class="p">,</span> <span class="n">eta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
        
    <span class="k">return</span> <span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span>   
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We test the algorithm on a very simple dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="n">n</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">pi_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">/</span><span class="n">K</span>
<span class="n">p_0</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">M</span><span class="p">))</span>

<span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">em_bern</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">pi_0</span><span class="p">,</span> <span class="n">p_0</span><span class="p">,</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.66500949 0.33499051]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">p_km</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0.74982646 0.74982646 0.99800266]
 [0.00496739 0.00496739 0.25487292]]
</pre></div>
</div>
</div>
</div>
<p>We compute the probability that the vector <span class="math notranslate nohighlight">\((0, 0, 1)\)</span> is in each cluster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">responsibility</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">x_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.32947702 0.67052298]
</pre></div>
</div>
</div>
</div>
<p><strong>CHAT &amp; LEARN</strong> The EM algorithm can sometimes get stuck in local optima. Ask your favorite AI chatbot to discuss strategies for initializing the EM algorithm to avoid this issue, such as using multiple random restarts or using the k-means algorithm for initialization. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open In Colab</a>) <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
&#13;

<h2><span class="section-number">6.4.3. </span>Clustering handwritten digits<a class="headerlink" href="#clustering-handwritten-digits" title="Link to this heading">#</a></h2>
<p>To give a more involved example, we use the MNIST dataset.</p>
<p>Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">Wikipedia</a> again:</p>
<blockquote>
<div><p>The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by “re-mixing” the samples from NIST’s original datasets. The creators felt that since NIST’s training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST’s training dataset, while the other half of the training set and the other half of the test set were taken from NIST’s testing dataset.</p>
</div></blockquote>
<p><strong>Figure:</strong> MNIST sample images (<a class="reference external" href="https://commons.wikimedia.org/wiki/File:MnistExamples.png">Source</a>)</p>
<p><img alt="MNIST sample images" src="../Images/4b9b7aff5e0fc5aab0dbfcb205c470d7.png" data-original-src="https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We load it from PyTorch. The data can be accessed with <a class="reference external" href="https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html"><code class="docutils literal notranslate"><span class="pre">torchvision.datasets.MNIST</span></code></a>. The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html"><code class="docutils literal notranslate"><span class="pre">squeeze()</span></code></a> below removes the color dimension in the image, which is grayscale. The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html"><code class="docutils literal notranslate"><span class="pre">numpy()</span></code></a> converts the PyTorch tensors into NumPy arrays. See <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a> for details on the data loading. We will say more about PyTorch in a later chapter.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">mnist</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                       <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">mnist</span><span class="p">),</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">imgs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_loader</span><span class="p">))</span>
<span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>We turn the grayscale images into a black-and-white images by rounding the pixels.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">imgs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>There are two common ways to write a <span class="math notranslate nohighlight">\(2\)</span>. Let’s see if a mixture of multivariate Bernoullis can find them. We extract the images labelled <span class="math notranslate nohighlight">\(2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mask</span> <span class="o">=</span> <span class="n">labels</span> <span class="o">==</span> <span class="mi">2</span>
<span class="n">imgs2</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">labels2</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The first image is the following.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">imgs2</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'gray_r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/09f5ba1d22597b26a8db0ef902985cfc9e10b9c5d6781e9341a9055390573fe8.png" src="../Images/e8eea00af5868fb166365334b2072575.png" data-original-src="https://mmids-textbook.github.io/_images/09f5ba1d22597b26a8db0ef902985cfc9e10b9c5d6781e9341a9055390573fe8.png"/>
</div>
</div>
<p>Next, we transform the images into vectors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="n">imgs2</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">imgs2</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We run the algorithm with <span class="math notranslate nohighlight">\(2\)</span> clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">n</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">pi_0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">/</span><span class="n">K</span>
<span class="n">p_0</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">K</span><span class="p">,</span><span class="n">M</span><span class="p">))</span>

<span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">em_bern</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">pi_0</span><span class="p">,</span> <span class="n">p_0</span><span class="p">,</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[nan nan]
</pre></div>
</div>
</div>
</div>
<p>Uh-oh. Something went wrong. We encountered a numerical issue, underflow, which we discussed briefly previously. To confirm this, we run the code again but ask Python to warn us about it using <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.seterr.html"><code class="docutils literal notranslate"><span class="pre">numpy.seterr</span></code></a>. (By default, warnings are turned off in the book, but they can be reactivated using <a class="reference external" href="https://docs.python.org/3/library/warnings.html#warnings.resetwarnings"><code class="docutils literal notranslate"><span class="pre">warnings.resetwarnings</span></code></a>.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">warnings</span><span class="o">.</span><span class="n">resetwarnings</span><span class="p">()</span>
<span class="n">old_settings</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">seterr</span><span class="p">(</span><span class="nb">all</span><span class="o">=</span><span class="s1">'warn'</span><span class="p">)</span>

<span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">em_bern</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">pi_0</span><span class="p">,</span> <span class="n">p_0</span><span class="p">,</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>/var/folders/k0/7k0fxl7j54q4k8dyqnrc6sz00000gr/T/ipykernel_84428/861379570.py:10: RuntimeWarning: underflow encountered in exp
  r_k = np.exp(-score_k)/(np.sum(np.exp(-score_k)))
/var/folders/k0/7k0fxl7j54q4k8dyqnrc6sz00000gr/T/ipykernel_84428/861379570.py:10: RuntimeWarning: invalid value encountered in divide
  r_k = np.exp(-score_k)/(np.sum(np.exp(-score_k)))
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>When we compute the responsibilities</p>
<div class="math notranslate nohighlight">
\[
r_{k,i}^t
= \frac{\pi_k^t \prod_{m=1}^M (p_{k,m}^t)^{x_{i,m}} (1-p_{k,m}^t)^{1-x_{i,m}}}
{\sum_{k'=1}^K \pi_{k'}^t \prod_{m=1}^M (p_{k',m}^t)^{x_{i,m}} (1-p_{k',m}^t)^{1-x_{i,m}}},
\]</div>
<p>we first compute the negative logarithm of each term in the numerator as we did in the Naive Bayes case. But then we apply the function <span class="math notranslate nohighlight">\(e^{-x}\)</span>, because this time we are not simply computing an optimal score. When all scores are high, this last step may result in underflow, that is, produces numbers so small that they get rounded down to zero by NumPy. Then the ratio defining <code class="docutils literal notranslate"><span class="pre">r_k</span></code> is not well-defined.</p>
<p>To deal with this, we introduce a technique called the log-sum-exp trick<span class="math notranslate nohighlight">\(\idx{log-sum-exp trick}\xdi\)</span> (with some help from ChatGPT). Consider the computation of a function of <span class="math notranslate nohighlight">\(\mathbf{a} = (a_1, \ldots, a_n)\)</span> of the form</p>
<div class="math notranslate nohighlight">
\[
h(\mathbf{a}) = \log \left( \sum_{i=1}^{n} e^{-a_i} \right).
\]</div>
<p>When the <span class="math notranslate nohighlight">\(a_i\)</span> values are large positive numbers, the terms <span class="math notranslate nohighlight">\(e^{-a_i}\)</span> can be so small that they underflow to zero. To counter this, the log-sum-exp trick involves a shift to bring these terms into a more favorable numerical range.</p>
<p>It proceeds as follows:</p>
<ol class="arabic">
<li><p>Identify the minimum value <span class="math notranslate nohighlight">\(M\)</span> among the <span class="math notranslate nohighlight">\(a_i\)</span>s</p>
<div class="math notranslate nohighlight">
\[
   M = \min\{a_1, a_2, \ldots, a_n\}.
   \]</div>
</li>
<li><p>Subtract <span class="math notranslate nohighlight">\(M\)</span> from each <span class="math notranslate nohighlight">\(a_i\)</span> before exponentiation</p>
<div class="math notranslate nohighlight">
\[
   \log \left( \sum_{i=1}^{n} e^{-a_i} \right) 
   = \log \left( e^{-M} \sum_{i=1}^{n} e^{- (a_i - M)} \right).
   \]</div>
</li>
<li><p>Rewrite using log properties</p>
<div class="math notranslate nohighlight">
\[
   = -M + \log \left( \sum_{i=1}^{n} e^{-(a_i - M)} \right).
   \]</div>
</li>
</ol>
<p>Why does this help with underflow? By subtracting <span class="math notranslate nohighlight">\(M\)</span>, the smallest value in the set, from each <span class="math notranslate nohighlight">\(a_i\)</span>:
(i) the largest term in <span class="math notranslate nohighlight">\(\{e^{-(a_i - M)} :  i = 1,\ldots,n\}\)</span> becomes <span class="math notranslate nohighlight">\(e^0 = 1\)</span>; and (ii)
all other terms are between 0 and 1, as they are exponentiations of nonpositive numbers. This manipulation avoids terms underflowing to zero because even very large values, when shifted by <span class="math notranslate nohighlight">\(M\)</span>, are less likely to hit the underflow threshold.</p>
<p>Here is an example. Imagine you have <span class="math notranslate nohighlight">\(\mathbf{a} = (1000, 1001, 1002)\)</span>.</p>
<ul class="simple">
<li><p>Direct computation: <span class="math notranslate nohighlight">\(e^{-1000}\)</span>, <span class="math notranslate nohighlight">\(e^{-1001}\)</span>, and <span class="math notranslate nohighlight">\(e^{-1002}\)</span> might all underflow to zero.</p></li>
<li><p>With the log-sum-exp trick: Subtract <span class="math notranslate nohighlight">\(M = 1000\)</span>, leading to <span class="math notranslate nohighlight">\(e^{0}\)</span>, <span class="math notranslate nohighlight">\(e^{-1}\)</span>, and <span class="math notranslate nohighlight">\(e^{-2}\)</span>, all meaningful, non-zero results that accurately contribute to the sum.</p></li>
</ul>
<p>We implement in NumPy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">log_sum_exp_trick</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="n">min_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">min_val</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="n">a</span> <span class="o">+</span> <span class="n">min_val</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We try it on a simple example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">1001</span><span class="p">,</span> <span class="mi">1002</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We first attempt a direct computation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">a</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>/var/folders/k0/7k0fxl7j54q4k8dyqnrc6sz00000gr/T/ipykernel_84428/214275762.py:1: RuntimeWarning: underflow encountered in exp
  np.log(np.sum(np.exp(-a)))
/var/folders/k0/7k0fxl7j54q4k8dyqnrc6sz00000gr/T/ipykernel_84428/214275762.py:1: RuntimeWarning: divide by zero encountered in log
  np.log(np.sum(np.exp(-a)))
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>-inf
</pre></div>
</div>
</div>
</div>
<p>Predictly, we get an underflow error and a useless output.</p>
<p>Next, we try the log-sum-exp trick.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">log_sum_exp_trick</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>-999.5923940355556
</pre></div>
</div>
</div>
</div>
<p>This time we get an output which seems reasonable, something slightly larger than <span class="math notranslate nohighlight">\(-1000\)</span> as expected (Why?).</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>After this long – but important! – parenthesis, we return to the EM algorithm. We modify it by implementing the log-sum-exp trick in the subroutine <code class="docutils literal notranslate"><span class="pre">responsibility</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">responsibility</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
   
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
    <span class="n">score_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
       
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pi_k</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span> 
                             <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:]))</span>
    <span class="n">r_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">score_k</span> <span class="o">-</span> <span class="n">log_sum_exp_trick</span><span class="p">(</span><span class="n">score_k</span><span class="p">))</span>
            
    <span class="k">return</span> <span class="n">r_k</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We go back to the MNIST example with only the 2s.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">em_bern</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">pi_0</span><span class="p">,</span> <span class="n">p_0</span><span class="p">,</span> <span class="n">maxiters</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here are the parameters for one cluster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/140b9ccf2e31df7febe808141c26d248ff25b7748bcf7be623f2bd60365c407b.png" src="../Images/4e3661e0a1a8051341921aba94079c7b.png" data-original-src="https://mmids-textbook.github.io/_images/140b9ccf2e31df7febe808141c26d248ff25b7748bcf7be623f2bd60365c407b.png"/>
</div>
</div>
<p>Here is the other one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/123b40b50b0c6ee77988c143d5de20e5dc96e914b6fdf3634b015b34143479ee.png" src="../Images/372533fd8fad14df01f1cf71749a0141.png" data-original-src="https://mmids-textbook.github.io/_images/123b40b50b0c6ee77988c143d5de20e5dc96e914b6fdf3634b015b34143479ee.png"/>
</div>
</div>
<p>Now that the model is trained, we compute the probability that an example image is in each cluster. We use the first image in the dataset that we plotted earlier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">responsibility</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>array([1.00000000e+00, 5.09357087e-17])
</pre></div>
</div>
</div>
</div>
<p>It indeed identifies the second cluster as significantly more likely.</p>
<p><strong>TRY IT!</strong> In the MNIST example, as we have seen, the probabilities involved are extremely small and the responsibilities are close to <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. Implement a variant of EM, called hard EM, which replaces responsibilities with the one-hot encoding of the largest responsibility. Test it on the MNIST example again. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open In Colab</a>)</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The mixture of multivariate Bernoullis model is a simple example of a latent variable model. Ask your favorite AI chatbot to discuss more complex latent variable models, such as the variational autoencoder or the Gaussian process latent variable model, and their applications in unsupervised learning. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> In the mixture of multivariate Bernoullis model, the joint distribution is given by:</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[C = k, \mathbf{X} = \mathbf{x}]\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X} = \mathbf{x}|C = k] \mathbb{P}[C = k]\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \prod_{k=1}^K \mathbb{P}[\mathbf{X} = \mathbf{x}|C = k] \mathbb{P}[C = k]\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{\mathbf{x}} \mathbb{P}[C = k, \mathbf{X} = \mathbf{x}]\)</span></p>
<p><strong>2</strong> The majorization-minimization principle states that:</p>
<p>a) If <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> majorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then a global minimum <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> of <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> satisfies <span class="math notranslate nohighlight">\(f(\mathbf{x}') \geq f(\mathbf{x})\)</span>.</p>
<p>b) If <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> majorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then a global minimum <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> of <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> satisfies <span class="math notranslate nohighlight">\(f(\mathbf{x}') \leq f(\mathbf{x})\)</span>.</p>
<p>c) If <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> minorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then a global minimum <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> of <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> satisfies <span class="math notranslate nohighlight">\(f(\mathbf{x}') \geq f(\mathbf{x})\)</span>.</p>
<p>d) If <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> minorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then a global minimum <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> of <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> satisfies <span class="math notranslate nohighlight">\(f(\mathbf{x}') \leq f(\mathbf{x})\)</span>.</p>
<p><strong>3</strong> In the EM algorithm for mixtures of multivariate Bernoullis, the M-step involves:</p>
<p>a) Updating the parameters <span class="math notranslate nohighlight">\(\pi_k\)</span> and <span class="math notranslate nohighlight">\(p_{k,m}\)</span></p>
<p>b) Computing the responsibilities <span class="math notranslate nohighlight">\(r_{k,i}^t\)</span></p>
<p>c) Minimizing the negative log-likelihood</p>
<p>d) Applying the log-sum-exp trick</p>
<p><strong>4</strong> The mixture of multivariate Bernoullis model is represented by the following graphical model:</p>
<p>a)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"gray"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"white"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="s2">"X"</span><span class="p">)</span>
</pre></div>
</div>
<p>b)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"white"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"gray"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="s2">"X"</span><span class="p">)</span>
</pre></div>
</div>
<p>c)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"gray"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"gray"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="s2">"X"</span><span class="p">)</span>
</pre></div>
</div>
<p>d)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">G</span> <span class="o">=</span> <span class="n">nx</span><span class="o">.</span><span class="n">DiGraph</span><span class="p">()</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"X"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"white"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_node</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="s2">"circle"</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">"filled"</span><span class="p">,</span> <span class="n">fillcolor</span><span class="o">=</span><span class="s2">"white"</span><span class="p">)</span>
<span class="n">G</span><span class="o">.</span><span class="n">add_edge</span><span class="p">(</span><span class="s2">"C"</span><span class="p">,</span> <span class="s2">"X"</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>5</strong> In the context of clustering, what is the interpretation of the responsibilities computed in the E-step of the EM algorithm?</p>
<p>a) They represent the distance of each data point to the cluster centers.</p>
<p>b) They indicate the probability of each data point belonging to each cluster.</p>
<p>c) They determine the optimal number of clusters.</p>
<p>d) They are used to initialize the cluster centers in the M-step.</p>
<p>Answer for 1: b. Justification: The text states, “<span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[C = k, \mathbf{X} = \mathbf{x}] = \sum_{k=1}^K \mathbb{P}[\mathbf{X} = \mathbf{x}|C = k] \mathbb{P}[C = k]\)</span>.”</p>
<p>Answer for 2: b. Justification: “Let <span class="math notranslate nohighlight">\(f: \mathbb{R}^d \to \mathbb{R}\)</span> and suppose <span class="math notranslate nohighlight">\(U_{\mathbf{x}}\)</span> majorizes <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{x}'\)</span> be a global minimizer of <span class="math notranslate nohighlight">\(U_{\mathbf{x}}(\mathbf{z})\)</span> as a function of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, provided it exists. Then <span class="math notranslate nohighlight">\(f(\mathbf{x}') \leq f(\mathbf{x})\)</span>.”</p>
<p>Answer for 3: a. Justification: In the summary of the EM algorithm, the M-step is described as updating the parameters: “<span class="math notranslate nohighlight">\(\pi_k^{t+1} = \frac{\eta_k^t}{n}\)</span> and <span class="math notranslate nohighlight">\(p_{k,m}^{t+1} = \frac{\eta_{k,m}^t}{\eta_k^t}\)</span>,” which require the responsibilities.</p>
<p>Answer for 4: b. Justification: The text states, “Mathematically, that corresponds to applying the law of total probability as we did previously. Further, we let the vertex for <span class="math notranslate nohighlight">\(X\)</span> be shaded to indicate that it is observed, while the vertex for <span class="math notranslate nohighlight">\(Y\)</span> is not shaded to indicate that it is not.”</p>
<p>Answer for 5: b. Justification: The text refers to responsibilities as “our estimate – under the current parameter – of the probability that the sample comes from cluster <span class="math notranslate nohighlight">\(k\)</span>.”</p>
    
</body>
</html>