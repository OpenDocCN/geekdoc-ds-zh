<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Newton's Method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Newton's Method</h1>
<blockquote>原文：<a href="https://en.algorithmica.org/hpc/arithmetic/newton/">https://en.algorithmica.org/hpc/arithmetic/newton/</a></blockquote><div id="search"><input id="search-bar" type="search" placeholder="Search this book…" oninput="search()"/><div id="search-count"/><div id="search-results"/></div><header><div class="info"/></header><article><p>Reaching the maximum possible precision is rarely required from a practical algorithm. In real-world data, modeling and measurement errors are usually several orders of magnitude larger than the errors that come from rounding floating-point numbers and such, and we are often perfectly happy with picking an approximate method that trades off precision for speed.</p><p>In this section, we introduce one of the most important building blocks in such approximate, numerical algorithms: <em>Newton’s method</em>.</p><span class="anchor" id="newtons-method"/><h2><a class="anchor-link" href="https://en.algorithmica.org/hpc/arithmetic/newton/#newtons-method">#</a>Newton’s Method</h2><p>Newton’s method is a simple yet very powerful algorithm for finding approximate roots of real-valued functions, that is, the solutions to the following generic equation:</p>$$
f(x) = 0
$$<p>The only thing assumed about the function $f$ is that at least one root exists and that $f(x)$ is continuous and differentiable on the search interval. There are also some <a href="https://en.wikipedia.org/wiki/Newton%27s_method#Failure_analysis">boring corner cases</a>, but they almost never occur in practice, so we will just informally say that the function is “good.”</p><p>The main idea of the algorithm is to start with some initial approximation $x_0$ and then iteratively improve it by drawing the tangent to the graph of the function at $x = x_i$ and setting the next approximation $x_{i+1}$ equal to the $x$-coordinate of its intersection with the $x$-axis. The intuition is that if the function $f$ is “<a href="https://en.wikipedia.org/wiki/Smoothness">good</a>” and $x_i$ is already close enough to the root, then $x_{i+1}$ will be even closer.</p><p><figure><img src="../Images/18f0c447cf274c200cca85ada7b37649.png" data-original-src="https://en.algorithmica.org/hpc/arithmetic/img/newton.png"/><figcaption/></figure></p><p>To obtain the point of intersection for $x_n$, we need to equal its tangent line function to zero:</p>$$
0 = f(x_i) + (x_{i+1} - x_i) f'(x_i)
$$
from which we derive
$$
x_{i+1} = x_i - \frac{f(x_i)}{f'(x_i)}
$$<p>Newton’s method is very important: it is the basis of a wide range of optimization solvers in science and engineering.</p><span class="anchor" id="square-root"/><h3><a class="anchor-link" href="https://en.algorithmica.org/hpc/arithmetic/newton/#square-root">#</a>Square Root</h3><p>As a simple example, let’s derive the algorithm for the problem of finding square roots:</p>$$
x = \sqrt n \iff x^2 = n \iff f(x) = x^2 - n = 0
$$
If we substitute $f(x) = x^2 - n$ into the generic formula above, we can obtain the following update rule:
$$
x_{i+1} = x_i - \frac{x_i^2 - n}{2 x_i} = \frac{x_i + n / x_i}{2}
$$<p>In practice we also want to stop it as soon as it is close enough to the right answer, which we can simply check after each iteration:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">double</span> <span class="n">EPS</span> <span class="o">=</span> <span class="mf">1e-9</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kt">double</span> <span class="nf">sqrt</span><span class="p">(</span><span class="kt">double</span> <span class="n">n</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">double</span> <span class="n">x</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="p">(</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="n">n</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">n</span> <span class="o">/</span> <span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>The algorithm converges for many functions, although it does so reliably and provably only for a certain subset of them (e.g., convex functions). Another question is how fast the convergence is, if it occurs.</p><span class="anchor" id="rate-of-convergence"/><h3><a class="anchor-link" href="https://en.algorithmica.org/hpc/arithmetic/newton/#rate-of-convergence">#</a>Rate of Convergence</h3><p>Let’s run a few iterations of Newton’s method to find the square root of $2$, starting with $x_0 = 1$, and check how many digits it got correct after each iteration:</p><pre class="center-pre">
<b>1</b>.0000000000000000000000000000000000000000000000000000000000000
<b>1</b>.5000000000000000000000000000000000000000000000000000000000000
<b>1.41</b>66666666666666666666666666666666666666666666666666666666675
<b>1.41421</b>56862745098039215686274509803921568627450980392156862745
<b>1.41421356237</b>46899106262955788901349101165596221157440445849057
<b>1.41421356237309504880168</b>96235025302436149819257761974284982890
<b>1.41421356237309504880168872420969807856967187537</b>72340015610125
<b>1.4142135623730950488016887242096980785696718753769480731766796</b>
</pre><p>Looking carefully, we can see that the number of accurate digits approximately doubles on each iteration. This fantastic convergence rate is not a coincidence.</p><p>To analyze convergence rate quantitatively, we need to consider a small relative error $\delta_i$ on the $i$-th iteration and determine how much smaller the error $\delta_{i+1}$ is on the next iteration:</p>$$
|\delta_i| = \frac{|x_n - x|}{x}
$$
We can express $x_i$ as $x \cdot (1 + \delta_i)$. Plugging it into the Newton iteration formula and dividing both sides by $x$ we get
$$
1 + \delta_{i+1} = \frac{1}{2} (1 + \delta_i + \frac{1}{1 + \delta_i}) = \frac{1}{2} (1 + \delta_i + 1 - \delta_i + \delta_i^2 + o(\delta_i^2)) = 1 + \frac{\delta_i^2}{2} + o(\delta_i^2)
$$<p>Here we have Taylor-expanded $(1 + \delta_i)^{-1}$ at $0$, using the assumption that the error $d_i$ is small (since the sequence converges, $d_i \ll 1$ for sufficiently large $n$).</p><p>Rearranging for $\delta_{i+1}$, we obtain</p>$$
\delta_{i+1} = \frac{\delta_i^2}{2} + o(\delta_i^2)
$$<p>which means that the error roughly squares (and halves) on each iteration once we are close to the solution. Since the logarithm $(- \log_{10} \delta_i)$ is roughly the number of accurate significant digits in the answer $x_i$, squaring the relative error corresponds precisely to doubling the number of significant
digits that we had observed.</p><p>This is known as <em>quadratic convergence</em>, and in fact, this is not limited to finding square roots. With detailed proof being left as an exercise to the reader, it can be shown that, in general</p>$$
|\delta_{i+1}| = \frac{|f''(x_i)|}{2 \cdot |f'(x_n)|} \cdot \delta_i^2
$$<p>which results in at least quadratic convergence under a few additional assumptions, namely $f’(x)$ not being equal to $0$ and $f’’(x)$ being continuous.</p><span class="anchor" id="further-reading"/><h2><a class="anchor-link" href="https://en.algorithmica.org/hpc/arithmetic/newton/#further-reading">#</a>Further Reading</h2><p><a href="https://ocw.mit.edu/courses/mathematics/18-330-introduction-to-numerical-analysis-spring-2012/lecture-notes/MIT18_330S12_Chapter4.pdf">Introduction to numerical methods at MIT</a>.</p></article><div class="nextprev"><div class="left"><a href="https://en.algorithmica.org/hpc/arithmetic/errors/" id="prev-article">← Rounding Errors</a></div><div class="right"><a href="https://en.algorithmica.org/hpc/arithmetic/rsqrt/" id="next-article">Fast Inverse Square Root →</a></div></div>    
</body>
</html>