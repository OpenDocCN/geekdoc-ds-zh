- en: '2.3\. Geometry of least squares: the orthogonal projection#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap02_ls/03_orthog/roch-mmids-ls-orthog.html](https://mmids-textbook.github.io/chap02_ls/03_orthog/roch-mmids-ls-orthog.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We consider the following problem: we are given \(A \in \mathbb{R}^{n\times
    m}\) an \(n\times m\) matrix and \(\mathbf{b} \in \mathbb{R}^n\) a vector. We
    are looking to solve the system \(A \mathbf{x} \approx \mathbf{b}\). In the special
    case where \(A\) is invertible, a unique exact solution exists. In general, however,
    a solution may not exist or may not be unique. We focus here on the over-determined
    case where the former situation generically occurs. We begin by rewieving the
    concept of orthogonality.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.3.1\. A key concept: orthogonality[#](#a-key-concept-orthogonality "Link
    to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Orthogonality plays a key role in linear algebra for data science thanks to
    its computational properties and its connection to the least-squares problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Orthogonality)** \(\idx{orthogonality}\xdi\) Vectors \(\mathbf{u}\)
    and \(\mathbf{v}\) in \(\mathbb{R}^n\) (as column vectors) are orthogonal if their
    inner product is zero'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{u}, \mathbf{v} \rangle =\mathbf{u}^T \mathbf{v} = \sum_{i=1}^n
    u_i v_i = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonality has important implications. The following classical result will
    be useful below. Throughout, we use \(\|\mathbf{u}\|\) for the Euclidean norm
    of \(\mathbf{u}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Pythagoras)** \(\idx{Pythagoras'' theorem}\xdi\) Let \(\mathbf{u},
    \mathbf{v} \in \mathbb{R}^n\) be orthogonal. Then \(\|\mathbf{u} + \mathbf{v}\|^2
    = \|\mathbf{u}\|^2 + \|\mathbf{v}\|^2\). \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Using \(\|\mathbf{w}\|^2 = \langle \mathbf{w}, \mathbf{w}\rangle\),
    we get'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{u} + \mathbf{v}\|^2 &= \langle \mathbf{u} + \mathbf{v},
    \mathbf{u} + \mathbf{v}\rangle\\ &= \langle \mathbf{u}, \mathbf{u}\rangle + 2
    \,\langle \mathbf{u}, \mathbf{v}\rangle + \langle \mathbf{v}, \mathbf{v}\rangle\\
    &= \|\mathbf{u}\|^2 + \|\mathbf{v}\|^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: An application of *Pythagoras’ Theorem* is a proof of the *Cauchy-Schwarz Inequality*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Cauchy-Schwarz)* \(\idx{Cauchy-Schwarz inequality}\xdi\) Let \(\mathbf{q}
    = \frac{\mathbf{v}}{\|\mathbf{v}\|}\) be the unit vector in the direction of \(\mathbf{v}\).
    We want to show \(|\langle \mathbf{u}, \mathbf{q}\rangle| \leq \|\mathbf{u}\|\).
    Decompose \(\mathbf{u}\) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{u} = \left\langle \mathbf{u}, \mathbf{q}\right\rangle \mathbf{q}
    + \left\{\mathbf{u} - \left\langle \mathbf{u}, \mathbf{q}\right\rangle \mathbf{q}\right\}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The two terms on the right-hand side are orthogonal:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left\langle \left\langle \mathbf{u}, \mathbf{q}\right\rangle \mathbf{q},
    \mathbf{u} - \left\langle \mathbf{u}, \mathbf{q}\right\rangle \mathbf{q} \right\rangle
    = \left\langle \mathbf{u}, \mathbf{q}\right\rangle^2 - \left\langle \mathbf{u},
    \mathbf{q}\right\rangle^2 \left\langle \mathbf{q}, \mathbf{q}\right\rangle = 0.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So *Pythagoras* gives
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{u}\|^2 = \left\|\left\langle \mathbf{u}, \mathbf{q}\right\rangle
    \mathbf{q}\right\|^2 + \left\|\mathbf{u} - \left\langle \mathbf{u}, \mathbf{q}\right\rangle
    \mathbf{q}\right\|^2 \geq \left\|\left\langle \mathbf{u}, \mathbf{q}\right\rangle
    \mathbf{q}\right\|^2 = \left\langle \mathbf{u}, \mathbf{q}\right\rangle^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Taking a square root gives the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Orthonormal basis expansion** To begin to see the power of orthogonality,
    consider the following. A list of vectors \(\{\mathbf{u}_1,\ldots,\mathbf{u}_m\}\)
    is an orthonormal list if the \(\mathbf{u}_i\)’s are pairwise orthogonal and each
    has norm 1, that is, for all \(i\) and all \(j \neq i\), we have \(\|\mathbf{u}_i\|
    = 1\) and \(\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0\). Alternatively,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \langle \mathbf{u}_i, \mathbf{u}_j \rangle = \begin{cases} 1
    & \text{if $i=j$}\\ 0 & \text{if $i\neq j$}. \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Properties of Orthonormal Lists)** \(\idx{properties of orthonormal
    lists}\xdi\) Let \(\{\mathbf{u}_1,\ldots,\mathbf{u}_m\}\) be an orthonormal list.
    Then:'
  prefs: []
  type: TYPE_NORMAL
- en: for any \(\alpha_j \in \mathbb{R}\), \(j=1,\ldots,m\), we have
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \left\|\sum_{j=1}^m \alpha_j \mathbf{u}_j\right\|^2 = \sum_{j=1}^m \alpha_j^2,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: the vectors \(\{\mathbf{u}_1,\ldots,\mathbf{u}_m\}\) are linearly independent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For 1., using that \(\|\mathbf{x}\|^2 = \langle \mathbf{x}, \mathbf{x}
    \rangle\), we have'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left\|\sum_{j=1}^m \alpha_j \mathbf{u}_j\right\|^2 &= \left\langle
    \sum_{i=1}^m \alpha_i \mathbf{u}_i, \sum_{j=1}^m \alpha_j \mathbf{u}_j \right\rangle\\
    &= \sum_{i=1}^m \alpha_i \left\langle \mathbf{u}_i, \sum_{j=1}^m \alpha_j \mathbf{u}_j
    \right\rangle\\ &= \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j \left\langle \mathbf{u}_i,
    \mathbf{u}_j \right\rangle\\ &= \sum_{i=1}^m \alpha_i^2 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used orthonormality in the last equation, that is, \(\langle \mathbf{u}_i,
    \mathbf{u}_j \rangle\) is \(1\) if \(i=j\) and \(0\) otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: For 2., suppose \(\sum_{i=1}^m \beta_i \mathbf{u}_i = \mathbf{0}\), then we
    must have by 1\. that \(\sum_{i=1}^m \beta_i^2 = 0\). That implies \(\beta_i =
    0\) for all \(i\). Hence the \(\mathbf{u}_i\)’s are linearly independent. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a basis \(\{\mathbf{u}_1,\ldots,\mathbf{u}_m\}\) of \(U\), we know that:
    for any \(\mathbf{w} \in U\), \(\mathbf{w} = \sum_{i=1}^m \alpha_i \mathbf{u}_i\)
    for some \(\alpha_i\)’s. It is not immediately obvious in general how to find
    the \(\alpha_i\)’s – one must solve a system of linear equations. In the orthonormal
    case, however, there is a formula. We say that the basis \(\{\mathbf{u}_1,\ldots,\mathbf{u}_m\}\)
    is orthonormal if it forms an orthonormal list.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Orthonormal Expansion)** \(\idx{orthonormal expansion theorem}\xdi\)
    Let \(\mathbf{q}_1,\ldots,\mathbf{q}_m\) be an orthonormal basis of \(U\) and
    let \(\mathbf{w} \in U\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w} = \sum_{j=1}^m \langle \mathbf{w}, \mathbf{q}_j\rangle \,\mathbf{q}_j.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Because \(\mathbf{w} \in U\), \(\mathbf{w} = \sum_{i=1}^m \alpha_i
    \mathbf{q}_i\) for some \(\alpha_i\). Take the inner product with \(\mathbf{q}_j\)
    and use orthonormality:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{w}, \mathbf{q}_j\rangle = \left\langle \sum_{i=1}^m \alpha_i
    \mathbf{q}_i, \mathbf{q}_j\right\rangle = \sum_{i=1}^m \alpha_i \langle \mathbf{q}_i,
    \mathbf{q}_j\rangle = \alpha_j. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we have determined all \(\alpha_j\)’s in the basis expansion of \(\mathbf{w}\).
    \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Consider again the linear subspace \(W = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3)\),
    where \(\mathbf{w}_1 = (1,0,1)\), \(\mathbf{w}_2 = (0,1,1)\), and \(\mathbf{w}_3
    = (1,-1,0)\). We have shown that in fact'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3) = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2),
    \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(\mathbf{w}_1,\mathbf{w}_2\) form a basis of \(W\). On the other hand,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle\mathbf{w}_1,\mathbf{w}_2\rangle = (1)(0) + (0)(1) + (1)(1) = 0 +
    0 + 1 = 1 \neq 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: so this basis is not orthonormal. Indeed, an orthonormal list is necessarily
    an independent list, but the opposite may not hold.
  prefs: []
  type: TYPE_NORMAL
- en: To produce an orthonormal basis of \(W\), we can first proceed by normalizing
    \(\mathbf{w}_1\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{q}_1 = \frac{\mathbf{w}_1}{\|\mathbf{w}_1\|} = \frac{\mathbf{w}_1}{\sqrt{1^2
    + 0^2 + 1^2}} = \frac{1}{\sqrt{2}} \mathbf{w}_1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then \(\|\mathbf{q}_1\| = 1\) since, in general, by absolute homogeneity of
    the norm
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left\|\frac{\mathbf{w}_1}{\|\mathbf{w}_1\|}\right\| = \frac{1}{\|\mathbf{w}_1\|}
    \|\mathbf{w}_1\| = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We then seek a second basis vector. It must satisfy two conditions in this
    case:'
  prefs: []
  type: TYPE_NORMAL
- en: it must be of unit norm and be orthogonal to \(\mathbf{q}_1\); and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbf{w}_2\) must be a linear combination of \(\mathbf{q}_1\) and \(\mathbf{q}_2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latter condition guarantees that \(\mathrm{span}(\mathbf{q}_1,\mathbf{q}_2)
    = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2)\). (Formally, that would imply only
    that \(\mathrm{span}(\mathbf{w}_1,\mathbf{w}_2) \subseteq \mathrm{span}(\mathbf{q}_1,\mathbf{q}_2)\).
    In this case, it is easy to see that the containment must go in the opposite direction
    as well. Why?)
  prefs: []
  type: TYPE_NORMAL
- en: The first condition translates into
  prefs: []
  type: TYPE_NORMAL
- en: \[ 1 = \|\mathbf{q}_2\|^2 = q_{21}^2 + q_{22}^2 + q_{23}^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{q}_2 = (q_{21}, q_{22}, q_{23})\), and
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \langle\mathbf{q}_1, \mathbf{q}_2\rangle = \frac{1}{\sqrt{2}}\left[1\cdot
    q_{21} + 0 \cdot q_{22} + 1 \cdot q_{23}\right] = \frac{1}{\sqrt{2}}\left[q_{21}
    + q_{23}\right]. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, simplifying the second display and plugging into the first, \(q_{23}
    = -q_{21}\) and \(q_{22} = \sqrt{1 - 2 q_{21}^2}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second condition translates into: there is \(\beta_1, \beta_2 \in \mathbb{R}\)
    such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{w}_2 = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix} = \beta_1
    \mathbf{q}_1 + \beta_2 \mathbf{q}_2 = \beta_1 \frac{1}{\sqrt{2}} \begin{pmatrix}
    1 \\ 0 \\ 1 \end{pmatrix} + \beta_2 \begin{pmatrix} q_{21} \\ \sqrt{1-2 q_{21}^2}
    \\ -q_{21} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The first entry gives \(\beta_1/\sqrt{2} + \beta_2 q_{21} = 0\) while the third
    entry gives \(\beta_1/\sqrt{2} - \beta_2 q_{21} = 1\). Adding up the equations
    gives \(\beta_1 = 1/\sqrt{2}\). Plugging back into the first one gives \(\beta_2
    = -1/(2q_{21})\). Returning to the equation for \(\mathbf{w}_2\), we get from
    the second entry
  prefs: []
  type: TYPE_NORMAL
- en: \[ 1 = - \frac{1}{2 q_{21}} \sqrt{1 - 2 q_{21}^2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Rearranging and taking a square, we want the negative solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ 4 q_{21}^2 = 1 - 2 q_{21}^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, \(q_{21} = - 1/\sqrt{6}\). Finally, we get \(q_{23} = - q_{21} = 1/\sqrt{6}\)
    and \(q_{22} = \sqrt{1 - 2 q_{21}^2} = \sqrt{1 - 1/3} = \sqrt{2/3} = 2/\sqrt{6}\).
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we have
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\ 0\\ 1
    \end{pmatrix}, \quad \mathbf{q}_2 = \frac{1}{\sqrt{6}} \begin{pmatrix} -1\\ 2\\
    1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We confirm that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{q}_1, \mathbf{q}_2\rangle = \frac{1}{\sqrt{2}\sqrt{6}}[(1)(-1)
    + (0)(2) + (1)(1)] = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{q}_2\|^2 = \left(-\frac{1}{\sqrt{6}}\right)^2 + \left(\frac{2}{\sqrt{6}}\right)^2
    + \left(\frac{1}{\sqrt{6}}\right)^2 = \frac{1}{6} + \frac{4}{6} + \frac{1}{6}
    = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: We can use the *Orthonormal Expansion Theorem* to write \(\mathbf{w}_2\) as
    a linear combination of \(\mathbf{q}_1\) and \(\mathbf{q}_2\). The inner products
    are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{w}_2, \mathbf{q}_1 \rangle = 0 \left(\frac{1}{\sqrt{2}}\right)
    + 1 \left(\frac{0}{\sqrt{2}}\right) + 1 \left(\frac{1}{\sqrt{2}}\right) = \frac{1}{\sqrt{2}},
    \]\[ \langle \mathbf{w}_2, \mathbf{q}_2 \rangle = 0 \left(-\frac{1}{\sqrt{6}}\right)
    + 1 \left(\frac{2}{\sqrt{6}}\right) + 1 \left(\frac{1}{\sqrt{6}}\right) = \frac{3}{\sqrt{6}}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w}_2 = \frac{1}{\sqrt{2}} \mathbf{q}_1 + \frac{3}{\sqrt{6}} \mathbf{q}_2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Check it! Try \(\mathbf{w}_3\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Gram-Schmidt** We have shown that working with orthonormal bases is desirable.
    What if we do not have one? We could try to construct one by hand as we did in
    the previous example. But there are better ways. We review the [Gram-Schmidt algorithm](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process)
    in an upcoming section, which will imply that every linear subspace has an orthonormal
    basis. That is, we will prove the following theorem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Gram-Schmidt)** \(\idx{Gram-Schmidt theorem}\xdi\) Let \(\mathbf{a}_1,\ldots,\mathbf{a}_m\)
    be linearly independent. Then there exists an orthonormal basis \(\mathbf{q}_1,\ldots,\mathbf{q}_m\)
    of \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\). \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: But, first, we will need to define the orthogonal projection, which will play
    a key role in our applications. This is done next.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2\. Orthogonal projection[#](#orthogonal-projection "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To solve the overdetermined case, i.e., when \(n > m\), we consider the following
    more general problem first. We have a linear subspace \(U \subseteq \mathbb{R}^n\)
    and a vector \(\mathbf{v} \notin U\). We want to find the vector \(\mathbf{p}\)
    in \(U\) that is closest to \(\mathbf{v}\) in Euclidean norm, that is, we want
    to solve
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{p} \in U} \|\mathbf{p} - \mathbf{v}\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Consider the two-dimensional case with a one-dimensional subspace,
    say \(U = \mathrm{span}(\mathbf{u}_1)\) with \(\|\mathbf{u}_1\|=1\). The geometrical
    intuition is in the following figure. The solution \(\mathbf{p} = \mathbf{v}^*\)
    has the property that the difference \(\mathbf{v} - \mathbf{v}^*\) makes a right
    angle with \(\mathbf{u}_1\), that is, it is orthogonal to it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Orthogonal projection on a line (with help from ChatGPT and Claude; inspired
    by Source)](../Images/067d2c4143b5d805e9e1b2381a00c9db.png)'
  prefs: []
  type: TYPE_IMG
- en: Letting \(\mathbf{v}^* = \alpha^* \,\mathbf{u}_1\), the geometrical condition
    above translates into
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \langle \mathbf{u}_1, \mathbf{v} - \mathbf{v}^* \rangle = \langle \mathbf{u}_1,
    \mathbf{v} - \alpha^* \,\mathbf{u}_1 \rangle = \langle \mathbf{u}_1, \mathbf{v}
    \rangle - \alpha^* \,\langle \mathbf{u}_1, \mathbf{u}_1 \rangle = \langle \mathbf{u}_1,
    \mathbf{v} \rangle - \alpha^* \]
  prefs: []
  type: TYPE_NORMAL
- en: so
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}^* = \langle \mathbf{u}_1, \mathbf{v} \rangle \,\mathbf{u}_1. \]
  prefs: []
  type: TYPE_NORMAL
- en: By *Pythagoras’ Theorem*, we then have for any \(\alpha \in \mathbb{R}\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{v} - \alpha \,\mathbf{u}_1\|^2 &= \|\mathbf{v}- \mathbf{v}^*
    + \mathbf{v}^* - \alpha \,\mathbf{u}_1\|^2\\ &= \|\mathbf{v}- \mathbf{v}^* + (\alpha^*
    - \alpha) \,\mathbf{u}_1\|^2\\ &= \|\mathbf{v}- \mathbf{v}^*\|^2 + \| (\alpha^*
    - \alpha) \,\mathbf{u}_1\|^2\\ &\geq \|\mathbf{v}- \mathbf{v}^*\|^2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(\mathbf{v} - \mathbf{v}^*\) is orthogonal to \(\mathbf{u}_1\)
    (and therefore \((\alpha^* - \alpha) \mathbf{u}_1\)) on the third line.
  prefs: []
  type: TYPE_NORMAL
- en: That confirms the optimality of \(\mathbf{v}^*\). The argument in this example
    carries through in higher dimension, as we show next. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Orthogonal Projection on an Orthonormal List)** \(\idx{orthogonal
    projection on an orthonormal list}\xdi\) Let \(\mathbf{q}_1,\ldots,\mathbf{q}_m\)
    be an orthonormal list. The orthogonal projection of \(\mathbf{v} \in \mathbb{R}^n\)
    on \(\{\mathbf{q}_i\}_{i=1}^m\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{proj}_{\{\mathbf{q}_i\}_{i=1}^m} \mathbf{v} = \sum_{j=1}^m \langle
    \mathbf{v}, \mathbf{q}_j \rangle \,\mathbf{q}_j. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Orthogonal Projection)** \(\idx{orthogonal projection theorem}\xdi\)
    Let \(U \subseteq V\) be a linear subspace and let \(\mathbf{v} \in \mathbb{R}^n\).
    Then:'
  prefs: []
  type: TYPE_NORMAL
- en: a) There exists a unique solution \(\mathbf{p}^*\) to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{p} \in U} \|\mathbf{p} - \mathbf{v}\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: We denote it by \(\mathbf{p}^* = \mathrm{proj}_U \mathbf{v}\) and refer to it
    as the orthogonal projection of \(\mathbf{v}\) onto \(U\).
  prefs: []
  type: TYPE_NORMAL
- en: b) The solution \(\mathbf{p}^* \in U\) is characterized geometrically by
  prefs: []
  type: TYPE_NORMAL
- en: \[ (*) \qquad \left\langle \mathbf{v} - \mathbf{p}^*, \mathbf{u}\right\rangle
    =0, \quad \forall \mathbf{u} \in U. \]
  prefs: []
  type: TYPE_NORMAL
- en: c) For any orthonormal basis \(\mathbf{q}_1,\ldots,\mathbf{q}_m\) of \(U\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{proj}_U \mathbf{v} = \mathrm{proj}_{\{\mathbf{q}_i\}_{i=1}^m} \mathbf{v}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(\mathbf{p}^*\) be any vector in \(U\) satisfying \((*)\). We
    show first that it necessarily satisfies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (**) \qquad \|\mathbf{p}^* - \mathbf{v}\| \leq \|\mathbf{p} - \mathbf{v}\|,
    \quad \forall \mathbf{p} \in U. \]
  prefs: []
  type: TYPE_NORMAL
- en: Note that for any \(\mathbf{p} \in U\) the vector \(\mathbf{u} = \mathbf{p}
    - \mathbf{p}^*\) is also in \(U\). Hence by \((*)\) and *Pythagoras*,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{p} - \mathbf{v}\|^2 &= \|\mathbf{p} - \mathbf{p}^*
    + \mathbf{p}^* - \mathbf{v}\|^2\\ &= \|\mathbf{p} - \mathbf{p}^*\|^2 + \|\mathbf{p}^*
    - \mathbf{v}\|^2\\ &\geq \|\mathbf{p}^* - \mathbf{v}\|^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, equality holds only if \(\|\mathbf{p} - \mathbf{p}^*\|^2 = 0\)
    which holds only if \(\mathbf{p} = \mathbf{p}^*\) by the point-separating property
    of the Euclidean norm. Hence, if such a vector \(\mathbf{p}^*\) exists, it is
    unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we show that any minimizer must satisfy \((*)\). Let \(\mathbf{p}^*\)
    be a minimizer and suppose, for contradiction, that \((*)\) does not hold. Then
    there exists \(\mathbf{u} \in U\) with \(\langle \mathbf{v} - \mathbf{p}^*, \mathbf{u}
    \rangle = c \neq 0\). Consider \(\mathbf{p}_t = \mathbf{p}^* + t\mathbf{u}\) for
    small \(t\). Then:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{p}_t - \mathbf{v}\|^2 &= \|(\mathbf{p}^* - \mathbf{v})
    + t\mathbf{u}\|^2\\ &= \|\mathbf{p}^* - \mathbf{v}\|^2 + 2t\langle \mathbf{v}
    - \mathbf{p}^*, \mathbf{u} \rangle + t^2\|\mathbf{u}\|^2\\ &= \|\mathbf{p}^* -
    \mathbf{v}\|^2 + 2tc + t^2\|\mathbf{u}\|^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: For small \(t\) with appropriate sign, this is smaller than \(\|\mathbf{p}^*
    - \mathbf{v}\|^2\), contradicting minimality.
  prefs: []
  type: TYPE_NORMAL
- en: It remains to show that there is at least one vector in \(U\) satisfying \((*)\).
    By the *Gram-Schmidt Theorem*, the linear subspace \(U\) has an orthonormal basis
    \(\mathbf{q}_1,\ldots,\mathbf{q}_m\). By definition, \(\mathrm{proj}_{\{\mathbf{q}_i\}_{i=1}^m}
    \mathbf{v} \in \mathrm{span}(\{\mathbf{q}_i\}_{i=1}^m) = U\). We show that \(\mathrm{proj}_{\{\mathbf{q}_i\}_{i=1}^m}
    \mathbf{v}\) satisfies \((*)\). We can write any \(\mathbf{u} \in U\) as \(\sum_{i=1}^m
    \alpha_i \mathbf{q}_i\) with \(\alpha_i = \langle \mathbf{u}, \mathbf{q}_i \rangle\).
    So, using this representation, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left\langle \mathbf{v} - \sum_{j=1}^m \langle \mathbf{v},
    \mathbf{q}_j \rangle \,\mathbf{q}_j, \sum_{i=1}^m \alpha_i \mathbf{q}_i \right\rangle
    &= \sum_{i=1}^m \langle \mathbf{v}, \mathbf{q}_i \rangle \,\alpha_i - \sum_{j=1}^m
    \sum_{i=1}^m \alpha_i \langle \mathbf{v}, \mathbf{q}_j \rangle \langle \mathbf{q}_j,
    \mathbf{q}_i \rangle\\ &= \sum_{i=1}^m \langle \mathbf{v}, \mathbf{q}_i \rangle
    \,\alpha_i - \sum_{j=1}^m \alpha_j \langle \mathbf{v}, \mathbf{q}_j \rangle\\
    &= 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the orthonormality of the \(\mathbf{q}_j\)’s on the second line.
    \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(continued)** Consider again the linear subspace \(W = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3)\),
    where \(\mathbf{w}_1 = (1,0,1)\), \(\mathbf{w}_2 = (0,1,1)\), and \(\mathbf{w}_3
    = (1,-1,0)\). We have shown that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\ 0\\ 1
    \end{pmatrix}, \quad \mathbf{q}_2 = \frac{1}{\sqrt{6}} \begin{pmatrix} -1\\ 2\\
    1 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: is an orthonormal basis. Let \(\mathbf{w}_4 = (0,0,2)\). It is immediate that
    \(\mathbf{w}_4 \notin \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2)\) since vectors
    in that span are of the form \((x,y,x+y)\) for some \(x,y \in \mathbb{R}\).
  prefs: []
  type: TYPE_NORMAL
- en: We can however compute the orthogonal projection \(\mathbf{w}_4\) onto \(W\).
    The inner products are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{w}_4, \mathbf{q}_1 \rangle = 0 \left(\frac{1}{\sqrt{2}}\right)
    + 0 \left(\frac{0}{\sqrt{2}}\right) + 2 \left(\frac{1}{\sqrt{2}}\right) = \frac{2}{\sqrt{2}},
    \]\[ \langle \mathbf{w}_4, \mathbf{q}_2 \rangle = 0 \left(-\frac{1}{\sqrt{6}}\right)
    + 0 \left(\frac{2}{\sqrt{6}}\right) + 2 \left(\frac{1}{\sqrt{6}}\right) = \frac{2}{\sqrt{6}}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathrm{proj}_W \mathbf{w}_4 = \frac{2}{\sqrt{2}} \mathbf{q}_1
    + \frac{2}{\sqrt{6}} \mathbf{q}_2 = \begin{pmatrix} 2/3\\ 2/3\\ 4/3 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: As a sanity check, note that \(\mathbf{w}_4 \in W\) since its third entry is
    equal to the sum of its first two entries. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The map \(\mathrm{proj}_U\) is linear, that is, \(\mathrm{proj}_U (\alpha \,\mathbf{x}
    + \mathbf{y}) = \alpha \,\mathrm{proj}_U \mathbf{x} + \mathrm{proj}_U\mathbf{y}\)
    for all \(\alpha \in \mathbb{R}\) and \(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\).
    Indeed,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathrm{proj}_U(\alpha \,\mathbf{x} + \mathbf{y}) &= \sum_{j=1}^m
    \langle \alpha \,\mathbf{x} + \mathbf{y}, \mathbf{q}_j \rangle \,\mathbf{q}_j\\
    &= \sum_{j=1}^m \left\{\alpha \, \langle \mathbf{x}, \mathbf{q}_j \rangle + \langle
    \mathbf{y}, \mathbf{q}_j \rangle\right\} \mathbf{q}_j\\ &= \alpha \,\mathrm{proj}_U
    \mathbf{x} + \mathrm{proj}_U \mathbf{y}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Any linear map from \(\mathbb{R}^n\) to \(\mathbb{R}^n\) can be encoded as an
    \(n \times n\) matrix \(P\).
  prefs: []
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \begin{pmatrix} | & & | \\ \mathbf{q}_1 & \ldots & \mathbf{q}_m
    \\ | & & | \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and note that computing
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q^T \mathbf{v} = \begin{pmatrix} \langle \mathbf{v}, \mathbf{q}_1
    \rangle \\ \cdots \\ \langle \mathbf{v}, \mathbf{q}_m \rangle \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: lists the coefficients in the expansion of \(\mathrm{proj}_U \mathbf{v}\) over
    the basis \(\mathbf{q}_1,\ldots,\mathbf{q}_m\).
  prefs: []
  type: TYPE_NORMAL
- en: Hence we see that
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = Q Q^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, for any vector \(\mathbf{v}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P \mathbf{v} = Q Q^T \mathbf{v} = Q [Q^T \mathbf{v}]. \]
  prefs: []
  type: TYPE_NORMAL
- en: So the output is a linear combination of the columns of \(Q\) (i.e., the \(\mathbf{q}_i\)’s)
    where the coefficients are the entries of the vector in square brackets \(Q^T
    \mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(continued)** Consider again the linear subspace \(W = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3)\),
    where \(\mathbf{w}_1 = (1,0,1)\), \(\mathbf{w}_2 = (0,1,1)\), and \(\mathbf{w}_3
    = (1,-1,0)\), with orthonormal basis'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\ 0\\ 1
    \end{pmatrix}, \quad \mathbf{q}_2 = \frac{1}{\sqrt{6}} \begin{pmatrix} -1\\ 2\\
    1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then orthogonal projection onto \(W\) can be written in matrix form as follows.
    The matrix \(Q\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{6}\\ 0 & 2/\sqrt{6}\\
    1/\sqrt{2} & 1/\sqrt{6} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} Q Q^T &= \begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{6}\\ 0 & 2/\sqrt{6}\\
    1/\sqrt{2} & 1/\sqrt{6} \end{pmatrix} \begin{pmatrix} 1/\sqrt{2} & 0 & 1/\sqrt{2}\\
    -1/\sqrt{6} & 2/\sqrt{6} & 1/\sqrt{6} \end{pmatrix}\\ &= \begin{pmatrix} 2/3 &
    -1/3 & 1/3\\ -1/3 & 2/3 & 1/3\\ 1/3 & 1/3 & 2/3 \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So the projection of \(\mathbf{w}_4 = (0,0,2)\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} 2/3 & -1/3 & 1/3\\ -1/3 & 2/3 & 1/3\\ 1/3 &
    1/3 & 2/3 \end{pmatrix} \begin{pmatrix} 0\\ 0\\ 2 \end{pmatrix} = \begin{pmatrix}
    2/3\\ 2/3\\ 4/3 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: as previously computed. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The matrix \(P= Q Q^T\) is not to be confused with
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q^T Q = \begin{pmatrix} \langle \mathbf{q}_1, \mathbf{q}_1 \rangle
    & \cdots & \langle \mathbf{q}_1, \mathbf{q}_m \rangle \\ \langle \mathbf{q}_2,
    \mathbf{q}_1 \rangle & \cdots & \langle \mathbf{q}_2, \mathbf{q}_m \rangle \\
    \vdots & \ddots & \vdots \\ \langle \mathbf{q}_m, \mathbf{q}_1 \rangle & \cdots
    & \langle \mathbf{q}_m, \mathbf{q}_m \rangle \end{pmatrix} = I_{m \times m} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(I_{m \times m}\) denotes the \(m \times m\) identity matrix. This follows
    from the fact that the \(\mathbf{q}_i\)’s are orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(\mathbf{q}_1,\ldots,\mathbf{q}_n\) be an orthonormal basis
    of \(\mathbb{R}^n\) and form the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \begin{pmatrix} | & & | \\ \mathbf{q}_1 & \ldots & \mathbf{q}_n
    \\ | & & | \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We show that \(Q^{-1} = Q^T\).
  prefs: []
  type: TYPE_NORMAL
- en: We just pointed out that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q^T Q = \begin{pmatrix} \langle \mathbf{q}_1, \mathbf{q}_1 \rangle
    & \cdots & \langle \mathbf{q}_1, \mathbf{q}_n \rangle \\ \langle \mathbf{q}_2,
    \mathbf{q}_1 \rangle & \cdots & \langle \mathbf{q}_2, \mathbf{q}_n \rangle \\
    \vdots & \ddots & \vdots \\ \langle \mathbf{q}_n, \mathbf{q}_1 \rangle & \cdots
    & \langle \mathbf{q}_n, \mathbf{q}_n \rangle \end{pmatrix} = I_{n \times n} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(I_{n \times n}\) denotes the \(n \times n\) identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In the other direction, we claim that \(Q Q^T = I_{n \times n}\) as well. Indeed
    the matrix \(Q Q^T\) is the orthogonal projection on the span of the \(\mathbf{q}_i\)’s,
    that is, \(\mathbb{R}^n\). By the *Orthogonal Projection Theorem*, the orthogonal
    projection \(Q Q^T \mathbf{v}\) finds the closest vector to \(\mathbf{v}\) in
    the span of the \(\mathbf{q}_i\)’s. But that span contains all vectors, including
    \(\mathbf{v}\), so we must have \(Q Q^T \mathbf{v} = \mathbf{v}\). Since this
    holds for all \(\mathbf{v} \in \mathbb{R}^n\), the matrix \(Q Q^T\) is the identity
    map and we have proved the claim. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Matrices that satisfy
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q^T Q = Q Q^T = I_{n \times n} \]
  prefs: []
  type: TYPE_NORMAL
- en: are called orthogonal matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Orthogonal Matrix)** \(\idx{orthogonal matrix}\xdi\) A square
    matrix \(Q \in \mathbb{R}^{m\times m}\) is orthogonal if \(Q^T Q = Q Q^T = I_{m
    \times m}\). \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** Let \(\mathcal{Z}\) be a linear subspace of \(\mathbb{R}^n\)
    and let \(\mathbf{v} \in \mathbb{R}^n\). Show that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathrm{proj}_{\mathcal{Z}}\mathbf{v}\|_2 \leq \|\mathbf{v}\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: '[*Hint:* Use the geometric characterization.] \(\checkmark\)'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3\. Orthogonal complement[#](#orthogonal-complement "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before returning to overdetermined systems, we take a little detour to derive
    a consequence of the orthogonal projection that will be useful later. The *Orthogonal
    Projection Theorem* implies that any \(\mathbf{v} \in \mathbb{R}^n\) can be decomposed
    into its orthogonal projection onto \(U\) and a vector orthogonal to it.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Orthogonal Complement)** \(\idx{orthogonal complement}\xdi\)
    Let \(U \subseteq \mathbb{R}^n\) be a linear subspace. The orthogonal complement
    of \(U\), denoted \(U^\perp\), is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ U^\perp = \{\mathbf{w} \in \mathbb{R}^n\,:\, \langle \mathbf{w}, \mathbf{u}\rangle
    = 0, \forall \mathbf{u} \in U\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Continuing a previous example, we compute the orthogonal complement
    of the linear subspace \(W = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3)\),
    where \(\mathbf{w}_1 = (1,0,1)\), \(\mathbf{w}_2 = (0,1,1)\), and \(\mathbf{w}_3
    = (1,-1,0)\). One way to proceed is to find all vectors that are orthogonal to
    the orthonormal basis'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\ 0\\ 1
    \end{pmatrix}, \quad \mathbf{q}_2 = \frac{1}{\sqrt{6}} \begin{pmatrix} -1\\ 2\\
    1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We require
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \langle \mathbf{u}, \mathbf{q}_1 \rangle = u_1 \left(\frac{1}{\sqrt{2}}\right)
    + u_2 \left(\frac{0}{\sqrt{2}}\right) + u_3 \left(\frac{1}{\sqrt{2}}\right) =
    \frac{u_1 + u_3}{\sqrt{2}}, \]\[ 0= \langle \mathbf{u}, \mathbf{q}_2 \rangle =
    u_1 \left(-\frac{1}{\sqrt{6}}\right) + u_2 \left(\frac{2}{\sqrt{6}}\right) + u_3
    \left(\frac{1}{\sqrt{6}}\right) = \frac{-u_1 + 2 u_2 + u_3}{\sqrt{6}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The first equation implies \(u_3 = -u_1\), which after replacing into the second
    equation and rearranging gives \(u_2 = u_1\).
  prefs: []
  type: TYPE_NORMAL
- en: So all vectors of the form \((u_1,u_1,-u_1)\) for some \(u_1 \in \mathbb{R}\)
    are orthogonal to all of \(W\). This is a one-dimensional linear subspace. We
    can choose an orthonormal basis by finding a solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ 1 = (u_1)^2 + (u_1)^2 + (-u_1)^2 = 3 u_1^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: Take \(u_1 = 1/\sqrt{3}\), that is, let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_3 = \frac{1}{\sqrt{3}} \begin{pmatrix} 1\\ 1\\ -1
    \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ W^\perp = \mathrm{span}(\mathbf{q}_3). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Orthogonal Decomposition)** \(\idx{orthogonal decomposition lemma}\xdi\)
    Let \(U \subseteq \mathbb{R}^n\) be a linear subspace and let \(\mathbf{v} \in
    \mathbb{R}^n\). Then \(\mathbf{v}\) can be decomposed as \(\mathrm{proj}_U \mathbf{v}
    + (\mathbf{v} - \mathrm{proj}_U\mathbf{v})\) where \(\mathrm{proj}_U \mathbf{v}
    \in U\) and \((\mathbf{v} - \mathrm{proj}_U \mathbf{v}) \in U^\perp\). Moreover,
    this decomposition is unique in the following sense: if \(\mathbf{v} = \mathbf{u}
    + \mathbf{u}^\perp\) with \(\mathbf{u} \in U\) and \(\mathbf{u}^\perp \in U^\perp\),
    then \(\mathbf{u} = \mathrm{proj}_U \mathbf{v}\) and \(\mathbf{u}^\perp = \mathbf{v}
    - \mathrm{proj}_U \mathbf{v}\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* The first part is an immediate consequence of the *Orthogonal Projection
    Theorem*. For the second part, assume \(\mathbf{v} = \mathbf{u} + \mathbf{u}^\perp\)
    with \(\mathbf{u} \in U\) and \(\mathbf{u}^\perp \in U^\perp\). Subtracting \(\mathbf{v}
    = \mathrm{proj}_U \mathbf{v} + (\mathbf{v} - \mathrm{proj}_U\mathbf{v})\), we
    see that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (*) \qquad \mathbf{0} = \mathbf{w}_1 + \mathbf{w}_2 \]
  prefs: []
  type: TYPE_NORMAL
- en: with
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w}_1 = \mathbf{u} - \mathrm{proj}_U \mathbf{v} \in U, \qquad \mathbf{w}_2
    = \mathbf{u}^\perp - (\mathbf{v} - \mathrm{proj}_U\mathbf{v}) \in U^\perp. \]
  prefs: []
  type: TYPE_NORMAL
- en: If \(\mathbf{w}_1 = \mathbf{w}_2 = \mathbf{0}\), we are done. Otherwise, they
    must both be nonzero by \((*)\). Further, by the *Properties of Orthonormal Lists*,
    \(\mathbf{w}_1\) and \(\mathbf{w}_2\) must be linearly independent. But this is
    contradicted by the fact that \(\mathbf{w}_2 = - \mathbf{w}_1\) by \((*)\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Orthogonal decomposition ([Source](https://commons.wikimedia.org/wiki/File:Orthogonal_Decomposition_qtl1.svg))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Orthogonal decomposition](../Images/0fbd201d54de3ed8d4d4cbf69a88afc2.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, the *Orthogonal Decomposition Lemma* states that \(\mathbb{R}^n\)
    is a direct sum of any linear subspace \(U\) and of its orthogonal complement
    \(U^\perp\): that is, any vector \(\mathbf{v} \in \mathbb{R}^n\) can be written
    uniquely as \(\mathbf{v} = \mathbf{u} + \mathbf{u}^\perp\) with \(\mathbf{u} \in
    U\) and \(\mathbf{u}^\perp \in U^\perp\). This is denoted \(\mathbb{R}^n = U \oplus
    U^\perp\).'
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{a}_1,\ldots,\mathbf{a}_\ell\) be an orthonormal basis of \(U\)
    and \(\mathbf{b}_1,\ldots,\mathbf{b}_k\) be an orthonormal basis of \(U^\perp\).
    By definition of the orthogonal complement, the list
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = \{\mathbf{a}_1,\ldots,\mathbf{a}_\ell, \mathbf{b}_1,\ldots,\mathbf{b}_k\}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: is orthonormal, so it forms a basis of its span. Because any vector in \(\mathbb{R}^n\)
    can be written as a sum of a vector from \(U\) and a vector from \(U^\perp\),
    all of \(\mathbb{R}^n\) is in the span of \(\mathcal{L}\). It follows from the
    *Dimension Theorem* that \(n = \ell + k\), that is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}(U) + \mathrm{dim}(U^\perp) = n. \]
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4\. Overdetermined systems[#](#overdetermined-systems "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss the least-squares problem. Let again \(A \in \mathbb{R}^{n\times
    m}\) be an \(n\times m\) matrix with linearly independent columns and let \(\mathbf{b}
    \in \mathbb{R}^n\) be a vector. We are looking to solve the system
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x} \approx \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'If \(n=m\), we can use the matrix inverse to solve the system (provided \(A\)
    is nonsingular of course). But we are interested in the overdetermined case, i.e.
    when \(n > m\): there are more equations than variables. We cannot use the matrix
    inverse then. Indeed, because the columns do not span all of \(\mathbb{R}^n\),
    there is a vector \(\mathbf{b} \in \mathbb{R}^n\) that is not in the column space
    of \(A\).'
  prefs: []
  type: TYPE_NORMAL
- en: A natural way to make sense of the overdetermined problem is to cast it as the
    [linear least-squares problem](https://en.wikipedia.org/wiki/Least_squares)\(\idx{least
    squares problem}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, we look for the best-fitting solution under the squared Euclidean
    norm. Equivalently, writing
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} = \begin{pmatrix} a_{11} & \cdots & a_{1m} \\ a_{21}
    & \cdots & a_{2m} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nm} \end{pmatrix}
    \quad \text{and} \quad \mathbf{b} = \begin{pmatrix} b_1 \\ \vdots \\ b_n \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: we seek a linear combination of the columns of \(A\) that minimizes the objective
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left\|\,\sum_{j=1}^m x_j \mathbf{a}_j - \mathbf{b}\,\right\|^2 = \sum_{i=1}^n
    \left( \sum_{j=1}^m a_{ij} x_j - b_i \right)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: We have already solved a closely related problem when we introduced the orthogonal
    projection. We make the connection explicit next.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Normal Equations)** \(\idx{normal equations}\xdi\) Let \(A \in
    \mathbb{R}^{n\times m}\) be an \(n\times m\) matrix with \(n \geq m\) and let
    \(\mathbf{b} \in \mathbb{R}^n\) be a vector. A solution \(\mathbf{x}^*\) to the
    linear least-squares problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: satisfies the normal equations
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A \mathbf{x}^* = A^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: If further the columns of \(A\) are linearly independent, then there exists
    a unique solution \(\mathbf{x}^*\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* Apply our characterization of the orthogonal projection onto
    the column space of \(A\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(U = \mathrm{col}(A) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\).
    By the *Orthogonal Projection Theorem*, the orthogonal projection \(\mathbf{p}^*
    = \mathrm{proj}_{U} \mathbf{b}\) of \(\mathbf{b}\) onto \(U\) is the unique, closest
    vector to \(\mathbf{b}\) in \(U\), that is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{p}^* = \arg\min_{\mathbf{p} \in U} \|\mathbf{p} - \mathbf{b}\| =
    \arg\min_{\mathbf{p} \in U} \|\mathbf{p} - \mathbf{b}\|^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Composing with a Non-Decreasing Function Lemma* to justify
    taking a square in the rightmost expression. Because \(\mathbf{p}^*\) is in \(U
    = \mathrm{col}(A)\), it must be of the form \(\mathbf{p}^* = A \mathbf{x}^*\).
    This establishes that \(\mathbf{x}^*\) is a solution to the linear least-squares
    problem in the statement. By the *Orthogonal Projection Theorem*, it must satisfy
    \(\langle \mathbf{b} - A \mathbf{x}^*, \mathbf{u}\rangle = 0\) for all \(\mathbf{u}
    \in U\). Because the columns \(\mathbf{a}_i\) are in \(U\), that implies that
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \langle \mathbf{b} - A \mathbf{x}^*, \mathbf{a}_i\rangle = \mathbf{a}_i^T
    (\mathbf{b} - A \mathbf{x}^*) ,\qquad \forall i\in [m]. \]
  prefs: []
  type: TYPE_NORMAL
- en: Stacking up these equations gives in matrix form
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T (\mathbf{b} - A\mathbf{x}^*) = \mathbf{0}, \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed (after rearranging).
  prefs: []
  type: TYPE_NORMAL
- en: 'Important observation: While we have shown that \(\mathbf{p}^*\) is unique
    (by the *Orthogonal Projection Theorem*), it is not clear at all that \(\mathbf{x}^*\)
    (i.e., the linear combination of columns of \(A\) corresponding to \(\mathbf{p}^*\))
    is unique. We have seen in a previous example that, when \(A\) has full column
    rank, the matrix \(A^T A\) is invertible. That implies the uniqueness claim. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** To solve a linear system in NumPy, use [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html).
    As an example, we consider the overdetermined system with'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ 0 & 1\\ 1 & 1 \end{pmatrix} \quad
    \text{and} \quad \mathbf{b} = \begin{pmatrix} 0\\ 0\\ 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We use [`numpy.ndarray.T`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html)
    for the transpose and [`@`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)
    for matrix-matrix or matrix-vector product.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can also use [`numpy.linalg.lstsq`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html)
    directly on the overdetermined system to compute the least-square solution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Let \(\mathbf{q}_1, \dots, \mathbf{q}_m\) be an orthonormal list of vectors
    in \(\mathbb{R}^n\). Which of the following is the orthogonal projection of a
    vector \(\mathbf{v} \in \mathbb{R}^n\) onto \(\mathrm{span}(\mathbf{q}_1, \dots,
    \mathbf{q}_m)\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\sum_{i=1}^m \mathbf{q}_i\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\sum_{i=1}^m \langle \mathbf{v}, \mathbf{q}_i \rangle\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\sum_{i=1}^m \langle \mathbf{v}, \mathbf{q}_i \rangle \mathbf{q}_i\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\sum_{i=1}^m \langle \mathbf{q}_i, \mathbf{q}_i \rangle \mathbf{v}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** According to the Normal Equations Theorem, what condition must a solution
    \(\bx^*\) to the linear least squares problem satisfy?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(A^T A\bx^* = \bb\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(A^T A\bx^* = A^T \bb\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(A\bx^* = A^T \bb\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(A\bx^* = \bb\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Which property characterizes the orthogonal projection \(\mathrm{proj}_U
    \mathbf{v}\) of a vector \(\mathbf{v}\) onto a subspace \(U\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{v} - \mathrm{proj}_U \mathbf{v}\) is a scalar multiple of \(\mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{v} - \mathrm{proj}_U \mathbf{v}\) is orthogonal to \(\mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{v} - \mathrm{proj}_U \mathbf{v}\) is orthogonal to \(U\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathrm{proj}_U \mathbf{v}\) is always the zero vector.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** What is the interpretation of the linear least squares problem \(A\mathbf{x}
    \approx \mathbf{b}\) in terms of the column space of \(A\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) Finding the exact solution \(\mathbf{x}\) such that \(A\mathbf{x} = \mathbf{b}\).
  prefs: []
  type: TYPE_NORMAL
- en: b) Finding the vector \(\mathbf{x}\) that makes the linear combination \(A\mathbf{x}\)
    of the columns of \(A\) as close as possible to \(\mathbf{b}\) in Euclidean norm.
  prefs: []
  type: TYPE_NORMAL
- en: c) Finding the orthogonal projection of \(\mathbf{b}\) onto the column space
    of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: d) Finding the orthogonal complement of the column space of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which matrix equation must hold true for a matrix \(Q\) to be orthogonal?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(Q Q^T = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(Q Q^T = I\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(Q^T Q = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(Q^T = Q\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: c. Justification: This is the definition of the orthogonal projection
    onto an orthonormal list given in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The Normal Equations Theorem states that a
    solution \(\bx^*\) to the linear least squares problem satisfies \(A^T A\bx^*
    = A^T \bb\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text states that the orthogonal projection
    \(\mathrm{proj}_U \mathbf{v}\) has the property that “the difference \(\mathbf{v}
    - \mathrm{proj}_U \mathbf{v}\) is orthogonal to \(U\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: b. Justification: The text defines the linear least squares problem
    as seeking a linear combination of the columns of \(A\) that minimizes the distance
    to \(\mathbf{b}\) in Euclidean norm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text states that an orthogonal matrix \(Q\)
    must satisfy \(Q Q^T = I\).'
  prefs: []
  type: TYPE_NORMAL
- en: '2.3.1\. A key concept: orthogonality[#](#a-key-concept-orthogonality "Link
    to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Orthogonality plays a key role in linear algebra for data science thanks to
    its computational properties and its connection to the least-squares problem.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Orthogonality)** \(\idx{orthogonality}\xdi\) Vectors \(\mathbf{u}\)
    and \(\mathbf{v}\) in \(\mathbb{R}^n\) (as column vectors) are orthogonal if their
    inner product is zero'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{u}, \mathbf{v} \rangle =\mathbf{u}^T \mathbf{v} = \sum_{i=1}^n
    u_i v_i = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonality has important implications. The following classical result will
    be useful below. Throughout, we use \(\|\mathbf{u}\|\) for the Euclidean norm
    of \(\mathbf{u}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Pythagoras)** \(\idx{Pythagoras'' theorem}\xdi\) Let \(\mathbf{u},
    \mathbf{v} \in \mathbb{R}^n\) be orthogonal. Then \(\|\mathbf{u} + \mathbf{v}\|^2
    = \|\mathbf{u}\|^2 + \|\mathbf{v}\|^2\). \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Using \(\|\mathbf{w}\|^2 = \langle \mathbf{w}, \mathbf{w}\rangle\),
    we get'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{u} + \mathbf{v}\|^2 &= \langle \mathbf{u} + \mathbf{v},
    \mathbf{u} + \mathbf{v}\rangle\\ &= \langle \mathbf{u}, \mathbf{u}\rangle + 2
    \,\langle \mathbf{u}, \mathbf{v}\rangle + \langle \mathbf{v}, \mathbf{v}\rangle\\
    &= \|\mathbf{u}\|^2 + \|\mathbf{v}\|^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: An application of *Pythagoras’ Theorem* is a proof of the *Cauchy-Schwarz Inequality*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Cauchy-Schwarz)* \(\idx{Cauchy-Schwarz inequality}\xdi\) Let \(\mathbf{q}
    = \frac{\mathbf{v}}{\|\mathbf{v}\|}\) be the unit vector in the direction of \(\mathbf{v}\).
    We want to show \(|\langle \mathbf{u}, \mathbf{q}\rangle| \leq \|\mathbf{u}\|\).
    Decompose \(\mathbf{u}\) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{u} = \left\langle \mathbf{u}, \mathbf{q}\right\rangle \mathbf{q}
    + \left\{\mathbf{u} - \left\langle \mathbf{u}, \mathbf{q}\right\rangle \mathbf{q}\right\}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The two terms on the right-hand side are orthogonal:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left\langle \left\langle \mathbf{u}, \mathbf{q}\right\rangle \mathbf{q},
    \mathbf{u} - \left\langle \mathbf{u}, \mathbf{q}\right\rangle \mathbf{q} \right\rangle
    = \left\langle \mathbf{u}, \mathbf{q}\right\rangle^2 - \left\langle \mathbf{u},
    \mathbf{q}\right\rangle^2 \left\langle \mathbf{q}, \mathbf{q}\right\rangle = 0.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So *Pythagoras* gives
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{u}\|^2 = \left\|\left\langle \mathbf{u}, \mathbf{q}\right\rangle
    \mathbf{q}\right\|^2 + \left\|\mathbf{u} - \left\langle \mathbf{u}, \mathbf{q}\right\rangle
    \mathbf{q}\right\|^2 \geq \left\|\left\langle \mathbf{u}, \mathbf{q}\right\rangle
    \mathbf{q}\right\|^2 = \left\langle \mathbf{u}, \mathbf{q}\right\rangle^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: Taking a square root gives the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Orthonormal basis expansion** To begin to see the power of orthogonality,
    consider the following. A list of vectors \(\{\mathbf{u}_1,\ldots,\mathbf{u}_m\}\)
    is an orthonormal list if the \(\mathbf{u}_i\)’s are pairwise orthogonal and each
    has norm 1, that is, for all \(i\) and all \(j \neq i\), we have \(\|\mathbf{u}_i\|
    = 1\) and \(\langle \mathbf{u}_i, \mathbf{u}_j \rangle = 0\). Alternatively,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \langle \mathbf{u}_i, \mathbf{u}_j \rangle = \begin{cases} 1
    & \text{if $i=j$}\\ 0 & \text{if $i\neq j$}. \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Properties of Orthonormal Lists)** \(\idx{properties of orthonormal
    lists}\xdi\) Let \(\{\mathbf{u}_1,\ldots,\mathbf{u}_m\}\) be an orthonormal list.
    Then:'
  prefs: []
  type: TYPE_NORMAL
- en: for any \(\alpha_j \in \mathbb{R}\), \(j=1,\ldots,m\), we have
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \[ \left\|\sum_{j=1}^m \alpha_j \mathbf{u}_j\right\|^2 = \sum_{j=1}^m \alpha_j^2,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: the vectors \(\{\mathbf{u}_1,\ldots,\mathbf{u}_m\}\) are linearly independent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For 1., using that \(\|\mathbf{x}\|^2 = \langle \mathbf{x}, \mathbf{x}
    \rangle\), we have'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left\|\sum_{j=1}^m \alpha_j \mathbf{u}_j\right\|^2 &= \left\langle
    \sum_{i=1}^m \alpha_i \mathbf{u}_i, \sum_{j=1}^m \alpha_j \mathbf{u}_j \right\rangle\\
    &= \sum_{i=1}^m \alpha_i \left\langle \mathbf{u}_i, \sum_{j=1}^m \alpha_j \mathbf{u}_j
    \right\rangle\\ &= \sum_{i=1}^m \sum_{j=1}^m \alpha_i \alpha_j \left\langle \mathbf{u}_i,
    \mathbf{u}_j \right\rangle\\ &= \sum_{i=1}^m \alpha_i^2 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used orthonormality in the last equation, that is, \(\langle \mathbf{u}_i,
    \mathbf{u}_j \rangle\) is \(1\) if \(i=j\) and \(0\) otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: For 2., suppose \(\sum_{i=1}^m \beta_i \mathbf{u}_i = \mathbf{0}\), then we
    must have by 1\. that \(\sum_{i=1}^m \beta_i^2 = 0\). That implies \(\beta_i =
    0\) for all \(i\). Hence the \(\mathbf{u}_i\)’s are linearly independent. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a basis \(\{\mathbf{u}_1,\ldots,\mathbf{u}_m\}\) of \(U\), we know that:
    for any \(\mathbf{w} \in U\), \(\mathbf{w} = \sum_{i=1}^m \alpha_i \mathbf{u}_i\)
    for some \(\alpha_i\)’s. It is not immediately obvious in general how to find
    the \(\alpha_i\)’s – one must solve a system of linear equations. In the orthonormal
    case, however, there is a formula. We say that the basis \(\{\mathbf{u}_1,\ldots,\mathbf{u}_m\}\)
    is orthonormal if it forms an orthonormal list.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Orthonormal Expansion)** \(\idx{orthonormal expansion theorem}\xdi\)
    Let \(\mathbf{q}_1,\ldots,\mathbf{q}_m\) be an orthonormal basis of \(U\) and
    let \(\mathbf{w} \in U\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w} = \sum_{j=1}^m \langle \mathbf{w}, \mathbf{q}_j\rangle \,\mathbf{q}_j.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Because \(\mathbf{w} \in U\), \(\mathbf{w} = \sum_{i=1}^m \alpha_i
    \mathbf{q}_i\) for some \(\alpha_i\). Take the inner product with \(\mathbf{q}_j\)
    and use orthonormality:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{w}, \mathbf{q}_j\rangle = \left\langle \sum_{i=1}^m \alpha_i
    \mathbf{q}_i, \mathbf{q}_j\right\rangle = \sum_{i=1}^m \alpha_i \langle \mathbf{q}_i,
    \mathbf{q}_j\rangle = \alpha_j. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we have determined all \(\alpha_j\)’s in the basis expansion of \(\mathbf{w}\).
    \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Consider again the linear subspace \(W = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3)\),
    where \(\mathbf{w}_1 = (1,0,1)\), \(\mathbf{w}_2 = (0,1,1)\), and \(\mathbf{w}_3
    = (1,-1,0)\). We have shown that in fact'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3) = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2),
    \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(\mathbf{w}_1,\mathbf{w}_2\) form a basis of \(W\). On the other hand,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle\mathbf{w}_1,\mathbf{w}_2\rangle = (1)(0) + (0)(1) + (1)(1) = 0 +
    0 + 1 = 1 \neq 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: so this basis is not orthonormal. Indeed, an orthonormal list is necessarily
    an independent list, but the opposite may not hold.
  prefs: []
  type: TYPE_NORMAL
- en: To produce an orthonormal basis of \(W\), we can first proceed by normalizing
    \(\mathbf{w}_1\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{q}_1 = \frac{\mathbf{w}_1}{\|\mathbf{w}_1\|} = \frac{\mathbf{w}_1}{\sqrt{1^2
    + 0^2 + 1^2}} = \frac{1}{\sqrt{2}} \mathbf{w}_1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then \(\|\mathbf{q}_1\| = 1\) since, in general, by absolute homogeneity of
    the norm
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left\|\frac{\mathbf{w}_1}{\|\mathbf{w}_1\|}\right\| = \frac{1}{\|\mathbf{w}_1\|}
    \|\mathbf{w}_1\| = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We then seek a second basis vector. It must satisfy two conditions in this
    case:'
  prefs: []
  type: TYPE_NORMAL
- en: it must be of unit norm and be orthogonal to \(\mathbf{q}_1\); and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\mathbf{w}_2\) must be a linear combination of \(\mathbf{q}_1\) and \(\mathbf{q}_2\).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latter condition guarantees that \(\mathrm{span}(\mathbf{q}_1,\mathbf{q}_2)
    = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2)\). (Formally, that would imply only
    that \(\mathrm{span}(\mathbf{w}_1,\mathbf{w}_2) \subseteq \mathrm{span}(\mathbf{q}_1,\mathbf{q}_2)\).
    In this case, it is easy to see that the containment must go in the opposite direction
    as well. Why?)
  prefs: []
  type: TYPE_NORMAL
- en: The first condition translates into
  prefs: []
  type: TYPE_NORMAL
- en: \[ 1 = \|\mathbf{q}_2\|^2 = q_{21}^2 + q_{22}^2 + q_{23}^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{q}_2 = (q_{21}, q_{22}, q_{23})\), and
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \langle\mathbf{q}_1, \mathbf{q}_2\rangle = \frac{1}{\sqrt{2}}\left[1\cdot
    q_{21} + 0 \cdot q_{22} + 1 \cdot q_{23}\right] = \frac{1}{\sqrt{2}}\left[q_{21}
    + q_{23}\right]. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, simplifying the second display and plugging into the first, \(q_{23}
    = -q_{21}\) and \(q_{22} = \sqrt{1 - 2 q_{21}^2}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second condition translates into: there is \(\beta_1, \beta_2 \in \mathbb{R}\)
    such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{w}_2 = \begin{pmatrix} 0 \\ 1 \\ 1 \end{pmatrix} = \beta_1
    \mathbf{q}_1 + \beta_2 \mathbf{q}_2 = \beta_1 \frac{1}{\sqrt{2}} \begin{pmatrix}
    1 \\ 0 \\ 1 \end{pmatrix} + \beta_2 \begin{pmatrix} q_{21} \\ \sqrt{1-2 q_{21}^2}
    \\ -q_{21} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The first entry gives \(\beta_1/\sqrt{2} + \beta_2 q_{21} = 0\) while the third
    entry gives \(\beta_1/\sqrt{2} - \beta_2 q_{21} = 1\). Adding up the equations
    gives \(\beta_1 = 1/\sqrt{2}\). Plugging back into the first one gives \(\beta_2
    = -1/(2q_{21})\). Returning to the equation for \(\mathbf{w}_2\), we get from
    the second entry
  prefs: []
  type: TYPE_NORMAL
- en: \[ 1 = - \frac{1}{2 q_{21}} \sqrt{1 - 2 q_{21}^2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Rearranging and taking a square, we want the negative solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ 4 q_{21}^2 = 1 - 2 q_{21}^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, \(q_{21} = - 1/\sqrt{6}\). Finally, we get \(q_{23} = - q_{21} = 1/\sqrt{6}\)
    and \(q_{22} = \sqrt{1 - 2 q_{21}^2} = \sqrt{1 - 1/3} = \sqrt{2/3} = 2/\sqrt{6}\).
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we have
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\ 0\\ 1
    \end{pmatrix}, \quad \mathbf{q}_2 = \frac{1}{\sqrt{6}} \begin{pmatrix} -1\\ 2\\
    1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We confirm that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{q}_1, \mathbf{q}_2\rangle = \frac{1}{\sqrt{2}\sqrt{6}}[(1)(-1)
    + (0)(2) + (1)(1)] = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{q}_2\|^2 = \left(-\frac{1}{\sqrt{6}}\right)^2 + \left(\frac{2}{\sqrt{6}}\right)^2
    + \left(\frac{1}{\sqrt{6}}\right)^2 = \frac{1}{6} + \frac{4}{6} + \frac{1}{6}
    = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: We can use the *Orthonormal Expansion Theorem* to write \(\mathbf{w}_2\) as
    a linear combination of \(\mathbf{q}_1\) and \(\mathbf{q}_2\). The inner products
    are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{w}_2, \mathbf{q}_1 \rangle = 0 \left(\frac{1}{\sqrt{2}}\right)
    + 1 \left(\frac{0}{\sqrt{2}}\right) + 1 \left(\frac{1}{\sqrt{2}}\right) = \frac{1}{\sqrt{2}},
    \]\[ \langle \mathbf{w}_2, \mathbf{q}_2 \rangle = 0 \left(-\frac{1}{\sqrt{6}}\right)
    + 1 \left(\frac{2}{\sqrt{6}}\right) + 1 \left(\frac{1}{\sqrt{6}}\right) = \frac{3}{\sqrt{6}}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w}_2 = \frac{1}{\sqrt{2}} \mathbf{q}_1 + \frac{3}{\sqrt{6}} \mathbf{q}_2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Check it! Try \(\mathbf{w}_3\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Gram-Schmidt** We have shown that working with orthonormal bases is desirable.
    What if we do not have one? We could try to construct one by hand as we did in
    the previous example. But there are better ways. We review the [Gram-Schmidt algorithm](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process)
    in an upcoming section, which will imply that every linear subspace has an orthonormal
    basis. That is, we will prove the following theorem.'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Gram-Schmidt)** \(\idx{Gram-Schmidt theorem}\xdi\) Let \(\mathbf{a}_1,\ldots,\mathbf{a}_m\)
    be linearly independent. Then there exists an orthonormal basis \(\mathbf{q}_1,\ldots,\mathbf{q}_m\)
    of \(\mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\). \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: But, first, we will need to define the orthogonal projection, which will play
    a key role in our applications. This is done next.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2\. Orthogonal projection[#](#orthogonal-projection "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To solve the overdetermined case, i.e., when \(n > m\), we consider the following
    more general problem first. We have a linear subspace \(U \subseteq \mathbb{R}^n\)
    and a vector \(\mathbf{v} \notin U\). We want to find the vector \(\mathbf{p}\)
    in \(U\) that is closest to \(\mathbf{v}\) in Euclidean norm, that is, we want
    to solve
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{p} \in U} \|\mathbf{p} - \mathbf{v}\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Consider the two-dimensional case with a one-dimensional subspace,
    say \(U = \mathrm{span}(\mathbf{u}_1)\) with \(\|\mathbf{u}_1\|=1\). The geometrical
    intuition is in the following figure. The solution \(\mathbf{p} = \mathbf{v}^*\)
    has the property that the difference \(\mathbf{v} - \mathbf{v}^*\) makes a right
    angle with \(\mathbf{u}_1\), that is, it is orthogonal to it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Orthogonal projection on a line (with help from ChatGPT and Claude; inspired
    by Source)](../Images/067d2c4143b5d805e9e1b2381a00c9db.png)'
  prefs: []
  type: TYPE_IMG
- en: Letting \(\mathbf{v}^* = \alpha^* \,\mathbf{u}_1\), the geometrical condition
    above translates into
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \langle \mathbf{u}_1, \mathbf{v} - \mathbf{v}^* \rangle = \langle \mathbf{u}_1,
    \mathbf{v} - \alpha^* \,\mathbf{u}_1 \rangle = \langle \mathbf{u}_1, \mathbf{v}
    \rangle - \alpha^* \,\langle \mathbf{u}_1, \mathbf{u}_1 \rangle = \langle \mathbf{u}_1,
    \mathbf{v} \rangle - \alpha^* \]
  prefs: []
  type: TYPE_NORMAL
- en: so
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{v}^* = \langle \mathbf{u}_1, \mathbf{v} \rangle \,\mathbf{u}_1. \]
  prefs: []
  type: TYPE_NORMAL
- en: By *Pythagoras’ Theorem*, we then have for any \(\alpha \in \mathbb{R}\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{v} - \alpha \,\mathbf{u}_1\|^2 &= \|\mathbf{v}- \mathbf{v}^*
    + \mathbf{v}^* - \alpha \,\mathbf{u}_1\|^2\\ &= \|\mathbf{v}- \mathbf{v}^* + (\alpha^*
    - \alpha) \,\mathbf{u}_1\|^2\\ &= \|\mathbf{v}- \mathbf{v}^*\|^2 + \| (\alpha^*
    - \alpha) \,\mathbf{u}_1\|^2\\ &\geq \|\mathbf{v}- \mathbf{v}^*\|^2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(\mathbf{v} - \mathbf{v}^*\) is orthogonal to \(\mathbf{u}_1\)
    (and therefore \((\alpha^* - \alpha) \mathbf{u}_1\)) on the third line.
  prefs: []
  type: TYPE_NORMAL
- en: That confirms the optimality of \(\mathbf{v}^*\). The argument in this example
    carries through in higher dimension, as we show next. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Orthogonal Projection on an Orthonormal List)** \(\idx{orthogonal
    projection on an orthonormal list}\xdi\) Let \(\mathbf{q}_1,\ldots,\mathbf{q}_m\)
    be an orthonormal list. The orthogonal projection of \(\mathbf{v} \in \mathbb{R}^n\)
    on \(\{\mathbf{q}_i\}_{i=1}^m\) is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{proj}_{\{\mathbf{q}_i\}_{i=1}^m} \mathbf{v} = \sum_{j=1}^m \langle
    \mathbf{v}, \mathbf{q}_j \rangle \,\mathbf{q}_j. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Orthogonal Projection)** \(\idx{orthogonal projection theorem}\xdi\)
    Let \(U \subseteq V\) be a linear subspace and let \(\mathbf{v} \in \mathbb{R}^n\).
    Then:'
  prefs: []
  type: TYPE_NORMAL
- en: a) There exists a unique solution \(\mathbf{p}^*\) to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{p} \in U} \|\mathbf{p} - \mathbf{v}\|. \]
  prefs: []
  type: TYPE_NORMAL
- en: We denote it by \(\mathbf{p}^* = \mathrm{proj}_U \mathbf{v}\) and refer to it
    as the orthogonal projection of \(\mathbf{v}\) onto \(U\).
  prefs: []
  type: TYPE_NORMAL
- en: b) The solution \(\mathbf{p}^* \in U\) is characterized geometrically by
  prefs: []
  type: TYPE_NORMAL
- en: \[ (*) \qquad \left\langle \mathbf{v} - \mathbf{p}^*, \mathbf{u}\right\rangle
    =0, \quad \forall \mathbf{u} \in U. \]
  prefs: []
  type: TYPE_NORMAL
- en: c) For any orthonormal basis \(\mathbf{q}_1,\ldots,\mathbf{q}_m\) of \(U\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{proj}_U \mathbf{v} = \mathrm{proj}_{\{\mathbf{q}_i\}_{i=1}^m} \mathbf{v}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(\mathbf{p}^*\) be any vector in \(U\) satisfying \((*)\). We
    show first that it necessarily satisfies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (**) \qquad \|\mathbf{p}^* - \mathbf{v}\| \leq \|\mathbf{p} - \mathbf{v}\|,
    \quad \forall \mathbf{p} \in U. \]
  prefs: []
  type: TYPE_NORMAL
- en: Note that for any \(\mathbf{p} \in U\) the vector \(\mathbf{u} = \mathbf{p}
    - \mathbf{p}^*\) is also in \(U\). Hence by \((*)\) and *Pythagoras*,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{p} - \mathbf{v}\|^2 &= \|\mathbf{p} - \mathbf{p}^*
    + \mathbf{p}^* - \mathbf{v}\|^2\\ &= \|\mathbf{p} - \mathbf{p}^*\|^2 + \|\mathbf{p}^*
    - \mathbf{v}\|^2\\ &\geq \|\mathbf{p}^* - \mathbf{v}\|^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, equality holds only if \(\|\mathbf{p} - \mathbf{p}^*\|^2 = 0\)
    which holds only if \(\mathbf{p} = \mathbf{p}^*\) by the point-separating property
    of the Euclidean norm. Hence, if such a vector \(\mathbf{p}^*\) exists, it is
    unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we show that any minimizer must satisfy \((*)\). Let \(\mathbf{p}^*\)
    be a minimizer and suppose, for contradiction, that \((*)\) does not hold. Then
    there exists \(\mathbf{u} \in U\) with \(\langle \mathbf{v} - \mathbf{p}^*, \mathbf{u}
    \rangle = c \neq 0\). Consider \(\mathbf{p}_t = \mathbf{p}^* + t\mathbf{u}\) for
    small \(t\). Then:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{p}_t - \mathbf{v}\|^2 &= \|(\mathbf{p}^* - \mathbf{v})
    + t\mathbf{u}\|^2\\ &= \|\mathbf{p}^* - \mathbf{v}\|^2 + 2t\langle \mathbf{v}
    - \mathbf{p}^*, \mathbf{u} \rangle + t^2\|\mathbf{u}\|^2\\ &= \|\mathbf{p}^* -
    \mathbf{v}\|^2 + 2tc + t^2\|\mathbf{u}\|^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: For small \(t\) with appropriate sign, this is smaller than \(\|\mathbf{p}^*
    - \mathbf{v}\|^2\), contradicting minimality.
  prefs: []
  type: TYPE_NORMAL
- en: It remains to show that there is at least one vector in \(U\) satisfying \((*)\).
    By the *Gram-Schmidt Theorem*, the linear subspace \(U\) has an orthonormal basis
    \(\mathbf{q}_1,\ldots,\mathbf{q}_m\). By definition, \(\mathrm{proj}_{\{\mathbf{q}_i\}_{i=1}^m}
    \mathbf{v} \in \mathrm{span}(\{\mathbf{q}_i\}_{i=1}^m) = U\). We show that \(\mathrm{proj}_{\{\mathbf{q}_i\}_{i=1}^m}
    \mathbf{v}\) satisfies \((*)\). We can write any \(\mathbf{u} \in U\) as \(\sum_{i=1}^m
    \alpha_i \mathbf{q}_i\) with \(\alpha_i = \langle \mathbf{u}, \mathbf{q}_i \rangle\).
    So, using this representation, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left\langle \mathbf{v} - \sum_{j=1}^m \langle \mathbf{v},
    \mathbf{q}_j \rangle \,\mathbf{q}_j, \sum_{i=1}^m \alpha_i \mathbf{q}_i \right\rangle
    &= \sum_{i=1}^m \langle \mathbf{v}, \mathbf{q}_i \rangle \,\alpha_i - \sum_{j=1}^m
    \sum_{i=1}^m \alpha_i \langle \mathbf{v}, \mathbf{q}_j \rangle \langle \mathbf{q}_j,
    \mathbf{q}_i \rangle\\ &= \sum_{i=1}^m \langle \mathbf{v}, \mathbf{q}_i \rangle
    \,\alpha_i - \sum_{j=1}^m \alpha_j \langle \mathbf{v}, \mathbf{q}_j \rangle\\
    &= 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the orthonormality of the \(\mathbf{q}_j\)’s on the second line.
    \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(continued)** Consider again the linear subspace \(W = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3)\),
    where \(\mathbf{w}_1 = (1,0,1)\), \(\mathbf{w}_2 = (0,1,1)\), and \(\mathbf{w}_3
    = (1,-1,0)\). We have shown that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\ 0\\ 1
    \end{pmatrix}, \quad \mathbf{q}_2 = \frac{1}{\sqrt{6}} \begin{pmatrix} -1\\ 2\\
    1 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: is an orthonormal basis. Let \(\mathbf{w}_4 = (0,0,2)\). It is immediate that
    \(\mathbf{w}_4 \notin \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2)\) since vectors
    in that span are of the form \((x,y,x+y)\) for some \(x,y \in \mathbb{R}\).
  prefs: []
  type: TYPE_NORMAL
- en: We can however compute the orthogonal projection \(\mathbf{w}_4\) onto \(W\).
    The inner products are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{w}_4, \mathbf{q}_1 \rangle = 0 \left(\frac{1}{\sqrt{2}}\right)
    + 0 \left(\frac{0}{\sqrt{2}}\right) + 2 \left(\frac{1}{\sqrt{2}}\right) = \frac{2}{\sqrt{2}},
    \]\[ \langle \mathbf{w}_4, \mathbf{q}_2 \rangle = 0 \left(-\frac{1}{\sqrt{6}}\right)
    + 0 \left(\frac{2}{\sqrt{6}}\right) + 2 \left(\frac{1}{\sqrt{6}}\right) = \frac{2}{\sqrt{6}}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathrm{proj}_W \mathbf{w}_4 = \frac{2}{\sqrt{2}} \mathbf{q}_1
    + \frac{2}{\sqrt{6}} \mathbf{q}_2 = \begin{pmatrix} 2/3\\ 2/3\\ 4/3 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: As a sanity check, note that \(\mathbf{w}_4 \in W\) since its third entry is
    equal to the sum of its first two entries. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The map \(\mathrm{proj}_U\) is linear, that is, \(\mathrm{proj}_U (\alpha \,\mathbf{x}
    + \mathbf{y}) = \alpha \,\mathrm{proj}_U \mathbf{x} + \mathrm{proj}_U\mathbf{y}\)
    for all \(\alpha \in \mathbb{R}\) and \(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\).
    Indeed,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathrm{proj}_U(\alpha \,\mathbf{x} + \mathbf{y}) &= \sum_{j=1}^m
    \langle \alpha \,\mathbf{x} + \mathbf{y}, \mathbf{q}_j \rangle \,\mathbf{q}_j\\
    &= \sum_{j=1}^m \left\{\alpha \, \langle \mathbf{x}, \mathbf{q}_j \rangle + \langle
    \mathbf{y}, \mathbf{q}_j \rangle\right\} \mathbf{q}_j\\ &= \alpha \,\mathrm{proj}_U
    \mathbf{x} + \mathrm{proj}_U \mathbf{y}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Any linear map from \(\mathbb{R}^n\) to \(\mathbb{R}^n\) can be encoded as an
    \(n \times n\) matrix \(P\).
  prefs: []
  type: TYPE_NORMAL
- en: Let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \begin{pmatrix} | & & | \\ \mathbf{q}_1 & \ldots & \mathbf{q}_m
    \\ | & & | \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and note that computing
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q^T \mathbf{v} = \begin{pmatrix} \langle \mathbf{v}, \mathbf{q}_1
    \rangle \\ \cdots \\ \langle \mathbf{v}, \mathbf{q}_m \rangle \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: lists the coefficients in the expansion of \(\mathrm{proj}_U \mathbf{v}\) over
    the basis \(\mathbf{q}_1,\ldots,\mathbf{q}_m\).
  prefs: []
  type: TYPE_NORMAL
- en: Hence we see that
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = Q Q^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, for any vector \(\mathbf{v}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ P \mathbf{v} = Q Q^T \mathbf{v} = Q [Q^T \mathbf{v}]. \]
  prefs: []
  type: TYPE_NORMAL
- en: So the output is a linear combination of the columns of \(Q\) (i.e., the \(\mathbf{q}_i\)’s)
    where the coefficients are the entries of the vector in square brackets \(Q^T
    \mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(continued)** Consider again the linear subspace \(W = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3)\),
    where \(\mathbf{w}_1 = (1,0,1)\), \(\mathbf{w}_2 = (0,1,1)\), and \(\mathbf{w}_3
    = (1,-1,0)\), with orthonormal basis'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\ 0\\ 1
    \end{pmatrix}, \quad \mathbf{q}_2 = \frac{1}{\sqrt{6}} \begin{pmatrix} -1\\ 2\\
    1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then orthogonal projection onto \(W\) can be written in matrix form as follows.
    The matrix \(Q\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{6}\\ 0 & 2/\sqrt{6}\\
    1/\sqrt{2} & 1/\sqrt{6} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} Q Q^T &= \begin{pmatrix} 1/\sqrt{2} & -1/\sqrt{6}\\ 0 & 2/\sqrt{6}\\
    1/\sqrt{2} & 1/\sqrt{6} \end{pmatrix} \begin{pmatrix} 1/\sqrt{2} & 0 & 1/\sqrt{2}\\
    -1/\sqrt{6} & 2/\sqrt{6} & 1/\sqrt{6} \end{pmatrix}\\ &= \begin{pmatrix} 2/3 &
    -1/3 & 1/3\\ -1/3 & 2/3 & 1/3\\ 1/3 & 1/3 & 2/3 \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So the projection of \(\mathbf{w}_4 = (0,0,2)\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} 2/3 & -1/3 & 1/3\\ -1/3 & 2/3 & 1/3\\ 1/3 &
    1/3 & 2/3 \end{pmatrix} \begin{pmatrix} 0\\ 0\\ 2 \end{pmatrix} = \begin{pmatrix}
    2/3\\ 2/3\\ 4/3 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: as previously computed. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The matrix \(P= Q Q^T\) is not to be confused with
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q^T Q = \begin{pmatrix} \langle \mathbf{q}_1, \mathbf{q}_1 \rangle
    & \cdots & \langle \mathbf{q}_1, \mathbf{q}_m \rangle \\ \langle \mathbf{q}_2,
    \mathbf{q}_1 \rangle & \cdots & \langle \mathbf{q}_2, \mathbf{q}_m \rangle \\
    \vdots & \ddots & \vdots \\ \langle \mathbf{q}_m, \mathbf{q}_1 \rangle & \cdots
    & \langle \mathbf{q}_m, \mathbf{q}_m \rangle \end{pmatrix} = I_{m \times m} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(I_{m \times m}\) denotes the \(m \times m\) identity matrix. This follows
    from the fact that the \(\mathbf{q}_i\)’s are orthonormal.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(\mathbf{q}_1,\ldots,\mathbf{q}_n\) be an orthonormal basis
    of \(\mathbb{R}^n\) and form the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \begin{pmatrix} | & & | \\ \mathbf{q}_1 & \ldots & \mathbf{q}_n
    \\ | & & | \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We show that \(Q^{-1} = Q^T\).
  prefs: []
  type: TYPE_NORMAL
- en: We just pointed out that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q^T Q = \begin{pmatrix} \langle \mathbf{q}_1, \mathbf{q}_1 \rangle
    & \cdots & \langle \mathbf{q}_1, \mathbf{q}_n \rangle \\ \langle \mathbf{q}_2,
    \mathbf{q}_1 \rangle & \cdots & \langle \mathbf{q}_2, \mathbf{q}_n \rangle \\
    \vdots & \ddots & \vdots \\ \langle \mathbf{q}_n, \mathbf{q}_1 \rangle & \cdots
    & \langle \mathbf{q}_n, \mathbf{q}_n \rangle \end{pmatrix} = I_{n \times n} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(I_{n \times n}\) denotes the \(n \times n\) identity matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In the other direction, we claim that \(Q Q^T = I_{n \times n}\) as well. Indeed
    the matrix \(Q Q^T\) is the orthogonal projection on the span of the \(\mathbf{q}_i\)’s,
    that is, \(\mathbb{R}^n\). By the *Orthogonal Projection Theorem*, the orthogonal
    projection \(Q Q^T \mathbf{v}\) finds the closest vector to \(\mathbf{v}\) in
    the span of the \(\mathbf{q}_i\)’s. But that span contains all vectors, including
    \(\mathbf{v}\), so we must have \(Q Q^T \mathbf{v} = \mathbf{v}\). Since this
    holds for all \(\mathbf{v} \in \mathbb{R}^n\), the matrix \(Q Q^T\) is the identity
    map and we have proved the claim. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Matrices that satisfy
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q^T Q = Q Q^T = I_{n \times n} \]
  prefs: []
  type: TYPE_NORMAL
- en: are called orthogonal matrices.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Orthogonal Matrix)** \(\idx{orthogonal matrix}\xdi\) A square
    matrix \(Q \in \mathbb{R}^{m\times m}\) is orthogonal if \(Q^T Q = Q Q^T = I_{m
    \times m}\). \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** Let \(\mathcal{Z}\) be a linear subspace of \(\mathbb{R}^n\)
    and let \(\mathbf{v} \in \mathbb{R}^n\). Show that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathrm{proj}_{\mathcal{Z}}\mathbf{v}\|_2 \leq \|\mathbf{v}\|_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: '[*Hint:* Use the geometric characterization.] \(\checkmark\)'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3\. Orthogonal complement[#](#orthogonal-complement "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before returning to overdetermined systems, we take a little detour to derive
    a consequence of the orthogonal projection that will be useful later. The *Orthogonal
    Projection Theorem* implies that any \(\mathbf{v} \in \mathbb{R}^n\) can be decomposed
    into its orthogonal projection onto \(U\) and a vector orthogonal to it.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Orthogonal Complement)** \(\idx{orthogonal complement}\xdi\)
    Let \(U \subseteq \mathbb{R}^n\) be a linear subspace. The orthogonal complement
    of \(U\), denoted \(U^\perp\), is defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ U^\perp = \{\mathbf{w} \in \mathbb{R}^n\,:\, \langle \mathbf{w}, \mathbf{u}\rangle
    = 0, \forall \mathbf{u} \in U\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Continuing a previous example, we compute the orthogonal complement
    of the linear subspace \(W = \mathrm{span}(\mathbf{w}_1,\mathbf{w}_2,\mathbf{w}_3)\),
    where \(\mathbf{w}_1 = (1,0,1)\), \(\mathbf{w}_2 = (0,1,1)\), and \(\mathbf{w}_3
    = (1,-1,0)\). One way to proceed is to find all vectors that are orthogonal to
    the orthonormal basis'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_1 = \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\ 0\\ 1
    \end{pmatrix}, \quad \mathbf{q}_2 = \frac{1}{\sqrt{6}} \begin{pmatrix} -1\\ 2\\
    1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We require
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \langle \mathbf{u}, \mathbf{q}_1 \rangle = u_1 \left(\frac{1}{\sqrt{2}}\right)
    + u_2 \left(\frac{0}{\sqrt{2}}\right) + u_3 \left(\frac{1}{\sqrt{2}}\right) =
    \frac{u_1 + u_3}{\sqrt{2}}, \]\[ 0= \langle \mathbf{u}, \mathbf{q}_2 \rangle =
    u_1 \left(-\frac{1}{\sqrt{6}}\right) + u_2 \left(\frac{2}{\sqrt{6}}\right) + u_3
    \left(\frac{1}{\sqrt{6}}\right) = \frac{-u_1 + 2 u_2 + u_3}{\sqrt{6}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The first equation implies \(u_3 = -u_1\), which after replacing into the second
    equation and rearranging gives \(u_2 = u_1\).
  prefs: []
  type: TYPE_NORMAL
- en: So all vectors of the form \((u_1,u_1,-u_1)\) for some \(u_1 \in \mathbb{R}\)
    are orthogonal to all of \(W\). This is a one-dimensional linear subspace. We
    can choose an orthonormal basis by finding a solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ 1 = (u_1)^2 + (u_1)^2 + (-u_1)^2 = 3 u_1^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: Take \(u_1 = 1/\sqrt{3}\), that is, let
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{q}_3 = \frac{1}{\sqrt{3}} \begin{pmatrix} 1\\ 1\\ -1
    \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ W^\perp = \mathrm{span}(\mathbf{q}_3). \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Orthogonal Decomposition)** \(\idx{orthogonal decomposition lemma}\xdi\)
    Let \(U \subseteq \mathbb{R}^n\) be a linear subspace and let \(\mathbf{v} \in
    \mathbb{R}^n\). Then \(\mathbf{v}\) can be decomposed as \(\mathrm{proj}_U \mathbf{v}
    + (\mathbf{v} - \mathrm{proj}_U\mathbf{v})\) where \(\mathrm{proj}_U \mathbf{v}
    \in U\) and \((\mathbf{v} - \mathrm{proj}_U \mathbf{v}) \in U^\perp\). Moreover,
    this decomposition is unique in the following sense: if \(\mathbf{v} = \mathbf{u}
    + \mathbf{u}^\perp\) with \(\mathbf{u} \in U\) and \(\mathbf{u}^\perp \in U^\perp\),
    then \(\mathbf{u} = \mathrm{proj}_U \mathbf{v}\) and \(\mathbf{u}^\perp = \mathbf{v}
    - \mathrm{proj}_U \mathbf{v}\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* The first part is an immediate consequence of the *Orthogonal Projection
    Theorem*. For the second part, assume \(\mathbf{v} = \mathbf{u} + \mathbf{u}^\perp\)
    with \(\mathbf{u} \in U\) and \(\mathbf{u}^\perp \in U^\perp\). Subtracting \(\mathbf{v}
    = \mathrm{proj}_U \mathbf{v} + (\mathbf{v} - \mathrm{proj}_U\mathbf{v})\), we
    see that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (*) \qquad \mathbf{0} = \mathbf{w}_1 + \mathbf{w}_2 \]
  prefs: []
  type: TYPE_NORMAL
- en: with
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{w}_1 = \mathbf{u} - \mathrm{proj}_U \mathbf{v} \in U, \qquad \mathbf{w}_2
    = \mathbf{u}^\perp - (\mathbf{v} - \mathrm{proj}_U\mathbf{v}) \in U^\perp. \]
  prefs: []
  type: TYPE_NORMAL
- en: If \(\mathbf{w}_1 = \mathbf{w}_2 = \mathbf{0}\), we are done. Otherwise, they
    must both be nonzero by \((*)\). Further, by the *Properties of Orthonormal Lists*,
    \(\mathbf{w}_1\) and \(\mathbf{w}_2\) must be linearly independent. But this is
    contradicted by the fact that \(\mathbf{w}_2 = - \mathbf{w}_1\) by \((*)\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Orthogonal decomposition ([Source](https://commons.wikimedia.org/wiki/File:Orthogonal_Decomposition_qtl1.svg))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Orthogonal decomposition](../Images/0fbd201d54de3ed8d4d4cbf69a88afc2.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, the *Orthogonal Decomposition Lemma* states that \(\mathbb{R}^n\)
    is a direct sum of any linear subspace \(U\) and of its orthogonal complement
    \(U^\perp\): that is, any vector \(\mathbf{v} \in \mathbb{R}^n\) can be written
    uniquely as \(\mathbf{v} = \mathbf{u} + \mathbf{u}^\perp\) with \(\mathbf{u} \in
    U\) and \(\mathbf{u}^\perp \in U^\perp\). This is denoted \(\mathbb{R}^n = U \oplus
    U^\perp\).'
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathbf{a}_1,\ldots,\mathbf{a}_\ell\) be an orthonormal basis of \(U\)
    and \(\mathbf{b}_1,\ldots,\mathbf{b}_k\) be an orthonormal basis of \(U^\perp\).
    By definition of the orthogonal complement, the list
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = \{\mathbf{a}_1,\ldots,\mathbf{a}_\ell, \mathbf{b}_1,\ldots,\mathbf{b}_k\}
    \]
  prefs: []
  type: TYPE_NORMAL
- en: is orthonormal, so it forms a basis of its span. Because any vector in \(\mathbb{R}^n\)
    can be written as a sum of a vector from \(U\) and a vector from \(U^\perp\),
    all of \(\mathbb{R}^n\) is in the span of \(\mathcal{L}\). It follows from the
    *Dimension Theorem* that \(n = \ell + k\), that is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}(U) + \mathrm{dim}(U^\perp) = n. \]
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4\. Overdetermined systems[#](#overdetermined-systems "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss the least-squares problem. Let again \(A \in \mathbb{R}^{n\times
    m}\) be an \(n\times m\) matrix with linearly independent columns and let \(\mathbf{b}
    \in \mathbb{R}^n\) be a vector. We are looking to solve the system
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x} \approx \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'If \(n=m\), we can use the matrix inverse to solve the system (provided \(A\)
    is nonsingular of course). But we are interested in the overdetermined case, i.e.
    when \(n > m\): there are more equations than variables. We cannot use the matrix
    inverse then. Indeed, because the columns do not span all of \(\mathbb{R}^n\),
    there is a vector \(\mathbf{b} \in \mathbb{R}^n\) that is not in the column space
    of \(A\).'
  prefs: []
  type: TYPE_NORMAL
- en: A natural way to make sense of the overdetermined problem is to cast it as the
    [linear least-squares problem](https://en.wikipedia.org/wiki/Least_squares)\(\idx{least
    squares problem}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, we look for the best-fitting solution under the squared Euclidean
    norm. Equivalently, writing
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} | & & | \\ \mathbf{a}_1 & \ldots & \mathbf{a}_m
    \\ | & & | \end{pmatrix} = \begin{pmatrix} a_{11} & \cdots & a_{1m} \\ a_{21}
    & \cdots & a_{2m} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nm} \end{pmatrix}
    \quad \text{and} \quad \mathbf{b} = \begin{pmatrix} b_1 \\ \vdots \\ b_n \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: we seek a linear combination of the columns of \(A\) that minimizes the objective
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left\|\,\sum_{j=1}^m x_j \mathbf{a}_j - \mathbf{b}\,\right\|^2 = \sum_{i=1}^n
    \left( \sum_{j=1}^m a_{ij} x_j - b_i \right)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: We have already solved a closely related problem when we introduced the orthogonal
    projection. We make the connection explicit next.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Normal Equations)** \(\idx{normal equations}\xdi\) Let \(A \in
    \mathbb{R}^{n\times m}\) be an \(n\times m\) matrix with \(n \geq m\) and let
    \(\mathbf{b} \in \mathbb{R}^n\) be a vector. A solution \(\mathbf{x}^*\) to the
    linear least-squares problem'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: satisfies the normal equations
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T A \mathbf{x}^* = A^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: If further the columns of \(A\) are linearly independent, then there exists
    a unique solution \(\mathbf{x}^*\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* Apply our characterization of the orthogonal projection onto
    the column space of \(A\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Let \(U = \mathrm{col}(A) = \mathrm{span}(\mathbf{a}_1,\ldots,\mathbf{a}_m)\).
    By the *Orthogonal Projection Theorem*, the orthogonal projection \(\mathbf{p}^*
    = \mathrm{proj}_{U} \mathbf{b}\) of \(\mathbf{b}\) onto \(U\) is the unique, closest
    vector to \(\mathbf{b}\) in \(U\), that is,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{p}^* = \arg\min_{\mathbf{p} \in U} \|\mathbf{p} - \mathbf{b}\| =
    \arg\min_{\mathbf{p} \in U} \|\mathbf{p} - \mathbf{b}\|^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Composing with a Non-Decreasing Function Lemma* to justify
    taking a square in the rightmost expression. Because \(\mathbf{p}^*\) is in \(U
    = \mathrm{col}(A)\), it must be of the form \(\mathbf{p}^* = A \mathbf{x}^*\).
    This establishes that \(\mathbf{x}^*\) is a solution to the linear least-squares
    problem in the statement. By the *Orthogonal Projection Theorem*, it must satisfy
    \(\langle \mathbf{b} - A \mathbf{x}^*, \mathbf{u}\rangle = 0\) for all \(\mathbf{u}
    \in U\). Because the columns \(\mathbf{a}_i\) are in \(U\), that implies that
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = \langle \mathbf{b} - A \mathbf{x}^*, \mathbf{a}_i\rangle = \mathbf{a}_i^T
    (\mathbf{b} - A \mathbf{x}^*) ,\qquad \forall i\in [m]. \]
  prefs: []
  type: TYPE_NORMAL
- en: Stacking up these equations gives in matrix form
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T (\mathbf{b} - A\mathbf{x}^*) = \mathbf{0}, \]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed (after rearranging).
  prefs: []
  type: TYPE_NORMAL
- en: 'Important observation: While we have shown that \(\mathbf{p}^*\) is unique
    (by the *Orthogonal Projection Theorem*), it is not clear at all that \(\mathbf{x}^*\)
    (i.e., the linear combination of columns of \(A\) corresponding to \(\mathbf{p}^*\))
    is unique. We have seen in a previous example that, when \(A\) has full column
    rank, the matrix \(A^T A\) is invertible. That implies the uniqueness claim. \(\square\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** To solve a linear system in NumPy, use [`numpy.linalg.solve`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html).
    As an example, we consider the overdetermined system with'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ 0 & 1\\ 1 & 1 \end{pmatrix} \quad
    \text{and} \quad \mathbf{b} = \begin{pmatrix} 0\\ 0\\ 2 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We use [`numpy.ndarray.T`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html)
    for the transpose and [`@`](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)
    for matrix-matrix or matrix-vector product.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We can also use [`numpy.linalg.lstsq`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html)
    directly on the overdetermined system to compute the least-square solution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Let \(\mathbf{q}_1, \dots, \mathbf{q}_m\) be an orthonormal list of vectors
    in \(\mathbb{R}^n\). Which of the following is the orthogonal projection of a
    vector \(\mathbf{v} \in \mathbb{R}^n\) onto \(\mathrm{span}(\mathbf{q}_1, \dots,
    \mathbf{q}_m)\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\sum_{i=1}^m \mathbf{q}_i\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\sum_{i=1}^m \langle \mathbf{v}, \mathbf{q}_i \rangle\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\sum_{i=1}^m \langle \mathbf{v}, \mathbf{q}_i \rangle \mathbf{q}_i\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\sum_{i=1}^m \langle \mathbf{q}_i, \mathbf{q}_i \rangle \mathbf{v}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** According to the Normal Equations Theorem, what condition must a solution
    \(\bx^*\) to the linear least squares problem satisfy?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(A^T A\bx^* = \bb\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(A^T A\bx^* = A^T \bb\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(A\bx^* = A^T \bb\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(A\bx^* = \bb\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Which property characterizes the orthogonal projection \(\mathrm{proj}_U
    \mathbf{v}\) of a vector \(\mathbf{v}\) onto a subspace \(U\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{v} - \mathrm{proj}_U \mathbf{v}\) is a scalar multiple of \(\mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{v} - \mathrm{proj}_U \mathbf{v}\) is orthogonal to \(\mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{v} - \mathrm{proj}_U \mathbf{v}\) is orthogonal to \(U\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathrm{proj}_U \mathbf{v}\) is always the zero vector.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** What is the interpretation of the linear least squares problem \(A\mathbf{x}
    \approx \mathbf{b}\) in terms of the column space of \(A\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) Finding the exact solution \(\mathbf{x}\) such that \(A\mathbf{x} = \mathbf{b}\).
  prefs: []
  type: TYPE_NORMAL
- en: b) Finding the vector \(\mathbf{x}\) that makes the linear combination \(A\mathbf{x}\)
    of the columns of \(A\) as close as possible to \(\mathbf{b}\) in Euclidean norm.
  prefs: []
  type: TYPE_NORMAL
- en: c) Finding the orthogonal projection of \(\mathbf{b}\) onto the column space
    of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: d) Finding the orthogonal complement of the column space of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which matrix equation must hold true for a matrix \(Q\) to be orthogonal?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(Q Q^T = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(Q Q^T = I\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(Q^T Q = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(Q^T = Q\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: c. Justification: This is the definition of the orthogonal projection
    onto an orthonormal list given in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The Normal Equations Theorem states that a
    solution \(\bx^*\) to the linear least squares problem satisfies \(A^T A\bx^*
    = A^T \bb\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text states that the orthogonal projection
    \(\mathrm{proj}_U \mathbf{v}\) has the property that “the difference \(\mathbf{v}
    - \mathrm{proj}_U \mathbf{v}\) is orthogonal to \(U\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: b. Justification: The text defines the linear least squares problem
    as seeking a linear combination of the columns of \(A\) that minimizes the distance
    to \(\mathbf{b}\) in Euclidean norm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text states that an orthogonal matrix \(Q\)
    must satisfy \(Q Q^T = I\).'
  prefs: []
  type: TYPE_NORMAL
