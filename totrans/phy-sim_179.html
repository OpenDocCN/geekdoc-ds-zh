<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Summary</h1>
<blockquote>原文：<a href="https://phys-sim-book.github.io/lec33.4-summary.html">https://phys-sim-book.github.io/lec33.4-summary.html</a></blockquote>
                        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css"/>

<p>In this section, we explored fundamental approaches for solving large, sparse linear systems of the form <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"/><span class="mord mathnormal">A</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2778em;"/><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"/></span><span class="base"><span class="strut" style="height:0.6944em;"/><span class="mord mathnormal">b</span></span></span></span> that arise in optimization-based simulation, particularly when computing search directions in Newton-type methods.</p>
<p>We established the context: at each iteration of projected Newton methods, we solve <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"/><span class="mord mathnormal">Pp</span><span class="mspace" style="margin-right:0.2778em;"/><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"/></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"/><span class="mord">−</span><span class="mord">∇</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span> where <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"/><span class="mord mathnormal" style="margin-right:0.13889em;">P</span></span></span></span> is typically a symmetric positive definite (SPD) proxy matrix. This reduces to solving linear systems crucial for optimization time integrators.</p>
<p>Direct solvers provide exact solutions through matrix factorization. For SPD systems, Cholesky decomposition <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"/><span class="mord mathnormal">A</span><span class="mspace" style="margin-right:0.2778em;"/><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"/></span><span class="base"><span class="strut" style="height:0.8413em;"/><span class="mord mathnormal">L</span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"/><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span> reduces the problem to two triangular solves. While robust and accurate, direct methods become impractical for very large systems due to computational constraints.</p>
<p>Iterative methods offer alternatives for large, sparse systems. Basic methods include Jacobi (naturally parallelizable) and Gauss-Seidel (faster convergence but sequential). Both converge when the spectral radius of their iteration matrices is less than 1. The Conjugate Gradient (CG) method is a more sophisticated approach for SPD systems. CG constructs <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"/><span class="mord mathnormal">A</span></span></span></span>-conjugate search directions ensuring progress is never undone, achieving remarkable efficiency through orthogonality relationships. In exact arithmetic, CG converges in at most <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"/><span class="mord mathnormal">n</span></span></span></span> steps, while in practice it often converges much faster with preconditioning.</p>
<p>The choice between direct and iterative methods depends on problem size, sparsity structure, and accuracy requirements. Direct methods excel for moderate-sized problems requiring high precision, while iterative methods, particularly CG with preconditioning, are essential for large-scale simulations.</p>

                        
</body>
</html>