<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Why GPUs?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Why GPUs?</h1>
<blockquote>原文：<a href="https://enccs.github.io/gpu-programming/1-gpu-history/">https://enccs.github.io/gpu-programming/1-gpu-history/</a></blockquote><nav class="wy-nav-top" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"/>
          <a href="../">GPU programming: why, when and how?</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"/></li>
      <li class="breadcrumb-item active">Why GPUs?</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/gpu-programming/blob/main/content/1-gpu-history.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="why-gpus">
<span id="gpu-history"/>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>What is Moore’s law?</p></li>
<li><p>What problems do GPUs solve?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Explain the historical development of microprocessors and how GPUs enable
continued scaling in computational power</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>15 min teaching</p></li>
<li><p>0 min exercises</p></li>
</ul>
</div>
<section id="moore-s-law">
<h2>Moore’s law</h2>
<p>It states that the number of transistors in a dense integrated circuit doubles about every two years.
More transistors means smaller size of a single element, so higher core frequency can be achieved.
However, power consumption scales with frequency to the third power, therefore the growth in the core frequency has slowed down significantly.
Higher performance of a single node has to rely on its more complicated structure and can still be achieved with SIMD (single instruction multiple data), branch prediction, etc.</p>
<figure class="align-center" id="id2">
<img alt="../_images/microprocessor-trend-data.png" src="../Images/f2be4440984524ab75d46846f1f2567d.png" data-original-src="https://enccs.github.io/gpu-programming/_images/microprocessor-trend-data.png"/>
<figcaption>
<p><span class="caption-text">The evolution of microprocessors.
The number of transistors per chip doubles roughly every 2 years.
However, it can no longer be explored by the core frequency due to the power consumption limits.
Before 2000, the increase in the single core clock frequency was the major source of the
increase in the performance. Mid 2000 mark a transition towards multi-core processors.</span></p>
</figcaption>
</figure>
<p>Increasing performance has been sustained with two main strategies over the years:</p>
<blockquote>
<div><ul class="simple">
<li><p>Increase the single processor performance:</p></li>
<li><p>More recently, increase the number of physical cores.</p></li>
</ul>
</div></blockquote>
</section>
<section id="computing-in-parallel">
<h2>Computing in parallel</h2>
<p>The underlying idea of parallel computing is to split a computational problem into smaller
subtasks. Many subtasks can then be solved <em>simultaneously</em> by multiple processing units.</p>
<figure class="align-center" id="id3">
<img alt="../_images/compp.png" src="../Images/73377a40a5b11000e154cc02c6a95c93.png" data-original-src="https://enccs.github.io/gpu-programming/_images/compp.png"/>
<figcaption>
<p><span class="caption-text">Computing in parallel.</span></p>
</figcaption>
</figure>
<p>How a problem is split into smaller subtasks strongly depends on the problem.
There are various paradigms and programming approaches to do this.</p>
</section>
<section id="graphics-processing-units">
<h2>Graphics processing units</h2>
<p>Graphics processing units (GPU) have been the most common accelerators during the last few years, the term GPU sometimes is used interchangeably with the term <em>accelerator</em>.
GPUs were initially developed for highly-parallel tasks of graphic processing.
But over the years, they were used more and more in high-performance computing (HPC).</p>
<p>GPUs are a specialized parallel hardware for floating point operations.
They are basically co-processors (helpers) for traditional CPUs: a CPU still controls the work flow
but it delegates highly parallel tasks to the GPU.
GPUs are based on highly parallel architectures, which allows taking advantage of the
increasing number of transistors.</p>
<p>Using GPUs allows one to achieve extreme performance per node.
As a result, a single GPU-equipped workstation can outperform small CPU-based clusters
for some types of computational tasks. The drawback is: usually major rewrites of programs are required
with an accompanying change in the programming paradigm.</p>
<div class="admonition-host-vs-device callout admonition" id="callout-0">
<p class="admonition-title">Host vs device</p>
<p>GPU-enabled systems require a heterogeneous programming model that involves both
CPU and GPU, where the CPU and its memory are referred to as the host,
and the GPU and its memory as the device.</p>
</div>
<figure class="align-center" id="id4">
<img alt="../_images/CPU_and_GPU_separated.png" src="../Images/46819c90dafc7b2c29de61c8e3e002a6.png" data-original-src="https://enccs.github.io/gpu-programming/_images/CPU_and_GPU_separated.png"/>
<figcaption>
<p><span class="caption-text">Figure adapted from the Carpentry <a class="reference external" href="https://carpentries-incubator.github.io/lesson-gpu-programming/">GPU Programming lesson</a>.</span></p>
</figcaption>
</figure>
</section>
<section id="a-look-at-the-top500-list">
<h2>A look at the TOP500 list</h2>
<p>The <a class="reference external" href="https://www.top500.org/">TOP500 project</a> ranks and details the 500 most powerful non-distributed computer systems in the world. The project was started in 1993 and publishes an updated list of the supercomputers twice a year. The snapshot below shows the top-5 HPC systems as of June 2025, where the columns show:</p>
<ul class="simple">
<li><p><strong>Cores</strong> - Number of processors</p></li>
<li><p><strong>Rmax</strong> - Maximal LINPACK performance achieved</p></li>
<li><p><strong>Rpeak</strong> - Theoretical peak performance</p></li>
<li><p><strong>Power</strong> - Power consumption</p></li>
</ul>
<figure class="align-center" id="id5">
<img alt="../_images/top-5.png" src="../Images/d6ce0baa5ab71bc74512f07814e92c25.png" data-original-src="https://enccs.github.io/gpu-programming/_images/top-5.png"/>
<figcaption>
<p><span class="caption-text">Snapshot from the <a class="reference external" href="https://www.top500.org/lists/top500/2024/05/">TOP500 list from June, 2025</a>.</span></p>
</figcaption>
</figure>
<p>All systems in the top-5 positions contain GPUs from AMD, Intel, or NVIDIA.</p>
</section>
<section id="id1">
<h2>Why GPUs?</h2>
<ul class="simple">
<li><p><strong>Speed</strong>: GPU computing can significantly accelerate many types of scientific workloads.</p></li>
<li><p><strong>Improved energy efficiency</strong>: Compared to CPUs, GPUs can perform more calculations per watt of power consumed,
which can result in significant energy savings. This is indeed evident from the <a class="reference external" href="https://www.top500.org/lists/green500/2025/06/">GREEN500 list</a>.</p></li>
<li><p><strong>Cost-effectiveness</strong>: GPUs can be more cost-effective than traditional CPU-based systems for certain workloads.</p></li>
</ul>
</section>
<section id="limitations-and-drawbacks">
<h2>Limitations and drawbacks</h2>
<ul class="simple">
<li><p><strong>Only for certain workloads</strong>: Not all workloads can be efficiently parallelized and accelerated on GPUs. Certain types of workloads, such as those with irregular data access patterns or high branching behavior, may not see significant performance improvements on GPUs.</p></li>
<li><p><strong>Steeper learning curve</strong>: Depending on the GPU programming API that you choose, GPU computing could require specialized skills in GPU programming and knowledge of GPU architecture, leading to a steeper learning curve compared to CPU programming. Fortunately, if you study this training material closely you will become productive with GPU programming quickly!</p></li>
</ul>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>GPUs are accelerators for some types of tasks</p></li>
<li><p>Highly parallelizable compute-intensive tasks are suitable for GPUs</p></li>
<li><p>GPU-based systems dominate the top spots of the TOP500 list</p></li>
<li><p>New programming skills are needed to use GPUs efficiently</p></li>
</ul>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../0-setup/" class="btn btn-neutral float-left" title="Setup" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"/> Previous</a>
        <a href="../2-gpu-ecosystem/" class="btn btn-neutral float-right" title="The GPU hardware and software ecosystem" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"/></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>© Copyright 2023-2024, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    &#13;

<span id="gpu-history"/>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>What is Moore’s law?</p></li>
<li><p>What problems do GPUs solve?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Explain the historical development of microprocessors and how GPUs enable
continued scaling in computational power</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>15 min teaching</p></li>
<li><p>0 min exercises</p></li>
</ul>
</div>
<section id="moore-s-law">
<h2>Moore’s law</h2>
<p>It states that the number of transistors in a dense integrated circuit doubles about every two years.
More transistors means smaller size of a single element, so higher core frequency can be achieved.
However, power consumption scales with frequency to the third power, therefore the growth in the core frequency has slowed down significantly.
Higher performance of a single node has to rely on its more complicated structure and can still be achieved with SIMD (single instruction multiple data), branch prediction, etc.</p>
<figure class="align-center" id="id2">
<img alt="../_images/microprocessor-trend-data.png" src="../Images/f2be4440984524ab75d46846f1f2567d.png" data-original-src="https://enccs.github.io/gpu-programming/_images/microprocessor-trend-data.png"/>
<figcaption>
<p><span class="caption-text">The evolution of microprocessors.
The number of transistors per chip doubles roughly every 2 years.
However, it can no longer be explored by the core frequency due to the power consumption limits.
Before 2000, the increase in the single core clock frequency was the major source of the
increase in the performance. Mid 2000 mark a transition towards multi-core processors.</span></p>
</figcaption>
</figure>
<p>Increasing performance has been sustained with two main strategies over the years:</p>
<blockquote>
<div><ul class="simple">
<li><p>Increase the single processor performance:</p></li>
<li><p>More recently, increase the number of physical cores.</p></li>
</ul>
</div></blockquote>
</section>
<section id="computing-in-parallel">
<h2>Computing in parallel</h2>
<p>The underlying idea of parallel computing is to split a computational problem into smaller
subtasks. Many subtasks can then be solved <em>simultaneously</em> by multiple processing units.</p>
<figure class="align-center" id="id3">
<img alt="../_images/compp.png" src="../Images/73377a40a5b11000e154cc02c6a95c93.png" data-original-src="https://enccs.github.io/gpu-programming/_images/compp.png"/>
<figcaption>
<p><span class="caption-text">Computing in parallel.</span></p>
</figcaption>
</figure>
<p>How a problem is split into smaller subtasks strongly depends on the problem.
There are various paradigms and programming approaches to do this.</p>
</section>
<section id="graphics-processing-units">
<h2>Graphics processing units</h2>
<p>Graphics processing units (GPU) have been the most common accelerators during the last few years, the term GPU sometimes is used interchangeably with the term <em>accelerator</em>.
GPUs were initially developed for highly-parallel tasks of graphic processing.
But over the years, they were used more and more in high-performance computing (HPC).</p>
<p>GPUs are a specialized parallel hardware for floating point operations.
They are basically co-processors (helpers) for traditional CPUs: a CPU still controls the work flow
but it delegates highly parallel tasks to the GPU.
GPUs are based on highly parallel architectures, which allows taking advantage of the
increasing number of transistors.</p>
<p>Using GPUs allows one to achieve extreme performance per node.
As a result, a single GPU-equipped workstation can outperform small CPU-based clusters
for some types of computational tasks. The drawback is: usually major rewrites of programs are required
with an accompanying change in the programming paradigm.</p>
<div class="admonition-host-vs-device callout admonition" id="callout-0">
<p class="admonition-title">Host vs device</p>
<p>GPU-enabled systems require a heterogeneous programming model that involves both
CPU and GPU, where the CPU and its memory are referred to as the host,
and the GPU and its memory as the device.</p>
</div>
<figure class="align-center" id="id4">
<img alt="../_images/CPU_and_GPU_separated.png" src="../Images/46819c90dafc7b2c29de61c8e3e002a6.png" data-original-src="https://enccs.github.io/gpu-programming/_images/CPU_and_GPU_separated.png"/>
<figcaption>
<p><span class="caption-text">Figure adapted from the Carpentry <a class="reference external" href="https://carpentries-incubator.github.io/lesson-gpu-programming/">GPU Programming lesson</a>.</span></p>
</figcaption>
</figure>
</section>
<section id="a-look-at-the-top500-list">
<h2>A look at the TOP500 list</h2>
<p>The <a class="reference external" href="https://www.top500.org/">TOP500 project</a> ranks and details the 500 most powerful non-distributed computer systems in the world. The project was started in 1993 and publishes an updated list of the supercomputers twice a year. The snapshot below shows the top-5 HPC systems as of June 2025, where the columns show:</p>
<ul class="simple">
<li><p><strong>Cores</strong> - Number of processors</p></li>
<li><p><strong>Rmax</strong> - Maximal LINPACK performance achieved</p></li>
<li><p><strong>Rpeak</strong> - Theoretical peak performance</p></li>
<li><p><strong>Power</strong> - Power consumption</p></li>
</ul>
<figure class="align-center" id="id5">
<img alt="../_images/top-5.png" src="../Images/d6ce0baa5ab71bc74512f07814e92c25.png" data-original-src="https://enccs.github.io/gpu-programming/_images/top-5.png"/>
<figcaption>
<p><span class="caption-text">Snapshot from the <a class="reference external" href="https://www.top500.org/lists/top500/2024/05/">TOP500 list from June, 2025</a>.</span></p>
</figcaption>
</figure>
<p>All systems in the top-5 positions contain GPUs from AMD, Intel, or NVIDIA.</p>
</section>
<section id="id1">
<h2>Why GPUs?</h2>
<ul class="simple">
<li><p><strong>Speed</strong>: GPU computing can significantly accelerate many types of scientific workloads.</p></li>
<li><p><strong>Improved energy efficiency</strong>: Compared to CPUs, GPUs can perform more calculations per watt of power consumed,
which can result in significant energy savings. This is indeed evident from the <a class="reference external" href="https://www.top500.org/lists/green500/2025/06/">GREEN500 list</a>.</p></li>
<li><p><strong>Cost-effectiveness</strong>: GPUs can be more cost-effective than traditional CPU-based systems for certain workloads.</p></li>
</ul>
</section>
<section id="limitations-and-drawbacks">
<h2>Limitations and drawbacks</h2>
<ul class="simple">
<li><p><strong>Only for certain workloads</strong>: Not all workloads can be efficiently parallelized and accelerated on GPUs. Certain types of workloads, such as those with irregular data access patterns or high branching behavior, may not see significant performance improvements on GPUs.</p></li>
<li><p><strong>Steeper learning curve</strong>: Depending on the GPU programming API that you choose, GPU computing could require specialized skills in GPU programming and knowledge of GPU architecture, leading to a steeper learning curve compared to CPU programming. Fortunately, if you study this training material closely you will become productive with GPU programming quickly!</p></li>
</ul>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>GPUs are accelerators for some types of tasks</p></li>
<li><p>Highly parallelizable compute-intensive tasks are suitable for GPUs</p></li>
<li><p>GPU-based systems dominate the top spots of the TOP500 list</p></li>
<li><p>New programming skills are needed to use GPUs efficiently</p></li>
</ul>
</div>
</section>
&#13;

<h2>Moore’s law</h2>
<p>It states that the number of transistors in a dense integrated circuit doubles about every two years.
More transistors means smaller size of a single element, so higher core frequency can be achieved.
However, power consumption scales with frequency to the third power, therefore the growth in the core frequency has slowed down significantly.
Higher performance of a single node has to rely on its more complicated structure and can still be achieved with SIMD (single instruction multiple data), branch prediction, etc.</p>
<figure class="align-center" id="id2">
<img alt="../_images/microprocessor-trend-data.png" src="../Images/f2be4440984524ab75d46846f1f2567d.png" data-original-src="https://enccs.github.io/gpu-programming/_images/microprocessor-trend-data.png"/>
<figcaption>
<p><span class="caption-text">The evolution of microprocessors.
The number of transistors per chip doubles roughly every 2 years.
However, it can no longer be explored by the core frequency due to the power consumption limits.
Before 2000, the increase in the single core clock frequency was the major source of the
increase in the performance. Mid 2000 mark a transition towards multi-core processors.</span></p>
</figcaption>
</figure>
<p>Increasing performance has been sustained with two main strategies over the years:</p>
<blockquote>
<div><ul class="simple">
<li><p>Increase the single processor performance:</p></li>
<li><p>More recently, increase the number of physical cores.</p></li>
</ul>
</div></blockquote>
&#13;

<h2>Computing in parallel</h2>
<p>The underlying idea of parallel computing is to split a computational problem into smaller
subtasks. Many subtasks can then be solved <em>simultaneously</em> by multiple processing units.</p>
<figure class="align-center" id="id3">
<img alt="../_images/compp.png" src="../Images/73377a40a5b11000e154cc02c6a95c93.png" data-original-src="https://enccs.github.io/gpu-programming/_images/compp.png"/>
<figcaption>
<p><span class="caption-text">Computing in parallel.</span></p>
</figcaption>
</figure>
<p>How a problem is split into smaller subtasks strongly depends on the problem.
There are various paradigms and programming approaches to do this.</p>
&#13;

<h2>Graphics processing units</h2>
<p>Graphics processing units (GPU) have been the most common accelerators during the last few years, the term GPU sometimes is used interchangeably with the term <em>accelerator</em>.
GPUs were initially developed for highly-parallel tasks of graphic processing.
But over the years, they were used more and more in high-performance computing (HPC).</p>
<p>GPUs are a specialized parallel hardware for floating point operations.
They are basically co-processors (helpers) for traditional CPUs: a CPU still controls the work flow
but it delegates highly parallel tasks to the GPU.
GPUs are based on highly parallel architectures, which allows taking advantage of the
increasing number of transistors.</p>
<p>Using GPUs allows one to achieve extreme performance per node.
As a result, a single GPU-equipped workstation can outperform small CPU-based clusters
for some types of computational tasks. The drawback is: usually major rewrites of programs are required
with an accompanying change in the programming paradigm.</p>
<div class="admonition-host-vs-device callout admonition" id="callout-0">
<p class="admonition-title">Host vs device</p>
<p>GPU-enabled systems require a heterogeneous programming model that involves both
CPU and GPU, where the CPU and its memory are referred to as the host,
and the GPU and its memory as the device.</p>
</div>
<figure class="align-center" id="id4">
<img alt="../_images/CPU_and_GPU_separated.png" src="../Images/46819c90dafc7b2c29de61c8e3e002a6.png" data-original-src="https://enccs.github.io/gpu-programming/_images/CPU_and_GPU_separated.png"/>
<figcaption>
<p><span class="caption-text">Figure adapted from the Carpentry <a class="reference external" href="https://carpentries-incubator.github.io/lesson-gpu-programming/">GPU Programming lesson</a>.</span></p>
</figcaption>
</figure>
&#13;

<h2>A look at the TOP500 list</h2>
<p>The <a class="reference external" href="https://www.top500.org/">TOP500 project</a> ranks and details the 500 most powerful non-distributed computer systems in the world. The project was started in 1993 and publishes an updated list of the supercomputers twice a year. The snapshot below shows the top-5 HPC systems as of June 2025, where the columns show:</p>
<ul class="simple">
<li><p><strong>Cores</strong> - Number of processors</p></li>
<li><p><strong>Rmax</strong> - Maximal LINPACK performance achieved</p></li>
<li><p><strong>Rpeak</strong> - Theoretical peak performance</p></li>
<li><p><strong>Power</strong> - Power consumption</p></li>
</ul>
<figure class="align-center" id="id5">
<img alt="../_images/top-5.png" src="../Images/d6ce0baa5ab71bc74512f07814e92c25.png" data-original-src="https://enccs.github.io/gpu-programming/_images/top-5.png"/>
<figcaption>
<p><span class="caption-text">Snapshot from the <a class="reference external" href="https://www.top500.org/lists/top500/2024/05/">TOP500 list from June, 2025</a>.</span></p>
</figcaption>
</figure>
<p>All systems in the top-5 positions contain GPUs from AMD, Intel, or NVIDIA.</p>
&#13;

<h2>Why GPUs?</h2>
<ul class="simple">
<li><p><strong>Speed</strong>: GPU computing can significantly accelerate many types of scientific workloads.</p></li>
<li><p><strong>Improved energy efficiency</strong>: Compared to CPUs, GPUs can perform more calculations per watt of power consumed,
which can result in significant energy savings. This is indeed evident from the <a class="reference external" href="https://www.top500.org/lists/green500/2025/06/">GREEN500 list</a>.</p></li>
<li><p><strong>Cost-effectiveness</strong>: GPUs can be more cost-effective than traditional CPU-based systems for certain workloads.</p></li>
</ul>
&#13;

<h2>Limitations and drawbacks</h2>
<ul class="simple">
<li><p><strong>Only for certain workloads</strong>: Not all workloads can be efficiently parallelized and accelerated on GPUs. Certain types of workloads, such as those with irregular data access patterns or high branching behavior, may not see significant performance improvements on GPUs.</p></li>
<li><p><strong>Steeper learning curve</strong>: Depending on the GPU programming API that you choose, GPU computing could require specialized skills in GPU programming and knowledge of GPU architecture, leading to a steeper learning curve compared to CPU programming. Fortunately, if you study this training material closely you will become productive with GPU programming quickly!</p></li>
</ul>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>GPUs are accelerators for some types of tasks</p></li>
<li><p>Highly parallelizable compute-intensive tasks are suitable for GPUs</p></li>
<li><p>GPU-based systems dominate the top spots of the TOP500 list</p></li>
<li><p>New programming skills are needed to use GPUs efficiently</p></li>
</ul>
</div>
    
</body>
</html>