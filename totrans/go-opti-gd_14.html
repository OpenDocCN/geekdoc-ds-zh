<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Atomic Operations and Synchronization Primitives¶</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Atomic Operations and Synchronization Primitives¶</h1>
<blockquote>原文：<a href="https://goperf.dev/01-common-patterns/atomic-ops/">https://goperf.dev/01-common-patterns/atomic-ops/</a></blockquote>
                
                  


  
  



<p>In high-concurrency systems, performance isn't just about what you do—it's about what you avoid. Lock contention, cache line bouncing and memory fences quietly shape throughput long before you hit your scaling ceiling. Atomic operations are among the leanest tools Go offers to sidestep these pitfalls.</p>
<p>While Go provides the full suite of synchronization primitives, there's a class of problems where locks feel like overkill. Atomics offers clarity and speed for low-level coordination—counters, flags, and simple state machines, especially under pressure.</p>
<h2 id="understanding-atomic-operations">Understanding Atomic Operations<a class="headerlink" href="#understanding-atomic-operations" title="Permanent link">¶</a></h2>
<p>Atomic operations allow safe concurrent access to shared data without explicit locking mechanisms like mutexes. The <code>sync/atomic</code> package provides low-level atomic memory primitives ideal for counters, flags, or simple state transitions.</p>
<p>The key benefit of atomic operations is performance under contention. Locking introduces coordination overhead—when many goroutines contend for a mutex, performance can degrade due to context switching and lock queue management. Atomics avoids this by operating directly at the hardware level using CPU instructions like <code>CAS</code> (compare-and-swap). This makes them particularly useful for:</p>
<ul>
<li>High-throughput counters and flags</li>
<li>Lock-free queues and freelists</li>
<li>Low-latency paths where locks are too expensive</li>
</ul>
<h3 id="memory-model-and-comparison-to-c">Memory Model and Comparison to C++<a class="headerlink" href="#memory-model-and-comparison-to-c" title="Permanent link">¶</a></h3>
<p>Understanding memory models is crucial when reasoning about concurrency. In C++, developers have fine-grained control over atomic operations via memory orderings, which allows them to trade-off between performance and consistency. By default, Go's atomic operations enforce sequential consistency, which means they behave like <code>std::memory_order_seq_cst</code> in C++. This is the strongest and safest memory ordering:</p>
<ul>
<li>All threads observe atomic operations in the same order.</li>
<li>Full memory barrier are applied before and after each operation.</li>
<li>Reads and writes are not reordered across atomic operations.</li>
</ul>
<table>
<thead>
<tr>
<th>C++ Memory Order</th>
<th>Go Equivalent</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>memory_order_seq_cst</code></td>
<td>All <code>atomic.*</code> ops</td>
<td>Full sequential consistency</td>
</tr>
<tr>
<td><code>memory_order_acquire</code></td>
<td>Not exposed</td>
<td>Not available in Go</td>
</tr>
<tr>
<td><code>memory_order_release</code></td>
<td>Not exposed</td>
<td>Not available in Go</td>
</tr>
<tr>
<td><code>memory_order_relaxed</code></td>
<td>Not exposed</td>
<td>Not available in Go</td>
</tr>
</tbody>
</table>
<p>Go does not expose weaker memory models like <code>relaxed</code>, <code>acquire</code>, or <code>release</code>. This is an intentional simplification to promote safety and reduce the risk of subtle data races. All atomic operations in Go imply synchronization across goroutines, ensuring correct behavior without manual memory fencing.</p>
<p>This means you don’t have to reason about instruction reordering or memory visibility at a low level—but it also means you can’t fine-tune for performance in the way C++ or Rust developers might use relaxed atomics.</p>
<p>Low-level access to relaxed memory ordering in Go exists internally (e.g., in the runtime or through <code>go:linkname</code>), but it’s not safe or supported for use in application-level code.</p>
<h3 id="common-atomic-operations">Common Atomic Operations<a class="headerlink" href="#common-atomic-operations" title="Permanent link">¶</a></h3>
<ul>
<li><code>atomic.AddInt64</code>, <code>atomic.AddUint32</code>, etc.: Adds values atomically.</li>
<li><code>atomic.LoadInt64</code>, <code>atomic.LoadPointer</code>: Reads values atomically.</li>
<li><code>atomic.StoreInt64</code>, <code>atomic.StorePointer</code>: Writes values atomically.</li>
<li><code>atomic.CompareAndSwapInt64</code>: Conditionally updates a value atomically.</li>
</ul>
<h3 id="when-to-use-atomic-operations-in-real-life">When to Use Atomic Operations in Real Life<a class="headerlink" href="#when-to-use-atomic-operations-in-real-life" title="Permanent link">¶</a></h3>
<h4 id="high-throughput-metrics-and-counters">High-throughput metrics and Counters<a class="headerlink" href="#high-throughput-metrics-and-counters" title="Permanent link">¶</a></h4>
<p>Tracking request counts, dropped packets, or other lightweight stats:</p>
<div class="highlight"><pre><code>var requests atomic.Int64

func handleRequest() {
    requests.Add(1)
}
</code></pre></div>
<p>This code allows multiple goroutines to safely increment a shared counter without using locks. <code>atomic.AddInt64</code> ensures each addition is performed atomically, preventing race conditions and keeping performance high under heavy load.</p>
<h4 id="fast-lock-free-flags">Fast, Lock-Free Flags<a class="headerlink" href="#fast-lock-free-flags" title="Permanent link">¶</a></h4>
<p>Simple boolean state shared across threads:</p>
<div class="highlight"><pre><code>var shutdown atomic.Int32

func mainLoop() {
    for {
        if shutdown.Load() == 1 {
            break
        }
        // do work
    }
}

func stop() {
    shutdown.Store(1)
}
</code></pre></div>
<p>This pattern allows one goroutine to signal another to stop. <code>atomic.LoadInt32</code> reads the flag with synchronization guarantees, and <code>atomic.StoreInt32</code> sets the flag in a way visible to all goroutines. It helps implement safe shutdown signals.</p>
<h4 id="once-only-initialization">Once-Only Initialization<a class="headerlink" href="#once-only-initialization" title="Permanent link">¶</a></h4>
<p>For scenarios where <code>sync.Once</code> isn’t flexible enough—such as needing retryable or restartable initialization – a more precise control can be achieved using atomic operations:</p>
<div class="highlight"><pre><code>import (
    "runtime"
    "sync/atomic"
    "unsafe"
)

var resource unsafe.Pointer
var initStatus int32 // 0: not started, 1: in progress, 2: completed

func getResource() *MyResource {
    if atomic.LoadInt32(&amp;initStatus) == 2 {
        return (*MyResource)(atomic.LoadPointer(&amp;resource))
    }

    if atomic.CompareAndSwapInt32(&amp;initStatus, 0, 1) {
        newRes := expensiveInit() // initialization logic
        atomic.StorePointer(&amp;resource, unsafe.Pointer(newRes))
        atomic.StoreInt32(&amp;initStatus, 2)
        return newRes
    }

    for atomic.LoadInt32(&amp;initStatus) != 2 {
        runtime.Gosched() // yield until the initializer finishes
    }
    return (*MyResource)(atomic.LoadPointer(&amp;resource))
}
</code></pre></div>
<p>This pattern uses an explicit three-state protocol:</p>
<ul>
<li><code>0</code> = uninitialized</li>
<li><code>1</code> = initialization in progress</li>
<li><code>2</code> = initialized</li>
</ul>
<p>The first goroutine that successfully flips the state from <code>0</code> to <code>1</code> takes charge of the initialization. The rest wait in a lightweight spin loop, briefly yielding with <code>runtime.Gosched()</code> until initialization completes. Once the state flips to <code>2</code>, they read the resource and continue.</p>
<p>Unlike <code>sync.Once</code>, this approach avoids mutex overhead and gives you full control over how and when initialization happens. It’s well-suited for high-performance paths or systems where partial or retryable initialization is necessary.</p>
<h4 id="lock-free-queues-or-freelist-structures">Lock-Free Queues or Freelist Structures<a class="headerlink" href="#lock-free-queues-or-freelist-structures" title="Permanent link">¶</a></h4>
<p>Building high-performance data structures:</p>
<div class="highlight"><pre><code>type node struct {
    next *node
    val  any
}

var head atomic.Pointer[node]

func push(n *node) {
    for {
        old := head.Load()
        n.next = old
        if head.CompareAndSwap(old, n) {
            return
        }
    }
}
</code></pre></div>
<p>This implements a lock-free stack (LIFO queue). It repeatedly tries to insert a node at the head of the list by atomically replacing the head pointer only if it hasn't changed—a classic <code>CAS</code> loop. It's commonly used in object pools and work-stealing queues.</p>
<h4 id="reducing-lock-contention">Reducing Lock Contention<a class="headerlink" href="#reducing-lock-contention" title="Permanent link">¶</a></h4>
<p>This approach is common in real-world systems to reduce unnecessary lock contention, such as feature toggles, one-time initialization paths, or conditional caching mechanisms. Atomics serves as a fast-path filter before acquiring a more expensive lock.</p>
<p>Combining atomics with mutexes to gate expensive work:</p>
<div class="highlight"><pre><code>if atomic.LoadInt32(&amp;someFlag) == 0 {
    return
}
mu.Lock()
defer mu.Unlock()
// do something heavy
</code></pre></div>
<p>This pattern is effective when <code>someFlag</code> is set by another goroutine, and the current goroutine only uses it as a read-only signal to determine if it should proceed. It avoids unnecessary lock acquisition in high-throughput paths, such as short-circuiting when a feature is disabled or a task has already been completed.</p>
<p>However, if the same goroutine is also responsible forsetting the flag, a simple load followed by a lock is not safe. Another goroutine could interleave between the check and the lock, leading to inconsistent behavior.</p>
<p>To make the operation safe and atomic, use <code>CompareAndSwap</code>:</p>
<div class="highlight"><pre><code>if !atomic.CompareAndSwapInt32(&amp;someFlag, 0, 1) {
    return // work already in progress or completed
}
mu.Lock()
defer mu.Unlock()
// perform one-time expensive initialization
</code></pre></div>
<p>This version guarantees that only one goroutine proceeds and others exit early. It ensures both the check and the update to <code>someFlag</code> happen atomically.</p>
<p>Here, the atomic read acts as a fast gatekeeper. If the flag is unset, acquiring the mutex is unnecessary. This avoids unnecessary locking in high-frequency code paths, improving responsiveness under load.</p>
<h2 id="synchronization-primitives">Synchronization Primitives<a class="headerlink" href="#synchronization-primitives" title="Permanent link">¶</a></h2>
<p>This section is intentionally kept minimal. Go's synchronization primitives—such as <code>sync.Mutex</code>, <code>sync.RWMutex</code>, and <code>sync.Cond</code>—are already thoroughly documented and widely understood. They are essential tools for managing shared memory and coordinating goroutines, but they are not the focus here.</p>
<p>In the context of this article, we reference them only as a <strong>performance comparison baseline</strong> against atomic operations. When appropriate, these primitives offer clarity and correctness, but they often come at a higher cost in high-contention scenarios, where atomics can provide leaner alternatives.</p>
<p>We’ll use them as contrast points to highlight when and why atomic operations might offer performance advantages.</p>
<h2 id="benchmarking-impact">Benchmarking Impact<a class="headerlink" href="#benchmarking-impact" title="Permanent link">¶</a></h2>
<p>To understand the impact of atomic operations versus mutex locks, we can compare the time taken to increment a shared counter across goroutines using a simple benchmark.</p>
<div class="highlight"><pre><code>func BenchmarkAtomicIncrement(b *testing.B) {
    var counter int64
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            atomic.AddInt64(&amp;counter, 1)
        }
    })
}

func BenchmarkMutexIncrement(b *testing.B) {
    var (
        counter int64
        mu      sync.Mutex
    )
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            mu.Lock()
            counter++
            mu.Unlock()
        }
    })
}
</code></pre></div>
<p>Benchmark results:</p>
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>Iterations</th>
<th>Time per op (ns)</th>
<th>Bytes per op</th>
<th>Allocs per op</th>
</tr>
</thead>
<tbody>
<tr>
<td>BenchmarkAtomicIncrement-14</td>
<td>39,910,514</td>
<td>80.40</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>BenchmarkMutexIncrement-14</td>
<td>32,629,298</td>
<td>110.7</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Atomic operations outperform mutex-based increments in both throughput and latency. The difference becomes more significant under higher contention, where avoiding lock acquisition helps reduce context switching and scheduler overhead.</p>
<details class="example">
<summary>Show the complete benchmark file</summary>
<div class="highlight"><pre><code>package perf

import (
    "testing"
    "sync/atomic"
    "sync"
)

// bench-start
func BenchmarkAtomicIncrement(b *testing.B) {
    var counter int64
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            atomic.AddInt64(&amp;counter, 1)
        }
    })
}

func BenchmarkMutexIncrement(b *testing.B) {
    var (
        counter int64
        mu      sync.Mutex
    )
    b.RunParallel(func(pb *testing.PB) {
        for pb.Next() {
            mu.Lock()
            counter++
            mu.Unlock()
        }
    })
}
// bench-end
</code></pre></div>
</details>
<h2 id="when-to-use-atomic-operations-vs-mutexes">When to Use Atomic Operations vs. Mutexes<a class="headerlink" href="#when-to-use-atomic-operations-vs-mutexes" title="Permanent link">¶</a></h2>
<p><svg viewbox="0 0 24 24"><path d="M3.5 19A1.5 1.5 0 0 1 5 20.5 1.5 1.5 0 0 1 3.5 22 1.5 1.5 0 0 1 2 20.5 1.5 1.5 0 0 1 3.5 19m5-3a2.5 2.5 0 0 1 2.5 2.5A2.5 2.5 0 0 1 8.5 21 2.5 2.5 0 0 1 6 18.5 2.5 2.5 0 0 1 8.5 16m6-1c-1.19 0-2.27-.5-3-1.35-.73.85-1.81 1.35-3 1.35-1.96 0-3.59-1.41-3.93-3.26A4.02 4.02 0 0 1 2 8a4 4 0 0 1 4-4l.77.07C7.5 3.41 8.45 3 9.5 3c1.19 0 2.27.5 3 1.35.73-.85 1.81-1.35 3-1.35 1.96 0 3.59 1.41 3.93 3.26A4.02 4.02 0 0 1 22 10a4 4 0 0 1-4 4l-.77-.07c-.73.66-1.68 1.07-2.73 1.07M6 6a2 2 0 0 0-2 2 2 2 0 0 0 2 2c.33 0 .64-.08.92-.22A2 2 0 0 0 6.5 11a2 2 0 0 0 2 2c.6 0 1.14-.27 1.5-.69l1.47-1.68L13 12.34c.38.4.91.66 1.5.66 1 0 1.83-.74 2-1.7.34.43.89.7 1.5.7a2 2 0 0 0 2-2 2 2 0 0 0-2-2c-.33 0-.64.08-.92.22A2 2 0 0 0 17.5 7a2 2 0 0 0-2-2c-.59 0-1.12.26-1.5.66l-1.53 1.71L11 5.69c-.36-.42-.9-.69-1.5-.69-1 0-1.83.74-2 1.7C7.16 6.27 6.61 6 6 6m2.5 11.5a1 1 0 0 0-1 1 1 1 0 0 0 1 1 1 1 0 0 0 1-1 1 1 0 0 0-1-1"/></svg> Atomic operations shine in simple, high-frequency scenarios—counters, flags, coordination signals—where the cost of a lock would be disproportionate. They avoid lock queues and reduce context switching. But they come with limitations: no grouping of multiple operations, no rollback, and increased complexity when applied beyond their niche.</p>
<p><svg viewbox="0 0 24 24"><path d="M3.5 19A1.5 1.5 0 0 1 5 20.5 1.5 1.5 0 0 1 3.5 22 1.5 1.5 0 0 1 2 20.5 1.5 1.5 0 0 1 3.5 19m5-3a2.5 2.5 0 0 1 2.5 2.5A2.5 2.5 0 0 1 8.5 21 2.5 2.5 0 0 1 6 18.5 2.5 2.5 0 0 1 8.5 16m6-1c-1.19 0-2.27-.5-3-1.35-.73.85-1.81 1.35-3 1.35-1.96 0-3.59-1.41-3.93-3.26A4.02 4.02 0 0 1 2 8a4 4 0 0 1 4-4l.77.07C7.5 3.41 8.45 3 9.5 3c1.19 0 2.27.5 3 1.35.73-.85 1.81-1.35 3-1.35 1.96 0 3.59 1.41 3.93 3.26A4.02 4.02 0 0 1 22 10a4 4 0 0 1-4 4l-.77-.07c-.73.66-1.68 1.07-2.73 1.07M6 6a2 2 0 0 0-2 2 2 2 0 0 0 2 2c.33 0 .64-.08.92-.22A2 2 0 0 0 6.5 11a2 2 0 0 0 2 2c.6 0 1.14-.27 1.5-.69l1.47-1.68L13 12.34c.38.4.91.66 1.5.66 1 0 1.83-.74 2-1.7.34.43.89.7 1.5.7a2 2 0 0 0 2-2 2 2 0 0 0-2-2c-.33 0-.64.08-.92.22A2 2 0 0 0 17.5 7a2 2 0 0 0-2-2c-.59 0-1.12.26-1.5.66l-1.53 1.71L11 5.69c-.36-.42-.9-.69-1.5-.69-1 0-1.83.74-2 1.7C7.16 6.27 6.61 6 6 6m2.5 11.5a1 1 0 0 0-1 1 1 1 0 0 0 1 1 1 1 0 0 0 1-1 1 1 0 0 0-1-1"/></svg> Mutexes remain the right tool for managing complex shared state, protecting multi-step critical sections, and maintaining invariants. They're easier to reason and generally safer when logic grows beyond a few lines.</p>
<p>Choosing between atomics and locks isn't about ideology but scope. When the job is simple, atomics get out of the way. When the job gets complex, locks keep you safe.</p>









  




                
                  
</body>
</html>