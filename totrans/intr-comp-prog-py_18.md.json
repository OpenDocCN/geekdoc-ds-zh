["```py\n`def square_root(x, epsilon):     \"\"\"Assumes x and epsilon are of type float; x >= 0 and epsilon > 0        Returns float y such that x-epsilon <= y*y <= x+epsilon\"\"\"`\n```", "```py\n`def roll_die():     \"\"\"Returns an int between 1 and 6\"\"\"` \n```", "```py\n﻿import random\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.integrate\n```", "```py\nMean = 0.2\nMean = 0.6\n```", "```py\nMean = 0.5029999999999999\nMean = 0.496\n```", "```py\nMean = 0.5005000000000038\nMean = 0.5003139999999954\n```", "```py`## 17.4 Distributions    A **histogram** is a plot designed to show the distribution of values in a set of data. The values are first sorted, and then divided into a fixed number of equal-width bins. A plot is then drawn that shows the number of elements in each bin. The code on the left of [Figure 17-19](#c17-fig-0020) produces the plot on the right of that figure.  ![c17-fig-0019.jpg](../images/c17-fig-0019.jpg)    [Figure 17-19](#c17-fig-0020a) Code and the histogram it generates      The function call `﻿plt.hist(vals, bins = 10, ec = 'k')` produces a histogram with 10 bins and a black line separating the bars. Matplotlib has automatically chosen the width of each bin based on the number of bins and the range of values. Looking at the code, we know that the smallest number that might appear in `vals` is `0` and the largest number `200`. Therefore, the possible values on the x-axis range from `0` to `200`. Each bin represents an equal fraction of the values on the x-axis, so the first bin will contain the elements `0-19`, the next bin the elements `20-39`, etc.    **Finger exercise**: In [Figure 17-19](#c17-fig-0020), why are the bins near the middle of the histogram taller than the bins near the sides? Hint: think about why `7` is the most common outcome of rolling a pair of dice.    By now you must be getting awfully bored with flipping coins. Nevertheless, we are going to ask you to look at yet one more coin-flipping simulation. The simulation in [Figure 17-20](#c17-fig-0021) illustrates more of Matplotlib's plotting capabilities and gives us an opportunity to get a visual notion of what standard deviation means. It produces two histograms. The first shows the result of a simulation of `100,000` trials of `100` flips of a fair coin. The second shows the result of a simulation of `100,000` trials of `1,000` flips of a fair coin.    The method `plt.annotate` is used to place some statistics on the figure showing the histogram. The first argument is the string to be displayed on the figure. The next two arguments control where the string is placed. The argument `xycoords =` `'axes fraction'` indicates the placement of the text will be expressed as a fraction of the width and height of the figure. The argument `xy = (0.67, 0.5)` indicates that the text should begin two-thirds of the way from the left edge of the figure and halfway from the bottom edge of the figure.  ![c17-fig-0020.jpg](../images/c17-fig-0020.jpg)    [Figure 17-20](#c17-fig-0021a) Plot histograms of coin flips      To facilitate comparing the two figures, we have used `plt.xlim` to force the bounds of the x-axis in the second plot to match those in the first plot, rather than letting Matplotlib choose the bounds.    When the code in [Figure 17-20](#c17-fig-0021) is run, it produces the plots in [Figure 17-21](#c17-fig-0022). Notice that while the means in both plots are about the same, the standard deviations are quite different. The spread of outcomes is much tighter when we flip the coin `1000` times per trial than when we flip the coin `100` times per trial.  ![c17-fig-0021.jpg](../images/c17-fig-0021.jpg)    [Figure 17-21](#c17-fig-0022a) Histograms of coin flips      ### 17.4.1 Probability Distributions    A histogram is a depiction of a **frequency distribution**. It tells us how often a random variable has taken on a value in some range, e.g., how often the fraction of times a coin came up heads was between `0.4` and `0.5`. It also provides information about the relative frequency of various ranges. For example, we can easily see that the fraction of heads falls between `0.4` and `0.5` far more frequently than it falls between `0.3` and `0.4`. A **probability distribution** captures the notion of relative frequency by giving the probability of a random value taking on a value within a range.    Probability distributions fall into two groups: discrete probability distributions and continuous probability distributions, depending upon whether they define the probability distribution for a discrete or a continuous random variable. A **discrete random variable** can take on one of a finite set of values, e.g., the values associated with a roll of a die. A **continuous random variable** can take on any of the infinite real values between two real numbers, e.g., the speed of a car traveling between `0` miles per hour and the car's maximum speed.    **Discrete probability distributions** are easy to describe. Since there are a finite number of values that the variable can take on, the distribution can be described by simply listing the probability of each value.    **Continuous probability distributions** are trickier. Since there are an infinite number of possible values, the probability that a continuous random variable will take on a specific value is usually close to 0\\. For example, the probability that a car is travelling at exactly `81.3457283` miles per hour is almost 0\\. Mathematicians like to describe continuous probability distributions using a **probability density function**, often abbreviated as **PDF**. A PDF describes the probability of a random variable lying between two values. Think of the PDF as defining a curve where the values on the x-axis lie between the minimum and maximum value of the random variable. (In some cases the x-axis is infinitely long.) Under the assumption that `x1` and `x2` lie in the domain of the random variable, the probability of the variable having a value between `x1` and `x2` is the area under the curve between `x1` and `x2`. [Figure 17-22](#c17-fig-0023) shows the probability density functions for the expressions `random.random()` and `random.random() + random.random()`.  ![c17-fig-0022.jpg](../images/c17-fig-0022.jpg)    [Figure 17-22](#c17-fig-0023a) PDF for random.random      For `random.random()`, the area under the curve from `0` to `1` is `1`. This makes sense because we know that the probability of `random.random()` returning a value between `0` and `1` is `1`. If we consider the area under the part of the curve for `random.random()` between `0.2` and `0.4,` it is 1.0*0.2 = `0.2`—indicating that the probability of `random.random()` returning a value between `0.2` and `0.4` is `0.2`. Similarly, the area under the curve for `random.random() + random.random()` from `0` to `2` is `1`, and the area under the curve from `0` to `1` is `0.5`. Notice, by the way, that the PDF for `random.random()` indicates that every possible interval of the same length has the same probability, whereas the PDF for `random.random() + random.random()` indicates that some ranges of values are more probable than others.    ### 17.4.2 Normal Distributions    A **normal** (or **Gaussian**) **distribution** is defined by the probability density function in [Figure 17-23](#c17-fig-0024).  ![c17-fig-0023.jpg](../images/c17-fig-0023.jpg)    [Figure 17-23](#c17-fig-0024a) PDF for Gaussian distribution      where **μ** is the mean, **σ** the standard deviation, and `*e*` is Euler's number (roughly `2.718`).[^(116)](#c17-fn-0009)    If you don't feel like studying this equation, that's fine. Just remember that normal distributions peak at the mean, fall off symmetrically above and below the mean, and asymptotically approach `0`. They have the nice mathematical property of being completely specified by two parameters: the mean and the standard deviation (the only two parameters in the equation). Knowing these is equivalent to knowing the entire distribution. The shape of the normal distribution resembles (in the eyes of some) that of a bell, so it sometimes is referred to as a **bell curve**.    [Figure 17-24](#c17-fig-0025) shows part of the PDF for a normal distribution with a mean of `0` and a standard deviation of 1\\. We can only show a portion of the PDF because the tails of a normal distribution converge towards 0, but don't reach it. In principle, no value has a zero probability of occurring.  ![c17-fig-0024.jpg](../images/c17-fig-0024.jpg)    [Figure 17-24](#c17-fig-0025a) A normal distribution      Normal distributions can be easily generated in Python programs by calling `random.gauss(mu, sigma)`, which returns a randomly chosen floating-point number from a normal distribution with mean and standard deviation `sigma`.    Normal distributions are frequently used in constructing probabilistic models because they have nice mathematical properties. Of course, finding a mathematically nice model is of no use if it provides a bad model of the actual data. Fortunately, many random variables have an approximately normal distribution. For example, physical properties of populations of plants and animals (e.g., height, weight, body temperature) typically have approximately normal distributions. Importantly, many experiments have normally distributed measurement errors. This assumption was used in the early 1800s by the German mathematician and physicist Karl Gauss, who assumed a normal distribution of measurement errors in his analysis of astronomical data (which led to the normal distribution becoming known as the Gaussian distribution in much of the scientific community).    One of the nice properties of normal distributions is that independent of the mean and standard deviation, the number of standard deviations from the mean needed to encompass a fixed fraction of the data is a constant. For example, `~68.27%` of the data will always lie within one standard deviation of the mean, `~95.45%` within two standard deviations of the mean, and ~`99.73%` within three standard deviations of the mean. This is sometimes called the `**68-95-99.7**` rule, but is more often called the **empirical rule**.    The rule can be derived by integrating the formula defining a normal distribution to get the area under the curve. Looking at [Figure 17-24](#c17-fig-0025), it is easy to believe that roughly two-thirds of the total area under the curve lies between `–1` and `1`, roughly `95%` between `-2` and `2`, and almost all of it between `-3` and `3`. But that's only one example, and it is always dangerous to generalize from a single example. We could accept the empirical rule on the unimpeachable authority of Wikipedia. However, just to be sure, and as an excuse to introduce a Python library worth knowing about, let's check it ourselves.    The **SciPy** library contains many mathematical functions commonly used by scientists and engineers. SciPy is organized into modules covering different scientific computing domains, such as signal processing and image processing. We will use a number of functions from SciPy later in this book. Here we use the function `scipy.integrate.quad`, which finds an approximation to the value of integrating a function between two points.    The function `scipy.integrate.quad` has three required parameters and one optional parameter    *   A function or method to be integrated (if the function takes more than one argument, it is integrated along the axis corresponding to the first argument) *   A number representing the lower limit of the integration *   A number representing the upper limit of the integration *   An optional tuple supplying values for all arguments, except the first, of the function to be integrated    The `quad` function returns a tuple of two floating-point numbers. The first approximates the value of the integral, and the second estimates the absolute error in the result.    Consider, for example, evaluating the integral of the unary function `abs` in the interval `0` to `5`, as pictured in [Figure 17-25](#c17-fig-0026)  ![c17-fig-0025.jpg](../images/c17-fig-0025.jpg)    [Figure 17-25](#c17-fig-0026a) Plot of absolute value of x      We don't need any fancy math to compute the area under this curve: it's simply the area of a right triangle with base and altitude of length `5`, i.e., `12.5`. So, it shouldn't be a surprise that    ```", "```py    prints `12.5`. (The second value in the tuple returned by `quad` is roughly `10`^(-13), indicating that the approximation is quite good.)    The code in [Figure 17-26](#c17-fig-0027) computes the area under portions of normal distributions for some randomly chosen means and standard deviations. The ternary function `gaussian` evaluates the formula in [Figure 17-23](#c17-fig-0024) for a Gaussian distribution with mean `mu` and standard deviation `sigma` at the point `x`. For example,    ```", "```py    ```", "```py    So, to find the area between `min_val` and `max_val` under a Gaussian with mean `mu` and standard deviation `sigma`, we need only evaluate    ```", "```py    **Finger exercise:** Find the area between -1 and 1 for a standard normal distribution.  ![c17-fig-0026.jpg](../images/c17-fig-0026.jpg)    [Figure 17-26](#c17-fig-0027a) Checking the empirical rule      When we ran the code in [Figure 17-26](#c17-fig-0027), it printed what the empirical rule predicts:    ```", "```py    People frequently use the empirical rule to derive confidence intervals. Instead of estimating an unknown value (e.g., the expected number of heads) by a single value, a **confidence interval** provides a range that is likely to contain the unknown value and a degree of confidence that the unknown value lies within that range. For example, a political poll might indicate that a candidate is likely to get `52%` of the vote `±4%` (i.e., the confidence interval is of size `8`) with a **confidence level** of `95%`. What this means is that the pollster believes that `95%` of the time the candidate will receive between `48%` and `56%` of the vote.[^(117)](#c17-fn-0010) Together the confidence interval and the confidence level are intended to indicate the reliability of the estimate.[^(118)](#c17-fn-0011) Almost always, increasing the confidence level will require widening the confidence interval.    Suppose that we run `100` trials of `100` coin flips each. Suppose further that the mean fraction of heads is `0.4999` and the standard deviation `0.0497`. For reasons we will discuss in Section 19.2, we can assume that the distribution of the means of the trials was normal. Therefore, we can conclude that if we conducted more trials of `100` flips each,    *   `~95%` of the time the fraction of heads will be `0.4999 ±0.0994` and *   `>99%` of the time the fraction of heads will be `0.4999 ±0.1491.`    It is often useful to visualize confidence intervals using **error bars**. The function `show_error_bars` in [Figure 17-27](#c17-fig-0028) calls the version of `flip_sim` in [Figure 17-20](#c17-fig-0021) and then uses    ```", "```py    to produce a plot. The first two arguments give the x and y values to be plotted. The third argument says that the values in `sds` should be multiplied by `1.96` and used to create vertical error bars. We multiply by `1.96` because `95%` of the data in a normal distribution falls within `1.96` standard deviations of the mean.  ![c17-fig-0027.jpg](../images/c17-fig-0027.jpg)    [Figure 17-27](#c17-fig-0028a) Produce plot with error bars      The call `﻿show_error_bars(3, 10, 100)` produces the plot in [Figure 17-28](#c17-fig-0029). Unsurprisingly, the error bars shrink (the standard deviation gets smaller) as the number of flips per trial grows.  ![c17-fig-0028.jpg](../images/c17-fig-0028.jpg)    [Figure 17-28](#c17-fig-0029a) Estimates with error bars      Error bars provide a lot of useful information. When practical, they should always be provided on plots. Experienced users of statistical data are justifiably suspicious when they are not.    ### 17.4.3 Continuous and Discrete Uniform Distributions    Imagine that you take a bus that arrives at your stop every `15` minutes. If you make no effort to time your arrival at the stop to correspond to the bus schedule, your expected waiting time is uniformly distributed between `0` and `15` minutes.    A uniform distribution can be either discrete or continuous. A **continuous** **uniform distribution**, also called a **rectangular distribution**, has the property that all intervals of the same length have the same probability. Consider the function `random.random`. As we saw in Section 17.4.1, the area under the PDF for any interval of a given length is the same. For example, the area under the curve between `0.23` and `0.33` is the same as the area under the curve between `0.53` and `0.63`.    A continuous uniform distribution is fully characterized by a single parameter, its range (i.e., minimum and maximum values). If the range of possible values is from *min* to *max*, the probability of a value falling in the range *x* to *y* is given by  ![c17-fig-5004.jpg](../images/c17-fig-5004.jpg)      Elements drawn from a continuous uniform distribution can be generated by calling `random.uniform(min, max)`, which returns a randomly chosen floating-point number between `min` and `max`.    In **discrete uniform distributions** each possible value is equally likely to occur, but the space of possible values is not continuous. For example, when a fair die is rolled, each of the six possible values is equally probable, but the outcomes are not uniformly distributed over the real numbers between `1` and `6`—most values, e.g., `2.5`, have a probability of 0 and a few values, e.g. `3`, have a probability of ![c17-fig-5005.jpg](../images/c17-fig-5005.jpg). One can fully characterize a discrete uniform distribution by  ![c17-fig-5006.jpg](../images/c17-fig-5006.jpg)      where *S* is the set of possible values and |*S*| the number of elements in *S*.    ### 17.4.4 Binomial and Multinomial Distributions    Random variables that can take on only a discrete set of values are called **categorical** (also **nominal** or **discrete**) **variables**.    When a categorical variable has only two possible values (e.g., success or failure), the probability distribution is called a **binomial distribution**. One way to think about a binomial distribution is as the probability of a test succeeding exactly `k` times in `n` independent trials. If the probability of a success in a single trial is `p`, the probability of exactly `k` successes in `n` independent trials is given by the formula  ![c17-fig-5007.jpg](../images/c17-fig-5007.jpg)      where  ![c17-fig-5008.jpg](../images/c17-fig-5008.jpg)      The formula ![c17-fig-5009.jpg](../images/c17-fig-5009.jpg) is known as the **binomial coefficient**. One way to read it is as “`n` choose `k`,” since it is equivalent to the number of subsets of size `k` that can be constructed from a set of size `n`. For example, there are  ![c17-fig-5010.jpg](../images/c17-fig-5010.jpg)      subsets of size two that can be constructed from the set {1,2,3,4}.    In Section 17.2, we asked about the probability of rolling exactly two `1's` in 10 rolls of a die. We now have the tools in hand to calculate this probability. Think of the 10 rolls as 10 independent trials, where the trial is a success if a `1` is rolled and a failure otherwise. A trial in which there are exactly two possible outcomes for each repetition of the experiment (success and failure in this case) is called a **Bernoulli trial**.    The binomial distribution tells us that the probability of having exactly two successful trials out of ten is    Total # pairs * prob that two trials are successful * probability that 8 trials fail  ![c17-fig-5011.jpg](../images/c17-fig-5011.jpg)      That is to say, the total number of trials multiplied by the product of the fraction of trials that succeed twice and fail eight times.    **Finger exercise:** Use the above formula to implement a function that calculates the probability of rolling exactly two `3’`s in `k` rolls of a fair die. Use this function to plot the probability as `k` varies from `2` to `100`.    The **multinomial distribution** is a generalization of the binomial distribution to categorical data with more than two possible values. It applies when there are `n` independent trials each of which has `m` mutually exclusive outcomes, with each outcome having a fixed probability of occurring. The multinomial distribution gives the probability of any given combination of numbers of occurrences of the various categories.    ### 17.4.5 Exponential and Geometric Distributions    **Exponential distributions** occur quite commonly. They are often used to model inter-arrival times, e.g., of cars entering a highway or requests for a webpage.    Consider, for example, the concentration of a drug in the human body. Assume that at each time step each molecule has a constant probability `p` of being cleared (i.e., of no longer being in the body). The system is **memoryless** in the sense that at each time step, the probability of a molecule being cleared is independent of what happened at previous times. At time `t = 0`, the probability of an individual molecule still being in the body is `1`. At time `t = 1`, the probability of that molecule still being in the body is `1 – p`. At time `t = 2`, the probability of that molecule still being in the body is `(1 – p)`². More generally, at time `t`, the probability of an individual molecule having survived is `(1 – p)`^t, i.e., it is exponential in `t`.    Suppose that at time `t`[0] there are `M`[0] molecules of the drug. In general, at time `t`, the number of molecules will be `M`[0] multiplied by the probability that an individual module has survived to time `t`. The function `clear` implemented in [Figure 17-29](#c17-fig-0036) plots the expected number of remaining molecules versus time.  ![c17-fig-0029.jpg](../images/c17-fig-0029.jpg)    [Figure 17-29](#c17-fig-0036a) Exponential clearance of molecules      The call `clear(1000, 0.01, 1000)` produces the plot in [Figure 17-30](#c17-fig-0037).  ![c17-fig-0030.jpg](../images/c17-fig-0030.jpg)    [Figure 17-30](#c17-fig-0037a) Exponential decay      This is an example of **exponential decay**. In practice, exponential decay is often talked about in terms of **half-life**, i.e., the expected time required for the initial value to decay by `50%`. We can also talk about the half-life of a single item. For example, the half-life of a single molecule is the time at which the probability of that molecule having been cleared is `0.5`. Notice that as time increases, the number of remaining molecules approaches 0\\. But it will never quite get there. This should not be interpreted as suggesting that a fraction of a molecule remains. Rather it should be interpreted as saying that since the system is probabilistic, we can never guarantee that all of the molecules have been cleared.    What happens if we make the y-axis logarithmic (by using `plt.semilogy`)? We get the plot in [Figure 17-31](#c17-fig-0038). In the plot in [Figure 17-30](#c17-fig-0037), the values on the y-axis are changing exponentially quickly relative to the values on the x-axis. If we make the y-axis itself change exponentially quickly, we get a straight line. The slope of that line is the **rate of decay**.  ![c17-fig-0031.jpg](../images/c17-fig-0031.jpg)    [Figure 17-31](#c17-fig-0038a) Plotting exponential decay with a logarithmic axis      **Exponential growth** is the inverse of exponential decay. It too is commonly seen in nature. Compound interest, the growth of algae in a swimming pool, and the chain reaction in an atomic bomb are all examples of exponential growth.    Exponential distributions can easily be generated in Python by calling the function `random.expovariate(lambd),`[^(119)](#c17-fn-0012) where `lambd` is `1.0` divided by the desired mean. The function returns a value between `0` and positive infinity if `lambd` is positive, and between negative infinity and `0` if `lambd` is negative.    The **geometric distribution** is the discrete analog of the exponential distribution.[^(120)](#c17-fn-0013) It is usually thought of as describing the number of independent attempts required to achieve a first success (or a first failure). Imagine, for example, that you have a balky car that starts only half of the time you turn the key (or push the starter button). A geometric distribution could be used to characterize the expected number of times you would have to attempt to start the car before being successful. This is illustrated by the histogram in [Figure 17-33](#c17-fig-0039), which was produced by the code in [Figure 17-32](#c17-fig-0040).  ![c17-fig-0032.jpg](../images/c17-fig-0032.jpg)    [Figure 17-32](#c17-fig-0040a) Producing a geometric distribution    ![c17-fig-0033.jpg](../images/c17-fig-0033.jpg)    [Figure 17-33](#c17-fig-0039a) A geometric distribution      The histogram implies that most of the time you'll get the car going within a few attempts. On the other hand, the long tail suggests that on occasion you may run the risk of draining your battery before the car gets going.    ### 17.4.6 Benford's Distribution    Benford's law defines a really strange distribution. Let S be a large set of decimal integers. How frequently would you expect each nonzero digit to appear as the first digit? Most of us would probably guess one ninth of the time. And when people are making up sets of numbers (e.g., faking experimental data or perpetrating financial fraud) this is typically true. It is not, however, typically true of many naturally occurring data sets. Instead, they follow a distribution predicted by Benford's law.    A set of decimal numbers is said to satisfy **Benford's law**[^(121)](#c17-fn-0014) if the probability of the first digit being `d` is consistent with `P(d) = log`[10]`(1 + 1/d)`.    For example, this law predicts that the probability of the first digit being `1` is `about 30%`! Shockingly, many actual data sets seem to observe this law. It is possible to show that the Fibonacci sequence, for example, satisfies it perfectly. That's kind of plausible since the sequence is generated by a formula. It's less easy to understand why such diverse data sets as iPhone pass codes, the number of Twitter followers per user, the population of countries, or the distances of stars from the Earth closely approximate Benford's law.[^(122)](#c17-fn-0015)    ## 17.5 Hashing and Collisions    In Section 12.3 we pointed out that by using a larger hash table, we could reduce the incidence of collisions, and thus reduce the expected time to retrieve a value. We now have the intellectual tools we need to examine that tradeoff more precisely.    First, let's get a precise formulation of the problem.    *   Assume:     *   ○ The range of the hash function is `1` to `n`     *   ○ The number of insertions is `K`     *   ○ The hash function produces a perfectly uniform distribution of the keys used in insertions, i.e., for all keys, key, and for all integers, i, in the range 1 to `n`, the probability that `hash(key) = i` is `1/n` *   What is the probability that at least one collision occurs?    The question is exactly equivalent to asking, “given `K` randomly generated integers in the range `1` to `n`, what is the probability that at least two of them are equal?” If `K ≥ n`, the probability is clearly `1`. But what about when `K < n`?    As is often the case, it is easiest to start by answering the inverse question, “given `K` randomly generated integers in the range `1` to `n`, what is the probability that none of them are equal?”    When we insert the first element, the probability of not having a collision is clearly `1`. How about the second insertion? Since there are `n-1` hash results left that are not equal to the result of the first hash, `n-1` out of `n` choices will not yield a collision. So, the probability of not getting a collision on the second insertion is ![c17-fig-5012.jpg](../images/c17-fig-5012.jpg), and the probability of not getting a collision on either of the first two insertions is ![c17-fig-5013.jpg](../images/c17-fig-5013.jpg). We can multiply these probabilities because for each insertion, the value produced by the hash function is independent of anything that has preceded it.    The probability of not having a collision after three insertions is ![c17-fig-5014.jpg](../images/c17-fig-5014.jpg). After `K` insertions it is ![c17-fig-5015.jpg](../images/c17-fig-5015.jpg).    To get the probability of having at least one collision, we subtract this value from `1`, i.e., the probability is  ![c17-fig-5016.jpg](../images/c17-fig-5016.jpg)      Given the size of the hash table and the number of expected insertions, we can use this formula to calculate the probability of at least one collision. If `K` were reasonably large, say `10,000`, it would be a bit tedious to compute the probability with pencil and paper. That leaves two choices, mathematics and programming. Mathematicians have used some fairly advanced techniques to find a way to approximate the value of this series. But unless `K` is very large, it is easier to run some code to compute the exact value of the series:    ```", "```py    If we try `collision_prob(1000, 50)` we get a probability of about `0.71` of there being at least one collision. If we consider `200` insertions, the probability of a collision is nearly `1`. Does that seem a bit high to you? Let's write a simulation, [Figure 17-34](#c17-fig-0042), to estimate the probability of at least one collision, and see if we get similar results.  ![c17-fig-0034.jpg](../images/c17-fig-0034.jpg)    [Figure 17-34](#c17-fig-0042a) Simulating a hash table      If we run the code    ```", "```py    it prints    ```", "```py    The simulation results are comfortingly similar to what we derived analytically.    Should the high probability of a collision make us think that hash tables have to be enormous to be useful? No. The probability of there being at least one collision tells us little about the expected lookup time. The expected time to look up a value depends upon the average length of the lists implementing the buckets holding the values that collided. Assuming a uniform distribution of hash values, this is simply the number of insertions divided by the number of buckets.    ## 17.6 How Often Does the Better Team Win?    Almost every October, two teams from American Major League Baseball meet in something called the World Series. They play each other repeatedly until one of the teams has won four games, and that team is called (at least in the U.S.) the “world champion.”    Setting aside the question of whether there is reason to believe that one of the participants in the World Series is indeed the best team in the world, how likely is it that a contest that can be at most seven games long will determine which of the two participants is better?    Clearly, each year one team will emerge victorious. So the question is whether we should attribute that victory to skill or to luck.    [Figure 17-35](#c17-fig-0043) contains code that can provide us with some insight into that question. The function `sim_series` has one argument, `num_series`, a positive integer describing the number of seven-game series to be simulated. It plots the probability of the better team winning the series against the probability of that team winning a single game. It varies the probability of the better team winning a single game from `0.5` to `1.0`, and produces the plot in [Figure 17-36](#c17-fig-0044).    Notice that for the better team to win `95%` of the time (`0.95` on the y‑axis), it needs to be so much better that it would win more than three out of every four games between the two teams. For comparison, in 2019, the two teams in the World Series had regular season winning percentages of `66%` (Houston Astros) and `57.4%` (Washington Nationals).[^(123)](#c17-fn-0016)  ![c17-fig-0035.jpg](../images/c17-fig-0035.jpg)    [Figure 17-35](#c17-fig-0043a) World Series simulation    ![c17-fig-0036.jpg](../images/c17-fig-0036.jpg)    [Figure 17-36](#c17-fig-0044a) Probability of winning a 7-game series      ## 17.7 Terms Introduced in Chapter    *   causal nondeterminism *   predictive nondeterminism *   deterministic program *   pseudorandom numbers *   independent event *   probability *   multiplicative law *   inferential statistics *   law of large numbers *   Bernoulli's theorem *   gambler's fallacy *   regression to the mean *   linear scaling *   variance *   standard deviation *   coefficient of variation *   histogram *   frequency distribution *   probability distribution *   discrete random variable *   continuous random variable *   discrete probability distribution *   continuous probability distribution *   probability density function (PDF) *   normal (Gaussian) distribution *   bell curve *   area under curve *   empirical (68-95-99.7) rule *   confidence interval *   confidence level *   error bars *   continuous uniform distribution *   rectangular distribution *   discrete uniform distribution *   categorical (nominal) variable *   binomial distribution *   binomial coefficient *   Bernoulli trial *   multinomial distribution *   exponential distribution *   memoryless *   exponential decay *   half-life *   rate of decay *   exponential growth *   geometric distribution *   Benford's Law```"]