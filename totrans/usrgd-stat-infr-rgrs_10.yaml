- en: 5  Linear regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5  线性回归
- en: 原文：[https://mattblackwell.github.io/gov2002-book/linear_model.html](https://mattblackwell.github.io/gov2002-book/linear_model.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mattblackwell.github.io/gov2002-book/linear_model.html](https://mattblackwell.github.io/gov2002-book/linear_model.html)
- en: '[Regression](./linear_model.html)'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[回归](./linear_model.html)'
- en: '[5  Linear regression](./linear_model.html)'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5  线性回归](./linear_model.html)'
- en: Regression is simply a set of tools for evaluating the relationship between
    an **outcome variable**, \(Y_i\), and a set of **covariates**, \(\X_i\). In particular,
    these tools show how the conditional mean of \(Y_i\) varies as a function of \(\X_i\).
    For example, we may want to know how wait times at voting precincts vary as a
    function of various socioeconomic features of the precinct, like income and racial
    composition. We can accomplish this by estimating the **regression function**
    or **conditional expectation function** (CEF) of the outcome given the covariates,
    \[ \mu(\bfx) = \E[Y_i \mid \X_i = \bfx]. \] Why are estimation and inference for
    this regression function special? Why can’t we just use the approaches we have
    seen for the mean, variance, covariance, and so on? The fundamental problem with
    the CEF is that there may be many values \(\bfx\) that can occur and many different
    conditional expectations that we will need to estimate. If any variable in \(\X_i\)
    is continuous, we must estimate an infinite number of possible values of \(\mu(\bfx)\),
    and this worsens as we add covariates to \(\X_i\). Because of that, we refer to
    this problem as the **curse of dimensionality**. How can we resolve this with
    our measly finite data?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 回归简单来说是一套评估**结果变量**，\(Y_i\)，和一组**协变量**，\(\X_i\)之间关系的工具。特别是，这些工具展示了\(Y_i\)的条件均值如何随\(\X_i\)的变化而变化。例如，我们可能想知道投票点的等待时间如何随该地区的各种社会经济特征（如收入和种族构成）的变化而变化。我们可以通过估计给定协变量的**回归函数**或**条件期望函数**（CEF）来完成这项任务，\[
    \mu(\bfx) = \E[Y_i \mid \X_i = \bfx]. \] 为什么这个回归函数的估计和推断是特殊的？为什么我们不能仅仅使用我们之前看到的均值、方差、协方差等方法呢？CEF的基本问题是可能存在许多可以发生的值\(\bfx\)和许多不同的条件期望需要估计。如果\(\X_i\)中的任何变量是连续的，我们必须估计\(\mu(\bfx)\)的无限多个可能值，并且当我们向\(\X_i\)添加协变量时，这个问题会变得更糟。正因为如此，我们将这个问题称为**维度诅咒**。我们如何用我们可怜的有限数据来解决这个问题呢？
- en: 'In this chapter, we will explore two ways of “solving” the curse of dimensionality:
    (1) assuming it away, and (2) changing the quantity of interest to something easier
    to estimate.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨两种解决“维度诅咒”的方法：（1）假设它不存在，和（2）将感兴趣的量改变为更容易估计的东西。
- en: 'Regression is so ubiquitous across many scientific fields that it has generated
    a lot of acquired notational baggage. In particular, the labels of the \(Y_i\)
    and \(\X_i\) vary greatly:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 回归在许多科学领域都非常普遍，以至于它产生了很多习得的符号负担。特别是，\(Y_i\)和\(\X_i\)的标签差异很大：
- en: 'The outcome can also be called: the response variable, the dependent variable,
    the labels (in machine learning), the left-hand side variable, or the regressand'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果也可以称为：响应变量、因变量、标签（在机器学习中）、左侧变量或回归量
- en: 'The covariates are also called: the explanatory variables, the independent
    variables, the predictors, the right-hand side variables, the regressors, inputs,
    or features'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协变量也被称为：解释变量、自变量、预测变量、右侧变量、回归变量、输入或特征
- en: 5.1 Why do we need models?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 我们为什么需要模型？
- en: 'At first glance, the connection between the CEF and parametric models might
    be hazy. For example, imagine we are interested in estimating the average wait
    times at a voting precinct (\(Y_i\)) for Black voters (\(X_i = 1\)) versus non-Black
    voters (\(X_i=0\)). In that case, there are two parameters to estimate, \[ \mu(1)
    = \E[Y_i \mid X_i = 1] \quad \text{and}\quad \mu(0) = \E[Y_i \mid X_i = 0], \]
    which we could estimate by using the plug-in estimators that replace the population
    averages with their sample counterparts, \[ \widehat{\mu}(1) = \frac{\sum_{i=1}^{n}
    Y_{i}\mathbb{1}(X_{i} = 1)}{\sum_{i=1}^{n}\mathbb{1}(X_{i} = 1)} \qquad \widehat{\mu}(0)
    = \frac{\sum_{i=1}^{n} Y_{i}\mathbb{1}(X_{i} = 0)}{\sum_{i=1}^{n}\mathbb{1}(X_{i}
    = 0)}. \] These are just the sample averages of the wait times for Black and non-Black
    voters, respectively. And because the race variable here is discrete, we are simply
    estimating sample means within subpopulations defined by race. The same logic
    would apply if we had \(k\) racial categories: we would have \(k\) conditional
    expectations to estimate and \(k\) (conditional) sample means.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，CEF与参数模型之间的联系可能不太清晰。例如，假设我们感兴趣的是估计投票点（\(Y_i\)）中黑人选民（\(X_i = 1\)）与非黑人选民（\(X_i=0\)）的平均等待时间。在这种情况下，有两个参数需要估计，\[
    \mu(1) = \E[Y_i \mid X_i = 1] \quad \text{和}\quad \mu(0) = \E[Y_i \mid X_i = 0],
    \] 我们可以通过使用插值估计量来估计这些参数，这些估计量用样本均值代替总体均值，\[ \widehat{\mu}(1) = \frac{\sum_{i=1}^{n}
    Y_{i}\mathbb{1}(X_{i} = 1)}{\sum_{i=1}^{n}\mathbb{1}(X_{i} = 1)} \qquad \widehat{\mu}(0)
    = \frac{\sum_{i=1}^{n} Y_{i}\mathbb{1}(X_{i} = 0)}{\sum_{i=1}^{n}\mathbb{1}(X_{i}
    = 0)}. \] 这些分别是黑人和非黑人选民等待时间的样本均值。由于这里的种族变量是离散的，我们只是在由种族定义的子群体内估计样本均值。如果有\(k\)个种族类别，我们就会有\(k\)个条件期望需要估计和\(k\)个（条件）样本均值。
- en: 'Now imagine that we want to know how the average wait time varies as a function
    of income so that \(X_i\) is (essentially) continuous. (Perhaps the theory here
    is that wait times may be lower in precincts with more affluent voters.) Now we
    have a different conditional expectation for every possible dollar amount from
    0 to however much the wealthiest earner makes. Suppose we choose one particular
    income, $42,238, and that we are interested in the conditional expectation \(\mu(42,238)=
    \E[Y_{i}\mid X_{i} = 42,238]\). We could use the same plug-in estimator as in
    the discrete case, \[ \widehat{\mu}(42,238) = \frac{\sum_{i=1}^{n} Y_{i}\mathbb{1}(X_{i}
    = 42,238)}{\sum_{i=1}^{n}\mathbb{1}(X_{i} = 42,238)}. \] This is straightforward,
    but there is one glaring problem with this estimator: in all likelihood, no units
    in the particular dataset have that exact income, meaning this estimator is undefined
    because we would be dividing by zero.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想知道平均等待时间如何随着收入的变化而变化，使得\(X_i\)（本质上）是连续的。（也许这里的理论是，在选民更富裕的投票点中等待时间可能更低。）现在，对于从0到最富有收入者的收入，我们都有不同的条件期望。假设我们选择一个特定的收入，$42,238，并且我们感兴趣的条件期望是\(\mu(42,238)=
    \E[Y_{i}\mid X_{i} = 42,238]\)。我们可以使用与离散情况相同的插值估计量，\[ \widehat{\mu}(42,238) =
    \frac{\sum_{i=1}^{n} Y_{i}\mathbb{1}(X_{i} = 42,238)}{\sum_{i=1}^{n}\mathbb{1}(X_{i}
    = 42,238)}. \] 这很简单，但这个估计量有一个明显的问题：在所有可能的情况下，特定的数据集中没有单位具有确切的收入，这意味着这个估计量是未定义的，因为我们将会除以零。
- en: 'One solution to this problem is to use **subclassification** to turn the continuous
    variable into a discrete one and then proceed with the discrete approach above.
    For example, we could group incomes into $25,000 bins and then calculate the average
    wait times of anyone between, say, $25,000 and $50,000 income. When we make this
    estimator switch for practical purposes, we need to connect it back to the DGP
    of interest. We could **assume** that the CEF of interest only depends on these
    binned means, giving us:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法之一是使用**子分类**将连续变量转换为离散变量，然后按照上述离散方法进行操作。例如，我们可以将收入分为$25,000个区间，然后计算收入在，比如说，$25,000到$50,000之间的任何人的平均等待时间。当我们出于实际目的切换这个估计器时，我们需要将其与感兴趣的DGP（数据生成过程）联系起来。我们可以**假设**感兴趣的CEF（条件期望函数）只依赖于这些分组均值，从而得到：
- en: \[ \mu(x) = \begin{cases} \E[Y_{i} \mid 0 \leq X_{i} < 25,000] &\text{if } 0
    \leq x < 25,000 \\ \E[Y_{i} \mid 25,000 \leq X_{i} < 50,000] &\text{if } 25,000
    \leq x < 50,000\\ \E[Y_{i} \mid 50,000 \leq X_{i} < 100,000] &\text{if } 50,000
    \leq x < 100,000\\ \vdots \\ \E[Y_{i} \mid 200,000 \leq X_{i}] &\text{if } 200,000
    \leq x\\ \end{cases} \] This approach assumes, perhaps incorrectly, that the average
    wait time does not vary within the bins. [Figure 5.1](#fig-cef-binned) shows a
    hypothetical joint distribution between income and wait times with the true CEF,
    \(\mu(x)\), shown in red. The figure also shows the bins created by subclassification
    and the implied CEF if we assume bin-constant means in blue. Note that the blue
    function approximates the true CEF but deviates from it close to the bin edges.
    The trade-off is that once we make the assumption that wait times do not vary
    within the bins, we only have to estimate one mean for every bin rather than an
    infinite number of means for each possible income.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mu(x) = \begin{cases} \E[Y_{i} \mid 0 \leq X_{i} < 25,000] &\text{if } 0
    \leq x < 25,000 \\ \E[Y_{i} \mid 25,000 \leq X_{i} < 50,000] &\text{if } 25,000
    \leq x < 50,000\\ \E[Y_{i} \mid 50,000 \leq X_{i} < 100,000] &\text{if } 50,000
    \leq x < 100,000\\ \vdots \\ \E[Y_{i} \mid 200,000 \leq X_{i}] &\text{if } 200,000
    \leq x\\ \end{cases} \] 这种方法假设，也许是不正确的，平均等待时间在分组内不会变化。[图5.1](#fig-cef-binned)显示了收入和等待时间的假设联合分布，其中真实的CEF，\(\mu(x)\)，以红色显示。该图还显示了由子分类创建的分组以及如果我们假设分组内均值恒定，所隐含的CEF（蓝色）。请注意，蓝色函数近似真实的CEF，但在分组边缘偏离。权衡是，一旦我们假设等待时间在分组内不变化，我们只需要为每个分组估计一个均值，而不是为每个可能的收入估计无限多个均值。
- en: '![](../Images/131afefbd3d1276699460b27af62c570.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/131afefbd3d1276699460b27af62c570.png)'
- en: 'Figure 5.1: Hypothetical joint distribution of income and poll wait times (contour
    plot), conditional expectation function (red), and the conditional expectation
    of the binned income (blue).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：收入和排队等待时间的假设联合分布（等高线图），条件期望函数（红色），以及分箱收入的条件期望（蓝色）。
- en: 'Similarly, we could **assume** that the CEF follows a simple functional form
    such as a line: \[ \mu(x) = \E[Y_{i}\mid X_{i} = x] = \beta_{0} + \beta_{1} x.
    \] This assumption reduces our infinite number of unknowns (the conditional mean
    at every possible income) to just two unknowns: (1) the slope and (2) the intercept.
    As we will see, we can use the standard ordinary least squares to estimate these
    parameters. Note that if the true CEF is nonlinear, this assumption is incorrect,
    and any estimate based on this assumption might be biased or even inconsistent.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以**假设**CEF遵循简单的函数形式，例如一条直线：\[ \mu(x) = \E[Y_{i}\mid X_{i} = x] = \beta_{0}
    + \beta_{1} x. \] 这个假设将我们无限多的未知数（每个可能收入的条件均值）减少到只有两个未知数：（1）斜率和（2）截距。正如我们将看到的，我们可以使用标准的普通最小二乘法来估计这些参数。请注意，如果真实的CEF是非线性的，这个假设是不正确的，基于这个假设的任何估计可能是有偏的，甚至是不一致的。
- en: We call the binning and linear assumptions on \(\mu(x)\) **functional form**
    assumptions because they restrict the class of functions that \(\mu(x)\) can take.
    While powerful, these types of assumptions can muddy the roles of defining the
    quantity of interest and estimation. If our estimator \(\widehat{\mu}(x)\) performs
    poorly, it will be difficult to tell if this is because the estimator is flawed
    or our functional form assumptions are incorrect.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称对\(\mu(x)\)的分组和线性假设为**函数形式**假设，因为它们限制了\(\mu(x)\)可以采取的函数类。虽然这些假设很有力，但它们可能会模糊定义感兴趣的数量和估计的角色。如果我们的估计量\(\widehat{\mu}(x)\)表现不佳，将很难判断这是否是因为估计量本身有缺陷，或者我们的函数形式假设是不正确的。
- en: 'To clarify these issues, we will pursue a different approach: understanding
    what linear regression can estimate under minimal assumptions and then investigating
    how well this estimand approximates the true CEF.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了阐明这些问题，我们将采取不同的方法：首先理解线性回归在最小假设下可以估计什么，然后调查这种估计量如何近似真实的CEF。
- en: 5.2 Population linear regression
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 人口线性回归
- en: 5.2.1 Bivariate linear regression
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 双变量线性回归
- en: Let’s set aside the idea of the conditional expectation function and instead
    focus on finding the **linear** function of a single covariate \(X_i\) that best
    predicts the outcome. Recall from your earlier mathematical training that linear
    functions have the form \(a + bX_i\). The **best linear predictor** (BLP) or **population
    linear regression** of \(Y_i\) on \(X_i\) is defined as \[ m(x) = \beta_0 + \beta_1
    x \quad\text{where, }\quad (\beta_{0}, \beta_{1}) = \argmin_{(b_{0}, b_{1}) \in
    \mathbb{R}^{2}}\; \E[(Y_{i} - b_{0} - b_{1}X_{i} )^{2}]. \] The expression being
    minimized is the expected prediction error, or the (squared) distance between
    the observed outcome and the outcome as predicted with a particular slope and
    intercept. The best linear predictor is the line (that is, slope and intercept
    values) that results in the lowest expected prediction error. Note that this function
    is a feature of the joint distribution of the data—the DGP—and so we cannot observe
    it directly. It must be estimated. The BLP is an alternative to the CEF for summarizing
    the relationship between the outcome and the covariate, though as we discuss later
    they will sometimes be equal. We call \((\beta_{0}, \beta_{1})\) the **population
    linear regression coefficients**. Note that \(m(x)\) could differ greatly from
    the CEF \(\mu(x)\) if the latter is nonlinear.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时放下条件期望函数的概念，转而寻找单个协变量 \(X_i\) 的最佳线性函数来预测结果。回想一下你早期的数学训练，线性函数的形式是 \(a +
    bX_i\)。\(Y_i\) 关于 \(X_i\) 的最佳线性预测器（BLP）或**总体线性回归**定义为 \[ m(x) = \beta_0 + \beta_1
    x \quad\text{其中， }\quad (\beta_{0}, \beta_{1}) = \argmin_{(b_{0}, b_{1}) \in \mathbb{R}^{2}}\;
    \E[(Y_{i} - b_{0} - b_{1}X_{i} )^{2}]. \] 被最小化的表达式是预测误差的期望，或者说是观察到的结果与使用特定斜率和截距预测的结果之间的（平方）距离。最佳线性预测器是导致最低期望预测误差的线（即斜率和截距值）。请注意，这个函数是数据联合分布（DGP）的特征，因此我们无法直接观察到它。它必须被估计。BLP是总结结果和协变量之间关系的另一种方法，尽管我们稍后讨论它们有时会相等。我们将
    \((\beta_{0}, \beta_{1})\) 称为**总体线性回归系数**。请注意，如果后者是非线性的，\(m(x)\) 可能会与CEF \(\mu(x)\)
    差异很大。
- en: We can solve for the best linear predictor using standard calculus (taking the
    derivative with respect to each coefficient, setting those equations equal to
    0, and solving the system of equations). The first-order conditions, in this case,
    are \[ \begin{aligned} \frac{\partial \E[(Y_{i} - b_{0} - b_{1}X_{i} )^{2}]}{\partial
    b_{0}} = \E[-2(Y_{i} - \beta_{0} - \beta_{1}X_{i})] = 0 \\ \frac{\partial \E[(Y_{i}
    - b_{0} - b_{1}X_{i} )^{2}]}{\partial b_{1}} = \E[-2(Y_{i} - \beta_{0} - \beta_{1}X_{i})X_{i}]
    = 0 \end{aligned} \] Given the linearity of expectations, it is easy to solve
    for \(\beta_0\) in terms of \(\beta_1\), \[ \beta_{0} = \E[Y_{i}] - \beta_{1}\E[X_{i}].
    \] We can plug this into the first-order condition for \(\beta_1\) to get \[ \begin{aligned}
    0 &= \E[Y_{i}X_{i}] - (\E[Y_{i}] - \beta_{1}\E[X_{i}])\E[X_{i}] - \beta_{1}\E[X_{i}^{2}]
    \\ &= \E[Y_{i}X_{i}] - \E[Y_{i}]\E[X_{i}] - \beta_{1}(\E[X_{i}^{2}] - \E[X_{i}]^{2})
    \\ &= \cov(X_{i},Y_{i}) - \beta_{1}\V[X_{i}]\\ \beta_{1} &= \frac{\cov(X_{i},Y_{i})}{\V[X_{i}]}
    \end{aligned} \]
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用标准微积分（对每个系数求导，将那些方程设为等于0，并解这个方程组）来求解最佳线性预测器。在这种情况下，一阶条件是 \[ \begin{aligned}
    \frac{\partial \E[(Y_{i} - b_{0} - b_{1}X_{i} )^{2}]}{\partial b_{0}} = \E[-2(Y_{i}
    - \beta_{0} - \beta_{1}X_{i})] = 0 \\ \frac{\partial \E[(Y_{i} - b_{0} - b_{1}X_{i}
    )^{2}]}{\partial b_{1}} = \E[-2(Y_{i} - \beta_{0} - \beta_{1}X_{i})X_{i}] = 0
    \end{aligned} \] 由于期望的线性，我们可以很容易地用 \(\beta_1\) 来求解 \(\beta_0\)，\[ \beta_{0} =
    \E[Y_{i}] - \beta_{1}\E[X_{i}]. \] 我们可以将这个结果代入 \(\beta_1\) 的一阶条件中，得到 \[ \begin{aligned}
    0 &= \E[Y_{i}X_{i}] - (\E[Y_{i}] - \beta_{1}\E[X_{i}])\E[X_{i}] - \beta_{1}\E[X_{i}^{2}]
    \\ &= \E[Y_{i}X_{i}] - \E[Y_{i}]\E[X_{i}] - \beta_{1}(\E[X_{i}^{2}] - \E[X_{i}]^{2})
    \\ &= \cov(X_{i},Y_{i}) - \beta_{1}\V[X_{i}]\\ \beta_{1} &= \frac{\cov(X_{i},Y_{i})}{\V[X_{i}]}
    \end{aligned} \]
- en: 'Thus, the slope on the population linear regression of \(Y_i\) on \(X_i\) is
    equal to the ratio of the covariance of the two variables divided by the variance
    of \(X_i\). It follows from this that the covariance will determine the sign of
    the slope: positive covariances will lead to positive \(\beta_1\) and negative
    covariances will lead to negative \(\beta_1\). In addition, if \(Y_i\) and \(X_i\)
    are independent, \(\beta_1 = 0\). The slope scales this covariance by the variance
    of the covariate, so slopes will be lower for more spread-out covariates and higher
    for less spread-out covariates. If we define the correlation between these variables
    as \(\rho_{YX}\), then we can relate the coefficient to this quantity as \[ \beta_1
    = \rho_{YX}\sqrt{\frac{\V[Y_i]}{\V[X_i]}}. \]'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，\(Y_i\) 在 \(X_i\) 上的种群线性回归的斜率等于这两个变量协方差与 \(X_i\) 方差的比值。由此可知，协方差将决定斜率的符号：正协方差将导致正的
    \(\beta_1\)，而负协方差将导致负的 \(\beta_1\)。此外，如果 \(Y_i\) 和 \(X_i\) 是独立的，则 \(\beta_1 =
    0\)。斜率通过协变量的方差来缩放这个协方差，因此对于分布更广的协变量，斜率会较低，而对于分布较窄的协变量，斜率会较高。如果我们定义这些变量之间的相关系数为
    \(\rho_{YX}\)，那么我们可以将系数与这个量联系起来，如下所示 \[ \beta_1 = \rho_{YX}\sqrt{\frac{\V[Y_i]}{\V[X_i]}}.
    \]
- en: Collecting these various results together, we can write the population linear
    regression as \[ m(x) = \beta_0 + \beta_1x = \E[Y_i] + \beta_1(x - \E[X_i]), \]
    which shows how we adjust our best guess about \(Y_i\) from the mean of the outcome
    using the covariate.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些各种结果汇总起来，我们可以将人口线性回归表示为 \[ m(x) = \beta_0 + \beta_1x = \E[Y_i] + \beta_1(x
    - \E[X_i]), \] 这显示了我们是怎样通过协变量来调整对 \(Y_i\) 的最佳猜测，即从结果的均值出发。
- en: Be sure to remember that the BLP, \(m(x)\), and the CEF, \(\mu(x)\), are distinct
    entities. If the CEF is nonlinear, as in [Figure 5.2](#fig-cef-blp), there will
    be a difference between these functions, meaning that the BLP might produce subpar
    predictions. We will derive a formal connection between the BLP and the CEF below.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一定要记住，BLP（\(m(x)\)）、CEF（\(\mu(x)\)）是不同的实体。如果CEF是非线性的，如[图5.2](#fig-cef-blp)所示，这两个函数之间将存在差异，这意味着BLP可能产生较差的预测。我们将在下面推导BLP和CEF之间的正式联系。
- en: '![](../Images/6cb1404c7f6a85d9c5ad419c9e20b557.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6cb1404c7f6a85d9c5ad419c9e20b557.png)'
- en: 'Figure 5.2: Comparison of the CEF and the best linear predictor.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：CEF和最佳线性预测器的比较。
- en: 5.2.2 Beyond linear approximations
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 超越线性近似
- en: The linear part of the “best linear predictor” is less restrictive than it appears
    at first glance. We can easily modify the minimum MSE problem to find the best
    quadratic, cubic, or general polynomial function of \(X_i\) that predicts \(Y_i\).
    For example, the quadratic function of \(X_i\) that best predicts \(Y_i\) would
    be \[ m(X_i, X_i^2) = \beta_0 + \beta_1X_i + \beta_2X_i^2 \quad\text{where}\quad
    \argmin_{(b_0,b_1,b_2) \in \mathbb{R}^3}\;\E[(Y_{i} - b_{0} - b_{1}X_{i} - b_{2}X_{i}^{2})^{2}].
    \] While the equation is now a quadratic function of the covariates, it is still
    a linear function of the unknown parameters \((\beta_{0}, \beta_{1}, \beta_{2})\),
    so we still call this a best linear predictor.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: “最佳线性预测器”的线性部分比乍一看要宽松。我们可以轻松地将最小均方误差问题修改为寻找最佳二次、三次或一般多项式函数 \(X_i\)，以预测 \(Y_i\)。例如，最佳预测
    \(Y_i\) 的 \(X_i\) 的二次函数将是 \[ m(X_i, X_i^2) = \beta_0 + \beta_1X_i + \beta_2X_i^2
    \quad\text{其中}\quad \argmin_{(b_0,b_1,b_2) \in \mathbb{R}^3}\;\E[(Y_{i} - b_{0}
    - b_{1}X_{i} - b_{2}X_{i}^{2})^{2}]. \] 虽然现在方程是协变量的二次函数，但它仍然是未知参数 \((\beta_{0},
    \beta_{1}, \beta_{2})\) 的线性函数，所以我们仍然称这为最佳线性预测器。
- en: We could include higher-order terms of \(X_i\) in the same manner, and, including
    more polynomial terms, \(X_i^p\), will allow the BLP to be a more flexible function
    of \(X_i\). When we estimate the BLP, however, we usually pay for this flexibility
    with overfitting and high variance in our estimates.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用相同的方式将 \(X_i\) 的高阶项包括在内，并且包括更多的多项式项，\(X_i^p\)，这将使BLP成为 \(X_i\) 的更灵活的函数。然而，当我们估计BLP时，我们通常为此灵活性付出过拟合和估计高方差的成本。
- en: 5.2.3 Linear prediction with multiple covariates
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 基于多个协变量的线性预测
- en: We now generalize the idea of a best linear predictor to a setting with an arbitrary
    number of covariates, which more flexibly captures real-life empirical research
    scenarios. In this setting, recall that the linear function will be
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将最佳线性预测器的概念推广到具有任意数量协变量的环境中，这可以更灵活地捕捉现实生活中的实证研究场景。在这个环境中，请记住，线性函数将是
- en: \[ \bfx'\bfbeta = x_{1}\beta_{1} + x_{2}\beta_{2} + \cdots + x_{k}\beta_{k}.
    \] We will define the **best linear predictor** (BLP) to be \[ m(\bfx) = \bfx'\bfbeta,
    \quad \text{where}\quad \bfbeta = \argmin_{\mb{b} \in \real^k}\; \E\bigl[ \bigl(Y_{i}
    - \mb{X}_{i}'\mb{b} \bigr)^2\bigr] \]
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfx'\bfbeta = x_{1}\beta_{1} + x_{2}\beta_{2} + \cdots + x_{k}\beta_{k}.
    \] 我们将定义**最佳线性预测器**（BLP）为 \[ m(\bfx) = \bfx'\bfbeta, \quad \text{其中}\quad \bfbeta
    = \argmin_{\mb{b} \in \real^k}\; \E\bigl[ \bigl(Y_{i} - \mb{X}_{i}'\mb{b} \bigr)^2\bigr]
    \]
- en: 'This BLP solves the same fundamental optimization problem as in the bivariate
    case: it chooses the set of coefficients that minimizes the expected mean-squared
    error, where the expectation is over the joint distribution of the data.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个BLP解决了与双变量情况相同的根本优化问题：它选择一组系数，以最小化期望均方误差，这里的期望是关于数据的联合分布。
- en: '*Best linear projection assumptions* *Without some assumptions on the joint
    distribution of the data, the following “regularity conditions” will ensure the
    existence of the BLP:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '*最佳线性投影假设* *如果没有对数据的联合分布做出一些假设，以下“正则条件”将确保BLP的存在：'
- en: \(\E[Y^2] < \infty\) (outcome has finite mean/variance)
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(\E[Y^2] < \infty\) (outcome has finite mean/variance)
- en: \(\E\Vert \mb{X} \Vert^2 < \infty\) (\(\mb{X}\) has finite means/variances/covariances)
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(\E\Vert \mb{X} \Vert^2 < \infty\) (\(\mb{X}\) has finite means/variances/covariances)
- en: '\(\mb{Q}_{\mb{XX}} = \E[\mb{XX}'']\) is positive definite (columns of \(\X\)
    are linearly independent)*  *Under these assumptions, it is possible to derive
    a closed-form expression for the **population coefficients** \(\bfbeta\) using
    matrix calculus. To set up the optimization problem, we find the first-order condition
    by taking the derivative of the expectation of the squared errors. First, take
    the derivative of the squared prediction errors using the chain rule: \[ \begin{aligned}
    \frac{\partial}{\partial \mb{b}}\left(Y_{i} - \X_{i}''\mb{b}\right)^{2} &= 2\left(Y_{i}
    - \X_{i}''\mb{b}\right)\frac{\partial}{\partial \mb{b}}(Y_{i} - \X_{i}''\mb{b})
    \\ &= -2\left(Y_{i} - \X_{i}''\mb{b}\right)\X_{i} \\ &= -2\X_{i}\left(Y_{i} -
    \X_{i}''\mb{b}\right) \\ &= -2\left(\X_{i}Y_{i} - \X_{i}\X_{i}''\mb{b}\right),
    \end{aligned} \] where the third equality comes from the fact that \((Y_{i} -
    \X_{i}''\bfbeta)\) is a scalar. We can plug this into the expectation to get the
    first-order condition and solve for \(\bfbeta\), \[ \begin{aligned} 0 &= -2\E[\X_{i}Y_{i}
    - \X_{i}\X_{i}''\bfbeta ] \\ \E[\X_{i}\X_{i}''] \bfbeta &= \E[\X_{i}Y_{i}], \end{aligned}
    \] which implies the population coefficients are \[ \bfbeta = \left(\E[\X_{i}\X_{i}'']\right)^{-1}\E[\X_{i}Y_{i}]
    = \mb{Q}_{\mb{XX}}^{-1}\mb{Q}_{\mb{X}Y} \] This gives us an expression for the
    coefficients for the population best linear predictor in terms of the joint distribution
    \((Y_{i}, \X_{i})\).'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(\mb{Q}_{\mb{XX}} = \E[\mb{XX}']\)是正定的（\(\X\)的列线性无关）*  *在这些假设下，可以使用矩阵微积分推导出**总体系数**\(\bfbeta\)的闭式表达式。为了建立优化问题，我们通过对平方误差的期望求导来找到一阶条件。首先，使用链式法则对平方预测误差求导：\[
    \begin{aligned} \frac{\partial}{\partial \mb{b}}\left(Y_{i} - \X_{i}'\mb{b}\right)^{2}
    &= 2\left(Y_{i} - \X_{i}'\mb{b}\right)\frac{\partial}{\partial \mb{b}}(Y_{i} -
    \X_{i}'\mb{b}) \\ &= -2\left(Y_{i} - \X_{i}'\mb{b}\right)\X_{i} \\ &= -2\X_{i}\left(Y_{i}
    - \X_{i}'\mb{b}\right) \\ &= -2\left(\X_{i}Y_{i} - \X_{i}\X_{i}'\mb{b}\right),
    \end{aligned} \] 其中第三个等式来自于\((Y_{i} - \X_{i}'\bfbeta)\)是一个标量的事实。我们可以将这个结果代入期望中，得到一阶条件并求解\(\bfbeta\)，\[
    \begin{aligned} 0 &= -2\E[\X_{i}Y_{i} - \X_{i}\X_{i}'\bfbeta ] \\ \E[\X_{i}\X_{i}']
    \bfbeta &= \E[\X_{i}Y_{i}], \end{aligned} \] 这意味着总体系数是 \[ \bfbeta = \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}]
    = \mb{Q}_{\mb{XX}}^{-1}\mb{Q}_{\mb{X}Y} \] 这给出了关于联合分布\((Y_{i}, \X_{i})\)的总体最佳线性预测系数的表达式。
- en: A couple of facts might be useful for interpreting this expression substantively.
    Recall that \(\mb{Q}_{\mb{XX}} = \E[\X_{i}\X_{i}']\) is a \(k\times k\) matrix
    and \(\mb{Q}_{\X Y} = \E[\X_{i}Y_{i}]\) is a \(k\times 1\) column vector, which
    implies that \(\bfbeta\) is also a \(k \times 1\) column vector.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一些事实可能有助于实质性地解释这个表达式。回想一下，\(\mb{Q}_{\mb{XX}} = \E[\X_{i}\X_{i}']\)是一个\(k\times
    k\)矩阵，\(\mb{Q}_{\X Y} = \E[\X_{i}Y_{i}]\)是一个\(k\times 1\)列向量，这意味着\(\bfbeta\)也是一个\(k
    \times 1\)列向量。
- en: '*Note* *What does the expression for the population regression coefficients
    mean? It is helpful to separate the intercept or constant term so that we have
    \[ Y_{i} = \beta_{0} + \X''\bfbeta + e_{i}, \] so \(\bfbeta\) refers to just the
    vector of coefficients for the covariates. In this case, we can write the coefficients
    in a more interpretable way: \[ \bfbeta = \V[\X]^{-1}\text{Cov}(\X, Y), \qquad
    \beta_0 = \mu_Y - \mb{\mu}''_{\mb{X}}\bfbeta \]'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* *人口回归系数的表达式意味着什么？将截距或常数项分开是有帮助的，这样我们就有 \[ Y_{i} = \beta_{0} + \X''\bfbeta
    + e_{i}, \] 因此 \(\bfbeta\) 仅指协变量的系数向量。在这种情况下，我们可以以更可解释的方式写出系数：\[ \bfbeta = \V[\X]^{-1}\text{Cov}(\X,
    Y), \qquad \beta_0 = \mu_Y - \mb{\mu}''_{\mb{X}}\bfbeta \]'
- en: Thus, the population coefficients take the covariance between the outcome and
    the covariates and “divide” it by information about variances and covariances
    of the covariates. The intercept recenters the regression so that projection errors
    are mean zero. This means that these coefficients generalize the bivariate formula
    to this multiple covariate context.*  *With an expression for the population linear
    regression coefficients, we can write the linear projection as \[ m(\X_{i}) =
    \X_{i}'\left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}] = \X_{i}'\mb{Q}_{\mb{XX}}^{-1}\mb{Q}_{\mb{X}Y}
    \]**  **### 5.2.4 Projection error
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，人口系数取结果与协变量的协方差，并将其“除以”协变量方差和协方差的信息。截距将回归重新居中，使得投影误差的均值为零。这意味着这些系数将双变量公式推广到这个多个协变量环境中。*  *有了人口线性回归系数的表达式，我们可以将线性投影写为
    \[ m(\X_{i}) = \X_{i}'\left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}] = \X_{i}'\mb{Q}_{\mb{XX}}^{-1}\mb{Q}_{\mb{X}Y}
    \]**  **### 5.2.4 投影误差
- en: 'The **projection error** or is the difference between the actual value of \(Y_i\)
    and the projection, \[ e_{i} = Y_{i} - m(\X_{i}) = Y_i - \X_{i}''\bfbeta, \] where
    we have made no assumptions about this error yet. The projection error is simply
    the prediction error of the best linear prediction for a particular unit in the
    data. Rewriting this definition, we can see that we can always write the outcome
    as the linear projection plus the projection error, \[ Y_{i} = \X_{i}''\bfbeta
    + e_{i}. \] Notice that this looks suspiciously similar to a linearity assumption
    on the CEF, but we haven’t made any assumptions here. Instead, we just used the
    definition of the projection error to write a tautological statement: \[ Y_{i}
    = \X_{i}''\bfbeta + e_{i} = \X_{i}''\bfbeta + Y_{i} - \X_{i}''\bfbeta = Y_{i}.
    \] The critical difference between this representation and the usual linear model
    assumption is what properties \(e_{i}\) possesses.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**投影误差**或是指 \(Y_i\) 的实际值与投影之间的差异，\[ e_{i} = Y_{i} - m(\X_{i}) = Y_i - \X_{i}''\bfbeta,
    \] 其中我们尚未对此误差做出任何假设。投影误差仅仅是针对数据中特定单位的最佳线性预测的预测误差。重新表述这个定义，我们可以看到我们总是可以将结果写成线性投影加上投影误差，\[
    Y_{i} = \X_{i}''\bfbeta + e_{i}. \] 注意这看起来与对 CEF 的线性假设非常相似，但我们在这里没有做出任何假设。相反，我们只是使用了投影误差的定义来写出一个自相矛盾的说法：\[
    Y_{i} = \X_{i}''\bfbeta + e_{i} = \X_{i}''\bfbeta + Y_{i} - \X_{i}''\bfbeta =
    Y_{i}. \] 与通常线性模型假设的关键区别在于 \(e_{i}\) 所具有的性质。'
- en: A key property of the projection errors is that when the covariate vector includes
    an “intercept” or constant term, the projection errors are uncorrelated with the
    covariates. To see this, first note that \(\E[\X_{i}e_{i}] = 0\) since \[ \begin{aligned}
    \E[\X_{i}e_{i}] &= \E[\X_{{i}}(Y_{i} - \X_{i}'\bfbeta)] \\ &= \E[\X_{i}Y_{i}]
    - \E[\X_{i}\X_{i}']\bfbeta \\ &= \E[\X_{i}Y_{i}] - \E[\X_{i}\X_{i}']\left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}]
    \\ &= \E[\X_{i}Y_{i}] - \E[\X_{i}Y_{i}] = 0 \end{aligned} \] Thus, for every \(X_{ij}\)
    in \(\X_{i}\), we have \(\E[X_{ij}e_{i}] = 0\). If one of the entries in \(\X_i\)
    is a constant 1, then this also implies that \(\E[e_{i}] = 0\). Together, these
    facts imply that the projection error is uncorrelated with each \(X_{ij}\), since
    \[ \cov(X_{ij}, e_{i}) = \E[X_{ij}e_{i}] - \E[X_{ij}]\E[e_{i}] = 0 - 0 = 0 \]
    Note that we still have made no assumptions about these projection errors except
    for some mild regularity conditions on the joint distribution of the outcome and
    covariates. Thus, in very general settings, we can write the linear projection
    model \(Y_i = \X_i'\bfbeta + e_i\) where \(\bfbeta = \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}]\)
    and conclude that \(\E[\X_{i}e_{i}] = 0\) by definition, not by assumption.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 投影误差的一个关键性质是，当协变量向量包括一个“截距”或常数项时，投影误差与协变量不相关。为了理解这一点，首先注意 \(\E[\X_{i}e_{i}]
    = 0\)，因为 \[ \begin{aligned} \E[\X_{i}e_{i}] &= \E[\X_{{i}}(Y_{i} - \X_{i}'\bfbeta)]
    \\ &= \E[\X_{i}Y_{i}] - \E[\X_{i}\X_{i}']\bfbeta \\ &= \E[\X_{i}Y_{i}] - \E[\X_{i}\X_{i}']\left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}]
    \\ &= \E[\X_{i}Y_{i}] - \E[\X_{i}Y_{i}] = 0 \end{aligned} \] 因此，对于 \(\X_{i}\)
    中的每一个 \(X_{ij}\)，我们有 \(\E[X_{ij}e_{i}] = 0\)。如果 \(\X_i\) 中的一个条目是常数1，那么这也意味着 \(\E[e_{i}]
    = 0\)。这些事实共同表明，投影误差与每个 \(X_{ij}\) 不相关，因为 \[ \cov(X_{ij}, e_{i}) = \E[X_{ij}e_{i}]
    - \E[X_{ij}]\E[e_{i}] = 0 - 0 = 0 \] 注意，我们除了对结果和协变量的联合分布的一些温和的正规性条件外，对这些投影误差没有做出任何假设。因此，在非常一般的设置中，我们可以写出线性投影模型
    \(Y_i = \X_i'\bfbeta + e_i\)，其中 \(\bfbeta = \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}]\)，并通过定义得出
    \(\E[\X_{i}e_{i}] = 0\)，而不是通过假设。
- en: The projection error is uncorrelated with the covariates, so does this mean
    that the CEF is linear? Unfortunately, no. Recall that while independence implies
    this lack of correlation, the reverse does not hold. So when we look at the CEF,
    we have \[ \E[Y_{i} \mid \X_{i}] = \X_{i}'\bfbeta + \E[e_{i} \mid \X_{i}], \]
    and the last term \(\E[e_{i} \mid \X_{i}]\) would only be 0 if the errors were
    independent of the covariates, so \(\E[e_{i} \mid \X_{i}] = \E[e_{i}] = 0\). But
    nowhere in the linear projection model did we assume this. So while we can (almost)
    always write the outcome as \(Y_i = \X_i'\bfbeta + e_i\) and have those projection
    errors be uncorrelated with the covariates, it will require additional assumptions
    to ensure that the true CEF is, in fact, linear \(\E[Y_{i} \mid \X_{i}] = \X_{i}'\bfbeta\).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 投影误差与协变量不相关，这意味着CEF是线性的吗？不幸的是，不是的。回想一下，虽然独立性意味着这种缺乏相关性，但反之则不然。因此，当我们观察CEF时，我们有
    \[ \E[Y_{i} \mid \X_{i}] = \X_{i}'\bfbeta + \E[e_{i} \mid \X_{i}], \] 而最后一个项 \(\E[e_{i}
    \mid \X_{i}]\) 只有在误差与协变量独立时才会是0，即 \(\E[e_{i} \mid \X_{i}] = \E[e_{i}] = 0\)。但在线性投影模型中，我们并没有假设这一点。因此，虽然我们几乎总是可以写出结果
    \(Y_i = \X_i'\bfbeta + e_i\) 并使这些投影误差与协变量不相关，但需要额外的假设来确保真实的CEF确实是线性的 \(\E[Y_{i}
    \mid \X_{i}] = \X_{i}'\bfbeta\)。
- en: To step back for a moment, what have we shown here? In a nutshell, we showed
    that a population linear regression exists under very general conditions and that
    we can write the coefficients of that population linear regression as a function
    of expectations of the joint distribution of the data. We did not, however, assume
    that the CEF was linear nor that the projection errors were normally distributed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了暂时回顾一下，我们在这里展示了什么？简而言之，我们展示了在非常一般的条件下存在总体线性回归，并且我们可以将总体线性回归的系数写成数据联合分布期望的函数。然而，我们没有假设CEF是线性的，也没有假设投影误差是正态分布的。
- en: 'Why is this important? The ordinary least squares estimator, the workhorse
    regression estimator of the social sciences, targets this quantity of interest
    in large samples, regardless of whether the true CEF is linear or not. Thus, even
    when a linear CEF assumption is incorrect and the projection errors are not normally
    distributed, OLS still targets a perfectly valid quantity of interest: the coefficients
    from this population linear projection.**  **## 5.3 Linear CEFs without assumptions'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这很重要？普通最小二乘估计量，社会科学中主要的回归估计量，在大样本中对这个感兴趣的数量进行估计，无论真实的CEF是否线性。因此，即使线性CEF假设不正确，投影误差不是正态分布的，OLS仍然针对一个完全有效的感兴趣数量：从这一总体线性投影中得到的系数。**  **
- en: 'What is the relationship between the best linear predictor (which we just saw
    generally exists) and the CEF? To draw the connection, remember that the conditional
    expectation is importantly the function of \(\X_i\) that best predicts \(Y_{i}\).
    The population regression was the best **linear** predictor, but the CEF is the
    best predictor among all nicely behaved functions of \(\X_{i}\), linear or nonlinear.
    In particular, if we label \(L_2\) to be the set of all functions of the covariates
    \(g()\) that have finite squared expectation, \(\E[g(\X_{i})^{2}] < \infty\),
    then we can show that the CEF has the lowest squared prediction error in this
    class of functions: \[ \mu(\X) = \E[Y_{i} \mid \X_{i}] = \argmin_{g(\X_i) \in
    L_2}\; \E\left[(Y_{i} - g(\X_{i}))^{2}\right], \]'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳线性预测器（我们刚刚看到它普遍存在）与CEF之间的关系是什么？为了建立联系，请记住条件期望是\(\X_i\)的函数，它能最好地预测\(Y_{i}\)。总体回归是最好的**线性**预测器，但CEF是在所有表现良好的\(\X_{i}\)函数中（无论是线性的还是非线性的）的最佳预测器。特别是，如果我们把\(L_2\)定义为所有协变量函数\(g()\)的集合，这些函数具有有限的平方期望，即\(\E[g(\X_{i})^{2}]
    < \infty\)，那么我们可以证明CEF在这一类函数中具有最低的平方预测误差：\[ \mu(\X) = \E[Y_{i} \mid \X_{i}] =
    \argmin_{g(\X_i) \in L_2}\; \E\left[(Y_{i} - g(\X_{i}))^{2}\right], \]
- en: So we have established that the CEF is the best predictor and the population
    linear regression \(m(\X_{i})\) is the best linear predictor. These two facts
    allow us to connect the CEF to the population regression.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经确定CEF是最好的预测器，总体线性回归\(m(\X_{i})\)是最好的线性预测器。这两个事实使我们能够将CEF与总体回归联系起来。
- en: '**Theorem 5.1** If \(\mu(\X_{i})\) is a linear function of \(\X_i\), then \(\mu(\X_{i})
    = m(\X_{i}) = \X_i''\bfbeta\).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理5.1** 如果\(\mu(\X_{i})\)是\(\X_i\)的线性函数，那么\(\mu(\X_{i}) = m(\X_{i}) = \X_i''\bfbeta\)。'
- en: 'This theorem says that if the true CEF is linear, it must equal the population
    linear regression. The proof of this is straightforward: the CEF is the best predictor,
    so if it is linear, it must also be the best linear predictor.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定理表明，如果真实的CEF是线性的，它必须等于总体线性回归。这个证明是直接的：CEF是最好的预测器，所以如果它是线性的，它也必须是最好的线性预测器。
- en: 'In general, we are in the business of learning about the CEF, so we are unlikely
    to know if it genuinely is linear or not. In some situations, however, we can
    show that the CEF is linear without any additional assumptions. These are situations
    where the covariates take on a finite number of possible values. Going back to
    the example from the chapter introduction, suppose we are interested in the CEF
    of wait times at voting precincts for Black (\(X_i = 1\)) vs. non-Black (\(X_i
    = 0\)) voters. In this case, there are two possible values of the CEF, \(\mu(1)
    = \E[Y_{i}\mid X_{i}= 1]\), the average wait time for Black voters, and \(\mu(0)
    = \E[Y_{i}\mid X_{i} = 0]\), the average wait time for non-Black voters. Notice
    that we can write the CEF as \[ \mu(x) = x \mu(1) + (1 - x) \mu(0) = \mu(0) +
    x\left(\mu(1) - \mu(0)\right)= \beta_0 + x\beta_1, \] which is clearly a linear
    function of \(x\). Based on this derivation, we obtain coefficients of this linear
    CEF that have clear substantive interpretations:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们是在学习CEF，所以我们不太可能知道它是否真正是线性的。然而，在某些情况下，我们可以证明CEF是线性的，而无需任何额外的假设。这些情况是协变量具有有限个可能值的情况。回到章节引言中的例子，假设我们感兴趣的是黑人（\(X_i
    = 1\)）和非黑人（\(X_i = 0\)）选民在投票点的等待时间的CEF。在这种情况下，CEF有两个可能的值，\(\mu(1) = \E[Y_{i}\mid
    X_{i}= 1]\)，黑人选民的平均等待时间，和\(\mu(0) = \E[Y_{i}\mid X_{i} = 0]\)，非黑人选民的平均等待时间。请注意，我们可以将CEF写成\[
    \mu(x) = x \mu(1) + (1 - x) \mu(0) = \mu(0) + x\left(\mu(1) - \mu(0)\right)= \beta_0
    + x\beta_1, \]这显然是\(x\)的线性函数。基于这个推导，我们得到这个线性CEF的系数，它们具有明确的实质性解释：
- en: '\(\beta_0 = \mu(0)\): the expected wait time for a Black voter.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\beta_0 = \mu(0)\)：黑人选民的平均等待时间。
- en: '\(\beta_1 = \mu(1) - \mu(0)\): the difference in average wait times between
    Black and non-Black voters. How \(X_{i}\) is defined here is important since the
    intercept will always be the average outcome when \(X_i = 0\), and the slope will
    always be the difference in means between the \(X_i = 1\) group and the \(X_i
    = 0\) group.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\beta_1 = \mu(1) - \mu(0)\)：黑人和非黑人选民之间平均等待时间的差异。在这里如何定义\(X_{i}\)很重要，因为截距将始终是当\(X_i
    = 0\)时的平均结果，而斜率将始终是\(X_i = 1\)组和\(X_i = 0\)组之间均值的差异。
- en: What about a categorical covariate with more than two levels? For example, we
    may be interested in wait times by party identification, where \(X_i = 1\) indicates
    Democratic voters, \(X_i = 2\) indicates Republican voters, and \(X_i = 3\) indicates
    Independent voters. We could write the CEF of wait times as a linear function
    of this variable, but that would assume that the difference between Democrats
    and Republicans is exactly the same as for Independents and Republicans – which
    is probably false. With more than two levels, we can represent a categorical variable
    as a vector of binary variables, \(\X_i = (X_{i1}, X_{i2})\), where \[ \begin{aligned}
    X_{{i1}} &= \begin{cases} 1&\text{if Republican} \\ 0 & \text{if not Republican}
    \end{cases} \\ X_{{i2}} &= \begin{cases} 1&\text{if independent} \\ 0 & \text{if
    not independent} \end{cases} \\ \end{aligned} \] These two indicator variables
    encode the same information as the original single three-level variable, \(X_{i}\),
    so if we know the values of \(X_{i1}\) and \(X_{i2}\), then we know exactly to
    which party \(i\) belongs. Thus, the CEFs for \(X_i\) and the pair of indicator
    variables, \(\X_i\), are precisely the same, but the latter allows for a lovely
    linear representation, \[ \E[Y_i \mid X_{i1}, X_{i2}] = \beta_0 + \beta_1 X_{i1}
    + \beta_2 X_{i2}, \] where
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 那么对于具有超过两个水平的分类协变量呢？例如，我们可能对按政党身份划分的等待时间感兴趣，其中 \(X_i = 1\) 表示民主党选民，\(X_i = 2\)
    表示共和党选民，而 \(X_i = 3\) 表示无党派选民。我们可以将等待时间的CEF表示为该变量的线性函数，但这将假设民主党与共和党之间的差异与无党派与共和党之间的差异完全相同——这可能是错误的。当有超过两个水平时，我们可以将分类变量表示为二进制变量的向量，\(\X_i
    = (X_{i1}, X_{i2})\)，其中 \[ \begin{aligned} X_{{i1}} &= \begin{cases} 1&\text{如果为共和党}
    \\ 0 & \text{如果不是共和党} \end{cases} \\ X_{{i2}} &= \begin{cases} 1&\text{如果为无党派}
    \\ 0 & \text{如果不是无党派} \end{cases} \\ \end{aligned} \] 这两个指标变量编码了与原始单三个水平变量 \(X_{i}\)
    相同的信息，因此如果我们知道 \(X_{i1}\) 和 \(X_{i2}\) 的值，那么我们就知道 \(i\) 属于哪个政党。因此，\(X_i\) 和指标变量对
    \(\X_i\) 的CEF是精确相同的，但后者允许有一个漂亮的线性表示，\[ \E[Y_i \mid X_{i1}, X_{i2}] = \beta_0 +
    \beta_1 X_{i1} + \beta_2 X_{i2}, \] 其中
- en: \(\beta_0 = \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 0]\) is the average wait time
    for the group who does not get an indicator variable (Democrats in this case).
    This group is sometimes called the baseline group or the omitted group.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\beta_0 = \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 0]\) 是没有获得指标变量（在这种情况下为民主党）的群体的平均等待时间。这个群体有时被称为基准组或省略组。
- en: \(\beta_1 = \E[Y_{i} \mid X_{i1} = 1, X_{i2} = 0] - \E[Y_{i} \mid X_{i1} = 0,
    X_{i2} = 0]\) is the difference in means between Republican voters and Democratic
    voters, or the difference between the first indicator group and the baseline group.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\beta_1 = \E[Y_{i} \mid X_{i1} = 1, X_{i2} = 0] - \E[Y_{i} \mid X_{i1} = 0,
    X_{i2} = 0]\) 表示共和党选民和民主党选民之间的均值差异，或者第一指标组和基准组之间的差异。
- en: \(\beta_2 = \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 1] - \E[Y_{i} \mid X_{i1} = 0,
    X_{i2} = 0]\) is the difference in means between independent voters and Democratic
    voters, or the difference between the second indicator group and the baseline
    group.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\beta_2 = \E[Y_{i} \mid X_{i1} = 0, X_{i2} = 1] - \E[Y_{i} \mid X_{i1} = 0,
    X_{i2} = 0]\) 表示无党派选民和民主党选民之间的均值差异，或者第二指标组和基准组之间的差异。
- en: This approach easily generalizes to categorical variables with an arbitrary
    number of levels.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以轻松推广到具有任意数量水平的分类变量。
- en: 'What have we shown? The CEF is linear without additional assumptions when there
    is a categorical covariate. We can show that this continues to hold even when
    we have multiple categorical variables. We now have two binary covariates: \(X_{i1}=1\)
    indicating a Black voter, and \(X_{i2} = 1\) indicating a retired voter versus
    a working-age voter. These two binary variables give us four possible values of
    the CEF: \[ \mu(x_1, x_2) = \begin{cases} \mu_{00} & \text{if } x_1 = 0 \text{
    and } x_2 = 0 \text{ (non-Black, working age)} \\ \mu_{10} & \text{if } x_1 =
    1 \text{ and } x_2 = 0 \text{ (Black, working age)} \\ \mu_{01} & \text{if } x_1
    = 0 \text{ and } x_2 = 1 \text{ (non-Black, retired)} \\ \mu_{11} & \text{if }
    x_1 = 1 \text{ and } x_2 = 1 \text{ (Black, retired)} \end{cases} \] We can write
    this as \[ \mu(x_{1}, x_{2}) = (1 - x_{1})(1 - x_{2})\mu_{00} + x_{1}(1 -x_{2})\mu_{10}
    + (1-x_{1})x_{2}\mu_{01} + x_{1}x_{2}\mu_{11}, \] which we can rewrite as \[ \mu(x_1,
    x_2) = \beta_0 + x_1\beta_1 + x_2\beta_2 + x_1x_2\beta_3, \] where the substantive
    interpretations are'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了什么？当存在分类协变量时，CEF在没有额外假设的情况下是线性的。我们可以证明，即使我们有多个分类变量，这也仍然成立。我们现在有两个二元协变量：\(X_{i1}=1\)
    表示黑人选民，\(X_{i2} = 1\) 表示退休选民与工作年龄选民。这两个二元变量给出了CEF的四个可能值：\[ \mu(x_1, x_2) = \begin{cases}
    \mu_{00} & \text{if } x_1 = 0 \text{ and } x_2 = 0 \text{ (非黑人，工作年龄)} \\ \mu_{10}
    & \text{if } x_1 = 1 \text{ and } x_2 = 0 \text{ (黑人，工作年龄)} \\ \mu_{01} & \text{if
    } x_1 = 0 \text{ and } x_2 = 1 \text{ (非黑人，退休)} \\ \mu_{11} & \text{if } x_1 =
    1 \text{ and } x_2 = 1 \text{ (黑人，退休)} \end{cases} \] 我们可以将其写为 \[ \mu(x_{1}, x_{2})
    = (1 - x_{1})(1 - x_{2})\mu_{00} + x_{1}(1 -x_{2})\mu_{10} + (1-x_{1})x_{2}\mu_{01}
    + x_{1}x_{2}\mu_{11}, \] 我们可以将其重写为 \[ \mu(x_1, x_2) = \beta_0 + x_1\beta_1 + x_2\beta_2
    + x_1x_2\beta_3, \] 其中实质性解释是
- en: '\(\beta_0 = \mu_{00}\): average wait times for working-age non-Black voters.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_0 = \mu_{00}\): 工作年龄非黑人选民的平均等待时间。'
- en: '\(\beta_1 = \mu_{10} - \mu_{00}\): difference in means for working-age Black
    vs. working-age non-Black voters.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_1 = \mu_{10} - \mu_{00}\): 工作年龄黑人选民与工作年龄非黑人选民在均值上的差异。'
- en: '\(\beta_2 = \mu_{01} - \mu_{00}\): difference in means for retired non-Black
    vs. working-age non-Black voters.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_2 = \mu_{01} - \mu_{00}\): 退休的非黑人选民与工作年龄的非黑人选民在均值上的差异。'
- en: '\(\beta_3 = (\mu_{11} - \mu_{01}) - (\mu_{10} - \mu_{00})\): difference in
    retired racial difference vs working-age racial difference.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_3 = (\mu_{11} - \mu_{01}) - (\mu_{10} - \mu_{00})\): 退休种族差异与工作年龄种族差异的差异。'
- en: Thus, we can write the CEF with two binary covariates as linear when the linear
    specification includes a multiplicative interaction between them (\(x_1x_2\)).
    This result holds for all pairs of binary covariates, and we can generalize the
    interpretation of the coefficients in the CEF as
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当线性规范包括它们之间的乘法交互作用（\(x_1x_2\)）时，我们可以将包含两个二元协变量的CEF写成线性形式。这个结果适用于所有二元协变量的所有配对，并且我们可以将CEF中系数的解释推广为
- en: '\(\beta_0 = \mu_{00}\): average outcome when both variables are 0.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_0 = \mu_{00}\): 当两个变量都为0时的平均结果。'
- en: '\(\beta_1 = \mu_{10} - \mu_{00}\): difference in average outcomes for the first
    covariate when the second covariate is 0.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_1 = \mu_{10} - \mu_{00}\): 当第二个协变量为0时，第一个协变量的平均结果差异。'
- en: '\(\beta_2 = \mu_{01} - \mu_{00}\): difference in average outcomes for the second
    covariate when the first covariate is 0.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_2 = \mu_{01} - \mu_{00}\): 当第一个协变量为0时，第二个协变量的平均结果差异。'
- en: '\(\beta_3 = (\mu_{11} - \mu_{01}) - (\mu_{10} - \mu_{00})\): change in the
    “effect” of the first (second) covariate when the second (first) covariate goes
    from 0 to 1.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_3 = (\mu_{11} - \mu_{01}) - (\mu_{10} - \mu_{00})\): 当第二个（第一个）协变量从0变为1时，第一个（第二个）协变量的“效应”变化。'
- en: This result also generalizes to an arbitrary number of binary covariates. If
    we have \(p\) binary covariates, then the CEF will be linear with all two-way
    interactions, \(x_1x_2\), all three-way interactions, \(x_1x_2x_3\), up to the
    \(p\)-way interaction \(x_1\times\cdots\times x_p\). Furthermore, we can generalize
    to arbitrary numbers of categorical variables by expanding each into a series
    of binary variables and then including all interactions between the resulting
    binary variables.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果也推广到任意数量的二元协变量。如果我们有 \(p\) 个二元协变量，那么CEF将是线性的，包括所有双向交互作用，\(x_1x_2\)，所有三向交互作用，\(x_1x_2x_3\)，直到
    \(p\) 向交互作用 \(x_1\times\cdots\times x_p\)。此外，我们可以通过将每个变量扩展成一系列二元变量，然后包括这些二元变量之间的所有交互作用，推广到任意数量的分类变量。
- en: We have established that when we have a set of categorical covariates, the true
    CEF will be linear, and we have seen the various ways to represent that CEF. Note
    that when we use, for example, ordinary least squares, we are free to choose how
    to include our variables. We could run a regression of \(Y_i\) on \(X_{i1}\) and
    \(X_{i2}\) without an interaction term, but this model will only be correct if
    \(\beta_3\) is equal to 0, and so the interaction term is irrelevant. We call
    a model **saturated** if there are as many coefficients as the CEF’s unique values.
    A saturated model can, by its nature, always be written as a linear function without
    assumptions. The above examples show how to construct saturated models in various
    situations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定，当我们有一组分类协变量时，真实的CEF将是线性的，我们也看到了表示该CEF的各种方法。请注意，当我们使用，例如，普通最小二乘法时，我们可以自由选择如何包含我们的变量。我们可以对
    \(Y_i\) 在 \(X_{i1}\) 和 \(X_{i2}\) 上进行回归，而不包含交互项，但这个模型只有在 \(\beta_3\) 等于 0 时才是正确的，因此交互项是不相关的。如果一个模型中的系数数量与CEF的唯一值一样多，我们称其为**饱和模型**。一个饱和模型本质上总是可以写成没有假设的线性函数。上述例子展示了如何在各种情况下构建饱和模型。
- en: 5.4 Interpretation of the regression coefficients
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 回归系数的解释
- en: We have seen how to interpret population regression coefficients when the CEF
    is linear without assumptions. How do we interpret the population coefficients
    \(\bfbeta\) in other settings?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，当CEF线性且没有假设时，如何解释总体回归系数。在其他情况下，我们如何解释总体系数 \(\bfbeta\)？
- en: Consider the simplest case, one in which every entry in \(\X_{i}\) represents
    a different covariate and no covariate is any function of another (we will see
    why this caveat is necessary below). In this simple case, the \(k\)th coefficient,
    \(\beta_{k}\), represents the change in the predicted outcome for a one-unit change
    in the \(k\)th covariate \(X_{ik}\), holding all other covariates fixed. We can
    see this from \[ \begin{aligned} m(x_{1} + 1, x_{2}) & = \beta_{0} + \beta_{1}(x_{1}
    + 1) + \beta_{2}x_{2} \\ m(x_{1}, x_{2}) &= \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2},
    \end{aligned} \] so that the change in the predicted outcome for increasing \(X_{i1}\)
    by one unit is \[ m(x_{1} + 1, x_{2}) - m(x_{1}, x_{2}) = \beta_1 \] Notice that
    nothing changes in this interpretation when adding more covariates to the vector,
    \[ m(x_{1} + 1, \bfx_{2}) - m(x_{1}, \bfx_{2}) = \beta_1, \] The coefficient on
    a particular variable is the change in the predicted outcome corresponding to
    a one-unit change in the covariate holding all other covariates constant. Each
    coefficient summarizes the “all else equal” difference in the predicted outcome
    for each covariate.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑最简单的情况，其中 \(\X_{i}\) 中的每个条目代表一个不同的协变量，并且没有任何协变量是另一个协变量的函数（我们将在下面看到为什么这个注意事项是必要的）。在这种情况下，第
    \(k\) 个系数 \(\beta_{k}\) 代表在保持所有其他协变量不变的情况下，第 \(k\) 个协变量 \(X_{ik}\) 变化一个单位时预测结果的改变。我们可以从以下公式中看出这一点：\[
    \begin{aligned} m(x_{1} + 1, x_{2}) & = \beta_{0} + \beta_{1}(x_{1} + 1) + \beta_{2}x_{2}
    \\ m(x_{1}, x_{2}) &= \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2}, \end{aligned}
    \] 因此，当 \(X_{i1}\) 增加 1 个单位时，预测结果的改变为 \[ m(x_{1} + 1, x_{2}) - m(x_{1}, x_{2})
    = \beta_1 \] 注意，当向向量中添加更多协变量时，这种解释不会发生变化，\[ m(x_{1} + 1, \bfx_{2}) - m(x_{1},
    \bfx_{2}) = \beta_1, \] 特定变量的系数是在保持所有其他协变量不变的情况下，对应协变量变化一个单位时预测结果的改变。每个系数总结了每个协变量在预测结果中的“其他条件相同”的差异。
- en: 5.4.1 Polynomial functions of the covariates
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 协变量的多项式函数
- en: 'The interpretation of the population regression coefficients becomes more complicated
    when including nonlinear functions of the covariates. In that case, multiple coefficients
    control how a change in a covariate will change the predicted value of \(Y_i\).
    For example, suppose we have a quadratic function of \(X_{i1}\), \[ m(x_1, x_1^2,
    x_{2}) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{1}^{2} + \beta_{3}x_{2}, \]
    and try to look at a one-unit change in \(x_1\), \[ \begin{aligned} m(x_{1} +
    1, (x_{1} + 1)^{2}, x_{2}) & = \beta_{0} + \beta_{1}(x_{1} + 1) + \beta_{2}(x_{1}
    + 1)^{2}+ \beta_{3}x_{2} \\ m(x_{1}, x_{1}^{2}, x_{2}) &= \beta_{0} + \beta_{1}x_{1}
    + \beta_{2}x_{1}^{2} + \beta_{3}x_{2}, \end{aligned} \] resulting in \(\beta_1
    + \beta_2(2x_{1} + 1)\). This formula might be an interesting quantity, but we
    more commonly use the derivative of \(m(\bfx)\) with respect to \(x_1\) as a measure
    of the marginal effect of \(X_{i1}\) on the predicted value of \(Y_i\) (holding
    all other variables constant), where “marginal” here means the change in prediction
    for a very small change in \(X_{i1}\).[¹](#fn1) In the case of the quadratic covariate,
    we have \[ \frac{\partial m(x_{1}, x_{1}^{2}, x_{2})}{\partial x_{1}} = \beta_{1}
    + 2\beta_{2}x_{1}, \] so the marginal effect on prediction varies as a function
    of \(x_1\). From this, we see that the individual interpretations of the coefficients
    are less interesting: \(\beta_1\) is the marginal effect when \(X_{i1} = 0\) and
    \(\beta_2 / 2\) describes how a one-unit change in \(X_{i1}\) changes the marginal
    effect. As is hopefully clear, it will often be more straightforward to visualize
    the nonlinear predictor function (perhaps using the orthogonalization techniques
    in [Section 5.5](#sec-fwl)).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 当包括协变量的非线性函数时，人口回归系数的解释变得更加复杂。在这种情况下，多个系数控制协变量变化如何影响\(Y_i\)的预测值。例如，假设我们有一个关于\(X_{i1}\)的二次函数，\[
    m(x_1, x_1^2, x_{2}) = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{1}^{2} + \beta_{3}x_{2},
    \]并尝试观察\(x_1\)的一个单位变化，\[ \begin{aligned} m(x_{1} + 1, (x_{1} + 1)^{2}, x_{2})
    & = \beta_{0} + \beta_{1}(x_{1} + 1) + \beta_{2}(x_{1} + 1)^{2}+ \beta_{3}x_{2}
    \\ m(x_{1}, x_{1}^{2}, x_{2}) &= \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{1}^{2}
    + \beta_{3}x_{2}, \end{aligned} \]结果得到\(\beta_1 + \beta_2(2x_{1} + 1)\)。这个公式可能是一个有趣的量，但我们更常用\(m(\bfx)\)关于\(x_1\)的导数作为\(X_{i1}\)对\(Y_i\)预测值的边际效应的度量（保持所有其他变量不变），这里的“边际”意味着\(X_{i1}\)发生非常小的变化时预测的变化。[¹](#fn1)在二次协变量的情况下，我们有
    \[ \frac{\partial m(x_{1}, x_{1}^{2}, x_{2})}{\partial x_{1}} = \beta_{1} + 2\beta_{2}x_{1},
    \]因此预测的边际效应随\(x_1\)的变化而变化。从这一点我们可以看出，系数的个体解释不太有趣：\(\beta_1\)是当\(X_{i1} = 0\)时的边际效应，而\(\beta_2
    / 2\)描述了\(X_{i1}\)的一个单位变化如何改变边际效应。如希望所清楚，可视化非线性预测函数（可能使用[第5.5节](#sec-fwl)中的正交化技术）通常会更为直接。
- en: 5.4.2 Interactions
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 交互
- en: 'Another common nonlinear function occurs when including **interaction terms**
    or covariates that are products of two other covariates, \[ m(x_{1}, x_{2}, x_{1}x_{2})
    = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{1}x_{2}. \] In these
    situations, we can use the derivative of the BLP to measure the marginal effect
    of one variable or the other on the predicted value of \(Y_i\). In particular,
    we have \[ \begin{aligned} \frac{\partial m(x_{1}, x_{2}, x_{1}x_{2})}{\partial
    x_1} &= \beta_1 + \beta_3x_2, \\ \frac{\partial m(x_{1}, x_{2}, x_{1}x_{2})}{\partial
    x_2} &= \beta_2 + \beta_3x_1. \end{aligned} \] Here, the coefficients are slightly
    more interpretable:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的非线性函数出现在包含**交互项**或协变量时，这些协变量是另外两个协变量的乘积，\[ m(x_{1}, x_{2}, x_{1}x_{2})
    = \beta_{0} + \beta_{1}x_{1} + \beta_{2}x_{2} + \beta_{3}x_{1}x_{2}. \]在这些情况下，我们可以使用BLP的导数来衡量一个变量或另一个变量对预测值\(Y_i\)的边际效应。特别是，我们有
    \[ \begin{aligned} \frac{\partial m(x_{1}, x_{2}, x_{1}x_{2})}{\partial x_1} &=
    \beta_1 + \beta_3x_2, \\ \frac{\partial m(x_{1}, x_{2}, x_{1}x_{2})}{\partial
    x_2} &= \beta_2 + \beta_3x_1. \end{aligned} \]在这里，系数的解释稍微容易一些：
- en: '\(\beta_1\): the marginal effect of \(X_{i1}\) on predicted \(Y_i\) when \(X_{i2}
    = 0\).'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_1\): 当\(X_{i2} = 0\)时，\(X_{i1}\)对预测\(Y_i\)的边际效应。'
- en: '\(\beta_2\): the marginal effect of \(X_{i2}\) on predicted \(Y_i\) when \(X_{i1}
    = 0\).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_2\): 当\(X_{i1} = 0\)时，\(X_{i2}\)对预测\(Y_i\)的边际效应。'
- en: '\(\beta_3\): the change in the marginal effect of \(X_{i1}\) due to a one-unit
    change in \(X_{i2}\) **OR** the change in the marginal effect of \(X_{i2}\) due
    to a one-unit change in \(X_{i1}\).'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '\(\beta_3\): 由于\(X_{i2}\)增加一个单位而导致的\(X_{i1}\)的边际效应的变化**或者**由于\(X_{i1}\)增加一个单位而导致的\(X_{i2}\)的边际效应的变化。'
- en: If we add more covariates to this BLP, these interpretations change to “holding
    all other covariates constant.”
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们向这个BLP添加更多协变量，这些解释将变为“保持所有其他协变量不变”。
- en: Interactions are a standard part of social science research because they allow
    us to assess how the relationship between the outcome and an independent variable
    varies by the values of another variable. In the context of our study of wait
    times at a voting precinct, if \(X_{i1}\) is income and \(X_{i2}\) is the Black/non-Black
    voter indicator, then \(\beta_3\) represents the change in the slope of the wait
    time-income relationship between Black and non-Black voters.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 交互作用是社会科学研究中的一个标准部分，因为它允许我们评估结果与自变量之间的关系如何随着另一个变量的值而变化。在我们对投票站等待时间的研究中，如果 \(X_{i1}\)
    是收入，\(X_{i2}\) 是黑人/非黑人选民指标，那么 \(\beta_3\) 代表了黑人和非黑人选民之间等待时间-收入关系斜率的变化。
- en: '*Centering variables to improve interpretability* *In many cases, the so-called
    marginal coefficients on the lower-order terms (\(\beta_1\) for \(X_{i1}\) and
    \(\beta_2\) for \(X_{i2}\)) are uninteresting because they represent the marginal
    effect of one variable when the other is 0\. If \(X_{i1}\) is age and \(X_{i2}\)
    is the Black/non-Black indicator, then \(\beta_2\) is the estimated difference
    in average voter wait times for voters who are zero years old, an obviously nonsensical
    parameter. We can improve the interpretability of the coefficient by recentering
    the age variable. Suppose we include a mean-centered version of age, \[ \widetilde{X}_{i1}
    = X_{i1} - \overline{X}_1 \] in place of \(X_{i1}\). That is, we regress \(Y_i\)
    on \(\widetilde{X}_{i1}\), \(X_{i2}\), and \(\widetilde{X}_{i1}X_{i2}\). In this
    case, \(\beta_2\) (the coefficient on the race indicator \(X_{i2}\)) is the marginal
    effect of \(X_{i2}\) when \(\widetilde{X}_{i1} = 0\) or when \(X_{i1} = \overline{X}_1\).
    Thus, this coefficient is now the estimated difference in average voter wait times
    for the average-aged voter, which is far more interpretable. This recentering
    has no effect on either \(\beta_1\) or \(\beta_3\), which is rather remarkable.*  *##
    5.5 Multiple regression from bivariate regression'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*中心化变量以提高可解释性* *在许多情况下，低阶项（如 \(X_{i1}\) 的 \(\beta_1\) 和 \(X_{i2}\) 的 \(\beta_2\)）的边际系数可能没有意义，因为它们代表了一个变量在另一个变量为0时的边际效应。如果
    \(X_{i1}\) 是年龄，\(X_{i2}\) 是黑人/非黑人指标，那么 \(\beta_2\) 就是估计的平均选民等待时间的差异，对于一个年龄为零年的选民来说，这是一个显然没有意义的参数。我们可以通过重新中心化年龄变量来提高系数的可解释性。假设我们用年龄的均值中心化版本
    \[ \widetilde{X}_{i1} = X_{i1} - \overline{X}_1 \] 来代替 \(X_{i1}\)。也就是说，我们将 \(Y_i\)
    对 \(\widetilde{X}_{i1}\)，\(X_{i2}\) 和 \(\widetilde{X}_{i1}X_{i2}\) 进行回归。在这种情况下，\(\beta_2\)（种族指标的系数）是当
    \(\widetilde{X}_{i1} = 0\) 或 \(X_{i1} = \overline{X}_1\) 时 \(X_{i2}\) 的边际效应。因此，这个系数现在是估计的平均年龄选民的平均选民等待时间的差异，这要远比原来的参数更有意义。这种重新中心化对
    \(\beta_1\) 或 \(\beta_3\) 没有影响，这是相当引人注目的。*  *## 5.5 从二元回归到多元回归'
- en: 'With a regression of an outcome on two covariates, understanding how the coefficients
    of one variable relate to the other is helpful. Consider the following best linear
    projection: \[ (\alpha, \beta, \gamma) = \argmin_{(a,b,c) \in \mathbb{R}^{3}}
    \; \E[(Y_{i} - (a + bX_{i} + cZ_{i}))^{2}] \tag{5.1}\] Can we understand the \(\beta\)
    coefficient here in terms of a bivariate regression? As it turns out, yes. From
    the above results, we know that the intercept has a simple form: \[ \alpha = \E[Y_i]
    - \beta\E[X_i] - \gamma\E[Z_i]. \] Let’s investigate the first order condition
    for \(\beta\): \[ \begin{aligned} 0 &= \E[Y_{i}X_{i}] - \alpha\E[X_{i}] - \beta\E[X_{i}^{2}]
    - \gamma\E[X_{i}Z_{i}] \\ &= \E[Y_{i}X_{i}] - \E[Y_{i}]\E[X_{i}] + \beta\E[X_{i}]^{2}
    + \gamma\E[X_{i}]\E[Z_{i}] - \beta\E[X_{i}^{2}] - \gamma\E[X_{i}Z_{i}] \\ &= \cov(Y,
    X) - \beta\V[X_{i}] - \gamma \cov(X_{i}, Z_{i}) \end{aligned} \] We can see from
    this that if \(\cov(X_{i}, Z_{i}) = 0\), then the coefficient on \(X_i\) will
    be the same as in the simple regression case, \(\cov(Y_{i}, X_{i})/\V[X_{i}]\).
    When \(X_i\) and \(Z_i\) are uncorrelated, we sometimes call them **orthogonal**.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在对两个协变量进行结果回归的情况下，理解一个变量的系数如何与另一个变量相关是有帮助的。考虑以下最佳线性投影：\[ (\alpha, \beta, \gamma)
    = \argmin_{(a,b,c) \in \mathbb{R}^{3}} \; \E[(Y_{i} - (a + bX_{i} + cZ_{i}))^{2}]
    \tag{5.1}\] 我们能否从二元回归的角度来理解这里的 \(\beta\) 系数？结果证明，可以。从上述结果中，我们知道截距具有简单的形式：\[ \alpha
    = \E[Y_i] - \beta\E[X_i] - \gamma\E[Z_i]. \] 让我们来研究 \(\beta\) 的一阶条件：\[ \begin{aligned}
    0 &= \E[Y_{i}X_{i}] - \alpha\E[X_{i}] - \beta\E[X_{i}^{2}] - \gamma\E[X_{i}Z_{i}]
    \\ &= \E[Y_{i}X_{i}] - \E[Y_{i}]\E[X_{i}] + \beta\E[X_{i}]^{2} + \gamma\E[X_{i}]\E[Z_{i}]
    - \beta\E[X_{i}^{2}] - \gamma\E[X_{i}Z_{i}] \\ &= \cov(Y, X) - \beta\V[X_{i}]
    - \gamma \cov(X_{i}, Z_{i}) \end{aligned} \] 从这个公式中我们可以看出，如果 \(\cov(X_{i}, Z_{i})
    = 0\)，那么 \(X_i\) 的系数将与简单回归情况相同，即 \(\cov(Y_{i}, X_{i})/\V[X_{i}]\)。当 \(X_i\) 和
    \(Z_i\) 不相关时，我们有时称它们为**正交**。
- en: 'To write a simple formula for \(\beta\) when the covariates are not orthogonal,
    we **orthogonalize** \(X_i\) by obtaining the prediction errors from a population
    linear regression of \(X_i\) on \(Z_i\): \[ \widetilde{X}_{i} = X_{i} - (\delta_{0}
    + \delta_{1}Z_{i}) \quad\text{where}\quad (\delta_{0}, \delta_{1}) = \argmin_{(d_{0},d_{1})
    \in \mathbb{R}^{2}} \; \E[(X_{i} - (d_{0} + d_{1}Z_{i}))^{2}] \] Given the properties
    of projection errors, we know that this orthogonalized version of \(X_{i}\) will
    be uncorrelated with \(Z_{i}\) since \(\E[\widetilde{X}_{i}Z_{i}] = 0\). Remarkably,
    the coefficient on \(X_i\) from the “long” BLP in [Equation 5.1](#eq-two-var-blp)
    is the same as the regression of \(Y_i\) on this orthogonalized \(\widetilde{X}_i\),
    \[ \beta = \frac{\text{cov}(Y_{i}, \widetilde{X}_{i})}{\V[\widetilde{X}_{i}]}
    \]'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 当协变量不正交时，为了为 \(\beta\) 写一个简单的公式，我们需要通过从 \(X_i\) 在 \(Z_i\) 上的群体线性回归中获得预测误差来**正交化**
    \(X_i\)：\[ \widetilde{X}_{i} = X_{i} - (\delta_{0} + \delta_{1}Z_{i}) \quad\text{其中}\quad
    (\delta_{0}, \delta_{1}) = \argmin_{(d_{0},d_{1}) \in \mathbb{R}^{2}} \; \E[(X_{i}
    - (d_{0} + d_{1}Z_{i}))^{2}] \]鉴于投影误差的性质，我们知道这个正交化的 \(X_i\) 版本将与 \(Z_i\) 不相关，因为
    \(\E[\widetilde{X}_{i}Z_{i}] = 0\)。值得注意的是，[方程 5.1](#eq-two-var-blp) 中的“长”BLP 在
    \(X_i\) 上的系数与 \(Y_i\) 对这个正交化的 \(\widetilde{X}_i\) 的回归相同，\[ \beta = \frac{\text{cov}(Y_{i},
    \widetilde{X}_{i})}{\V[\widetilde{X}_{i}]} \]
- en: We can expand this idea to when there are several other covariates. Suppose
    now that we are interested in a regression of \(Y_i\) on \(\X_i\) and we are interested
    in the coefficient on the \(k\)th covariate. Let \(\X_{i,-k}\) be the vector of
    covariates omitting the \(k\)th entry and let \(m_k(\X_{i,-k})\) represent the
    BLP of \(X_{ik}\) on these other covariates. We can define \(\widetilde{X}_{ik}
    = X_{ik} - m_{k}(\X_{i,-k})\) as the \(k\)th variable orthogonalized with respect
    to the rest of the variables and we can write the coefficient on \(X_{ik}\) as
    \[ \beta_k = \frac{\cov(Y_i, \widetilde{X}_{ik})}{\V[\widetilde{X}_{ik}]}. \]
    Thus, the population regression coefficient in the BLP is the same as from a bivariate
    regression of the outcome on the projection error for \(X_{ik}\) projected on
    all other covariates. One interpretation of coefficients in a population multiple
    regression is that they represent the relationship between the outcome and the
    covariate after removing the linear relationships of all other variables.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这个想法扩展到存在多个其他协变量的情况。假设我们现在对 \(Y_i\) 在 \(\X_i\) 上的回归感兴趣，并且我们感兴趣的是第 \(k\)
    个协变量的系数。令 \(\X_{i,-k}\) 为省略第 \(k\) 个条目的协变量向量，并让 \(m_k(\X_{i,-k})\) 表示 \(X_{ik}\)
    在这些其他协变量上的BLP。我们可以定义 \(\widetilde{X}_{ik} = X_{ik} - m_{k}(\X_{i,-k})\) 作为相对于其他变量的第
    \(k\) 个变量正交化，并且我们可以将 \(X_{ik}\) 上的系数写为 \[ \beta_k = \frac{\cov(Y_i, \widetilde{X}_{ik})}{\V[\widetilde{X}_{ik}]}.
    \] 因此，BLP中的群体回归系数与对 \(X_{ik}\) 在所有其他协变量上的投影误差进行二元回归的系数相同。群体多重回归中系数的一种解释是，它们代表在移除所有其他变量的线性关系后，结果与协变量之间的关系。
- en: 5.6 Omitted variable bias
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 漏掉变量偏差
- en: In many situations, we may need to choose whether to include a variable in a
    regression, so it can be helpful to understand how this choice might affect the
    population coefficients on the other variables in the regression. Suppose we have
    a variable \(Z_i\) that we may add to our regression, which currently has \(\X_i\)
    as the covariates. We can write this new projection as \[ m(\X_i, Z_i) = \X_i'\bfbeta
    + Z_i\gamma, \qquad m(\X_{i}) = \X_i'\bs{\delta}, \] where we often refer to \(m(\X_i,
    Z_i)\) as the long regression and \(m(\X_i)\) as the short regression.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们可能需要选择是否将变量包含在回归中，因此了解这种选择如何影响回归中其他变量的群体系数可能会有所帮助。假设我们有一个变量 \(Z_i\)，我们可能将其添加到我们的回归中，目前该回归以
    \(\X_i\) 作为协变量。我们可以将这个新的投影写为 \[ m(\X_i, Z_i) = \X_i'\bfbeta + Z_i\gamma, \qquad
    m(\X_{i}) = \X_i'\bs{\delta}, \] 其中我们通常将 \(m(\X_i, Z_i)\) 称为长回归，而将 \(m(\X_i)\)
    称为短回归。
- en: From the definition of the BLP, we can write the short coefficients as \[ \bs{\delta}
    = \left(\E[\X_{i}\X_{i}']\right)^{-1} \E[\X_{i}Y_{i}]. \] Letting \(e_i = Y_i
    - m(\X_{i}, Z_{i})\) be the projection errors from the long regression, we can
    write this as \[ \begin{aligned} \bs{\delta} &= \left(\E[\X_{i}\X_{i}']\right)^{-1}
    \E[\X_{i}(\X_{i}'\bfbeta + Z_{i}\gamma + e_{i})] \\ &= \left(\E[\X_{i}\X_{i}']\right)^{-1}(\E[\X_{i}\X_{i}']\bfbeta
    + \E[\X_{i}Z_{i}]\gamma + \E[\X_{i}e_{i}]) \\ &= \bfbeta + \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Z_{i}]\gamma
    \end{aligned} \] Note that the vector in the second term is the vector of linear
    projection coefficients of a population linear regression of \(Z_i\) on the \(\X_i\).
    If we call these coefficients \(\bs{\pi}\), then the short coefficients are \[
    \bs{\delta} = \bfbeta + \bs{\pi}\gamma. \]
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从 BLP 的定义出发，我们可以将短系数写成 \[ \bs{\delta} = \left(\E[\X_{i}\X_{i}']\right)^{-1}
    \E[\X_{i}Y_{i}]. \] 令 \(e_i = Y_i - m(\X_{i}, Z_{i})\) 为长回归的投影误差，我们可以将其写成 \[ \begin{aligned}
    \bs{\delta} &= \left(\E[\X_{i}\X_{i}']\right)^{-1} \E[\X_{i}(\X_{i}'\bfbeta +
    Z_{i}\gamma + e_{i})] \\ &= \left(\E[\X_{i}\X_{i}']\right)^{-1}(\E[\X_{i}\X_{i}']\bfbeta
    + \E[\X_{i}Z_{i}]\gamma + \E[\X_{i}e_{i}]) \\ &= \bfbeta + \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Z_{i}]\gamma
    \end{aligned} \] 注意到第二项中的向量是 \(Z_i\) 在 \(\X_i\) 上的总体线性回归的线性投影系数向量。如果我们称这些系数为 \(\bs{\pi}\)，那么短系数就是
    \[ \bs{\delta} = \bfbeta + \bs{\pi}\gamma. \]
- en: We can rewrite this to show that the difference between the coefficients in
    these two projections is \(\bs{\delta} - \bfbeta= \bs{\pi}\gamma\) or the product
    of the coefficient on the “excluded” \(Z_i\) and the coefficient of the included
    \(\X_i\) on the excluded. Most textbooks refer to this difference as the **omitted
    variable bias** of omitting \(Z_i\) under the idea that \(\bfbeta\) is the true
    target of inference. But the result is much broader than this since it tells us
    how to relate the coefficients of two nested projections.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重写这个公式来展示这两个投影系数之间的差异是 \(\bs{\delta} - \bfbeta= \bs{\pi}\gamma\) 或“排除”的
    \(Z_i\) 的系数与包含的 \(\X_i\) 在排除项上的系数的乘积。大多数教科书将这个差异称为在 \(\bfbeta\) 是真实推断目标的情况下，遗漏
    \(Z_i\) 的**遗漏变量偏差**。但这个结果比这更广泛，因为它告诉我们如何关联两个嵌套投影的系数。
- en: The last two results (multiple regressions from bivariate and omitted variable
    bias) are sometimes presented as results for the ordinary least squares estimator
    that we will show in the next chapter. We introduce them here as features of a
    particular population quantity, the linear projection or population linear regression.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个结果（从双变量回归和遗漏变量偏差得到的多元回归）有时被呈现为下一章将要展示的普通最小二乘估计量的结果。我们在这里引入它们，作为特定人口数量特征，即线性投影或总体线性回归的特征。
- en: 5.7 Drawbacks of the BLP
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 BLP 的缺点
- en: The best linear predictor is, of course, a *linear* approximation to the CEF,
    and this approximation could be quite poor if the true CEF is highly nonlinear.
    A more subtle issue with the BLP is that it is sensitive to the marginal distribution
    of the covariates when the CEF is nonlinear. Let’s return to our example of wait
    times at voting precincts and income. In [Figure 5.3](#fig-blp-limits), we show
    the true CEF and the BLP when we restrict income below $50,000 or above $100,000\.
    The BLP can vary quite dramatically here. This figure is an extreme example, but
    the essential point still holds as the marginal distribution of \(X_i\) changes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳线性预测器当然是 CEF 的**线性**近似，如果真实 CEF 高度非线性，这种近似可能相当差。BLP 的一个更微妙的问题是，当 CEF 非线性时，它对协变量的边缘分布敏感。让我们回到我们关于投票地点等待时间和收入的例子。在
    [图 5.3](#fig-blp-limits) 中，我们展示了当限制收入低于 50,000 美元或高于 100,000 美元时的真实 CEF 和 BLP。BLP
    在这里可能会有很大的变化。这个图是一个极端例子，但基本观点仍然成立，因为 \(X_i\) 的边缘分布发生了变化。
- en: '![](../Images/f7e30c3e52f09aea429ba256809df2e5.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/f7e30c3e52f09aea429ba256809df2e5.png)'
- en: 'Figure 5.3: Linear projections for when truncating income distribution below
    $50k and above $100k.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：当截断收入分布低于 50k 和高于 100k 时的线性投影。
- en: 5.8 Summary
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.8 摘要
- en: As we discussed in this chapter, with even a moderate number of covariates,
    conditional expectation functions (also known as regressions) become very difficult
    to estimate because of the high dimensionality involved. To avoid this problem,
    we can focus on a different quantity, the **best linear predictor** which is the
    linear function of the covariates that best predicts the outcome in mean-squared
    error. The BLP exists under very mild conditions and has very interpretable parameters.
    Another strategy is to impose a linearity assumption on the **conditional expectation
    function** to make it more estimable, in which case, the BLP and the CEF are the
    same function. With a small number of discrete covariates it is possible to **saturate**
    a model so that linearity holds mechanically. Coefficients on population linear
    regressions with multiple independent variables can always be written in terms
    of a regression of the outcome on one variable orthogonalized relative to the
    rest of the independent variables. The **omitted variable bias** formula shows
    how leaving a variable out of the best linear affects the coefficients on other
    independent variables. In the next chapter, we will turn to using data to estimate
    the coefficients for these population linear regressions.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中讨论的那样，即使只有少量协变量，由于涉及的高维性，条件期望函数（也称为回归）变得非常难以估计。为了避免这个问题，我们可以关注一个不同的量，即**最佳线性预测器**，它是协变量的线性函数，在均方误差意义上最好地预测结果。在非常温和的条件下，BLP
    存在，并且具有非常可解释的参数。另一种策略是对**条件期望函数**施加线性假设，使其更容易估计，在这种情况下，BLP 和 CEF 是同一个函数。对于少量离散协变量，可以**饱和**一个模型，使得线性关系机械地成立。具有多个独立变量的总体线性回归的系数总是可以写成对结果在一个相对于其他独立变量正交化的变量上的回归的形式。**遗漏变量偏差**公式显示了遗漏变量如何影响其他独立变量的系数。在下一章中，我们将转向使用数据来估计这些总体线性回归的系数。
- en: '* * *'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note the choice of language here. The marginal effect is on the predicted value
    of \(Y_i\), not on \(Y_i\) itself. So these marginal effects are associational,
    not necessarily causal quantities.[↩︎](#fnref1)***
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意这里语言的选择。边际效应是在 \(Y_i\) 的预测值上，而不是在 \(Y_i\) 本身上。因此，这些边际效应是关联量，不一定是因果量。[↩︎](#fnref1)***
