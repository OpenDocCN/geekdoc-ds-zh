- en: Chapter 10\. CUDA in a Cloud and Cluster EnvironmentsDistributed GPUs are powering
    the fastest supercomputers as CUDA scales to thousands of nodes using MPI, the
    *de facto* standard for distributed computing. NVIDIA's GPUDirect has accelerated
    the key operations in MPI, send (MPI_Send) and receive (MPI_Recv), for communication
    over the network instead of the PCIe bus. As the name implies, GPUDirect moves
    data between GPUs without involving the processor on any of the host systems.
    This chapter focuses on the use of MPI and GPUDirect so that CUDA programmers
    can incorporate these APIs to create applications for cloud computing and computational
    clusters. The performance benefits of distributed GPU computing are very real
    but dependent on the bandwidth and latency characteristics of the distributed
    communications infrastructure. For example, the Chinese Nebulae supercomputer
    can deliver a peak 2.98 PFlops (or 2,980 trillion floating point per second) to
    run some of the most computationally intensive applications ever attempted. Meanwhile,
    commodity GPU clusters and cloud computing provide turn-key resources for users
    and organizations. To run effectively in a distributed environment, CUDA developers
    must develop applications and use algorithms that can scale given the limitations
    of the communications network. In particular, network bandwidth and latency are
    of paramount importance.**Keywords**MPI, petaflop, data-mining, scalability, infiniband,
    gpuDirect, Kiviat, balance ratios, top 500Distributed GPUs are powering the fastest
    supercomputers as CUDA scales to thousands of nodes using MPI, the *de facto*
    standard for distributed computing. NVIDIA's GPUDirect has accelerated the key
    operations in MPI, send (MPI_Send) and receive (MPI_Recv), for communication over
    the network instead of the PCIe bus. As the name implies, GPUDirect moves data
    between GPUs without involving the processor on any of the host systems. This
    chapter focuses on the use of MPI and GPUDirect so that CUDA programmers can incorporate
    these APIs to create applications for cloud computing and computational clusters.
    The performance benefits of distributed GPU computing are very real but dependent
    on the bandwidth and latency characteristics of the distributed communications
    infrastructure. For example, the Chinese Nebulae supercomputer can deliver a peak
    2.98 PFlops (or 2,980 trillion floating point per second) to run some of the most
    computationally intensive applications ever attempted. Meanwhile, commodity GPU
    clusters and cloud computing provide turn-key resources for users and organizations.
    To run effectively in a distributed environment, CUDA developers must develop
    applications and use algorithms that can scale given the limitations of the communications
    network. In particular, network bandwidth and latency are of paramount importance.At
    the end of this chapter, the reader will have a basic understanding of:■ The Message
    Passing Interface (MPI).■ NVIDIA's GPUDirect 2.0 technology.■ How balance ratios
    act as an indicator of application performance on new platforms.■ The importance
    of latency and bandwidth to distributed and cloud computing.■ Strong scaling.
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '第10章. 云计算与集群环境中的CUDA  '
- en: The Message Passing Interface (MPI)
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消息传递接口（MPI）
- en: MPI is a standard library based on the consensus of the MPI Forum ([http://www.mpi-forum.org/](http://www.mpi-forum.org/)),
    which has more than 40 participating organizations, including vendors, researchers,
    software library developers, and users. The goal of the forum is to establish
    a portable, efficient, and flexible standard that will be widely used in a variety
    of languages. Wide adoption has made MPI the “industry standard” even though it
    is not an IEEE (Institute of Electrical and Electronics Engineers) or ISO (International
    Organization for Standardization) standard. It is safe to assume that some version
    of MPI will be available on most distributed computing platforms regardless of
    vendor or operating system.Reasons for using MPI:■ **Standardization:** MPI is
    considered a standard. It is supported on virtually all HPC platforms.■ **Portability:**
    MPI library calls will not need to be changed when running on platforms that support
    a version of MPI that is compliant with the standard.■ **Performance:** Vendor
    implementations can exploit native hardware features to optimize performance.
    One example is the optimized data transport provided by GPUDirect.■ **Availability:**
    A variety of implementations are available from both vendors and public domain
    software.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'MPI 是基于 MPI 论坛（[http://www.mpi-forum.org/](http://www.mpi-forum.org/)）共识的标准库，论坛成员包括超过
    40 个参与组织，涵盖了供应商、研究人员、软件库开发者和用户。论坛的目标是建立一个可移植、高效且灵活的标准，广泛应用于多种编程语言。尽管 MPI 不是 IEEE（电气和电子工程师协会）或
    ISO（国际标准化组织）标准，但其广泛的采用使其成为“行业标准”。可以合理地假设，无论供应商或操作系统如何，大多数分布式计算平台上都将提供某个版本的 MPI。使用
    MPI 的原因：  '
- en: The MPI Programming Model
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 'MPI 编程模型  '
- en: MPI is an API that was originally designed to support distributed computing
    for C and FORTRAN applications. It was first implemented in 1992, with the first
    standard appearing in 1994\. Since then, language bindings and wrappers have been
    created for most application languages, including Perl, Python, Ruby, and Java.
    C bindings use the format *MPI_Xxxx* in which “*Xxxx*” specifies the operation.
    The methods **MPI_Send()** and **MPI_Recv()** are two examples that use this binding.Just
    as with a CUDA execution configuration, MPI defines a parallel computing topology
    to connect groups of processes in an MPI session. It is important to note that
    with MPI *the session size is fixed for the lifetime of the application*. This
    differs from *MapReduce* (discussed later in this chapter), which is a popular
    framework for cloud computing that was designed for fault-tolerant distributed
    computing. All parallelism is explicit with MPI, which means that the programmer
    is responsible for correctly identifying parallelism and implementing parallel
    algorithms with the MPI constructs.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 是一个最初设计用来支持 C 和 FORTRAN 应用程序分布式计算的 API。它于 1992 年首次实现，1994 年发布了第一个标准。从那时起，已经为大多数应用程序语言创建了语言绑定和封装器，包括
    Perl、Python、Ruby 和 Java。C 语言绑定使用格式 *MPI_Xxxx*，其中 “*Xxxx*” 指定操作。**MPI_Send()**
    和 **MPI_Recv()** 是两个使用此绑定的示例。就像 CUDA 执行配置一样，MPI 定义了一个并行计算拓扑，以连接 MPI 会话中的进程组。需要注意的是，MPI
    *会话大小在应用程序生命周期内是固定的*。这与 *MapReduce*（本章稍后讨论）不同，后者是为容错分布式计算设计的流行云计算框架。MPI 中的所有并行性都是显式的，这意味着程序员有责任正确识别并行性并使用
    MPI 构造实现并行算法。
- en: The MPI Communicator
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MPI通信器
- en: A fundamental concept in MPI is the *communicator*, a distributed object that
    supports both *collective* and *point-to-point* communication. As the name implies,
    collective communications refers to those MPI functions involving all the processors
    within the defined communicator group. Point-to-point communications are used
    by individual MPI processes to send messages to each other.By default, MPI creates
    the **MPI_COMM_WORLD** communicator immediately after the call to **MPI_Init().
    MPI_COMM_WORLD** includes all the MPI processes in the application An MPI application
    can create multiple, separate communicators to separate messages associated with
    one set of tasks or group of processes from those associated with another. This
    chapter uses only the default communicator. Look to the many MPI books and tutorials
    on the Internet for more information about the use of communicator groups.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 的一个基本概念是 *通信器*，它是一个分布式对象，支持 *集体* 和 *点对点* 通信。顾名思义，集体通信是指涉及所有定义的通信器组中的处理器的
    MPI 函数。点对点通信由各个 MPI 进程使用，用于相互发送消息。默认情况下，MPI 在调用 **MPI_Init()** 后立即创建 **MPI_COMM_WORLD**
    通信器。**MPI_COMM_WORLD** 包含应用程序中的所有 MPI 进程。一个 MPI 应用程序可以创建多个独立的通信器，将与一组任务或进程组相关的消息与另一组任务或进程组相关的消息分开。本章只使用默认的通信器。有关通信器组使用的更多信息，可以参考互联网上的许多
    MPI 书籍和教程。
- en: MPI Rank
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MPI 等级
- en: Within a communicator, every process has its own unique, integer identifier
    assigned by the system when the process initializes. A rank is sometimes also
    called a “task ID.” Ranks are contiguous and begin at 0\. They are often used
    in conditional operations to control execution of the MPI processes. A general
    paradigm in MPI applications is to use a master process, noted as rank 0, to control
    all other slave process of rank greater than 0.MPI programs are generally structured
    as shown in [Figure 10.1](#f0010).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个通信域内，每个进程都有一个由系统在进程初始化时分配的独特整数标识符。等级有时也叫做“任务 ID”。等级是连续的，从 0 开始。它们通常用于条件操作中，以控制
    MPI 进程的执行。在 MPI 应用程序中，一个常见的模式是使用一个主进程，记作等级 0，来控制所有其他等级大于 0 的从进程。[图 10.1](#f0010)
    显示了 MPI 程序的一般结构。
- en: '| ![B9780123884268000100/f10-01-9780123884268.jpg is missing](B9780123884268000100/f10-01-9780123884268.jpg)
    |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000100/f10-01-9780123884268.jpg 缺失](B9780123884268000100/f10-01-9780123884268.jpg)
    |'
- en: '| **Figure 10.1**General structure of an MPI program. |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| **图 10.1** MPI 程序的一般结构。 |'
- en: '[Example 10.1](#tb0010), “A Basic MPI Program,” illustrates a C implementation
    of this framework by having each MPI process print out its rank:`#include "mpi.h"``#include
    <stdio.h>``int main(int argc, char *argv[])``{``int numtasks, rank, ret;``ret
    = MPI_Init(&argc,&argv);``if (ret != MPI_SUCCESS) {``printf ("Error in MPI_Init()!\n");``MPI_Abort(MPI_COMM_WORLD,
    ret);``}``MPI_Comm_size(MPI_COMM_WORLD,&numtasks);``MPI_Comm_rank(MPI_COMM_WORLD,&rank);``printf
    ("Number of tasks= %d My rank= %d\n", numtasks,rank);``/******* do some work *******/``MPI_Finalize();``}`MPI
    is usually installed and configured by the systems administrator. See the documentation
    for your cluster on how to compile and run this application.NVIDIA, for example,
    makes the suggestion shown in [Example 10.2](#tb0015), “NVIDIA Comment in the
    *simpleMPI* SDK Example,” to build their *simpleMPI* SDK example:`* simpleMPI.cpp:
    main program, compiled with mpicxx on linux/Mac platforms``*on Windows, please
    download the Microsoft HPC Pack SDK 2008`To build an application using CUDA and
    MPI in the same file, the **nvcc** command line in [Example 10.3](#tb0020), “nvcc
    Command Line to Build *basicMPI .cu*,” works under Linux. The method links to
    the MPI library:`nvcc -I $MPI_INC_PATH basicMPI.cu -L $MPI_LIB_PATH –lmpich –o
    basicMPI`Usually **mpiexec** is used to start an MPI application. Some legacy
    implementations use **mpirun**. [Example 10.4](#tb0025) shows the command and
    output when running the example with two MPI processes:`$ mpiexec -np 2 ./basicMPI``Number
    of tasks= 2 My rank= 1``Number of tasks= 2 My rank= 0`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 10.1](#tb0010)，“一个基本的 MPI 程序”，通过让每个 MPI 进程打印出其 rank，展示了此框架的 C 语言实现：`#include
    "mpi.h"``#include <stdio.h>``int main(int argc, char *argv[])``{``int numtasks,
    rank, ret;``ret = MPI_Init(&argc,&argv);``if (ret != MPI_SUCCESS) {``printf ("MPI_Init()
    出错！\n");``MPI_Abort(MPI_COMM_WORLD, ret);``}``MPI_Comm_size(MPI_COMM_WORLD,&numtasks);``MPI_Comm_rank(MPI_COMM_WORLD,&rank);``printf
    ("任务数= %d 我的 rank= %d\n", numtasks,rank);``/******* 做一些工作 *******/``MPI_Finalize();``}`MPI
    通常由系统管理员安装和配置。请参阅集群文档，了解如何编译和运行此应用程序。例如，NVIDIA 在[示例 10.2](#tb0015)，“NVIDIA 在 *simpleMPI*
    SDK 示例中的注释”中，建议构建其 *simpleMPI* SDK 示例：`* simpleMPI.cpp：主程序，使用 mpicxx 在 Linux/Mac
    平台上编译``* 在 Windows 上，请下载 Microsoft HPC Pack SDK 2008`要在同一文件中使用 CUDA 和 MPI 构建应用程序，**nvcc**
    命令行在[示例 10.3](#tb0020)，“用于构建 *basicMPI .cu* 的 nvcc 命令行”中，适用于 Linux。此方法会链接到 MPI
    库：`nvcc -I $MPI_INC_PATH basicMPI.cu -L $MPI_LIB_PATH –lmpich –o basicMPI`通常使用
    **mpiexec** 启动 MPI 应用程序。一些遗留实现使用 **mpirun**。[示例 10.4](#tb0025) 显示了使用两个 MPI 进程运行示例时的命令和输出：`$
    mpiexec -np 2 ./basicMPI``任务数= 2 我的 rank= 1``任务数= 2 我的 rank= 0`'
- en: Master-Slave
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主从模式
- en: A common design pattern in MPI programs is a master-slave paradigm. Usually
    the process that is designated as rank 0 is defined as the master. This process
    then directs the activities of all the other processes. The code snippet in [Example
    10.5](#tb0030), “A Master-Slave MPI Snippet,” illustrates how to structure master-slave
    MPI code:`MPI_Comm_size(MPI_COMM_WORLD,&numtasks);``MPI_Comm_rank(MPI_COMM_WORLD,&rank);``if(
    rank == 0){``// put master code here``} else {``// put slave code here``}`This
    chapter uses the master-slave programming pattern. MPI supports many other design
    patterns. Consult the Internet or one of the many MPI books for more information.
    A good reference is *Using MPI* ([Gropp, Lusk, & Skjellum, 1999](B978012388426800015X.xhtml#ref55)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MPI 程序中，一个常见的设计模式是主从模式。通常，被指定为 rank 0 的进程被定义为主进程。该进程随后会指挥所有其他进程的活动。[示例 10.5](#tb0030)《主从
    MPI 代码片段》中的代码片段展示了如何构建主从 MPI 代码：`MPI_Comm_size(MPI_COMM_WORLD,&numtasks);``MPI_Comm_rank(MPI_COMM_WORLD,&rank);``if(
    rank == 0){``// 在这里写主进程代码``} else {``// 在这里写从进程代码``}`本章使用了主从编程模式。MPI 支持许多其他设计模式。更多信息请参考互联网或多本
    MPI 书籍。一个很好的参考书是 *使用 MPI* （[Gropp, Lusk, & Skjellum, 1999](B978012388426800015X.xhtml#ref55)）。
- en: Point-to-Point Basics
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 点对点基础
- en: MPI point-to-point communication sends messages between two different MPI processes.
    One process performs a send operation while the other performs a matching read.
    MPI guarantees that every message will arrive intact without errors. Care must
    be exercised when using MPI, as *deadlock* will occur when the send and receive
    operations do not match. Deadlock means that neither the sending nor receiving
    process can proceed until the other completes its action, which will never happen
    when the send and receive operations do not match. CUDA avoids deadlock by sharing
    data between the threads of a thread block with shared memory. This frees the
    CUDA programmer from having to explicitly match read and write operations but
    still requires the programmer to ensure that data is updated appropriately.**MPI_Send()**
    and **MPI_Recv()** are commonly used *blocking* methods for sending messages between
    two MPI processes. Blocking means that the sending process will wait until the
    complete message has been correctly sent and the receiving process will block
    while waiting to correctly receive the complete message. More complex communications
    methods can be built upon these two methods.Message size is determined by the
    **count** of an MPI data type and the type of the data. For portability, MPI defines
    the elementary data types. [Table 10.1](#t0010) lists data types required by the
    standard.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 点对点通信在两个不同的 MPI 进程之间发送消息。一个进程执行发送操作，而另一个执行匹配的读取操作。MPI 保证每条消息都会完整无误地到达。使用
    MPI 时需要小心，因为如果发送和接收操作不匹配，会发生*死锁*。死锁意味着发送进程和接收进程都无法继续，直到对方完成操作，而当发送和接收操作不匹配时，这种情况永远不会发生。CUDA
    通过在线程块之间共享内存来避免死锁，这样程序员不必显式匹配读写操作，但仍需确保数据得到适当更新。**MPI_Send()** 和 **MPI_Recv()**
    是常用的*阻塞*方法，用于在两个 MPI 进程之间发送消息。阻塞意味着发送进程将等待直到消息完全正确地发送，而接收进程将在等待正确接收完整消息时被阻塞。更复杂的通信方法可以基于这两种方法构建。消息的大小由
    MPI 数据类型的**计数**和数据类型决定。为了便于移植，MPI 定义了基本数据类型。[表 10.1](#t0010)列出了标准要求的数据类型。
- en: '**Table 10.1** Data Types Required by the MPI Standard'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10.1** MPI 标准要求的数据类型'
- en: '| C Data Types | Fortran Data Types |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| C 数据类型 | Fortran 数据类型 |'
- en: '| MPI_CHAR | signed char | MPI_CHARACTER | character(1) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| MPI_CHAR | 有符号字符型 | MPI_CHARACTER | character(1) |'
- en: '| MPI_SHORT | signed short int |  |  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| MPI_SHORT | 有符号短整型 |  |  |'
- en: '| MPI_INT | signed int | MPI_INTEGER | integer |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| MPI_INT | 有符号整型 | MPI_INTEGER | 整型 |'
- en: '| MPI_LONG | signed long int |  |  |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| MPI_LONG | 有符号长整型 |  |  |'
- en: '| MPI_UNSIGNED_CHAR | unsigned char |  |  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| MPI_UNSIGNED_CHAR | 无符号字符型 |  |  |'
- en: '| MPI_UNSIGNED_SHORT | unsigned short int |  |  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| MPI_UNSIGNED_SHORT | 无符号短整型 |  |  |'
- en: '| MPI_UNSIGNED | unsigned int |  |  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| MPI_UNSIGNED | 无符号整型 |  |  |'
- en: '| MPI_UNSIGNED_LONG | unsigned long int |  |  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| MPI_UNSIGNED_LONG | 无符号长整型 |  |  |'
- en: '| MPI_FLOAT | float | MPI_REAL | real |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| MPI_FLOAT | 浮点型 | MPI_REAL | real |'
- en: '| MPI_DOUBLE | double | MPI_DOUBLE_PRECISION | double precision |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| MPI_DOUBLE | 双精度浮点型 | MPI_DOUBLE_PRECISION | 双精度 |'
- en: '| MPI_LONG_DOUBLE | long double |  |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| MPI_LONG_DOUBLE | 长双精度浮点型 |  |  |'
- en: '|  |  | MPI_COMPLEX | complex |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MPI_COMPLEX | 复数 |'
- en: '|  |  | MPI_DOUBLE_COMPLEX | double complex |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MPI_DOUBLE_COMPLEX | 双精度复数 |'
- en: '|  |  | MPI_LOGICAL | logical |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MPI_LOGICAL | 逻辑 |'
- en: '| MPI_BYTE | 8 binary digits | MPI_BYTE | 8 binary digits |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| MPI_BYTE | 8个二进制位 | MPI_BYTE | 8个二进制位 |'
- en: '| MPI_PACKED | For data used with MPI_ Pack()/MPI_Unpack() | MPI_PACKED | For
    data used with MPI_ Pack()/MPI_Unpack() |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| MPI_PACKED | 用于MPI_Pack()/MPI_Unpack()的数据 | MPI_PACKED | 用于MPI_Pack()/MPI_Unpack()的数据
    |'
- en: How MPI Communicates
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MPI如何通信
- en: 'MPI can use optimized data paths to send and receive messages depending on
    the network communications hardware. For example, MPI applications running within
    a computational node can communicate via fast shared memory rather than send data
    over a physical network interface card (NIC). Minimizing data movement is an overarching
    goal for MPI vendors and those who develop the libraries because moving data simply
    wastes precious time and degrades MPI performance.In collaboration with vendors
    and software organizations, NVIDIA created GPUDirect to accelerate MPI for GPU
    computing. The idea is very simple: take the GPU data at the pointer passed to
    **MPI_Send()** and move it to the memory at the pointer to GPU memory used in
    the **MPI_Recv()** call. Don''t perform any other data movements or rely on other
    processors. This process is illustrated in [Figure 10.2](#f0015). Although simple
    in concept, implementation requires collaboration between vendors, drivers, operating
    systems, and hardware.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: MPI可以根据网络通信硬件使用优化的数据路径来发送和接收消息。例如，在计算节点内运行的MPI应用程序可以通过快速共享内存进行通信，而不是通过物理网络接口卡（NIC）发送数据。最小化数据移动是MPI供应商和开发库人员的共同目标，因为数据移动只会浪费宝贵的时间，并且降低MPI性能。在与供应商和软件组织的合作下，NVIDIA创建了GPUDirect来加速GPU计算的MPI。其思路非常简单：获取传递给**MPI_Send()**的GPU数据指针，并将其移动到**MPI_Recv()**调用中使用的GPU内存指针所指向的内存位置。不要进行其他数据移动或依赖其他处理器。此过程如[图10.2](#f0015)所示。虽然概念上很简单，但实现需要供应商、驱动程序、操作系统和硬件的合作。
- en: '| ![B9780123884268000100/f10-02-9780123884268.jpg is missing](B9780123884268000100/f10-02-9780123884268.jpg)
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000100/f10-02-9780123884268.jpg is missing](B9780123884268000100/f10-02-9780123884268.jpg)
    |'
- en: '| **Figure 10.2**MPI GPUDirect data path. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **图10.2**MPI GPUDirect数据路径。|'
- en: '*InfiniBand* (IB) is a commonly used high-speed, low-latency communications
    link in HPC. It is designed to be scalable and is used in many of the largest
    supercomputers in the world. Most small computational clusters also use InfiniBand
    for price and performance reasons. Mellanox is a well-known and respected vendor
    of InfiniBand products.NVIDIA lists the following changes in the Linux kernel,
    the NVIDIA Linux CUDA driver, and the Mellanox InfiniBand driver:■ Linux kernel
    modifications:■ Support for sharing pinned pages between different drivers.■ The
    Linux Kernel Memory Manager (MM) allows NVIDIA and Mellanox drivers to share the
    host memory and provides direct access for the latter to the buffers allocated
    by the NVIDIA CUDA library, thus providing Zero Copy of data and better performance.■
    NVIDIA driver:■ Allocated buffers by the CUDA library are managed by the NVIDIA
    driver.■ We have added the modifications to mark these pages to be shared so the
    Kernel MM will allow the Mellanox InfiniBand drivers to access them and use them
    for transportation without the need for copying or repinning them.■ Mellanox OFED[¹](#fn9000)
    drivers:¹OpenFabrics Enterprise Distribution.■ We have modified the Mellanox InfiniBand
    driver to query memory and to be able to share it with the NVIDIA Tesla drivers
    using the new Linux Kernel MM API.■ In addition, the Mellanox driver registers
    special callbacks to allow other drivers sharing the memory to notify any changes
    performed during runtime in the shared buffers state in order for the Mellanox
    driver to use the memory accordingly and to avoid invalid access to any shared
    pinned buffers.The result is the direct data transfer path between the send and
    receive buffers that provides a 30 percent increase in MPI performance.[Figure
    10.2](#f0015) shows that a registered region of memory can be directly transferred
    to a buffer in the device driver. Both the NIC and the device driver on the sending
    host know about this buffer, which lets the NIC send the data over the network
    interconnect without any host processor intervention. Similarly, the device driver
    and NIC on the receiving host know about a common receive buffer. When the GPU
    driver is notified that the data has arrived intact, it is transferred to the
    receiving GPU (GPU[R] in [Figure 10.2](#f0015)).There are two ways for a CUDA
    developer to allocate memory in this model:■ Use cudaHostRegister().■ With the
    4.0 driver, just set the environmental variable CUDA_NIC_INTEROP=1\. This will
    tell the driver to use an alternate path for cudaMallocHost() that is compatible
    with the IB drivers. There is no need to use cudaHostRegister().'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Bandwidth
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带宽
- en: Network bandwidth is a challenge in running distributed computations. The bandwidths
    listed in [Table 10.2](#t0015) show that most network interconnects transfer data
    significantly slower than the PCIe bus. (QDR stands for Quad Data Rate InfiniBand.
    DDR and SDR stand for double and single data rate, respectively.)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 网络带宽是分布式计算中的一个挑战。[表 10.2](#t0015) 中列出的带宽数据显示，大多数网络互连的传输速度明显慢于 PCIe 总线。（QDR 代表四倍数据速率
    InfiniBand，DDR 和 SDR 分别代表双倍和单倍数据速率。）
- en: '**Table 10.2** Data Accessibility, Including InfiniBand Messages'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10.2** 数据可访问性，包括 InfiniBand 消息'
- en: '| Data Access | Memory Type | Bandwidth |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 数据访问 | 内存类型 | 带宽 |'
- en: '| Internal to the SM | Register memory | ≈8,000 GB/s |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| SM内部 | 寄存器内存 | ≈8,000 GB/s |'
- en: '| Internal to the SM | Shared memory | ≈1,600 GB/s |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| SM内部 | 共享内存 | ≈1,600 GB/s |'
- en: '| GPU | Global memory | 177 GB/s |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| GPU | 全局内存 | 177 GB/s |'
- en: '| PCIe | Mapped memory | ≈8 GB/s one-way |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| PCIe | 映射内存 | ≈8 GB/s 单向 |'
- en: '| MPI | 12x InfiniBand | QDR | 12 GB/s |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| MPI | 12x InfiniBand | QDR | 12 GB/s |'
- en: '| DDR | 6 GB/s |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| DDR | 6 GB/s |'
- en: '| SDR | 3 GB/s |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| SDR | 3 GB/s |'
- en: '| MPI | 4x InfiniBand | QDR | 4 GB/s |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| MPI | 4x InfiniBand | QDR | 4 GB/s |'
- en: '| DDR | 2 GB/s |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| DDR | 2 GB/s |'
- en: '| SDR | 1 GB/s |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| SDR | 1 GB/s |'
- en: '| MPI | 1x InfiniBand | QDR | 1 GB/s |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| MPI | 1x InfiniBand | QDR | 1 GB/s |'
- en: '| DDR | 0.5 GB/s |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| DDR | 0.5 GB/s |'
- en: '| SDR | 0.25 GB/s |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| SDR | 0.25 GB/s |'
- en: 'A poor InfiniBand network can introduce an order of magnitude slowdown relative
    to PCIe speeds, which emphasizes the first of the three rules of efficient GPU
    programming introduced in [Chapter 1](B978012388426800001X.xhtml#B978-0-12-388426-8.00001-X):
    “Get the data on the GPGPU and keep it there.” It also raises portability issues
    for those who wish to ship distributed MPI applications. Unfortunately, users
    tend to blame the application first for poor performance rather than the hardware.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一个差的 InfiniBand 网络可能会引入比 PCIe 更大数量级的性能下降，这强调了[第 1 章](B978012388426800001X.xhtml#B978-0-12-388426-8.00001-X)中介绍的高效
    GPU 编程的三条规则中的第一条：“将数据加载到 GPGPU 上并保持在那里。”这也为那些希望部署分布式 MPI 应用程序的人带来了可移植性问题。不幸的是，用户往往会首先归咎于应用程序的性能问题，而不是硬件本身。
- en: Balance Ratios
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平衡比率
- en: Deciding whether an application or algorithm will run effectively across a network
    is a challenge. Of course, the most accurate approach is to port the code and
    benchmark it. This may be impossible due to lack of funding and time. It may also
    result in a lot of needless work.Balance ratios are a commonsense approach that
    can provide a reasonable estimation about application or algorithm performance
    that does not require first porting the application. These metrics are also used
    to evaluate hardware systems as part of a procurement process ([Farber, 2007](B978012388426800015X.xhtml#ref40)).Balance
    ratios define a system balance that is quantifiable and—with the right choice
    of benchmarks—provides some assurance that an existing application will run well
    on a new computer. The challenge lies in deciding what characteristics need to
    be measured to determine whether an application will run well on a GPU or network
    of GPUs.Most GPU applications are numerically intensive, which suggests that all
    GPU-based metrics should be tied to floating-point performance. Comparisons can
    then be made between systems and cards with different floating-point capabilities.
    Bandwidth limitations are an important metric to tie to numerical performance,
    as global memory, the PCIe bus, and network connectivity are known to bottleneck
    GPU performance for most applications. As shown in [Equation 10.1](#fm0010) the
    ratios between floating-point performance and the bandwidths available to keep
    the floating-point processors busy can be calculated from vendor and hardware
    information. This ratio makes it easy to see that a hardware configuration that
    moves data over a four-times DDR InfiniBand connection can cause an order of magnitude
    decrease in application performance versus the same application running on hardware
    that moves data across the PCIe bus.(10.1)![B9780123884268000100/si1.gif is missing](B9780123884268000100/si1.gif)These
    observations are not new in HPC. The January 2003 *Report of the National Science
    Foundation Blue-Ribbon Advisory Panel on Cyberinfrastructure*, [²](#fn0010) known
    as the Atkins Report, specifies some desirable metrics that are tied to floating-point
    performance:²[http://www.nsf.gov/cise/sci/reports/atkins.pdf](http://www.nsf.gov/cise/sci/reports/atkins.pdf).■
    At least 1 byte of memory per flop/s.■ Memory bandwidth (byte/s/flop/s) ≥ 1.■
    Internal network aggregate link bandwidth (bytes/s/flop/s) ≥ 0.2.■ Internal network
    bi-section bandwidth (bytes/s/flop/s) ≥ 0.1.■ System sustained productive disk
    I/O bandwidth (byte/s/flop/s) ≥ 0.001.Keep in mind that numerical values presented
    in the Atkins Report are for desirable ratios based on *their* definition of a
    “representative” workload. Your application may have dramatically different needs.
    For this reason, it is worthwhile to evaluate your applications to see what ratios
    currently work and what ratios need to be improved.A good approach is to use benchmarks
    to evaluate system performance. This is the thought behind the Top 500 list that
    is used to rank the fastest supercomputers in the world. The HPC Challenge (HPCC)
    website[³](#fn0015) provides a more extensive test suite. It will also generate
    a Kiviat diagram (Kiviat diagrams are similar to the radar plots in Microsoft
    Excel) to compare systems based on the standard HPCC benchmarks. A well-balanced
    system looks symmetrical on these plots because it performs well on all tests.
    High-performance balanced systems visually stand out because they occupy the outermost
    rings. Out-of-balance systems are distorted, as can be seen in [Figure 10.3](#f0020).³[http://icl.cs.utk.edu/hpcc](http://icl.cs.utk.edu/hpcc).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 决定一个应用程序或算法是否能够在网络中有效运行是一个挑战。当然，最准确的方法是移植代码并进行基准测试。但由于缺乏资金和时间，这可能是不可行的。也可能导致大量不必要的工作。平衡比率是一种常识性的方法，可以提供关于应用程序或算法性能的合理估计，而无需首先移植应用程序。这些指标还用于作为采购过程的一部分来评估硬件系统（[Farber,
    2007](B978012388426800015X.xhtml#ref40)）。平衡比率定义了一个系统的平衡，这个平衡是可量化的，并且在选择合适的基准测试的情况下，能够提供一些保证，确保现有的应用程序能够在新计算机上良好运行。挑战在于确定需要测量哪些特征，以确定一个应用程序是否能够在GPU或GPU网络上运行良好。大多数GPU应用程序都是数值密集型的，这意味着所有基于GPU的指标都应该与浮点性能挂钩。然后，可以在具有不同浮点能力的系统和显卡之间进行比较。带宽限制是与数值性能相关的重要指标，因为全局内存、PCIe总线和网络连接已知会成为大多数应用程序的GPU性能瓶颈。如[公式
    10.1](#fm0010)所示，浮点性能与保持浮点处理器忙碌的带宽之间的比率可以通过供应商和硬件信息来计算。这个比率使我们容易看出，数据通过四倍DDR InfiniBand连接传输的硬件配置，可能会导致应用程序性能比在通过PCIe总线传输数据的硬件上运行同一应用程序时下降一个数量级。(10.1)![B9780123884268000100/si1.gif
    is missing](B9780123884268000100/si1.gif)这些观察在高性能计算（HPC）领域并不新鲜。2003年1月的《国家科学基金会蓝带咨询小组关于网络基础设施的报告》[²](#fn0010)，即著名的Atkins报告，明确了与浮点性能相关的一些理想指标：²[http://www.nsf.gov/cise/sci/reports/atkins.pdf](http://www.nsf.gov/cise/sci/reports/atkins.pdf)。■
    每个浮点操作每秒至少需要1字节的内存。■ 内存带宽（字节/秒/浮点操作每秒）≥ 1。■ 内部网络总链路带宽（字节/秒/浮点操作每秒）≥ 0.2。■ 内部网络二分带宽（字节/秒/浮点操作每秒）≥
    0.1。■ 系统持续的生产性磁盘I/O带宽（字节/秒/浮点操作每秒）≥ 0.001。请记住，Atkins报告中给出的数值是基于*他们*对“代表性”工作负载的定义所提出的理想比率。你的应用程序可能有完全不同的需求。因此，评估你的应用程序，看看当前哪些比率有效，哪些比率需要改进，还是很值得的。一个好的方法是使用基准测试来评估系统性能。这也是用于排名全球最快超级计算机的Top
    500榜单背后的思想。HPC挑战（HPCC）网站[³](#fn0015)提供了一个更为全面的测试套件。它还会生成一个Kiviat图（Kiviat图类似于Microsoft
    Excel中的雷达图），以基于标准的HPCC基准测试来比较系统。一个良好平衡的系统在这些图中看起来是对称的，因为它在所有测试中表现良好。高性能的平衡系统在视觉上非常突出，因为它们占据了最外圈。失衡的系统则会扭曲，正如[图
    10.3](#f0020)中所看到的那样。³[http://icl.cs.utk.edu/hpcc](http://icl.cs.utk.edu/hpcc)。
- en: '| ![B9780123884268000100/f10-03-9780123884268.jpg is missing](B9780123884268000100/f10-03-9780123884268.jpg)
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000100/f10-03-9780123884268.jpg 缺失](B9780123884268000100/f10-03-9780123884268.jpg)
    |'
- en: '| **Figure 10.3**An example Kiviat diagram. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **图 10.3** 一个 Kiviat 图示例。 |'
- en: The HPCC website is a good source for benchmarks and comparative data on different
    computational systems. Be aware that synthetic benchmarks such as those on the
    HPCC website are very good at stressing certain aspects of machine performance,
    but they do represent a narrow view into machine performance.To provide a more
    realistic estimate, most organizations utilize benchmark evaluation suites that
    include some sample production codes to complement synthetic benchmarks—just to
    see if there is any unexpected performance change, either good or bad. Such benchmarks
    can also provide valuable insight into how well the processor and system components
    work together on real-world applications; plus, they can uncover issues that can
    adversely affect performance such as immature compilers and/or software drivers.The
    key point behind balance ratios is that they can help the application programmer
    or system designer decide whether some aspect of a new hardware system will bottleneck
    an application. Although floating-point performance is important, other metrics
    such as memory capacity, storage bandwidth, and storage capacity might be a gating
    factor. The memory capacity of current GPUs, for example, can preclude the use
    of some applications or algorithms.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: HPCC 网站是关于不同计算系统基准测试和比较数据的一个良好来源。请注意，像 HPCC 网站上的合成基准测试非常擅长强调机器性能的某些方面，但它们仅代表了机器性能的一个狭窄视角。为了提供更为真实的估算，大多数组织使用包含一些样本生产代码的基准评估套件来补充合成基准测试——仅仅是为了查看是否存在任何意外的性能变化，无论是好的还是坏的。这些基准测试还可以提供有价值的洞察，帮助了解处理器和系统组件在实际应用中的协同工作情况；此外，它们还能够发现一些可能会影响性能的问题，比如不成熟的编译器和/或软件驱动程序。平衡比率背后的关键点在于，它们可以帮助应用程序开发者或系统设计师判断新硬件系统的某个方面是否会成为应用程序的瓶颈。尽管浮点性能很重要，但其他指标如内存容量、存储带宽和存储容量也可能是决定性因素。例如，当前
    GPU 的内存容量可能会限制某些应用程序或算法的使用。
- en: Considerations for Large MPI Runs
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型 MPI 运行的考虑事项
- en: In designing distributed applications, it is important to consider the scalability
    of all aspects of the application—not just of the computation!Many legacy MPI
    applications were designed at a time when an MPI run that used from 10 to 100
    processing cores was considered a “large” run. (Such applications might be good
    candidates to port to CUDA so that they can run on a single GPU.) A common shortcut
    taken in these legacy applications is to have the master process read data from
    a file and distribute it to the clients according to some partitioning scheme.
    This type of data load cannot scale, as the master node simply cannot transfer
    all the data for potentially tens of thousands of other processes. Even with modern
    hardware, the master process will become a bottleneck, which can cause the data
    load to take longer than the actual calculation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计分布式应用程序时，考虑应用程序各个方面的可扩展性是非常重要的——不仅仅是计算部分！许多传统的 MPI 应用程序是在一个 MPI 运行使用 10 到
    100 个处理核心被认为是“大型”运行的时期设计的。（这种应用可能是迁移到 CUDA 的好候选对象，从而可以在单个 GPU 上运行。）这些传统应用程序常用的一个快捷方式是让主进程从文件中读取数据，并根据某种分区方案将数据分发给客户端。这种数据加载方式无法扩展，因为主节点根本无法为可能达到数万个其他进程传输所有数据。即使是现代硬件，主进程也会成为瓶颈，这可能导致数据加载比实际计算所需的时间还要长。
- en: Scalability of the Initial Data Load
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始数据加载的可扩展性
- en: A better approach is to have each process read its own data from a file on a
    distributed file system. Each process is provided with the filename of a data
    file that contains the data. They then open the file and perform whatever seek
    and other I/O operations are needed to access and load the data. All the clients
    then close the file to free system resources. This simple technique can load hundreds
    of gigabytes of data into large supercomputers very quickly. The process is illustrated
    in [Figure 10.4](#f0025).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更好的方法是让每个进程从分布式文件系统中的文件中读取自己的数据。每个进程会获得一个数据文件的文件名，文件中包含了数据。然后，它们打开文件，执行任何必要的查找和其他
    I/O 操作，以访问和加载数据。所有客户端随后关闭文件以释放系统资源。这种简单的技术能够非常快速地将数百吉字节的数据加载到大型超级计算机中。这个过程在[图
    10.4](#f0025)中进行了说明。
- en: '| ![B9780123884268000100/f10-04-9780123884268.jpg is missing](B9780123884268000100/f10-04-9780123884268.jpg)
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000100/f10-04-9780123884268.jpg 图片缺失](B9780123884268000100/f10-04-9780123884268.jpg)
    |'
- en: '| **Figure 10.4**A scalable MPI data load for massive data sets. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| **图 10.4** 可扩展的 MPI 数据加载，用于大规模数据集。 |'
- en: With this I/O model, much depends on the bandwidth capabilities of the file
    system that holds the data. Modern parallel distributed file systems can deliver
    hundreds of gigabytes per second of storage bandwidth when concurrently accessed
    by multiple processes. The freely available Lustre file system[⁴](#fn0020) can
    deliver hundreds of gigabytes per second of file system bandwidth using this technique
    and demonstrated scaling to 60,000 MPI processes as shown in [Figure 10.5](#f0030).
    Commercial file systems such as GPFS (General Parallel File System) by IBM and
    PanFS by Panasas also scale to hundreds of gigabytes per second and tens of thousands
    of MPI processes. To achieve high performance, these file systems need to be accessed
    by a number of concurrent distributed processes. It is not recommended to use
    this technique with large numbers of MPI clients on older NFS and CIFS file systems.⁴[http://lustre.org](http://lustre.org).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种I/O模型下，数据存储的文件系统带宽能力至关重要。现代并行分布式文件系统在多个进程并发访问时，能够提供每秒数百GB的存储带宽。免费提供的Lustre文件系统[⁴](#fn0020)利用这一技术可以提供每秒数百GB的文件系统带宽，并在[图
    10.5](#f0030)中展示了在60,000个MPI进程下的扩展能力。IBM的GPFS（通用并行文件系统）和Panasas的PanFS等商业文件系统同样能够扩展到每秒数百GB的带宽，并支持数万个MPI进程。为了实现高性能，这些文件系统需要由多个并发分布式进程访问。不建议在旧版的NFS和CIFS文件系统上使用这种技术进行大量MPI客户端的访问。⁴[http://lustre.org](http://lustre.org)。
- en: '| ![B9780123884268000100/f10-05-9780123884268.jpg is missing](B9780123884268000100/f10-05-9780123884268.jpg)
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000100/f10-05-9780123884268.jpg 文件缺失](B9780123884268000100/f10-05-9780123884268.jpg)
    |'
- en: '| **Figure 10.5**Example of near-linear scaling to 386 TF/s using 60,000 processing
    cores. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **图 10.5** 使用60,000个处理核心实现接近线性扩展到386 TF/s的示例。|'
- en: Using MPI to Perform a Calculation
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用MPI执行计算
- en: The general mapping introduced in [Chapter 2](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1)
    has been used in an MPI distributed environment to scale to 500 GPUs on the TACC
    Longhorn supercomputer. The same mapping has been used to scale to more than 60,000
    processing cores on both the CM-200 Connection Machine ([Farber, 1992](B978012388426800015X.xhtml#ref44))
    and the TACC Ranger Supercomputer ([Farber & Trease, 2008](B978012388426800015X.xhtml#ref47)).During
    these runs, the rank 0 process was designated as the master process in a conventional
    master-slave configuration. The master then:■ Initiated the data load as described
    previously. Even when using hundreds of gigabytes of data, the data load took
    only a few seconds using a modern parallel distributed file system.■ Performed
    other initializations.■ Runs the optimization procedure and directs the slave
    processes.■ To make the best use of the available resources, the master node also
    evaluates the objective function along with the slave processes.■ Most production
    runs used a variant of Powell's method, as described in *Numerical Recipes* ([Press,
    Teukolsky, & Vetterling, 2007a](B978012388426800015X.xhtml#ref106)) or *Conjugant
    Gradient* (also described in Numerical Recipies).■ On each evaluation of the objective
    function:■ Uses **MPI_Bcast()** to broadcast the parameters to all the slave processes.■
    Each slave process (along with the master) calculates the partial sum of the objective
    function. This does not require any communication and can be optimized to run
    at full speed on the device as discussed in [Chapter 2](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1)
    and [Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3).■ Performs
    its own local calculation of the objective function.■ Calls **MPI_Reduce()** to
    retrieve the total sum of all partial results.■ In general, the **MPI_Reduce()**
    operation is highly optimized and scales according to O(log2(**ntasks**)), where
    **ntasks** is the number reported by **MPI_Comm_size()**.■ Supplies the result
    of the objective function to the optimization method, which causes either:■ Another
    evaluation of the objective function.■ Completion of the optimization process.The
    slave processes (those processes with a rank greater than 0):■ Initiate the data
    load as described previously.■ Remain in an infinite loop until told it is time
    to exit the application by the master. Inside the loop, each slave:■ Reads the
    parameters from the master node.■ Calculates the partial sum of the objective
    function.■ Sends the partial results to the master when requested.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Check Scalability
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查可扩展性
- en: The scaling behavior of a distributed application is a key performance metric.
    Applications that exhibit s*trong scaling* run faster as the number of processors
    are added to a fixed problem size. *Linear scaling* is a goal for distributed
    applications because the runtime of an application scales linearly with the amount
    of computational available (e.g., a two-times larger system will run the problem
    in half the time, a four-times larger system will run the problem in one-fourth
    the time, etc.). *Embarrassingly parallel* problems can exhibit linear runtime
    behavior because each computation is independent of each other. Applications that
    require communication to satisfy some dependency can generally achieve at best
    near-linear scaling. For example, an O(log[2](*N*)) reduction operation will cause
    the scaling behavior to slowly diverge from a linear speedup according to processor
    count. [Figure 10.5](#f0030) illustrates near-linear scaling to 60,000 processing
    cores using the PCA objective function discussed in [Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3).The
    386 teraflop per second *effective rate* for the AMD Barcelona based TACC Ranger
    supercomputer reported in [Figure 10.5](#f0030) includes all communications overhead.
    A 500 GPU-based supercomputer run could deliver nearly 500 teraflops of single-precision
    performance. A colleague refers to these as “honest flops,” as they reflect the
    performance a real production run would deliver.The effective rate is defined
    for the PCA mapping is shown in [Equation 10.2](#fm0015), “Definition of effective
    rate for the PCA parallel mapping”:(10.2)![B9780123884268000100/si2.gif is missing](B9780123884268000100/si2.gif)where:■
    *TotalOpCount* is the number of floating-point operations performed in a call
    to an objective function.■ *T[broadcast]* is the time required to broadcast the
    parameters to all the slave processes.■ *T[objectfunc]* is the time consumed in
    calculating the partial results across all the clients. This is generally the
    time taken by the slowest client.■ *T[reduce]* is the time required by the reduction
    of the partial results.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '分布式应用程序的扩展性行为是一个关键的性能指标。表现出**强扩展性**的应用程序随着处理器数量的增加，固定问题规模下运行速度会变得更快。**线性扩展性**是分布式应用程序的目标，因为应用程序的运行时间会随着可用计算资源的增加而线性缩放（例如，系统规模扩大两倍时，问题运行时间将减少一半，扩大四倍时，运行时间将减少四分之一，等等）。**令人惊讶的并行**问题可以表现出线性的运行时间行为，因为每个计算相互独立。需要通信以满足某些依赖关系的应用程序通常最多只能达到接近线性的扩展性。例如，O(log[2](*N*))
    的归约操作会导致扩展性行为随着处理器数量的增加而逐渐偏离线性加速。[图 10.5](#f0030)展示了使用 [第 3 章](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)中讨论的
    PCA 目标函数，扩展到 60,000 个处理核心的近线性扩展性。AMD 巴塞罗那架构的 TACC Ranger 超级计算机在 [图 10.5](#f0030)
    中报告的386 teraflop每秒的*有效速率*包含了所有的通信开销。基于 500 个 GPU 的超级计算机运行可以提供接近 500 teraflops
    的单精度性能。一位同事称其为“真实浮点运算”，因为它们反映了实际生产运行所能提供的性能。PCA 映射的有效速率定义如 [方程 10.2](#fm0015)所示，“PCA
    并行映射的有效速率定义”：  '
- en: Cloud Computing
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云计算
- en: 'Cloud computing is an infrastructure paradigm that moves computation to the
    Web in the form of a web-based service. The idea is simplicity itself: institutions
    contract with Internet vendors for computational resources instead of providing
    those resources themselves through the purchase and maintenance of computational
    clusters and supercomputer hardware. As expected with any computational platform—especially
    one utilized for high performance and scientific computing—performance limitations
    within the platform define which computational problems will run well.Unlike MPI
    frameworks, cloud computing requires that application be tolerant of both failures
    and high latency in point-to-point communications. Still, MPI is such a prevalent
    API that most cloud computing services provide and support it. Be aware that bandwidth
    and latency issues may cause poor performance when running MPI on a cloud computing
    infrastructure.MPI utilizes a fixed number of processes that participate in the
    distributed application. This number is defined at application startup—generally
    with **mpiexec**. In MPI, the failure of any one process in a communicator affects
    all processes in the communicator, even those that are not in direct communication
    with the failed process. This factor contributes to the lack of fault tolerance
    in MPI applications.*MapReduce* is a fault-tolerant framework for distributed
    computing that has become very popular and is widely used ([Dean & Ghemawat, 2010](B978012388426800015X.xhtml#ref27)).
    In other words, the failure of a client has no significant effect on the server.
    Instead, the server can continue to service other clients. What makes the MapReduce
    structure robust is that all communication occurs in a two-party context where
    one party (e.g., process) can easily recognize that the other party has failed
    and can decide to stop communicating with it. Moreover, each party can easily
    keep track of the state held by the other party, which facilitates failover.A
    challenge with cloud computing is that many service providers use virtual machines
    that run on busy internal networks. As a result, communications time can increase.
    In particular, the time *T[reduce]* is very latency-dependent, as can be seen
    in [Equation 10.2](#fm0015). For tightly coupled applications (e.g., applications
    with frequent communication between processes), it is highly recommended that
    dedicated clusters be utilized. In particular, reduction operations will be rate-limited
    by the slowest process in the group. That said, papers like “Multi-GPU MapReduce
    on GPU Clusters” ([Stuart & Owens, 2011](B978012388426800015X.xhtml#ref127)) demonstrate
    that MapReduce is an active area of research.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: A Code Example
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个代码示例
- en: The objective function from the *nlpcaNM.cu* example from [Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)
    was adapted to use MPI and this data load technique.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 来自*[nlpcaNM.cu]*示例的目标函数已经被修改，以使用MPI和这种数据加载技术，示例来源于[第3章](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)。
- en: Data Generation
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据生成
- en: 'A simple data generator was created that uses the **genData()** method from
    *nlpcaNM.cu*. This program simply writes a number of examples to a file. Both
    the filename and number of examples are specified on the command line. The complete
    source listing is given in [Example 10.6](#tb0035), “Source for *genData.cu*”:`#include
    <iostream>``#include <fstream>``#include <stdlib.h>``using namespace std;``//
    get a uniform random number between -1 and 1``inline float f_rand() {``return
    2*(rand()/((float)RAND_MAX)) -1.;``}``template <typename Real>``void genData(ofstream
    &outFile, int nVec, Real xVar)``{``Real xMax = 1.1; Real xMin = -xMax;``Real xRange
    = (xMax - xMin);``for(int i=0; i < nVec; i++) {``Real t = xRange * f_rand();``Real
    z1 = t + xVar * f_rand();``Real z2 = t*t*t + xVar * f_rand();``outFile.write((char*)
    &z1, sizeof(Real));``outFile.write((char*) &z2, sizeof(Real));``}``}``int main(int
    argc, char *argv[])``{``if(argc < 3) {``fprintf(stderr,"Use: filename nVec\n");``exit(1);``}``ofstream
    outFile (argv[1], ios::out | ios::binary);``int nVec = atoi(argv[2]);``#ifdef
    USE_DBL``genData<double>(outFile, nVec, 0.1);``#else``genData<float>(outFile,
    nVec, 0.1);``#endif``outFile.close();``return 0;``}`The program can be saved in
    a file and compiled as shown in [Example 10.7](#tb0040), “Building *genData.cu*.”
    The C preprocessor variable **USE_DBL** specifies whether the data will be written
    as 32-bit or 64-bit binary floating-point numbers:`nvcc -D USE_DBL genData.cu
    -o bin/genData64``nvcc genData.cu -o bin/genData32`Data sets can be written via
    the command line. For example, [Example 10.8](#tb0045), “Creating a 32-Bit File
    with *genData.cu*,” writes 10M 32-bit floats to a file *nlpca32.dat*.`bin/genData32
    nlpca32.dat 10000000`It is very easy to specify the wrong file during runtime.
    This simple example does not provide version, size, or any other information useful
    for sanity checking. The Google ProtoBufs project is recommend for a production
    quality file and streaming data format. [⁵](#fn0025) Google uses this project
    for almost all of its internal RPC protocols and file formats.⁵[http://code.google.com/p/protobuf](http://code.google.com/p/protobuf).The
    *nlpcaNM.cu* example code from [Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)
    was modified to run in an MPI environment. The changes are fairly minimal but
    are distributed throughout the code. Only code snippets are provided in the following
    discussion. The entire code can be downloaded from the book website. [⁶](#fn9005)⁶[http://booksite.mkp.com/9780123884268](http://booksite.mkp.com/9780123884268).The
    variable **nGPUperNode** was defined at the beginning of the file. It is assumed
    that the number of MPI processes per node will equal the number of GPUs in each
    node (meaning one MPI process is per GPU). The value of **nGPUperNode** allows
    each MPI process to call **cudaSetDevice()** correctly to use different devices.
    Other topologies are possible including using one MPI process per node for all
    GPUs. See [Example 10.9](#tb0050), “Modification of the Top of *nlpcaNM.*cu to
    Use Multiple GPUs per Node”:`#include "mpi.h"``const static int nGPUperNode=2;``}`The
    **main()** method of *nlpcaNM.cu* has been modified with the MPI framework shown
    in [Figure 10.1](#f0010) using the code shown in [Example 10.1](#tb0010) (see
    [Example 10.10](#tb0055), “Modified main() method for *nlpcaNM.cu*”:`#include
    <stdio.h>``int main(int argc, char *argv[])``{``int numtasks, rank, ret;``if(argc
    < 2) {``fprintf(stderr,"Use: filename\n");``exit(1);``}``ret = MPI_Init(&argc,&argv);``if
    (ret != MPI_SUCCESS) {``printf ("Error in MPI_Init()!\n");``MPI_Abort(MPI_COMM_WORLD,
    ret);``}``MPI_Comm_size(MPI_COMM_WORLD,&numtasks);``MPI_Comm_rank(MPI_COMM_WORLD,&rank);``printf
    ("Number of tasks= %d My rank= %d\n", numtasks,rank);``/******* do some work *******/``#ifdef
    USE_DBL``trainTest<double> ( argv[1], rank, numtasks );``#else``trainTest<float>
    ( argv[1], rank, numtasks);``#endif``MPI_Finalize();``return 0;``}`The **setExamples()**
    method was modified to allocate memory on different GPUs. See [Example 10.11](#tb0060),
    “Modified setExamples Method for *nlpcaNM.cu*”:`void setExamples(thrust::host_vector<Real>&
    _h_data) {``nExamples = _h_data.size()/exLen;``// copy data to the device``int
    rank;``MPI_Comm_rank(MPI_COMM_WORLD,&rank);``cudaSetDevice(rank%nGPUperNode);``d_data
    = thrust::device_vector<Real>(nExamples*exLen);``thrust::copy(_h_data.begin(),
    _h_data.end(), d_data.begin());``d_param = thrust::device_vector<Real>(nParam);``}`[Example
    10.12](#tb0065), “Modified trainTest Method for *nlpcaNM.cu*,” shows how the data
    is loaded from disk:`#include <fstream>``template <typename Real>``void trainTest(char*
    filename, int rank, int numtasks)``{``ObjFunc<Real> testObj;``const int nParam
    = testObj.nParam;``cout << "nParam " << nParam << endl;``// read the test data``ifstream
    inFile (filename, ios::in | ios::binary);``// position 0 bytes from end``inFile.seekg(0,
    ios::end);``// determine the file size in bytes``ios::pos_type size = inFile.tellg();``//
    allocate number of Real values for this task``//(assumes a multiple of numtasks)``int
    nExamplesPerGPU = (size/(sizeof(Real)*testObj.exLen))/numtasks;``thrust::host_vector<Real>
    h_data(nExamplesPerGPU*testObj.exLen);``// seek to the byte location in the file``inFile.seekg(rank*h_data.size()*sizeof(Real),
    ios::beg);// allocate number of Real values for this task``inFile.seekg(rank*h_data.size()*sizeof(Real));``//
    read bytes from the file into h_data``inFile.read((char*)&h_data[0], h_data.size()*sizeof(Real));``//
    close the file``inFile.close();``testObj.setExamples(h_data);``int nExamples =
    testObj.get_nExamples();``if(rank > 0) {``testObj.objFunc( NULL );``return;``}``...Nelder-Mead
    code goes here``int op=0; // shut down slave processes``MPI_Bcast(&op, 1, MPI_INT,
    0, MPI_COMM_WORLD);``}`In this example, binary input stream, **infile** is opened
    to read the data in the file **filename**. After that:■ The size of the file is
    determined.■ Each of the **numtasks** processes is assigned an equal amount of
    the data based on the file size and the number of data per example. The logic
    is simplified by assuming that the number of examples is a multiple of **numtasks**.■
    The host vector **h_data** is allocated.■ A seek operation moves to the appropriate
    position in the file.■ The data is read into **h_data** and handed off to **setExamples()**
    to be loaded onto the device.■ As shown in [Figure 10.6](#f0035), the rank of
    the process is tested:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '创建了一个简单的数据生成器，使用来自 *nlpcaNM.cu* 的**genData()**方法。该程序仅仅是将若干个示例写入文件。文件名和示例数量可以在命令行中指定。完整的源代码请参见[示例
    10.6](#tb0035)，“*genData.cu* 的源代码”：`#include <iostream>``#include <fstream>``#include
    <stdlib.h>``using namespace std;``// 获取一个在 -1 和 1 之间均匀分布的随机数``inline float f_rand()
    {``return 2*(rand()/((float)RAND_MAX)) -1.;``}``template <typename Real>``void
    genData(ofstream &outFile, int nVec, Real xVar)``{``Real xMax = 1.1; Real xMin
    = -xMax;``Real xRange = (xMax - xMin);``for(int i=0; i < nVec; i++) {``Real t
    = xRange * f_rand();``Real z1 = t + xVar * f_rand();``Real z2 = t*t*t + xVar *
    f_rand();``outFile.write((char*) &z1, sizeof(Real));``outFile.write((char*) &z2,
    sizeof(Real));``}``}``int main(int argc, char *argv[])``{``if(argc < 3) {``fprintf(stderr,"Use:
    filename nVec\n");``exit(1);``}``ofstream outFile (argv[1], ios::out | ios::binary);``int
    nVec = atoi(argv[2]);``#ifdef USE_DBL``genData<double>(outFile, nVec, 0.1);``#else``genData<float>(outFile,
    nVec, 0.1);``#endif``outFile.close();``return 0;``}`该程序可以保存为文件并按照[示例 10.7](#tb0040)中的方式编译，“构建
    *genData.cu*。” C 预处理器变量 **USE_DBL** 指定数据是否将作为 32 位或 64 位二进制浮点数写入：`nvcc -D USE_DBL
    genData.cu -o bin/genData64``nvcc genData.cu -o bin/genData32`数据集可以通过命令行写入。例如，[示例
    10.8](#tb0045)，“使用 *genData.cu* 创建一个 32 位文件，”会将 10M 个 32 位浮点数写入文件 *nlpca32.dat*。`bin/genData32
    nlpca32.dat 10000000`在运行时很容易指定错误的文件。这个简单的例子没有提供版本、大小或其他有助于检查的任何信息。对于生产质量的文件和流数据格式，推荐使用
    Google 的 ProtoBuf 项目。[⁵](#fn0025) Google 几乎在所有的内部 RPC 协议和文件格式中都使用了这个项目。⁵[http://code.google.com/p/protobuf](http://code.google.com/p/protobuf)。从[第
    3 章](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)中修改的 *nlpcaNM.cu* 示例代码被修改以适应
    MPI 环境。修改相当少，但分布在代码的各个部分。以下讨论只提供了代码片段。完整代码可以从本书网站下载。[⁶](#fn9005)⁶[http://booksite.mkp.com/9780123884268](http://booksite.mkp.com/9780123884268)。变量
    **nGPUperNode** 在文件开始时定义。假设每个节点上的 MPI 进程数等于每个节点中的 GPU 数量（即每个 GPU 对应一个 MPI 进程）。**nGPUperNode**
    的值允许每个 MPI 进程正确调用 **cudaSetDevice()** 以使用不同的设备。其他拓扑也是可能的，包括每个节点使用一个 MPI 进程来处理所有
    GPU。请参见[示例 10.9](#tb0050)，“修改 *nlpcaNM.cu* 顶部以使用每个节点多个 GPU”：`#include "mpi.h"``const
    static int nGPUperNode=2;``}`*nlpcaNM.cu* 的 **main()** 方法已根据 MPI 框架进行了修改，如[图 10.1](#f0010)所示，使用[示例
    10.1](#tb0010)中的代码（见[示例 10.10](#tb0055)，“修改后的 *nlpcaNM.cu* 的 main() 方法”）：`#include
    <stdio.h>``int main(int argc, char *argv[])``{``int numtasks, rank, ret;``if(argc
    < 2) {``fprintf(stderr,"Use: filename\n");``exit(1);``}``ret = MPI_Init(&argc,&argv);``if
    (ret != MPI_SUCCESS) {``printf ("Error in MPI_Init()!\n");``MPI_Abort(MPI_COMM_WORLD,
    ret);``}``MPI_Comm_size(MPI_COMM_WORLD,&numtasks);``MPI_Comm_rank(MPI_COMM_WORLD,&rank);``printf
    ("Number of tasks= %d My rank= %d\n", numtasks,rank);``/******* 执行一些工作 *******/``#ifdef
    USE_DBL``trainTest<double> ( argv[1], rank, numtasks );``#else``trainTest<float>
    ( argv[1], rank, numtasks);``#endif``MPI_Finalize();``return 0;``}`**setExamples()**
    方法已修改为在不同的 GPU 上分配内存。请参见[示例 10.11](#tb0060)，“修改后的 *nlpcaNM.cu* 中的 setExamples
    方法”：`void setExamples(thrust::host_vector<Real>& _h_data) {``nExamples = _h_data.size()/exLen;``//
    将数据复制到设备上``int rank;``MPI_Comm_rank(MPI_COMM_WORLD,&rank);``cudaSetDevice(rank%nGPUperNode);``d_data
    = thrust::device_vector<Real>(nExamples*exLen);``thrust::copy(_h_data.begin(),
    _h_data.end(), d_data.begin());``d_param = thrust::device_vector<Real>(nParam);``}`[示例
    10.12](#tb0065)，“修改后的 *nlpcaNM.cu* 中的 trainTest 方法”，展示了如何从磁盘加载数据：`#include <fstream>``template
    <typename Real>``void trainTest(char* filename, int rank, int numtasks)``{``ObjFunc<Real>
    testObj;``const int nParam = testObj.nParam;``cout << "nParam " << nParam << endl;``//
    读取测试数据``ifstream inFile (filename, ios::in | ios::binary);``// 将文件指针移到末尾``inFile.seekg(0,
    ios::end);``// 确定文件大小（以字节为单位）``ios::pos_type size = inFile.tellg();``// 为此任务分配
    Real 类型的值的数量``//（假设数量是 numtasks 的倍数）``int nExamplesPerGPU = (size/(sizeof(Real)*testObj.exLen))/numtasks;``thrust::host_vector<Real>
    h_data(nExamplesPerGPU*testObj.exLen);``// 将文件指针移到正确的位置``inFile.seekg(rank*h_data.size()*sizeof(Real),
    ios::beg);// 为此任务分配 Real 类型的值的数量``inFile.seekg(rank*h_data.size()*sizeof(Real));``//
    从文件中读取字节到 h_data``inFile.read((char*)&h_data[0], h_data.size()*sizeof(Real));``//
    关闭文件``inFile.close();``testObj.setExamples(h_data);``int nExamples = testObj.get_nExamples();``if(rank
    > 0) {``testObj.objFunc( NULL );``return;``}``...Nelder-Mead 代码在这里``int op=0;
    // 关闭从属进程``MPI_Bcast(&op, 1, MPI_INT, 0, MPI_COMM_WORLD);``}`在这个例子中，二进制输入流 **infile**
    被打开以读取文件 **filename** 中的数据。之后：■ 确定文件的大小。■ 每个 **numtasks** 进程根据文件大小和每个示例的数据数量被分配一个相等的任务量。通过假设示例数量是
    **numtasks** 的倍数，简化了逻辑。■ 分配了主机向量 **h_data**。■ 通过寻址操作移动到文件中的正确位置。■ 读取数据到 **h_data**
    中并传递给 **setExamples()** 以加载到设备上。■ 如[图 10.6](#f0035)所示，进程的 rank 会被测试：'
- en: '| ![B9780123884268000100/f10-06-9780123884268.jpg is missing](B9780123884268000100/f10-06-9780123884268.jpg)
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000100/f10-06-9780123884268.jpg is missing](B9780123884268000100/f10-06-9780123884268.jpg)
    |'
- en: '| **Figure 10.6**Logic separating the master from the slave processes. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **图 10.6**将主进程与从属进程分离的逻辑。 |'
- en: ■ If the rank is greater than zero, this process is a slave. The objective function
    is called with a NULL parameters argument and the training finishes when that
    method exits.■ If the process is a master, the Nelder-Mead optimization code is
    called.The objective function implements the logic for both the master and slave
    processes, as illustrated in [Figure 10.7](#f0040).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ■ 如果等级大于零，那么该进程是从属进程。目标函数会使用 NULL 参数被调用，训练会在该方法退出时结束。■ 如果进程是主进程，则调用 Nelder-Mead
    优化代码。目标函数实现了主从进程的逻辑，如[图 10.7](#f0040)所示。
- en: '| ![B9780123884268000100/f10-07-9780123884268.jpg is missing](B9780123884268000100/f10-07-9780123884268.jpg)
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000100/f10-07-9780123884268.jpg is missing](B9780123884268000100/f10-07-9780123884268.jpg)
    |'
- en: '| **Figure 10.7**MPI control flow. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| **图 10.7**MPI 控制流程。 |'
- en: The objective function ([Example 10.13](#tb0070)) belongs to a slave process
    when the MPI rank is greater than 0\. In this case, the slave state machine:1\.
    Allocates space in pinned memory for the parameters.2\. It waits for an operation
    code to be broadcast from the master.a. The master tells the slave that all work
    is done with a zero op code and that it should return.b. Otherwise, the slave
    will move to the next step.3\. The slave waits for the host to broadcast the parameters.4\.
    After the master broadcasts the parameters, the slave runs the objective function
    on its data and blocks waiting for the master to ask for the result.5\. After
    the master requests the reduction result the slave returns to wait for an op code
    in step 2\. This process continues until the master sends a 0 op code.The objective
    function ([Example 10.13](#tb0070)) belongs to the master process when the rank
    is 0\. In this case, the master:1\. Broadcasts a nonzero op code to let all the
    slaves know there is work to do.2\. Broadcasts the parameters to all the slaves.3\.
    Runs the objective function on its data.4\. Performs a reduction to sum the partial
    values from the slave processes.5\. Returns to continue running the optimization
    method.`Real objFunc(Real *p)``{``int rank,op;``Real sum=0.;``MPI_Comm_rank(MPI_COMM_WORLD,&rank);``cudaSetDevice(rank%nGPUperNode);``if(nExamples
    == 0) {``cerr << "data not set " << endl; exit(1);``}``CalcError getError(thrust::raw_pointer_cast(&d_data[0]),``thrust::raw_pointer_cast(&d_param[0]),``nInput,
    exLen);``if(rank > 0) { // slave objective function``Real *param;``cudaHostAlloc(&param,
    sizeof(Real)*nParam,cudaHostAllocPortable);``for(;;) { // loop until the master
    says I am done.``MPI_Bcast(&op, 1, MPI_INT, 0, MPI_COMM_WORLD);``if(op==0) {``cudaFreeHost(param);``return(0);``}``if(sizeof(Real)
    == sizeof(float))``MPI_Bcast(&param[0], nParam, MPI_FLOAT, 0, MPI_COMM_WORLD);``else``MPI_Bcast(&param[0],
    nParam, MPI_DOUBLE, 0, MPI_COMM_WORLD);``thrust::copy(param, param+nParam, d_param.begin());``Real
    mySum = thrust::transform_reduce(``thrust::counting_iterator<unsigned int>(0),``thrust::counting_iterator<unsigned
    int> (nExamples),``getError,``(Real) 0.,``thrust::plus<Real>());``if(sizeof(Real)
    == sizeof(float))``MPI_Reduce(&mySum, &sum, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD);``else``MPI_Reduce(&mySum,
    &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);``}``} else { // master process``double
    startTime=omp_get_wtime();``op=1;``MPI_Bcast(&op, 1, MPI_INT, 0, MPI_COMM_WORLD);``if(sizeof(Real)
    == sizeof(float))``MPI_Bcast(&p[0], nParam, MPI_FLOAT, 0, MPI_COMM_WORLD);``else``MPI_Bcast(&p[0],
    nParam, MPI_DOUBLE, 0, MPI_COMM_WORLD);``thrust::copy(p, p+nParam, d_param.begin());``Real
    mySum = thrust::transform_reduce(``thrust::counting_iterator<unsigned int>(0),``thrust::counting_iterator<unsigned
    int>(nExamples),``getError,``(Real) 0.,``thrust::plus<Real>());``if(sizeof(Real)
    == sizeof(float))``MPI_Reduce(&mySum, &sum, 1, MPI_FLOAT, MPI_SUM, 0, MPI_COMM_WORLD);``else``MPI_Reduce(&mySum,
    &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);``objFuncCallTime += (omp_get_wtime()
    - startTime);``objFuncCallCount++;``}``return(sum);``}`The application completes
    after the optimization method returns in the master process. The final line in
    **testTrain()** broadcasts a zero opcode to all the slaves to tell them to exit.
    The master process then returns to **main()** to shut down MPI normally.[Table
    10.3](#t0020) shows that running the MPI version of *nlpcaNM.cu* on two GPUs provides
    a nearly double increase in performance. No network was used as both GPUs were
    inside the same system. The example code from this chapter ran unchanged to deliver
    near-linear scalability to 500 GPUs on the Texas Advanced Computing Center (TACC)
    Longhorn supercomputer cluster. Many thanks to TACC for providing access to this
    machine. [⁸](#fn9015)⁷More current and extensive results reported on the wiki.⁸[http://gpucomputing.net/CUDAapplicationdesignanddevelopment](http://gpucomputing.net/CUDAapplicationdesignanddevelopment).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 10.3** Table of Results[⁷](#fn9010)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 10.3** 结果表[⁷](#fn9010)'
- en: '| Number of GPUs | Time per Objective Function | Speedup over one GPU |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GPU 数量 | 每个目标函数的时间 | 相比单个 GPU 的加速比 |'
- en: '| 1 | 0.0102073 |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.0102073 |  |'
- en: '| 2 | 0.00516948 | 1.98 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.00516948 | 1.98 |'
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: MPI and GPUDirect give CUDA programmers the ability to run high-performance
    applications that far exceed the capabilities of a single GPU or collection of
    GPUs inside a single system. Scalability and performance within a node are key
    metrics to evaluate distributed applications. Those applications that exhibit
    linear or near-linear scalability have the ability to run on the largest current
    and future machines. For example, the parallel mapping discussed in this chapter,
    plus [Chapter 2](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1) and [Chapter
    3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3), is now nearly 30 years
    old. At the time it was designed, a teraflop of computing power was decades beyond
    the reach of even the largest government research organizations. Today, it runs
    very effectively on GPU devices, clusters, and even the latest supercomputers.
    It will be interesting to see how many yottaFlops (10^(24) flops) will be available
    30 years from now. Meanwhile, this mapping provides a scalable solution for large
    data-mining and optimization tasks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: MPI 和 GPUDirect 使 CUDA 程序员能够运行远超单个 GPU 或单个系统中 GPU 集合能力的高性能应用程序。节点内的可扩展性和性能是评估分布式应用程序的关键指标。那些表现出线性或接近线性可扩展性的应用程序具有在当前及未来最大的机器上运行的能力。例如，本章讨论的并行映射，加上[第
    2 章](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1)和[第 3 章](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)，已经有近
    30 年历史。设计时，1 拓扑计算能力还远超即便是最大型的政府研究机构能达到的水平。如今，它能够在 GPU 设备、集群甚至最新的超级计算机上高效运行。未来
    30 年后，我们将看到多少 YottaFlops（10^(24) flops）可用。与此同时，这一映射为大规模数据挖掘和优化任务提供了可扩展的解决方案。
