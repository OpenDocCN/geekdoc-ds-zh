["```r\ndata_allbus <- read_dta(\"data/allbus2021_reduziert.dta\") \n # Here all cases are deleted, which have missing values on any of the variables. This is only recommended for reduced data sets with few variables, otherwise you would \"throw out\" observations based on non-relevant variables\ndata_allbus <- [na.omit](https://rdrr.io/r/stats/na.fail.html)(data_allbus)\n # We also delete three observations that indicated \"diverse\" for the gender variable, as this simplifies hypothesis generation\ndata_allbus<-[subset](https://rdrr.io/r/base/subset.html)(data_allbus, sex!=3)\n # We now recode variables\ndata_allbus$income <- data_allbus$incc\n data_allbus <- data_allbus %>% \n mutate(female =\n case_when(sex == 1 ~ 0,\n sex == 2 ~ 1))\n data_allbus <- data_allbus %>% \n mutate(east =\n case_when(eastwest == 1 ~ 0,\n eastwest == 2 ~ 1))\n data_allbus <- data_allbus %>% \n mutate(unemployed =\n case_when(dw18 == 1 ~ 1,\n dw18 == 2 ~ 0))\n # Summarize and overview data\n[summary](https://rdrr.io/r/base/summary.html)(data_allbus)\n```", "```r\n##     eastwest          sex             im19            im20           im21      \n##  Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.00   Min.   :1.000  \n##  1st Qu.:1.000   1st Qu.:1.000   1st Qu.:2.000   1st Qu.:2.00   1st Qu.:2.000  \n##  Median :1.000   Median :1.000   Median :3.000   Median :3.00   Median :3.000  \n##  Mean   :1.308   Mean   :1.486   Mean   :2.771   Mean   :2.82   Mean   :2.984  \n##  3rd Qu.:2.000   3rd Qu.:2.000   3rd Qu.:3.000   3rd Qu.:3.00   3rd Qu.:4.000  \n##  Max.   :2.000   Max.   :2.000   Max.   :4.000   Max.   :4.00   Max.   :4.000  \n##       dw18            incc          income         female      \n##  Min.   :1.000   Min.   : 1.0   Min.   : 1.0   Min.   :0.0000  \n##  1st Qu.:2.000   1st Qu.:13.0   1st Qu.:13.0   1st Qu.:0.0000  \n##  Median :2.000   Median :15.0   Median :15.0   Median :0.0000  \n##  Mean   :1.812   Mean   :15.3   Mean   :15.3   Mean   :0.4859  \n##  3rd Qu.:2.000   3rd Qu.:19.0   3rd Qu.:19.0   3rd Qu.:1.0000  \n##  Max.   :2.000   Max.   :26.0   Max.   :26.0   Max.   :1.0000  \n##       east          unemployed    \n##  Min.   :0.0000   Min.   :0.0000  \n##  1st Qu.:0.0000   1st Qu.:0.0000  \n##  Median :0.0000   Median :0.0000  \n##  Mean   :0.3079   Mean   :0.1884  \n##  3rd Qu.:1.0000   3rd Qu.:0.0000  \n##  Max.   :1.0000   Max.   :1.0000\n```", "```r\nkable([head](https://rdrr.io/r/utils/head.html)(data_allbus, 10), format = \"html\", booktabs = TRUE, caption = \"ALLBUS survey data\") %>%\n kable_paper() %>% #Font scheme of the table\n scroll_box(width = \"100%\", height = \"100%\") #Scrollable box\n```", "```r\n<p><br/></p> <details><summary> Solution: </summary><p><br/> Yes, if we were to recode all items, that would be just as possible. We would then have to label an index as “less justice” or “inequality,” since high values represent acceptance of social inequality. Recoding only a few variables is inadmissible, the index would no longer be valid.</p> <br/></details><hr/> <p><br/></p> <p>For building scales or indices of items, it is always a good idea to check whether there are substantial correlations between the items. There are also other ways to look at the reliability of an index, for example, by calculating Cronbach’s alpha. We go with correlations and also look at the distribution of the index variable.</p> <div class=\"sourceCode\" id=\"cb123\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\"># Do the items correlate sufficiently high for the index? (>.4 would be desirable)</span></span> <span><span class=\"co\"># Calculate correlation and output </span></span> <span><span class=\"va\">vars</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/c.html\">c</a></span><span class=\"op\">(</span><span class=\"st\">\"im19\"</span>, <span class=\"st\">\"im20\"</span>, <span class=\"st\">\"im21\"</span><span class=\"op\">)</span></span> <span><span class=\"va\">cor.vars</span> <span class=\"op\"><-</span> <span class=\"va\">data_allbus</span><span class=\"op\">[</span><span class=\"va\">vars</span><span class=\"op\">]</span></span> <span><span class=\"fu\">rcorr</span><span class=\"op\">(</span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/matrix.html\">as.matrix</a></span><span class=\"op\">(</span><span class=\"va\">cor.vars</span><span class=\"op\">)</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## im19 im20 im21 ## im19 1.00 0.46 0.33 ## im20 0.46 1.00 0.51 ## im21 0.33 0.51 1.00 ## ## n= 2595 ## ## ## P ## im19 im20 im21 ## im19 0 0 ## im20 0 0 ## im21 0 0</code></pre> <div class=\"sourceCode\" id=\"cb125\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\"># Index creation</span></span> <span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">morejustice</span> <span class=\"op\"><-</span> <span class=\"op\">(</span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">im19</span> <span class=\"op\">+</span> <span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">im20</span> <span class=\"op\">+</span> <span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">im21</span><span class=\"op\">)</span><span class=\"op\">/</span><span class=\"fl\">3</span></span> <span><span class=\"fu\"><a href=\"https://rdrr.io/r/graphics/hist.html\">hist</a></span><span class=\"op\">(</span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">morejustice</span><span class=\"op\">)</span></span></code></pre></div> <div class=\"inline-figure\"><img src=\"../Images/45d25584070a36632f1969fb727af07f.png\" width=\"90%\" height=\"90%\" style=\"display: block; margin: auto;\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/04-regression_files/figure-html/unnamed-chunk-4-1.png\"/></div> <p>The correlations show that at least most of the indicators are correlated with each other by r > .4\\. The histogram suggests that the variable values are approximately normally distributed.</p> <p><br/></p> <div id=\"the-foundations-of-bivariate-regression\" class=\"section level3\" number=\"5.1.1\"> <h3> <span class=\"header-section-number\">5.1.1</span> The foundations of (bivariate) regression </h3> <p>In the next step, we will work our way into bivariate regression analysis. In order to calculate the relationship between two variables, a so-called slope line is calculated using the least squares method. The following figure shows the basic idea. These are fictitious data points.</p> <p><img src=\"../Images/e8d9f1f59ed6591da82a07932888e11a.png\" id=\"id\" class=\"class\" style=\"width:90.0%;height:90.0%\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/picture/ols.png\"/> Source: <a href=\"https://seeing-theory.brown.edu/regression-analysis/index.html\" target=\"„_blank“\">https://seeing-theory.brown.edu/regression-analysis/index.html</a></p> <p>Basically, this method allows us to find a straight line that optimally travels through the dots of observations. The smaller the squares, the better the line fits the data. We have found the optimal solution if any different slope of the line would lead to an increase in the sum of squared deviations. Hence, this estimator is called <strong>ordinary least squares - the OLS estimator</strong>. If we had more than one variable in the regression model, another spatial dimension would be added (a z-axis) and the straight line would become a surface. With more than two independent variables, we are dealing with a multidimensional surface that is no longer depictable. The least squares method remains the same in such a case, except that the computation becomes more complex (done with a software algorithm).</p> <p>The software program computes a so-called regression coefficient for each independent variable (e.g., <span class=\"math inline\">\\(\\beta_1\\)</span> for the corresponding variable <span class=\"math inline\">\\(X_1\\)</span>). In the bivariate case (one independent variable), the regression coefficient represents the “steepness” of the slope in the following way:</p> <p><strong>If <span class=\"math inline\">\\(X_1\\)</span> increases by one unit, then <span class=\"math inline\">\\(Y\\)</span> increases or decreases by <span class=\"math inline\">\\(\\beta_1\\)</span> units (depending on the sign).</strong></p> <p><br/></p> <p>Let us now briefly remember the example from case study on univariate statistics, where we focused in the outlook on the empirical relationship between unemployment and crime rates in German counties. There we found a positive bivariate relationship between the two variables. The table below shows the coefficient estimate and the figure that depicts the positive association. If unemployment increases by one unit (here: 1 percentage point), then crime rates increase by 517.46 units (here: cases per 100T inhabitants), on average.</p> <div class=\"sourceCode\" id=\"cb126\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"va\">data_nrw</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://readxl.tidyverse.org/reference/read_excel.html\">read_excel</a></span><span class=\"op\">(</span><span class=\"st\">\"data/inkar_nrw.xls\"</span><span class=\"op\">)</span> </span> <span><span class=\"va\">model1</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/lm.html\">lm</a></span><span class=\"op\">(</span><span class=\"va\">crimerate</span> <span class=\"op\">~</span> <span class=\"fl\">1</span> <span class=\"op\">+</span> <span class=\"va\">unemp</span>, data <span class=\"op\">=</span> <span class=\"va\">data_nrw</span><span class=\"op\">)</span></span> <span><span class=\"fu\">stargazer</span><span class=\"op\">(</span><span class=\"va\">model1</span>, type <span class=\"op\">=</span> <span class=\"st\">\"text\"</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## ## =============================================== ## Dependent variable: ## --------------------------- ## crimerate ## ----------------------------------------------- ## unemp 517.460*** ## (69.412) ## ## Constant 2,398.594*** ## (543.248) ## ## ----------------------------------------------- ## Observations 53 ## R2 0.521 ## Adjusted R2 0.512 ## Residual Std. Error 1,240.129 (df = 51) ## F Statistic 55.575*** (df = 1; 51) ## =============================================== ## Note: *p<0.1; **p<0.05; ***p<0.01</code></pre> <div class=\"inline-figure\"><img src=\"../Images/c1721c8ad7244d1148aa7aa0ffc51edc.png\" id=\"id\" class=\"class\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/picture/interpret_en.png\"/></div> <p>We can also think about what would happen if the dots (i.e., the observed values on the two variables) were different. If the dots and the corresponding slope line were steeper (and thus the coefficient larger), then an increase in unemployment by one unit would lead to an even higher increase in crime rates. If the dots and the line indicated a flat pattern, then this would mean that there is no change in crime rates when unemployment increases by one unit. If the line (and the coefficient) was negative, then a one-unit increase in unemployment would be associated with a <em>decrease</em> (by <span class=\"math inline\">\\(\\beta_1\\)</span> units) in crime.</p> <p>In addition to the coefficients for the independent variables, the regression output displays the so-called constant (or intercept), which is abbreviated as <span class=\"math inline\">\\(a\\)</span>, <span class=\"math inline\">\\(\\alpha\\)</span>, or <span class=\"math inline\">\\(\\beta_0\\)</span>. It provides the predicted value for the outcome variable <span class=\"math inline\">\\(Y\\)</span> if all independent variables in the model have the value of 0\\. Usually, this information is not of much interest and thus ignored.</p> <p><br/></p> <hr/> <div class=\"alert alert-info\"> <strong>Question:</strong> What is the value of the constant in the unemployment-crime example (Model 1)? What information content does it carry? </div> <p><br/><br/> Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"/><p><br/></p> <details><summary> Solution: </summary><p><br/> The value of the constant in Model 1 is 2399\\. This indicates the average crime rate (Y) in a municipality with 0% unemployment. In our case, the constant is uninformative because we don’t have observations with 0% unemployment in our sample.</p> <br/></details><hr/> <p><br/></p> </div> <div id=\"bivariate-regression-of-income-and-attitudes-toward-justice\" class=\"section level3\" number=\"5.1.2\"> <h3> <span class=\"header-section-number\">5.1.2</span> Bivariate regression of income and attitudes toward justice </h3> <p>We now estimate a bivariate regression for the relationship between income and attitudes toward justice. Our working hypothesis <span class=\"math inline\">\\(H_A\\)</span> is that we assume a <em>negative</em> relationship - that is, individuals with a higher income advocate <em>less</em> strongly social justice compared to individuals with a lower income (e.g., because they fear a loss of income due to redistributive policies).</p> <p><em>Formally</em>, <span class=\"math inline\">\\(H_A\\)</span> can be represented with reference to either a correlation coefficient <span class=\"math inline\">\\(r<0\\)</span> or a regression coefficient <span class=\"math inline\">\\(\\beta_1<0\\)</span>. Accordingly, the null hypothesis would be <span class=\"math inline\">\\(\\beta_1\\geq0\\)</span>.</p> <p><br/></p> <hr/> <div class=\"alert alert-info\"> <strong>Question:</strong> Would it also make sense to formulate a different hypothesis <span class=\"math inline\">\\(H_A\\)</span>? </div> <p><br/><br/> Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"/><p><br/></p> <details><summary> Solution: </summary><p><br/> Yes: For example, the higher the income, the higher the preference for social justice (since wealthier people can afford it). Which hypothesis makes sense depends on theory and previous research.</p> <br/></details><hr/> <p><br/></p> <p>In a next step, the bivariate regression model is estimated. To do so, we use the index <code>morejustice</code> as the outcome and the variable <code>income</code> as an independent variable (or predictor), which measures income classes in 26 levels (e.g., 1 = under 200 EUR monthly net income; 14 = 1750 - 1999 EUR; 26 = 10000 EUR and more).</p> <div class=\"sourceCode\" id=\"cb128\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"va\">model_biv</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/lm.html\">lm</a></span><span class=\"op\">(</span><span class=\"va\">morejustice</span> <span class=\"op\">~</span> <span class=\"fl\">1</span> <span class=\"op\">+</span> <span class=\"va\">income</span>, data <span class=\"op\">=</span> <span class=\"va\">data_allbus</span><span class=\"op\">)</span> <span class=\"co\"># 1 refers to the constant or intercept </span></span> <span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/summary.html\">summary</a></span><span class=\"op\">(</span><span class=\"va\">model_biv</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## ## Call: ## lm(formula = morejustice ~ 1 + income, data = data_allbus) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.12251 -0.49371 0.06216 0.46936 1.33916 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 3.140981 0.045227 69.449 < 2e-16 *** ## income -0.018467 0.002833 -6.518 8.53e-11 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.6574 on 2593 degrees of freedom ## Multiple R-squared: 0.01612, Adjusted R-squared: 0.01574 ## F-statistic: 42.48 on 1 and 2593 DF, p-value: 8.532e-11</code></pre> <div class=\"sourceCode\" id=\"cb130\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"fu\">stargazer</span><span class=\"op\">(</span><span class=\"va\">model_biv</span>, type <span class=\"op\">=</span> <span class=\"st\">\"text\"</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## ## =============================================== ## Dependent variable: ## --------------------------- ## morejustice ## ----------------------------------------------- ## income -0.018*** ## (0.003) ## ## Constant 3.141*** ## (0.045) ## ## ----------------------------------------------- ## Observations 2,595 ## R2 0.016 ## Adjusted R2 0.016 ## Residual Std. Error 0.657 (df = 2593) ## F Statistic 42.484*** (df = 1; 2593) ## =============================================== ## Note: *p<0.1; **p<0.05; ***p<0.01</code></pre> <div class=\"alert alert-info\"> <p><strong>Task:</strong> Interpret the output from the regression model using the following heuristics.</p> <p>“The empirical relationship between income and attitudes toward justice is positive/negative.”</p> <p>“The relationship is/is not statistically significant.”</p> <p>Use the technically correct interpretation of p-values: “The probability of finding such a relationship in the population, …”</p> <p>“Thus, the null hypothesis can be rejected/not be rejected.”</p> <p>The detailed interpretation of the regression coefficient for income is: “…”</p> Interpret additionally the constant and R2 from the model output. What is the value of the constant in the unemployment-crime example (Model 1)? What information content does it carry? </div> <p><br/><br/></p> <p>Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"/><p><br/></p> <details><summary> Solution: </summary><p><br/> The empirical relationship between income and attitudes toward justice is negative.</p> <p>The found relationship is statistically significant (given a probability of error of 0.01 or 1% due to ***).</p> <p>The probability of finding such a relationship in the population, even though the null hypothesis is true, is less than 1%. Thus, the null hypothesis can be rejected.</p> <p>The detailed interpretation of the regression coefficient for income (b=-0.018) is as follows: If income increases by one unit (income category), then (variant 1:) attitudes toward justice decrease (on average) by 0.018 units (scale points) / (variant 2:) changes by -0.018 units.</p> <p>Constant: An interpretation is tedious because the income variable we used has no zero point. (If we would center the income variable at the mean value, then 0 indicated an average income, and the constant was the predicted level of justice attitudes for individuals with an average income.)</p> <p>R2 is a measure of model fit and indicates how well the included independent variables explain the outcome. It can be interpreted as the ratio of explained variance by the model to the total variance: R2=0.016 -> 1.6% of the total variance of the outcome variable can be explained by the predictor variables in the model. If R2 = 0.3, then the variance explained by the model would be 30%.</p> Note: The so-called <em>adjusted R2</em> is used to compare models with a different number of predictor variables. While it can no longer be interpreted as % of variance explained, it can be compared across models (better model with higher Adj-R2). <br/></details><hr/> <p><br/></p> </div> <div id=\"on-the-correspondence-of-correlation-coefficient-and-standardized-regression-coefficient-in-the-bivariate-case\" class=\"section level3\" number=\"5.1.3\"> <h3> <span class=\"header-section-number\">5.1.3</span> On the correspondence of correlation coefficient and standardized regression coefficient in the bivariate case </h3> <p>Pearson’s correlation coefficient is the standardized covariance between two variables and corresponds to the so-called <em>standardized</em> regression coefficient from bivariate regression. To obtain the standardized regression coefficient, we can either z-transform the variables (<span class=\"math inline\">\\(z=\\frac{(x-\\bar{x})}{S_x}\\)</span>) before they enter the regression model or transform the coefficient after estimation by multiplying the unstandardized regression coefficient by the standard deviation of <span class=\"math inline\">\\(X\\)</span> and dividing the result by the standard deviation of <span class=\"math inline\">\\(Y\\)</span>: <span class=\"math inline\">\\(\\beta_s=\\frac{(\\beta*S_y)}{S_x}\\)</span>. In the present case, we obtain the standard deviations and then calculate the standardized coefficient by hand, as well as use the <code>scale</code> function in R that automatically z-transforms variables and then estimates the regression model. We will check whether both procedures and the correlation correspond to each other.</p> <div class=\"sourceCode\" id=\"cb132\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\"># Standardized regression coefficient by hand</span></span> <span><span class=\"va\">sdx</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/sd.html\">sd</a></span><span class=\"op\">(</span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">income</span><span class=\"op\">)</span></span> <span><span class=\"va\">sdy</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/sd.html\">sd</a></span><span class=\"op\">(</span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">morejustice</span><span class=\"op\">)</span></span> <span/> <span><span class=\"va\">beta_s</span> <span class=\"op\"><-</span> <span class=\"op\">(</span><span class=\"op\">-</span><span class=\"fl\">0.018281</span><span class=\"op\">*</span><span class=\"va\">sdx</span><span class=\"op\">)</span><span class=\"op\">/</span><span class=\"va\">sdy</span></span> <span><span class=\"va\">beta_s</span></span></code></pre></div> <pre><code>## [1] -0.1256859</code></pre> <div class=\"sourceCode\" id=\"cb134\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\"># Standardized regression coefficient with model</span></span> <span><span class=\"va\">model_biv_s</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/lm.html\">lm</a></span><span class=\"op\">(</span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/scale.html\">scale</a></span><span class=\"op\">(</span><span class=\"va\">morejustice</span><span class=\"op\">)</span> <span class=\"op\">~</span> <span class=\"fl\">1</span> <span class=\"op\">+</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/scale.html\">scale</a></span><span class=\"op\">(</span><span class=\"va\">income</span><span class=\"op\">)</span>, data <span class=\"op\">=</span> <span class=\"va\">data_allbus</span><span class=\"op\">)</span></span> <span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/summary.html\">summary</a></span><span class=\"op\">(</span><span class=\"va\">model_biv_s</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## ## Call: ## lm(formula = scale(morejustice) ~ 1 + scale(income), data = data_allbus) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.2030 -0.7450 0.0938 0.7083 2.0209 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 2.553e-17 1.948e-02 0.000 1 ## scale(income) -1.270e-01 1.948e-02 -6.518 8.53e-11 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 0.9921 on 2593 degrees of freedom ## Multiple R-squared: 0.01612, Adjusted R-squared: 0.01574 ## F-statistic: 42.48 on 1 and 2593 DF, p-value: 8.532e-11</code></pre> <div class=\"sourceCode\" id=\"cb136\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\"># Calculate correlation</span></span> <span><span class=\"va\">vars</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/c.html\">c</a></span><span class=\"op\">(</span><span class=\"st\">\"income\"</span>, <span class=\"st\">\"morejustice\"</span><span class=\"op\">)</span></span> <span><span class=\"va\">cor.vars</span> <span class=\"op\"><-</span> <span class=\"va\">data_allbus</span><span class=\"op\">[</span><span class=\"va\">vars</span><span class=\"op\">]</span></span> <span><span class=\"fu\">rcorr</span><span class=\"op\">(</span><span class=\"fu\"><a href=\"https://rdrr.io/r/base/matrix.html\">as.matrix</a></span><span class=\"op\">(</span><span class=\"va\">cor.vars</span><span class=\"op\">)</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## income morejustice ## income 1.00 -0.13 ## morejustice -0.13 1.00 ## ## n= 2595 ## ## ## P ## income morejustice ## income 0 ## morejustice 0</code></pre> <blockquote> <font size=\"-1\"> <i><strong>Interpret the standardized regression coefficient as follows: If <span class=\"math inline\">\\(X\\)</span> (income) increases by one <em>standard deviation</em>, then Y increases/decreases by <span class=\"math inline\">\\(\\beta_s\\)</span>-<em>standard deviations</em>. Here, it is a decrease by 0.13 <em>standard deviations</em>.</strong> </i> </font> </blockquote> <p>Standardized regression coefficients are also widely used in multiple regression (i.e., regressions with more than one independent variable). The advantage is that we “free” the variables from their original scale of measurement and use “standard deviations” as a comparable scale of measurement. We can thus directly compare regression coefficients. If a coefficient is larger, then it also has a larger “effect” on the outcome variable. For example, an increase of one unit on a scale of income that is measured on a scale ranging between 0 and 10,000 would result in a smaller change in the outcome variable than if it was measured on a scale with only 12 categories. Using standardized versions of each variable and using these two versions in two separate regression models should lead to comparable results. As an example of competing explanations that are represented by two independent variables in the same model: If income has a standardized coefficient of -0.13 and age has a standardized coefficient of -0.05, then income has a “greater effect” (on the outcome variable) than age.</p> <p>Note that we typically report z-standardized coefficients only for metric (but not binary) predictor variables and that there are other routines for comparing coefficients, such as the min-max normalization where all variables have a minimum of 0 and a maximum of 1 (and ordinal or metric variables additional values in between 0 and 1).</p> <p><br/></p> </div> </div> <div id=\"multiple-regression\" class=\"section level2\" number=\"5.2\"> <h2> <span class=\"header-section-number\">5.2</span> Multiple Regression </h2> <div id=\"basic-idea-1\" class=\"section level3\" number=\"5.2.1\"> <h3> <span class=\"header-section-number\">5.2.1</span> Basic Idea </h3> <p>The goal of multiple regression is to explain the outcome variable with multiple independent variables. This is useful when we want to test different explanations for a phenomenon (e.g., attitudes toward justice). The multiple regression model produces <em>adjusted</em> estimates because the independent variables control for their mutual influences. Recall the example of unemployment and crime from the first case study, where we also included population density as an additional variable. The interpretation of the influence of unemployment (on crime) is in this case adjusted for the influence of population density. For unemployment, we obtained a coefficient estimate of <span class=\"math inline\">\\(\\beta_1=215.4\\)</span>. As a thought experiment: If we think of municipalities with the same population density and compare them with each other, then the municipalities with one percentage point more unemployment have on average a 215 higher crime rate per 100T inhabitants. The influence of population density on crime can be interpreted in a corresponding way - also here, the influence of the other factor (unemployment) is “held constant” or is controlled for.</p> <p>This mutual “holding constant” or “controlling” in the multiple regression model means that we can account for alternative explanations with additional variables. In the example of attitudes toward justice, we use the variables gender (<code>female</code> with the categories male = 0 and female = 1), whether or not respondents were unemployed in the last 10 years (<code>unemployed</code> with the categories not unemployed = 0 and been unemployed = 1), and whether respondents live in West or East Germany (<code>east</code> with the categories West Germany = 0 and East Germany = 1) as additional variables. Regarding the influence of gender, we would assume that women are more supportive of social justice than men due to their socialization and societal norms and/or because they earn less than men, on average, due to the gender pay gap. Hence, it is important to control for gender so that income does not partially “transport” a gender effect. Controlling for gender in the multiple regression model allows us to obtain a “net” income effect that is separated from the effect of gender that otherwise could confound the relationship between income and attitudes toward justice.</p> <p><br/></p> <hr/> <div class=\"alert alert-info\"> <p><strong>Question:</strong> How relate past or current unemployment and living in East Germany to both variables of interest income and attitudes toward justice?</p> </div> <p><br/><br/> Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"> Unemployment: East Germany:\n```", "```r\nIncome H_0: H_A: Female H_0: H_A: Unemployed H_0: H_A: East Germany H_0: H_A:\n```", "```r\nmodel_mult <- [lm](https://rdrr.io/r/stats/lm.html)(morejustice ~ 1 + income + female + unemployed + east, data = data_allbus)\n stargazer(model_biv, model_mult, type = \"text\")\n```", "```r\n## \n## =====================================================================\n##                                    Dependent variable:               \n##                     -------------------------------------------------\n##                                        morejustice                   \n##                               (1)                      (2)           \n## ---------------------------------------------------------------------\n## income                     -0.018***                -0.011***        \n##                             (0.003)                  (0.003)         \n##                                                                      \n## female                                               0.128***        \n##                                                      (0.028)         \n##                                                                      \n## unemployed                                           0.066**         \n##                                                      (0.033)         \n##                                                                      \n## east                                                 0.075***        \n##                                                      (0.028)         \n##                                                                      \n## Constant                    3.141***                 2.936***        \n##                             (0.045)                  (0.059)         \n##                                                                      \n## ---------------------------------------------------------------------\n## Observations                 2,595                    2,595          \n## R2                           0.016                    0.027          \n## Adjusted R2                  0.016                    0.026          \n## Residual Std. Error    0.657 (df = 2593)        0.654 (df = 2590)    \n## F Statistic         42.484*** (df = 1; 2593) 18.263*** (df = 4; 2590)\n## =====================================================================\n## Note:                                     *p<0.1; **p<0.05; ***p<0.01\n```", "```r\nIncome Female Unemployed East Germany\n```", "```r\n<p><br/></p> <details><summary> Solution: </summary><p><br/> In the multiple model (compared to the bivariate model), the coefficient of income (or the relationship between income and attitudes toward justice) is smaller. The underlying reason is that in the bivariate model, the coefficient partly represented unobserved factors, which is corrected in the multiple regression model (at least with respect to the variables included in the model).</p> In terms of model fit, the multiple model has a better fit to the data (since adj. R2 is higher). <br/></details><hr/> <p><br/></p> </div> </div> <div id=\"ols-assumptions-and-model-diagnostics-non-linear-relationships-between-variables-and-interactions\" class=\"section level2\" number=\"5.3\"> <h2> <span class=\"header-section-number\">5.3</span> OLS assumptions and model diagnostics, non-linear relationships between variables, and interactions </h2> <div id=\"ols-assumptions\" class=\"section level3\" number=\"5.3.1\"> <h3> <span class=\"header-section-number\">5.3.1</span> OLS assumptions </h3> <p>To obtain unbiased and efficient coefficient estimates from OLS regression models, several assumptions must be met. These assumptions refer to three areas.</p> <ol style=\"list-style-type: decimal\"> <li>The first area ensures that the <strong>estimation method</strong> works at all. The first assumption is that the <em>coefficients</em> are additive because otherwise, the coefficients would not be estimable by the least squares method. This assumption is trivial and usually holds. Note that some statistic books erroneously cite this assumption that the empirical relationship between the independent and dependent variables must be linear. This is not the case and multiple linear regression can easily address non-linear relationships by multiplying <em>variables</em> with each other (see also below). The second assumption relates to the collinearity of independent variables. This means that two independent variables must not be perfectly correlated. Otherwise, the influence of one variable would be completely displaced by the influence of the other variable, which inhibits the reliable estimation of coefficients. It is rather rare that variables are too highly correlated with each other, and if there are variables in the dataset that measure similar constructs (and are thus highly correlated), one would build an index from those variables, anyway.</li> </ol> <div class=\"inline-figure\"><img src=\"../Images/2f394b1012756315d6492d0d14168844.png\" id=\"id\" class=\"class\" style=\"width:100.0%;height:100.0%\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/picture/OLS1en.png\"/></div> <p><br/></p> <ol start=\"2\" style=\"list-style-type: decimal\"> <li>The second area concerns the unbiasedness of coefficient estimates. The underlying assumption is <strong>exogeneity of independent variables</strong>. If a variable is exogenous, this ensures that the coefficient of this variable reflects the actual relationship and does not represent something else. This is defined as the error term (i.e., the variance of the outcome left unexplained which represents possible unobserved factors that yield an influence on the outcome) is unrelated to the predictor variables. In a randomized experiment, the treatment variable is an exogenous predictor (per design due to the randomization) that is unrelated to unobserved third variables. In a model with observational data (e.g., from a survey), the exogeneity assumption requires the exclusion of alternative explanations by including relevant control variables. If we have all relevant control variables in the model, this assumption would be met. However, if there are unobserved variables that are correlated with the predictor and the outcome variable (which is the rule rather than the exception), we have a problem: the exogeneity assumption is not met and the regression coefficient is biased. We cannot empirically determine the extent of this bias: <em>There are no diagnostic tests for this assumption</em>. This means that we need to do good theory work to get clarity about causal relationships between variables and to include relevant control variables in the regression model. Another way to go is - as already mentioned - using experimental designs to rule out alternative explanations.</li> </ol> <div class=\"inline-figure\"><img src=\"../Images/d0558e3576cbff6a8864bd1a94efede6.png\" id=\"id\" class=\"class\" style=\"width:100.0%;height:100.0%\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/picture/OLS2en.png\"/></div> <p><br/></p> <ol start=\"3\" style=\"list-style-type: decimal\"> <li>The third area refers to the process of <strong>statistical inference</strong>. If the assumptions are violated, the standard errors and thus the test statistics (e.g., t/z statistics), and significance tests are biased. The unbiasedness of the coefficients (as in 2.) operates independently of this. The first assumption in this area relates to whether we work with a random sample. Without one, there is no basis for inferential statistics, although in practice inferential statistics are done even without a random sample. (Here one uses, for example, significance tests as information about how “reliable” or precise an empirical relationship is estimated.) <br/> The assumption of <strong>uncorrelated errors</strong> means that each observation must carry a unique piece of information, independent of the information content of other observations in the sample. If we use, for example, cross-national survey data from different countries, observations within countries are possibly similar to each other regarding the outcome variable. For panel data, where the same units are observed multiple times over time, the similarity between observations from the same units is labeled serial correlation. If such a similarity between observations is not addressed by specific modeling choices (e.g., using multilevel models) or the variables included in the regression model, standard errors are biased. There are formal tests of uncorrelated errors, such as the Durbin-Watson Test. <br/> The assumption that the <strong>errors must be normally distributed</strong> often serves as a rationale for assessing the distribution of variable values using a histogram and, if necessary, for transforming a variable so that its distribution is more “normal” (e.g., logarithmitize a right-skewed variable). This can be a useful strategy. At the same time, it is important to note that essentially the assumption refers to the distribution of errors (i.e., the outcome after accounting for variables in the model) and that with a sufficiently large number of observations, a violation becomes less problematic in terms of biased standard errors. <br/> Finally, the assumption of <strong>homoscedastic standard errors</strong> states that the distribution of errors must be the same (or constant) for different values of the independent variable(s). If the assumption is violated and <em>heteroscedasticity</em> is present, then the standard errors are biased. Typically (but by no means always), there is an underestimation of the standard errors, which in turn is associated with overly optimistic significance tests (and type I errors). That is, we find a statistically significant result (and reject the null hypothesis) even though it is not actually a significant result. As a remedy, so-called robust standard errors can be applied.</li> </ol> <div class=\"inline-figure\"><img src=\"../Images/dfafcd52d6d94a3ecd572a06bd984e72.png\" id=\"id\" class=\"class\" style=\"width:100.0%;height:100.0%\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/picture/OLS3en.png\"/></div> <p><br/></p> </div> <div id=\"model-diagnostics\" class=\"section level3\" number=\"5.3.2\"> <h3> <span class=\"header-section-number\">5.3.2</span> Model diagnostics </h3> <div id=\"collinearity-between-variables---vif\" class=\"section level4\" number=\"5.3.2.1\"> <h4> <span class=\"header-section-number\">5.3.2.1</span> Collinearity between variables -> VIF </h4> <p>To illustrate how to employ several existing tests of OLS assumptions, we use the above-estimated model <em>model_mult</em>.</p> <p>The variance inflation factor (VIF) provides information on whether variables in the model are excessively correlated with other variables (this would also be registered in a correlation matrix of the variables, and one should be cautious if variables correlate with each other by more than 0.85). Regarding the VIF, a VIF above 5 is already a bit suspicious and a value above 10 indicates strong collinearity.</p> <div class=\"sourceCode\" id=\"cb140\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\">##Regression diagnostics</span></span> <span><span class=\"fu\">vif</span><span class=\"op\">(</span><span class=\"va\">model_mult</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## income female unemployed east ## 1.215933 1.175482 1.037561 1.014182</code></pre> <p>The VIF is modest (< 5) for all variables in the model. We can conclude that there is no multicollinearity present.</p> </div> <div id=\"uncorrelated-errors\" class=\"section level4\" number=\"5.3.2.2\"> <h4> <span class=\"header-section-number\">5.3.2.2</span> Uncorrelated errors </h4> <p>Since we are using a one-stage random sample, this assumption is most likely met.</p> </div> <div id=\"normality-of-the-residuals\" class=\"section level4\" number=\"5.3.2.3\"> <h4> <span class=\"header-section-number\">5.3.2.3</span> Normality of the residuals </h4> <p>As a test for normality, we can (a) look at the distribution of residuals using a histogram, (b) plot a so-called Q-Q plot and interpret it, or (c) look at the distribution of variables in the model using a histogram (note that this can only give an indirect cue about the distribution of errors).</p> <div class=\"sourceCode\" id=\"cb142\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"va\">predict.model</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/fitted.values.html\">fitted</a></span><span class=\"op\">(</span><span class=\"va\">model_mult</span><span class=\"op\">)</span></span> <span><span class=\"va\">error.model</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/residuals.html\">residuals</a></span><span class=\"op\">(</span><span class=\"va\">model_mult</span><span class=\"op\">)</span></span> <span/> <span><span class=\"fu\"><a href=\"https://rdrr.io/r/graphics/hist.html\">hist</a></span><span class=\"op\">(</span><span class=\"va\">error.model</span><span class=\"op\">)</span></span></code></pre></div> <div class=\"inline-figure\"><img src=\"../Images/50507ff084a0d5a51e90e4c682db9c3e.png\" width=\"672\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/04-regression_files/figure-html/unnamed-chunk-10-1.png\"/></div> <div class=\"sourceCode\" id=\"cb143\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"fu\">qqPlot</span><span class=\"op\">(</span><span class=\"va\">error.model</span><span class=\"op\">)</span></span></code></pre></div> <div class=\"inline-figure\"><img src=\"../Images/b33209c19fb8ea51c5c2426bfecd2c90.png\" width=\"672\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/04-regression_files/figure-html/unnamed-chunk-11-1.png\"/></div> <pre><code>## [1] 741 1011</code></pre> <p>From the visual inspection, the distribution of errors looks similar to a normal distribution. The center of the distribution is about zero and the distribution is only slightly left-skewed.</p> <p>Regarding the Q-Q plot, the residual values shown should be on (or close to) the line as best as possible. A bit of “fraying” for high or low values is acceptable. We see slight deviations on the right, so there are a little too few values in the positive extreme range. Otherwise, it looks fine.</p> </div> <div id=\"homoscedasticity\" class=\"section level4\" number=\"5.3.2.4\"> <h4> <span class=\"header-section-number\">5.3.2.4</span> Homoscedasticity </h4> <p>Next, we inspect the assumption of homoscedasticity. We compare the predicted values from the model with the residual values. The more evenly the dots are scattered around the blue line, the more likely we can assume that the assumption is met.</p> <div class=\"sourceCode\" id=\"cb145\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"fu\">scatterplot</span><span class=\"op\">(</span><span class=\"va\">error.model</span><span class=\"op\">~</span><span class=\"va\">predict.model</span>, regLine<span class=\"op\">=</span><span class=\"cn\">TRUE</span>, smooth<span class=\"op\">=</span><span class=\"cn\">FALSE</span>, boxplots<span class=\"op\">=</span><span class=\"cn\">FALSE</span><span class=\"op\">)</span></span></code></pre></div> <div class=\"inline-figure\"><img src=\"../Images/b35c515a327026bbc8546e6a84cacffb.png\" width=\"672\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/04-regression_files/figure-html/unnamed-chunk-12-1.png\"/></div> <p>In our case, the errors scatter quite evenly. This means that the model fits similarly at low, medium, and high prediction values. To be certain, we could plot the (standardized) residuals for all independent variables in the model and their ranges of values. Furthermore, there is a statistical test for homoscedasticity: the <em>Breusch-Pagan test</em>. The null hypothesis here would be that homoscedasticity exists (i.e., the residuals are independent of the predictors). A non-significant result is therefore the desired result of the test.</p> <div class=\"sourceCode\" id=\"cb146\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"fu\">bptest</span><span class=\"op\">(</span><span class=\"va\">model_mult</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## ## studentized Breusch-Pagan test ## ## data: model_mult ## BP = 5.4228, df = 4, p-value = 0.2466</code></pre> <p>The non-significant result indicates that the assumption of homoscedasticity appears to be met. If this was not the case, we would have to work with robust standard errors.</p> <p><br/></p> </div> </div> <div id=\"non-linear-effects\" class=\"section level3\" number=\"5.3.3\"> <h3> <span class=\"header-section-number\">5.3.3</span> Non-linear effects </h3> <p>Including a non-linear variable relationship in a regression model can be based on several motives. One reason may be that we may notice from a scatter plot that the relationship between two variables is u-shaped or inversely u-shaped. Another reason might be that we can theoretically reason that the relationship is non-linear. A third motivation is to include non-linear relationships to reduce heteroskedasticity.</p> <p>Below, we test whether or not income has a non-linear effect on attitudes toward justice. Specifically, we assume that particularly middle-class individuals prefer justice and poor and rich individuals do so less. For rich people, the reason could be self-interest, while people with a low income might oppose redistributive policies because they are afraid of welfare state overuse by newcomers such as immigrants. The latter phenomenon has been discussed in the literature as “welfare chauvinism.”</p> <p>Let us first look at the relationship graphically.</p> <div class=\"sourceCode\" id=\"cb148\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\">#Create and print scatter plot</span></span> <span><span class=\"va\">sc1</span> <span class=\"op\"><-</span> <span class=\"fu\">ggplot</span><span class=\"op\">(</span>data<span class=\"op\">=</span><span class=\"va\">data_allbus</span>, <span class=\"fu\">aes</span><span class=\"op\">(</span><span class=\"va\">income</span>, <span class=\"va\">morejustice</span><span class=\"op\">)</span><span class=\"op\">)</span> <span class=\"op\">+</span></span> <span> <span class=\"fu\">geom_point</span><span class=\"op\">(</span><span class=\"op\">)</span> </span> <span><span class=\"va\">sc1</span></span></code></pre></div> <div class=\"inline-figure\"><img src=\"../Images/eb0bba4d687911f03f2afd69f7acd93c.png\" width=\"672\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/04-regression_files/figure-html/unnamed-chunk-14-1.png\"/></div> <div class=\"sourceCode\" id=\"cb149\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"va\">sc2</span> <span class=\"op\"><-</span> <span class=\"fu\">ggplot</span><span class=\"op\">(</span>data<span class=\"op\">=</span><span class=\"va\">data_allbus</span>, <span class=\"fu\">aes</span><span class=\"op\">(</span><span class=\"va\">income</span>, <span class=\"va\">morejustice</span><span class=\"op\">)</span><span class=\"op\">)</span> <span class=\"op\">+</span></span> <span> <span class=\"fu\">geom_jitter</span><span class=\"op\">(</span><span class=\"fu\">aes</span><span class=\"op\">(</span>income <span class=\"op\">=</span> <span class=\"va\">morejustice</span><span class=\"op\">)</span>, size <span class=\"op\">=</span> <span class=\"fl\">0.5</span><span class=\"op\">)</span> <span class=\"op\">+</span> <span class=\"fu\">geom_smooth</span><span class=\"op\">(</span><span class=\"op\">)</span></span> <span><span class=\"va\">sc2</span></span></code></pre></div> <div class=\"inline-figure\"><img src=\"../Images/8c1841aed948c302d7424b358d4b2e0f.png\" width=\"672\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/04-regression_files/figure-html/unnamed-chunk-15-1.png\"/></div> <p>In the first figure, one cannot see a pattern because many points are simply stacked on top of each other. To make the density of the points more visible, we can add a small random fluctuation. This makes the plot somewhat less accurate, but we can now see the pattern, which is also illustrated by the smoothed correlation function (and confidence intervals) represented by the blue line. There is some visual evidence for a non-linear relationship.</p> <p>We now test for the non-linear relationship in the regression model. To do so, we add the squared variable for income as an additional predictor in the multiple model. The idea here is that the effect of income now depends on the value of “another” variable, namely income itself. In other words, the effect of income on attitudes toward justice is now allowed to vary depending on where you are on the income scale.</p> <div class=\"sourceCode\" id=\"cb150\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">income_squared</span> <span class=\"op\"><-</span> <span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">income</span><span class=\"op\">*</span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">income</span></span> <span/> <span><span class=\"va\">model_nlin</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/lm.html\">lm</a></span><span class=\"op\">(</span><span class=\"va\">morejustice</span> <span class=\"op\">~</span> <span class=\"fl\">1</span> <span class=\"op\">+</span> <span class=\"va\">income</span> <span class=\"op\">+</span> <span class=\"va\">income_squared</span> <span class=\"op\">+</span> <span class=\"va\">female</span> <span class=\"op\">+</span> <span class=\"va\">unemployed</span> <span class=\"op\">+</span> <span class=\"va\">east</span>, data <span class=\"op\">=</span> <span class=\"va\">data_allbus</span><span class=\"op\">)</span></span> <span/> <span><span class=\"fu\">stargazer</span><span class=\"op\">(</span><span class=\"va\">model_biv</span>, <span class=\"va\">model_mult</span>, <span class=\"va\">model_nlin</span>, type <span class=\"op\">=</span> <span class=\"st\">\"text\"</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## ## ============================================================================================== ## Dependent variable: ## -------------------------------------------------------------------------- ## morejustice ## (1) (2) (3) ## ---------------------------------------------------------------------------------------------- ## income -0.018*** -0.011*** 0.048*** ## (0.003) (0.003) (0.013) ## ## income_squared -0.002*** ## (0.0004) ## ## female 0.128*** 0.125*** ## (0.028) (0.028) ## ## unemployed 0.066** 0.055* ## (0.033) (0.033) ## ## east 0.075*** 0.058** ## (0.028) (0.028) ## ## Constant 3.141*** 2.936*** 2.546*** ## (0.045) (0.059) (0.101) ## ## ---------------------------------------------------------------------------------------------- ## Observations 2,595 2,595 2,595 ## R2 0.016 0.027 0.036 ## Adjusted R2 0.016 0.026 0.034 ## Residual Std. Error 0.657 (df = 2593) 0.654 (df = 2590) 0.651 (df = 2589) ## F Statistic 42.484*** (df = 1; 2593) 18.263*** (df = 4; 2590) 19.189*** (df = 5; 2589) ## ============================================================================================== ## Note: *p<0.1; **p<0.05; ***p<0.01</code></pre> <p><br/></p> <blockquote> <p><font size=\"-1\"> <i><strong>Interpretation:</strong></i></font></p> <ul> <li><p>First, we look if the coefficient for the squared variable <code>income_squared</code> is statistically significant -> <strong>yes, so we can infer a non-linear relationship</strong></p></li> <li><p>Next, we look at the sign of the coefficient of <code>income</code> -> <strong>positive</strong></p></li> <li><p>And the sign of the coefficient of <code>income_squared</code> -> <strong>negative</strong></p></li> <li><p>To summarize: The effect of income on justice attitudes is <strong>positive</strong> and <strong>decreases</strong> with higher values of income.</p></li> </ul> </blockquote> <p><br/></p> <hr/> <div class=\"alert alert-info\"> <p><strong>Question:</strong> How would the effect be interpreted if both signs were negative?</p> </div> <p><br/><br/> Your answer:<br/></p> <textarea name=\"textarea\" cols=\"50\" rows=\"5\"/><p><br/></p> <details><summary> Solution: </summary><p><br/> The effect of income on attitudes toward inequality would be negative and decrease for high values of income.</p> <br/></details><hr/> <p><br/></p> </div> <div id=\"interaction-between-two-different-variables\" class=\"section level3\" number=\"5.3.4\"> <h3> <span class=\"header-section-number\">5.3.4</span> Interaction between two different variables </h3> <p>Another important tool in regression analysis is to model interactions between variables. The idea is that the effect of one variable on the outcome variable depends on the <em>value</em> of another variable in the model. Specifically, the question we ask here is whether the effect of gender on attitudes toward justice differs between West and East Germany. The assumption would be that in East Germany the difference between men and women in justice attitudes is less pronounced than in West Germany. On the one hand, this may be because men in East Germany are more supportive of justice (and thus have similarly high levels of support compared to women). On the other hand, it could be (but this is implausible) that women in the East have similarly “low” values as men.</p> <p>Let us look at this as an interaction in the regression model. For this, it is important to have the two variables <code>female</code> and <code>east</code> in the model, as well as a variable that is the product of both variables (interaction). This variable can be built beforehand or we build it “on the fly” in R by adding a “:” between two existing variables (“*” also works).</p> <div class=\"sourceCode\" id=\"cb152\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"co\"># Letting R know that the two nominal variables \"female\" and \"east\" are indeed nominal or \"factor variables\" is important for the graphical representation of the interaction below</span></span> <span/> <span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">female</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/factor.html\">factor</a></span><span class=\"op\">(</span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">female</span><span class=\"op\">)</span></span> <span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">east</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/base/factor.html\">factor</a></span><span class=\"op\">(</span><span class=\"va\">data_allbus</span><span class=\"op\">$</span><span class=\"va\">east</span><span class=\"op\">)</span></span> <span/> <span><span class=\"co\"># The interactions can be specified with \":\" or \"*\".</span></span> <span><span class=\"va\">model_interaktion</span> <span class=\"op\"><-</span> <span class=\"fu\"><a href=\"https://rdrr.io/r/stats/lm.html\">lm</a></span><span class=\"op\">(</span><span class=\"va\">morejustice</span> <span class=\"op\">~</span> <span class=\"fl\">1</span> <span class=\"op\">+</span> <span class=\"va\">income</span> <span class=\"op\">+</span> <span class=\"va\">unemployed</span> <span class=\"op\">+</span> <span class=\"va\">female</span> <span class=\"op\">+</span> <span class=\"va\">east</span> <span class=\"op\">+</span> <span class=\"va\">female</span> <span class=\"op\">:</span> <span class=\"va\">east</span> , data <span class=\"op\">=</span> <span class=\"va\">data_allbus</span><span class=\"op\">)</span></span> <span/> <span><span class=\"fu\">stargazer</span><span class=\"op\">(</span><span class=\"va\">model_interaktion</span>, type <span class=\"op\">=</span> <span class=\"st\">\"text\"</span><span class=\"op\">)</span></span></code></pre></div> <pre><code>## ## =============================================== ## Dependent variable: ## --------------------------- ## morejustice ## ----------------------------------------------- ## income -0.011*** ## (0.003) ## ## unemployed 0.065* ## (0.033) ## ## female1 0.163*** ## (0.033) ## ## east1 0.128*** ## (0.040) ## ## female1:east1 -0.106* ## (0.056) ## ## Constant 2.912*** ## (0.060) ## ## ----------------------------------------------- ## Observations 2,595 ## R2 0.029 ## Adjusted R2 0.027 ## Residual Std. Error 0.654 (df = 2589) ## F Statistic 15.351*** (df = 5; 2589) ## =============================================== ## Note: *p<0.1; **p<0.05; ***p<0.01</code></pre> <p><br/></p> <blockquote> <p><font size=\"-1\"> <i><strong>Interpretation:</strong></i></font></p> <ul> <li><p>First, we look to see if the coefficient for the interaction variable <code>female1:east1</code> is statistically significant -> <strong>Yes, so we can assume that the effect of <code>female</code> on <code>morejustice</code> depends on the value of <code>east</code>. In other words, the gender difference in justice attitudes differs between the two parts of Germany. (Note: It would also be correct to interpret the interaction effect symmetrically: The effect of <code>east</code> on <code>morejustice</code> depends on the value of <code>female</code>. For simplicity, we do not pursue this interpretation here.)</strong></p></li> <li><p>Then, we look at the sign and significance of the coefficient of <code>female</code> -> <strong>positive</strong> (= positive effect of <code>female</code> if <code>east</code> = 0, i.e., in West Germany).</p></li> <li><p>Then, we look at the sign of the coefficient of the interaction term <code>woman:east</code> -> <strong>negative</strong>.</p></li> <li><p>To summarize: The <strong>positive</strong> effect of being a woman on justice attitudes (= gender difference) is less strong (“increasingly <strong>negative</strong>”) at higher values of <code>east</code>, i.e., in East Germany.</p></li> </ul> </blockquote> <p><br/></p> <p>Below, we see a graphical representation of the package <code>interactions</code> (see <a href=\"https://interactions.jacob-long.com/index.html\" target=\"„_blank“\">https://interactions.jacob-long.com/index.html</a> for more examples).</p> <div class=\"sourceCode\" id=\"cb154\"><pre class=\"downlit sourceCode r\"> <code class=\"sourceCode R\"><span><span class=\"fu\">cat_plot</span><span class=\"op\">(</span><span class=\"va\">model_interaktion</span>, pred <span class=\"op\">=</span> <span class=\"va\">female</span>, modx <span class=\"op\">=</span> <span class=\"va\">east</span>, geom <span class=\"op\">=</span> <span class=\"st\">\"line\"</span>, point.shape <span class=\"op\">=</span> <span class=\"cn\">TRUE</span>,</span> <span> vary.lty <span class=\"op\">=</span> <span class=\"cn\">TRUE</span><span class=\"op\">)</span></span></code></pre></div> <div class=\"inline-figure\"><img src=\"../Images/889d2030ff1be873e39fb08fe725e5d2.png\" width=\"672\" data-original-src=\"https://bookdown.org/conradziller/introstatistics/04-regression_files/figure-html/unnamed-chunk-18-1.png\"/></div> <p>The blue line represents West Germany, and the orange line East Germany. The lines with the bars at the end are 95% confidence intervals, thus indirectly showing how “significant” the respective effects are estimated to be.</p> <p>Three things stand out:</p> <ul> <li><p>Women and men in East Germany are more supportive of social justice than women and men in West Germany (orange triangles compared to blue dots). The difference is statistically significant for men in West Germany (confidence intervals on the left side do not overlap). To know whether the overall level of support (women and men combined) is statistically significantly higher in the East, we have to go back to <strong>model_mult</strong> and look at the <em>average</em> relationship (<span class=\"math inline\">\\(\\beta_4 = 0.075, p < 0.01\\)</span>).</p></li> <li><p>Looking at the lines and thus the effect of gender separately for both parts of the country, the difference between women and men is statistically significant in West Germany (see coefficient for <code>female</code>, 0.163). In East Germany, the difference is not significant. (This can also be visually inferred by comparing the orange and the blue estimates to each other.)</p></li> <li><p>The effect of being a woman is larger in West Germany than in East Germany (the blue line is steeper than the orange line). This is represented by the coefficient for the interaction term.</p></li> </ul> <p><br/></p> <p><strong>We can now insert the regression coefficients into a regression formula:</strong></p> <p><span class=\"math inline\">\\(y = \\beta_0 + \\beta_1 * x_1 + \\beta_2 * x_2 + \\beta_3 * x_3 + \\beta_4 * x_4 + \\beta_5 * x_3*x_4 + e\\)</span></p> <p><span class=\"math inline\">\\(morejustice = 2.912 - 0.011 * income + 0.065 * unemployed + 0.163*female + 0.128*east - 0.106*female*east + e\\)</span></p> <p><br/></p> <p><strong>We can also derive the formulas for the so-called marginal effects by taking the first derivative and entering the variable values in a combination that informs us about the effects for the different groups.</strong></p> <ol style=\"list-style-type: decimal\"> <li>Marginal Effect for the variable <code>east</code> (first derivative) <span class=\"math inline\">\\(dy/d(east) = 0.128 – 0.106*female\\)</span> </li> </ol> <ul> <li><p>We can substitute 0 for the variable female, i.e., the effect of <code>east</code> for men: <span class=\"math inline\">\\(b(east=1, female=0) = 0.128 – 0.106 * 0 = 0.128\\)</span>.</p></li> <li><p>This refers to the difference between East and West in men (plot: distance between blue dot and orange triangle on the left).</p></li> <li><p>If we substitute 1 for the variable female: i.e., the effect of <code>east</code> for women: <span class=\"math inline\">\\(b(east=1, female=1) = 0.128 – 0.106 * 1 = 0.022\\)</span>.</p></li> <li><p>This refers to the small difference between East and West in women (distance between blue dot and orange triangle on the right side).</p></li> </ul> <p><br/></p> <ol start=\"2\" style=\"list-style-type: decimal\"> <li>Marginal Effect for the variable <code>female</code> (first derivative) <span class=\"math inline\">\\(dy/d(female) = 0.163 – 0.106*east\\)</span> </li> </ol> <ul> <li><p>We can substitute 0 for the variable east, i.e., the effect of <code>female</code> for West Germans: <span class=\"math inline\">\\(b(female=1, east=0) = 0.163 – 0.106 * 0 = 0.163\\)</span>.</p></li> <li><p>This refers to the difference between women and men in West Germany (plot: distance between both blue dots).</p></li> <li><p>If we substitute 1 for the variable east, i.e., the effect of <code>female</code> for East Germans: <span class=\"math inline\">\\(b(female=1, east=1) = 0.163 – 0.106 * 1 = 0.057\\)</span>.</p></li> <li><p>This refers to the difference between women and men in East Germany (distance between both orange triangles).</p></li> </ul> <p><br/></p> <ol start=\"3\" style=\"list-style-type: decimal\"> <li>The effect difference of women (different in both slopes) is represented by the coefficient for the interaction, i.e., –0.106.</li> </ol> <p><br/></p> <hr/> <p><strong>Thank you for engaging with the Introduction to Statistics and Data Analysis - A Case-Based Approach!</strong></p> <p><strong>Feedback to <a href=\"mailto:conrad.ziller@uni-due.de\" class=\"email\">conrad.ziller@uni-due.de</a> is much appreciated.</strong></p> <hr/> </div> </div> </div> <div class=\"chapter-nav\"> <div class=\"prev\"><a href=\"statistical-inference---case-study-satisfaction-with-government.html\"><span class=\"header-section-number\">4</span> Statistical Inference - Case Study Satisfaction with Government</a></div> <div class=\"empty\"/> </div> </body> </html>\n```"]