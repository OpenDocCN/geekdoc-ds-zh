<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>6.3. Modeling more complex dependencies 1: using conditional independence#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>6.3. Modeling more complex dependencies 1: using conditional independence#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap06_prob/03_joint/roch-mmids-prob-joint.html">https://mmids-textbook.github.io/chap06_prob/03_joint/roch-mmids-prob-joint.html</a></blockquote>

<p>In this section, we discuss the first of two standard techniques for constructing joint distributions from simpler building blocks: (1) imposing conditional independence relations and (2) marginalizing out an unobserved random variable. Combining them produces a large class of models known as probabilistic graphical models, which we do not discuss in generality. As before, we make our rigorous derivations in the finite support case, but these can be adapted to the continuous or hybrid cases.</p>
<section id="review-of-conditioning">
<h2><span class="section-number">6.3.1. </span>Review of conditioning<a class="headerlink" href="#review-of-conditioning" title="Link to this heading">#</a></h2>
<p>We first review the concept of conditioning, which generally plays a key role in probabilistic modeling and reasoning.</p>
<p><strong>Conditional probability</strong> We start with events. Throughout, we work on a fixed probability space <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, \P)\)</span>, which we assume is discrete, i.e., the number of elements in <span class="math notranslate nohighlight">\(\Omega\)</span> is countable.</p>
<p><strong>DEFINITION</strong> <strong>(Conditional Probability)</strong> <span class="math notranslate nohighlight">\(\idx{conditional probability}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> be two events with <span class="math notranslate nohighlight">\(\mathbb{P}[B] &gt; 0\)</span>. The conditional probability of <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(B\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
\P[A|B] = \frac{\P[A \cap B]}{\P[B]}.
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>The intuitive interpretation goes something like this: knowing that event <span class="math notranslate nohighlight">\(B\)</span> has occurred, the updated probability of observing <span class="math notranslate nohighlight">\(A\)</span> is the probability of its restriction to <span class="math notranslate nohighlight">\(B\)</span> properly normalized to reflect that outcomes outside <span class="math notranslate nohighlight">\(B\)</span> have updated probability <span class="math notranslate nohighlight">\(0\)</span>.</p>
<!--
This is illustrated next.

![Conditional probability](https://courses.cs.cornell.edu/cs2800/wiki/images/3/3b/Conditional-probability.svg)

([Source](https://courses.cs.cornell.edu/cs2800/wiki/index.php/File:Conditional-probability.svg))
--><p>Conditional probabilities generally behave like “unconditional” probabilities. (See for instance Problems 6.8, 7.1, and 7.9.)</p>
<p>Independence can be characterized in terms of conditional probability. In words, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent if conditioning on one of them having taken place does not change the probability of the other occurring.</p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> be two events of positive probability. Then <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent, which we will denote as <span class="math notranslate nohighlight">\(A \indep B\)</span>, if and only if <span class="math notranslate nohighlight">\(\P[A|B] = \P[A]\)</span> and <span class="math notranslate nohighlight">\(\P[B|A] = \P[B]\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent, then <span class="math notranslate nohighlight">\(\P[A \cap B] = \P[A] \P[B]\)</span> which implies</p>
<div class="math notranslate nohighlight">
\[
\P[A|B] = \frac{\P[A \cap B]}{\P[B]} = \frac{\P[A] \P[B]}{\P[B]} = \P[A].
\]</div>
<p>In the other direction,</p>
<div class="math notranslate nohighlight">
\[
\P[A] = \P[A|B] = \frac{\P[A \cap B]}{\P[B]}
\]</div>
<p>implies <span class="math notranslate nohighlight">\(\P[A \cap B] = \P[A]\P[B]\)</span> after rearranging. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>The conditional probability is often used in three fundamental ways, which we recall next. Proofs can be found in most probability textbooks.</p>
<ul class="simple">
<li><p><strong>Multiplication Rule:</strong> <span class="math notranslate nohighlight">\(\idx{multiplication rule}\xdi\)</span> For any collection of events <span class="math notranslate nohighlight">\(A_1,\ldots,A_r\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\P\left[\cap_{i=1}^r A_i\right]
= \prod_{i=1}^r \P\left[A_i \,\middle|\, \cap_{j=1}^{i-1} A_j \right].
\]</div>
<ul class="simple">
<li><p><strong>Law of Total Probability:</strong> <span class="math notranslate nohighlight">\(\idx{law of total probability}\xdi\)</span> For any event <span class="math notranslate nohighlight">\(B\)</span> and any <a class="reference external" href="https://en.wikipedia.org/wiki/Partition_of_a_set#Definition_and_Notation">partition</a><span class="math notranslate nohighlight">\(\idx{partition}\xdi\)</span> <span class="math notranslate nohighlight">\(A_1,\ldots,A_r\)</span> of <span class="math notranslate nohighlight">\(\Omega\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\P[B] 
= \sum_{i=1}^r \P[B|A_i] \P[A_i].
\]</div>
<ul class="simple">
<li><p><strong>Bayes’ Rule:</strong> <span class="math notranslate nohighlight">\(\idx{Bayes' yule}\xdi\)</span> For any events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> with positive probability,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\P[A|B]
= \frac{\P[B|A]\P[A]}{\P[B]}.
\]</div>
<p>It is implicit that all formulas above hold provided all conditional probabilities are well-defined.</p>
<p><strong>Conditioning on a random variable</strong> Conditional probabilities extend naturally to random variables. If <span class="math notranslate nohighlight">\(X\)</span> is a discrete random variable, we let <span class="math notranslate nohighlight">\(p_X\)</span> be its probability mass function and <span class="math notranslate nohighlight">\(\S_X\)</span> be its support, that is, the set of values where it has positive probability. Then we can for instance condition on the event <span class="math notranslate nohighlight">\(\{X = x\}\)</span> for any <span class="math notranslate nohighlight">\(x \in \S_X\)</span>.</p>
<p>We define next the conditional probability mass function.</p>
<p><strong>DEFINITION</strong> <strong>(Conditional Probability Mass Function)</strong> Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be discrete random variables with joint probability mass function <span class="math notranslate nohighlight">\(p_{X, Y}\)</span> and marginals <span class="math notranslate nohighlight">\(p_X\)</span> and <span class="math notranslate nohighlight">\(p_Y\)</span>. The conditional probability mass function<span class="math notranslate nohighlight">\(\idx{conditional probability mass function}\xdi\)</span> of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(Y\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
p_{X|Y}(x|y) := P[X=x|Y=y]  = \frac{p_{X,Y}(x,y)}{p_Y(y)}
\]</div>
<p>which is defined for all <span class="math notranslate nohighlight">\(x \in \S_X\)</span> and <span class="math notranslate nohighlight">\(y \in \S_Y\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>The conditional expectation can then be defined in a natural way as the expectation over the conditional probability mass function.</p>
<p><strong>DEFINITION</strong> <strong>(Conditional Expectation)</strong> <span class="math notranslate nohighlight">\(\idx{conditional expectation}\xdi\)</span> Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be discrete random variables where <span class="math notranslate nohighlight">\(X\)</span> takes real values and has a finite mean. The conditional expectation of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(Y = y\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\E[X|Y=y] = \sum_{x \in \S_X} x\, p_{X|Y}(x|y). 
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>More generally, for a function <span class="math notranslate nohighlight">\(f\)</span> over the range of <span class="math notranslate nohighlight">\(X\)</span>, we can define</p>
<div class="math notranslate nohighlight">
\[
\E[f(X)|Y=y] = \sum_{x \in \S_X} f(x)\, p_{X|Y}(x|y). 
\]</div>
<p>We mention one useful formula: the <em>Law of Total Expectation</em><span class="math notranslate nohighlight">\(\idx{law of total expectation}\xdi\)</span>, the expectation version of the <em>Law of Total Probability</em>. It reads</p>
<div class="math notranslate nohighlight">
\[
\E[f(X)] = \sum_{y \in \S_Y} \E[f(X)|Y=y] \,p_Y(y).
\]</div>
<p><strong>Conditional expectation as least-squares estimator</strong> Thinking of <span class="math notranslate nohighlight">\(\E[X|Y=y]\)</span> as a function of <span class="math notranslate nohighlight">\(y\)</span> leads to a fundamental characterization of the conditional expectation.</p>
<p><strong>THEOREM</strong> Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be discrete random variables where <span class="math notranslate nohighlight">\(X\)</span> takes real values and has a finite variance. Then the conditional expectation <span class="math notranslate nohighlight">\(h(y) = \E[X|Y=y]\)</span> minimizes the least squares criterion</p>
<div class="math notranslate nohighlight">
\[
\min_{h} \E\left[(X - h(Y))^2\right]
\]</div>
<p>where the minimum is over all real-valued functions of <span class="math notranslate nohighlight">\(y\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> Think of <span class="math notranslate nohighlight">\(h(y)\)</span> as a vector <span class="math notranslate nohighlight">\(\mathbf{h} = (h_y)_{y \in \S_Y}\)</span>, indexed by <span class="math notranslate nohighlight">\(\S_Y\)</span> (which is countable by assumption), with <span class="math notranslate nohighlight">\(h_y = h(y) \in \mathbb{R}\)</span>. Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal{L}(\mathbf{h})
&amp;=\E\left[(X - h(Y))^2\right]\\
&amp;= \sum_{x\in \S_X} \sum_{y \in \S_Y} (x - h_y)^2 p_{X,Y}(x,y)\\
&amp;= \sum_{y \in \S_Y} \left[\sum_{x\in \S_X}  (x - h_y)^2 p_{X,Y}(x,y)\right].
\end{align*}\]</div>
<p>Expanding the sum in the square brackets (which we denote by <span class="math notranslate nohighlight">\(q_y\)</span> and think of as a function of <span class="math notranslate nohighlight">\(h_y\)</span>) gives</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
q_y(h_y)
&amp;:= \sum_{x\in \S_X}  (x - h_y)^2 p_{X,Y}(x,y)\\
&amp;= \sum_{x\in \S_X}  [x^2 - 2 x h_y + h_y^2] \,p_{X,Y}(x,y)\\
&amp;= \left\{\sum_{x\in \S_X} x^2 p_{X,Y}(x,y)\right\}
+ \left\{- 2 \sum_{x\in \S_X} x p_{X,Y}(x,y)\right\} h_y
+ \left\{p_Y(y)\right\} h_y^2.
\end{align*}\]</div>
<p>By the <em>Miminizing a Quadratic Function Lemma</em>, the unique global minimum of <span class="math notranslate nohighlight">\(q_y(h_y)\)</span> - provided <span class="math notranslate nohighlight">\(p_Y(y) &gt; 0\)</span> - is attained at</p>
<div class="math notranslate nohighlight">
\[
h_y 
= - \frac{- 2 \sum_{x\in \S_X} x p_{X,Y}(x,y)}{2 p_Y(y)}.
\]</div>
<p>After rearranging, we get</p>
<div class="math notranslate nohighlight">
\[
h_y 
= \sum_{x\in \S_X}  x \frac{p_{X,Y}(x,y)}{p_Y(y)}
= \sum_{x\in \S_X}  x p_{X|Y}(x|y)
= \E[X|Y=y]
\]</div>
<p>as claimed. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Conditional independence</strong> Next, we discuss conditional independence. We begin with the formal definition.</p>
<p><strong>DEFINITION</strong> <strong>(Conditional Independence)</strong> <span class="math notranslate nohighlight">\(\idx{conditional independence}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A, B, C\)</span> be events such that <span class="math notranslate nohighlight">\(\P[C] &gt; 0\)</span>. Then <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(C\)</span>, denoted <span class="math notranslate nohighlight">\(A \indep B | C\)</span>, if</p>
<div class="math notranslate nohighlight">
\[
\P[A \cap B| C] = \P[A|C] \,\P[B|C].
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>In words, quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Conditional_independence">Wikipedia</a>:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(C\)</span> if and only if, given knowledge that <span class="math notranslate nohighlight">\(C\)</span> occurs, knowledge of whether <span class="math notranslate nohighlight">\(A\)</span> occurs provides no information on the likelihood of <span class="math notranslate nohighlight">\(B\)</span> occurring, and knowledge of whether <span class="math notranslate nohighlight">\(B\)</span> occurs provides no information on the likelihood of <span class="math notranslate nohighlight">\(A\)</span> occurring.</p>
</div></blockquote>
<p>In general, conditionally independent events are not (unconditionally) independent.</p>
<p><strong>EXAMPLE:</strong> Imagine I have two six-sided dice. Die 1 has faces <span class="math notranslate nohighlight">\(\{1,3,5,7,9,11\}\)</span> and die 2 has faces <span class="math notranslate nohighlight">\(\{2, 4, 6, 8, 10, 12\}\)</span>. Suppose I perform the following experiment: I pick one of the two dice uniformly at random, and then I roll that die twice. Let <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> be the outcomes of the rolls. Consider the events <span class="math notranslate nohighlight">\(A = \{X_1 = 1\}\)</span>, <span class="math notranslate nohighlight">\(B = \{X_2 = 2\}\)</span>, and <span class="math notranslate nohighlight">\(C = \{\text{die 1 is picked}\}\)</span>. The events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are clearly dependent: if <span class="math notranslate nohighlight">\(A\)</span> occurs, then I know that die 1 was picked, and hence <span class="math notranslate nohighlight">\(B\)</span> cannot occur. Knowledge of one event provides information about the likelihood of the other event occurring. Formally, by the law of total probability,</p>
<div class="math notranslate nohighlight">
\[
\P[A] 
= \P[A|C]\P[C] + \P[A|C^c]\P[C^c]
= \frac{1}{6}\frac{1}{2} + 0 \frac{1}{2}
= \frac{1}{12}.
\]</div>
<p>Similarly <span class="math notranslate nohighlight">\(\P[B] = \frac{1}{12}\)</span>. Yet <span class="math notranslate nohighlight">\(\P[A \cap B] = 0 \neq \frac{1}{12} \frac{1}{12}\)</span>.</p>
<p>On the other hand, we claim that <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(C\)</span>. Again this is intuitively clear: once I pick a die, the two rolls are independent. For a given die choice, knowledge of one roll provides no information about the likelihood of the other roll. Note that the phrase “for a given die choice” is critical in the last statement. Formally, by our experiment, we have <span class="math notranslate nohighlight">\(\P[A|C] = 1/6\)</span>, <span class="math notranslate nohighlight">\(\P[B|C] = 0\)</span> and <span class="math notranslate nohighlight">\(\P[A \cap B|C] = 0\)</span>. So indeed</p>
<div class="math notranslate nohighlight">
\[
\P[A \cap B| C] = \P[A|C] \,\P[B|C]
\]</div>
<p>as claimed. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Conditional independence is naturally extended to random vectors.</p>
<p><strong>DEFINITION</strong> <strong>(Conditional Independence of Random Vectors)</strong> Let <span class="math notranslate nohighlight">\(\bX, \bY, \bW\)</span> be discrete random vectors. Then <span class="math notranslate nohighlight">\(\bX\)</span> and <span class="math notranslate nohighlight">\(\bY\)</span> are said to be conditionally independent given <span class="math notranslate nohighlight">\(\bW\)</span>, denoted <span class="math notranslate nohighlight">\(\bX \indep \bY | \bW\)</span>, if for all <span class="math notranslate nohighlight">\(\bx \in \S_\bX\)</span>, <span class="math notranslate nohighlight">\(\by \in \S_\bY\)</span> and <span class="math notranslate nohighlight">\(\bw \in \S_\bW\)</span></p>
<div class="math notranslate nohighlight">
\[
\P[\bX = \bx, \bY = \by|\bW = \bw] 
= \P[\bX = \bx |\bW = \bw] 
\,\P[\bY = \by|\bW = \bw].
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>An important consequence is that we can drop the conditioning by the independent variable.</p>
<p><strong>LEMMA</strong> <strong>(Role of Independence)</strong> <span class="math notranslate nohighlight">\(\idx{role of independence lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\bX, \bY, \bW\)</span> be discrete random vectors such that <span class="math notranslate nohighlight">\(\bX \indep \bY | \bW\)</span>. For all <span class="math notranslate nohighlight">\(\bx \in \S_\bX\)</span>, <span class="math notranslate nohighlight">\(\by \in \S_\bY\)</span> and <span class="math notranslate nohighlight">\(\bw \in \S_\bW\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\P[\bX = \bx | \bY=\by, \bW=\bw]
= \P[\bX = \bx | \bW = \bw].
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> In a previous exercise, we showed that <span class="math notranslate nohighlight">\(A \indep B | C\)</span> implies <span class="math notranslate nohighlight">\(\P[A | B\cap C] = \P[A | C]\)</span>. That implies the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The concept of conditional independence is closely related to the concept of d-separation in probabilistic graphical models. Ask your favorite AI chatbot to explain d-separation. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
</section>
<section id="the-basic-configurations">
<h2><span class="section-number">6.3.2. </span>The basic configurations<a class="headerlink" href="#the-basic-configurations" title="Link to this heading">#</a></h2>
<p>A powerful approach for constructing complex probability distributions is the use of conditional independence. The case of three random variables exemplifies key probabilistic relationships. By the product rule, we can write</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y, Z=z]
= \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x, Y=y].
\]</div>
<p>This is conveniently represented through a digraph where the vertices are the variables. Recall that an arrow <span class="math notranslate nohighlight">\((i,j)\)</span>, from <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>, indicates that <span class="math notranslate nohighlight">\(i\)</span> is a parent of <span class="math notranslate nohighlight">\(j\)</span> and that <span class="math notranslate nohighlight">\(j\)</span> is a child of <span class="math notranslate nohighlight">\(i\)</span>. Let <span class="math notranslate nohighlight">\(\pa(i)\)</span> be the set of parents of <span class="math notranslate nohighlight">\(i\)</span>. The digraph <span class="math notranslate nohighlight">\(G = (V, E)\)</span> below encodes the following sampling scheme, referred as ancestral sampling:</p>
<ol class="arabic simple">
<li><p>First we pick <span class="math notranslate nohighlight">\(X\)</span> according to its marginal <span class="math notranslate nohighlight">\(\P[X=x]\)</span>. Note that <span class="math notranslate nohighlight">\(X\)</span> has no parent in <span class="math notranslate nohighlight">\(G\)</span>.</p></li>
<li><p>Second we pick <span class="math notranslate nohighlight">\(Y\)</span> according to the conditional probability distribution (CPD) <span class="math notranslate nohighlight">\(\P[Y=y|X=x]\)</span>. Note that <span class="math notranslate nohighlight">\(X\)</span> is the only parent of <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p>Finally we pick <span class="math notranslate nohighlight">\(Z\)</span> according to the CPD <span class="math notranslate nohighlight">\(\P[Z=z|X=x, Y=y]\)</span>. Note that the parents of <span class="math notranslate nohighlight">\(Z\)</span> are <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
</ol>
<p><img alt="The full case" src="../Images/b73cb01c1b1f5d32087884467af21283.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_full1_networkx.png"/></p>
<p>The graph above is acyclic, that is, it has no directed cycle. The variables <span class="math notranslate nohighlight">\(X, Y, Z\)</span> are in <a class="reference external" href="https://en.wikipedia.org/wiki/Topological_sorting">topological order</a><span class="math notranslate nohighlight">\(\idx{topological order}\xdi\)</span>, that is, all edges <span class="math notranslate nohighlight">\((i,j)\)</span> are such that <span class="math notranslate nohighlight">\(i\)</span> comes before <span class="math notranslate nohighlight">\(j\)</span> in that order.</p>
<p>The same joint distribution can be represented by a different digraph if the product rule is used in a different order. For instance,</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y, Z=z]
= \P[Z=z] \,\P[Y=y|Z=z] \,\P[X=x | Z=z, Y=y]
\]</div>
<p>is represented by the following digraph. A topological order this time is <span class="math notranslate nohighlight">\(Z, Y, X\)</span>.</p>
<p><img alt="Another full case" src="../Images/ed39c216e1efdc59d602be7c9e2c744a.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_full2_networkx.png"/></p>
<p><strong>The fork</strong> <span class="math notranslate nohighlight">\(\idx{fork}\xdi\)</span> Removing edges in the first graph above encodes conditional independence relations. For instance, removing the edge from <span class="math notranslate nohighlight">\(Y\)</span> to <span class="math notranslate nohighlight">\(Z\)</span> gives the following graph, known as a fork. We denote this configuration as <span class="math notranslate nohighlight">\(Y \leftarrow X \rightarrow Z\)</span>.</p>
<p><img alt="The fork" src="../Images/77cb78740ee4952c17900d2626c7ad2a.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_fork_networkx.png"/></p>
<p>The joint distribution simplifies as follows:</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y, Z=z]
= \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x].
\]</div>
<p>So, in this case, what has changed is that the CPD of <span class="math notranslate nohighlight">\(Z\)</span> does not depend on the value of <span class="math notranslate nohighlight">\(Y\)</span>. From the <em>Role of Independence</em> lemma, this corresponds to assuming the conditional independence <span class="math notranslate nohighlight">\(Z \indep Y|X\)</span>. Indeed, we can check that claim directly from the joint distribution</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[Y= y, Z=z|X=x]
&amp;= \frac{\P[X=x, Y= y, Z=z]}{\P[X=x]}\\
&amp;= \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x]}{\P[X=x]}\\
&amp;= \P[Y=y|X=x] \,\P[Z=z | X=x]
\end{align*}\]</div>
<p>as claimed.</p>
<p><strong>The chain</strong> <span class="math notranslate nohighlight">\(\idx{chain}\xdi\)</span> Removing the edge from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Z\)</span> gives the following graph, known as a chain (or pipe). We denote this configuration as <span class="math notranslate nohighlight">\(X \rightarrow Y \rightarrow Z\)</span>.</p>
<p><img alt="The chain" src="../Images/5bfd7c9b4911ca0e5931761d25c8457e.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_chain_networkx.png"/></p>
<p>The joint distribution simplifies as follows:</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y, Z=z]
= \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y].
\]</div>
<p>In this case, what has changed is that the CPD of <span class="math notranslate nohighlight">\(Z\)</span> does not depend on the value of <span class="math notranslate nohighlight">\(X\)</span>. Compare that to the fork. The corresponding conditional independence relation is <span class="math notranslate nohighlight">\(Z \indep X|Y\)</span>. Indeed, we can check that claim directly</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[X= x, Z=z|Y=y]
&amp;= \frac{\P[X=x, Y= y, Z=z]}{\P[Y=y]}\\
&amp;= \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]}{\P[Y=y]}
\end{align*}\]</div>
<p>Now we have to use <em>Bayes’ Rule</em> to get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;= \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]}{\P[Y=y]}\\
&amp;= \frac{\P[Y=y|X=x]\,\P[X=x]}{\P[Y=y]} \P[Z=z | Y=y]\\
&amp;= \P[X=x|Y=y] \,\P[Z=z | Y=y]
\end{align*}\]</div>
<p>as claimed.</p>
<p>For any <span class="math notranslate nohighlight">\(x, y, z\)</span> where the joint probability is positive, we can re-write</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P[X=x, Y=y, Z=z]\\
&amp;= \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]\\
&amp;= \P[Y=y] \,\P[X=x|Y=y] \,\P[Z=z | Y=y],
\end{align*}\]</div>
<p>where we used that</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y]
= \P[X=x] \,\P[Y=y|X=x]
= \P[Y=y] \,\P[X=x|Y=y]
\]</div>
<p>by definition of the conditional probability. In other words, we have shown that the chain <span class="math notranslate nohighlight">\(X \rightarrow Y \rightarrow Z\)</span> is in fact equivalent to the fork <span class="math notranslate nohighlight">\(X \leftarrow Y \rightarrow Z\)</span>. In particular, they both correspond to assuming the conditional independence relation <span class="math notranslate nohighlight">\(Z \indep X|Y\)</span>, although they capture a different way to sample the joint distribution.</p>
<p><strong>The collider</strong> <span class="math notranslate nohighlight">\(\idx{collider}\xdi\)</span> Removing the edge from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span> gives the following graph, known as a collider. We denote this configuration as <span class="math notranslate nohighlight">\(X \rightarrow Z \leftarrow Y\)</span>.</p>
<p><img alt="The collider" src="../Images/a4600775ea580d4809bd8b11895a447d.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_collider_networkx.png"/></p>
<p>The joint distribution simplifies as follows:</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y, Z=z]
= \P[X=x] \,\P[Y=y] \,\P[Z=z | X=x, Y=y].
\]</div>
<p>In this case, what has changed is that the CPD of <span class="math notranslate nohighlight">\(Y\)</span> does not depend on the value of <span class="math notranslate nohighlight">\(X\)</span>. Compare that to the fork and the chain. This time we have <span class="math notranslate nohighlight">\(X \indep Y\)</span>. Indeed, we can check that claim directly</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[X= x, Y=y]
&amp;= \sum_{z \in \S_z} \P[X=x, Y=y, Z=z]\\
&amp;=  \sum_{z \in \S_z} \P[X=x] \,\P[Y=y] \,\P[Z=z | X=x, Y=y]\\
&amp;= \P[X=x] \,\P[Y=y]
\end{align*}\]</div>
<p>as claimed. In particular, the collider cannot be reframed as a chain or fork as its underlying assumption is stronger.</p>
<p>Perhaps counter-intuitively, conditioning on <span class="math notranslate nohighlight">\(Z\)</span> makes <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> dependent in general. This is known as explaining away or Berkson’s Paradox.</p>
</section>
<section id="example-naive-bayes">
<h2><span class="section-number">6.3.3. </span>Example: Naive Bayes<a class="headerlink" href="#example-naive-bayes" title="Link to this heading">#</a></h2>
<p>The model-based justification we gave for logistic regression in the  subsection on generalized linear models used a so-called <a class="reference external" href="https://en.wikipedia.org/wiki/Discriminative_model">discriminative approach</a><span class="math notranslate nohighlight">\(\idx{discriminative model}\xdi\)</span>, where the conditional distribution of the target <span class="math notranslate nohighlight">\(y\)</span> given the features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is specified – but not the full distribution of the data <span class="math notranslate nohighlight">\((\mathbf{x}, y)\)</span>. Here we give an example of the <a class="reference external" href="https://en.wikipedia.org/wiki/Generative_model">generative approach</a><span class="math notranslate nohighlight">\(\idx{generative model}\xdi\)</span>, which models the full distribution. For a discussion of the benefits and drawbacks of each approach, see for example <a class="reference external" href="https://en.wikipedia.org/wiki/Discriminative_model#Contrast_with_generative_model">here</a>.</p>
<p>The Naive Bayes<span class="math notranslate nohighlight">\(\idx{Naive Bayes}\xdi\)</span> model is a simple discrete model for supervised learning. It is useful for document classification for instance, and we will use that terminology here to be concrete. We assume that a document has a single topic <span class="math notranslate nohighlight">\(C\)</span> from a list <span class="math notranslate nohighlight">\(\mathcal{C} = \{1, \ldots, K\}\)</span> with probability distribution <span class="math notranslate nohighlight">\(\pi_k = \P[C = k]\)</span>. There is a vocabulary of size <span class="math notranslate nohighlight">\(M\)</span> and we record the presence or absence of a word <span class="math notranslate nohighlight">\(m\)</span> in the document with a Bernoulli variable <span class="math notranslate nohighlight">\(X_m \in \{0,1\}\)</span>, where <span class="math notranslate nohighlight">\(p_{k,m} = \P[X_m = 1|C = k]\)</span>. We denote by <span class="math notranslate nohighlight">\(\bX = (X_1, \ldots, X_M)\)</span> the corresponding vector.</p>
<p>The conditional independence assumption comes next: we assume that, given a topic <span class="math notranslate nohighlight">\(C\)</span>, the word occurrences are independent. That is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[\bX = \bx|C=k]
&amp;= \prod_{m=1}^M \P[X_m = x_m|C = k]\\
&amp;= \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}.
\end{align*}\]</div>
<p>Finally, the joint distribution is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[C = k, \bX = \bx]
&amp;= \P[\bX = \bx|C=k] \,\P[C=k]\\
&amp;= \pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}.
\end{align*}\]</div>
<p>Graphically, this is similar to a fork with <span class="math notranslate nohighlight">\(C\)</span> at its center and <span class="math notranslate nohighlight">\(M\)</span> prongs for the <span class="math notranslate nohighlight">\(X_m\)</span>s. This is represented using the so-called plate notation. The box with the <span class="math notranslate nohighlight">\(M\)</span> in the corner below indicates that <span class="math notranslate nohighlight">\(X_m\)</span> is repeated <span class="math notranslate nohighlight">\(M\)</span> times, all copies being conditionally independent given <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p><img alt="Naives Bayes" src="../Images/1e486313a3d40a4fea7001344a53121b.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_naive_networkx.png"/></p>
<p><strong>Model fitting</strong> Before using the model for prediction, one must first fit the model from training data <span class="math notranslate nohighlight">\(\{\bx_i, c_i\}_{i=1}^n\)</span>. In this case, it means estimating the unknown parameters <span class="math notranslate nohighlight">\(\bpi\)</span> and <span class="math notranslate nohighlight">\(\{\bp_k\}_{k=1}^K\)</span>, where <span class="math notranslate nohighlight">\(\bp_k = (p_{k,1},\ldots, p_{k,M})\)</span>. For each <span class="math notranslate nohighlight">\(k, m\)</span> let</p>
<div class="math notranslate nohighlight">
\[
N_{k,m} = \sum_{i=1}^n \mathbf{1}_{\{c_i = k\}} x_{i,m},
\quad 
N_{k} = \sum_{i=1}^n \mathbf{1}_{\{c_i = k\}}.
\]</div>
<p>We use maximum likelihood estimation which, recall, entails finding the parameters that maximize the probability of observing the data</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i, c_i\})
= \prod_{i=1}^n \pi_{c_i} \prod_{m=1}^M p_{c_i, m}^{x_{i,m}} (1-p_{c_i, m})^{1-x_{i,m}}.
\]</div>
<p>Here, as usual, we assume that the samples are independent and identically distributed. We take a logarithm to turn the products into sums and consider the negative log-likelihood (NLL)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp; L_n(\bpi, \{\bp_k\}; \{\bx_i, c_i\})\\
&amp;\quad = - \sum_{i=1}^n \log \pi_{c_i} - \sum_{i=1}^n \sum_{m=1}^M [x_{i,m} \log p_{c_{i}, m} + (1-x_{i,m}) \log (1-p_{c_i, m})]\\
&amp;\quad = - \sum_{k=1}^K N_k \log \pi_k - \sum_{k=1}^K \sum_{m=1}^M [N_{k,m} \log p_{k,m} + (N_k-N_{k,m}) \log (1-p_{k,m})].
\end{align*}\]</div>
<p>The NLL can be broken up naturally into several terms that depend on different sets of parameters – and therefore can be optimized separately. First, there is a term that depends only on the <span class="math notranslate nohighlight">\(\pi_k\)</span>’s</p>
<div class="math notranslate nohighlight">
\[
J_0(\bpi; \{\bx_i, c_i\}) = - \sum_{k=1}^K N_k \log \pi_k.
\]</div>
<p>The rest of the sum can be further split into <span class="math notranslate nohighlight">\(KM\)</span> terms, each depending only on <span class="math notranslate nohighlight">\(p_{km}\)</span> for a fixed <span class="math notranslate nohighlight">\(k\)</span> and m</p>
<div class="math notranslate nohighlight">
\[
J_{k,m}(p_{k,m}; \{\bx_i, c_i\})
= - N_{k,m} \log p_{k,m} - (N_k-N_{k,m}) \log (1-p_{k,m}).
\]</div>
<p>So</p>
<div class="math notranslate nohighlight">
\[
L_n(\bpi, \{\bp_k\}; \{\bx_i, c_i\})
= J_0(\bpi; \{\bx_i, c_i\}) + \sum_{k=1}^K \sum_{m=1}^M J_{k,m}(p_{k,m}; \{\bx_i, c_i\}).
\]</div>
<p>We minimize these terms separately. We assume that <span class="math notranslate nohighlight">\(N_k &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>We use a special case of maximum likelihood estimation, which we previously worked out in an example, where we consider the space of all probability distributions over a finite set. The maximum likelihood estimator in that case is given by the empirical frequencies. Notice that minimizing <span class="math notranslate nohighlight">\(J_0(\bpi; \{\bx_i, c_i\})\)</span> is precisely of this form: we observe <span class="math notranslate nohighlight">\(N_k\)</span> samples from class <span class="math notranslate nohighlight">\(k\)</span> and we seek the maximum likelihood estimator of, <span class="math notranslate nohighlight">\(\pi_k\)</span>, the probability of observing <span class="math notranslate nohighlight">\(k\)</span>. Hence the solution is simply</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_k = \frac{N_k}{N},
\]</div>
<p>for all <span class="math notranslate nohighlight">\(k\)</span>. Similarly, for each <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(m\)</span>, <span class="math notranslate nohighlight">\(J_{k,m}\)</span> is of that form as well. Here the states correspond to word <span class="math notranslate nohighlight">\(m\)</span> being present or absent in a document of class <span class="math notranslate nohighlight">\(k\)</span>, and we observe <span class="math notranslate nohighlight">\(N_{k,m}\)</span> documents of type <span class="math notranslate nohighlight">\(k\)</span> where the word <span class="math notranslate nohighlight">\(m\)</span> is present. So the solution is</p>
<div class="math notranslate nohighlight">
\[
\hat{p}_{k,m} = \frac{N_{k,m}}{N_k}
\]</div>
<p>for all <span class="math notranslate nohighlight">\(k, m\)</span>.</p>
<p><strong>Prediction</strong> To predict the class of a new document, it is natural to maximize over <span class="math notranslate nohighlight">\(k\)</span> the probability that <span class="math notranslate nohighlight">\(\{C=k\}\)</span> given <span class="math notranslate nohighlight">\(\{\bX = \bx\}\)</span>. By Bayes’ rule,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[C=k | \bX = \bx]
&amp;= \frac{\P[C = k, \bX = \bx]}{\P[\bX = \bx]}\\
&amp;= \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}}
{\sum_{k'=1}^K \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_m} (1-p_{k',m})^{1-x_m}}.
\end{align*}\]</div>
<p>As the denominator does not in fact depend on <span class="math notranslate nohighlight">\(k\)</span>, maximizing <span class="math notranslate nohighlight">\(\P[C=k | \bX = \bx]\)</span> boils down to maximizing the numerator <span class="math notranslate nohighlight">\(\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}\)</span>, which is straighforward to compute. As we did previously, we take a negative logarithm – which has some numerical advantages – and we refer to it as the <em>score</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;- \log\left(\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}\right)\\
&amp;\qquad = -\log\pi_k - \sum_{m=1}^M [x_m \log p_{k,m} + (1-x_m) \log (1-p_{k,m})].
\end{align*}\]</div>
<p>More specifically, taking a negative logarithm turns out to be a good idea here because computing a product of probabilities can produce very small numbers that, when they fall beneath machine precision, are approximated by zero. This is called <a class="reference external" href="https://en.wikipedia.org/wiki/Arithmetic_underflow">underflow</a><span class="math notranslate nohighlight">\(\idx{underflow}\xdi\)</span>. By taking a negative logarithm, these probabilities are transformed into positive numbers of reasonable magnitude and the product becomes of sum of these. Moreover, because this transformation is monotone, we can use the transformed values directly to compute the optimal score, which is our ultimate goal in the prediction step. Since the parameters are unknown, we use <span class="math notranslate nohighlight">\(\hat{\pi}_k\)</span> and <span class="math notranslate nohighlight">\(\hat{p}_{k,m}\)</span> in place of <span class="math notranslate nohighlight">\(\pi_k\)</span> and <span class="math notranslate nohighlight">\(p_{k,m}\)</span>.</p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot for more information on the issue of underflow, and its cousin overflow<span class="math notranslate nohighlight">\(\idx{overflow}\xdi\)</span>, in particular in the context of multiypling probabilities. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p>While maximum likehood estimation has <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties">desirable theoretical properties</a>, it does suffer from <a class="reference external" href="https://towardsdatascience.com/parameter-inference-maximum-aposteriori-estimate-49f3cd98267a">overfitting</a>. If for instance a particular word <span class="math notranslate nohighlight">\(m\)</span> does not occur in any training document, then the probability of observing a new document that happens to contain that word is estimated to be <span class="math notranslate nohighlight">\(0\)</span> for any class (i.e., <span class="math notranslate nohighlight">\(\hat{p}_{k,m} = 0\)</span> for all <span class="math notranslate nohighlight">\(k\)</span> so that <span class="math notranslate nohighlight">\(\hat \pi_k \prod_{m=1}^M \hat{p}_{k,m}^{x_m} (1-\hat{p}_{k,m})^{1-x_m} = 0\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>
) and the maximization problem above is not well-defined.</p>
<p>One approach to deal with this is <a class="reference external" href="https://en.wikipedia.org/wiki/Additive_smoothing">Laplace smoothing</a><span class="math notranslate nohighlight">\(\idx{Laplace smoothing}\xdi\)</span></p>
<div class="math notranslate nohighlight">
\[
\bar{\pi}_k = \frac{N_k + \alpha}{N + K \alpha},
\quad \bar{p}_{k,m} = \frac{N_{k,m} + \beta}{N_k + 2 \beta}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha, \beta &gt; 0\)</span>, which can be justified using a Bayesian or regularization perspective.</p>
<p>We implement the Naive Bayes model with Laplace smoothing.</p>
<p>We encode the data into a table, where the rows are the classes and the columns are the features. The entries are the corresponding <span class="math notranslate nohighlight">\(N_{k,m}\)</span>s. In addition we provide the vector <span class="math notranslate nohighlight">\((N_k)_k\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">nb_fit_table</span><span class="p">(</span><span class="n">N_km</span><span class="p">,</span> <span class="n">N_k</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    
    <span class="n">K</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">N_km</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">N_k</span><span class="p">)</span>
    <span class="n">pi_k</span> <span class="o">=</span> <span class="p">(</span><span class="n">N_k</span> <span class="o">+</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">K</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">p_km</span> <span class="o">=</span> <span class="p">(</span><span class="n">N_km</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_k</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">beta</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span>
</pre></div>
</div>
</div>
</div>
<p>Using <code class="docutils literal notranslate"><span class="pre">N_k[:,</span> <span class="pre">np.newaxis]</span></code> reshapes the one-dimensional array <code class="docutils literal notranslate"><span class="pre">N_k</span></code> into a two-dimensional column vector. For example, if <code class="docutils literal notranslate"><span class="pre">N_k</span></code> has a shape of <span class="math notranslate nohighlight">\((K,)\)</span>, then <code class="docutils literal notranslate"><span class="pre">N_k[:,</span> <span class="pre">np.newaxis]</span></code> changes its shape to <span class="math notranslate nohighlight">\((K, 1)\)</span>. This allows the division in the expression</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">p_km</span> <span class="o">=</span> <span class="p">(</span><span class="n">N_km</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_k</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
<p>to work correctly with <a class="reference external" href="https://numpy.org/doc/stable/user/basics.broadcasting.html">broadcasting</a>, ensuring that each element in a row of <code class="docutils literal notranslate"><span class="pre">N_km</span></code> is divided by the corresponding value in <code class="docutils literal notranslate"><span class="pre">N_k</span></code>.</p>
<p>The next function computes the negative logarithm of <span class="math notranslate nohighlight">\(\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}\)</span>, that is, the score of <span class="math notranslate nohighlight">\(k\)</span>, and outputs a <span class="math notranslate nohighlight">\(k\)</span> achieving the minimum score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">label_set</span><span class="p">):</span>
   
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
    
    <span class="n">score_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
       
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pi_k</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span> 
                               <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:]))</span>

    <span class="k">return</span> <span class="n">label_set</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">score_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We use a simple example from <a class="reference external" href="https://stackoverflow.com/questions/10059594/">Stack Overflow</a>:</p>
<blockquote>
<div><p><strong>Example:</strong> Let’s say we have data on 1000 pieces of fruit. They happen to be Banana, Orange or some Other Fruit. We know 3 characteristics about each fruit: whether it is long, whether it is sweet, and if its color is yellow[, as displayed in the table below].</p>
</div></blockquote>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Fruit</p></th>
<th class="head"><p>Long</p></th>
<th class="head"><p>Sweet</p></th>
<th class="head"><p>Yellow</p></th>
<th class="head"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Banana</p></td>
<td><p>400</p></td>
<td><p>350</p></td>
<td><p>450</p></td>
<td><p>500</p></td>
</tr>
<tr class="row-odd"><td><p>Orange</p></td>
<td><p>0</p></td>
<td><p>150</p></td>
<td><p>300</p></td>
<td><p>300</p></td>
</tr>
<tr class="row-even"><td><p>Other</p></td>
<td><p>100</p></td>
<td><p>150</p></td>
<td><p>50</p></td>
<td><p>200</p></td>
</tr>
<tr class="row-odd"><td><p>Total</p></td>
<td><p>500</p></td>
<td><p>650</p></td>
<td><p>800</p></td>
<td><p>1000</p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">N_km</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">400.</span><span class="p">,</span> <span class="mf">350.</span><span class="p">,</span> <span class="mf">450.</span><span class="p">],</span>
                 <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">150.</span><span class="p">,</span> <span class="mf">300.</span><span class="p">],</span>
                 <span class="p">[</span><span class="mf">100.</span><span class="p">,</span> <span class="mf">150.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">]])</span>
<span class="n">N_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">500.</span><span class="p">,</span> <span class="mf">300.</span><span class="p">,</span> <span class="mf">200.</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We run <code class="docutils literal notranslate"><span class="pre">nb_fit_table</span></code> on our simple dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">nb_fit_table</span><span class="p">(</span><span class="n">N_km</span><span class="p">,</span> <span class="n">N_k</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.4995015 0.3000997 0.2003988]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">p_km</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0.79880478 0.69920319 0.89840637]
 [0.00331126 0.5        0.99668874]
 [0.5        0.74752475 0.25247525]]
</pre></div>
</div>
</div>
</div>
<p>Continuing on with our previous example:</p>
<blockquote>
<div><p>Let’s say that we are given the properties of an unknown fruit, and asked to classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana? Is it an Orange? Or Is it some Other Fruit?</p>
</div></blockquote>
<p>We run <code class="docutils literal notranslate"><span class="pre">nb_predict</span></code> on our dataset with the additional fruit from the quote above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">label_set</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Banana'</span><span class="p">,</span> <span class="s1">'Orange'</span><span class="p">,</span> <span class="s1">'Other'</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">label_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>'Banana'
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> Laplace smoothing is a special case of a more general technique known as Bayesian parameter estimation. Ask your favorite AI chatbot to explain Bayesian parameter estimation and how it relates to maximum likelihood estimation and Laplace smoothing. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Which of the following statements is <strong>not</strong> true about conditional probabilities?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbb{P}[A|B] = \frac{\mathbb{P}[A \cap B]}{\mathbb{P}[B]}\)</span> for events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> with <span class="math notranslate nohighlight">\(\mathbb{P}[B] &gt; 0\)</span>.</p>
<p>b) If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent, then <span class="math notranslate nohighlight">\(\mathbb{P}[A|B] = \mathbb{P}[A]\)</span>.</p>
<p>c) Conditional probabilities can be used to express the multiplication rule and the law of total probability.</p>
<p>d) <span class="math notranslate nohighlight">\(\mathbb{P}[A|B] = \mathbb{P}[B|A]\)</span> for any events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p><strong>2</strong> Which of the following is the correct mathematical expression for the conditional independence of events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> given event <span class="math notranslate nohighlight">\(C\)</span>, denoted as <span class="math notranslate nohighlight">\(A \perp\!\!\!\perp B \mid C\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] + \mathbb{P}[B \mid C]\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\mathbb{P}[A \cup B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\mathbb{P}[A \mid B \cap C] = \mathbb{P}[A \mid C]\)</span></p>
<p><strong>3</strong> In the fork configuration <span class="math notranslate nohighlight">\(Y \leftarrow X \rightarrow Z\)</span>, which of the following conditional independence relations always holds?</p>
<p>a) <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Y \mid Z\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(Y \perp\!\!\!\perp Z \mid X\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Z \mid Y\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(Y \perp\!\!\!\perp Z\)</span></p>
<p><strong>4</strong> In the collider configuration <span class="math notranslate nohighlight">\(X \rightarrow Z \leftarrow Y\)</span>, which of the following conditional independence relations always holds?</p>
<p>a) <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Y \mid Z\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(Y \perp\!\!\!\perp Z \mid X\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Z \mid Y\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Y\)</span></p>
<p><strong>5</strong> Which of the following best describes the graphical representation of the Naive Bayes model for document classification?</p>
<p>a) A chain with the topic variable at the center and word variables as the links.</p>
<p>b) A collider with the topic variable at the center and word variables as the parents.</p>
<p>c) A fork with the topic variable at the center and word variables as the prongs.</p>
<p>d) A complete graph with edges between all pairs of variables.</p>
<p>Answer for 1: d. Justification: In general, <span class="math notranslate nohighlight">\(\mathbb{P}[A|B] \neq \mathbb{P}[B|A]\)</span>. Bayes’ rule provides the correct relationship between these two conditional probabilities.</p>
<p>Answer for 2: c. Justification: The text states, “Then <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(C\)</span>, denoted <span class="math notranslate nohighlight">\(A \perp\!\!\!\perp B \mid C\)</span>, if <span class="math notranslate nohighlight">\(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\)</span>.”</p>
<p>Answer for 3: b. Justification: The text states, “Removing the edge from <span class="math notranslate nohighlight">\(Y\)</span> to <span class="math notranslate nohighlight">\(Z\)</span> gives the following graph, known as a fork. We denote this configuration as <span class="math notranslate nohighlight">\(Y \leftarrow X \rightarrow Z\)</span>. […] The corresponding conditional independence relation is <span class="math notranslate nohighlight">\(Z \perp\!\!\!\perp Y \mid X\)</span>.”</p>
<p>Answer for 4: d. Justification: The text states, “Removing the edge from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span> gives the following graph, known as a collider. We denote this configuration as <span class="math notranslate nohighlight">\(X \rightarrow Z \leftarrow Y\)</span>. […] This time we have <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Y\)</span>.”</p>
<p>Answer for 5: c. Justification: The text states, “Graphically, this is similar to a fork with <span class="math notranslate nohighlight">\(C\)</span> at its center and <span class="math notranslate nohighlight">\(M\)</span> prongs for the <span class="math notranslate nohighlight">\(X_m\)</span>s.”</p>
</section>
&#13;

<h2><span class="section-number">6.3.1. </span>Review of conditioning<a class="headerlink" href="#review-of-conditioning" title="Link to this heading">#</a></h2>
<p>We first review the concept of conditioning, which generally plays a key role in probabilistic modeling and reasoning.</p>
<p><strong>Conditional probability</strong> We start with events. Throughout, we work on a fixed probability space <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, \P)\)</span>, which we assume is discrete, i.e., the number of elements in <span class="math notranslate nohighlight">\(\Omega\)</span> is countable.</p>
<p><strong>DEFINITION</strong> <strong>(Conditional Probability)</strong> <span class="math notranslate nohighlight">\(\idx{conditional probability}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> be two events with <span class="math notranslate nohighlight">\(\mathbb{P}[B] &gt; 0\)</span>. The conditional probability of <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(B\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
\P[A|B] = \frac{\P[A \cap B]}{\P[B]}.
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>The intuitive interpretation goes something like this: knowing that event <span class="math notranslate nohighlight">\(B\)</span> has occurred, the updated probability of observing <span class="math notranslate nohighlight">\(A\)</span> is the probability of its restriction to <span class="math notranslate nohighlight">\(B\)</span> properly normalized to reflect that outcomes outside <span class="math notranslate nohighlight">\(B\)</span> have updated probability <span class="math notranslate nohighlight">\(0\)</span>.</p>
<!--
This is illustrated next.

![Conditional probability](https://courses.cs.cornell.edu/cs2800/wiki/images/3/3b/Conditional-probability.svg)

([Source](https://courses.cs.cornell.edu/cs2800/wiki/index.php/File:Conditional-probability.svg))
--><p>Conditional probabilities generally behave like “unconditional” probabilities. (See for instance Problems 6.8, 7.1, and 7.9.)</p>
<p>Independence can be characterized in terms of conditional probability. In words, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent if conditioning on one of them having taken place does not change the probability of the other occurring.</p>
<p><strong>LEMMA</strong> Let <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> be two events of positive probability. Then <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent, which we will denote as <span class="math notranslate nohighlight">\(A \indep B\)</span>, if and only if <span class="math notranslate nohighlight">\(\P[A|B] = \P[A]\)</span> and <span class="math notranslate nohighlight">\(\P[B|A] = \P[B]\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent, then <span class="math notranslate nohighlight">\(\P[A \cap B] = \P[A] \P[B]\)</span> which implies</p>
<div class="math notranslate nohighlight">
\[
\P[A|B] = \frac{\P[A \cap B]}{\P[B]} = \frac{\P[A] \P[B]}{\P[B]} = \P[A].
\]</div>
<p>In the other direction,</p>
<div class="math notranslate nohighlight">
\[
\P[A] = \P[A|B] = \frac{\P[A \cap B]}{\P[B]}
\]</div>
<p>implies <span class="math notranslate nohighlight">\(\P[A \cap B] = \P[A]\P[B]\)</span> after rearranging. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>The conditional probability is often used in three fundamental ways, which we recall next. Proofs can be found in most probability textbooks.</p>
<ul class="simple">
<li><p><strong>Multiplication Rule:</strong> <span class="math notranslate nohighlight">\(\idx{multiplication rule}\xdi\)</span> For any collection of events <span class="math notranslate nohighlight">\(A_1,\ldots,A_r\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\P\left[\cap_{i=1}^r A_i\right]
= \prod_{i=1}^r \P\left[A_i \,\middle|\, \cap_{j=1}^{i-1} A_j \right].
\]</div>
<ul class="simple">
<li><p><strong>Law of Total Probability:</strong> <span class="math notranslate nohighlight">\(\idx{law of total probability}\xdi\)</span> For any event <span class="math notranslate nohighlight">\(B\)</span> and any <a class="reference external" href="https://en.wikipedia.org/wiki/Partition_of_a_set#Definition_and_Notation">partition</a><span class="math notranslate nohighlight">\(\idx{partition}\xdi\)</span> <span class="math notranslate nohighlight">\(A_1,\ldots,A_r\)</span> of <span class="math notranslate nohighlight">\(\Omega\)</span>,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\P[B] 
= \sum_{i=1}^r \P[B|A_i] \P[A_i].
\]</div>
<ul class="simple">
<li><p><strong>Bayes’ Rule:</strong> <span class="math notranslate nohighlight">\(\idx{Bayes' yule}\xdi\)</span> For any events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> with positive probability,</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\P[A|B]
= \frac{\P[B|A]\P[A]}{\P[B]}.
\]</div>
<p>It is implicit that all formulas above hold provided all conditional probabilities are well-defined.</p>
<p><strong>Conditioning on a random variable</strong> Conditional probabilities extend naturally to random variables. If <span class="math notranslate nohighlight">\(X\)</span> is a discrete random variable, we let <span class="math notranslate nohighlight">\(p_X\)</span> be its probability mass function and <span class="math notranslate nohighlight">\(\S_X\)</span> be its support, that is, the set of values where it has positive probability. Then we can for instance condition on the event <span class="math notranslate nohighlight">\(\{X = x\}\)</span> for any <span class="math notranslate nohighlight">\(x \in \S_X\)</span>.</p>
<p>We define next the conditional probability mass function.</p>
<p><strong>DEFINITION</strong> <strong>(Conditional Probability Mass Function)</strong> Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be discrete random variables with joint probability mass function <span class="math notranslate nohighlight">\(p_{X, Y}\)</span> and marginals <span class="math notranslate nohighlight">\(p_X\)</span> and <span class="math notranslate nohighlight">\(p_Y\)</span>. The conditional probability mass function<span class="math notranslate nohighlight">\(\idx{conditional probability mass function}\xdi\)</span> of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(Y\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
p_{X|Y}(x|y) := P[X=x|Y=y]  = \frac{p_{X,Y}(x,y)}{p_Y(y)}
\]</div>
<p>which is defined for all <span class="math notranslate nohighlight">\(x \in \S_X\)</span> and <span class="math notranslate nohighlight">\(y \in \S_Y\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>The conditional expectation can then be defined in a natural way as the expectation over the conditional probability mass function.</p>
<p><strong>DEFINITION</strong> <strong>(Conditional Expectation)</strong> <span class="math notranslate nohighlight">\(\idx{conditional expectation}\xdi\)</span> Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be discrete random variables where <span class="math notranslate nohighlight">\(X\)</span> takes real values and has a finite mean. The conditional expectation of <span class="math notranslate nohighlight">\(X\)</span> given <span class="math notranslate nohighlight">\(Y = y\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
\E[X|Y=y] = \sum_{x \in \S_X} x\, p_{X|Y}(x|y). 
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>More generally, for a function <span class="math notranslate nohighlight">\(f\)</span> over the range of <span class="math notranslate nohighlight">\(X\)</span>, we can define</p>
<div class="math notranslate nohighlight">
\[
\E[f(X)|Y=y] = \sum_{x \in \S_X} f(x)\, p_{X|Y}(x|y). 
\]</div>
<p>We mention one useful formula: the <em>Law of Total Expectation</em><span class="math notranslate nohighlight">\(\idx{law of total expectation}\xdi\)</span>, the expectation version of the <em>Law of Total Probability</em>. It reads</p>
<div class="math notranslate nohighlight">
\[
\E[f(X)] = \sum_{y \in \S_Y} \E[f(X)|Y=y] \,p_Y(y).
\]</div>
<p><strong>Conditional expectation as least-squares estimator</strong> Thinking of <span class="math notranslate nohighlight">\(\E[X|Y=y]\)</span> as a function of <span class="math notranslate nohighlight">\(y\)</span> leads to a fundamental characterization of the conditional expectation.</p>
<p><strong>THEOREM</strong> Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be discrete random variables where <span class="math notranslate nohighlight">\(X\)</span> takes real values and has a finite variance. Then the conditional expectation <span class="math notranslate nohighlight">\(h(y) = \E[X|Y=y]\)</span> minimizes the least squares criterion</p>
<div class="math notranslate nohighlight">
\[
\min_{h} \E\left[(X - h(Y))^2\right]
\]</div>
<p>where the minimum is over all real-valued functions of <span class="math notranslate nohighlight">\(y\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> Think of <span class="math notranslate nohighlight">\(h(y)\)</span> as a vector <span class="math notranslate nohighlight">\(\mathbf{h} = (h_y)_{y \in \S_Y}\)</span>, indexed by <span class="math notranslate nohighlight">\(\S_Y\)</span> (which is countable by assumption), with <span class="math notranslate nohighlight">\(h_y = h(y) \in \mathbb{R}\)</span>. Then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal{L}(\mathbf{h})
&amp;=\E\left[(X - h(Y))^2\right]\\
&amp;= \sum_{x\in \S_X} \sum_{y \in \S_Y} (x - h_y)^2 p_{X,Y}(x,y)\\
&amp;= \sum_{y \in \S_Y} \left[\sum_{x\in \S_X}  (x - h_y)^2 p_{X,Y}(x,y)\right].
\end{align*}\]</div>
<p>Expanding the sum in the square brackets (which we denote by <span class="math notranslate nohighlight">\(q_y\)</span> and think of as a function of <span class="math notranslate nohighlight">\(h_y\)</span>) gives</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
q_y(h_y)
&amp;:= \sum_{x\in \S_X}  (x - h_y)^2 p_{X,Y}(x,y)\\
&amp;= \sum_{x\in \S_X}  [x^2 - 2 x h_y + h_y^2] \,p_{X,Y}(x,y)\\
&amp;= \left\{\sum_{x\in \S_X} x^2 p_{X,Y}(x,y)\right\}
+ \left\{- 2 \sum_{x\in \S_X} x p_{X,Y}(x,y)\right\} h_y
+ \left\{p_Y(y)\right\} h_y^2.
\end{align*}\]</div>
<p>By the <em>Miminizing a Quadratic Function Lemma</em>, the unique global minimum of <span class="math notranslate nohighlight">\(q_y(h_y)\)</span> - provided <span class="math notranslate nohighlight">\(p_Y(y) &gt; 0\)</span> - is attained at</p>
<div class="math notranslate nohighlight">
\[
h_y 
= - \frac{- 2 \sum_{x\in \S_X} x p_{X,Y}(x,y)}{2 p_Y(y)}.
\]</div>
<p>After rearranging, we get</p>
<div class="math notranslate nohighlight">
\[
h_y 
= \sum_{x\in \S_X}  x \frac{p_{X,Y}(x,y)}{p_Y(y)}
= \sum_{x\in \S_X}  x p_{X|Y}(x|y)
= \E[X|Y=y]
\]</div>
<p>as claimed. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Conditional independence</strong> Next, we discuss conditional independence. We begin with the formal definition.</p>
<p><strong>DEFINITION</strong> <strong>(Conditional Independence)</strong> <span class="math notranslate nohighlight">\(\idx{conditional independence}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A, B, C\)</span> be events such that <span class="math notranslate nohighlight">\(\P[C] &gt; 0\)</span>. Then <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(C\)</span>, denoted <span class="math notranslate nohighlight">\(A \indep B | C\)</span>, if</p>
<div class="math notranslate nohighlight">
\[
\P[A \cap B| C] = \P[A|C] \,\P[B|C].
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>In words, quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Conditional_independence">Wikipedia</a>:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(C\)</span> if and only if, given knowledge that <span class="math notranslate nohighlight">\(C\)</span> occurs, knowledge of whether <span class="math notranslate nohighlight">\(A\)</span> occurs provides no information on the likelihood of <span class="math notranslate nohighlight">\(B\)</span> occurring, and knowledge of whether <span class="math notranslate nohighlight">\(B\)</span> occurs provides no information on the likelihood of <span class="math notranslate nohighlight">\(A\)</span> occurring.</p>
</div></blockquote>
<p>In general, conditionally independent events are not (unconditionally) independent.</p>
<p><strong>EXAMPLE:</strong> Imagine I have two six-sided dice. Die 1 has faces <span class="math notranslate nohighlight">\(\{1,3,5,7,9,11\}\)</span> and die 2 has faces <span class="math notranslate nohighlight">\(\{2, 4, 6, 8, 10, 12\}\)</span>. Suppose I perform the following experiment: I pick one of the two dice uniformly at random, and then I roll that die twice. Let <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> be the outcomes of the rolls. Consider the events <span class="math notranslate nohighlight">\(A = \{X_1 = 1\}\)</span>, <span class="math notranslate nohighlight">\(B = \{X_2 = 2\}\)</span>, and <span class="math notranslate nohighlight">\(C = \{\text{die 1 is picked}\}\)</span>. The events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are clearly dependent: if <span class="math notranslate nohighlight">\(A\)</span> occurs, then I know that die 1 was picked, and hence <span class="math notranslate nohighlight">\(B\)</span> cannot occur. Knowledge of one event provides information about the likelihood of the other event occurring. Formally, by the law of total probability,</p>
<div class="math notranslate nohighlight">
\[
\P[A] 
= \P[A|C]\P[C] + \P[A|C^c]\P[C^c]
= \frac{1}{6}\frac{1}{2} + 0 \frac{1}{2}
= \frac{1}{12}.
\]</div>
<p>Similarly <span class="math notranslate nohighlight">\(\P[B] = \frac{1}{12}\)</span>. Yet <span class="math notranslate nohighlight">\(\P[A \cap B] = 0 \neq \frac{1}{12} \frac{1}{12}\)</span>.</p>
<p>On the other hand, we claim that <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(C\)</span>. Again this is intuitively clear: once I pick a die, the two rolls are independent. For a given die choice, knowledge of one roll provides no information about the likelihood of the other roll. Note that the phrase “for a given die choice” is critical in the last statement. Formally, by our experiment, we have <span class="math notranslate nohighlight">\(\P[A|C] = 1/6\)</span>, <span class="math notranslate nohighlight">\(\P[B|C] = 0\)</span> and <span class="math notranslate nohighlight">\(\P[A \cap B|C] = 0\)</span>. So indeed</p>
<div class="math notranslate nohighlight">
\[
\P[A \cap B| C] = \P[A|C] \,\P[B|C]
\]</div>
<p>as claimed. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Conditional independence is naturally extended to random vectors.</p>
<p><strong>DEFINITION</strong> <strong>(Conditional Independence of Random Vectors)</strong> Let <span class="math notranslate nohighlight">\(\bX, \bY, \bW\)</span> be discrete random vectors. Then <span class="math notranslate nohighlight">\(\bX\)</span> and <span class="math notranslate nohighlight">\(\bY\)</span> are said to be conditionally independent given <span class="math notranslate nohighlight">\(\bW\)</span>, denoted <span class="math notranslate nohighlight">\(\bX \indep \bY | \bW\)</span>, if for all <span class="math notranslate nohighlight">\(\bx \in \S_\bX\)</span>, <span class="math notranslate nohighlight">\(\by \in \S_\bY\)</span> and <span class="math notranslate nohighlight">\(\bw \in \S_\bW\)</span></p>
<div class="math notranslate nohighlight">
\[
\P[\bX = \bx, \bY = \by|\bW = \bw] 
= \P[\bX = \bx |\bW = \bw] 
\,\P[\bY = \by|\bW = \bw].
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>An important consequence is that we can drop the conditioning by the independent variable.</p>
<p><strong>LEMMA</strong> <strong>(Role of Independence)</strong> <span class="math notranslate nohighlight">\(\idx{role of independence lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\bX, \bY, \bW\)</span> be discrete random vectors such that <span class="math notranslate nohighlight">\(\bX \indep \bY | \bW\)</span>. For all <span class="math notranslate nohighlight">\(\bx \in \S_\bX\)</span>, <span class="math notranslate nohighlight">\(\by \in \S_\bY\)</span> and <span class="math notranslate nohighlight">\(\bw \in \S_\bW\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\P[\bX = \bx | \bY=\by, \bW=\bw]
= \P[\bX = \bx | \bW = \bw].
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> In a previous exercise, we showed that <span class="math notranslate nohighlight">\(A \indep B | C\)</span> implies <span class="math notranslate nohighlight">\(\P[A | B\cap C] = \P[A | C]\)</span>. That implies the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The concept of conditional independence is closely related to the concept of d-separation in probabilistic graphical models. Ask your favorite AI chatbot to explain d-separation. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
&#13;

<h2><span class="section-number">6.3.2. </span>The basic configurations<a class="headerlink" href="#the-basic-configurations" title="Link to this heading">#</a></h2>
<p>A powerful approach for constructing complex probability distributions is the use of conditional independence. The case of three random variables exemplifies key probabilistic relationships. By the product rule, we can write</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y, Z=z]
= \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x, Y=y].
\]</div>
<p>This is conveniently represented through a digraph where the vertices are the variables. Recall that an arrow <span class="math notranslate nohighlight">\((i,j)\)</span>, from <span class="math notranslate nohighlight">\(i\)</span> to <span class="math notranslate nohighlight">\(j\)</span>, indicates that <span class="math notranslate nohighlight">\(i\)</span> is a parent of <span class="math notranslate nohighlight">\(j\)</span> and that <span class="math notranslate nohighlight">\(j\)</span> is a child of <span class="math notranslate nohighlight">\(i\)</span>. Let <span class="math notranslate nohighlight">\(\pa(i)\)</span> be the set of parents of <span class="math notranslate nohighlight">\(i\)</span>. The digraph <span class="math notranslate nohighlight">\(G = (V, E)\)</span> below encodes the following sampling scheme, referred as ancestral sampling:</p>
<ol class="arabic simple">
<li><p>First we pick <span class="math notranslate nohighlight">\(X\)</span> according to its marginal <span class="math notranslate nohighlight">\(\P[X=x]\)</span>. Note that <span class="math notranslate nohighlight">\(X\)</span> has no parent in <span class="math notranslate nohighlight">\(G\)</span>.</p></li>
<li><p>Second we pick <span class="math notranslate nohighlight">\(Y\)</span> according to the conditional probability distribution (CPD) <span class="math notranslate nohighlight">\(\P[Y=y|X=x]\)</span>. Note that <span class="math notranslate nohighlight">\(X\)</span> is the only parent of <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p>Finally we pick <span class="math notranslate nohighlight">\(Z\)</span> according to the CPD <span class="math notranslate nohighlight">\(\P[Z=z|X=x, Y=y]\)</span>. Note that the parents of <span class="math notranslate nohighlight">\(Z\)</span> are <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
</ol>
<p><img alt="The full case" src="../Images/b73cb01c1b1f5d32087884467af21283.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_full1_networkx.png"/></p>
<p>The graph above is acyclic, that is, it has no directed cycle. The variables <span class="math notranslate nohighlight">\(X, Y, Z\)</span> are in <a class="reference external" href="https://en.wikipedia.org/wiki/Topological_sorting">topological order</a><span class="math notranslate nohighlight">\(\idx{topological order}\xdi\)</span>, that is, all edges <span class="math notranslate nohighlight">\((i,j)\)</span> are such that <span class="math notranslate nohighlight">\(i\)</span> comes before <span class="math notranslate nohighlight">\(j\)</span> in that order.</p>
<p>The same joint distribution can be represented by a different digraph if the product rule is used in a different order. For instance,</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y, Z=z]
= \P[Z=z] \,\P[Y=y|Z=z] \,\P[X=x | Z=z, Y=y]
\]</div>
<p>is represented by the following digraph. A topological order this time is <span class="math notranslate nohighlight">\(Z, Y, X\)</span>.</p>
<p><img alt="Another full case" src="../Images/ed39c216e1efdc59d602be7c9e2c744a.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_full2_networkx.png"/></p>
<p><strong>The fork</strong> <span class="math notranslate nohighlight">\(\idx{fork}\xdi\)</span> Removing edges in the first graph above encodes conditional independence relations. For instance, removing the edge from <span class="math notranslate nohighlight">\(Y\)</span> to <span class="math notranslate nohighlight">\(Z\)</span> gives the following graph, known as a fork. We denote this configuration as <span class="math notranslate nohighlight">\(Y \leftarrow X \rightarrow Z\)</span>.</p>
<p><img alt="The fork" src="../Images/77cb78740ee4952c17900d2626c7ad2a.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_fork_networkx.png"/></p>
<p>The joint distribution simplifies as follows:</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y, Z=z]
= \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x].
\]</div>
<p>So, in this case, what has changed is that the CPD of <span class="math notranslate nohighlight">\(Z\)</span> does not depend on the value of <span class="math notranslate nohighlight">\(Y\)</span>. From the <em>Role of Independence</em> lemma, this corresponds to assuming the conditional independence <span class="math notranslate nohighlight">\(Z \indep Y|X\)</span>. Indeed, we can check that claim directly from the joint distribution</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[Y= y, Z=z|X=x]
&amp;= \frac{\P[X=x, Y= y, Z=z]}{\P[X=x]}\\
&amp;= \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | X=x]}{\P[X=x]}\\
&amp;= \P[Y=y|X=x] \,\P[Z=z | X=x]
\end{align*}\]</div>
<p>as claimed.</p>
<p><strong>The chain</strong> <span class="math notranslate nohighlight">\(\idx{chain}\xdi\)</span> Removing the edge from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Z\)</span> gives the following graph, known as a chain (or pipe). We denote this configuration as <span class="math notranslate nohighlight">\(X \rightarrow Y \rightarrow Z\)</span>.</p>
<p><img alt="The chain" src="../Images/5bfd7c9b4911ca0e5931761d25c8457e.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_chain_networkx.png"/></p>
<p>The joint distribution simplifies as follows:</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y, Z=z]
= \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y].
\]</div>
<p>In this case, what has changed is that the CPD of <span class="math notranslate nohighlight">\(Z\)</span> does not depend on the value of <span class="math notranslate nohighlight">\(X\)</span>. Compare that to the fork. The corresponding conditional independence relation is <span class="math notranslate nohighlight">\(Z \indep X|Y\)</span>. Indeed, we can check that claim directly</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[X= x, Z=z|Y=y]
&amp;= \frac{\P[X=x, Y= y, Z=z]}{\P[Y=y]}\\
&amp;= \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]}{\P[Y=y]}
\end{align*}\]</div>
<p>Now we have to use <em>Bayes’ Rule</em> to get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;= \frac{\P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]}{\P[Y=y]}\\
&amp;= \frac{\P[Y=y|X=x]\,\P[X=x]}{\P[Y=y]} \P[Z=z | Y=y]\\
&amp;= \P[X=x|Y=y] \,\P[Z=z | Y=y]
\end{align*}\]</div>
<p>as claimed.</p>
<p>For any <span class="math notranslate nohighlight">\(x, y, z\)</span> where the joint probability is positive, we can re-write</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\P[X=x, Y=y, Z=z]\\
&amp;= \P[X=x] \,\P[Y=y|X=x] \,\P[Z=z | Y=y]\\
&amp;= \P[Y=y] \,\P[X=x|Y=y] \,\P[Z=z | Y=y],
\end{align*}\]</div>
<p>where we used that</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y]
= \P[X=x] \,\P[Y=y|X=x]
= \P[Y=y] \,\P[X=x|Y=y]
\]</div>
<p>by definition of the conditional probability. In other words, we have shown that the chain <span class="math notranslate nohighlight">\(X \rightarrow Y \rightarrow Z\)</span> is in fact equivalent to the fork <span class="math notranslate nohighlight">\(X \leftarrow Y \rightarrow Z\)</span>. In particular, they both correspond to assuming the conditional independence relation <span class="math notranslate nohighlight">\(Z \indep X|Y\)</span>, although they capture a different way to sample the joint distribution.</p>
<p><strong>The collider</strong> <span class="math notranslate nohighlight">\(\idx{collider}\xdi\)</span> Removing the edge from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span> gives the following graph, known as a collider. We denote this configuration as <span class="math notranslate nohighlight">\(X \rightarrow Z \leftarrow Y\)</span>.</p>
<p><img alt="The collider" src="../Images/a4600775ea580d4809bd8b11895a447d.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_collider_networkx.png"/></p>
<p>The joint distribution simplifies as follows:</p>
<div class="math notranslate nohighlight">
\[
\P[X=x, Y=y, Z=z]
= \P[X=x] \,\P[Y=y] \,\P[Z=z | X=x, Y=y].
\]</div>
<p>In this case, what has changed is that the CPD of <span class="math notranslate nohighlight">\(Y\)</span> does not depend on the value of <span class="math notranslate nohighlight">\(X\)</span>. Compare that to the fork and the chain. This time we have <span class="math notranslate nohighlight">\(X \indep Y\)</span>. Indeed, we can check that claim directly</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[X= x, Y=y]
&amp;= \sum_{z \in \S_z} \P[X=x, Y=y, Z=z]\\
&amp;=  \sum_{z \in \S_z} \P[X=x] \,\P[Y=y] \,\P[Z=z | X=x, Y=y]\\
&amp;= \P[X=x] \,\P[Y=y]
\end{align*}\]</div>
<p>as claimed. In particular, the collider cannot be reframed as a chain or fork as its underlying assumption is stronger.</p>
<p>Perhaps counter-intuitively, conditioning on <span class="math notranslate nohighlight">\(Z\)</span> makes <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> dependent in general. This is known as explaining away or Berkson’s Paradox.</p>
&#13;

<h2><span class="section-number">6.3.3. </span>Example: Naive Bayes<a class="headerlink" href="#example-naive-bayes" title="Link to this heading">#</a></h2>
<p>The model-based justification we gave for logistic regression in the  subsection on generalized linear models used a so-called <a class="reference external" href="https://en.wikipedia.org/wiki/Discriminative_model">discriminative approach</a><span class="math notranslate nohighlight">\(\idx{discriminative model}\xdi\)</span>, where the conditional distribution of the target <span class="math notranslate nohighlight">\(y\)</span> given the features <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is specified – but not the full distribution of the data <span class="math notranslate nohighlight">\((\mathbf{x}, y)\)</span>. Here we give an example of the <a class="reference external" href="https://en.wikipedia.org/wiki/Generative_model">generative approach</a><span class="math notranslate nohighlight">\(\idx{generative model}\xdi\)</span>, which models the full distribution. For a discussion of the benefits and drawbacks of each approach, see for example <a class="reference external" href="https://en.wikipedia.org/wiki/Discriminative_model#Contrast_with_generative_model">here</a>.</p>
<p>The Naive Bayes<span class="math notranslate nohighlight">\(\idx{Naive Bayes}\xdi\)</span> model is a simple discrete model for supervised learning. It is useful for document classification for instance, and we will use that terminology here to be concrete. We assume that a document has a single topic <span class="math notranslate nohighlight">\(C\)</span> from a list <span class="math notranslate nohighlight">\(\mathcal{C} = \{1, \ldots, K\}\)</span> with probability distribution <span class="math notranslate nohighlight">\(\pi_k = \P[C = k]\)</span>. There is a vocabulary of size <span class="math notranslate nohighlight">\(M\)</span> and we record the presence or absence of a word <span class="math notranslate nohighlight">\(m\)</span> in the document with a Bernoulli variable <span class="math notranslate nohighlight">\(X_m \in \{0,1\}\)</span>, where <span class="math notranslate nohighlight">\(p_{k,m} = \P[X_m = 1|C = k]\)</span>. We denote by <span class="math notranslate nohighlight">\(\bX = (X_1, \ldots, X_M)\)</span> the corresponding vector.</p>
<p>The conditional independence assumption comes next: we assume that, given a topic <span class="math notranslate nohighlight">\(C\)</span>, the word occurrences are independent. That is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[\bX = \bx|C=k]
&amp;= \prod_{m=1}^M \P[X_m = x_m|C = k]\\
&amp;= \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}.
\end{align*}\]</div>
<p>Finally, the joint distribution is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[C = k, \bX = \bx]
&amp;= \P[\bX = \bx|C=k] \,\P[C=k]\\
&amp;= \pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}.
\end{align*}\]</div>
<p>Graphically, this is similar to a fork with <span class="math notranslate nohighlight">\(C\)</span> at its center and <span class="math notranslate nohighlight">\(M\)</span> prongs for the <span class="math notranslate nohighlight">\(X_m\)</span>s. This is represented using the so-called plate notation. The box with the <span class="math notranslate nohighlight">\(M\)</span> in the corner below indicates that <span class="math notranslate nohighlight">\(X_m\)</span> is repeated <span class="math notranslate nohighlight">\(M\)</span> times, all copies being conditionally independent given <span class="math notranslate nohighlight">\(C\)</span>.</p>
<p><img alt="Naives Bayes" src="../Images/1e486313a3d40a4fea7001344a53121b.png" data-original-src="https://mmids-textbook.github.io/_images/dgm_naive_networkx.png"/></p>
<p><strong>Model fitting</strong> Before using the model for prediction, one must first fit the model from training data <span class="math notranslate nohighlight">\(\{\bx_i, c_i\}_{i=1}^n\)</span>. In this case, it means estimating the unknown parameters <span class="math notranslate nohighlight">\(\bpi\)</span> and <span class="math notranslate nohighlight">\(\{\bp_k\}_{k=1}^K\)</span>, where <span class="math notranslate nohighlight">\(\bp_k = (p_{k,1},\ldots, p_{k,M})\)</span>. For each <span class="math notranslate nohighlight">\(k, m\)</span> let</p>
<div class="math notranslate nohighlight">
\[
N_{k,m} = \sum_{i=1}^n \mathbf{1}_{\{c_i = k\}} x_{i,m},
\quad 
N_{k} = \sum_{i=1}^n \mathbf{1}_{\{c_i = k\}}.
\]</div>
<p>We use maximum likelihood estimation which, recall, entails finding the parameters that maximize the probability of observing the data</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\bpi, \{\bp_k\}; \{\bx_i, c_i\})
= \prod_{i=1}^n \pi_{c_i} \prod_{m=1}^M p_{c_i, m}^{x_{i,m}} (1-p_{c_i, m})^{1-x_{i,m}}.
\]</div>
<p>Here, as usual, we assume that the samples are independent and identically distributed. We take a logarithm to turn the products into sums and consider the negative log-likelihood (NLL)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp; L_n(\bpi, \{\bp_k\}; \{\bx_i, c_i\})\\
&amp;\quad = - \sum_{i=1}^n \log \pi_{c_i} - \sum_{i=1}^n \sum_{m=1}^M [x_{i,m} \log p_{c_{i}, m} + (1-x_{i,m}) \log (1-p_{c_i, m})]\\
&amp;\quad = - \sum_{k=1}^K N_k \log \pi_k - \sum_{k=1}^K \sum_{m=1}^M [N_{k,m} \log p_{k,m} + (N_k-N_{k,m}) \log (1-p_{k,m})].
\end{align*}\]</div>
<p>The NLL can be broken up naturally into several terms that depend on different sets of parameters – and therefore can be optimized separately. First, there is a term that depends only on the <span class="math notranslate nohighlight">\(\pi_k\)</span>’s</p>
<div class="math notranslate nohighlight">
\[
J_0(\bpi; \{\bx_i, c_i\}) = - \sum_{k=1}^K N_k \log \pi_k.
\]</div>
<p>The rest of the sum can be further split into <span class="math notranslate nohighlight">\(KM\)</span> terms, each depending only on <span class="math notranslate nohighlight">\(p_{km}\)</span> for a fixed <span class="math notranslate nohighlight">\(k\)</span> and m</p>
<div class="math notranslate nohighlight">
\[
J_{k,m}(p_{k,m}; \{\bx_i, c_i\})
= - N_{k,m} \log p_{k,m} - (N_k-N_{k,m}) \log (1-p_{k,m}).
\]</div>
<p>So</p>
<div class="math notranslate nohighlight">
\[
L_n(\bpi, \{\bp_k\}; \{\bx_i, c_i\})
= J_0(\bpi; \{\bx_i, c_i\}) + \sum_{k=1}^K \sum_{m=1}^M J_{k,m}(p_{k,m}; \{\bx_i, c_i\}).
\]</div>
<p>We minimize these terms separately. We assume that <span class="math notranslate nohighlight">\(N_k &gt; 0\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>We use a special case of maximum likelihood estimation, which we previously worked out in an example, where we consider the space of all probability distributions over a finite set. The maximum likelihood estimator in that case is given by the empirical frequencies. Notice that minimizing <span class="math notranslate nohighlight">\(J_0(\bpi; \{\bx_i, c_i\})\)</span> is precisely of this form: we observe <span class="math notranslate nohighlight">\(N_k\)</span> samples from class <span class="math notranslate nohighlight">\(k\)</span> and we seek the maximum likelihood estimator of, <span class="math notranslate nohighlight">\(\pi_k\)</span>, the probability of observing <span class="math notranslate nohighlight">\(k\)</span>. Hence the solution is simply</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_k = \frac{N_k}{N},
\]</div>
<p>for all <span class="math notranslate nohighlight">\(k\)</span>. Similarly, for each <span class="math notranslate nohighlight">\(k\)</span>, <span class="math notranslate nohighlight">\(m\)</span>, <span class="math notranslate nohighlight">\(J_{k,m}\)</span> is of that form as well. Here the states correspond to word <span class="math notranslate nohighlight">\(m\)</span> being present or absent in a document of class <span class="math notranslate nohighlight">\(k\)</span>, and we observe <span class="math notranslate nohighlight">\(N_{k,m}\)</span> documents of type <span class="math notranslate nohighlight">\(k\)</span> where the word <span class="math notranslate nohighlight">\(m\)</span> is present. So the solution is</p>
<div class="math notranslate nohighlight">
\[
\hat{p}_{k,m} = \frac{N_{k,m}}{N_k}
\]</div>
<p>for all <span class="math notranslate nohighlight">\(k, m\)</span>.</p>
<p><strong>Prediction</strong> To predict the class of a new document, it is natural to maximize over <span class="math notranslate nohighlight">\(k\)</span> the probability that <span class="math notranslate nohighlight">\(\{C=k\}\)</span> given <span class="math notranslate nohighlight">\(\{\bX = \bx\}\)</span>. By Bayes’ rule,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\P[C=k | \bX = \bx]
&amp;= \frac{\P[C = k, \bX = \bx]}{\P[\bX = \bx]}\\
&amp;= \frac{\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}}
{\sum_{k'=1}^K \pi_{k'} \prod_{m=1}^M p_{k',m}^{x_m} (1-p_{k',m})^{1-x_m}}.
\end{align*}\]</div>
<p>As the denominator does not in fact depend on <span class="math notranslate nohighlight">\(k\)</span>, maximizing <span class="math notranslate nohighlight">\(\P[C=k | \bX = \bx]\)</span> boils down to maximizing the numerator <span class="math notranslate nohighlight">\(\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}\)</span>, which is straighforward to compute. As we did previously, we take a negative logarithm – which has some numerical advantages – and we refer to it as the <em>score</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;- \log\left(\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}\right)\\
&amp;\qquad = -\log\pi_k - \sum_{m=1}^M [x_m \log p_{k,m} + (1-x_m) \log (1-p_{k,m})].
\end{align*}\]</div>
<p>More specifically, taking a negative logarithm turns out to be a good idea here because computing a product of probabilities can produce very small numbers that, when they fall beneath machine precision, are approximated by zero. This is called <a class="reference external" href="https://en.wikipedia.org/wiki/Arithmetic_underflow">underflow</a><span class="math notranslate nohighlight">\(\idx{underflow}\xdi\)</span>. By taking a negative logarithm, these probabilities are transformed into positive numbers of reasonable magnitude and the product becomes of sum of these. Moreover, because this transformation is monotone, we can use the transformed values directly to compute the optimal score, which is our ultimate goal in the prediction step. Since the parameters are unknown, we use <span class="math notranslate nohighlight">\(\hat{\pi}_k\)</span> and <span class="math notranslate nohighlight">\(\hat{p}_{k,m}\)</span> in place of <span class="math notranslate nohighlight">\(\pi_k\)</span> and <span class="math notranslate nohighlight">\(p_{k,m}\)</span>.</p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot for more information on the issue of underflow, and its cousin overflow<span class="math notranslate nohighlight">\(\idx{overflow}\xdi\)</span>, in particular in the context of multiypling probabilities. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p>While maximum likehood estimation has <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties">desirable theoretical properties</a>, it does suffer from <a class="reference external" href="https://towardsdatascience.com/parameter-inference-maximum-aposteriori-estimate-49f3cd98267a">overfitting</a>. If for instance a particular word <span class="math notranslate nohighlight">\(m\)</span> does not occur in any training document, then the probability of observing a new document that happens to contain that word is estimated to be <span class="math notranslate nohighlight">\(0\)</span> for any class (i.e., <span class="math notranslate nohighlight">\(\hat{p}_{k,m} = 0\)</span> for all <span class="math notranslate nohighlight">\(k\)</span> so that <span class="math notranslate nohighlight">\(\hat \pi_k \prod_{m=1}^M \hat{p}_{k,m}^{x_m} (1-\hat{p}_{k,m})^{1-x_m} = 0\)</span> for all <span class="math notranslate nohighlight">\(k\)</span>
) and the maximization problem above is not well-defined.</p>
<p>One approach to deal with this is <a class="reference external" href="https://en.wikipedia.org/wiki/Additive_smoothing">Laplace smoothing</a><span class="math notranslate nohighlight">\(\idx{Laplace smoothing}\xdi\)</span></p>
<div class="math notranslate nohighlight">
\[
\bar{\pi}_k = \frac{N_k + \alpha}{N + K \alpha},
\quad \bar{p}_{k,m} = \frac{N_{k,m} + \beta}{N_k + 2 \beta}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha, \beta &gt; 0\)</span>, which can be justified using a Bayesian or regularization perspective.</p>
<p>We implement the Naive Bayes model with Laplace smoothing.</p>
<p>We encode the data into a table, where the rows are the classes and the columns are the features. The entries are the corresponding <span class="math notranslate nohighlight">\(N_{k,m}\)</span>s. In addition we provide the vector <span class="math notranslate nohighlight">\((N_k)_k\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">nb_fit_table</span><span class="p">(</span><span class="n">N_km</span><span class="p">,</span> <span class="n">N_k</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    
    <span class="n">K</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">N_km</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">N_k</span><span class="p">)</span>
    <span class="n">pi_k</span> <span class="o">=</span> <span class="p">(</span><span class="n">N_k</span> <span class="o">+</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">K</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="n">p_km</span> <span class="o">=</span> <span class="p">(</span><span class="n">N_km</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_k</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">beta</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span>
</pre></div>
</div>
</div>
</div>
<p>Using <code class="docutils literal notranslate"><span class="pre">N_k[:,</span> <span class="pre">np.newaxis]</span></code> reshapes the one-dimensional array <code class="docutils literal notranslate"><span class="pre">N_k</span></code> into a two-dimensional column vector. For example, if <code class="docutils literal notranslate"><span class="pre">N_k</span></code> has a shape of <span class="math notranslate nohighlight">\((K,)\)</span>, then <code class="docutils literal notranslate"><span class="pre">N_k[:,</span> <span class="pre">np.newaxis]</span></code> changes its shape to <span class="math notranslate nohighlight">\((K, 1)\)</span>. This allows the division in the expression</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span/><span class="n">p_km</span> <span class="o">=</span> <span class="p">(</span><span class="n">N_km</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_k</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">beta</span><span class="p">)</span>
</pre></div>
</div>
<p>to work correctly with <a class="reference external" href="https://numpy.org/doc/stable/user/basics.broadcasting.html">broadcasting</a>, ensuring that each element in a row of <code class="docutils literal notranslate"><span class="pre">N_km</span></code> is divided by the corresponding value in <code class="docutils literal notranslate"><span class="pre">N_k</span></code>.</p>
<p>The next function computes the negative logarithm of <span class="math notranslate nohighlight">\(\pi_k \prod_{m=1}^M p_{k,m}^{x_m} (1-p_{k,m})^{1-x_m}\)</span>, that is, the score of <span class="math notranslate nohighlight">\(k\)</span>, and outputs a <span class="math notranslate nohighlight">\(k\)</span> achieving the minimum score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">label_set</span><span class="p">):</span>
   
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
    
    <span class="n">score_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
       
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pi_k</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
        <span class="n">score_k</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:])</span> 
                               <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_km</span><span class="p">[</span><span class="n">k</span><span class="p">,:]))</span>

    <span class="k">return</span> <span class="n">label_set</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">score_k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We use a simple example from <a class="reference external" href="https://stackoverflow.com/questions/10059594/">Stack Overflow</a>:</p>
<blockquote>
<div><p><strong>Example:</strong> Let’s say we have data on 1000 pieces of fruit. They happen to be Banana, Orange or some Other Fruit. We know 3 characteristics about each fruit: whether it is long, whether it is sweet, and if its color is yellow[, as displayed in the table below].</p>
</div></blockquote>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Fruit</p></th>
<th class="head"><p>Long</p></th>
<th class="head"><p>Sweet</p></th>
<th class="head"><p>Yellow</p></th>
<th class="head"><p>Total</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Banana</p></td>
<td><p>400</p></td>
<td><p>350</p></td>
<td><p>450</p></td>
<td><p>500</p></td>
</tr>
<tr class="row-odd"><td><p>Orange</p></td>
<td><p>0</p></td>
<td><p>150</p></td>
<td><p>300</p></td>
<td><p>300</p></td>
</tr>
<tr class="row-even"><td><p>Other</p></td>
<td><p>100</p></td>
<td><p>150</p></td>
<td><p>50</p></td>
<td><p>200</p></td>
</tr>
<tr class="row-odd"><td><p>Total</p></td>
<td><p>500</p></td>
<td><p>650</p></td>
<td><p>800</p></td>
<td><p>1000</p></td>
</tr>
</tbody>
</table>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">N_km</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">400.</span><span class="p">,</span> <span class="mf">350.</span><span class="p">,</span> <span class="mf">450.</span><span class="p">],</span>
                 <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">150.</span><span class="p">,</span> <span class="mf">300.</span><span class="p">],</span>
                 <span class="p">[</span><span class="mf">100.</span><span class="p">,</span> <span class="mf">150.</span><span class="p">,</span> <span class="mf">50.</span><span class="p">]])</span>
<span class="n">N_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">500.</span><span class="p">,</span> <span class="mf">300.</span><span class="p">,</span> <span class="mf">200.</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We run <code class="docutils literal notranslate"><span class="pre">nb_fit_table</span></code> on our simple dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">nb_fit_table</span><span class="p">(</span><span class="n">N_km</span><span class="p">,</span> <span class="n">N_k</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.4995015 0.3000997 0.2003988]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">p_km</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0.79880478 0.69920319 0.89840637]
 [0.00331126 0.5        0.99668874]
 [0.5        0.74752475 0.25247525]]
</pre></div>
</div>
</div>
</div>
<p>Continuing on with our previous example:</p>
<blockquote>
<div><p>Let’s say that we are given the properties of an unknown fruit, and asked to classify it. We are told that the fruit is Long, Sweet and Yellow. Is it a Banana? Is it an Orange? Or Is it some Other Fruit?</p>
</div></blockquote>
<p>We run <code class="docutils literal notranslate"><span class="pre">nb_predict</span></code> on our dataset with the additional fruit from the quote above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">label_set</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'Banana'</span><span class="p">,</span> <span class="s1">'Orange'</span><span class="p">,</span> <span class="s1">'Other'</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">label_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>'Banana'
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> Laplace smoothing is a special case of a more general technique known as Bayesian parameter estimation. Ask your favorite AI chatbot to explain Bayesian parameter estimation and how it relates to maximum likelihood estimation and Laplace smoothing. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Which of the following statements is <strong>not</strong> true about conditional probabilities?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbb{P}[A|B] = \frac{\mathbb{P}[A \cap B]}{\mathbb{P}[B]}\)</span> for events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> with <span class="math notranslate nohighlight">\(\mathbb{P}[B] &gt; 0\)</span>.</p>
<p>b) If <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent, then <span class="math notranslate nohighlight">\(\mathbb{P}[A|B] = \mathbb{P}[A]\)</span>.</p>
<p>c) Conditional probabilities can be used to express the multiplication rule and the law of total probability.</p>
<p>d) <span class="math notranslate nohighlight">\(\mathbb{P}[A|B] = \mathbb{P}[B|A]\)</span> for any events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p><strong>2</strong> Which of the following is the correct mathematical expression for the conditional independence of events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> given event <span class="math notranslate nohighlight">\(C\)</span>, denoted as <span class="math notranslate nohighlight">\(A \perp\!\!\!\perp B \mid C\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] + \mathbb{P}[B \mid C]\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\mathbb{P}[A \cup B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\mathbb{P}[A \mid B \cap C] = \mathbb{P}[A \mid C]\)</span></p>
<p><strong>3</strong> In the fork configuration <span class="math notranslate nohighlight">\(Y \leftarrow X \rightarrow Z\)</span>, which of the following conditional independence relations always holds?</p>
<p>a) <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Y \mid Z\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(Y \perp\!\!\!\perp Z \mid X\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Z \mid Y\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(Y \perp\!\!\!\perp Z\)</span></p>
<p><strong>4</strong> In the collider configuration <span class="math notranslate nohighlight">\(X \rightarrow Z \leftarrow Y\)</span>, which of the following conditional independence relations always holds?</p>
<p>a) <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Y \mid Z\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(Y \perp\!\!\!\perp Z \mid X\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Z \mid Y\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Y\)</span></p>
<p><strong>5</strong> Which of the following best describes the graphical representation of the Naive Bayes model for document classification?</p>
<p>a) A chain with the topic variable at the center and word variables as the links.</p>
<p>b) A collider with the topic variable at the center and word variables as the parents.</p>
<p>c) A fork with the topic variable at the center and word variables as the prongs.</p>
<p>d) A complete graph with edges between all pairs of variables.</p>
<p>Answer for 1: d. Justification: In general, <span class="math notranslate nohighlight">\(\mathbb{P}[A|B] \neq \mathbb{P}[B|A]\)</span>. Bayes’ rule provides the correct relationship between these two conditional probabilities.</p>
<p>Answer for 2: c. Justification: The text states, “Then <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are conditionally independent given <span class="math notranslate nohighlight">\(C\)</span>, denoted <span class="math notranslate nohighlight">\(A \perp\!\!\!\perp B \mid C\)</span>, if <span class="math notranslate nohighlight">\(\mathbb{P}[A \cap B \mid C] = \mathbb{P}[A \mid C] \mathbb{P}[B \mid C]\)</span>.”</p>
<p>Answer for 3: b. Justification: The text states, “Removing the edge from <span class="math notranslate nohighlight">\(Y\)</span> to <span class="math notranslate nohighlight">\(Z\)</span> gives the following graph, known as a fork. We denote this configuration as <span class="math notranslate nohighlight">\(Y \leftarrow X \rightarrow Z\)</span>. […] The corresponding conditional independence relation is <span class="math notranslate nohighlight">\(Z \perp\!\!\!\perp Y \mid X\)</span>.”</p>
<p>Answer for 4: d. Justification: The text states, “Removing the edge from <span class="math notranslate nohighlight">\(X\)</span> to <span class="math notranslate nohighlight">\(Y\)</span> gives the following graph, known as a collider. We denote this configuration as <span class="math notranslate nohighlight">\(X \rightarrow Z \leftarrow Y\)</span>. […] This time we have <span class="math notranslate nohighlight">\(X \perp\!\!\!\perp Y\)</span>.”</p>
<p>Answer for 5: c. Justification: The text states, “Graphically, this is similar to a fork with <span class="math notranslate nohighlight">\(C\)</span> at its center and <span class="math notranslate nohighlight">\(M\)</span> prongs for the <span class="math notranslate nohighlight">\(X_m\)</span>s.”</p>
    
</body>
</html>