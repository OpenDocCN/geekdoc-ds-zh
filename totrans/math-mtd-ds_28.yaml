- en: '4.2\. Background: review of matrix rank and spectral decomposition#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/02_spectral/roch-mmids-svd-spectral.html](https://mmids-textbook.github.io/chap04_svd/02_spectral/roch-mmids-svd-spectral.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will need two additional concepts from linear algebra, the rank of a matrix
    and the spectral theorem.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Rank of a matrix[#](#rank-of-a-matrix "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that the dimension of the column space of \(A\) is called the column
    rank of \(A\). Similarly the row rank of \(A\) is the dimension of its row space.
    As it turns out, these two notions of rank are the same by the *Row Rank Equals
    Column Rank Theorem*\(\idx{row rank equals column rank theorem}\xdi\). We give
    a proof of that theorem next. We refer to the row rank and column rank of \(A\)
    simply as the rank, which we denote by \(\mathrm{rk}(A)\). \(\idx{rank}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* *(Row Rank Equals Column Rank)* Write \(A\) as a matrix factorization
    \(BC\) where the columns of \(B\) form a basis of \(\mathrm{col}(A)\). Then the
    rows of \(C\) necessarily form a spanning set of \(\mathrm{row}(A)\). So, because
    the number of columns of \(B\) and the number of rows of \(C\) match, we conclude
    that the row rank is less or equal than the column rank. Applying the same argument
    to the transpose gives the claim.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall the following observation from a previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation D1:** Any linear subspace \(U\) of \(\mathbb{R}^n\) has a basis.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation D2:** If \(U\) and \(V\) are linear subspaces such that \(U \subseteq
    V\), then \(\mathrm{dim}(U) \leq \mathrm{dim}(V)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation D3:** The dimension of \(\mathrm{span}(\mathbf{u}_1,\ldots,\mathbf{u}_m)\)
    is at most \(m\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Row Rank Equals Column Rank)* Assume that \(A\) has column rank
    \(r\). Then there exists a basis \(\mathbf{b}_1,\ldots, \mathbf{b}_r \in \mathbb{R}^n\)
    of \(\mathrm{col}(A)\) by *Observation D1* above, and we know that \(r \leq n\)
    by *Observation D2*. That is, for each \(j\), letting \(\mathbf{a}_{j} = A_{\cdot,j}\)
    be the \(j\)-th column of \(A\) we can write'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{a}_{j} = \sum_{\ell=1}^r \mathbf{b}_\ell c_{\ell j} \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(c_{\ell j}\)’s. Let \(B\) be the matrix whose columns are \(\mathbf{b}_1,
    \ldots, \mathbf{b}_r\) and let \(C\) be the matrix with entries \((C)_{\ell j}
    = c_{\ell j}\), \(\ell=1,\ldots,r\), \(j=1,\ldots,m\). Then the equation above
    can be re-written as the matrix factorization \(A = BC\). Indeed, by our previous
    observations about matrix-matrix products, the columns of \(A\) are linear combinations
    of the columns of \(B\) with coefficients taken from the corresponding column
    of \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The key point is the following: \(C\) necessarily has \(r\) rows. Let \(\boldsymbol{\alpha}_{i}^T
    = A_{i,\cdot}\) be the \(i\)-th row of \(A\) and \(\mathbf{c}_{\ell}^T = C_{\ell,\cdot}\)
    be the \(\ell\)-th row of \(C\). Using our alternative representation of matrix-matrix
    product in terms of rows, the decomposition is equivalent to'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \boldsymbol{\alpha}_{i}^T = \sum_{\ell=1}^r b_{i\ell} \mathbf{c}_\ell^T,
    \quad i=1,\ldots, n, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(b_{i\ell} = (\mathbf{b}_i)_\ell = (B)_{i\ell}\) is the \(i\)-th entry
    of the \(\ell\)-th column of \(B\). In words, the rows of \(A\) are linear combinations
    of the rows of \(C\) with coefficients taken from the corresponding row of \(B\).
    In particular, \(\mathcal{C} = \{\mathbf{c}_{j}:j=1,\ldots,r\}\) is a spanning
    list of the row space of \(A\), that is, each row of \(A\) can be written as a
    linear combination of \(\mathcal{C}\). Put differently, \(\mathrm{row}(A) \subseteq
    \mathrm{span}(\mathcal{C})\).
  prefs: []
  type: TYPE_NORMAL
- en: So the row rank of \(A\) is at most \(r\), the column rank of \(A\), by *Observation
    D2*.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the same argument to \(A^T\), which switches the role of the columns
    and the rows, gives that the column rank of \(A\) (i.e. the row rank of \(A^T\))
    is at most the row rank of \(A\) (i.e. the column rank of \(A^T\)). Hence the
    two notions of rank must be equal. (We also deduce \(r \leq m\) by *Observation
    D2* again.) \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(continued)** We illustrate the proof of the theorem. Continuing
    a previous example, let \(A\) be the matrix with columns \(\mathbf{w}_1 = (1,0,1)\),
    \(\mathbf{w}_2 = (0,1,1)\), and \(\mathbf{w}_3 = (1,-1,0)\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0 & 1\\ 0 & 1 & -1\\ 1 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We know that \(\mathbf{w}_1\) and \(\mathbf{w}_2\) form a basis of \(\mathrm{col}(A)\).
    We use them to construct our matrix \(B\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} 1 & 0\\ 0 & 1\\ 1 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that \(\mathbf{w}_3 = \mathbf{w}_1 - \mathbf{w}_2\). Hence the matrix
    \(C\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} C = \begin{pmatrix} 1 & 0 & 1\\ 0 & 1 & -1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, column \(j\) of \(C\) gives the coefficients in the linear combination
    of the columns of \(B\) that produces column \(j\) of A. Check that \(A = B C\).
    \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Numpy, one can compute the rank of a matrix using
    the function [`numpy.linalg.matrix_rank`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_rank.html).
    We will see later in the chapter how to compute it using the singular value decomposition
    (which is how `LA.matrix_rank` does it). Let’s try the example above.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We compute the rank of `A`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We take only the first two columns of `A` this time to form `B`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Recall that, in Numpy, `@` is used for matrix product.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times k}\) and \(B \in \mathbb{R}^{k
    \times m}\). Then we claim that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{rk}(AB) \leq \mathrm{rk}(A). \]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the columns of \(AB\) are linear combinations of the columns of \(A\).
    Hence \(\mathrm{col}(AB) \subseteq \mathrm{col}(A)\). The claim follows by *Observation
    D2*. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times k}\) and \(B \in \mathbb{R}^{n
    \times m}\). Then we claim that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{rk}(A + B) \leq \mathrm{rk}(A) + \mathrm{rk}(B). \]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the columns of \(A + B\) are linear combinations of the columns of \(A\)
    and of \(B\). Let \(\mathbf{u}_1,\ldots,\mathbf{u}_h\) be a basis of \(\mathrm{col}(A)\)
    and let \(\mathbf{v}_1,\ldots,\mathbf{v}_{\ell}\) be a basis of \(\mathrm{col}(B)\)
    by *Observation D1*. Then, we deduce
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{col}(A + B) \subseteq \mathrm{span}(\mathbf{u}_1,\ldots,\mathbf{u}_h,\mathbf{v}_1,\ldots,\mathbf{v}_{\ell}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: By *Observation D2*, it follows that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{rk}(A + B) \leq \mathrm{dim}(\mathrm{span}(\mathbf{u}_1,\ldots,\mathbf{u}_h,\mathbf{v}_1,\ldots,\mathbf{v}_{\ell})).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: By *Observation D3*, the right hand side is at most the length of the spanning
    list, i.e., \(h+\ell\). But by construction \(\mathrm{rk}(A) = h\) and \(\mathrm{rk}(B)
    = \ell\), so we are done. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(A Proof of the Rank-Nullity Theorem)** \(\idx{rank-nullity
    theorem}\xdi\) Let \(A \in \mathbb{R}^{n \times m}\). Recall that the column space
    of \(A\), \(\mathrm{col}(A) \subseteq \mathbb{R}^n\), is the span of its columns.
    We compute its othogonal complement. By definition, the columns of \(A\), which
    we denote by \(\mathbf{a}_1,\ldots,\mathbf{a}_m\), form a spanning list of \(\mathrm{col}(A)\).
    So \(\mathbf{u} \in \mathrm{col}(A)^\perp \subseteq \mathbb{R}^n\) if and only
    if'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{a}_i^T\mathbf{u} = \langle \mathbf{u}, \mathbf{a}_i \rangle = 0,
    \quad \forall i=1,\ldots,m. \]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed that then implies that for any \(\mathbf{v} \in \mathrm{col}(A)\), say
    \(\mathbf{v} = \beta_1 \mathbf{a}_1 + \cdots + \beta_m \mathbf{a}_m\) we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left\langle \mathbf{u}, \sum_{i=1}^m \beta_i \mathbf{a}_i \right\rangle
    = \sum_{i=1}^m \beta_i \langle \mathbf{u}, \mathbf{a}_i \rangle = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: The \(m\) conditions above can be written in matrix form as
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T \mathbf{u} = \mathbf{0}. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, the orthogonal complement of the column space of \(A\) is the null
    space of \(A^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{col}(A)^\perp = \mathrm{null}(A^T). \]
  prefs: []
  type: TYPE_NORMAL
- en: Applying the same argument to the column space of \(A^T\), it follows that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{col}(A^T)^\perp = \mathrm{null}(A), \]
  prefs: []
  type: TYPE_NORMAL
- en: where note that \(\mathrm{null}(A) \subseteq \mathbb{R}^m\). The four linear
    subspaces \(\mathrm{col}(A)\), \(\mathrm{col}(A^T)\), \(\mathrm{null}(A)\) and
    \(\mathrm{null}(A^T)\) are referred to as the fundamental subspaces of \(A\).
    We have shown
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{col}(A) \oplus \mathrm{null}(A^T) = \mathbb{R}^n \quad \text{and}
    \quad \mathrm{col}(A^T) \oplus \mathrm{null}(A) = \mathbb{R}^m \]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Row Rank Equals Column Rank Theorem*, \(\mathrm{dim}(\mathrm{col}(A))
    = \mathrm{dim}(\mathrm{col}(A^T))\). Moreover, by our previous observation about
    the dimensions of direct sums, we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ n = \mathrm{dim}(\mathrm{col}(A)) + \mathrm{dim}(\mathrm{null}(A^T)) = \mathrm{dim}(\mathrm{col}(A^T))
    + \mathrm{dim}(\mathrm{null}(A^T)) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ m = \mathrm{dim}(\mathrm{col}(A^T)) + \mathrm{dim}(\mathrm{null}(A)) = \mathrm{dim}(\mathrm{col}(A))
    + \mathrm{dim}(\mathrm{null}(A)). \]
  prefs: []
  type: TYPE_NORMAL
- en: So we deduce that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}(\mathrm{null}(A)) = m - \mathrm{rk}(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}(\mathrm{null}(A^T)) = n - \mathrm{rk}(A). \]
  prefs: []
  type: TYPE_NORMAL
- en: These formulas are referred to the *Rank-Nullity Theorem*. The dimension of
    the null space of \(A\) is called the nullity of \(A\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Outer products and rank-one matrices** Let \(\mathbf{u} = (u_1,\ldots,u_n)
    \in \mathbb{R}^n\) and \(\mathbf{v} = (v_1,\ldots,v_m) \in \mathbf{R}^m\) be two
    column vectors. Their outer product\(\idx{outer product}\xdi\) is defined as the
    matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{u} \mathbf{v}^T = \begin{pmatrix} u_1 v_1 & u_1 v_2
    & \cdots & u_1 v_m\\ u_2 v_1 & u_2 v_2 & \cdots & u_2 v_m\\ \vdots & \vdots &
    \ddots & \vdots\\ u_n v_1 & u_n v_2 & \cdots & u_n v_m \end{pmatrix} = \begin{pmatrix}
    | & & | \\ v_{1} \mathbf{u} & \ldots & v_{m} \mathbf{u} \\ | & & | \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This is not to be confused with the inner product \(\mathbf{u}^T \mathbf{v}\),
    which requires \(n = m\) and produces a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: If \(\mathbf{u}\) and \(\mathbf{v}\) are nonzero, the matrix \(\mathbf{u} \mathbf{v}^T\)
    has rank one. Indeed, its columns are all a multiple of the same vector \(\mathbf{u}\).
    So the column space spanned by the columns of \(\mathbf{u} \mathbf{v}^T\) is one-dimensional.
    Vice versa, any rank-one matrix can be written in this form by definition of the
    rank.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen many different interpretations of matrix-matrix products. Here
    is yet another one. Let \(A = (a_{ij})_{i,j} \in \mathbb{R}^{n \times k}\) and
    \(B = (b_{ij})_{i,j} \in \mathbb{R}^{k \times m}\). Denote by \(\mathbf{a}_1,\ldots,\mathbf{a}_k\)
    the columns of \(A\) and denote by \(\mathbf{b}_1^T,\ldots,\mathbf{b}_k^T\) the
    rows of \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A B &= \begin{pmatrix} \sum_{j=1}^k a_{1j} b_{j1} & \sum_{j=1}^k
    a_{1j} b_{j2} & \cdots & \sum_{j=1}^k a_{1j} b_{jm}\\ \sum_{j=1}^k a_{2j} b_{j1}
    & \sum_{j=1}^k a_{2j} b_{j2} & \cdots & \sum_{j=1}^k a_{2j} b_{jm}\\ \vdots &
    \vdots & \ddots & \vdots\\ \sum_{j=1}^k a_{nj} b_{j1} & \sum_{j=1}^k a_{nj} b_{j2}
    & \cdots & \sum_{j=1}^k a_{nj} b_{jm} \end{pmatrix}\\ &= \sum_{j=1}^k \begin{pmatrix}
    a_{1j} b_{j1} & a_{1j} b_{j2} & \cdots & a_{1j} b_{jm}\\ a_{2j} b_{j1} & a_{2j}
    b_{j2} & \cdots & a_{2j} b_{jm}\\ \vdots & \vdots & \ddots & \vdots\\ a_{nj} b_{j1}
    & a_{nj} b_{j2} & \cdots & a_{nj} b_{jm} \end{pmatrix}\\ &= \sum_{j=1}^k \mathbf{a}_j
    \mathbf{b}_j^T. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: In words, the matrix product \(AB\) can be interpreted as a sum of \(k\) rank-one
    matrices, each of which is the outer product of a column of \(A\) with the corresponding
    row of \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: Because the rank of a sum is at most the sum of the ranks (as shown in a previous
    example), it follows that the rank of \(AB\) is at most \(k\). This is consistent
    with the fact that the rank of a product is at most the minimum of the ranks (also
    shown in a previous example).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Eigenvalues and eigenvectors[#](#eigenvalues-and-eigenvectors "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall the concepts of eigenvalues\(\idx{eigenvalue}\xdi\) and eigenvectors\(\idx{eigenvector}\xdi\).
    We work on \(\mathbb{R}^d\).
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Eigenvalues and Eigenvectors)** Let \(A \in \mathbb{R}^{d
    \times d}\) be a square matrix. Then \(\lambda \in \mathbb{R}\) is an eigenvalue
    of \(A\) if there exists a nonzero vector \(\mathbf{x} \neq \mathbf{0}\) such
    that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x} = \lambda \mathbf{x}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The vector \(\mathbf{x}\) is referred to as an eigenvector. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: As the next example shows, not every matrix has a (real) eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(No Real Eigenvalues)** Set \(d = 2\) and let'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: For \(\lambda\) to be an eigenvalue, there must be an nonzero eigenvector \(\mathbf{x}
    = (x_1, x_2)\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x} = \lambda \mathbf{x} \]
  prefs: []
  type: TYPE_NORMAL
- en: or put differently \(- x_2 = \lambda x_1\) and \(x_1 = \lambda x_2\). Replacing
    these equations into each other, it must be that \(- x_2 = \lambda^2 x_2\) and
    \(x_1 = - \lambda^2 x_1\). Because \(x_1, x_2\) cannot both be \(0\), \(\lambda\)
    must satisfy the equation \(\lambda^2 = -1\) for which there is no real solution.
    \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: In general, \(A \in \mathbb{R}^{d \times d}\) has at most \(d\) distinct eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Number of Eigenvalues)** \(\idx{number of eigenvalues lemma}\xdi\)
    Let \(A \in \mathbb{R}^{d \times d}\) and let \(\lambda_1, \ldots, \lambda_m\)
    be distinct eigenvalues of \(A\) with corresponding eigenvectors \(\mathbf{x}_1,
    \ldots, \mathbf{x}_m\). Then \(\mathbf{x}_1, \ldots, \mathbf{x}_m\) are linearly
    independent. In particular, \(m \leq d\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Assume by contradiction that \(\mathbf{x}_1, \ldots, \mathbf{x}_m\)
    are linearly dependent. By the *Linear Dependence Lemma*, there is \(k \leq m\)
    such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}_k \in \mathrm{span}(\mathbf{x}_1, \ldots, \mathbf{x}_{k-1}) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{x}_1, \ldots, \mathbf{x}_{k-1}\) are linearly independent. In
    particular, there are \(a_1, \ldots, a_{k-1}\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}_k = a_1 \mathbf{x}_1 + \cdots + a_{k-1} \mathbf{x}_{k-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform the equation above in two ways: (1) multiply both sides by \(\lambda_k\)
    and (2) apply \(A\). Then subtract the resulting equations. That leads to'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{0} = a_1 (\lambda_k - \lambda_1) \mathbf{x}_1 + \cdots + a_{k-1}
    (\lambda_k - \lambda_{k-1}) \mathbf{x}_{k-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Because the \(\lambda_i\)’s are distinct and \(\mathbf{x}_1, \ldots, \mathbf{x}_{k-1}\)
    are linearly independent, we must have \(a_1 = \cdots = a_{k-1} = 0\). But that
    implies that \(\mathbf{x}_k = \mathbf{0}\), a contradiction.
  prefs: []
  type: TYPE_NORMAL
- en: For the second claim, if there were more than \(d\) distinct eigenvalues, then
    there would be more than \(d\) corresponding linearly independent eigenvectors
    by the first claim, a contradiction. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Diagonal (and Similar) Matrices)** Recall that we use the notation
    \(\mathrm{diag}(\lambda_1,\ldots,\lambda_d)\) for the diagonal matrix\(\idx{diagonal
    matrix}\xdi\) with diagonal entries \(\lambda_1,\ldots,\lambda_d\). The upper
    bound in the *Number of Eigenvalues Lemma* can be achieved, for instance, by diagonal
    matrices with distinct diagonal entries \(A = \mathrm{diag}(\lambda_1, \ldots,
    \lambda_d)\). Each standard basis vector \(\mathbf{e}_i\) is then an eigenvector'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{e}_i = \lambda_i \mathbf{e}_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: More generally, let \(A\) be similar to a matrix \(D = \mathrm{diag}(\lambda_1,
    \ldots, \lambda_d)\) with distinct diagonal entries, that is, there exists a nonsingular
    matrix \(P\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = P D P^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Let \(\mathbf{p}_1, \ldots, \mathbf{p}_d\) be the columns of \(P\). Note that,
    because the columns of \(P\) form a basis of \(\mathbb{R}^d\), the entries of
    the vector \(\mathbf{c} = P^{-1} \mathbf{x}\) are the coefficients of the unique
    linear combination of the \(\mathbf{p}_i\)’s equal to \(\mathbf{x}\). Indeed,
    \(P \mathbf{c} = \mathbf{x}\). Hence, \(A \mathbf{x}\) is can be thought of as:
    (1) expressing \(\mathbf{x}\) in the basis \(\mathbf{p}_1, \ldots, \mathbf{p}_d\),
    (2) and scaling the \(\mathbf{p}_i\)’s by the corresponding \(\lambda_i\)’s. In
    particular, the \(\mathbf{p}_i\)’s are eigenvectors of \(A\) since, by the above,
    \(P^{-1} \mathbf{p}_i = \mathbf{e}_i\) and so'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{p}_i = P D P^{-1} \mathbf{p}_i = P D \mathbf{e}_i = P (\lambda_i
    \mathbf{e}_i) = \lambda_i \mathbf{p}_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Numpy, the eigenvalues and eigenvectors of a matrix
    can be computed using [`numpy.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Above, `w` are the eigenvalues in an array, whereas the columns of `v` are the
    corresponding eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Some matrix algebra** We will need a few useful observations about matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: A (not necessarily square) matrix \(D \in \mathbb{R}^{k \times r}\) is diagonal
    if its non-diagonal entries are zero. That is, \(i \neq j\) implies that \(D_{ij}
    =0\). Note that a diagonal matrix is not necessarily square and that the diagonal
    elements are allowed to be zero.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying a matrix by a diagonal one has a very specific effect. Let \(A \in
    \mathbb{R}^{n \times k}\) and \(B \in \mathbb{R}^{r \times m}\). We focus here
    on the case \(k \geq r\). The matrix product \(A D\) produces a matrix whose columns
    are linear combinations of the columns of \(A\) where the coefficients are taken
    from the corresponding column of \(D\). But the columns of \(D\) have at most
    one nonzero elements, the diagonal one. So the columns of \(AD\) are in fact multiples
    of the columns of \(A\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} AD = \begin{pmatrix} | & & | \\ d_{11} \mathbf{a}_1 & \ldots
    & d_{rr} \mathbf{a}_r \\ | & & | \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{a}_1,\ldots,\mathbf{a}_k\) are the columns of \(A\) and \(d_{11},\ldots,d_{rr}\)
    are the diagonal elements of \(D\).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the rows of \(D B\) are linear combinations of the rows of \(B\)
    where the coefficients are taken from the corresponding row of \(D\). The rows
    of \(D\) have at most one nonzero element, the diagonal one. In the case, \(k
    \geq r\), rows \(r+1,\ldots,k\) necessarily have only zero entries since there
    is no diagonal entry there. Hence the first \(r\) rows of \(DB\) are multiples
    of the rows of \(B\) and the next \(k-r\) are \(\mathbf{0}\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} DB = \begin{pmatrix} \horz & d_{11} \mathbf{b}_1^T & \horz \\
    & \vdots &\\ \horz & d_{rr} \mathbf{b}_r^T & \horz\\ \horz & \mathbf{0} & \horz\\
    & \vdots &\\ \horz & \mathbf{0} & \horz \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{b}_1^T,\ldots,\mathbf{b}_r^T\) are the rows of \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** The following special case will be useful later in this chapter.
    Suppose \(D, F \in \mathbb{R}^{n \times n}\) are both square diagonal matrices.
    Then \(D F\) is the matrix whose diagonal elements are \(d_{11} f_{11}, \ldots,d_{nn}
    f_{nn}\). \(\lhd\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spectral theorem** When \(A\) is symmetric, a remarkable result is that there
    exists an orthonormal basis of \(\mathbb{R}^d\) made of eigenvectors of \(A\).
    We will prove this result in a subsequent chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** Let \(A \in \mathbb{R}^{d \times d}\) be symmetric. Show
    that for any \(\mathbf{u}\) and \(\mathbf{v}\) in \(\mathbb{R}^d\) it holds that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{u}, A \mathbf{v} \rangle = \langle A \mathbf{u}, \mathbf{v}
    \rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\checkmark\)
  prefs: []
  type: TYPE_NORMAL
- en: Before stating the result formally, we make a few observations. Let \(A \in
    \mathbb{R}^{d \times d}\) be symmetric. Suppose that \(\mathbf{v}_i\) and \(\mathbf{v}_j\)
    are eigenvectors corresponding respectively to distinct eigenvalues \(\lambda_i\)
    and \(\lambda_j\). Then the following quantity can be written in two ways
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}_i, A \mathbf{v}_j \rangle = \langle \mathbf{v}_i, \lambda_j
    \mathbf{v}_j \rangle = \lambda_j \langle \mathbf{v}_i, \mathbf{v}_j \rangle \]
  prefs: []
  type: TYPE_NORMAL
- en: and, by symmetry of \(A\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}_i, A \mathbf{v}_j \rangle = \mathbf{v}_i^T A \mathbf{v}_j
    = \mathbf{v}_i^T A^T \mathbf{v}_j = (A \mathbf{v}_i)^T \mathbf{v}_j = \langle
    A \mathbf{v}_i, \mathbf{v}_j \rangle = \langle \lambda_i \mathbf{v}_i, \mathbf{v}_j
    \rangle = \lambda_i \langle \mathbf{v}_i, \mathbf{v}_j \rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: Subtracting the two,
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\lambda_j - \lambda_i) \langle \mathbf{v}_i, \mathbf{v}_j \rangle = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: and using that \(\lambda_i \neq \lambda_j\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}_i, \mathbf{v}_j \rangle = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, \(\mathbf{v}_i\) and \(\mathbf{v}_j\) are necessarily orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: 'We proved:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** Let \(A \in \mathbb{R}^{d \times d}\) be symmetric. Suppose that
    \(\mathbf{v}_i\) and \(\mathbf{v}_j\) are eigenvectors corresponding to distinct
    eigenvalues. Then \(\mathbf{v}_i\) and \(\mathbf{v}_j\) are orthogonal. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: This lemma gives a different proof – in the symmetric case – that the number
    of eigenvalues is at most \(d\) since a list of pairwise orthogonal vectors are
    linearly independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact:'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Spectral)** \(\idx{spectral theorem}\xdi\) Let \(A \in \mathbb{R}^{d
    \times d}\) be a symmetric matrix, that is, \(A^T = A\). Then \(A\) has \(d\)
    orthonormal eigenvectors \(\mathbf{q}_1, \ldots, \mathbf{q}_d\) with corresponding
    (not necessarily distinct) real eigenvalues \(\lambda_1 \geq \lambda_2 \geq \cdots
    \geq \lambda_d\). Moreover, \(A\) can be written as the matrix factorization'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(Q\) has columns \(\mathbf{q}_1, \ldots, \mathbf{q}_d\) and \(\Lambda
    = \mathrm{diag}(\lambda_1, \ldots, \lambda_d)\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: We refer to this factorization as a spectral decomposition\(\idx{spectral decomposition}\xdi\)
    of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: Note that this decomposition indeed produces the eigenvectors of \(A\). For
    any \(j\), we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{q}_j = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \mathbf{q}_j
    = \lambda_j \mathbf{q}_j, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that, by orthonormality, \(\mathbf{q}_i^T \mathbf{q}_j = 0\) if
    \(i \neq j\) and \(\mathbf{q}_i^T \mathbf{q}_j = 1\) if \(i = j\). The equation
    above says precisely that \(\mathbf{q}_j\) is an eigenvector of \(A\) with corresponding
    eigenvalue \(\lambda_j\). Since we have found \(d\) eigenvalues (possibly with
    repetition), we have found all of them.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\lambda_1, \lambda_2, \ldots, \lambda_d\) be the eigenvalues whose existence
    is guaranteed by the *Spectral Theorem*. We first argue that there is no other
    eigenvalue. Indeed, assume \(\mu \neq \lambda_1, \lambda_2, \ldots, \lambda_d\)
    is an eigenvalue with corresponding eigenvector \(\mathbf{p}\). We have seen that
    \(\mathbf{p}\) is orthogonal to the eigenvectors \(\mathbf{q}_1, \ldots, \mathbf{q}_d\).
    Since the latter list forms an orthonormal basis of \(\mathbb{R}^d\), this cannot
    be the case and we have a contradiction.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the eigenvalues \(\lambda_1, \lambda_2, \ldots, \lambda_d\) can be repeated
    however, that is, there can be \(i, j\) such that \(\lambda_{i} = \lambda_{j}\).
    For instance, if \(A = I_{d \times d}\) is the identity matrix, then the eigenvalues
    are \(\lambda_i = 1\) for all \(i \in [d]\).
  prefs: []
  type: TYPE_NORMAL
- en: For a fixed eigenvalue \(\lambda\) of \(A\), the set of eigenvectors with eigenvalue
    \(\lambda\) satisfy
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{v} = \lambda \mathbf{v} = \lambda I_{d \times d} \mathbf{v} \]
  prefs: []
  type: TYPE_NORMAL
- en: or, rearranging,
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A - \lambda I_{d \times d})\mathbf{v} = \mathbf{0}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, it is the set \(\mathrm{null}(A - \lambda I_{d \times d})\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The eigenvectors in the *Spectral Theorem* corresponding the same eigenvalue
    \(\lambda\) can be replaced by any orthonormal basis of the subspace \(\mathrm{null}(A
    - \lambda I_{d \times d})\). But, beyond this freedom, we have the following characterization:
    let \(\mu_1,\ldots,\mu_f\) be the unique values in \(\lambda_1, \lambda_2, \ldots,
    \lambda_d\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{R}^d = \mathrm{null}(A - \mu_1 I_{d \times d}) \oplus \mathrm{null}(A
    - \mu_2 I_{d \times d}) \oplus \cdots \oplus \mathrm{null}(A - \mu_f I_{d \times
    d}), \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the fact that for any \(\mathbf{u} \in \mathrm{null}(A - \mu_i
    I_{d \times d})\) and \(\mathbf{v} \in \mathrm{null}(A - \mu_j I_{d \times d})\)
    with \(i \neq j\), we have that \(\mathbf{u}\) is orthogonal to \(\mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: We have shown in particular that the sequence of eigenvalues in the *Spectral
    Theorem* is unique (counting repeats).
  prefs: []
  type: TYPE_NORMAL
- en: Two matrices \(B, D \in \mathbb{R}^{d \times d}\) are [similar](https://en.wikipedia.org/wiki/Matrix_similarity)\(\idx{similar}\xdi\)
    if there is an invertible matrix \(P\) such that \(B = P^{-1} D P\). It can be
    shown that \(B\) and \(D\) correspond to the same linear map, but expressed in
    different bases. When \(P = Q\) is an orthogonal matrix, the transformation simplifies
    to \(B = Q^T D Q\).
  prefs: []
  type: TYPE_NORMAL
- en: Hence, a different way to think about a spectral decomposition is that it expresses
    the fact that any symmetric matrix is similar to a diagonal matrix through an
    orthogonal transformation. The basis in which the corresponding linear map is
    represented by a diagonal matrix is the basis of eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Eigendecomposition of \(2 \times 2\) symmetric matrix)** The
    simplest non-trivial case is the \(2 \times 2\) symmetric matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} a & b\\ b & d \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We derive a step-by-step recipe to compute its eigenvalues and eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: As shown previously, an eigenvalue \(\lambda\) corresponds to a nonempty \(\mathrm{null}(A
    - \lambda I_{2 \times 2})\) and the corresponding eigenvector solves
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{0} = (A - \lambda I_{2 \times 2})\mathbf{v} = \begin{pmatrix}a
    - \lambda & b\\ b & d - \lambda\end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, the matrix \(\begin{pmatrix}a - \lambda & b\\ b & d - \lambda\end{pmatrix}\)
    has linearly dependent columns. We have seen that one way to check this is to
    compute the determinant which in the \(2 \times 2\) case is simply
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathrm{det}\left[\begin{pmatrix}a - \lambda & b\\ b & d - \lambda\end{pmatrix}\right]
    = (a - \lambda ) (d - \lambda) - b^2. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This is a polynomial of degree \(2\) in \(\lambda\) called the characteristic
    polynomial of the matrix \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: The roots of the characteristic polynomial, that is, the solutions of
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = (a - \lambda ) (d - \lambda) - b^2 = \lambda^2 - (a + d)\lambda + (ad
    - b^2), \]
  prefs: []
  type: TYPE_NORMAL
- en: are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{1} = \frac{(a + d) + \sqrt{(a + d)^2 - 4(ad - b^2)}}{2} \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{2} = \frac{(a + d) - \sqrt{(a + d)^2 - 4(ad - b^2)}}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Expanding the expression in the square root
  prefs: []
  type: TYPE_NORMAL
- en: \[ (a + d)^2 - 4(ad - b^2) = a^2 + d^2 - 2ad + 4b^2 = (a - d)^2 + 4b^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: we see that the square root is well-defined (i.e., produces a real value) for
    any \(a, b, d\).
  prefs: []
  type: TYPE_NORMAL
- en: It remains to find the corresponding eigenvectors \(\mathbf{v}_{1} = (v_{1,1},
    v_{1,2})\) and \(\mathbf{v}_2 = (v_{2,1}, v_{2,2})\) by solving the \(2 \times
    2\) systems of linear equations
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} a - \lambda_i & b \\ b & d - \lambda_i \end{pmatrix}
    \begin{pmatrix} v_{i,1} \\ v_{i,2} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: which is guaranteed to have a solution. When \(\lambda_1 = \lambda_2\), one
    needs to find two linearly independent solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a numerical example. Consider the matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The characteristic polynomial equation is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda^2 - 6\lambda + 8 = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{1}, \lambda_{2} = \frac{6 \pm \sqrt{36 - 4(9-1))}}{2} = \frac{6
    \pm \sqrt{4}}{2} = 4, 2. \]
  prefs: []
  type: TYPE_NORMAL
- en: We then solve for the eigenvectors. For \(\lambda_1 = 4\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} 3 - \lambda_1 & 1 \\ 1 & 3 - \lambda_1 \end{pmatrix}
    \begin{pmatrix} v_{1,1} \\ v_{1,2} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
    \Leftrightarrow \begin{cases} - v_{1,1} + v_{1,2} = 0\\ v_{1,1} - v_{1,2} = 0
    \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so, after normalizing, we take
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{v}_1 = \frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ 1\end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(\lambda_2 = 2\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} 3 - \lambda_2 & 1 \\ 1 & 3 - \lambda_2 \end{pmatrix}
    \begin{pmatrix} v_{2,1} \\ v_{2,2} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
    \Leftrightarrow \begin{cases} v_{1,1} + v_{1,2} = 0\\ v_{1,1} + v_{1,2} = 0 \end{cases}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so, after normalizing, we take
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{v}_2 = \frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ -1\end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The fact that these are the eigenvectors can be checked by hand (try it!). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: More generally, for any matrix \(A \in \mathbb{R}^{d \times d}\), the roots
    of the characteristic polynomial \(\mathrm{det}(A - \lambda I_{d \times d})\)
    are eigenvalues of \(A\). We will derive a more efficient numerical approach to
    compute them in a subsequent section.
  prefs: []
  type: TYPE_NORMAL
- en: '**The case of positive semidefinite matrices** The eigenvalues of a symmetric
    matrix – while real – may be negative. There is however an important special case
    where the eigenvalues are nonnegative, positive semidefinite matrices. \(\idx{positive
    semidefinite matrix}\xdi\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Characterization of Positive Semidefiniteness)** \(\idx{characterization
    of positive semidefiniteness}\xdi\) Let \(A \in \mathbb{R}^{d \times d}\) be a
    symmetric matrix and let \(A = Q \Lambda Q^T\) be a spectral decomposition of
    \(A\) with \(\Lambda = \mathrm{diag}(\lambda_1, \ldots, \lambda_d)\). Then \(A
    \succeq 0\) if and only if its eigenvalues \(\lambda_1, \ldots, \lambda_d\) are
    nonnegative. \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Assume \(A \succeq 0\). Let \(\mathbf{q}_i\) be an eigenvector of
    \(A\) with corresponding eigenvalue \(\lambda_i\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{q}_i, A \mathbf{q}_i \rangle = \langle \mathbf{q}_i, \lambda_i
    \mathbf{q}_i \rangle = \lambda_i \]
  prefs: []
  type: TYPE_NORMAL
- en: which must be nonnegative by definition of a positive semidefinite matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In the other direction, assume \(\lambda_1, \ldots, \lambda_d \geq 0\). Then,
    by the spectral decomposition in outer-product form
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{x}, A \mathbf{x} \rangle = \mathbf{x}^T \left(\sum_{i=1}^d
    \lambda_i \mathbf{q}_i \mathbf{q}_i^T\right) \mathbf{x} = \sum_{i=1}^d \lambda_i
    \mathbf{x}^T\mathbf{q}_i \mathbf{q}_i^T\mathbf{x} = \sum_{i=1}^d \lambda_i \langle
    \mathbf{q}_i, \mathbf{x}\rangle^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: which is necessarily nonnegative. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a symmetric matrix is positive definite\(\idx{positive definite matrix}\xdi\)
    if and only if all its eigenvalues are strictly positive. The proof is essentially
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** Prove this last claim by modifying the proof above. \(\checkmark\)'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that an important application of positive semidefiniteness is as a characterization
    of convexity\(\idx{convex function}\xdi\). Here are some examples.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Convexity via eigenvalues of Hessian)** Consider the function'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x, y) = \frac{3}{2} x^2 + xy + \frac{3}{2} y^2 + 5x - 2y + 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: To show it is convex, we compute its Hessian
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} H_f(x,y) = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(x,y\). By a previous example, its eigenvalues are \(2\) and \(4\),
    both of which are strictly positive. That proves the claim by the *Second-Order
    Convexity Condition*. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Log-concavity)** A function \(f :\mathbb{R}^d \to \mathbb{R}\)
    is said to be log-concave if \(-\log f\) is convex. Put differently, we require
    for all \(\mathbf{x}, \mathbf{y} \in \mathbb{R}^d\) and \(\alpha \in (0,1)\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ - \log f((1-\alpha)\mathbf{x} + \alpha \mathbf{y}) \leq - (1-\alpha) \log
    f(\mathbf{x}) - \alpha \log f(\mathbf{y}), \]
  prefs: []
  type: TYPE_NORMAL
- en: This is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \log f((1-\alpha)\mathbf{x} + \alpha \mathbf{y}) \geq (1-\alpha) \log f(\mathbf{x})
    + \alpha \log f(\mathbf{y}) , \]
  prefs: []
  type: TYPE_NORMAL
- en: Or, because \(a \log b = \log b^a\) and the logarithm is strictly increasing,
  prefs: []
  type: TYPE_NORMAL
- en: \[ f((1-\alpha)\mathbf{x} + \alpha \mathbf{y}) \geq f(\mathbf{x})^{1-\alpha}
    f(\mathbf{y})^{\alpha}. \]
  prefs: []
  type: TYPE_NORMAL
- en: We will see later in the course that a multivariate Gaussian vector \(\mathbf{X}\)
    on \(\mathbb{R}^d\) with mean \(\bmu \in \mathbb{R}^d\) and positive definite
    covariance matrix \(\bSigma \in \mathbb{R}^{d \times d}\) has probability density
    function (PDF)
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} \,|\bSigma|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(|A|\) is the [determinant](https://en.wikipedia.org/wiki/Determinant)
    of \(A\), which in the case of a symmetric matrix is simply the product of its
    eigenvalues (with repeats). We claim that this PDF is log-concave.
  prefs: []
  type: TYPE_NORMAL
- en: From the definition,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &- \log f_{\bmu, \bSigma}(\mathbf{x})\\ &= \frac{1}{2}(\mathbf{x}
    - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu) + \log (2\pi)^{d/2} \,|\bSigma|^{1/2}\\
    &= \frac{1}{2}\mathbf{x}^T \bSigma^{-1} \mathbf{x} - \bmu^T \bSigma^{-1} \mathbf{x}
    + \left[\frac{1}{2}\bmu^T \bSigma^{-1} \bmu + \log (2\pi)^{d/2} \,|\bSigma|^{1/2}\right].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(P = \bSigma^{-1}\), \(\mathbf{q} = - \bmu^T \bSigma^{-1}\) and \(r = \frac{1}{2}\bmu^T
    \bSigma^{-1} \bmu + \log (2\pi)^{d/2} \,|\bSigma|^{1/2}\).
  prefs: []
  type: TYPE_NORMAL
- en: A previous example then implies that the PDF is log-concave if \(\bSigma^{-1}\)
    is positive semidefinite. Since \(\bSigma\) is positive definite by assumption,
    \(\bSigma = Q \Lambda Q^T\) has a spectral decomposition where all diagonal entries
    of \(\Lambda\) are stricly positive. Then \(\bSigma^{-1} = Q \Lambda^{-1} Q^T\)
    where the diagonal entries of \(\Lambda^{-1}\) are the inverses of those of \(\Lambda\)
    - and hence strictly positive as well. In particular, \(\bSigma^{-1}\) is positive
    semidefinite. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Hence, we can check whether a matrix is positive semidefinite
    by computing its eigenvalues using [`numpy.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**KNOWLEDGE CHECK:** Which one(s) of these matrices is positive semidefinite?'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & -1\\ -1 & 1 \end{pmatrix} \qquad B =
    \begin{pmatrix} 1 & -2\\ -2 & 1 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: a) Both
  prefs: []
  type: TYPE_NORMAL
- en: b) \(A\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(B\)
  prefs: []
  type: TYPE_NORMAL
- en: d) Neither
  prefs: []
  type: TYPE_NORMAL
- en: \(\checkmark\)
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** What is the rank of a matrix \(A \in \mathbb{R}^{n \times m}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The number of non-zero entries in \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: b) The dimension of the row space of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: c) The dimension of the null space of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: d) The trace of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Which of the following is true about the rank of a matrix \(A \in \mathbb{R}^{n
    \times m}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathrm{rk}(A) \leq \min\{n,m\}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathrm{rk}(A) \geq \max\{n,m\}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathrm{rk}(A) = \mathrm{rk}(A^T)\) only if \(A\) is symmetric
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathrm{rk}(A) = \mathrm{rk}(A^T)\) only if \(A\) is square
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Let \(A \in \mathbb{R}^{n \times k}\) and \(B \in \mathbb{R}^{k \times
    m}\). Which of the following is true in general?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathrm{rk}(AB) \leq \mathrm{rk}(A)\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathrm{rk}(AB) \geq \mathrm{rk}(A)\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathrm{rk}(AB) = \mathrm{rk}(A)\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathrm{rk}(AB) = \mathrm{rk}(B)\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Let \(A \in \mathbb{R}^{d \times d}\) be symmetric. Which of the following
    is true according to the Spectral Theorem?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(A\) has at most \(d\) distinct eigenvalues
  prefs: []
  type: TYPE_NORMAL
- en: b) \(A\) has exactly \(d\) distinct eigenvalues
  prefs: []
  type: TYPE_NORMAL
- en: c) \(A\) has at least \(d\) distinct eigenvalues
  prefs: []
  type: TYPE_NORMAL
- en: d) The number of distinct eigenvalues of \(A\) is unrelated to \(d\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which of the following is true about the outer product of two vectors
    \(\mathbf{u}\) and \(\mathbf{v}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It is a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: b) It is a vector.
  prefs: []
  type: TYPE_NORMAL
- en: c) It is a matrix of rank one.
  prefs: []
  type: TYPE_NORMAL
- en: d) It is a matrix of rank zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states “the row rank and column rank
    of \(A\) [are] simply […] the rank, which we denote by \(\mathrm{rk}(A)\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: a. Justification: The text states in the Row Rank Equals Column
    Rank Theorem that “the row rank of \(A\) equals the column rank of \(A\). Moreover,
    \(\mathrm{rk}(A) \leq \min\{n,m\}\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: a. Justification: The text shows that “the columns of \(AB\)
    are linear combinations of the columns of \(A\). Hence \(\mathrm{col}(AB) \subseteq
    \mathrm{col}(A)\). The claim follows by Observation D2.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: a. Justification: The Spectral Theorem states that “A symmetric
    matrix \(A\) has \(d\) orthonormal eigenvectors \(\mathbf{q}_1, \ldots, \mathbf{q}_d\)
    with corresponding (not necessarily distinct) real eigenvalues \(\lambda_1 \geq
    \lambda_2 \geq \cdots \geq \lambda_d\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: c. Justification: The text defines the outer product and states
    that “If \(\mathbf{u}\) and \(\mathbf{v}\) are nonzero, the matrix \(\mathbf{u}
    \mathbf{v}^T\) has rank one.”'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Rank of a matrix[#](#rank-of-a-matrix "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that the dimension of the column space of \(A\) is called the column
    rank of \(A\). Similarly the row rank of \(A\) is the dimension of its row space.
    As it turns out, these two notions of rank are the same by the *Row Rank Equals
    Column Rank Theorem*\(\idx{row rank equals column rank theorem}\xdi\). We give
    a proof of that theorem next. We refer to the row rank and column rank of \(A\)
    simply as the rank, which we denote by \(\mathrm{rk}(A)\). \(\idx{rank}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* *(Row Rank Equals Column Rank)* Write \(A\) as a matrix factorization
    \(BC\) where the columns of \(B\) form a basis of \(\mathrm{col}(A)\). Then the
    rows of \(C\) necessarily form a spanning set of \(\mathrm{row}(A)\). So, because
    the number of columns of \(B\) and the number of rows of \(C\) match, we conclude
    that the row rank is less or equal than the column rank. Applying the same argument
    to the transpose gives the claim.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall the following observation from a previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation D1:** Any linear subspace \(U\) of \(\mathbb{R}^n\) has a basis.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation D2:** If \(U\) and \(V\) are linear subspaces such that \(U \subseteq
    V\), then \(\mathrm{dim}(U) \leq \mathrm{dim}(V)\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Observation D3:** The dimension of \(\mathrm{span}(\mathbf{u}_1,\ldots,\mathbf{u}_m)\)
    is at most \(m\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Row Rank Equals Column Rank)* Assume that \(A\) has column rank
    \(r\). Then there exists a basis \(\mathbf{b}_1,\ldots, \mathbf{b}_r \in \mathbb{R}^n\)
    of \(\mathrm{col}(A)\) by *Observation D1* above, and we know that \(r \leq n\)
    by *Observation D2*. That is, for each \(j\), letting \(\mathbf{a}_{j} = A_{\cdot,j}\)
    be the \(j\)-th column of \(A\) we can write'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{a}_{j} = \sum_{\ell=1}^r \mathbf{b}_\ell c_{\ell j} \]
  prefs: []
  type: TYPE_NORMAL
- en: for some \(c_{\ell j}\)’s. Let \(B\) be the matrix whose columns are \(\mathbf{b}_1,
    \ldots, \mathbf{b}_r\) and let \(C\) be the matrix with entries \((C)_{\ell j}
    = c_{\ell j}\), \(\ell=1,\ldots,r\), \(j=1,\ldots,m\). Then the equation above
    can be re-written as the matrix factorization \(A = BC\). Indeed, by our previous
    observations about matrix-matrix products, the columns of \(A\) are linear combinations
    of the columns of \(B\) with coefficients taken from the corresponding column
    of \(C\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The key point is the following: \(C\) necessarily has \(r\) rows. Let \(\boldsymbol{\alpha}_{i}^T
    = A_{i,\cdot}\) be the \(i\)-th row of \(A\) and \(\mathbf{c}_{\ell}^T = C_{\ell,\cdot}\)
    be the \(\ell\)-th row of \(C\). Using our alternative representation of matrix-matrix
    product in terms of rows, the decomposition is equivalent to'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \boldsymbol{\alpha}_{i}^T = \sum_{\ell=1}^r b_{i\ell} \mathbf{c}_\ell^T,
    \quad i=1,\ldots, n, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(b_{i\ell} = (\mathbf{b}_i)_\ell = (B)_{i\ell}\) is the \(i\)-th entry
    of the \(\ell\)-th column of \(B\). In words, the rows of \(A\) are linear combinations
    of the rows of \(C\) with coefficients taken from the corresponding row of \(B\).
    In particular, \(\mathcal{C} = \{\mathbf{c}_{j}:j=1,\ldots,r\}\) is a spanning
    list of the row space of \(A\), that is, each row of \(A\) can be written as a
    linear combination of \(\mathcal{C}\). Put differently, \(\mathrm{row}(A) \subseteq
    \mathrm{span}(\mathcal{C})\).
  prefs: []
  type: TYPE_NORMAL
- en: So the row rank of \(A\) is at most \(r\), the column rank of \(A\), by *Observation
    D2*.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the same argument to \(A^T\), which switches the role of the columns
    and the rows, gives that the column rank of \(A\) (i.e. the row rank of \(A^T\))
    is at most the row rank of \(A\) (i.e. the column rank of \(A^T\)). Hence the
    two notions of rank must be equal. (We also deduce \(r \leq m\) by *Observation
    D2* again.) \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(continued)** We illustrate the proof of the theorem. Continuing
    a previous example, let \(A\) be the matrix with columns \(\mathbf{w}_1 = (1,0,1)\),
    \(\mathbf{w}_2 = (0,1,1)\), and \(\mathbf{w}_3 = (1,-1,0)\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0 & 1\\ 0 & 1 & -1\\ 1 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We know that \(\mathbf{w}_1\) and \(\mathbf{w}_2\) form a basis of \(\mathrm{col}(A)\).
    We use them to construct our matrix \(B\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = \begin{pmatrix} 1 & 0\\ 0 & 1\\ 1 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that \(\mathbf{w}_3 = \mathbf{w}_1 - \mathbf{w}_2\). Hence the matrix
    \(C\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} C = \begin{pmatrix} 1 & 0 & 1\\ 0 & 1 & -1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, column \(j\) of \(C\) gives the coefficients in the linear combination
    of the columns of \(B\) that produces column \(j\) of A. Check that \(A = B C\).
    \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Numpy, one can compute the rank of a matrix using
    the function [`numpy.linalg.matrix_rank`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_rank.html).
    We will see later in the chapter how to compute it using the singular value decomposition
    (which is how `LA.matrix_rank` does it). Let’s try the example above.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We compute the rank of `A`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We take only the first two columns of `A` this time to form `B`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Recall that, in Numpy, `@` is used for matrix product.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times k}\) and \(B \in \mathbb{R}^{k
    \times m}\). Then we claim that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{rk}(AB) \leq \mathrm{rk}(A). \]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the columns of \(AB\) are linear combinations of the columns of \(A\).
    Hence \(\mathrm{col}(AB) \subseteq \mathrm{col}(A)\). The claim follows by *Observation
    D2*. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Let \(A \in \mathbb{R}^{n \times k}\) and \(B \in \mathbb{R}^{n
    \times m}\). Then we claim that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{rk}(A + B) \leq \mathrm{rk}(A) + \mathrm{rk}(B). \]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the columns of \(A + B\) are linear combinations of the columns of \(A\)
    and of \(B\). Let \(\mathbf{u}_1,\ldots,\mathbf{u}_h\) be a basis of \(\mathrm{col}(A)\)
    and let \(\mathbf{v}_1,\ldots,\mathbf{v}_{\ell}\) be a basis of \(\mathrm{col}(B)\)
    by *Observation D1*. Then, we deduce
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{col}(A + B) \subseteq \mathrm{span}(\mathbf{u}_1,\ldots,\mathbf{u}_h,\mathbf{v}_1,\ldots,\mathbf{v}_{\ell}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: By *Observation D2*, it follows that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{rk}(A + B) \leq \mathrm{dim}(\mathrm{span}(\mathbf{u}_1,\ldots,\mathbf{u}_h,\mathbf{v}_1,\ldots,\mathbf{v}_{\ell})).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: By *Observation D3*, the right hand side is at most the length of the spanning
    list, i.e., \(h+\ell\). But by construction \(\mathrm{rk}(A) = h\) and \(\mathrm{rk}(B)
    = \ell\), so we are done. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(A Proof of the Rank-Nullity Theorem)** \(\idx{rank-nullity
    theorem}\xdi\) Let \(A \in \mathbb{R}^{n \times m}\). Recall that the column space
    of \(A\), \(\mathrm{col}(A) \subseteq \mathbb{R}^n\), is the span of its columns.
    We compute its othogonal complement. By definition, the columns of \(A\), which
    we denote by \(\mathbf{a}_1,\ldots,\mathbf{a}_m\), form a spanning list of \(\mathrm{col}(A)\).
    So \(\mathbf{u} \in \mathrm{col}(A)^\perp \subseteq \mathbb{R}^n\) if and only
    if'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{a}_i^T\mathbf{u} = \langle \mathbf{u}, \mathbf{a}_i \rangle = 0,
    \quad \forall i=1,\ldots,m. \]
  prefs: []
  type: TYPE_NORMAL
- en: Indeed that then implies that for any \(\mathbf{v} \in \mathrm{col}(A)\), say
    \(\mathbf{v} = \beta_1 \mathbf{a}_1 + \cdots + \beta_m \mathbf{a}_m\) we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left\langle \mathbf{u}, \sum_{i=1}^m \beta_i \mathbf{a}_i \right\rangle
    = \sum_{i=1}^m \beta_i \langle \mathbf{u}, \mathbf{a}_i \rangle = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: The \(m\) conditions above can be written in matrix form as
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^T \mathbf{u} = \mathbf{0}. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, the orthogonal complement of the column space of \(A\) is the null
    space of \(A^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{col}(A)^\perp = \mathrm{null}(A^T). \]
  prefs: []
  type: TYPE_NORMAL
- en: Applying the same argument to the column space of \(A^T\), it follows that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{col}(A^T)^\perp = \mathrm{null}(A), \]
  prefs: []
  type: TYPE_NORMAL
- en: where note that \(\mathrm{null}(A) \subseteq \mathbb{R}^m\). The four linear
    subspaces \(\mathrm{col}(A)\), \(\mathrm{col}(A^T)\), \(\mathrm{null}(A)\) and
    \(\mathrm{null}(A^T)\) are referred to as the fundamental subspaces of \(A\).
    We have shown
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{col}(A) \oplus \mathrm{null}(A^T) = \mathbb{R}^n \quad \text{and}
    \quad \mathrm{col}(A^T) \oplus \mathrm{null}(A) = \mathbb{R}^m \]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Row Rank Equals Column Rank Theorem*, \(\mathrm{dim}(\mathrm{col}(A))
    = \mathrm{dim}(\mathrm{col}(A^T))\). Moreover, by our previous observation about
    the dimensions of direct sums, we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ n = \mathrm{dim}(\mathrm{col}(A)) + \mathrm{dim}(\mathrm{null}(A^T)) = \mathrm{dim}(\mathrm{col}(A^T))
    + \mathrm{dim}(\mathrm{null}(A^T)) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ m = \mathrm{dim}(\mathrm{col}(A^T)) + \mathrm{dim}(\mathrm{null}(A)) = \mathrm{dim}(\mathrm{col}(A))
    + \mathrm{dim}(\mathrm{null}(A)). \]
  prefs: []
  type: TYPE_NORMAL
- en: So we deduce that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}(\mathrm{null}(A)) = m - \mathrm{rk}(A) \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{dim}(\mathrm{null}(A^T)) = n - \mathrm{rk}(A). \]
  prefs: []
  type: TYPE_NORMAL
- en: These formulas are referred to the *Rank-Nullity Theorem*. The dimension of
    the null space of \(A\) is called the nullity of \(A\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Outer products and rank-one matrices** Let \(\mathbf{u} = (u_1,\ldots,u_n)
    \in \mathbb{R}^n\) and \(\mathbf{v} = (v_1,\ldots,v_m) \in \mathbf{R}^m\) be two
    column vectors. Their outer product\(\idx{outer product}\xdi\) is defined as the
    matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{u} \mathbf{v}^T = \begin{pmatrix} u_1 v_1 & u_1 v_2
    & \cdots & u_1 v_m\\ u_2 v_1 & u_2 v_2 & \cdots & u_2 v_m\\ \vdots & \vdots &
    \ddots & \vdots\\ u_n v_1 & u_n v_2 & \cdots & u_n v_m \end{pmatrix} = \begin{pmatrix}
    | & & | \\ v_{1} \mathbf{u} & \ldots & v_{m} \mathbf{u} \\ | & & | \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This is not to be confused with the inner product \(\mathbf{u}^T \mathbf{v}\),
    which requires \(n = m\) and produces a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: If \(\mathbf{u}\) and \(\mathbf{v}\) are nonzero, the matrix \(\mathbf{u} \mathbf{v}^T\)
    has rank one. Indeed, its columns are all a multiple of the same vector \(\mathbf{u}\).
    So the column space spanned by the columns of \(\mathbf{u} \mathbf{v}^T\) is one-dimensional.
    Vice versa, any rank-one matrix can be written in this form by definition of the
    rank.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen many different interpretations of matrix-matrix products. Here
    is yet another one. Let \(A = (a_{ij})_{i,j} \in \mathbb{R}^{n \times k}\) and
    \(B = (b_{ij})_{i,j} \in \mathbb{R}^{k \times m}\). Denote by \(\mathbf{a}_1,\ldots,\mathbf{a}_k\)
    the columns of \(A\) and denote by \(\mathbf{b}_1^T,\ldots,\mathbf{b}_k^T\) the
    rows of \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} A B &= \begin{pmatrix} \sum_{j=1}^k a_{1j} b_{j1} & \sum_{j=1}^k
    a_{1j} b_{j2} & \cdots & \sum_{j=1}^k a_{1j} b_{jm}\\ \sum_{j=1}^k a_{2j} b_{j1}
    & \sum_{j=1}^k a_{2j} b_{j2} & \cdots & \sum_{j=1}^k a_{2j} b_{jm}\\ \vdots &
    \vdots & \ddots & \vdots\\ \sum_{j=1}^k a_{nj} b_{j1} & \sum_{j=1}^k a_{nj} b_{j2}
    & \cdots & \sum_{j=1}^k a_{nj} b_{jm} \end{pmatrix}\\ &= \sum_{j=1}^k \begin{pmatrix}
    a_{1j} b_{j1} & a_{1j} b_{j2} & \cdots & a_{1j} b_{jm}\\ a_{2j} b_{j1} & a_{2j}
    b_{j2} & \cdots & a_{2j} b_{jm}\\ \vdots & \vdots & \ddots & \vdots\\ a_{nj} b_{j1}
    & a_{nj} b_{j2} & \cdots & a_{nj} b_{jm} \end{pmatrix}\\ &= \sum_{j=1}^k \mathbf{a}_j
    \mathbf{b}_j^T. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: In words, the matrix product \(AB\) can be interpreted as a sum of \(k\) rank-one
    matrices, each of which is the outer product of a column of \(A\) with the corresponding
    row of \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: Because the rank of a sum is at most the sum of the ranks (as shown in a previous
    example), it follows that the rank of \(AB\) is at most \(k\). This is consistent
    with the fact that the rank of a product is at most the minimum of the ranks (also
    shown in a previous example).
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2\. Eigenvalues and eigenvectors[#](#eigenvalues-and-eigenvectors "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall the concepts of eigenvalues\(\idx{eigenvalue}\xdi\) and eigenvectors\(\idx{eigenvector}\xdi\).
    We work on \(\mathbb{R}^d\).
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Eigenvalues and Eigenvectors)** Let \(A \in \mathbb{R}^{d
    \times d}\) be a square matrix. Then \(\lambda \in \mathbb{R}\) is an eigenvalue
    of \(A\) if there exists a nonzero vector \(\mathbf{x} \neq \mathbf{0}\) such
    that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x} = \lambda \mathbf{x}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The vector \(\mathbf{x}\) is referred to as an eigenvector. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: As the next example shows, not every matrix has a (real) eigenvalue.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(No Real Eigenvalues)** Set \(d = 2\) and let'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 0 & -1 \\ 1 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: For \(\lambda\) to be an eigenvalue, there must be an nonzero eigenvector \(\mathbf{x}
    = (x_1, x_2)\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{x} = \lambda \mathbf{x} \]
  prefs: []
  type: TYPE_NORMAL
- en: or put differently \(- x_2 = \lambda x_1\) and \(x_1 = \lambda x_2\). Replacing
    these equations into each other, it must be that \(- x_2 = \lambda^2 x_2\) and
    \(x_1 = - \lambda^2 x_1\). Because \(x_1, x_2\) cannot both be \(0\), \(\lambda\)
    must satisfy the equation \(\lambda^2 = -1\) for which there is no real solution.
    \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: In general, \(A \in \mathbb{R}^{d \times d}\) has at most \(d\) distinct eigenvalues.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Number of Eigenvalues)** \(\idx{number of eigenvalues lemma}\xdi\)
    Let \(A \in \mathbb{R}^{d \times d}\) and let \(\lambda_1, \ldots, \lambda_m\)
    be distinct eigenvalues of \(A\) with corresponding eigenvectors \(\mathbf{x}_1,
    \ldots, \mathbf{x}_m\). Then \(\mathbf{x}_1, \ldots, \mathbf{x}_m\) are linearly
    independent. In particular, \(m \leq d\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Assume by contradiction that \(\mathbf{x}_1, \ldots, \mathbf{x}_m\)
    are linearly dependent. By the *Linear Dependence Lemma*, there is \(k \leq m\)
    such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}_k \in \mathrm{span}(\mathbf{x}_1, \ldots, \mathbf{x}_{k-1}) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{x}_1, \ldots, \mathbf{x}_{k-1}\) are linearly independent. In
    particular, there are \(a_1, \ldots, a_{k-1}\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}_k = a_1 \mathbf{x}_1 + \cdots + a_{k-1} \mathbf{x}_{k-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform the equation above in two ways: (1) multiply both sides by \(\lambda_k\)
    and (2) apply \(A\). Then subtract the resulting equations. That leads to'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{0} = a_1 (\lambda_k - \lambda_1) \mathbf{x}_1 + \cdots + a_{k-1}
    (\lambda_k - \lambda_{k-1}) \mathbf{x}_{k-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Because the \(\lambda_i\)’s are distinct and \(\mathbf{x}_1, \ldots, \mathbf{x}_{k-1}\)
    are linearly independent, we must have \(a_1 = \cdots = a_{k-1} = 0\). But that
    implies that \(\mathbf{x}_k = \mathbf{0}\), a contradiction.
  prefs: []
  type: TYPE_NORMAL
- en: For the second claim, if there were more than \(d\) distinct eigenvalues, then
    there would be more than \(d\) corresponding linearly independent eigenvectors
    by the first claim, a contradiction. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Diagonal (and Similar) Matrices)** Recall that we use the notation
    \(\mathrm{diag}(\lambda_1,\ldots,\lambda_d)\) for the diagonal matrix\(\idx{diagonal
    matrix}\xdi\) with diagonal entries \(\lambda_1,\ldots,\lambda_d\). The upper
    bound in the *Number of Eigenvalues Lemma* can be achieved, for instance, by diagonal
    matrices with distinct diagonal entries \(A = \mathrm{diag}(\lambda_1, \ldots,
    \lambda_d)\). Each standard basis vector \(\mathbf{e}_i\) is then an eigenvector'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{e}_i = \lambda_i \mathbf{e}_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: More generally, let \(A\) be similar to a matrix \(D = \mathrm{diag}(\lambda_1,
    \ldots, \lambda_d)\) with distinct diagonal entries, that is, there exists a nonsingular
    matrix \(P\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = P D P^{-1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Let \(\mathbf{p}_1, \ldots, \mathbf{p}_d\) be the columns of \(P\). Note that,
    because the columns of \(P\) form a basis of \(\mathbb{R}^d\), the entries of
    the vector \(\mathbf{c} = P^{-1} \mathbf{x}\) are the coefficients of the unique
    linear combination of the \(\mathbf{p}_i\)’s equal to \(\mathbf{x}\). Indeed,
    \(P \mathbf{c} = \mathbf{x}\). Hence, \(A \mathbf{x}\) is can be thought of as:
    (1) expressing \(\mathbf{x}\) in the basis \(\mathbf{p}_1, \ldots, \mathbf{p}_d\),
    (2) and scaling the \(\mathbf{p}_i\)’s by the corresponding \(\lambda_i\)’s. In
    particular, the \(\mathbf{p}_i\)’s are eigenvectors of \(A\) since, by the above,
    \(P^{-1} \mathbf{p}_i = \mathbf{e}_i\) and so'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{p}_i = P D P^{-1} \mathbf{p}_i = P D \mathbf{e}_i = P (\lambda_i
    \mathbf{e}_i) = \lambda_i \mathbf{p}_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Numpy, the eigenvalues and eigenvectors of a matrix
    can be computed using [`numpy.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Above, `w` are the eigenvalues in an array, whereas the columns of `v` are the
    corresponding eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Some matrix algebra** We will need a few useful observations about matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: A (not necessarily square) matrix \(D \in \mathbb{R}^{k \times r}\) is diagonal
    if its non-diagonal entries are zero. That is, \(i \neq j\) implies that \(D_{ij}
    =0\). Note that a diagonal matrix is not necessarily square and that the diagonal
    elements are allowed to be zero.
  prefs: []
  type: TYPE_NORMAL
- en: Multiplying a matrix by a diagonal one has a very specific effect. Let \(A \in
    \mathbb{R}^{n \times k}\) and \(B \in \mathbb{R}^{r \times m}\). We focus here
    on the case \(k \geq r\). The matrix product \(A D\) produces a matrix whose columns
    are linear combinations of the columns of \(A\) where the coefficients are taken
    from the corresponding column of \(D\). But the columns of \(D\) have at most
    one nonzero elements, the diagonal one. So the columns of \(AD\) are in fact multiples
    of the columns of \(A\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} AD = \begin{pmatrix} | & & | \\ d_{11} \mathbf{a}_1 & \ldots
    & d_{rr} \mathbf{a}_r \\ | & & | \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{a}_1,\ldots,\mathbf{a}_k\) are the columns of \(A\) and \(d_{11},\ldots,d_{rr}\)
    are the diagonal elements of \(D\).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the rows of \(D B\) are linear combinations of the rows of \(B\)
    where the coefficients are taken from the corresponding row of \(D\). The rows
    of \(D\) have at most one nonzero element, the diagonal one. In the case, \(k
    \geq r\), rows \(r+1,\ldots,k\) necessarily have only zero entries since there
    is no diagonal entry there. Hence the first \(r\) rows of \(DB\) are multiples
    of the rows of \(B\) and the next \(k-r\) are \(\mathbf{0}\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} DB = \begin{pmatrix} \horz & d_{11} \mathbf{b}_1^T & \horz \\
    & \vdots &\\ \horz & d_{rr} \mathbf{b}_r^T & \horz\\ \horz & \mathbf{0} & \horz\\
    & \vdots &\\ \horz & \mathbf{0} & \horz \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{b}_1^T,\ldots,\mathbf{b}_r^T\) are the rows of \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** The following special case will be useful later in this chapter.
    Suppose \(D, F \in \mathbb{R}^{n \times n}\) are both square diagonal matrices.
    Then \(D F\) is the matrix whose diagonal elements are \(d_{11} f_{11}, \ldots,d_{nn}
    f_{nn}\). \(\lhd\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spectral theorem** When \(A\) is symmetric, a remarkable result is that there
    exists an orthonormal basis of \(\mathbb{R}^d\) made of eigenvectors of \(A\).
    We will prove this result in a subsequent chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** Let \(A \in \mathbb{R}^{d \times d}\) be symmetric. Show
    that for any \(\mathbf{u}\) and \(\mathbf{v}\) in \(\mathbb{R}^d\) it holds that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{u}, A \mathbf{v} \rangle = \langle A \mathbf{u}, \mathbf{v}
    \rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\checkmark\)
  prefs: []
  type: TYPE_NORMAL
- en: Before stating the result formally, we make a few observations. Let \(A \in
    \mathbb{R}^{d \times d}\) be symmetric. Suppose that \(\mathbf{v}_i\) and \(\mathbf{v}_j\)
    are eigenvectors corresponding respectively to distinct eigenvalues \(\lambda_i\)
    and \(\lambda_j\). Then the following quantity can be written in two ways
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}_i, A \mathbf{v}_j \rangle = \langle \mathbf{v}_i, \lambda_j
    \mathbf{v}_j \rangle = \lambda_j \langle \mathbf{v}_i, \mathbf{v}_j \rangle \]
  prefs: []
  type: TYPE_NORMAL
- en: and, by symmetry of \(A\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}_i, A \mathbf{v}_j \rangle = \mathbf{v}_i^T A \mathbf{v}_j
    = \mathbf{v}_i^T A^T \mathbf{v}_j = (A \mathbf{v}_i)^T \mathbf{v}_j = \langle
    A \mathbf{v}_i, \mathbf{v}_j \rangle = \langle \lambda_i \mathbf{v}_i, \mathbf{v}_j
    \rangle = \lambda_i \langle \mathbf{v}_i, \mathbf{v}_j \rangle. \]
  prefs: []
  type: TYPE_NORMAL
- en: Subtracting the two,
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\lambda_j - \lambda_i) \langle \mathbf{v}_i, \mathbf{v}_j \rangle = 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: and using that \(\lambda_i \neq \lambda_j\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{v}_i, \mathbf{v}_j \rangle = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, \(\mathbf{v}_i\) and \(\mathbf{v}_j\) are necessarily orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: 'We proved:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** Let \(A \in \mathbb{R}^{d \times d}\) be symmetric. Suppose that
    \(\mathbf{v}_i\) and \(\mathbf{v}_j\) are eigenvectors corresponding to distinct
    eigenvalues. Then \(\mathbf{v}_i\) and \(\mathbf{v}_j\) are orthogonal. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: This lemma gives a different proof – in the symmetric case – that the number
    of eigenvalues is at most \(d\) since a list of pairwise orthogonal vectors are
    linearly independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact:'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Spectral)** \(\idx{spectral theorem}\xdi\) Let \(A \in \mathbb{R}^{d
    \times d}\) be a symmetric matrix, that is, \(A^T = A\). Then \(A\) has \(d\)
    orthonormal eigenvectors \(\mathbf{q}_1, \ldots, \mathbf{q}_d\) with corresponding
    (not necessarily distinct) real eigenvalues \(\lambda_1 \geq \lambda_2 \geq \cdots
    \geq \lambda_d\). Moreover, \(A\) can be written as the matrix factorization'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(Q\) has columns \(\mathbf{q}_1, \ldots, \mathbf{q}_d\) and \(\Lambda
    = \mathrm{diag}(\lambda_1, \ldots, \lambda_d)\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: We refer to this factorization as a spectral decomposition\(\idx{spectral decomposition}\xdi\)
    of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: Note that this decomposition indeed produces the eigenvectors of \(A\). For
    any \(j\), we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{q}_j = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \mathbf{q}_j
    = \lambda_j \mathbf{q}_j, \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that, by orthonormality, \(\mathbf{q}_i^T \mathbf{q}_j = 0\) if
    \(i \neq j\) and \(\mathbf{q}_i^T \mathbf{q}_j = 1\) if \(i = j\). The equation
    above says precisely that \(\mathbf{q}_j\) is an eigenvector of \(A\) with corresponding
    eigenvalue \(\lambda_j\). Since we have found \(d\) eigenvalues (possibly with
    repetition), we have found all of them.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\lambda_1, \lambda_2, \ldots, \lambda_d\) be the eigenvalues whose existence
    is guaranteed by the *Spectral Theorem*. We first argue that there is no other
    eigenvalue. Indeed, assume \(\mu \neq \lambda_1, \lambda_2, \ldots, \lambda_d\)
    is an eigenvalue with corresponding eigenvector \(\mathbf{p}\). We have seen that
    \(\mathbf{p}\) is orthogonal to the eigenvectors \(\mathbf{q}_1, \ldots, \mathbf{q}_d\).
    Since the latter list forms an orthonormal basis of \(\mathbb{R}^d\), this cannot
    be the case and we have a contradiction.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the eigenvalues \(\lambda_1, \lambda_2, \ldots, \lambda_d\) can be repeated
    however, that is, there can be \(i, j\) such that \(\lambda_{i} = \lambda_{j}\).
    For instance, if \(A = I_{d \times d}\) is the identity matrix, then the eigenvalues
    are \(\lambda_i = 1\) for all \(i \in [d]\).
  prefs: []
  type: TYPE_NORMAL
- en: For a fixed eigenvalue \(\lambda\) of \(A\), the set of eigenvectors with eigenvalue
    \(\lambda\) satisfy
  prefs: []
  type: TYPE_NORMAL
- en: \[ A \mathbf{v} = \lambda \mathbf{v} = \lambda I_{d \times d} \mathbf{v} \]
  prefs: []
  type: TYPE_NORMAL
- en: or, rearranging,
  prefs: []
  type: TYPE_NORMAL
- en: \[ (A - \lambda I_{d \times d})\mathbf{v} = \mathbf{0}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, it is the set \(\mathrm{null}(A - \lambda I_{d \times d})\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The eigenvectors in the *Spectral Theorem* corresponding the same eigenvalue
    \(\lambda\) can be replaced by any orthonormal basis of the subspace \(\mathrm{null}(A
    - \lambda I_{d \times d})\). But, beyond this freedom, we have the following characterization:
    let \(\mu_1,\ldots,\mu_f\) be the unique values in \(\lambda_1, \lambda_2, \ldots,
    \lambda_d\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{R}^d = \mathrm{null}(A - \mu_1 I_{d \times d}) \oplus \mathrm{null}(A
    - \mu_2 I_{d \times d}) \oplus \cdots \oplus \mathrm{null}(A - \mu_f I_{d \times
    d}), \]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the fact that for any \(\mathbf{u} \in \mathrm{null}(A - \mu_i
    I_{d \times d})\) and \(\mathbf{v} \in \mathrm{null}(A - \mu_j I_{d \times d})\)
    with \(i \neq j\), we have that \(\mathbf{u}\) is orthogonal to \(\mathbf{v}\).
  prefs: []
  type: TYPE_NORMAL
- en: We have shown in particular that the sequence of eigenvalues in the *Spectral
    Theorem* is unique (counting repeats).
  prefs: []
  type: TYPE_NORMAL
- en: Two matrices \(B, D \in \mathbb{R}^{d \times d}\) are [similar](https://en.wikipedia.org/wiki/Matrix_similarity)\(\idx{similar}\xdi\)
    if there is an invertible matrix \(P\) such that \(B = P^{-1} D P\). It can be
    shown that \(B\) and \(D\) correspond to the same linear map, but expressed in
    different bases. When \(P = Q\) is an orthogonal matrix, the transformation simplifies
    to \(B = Q^T D Q\).
  prefs: []
  type: TYPE_NORMAL
- en: Hence, a different way to think about a spectral decomposition is that it expresses
    the fact that any symmetric matrix is similar to a diagonal matrix through an
    orthogonal transformation. The basis in which the corresponding linear map is
    represented by a diagonal matrix is the basis of eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Eigendecomposition of \(2 \times 2\) symmetric matrix)** The
    simplest non-trivial case is the \(2 \times 2\) symmetric matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} a & b\\ b & d \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We derive a step-by-step recipe to compute its eigenvalues and eigenvectors.
  prefs: []
  type: TYPE_NORMAL
- en: As shown previously, an eigenvalue \(\lambda\) corresponds to a nonempty \(\mathrm{null}(A
    - \lambda I_{2 \times 2})\) and the corresponding eigenvector solves
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{0} = (A - \lambda I_{2 \times 2})\mathbf{v} = \begin{pmatrix}a
    - \lambda & b\\ b & d - \lambda\end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Put differently, the matrix \(\begin{pmatrix}a - \lambda & b\\ b & d - \lambda\end{pmatrix}\)
    has linearly dependent columns. We have seen that one way to check this is to
    compute the determinant which in the \(2 \times 2\) case is simply
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathrm{det}\left[\begin{pmatrix}a - \lambda & b\\ b & d - \lambda\end{pmatrix}\right]
    = (a - \lambda ) (d - \lambda) - b^2. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This is a polynomial of degree \(2\) in \(\lambda\) called the characteristic
    polynomial of the matrix \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: The roots of the characteristic polynomial, that is, the solutions of
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 = (a - \lambda ) (d - \lambda) - b^2 = \lambda^2 - (a + d)\lambda + (ad
    - b^2), \]
  prefs: []
  type: TYPE_NORMAL
- en: are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{1} = \frac{(a + d) + \sqrt{(a + d)^2 - 4(ad - b^2)}}{2} \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{2} = \frac{(a + d) - \sqrt{(a + d)^2 - 4(ad - b^2)}}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Expanding the expression in the square root
  prefs: []
  type: TYPE_NORMAL
- en: \[ (a + d)^2 - 4(ad - b^2) = a^2 + d^2 - 2ad + 4b^2 = (a - d)^2 + 4b^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: we see that the square root is well-defined (i.e., produces a real value) for
    any \(a, b, d\).
  prefs: []
  type: TYPE_NORMAL
- en: It remains to find the corresponding eigenvectors \(\mathbf{v}_{1} = (v_{1,1},
    v_{1,2})\) and \(\mathbf{v}_2 = (v_{2,1}, v_{2,2})\) by solving the \(2 \times
    2\) systems of linear equations
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} a - \lambda_i & b \\ b & d - \lambda_i \end{pmatrix}
    \begin{pmatrix} v_{i,1} \\ v_{i,2} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: which is guaranteed to have a solution. When \(\lambda_1 = \lambda_2\), one
    needs to find two linearly independent solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a numerical example. Consider the matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The characteristic polynomial equation is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda^2 - 6\lambda + 8 = 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: The eigenvalues are
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{1}, \lambda_{2} = \frac{6 \pm \sqrt{36 - 4(9-1))}}{2} = \frac{6
    \pm \sqrt{4}}{2} = 4, 2. \]
  prefs: []
  type: TYPE_NORMAL
- en: We then solve for the eigenvectors. For \(\lambda_1 = 4\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} 3 - \lambda_1 & 1 \\ 1 & 3 - \lambda_1 \end{pmatrix}
    \begin{pmatrix} v_{1,1} \\ v_{1,2} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
    \Leftrightarrow \begin{cases} - v_{1,1} + v_{1,2} = 0\\ v_{1,1} - v_{1,2} = 0
    \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so, after normalizing, we take
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{v}_1 = \frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ 1\end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(\lambda_2 = 2\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} 3 - \lambda_2 & 1 \\ 1 & 3 - \lambda_2 \end{pmatrix}
    \begin{pmatrix} v_{2,1} \\ v_{2,2} \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
    \Leftrightarrow \begin{cases} v_{1,1} + v_{1,2} = 0\\ v_{1,1} + v_{1,2} = 0 \end{cases}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so, after normalizing, we take
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{v}_2 = \frac{1}{\sqrt{2}}\begin{pmatrix}1 \\ -1\end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The fact that these are the eigenvectors can be checked by hand (try it!). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: More generally, for any matrix \(A \in \mathbb{R}^{d \times d}\), the roots
    of the characteristic polynomial \(\mathrm{det}(A - \lambda I_{d \times d})\)
    are eigenvalues of \(A\). We will derive a more efficient numerical approach to
    compute them in a subsequent section.
  prefs: []
  type: TYPE_NORMAL
- en: '**The case of positive semidefinite matrices** The eigenvalues of a symmetric
    matrix – while real – may be negative. There is however an important special case
    where the eigenvalues are nonnegative, positive semidefinite matrices. \(\idx{positive
    semidefinite matrix}\xdi\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Characterization of Positive Semidefiniteness)** \(\idx{characterization
    of positive semidefiniteness}\xdi\) Let \(A \in \mathbb{R}^{d \times d}\) be a
    symmetric matrix and let \(A = Q \Lambda Q^T\) be a spectral decomposition of
    \(A\) with \(\Lambda = \mathrm{diag}(\lambda_1, \ldots, \lambda_d)\). Then \(A
    \succeq 0\) if and only if its eigenvalues \(\lambda_1, \ldots, \lambda_d\) are
    nonnegative. \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Assume \(A \succeq 0\). Let \(\mathbf{q}_i\) be an eigenvector of
    \(A\) with corresponding eigenvalue \(\lambda_i\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{q}_i, A \mathbf{q}_i \rangle = \langle \mathbf{q}_i, \lambda_i
    \mathbf{q}_i \rangle = \lambda_i \]
  prefs: []
  type: TYPE_NORMAL
- en: which must be nonnegative by definition of a positive semidefinite matrix.
  prefs: []
  type: TYPE_NORMAL
- en: In the other direction, assume \(\lambda_1, \ldots, \lambda_d \geq 0\). Then,
    by the spectral decomposition in outer-product form
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{x}, A \mathbf{x} \rangle = \mathbf{x}^T \left(\sum_{i=1}^d
    \lambda_i \mathbf{q}_i \mathbf{q}_i^T\right) \mathbf{x} = \sum_{i=1}^d \lambda_i
    \mathbf{x}^T\mathbf{q}_i \mathbf{q}_i^T\mathbf{x} = \sum_{i=1}^d \lambda_i \langle
    \mathbf{q}_i, \mathbf{x}\rangle^2 \]
  prefs: []
  type: TYPE_NORMAL
- en: which is necessarily nonnegative. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, a symmetric matrix is positive definite\(\idx{positive definite matrix}\xdi\)
    if and only if all its eigenvalues are strictly positive. The proof is essentially
    the same.
  prefs: []
  type: TYPE_NORMAL
- en: '**KNOWLEDGE CHECK:** Prove this last claim by modifying the proof above. \(\checkmark\)'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that an important application of positive semidefiniteness is as a characterization
    of convexity\(\idx{convex function}\xdi\). Here are some examples.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Convexity via eigenvalues of Hessian)** Consider the function'
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(x, y) = \frac{3}{2} x^2 + xy + \frac{3}{2} y^2 + 5x - 2y + 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: To show it is convex, we compute its Hessian
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} H_f(x,y) = \begin{pmatrix} 3 & 1 \\ 1 & 3 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(x,y\). By a previous example, its eigenvalues are \(2\) and \(4\),
    both of which are strictly positive. That proves the claim by the *Second-Order
    Convexity Condition*. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Log-concavity)** A function \(f :\mathbb{R}^d \to \mathbb{R}\)
    is said to be log-concave if \(-\log f\) is convex. Put differently, we require
    for all \(\mathbf{x}, \mathbf{y} \in \mathbb{R}^d\) and \(\alpha \in (0,1)\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ - \log f((1-\alpha)\mathbf{x} + \alpha \mathbf{y}) \leq - (1-\alpha) \log
    f(\mathbf{x}) - \alpha \log f(\mathbf{y}), \]
  prefs: []
  type: TYPE_NORMAL
- en: This is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \log f((1-\alpha)\mathbf{x} + \alpha \mathbf{y}) \geq (1-\alpha) \log f(\mathbf{x})
    + \alpha \log f(\mathbf{y}) , \]
  prefs: []
  type: TYPE_NORMAL
- en: Or, because \(a \log b = \log b^a\) and the logarithm is strictly increasing,
  prefs: []
  type: TYPE_NORMAL
- en: \[ f((1-\alpha)\mathbf{x} + \alpha \mathbf{y}) \geq f(\mathbf{x})^{1-\alpha}
    f(\mathbf{y})^{\alpha}. \]
  prefs: []
  type: TYPE_NORMAL
- en: We will see later in the course that a multivariate Gaussian vector \(\mathbf{X}\)
    on \(\mathbb{R}^d\) with mean \(\bmu \in \mathbb{R}^d\) and positive definite
    covariance matrix \(\bSigma \in \mathbb{R}^{d \times d}\) has probability density
    function (PDF)
  prefs: []
  type: TYPE_NORMAL
- en: \[ f_{\bmu, \bSigma}(\mathbf{x}) = \frac{1}{(2\pi)^{d/2} \,|\bSigma|^{1/2}}
    \exp\left(-\frac{1}{2}(\mathbf{x} - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu)\right)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(|A|\) is the [determinant](https://en.wikipedia.org/wiki/Determinant)
    of \(A\), which in the case of a symmetric matrix is simply the product of its
    eigenvalues (with repeats). We claim that this PDF is log-concave.
  prefs: []
  type: TYPE_NORMAL
- en: From the definition,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &- \log f_{\bmu, \bSigma}(\mathbf{x})\\ &= \frac{1}{2}(\mathbf{x}
    - \bmu)^T \bSigma^{-1} (\mathbf{x} - \bmu) + \log (2\pi)^{d/2} \,|\bSigma|^{1/2}\\
    &= \frac{1}{2}\mathbf{x}^T \bSigma^{-1} \mathbf{x} - \bmu^T \bSigma^{-1} \mathbf{x}
    + \left[\frac{1}{2}\bmu^T \bSigma^{-1} \bmu + \log (2\pi)^{d/2} \,|\bSigma|^{1/2}\right].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(P = \bSigma^{-1}\), \(\mathbf{q} = - \bmu^T \bSigma^{-1}\) and \(r = \frac{1}{2}\bmu^T
    \bSigma^{-1} \bmu + \log (2\pi)^{d/2} \,|\bSigma|^{1/2}\).
  prefs: []
  type: TYPE_NORMAL
- en: A previous example then implies that the PDF is log-concave if \(\bSigma^{-1}\)
    is positive semidefinite. Since \(\bSigma\) is positive definite by assumption,
    \(\bSigma = Q \Lambda Q^T\) has a spectral decomposition where all diagonal entries
    of \(\Lambda\) are stricly positive. Then \(\bSigma^{-1} = Q \Lambda^{-1} Q^T\)
    where the diagonal entries of \(\Lambda^{-1}\) are the inverses of those of \(\Lambda\)
    - and hence strictly positive as well. In particular, \(\bSigma^{-1}\) is positive
    semidefinite. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Hence, we can check whether a matrix is positive semidefinite
    by computing its eigenvalues using [`numpy.linalg.eig`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eig.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '**KNOWLEDGE CHECK:** Which one(s) of these matrices is positive semidefinite?'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & -1\\ -1 & 1 \end{pmatrix} \qquad B =
    \begin{pmatrix} 1 & -2\\ -2 & 1 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: a) Both
  prefs: []
  type: TYPE_NORMAL
- en: b) \(A\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(B\)
  prefs: []
  type: TYPE_NORMAL
- en: d) Neither
  prefs: []
  type: TYPE_NORMAL
- en: \(\checkmark\)
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** What is the rank of a matrix \(A \in \mathbb{R}^{n \times m}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The number of non-zero entries in \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: b) The dimension of the row space of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: c) The dimension of the null space of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: d) The trace of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Which of the following is true about the rank of a matrix \(A \in \mathbb{R}^{n
    \times m}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathrm{rk}(A) \leq \min\{n,m\}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathrm{rk}(A) \geq \max\{n,m\}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathrm{rk}(A) = \mathrm{rk}(A^T)\) only if \(A\) is symmetric
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathrm{rk}(A) = \mathrm{rk}(A^T)\) only if \(A\) is square
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Let \(A \in \mathbb{R}^{n \times k}\) and \(B \in \mathbb{R}^{k \times
    m}\). Which of the following is true in general?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathrm{rk}(AB) \leq \mathrm{rk}(A)\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathrm{rk}(AB) \geq \mathrm{rk}(A)\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathrm{rk}(AB) = \mathrm{rk}(A)\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathrm{rk}(AB) = \mathrm{rk}(B)\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Let \(A \in \mathbb{R}^{d \times d}\) be symmetric. Which of the following
    is true according to the Spectral Theorem?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(A\) has at most \(d\) distinct eigenvalues
  prefs: []
  type: TYPE_NORMAL
- en: b) \(A\) has exactly \(d\) distinct eigenvalues
  prefs: []
  type: TYPE_NORMAL
- en: c) \(A\) has at least \(d\) distinct eigenvalues
  prefs: []
  type: TYPE_NORMAL
- en: d) The number of distinct eigenvalues of \(A\) is unrelated to \(d\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which of the following is true about the outer product of two vectors
    \(\mathbf{u}\) and \(\mathbf{v}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It is a scalar.
  prefs: []
  type: TYPE_NORMAL
- en: b) It is a vector.
  prefs: []
  type: TYPE_NORMAL
- en: c) It is a matrix of rank one.
  prefs: []
  type: TYPE_NORMAL
- en: d) It is a matrix of rank zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states “the row rank and column rank
    of \(A\) [are] simply […] the rank, which we denote by \(\mathrm{rk}(A)\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: a. Justification: The text states in the Row Rank Equals Column
    Rank Theorem that “the row rank of \(A\) equals the column rank of \(A\). Moreover,
    \(\mathrm{rk}(A) \leq \min\{n,m\}\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: a. Justification: The text shows that “the columns of \(AB\)
    are linear combinations of the columns of \(A\). Hence \(\mathrm{col}(AB) \subseteq
    \mathrm{col}(A)\). The claim follows by Observation D2.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: a. Justification: The Spectral Theorem states that “A symmetric
    matrix \(A\) has \(d\) orthonormal eigenvectors \(\mathbf{q}_1, \ldots, \mathbf{q}_d\)
    with corresponding (not necessarily distinct) real eigenvalues \(\lambda_1 \geq
    \lambda_2 \geq \cdots \geq \lambda_d\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: c. Justification: The text defines the outer product and states
    that “If \(\mathbf{u}\) and \(\mathbf{v}\) are nonzero, the matrix \(\mathbf{u}
    \mathbf{v}^T\) has rank one.”'
  prefs: []
  type: TYPE_NORMAL
