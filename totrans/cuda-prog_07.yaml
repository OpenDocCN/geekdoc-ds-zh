- en: Chapter 7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章
- en: Using CUDA in Practice
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践中的 CUDA 使用
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter we’ll look at a few examples of the not-so-common uses of GPUs
    to provide insight into how to solve a number of different types of computer problems.
    We’ll look at the problems involved in using GPUs for such computations.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过几个不常见的 GPU 使用案例，提供一些如何解决不同类型计算问题的见解。我们将讨论使用 GPU 进行这些计算时涉及的问题。
- en: Serial and Parallel Code
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 串行与并行代码
- en: Design goals of CPUs and GPUs
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU 和 GPU 的设计目标
- en: CPUs and GPUs, although both execute programs, are a world apart in their design
    goals. CPUs use an MIMD (multiple instruction, multiple data) approach, while
    GPUs use an SIMT (single instruction, multiple thread) instruction model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 和 GPU 尽管都执行程序，但它们的设计目标截然不同。CPU 使用 MIMD（多重指令多重数据）方法，而 GPU 使用 SIMT（单指令多线程）指令模型。
- en: The CPU approach to parallelism is to execute multiple independent instruction
    streams. Within those instruction streams it seeks to extract instruction level
    parallelism. That is, it fills a very long pipeline of instructions and looks
    for instructions that can be sent to independent execution units. These execution
    units usually consist of one or more floating-point units, one or more integer
    units, a branch prediction unit, and one or more load/store units.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 对并行的处理方式是执行多个独立的指令流。在这些指令流中，它试图提取指令级并行性。也就是说，它填充一个非常长的指令管道，并寻找可以发送到独立执行单元的指令。这些执行单元通常由一个或多个浮点单元、一个或多个整数单元、一个分支预测单元和一个或多个加载/存储单元组成。
- en: Branch prediction is something computer architects have worked extensively on
    for over a decade or so. The problem with branching is that the single instruction
    stream turns into two streams, the branch taken path and the branch not taken
    path. Programming constructs such as `for, while` loops typically branch backwards
    to the start of the loop until the loop completes. Thus, in a lot of cases, the
    branch can be predicted statically. Some compilers help with this in setting a
    bit within the branch instruction to say if the branch is likely to be met or
    not. Thus, loops that branch backwards can be predicated as taken, whereas conditionals
    are usually predicated as not taken, thus avoiding the branch altogether. This
    has the added advantage that the next instructions have typically already been
    prefetched into the cache.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 分支预测是计算机架构师研究了十多年的一项技术。分支问题在于单一指令流变成了两个流，一个是执行分支路径，另一个是未执行分支路径。像 `for, while`
    这样的编程结构通常会向后分支到循环开始处，直到循环完成。因此，在许多情况下，分支可以静态预测。一些编译器通过在分支指令中设置一个位来帮助预测分支是否会被执行。因此，向后分支的循环可以预测为已执行，而条件语句通常预测为未执行，从而避免了分支的发生。这还有一个附加优点，即下一条指令通常已经被预取到缓存中。
- en: Branch prediction evolved from the simple but quite effective static model,
    to use a dynamic model that records previous branching history. Multiple levels
    of complex branch prediction are actually present in modern processors due to
    the very high cost of a mispredicted branch and the consequential refilling of
    the long execution pipeline.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 分支预测从简单但相当有效的静态模型发展到使用记录先前分支历史的动态模型。由于错误预测分支的代价非常高，以及随之而来的长执行管道重新填充，现代处理器中实际上存在多级复杂的分支预测。
- en: Along with branch prediction, a technique called *speculative execution* is
    used. Given the CPU will likely have predicted a branch correctly, it makes sense
    to start executing the instruction stream at that branch address. However, this
    adds to the cost of branch misprediction, as now the instruction stream that has
    been executed has to be undone or discarded.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分支预测外，还使用了一种称为*推测执行*的技术。由于 CPU 很可能已经正确预测了分支，因而从该分支地址开始执行指令流是有意义的。然而，这增加了分支错误预测的代价，因为现在已经执行的指令流必须被撤销或丢弃。
- en: The optimal model for both branch prediction and speculative execution is simply
    to execute both paths of the branch and then commit the results when the actual
    branch is known. As branches are often nested, in practice such an approach requires
    multiple levels of hardware and is therefore rarely used.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分支预测和推测执行的最优模型，就是执行分支的两个路径，然后在实际的分支确定后提交结果。由于分支经常嵌套，因此这种方法在实践中需要多个层级的硬件，因此很少使用。
- en: Finally, we have the other major difference, seen until recently, which is the
    amount and number of cache memory levels. The CPU programming model works on the
    nice principle of abstraction, that is, the programmer doesn’t have to care where
    the memory is because the hardware takes care of it. For most programs, except
    those that need to run fast, this works quite well. It used to be that instruction
    cycles were expensive, but with ever-increasing chip density, instruction cycles
    are now cheap. Accessing memory is now the bottleneck on modern processor design
    and this is addressed by the multiple levels of cache.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们看到另一个主要区别，直到最近才出现，那就是缓存内存层次的数量和大小。CPU的编程模型遵循良好的抽象原理，也就是说，程序员不必关心内存在哪里，因为硬件会处理这些。对于大多数程序，除非是那些需要快速运行的程序，这个原理运行得非常好。过去，指令周期是昂贵的，但随着芯片密度的不断增加，指令周期变得便宜了。如今，访问内存成为现代处理器设计的瓶颈，而这一问题通过多个级别的缓存得到了缓解。
- en: GPUs, until the introduction of Fermi, took an alternative approach to this
    design. Fermi designers believe the programmer is best placed to make use of the
    high-speed memory that can be placed close to the processor, in this case, the
    shared memory on each SM. This is the same as the L1 cache found on a conventional
    processor, a small area of low latency and higher bandwidth memory. If you think
    about most programs, this makes a lot of sense. A programmer knows the program
    better than anyone else and therefore should be able to identify which off-chip
    memory accesses can be prevented by using shared memory.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 直到Fermi的引入，GPU采取了一种不同的设计方法。Fermi的设计者认为，程序员最适合利用可以靠近处理器放置的高速内存，在这种情况下，就是每个SM上的共享内存。这与常规处理器上的L1缓存相同，都是低延迟和更高带宽的内存小区域。如果你考虑到大多数程序，这非常有意义。程序员比任何人都更了解程序，因此应该能够识别哪些离芯片的内存访问可以通过使用共享内存来避免。
- en: Fermi expanded the on-chip memory space to 64K, 16 K of which must be allocated
    to an L1 cache. So that there was always some shared memory present, they did
    not allow the entire space to be allocated to either cache or shared memory. By
    default, Fermi allocates 48 K to shared memory and 16 K to cache. However, you
    can switch this and have 48 K of cache and 16 K of shared memory. Kepler also
    introduces a 32K/32K split option. In programs that make no use of shared memory,
    setting this switch can significantly to prefer L1 cache instead of shared memory
    improve performance for memory-bound kernels. This is done with a call to
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi将片上内存空间扩展到64K，其中16K必须分配给L1缓存。因此，为了保证总有一些共享内存存在，他们不允许将整个空间分配给缓存或共享内存。默认情况下，Fermi将48K分配给共享内存，16K分配给缓存。然而，你可以切换这些设置，使得48K为缓存，16K为共享内存。Kepler还引入了一个32K/32K的分配选项。在不使用共享内存的程序中，设置此开关可以显著提高内存受限内核的性能，使其优先使用L1缓存而非共享内存。这可以通过一个调用来实现。
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the sample sort program we use to look at optimizing later in this chapter,
    this simple change reduced the overall execution time by 15%. This is a huge bonus
    for enabling a feature that is disabled by default.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面我们用来优化的示例排序程序中，这个简单的变化将整体执行时间减少了15%。这是启用一个默认禁用的功能所带来的巨大好处。
- en: With the inclusion of an L1 cache, GPUs and CPUs moved closer to one another
    in terms of the data fetched from memory. With previous GPU generations, memory
    accesses needed to be coalesced to achieve any sort of performance. Consider a
    noncoalesced memory fetch on the G80 and GT200 based hardware. If thread 0 reads
    from memory address `0x1000`, thread 1 reads from `0x2000`, thread 3 reads from
    `0x3000`, etc., this results in one memory fetch per thread of 32 bytes. Not 32
    bits, but 32 bytes, the minimum memory transaction size. The next access (`0x1004`,
    `0x2004`, `0x3004`, etc.) did exactly the same.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着L1缓存的引入，GPU和CPU在从内存获取数据方面变得更加接近。在之前的GPU代中，内存访问需要进行合并才能实现任何形式的性能提升。考虑一下基于G80和GT200硬件的非合并内存提取。如果线程0从内存地址`0x1000`读取，线程1从`0x2000`读取，线程3从`0x3000`读取，以此类推，那么每个线程都会进行一次32字节的内存提取。不是32位，而是32字节，这是最小的内存事务大小。下一个访问（`0x1004`、`0x2004`、`0x3004`等）会完全相同。
- en: In Fermi, as with CPUs, a cache line of 128 bytes is fetched per memory access.
    Thus, subsequent access by an adjacent thread will usually hit the cache instead
    of having to go out to global memory on the device. This allows for a far more
    flexible programming model and is more akin to the CPU programming model most
    programmers are familiar with.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Fermi 中，与 CPU 一样，每次内存访问都会获取 128 字节的缓存行。因此，相邻线程的后续访问通常会命中缓存，而不需要访问设备上的全局内存。这使得编程模型更加灵活，并且更像大多数程序员熟悉的
    CPU 编程模型。
- en: One of the aspects of GPU design that differs significantly from CPU design
    is the SIMT model of execution. In the MIMD model, there is separate hardware
    for each thread, allowing entirely separate instruction streams. In the case where
    the threads are processing the same instruction flow, but with different data,
    this approach is very wasteful of hardware resources. The GPU thus provides a
    single set of hardware to run *N* threads, where *N* is currently 32, the warp
    size.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 设计与 CPU 设计显著不同的一个方面是 SIMT 执行模型。在 MIMD 模型中，每个线程都有独立的硬件，允许完全独立的指令流。在线程处理相同的指令流但数据不同的情况下，这种方法非常浪费硬件资源。因此，GPU
    提供了一套硬件来运行 *N* 个线程，其中 *N* 当前为 32，即 warp 大小。
- en: This has a significant impact on GPU program design. SIMT implementation in
    the GPU is similar to the old vector architecture SIMD model. This was largely
    abandoned in the early 1970s when the ever-increasing speed of serial CPUs made
    the “hard” programming of SIMD machines less than appealing. SIMT solves one of
    the key issues, in that programmers are no longer forced to write code in which
    every thread follows the same execution path. Threads can diverge and then converge
    at some later point. The downside of this flexibility is that there is only one
    set of hardware to follow multiple divergent program paths. Thus, each path must
    be executed in turn, or serialized, until the control flow converges once more.
    As a programmer you must be aware of this and think about it in the design of
    your kernels.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这对 GPU 程序设计有重大影响。GPU 中的 SIMT 实现类似于旧的向量架构 SIMD 模型。早在 1970 年代初期，当串行 CPU 的速度不断提高时，SIMD
    机器的“硬”编程方式逐渐不再受欢迎。SIMT 解决了其中的一个关键问题，即程序员不再被迫编写每个线程都遵循相同执行路径的代码。线程可以分歧，然后在稍后某个时刻重新合并。这种灵活性的缺点是，只有一套硬件来处理多个分歧的程序路径。因此，每条路径必须依次执行，或序列化，直到控制流再次合并。作为程序员，你必须意识到这一点，并在设计内核时考虑到它。
- en: Finally, we’ll come to one other significant difference between CPUs and GPUs.
    On the CPU model, there is serial control flow. Executing an instruction that
    requires a number of cycles to complete will stall the current thread. This is
    one of the reasons why Intel uses hyperthreading. The hardware internally switches
    to another thread when the current one stalls. GPUs have not just one other thread,
    but are designed to have thousands of other threads that they can potentially
    switch to. Such a stall happens as a result of both instruction latency and memory
    latency, that is, where the processor is waiting on the completion of an operation.
    The threading model is designed to hide both.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将讨论 CPU 和 GPU 之间的另一个重要区别。在 CPU 模型中，有串行控制流。执行一个需要多个周期才能完成的指令会使当前线程停滞。这也是英特尔使用超线程技术的原因之一。当当前线程停滞时，硬件会内部切换到另一个线程。GPU
    不仅有其他线程，而且设计上能够切换到成千上万的其他线程。这样的停滞是由于指令延迟和内存延迟导致的，也就是说，处理器在等待某个操作完成。线程模型的设计旨在隐藏这两种延迟。
- en: However, the GPU has one other benefit in that it uses lazy evaluation. That
    is, it will not stall the current thread until there is an access to the dependent
    register. Thus, you may read a value into a register early in the kernel, and
    the thread will not stall until such time as (sometime later) the register is
    actually used. The CPU model stalls at a memory load or long latency instruction.
    Consider the following program segments.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GPU 还有一个优势，即使用惰性求值。也就是说，直到访问依赖的寄存器时，它才会停滞当前线程。因此，你可以在内核中较早地将一个值读入寄存器，直到该寄存器实际被使用时，线程才会停滞。CPU
    模型会在内存加载或长延迟指令时停滞。考虑以下程序片段。
- en: 'Segment 1:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If we look at the first segment, the program must calculate the address of `src_array[i]`,
    then load the data, and finally add it to the existing value of `sum`. Each operation
    is dependent on the previous operation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看第一部分，程序必须计算 `src_array[i]` 的地址，然后加载数据，最后将其加到现有的 `sum` 值中。每个操作都依赖于前一个操作。
- en: 'Segment 2:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If we look at the second segment, we iterate in steps of four. Four independent
    `sum` values are used, allowing four independent summations to be computed in
    the hardware. How many operations are actually run in parallel depends on the
    number of execution units available on the processor. This could be execution
    units, in terms of processor cores (using threads), and/or execution units within
    a superscalar processor design.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看第二部分，我们每四步进行一次迭代。使用四个独立的`sum`值，这样可以在硬件中计算四个独立的求和操作。实际并行运行的操作数量取决于处理器上可用的执行单元数量。这些执行单元可以是处理器核心（使用线程）中的执行单元，或者是超标量处理器设计中的执行单元。
- en: 'Segment 3:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, looking at the third segment, we move the load from memory operations
    out of the computation steps. Thus, the load operation for `a1` has three further
    load operations after it, plus some array index calculations, prior to its usage
    in the `sum1` calculation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，看看第三部分，我们将内存操作的加载移出了计算步骤。因此，`a1` 的加载操作后面有三个进一步的加载操作，再加上一些数组索引计算，之后才在 `sum1`
    计算中使用。
- en: In the eager evaluation model used by CPUs we stall at the first read into `a1`,
    and on each subsequent read. With the lazy evaluation model used by GPUs we stall
    only on consumption of the data, the additions in the third code segment, if that
    data is not currently available. As most CPU and GPU designs are superscalar processors,
    using pipelined instructions, both benefit from such an approach within a single
    thread of execution.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 使用的急切求值模型中，我们在第一次读取 `a1` 时发生停顿，在随后的每次读取时也会停顿。而在 GPU 使用的懒惰求值模型中，我们仅在消费数据时发生停顿（即第三段代码中的加法操作），如果该数据当前不可用。由于大多数
    CPU 和 GPU 设计都是超标量处理器，使用流水线指令，因此它们在单线程执行中都能从这种方法中受益。
- en: Algorithms that work best on the CPU versus the GPU
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 CPU 和 GPU 上表现最好的算法
- en: There are many hundreds of computer science algorithms that for decades have
    been developed and optimized for serial CPUs. Not all of these can be applied
    easily to parallel problems. However, the vast majority of problems exhibit parallelism
    in one form or another. A significant number of problems can be broken down into
    operations on a dataset. In many cases, these operations are inherently parallel
    if viewed from either a data or task parallelism viewpoint.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学中有成百上千种算法，这些算法几十年来一直在为串行 CPU 开发和优化。并不是所有这些算法都能轻松应用于并行问题。然而，绝大多数问题在某种形式上都表现出并行性。许多问题可以分解为对数据集的操作。在许多情况下，如果从数据并行性或任务并行性的角度来看，这些操作本质上是并行的。
- en: One of the most important algorithms in parallel work is something called *scan*,
    otherwise known as prefix sum. In the world of serial computing this does not
    exist as it’s not needed. Suppose we have a variable number of elements per output
    of some function. We could allocate a fixed amount of storage per output, such
    as an array, but this would mean there would be gaps in the memory. Output 0 might
    generate 10 entries, output 1, 5 entries, and output 3, 9 entries. We’d need an
    array with at least 10 entries, so we would have 6 wasted slots.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 并行工作中最重要的算法之一是所谓的*扫描*，也叫前缀和。在串行计算中，这种算法并不存在，因为它并不需要。假设我们每个函数输出的元素数量是可变的。我们可以为每个输出分配一个固定大小的存储空间，比如一个数组，但这会导致内存中出现空隙。输出
    0 可能生成 10 个条目，输出 1 生成 5 个条目，输出 3 生成 9 个条目。我们需要一个至少有 10 个条目的数组，这样就会浪费 6 个位置。
- en: Prefix sum stores, in a separate array, the number of elements used for each
    output. The actual data is then compressed (i.e., all the blanks removed) to form
    a single linear array. The problem we have now is where does output for thread
    2 write its values to? To calculate the output index for each output, we simply
    add up all the outputs prior to the current one. Thus, output 2 must write to
    array index 10 as output 1 wrote 10 elements (0…9). Output 2 will write 5 elements
    (10…14), so output 3 will start writing at element 15, and so on.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀和将每个输出所使用的元素数量存储在一个单独的数组中。然后，实际数据被压缩（即去除所有空白）形成一个单一的线性数组。我们现在面临的问题是，线程 2 的输出应该写入哪个位置？为了计算每个输出的输出索引，我们只需将当前输出之前的所有输出加起来。因此，输出
    2 必须写入数组索引 10，因为输出 1 写入了 10 个元素（0…9）。输出 2 将写入 5 个元素（10…14），所以输出 3 将从元素 15 开始写入，依此类推。
- en: We covered in [Chapter 6](CHP006.html) on memory access an example using Sample
    Sort, which uses prefix sum, so I will not repeat here how they can be calculated
    in parallel. The important point to understand is that through the use of prefix
    sum we can convert a great many algorithms to *N* independent outputs. It’s important
    that we can write outputs independently and are not limited by atomics, in effect,
    contention of resources. Such limits, depending on how overloaded they are, can
    severely slow a kernel execution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第6章](CHP006.html)中讨论了内存访问的一个示例，使用了样本排序（Sample Sort），它利用前缀和，因此我在这里不会重复如何并行计算这些前缀和。需要理解的重要一点是，通过使用前缀和，我们可以将许多算法转换为*N*个独立的输出。重要的是，我们可以独立地编写输出，而不受原子操作的限制，实际上，也就是资源的争用。这种限制，取决于它们的过载程度，可能会严重减慢内核执行速度。
- en: Not all parallel architectures are created equal. Many parallel programs and
    parallel languages assume the MIMD model, that is, that threads are independent
    and do not need to execute in groups (or warps) as on the GPU. Thus, not even
    all parallel programs can work on GPUs unchanged. In fact, this has been one problem
    with parallel programs to date; optimization for a specific architecture often
    ties the application to that particular hardware.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 不是所有并行架构都相同。许多并行程序和并行语言假设使用MIMD模型，也就是说，线程是独立的，并且不需要像GPU一样按组（或warp）执行。因此，并不是所有并行程序在GPU上都能原封不动地运行。事实上，这一直是并行程序至今的一个问题；针对特定架构的优化往往将应用程序与该特定硬件绑定在一起。
- en: Standards like MPI and OpenMP don’t really fit well to the GPU model. OpenMP
    is perhaps the closest, in that it requires a shared view of memory. In OpenMP
    the compiler takes care of spawning threads that share a common data area. The
    programmer specifies which loop can be parallelized through various compiler pragmas
    and the compiler takes care of all that nasty “parallel stuff.” MPI, on the other
    hand, considers all processes to be identical and is more suited to clusters of
    nodes than single-node machines.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 像MPI和OpenMP这样的标准与GPU模型并不完全契合。OpenMP可能最接近，因为它要求共享内存视图。在OpenMP中，编译器负责生成共享公共数据区的线程。程序员通过各种编译器指令指定哪些循环可以并行化，编译器会处理所有那些“并行相关的事情”。另一方面，MPI则认为所有进程是相同的，更适合用于节点集群而非单节点机器。
- en: You might take the approach of allocating one GPU thread per MPI process, or
    one block per MPI process. Neither would work particularly well on the GPU, unless
    you could identify that groups of MPI processes were, in fact, following the same
    execution flow and could combine them into warps on the GPU. Typically, MPI is
    implemented as shared CPU/GPU pairs with the CPU handling the network and disk
    input/output (I/O). Implementations using GPU Direct allow transfers to certain
    InfiniBand network cards via a common shared-memory host page. Direct peer-to-peer
    (P2P) transfers over the PCI-E bus without the use of host memory is preferable,
    however. The RDMA (remote DMA) is a feature of the new Kepler architecture that
    enables such features and thus makes GPUs much more of a standalone peer on such
    networks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以采取为每个MPI进程分配一个GPU线程，或为每个MPI进程分配一个块的方法。然而，这两种方法在GPU上都不会特别有效，除非你能识别出一组MPI进程实际上遵循相同的执行流，并能将它们组合成GPU上的“warp”。通常，MPI实现为共享的CPU/GPU对，CPU负责网络和磁盘输入输出（I/O）。使用GPU
    Direct的实现允许通过共享内存主机页面将数据传输到某些InfiniBand网络卡。然而，直接通过PCI-E总线进行的点对点（P2P）传输，而不使用主机内存，通常是更优选的方式。RDMA（远程DMA）是新一代Kepler架构的一个特性，使得GPU在这样的网络中成为一个独立的节点。
- en: With GPUs being included in an ever-higher number into data centers and supercomputer
    installations, both OpenMP and MPI will inevitably evolve to accommodate hardware
    designed to accelerate computations. In [Chapter 10](CHP010.html) we discuss the
    use of OpenACC, the directive-based approach to GPU computing. The OpenMP4ACC
    (OpenMP for accelerators) standard may well move such directives into the mainstream
    OpenMP standard.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随着GPU被越来越多地应用于数据中心和超级计算机的安装中，OpenMP和MPI将不可避免地发展以适应加速计算的硬件。在[第10章](CHP010.html)中，我们讨论了OpenACC的使用，它是基于指令的GPU计算方法。OpenMP4ACC（OpenMP加速器）标准可能会将这些指令纳入主流的OpenMP标准。
- en: With the GPU you have to consider that there are a limited number of threads
    that can easily work *together* on any given problem. Typically, we’re looking
    at up to 1024 threads on Fermi and Kepler, less on older hardware. In reality,
    any reasonably complex kernel is limited to 256 or 512 threads due to register
    usage limitations. The interthread communication considerations dominate any decomposition
    of the problem. Interthread communication is performed via high-speed shared memory,
    so threads in the same block can communicate quickly and with little latency.
    By contrast, interblock communication can only be performed via separate kernel
    invocations, and global memory that is an order of magnitude slower. Kepler also
    extends this model to allow interwarp-based communication without the use of shared
    memory.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU时，你需要考虑到能够轻松地*一起*处理给定问题的线程数量是有限的。通常，我们在Fermi和Kepler架构上最多可以处理1024个线程，较旧的硬件则更少。实际上，任何合理复杂的内核由于寄存器使用限制，最多只能支持256或512个线程。线程间的通信考虑因素主导了问题的任何分解。线程间通信是通过高速共享内存来执行的，因此同一块内存中的线程可以快速且低延迟地进行通信。相比之下，块间通信只能通过单独的内核调用来执行，而且全局内存的访问速度要慢一个数量级。Kepler还扩展了这种模型，允许无需使用共享内存的warp间通信。
- en: The other major consideration for GPU algorithms is the memory available on
    the device. The largest single GPU memory space available is 6 GB on the Tesla
    M2090 cards. Compared with typically 16 to 64 GB on the host, this may be problematic.
    However, this can be solved by using multiple GPU cards, with many high-end motherboards
    able to take up to four PCI-E cards, thus providing up to 24 GB per node of GPU
    memory.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: GPU算法的另一个主要考虑因素是设备上可用的内存。最大单个GPU内存空间为Tesla M2090卡上的6 GB。相比于主机上通常有16至64 GB的内存，这可能会成为问题。然而，通过使用多个GPU卡，这个问题可以解决，许多高端主板能够支持最多四个PCI-E卡，从而为每个节点提供最多24
    GB的GPU内存。
- en: Recursion is also problematic on GPUs, as it’s only supported on compute 2.x
    GPUs, and then only for `__device__` functions and not `__global__` functions.
    The upcoming dynamic parallelism feature found in the Kepler K20 design will help
    in many respects with recursive algorithms.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 递归在GPU上也存在问题，因为它仅在计算能力为2.x的GPU上得到支持，并且仅支持`__device__`函数，而不支持`__global__`函数。Kepler
    K20设计中即将推出的动态并行性功能将在许多方面帮助递归算法。
- en: Many CPU algorithms make use of recursion. Often it’s convenient to break down
    a problem into a smaller problem that is then broken down further and so on until
    it becomes a trivial problem. Binary search is a classic example of this. Binary
    search splits a sorted list of numbers in half and simply asks the question of
    whether the data we’re looking for exists in the left or right set. It then repeats
    the split until either the item is found or the problem becomes just two items
    and is thus trivial to solve.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 许多CPU算法使用递归。通常，将一个问题分解为更小的子问题，然后进一步分解，直到问题变得琐碎是非常方便的。二分查找就是一个经典的例子。二分查找将一个已排序的数字列表分成两半，并简单地询问我们要找的数据是否存在于左边或右边的集合中。然后，它继续分割，直到找到该项，或者问题变成只有两个项，因此可以轻松解决。
- en: However, any recursive algorithm can also be represented as an iterative algorithm.
    The binary search problem just mentioned is shown as an iterative solution within
    the sample sort example (see [Chapter 6](CHP006.html)). Quick sort is also a common
    example of an algorithm that is typically implemented recursively. The algorithm
    picks a pivot point and then sorts all items less than the pivot point to the
    left and less than or equal to the pivot point to the right. You now have 2 independent
    datasets that can be sorted by two independent threads. This then becomes 4 threads
    on the next iteration, then 8, then 16, and so on.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，任何递归算法也可以表示为迭代算法。前面提到的二分查找问题在示例排序示例中展示了作为迭代解法（见[第6章](CHP006.html)）。快速排序也是通常使用递归实现的算法之一。该算法选择一个枢轴点，然后将所有小于枢轴点的元素排到左边，所有小于或等于枢轴点的元素排到右边。现在，你有了两个独立的数据集，可以由两个独立的线程进行排序。接着，在下一次迭代中，这变成了4个线程，然后是8个线程，再到16个线程，依此类推。
- en: The GPU kernel invocation requires a *fixed* number of threads. It cannot currently
    exploit dynamic parallelism, although this will change with the Kepler K20 release.
    Dynamic parallelism is where the amount of parallelism in the problem changes
    over time. In the quick sort problem it grows by a factor of two at every level.
    In path finding–type problems, discovery of a new node may introduce 30,000 or
    more additional paths into a problem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: GPU内核调用需要*固定*数量的线程。目前，它无法利用动态并行性，尽管这将在Kepler K20发布时发生变化。动态并行性是指问题中的并行度随着时间变化而变化。在快速排序问题中，每一层的并行度都会翻倍。在路径寻找类问题中，发现一个新节点可能会为问题引入30,000条或更多的额外路径。
- en: How do you replicate such algorithms on a GPU? There are a number of approaches.
    The easiest is when the parallelism scales in some known manner, as with quick
    sort. You can then simply invoke one kernel per level or one kernel per *N* levels
    of the algorithm back-to-back in a single stream. As one level finishes, it writes
    its state to global memory and the next kernel execution picks up on the next
    level. As the kernels are already pushed into a stream ready to execute, there
    is no CPU intervention needed to launch the next stream. See [Figure 7.1](#F0010).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在GPU上复制此类算法？有多种方法。最简单的方法是当并行度以某种已知方式扩展时，就像快速排序一样。你可以简单地在一个流中依次调用每一层的内核，或者每*N*层的内核。每当一层完成时，它会将状态写入全局内存，下一次内核执行就会处理下一层。由于内核已经被推入流中准备执行，因此不需要CPU干预来启动下一个流。请参见[图7.1](#F0010)。
- en: '![image](../images/F000077f07-01-9780124159334.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-01-9780124159334.jpg)'
- en: FIGURE 7.1 Kernel invocations for a recursive algorithm.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 递归算法的内核调用。
- en: Where the parallelism grows by some indeterminate amount per iteration, you
    can also store the state in global memory. You have to then communicate back to
    the host the number of the amount of parallelism that the next iteration will
    explore. You can do this with an atomic write to shared memory within the block
    and then an atomic add to global memory prior to block completion. Then use a
    `memcpy` to copy the data back to the host that can use this to adjust the next
    kernel launch.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当并行度在每次迭代中以某个不确定的数量增长时，你也可以将状态存储在全局内存中。然后，你必须将下一次迭代将要探索的并行度数量传回主机。你可以通过在块内执行原子写操作到共享内存，再在块完成前进行原子加法到全局内存来实现这一点。接着，使用`memcpy`将数据复制回主机，主机可以利用这些数据来调整下一次内核的启动。
- en: As an example with the first level of quick sort, you can use one block of data
    with a single thread. You then continue invoking single-thread block kernels until
    you reach some multiple of the number of SMs on the GPU. At the point where you
    would saturate the number of blocks on the SM, up to 16 blocks per SM, you extend
    the number of threads per block. At the point you reach 256 threads per block,
    you start again extending the number of blocks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以快速排序的第一层为例，你可以使用一个数据块和一个线程。然后，继续调用单线程块内核，直到达到GPU上SM数量的某个倍数。当你即将填满SM上的块数量时（每个SM最多16个块），你可以增加每个块的线程数量。当每个块达到256个线程时，你可以再次开始增加块的数量。
- en: This approach, although relatively easy to implement, has some disadvantages.
    First, at least initially there is not enough work to saturate the GPU. With just
    one thread at the first level, the kernel overhead is significant. Even at level
    four, we’re invoking just eight blocks, filling half the 16 SMs on a GTX580 device.
    Not until we reach level five would we have one block per SM. With 16 SMs, eight
    blocks per SM, and 256 threads per SM, we’d need 32 K points before all SMs were
    working at full efficiency. This would require 16 kernel invocations.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种方法相对容易实现，但也有一些缺点。首先，至少在初期，没有足够的工作量来使GPU达到饱和。在第一层只使用一个线程时，内核开销非常大。即使到了第四层，我们也只调用了八个块，占用了GTX580设备上16个SM的一半。直到第五层，我们才会有每个SM一个块。当达到16个SM，每个SM有八个块，每个SM有256个线程时，我们需要32K个点才能让所有SM都以最大效率工作。这将需要16次内核调用。
- en: With compute 2.x devices this is not such an issue, as the initial few layers
    can simply be calculated using a recursive call, until you reach the desired depth
    into the structure to warrant relaunching the kernel with many more thread blocks.
    An alternative approach is to do some of the initial work on the CPU and only
    go to the GPU once there is enough parallelism in the problem. Don’t think everything
    has to be done on the GPU. The CPU can be a very useful partner, especially for
    this type of less-parallel work.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算能力为2.x的设备，这并不是一个问题，因为初始的几层可以通过递归调用来计算，直到你达到足够的深度，需要通过更多的线程块重新启动内核。另一种方法是在CPU上完成一些初步工作，只有当问题中具有足够的并行性时，才使用GPU。不要认为所有事情都必须在GPU上完成。CPU可以是一个非常有用的伙伴，尤其是对于这种较少并行的工作类型。
- en: One other solution to these types of problems is to use a special type of scan
    operation called a segmented scan. With a segmented scan you have a regular scan
    operation over a dataset (`min`, `max`, `sum`, etc.) plus an additional array
    that splits the source array into variable size blocks. A single thread or multiple
    threads are assigned per region to calculate the operation. As the additional
    array can also be updated at runtime, this can reduce the need to invoke multiple
    kernels if the segmented scan can be kept within a single block. Otherwise, you
    might just as well adopt the simpler solution, which in many cases works just
    as well and allows the flexibility of changing the number of threads/blocks as
    the problem grows and shrinks in parallelism.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些问题的另一种方法是使用一种特殊的扫描操作，称为分段扫描。通过分段扫描，你可以对数据集执行常规的扫描操作（如`min`、`max`、`sum`等），并使用一个额外的数组将源数组拆分为可变大小的块。每个区域分配一个或多个线程来计算该操作。由于额外的数组也可以在运行时更新，如果分段扫描能够保持在单个块内，这将减少需要调用多个内核的情况。否则，你完全可以采用更简单的解决方案，在许多情况下，它同样有效，并且允许根据问题的并行性变化来灵活调整线程/块的数量。
- en: All of these approaches try to deal with problems GPUs were not natively designed
    to deal with. As a programmer you should be aware of how well an algorithm does
    or does not fit the design model of the hardware. Recursive problems with today’s
    GPUs are often best framed as iterative problems. Selecting an algorithm that
    is appropriate to the hardware and getting the data in the correct layout is often
    key to good performance on the GPU.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法都试图解决GPU天生并未设计来处理的问题。作为程序员，你应该意识到算法与硬件设计模型的契合程度。今天的GPU对于递归问题通常最好将其转化为迭代问题。选择一个适合硬件的算法，并确保数据布局正确，通常是确保GPU上良好性能的关键。
- en: Processing Datasets
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理数据集
- en: With a typical data acquisition you will get data that is interesting, periods
    of data of no interest, and noise on the signal. One simple way of removing noise
    is to filter data above or below some threshold. With the dataset shown in [Figure
    7.2](#F0015), we’ve placed a white line to show where the threshold level has
    been set. As you raise the threshold, you filter out low levels of noise. At the
    far right of the acquisition data you may wish to remove it altogether because
    you are only interested in the data peaks.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的数据采集过程中，你会得到一些有趣的数据，部分无关的数据周期，以及信号中的噪声。去除噪声的一种简单方法是过滤掉高于或低于某个阈值的数据。在[图 7.2](#F0015)中所示的数据集，我们用一条白线表示阈值水平的位置。随着阈值的提高，你会过滤掉较低水平的噪声。在采集数据的最右侧，你可能希望完全去除这些数据，因为你只对数据的峰值感兴趣。
- en: '![image](../images/F000077f07-02-9780124159334.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-02-9780124159334.jpg)'
- en: FIGURE 7.2 Sample data and threshold level.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 示例数据和阈值水平。
- en: With a dataset that you expect to have a very small number of items being filtered
    you can easily append to the same data list, as the frequency of the append operation
    itself is very low. However, as the frequency of the filtered data becomes higher,
    the contention for the single list becomes a bottleneck. While this approach may
    work for a small number of parallel operations, say up to four that you might
    find on a quad-core CPU, the write to a single list, with locking approach does
    not scale.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个你预计将会有非常少量项目需要过滤的数据集，你可以轻松地将数据附加到同一个数据列表中，因为附加操作本身的频率非常低。然而，随着过滤数据的频率增高，对单个列表的争用将成为瓶颈。虽然这种方法对于少量并行操作可能有效，例如四核CPU上可能找到的四个操作，但写入单一列表的锁定方式是无法扩展的。
- en: A much better approach is to have a number of lists and then combine the lists
    together at a later stage. In fact, almost all parallel data processing algorithms
    use this approach in one way or another to avoid the serial bottleneck trying
    to update common data structure causes. This approach also maps very well to the
    model CUDA uses to decompose problems, the tiling approach.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更好的方法是拥有多个列表，然后在后期将这些列表合并。实际上，几乎所有的并行数据处理算法都会以某种方式使用这种方法，以避免由于更新共享数据结构而导致的串行瓶颈。这种方法也非常适合CUDA用于分解问题的模型——瓦片化方法。
- en: We should also recognize that a filtering operation is actually a common parallel
    pattern, a split operation. A split operation takes a given dataset and splits
    it into *N* parts based on some primary key. In our filtering example we’re using
    the threshold condition as the primary key and trying to extract the data that
    is above a given threshold. We may or may not be interested in keeping the data
    that is below the threshold. The split operation simply generates two lists, one
    matching some criteria and the other for the data that did not match.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该认识到，过滤操作实际上是一种常见的并行模式，即拆分操作。拆分操作将给定的数据集根据某些主键拆分成*N*个部分。在我们的过滤示例中，我们使用阈值条件作为主键，并试图提取高于给定阈值的数据。我们可能对保留低于阈值的数据不感兴趣，也可能感兴趣。拆分操作简单地生成两个列表，一个符合某些标准，另一个则是未符合标准的数据。
- en: When performing such an operation in parallel, we have a number of considerations.
    The first problem is we do not know how many data items will meet the matching
    criteria and how many would therefore be on the nonmatching list. The second is
    that we have many processing elements that need to cooperate in some way to build
    an output list. Finally, any ordering present in the original dataset must usually
    be maintained.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行执行这种操作时，我们需要考虑多个因素。第一个问题是我们不知道有多少数据项会符合匹配标准，因而有多少数据项会出现在不匹配的列表中。第二个问题是我们有许多处理元素需要以某种方式合作，以构建输出列表。最后，原始数据集中的任何顺序通常必须被保持。
- en: The scan primitive is incredibly powerful and can be used in a number of data
    processing scenarios. Suppose, for example, we have a list of students in a database
    in no particular order. We might want to extract, from that student list, all
    students who are in class CS-192\. We thus end up with two datasets, those matching
    the criteria and those that do not.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描原语非常强大，可以应用于许多数据处理场景。例如，假设我们在数据库中有一个学生列表，且没有特定顺序。我们可能希望从这个学生列表中提取所有属于CS-192班的学生。这样我们最终会得到两个数据集，一个是符合标准的，另一个是不符合标准的。
- en: Suppose we have a weather station near the equator that is collecting the temperature
    once per minute over several years. We might want to know how many sample points,
    or minutes, the temperature was in excess of 40 degrees centigrade over the sample
    period.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个位于赤道附近的气象站，每分钟收集一次温度数据，持续多年。我们可能想知道在整个样本期间内，温度超过40摄氏度的样本点或分钟数有多少。
- en: Equally, the data we are looking at may be financial data—for example, the value
    of transactions. You might wish to screen the data to know if there are transactions
    over a certain value, and how many. Certain high-value transactions may have a
    regulatory requirement to report or record, for example, to avoid money laundering.
    Your company policy may also dictate that transactions over a certain value require
    some additional checks. We want to extract from a vast set of data, easily and
    quickly, those that are of interest.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们正在查看的数据可能是财务数据——例如交易的金额。你可能希望筛选数据，以了解是否存在超过某个金额的交易，以及有多少笔。某些高金额交易可能有法规要求进行报告或记录，例如为了防止洗钱。你的公司政策也可能要求对超过某个金额的交易进行额外检查。我们希望从庞大的数据集中快速、轻松地提取出那些感兴趣的数据。
- en: If the data is data from a scientific instrument, you may wish to screen the
    packet of data for “interesting” anomalies. Those packets that contain some anomaly
    are forwarded for further analysis, while the regular packets are sent elsewhere
    or discarded. How we define “interesting” varies according to the application,
    but the fundamental need to be able to scan and filter data is something we find
    in many domains.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是来自科学仪器的数据，你可能希望筛选数据包中的“有趣”异常。那些包含异常的数据包将被转发以供进一步分析，而常规数据包则会被发送到其他地方或丢弃。我们如何定义“有趣”取决于具体应用，但能够扫描和过滤数据的基本需求在许多领域中都有体现。
- en: Scanning one million data elements on a CPU can be time consuming. It’s the
    standard “for *i* equals 0 to size of dataset” problem. Using a GPU we can scan
    the dataset in parallel. If the dataset is large, the only limit to this is the
    number of GPUs we can assign to the problem. As the largest GPU card to date,
    the Tesla M2090 can hold 6 GB of data, however, you are limited to a problem size
    of 18–24 GB per node before you need to use host or even disk-based storage.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 上扫描一百万个数据元素可能会非常耗时。这是标准的“*i* 从 0 到数据集大小”的问题。使用 GPU，我们可以并行扫描数据集。如果数据集很大，那么唯一的限制就是我们可以分配给该问题的
    GPU 数量。到目前为止，最大的 GPU 卡 Tesla M2090 可以容纳 6 GB 的数据，但每个节点的最大问题规模限制在 18–24 GB 之间，超过这个限制就需要使用主机或甚至基于磁盘的存储。
- en: Next we will look at using some of the less well-known features of CUDA to address
    data processing. This is, of course, applicable to any form of data as almost
    all problems involve processing input data in one form or another.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何使用 CUDA 的一些不太为人所知的特性来处理数据。这当然适用于任何形式的数据，因为几乎所有问题都涉及以某种形式处理输入数据。
- en: Using ballot and other intrinsic operations
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 `ballot` 和其他内建操作
- en: 'As of compute 2.0 devices, NVIDIA introduced a very useful function:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从计算 2.0 设备开始，NVIDIA 引入了一个非常有用的函数：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This function evaluates the predicate value passed to it by a given thread.
    A predicate, in this context, is simply a true or false value. If the predicate
    value is nonzero, it returns a value with the *N*th bit set, where *N* is the
    value of the thread (`threadIdx.x`). This atomic operation can be implemented
    as C source code as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数会评估由给定线程传递的谓词值。在此上下文中，谓词只是一个真假值。如果谓词值非零，它会返回一个将第 *N* 位设置为 1 的值，其中 *N* 是线程的值（`threadIdx.x`）。这个原子操作可以通过如下的
    C 源代码实现：
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The nonatomic version just shown is a similar speed to the intrinsic version,
    but will work on all compute versions. We’ll use it later to provide backward
    compatibility with older hardware.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 前面展示的非原子版本的速度与内建版本相似，但可以在所有计算版本上运行。稍后我们将使用它来提供对旧硬件的向后兼容性。
- en: The usefulness of ballot may not be immediately obvious, unless you combine
    it with another atomic operation, `atomicOr`. The prototype for this is
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`ballot` 的有用性可能不会立即显现，除非你将它与另一个原子操作 `atomicOr` 结合使用。其原型如下：'
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It reads the value pointed to by `address`, performs a bitwise `OR` operation
    (the | operator in C) with the contents of `val`, and writes the value back to
    the address. It also returns the old value. It can be used in conjunction with
    the `__ballot` function as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 它读取 `address` 所指向的值，与 `val` 的内容进行按位 `OR` 运算（C 中的 | 运算符），然后将结果写回该地址。它还会返回旧值。可以与
    `__ballot` 函数结合使用，示例如下：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`                              __ballot(data[tid] > threshold) );`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`                              __ballot(data[tid] > threshold) );`'
- en: In this call we use an array that can be either in shared memory or global memory,
    but obviously shared memory is preferable due to it’s speed. We write to an array
    index based on the warp number, which we implicitly assume here is 32\. Thus,
    each thread of every warp contributes 1 bit to the result for that warp.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个调用中，我们使用一个数组，这个数组可以位于共享内存或全局内存中，但显然由于速度原因，共享内存是更好的选择。我们根据 warp 的编号写入数组索引，在这里我们隐式地假设
    warp 数量为 32。因此，每个 warp 的每个线程都会对该 warp 的结果贡献 1 位。
- en: For the predicate condition, we asked if the value in `data[tid]`, our source
    data, is greater than a given threshold. Each thread reads one element from this
    dataset. The results of each thread are combined to form a bitwise `OR` of the
    result where thread 0 sets (or not) bit 0, thread 1 sets (or not) bit 1, etc.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于谓词条件，我们询问 `data[tid]` 中的值（我们的源数据）是否大于给定阈值。每个线程从这个数据集中读取一个元素。每个线程的结果被组合形成按位
    `OR`，其中线程 0 设置（或不设置）第 0 位，线程 1 设置（或不设置）第 1 位，依此类推。
- en: 'We can then make use of another compiler intrinsic, the `__popc` function.
    This returns the number of bits set within a 32-bit parameter. It can be used
    to accumulate a block-based sum for all warps in the block, as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以利用另一个编译器内建函数 `__popc`。它返回一个 32 位参数中已设置的位数。它可以用于计算所有 warp 在该块中的位计数和，如下所示：
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Thus, we can accumulate for a given CUDA block the number of threads in every
    warp that had the condition we used for the predicate set. In this example, the
    condition is that the data value was larger than a threshold. A block-based sum
    is useful in many algorithms, but a CUDA kernel will consist of many blocks, typically
    thousands. If you’d like to know how many data items match the predicate across
    the whole dataset, you have to add up the sums from each block.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以为给定的CUDA块累加每个warp中满足我们用于谓词设置的条件的线程数。在这个例子中，条件是数据值大于某个阈值。基于块的求和在许多算法中都很有用，但一个CUDA内核通常由许多块组成，通常是成千上万的。如果你想知道整个数据集上有多少数据项符合谓词，你必须将每个块的和加起来。
- en: There are a number of choices for doing this. For a small number of blocks,
    we can simply ship the resultant block counts back to the CPU and have the CPU
    perform a summation. This may be a useful strategy if the CPU would otherwise
    be idle and there are other streams of GPU work that could be performed on the
    GPU (see [Chapter 8](CHP008.html) for a discussion of how to do this).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以做到这一点。对于少量块，我们可以简单地将结果块计数发送回CPU，并让CPU执行求和。如果CPU本来是空闲的，而GPU上还有其他可以执行的工作流，这可能是一个有用的策略（有关如何执行此操作的讨论，请参见[第8章](CHP008.html)）。
- en: Another strategy is to write all the partial sums from the blocks to global
    memory on the GPU. However, to complete a summation of all the individual block
    components, all the blocks in all the SMs have to have completed the evaluation
    of the predicate. The only way to ensure this is to complete the current kernel
    and invoke another one. Then all global memory values previously written have
    to be re-read in some way, likely via a parallel reduction, and a final sum calculated.
    Although this might be the way taught in traditional CPU parallel programming,
    it’s not the best way from a performance perspective on a GPU.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略是将所有块的部分和写入GPU的全局内存。然而，为了完成所有单独块组件的求和，所有SM中的所有块必须完成谓词的评估。确保这一点的唯一方法是完成当前内核并调用另一个内核。然后，之前写入的所有全局内存值必须以某种方式重新读取，可能通过并行归约，然后计算最终和。虽然这种方法可能是传统CPU并行编程中教授的方式，但从GPU的性能角度来看，这并不是最好的方式。
- en: If we look at the number of blocks that are resident on a Fermi SM, up to eight
    blocks *can* be resident, although typically you see a maximum of six. Let’s assume
    the maximum for now is eight blocks. There are 16 SMs in the largest Fermi device.
    Thus, there are a maximum of 8 × 16 = 128 blocks resident on the device at any
    one time. We can therefore simply accumulate to a *single value* in global memory
    using the `atomicAdd` function as we produce only one update per block.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看一个Fermi SM上常驻的块的数量，最多可以有八个块*是*常驻的，尽管通常最多见到的是六个。假设现在最大为八个块。最大的Fermi设备有16个SM。因此，设备上一次最多可以有8
    × 16 = 128个块常驻。我们可以因此使用`atomicAdd`函数在全局内存中简单地累加到*一个单一的值*，因为每个块只产生一个更新。
- en: Statistically, the probability of more than one block arriving at the atomic
    add instruction, at the same time, is quite small. Given that the memory transactions
    to read the source data will likely arrive in sequence, this in fact nicely sequences
    the execution flow within the SMs and consequently ensures the atomic add operations
    do not compete with one another.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从统计学角度来看，同时有多个块到达原子加法指令的概率是相当小的。因为读取源数据的内存事务很可能是顺序到达的，实际上这很好地排定了SM内部的执行流程，因此确保原子加法操作不会相互竞争。
- en: Using this technique takes around 5 ms to scan one million elements, excluding
    the transfer time to and from the GPU. We exclude the transfer time because it’s
    likely the data will remain entirely resident on the GPU. Consequently, we could
    process around two hundred million queries like this on the dataset per second.
    In practice, the predicates may be much more complex and we’ll look at how this
    impacts the performance later.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术扫描一百万个元素大约需要5毫秒，排除GPU之间的数据传输时间。我们排除传输时间是因为数据很可能会完全常驻在GPU上。因此，我们每秒钟可以对数据集执行大约两亿次这样的查询。实际上，谓词可能更复杂，我们稍后将讨论这如何影响性能。
- en: 'For the moment, let’s look at the complete function to do this in a little
    more detail:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地看一下执行此操作的完整函数：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We declare a couple of functions: a device function that calculates the predicate
    condition and a global function that provides a wrapper to call the ballot function.
    To the ballot function we pass the dataset to search through an area of memory
    to place the block results into, an area of memory to place the accumulated result
    into, the number of elements to process, and finally a threshold for the comparison.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声明了几个函数：一个设备函数，用于计算谓词条件，一个全局函数，作为调用投票函数的封装器。我们将数据集传递给投票函数，投票函数将数据存储到一个内存区域中，并将块结果存放到一个内存区域中，同时还传递处理元素的数量，最后传递比较的阈值。
- en: 'Notice with such a format we could easily implement other operations such as
    less than, equal to, etc. by writing a new predicate function and wrapper, as
    follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，使用这种格式，我们可以通过编写新的谓词函数和封装器，轻松实现其他操作，例如小于、等于等，如下所示：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '` const u32 threshold,`'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '` const u32 threshold,`'
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`  // of bits set from each warp.`'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`  // 从每个warp中设置的位数。`'
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The first part of the function calculates the absolute thread ID:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的第一部分计算绝对线程ID：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This function is designed to work with a single dimension of threads. With large
    datasets (around 16 million elements plus), we’ll need to make use of another
    dimension, as we would otherwise launch more than 64K blocks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数设计用于处理单维度的线程。对于大型数据集（大约1600万个元素及以上），我们需要利用另一个维度，否则我们将启动超过64K个块。
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We then calculate our current warp by simply dividing (right shifting) the current
    thread index by 32\. We do the same with the block dimension to work out the number
    of warps in the current block.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过简单地将当前线程索引右移32位来计算当前的warp。对于块维度，我们也进行同样的操作，以计算当前块中的warp数量。
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We then have to check if our absolute thread ID, `tid`, is within the dataset.
    In cases where the number of elements is not a power of two the `tid` calculation
    for the last block would end up after the end of the source data. We neither want
    to read or write out-of-bounds arrays, so this check is necessary.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要检查我们的绝对线程ID，`tid`，是否在数据集范围内。在元素数量不是2的幂时，最后一个块的`tid`计算结果会超出源数据的范围。我们既不希望读取也不希望写入越界的数组，因此这个检查是必要的。
- en: Note that this also implicitly means we cannot perform a `__syncthreads` operation
    within this `if` block, as all threads, even those off the end of the array, must
    participate in such a synchronization operation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这也意味着我们不能在这个`if`块内执行`__syncthreads`操作，因为即使是那些在数组末尾的线程，也必须参与这样的同步操作。
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next we have to clear the value of the shared memory we’re about to use. Shared
    memory can hold the value from the last kernel run and is not implicitly initialized
    to zero. As we need only a single writer, the first thread in each warp clears
    the value. Note we do not require any synchronization here because the first thread
    in every warp does the write. Branching within a warp in this way causes the other
    threads to implicitly wait at the end of the `if` statement.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要清除即将使用的共享内存的值。共享内存可以保存上次内核运行的值，并不会自动初始化为零。由于我们只需要一个写入者，warp中的第一个线程会清除这个值。需要注意的是，这里不需要任何同步操作，因为每个warp中的第一个线程进行写操作。通过这种方式在warp内的分支使得其他线程在`if`语句的末尾隐式等待。
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We can now have every thread in every active warp call the `atomicOr` function
    with the address of the shared memory element for this current warp. We pass to
    the `OR` operation the value returned from the `__ballot` call. We pass to `__ballot`
    the return value from calling the `predicate_func` function pointer, passing it
    the two data items to evaluate. This then jumps off and does the evaluation, in
    this case calling the `predicate_gt` function defined earlier.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以让每个活跃warp中的每个线程调用`atomicOr`函数，传入当前warp的共享内存元素地址。我们将`__ballot`调用返回的值传递给`OR`操作。我们将从调用`predicate_func`函数指针返回的值传递给`__ballot`，并传递给它要评估的两个数据项。然后它跳转并进行评估，在这种情况下调用之前定义的`predicate_gt`函数。
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now we have to wait for all warps within the block to execute before we can
    do the second part, the block level accumulate.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须等待块内的所有warp执行完毕，然后才能进行第二部分，即块级累加。
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As the maximum number of threads per block is 1024, the maximum number of warps
    per block is 32 (1024 ÷ 32 = 32). Thus, we can process the accumulate using just
    a single warp. We could have used thread 0 from each warp as we did before, but
    in this case we want the other warps to complete, not be left executing a single
    thread each.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个块的最大线程数是 1024，因此每个块的最大 warp 数量是 32（1024 ÷ 32 = 32）。因此，我们可以仅使用一个 warp 来处理累加。我们本可以像之前那样使用每个
    warp 的线程 0，但在这种情况下，我们希望其他 warp 完成，而不是让每个 warp 只执行一个线程。
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Again we have no idea of the existing value in the shared memory element we’re
    about to use to accumulate into, so we need to zero it. Note that, as we now have
    only one warp running, no synchronization is required. Thread 0 will enter the
    condition while threads 1…31 will pass over it and implicitly wait for thread
    0 to reconverge with them.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们不知道即将用来进行累加的共享内存元素中的现有值，因此需要先将其清零。注意，由于现在只运行一个 warp，因此不需要同步。线程 0 将进入条件，而线程
    1 到 31 会跳过该条件，并隐式地等待线程 0 与它们重新汇合。
- en: '[PRE21]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We now add to the block-based shared memory accumulator the number of bits that
    were set in the result produced for the other warps in the block. These are in
    adjacent elements of shared memory, one element per warp. Thus, there are no read
    shared memory bank conflicts. However, the threads need to serialize the writes
    to the accumulator to ensure correctness. As you typically have 256 threads per
    block, this gives eight warps. This serialization does not really warrant a parallel-type
    reduction. However, with a larger number of warps a parallel reduction might work
    slightly faster.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将块基共享内存累加器中的位数增加到其他 warp 生成的结果中。这些结果存储在共享内存的相邻元素中，每个 warp 一个元素。因此，避免了共享内存银行冲突。然而，线程需要序列化写操作到累加器，以确保正确性。通常每个块有
    256 个线程，因此有 8 个 warp。这个序列化操作实际上不需要并行类型的归约。然而，如果 warp 数量更大，并行归约可能会稍微加速执行。
- en: '[PRE22]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As we need only one writer, we select thread 0 to perform the next operation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只需要一个写入器，我们选择线程 0 来执行下一步操作。
- en: '[PRE23]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Finally, we read the block level accumulator from shared memory into a register,
    as we’ll make use of it twice. We then write the block result to global memory,
    something we only have to do if we’re interested in the block results in addition
    to the overall accumulated result.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将块级累加器从共享内存读入寄存器，因为我们将两次使用它。然后，我们将块级结果写入全局内存，只有在我们需要块结果以及整体累加结果时，才需要执行此操作。
- en: We then call the `atomicAdd` function to add into the single global accumulator
    the overall result. Note that we cannot zero the result of the final accumulator
    in any of the blocks. It must be done by the host prior to the call to the function.
    The reason for this is simple. The blocks, and the warps within those blocks,
    may execute in any order. Thus, we cannot say something like `if (threadIdx.x
    == 0) && (blockIdx.x ==0)` then zero the accumulator. Doing this *may* work because
    it just so happens that warp 0 of block 0 executed first, but this is poor practice.
    CUDA’s execution model is such that blocks can be, and are, executed out of order.
    You cannot assume any implicit order of block execution.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们调用 `atomicAdd` 函数，将总结果加到单一的全局累加器中。注意，我们不能将最终累加器的结果清零，这个操作必须在主机端完成，在调用函数之前。原因很简单：块及其中的
    warp 可以以任何顺序执行。因此，我们不能像 `if (threadIdx.x == 0) && (blockIdx.x == 0)` 那样清零累加器。这样做*可能*有效，因为刚好是块
    0 的 warp 0 最先执行，但这并不是好做法。CUDA 的执行模型是块的执行顺序是随机的，你不能假设块执行有任何隐式顺序。
- en: With a minor modification to supply the missing `__ballot` function for the
    GTX 260 (a compute 1.3 device), we can run this kernel on a range of devices.
    Note we can’t use the 9800GT as it’s a compute 1.1 device and therefore does not
    support shared memory based atomic operations.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个小的修改来提供 GTX 260（一个 compute 1.3 设备）所缺失的 `__ballot` 函数，我们可以在多个设备上运行这个内核。注意，我们不能使用
    9800GT，因为它是 compute 1.1 设备，因此不支持基于共享内存的原子操作。
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: What is perhaps strange at first glance is that the GTX260 is 50% faster than
    the more modern GTX460\. However, the GTX260 has approximately four times the
    number of SMs. Each SM has its own internal set of shared memory so the GTX260
    has a much wider bandwidth to the shared memory than the GTX460.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，可能会觉得 GTX260 比较现代的 GTX460 快 50%。然而，GTX260 的 SM 数量大约是 GTX460 的四倍。每个 SM 都有自己的内部共享内存，因此
    GTX260 相较于 GTX460 具有更大的共享内存带宽。
- en: We can also make one small modification. As we’re using the `atomicOr` function
    we actually don’t need the additional atomic functionality of `__ballot`, so we
    can in all cases use the nonatomic version. This revises the timing a little.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以做一个小的修改。由于我们使用的是`atomicOr`函数，实际上不需要`__ballot`的附加原子功能，因此我们在所有情况下都可以使用非原子版本。这会稍微修改计时。
- en: '[PRE25]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You can see that this drops the time significantly on Fermi devices, as the
    GTX260 is already using the nonatomic version. The time for the GTX470 is reduced
    by 15% and the time for the GTX460 is reduced by 21%. This slightly improved time
    allows us to scan some 1632 million elements per second on a single GTX470\. This
    will, however, be reduced if we use more complex predicates and/or a dataset requiring
    more than one block dimension.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，这大大减少了 Fermi 设备上的时间，因为 GTX260 已经在使用非原子版本。GTX470 的时间减少了 15%，而 GTX460 的时间减少了
    21%。这个略微改进的时间使我们能够在单个 GTX470 上每秒扫描大约 1632 万个元素。然而，如果我们使用更复杂的谓词和/或需要多个块维度的数据集，这个时间将会减少。
- en: 'To get a feel for this, what happens to the timing if we change the results
    to within a boundary, rather than simply larger than a threshold? For this we
    need to modify the predicate condition as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这一点，如果我们将结果更改为在边界内，而不仅仅是大于某个阈值，那么计时会发生什么变化？为此，我们需要修改谓词条件如下：
- en: '[PRE26]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Thus, we have introduced another condition, potentially increasing significantly
    the overall timing. What is the effect in practice?
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们引入了另一个条件，可能会显著增加总体计时。那么，实际效果如何呢？
- en: '[PRE27]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: You can see that the effect of adding another condition is marginal at best,
    with a 0.1 ms difference in execution time. This would imply the predicate could
    become reasonably complex without causing a significant slowdown.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，添加另一个条件的效果最多是微乎其微，执行时间差异为 0.1 毫秒。这意味着谓词可以变得相当复杂，而不会导致显著的减速。
- en: The fact that we can use very complex predicate conditions allows for very complex
    operations to be coded efficiently on a GPU. Even codes where the data points
    must be gathered in some way can use such a set of primitives. All we need to
    do in such cases is adjust the predicate to take more data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用非常复杂的谓词条件，这使得在 GPU 上高效编码非常复杂的操作成为可能。即使是需要以某种方式收集数据点的代码，也可以使用这样的原语集合。在这种情况下，我们需要做的就是调整谓词，以便获取更多数据。
- en: Profiling
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能分析
- en: We’ll pick up the example we looked at in [Chapter 6](CHP006.html), sample sort,
    and use it to look at how we can use profiling tools to identify problems in the
    implementation of a given algorithm.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用[第 6 章](CHP006.html)中讨论的示例排序，并通过它来探讨如何使用性能分析工具识别给定算法实现中的问题。
- en: The sample sort example already contains a number of timing elements, which
    we can use to adjust various parameters. Please re-read the sample sort example
    in [Chapter 6](CHP006.html) if you’re not familiar with how sample sort works.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 示例排序示例已经包含了多个计时元素，我们可以利用这些元素调整各种参数。如果你不熟悉示例排序是如何工作的，请重新阅读[第 6 章](CHP006.html)中的示例排序部分。
- en: The major parameters are the number of samples and the number of threads. If
    we ask the program to explore the possible search space, doubling the number of
    samples per iterations and using 32, 64, 128, or 256 threads, we find the following
    promising cases.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 主要参数是样本数量和线程数量。如果我们要求程序探索可能的搜索空间，双倍增加每次迭代的样本数量，并使用 32、64、128 或 256 个线程，我们会发现以下有前景的情况。
- en: '[PRE28]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`Sort Bins Time -    CPU:   37.38  GPU:57.57  39.40  44.81  41.66`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sort Bins Time -    CPU:   37.38  GPU:57.57  39.40  44.81  41.66`'
- en: '[PRE29]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If we view one example as a pie chart, it makes it easy to see where we’re spending
    our time ([Figure 7.3](#F0020)).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将一个示例视为饼图，就可以轻松看到我们在时间上花费的地方（[图 7.3](#F0020)）。
- en: '![image](../images/F000077f07-03-9780124159334.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-03-9780124159334.jpg)'
- en: FIGURE 7.3 Sample sort time distribution, 16 K samples.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 示例排序时间分布，16 K 样本。
- en: So it’s clear from the chart that approximately three-quarters of the time is
    used for sorting and one-quarter for setting up the sample sort. However, as we
    increase the number of samples used, this changes ([Figure 7.4](#F0025)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中可以明显看出，大约四分之三的时间用于排序，四分之一用于设置示例排序。然而，随着样本数量的增加，这种情况会发生变化（[图 7.4](#F0025)）。
- en: '![image](../images/F000077f07-04-9780124159334.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-04-9780124159334.jpg)'
- en: FIGURE 7.4 Sample sort time distribution, 64 K samples.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 示例排序时间分布，64 K 样本。
- en: As you can see from [Figure 7.4](#F0025), suddenly the time to sort the sample
    jumps to around one-third of the total time. We also see quite a lot of variability
    depending on the number of samples and the number of threads used. We’ll concentrate
    on optimizing the middle case, 32 K samples using 64 threads per block.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从[图 7.4](#F0025)中看到的，排序样本的时间突然跃升到总时间的大约三分之一。我们还可以看到，根据样本数量和使用的线程数的不同，结果变化很大。我们将集中优化中间情况，即使用
    64 个线程每块处理 32K 样本。
- en: Parallel Nsight provides a very useful feature listed under the “New Analysis
    Activity.” Parallel Nsight is a free debugging and analysis tool that is incredibly
    useful for identifying bottlenecks.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Parallel Nsight 提供了一个非常有用的功能，列在“新分析活动”下。Parallel Nsight 是一个免费的调试和分析工具，非常有助于识别瓶颈。
- en: The first option in Nsight to be sure is to select the “Profile” activity type
    ([Figure 7.5](#F0030)). By default this will run a couple of experiments, “Achieved
    Occupancy” and “Instruction Statistics.” Running these on the sample sort example
    produces a summary. At the top of the summary page is a dropdown box. Selecting
    “CUDA Launches” shows some useful information, as shown in [Figure 7.6](#F0035).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Nsight 中，第一个要选择的选项是选择“Profile”活动类型（[图 7.5](#F0030)）。默认情况下，这将运行几个实验，“已实现占用率”和“指令统计”。在样本排序示例上运行这些实验会产生一个总结。在总结页面的顶部是一个下拉框。选择“CUDA
    启动”会显示一些有用的信息，如[图 7.6](#F0035)所示。
- en: '![image](../images/F000077f07-05-9780124159334.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-05-9780124159334.jpg)'
- en: FIGURE 7.5 Parallel Nsight launch options.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 Parallel Nsight 启动选项。
- en: '![image](../images/F000077f07-06-9780124159334.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-06-9780124159334.jpg)'
- en: FIGURE 7.6 Parallel Nsight analysis.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 Parallel Nsight 分析。
- en: The first view is the “Occupancy View” (bottom left corner in [Figure 7.6](#F0035)).
    What you should notice here is that there is a summary of the launch parameters
    for the kernel and what factors are limiting occupancy in red. In our case, the
    block limit per device, eight blocks, is limiting the maximum number of active
    warps on the device. Remember that warps are groups of threads from which the
    scheduler can select. The scheduler switches between warps to hide memory and
    instruction latency. If there are not enough warps resident, then this *may* limit
    performance if the GPU has no other warps to run.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个视图是“占用率视图”（[图 7.6](#F0035)左下角）。你应该注意到，这里有一个关于内核启动参数的总结，以及以红色标出的限制占用率的因素。在我们的例子中，每个设备的块限制为
    8 个块，这限制了设备上活动 warp 的最大数量。请记住，warp 是线程的组，调度器可以从中选择。调度器在不同的 warp 之间切换，以隐藏内存和指令的延迟。如果没有足够的
    warp 存在，那么这*可能*会限制性能，因为 GPU 没有其他 warp 可供运行。
- en: We have launched around 16 warps, when the maximum per device is 48, achieving
    one-third of the maximum occupancy of the device. This would suggest that we should
    improve occupancy by increasing the number of warps per device, which in turn
    means increasing the number of threads. However, measured results show this produces
    the opposite effect, actually reducing performance.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经启动了大约 16 个 warp，而每个设备的最大数量为 48，达到了设备最大占用率的三分之一。这表明我们应该通过增加每个设备的 warp 数量来提高占用率，这反过来意味着增加线程数。然而，测量结果表明，这会产生相反的效果，实际上会降低性能。
- en: The second screen that is interesting is the “Instruction Stats” ([Figure 7.7](#F0040)).
    What is noticeable here (IPC section) is there is a large block of issued instructions
    that were never executed. The executed instructions are shown, on screen, in the
    pink section on the first bar chart on the bottom left where the lower line is
    drawn through the bars. The blue bars indicate that instructions are being reissued
    due to serialization. Serialization is where, for whatever reason, threads are
    not able to execute as a complete warp (set of 32 threads). This is usually associated
    with divergent control flow, uncoalesced memory accesses, or operations that have
    limited throughput because of conflicts (shared memory or atomics).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个有趣的屏幕是“指令统计”（[图 7.7](#F0040)）。在这里（IPC 部分）值得注意的是，有一大块发出的指令从未被执行。已执行的指令显示在屏幕上，在左下角的第一个条形图的粉红色部分，其中下方的线穿过条形图。蓝色的条形表示由于串行化，指令正在被重新发出。串行化是指由于某些原因，线程无法作为完整的
    warp（32 个线程的集合）来执行。这通常与分支控制流、不合并的内存访问或由于冲突（共享内存或原子操作）导致的吞吐量受限操作相关。
- en: '![image](../images/F000077f07-07-9780124159334.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-07-9780124159334.jpg)'
- en: FIGURE 7.7 Parallel Nsight analysis.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 Parallel Nsight 分析。
- en: Also notice the distribution of work to the SMs is uneven (SM Activity block,
    [Figure 7.7](#F0040)). We launched 512 blocks of 64 threads. Given 14 SMs on the
    GTX470 device being used, we’d expect just over 36 blocks (72 warps) per SM. In
    practice, some SMs got 68 warps while others got 78 warps (Warps Launched section,
    [Figure 7.7](#F0040)). Also notice that, despite being given the same number of
    warps, some SMs take longer, implying all warps are not being given an equal amount
    of work in terms of execution time.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，注意到工作分配给SM的方式是不均匀的（SM活动块，[图7.7](#F0040)）。我们启动了512个64线程的块。考虑到使用的GTX470设备上有14个SM，我们预计每个SM大约有36个块（72个warp）。实际上，一些SM获得了68个warp，而另一些则获得了78个warp（已启动warp部分，[图7.7](#F0040)）。还需要注意的是，尽管给定了相同数量的warp，一些SM的执行时间较长，意味着并非所有warp在执行时间上都得到了相同的工作量。
- en: When we move to 256 threads per block, the variability we see in issued versus
    executed instructions grows. The number of scheduled blocks drops from eight to
    just three due to the use of 34 registers per thread. Although not an issue with
    64 threads per block, 256 threads per block limits the overall number of blocks
    that can be scheduled per SM. However, despite this, the number of warps scheduled
    climbs to 24 instead of 16, providing a 50% occupancy rate. Does further increasing
    occupancy help?
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将每块的线程数增加到256时，发布指令与实际执行的指令之间的差异增大。由于每个线程使用34个寄存器，调度的块数从八个降至仅三个。虽然在每块64个线程时不存在这个问题，但每块256个线程限制了每个SM可以调度的总块数。然而，尽管如此，调度的warp数从16增加到24，提供了50%的占用率。进一步增加占用率是否有帮助？
- en: Simply asking the compiler to use a maximum of 32 registers (the `-maxregcount=32`
    compiler flag) proves to be a terrible optimization. The compiler then uses just
    18 registers, allowing for six blocks to be scheduled, the maximum permitted.
    This increases the theoretical occupancy to 100%, but results in an increase in
    execution time from 63 ms to 86 ms.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地要求编译器最多使用32个寄存器（`-maxregcount=32`编译器标志）证明是一个糟糕的优化。编译器随后只使用了18个寄存器，允许调度六个块，这是允许的最大值。这将理论上的占用率提高到100%，但导致执行时间从63毫秒增加到86毫秒。
- en: This is due to the GPU having to push registers into “local” storage, which
    on Fermi is the L1 cache and global memory on the earlier-generation GPUs. On
    earlier-generation GPUs the time taken to use global memory would more than eliminate
    any gain due to better occupancy. On Fermi, pushing more data into the L1 cache
    reduces the available cache space for other purposes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由于GPU必须将寄存器推送到“本地”存储中，在Fermi架构上是L1缓存，而在早期的GPU上则是全局内存。在早期的GPU上，使用全局内存的时间将完全抵消由于更好的占用率带来的任何收益。在Fermi上，将更多数据推送到L1缓存中会减少用于其他目的的缓存空间。
- en: 'We can also go down the opposite path, to increase register usage. The original
    C code for the function that performs the sort bins time output shown earlier
    is as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以走相反的路径，增加寄存器的使用量。执行排序箱时间输出的原始C代码如下所示：
- en: '[PRE30]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`   else`'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`   else`'
- en: '[PRE31]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If we look at the PTX code generated for the kernel (see [Chapter 9](CHP009.html)
    for details on how to do this) we see the following code extract:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看为内核生成的PTX代码（有关如何做到这一点的详细信息，请参见[第9章](CHP009.html)），我们会看到以下代码片段：
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This equates to the C source code line for
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于C源代码中的一行：
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: There are a number of issues here. First, array indexing is causing the use
    of a multiply instruction. As `elem` is used immediately in the next C instruction
    to branch, the data load needs to have completed, so the thread stalls at this
    point. Multiply and divide instructions usually require many cycles to complete
    the instruction pipeline and there may be limited execution units that can perform
    such complex instructions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些问题。首先，数组索引导致使用了乘法指令。由于`elem`在下一个C指令中立即用于分支，数据加载需要完成，因此线程在此处停顿。乘法和除法指令通常需要很多周期才能完成指令流水线，而且可能有有限的执行单元来执行这些复杂的指令。
- en: We can replace all array indexes with a pointer to the array and then increment
    the pointer after each usage. Thus, the code extract we looked at earlier becomes
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用指针替换所有的数组索引，然后在每次使用后递增指针。因此，我们之前看到的代码片段变为：
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This means the compiler now translates this to the following PTX code:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着编译器现在将其翻译为以下PTX代码：
- en: '[PRE35]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`mov.s64   %rd14, %rd22;`'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`mov.s64   %rd14, %rd22;`'
- en: We still have a total of six instructions, but now the first set does the load
    and the second set the increment of the pointer. The increment of the pointer
    is now a simple addition, much simpler than a multiply, and the result is not
    needed until the next iteration of the loop.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然有六条指令，但现在第一组指令负责加载数据，第二组负责指针的增量。指针的增量现在是一个简单的加法，比乘法要简单得多，而且结果直到下一次循环迭代才需要。
- en: Applying the same strategy to the other array operations yields a reduction
    in execution time from 39.4 ms to 36.3 ms, a drop of 3 ms or around 10%. However,
    what about this variability in work done by each warp? Where does this come from?
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对其他数组操作应用相同的策略，执行时间从39.4毫秒减少到36.3毫秒，下降了3毫秒，约为10%的减少。然而，每个warp执行工作量的波动从哪里来呢？这是为什么？
- en: Sample sort sorts data into blocks, or bins, which we independently sort using
    a single warp. If we do a dump of the values from single warp, we see something
    interesting.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 样本排序将数据排序到块或桶中，我们使用单个warp独立地对其进行排序。如果我们对单个warp的值进行转储，会看到一些有趣的现象。
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: There are a significant number of bins where the entries are zero. There are
    others where the total number of entries is very large. As one thread processes
    each bin, to iterate over the entire dataset, we need to iterate for the maximum
    of the bins from a given warp. The first warp shown has a maximum value of `0x9d`
    (157 decimals) and a minimum value of zero. By the time we’re at iteration 157,
    only a single thread from the entire warp is active. We see this reflected in
    the large difference between issued and executed instructions we saw earlier (Instructions
    per clock, [Figure 7.7](#F0040)). It’s the bins with very large iteration counts
    that are taking the time.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 有大量的桶中条目为零。还有一些桶中的条目总数非常大。由于每个线程处理每个桶，要遍历整个数据集，我们需要遍历给定warp的最大桶数。第一个warp的最大值为`0x9d`（157十进制），最小值为零。到达第157次迭代时，整个warp中只有一个线程处于活动状态。我们可以在之前看到的发出和执行指令之间的巨大差异中看到这一点（每时钟周期指令数，[图7.7](#F0040)）。正是那些迭代次数非常大的桶占用了时间。
- en: We see a reduction in the execution time of the radix sort when we double the
    number of samples, because the peaks are pushed down and split out into more bins.
    However, sorting the samples then becomes the dominating issue. The problem is
    the distribution of samples to bins.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将样本数量加倍时，我们看到基数排序的执行时间减少了，因为峰值被压低并分配到更多的桶中。然而，排序样本本身成了主导问题。问题出在样本分配到桶中的方式。
- en: The large number of zero bins is actually caused by duplicates in the sample
    dataset. The source data array is filled with data via a simple call to `rand()`,
    which returns a not-so-random number. After a certain period these repeat. As
    the samples are selected at a uniform distance to one another, the sample set
    contains many duplicates. Removing this error in the random dataset removes almost
    all zeros from the bin count, but has an unintended effect that the execution
    time now climbs back up to the original 40 ms.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 大量零桶实际上是由样本数据集中的重复数据引起的。源数据数组通过简单调用`rand()`填充，`rand()`返回的不是完全随机的数字。经过一段时间后，这些数字会重复。由于样本是均匀间隔选择的，因此样本集包含许多重复项。去除随机数据集中的这个错误后，几乎所有零桶都会被移除，但有一个意外效果就是执行时间又回升到了原来的40毫秒。
- en: 'We can, however, apply another technique to this problem, that of loop unrolling
    and tail reduction, both of which we cover in [Chapter 9](CHP009.html). We replace
    the following code segment:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以应用另一种技术来解决这个问题，那就是循环展开和尾部优化，二者我们将在[第9章](CHP009.html)中讨论。我们将替换以下代码段：
- en: '[PRE37]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: with
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用
- en: '[PRE38]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`u32 i=start_idx;`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`u32 i=start_idx;`'
- en: '[PRE39]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Suppose the difference between `start_idx` and `end_idx` is 32, one of the common
    cases. The number of iterations in the first loop will be 32\. However, by unrolling
    the loop by a factor of four, we reduce the number of operations by a factor of
    four, that is, eight iterations. There are a few other important effects of loop
    unrolling. Notice we need, in the case of a factor of four, three additional registers
    to store three additional data points. We also need to handle the end loop condition
    where we may still have zero to three elements to process.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`start_idx`和`end_idx`之间的差值为32，这是常见的情况。第一次循环的迭代次数将是32。然而，通过将循环展开四倍，我们将操作数减少了四倍，也就是八次迭代。循环展开还有一些其他重要的效果。注意，在四倍展开的情况下，我们需要额外的三个寄存器来存储三个额外的数据点。我们还需要处理循环结束条件，其中可能仍然有零到三个元素需要处理。
- en: 'Looking at the PTX code we see:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 查看PTX代码，我们看到：
- en: '[PRE40]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We’re doing something quite important here, introducing instruction level parallelism
    through the use of independent elements per thread. [Table 7.1](#T0010) and [Figure
    7.8](#F0045) show the effect loop unrolling has.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做了一个非常重要的事情，通过每个线程使用独立的元素引入了指令级并行性。[表 7.1](#T0010)和[图 7.8](#F0045)展示了循环展开的效果。
- en: Table 7.1 Unroll Level Versus Time and Register Usage
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.1 展开级别与时间和寄存器使用的关系
- en: '![Image](../images/T000077tabT0010.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000077tabT0010.jpg)'
- en: '![image](../images/F000077f07-08-9780124159334.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-08-9780124159334.jpg)'
- en: FIGURE 7.8 Unroll level versus time and register usage.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 展开级别与时间和寄存器使用的关系。
- en: 'As you can see from [Table 7.1](#T0010) and [Figure 7.8](#F0045), introducing
    a small amount of thread level parallelism significantly drops the execution time
    of the radix sort. However, notice something else: The number of registers never
    climbs above 44, even though we can use up to 63 in Fermi. What is happening at
    this point is the compiler introduces a call stack and no longer grows the number
    of registers used.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从[表 7.1](#T0010)和[图 7.8](#F0045)中可以看到，引入少量线程级并行性显著降低了基数排序的执行时间。然而，还需要注意另一点：即使我们可以在Fermi中使用最多63个寄存器，寄存器的数量始终不会超过44个。此时发生的情况是，编译器引入了调用栈，并且不再增加使用的寄存器数量。
- en: We’ve applied a couple of optimization techniques to the source code, which
    you might reasonably expect a compiler to automatically apply. We’ll not remove
    any of these, so any gain should come from the compiler adding additional optimizations.
    Let’s see if this is the case by switching to the release mode, which enables
    all the compiler optimizations by default ([Table 7.2](#T0015) and [Figure 7.9](#F0050)).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对源代码应用了几种优化技术，这些技术本应由编译器自动应用。我们不会移除这些优化，因此任何提升应该来自于编译器添加的额外优化。让我们通过切换到发布模式来查看是否如此，这样可以默认启用所有编译器优化（[表
    7.2](#T0015) 和 [图 7.9](#F0050)）。
- en: Table 7.2 Debug Versus Release Version Timing
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7.2 调试版与发布版的时间对比
- en: '![Image](../images/T000077tabT0015.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000077tabT0015.jpg)'
- en: '![image](../images/F000077f07-09-9780124159334.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-09-9780124159334.jpg)'
- en: FIGURE 7.9 Debug versus release timing.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 调试版与发布版时间对比。
- en: We see from [Table 7.2](#T0015) and [Figure 7.9](#F0050) a very similar pattern
    to the release or optimized version, indicating that the optimizations we have
    just applied are not themselves applied automatically by the compiler. What is
    also noticable is again we see the same pattern, that four elements per thread
    helps considerably, but beyond this the effect is marginal. Notice, even with
    optimizations enabled, the compiler does not automatically unroll the loop. Thus,
    we’ll stick with manual unrolling by four, as the additional speed versus extra
    register usage is not a good tradeoff.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 从[表 7.2](#T0015)和[图 7.9](#F0050)中我们看到与发布版本或优化版本非常相似的模式，表明我们刚刚应用的优化并没有被编译器自动应用。另一个明显的现象是，我们再次看到相同的模式，每个线程四个元素有很大帮助，但超过这个数量的效果边际递减。请注意，即使启用了优化，编译器也没有自动展开循环。因此，我们将坚持手动四个元素的展开，因为额外的速度与额外的寄存器使用并不是一个好的权衡。
- en: You might have expected the compiler to have pulled, or hoisted, out the read
    operations and placed them at the start of an unrolled loop. In many cases it
    will do this, except in the difficult cases, which are unfortunately all too often
    what we hit. Where you have a read followed by write followed by another read,
    the compiler cannot easily know if the write operation wrote to the same data
    area that is being read from. Thus, it must maintain the read-write-read sequence
    to ensure correctness. As the programmer however, you know if the read operations
    are affected by the preceding write operations and can replace the read-write-read
    sequence with a much more efficient read-read-write sequence.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能期待编译器能够将读操作提取（或提升）并将其放置在展开循环的开始处。在许多情况下，它会这样做，除了在一些困难的情况下，这些情况不幸地往往是我们遇到的情况。当你有一个读操作后跟一个写操作，再跟另一个读操作时，编译器不能轻易判断写操作是否写入了与正在读取的数据区域相同的地方。因此，编译器必须保持读-写-读的顺序以确保正确性。然而，作为程序员，你知道读操作是否受到前面的写操作影响，并且可以将读-写-读顺序替换为一个更高效的读-读-写顺序。
- en: As we’ve now radically changed the timing on one aspect, dropping it from 40
    ms to 25 ms, we should rerun the scan of the problem space to see if this now
    changes the optimum number of samples/threads.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经在一个方面做出了根本性的改变，将时间从40毫秒降到25毫秒，因此我们应该重新扫描问题空间，以查看这是否改变了最佳的样本/线程数量。
- en: 'One thing that becomes noticable is the release version of QSort is actually
    much faster, over twice the speed in fact. This makes it considerably harder to
    produce a faster sort. However, quick sort is now a large component of the sample
    sort, as we presort the samples on the CPU. Thus, this reduction in execution
    time helps considerably. The best timing is as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 一件显而易见的事情是，QSort 的发布版本实际上要快得多，速度是原来的两倍多。这使得生成一个更快的排序算法变得相当困难。然而，快速排序现在是样本排序的重要组成部分，因为我们在
    CPU 上对样本进行预排序。因此，这种执行时间的缩短帮助显著提高了性能。最佳的时间如下：
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Sort Bins Time -    CPU:  62.81  GPU:27.37 25.10 36.28 39.87`'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sort Bins Time -    CPU:  62.81  GPU:27.37 25.10 36.28 39.87`'
- en: '[PRE42]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: So in fact both the 16 K and 32 K sample versions come out above even, with
    0.6 ms between them. This is a 4.4× speedup over the CPU-based quick sort. Cache
    utilization is a key factor in play here. See the “Thread Memory Patterns” section
    in [Chapter 9](CHP009.html) where we look at the impact of this.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 所以实际上，16 K 和 32 K 的样本版本都超过了基准，二者之间相差 0.6 毫秒。这是 CPU 基于快速排序的 4.4 倍加速。缓存利用率在这里起到了关键作用。请参见
    [第 9 章](CHP009.html)中的“线程内存模式”部分，我们在其中分析了这一点的影响。
- en: In summary, we used Parallel Nsight to show the impact of altering the number
    of and size of the blocks we used and saw how this could radically affect the
    overall performance. We then drilled down into this data and noticed there was,
    ultimately, a problem with the design of the sample sort. Serialization caused
    through the differing number of elements processed per thread was the cause of
    this. Despite this issue, we could optimize the implementation through thread
    level parallelism by using multiple elements per thread. Enabling additional compiler
    level optimization brought considerable additional benefits to both CPU and GPU
    code.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们使用 Parallel Nsight 展示了改变使用的块的数量和大小对整体性能的影响，并观察到这可能会极大地影响整体性能。随后我们深入分析了这些数据，发现最终问题出在样本排序的设计上。由于每个线程处理的元素数量不同，导致了序列化问题。尽管有这个问题，我们还是能够通过线程级别的并行性来优化实现，方法是每个线程使用多个元素。启用额外的编译器级优化为
    CPU 和 GPU 代码带来了显著的附加收益。
- en: An Example Using AES
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 AES 的示例
- en: The AES (Advanced Encryption Standard) is an algorithm used to provide encryption
    in programs like WinZip, Bitlocker, TrueCrypt, etc. Depending on your industry,
    encryption may be something you already use or something that may seem irrelevant.
    Many companies make the mistake of thinking the data they create doesn’t need
    to be kept securely on a local machine. All the nasty programs and hackers are
    outside the company firewall and therefore any data kept locally doesn’t need
    security. This type of thinking is flawed, as very often a machine, employee,
    or contractor may create holes in such a firewall to enable working at home or
    outside the office, etc. Security needs to have a multilayered approach.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: AES（高级加密标准）是一种在像 WinZip、Bitlocker、TrueCrypt 等程序中提供加密的算法。根据你的行业，加密可能是你已经在使用的技术，也可能看起来与自己无关。许多公司犯的一个错误是认为它们创建的数据不需要在本地机器上安全存储。所有恶意程序和黑客都在公司防火墙之外，因此任何保存在本地的数据都不需要安全保护。这种思维方式是错误的，因为很常见的情况是，机器、员工或承包商可能通过某些方式在防火墙上打孔，从而实现远程工作或在办公室外工作等需求。安全性需要采取多层次的防护措施。
- en: The idea of encryption is that we take some data and apply an algorithm to it
    that obscures the data. Thus, the data, or the machine holding that data, such
    as a laptop, can be compromised, lost, or stolen, but the data itself is not accessible.
    Significant numbers of data breaches are a result of compromised machines. Moving
    the protection to the data means that to access it requires a “key.” Applying
    that key and a given algorithm results in the data becoming unencrypted.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 加密的思想是，我们对某些数据应用一个算法来使其变得模糊。这样，即使数据或存储数据的机器，如笔记本电脑，可能会被妥协、丢失或被盗，但数据本身是无法访问的。大量的数据泄露事件都是由于机器被攻破所致。将保护转移到数据本身意味着，访问这些数据需要一个“密钥”。应用该密钥和特定的算法后，数据将恢复为明文。
- en: Encryption can also be used for secure connections between hosts on an insecure
    network such as the Internet. If you have a distributed application over a public
    network, how do you ensure that if you send a packet of data to another machine
    that packet is not intercepted and changed? Standards such as OpenSSL (Open Secure
    Socket Layer) are used by browsers when logging into secure servers such as those
    for online banking to ensure no one listens in on the exchange of login data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 加密也可以用于在不安全的网络（如互联网）上进行主机之间的安全连接。如果你有一个分布式应用程序并且运行在公共网络上，如何确保你发送的数据包不会被拦截或篡改？像
    OpenSSL（开放安全套接字层）这样的标准在浏览器登录安全服务器时被使用，比如在线银行，以确保没有人窃听登录数据的交换。
- en: When you design software, you will need to consider the security aspects of
    it and how data is transmitted to and from various machines in any solution. The
    ITEF (Internet Engineering Task Force), the body that approves new Internet standards,
    requires all standard proposals to include a section on security. The fines levied
    against organizations for loss of consumer or corporate data are significant.
    It therefore pays to have a good understanding of at least some encryption standards
    if you are in any way networking computers or storing sensitive or personal data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计软件时，你需要考虑它的安全性，以及数据如何在各种计算机之间传输。批准新互联网标准的组织——互联网工程任务组（ITEF）要求所有标准提案都包含一部分关于安全的内容。对于因消费者或企业数据丢失而被罚款的组织，处罚是相当严重的。因此，如果你涉及计算机网络或存储敏感或个人数据，理解至少一些加密标准是非常有益的。
- en: AES is mandated by many U.S. government organizations when storing data. As
    an algorithm in use today, we’ll use this as a case study to see how you might
    approach AES-based encryption using a GPU. However, before we can dive into the
    implementation details, we first need to analyze the algorithm, understand it,
    and look for elements that can be computed in parallel. The AES algorithm contains
    many complexities, yet at the same time is understandable to someone with no cryptographic
    background. It is therefore a useful algorithm to look at to see how we can apply
    some of the techniques discussed to date.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 许多美国政府机构在存储数据时都要求使用 AES。作为当前使用的算法，我们将以此为案例，探讨如何使用 GPU 实现基于 AES 的加密。然而，在深入实现细节之前，我们首先需要分析该算法，理解它，并寻找可以并行计算的元素。AES
    算法包含许多复杂性，但同时也可以为没有密码学背景的人所理解。因此，它是一个很有用的算法，可以看看如何应用到目前为止讨论的一些技术。
- en: The algorithm
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法
- en: AES is a block-based encryption algorithm. An encryption algorithm is often
    referred to as a *cipher*. Thus, the text to be encoded is referred to as plain
    text when not encoded and cipher text when encoded. To encode plain text into
    cipher text requires an algorithm and a key. The key is simply a series of numbers
    that acts very much like a mechanical key, the algorithm being the lock.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: AES 是一种基于块的加密算法。加密算法通常被称为*密码*。因此，未加密的文本被称为明文，而加密后的文本称为密文。将明文编码为密文需要一个算法和一个密钥。密钥仅仅是一系列数字，像机械钥匙一样起作用，而算法则像锁一样。
- en: AES supports a number of modes of operation, the simplest being ECB (Electronic
    Cook Book), the one we’ll look at here. AES splits up the data to be encoded into
    a number of blocks 128 bits in length (16 bytes). Each block in ECB mode is independently
    encoded based on a series of values derived from the encryption key. The encoding
    takes place in a series of “rounds,” each of which uses a new derived key to further
    encrypt the data. See [Figure 7.10](#F0055).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: AES 支持多种操作模式，其中最简单的是 ECB（电子密码本模式），我们在这里讨论的就是这个模式。AES 将要编码的数据分成多个 128 位（16 字节）长度的块。在
    ECB 模式下，每个块根据从加密密钥派生的一系列值独立进行编码。编码过程分为多个“轮次”，每一轮都使用一个新派生的密钥来进一步加密数据。见[图 7.10](#F0055)。
- en: '![image](../images/F000077f07-10-9780124159334.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-10-9780124159334.jpg)'
- en: FIGURE 7.10 AES overview.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 AES 概述。
- en: The 128-bit key is independently adapted for each round and is independent of
    the text to be encoded or the previous round of encryption. Thus, the extraction
    of the keys for the various rounds can be done independently of the encoding round
    for the AES algorithm. Usually, as the key is constant for all blocks, this will
    be done before any encryption begins.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 128 位密钥为每一轮独立调整，并且与要编码的文本或前一轮加密无关。因此，提取各个轮次的密钥可以独立于 AES 算法的编码轮次进行。通常，由于密钥对于所有块都是恒定的，这个过程会在任何加密开始之前完成。
- en: 'AES uses 128-, 192-, or 256-bit keys, although the block size (the size of
    the plain text) is always 128 bits. The number of rounds used changes according
    to the key length chosen: 10, 12, and 14 rounds, respectively.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: AES 使用 128 位、192 位或 256 位密钥，尽管块大小（明文的大小）始终为 128 位。使用的轮次数根据选择的密钥长度而变化：分别为 10、12
    和 14 轮。
- en: The plain text is represented as a 4 × 4 matrix of byte data, known as the state
    space.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 明文表示为一个 4 × 4 的字节数据矩阵，称为状态空间。
- en: 'An encryption round itself consists of the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一个加密轮次本身包括以下步骤：
- en: • Substitution—Bytes within the 4 × 4 matrix are swapped with other bytes from
    a lookup table.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: • 替代——在 4 × 4 矩阵中的字节与查找表中的其他字节交换。
- en: • Row rotate left—Rows 1, 2, and 3 are rotated left by one, two, or three positions,
    respectively. Row 0 is unchanged.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: • 行向左旋转——第一行、第二行和第三行分别向左旋转一、二或三位，第 0 行不变。
- en: • Mix columns—Each column has a step applied to diffuse the values.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: • 混合列——每列都会应用一个步骤来扩散其值。
- en: • Round key—The data is XOR’d with the appropriate current round key extracted
    from the original key.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: • 轮密钥——数据与从原始密钥中提取出的当前轮密钥进行异或操作。
- en: The initial round, also known as round zero, consists only of the round key
    operation. The final round drops the mix columns operation. Decryption is simply
    the inverse of the encryption process, starting at the last round and working
    backwards to the start.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 初始轮次，也称为零轮，仅包括轮密钥操作。最终轮次则省略了混合列操作。解密过程仅是加密过程的逆过程，从最后一轮开始，逆向执行直到开始。
- en: Thus, to implement the algorithm, we need to look in detail at the five key
    aspects, those just shown plus the extraction of the round keys from the original
    128 bit key.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了实现该算法，我们需要详细查看五个关键方面，这些方面包括刚才所示的内容以及从原始 128 位密钥中提取轮密钥的过程。
- en: Substitution
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 替代
- en: The substitution step swaps every byte in the 4 × 4 data block, the state space,
    with a value from a constant lookup table known as the Rijndael s-box.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 替代步骤将 4 × 4 数据块中的每个字节，即状态空间中的字节，与来自常量查找表的值进行交换，这个查找表称为 Rijndael s-box。
- en: '[PRE43]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '` 0xE7, 0xC8, 0x37, 0x6D, 0x8D, 0xD5, 0x4E, 0xA9, 0x6C, 0x56, 0xF4, 0xEA, 0x65,
    0x7A, 0xAE, 0x08, /∗ B ∗/`'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '` 0xE7, 0xC8, 0x37, 0x6D, 0x8D, 0xD5, 0x4E, 0xA9, 0x6C, 0x56, 0xF4, 0xEA, 0x65,
    0x7A, 0xAE, 0x08, /∗ B ∗/`'
- en: '[PRE44]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: For each of the 16-byte elements in the state space we have to extract out a
    single hex digit. The first digit, or high nibble of the byte, (0…F), is used
    as row reference. The second digit of the byte, or low nibble, is used as the
    column index. Thus, a value of `0x3E` in the state space would result in a row
    value of 3 and a column value of E. If we look up this in the `s_box` table, we
    get `0xB2`. Thus, the byte `0x3E` in the state space is replaced by `0xB2`. The
    same operation is performed for all the other bytes in the state space.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 对于状态空间中的每一个 16 字节元素，我们需要提取出一个十六进制数字。字节的第一个数字，或者高 nibble（0…F），用作行索引。字节的第二个数字，或者低
    nibble，用作列索引。因此，状态空间中的 `0x3E` 会得到行值为 3，列值为 E。如果我们在 `s_box` 表中查找这个值，我们得到 `0xB2`。因此，状态空间中的字节
    `0x3E` 被替换为 `0xB2`。对状态空间中的其他字节执行相同的操作。
- en: Row rotate left
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 行向左旋转
- en: In this step, rows 1, 2, and 3 are rotated left by one, two, or three positions,
    respectively. Row 0 is left unchanged. A rotate left operation takes the row and
    shuffles all bytes to the left by one position. The byte at the far left wraps
    around and becomes the byte on the far right. In [Figure 7.11](#F0060) I’ve pulled
    out each row to show how the rotation of the bytes works.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在此步骤中，第一行、第二行和第三行分别向左旋转一、二或三位。第 0 行保持不变。左旋操作将行中的所有字节向左移动一位，最左侧的字节会绕回并变成最右侧的字节。在[图
    7.11](#F0060)中，我提取了每一行以展示字节旋转的过程。
- en: '![image](../images/F000077f07-11-9780124159334.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-11-9780124159334.jpg)'
- en: FIGURE 7.11 AES row rotate left.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 AES 行向左旋转。
- en: Mix columns
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混合列
- en: The Rijndael mix column step is a complex piece of code. It multiples the column
    `r` by a 4 × 4 matrix. The matrix is shown in [Figure 7.12](#F0065).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Rijndael 混合列步骤是一个复杂的代码部分。它将列 `r` 与一个 4 × 4 的矩阵相乘。矩阵见于[图 7.12](#F0065)。
- en: '![image](../images/F000077f07-12-9780124159334.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-12-9780124159334.jpg)'
- en: FIGURE 7.12 Mix columns matrix.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 混合列矩阵。
- en: A 1 in the matrix means leave the value unchanged. A 2 indicates multiplication
    by 2\. A 3 indicates a multiplication by 2 plus an XOR with the original value.
    In the 3 case, should the resultant value be larger than `0xFF`, then an additional
    XOR with `0x1B` needs to be performed. This is a simplification of Galois multiplication.
    A typical implementation in C code is shown here ([Wikipedia, Jan. 31, 2012](#BIB1)).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵中的1表示保持值不变。2表示乘以2。3表示先乘以2再与原始值进行异或运算。如果结果值大于`0xFF`，则需要进行额外的与`0x1B`的异或操作。这是伽罗瓦乘法的简化形式。一个典型的C语言实现示例如下（见[维基百科，2012年1月31日](#BIB1)）。
- en: '[PRE45]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This is not the most optimal, but the most likely example implementation you
    will find of this standard algorithm. In the preceding code, the input parameter
    `r` points to a 1 × 4 matrix that is a single column from the state space. It
    is copied to a temporary array `a` for use later. An array `b` is generated that
    holds the multiply by 2 (the `<<1`) operation. The multiply by 3 is actually a
    multiply by 2 followed by an XOR (`^`) operation. Thus, the final step becomes
    a series of XOR operations of the original data in `a` plus the result of the
    matrix multiplication in `b`. See [Figure 7.13](#F0070).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是最优化的实现，但它是你最可能找到的标准算法的示例实现。在前面的代码中，输入参数`r`指向一个1×4矩阵，这是状态空间中的一个列。它被复制到临时数组`a`中，以便稍后使用。然后生成一个数组`b`，保存乘以2（`<<1`）的操作。乘以3实际上是先乘以2再进行异或（`^`）操作。因此，最后一步变成了对原始数据`a`中的元素以及矩阵乘法结果`b`的异或操作。请参见[图
    7.13](#F0070)。
- en: '![image](../images/F000077f07-13-9780124159334.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-13-9780124159334.jpg)'
- en: FIGURE 7.13 `Mix columns` with column 0 (repeated for columns 1, 2, and 3).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 `混合列`与列0（对列1、2和3进行相同操作）。
- en: We’ll look a little more at this step later, as it’s one of the more time-consuming
    elements.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后我们会更详细地看一下这一步，因为它是比较耗时的步骤之一。
- en: Add round key
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 添加轮密钥
- en: The round key is the key extracted from the original cipher key for a given
    round or iteration of the encryption algorithm. It’s in the form of a 4 × 4 matrix
    and is simply XOR’d with the current result.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 轮密钥是从原始密钥中提取的，用于加密算法的给定轮次或迭代。它的形式是一个4×4矩阵，并与当前结果进行异或操作。
- en: Extracting the round keys
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提取轮密钥
- en: The AES algorithm uses a number of round keys, one for each round. Generating
    the keys is an iterative process where new keys depend on previous ones. The first
    part of the operation is to take the existing key and copy it as key 0, thus generating
    a 4 × 4 matrix providing the single starting round key.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: AES算法使用多个轮密钥，每个轮次都有一个密钥。密钥生成是一个迭代过程，新密钥依赖于前一个密钥。操作的第一部分是获取现有密钥并将其复制为密钥0，从而生成一个4×4矩阵，提供单个起始轮密钥。
- en: The next *N* round keys must be constructed one at a time. The first column
    of any round key takes the last column of the previous round key as its starting
    point. The operation for the first column in the new key contains some addition
    operations over and above the standard column generation function.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的*N*个轮密钥必须一次一个地构建。任何轮密钥的第一列都以上一轮密钥的最后一列作为起点。新密钥中第一列的操作包含了一些超出标准列生成函数的加法操作。
- en: For the first column of the key only, we need to do a column-based rotate such
    that the values move up the column. The value at the top of the column, row 0,
    moves to row 3\. An identical operation is to rotate the row left on the cipher
    data, but instead of a row, the rotate is over a column. We then again use the
    substitution method and the Rijndael s-box to substitute values as we did for
    the cipher text.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于密钥的第一列，我们需要进行基于列的旋转，使得值向上移动。列顶端（第0行）的值会移动到第3行。一个相同的操作是对密文数据进行行旋转，不过这次旋转是在列上进行的。然后我们再次使用替代方法和Rijndael
    S盒对值进行替代，就像我们处理密文时做的那样。
- en: The operation for all elements is then the same. The newly calculated value
    must be XOR’d with the key value at index minus 4\. For columns 1, 2, and 3 we’re
    now done. However, column 0 has an addition operation. The first element of column
    zero is then XOR’d with `0x01`, `0x02`, `0x04`, `0x08`, `0x10`, `0x20`, `0x40`,
    `0x80`, `0x1b`, or `0x36`, the RCON value, depending on the current round ([Figure
    7.14](#F0075)).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 所有元素的操作方式都是相同的。新计算的值必须与索引减去4的密钥值进行异或操作。对于列1、2和3，我们现在已经完成。然而，列0需要进行加法操作。列零的第一个元素会与`0x01`、`0x02`、`0x04`、`0x08`、`0x10`、`0x20`、`0x40`、`0x80`、`0x1b`或`0x36`，即RCON值，根据当前的轮次进行异或运算（见[图
    7.14](#F0075)）。
- en: '![image](../images/F000077f07-14-9780124159334.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-14-9780124159334.jpg)'
- en: FIGURE 7.14 AES round key generation (first column).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 AES轮密钥生成（第一列）。
- en: Thus, the first column of round key 1 becomes the next extracted column. The
    calculation of columns 1, 2, and 3 is simpler ([Figure 7.15](#F0080)). The column
    rotation and XOR with the RCON values is dropped. Thus, we simply have an XOR
    with the column at row minus 4\. At column 4, the pattern repeats.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，轮密钥1的第一列成为下一个提取的列。列1、2和3的计算更简单（见[图 7.15](#F0080)）。列旋转和与RCON值的异或操作被省略。因此，我们只需对比“行-4”的列进行异或操作。在第4列，模式重复。
- en: '![image](../images/F000077f07-15-9780124159334.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-15-9780124159334.jpg)'
- en: FIGURE 7.15 AES round key generation (columns 1, 2, and 3).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15 AES轮密钥生成（列1、2和3）。
- en: As the key generation always uses values from the previous key, this means the
    keys need to be generated in sequence. This in turn may form the bottleneck of
    any parallel implementation if many keys are needed. Thankfully for most uses,
    only a single set of keys is required. Thus, this step can be performed prior
    to any encoding or decoding and the keys simply stored in an array. As it’s not
    time consuming for a single key, it can be done on either the CPU or GPU.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 由于密钥生成始终使用前一个密钥的值，这意味着密钥需要按顺序生成。如果需要多个密钥，这可能会成为任何并行实现的瓶颈。幸运的是，对于大多数用途，只需要一组密钥。因此，这一步可以在任何编码或解码之前执行，密钥只需存储在数组中。由于单个密钥的生成不耗时，因此可以在CPU或GPU上完成。
- en: Serial implementations of AES
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AES的串行实现
- en: AES has been the subject of a lot of study. It was designed to run on 8-, 16-,
    or 32-bit machines without significant processing load. However, as we have seen
    from looking at the algorithm, it’s not a simple algorithm to implement. Let’s
    consider some of the design tradeoffs when thinking about optimizing such an algorithm
    for a GPU.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: AES已经成为大量研究的主题。它的设计旨在在8位、16位或32位的机器上运行，而不会造成显著的处理负担。然而，正如我们从查看算法中所见，这并不是一个简单的实现算法。让我们考虑一些设计权衡，以便在为GPU优化此类算法时思考。
- en: Access size
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 访问大小
- en: The first issue is that it is designed around byte-based access, to support
    8-bit simple processors. All modern processors are at least 32-bit designs. Thus,
    if we use just single byte operations, 75% of the space in the register and the
    potential work goes unused. Clearly with a 32-bit processor, an x86 or a Fermi
    GPU, we need to design a solution such that it uses 32 bits.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题是它围绕字节访问设计，以支持8位简单处理器。所有现代处理器至少是32位设计。因此，如果我们仅使用单字节操作，那么寄存器中的75%空间和潜在的工作将未被利用。显然，对于32位处理器、x86或Fermi
    GPU，我们需要设计一种解决方案，使其使用32位。
- en: We can naturally combine a single row into one 32-bit word. We can also combine
    the entire 4 × 4 matrix into a 16-byte vector (128 bits). Such vectors are supported
    by the Intel AVX (Advanced Vector eXtension) instruction set. The GPU `uint4`
    type would also allow for the GPU to fetch and store this data to/from memory
    in a single instruction. However, unlike Intel’s AVX, the GPU has no per thread
    wide vector instructions other than storing to or from memory.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以自然地将单行合并为一个32位字。我们还可以将整个4 × 4矩阵合并为一个16字节的向量（128位）。英特尔的AVX（高级向量扩展）指令集支持这种向量。GPU的`uint4`类型也允许GPU在单个指令中从内存读取和存储此数据。然而，与英特尔的AVX不同，GPU没有每线程宽向量指令，除了存取内存之外。
- en: We have to consider that any encoding of the state or key matrix that is larger
    than a single byte would necessitate bit mask and shift operations if the operation
    needed to be individually applied to a single byte. Providing these were not considerable,
    the benefit of less memory reads/writes, though fetching the data in larger transactions,
    would easily outweigh register-based mask and shift operations.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须考虑到，任何大于单个字节的状态或密钥矩阵编码，如果操作需要单独应用于单个字节，将需要进行位掩码和移位操作。只要这些操作的开销不大，那么尽管在更大的事务中提取数据，但减少内存读取/写入的好处很容易超过基于寄存器的掩码和移位操作。
- en: Memory versus operations tradeoff
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 内存与操作的权衡
- en: With most algorithms it’s possible to trade an increased memory footprint for
    a decreased execution time. It depends significantly on the speed of memory versus
    the cost and number of arithmetic instructions being traded.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数算法来说，可以通过增加内存占用来减少执行时间。这主要取决于内存的速度与被交换的算术指令的成本和数量。
- en: There are implementations of AES that simply expand the operations of the substitution,
    shift rows left, and mix columns operation to a series of lookups. With a 32-bit
    processor, this apparently requires a 4 K constant table and a small number of
    lookup and bitwise operations. Providing the 4 K lookup table remains in the cache,
    the execution time is greatly reduced using such a method on most processors.
    We will, however, implement at least initially the full algorithm before we look
    to this type of optimization.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 有些AES实现通过将替代、左移行和混合列操作展开为一系列查找来简化操作。对于32位处理器，这显然需要一个4K的常量表以及少量的查找和按位操作。如果4K查找表保持在缓存中，使用这种方法在大多数处理器上可以大幅减少执行时间。然而，在我们考虑这种优化类型之前，我们至少会首先实现完整的算法。
- en: Hardware acceleration
  id: totrans-280
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 硬件加速
- en: 'The Intel AES-NI extension to the x86 processor instruction set is available
    on most Intel Sandybridge I5 and I7 processors as well as the Westmere-based I7
    Xeon processors and their successors. The AES-NI instruction set consists of the
    following instructions:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔的AES-NI扩展指令集在大多数英特尔Sandybridge I5和I7处理器，以及基于Westmere的I7 Xeon处理器及其后继产品中可用。AES-NI指令集包含以下指令：
- en: • AESENC (cipher data, round key)—Standard round of encoding completely in hardware.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: • AESENC（密码数据，轮密钥）—硬件中完全执行的标准加密轮次。
- en: • AESENCLAST (cipher data, round key)—Last round of encoding completely in hardware.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: • AESENCLAST（密码数据，轮密钥）—硬件中完全执行的最后一轮加密。
- en: • AESKEYGENASSIST (round key, cipher key, round number)—Assist in the generation
    of the round keys.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: • AESKEYGENASSIST（轮密钥，密码密钥，轮次）—辅助生成轮密钥。
- en: • ASDEC (cipher data, round key)—Standard round of decryption in hardware.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: • ASDEC（密码数据，轮密钥）—硬件中的标准解密轮次。
- en: • ASDECLAST (cipher data, round key)—Last round of decryption in hardware.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: • ASDECLAST（密码数据，轮密钥）—硬件中的最后一轮解密。
- en: 'Thus, the entire AES encryption and decryption process can be done entirely
    in hardware. Special 128-bit xmm1 and xmm2 registers are used to contain the operands
    in single registers. We see that in practice when such AES-NI is used with real
    applications, there is something in the order of a 2× or more performance improvement
    over a nonaccelerated processor (Toms Hardware, “AES-NI Benchmark Results: Bitlocker,
    Everest, And WinZip 14,” [http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption,review-31801-7.html](http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption,review-31801-7.html)).
    Of course with handwritten assembler and optimal scheduling conditions over many
    cores, it’s possible to get significantly more. This, however, gives us a feel
    for the likely benefit of coding such a solution and therefore it seems worth
    the effort.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，整个AES加密和解密过程可以完全在硬件中完成。特殊的128位xmm1和xmm2寄存器用于在单个寄存器中容纳操作数。当实际应用中使用AES-NI时，我们可以看到性能提高大约2倍或更多（Toms
    Hardware，“AES-NI基准测试结果：Bitlocker、Everest和WinZip 14，”[http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption,review-31801-7.html](http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption,review-31801-7.html)）。当然，使用手写汇编并在多个核心上进行最佳调度条件时，可能获得更多的提升。然而，这给我们提供了一个对编码此类解决方案的预期效益的感觉，因此看起来值得付出努力。
- en: An initial kernel
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始内核
- en: Let’s look at an initial kernel for this algorithm.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这个算法的初始内核。
- en: '[PRE46]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: First, we have the function prototype. Here we pass in a pointer to a cipher
    block as a `uint4` vector type. A single `uint4` vector (four integers) is sufficient
    to hold a single set of 16 bytes, the 128-bit cipher data. Next we have the cipher
    key, which is a set of 10 `uint4` keys. Finally, we have a specifier for the number
    of rounds, which we will replace with a fixed value at some point later. Note
    both the `__host__` and `__device__` qualifiers that allow the function to be
    called from both the CPU and GPU.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有函数原型。在这里，我们传递一个指向密码块的指针，该指针是一个`uint4`向量类型。单个`uint4`向量（四个整数）足以容纳一组16字节的128位密码数据。接下来是密码密钥，它是一组10个`uint4`密钥。最后，我们有一个轮次的说明符，稍后我们会用固定值替换它。注意`__host__`和`__device__`限定符，它们允许从CPU和GPU调用该函数。
- en: '[PRE47]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Next we extract from the `uint4` vector type the four unsigned integer component
    parts.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从`uint4`向量类型中提取四个无符号整数组件部分。
- en: '[PRE48]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We next extract individual bytes from the four words into individual registers.
    Note the use of the `u8` type rather than the base C type, allowing an easy redefinition
    of this type. Note also the `EXTRACT` macro, which is used to allow support for
    both big-endian and little-endian representation of the bytes within the 32-bit
    words.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们从四个字中提取单独的字节到独立的寄存器中。注意使用了`u8`类型，而不是基础C类型，这使得重新定义此类型变得更容易。还要注意`EXTRACT`宏，它用于支持32位字内字节的大小端表示。
- en: '[PRE49]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '` w0 = (∗cipher_key)[round_num].w;`'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '` w0 = (∗cipher_key)[round_num].w;`'
- en: '[PRE50]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We then read a set of four values from the key, again from a `uint4` type into
    four 32-bit values.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们从密钥中读取一组四个值，再次从`uint4`类型读取到四个32位值中。
- en: '[PRE51]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The first round of the key encoding simply uses an XOR operation on the values
    in the columns.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 密钥编码的第一轮仅使用XOR操作对列中的值进行处理。
- en: '[PRE52]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`   a9 =  s_box_ptr[a9];`'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`   a9 =  s_box_ptr[a9];`'
- en: '[PRE53]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We then enter the main loop of the kernel. We run for `num_rounds` of iterations.
    As we later need the key and the key is to be fetched from memory, we initiate
    the read from memory as early as possible. Next we have the substitution step,
    which simply replaces the existing values with new ones from the `s_box` array
    shown earlier.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 然后进入内核的主循环。我们将执行`num_rounds`次迭代。由于后续需要使用密钥，并且密钥需要从内存中获取，因此我们尽早发起内存读取操作。接下来是替换步骤，它简单地用`s_box`数组中的新值替换现有的值。
- en: '[PRE54]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The next step is to rotate rows 1, 2, and 3\. As we have stored one byte per
    register, we cannot simply do a 32-bit rotate. As there is no native support in
    the GPU instruction set for such an operation, this is of little real relevance.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是旋转第1、2和3行。由于我们每个寄存器存储了一个字节，不能简单地进行32位旋转操作。由于GPU指令集并不原生支持这种操作，因此这对实际操作影响不大。
- en: '[PRE55]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '`   a9 = tmp9;`'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`   a9 = tmp9;`'
- en: '[PRE56]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The next step is to mix the columns operation, which is done in every round
    except the last one. The previous mix column code shown earlier has had the `c`
    loop unrolled to form the `MIX_COL` macro. Additionally, to control the order
    of the XOR, we implement an `XOR_5`, which is a five-input XOR macro.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是混合列操作，这在每一轮中都会进行，除了最后一轮。之前显示的混合列代码中的`c`循环已被展开，形成了`MIX_COL`宏。此外，为了控制XOR的顺序，我们实现了一个`XOR_5`，它是一个五输入XOR宏。
- en: '[PRE57]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We then implement the XOR operation with the key fetched at the start of the
    loop.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用在循环开始时获取的密钥执行XOR操作。
- en: '[PRE58]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Finally, the resultant key is combined into a 32-bit value and written back
    to the `uint4` cipher word. At this point we’ve completed all 10 rounds and the
    cipher block is encoded based on the set of 10 round keys.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，结果密钥被合并成一个32位值，并写回`uint4`密文字中。此时我们已经完成了所有10轮，且基于这10轮密钥集合对密文块进行了编码。
- en: 'For completeness purposes, the macros used are defined as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，所用的宏定义如下：
- en: '[PRE59]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '`#define EXTRACT_D1(x) ( ( (x) >> 16uL ) & 0xFFuL )`'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '`#define EXTRACT_D1(x) ( ( (x) >> 16uL ) & 0xFFuL )`'
- en: '[PRE60]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Kernel performance
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内核性能
- en: So how does such a kernel perform? How do we measure, understand, and predict
    performance? Initially, looking at the disassembled code for a compute 2.x target,
    we see something you might not expect. Declaring the registers as unsigned 8 bits
    results in sections of code to shift and mask data. The extract data macros are
    deliberately written to mask off the bits that are not used, so this is entirely
    unnecessary. In fact, we generate around four times the amount of code if we use
    a `u8` type instead of a `u32` type.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这个内核的性能如何呢？我们如何衡量、理解和预测性能？最初，看一下针对计算2.x目标的反汇编代码，我们看到了一些你可能不期望的东西。将寄存器声明为无符号8位会导致代码中出现移位和掩码数据的部分。提取数据宏被故意写成掩码掉不需要的位，因此这完全没有必要。实际上，如果我们使用`u8`类型而不是`u32`类型，我们会生成大约四倍数量的代码。
- en: Changing the `u8` definition to a `u32` definition means we *potentially* waste
    a lot of register space, but it eliminates huge numbers of instructions. In practice,
    the GPU implements `u8` registers as `u32` registers, so this doesn’t actually
    cost us anything in terms of register space.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 将`u8`定义更改为`u32`定义意味着我们*可能*浪费了大量寄存器空间，但它消除了大量指令。实际上，GPU将`u8`寄存器实现为`u32`寄存器，因此这在寄存器空间方面并不会对我们造成任何额外成本。
- en: Next we come to the number of registers used. Our initial kernel uses 43 registers,
    which is not altogether too surprising but is somewhat disappointing. If you load
    up the CUDA Occupancy Calculator, found in the “Tools” directory of the SDK, we
    can see that 43 registers will limit us to just a single block per SM of no more
    than 320 threads. This is just 10 active warps and nowhere near the maximum (24
    on compute 1.3 devices, 48 on compute 2.x devices, 64 on compute 3.× devices).
    We need to have more blocks than this, so there is a greater mix of instructions
    for the warp scheduler to select from. There are limits on, for example, the number
    of XOR operations an SM can perform (see [Chapter 9](CHP009.html)) and 10 warps
    will not hide the memory latency.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是使用的寄存器数量。我们的初始内核使用了 43 个寄存器，这并不算太令人惊讶，但多少有些令人失望。如果你打开 CUDA 占用率计算器（位于 SDK
    的“工具”目录下），你会看到 43 个寄存器将使我们每个 SM 只能有一个块，且最多 320 个线程。这仅仅是 10 个活跃的 warp，远远低于最大值（在计算
    1.3 设备上为 24，在计算 2.x 设备上为 48，在计算 3.x 设备上为 64）。我们需要更多的块，以便 warp 调度器能从中选择更多指令进行调度。例如，SM
    能执行的 XOR 操作数量是有限制的（见 [第 9 章](CHP009.html)），而 10 个 warp 也无法隐藏内存延迟。
- en: Thus, to achieve the best throughput, we don’t want to execute just a series
    of the same instructions one after another. By having more than one block per
    SM there is a good probability that while one block is performing the XOR section,
    another block may be doing the `s_box` substitution operation. This involves a
    number of address calculations and memory lookups. We need to somehow decrease
    the register count.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了获得最佳吞吐量，我们不希望只是连续执行一系列相同的指令。通过让每个 SM 上有多个块，可以提高概率，当一个块正在执行 XOR 部分时，另一个块可能正在进行
    `s_box` 替代操作。这个操作涉及多个地址计算和内存查找。我们需要以某种方式减少寄存器的使用。
- en: The compiler provides a switch for this. How does this perform? We’ll call the
    function with 16 blocks of 256 threads. Thus, we should see the improvement as
    and when we can schedule more blocks per SM. We’ll run this test on a NVIDIA ION
    (compute 1.2)–based laptop, which has two SMs.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器为此提供了一个开关。效果如何？我们将使用 16 块每块 256 线程的方式来调用该函数。因此，当我们可以每个 SM 调度更多的块时，应该会看到改进。我们将在一台基于
    NVIDIA ION（计算 1.2）的笔记本电脑上运行该测试，该笔记本有两个 SM。
- en: '[PRE61]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '` const int idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;`'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '` const int idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;`'
- en: '[PRE62]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: As our `encrypt` function is a device function, we need a `global` function
    to call it. The `global` function extracts the appropriate block of cipher data
    and uses the same cipher key for all blocks. This represents what most encoding
    algorithms would do.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的`encrypt`函数是设备函数，我们需要一个`global`函数来调用它。`global`函数提取适当的密文数据块，并为所有数据块使用相同的密钥。这代表了大多数编码算法的做法。
- en: We see that for the original case, we get 6.91 ms to encode 512 keys simultaneously
    (two blocks of 256 threads each, one block per SM). Forcing the compiler to use
    just 32 registers should result in two blocks per SM, four blocks in total. Selecting
    24 registers will result in three blocks per SM, six blocks in total. Indeed,
    we see a drop to 4.74 ms when using 32 registers, a huge improvement. However,
    when we try 24 registers, this time increases to 5.24 ms. Why is this?
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在原始情况下，我们需要 6.91 毫秒来同时编码 512 个密钥（两个 256 线程的块，每个 SM 一个块）。强制编译器仅使用 32 个寄存器应该导致每个
    SM 两个块，总共四个块。选择 24 个寄存器将导致每个 SM 三个块，总共六个块。实际上，当使用 32 个寄存器时，我们看到时间降至 4.74 毫秒，改进非常显著。然而，当我们尝试使用
    24 个寄存器时，时间却增加到了 5.24 毫秒。为什么会这样呢？
- en: Asking the compiler to use less registers does not cause them to magically disappear.
    The compiler has a number of strategies it can use. First, it can reload registers
    from memory. This may sound a bit counterintuitive, as we know global memory is
    very slow compared to registers. However, the additional block may bring in another
    set of warps that in turn may hide this memory latency. In the case of moving
    from 256 threads (1 block, 8 warps) to 512 threads (2 blocks, 16 warps), we gain
    significantly in terms of instruction mix and number of potential warps schedulable
    per SM.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 要求编译器使用更少的寄存器并不会让它们神奇地消失。编译器有很多可以使用的策略。首先，它可以从内存中重新加载寄存器。这可能听起来有些违反直觉，因为我们知道全局内存比寄存器慢得多。然而，额外的块可能会引入另一组
    warp，进而可能隐藏内存延迟。在从 256 线程（1 块，8 warp）增加到 512 线程（2 块，16 warp）的情况下，我们在指令组合和每个 SM
    可调度的 warp 数量上有显著提高。
- en: 'The second strategy is to move registers into other memory types: shared, constant,
    or local memory. If you use the `-v` option during compilation, the compiler tells
    you what amount of each memory type it is using. Shared memory is slower than
    registers. Constant memory is cached, but again slower than registers. Local memory
    is the L1 cache on Fermi (compute 2.x) and global memory on compute 1.x devices.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种策略是将寄存器移动到其他类型的内存中：共享内存、常量内存或局部内存。如果在编译时使用`-v`选项，编译器会告诉你每种内存类型使用的数量。共享内存比寄存器慢。常量内存是缓存的，但仍然比寄存器慢。局部内存是Fermi（计算2.x）的L1缓存，而在计算1.x设备上则是全局内存。
- en: Finally, the compiler can reuse registers if it can correctly identify the scope
    and usage of the registers within a section.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，编译器可以重用寄存器，只要它能够正确识别寄存器在某个部分的作用范围和使用情况。
- en: As we push the compiler to use ever fewer registers it eventually spills the
    registers into local memory. Although not too bad on Fermi, performance on our
    compute 1.2 test platform is thus terrible as, in fact, we’re then using global
    memory. The additional gain of a third block is just simply not enough to overcome
    this rather huge penalty. Thus, we see the kernel slow down instead of speed up.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们迫使编译器使用越来越少的寄存器时，它最终会将寄存器溢出到局部内存中。虽然在Fermi上这种情况还不算太差，但在我们的计算1.2测试平台上，性能就非常差，因为实际上我们正在使用全局内存。第三个块带来的额外收益不足以克服这种巨大的性能损失。因此，我们看到内核的运行速度变慢了，而不是加速。
- en: We achieved a 30% execution time reduction simply by setting a compiler switch,
    which is pretty impressive for five minutes of work. However, can we do any better
    by rewriting the C code? What is making the regular compilation take a massive
    43 registers? What can we do to reduce this?
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过设置一个编译器选项，成功实现了30%的执行时间减少，这对于五分钟的工作来说相当令人印象深刻。然而，我们能通过重写C代码做得更好吗？是什么导致常规编译需要使用大量43个寄存器？我们可以做些什么来减少这一点？
- en: Taking the existing code, we can comment out certain sections. This tells us
    easily what *additional* registers that code section requires. Thus, we start
    by localizing all registers to individual blocks. We can create a new scope level
    in C by simply placing braces (the {} symbols) around a block of code. This should
    allow the scope of a variable or constant to be identified and localized to within
    a section.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 通过修改现有代码，我们可以注释掉某些部分。这可以轻松告诉我们该代码部分需要哪些*额外*的寄存器。因此，我们首先将所有寄存器本地化到各个块中。通过简单地在代码块周围加上大括号（{}符号），我们可以在C语言中创建一个新的作用域级别。这应该能使变量或常量的作用域被识别，并限定在某个部分内。
- en: 'It turns out the most expensive part of the code is the mix columns section.
    Looking at the code it’s not too surprising. We calculate 16 `b<n>` values based
    on the 16 `a<n>` values, plus an additional 16 `tmp<n>` values. However, these
    are really just sets of four column parameters. The compiler should, when building
    the dependency tree, see it and rearrange the order of execution. Thus, instead
    of 32 additional registers, it needs only 8\. However, it does not do this reordering,
    perhaps because it simply does not model such a large number of parameters efficiently.
    Whatever the cause, it’s using far more registers than it needs to. We can therefore
    rewrite the mix column section:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，代码中最昂贵的部分是混合列（mix columns）部分。看代码其实也不难理解。我们基于16个`a<n>`值计算16个`b<n>`值，并加上另外16个`tmp<n>`值。然而，这些值其实只是四个列参数的集合。当编译器构建依赖关系树时，它应该能够看到这些并重新安排执行顺序。因此，它只需要8个寄存器，而不是32个额外的寄存器。然而，它并没有进行这种重排，可能是因为它根本无法高效地建模这么多参数。不管原因是什么，它使用的寄存器比需要的要多得多。因此，我们可以重写混合列部分：
- en: '[PRE63]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: For simplicity, only the operation on a single column is shown here. This, however,
    moves the usage of the variables closer to the setting of the variable or constant.
    This improves the instruction mix and also reduces the scope of where a variable
    or constant needs to exist. In effect, we make it easier for the compiler to identify
    and reuse these registers.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，这里只展示了对单列的操作。然而，这样做将变量的使用更接近于变量或常量的设定位置。这改善了指令混合，同时减少了变量或常量存在的作用域。实际上，我们使得编译器更容易识别和重用这些寄存器。
- en: 'We also change the reading of the key values. Previously we’d calculate the
    address calculation for each access:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还改变了密钥值的读取方式。之前我们会为每次访问计算地址：
- en: '[PRE64]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Here the `cipher_key` pointer is being dereferenced, then indexed by `round_num`,
    with a zero-byte offset for the structure member `w`. This calculation would normally
    be made once and the offset part (`w`, `x`, `y`, or `z`) would then be added.
    To avoid creating a dependency on the next instruction the compiler actually repeats
    this instruction four times, once for each `w<n>` value. As the instruction latency
    is on the order of 20 cycles, this approach produces four answers in quick succession.
    However, it uses more registers than performing the calculation once and then
    adding the offset. As more blocks will bring us significantly more warps that
    in turn hide more latency, this is a good tradeoff. Thus, we replace this section
    of code with a new one:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`cipher_key`指针被解引用，然后通过`round_num`索引，并使用零字节偏移量来访问结构成员`w`。通常，这个计算会只执行一次，偏移量部分（`w`、`x`、`y`或`z`）会随后被加上。为了避免创建对下一条指令的依赖，编译器实际上将此指令重复执行四次，每次处理不同的`w<n>`值。由于指令的延迟大约为20个周期，这种方法能够快速地产生四个答案。然而，这比一次性计算并添加偏移量使用更多的寄存器。由于更多的块会带来显著更多的warps，从而隐藏更多的延迟，这是一种不错的折衷。因此，我们用新的代码段替换了这一部分：
- en: '[PRE65]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Here we introduce a new pointer parameter that performs the base address calculation
    once. Accessing the members `w`, `x`, `y`, or `z` through the pointer just requires
    a simple addition of literal 0, 4, 8, or 12 to the base address when the compiler
    calculates the address offsets.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们引入了一个新的指针参数，该参数仅执行一次基地址计算。通过指针访问成员`w`、`x`、`y`或`z`时，编译器计算地址偏移量时只需要简单地将字面值0、4、8或12加到基地址上。
- en: Note we also tried simply reading the `uint4` key into a `uint4` local constant.
    Unfortunately, this resulted in the compiler placing the `uint4` constant into
    local memory (`lmem`), which is exactly what we do not want, and perhaps something
    later versions of the compiler may resolve. The LLVM compiler (CUDA 4.1) seems
    to prefer to place vector types into local memory rather than registers.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还尝试将`uint4`键直接读取到`uint4`本地常量中。不幸的是，这导致编译器将`uint4`常量放入本地内存（`lmem`），这是我们不希望发生的，或许编译器的后续版本可以解决这个问题。LLVM编译器（CUDA
    4.1）似乎更倾向于将向量类型放入本地内存，而不是寄存器。
- en: Finally, we moved the definition of `round_num` from the start of the function
    to just before the `while` loop and replaced its usage in round zero with an explicit
    zero index.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们将`round_num`的定义从函数开始处移到了`while`循环之前，并在零轮次中使用显式的零索引替代了它的用法。
- en: These steps brought the kernel register usage down from 43 registers to just
    25 registers and dropped the execution time to just 4.32 ms, somewhat faster than
    the forced register allocation version. Forcing this to just 24 again resulted
    in slower code due to the compiler’s usage of local memory. Unfortunately, we
    really want a maximum of 24 registers, not 25, as this will increase the block
    count and bring in another set of warps, increasing the overall amount of parallelism.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将内核的寄存器使用量从43个寄存器减少到仅25个寄存器，并将执行时间缩短到仅4.32毫秒，比强制寄存器分配版本稍快。将寄存器数强制限制为24个反而导致了更慢的代码，因为编译器使用了本地内存。不幸的是，我们真正想要的是最多24个寄存器，而不是25个，因为这将增加块数，并引入另一组warp，从而增加整体的并行性。
- en: Let’s replace
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们替换
- en: '[PRE66]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: with
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: with
- en: '[PRE67]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: This will eliminate the need to hold the formal parameter `num_rounds` in a
    register and allow the compiler to instead use a literal value of 10, the value
    of the `#define` for `NUM_ROUNDS`. Using a literal value serves two purposes.
    First, it allows the comparison of the register holding `num_rounds` with an immediate
    value, rather than a comparison of two registers. Second, it means the bounds
    of the loop are known, which in turn allows the compiler to safely unroll the
    entire loop or sections of the loop as needed.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这将消除将正式参数`num_rounds`保存在寄存器中的需要，并允许编译器改为使用字面值10，即`NUM_ROUNDS`的`#define`值。使用字面值有两个目的。首先，它允许将持有`num_rounds`的寄存器与立即数进行比较，而不是将两个寄存器进行比较。其次，这意味着循环的边界是已知的，从而允许编译器根据需要安全地展开整个循环或循环的部分。
- en: This indeed allows the compiler to use just 24 registers, the magic boundary
    number we need to potentially schedule another block. The savings are significant,
    although with 256 threads per block we do not bring in any additional blocks.
    Despite this the time for 16 blocks does drop. However, the timing becomes erratic
    and quite variable, with some runs now taking longer than before. We’re now starting
    to see the warps compete with one another. With such a small sample size (16 cipher
    blocks) the results become highly variable from run to run. Therefore, we’ll increase
    the number of cipher blocks to 2048 K and average the results.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实使编译器仅使用24个寄存器，这是我们可能调度另一个块所需的魔法边界数。节省是显著的，尽管在每个块使用256个线程时，我们并没有引入额外的块。尽管如此，16个块的执行时间确实有所下降。然而，时间变得不稳定且变化较大，一些运行的时间比之前还要长。现在我们开始看到warp之间相互竞争。由于样本量很小（16个密码块），结果在每次运行之间变得非常不稳定。因此，我们将增加密码块的数量到2048K，并对结果进行平均。
- en: The strategy CUDA adopts when allocating registers is to try for the smallest
    number of registers possible. With our transition from 25 to 24 registers, using
    256 threads per block, we can still only schedule two blocks. However, if we halve
    the number of threads per block, we can squeeze in another block of 128 threads.
    Thus, we can run five blocks per SM at 128 threads, 24 registers (640 total).
    This is compared with four blocks at 25 registers per block (512 threads). Does
    this make a difference? Yes, it does (see [Table 7.3](#T0020)).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA在分配寄存器时采用的策略是尽量使用最小数量的寄存器。通过将寄存器从25个减少到24个，在每个块使用256个线程时，我们仍然只能调度两个块。然而，如果我们将每个块的线程数减半，就可以挤入另一个包含128个线程的块。因此，我们可以在每个SM上运行五个128线程、24寄存器（共640个寄存器）块。与每块使用25个寄存器（共512个线程）的四个块相比，这有何不同？是的，确实有不同（见[表7.3](#T0020)）。
- en: Table 7.3 Effect of Using Different Numbers of Threads
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3 使用不同线程数的效果
- en: '| 64 Threads | 128 Threads | 256 Threads |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 64 线程 | 128 线程 | 256 线程 |'
- en: '| --- | --- | --- |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1150 ms | 1100 ms | 1220 ms |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 1150 ms | 1100 ms | 1220 ms |'
- en: '| 100% | 96% | 111% |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 100% | 96% | 111% |'
- en: If we use a 64-thread version as a baseline, we hit the maximum limit of eight
    blocks, which in turn limits us to a total of 512 threads. The 128-thread version
    is limited to five blocks, 640 threads in total. The 256-thread version is limited
    to two blocks, again 512 threads total.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以64线程版本为基准，我们达到了八个块的最大限制，这反过来又限制了我们总共只能有512个线程。128线程版本限制为五个块，总线程数为640个。256线程版本限制为两个块，依然是512个线程总数。
- en: You might expect the 64-thread version and the 256-thread version, given they
    both run a total of 512 threads, to take the same time. The 64-thread version
    is faster because it provides a better instruction mix, with different blocks
    performing different parts of the algorithm. The 256-thread version tends to have
    its threads all doing the same thing at the same time. Remember in this compute
    1.2 device there is no L1/L2 cache, so this is simply a comparison of instruction
    and memory throughput. It’s also far easier for the CUDA runtime to get a better
    load balance between the two SMs due to the smaller scheduling unit.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为64线程版本和256线程版本，因为它们总共运行512个线程，所需时间应该相同。实际上，64线程版本更快，因为它提供了更好的指令混合，不同的块执行算法的不同部分。而256线程版本往往让所有线程同时执行相同的任务。请记住，在这个计算1.2设备上，没有L1/L2缓存，因此这只是指令和内存吞吐量的比较。由于较小的调度单元，CUDA运行时也更容易在两个SM之间实现更好的负载平衡。
- en: Squeezing in that extra 64 threads by selecting a small number of threads per
    block gains us 120 ms, a 15% improvement over the 256-thread version on this compute
    1.2 device. We can only do this because we are within the 24 register threshold.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择每个块较少的线程数，挤入额外的64个线程，使我们获得了120毫秒的提升，比在这个计算1.2设备上使用256线程版本提高了15%。我们之所以能够做到这一点，是因为我们仍在24个寄存器的阈值内。
- en: With a small laptop ION-based GPU, we’re encoding around 1.8 million cipher
    blocks per second, which is approximately 28 MB/s including transfer times. Excluding
    the transfer times, this approximately doubles. This is the next area to address.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一台小型的基于ION的笔记本GPU，我们每秒钟编码大约180万个密码块，约合28MB/s（包括传输时间）。如果不计算传输时间，这个速度大约翻倍。这是接下来需要解决的领域。
- en: Transfer performance
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传输性能
- en: It’s necessary to transfer data to the GPU over the PCI-E data bus. Compared
    to access to memory, this bus is very slow. [Chapter 9](CHP009.html) explores
    in detail PCI-E transfer sizes and the effects of using paged or pinned memory.
    Pinned memory is memory that cannot be paged (swapped) out to disk by the virtual
    memory management of the OS. PCI-E transfer can, in fact, only be done using pinned
    memory, and if the application does not allocate pinned memory, the CUDA driver
    does this in the background for you. Unfortunately, this results in a needless
    copy operation from the regular (paged) memory to or from pinned memory. We can
    of course eliminate this by allocating pinned memory ourselves.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 必须通过PCI-E数据总线将数据传输到GPU。与访问内存相比，这条总线非常慢。[第9章](CHP009.html)详细探讨了PCI-E传输大小以及使用分页或固定内存的影响。固定内存是指不能被操作系统的虚拟内存管理分页（交换）到磁盘上的内存。实际上，PCI-E传输只能使用固定内存进行，如果应用程序没有分配固定内存，CUDA驱动会在后台为你处理这个问题。不幸的是，这会导致一个不必要的复制操作，从常规（分页）内存到固定内存或从固定内存复制回来。当然，我们可以通过自己分配固定内存来消除这个问题。
- en: 'In the application, we simply replace the following lines when allocating memory
    in the host application:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序中，我们只需要替换以下几行来分配主机应用程序中的内存：
- en: '[PRE68]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: with
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 与
- en: '[PRE69]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: And at the end, when cleaning up the memory allocation on the host, we replace
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，当清理主机上的内存分配时，我们会替换
- en: '[PRE70]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: with
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 与
- en: '[PRE71]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: So how does this affect performance? It reduces our 1100 ms time down to 1070
    ms, a drop of some 30 ms, just a 3% decrease in the execution time. The actual
    gain is *very* dependent on the processor and chipset being used. Typically you
    see anything up to 20% performance gain in transfer time using this method. However,
    the laptop we are using for this test is using an X1 PCI-E 2.0 link. The fact
    that we see a minor but consistent improvement would suggest removing the redundant
    copy is insignificant in comparison to the actual copy time over this rather slow
    link.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这如何影响性能呢？它将我们原本的1100毫秒时间缩短至1070毫秒，减少了大约30毫秒，执行时间下降了3%。实际的性能提升*非常*依赖于所使用的处理器和芯片组。通常，使用这种方法可以看到高达20%的传输时间性能提升。然而，我们用于此测试的笔记本电脑使用的是X1
    PCI-E 2.0链接。我们看到的微小但稳定的提升表明，移除冗余复制与通过这个相对较慢的链接进行的实际复制时间相比，几乎可以忽略不计。
- en: Despite the miserable gain pinned memory has brought us on this platform, we
    need to use pinned memory for the next step in the optimization of the transfers.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管固定内存在这个平台上带来的性能提升微乎其微，但我们仍需要在优化传输的下一步中使用固定内存。
- en: A single streaming version
  id: totrans-376
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单流版本
- en: We cover streams in detail in [Chapter 8](CHP008.html), as they are essential
    in using more than one GPU on a problem. We’ll use them here on a single-GPU problem,
    as they allow us to both execute memory transfers and perform kernels at the same
    time. In effect, you must try to overlap the kernel execution with the transfer
    time. If we’re lucky the transfer time is less than or equal to the calculation
    time. Thus, the transfer time is effectively hidden behind the compute time of
    a different stream and becomes free.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第8章](CHP008.html)中详细介绍了流，因为它们在处理多GPU问题时至关重要。我们将在这里处理单GPU问题，因为它们允许我们同时执行内存传输和内核操作。实际上，你必须尽量使内核执行与传输时间重叠。如果幸运的话，传输时间小于或等于计算时间。因此，传输时间实际上被隐藏在另一个流的计算时间后面，变得“免费”。
- en: Streams are simply virtual work queues that we’ll use here in a relatively simple
    manner. Initially we’ll create a single stream and move from a synchronous operation
    to an asynchronous operation with respect to the CPU. With this approach we will
    likely see a slight improvement due to the decreased synchronization needed for
    an asynchronous operation, but I’d expect this to be minor. Only once you introduce
    multiple streams can you really expect to see any significant speedup.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 流（Streams）只是虚拟的工作队列，我们将在这里以相对简单的方式使用它们。最初，我们将创建一个单一的流，并将CPU的同步操作转换为异步操作。采用这种方法，我们可能会看到轻微的性能提升，因为异步操作减少了所需的同步，但我预计这种提升是微小的。只有当你引入多个流时，才能真正期待看到显著的加速。
- en: Stream 0 is the default stream; the stream used if you do not specify one. This
    is a synchronous stream that helps significantly when debugging an application
    but is not the most efficient use of the GPU. Thus, we must first create an alternative
    stream. We then need to push the memory copy, events, and kernel operations into
    the stream.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: Stream 0是默认流；如果你没有指定流，则使用该流。它是一个同步流，在调试应用程序时非常有用，但并不是GPU最有效的使用方式。因此，我们必须首先创建一个替代流。接下来，我们需要将内存复制、事件和内核操作推送到该流中。
- en: The first thing we need to do is to create an alternative stream. This is done
    with
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建一个备用流。这可以通过以下方式完成：
- en: '[PRE72]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Conversely, we need to destroy the stream at the end of the host program once
    we’re finished with it:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，一旦完成主机程序中的流操作，我们需要在程序结束时销毁该流：
- en: '[PRE73]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Next the copy and event operations need to have the new stream added. Thus,
    we change
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，复制和事件操作需要添加新的流。因此，我们更改
- en: '[PRE74]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: to
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 到
- en: '[PRE75]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '`CUDA_CALL(cudaEventRecord(start_round_timer, **aes_async_stream**));`'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA_CALL(cudaEventRecord(start_round_timer, **aes_async_stream**));`'
- en: '[PRE76]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Notice how the newly created stream is used as the last parameter in each of
    the calls. The stream parameter is an optional parameter. Then we need to launch
    the kernel into the correct stream, which we again do by specifying the stream.
    As the stream parameter is actually the fourth parameter, we need to use zero
    as parameter 3\. Parameter 3 is the amount of dynamic shared memory the kernel
    will use. As we are using no dynamically allocated shared memory, we set this
    to zero. Thus,
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到新创建的流在每个调用中都作为最后一个参数使用。流参数是一个可选参数。然后，我们需要将内核启动到正确的流中，这同样是通过指定流来完成的。由于流参数实际上是第四个参数，我们需要使用零作为参数3。参数3是内核将使用的动态共享内存的数量。由于我们不使用动态分配的共享内存，因此将其设置为零。这样，
- en: '[PRE77]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: becomes
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 变为
- en: '[PRE78]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: We do the same for the copy back and stop timer event. As the stop timer event
    is at the end of the kernel, we also need to ensure we wait for this event.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回复制作和停止计时器事件，我们做了相同的操作。由于停止计时器事件位于内核的末尾，我们还需要确保等待此事件。
- en: '[PRE79]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: As the kernel, copy, and event operations are now entirely asynchronous it is
    critical that the data returned from the kernel is not used until such time as
    the kernel is actually complete. Forgetting to add such a synchronize operation
    after the final memory copy back to the host is often a cause for failure when
    moving to an asynchronous operation.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内核、复制和事件操作现在完全是异步的，因此关键的是在内核实际完成之前不要使用从内核返回的数据。忘记在最终的内存复制回主机后添加这样的同步操作，通常是将操作转换为异步操作时失败的原因。
- en: How does this change help? Running the test program reveals the time drops from
    1070 ms to just 940 ms, a drop of just over 12% in execution time. This is quite
    significant really, considering all we have done is to remove the implicit synchronization
    steps the CUDA driver was inserting when using stream 0.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这种变化有什么帮助呢？运行测试程序显示，时间从1070毫秒下降到仅940毫秒，执行时间下降了超过12%。考虑到我们所做的只是移除了CUDA驱动程序在使用流0时插入的隐式同步步骤，这真的是相当显著的变化。
- en: How do we compare with the CPU
  id: totrans-398
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们如何与CPU进行比较
- en: Intel provides a special extension to the AVX instruction set called AES-NI.
    This is based on a 128-bit-wide processing of the entire AES key state and key
    expansion. This equates to the `u4` type we’ve been using so far for memory load/stores.
    AES-NI has hardware support for both encode/decode and the expand key operation.
    Therefore, let’s look at how we can make use of this.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔提供了一种名为AES-NI的AVX指令集扩展。它基于对整个AES密钥状态和密钥扩展的128位宽度处理。这相当于我们目前用于内存加载/存储的`u4`类型。AES-NI为加密/解密操作和密钥扩展操作提供了硬件支持。因此，让我们来看一下如何利用这一点。
- en: Intel provides a AES-NI sample library, which is available at [*http://software.intel.com/en-us/articles/download-the-intel-aesni-sample-library/*](http://software.intel.com/en-us/articles/download-the-intel-aesni-sample-library/).
    The library, once downloaded, needs to be built, as there are no precompiled binary
    libraries to link to. This is still via an old command line interface. Those running
    Microsoft Visual Studio need to run a command `vcvars32.bat`, which sets a number
    of command line environment variables for the command line version. This in practice
    maps to the `vsvars32.bat` file, which actually sets the environment variables.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔提供了一个AES-NI示例库，可以在[*http://software.intel.com/en-us/articles/download-the-intel-aesni-sample-library/*](http://software.intel.com/en-us/articles/download-the-intel-aesni-sample-library/)下载。下载后，库需要构建，因为没有预编译的二进制库可以链接。这仍然通过旧的命令行接口进行。使用Microsoft
    Visual Studio的人需要运行一个命令`vcvars32.bat`，它为命令行版本设置了一些命令行环境变量。实际上，这对应于`vsvars32.bat`文件，后者实际设置了环境变量。
- en: Once the library is built, you need to add the library search path, and include
    the search path and library to the additional libraries in your Visual Studio
    project.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦库构建完成，您需要添加库搜索路径，并将搜索路径和库添加到Visual Studio项目中的附加库中。
- en: The Intel version of AES has one key difference to the GPU one we’ve developed
    to date. The original specification of AES lays out data in a column format, so
    A, B, C, and D are located in the same column. The Intel ASE-NI expects this to
    be transposed, so A, B, C, and D are all on the same row. AES-NI also, due to
    Intel’s byte ordering, requires the bytes to be ordered in memory in the reverse
    order compared to the order we have now.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: Intel版本的AES与我们目前开发的GPU版本有一个主要区别。AES的原始规范以列格式排列数据，因此A、B、C和D位于同一列。Intel的AES-NI期望这些数据进行转置，因此A、B、C和D都在同一行。由于Intel的字节顺序，AES-NI还要求字节在内存中的顺序与我们现在的顺序相反。
- en: 'Thus, we have two choices: either restructure the code to match the Intel AES-NI
    ordering, or perform a transformation on the data to convert one to the other.
    To allow memory blocks to be directly compared on the host, we’ll adapt our current
    solution to match the AES-NI format. As we also need AES-NI support, we’ll move
    all future development onto our Sandybridge-E (Core i7 3930 K @ 3.2 Ghz) platform
    with GTX470 GPUs. Thus, any further timings will no longer be comparable with
    our atom-based ION system used to date for this development.'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有两个选择：要么重构代码以匹配Intel AES-NI的顺序，要么对数据执行转换，将其从一种格式转换为另一种格式。为了允许内存块在主机上直接进行比较，我们将调整当前的解决方案，以匹配AES-NI格式。由于我们还需要AES-NI的支持，因此我们将所有未来的开发工作转移到我们的Sandybridge-E（Core
    i7 3930K @ 3.2 GHz）平台，并使用GTX470 GPU。因此，之后的任何计时结果将无法与我们迄今为止用于此开发的基于Atom的ION系统进行比较。
- en: 'The other major issue we should note at this point is the `uint4` type is encoded
    on the GPU as `x`, `y`, `z`, `w` and not `w`, `x`, `y`, `z`. Both my GPU and CPU
    version gave the same wrong answer, as it was based on the same wrong code. This
    was easily corrected once you understood the rather strange ordering of the `uint4`
    type (this is usually a red, green, blue, alpha representation where `w` is the
    alpha channel). Clearly, we should have based the CPU version on either an existing
    library or used the AES-NI library sooner to have detected such issues. The AES-NI
    code is quite simple, as shown here:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们需要注意的主要问题是，`uint4`类型在GPU上的编码顺序为`x`、`y`、`z`、`w`，而不是`w`、`x`、`y`、`z`。我的GPU和CPU版本都给出了相同的错误答案，因为它们基于相同的错误代码。一旦理解了`uint4`类型的这种奇怪顺序（通常这是红色、绿色、蓝色、透明度的表示，其中`w`是透明度通道），就很容易纠正了。显然，我们应该基于现有的库来编写CPU版本，或者更早地使用AES-NI库来检测此类问题。AES-NI代码非常简单，如下所示：
- en: '[PRE80]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: The interface for the AES code needs to be a byte-based interface. Here we show
    some sample code used to encode a single block of data `num_cipher_blocks` based
    on a single key. A similar set of code is used for the decode operation.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: AES代码的接口需要是基于字节的接口。这里展示了一些示例代码，用于基于单个密钥对单个数据块`num_cipher_blocks`进行编码。解码操作使用的是一套类似的代码。
- en: '[PRE81]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '`               const u8 ∗ key,`'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '`               const u8 ∗ key,`'
- en: '[PRE82]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: The key expansion operation is implicit in this operation as we pass an unexpanded
    key of just 16 bytes. However, it is done, internally, only once per encrypt/decrypt
    phase.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 密钥扩展操作在这个操作中是隐式的，因为我们传递了一个未扩展的仅包含16个字节的密钥。然而，它在内部每次加密/解密阶段仅执行一次。
- en: We’ll develop a program that will generate a set of four million random data
    blocks (around 64 MB of data), and encode it using a single key. We’ll then decode
    this data and check that the decoded data is the same as the original. We’ll run
    AES-NI, Serial, and CUDA versions of these operations and cross-check the results
    from each to ensure all implementations agree.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发一个程序，生成一组四百万个随机数据块（大约64MB的数据），并使用单个密钥进行编码。然后我们会解码这些数据，并检查解码后的数据是否与原始数据一致。我们将运行AES-NI、串行（Serial）和CUDA版本的这些操作，并交叉验证每个版本的结果，以确保所有实现结果一致。
- en: Once the GPU and CPU versions matched the AES-NI library, we were able to see
    just how fast the AES-NI instruction set is. On our Sandybridge-E system, the
    software-based serial expand key and decode block operation took 3880 ms, whereas
    the hardware-enabled AES-NI version took just 20 ms. By comparison, the CUDA version
    took 103 ms excluding any transfer times to or from the device. In fact, the copy
    to and copy from device operations took 27 ms and 26 ms, respectively. Given we’re
    using a GTX470 as our test device, and not a Tesla, we’d not be able to overlap
    both the transfer in and the transfer out as there is only a single memory transfer
    engine enabled in this device. Therefore, the absolute best case we could possibly
    achieve would be to entirely hide the kernel execution time behind one of these
    transfers, effectively eliminating it. However, to do this we’d need a 5× improvement
    in the kernel’s execution time. Let’s look therefore at the decode kernel in its
    revised form to be byte-for-byte compatible with the AES-NI output.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦GPU和CPU版本与AES-NI库匹配，我们就能看到AES-NI指令集的速度有多快。在我们的Sandybridge-E系统上，基于软件的串行扩展密钥和解码块操作需要3880毫秒，而硬件支持的AES-NI版本仅需20毫秒。相比之下，CUDA版本在不包括设备间数据传输时间的情况下需要103毫秒。实际上，数据传输到设备和从设备传输分别需要27毫秒和26毫秒。鉴于我们使用的是GTX470作为测试设备，而非Tesla，我们无法同时进行数据传入和传出，因为该设备仅启用了一个内存传输引擎。因此，我们能够实现的最佳情况是将内核执行时间完全隐藏在这些传输操作后面，从而有效地消除它。然而，要做到这一点，我们需要在内核执行时间上实现5倍的改进。因此，让我们来看一下经过修订后的解码内核，它与AES-NI输出字节对字节兼容。
- en: '[PRE83]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '` u32 w1 = key.y;`'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '`  u32 w1 = key.y;`'
- en: '[PRE84]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '`   EXTRACT_WORD_XOR(w1, a4, a5, a6, a7);`'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '`   EXTRACT_WORD_XOR(w1, a4, a5, a6, a7);`'
- en: '[PRE85]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '`  }`'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '`  }`'
- en: '[PRE86]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: The function first reads encrypted data and then decodes it into a set of 16
    registers. The decode function is the inverse of the encode function. Therefore,
    we count the rounds down from 10 to 0.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先读取加密数据，然后将其解码成一组16个寄存器。解码函数是编码函数的反向操作。因此，我们从10轮数到0轮。
- en: The decode side is more complex than encode, mainly because of the Galois multiplication
    that is used. The multiplication is precalculated into a table. Thus, the simple
    series of XOR operations now needs to perform a number of data-dependent lookups
    into one of four tables, each of which is 1 K bytes in size. This, however, generates
    a poor scattered memory access pattern.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 解码过程比编码更复杂，主要是因为使用了Galois乘法。乘法结果被预先计算到一个表格中。因此，原本简单的XOR操作系列现在需要执行若干次数据依赖的查找操作，这些查找操作会访问四个表格中的一个，每个表格的大小为1K字节。然而，这会生成一个较差的分散内存访问模式。
- en: We then rotate the values in the rows and finally perform the `s_box` substitution
    as before. As with the inverted mix column operation, the `s_box` function generates
    a scattered memory read pattern. Finally, a single 128-byte write is used to write
    out the data to global memory.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们旋转行中的值，最后像以前一样执行`s_box`替代操作。与反向混合列操作类似，`s_box`函数生成了一个分散的内存读取模式。最后，使用一个128字节的写操作将数据写入全局内存。
- en: Another significant problem with this initial implementation is that this to
    uses far too many registers, 44 in total. It’s a complex kernel. We succeed in
    the goal of keeping the computation within registers until the very last moment.
    Forcing this (via the `maxrregcount=42` compiler flag) to 42 registers allows
    the scheduling of one additional block into the SM. This in turn reduces the execution
    time to 97 ms. Forcing register usage down means more spilling to global memory,
    and in this case, we see the memory bandwidth requirements jump by 25%. This suggests
    there is room to improve by reducing the register usage, but it needs to be done
    by other means.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 这个初始实现的另一个显著问题是它使用了太多的寄存器，总共44个。它是一个复杂的内核。我们成功地实现了将计算保持在寄存器内，直到最后一刻。通过强制设置（通过`maxrregcount=42`编译器标志）为42个寄存器，可以将一个额外的块调度到SM中。这反过来将执行时间减少到97毫秒。强制减少寄存器使用意味着更多的数据溢出到全局内存，在这种情况下，我们看到内存带宽的需求增加了25%。这表明通过减少寄存器使用有改进的空间，但需要通过其他方式来实现。
- en: We can achieve the desired effect of allowing more blocks to get scheduled by
    reducing the number of threads per block. Dropping down from 128 threads to 96
    threads per block allows us to schedule the same number of warps as before, but
    with eight blocks instead of six. This drops the execution time to 96 ms. As the
    kernel uses no synchronization points, this is entirely down to the better instruction
    mix the additional blocks bring and also the effects of caching.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减少每个块的线程数，我们可以实现允许更多块被调度的效果。从每个块128个线程降到96个线程，可以让我们调度与之前相同数量的warp，但块数从六个增加到八个。这样，执行时间降至96毫秒。由于内核没有同步点，这完全归功于额外块带来的更好的指令组合以及缓存的效果。
- en: If we look at the memory view in [Figure 7.16](#F0085) from one of the experiments
    Parallel Nsight can run for use, we see that we have very high L1 cache usage,
    but nonetheless 281 MB is spilling out of this to the L2\. Worse still, 205 MB
    of that is spilling into global memory. The kernel reads and writes to global
    memory so we will have some global memory traffic, but how much should we expect?
    We have 4,195,328 blocks with each block being 16 bytes in size. Therefore, we
    have 67,125,248 or exactly 64 MB of data to read. Equally, we write out a decrypted
    block, so we have 64 MB of data to write out. The statistics for global memory
    are shown for the device as a whole and shows we’re reading/writing a total of
    205MB. Therefore, we are generating 160% of the global memory traffic necessary,
    which in turn is limiting the performance.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看[图 7.16](#F0085)中通过Parallel Nsight运行的实验结果，我们看到L1缓存使用率非常高，但有281 MB的数据溢出到L2。更糟糕的是，其中的205
    MB溢出到全球内存。内核对全球内存进行读写，因此我们会有一定的全球内存流量，但我们应该期待多少流量呢？我们有4,195,328个块，每个块为16字节。因此，我们需要读取67,125,248个字节，或者正好64
    MB的数据。同样，我们写出一个解密后的块，因此我们需要写出64 MB的数据。全球内存的统计数据显示设备的整体情况，显示我们读取/写入的总量为205 MB。因此，我们生成了必要的全球内存流量的160%，从而限制了性能。
- en: '![image](../images/F000077f07-16-9780124159334.jpg)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-16-9780124159334.jpg)'
- en: FIGURE 7.16 Initial memory bandwidth view.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16 初始内存带宽视图。
- en: Currently, the L1 cache is operating at peak efficiency, but there is 16 K of
    shared memory we’re not using at all. It does not have the coalescing requirements
    global memory has, so it would be a good candidate for a small data region with
    a scattered memory pattern. However, unlike the L1 cache, the shared memory has
    a per-block visibility, which would mean having to duplicate the data for every
    resident block on the SM.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，L1缓存的效率达到峰值，但我们有16 K的共享内存完全没有使用。它没有像全球内存那样的合并要求，因此它非常适合用于具有分散内存模式的小数据区域。然而，不同于L1缓存，共享内存具有每块可见性，这意味着必须为SM上的每个驻留块复制数据。
- en: The constant memory cache is not shown in [Figure 7.16](#F0085), but it would
    also be large enough to hold the Galios multiplication (`gmul`) and/or `s_box`
    tables. However, the constant cache has only one 32-bit element bandwidth per
    clock and is designed for the same element being accessed by every thread. Thus,
    the shared memory is a better candidate.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 常量内存缓存未在[图 7.16](#F0085)中显示，但它的大小也足够容纳Galios乘法（`gmul`）和/或`s_box`表。然而，常量缓存每个时钟周期只有一个32位元素带宽，且其设计是为了每个线程访问相同的元素。因此，共享内存是更好的选择。
- en: However, let’s first look at the two problem areas, `s_box` and the `gmul` tables.
    Both were declared as 32-bit unsigned types, to avoid huge numbers of instructions
    being added to shift and mask the 32-bit words. Given the memory traffic we’re
    generating, this was probably not a good choice. Changing these to a `u8` type,
    we see the off-chip memory accesses drop from 205 MB to 183 MB and the execution
    time drop from 96 ms to 63 ms. Clearly, this was causing a significant amount
    of overfetch from the global memory and reducing it helps considerably.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，让我们首先看看两个问题区域，`s_box`和`gmul`表。为了避免大量的指令被添加来进行32位字的移位和掩码，这两个表被声明为32位无符号类型。鉴于我们产生的内存流量，这可能并不是一个好的选择。将其改为`u8`类型后，我们看到离芯片的内存访问从205
    MB降到183 MB，执行时间从96毫秒降到63毫秒。显然，这造成了全球内存的过多读取，减少这一点有很大帮助。
- en: With a reduced memory footprint, each `gmul` table is now 256 bytes in size,
    so the four tables fit easily with 1 K. As we can place a maximum of eight blocks
    per SM, 8 K of shared memory is now sufficient to accommodate the `gmul` tables.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 通过减小内存占用，每个`gmul`表现在为256字节，因此四个表格可以轻松适配1 K。由于每个SM最多可以放置八个块，8 K的共享内存现在足够容纳`gmul`表。
- en: Performing this shared memory optimization, however, has a problem. Indeed we
    move 18 GB of memory bandwidth from the L1 cache to the shared memory, and the
    main memory bandwidth drops by 7 MB. However, we have to move 1 K of data at the
    start of each block, as the shared memory is not persistent or shared between
    blocks. The L1 cache, however, is shared between the blocks and is currently doing
    a very good job of dealing with this scattered memory pattern, as the tables are
    entirely resident within the cache. The net improvement of speed for our 8 K of
    shared memory usage is almost zero, so this optimization was removed, leaving
    the tables in the L1 cache instead. Note this would have brought considerable
    improvement on compute 1.x devices, compared to global memory accesses, where
    there are no L1/L2 caches.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，进行这种共享内存优化有一个问题。的确，我们将18 GB的内存带宽从L1缓存转移到了共享内存，而主内存的带宽下降了7 MB。然而，我们必须在每个块的开始时移动1
    K的数据，因为共享内存在块之间不是持久的或共享的。然而，L1缓存在块之间是共享的，并且目前在处理这种散布的内存模式时表现非常出色，因为表格完全驻留在缓存中。我们使用8
    K共享内存的速度净改善几乎为零，因此这个优化被移除，表格保留在L1缓存中。请注意，与全局内存访问相比，这在计算1.x设备上会带来显著的改进，因为那里没有L1/L2缓存。
- en: Looking back at [Figure 7.16](#F0085), did you notice something interesting?
    Did you notice we were using 1.91 GB of local storage? Local storage is the compiler
    spilling registers to the memory system. Prior to compute 2.0 devices this would
    actually go to global memory space. From compute 2.0 onward it gets contained
    within the L1 cache if possible, but can still cause significant unwanted global
    memory traffic.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾[图 7.16](#F0085)，你注意到什么有趣的事情了吗？你是否注意到我们使用了1.91 GB的局部存储？局部存储是编译器将寄存器溢出到内存系统的地方。在计算2.0设备之前，这部分会实际被转移到全局内存空间。从计算2.0开始，它会尽可能地被限制在L1缓存中，但仍然可能导致大量不必要的全局内存流量。
- en: 'When compiling, the `-v` option will display a summary of the register usage
    from the kernel. Anytime you see the following message you have local memory being
    used:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在编译时，`-v`选项将显示内核的寄存器使用摘要。每当你看到以下信息时，表示局部内存正在被使用：
- en: '[PRE87]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: The main issue here is the `uint4` type being used. In combination with the
    high register usage elsewhere this `uint4` load from global memory is immediately
    being spilled to local memory. A 128-bit `uint4` load was deliberately chosen
    to minimize the number of load transactions to global memory. By spilling it to
    local memory instead of holding in registers, the compiler is unnecessarily polluting
    the caches and causing writes back to global memory.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要问题是使用了`uint4`类型。由于其他地方高频繁使用寄存器，这个从全局内存加载的`uint4`立即被溢出到局部内存。选择128位的`uint4`加载是故意的，目的是最小化加载到全局内存的事务数量。通过将其溢出到局部内存，而不是保留在寄存器中，编译器不必要地污染了缓存并导致写回到全局内存。
- en: We can explicitly move this data item into shared memory instead of local memory
    by simply declaring it as an array of `__shared__` and indexing it by `threadIdx.x`.
    As shared memory is a per-block form of local memory, we can move the spilled
    register explicitly into the shared memory. Moving this parameter generates the
    memory view shown in [Figure 7.17](#F0090).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过简单地将数据项声明为`__shared__`数组，并通过`threadIdx.x`进行索引，显式地将其从局部内存移动到共享内存中。由于共享内存是每个块的局部内存形式，我们可以显式地将溢出的寄存器移到共享内存中。移动此参数会生成在[图
    7.17](#F0090)中显示的内存视图。
- en: '![image](../images/F000077f07-17-9780124159334.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-17-9780124159334.jpg)'
- en: FIGURE 7.17 Memory transfers after using shared memory.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.17 使用共享内存后的内存传输。
- en: Notice how simply moving this data item to shared memory drops the local memory
    usage from 1.91 GB to just 256 MB, and the traffic to global memory from 183 MB
    to 133 MB. Our shared memory traffic is approximately double what it was before
    to the L1, which is largely due to the shared memory bank conflicts. These are
    caused by placing a 128-bit (16-byte) value into a 32-bit (4-byte) shared memory
    system. The compiler, however, still insists on creating a stack frame, much smaller
    than before, but it’s still there. The overall execution time remains stubbornly
    stuck at 63 ms.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，仅仅将这个数据项移动到共享内存中，就能将局部内存的使用量从1.91 GB降到仅仅256 MB，而全局内存的流量也从183 MB降到133 MB。我们共享内存的流量大约是之前的两倍，主要是由于共享内存银行冲突。这是由于将128位（16字节）值放入32位（4字节）的共享内存系统造成的。然而，编译器仍然坚持创建一个堆栈帧，尽管比以前小得多，但它依然存在。整体执行时间依然顽固地保持在63毫秒。
- en: To see exactly what parameters are being spilled you have to look at the PTX
    code, the assembly code, generated within a given kernel. Any PTX instructions
    such as `st.local` or `ld.local` are operating on local data. Local data is also
    declared with `local` as a prefix. It turns out the remaining local data is actually
    the parameter data used between the `__global__` caller and the `__device__` function,
    that is,
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 要确切查看哪些参数被溢出，必须查看在给定内核中生成的 PTX 代码、汇编代码。任何 PTX 指令，如`st.local`或`ld.local`，都在操作本地数据。本地数据也通过
    `local` 前缀进行声明。事实证明，剩余的本地数据实际上是 `__global__` 调用者和 `__device__` 函数之间使用的参数数据，也就是说，
- en: '[PRE88]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '` const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;`'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '` const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;`'
- en: '[PRE89]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'The fact that we have passed a number of parameters to the device function,
    which in turn allows it to be called by a number of global functions and the host
    function, causes the compiler to insert a stack frame. We rarely if ever want
    the compiler to call a stack and instead want it to inline the call to the device
    function, thereby eliminating any need to use a stack. We can do this using the
    __`_forceinline__` directive when declaring the function as shown here:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将多个参数传递给设备函数，这使得该函数能够被多个全局函数和主机函数调用，从而导致编译器插入了堆栈框架。我们很少希望编译器调用堆栈，反而希望它内联调用设备函数，从而消除使用堆栈的需求。我们可以使用
    `__forceinline__` 指令在声明函数时实现这一点，如下所示：
- en: '[PRE90]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '`}`'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '`}`'
- en: Recompiling the code no longer produces the stack frame message. Due to the
    function now being a consolidated whole, the compiler can much better apply optimization
    techniques to it. The register usage drops to just 33 instead of the forced 42
    registers we were using before to accommodate eight blocks. We can verify local
    memory is no longer being used by looking at the memory overview in [Figure 7.18](#F0095).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 重新编译代码后不再生成堆栈框架消息。由于该函数现在是一个整体，编译器可以更好地对其应用优化技术。寄存器的使用量降至仅 33，而不是之前为了容纳 8 个块而强制使用的
    42 个寄存器。我们可以通过查看[图 7.18](#F0095)中的内存概览来验证不再使用本地内存。
- en: '![image](../images/F000077f07-18-9780124159334.jpg)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000077f07-18-9780124159334.jpg)'
- en: FIGURE 7.18 Memory usage after stack elimination.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.18 堆栈消除后的内存使用情况。
- en: We can see in [Figure 7.18](#F0095) the local memory traffic now falls to zero.
    What little L2 cache usage there was is eliminated. The global memory usage falls
    by another 5 MB to 128 MB, the magic figure we were expecting the global memory
    bandwidth to be based on for the size of data we’re processing. The execution
    time reduces marginally but still remains at 63 ms.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 7.18](#F0095)中我们可以看到，本地内存流量现在降为零。之前的少量 L2 缓存使用已被消除。全局内存使用量再次减少了 5 MB，降至
    128 MB，这是我们预期基于处理数据大小的全局内存带宽的理想值。执行时间略微减少，但仍保持在 63 毫秒。
- en: The kernel makes considerable use of the XOR operation, which is one of the
    instructions that is not available at full rate within the device. Thus, by ensuring
    we keep the maximum number of blocks in the SM, we ensure a good instruction mix
    and that everything doesn’t start backing up behind the units performing the XOR
    operations.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 该内核大量使用 XOR 操作，这是设备中无法以全速执行的指令之一。因此，通过确保在 SM 中保持最大数量的块，我们确保了良好的指令混合，避免了所有操作都排队等待执行
    XOR 操作的单元。
- en: At 96 threads per block with the previous high 42 register count we could schedule
    eight blocks using 24 warps. This is around 50% of the available capacity of the
    SM in terms of the number of warps it could run. However, we can see from looking
    at the Parallel Nsight “Issue Stalls” experiment how much of the SM capacity we’re
    actually using. We stall just 0.01% of the time, which means the SM is already
    almost at peak capacity. Increasing the occupancy figure by increasing the list
    of possible warps, therefore, is unlikely to help significantly. Increasing the
    number of threads from 96 to 128 allows us to increase the number of warps available
    for scheduling from 24 to 28\. This eliminates the remaining fractional stall
    issue and increases the fraction of the time that both warp scheduler have warps
    available, gaining us a 1.5 ms reduction in the timing. This brings the total
    execution time to 61.5 ms.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个块 96 个线程和之前的高 42 寄存器数目下，我们可以使用 24 个 warp 调度八个块。这大约占 SM 可用容量的 50%，即它能够运行的
    warp 数量。然而，通过查看 Parallel Nsight 的“问题停顿”实验，我们可以看到实际使用了多少 SM 容量。我们仅停顿了 0.01% 的时间，这意味着
    SM 已经几乎达到了峰值容量。因此，通过增加可调度 warp 的数量来增加占用率，可能不会显著帮助性能提升。将线程数从 96 增加到 128 使我们能够将可调度
    warp 的数量从 24 增加到 28。这消除了剩余的小停顿问题，并增加了两个 warp 调度器都有可用 warp 的时间比例，从而使时间减少了 1.5 毫秒。这将总执行时间减少到
    61.5 毫秒。
- en: Considerations for running on other GPUs
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在其他 GPU 上运行的注意事项
- en: Having now developed a program for a single, modern GPU, how well does i work
    on other GPUs? Often, especially if you are writing commercial applications, your
    program will need to work well on each level of hardware in the marketplace. Although
    programs will run on most GPU generations, you should be aware of what adaptations
    may be required to achieve good performance on that hardware. We’ll look at this
    with the AES program we’ve developed here.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经为单个现代 GPU 开发了一个程序，那么它在其他 GPU 上的表现如何呢？通常，尤其是当你编写商业应用程序时，你的程序需要在市场上每一层次的硬件上都能良好运行。虽然程序可以在大多数
    GPU 世代上运行，但你应该了解为了在这些硬件上获得良好的性能，可能需要做哪些适配。我们将通过我们在这里开发的 AES 程序来观察这一点。
- en: Out first target is the GTX460 card, a compute 2.1 card based on Fermi. Major
    differences are the compute 2.1 architecture (7 SMs × 48 CUDA cores vs. 14 SMs
    × 32 CUDA cores), reduced L2 cache size (512 K vs. 640 K), reduced L1 cache size
    per CUDA core (48 K L1 shared between 48 CUDA cores vs. 48K L1 shared between
    32 CUDA cores), and the reduced memory bandwidth (115 GB/s vs. 134 GB/s).
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个目标是 GTX460 卡，这是一款基于 Fermi 的计算 2.1 卡。主要区别包括计算 2.1 架构（7 个 SM × 48 个 CUDA
    核心 vs. 14 个 SM × 32 个 CUDA 核心）、减少的 L2 缓存大小（512K vs. 640K）、每个 CUDA 核心的 L1 缓存大小减少（每
    48 个 CUDA 核心共享 48K L1 vs. 每 32 个 CUDA 核心共享 48K L1）以及减少的内存带宽（115 GB/s vs. 134 GB/s）。
- en: Based purely on total CUDA core count (336 vs. 448), we’d expect around 75%
    of the performance. However, adjusting for clock speed differences, this gives
    us a little less than 10% performance difference between the two devices. Memory
    bandwidth is 15% less on the GTX460.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 单纯从 CUDA 核心总数（336 核 vs. 448 核）来看，我们预计性能大约为 75%。然而，考虑到时钟速度的差异，这使得两个设备之间的性能差异不到
    10%。GTX460 的内存带宽比 GTX 460 少了 15%。
- en: For the decrypt function the time actually measured is 100 ms compared with
    61.5ms, which is somewhat disappointing. Looking at the execution profile we see
    that the SMs on the GTX460 are able to clock through more instructions, so the
    ratio of when the data arrives to the compute has changed. We again see a tiny
    amount of stalling in the SMs. With 128 threads per block we manage to get seven
    blocks scheduled (28 warps). If we could just reduce the register usage slightly
    we could execute another block and make better use of the SM. We therefore apply
    the same technique we used in the encode operation and move the inverse mix columns
    operation closer to the decode operation. Thus,
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解密函数，实际测量的时间为 100 毫秒，而不是 61.5 毫秒，这有些令人失望。从执行概况来看，我们发现 GTX460 上的 SM 能够执行更多指令，因此数据到达与计算的比率发生了变化。我们再次看到
    SM 中出现了微小的停顿。每块 128 个线程时，我们设法调度了七个块（28 个 warps）。如果我们能稍微减少寄存器的使用，我们就能执行另一个块，更好地利用
    SM。因此，我们采用与编码操作相同的技术，将逆混合列操作移近解码操作。于是，
- en: '[PRE91]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '`  INV_MIX_COLUMN_PTR2(a12, a13, a14, a15,`'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '`  INV_MIX_COLUMN_PTR2(a12, a13, a14, a15,`'
- en: '[PRE92]'
  id: totrans-461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: becomes
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 变得
- en: '[PRE93]'
  id: totrans-463
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: This fusing of the operation allows the register usage to drop to the magic
    31 registers, which in turn allows us to schedule another block, giving a total
    of 32 warps per SM. This compensates for the compute 2.1 devices having a higher
    ratio of compute to load/store units than compute 2.0 devices. We see a small
    drop from 100 ms to 98 ms. However, our compute 2.0 device (the GTX470) was already
    using its compute cores to full capacity. This change, which introduces a few
    more tests, costs us 0.5 ms, bringing us back up to 62 ms on the compute 2.0 device.
    You may sometimes find this, especially with compute 2.0/compute 2.1 devices where
    the balance of execution units within an SM is different.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 这种操作融合使得寄存器的使用降到了神奇的 31 个寄存器，这反过来允许我们调度另一个块，从而在每个 SM 中得到 32 个 warp。这弥补了计算 2.1
    设备比计算 2.0 设备具有更高的计算与加载/存储单元比的情况。我们看到执行时间从 100 毫秒稍微下降到了 98 毫秒。然而，我们的计算 2.0 设备（GTX470）已经完全使用了它的计算核心。这一变化，虽然引入了更多的测试，但让我们损失了
    0.5 毫秒，回到了 62 毫秒，尤其是在计算 2.0/计算 2.1 设备中，SM 内部执行单元的平衡有所不同时，你可能会遇到这种情况。
- en: The second target is the GTX260, a compute 1.3 device. The major difference
    here is the complete lack of L1 and L2 caches. SM architecture is different with
    27 SMs versus 14 SMs, for a total of 216 CUDA cores versus 448 CUDA cores. Memory
    bandwidth is 112 GB/s versus 134 GB/s some 16% less and on par with the GTX460.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个目标是 GTX260，它是一个计算 1.3 设备。这里的主要区别是完全没有 L1 和 L2 缓存。SM 架构不同，有 27 个 SM，而不是 14
    个 SM，总共有 216 个 CUDA 核心，而不是 448 个 CUDA 核心。内存带宽为 112 GB/s，而 134 GB/s，减少了大约 16%，与
    GTX460 相当。
- en: The initial run was 650 ms for the decode function, over 10 times slower than
    the GTX470\. Why is this? One of the key reasons is the compute 1.x platform does
    not support a unified addressing mode. Thus, an explicit declaration of intended
    memory usage is needed. In the case of the `gmul` tables, they are generated on
    the device through a small compute kernel. As such, these tables exist in global
    memory. On compute 2.x platforms global memory is cached, whereas on compute 1.x
    platforms you have to explicitly make it cacheable. We can do this in a couple
    of ways.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 初始运行解码函数的时间为 650 毫秒，比 GTX470 慢了 10 倍以上。为什么会这样？其中一个关键原因是计算 1.x 平台不支持统一寻址模式。因此，需要明确声明预期的内存使用情况。在
    `gmul` 表的情况下，它们通过一个小的计算内核在设备上生成。因此，这些表存在于全局内存中。在计算 2.x 平台上，全局内存是有缓存的，而在计算 1.x
    平台上，你必须明确将其设为可缓存。我们可以通过几种方式做到这一点。
- en: First, we need to specify that the memory used for `gmul` is constant, which
    in turn means we can’t write to it from the device. As we have a copy of the data
    on the host we can either copy it to the device via the `cudaMemcpyToSymbol` call
    or simply declare it on the device as constant memory and initialize it there
    statically. Thus, the code to calculate the `gmul` table was replaced with a simple
    expanded definition of the table lookup. This then resides in the constant cache.
    Rerunning the code we see a drop from 650 ms to 265 ms, a drop in execution time
    of nearly 60%. However, the GTX260 is still a factor of 4.2× slower than the GTX470
    and 2.7× slower than the GTX460.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要指定 `gmul` 使用的内存是常量，这意味着我们无法从设备写入它。由于我们在主机上有一份数据副本，因此我们可以通过 `cudaMemcpyToSymbol`
    调用将其复制到设备，或者直接在设备上将其声明为常量内存并在那里进行静态初始化。因此，计算 `gmul` 表的代码被替换为对表查找的简单扩展定义。这样，表就存储在常量缓存中。重新运行代码后，我们看到执行时间从
    650 毫秒下降到 265 毫秒，下降了近 60%。然而，GTX260 仍然比 GTX470 慢 4.2 倍，比 GTX460 慢 2.7 倍。
- en: Finally, an older GT9800 card has approximately half the number of CUDA cores
    of the GTX260 and half the memory bandwidth. As might be expected, we see the
    265 ms approximately double (1.8×) to 478 ms.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一张旧的 GT9800 卡的 CUDA 核心数量大约是 GTX260 的一半，内存带宽也只有一半。正如预期的那样，我们看到 265 毫秒的时间大约是
    478 毫秒（增加了 1.8 倍）。
- en: The issue with both GTX260 and GT9800 is the organization of the data. Having
    the data match the format used for AES-NI means the data for a single key value
    is laid out sequentially in memory. To achieve much better performance we need
    to organize the memory such that each successive 32-bit value from the key appears
    as a column in memory rather than a row. The typical sequential arrangement that
    is ideal for the CPU is far from ideal for the GPU.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: GTX260 和 GT9800 的问题在于数据的组织。将数据与 AES-NI 使用的格式匹配意味着单个密钥值的数据会按顺序布局在内存中。为了获得更好的性能，我们需要组织内存，使得密钥中的每个连续的
    32 位值在内存中以列的形式出现，而不是按行排列。典型的顺序排列方式虽然对于 CPU 是理想的，但对于 GPU 来说却远非理想。
- en: 'The actual output of our AES encryption/decryption is shown here:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 AES 加密/解密的实际输出如下所示：
- en: '[PRE94]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '`Encrypt Copy From Device  :  25.428ms`'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '`设备加密复制：25.428毫秒`'
- en: '[PRE95]'
  id: totrans-473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Notice that with encrypt we’ve managed to get within approximately 2× of the
    AES-NI hardware, and for decrypt approximately within 3×. We’re using here a GTX470,
    which is hardware from the time of the regular Sandybridge CPU, rather than the
    more modern Sandybridge-E device. The regular Sandybridge device’s AES-NI performance
    is approximately half of the Sandybridge-E, which puts us on similar timings.
    The Kepler-based GTX680 would be a representative device to pair with a Sandybridge-E
    CPU. This would bring us in the order of a 2× performance improvement, bringing
    the GPU in line with the hardware-based AES-NI performance.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过加密，我们已经成功将性能提升到接近AES-NI硬件的约2倍，而解密则大约为3倍。我们这里使用的是GTX470，它是常规Sandybridge
    CPU时期的硬件，而不是更现代的Sandybridge-E设备。常规Sandybridge设备的AES-NI性能大约是Sandybridge-E的一半，这使得我们的性能与Sandybridge-E相近。基于Kepler的GTX680是与Sandybridge-E
    CPU搭配的代表性设备。这将带来约2倍的性能提升，使得GPU与基于硬件的AES-NI性能相当。
- en: The issue of what GPUs to support is a tricky one. There are a lot of older
    GPUs in the consumer market, so applications have to work well on these if you
    have a consumer application. Yet in large installations, simply the power bill
    means it makes no sense at all to keep the old GPUs running if they can be replaced
    with newer ones. The introduction of Kepler will hugely accelerate the retirement
    of the older Tesla boards.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 关于支持哪些GPU的问题相当棘手。消费者市场上有很多老旧GPU，因此如果你开发的是消费者应用程序，应用程序必须在这些旧设备上运行良好。然而，在大型安装环境中，仅仅电费就意味着，如果能够用更新的GPU替代旧的GPU，继续使用这些旧GPU毫无意义。Kepler的引入将大大加速旧款Tesla显卡的退役。
- en: If you need to support older hardware, then the best approach is to develop
    on that hardware from day one. You will then have a baseline application that
    will work reasonably well on the later-generation cards. Many of the optimizations
    you’d need to do for these cards would show significantly less benefit on the
    later-generation cards. However, almost all would show *some* benefit, it’s just
    a question of what return you get for the time you invest.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要支持旧硬件，那么最佳方法是从第一天开始就在该硬件上开发。这样，你将有一个在较新一代显卡上也能 reasonably 工作的基础应用程序。为这些显卡做的许多优化，在较新一代显卡上可能获得的好处会显著减少。然而，几乎所有的优化都会带来*某种*程度的好处，关键是你投入的时间能带来怎样的回报。
- en: Using multiple streams
  id: totrans-477
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多个流
- en: An example of multistream and multistream/multi-GPU programming is provided
    in [Chapter 8](CHP008.html). We’ll therefore not cover how to implement a streamed
    version of this algorithm. However, we’ll discuss some of the issues you’d need
    to think about to implement one, with this algorithm or a problem of your own.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](CHP008.html)提供了多流和多流/多GPU编程的示例。因此，我们不会讨论如何实现该算法的流式版本。然而，我们会讨论一些你在实现时需要考虑的问题，无论是针对这个算法还是你自己遇到的某个问题。'
- en: Multiple streams are useful in that they allow some overlap of kernel execution
    with PCI-E transfers. Their usefulness, however, is seriously hampered by the
    fact that one PCI-E transfer engine is only ever enabled on consumer cards. Only
    the Tesla series cards have both PCI-E transfer engines enabled, allowing for
    simultaneous bidirectional transfers.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 多个流的作用在于它们可以允许内核执行与PCI-E传输的重叠。然而，它们的有效性受到限制，原因在于只有消费级显卡上才会启用一个PCI-E传输引擎。只有Tesla系列显卡才同时启用了两个PCI-E传输引擎，允许实现双向传输的同时进行。
- en: We typically want to transfer data to the card, process some data, and then
    transfer the data out of the card. With a single PCI-E transfer engine enabled,
    we have just a single queue for all the memory transfers in the hardware. Despite
    being in separate streams, memory transfer requests feed into a *single* queue
    on Fermi and earlier hardware. Thus, the typical workflow pattern of transfer
    from host to device, invoke kernel, and then transfer from device to host creates
    a stall in the workflow. The transfer out of the device blocks the transfer into
    the device from the next stream. Thus, all streams actually run in series.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常希望将数据传输到显卡，处理一些数据，然后将数据从显卡传出。在启用单个PCI-E传输引擎的情况下，硬件中的所有内存传输只有一个队列。尽管在不同的流中，内存传输请求会被送入Fermi及更早版本硬件上的*单个*队列。因此，从主机到设备的传输、调用内核，然后从设备到主机的传输这种典型工作流模式会导致工作流停滞。设备中的传输会阻塞下一个流中设备的传输。因此，所有流实际上是串行执行的。
- en: The next issue we need to think about when using multiple streams is the resource
    usage. You need *N* sets of host and device memory, where *N* is the number of
    streams you wish to run. When you have multiple GPUs, this makes a lot of sense,
    as each GPU contributes significantly to the overall result. However, with a single-consumer
    GPU the gain is less easy to quantify. It works well only where either the input
    or output of the GPU workload is small in comparison to one another and the total
    transfer time is less than the kernel execution time.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个流时，我们需要考虑的下一个问题是资源使用。你需要 *N* 组主机和设备内存，其中 *N* 是你希望运行的流的数量。当你使用多个 GPU 时，这样做是很有意义的，因为每个
    GPU 都会显著贡献于整体结果。然而，对于单一消费者 GPU，增益就不容易量化。只有当 GPU 工作负载的输入或输出相比较小时，并且总传输时间少于内核执行时间时，这种做法才有效。
- en: In our application, we transfer in a set of blocks to be encoded in a single
    key set to use for the encoding. We transfer out the encoded blocks. The transfer
    in and transfer out are all but identical in size. The kernel execution time is
    around twice the size of the transfers. This means we have the opportunity to
    hide the input transfer time and only suffer the output transfer time.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的应用中，我们将一组块传入并在单一密钥集下进行编码。我们将编码后的块传出。传入和传出的大小几乎相同。内核执行时间大约是传输时间的两倍。这意味着我们有机会隐藏输入传输时间，仅遭受输出传输时间的影响。
- en: A single GPU can support up to 16 hardware streams (32 in Kepler), so it would
    be possible to perform 16 inbound transfers, 16 kernels, and then 16 outbound
    transfers and still be within the bounds of the memory on the device and the host.
    Transfers become more of an issue, as you will see in [Chapter 9](CHP009.html),
    where we introduce more than one GPU into the system. Due to contention for host
    resources, the transfer time itself may become longer the more concurrent transfers
    are in flight over the PCI-E bus.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 单个 GPU 最多支持 16 个硬件流（Kepler 中为 32 个），因此可以执行 16 次入站传输、16 个内核计算，然后是 16 次出站传输，并且仍然可以在设备和主机的内存范围内进行。随着我们在[第
    9 章](CHP009.html)中介绍多个 GPU 进入系统，传输将变得更加复杂。如你所见，随着并发传输越多，传输时间本身可能会因为 PCI-E 总线上的主机资源争用而变得更长。
- en: AES summary
  id: totrans-484
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AES 总结
- en: There were a number of issues we saw with AES that are worth summarizing here.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 AES 中遇到了一些值得总结的问题。
- en: • The ideal memory pattern for the CPU and GPU versions are different. Optimizing
    the memory pattern for the GPU would have brought considerable benefits (typically
    at least 2× on Fermi), especially on the earlier GPUs where this is far more critical.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: • CPU 和 GPU 版本的理想内存模式是不同的。优化 GPU 的内存模式将带来显著的收益（通常在 Fermi 上至少是 2×），尤其是在早期的 GPU
    上，这一点尤为重要。
- en: • For compute 1.x devices read-only memory needs to be explicitly declared as
    constant memory, rather than auto-designated by the compiler.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: • 对于 compute 1.x 设备，读写内存需要明确声明为常量内存，而不是由编译器自动指定。
- en: • It may be necessary to reorder or transform the kernel to allow the compiler
    to more easily see optimization opportunities.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: • 可能需要重新排序或转换内核，以便编译器更容易发现优化机会。
- en: • Efficient register usage and count were critical to achieving good performance.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: • 高效的寄存器使用和计数对于实现良好的性能至关重要。
- en: • You can share read-only data between blocks using the L1 cache, whereas holding
    the same read-only data shared memory necessitates *N* copies where *N* is the
    number of resident blocks.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: • 你可以通过 L1 缓存在不同块之间共享只读数据，而将相同的只读数据保存在共享内存中则需要 *N* 个副本，其中 *N* 是驻留块的数量。
- en: • Complex and thread-divergent algorithms, for example, the `gmul` function
    when decoding, can be replaced by nonthread-divergent memory lookups in the cache
    or shared memory. The cache was added specifically for such data-driven scattered
    memory patterns.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: • 复杂的线程分歧算法，例如解码时的 `gmul` 函数，可以通过非线程分歧的内存查找在缓存或共享内存中替代。缓存的添加就是为了应对这种数据驱动的分散内存模式。
- en: • Check the allocation of variables to registers and eliminate stack or local
    memory usage where possible.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: • 检查变量分配到寄存器中的情况，并在可能的情况下消除栈或本地内存的使用。
- en: • Always check correctness early in the solution, preferably with code developed
    independently.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: • 始终在解决方案初期检查正确性，最好使用独立开发的代码进行验证。
- en: • Always look at the *actual* timing of the program. Your mental model of how
    things work will not always be correct and often you will overlook something.
    Always look to the data for what effect each change has.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: • 永远要看程序的*实际*时间。你对事物如何运作的心理模型并不总是正确的，而且你经常会忽略某些东西。始终查看数据，了解每个变化带来的效果。
- en: Conclusion
  id: totrans-495
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We’ve looked at a couple of applications of GPU technology, deliberately chosen
    for not being a simple matrix multiply shown in so many other examples of CUDA
    programming. We looked at using GPUs to filter data, which is useful from the
    perspective of searching data for interesting facts and also from a pure signal
    processing perspective. We’ve also looked how to implement AES, a standard encryption
    algorithm on GPUs. Even if you never have to implement this in CUDA, you should
    now understand and feel happy about implementing or using such algorithms.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了GPU技术的几个应用，特意选择了不同于许多CUDA编程示例中的简单矩阵乘法的例子。我们研究了如何利用GPU过滤数据，这在从搜索数据中的有趣事实角度以及从纯信号处理角度来看都非常有用。我们还研究了如何在GPU上实现AES，一个标准的加密算法。即使你从未需要在CUDA中实现这个算法，你现在也应该理解并且对实现或使用这些算法感到自信。
- en: You should also have picked up on some of the tradeoffs and design points when
    targeting multiple compute levels and how design decisions early on in project
    development can affect the outcome later. Thinking about the usage of registers,
    shared memory, cache, and access patterns to global memory are all key aspects
    of a design that should be understood and worked out before you write a single
    line of code.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该注意到，当针对多个计算级别时的一些权衡和设计点，以及项目开发早期的设计决策如何影响后续的结果。考虑寄存器、共享内存、缓存的使用，以及访问全局内存的模式，都是设计中的关键方面，这些内容应该在编写第一行代码之前就理解并加以解决。
- en: One of the biggest issues programmers have today is growing up in a world where
    they are isolated from the hardware on which they are programming. To achieve
    great performance and not just average performance, it pays to understand, and
    understand thoroughly, the environment in which you are developing. Concepts such
    as various levels of memory hierarchy don’t really exist in traditional programming
    languages. The C language was invented back in the early 1970s and only in the
    C11 (as in 2011) standard do we finally see thread and local thread storage start
    to appear. CUDA, and its native language C, follows the principle of trusting
    the programmer. It exposes aspects of the hardware to you, and you should therefore
    consider it your responsibility to understand those features and use them well.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 今天程序员面临的最大问题之一是他们生活在一个与编程硬件隔离的世界里。为了获得卓越的性能，而不仅仅是平均性能，理解并彻底理解你所开发的环境是非常重要的。诸如各种级别的内存层次结构等概念在传统编程语言中并不存在。C语言是在1970年代初期发明的，直到C11（即2011年）标准中，我们才终于看到线程和本地线程存储开始出现。CUDA及其原生语言C遵循信任程序员的原则。它向你暴露硬件的某些方面，因此你应该视其为自己的责任，理解这些特性并将其使用得当。
- en: With a few examples now covered, we’ll move on to using multiple GPUs and optimizing
    applications, an area where we can extract massive speedups within a node simply
    by plugging more cards into the PCI-E bus and adapting our applications to be
    multi-GPU aware. The Kepler Tesla K10 product is the first Tesla dual-GPU solution,
    perhaps one of many we may see in the coming years. Multi-GPU programming, after
    CUDA 4.0, is actually not hard, as you’ll see in the subsequent chapters.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 通过几个例子，现在我们将转向使用多个GPU和优化应用程序，这是一个通过将更多显卡插入PCI-E总线并调整我们的应用程序以支持多GPU的方式，从单一节点中提取巨大加速的领域。Kepler
    Tesla K10产品是首款Tesla双GPU解决方案，也许是我们在未来几年中会看到的众多解决方案之一。在CUDA 4.0之后，多GPU编程其实并不难，正如你将在接下来的章节中看到的那样。
- en: Questions
  id: totrans-500
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题
- en: 1. What was the main reason why the AES application ran significantly slower
    on the GTX260 and GT9800 cards compared with the GTX460 and GTX470 cards? What
    would you do to address this?
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 为什么在GTX260和GT9800显卡上，AES应用程序的运行速度明显慢于GTX460和GTX470显卡？你会如何解决这个问题？
- en: 2. In the AES application, why did changing the `s_box` and `gmul` tables from
    `u32` to `u8` improve performance?
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 在AES应用程序中，为什么将`u32`类型的`s_box`和`gmul`表转换为`u8`后性能得到了提升？
- en: 3. What is thread level parallelism? Does it help, and if so why?
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 什么是线程级并行？它有帮助吗？如果有，为什么？
- en: 4. What problems are associated with using atomic operations?
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 使用原子操作有什么问题？
- en: Answers
  id: totrans-505
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 答案
- en: 1. The GTX260 and GT9800 cards are compute 1.3 and compute 1.1 cards, respectively.
    As such, they have no level one (L1) or level two (L2) caches as found on the
    compute 2.x cards. In the memory figures shown we were using the L1 cache with
    a 99% hit rate. Going from L1 to global memory means we move from terabytes of
    bandwidth to just the low hundreds of megabytes of bandwidth.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 1. GTX260和GT9800卡分别是计算1.3和计算1.1卡。因此，它们没有计算2.x卡上存在的一级（L1）或二级（L2）缓存。在所示的内存图表中，我们使用的是99%命中率的L1缓存。从L1到全局内存意味着我们从TB级的带宽降到只有几百MB的带宽。
- en: The memory coalescing also radically changes. The compute 2.x hardware fetches
    memory in 128-byte cache lines. If the thread fetches a single 128-byte value,
    `uint4` for example, the hardware can service this. On compute 1.x hardware coalescing
    requirements are much stricter.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 内存合并也发生了根本性变化。计算2.x硬件以128字节缓存行的形式获取内存。如果线程获取单个128字节的值，例如`uint4`，硬件可以处理这个请求。在计算1.x硬件上，合并要求要严格得多。
- en: The `uint4` type as currently compiled is hurting the algorithm. On compute
    2.x hardware a four-word vector load from memory is used followed by a four-word
    vector to shared memory. On the compute 1.x hardware, the CUDA 4.1 compiler generates
    code to load each 32-bit word separately and thus generates four more times the
    traffic in each direction than is necessary. The encrypted cipher data needs to
    be placed into a suitable form for coalescing.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 当前编译的`uint4`类型对算法造成了不利影响。在计算2.x硬件上，使用从内存中加载四字长向量，然后将四字长向量存入共享内存。而在计算1.x硬件上，CUDA
    4.1编译器生成的代码会分别加载每个32位字，因此每个方向的流量是必要流量的四倍。加密的密文数据需要以适合合并的形式存储。
- en: The constant cache is helpful. However, removing the `uint4` type from the shared
    memory, replacing it with register-held `u32` values, and then using the shared
    memory for the `gmul` and `s_box` tables would be more beneficial. You should
    also consider that on older devices, the texture cache can be a worthwhile additional
    resource worth the effort of exploiting.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 常量缓存很有帮助。然而，如果将`uint4`类型从共享内存中移除，替换为寄存器中持有的`u32`值，然后使用共享内存存储`gmul`和`s_box`表格，将会更有利。你还应该考虑到，在旧设备上，纹理缓存可能是一个值得利用的额外资源。
- en: 2. The s_box and `gmul` tables are accessed with a data-dependent pattern. We
    have a total of four tables, each of which is 256 entries in size. Using a `u8`
    type means we use 5 K of memory, which fits into both the L1 cache and the constant
    cache. Using `u32` values removed a number of `cvt` (convert type) instructions,
    but shifts four times the data from the L1 or constant cache. The extra compute
    overhead is easily worth the cost of not moving so much data. As a `u32` type,
    the caches need to store 20 K of data, easily exceeding the normal 16 K L1 cache
    allocation and the 8 K constant cache working set.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 2. `s_box`和`gmul`表格的访问模式依赖于数据。我们总共有四个表格，每个表格包含256个条目。使用`u8`类型意味着我们使用5KB内存，这适合L1缓存和常量缓存。使用`u32`值去除了许多`cvt`（类型转换）指令，但会使L1缓存或常量缓存移动四倍的数据。额外的计算开销完全值得为了不移动那么多数据而付出的成本。作为`u32`类型，缓存需要存储20KB的数据，轻松超过了常规16KB的L1缓存分配和8KB常量缓存工作集。
- en: 3. Thread level parallelism exploits the fact that most hardware is pipelined
    and thus able to accept nondependent instructions on successive clocks without
    blocking. A value of four independent items per thread is typically a good value
    to exploit to achieve thread level parallelism, something we look at in [Chapter
    9](CHP009.html).
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 线程级并行性利用了大多数硬件是流水线的这一事实，因此能够在连续的时钟周期内接收不依赖的指令而不发生阻塞。每个线程处理四个独立项的值通常是实现线程级并行性的一个好值，这是我们在[第9章](CHP009.html)中讨论的内容。
- en: 4. There are two main issues to consider. First, atomic operations, if oversubscribed,
    cause serialization. Thus, a warp of 32 values writing to the same memory address,
    be it shared or global memory, will serialize. Atomics, at least on Fermi, are
    warp-wide operations. Thus, having each thread in a warp perform an atomic operation
    to independent addressable locations will result in 32 atomic operations without
    serialization.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 需要考虑两个主要问题。首先，如果原子操作过度订阅，会导致串行化。因此，32个值的warp写入相同的内存地址（无论是共享内存还是全局内存）将会串行化。至少在Fermi架构中，原子操作是warp级的操作。因此，让warp中的每个线程对独立的可寻址位置执行原子操作，将导致32个原子操作而不发生串行化。
- en: The second problem is ordering of atomic writes. If all values in a warp write
    to one address, the order of the operation is not defined. You can obviously observe
    the order and it’s likely that this will remain consistent for a given device.
    Another device may, however, work differently. Thus, in using such knowledge,
    you’d be building a failure point into your application.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题是原子写操作的顺序。如果一个warp的所有值都写入同一地址，则操作的顺序是未定义的。显然，你可以观察到顺序，并且很可能在某个设备上这个顺序会保持一致。然而，另一个设备可能会有所不同。因此，在使用此类知识时，你会在应用程序中构建一个失败点。
- en: References
  id: totrans-514
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 1\. *Wikipedia, Rijndael Mix Columns*. Available at *[http://en.wikipedia.org/wiki/Rijndael_mix_columns](http://en.wikipedia.org/wiki/Rijndael_mix_columns)*;
    accessed Jan. 31, 2012.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. *维基百科，Rijndael混合列*。可在 *[http://en.wikipedia.org/wiki/Rijndael_mix_columns](http://en.wikipedia.org/wiki/Rijndael_mix_columns)*
    查阅；访问时间：2012年1月31日。
- en: 2\. *Federal Information Processing Standards Publication 197, Advanced Encryption
    Standard (AES)*. Available at *[http://csrc.nist.gov/publications/fips/fips197/fips-197.pdf](http://csrc.nist.gov/publications/fips/fips197/fips-197.pdf)*;
    accessed Feb. 5, 2012.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. *联邦信息处理标准出版物197，高级加密标准（AES）*。可在 *[http://csrc.nist.gov/publications/fips/fips197/fips-197.pdf](http://csrc.nist.gov/publications/fips/fips197/fips-197.pdf)*
    查阅；访问时间：2012年2月5日。
- en: '3\. *Toms Hardware, AES-NI Benchmark Results: Bitlocker, Everest, and WinZip
    14*. Available at *[http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption,
    review-31801–7.html](http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption_review-31801-7.html)*;
    accessed Apr. 26, 2012.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. *Toms Hardware，AES-NI基准测试结果：Bitlocker、Everest和WinZip 14*。可在 *[http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption,
    review-31801–7.html](http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption_review-31801-7.html)*
    查阅；访问时间：2012年4月26日。
