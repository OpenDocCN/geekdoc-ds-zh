- en: Chapter 7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using CUDA in Practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter we’ll look at a few examples of the not-so-common uses of GPUs
    to provide insight into how to solve a number of different types of computer problems.
    We’ll look at the problems involved in using GPUs for such computations.
  prefs: []
  type: TYPE_NORMAL
- en: Serial and Parallel Code
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Design goals of CPUs and GPUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CPUs and GPUs, although both execute programs, are a world apart in their design
    goals. CPUs use an MIMD (multiple instruction, multiple data) approach, while
    GPUs use an SIMT (single instruction, multiple thread) instruction model.
  prefs: []
  type: TYPE_NORMAL
- en: The CPU approach to parallelism is to execute multiple independent instruction
    streams. Within those instruction streams it seeks to extract instruction level
    parallelism. That is, it fills a very long pipeline of instructions and looks
    for instructions that can be sent to independent execution units. These execution
    units usually consist of one or more floating-point units, one or more integer
    units, a branch prediction unit, and one or more load/store units.
  prefs: []
  type: TYPE_NORMAL
- en: Branch prediction is something computer architects have worked extensively on
    for over a decade or so. The problem with branching is that the single instruction
    stream turns into two streams, the branch taken path and the branch not taken
    path. Programming constructs such as `for, while` loops typically branch backwards
    to the start of the loop until the loop completes. Thus, in a lot of cases, the
    branch can be predicted statically. Some compilers help with this in setting a
    bit within the branch instruction to say if the branch is likely to be met or
    not. Thus, loops that branch backwards can be predicated as taken, whereas conditionals
    are usually predicated as not taken, thus avoiding the branch altogether. This
    has the added advantage that the next instructions have typically already been
    prefetched into the cache.
  prefs: []
  type: TYPE_NORMAL
- en: Branch prediction evolved from the simple but quite effective static model,
    to use a dynamic model that records previous branching history. Multiple levels
    of complex branch prediction are actually present in modern processors due to
    the very high cost of a mispredicted branch and the consequential refilling of
    the long execution pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Along with branch prediction, a technique called *speculative execution* is
    used. Given the CPU will likely have predicted a branch correctly, it makes sense
    to start executing the instruction stream at that branch address. However, this
    adds to the cost of branch misprediction, as now the instruction stream that has
    been executed has to be undone or discarded.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal model for both branch prediction and speculative execution is simply
    to execute both paths of the branch and then commit the results when the actual
    branch is known. As branches are often nested, in practice such an approach requires
    multiple levels of hardware and is therefore rarely used.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have the other major difference, seen until recently, which is the
    amount and number of cache memory levels. The CPU programming model works on the
    nice principle of abstraction, that is, the programmer doesn’t have to care where
    the memory is because the hardware takes care of it. For most programs, except
    those that need to run fast, this works quite well. It used to be that instruction
    cycles were expensive, but with ever-increasing chip density, instruction cycles
    are now cheap. Accessing memory is now the bottleneck on modern processor design
    and this is addressed by the multiple levels of cache.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs, until the introduction of Fermi, took an alternative approach to this
    design. Fermi designers believe the programmer is best placed to make use of the
    high-speed memory that can be placed close to the processor, in this case, the
    shared memory on each SM. This is the same as the L1 cache found on a conventional
    processor, a small area of low latency and higher bandwidth memory. If you think
    about most programs, this makes a lot of sense. A programmer knows the program
    better than anyone else and therefore should be able to identify which off-chip
    memory accesses can be prevented by using shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Fermi expanded the on-chip memory space to 64K, 16 K of which must be allocated
    to an L1 cache. So that there was always some shared memory present, they did
    not allow the entire space to be allocated to either cache or shared memory. By
    default, Fermi allocates 48 K to shared memory and 16 K to cache. However, you
    can switch this and have 48 K of cache and 16 K of shared memory. Kepler also
    introduces a 32K/32K split option. In programs that make no use of shared memory,
    setting this switch can significantly to prefer L1 cache instead of shared memory
    improve performance for memory-bound kernels. This is done with a call to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the sample sort program we use to look at optimizing later in this chapter,
    this simple change reduced the overall execution time by 15%. This is a huge bonus
    for enabling a feature that is disabled by default.
  prefs: []
  type: TYPE_NORMAL
- en: With the inclusion of an L1 cache, GPUs and CPUs moved closer to one another
    in terms of the data fetched from memory. With previous GPU generations, memory
    accesses needed to be coalesced to achieve any sort of performance. Consider a
    noncoalesced memory fetch on the G80 and GT200 based hardware. If thread 0 reads
    from memory address `0x1000`, thread 1 reads from `0x2000`, thread 3 reads from
    `0x3000`, etc., this results in one memory fetch per thread of 32 bytes. Not 32
    bits, but 32 bytes, the minimum memory transaction size. The next access (`0x1004`,
    `0x2004`, `0x3004`, etc.) did exactly the same.
  prefs: []
  type: TYPE_NORMAL
- en: In Fermi, as with CPUs, a cache line of 128 bytes is fetched per memory access.
    Thus, subsequent access by an adjacent thread will usually hit the cache instead
    of having to go out to global memory on the device. This allows for a far more
    flexible programming model and is more akin to the CPU programming model most
    programmers are familiar with.
  prefs: []
  type: TYPE_NORMAL
- en: One of the aspects of GPU design that differs significantly from CPU design
    is the SIMT model of execution. In the MIMD model, there is separate hardware
    for each thread, allowing entirely separate instruction streams. In the case where
    the threads are processing the same instruction flow, but with different data,
    this approach is very wasteful of hardware resources. The GPU thus provides a
    single set of hardware to run *N* threads, where *N* is currently 32, the warp
    size.
  prefs: []
  type: TYPE_NORMAL
- en: This has a significant impact on GPU program design. SIMT implementation in
    the GPU is similar to the old vector architecture SIMD model. This was largely
    abandoned in the early 1970s when the ever-increasing speed of serial CPUs made
    the “hard” programming of SIMD machines less than appealing. SIMT solves one of
    the key issues, in that programmers are no longer forced to write code in which
    every thread follows the same execution path. Threads can diverge and then converge
    at some later point. The downside of this flexibility is that there is only one
    set of hardware to follow multiple divergent program paths. Thus, each path must
    be executed in turn, or serialized, until the control flow converges once more.
    As a programmer you must be aware of this and think about it in the design of
    your kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll come to one other significant difference between CPUs and GPUs.
    On the CPU model, there is serial control flow. Executing an instruction that
    requires a number of cycles to complete will stall the current thread. This is
    one of the reasons why Intel uses hyperthreading. The hardware internally switches
    to another thread when the current one stalls. GPUs have not just one other thread,
    but are designed to have thousands of other threads that they can potentially
    switch to. Such a stall happens as a result of both instruction latency and memory
    latency, that is, where the processor is waiting on the completion of an operation.
    The threading model is designed to hide both.
  prefs: []
  type: TYPE_NORMAL
- en: However, the GPU has one other benefit in that it uses lazy evaluation. That
    is, it will not stall the current thread until there is an access to the dependent
    register. Thus, you may read a value into a register early in the kernel, and
    the thread will not stall until such time as (sometime later) the register is
    actually used. The CPU model stalls at a memory load or long latency instruction.
    Consider the following program segments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Segment 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If we look at the first segment, the program must calculate the address of `src_array[i]`,
    then load the data, and finally add it to the existing value of `sum`. Each operation
    is dependent on the previous operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Segment 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If we look at the second segment, we iterate in steps of four. Four independent
    `sum` values are used, allowing four independent summations to be computed in
    the hardware. How many operations are actually run in parallel depends on the
    number of execution units available on the processor. This could be execution
    units, in terms of processor cores (using threads), and/or execution units within
    a superscalar processor design.
  prefs: []
  type: TYPE_NORMAL
- en: 'Segment 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, looking at the third segment, we move the load from memory operations
    out of the computation steps. Thus, the load operation for `a1` has three further
    load operations after it, plus some array index calculations, prior to its usage
    in the `sum1` calculation.
  prefs: []
  type: TYPE_NORMAL
- en: In the eager evaluation model used by CPUs we stall at the first read into `a1`,
    and on each subsequent read. With the lazy evaluation model used by GPUs we stall
    only on consumption of the data, the additions in the third code segment, if that
    data is not currently available. As most CPU and GPU designs are superscalar processors,
    using pipelined instructions, both benefit from such an approach within a single
    thread of execution.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms that work best on the CPU versus the GPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many hundreds of computer science algorithms that for decades have
    been developed and optimized for serial CPUs. Not all of these can be applied
    easily to parallel problems. However, the vast majority of problems exhibit parallelism
    in one form or another. A significant number of problems can be broken down into
    operations on a dataset. In many cases, these operations are inherently parallel
    if viewed from either a data or task parallelism viewpoint.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important algorithms in parallel work is something called *scan*,
    otherwise known as prefix sum. In the world of serial computing this does not
    exist as it’s not needed. Suppose we have a variable number of elements per output
    of some function. We could allocate a fixed amount of storage per output, such
    as an array, but this would mean there would be gaps in the memory. Output 0 might
    generate 10 entries, output 1, 5 entries, and output 3, 9 entries. We’d need an
    array with at least 10 entries, so we would have 6 wasted slots.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix sum stores, in a separate array, the number of elements used for each
    output. The actual data is then compressed (i.e., all the blanks removed) to form
    a single linear array. The problem we have now is where does output for thread
    2 write its values to? To calculate the output index for each output, we simply
    add up all the outputs prior to the current one. Thus, output 2 must write to
    array index 10 as output 1 wrote 10 elements (0…9). Output 2 will write 5 elements
    (10…14), so output 3 will start writing at element 15, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We covered in [Chapter 6](CHP006.html) on memory access an example using Sample
    Sort, which uses prefix sum, so I will not repeat here how they can be calculated
    in parallel. The important point to understand is that through the use of prefix
    sum we can convert a great many algorithms to *N* independent outputs. It’s important
    that we can write outputs independently and are not limited by atomics, in effect,
    contention of resources. Such limits, depending on how overloaded they are, can
    severely slow a kernel execution.
  prefs: []
  type: TYPE_NORMAL
- en: Not all parallel architectures are created equal. Many parallel programs and
    parallel languages assume the MIMD model, that is, that threads are independent
    and do not need to execute in groups (or warps) as on the GPU. Thus, not even
    all parallel programs can work on GPUs unchanged. In fact, this has been one problem
    with parallel programs to date; optimization for a specific architecture often
    ties the application to that particular hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Standards like MPI and OpenMP don’t really fit well to the GPU model. OpenMP
    is perhaps the closest, in that it requires a shared view of memory. In OpenMP
    the compiler takes care of spawning threads that share a common data area. The
    programmer specifies which loop can be parallelized through various compiler pragmas
    and the compiler takes care of all that nasty “parallel stuff.” MPI, on the other
    hand, considers all processes to be identical and is more suited to clusters of
    nodes than single-node machines.
  prefs: []
  type: TYPE_NORMAL
- en: You might take the approach of allocating one GPU thread per MPI process, or
    one block per MPI process. Neither would work particularly well on the GPU, unless
    you could identify that groups of MPI processes were, in fact, following the same
    execution flow and could combine them into warps on the GPU. Typically, MPI is
    implemented as shared CPU/GPU pairs with the CPU handling the network and disk
    input/output (I/O). Implementations using GPU Direct allow transfers to certain
    InfiniBand network cards via a common shared-memory host page. Direct peer-to-peer
    (P2P) transfers over the PCI-E bus without the use of host memory is preferable,
    however. The RDMA (remote DMA) is a feature of the new Kepler architecture that
    enables such features and thus makes GPUs much more of a standalone peer on such
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: With GPUs being included in an ever-higher number into data centers and supercomputer
    installations, both OpenMP and MPI will inevitably evolve to accommodate hardware
    designed to accelerate computations. In [Chapter 10](CHP010.html) we discuss the
    use of OpenACC, the directive-based approach to GPU computing. The OpenMP4ACC
    (OpenMP for accelerators) standard may well move such directives into the mainstream
    OpenMP standard.
  prefs: []
  type: TYPE_NORMAL
- en: With the GPU you have to consider that there are a limited number of threads
    that can easily work *together* on any given problem. Typically, we’re looking
    at up to 1024 threads on Fermi and Kepler, less on older hardware. In reality,
    any reasonably complex kernel is limited to 256 or 512 threads due to register
    usage limitations. The interthread communication considerations dominate any decomposition
    of the problem. Interthread communication is performed via high-speed shared memory,
    so threads in the same block can communicate quickly and with little latency.
    By contrast, interblock communication can only be performed via separate kernel
    invocations, and global memory that is an order of magnitude slower. Kepler also
    extends this model to allow interwarp-based communication without the use of shared
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: The other major consideration for GPU algorithms is the memory available on
    the device. The largest single GPU memory space available is 6 GB on the Tesla
    M2090 cards. Compared with typically 16 to 64 GB on the host, this may be problematic.
    However, this can be solved by using multiple GPU cards, with many high-end motherboards
    able to take up to four PCI-E cards, thus providing up to 24 GB per node of GPU
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Recursion is also problematic on GPUs, as it’s only supported on compute 2.x
    GPUs, and then only for `__device__` functions and not `__global__` functions.
    The upcoming dynamic parallelism feature found in the Kepler K20 design will help
    in many respects with recursive algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Many CPU algorithms make use of recursion. Often it’s convenient to break down
    a problem into a smaller problem that is then broken down further and so on until
    it becomes a trivial problem. Binary search is a classic example of this. Binary
    search splits a sorted list of numbers in half and simply asks the question of
    whether the data we’re looking for exists in the left or right set. It then repeats
    the split until either the item is found or the problem becomes just two items
    and is thus trivial to solve.
  prefs: []
  type: TYPE_NORMAL
- en: However, any recursive algorithm can also be represented as an iterative algorithm.
    The binary search problem just mentioned is shown as an iterative solution within
    the sample sort example (see [Chapter 6](CHP006.html)). Quick sort is also a common
    example of an algorithm that is typically implemented recursively. The algorithm
    picks a pivot point and then sorts all items less than the pivot point to the
    left and less than or equal to the pivot point to the right. You now have 2 independent
    datasets that can be sorted by two independent threads. This then becomes 4 threads
    on the next iteration, then 8, then 16, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The GPU kernel invocation requires a *fixed* number of threads. It cannot currently
    exploit dynamic parallelism, although this will change with the Kepler K20 release.
    Dynamic parallelism is where the amount of parallelism in the problem changes
    over time. In the quick sort problem it grows by a factor of two at every level.
    In path finding–type problems, discovery of a new node may introduce 30,000 or
    more additional paths into a problem.
  prefs: []
  type: TYPE_NORMAL
- en: How do you replicate such algorithms on a GPU? There are a number of approaches.
    The easiest is when the parallelism scales in some known manner, as with quick
    sort. You can then simply invoke one kernel per level or one kernel per *N* levels
    of the algorithm back-to-back in a single stream. As one level finishes, it writes
    its state to global memory and the next kernel execution picks up on the next
    level. As the kernels are already pushed into a stream ready to execute, there
    is no CPU intervention needed to launch the next stream. See [Figure 7.1](#F0010).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-01-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.1 Kernel invocations for a recursive algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Where the parallelism grows by some indeterminate amount per iteration, you
    can also store the state in global memory. You have to then communicate back to
    the host the number of the amount of parallelism that the next iteration will
    explore. You can do this with an atomic write to shared memory within the block
    and then an atomic add to global memory prior to block completion. Then use a
    `memcpy` to copy the data back to the host that can use this to adjust the next
    kernel launch.
  prefs: []
  type: TYPE_NORMAL
- en: As an example with the first level of quick sort, you can use one block of data
    with a single thread. You then continue invoking single-thread block kernels until
    you reach some multiple of the number of SMs on the GPU. At the point where you
    would saturate the number of blocks on the SM, up to 16 blocks per SM, you extend
    the number of threads per block. At the point you reach 256 threads per block,
    you start again extending the number of blocks.
  prefs: []
  type: TYPE_NORMAL
- en: This approach, although relatively easy to implement, has some disadvantages.
    First, at least initially there is not enough work to saturate the GPU. With just
    one thread at the first level, the kernel overhead is significant. Even at level
    four, we’re invoking just eight blocks, filling half the 16 SMs on a GTX580 device.
    Not until we reach level five would we have one block per SM. With 16 SMs, eight
    blocks per SM, and 256 threads per SM, we’d need 32 K points before all SMs were
    working at full efficiency. This would require 16 kernel invocations.
  prefs: []
  type: TYPE_NORMAL
- en: With compute 2.x devices this is not such an issue, as the initial few layers
    can simply be calculated using a recursive call, until you reach the desired depth
    into the structure to warrant relaunching the kernel with many more thread blocks.
    An alternative approach is to do some of the initial work on the CPU and only
    go to the GPU once there is enough parallelism in the problem. Don’t think everything
    has to be done on the GPU. The CPU can be a very useful partner, especially for
    this type of less-parallel work.
  prefs: []
  type: TYPE_NORMAL
- en: One other solution to these types of problems is to use a special type of scan
    operation called a segmented scan. With a segmented scan you have a regular scan
    operation over a dataset (`min`, `max`, `sum`, etc.) plus an additional array
    that splits the source array into variable size blocks. A single thread or multiple
    threads are assigned per region to calculate the operation. As the additional
    array can also be updated at runtime, this can reduce the need to invoke multiple
    kernels if the segmented scan can be kept within a single block. Otherwise, you
    might just as well adopt the simpler solution, which in many cases works just
    as well and allows the flexibility of changing the number of threads/blocks as
    the problem grows and shrinks in parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: All of these approaches try to deal with problems GPUs were not natively designed
    to deal with. As a programmer you should be aware of how well an algorithm does
    or does not fit the design model of the hardware. Recursive problems with today’s
    GPUs are often best framed as iterative problems. Selecting an algorithm that
    is appropriate to the hardware and getting the data in the correct layout is often
    key to good performance on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Processing Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a typical data acquisition you will get data that is interesting, periods
    of data of no interest, and noise on the signal. One simple way of removing noise
    is to filter data above or below some threshold. With the dataset shown in [Figure
    7.2](#F0015), we’ve placed a white line to show where the threshold level has
    been set. As you raise the threshold, you filter out low levels of noise. At the
    far right of the acquisition data you may wish to remove it altogether because
    you are only interested in the data peaks.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-02-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.2 Sample data and threshold level.
  prefs: []
  type: TYPE_NORMAL
- en: With a dataset that you expect to have a very small number of items being filtered
    you can easily append to the same data list, as the frequency of the append operation
    itself is very low. However, as the frequency of the filtered data becomes higher,
    the contention for the single list becomes a bottleneck. While this approach may
    work for a small number of parallel operations, say up to four that you might
    find on a quad-core CPU, the write to a single list, with locking approach does
    not scale.
  prefs: []
  type: TYPE_NORMAL
- en: A much better approach is to have a number of lists and then combine the lists
    together at a later stage. In fact, almost all parallel data processing algorithms
    use this approach in one way or another to avoid the serial bottleneck trying
    to update common data structure causes. This approach also maps very well to the
    model CUDA uses to decompose problems, the tiling approach.
  prefs: []
  type: TYPE_NORMAL
- en: We should also recognize that a filtering operation is actually a common parallel
    pattern, a split operation. A split operation takes a given dataset and splits
    it into *N* parts based on some primary key. In our filtering example we’re using
    the threshold condition as the primary key and trying to extract the data that
    is above a given threshold. We may or may not be interested in keeping the data
    that is below the threshold. The split operation simply generates two lists, one
    matching some criteria and the other for the data that did not match.
  prefs: []
  type: TYPE_NORMAL
- en: When performing such an operation in parallel, we have a number of considerations.
    The first problem is we do not know how many data items will meet the matching
    criteria and how many would therefore be on the nonmatching list. The second is
    that we have many processing elements that need to cooperate in some way to build
    an output list. Finally, any ordering present in the original dataset must usually
    be maintained.
  prefs: []
  type: TYPE_NORMAL
- en: The scan primitive is incredibly powerful and can be used in a number of data
    processing scenarios. Suppose, for example, we have a list of students in a database
    in no particular order. We might want to extract, from that student list, all
    students who are in class CS-192\. We thus end up with two datasets, those matching
    the criteria and those that do not.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have a weather station near the equator that is collecting the temperature
    once per minute over several years. We might want to know how many sample points,
    or minutes, the temperature was in excess of 40 degrees centigrade over the sample
    period.
  prefs: []
  type: TYPE_NORMAL
- en: Equally, the data we are looking at may be financial data—for example, the value
    of transactions. You might wish to screen the data to know if there are transactions
    over a certain value, and how many. Certain high-value transactions may have a
    regulatory requirement to report or record, for example, to avoid money laundering.
    Your company policy may also dictate that transactions over a certain value require
    some additional checks. We want to extract from a vast set of data, easily and
    quickly, those that are of interest.
  prefs: []
  type: TYPE_NORMAL
- en: If the data is data from a scientific instrument, you may wish to screen the
    packet of data for “interesting” anomalies. Those packets that contain some anomaly
    are forwarded for further analysis, while the regular packets are sent elsewhere
    or discarded. How we define “interesting” varies according to the application,
    but the fundamental need to be able to scan and filter data is something we find
    in many domains.
  prefs: []
  type: TYPE_NORMAL
- en: Scanning one million data elements on a CPU can be time consuming. It’s the
    standard “for *i* equals 0 to size of dataset” problem. Using a GPU we can scan
    the dataset in parallel. If the dataset is large, the only limit to this is the
    number of GPUs we can assign to the problem. As the largest GPU card to date,
    the Tesla M2090 can hold 6 GB of data, however, you are limited to a problem size
    of 18–24 GB per node before you need to use host or even disk-based storage.
  prefs: []
  type: TYPE_NORMAL
- en: Next we will look at using some of the less well-known features of CUDA to address
    data processing. This is, of course, applicable to any form of data as almost
    all problems involve processing input data in one form or another.
  prefs: []
  type: TYPE_NORMAL
- en: Using ballot and other intrinsic operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As of compute 2.0 devices, NVIDIA introduced a very useful function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This function evaluates the predicate value passed to it by a given thread.
    A predicate, in this context, is simply a true or false value. If the predicate
    value is nonzero, it returns a value with the *N*th bit set, where *N* is the
    value of the thread (`threadIdx.x`). This atomic operation can be implemented
    as C source code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The nonatomic version just shown is a similar speed to the intrinsic version,
    but will work on all compute versions. We’ll use it later to provide backward
    compatibility with older hardware.
  prefs: []
  type: TYPE_NORMAL
- en: The usefulness of ballot may not be immediately obvious, unless you combine
    it with another atomic operation, `atomicOr`. The prototype for this is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It reads the value pointed to by `address`, performs a bitwise `OR` operation
    (the | operator in C) with the contents of `val`, and writes the value back to
    the address. It also returns the old value. It can be used in conjunction with
    the `__ballot` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '`                              __ballot(data[tid] > threshold) );`'
  prefs: []
  type: TYPE_NORMAL
- en: In this call we use an array that can be either in shared memory or global memory,
    but obviously shared memory is preferable due to it’s speed. We write to an array
    index based on the warp number, which we implicitly assume here is 32\. Thus,
    each thread of every warp contributes 1 bit to the result for that warp.
  prefs: []
  type: TYPE_NORMAL
- en: For the predicate condition, we asked if the value in `data[tid]`, our source
    data, is greater than a given threshold. Each thread reads one element from this
    dataset. The results of each thread are combined to form a bitwise `OR` of the
    result where thread 0 sets (or not) bit 0, thread 1 sets (or not) bit 1, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then make use of another compiler intrinsic, the `__popc` function.
    This returns the number of bits set within a 32-bit parameter. It can be used
    to accumulate a block-based sum for all warps in the block, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we can accumulate for a given CUDA block the number of threads in every
    warp that had the condition we used for the predicate set. In this example, the
    condition is that the data value was larger than a threshold. A block-based sum
    is useful in many algorithms, but a CUDA kernel will consist of many blocks, typically
    thousands. If you’d like to know how many data items match the predicate across
    the whole dataset, you have to add up the sums from each block.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of choices for doing this. For a small number of blocks,
    we can simply ship the resultant block counts back to the CPU and have the CPU
    perform a summation. This may be a useful strategy if the CPU would otherwise
    be idle and there are other streams of GPU work that could be performed on the
    GPU (see [Chapter 8](CHP008.html) for a discussion of how to do this).
  prefs: []
  type: TYPE_NORMAL
- en: Another strategy is to write all the partial sums from the blocks to global
    memory on the GPU. However, to complete a summation of all the individual block
    components, all the blocks in all the SMs have to have completed the evaluation
    of the predicate. The only way to ensure this is to complete the current kernel
    and invoke another one. Then all global memory values previously written have
    to be re-read in some way, likely via a parallel reduction, and a final sum calculated.
    Although this might be the way taught in traditional CPU parallel programming,
    it’s not the best way from a performance perspective on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the number of blocks that are resident on a Fermi SM, up to eight
    blocks *can* be resident, although typically you see a maximum of six. Let’s assume
    the maximum for now is eight blocks. There are 16 SMs in the largest Fermi device.
    Thus, there are a maximum of 8 × 16 = 128 blocks resident on the device at any
    one time. We can therefore simply accumulate to a *single value* in global memory
    using the `atomicAdd` function as we produce only one update per block.
  prefs: []
  type: TYPE_NORMAL
- en: Statistically, the probability of more than one block arriving at the atomic
    add instruction, at the same time, is quite small. Given that the memory transactions
    to read the source data will likely arrive in sequence, this in fact nicely sequences
    the execution flow within the SMs and consequently ensures the atomic add operations
    do not compete with one another.
  prefs: []
  type: TYPE_NORMAL
- en: Using this technique takes around 5 ms to scan one million elements, excluding
    the transfer time to and from the GPU. We exclude the transfer time because it’s
    likely the data will remain entirely resident on the GPU. Consequently, we could
    process around two hundred million queries like this on the dataset per second.
    In practice, the predicates may be much more complex and we’ll look at how this
    impacts the performance later.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the moment, let’s look at the complete function to do this in a little
    more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We declare a couple of functions: a device function that calculates the predicate
    condition and a global function that provides a wrapper to call the ballot function.
    To the ballot function we pass the dataset to search through an area of memory
    to place the block results into, an area of memory to place the accumulated result
    into, the number of elements to process, and finally a threshold for the comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice with such a format we could easily implement other operations such as
    less than, equal to, etc. by writing a new predicate function and wrapper, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '` const u32 threshold,`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`  // of bits set from each warp.`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The first part of the function calculates the absolute thread ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This function is designed to work with a single dimension of threads. With large
    datasets (around 16 million elements plus), we’ll need to make use of another
    dimension, as we would otherwise launch more than 64K blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We then calculate our current warp by simply dividing (right shifting) the current
    thread index by 32\. We do the same with the block dimension to work out the number
    of warps in the current block.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We then have to check if our absolute thread ID, `tid`, is within the dataset.
    In cases where the number of elements is not a power of two the `tid` calculation
    for the last block would end up after the end of the source data. We neither want
    to read or write out-of-bounds arrays, so this check is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this also implicitly means we cannot perform a `__syncthreads` operation
    within this `if` block, as all threads, even those off the end of the array, must
    participate in such a synchronization operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Next we have to clear the value of the shared memory we’re about to use. Shared
    memory can hold the value from the last kernel run and is not implicitly initialized
    to zero. As we need only a single writer, the first thread in each warp clears
    the value. Note we do not require any synchronization here because the first thread
    in every warp does the write. Branching within a warp in this way causes the other
    threads to implicitly wait at the end of the `if` statement.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We can now have every thread in every active warp call the `atomicOr` function
    with the address of the shared memory element for this current warp. We pass to
    the `OR` operation the value returned from the `__ballot` call. We pass to `__ballot`
    the return value from calling the `predicate_func` function pointer, passing it
    the two data items to evaluate. This then jumps off and does the evaluation, in
    this case calling the `predicate_gt` function defined earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now we have to wait for all warps within the block to execute before we can
    do the second part, the block level accumulate.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As the maximum number of threads per block is 1024, the maximum number of warps
    per block is 32 (1024 ÷ 32 = 32). Thus, we can process the accumulate using just
    a single warp. We could have used thread 0 from each warp as we did before, but
    in this case we want the other warps to complete, not be left executing a single
    thread each.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Again we have no idea of the existing value in the shared memory element we’re
    about to use to accumulate into, so we need to zero it. Note that, as we now have
    only one warp running, no synchronization is required. Thread 0 will enter the
    condition while threads 1…31 will pass over it and implicitly wait for thread
    0 to reconverge with them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We now add to the block-based shared memory accumulator the number of bits that
    were set in the result produced for the other warps in the block. These are in
    adjacent elements of shared memory, one element per warp. Thus, there are no read
    shared memory bank conflicts. However, the threads need to serialize the writes
    to the accumulator to ensure correctness. As you typically have 256 threads per
    block, this gives eight warps. This serialization does not really warrant a parallel-type
    reduction. However, with a larger number of warps a parallel reduction might work
    slightly faster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As we need only one writer, we select thread 0 to perform the next operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we read the block level accumulator from shared memory into a register,
    as we’ll make use of it twice. We then write the block result to global memory,
    something we only have to do if we’re interested in the block results in addition
    to the overall accumulated result.
  prefs: []
  type: TYPE_NORMAL
- en: We then call the `atomicAdd` function to add into the single global accumulator
    the overall result. Note that we cannot zero the result of the final accumulator
    in any of the blocks. It must be done by the host prior to the call to the function.
    The reason for this is simple. The blocks, and the warps within those blocks,
    may execute in any order. Thus, we cannot say something like `if (threadIdx.x
    == 0) && (blockIdx.x ==0)` then zero the accumulator. Doing this *may* work because
    it just so happens that warp 0 of block 0 executed first, but this is poor practice.
    CUDA’s execution model is such that blocks can be, and are, executed out of order.
    You cannot assume any implicit order of block execution.
  prefs: []
  type: TYPE_NORMAL
- en: With a minor modification to supply the missing `__ballot` function for the
    GTX 260 (a compute 1.3 device), we can run this kernel on a range of devices.
    Note we can’t use the 9800GT as it’s a compute 1.1 device and therefore does not
    support shared memory based atomic operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: What is perhaps strange at first glance is that the GTX260 is 50% faster than
    the more modern GTX460\. However, the GTX260 has approximately four times the
    number of SMs. Each SM has its own internal set of shared memory so the GTX260
    has a much wider bandwidth to the shared memory than the GTX460.
  prefs: []
  type: TYPE_NORMAL
- en: We can also make one small modification. As we’re using the `atomicOr` function
    we actually don’t need the additional atomic functionality of `__ballot`, so we
    can in all cases use the nonatomic version. This revises the timing a little.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You can see that this drops the time significantly on Fermi devices, as the
    GTX260 is already using the nonatomic version. The time for the GTX470 is reduced
    by 15% and the time for the GTX460 is reduced by 21%. This slightly improved time
    allows us to scan some 1632 million elements per second on a single GTX470\. This
    will, however, be reduced if we use more complex predicates and/or a dataset requiring
    more than one block dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a feel for this, what happens to the timing if we change the results
    to within a boundary, rather than simply larger than a threshold? For this we
    need to modify the predicate condition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we have introduced another condition, potentially increasing significantly
    the overall timing. What is the effect in practice?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the effect of adding another condition is marginal at best,
    with a 0.1 ms difference in execution time. This would imply the predicate could
    become reasonably complex without causing a significant slowdown.
  prefs: []
  type: TYPE_NORMAL
- en: The fact that we can use very complex predicate conditions allows for very complex
    operations to be coded efficiently on a GPU. Even codes where the data points
    must be gathered in some way can use such a set of primitives. All we need to
    do in such cases is adjust the predicate to take more data.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll pick up the example we looked at in [Chapter 6](CHP006.html), sample sort,
    and use it to look at how we can use profiling tools to identify problems in the
    implementation of a given algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The sample sort example already contains a number of timing elements, which
    we can use to adjust various parameters. Please re-read the sample sort example
    in [Chapter 6](CHP006.html) if you’re not familiar with how sample sort works.
  prefs: []
  type: TYPE_NORMAL
- en: The major parameters are the number of samples and the number of threads. If
    we ask the program to explore the possible search space, doubling the number of
    samples per iterations and using 32, 64, 128, or 256 threads, we find the following
    promising cases.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`Sort Bins Time -    CPU:   37.38  GPU:57.57  39.40  44.81  41.66`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: If we view one example as a pie chart, it makes it easy to see where we’re spending
    our time ([Figure 7.3](#F0020)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-03-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.3 Sample sort time distribution, 16 K samples.
  prefs: []
  type: TYPE_NORMAL
- en: So it’s clear from the chart that approximately three-quarters of the time is
    used for sorting and one-quarter for setting up the sample sort. However, as we
    increase the number of samples used, this changes ([Figure 7.4](#F0025)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-04-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.4 Sample sort time distribution, 64 K samples.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from [Figure 7.4](#F0025), suddenly the time to sort the sample
    jumps to around one-third of the total time. We also see quite a lot of variability
    depending on the number of samples and the number of threads used. We’ll concentrate
    on optimizing the middle case, 32 K samples using 64 threads per block.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel Nsight provides a very useful feature listed under the “New Analysis
    Activity.” Parallel Nsight is a free debugging and analysis tool that is incredibly
    useful for identifying bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: The first option in Nsight to be sure is to select the “Profile” activity type
    ([Figure 7.5](#F0030)). By default this will run a couple of experiments, “Achieved
    Occupancy” and “Instruction Statistics.” Running these on the sample sort example
    produces a summary. At the top of the summary page is a dropdown box. Selecting
    “CUDA Launches” shows some useful information, as shown in [Figure 7.6](#F0035).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-05-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.5 Parallel Nsight launch options.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-06-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.6 Parallel Nsight analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The first view is the “Occupancy View” (bottom left corner in [Figure 7.6](#F0035)).
    What you should notice here is that there is a summary of the launch parameters
    for the kernel and what factors are limiting occupancy in red. In our case, the
    block limit per device, eight blocks, is limiting the maximum number of active
    warps on the device. Remember that warps are groups of threads from which the
    scheduler can select. The scheduler switches between warps to hide memory and
    instruction latency. If there are not enough warps resident, then this *may* limit
    performance if the GPU has no other warps to run.
  prefs: []
  type: TYPE_NORMAL
- en: We have launched around 16 warps, when the maximum per device is 48, achieving
    one-third of the maximum occupancy of the device. This would suggest that we should
    improve occupancy by increasing the number of warps per device, which in turn
    means increasing the number of threads. However, measured results show this produces
    the opposite effect, actually reducing performance.
  prefs: []
  type: TYPE_NORMAL
- en: The second screen that is interesting is the “Instruction Stats” ([Figure 7.7](#F0040)).
    What is noticeable here (IPC section) is there is a large block of issued instructions
    that were never executed. The executed instructions are shown, on screen, in the
    pink section on the first bar chart on the bottom left where the lower line is
    drawn through the bars. The blue bars indicate that instructions are being reissued
    due to serialization. Serialization is where, for whatever reason, threads are
    not able to execute as a complete warp (set of 32 threads). This is usually associated
    with divergent control flow, uncoalesced memory accesses, or operations that have
    limited throughput because of conflicts (shared memory or atomics).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-07-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.7 Parallel Nsight analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Also notice the distribution of work to the SMs is uneven (SM Activity block,
    [Figure 7.7](#F0040)). We launched 512 blocks of 64 threads. Given 14 SMs on the
    GTX470 device being used, we’d expect just over 36 blocks (72 warps) per SM. In
    practice, some SMs got 68 warps while others got 78 warps (Warps Launched section,
    [Figure 7.7](#F0040)). Also notice that, despite being given the same number of
    warps, some SMs take longer, implying all warps are not being given an equal amount
    of work in terms of execution time.
  prefs: []
  type: TYPE_NORMAL
- en: When we move to 256 threads per block, the variability we see in issued versus
    executed instructions grows. The number of scheduled blocks drops from eight to
    just three due to the use of 34 registers per thread. Although not an issue with
    64 threads per block, 256 threads per block limits the overall number of blocks
    that can be scheduled per SM. However, despite this, the number of warps scheduled
    climbs to 24 instead of 16, providing a 50% occupancy rate. Does further increasing
    occupancy help?
  prefs: []
  type: TYPE_NORMAL
- en: Simply asking the compiler to use a maximum of 32 registers (the `-maxregcount=32`
    compiler flag) proves to be a terrible optimization. The compiler then uses just
    18 registers, allowing for six blocks to be scheduled, the maximum permitted.
    This increases the theoretical occupancy to 100%, but results in an increase in
    execution time from 63 ms to 86 ms.
  prefs: []
  type: TYPE_NORMAL
- en: This is due to the GPU having to push registers into “local” storage, which
    on Fermi is the L1 cache and global memory on the earlier-generation GPUs. On
    earlier-generation GPUs the time taken to use global memory would more than eliminate
    any gain due to better occupancy. On Fermi, pushing more data into the L1 cache
    reduces the available cache space for other purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also go down the opposite path, to increase register usage. The original
    C code for the function that performs the sort bins time output shown earlier
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`   else`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at the PTX code generated for the kernel (see [Chapter 9](CHP009.html)
    for details on how to do this) we see the following code extract:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This equates to the C source code line for
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: There are a number of issues here. First, array indexing is causing the use
    of a multiply instruction. As `elem` is used immediately in the next C instruction
    to branch, the data load needs to have completed, so the thread stalls at this
    point. Multiply and divide instructions usually require many cycles to complete
    the instruction pipeline and there may be limited execution units that can perform
    such complex instructions.
  prefs: []
  type: TYPE_NORMAL
- en: We can replace all array indexes with a pointer to the array and then increment
    the pointer after each usage. Thus, the code extract we looked at earlier becomes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This means the compiler now translates this to the following PTX code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`mov.s64   %rd14, %rd22;`'
  prefs: []
  type: TYPE_NORMAL
- en: We still have a total of six instructions, but now the first set does the load
    and the second set the increment of the pointer. The increment of the pointer
    is now a simple addition, much simpler than a multiply, and the result is not
    needed until the next iteration of the loop.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the same strategy to the other array operations yields a reduction
    in execution time from 39.4 ms to 36.3 ms, a drop of 3 ms or around 10%. However,
    what about this variability in work done by each warp? Where does this come from?
  prefs: []
  type: TYPE_NORMAL
- en: Sample sort sorts data into blocks, or bins, which we independently sort using
    a single warp. If we do a dump of the values from single warp, we see something
    interesting.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: There are a significant number of bins where the entries are zero. There are
    others where the total number of entries is very large. As one thread processes
    each bin, to iterate over the entire dataset, we need to iterate for the maximum
    of the bins from a given warp. The first warp shown has a maximum value of `0x9d`
    (157 decimals) and a minimum value of zero. By the time we’re at iteration 157,
    only a single thread from the entire warp is active. We see this reflected in
    the large difference between issued and executed instructions we saw earlier (Instructions
    per clock, [Figure 7.7](#F0040)). It’s the bins with very large iteration counts
    that are taking the time.
  prefs: []
  type: TYPE_NORMAL
- en: We see a reduction in the execution time of the radix sort when we double the
    number of samples, because the peaks are pushed down and split out into more bins.
    However, sorting the samples then becomes the dominating issue. The problem is
    the distribution of samples to bins.
  prefs: []
  type: TYPE_NORMAL
- en: The large number of zero bins is actually caused by duplicates in the sample
    dataset. The source data array is filled with data via a simple call to `rand()`,
    which returns a not-so-random number. After a certain period these repeat. As
    the samples are selected at a uniform distance to one another, the sample set
    contains many duplicates. Removing this error in the random dataset removes almost
    all zeros from the bin count, but has an unintended effect that the execution
    time now climbs back up to the original 40 ms.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can, however, apply another technique to this problem, that of loop unrolling
    and tail reduction, both of which we cover in [Chapter 9](CHP009.html). We replace
    the following code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '`u32 i=start_idx;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Suppose the difference between `start_idx` and `end_idx` is 32, one of the common
    cases. The number of iterations in the first loop will be 32\. However, by unrolling
    the loop by a factor of four, we reduce the number of operations by a factor of
    four, that is, eight iterations. There are a few other important effects of loop
    unrolling. Notice we need, in the case of a factor of four, three additional registers
    to store three additional data points. We also need to handle the end loop condition
    where we may still have zero to three elements to process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the PTX code we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We’re doing something quite important here, introducing instruction level parallelism
    through the use of independent elements per thread. [Table 7.1](#T0010) and [Figure
    7.8](#F0045) show the effect loop unrolling has.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.1 Unroll Level Versus Time and Register Usage
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000077tabT0010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![image](../images/F000077f07-08-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.8 Unroll level versus time and register usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see from [Table 7.1](#T0010) and [Figure 7.8](#F0045), introducing
    a small amount of thread level parallelism significantly drops the execution time
    of the radix sort. However, notice something else: The number of registers never
    climbs above 44, even though we can use up to 63 in Fermi. What is happening at
    this point is the compiler introduces a call stack and no longer grows the number
    of registers used.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ve applied a couple of optimization techniques to the source code, which
    you might reasonably expect a compiler to automatically apply. We’ll not remove
    any of these, so any gain should come from the compiler adding additional optimizations.
    Let’s see if this is the case by switching to the release mode, which enables
    all the compiler optimizations by default ([Table 7.2](#T0015) and [Figure 7.9](#F0050)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.2 Debug Versus Release Version Timing
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000077tabT0015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![image](../images/F000077f07-09-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.9 Debug versus release timing.
  prefs: []
  type: TYPE_NORMAL
- en: We see from [Table 7.2](#T0015) and [Figure 7.9](#F0050) a very similar pattern
    to the release or optimized version, indicating that the optimizations we have
    just applied are not themselves applied automatically by the compiler. What is
    also noticable is again we see the same pattern, that four elements per thread
    helps considerably, but beyond this the effect is marginal. Notice, even with
    optimizations enabled, the compiler does not automatically unroll the loop. Thus,
    we’ll stick with manual unrolling by four, as the additional speed versus extra
    register usage is not a good tradeoff.
  prefs: []
  type: TYPE_NORMAL
- en: You might have expected the compiler to have pulled, or hoisted, out the read
    operations and placed them at the start of an unrolled loop. In many cases it
    will do this, except in the difficult cases, which are unfortunately all too often
    what we hit. Where you have a read followed by write followed by another read,
    the compiler cannot easily know if the write operation wrote to the same data
    area that is being read from. Thus, it must maintain the read-write-read sequence
    to ensure correctness. As the programmer however, you know if the read operations
    are affected by the preceding write operations and can replace the read-write-read
    sequence with a much more efficient read-read-write sequence.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve now radically changed the timing on one aspect, dropping it from 40
    ms to 25 ms, we should rerun the scan of the problem space to see if this now
    changes the optimum number of samples/threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing that becomes noticable is the release version of QSort is actually
    much faster, over twice the speed in fact. This makes it considerably harder to
    produce a faster sort. However, quick sort is now a large component of the sample
    sort, as we presort the samples on the CPU. Thus, this reduction in execution
    time helps considerably. The best timing is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`Sort Bins Time -    CPU:  62.81  GPU:27.37 25.10 36.28 39.87`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: So in fact both the 16 K and 32 K sample versions come out above even, with
    0.6 ms between them. This is a 4.4× speedup over the CPU-based quick sort. Cache
    utilization is a key factor in play here. See the “Thread Memory Patterns” section
    in [Chapter 9](CHP009.html) where we look at the impact of this.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we used Parallel Nsight to show the impact of altering the number
    of and size of the blocks we used and saw how this could radically affect the
    overall performance. We then drilled down into this data and noticed there was,
    ultimately, a problem with the design of the sample sort. Serialization caused
    through the differing number of elements processed per thread was the cause of
    this. Despite this issue, we could optimize the implementation through thread
    level parallelism by using multiple elements per thread. Enabling additional compiler
    level optimization brought considerable additional benefits to both CPU and GPU
    code.
  prefs: []
  type: TYPE_NORMAL
- en: An Example Using AES
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The AES (Advanced Encryption Standard) is an algorithm used to provide encryption
    in programs like WinZip, Bitlocker, TrueCrypt, etc. Depending on your industry,
    encryption may be something you already use or something that may seem irrelevant.
    Many companies make the mistake of thinking the data they create doesn’t need
    to be kept securely on a local machine. All the nasty programs and hackers are
    outside the company firewall and therefore any data kept locally doesn’t need
    security. This type of thinking is flawed, as very often a machine, employee,
    or contractor may create holes in such a firewall to enable working at home or
    outside the office, etc. Security needs to have a multilayered approach.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of encryption is that we take some data and apply an algorithm to it
    that obscures the data. Thus, the data, or the machine holding that data, such
    as a laptop, can be compromised, lost, or stolen, but the data itself is not accessible.
    Significant numbers of data breaches are a result of compromised machines. Moving
    the protection to the data means that to access it requires a “key.” Applying
    that key and a given algorithm results in the data becoming unencrypted.
  prefs: []
  type: TYPE_NORMAL
- en: Encryption can also be used for secure connections between hosts on an insecure
    network such as the Internet. If you have a distributed application over a public
    network, how do you ensure that if you send a packet of data to another machine
    that packet is not intercepted and changed? Standards such as OpenSSL (Open Secure
    Socket Layer) are used by browsers when logging into secure servers such as those
    for online banking to ensure no one listens in on the exchange of login data.
  prefs: []
  type: TYPE_NORMAL
- en: When you design software, you will need to consider the security aspects of
    it and how data is transmitted to and from various machines in any solution. The
    ITEF (Internet Engineering Task Force), the body that approves new Internet standards,
    requires all standard proposals to include a section on security. The fines levied
    against organizations for loss of consumer or corporate data are significant.
    It therefore pays to have a good understanding of at least some encryption standards
    if you are in any way networking computers or storing sensitive or personal data.
  prefs: []
  type: TYPE_NORMAL
- en: AES is mandated by many U.S. government organizations when storing data. As
    an algorithm in use today, we’ll use this as a case study to see how you might
    approach AES-based encryption using a GPU. However, before we can dive into the
    implementation details, we first need to analyze the algorithm, understand it,
    and look for elements that can be computed in parallel. The AES algorithm contains
    many complexities, yet at the same time is understandable to someone with no cryptographic
    background. It is therefore a useful algorithm to look at to see how we can apply
    some of the techniques discussed to date.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AES is a block-based encryption algorithm. An encryption algorithm is often
    referred to as a *cipher*. Thus, the text to be encoded is referred to as plain
    text when not encoded and cipher text when encoded. To encode plain text into
    cipher text requires an algorithm and a key. The key is simply a series of numbers
    that acts very much like a mechanical key, the algorithm being the lock.
  prefs: []
  type: TYPE_NORMAL
- en: AES supports a number of modes of operation, the simplest being ECB (Electronic
    Cook Book), the one we’ll look at here. AES splits up the data to be encoded into
    a number of blocks 128 bits in length (16 bytes). Each block in ECB mode is independently
    encoded based on a series of values derived from the encryption key. The encoding
    takes place in a series of “rounds,” each of which uses a new derived key to further
    encrypt the data. See [Figure 7.10](#F0055).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-10-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.10 AES overview.
  prefs: []
  type: TYPE_NORMAL
- en: The 128-bit key is independently adapted for each round and is independent of
    the text to be encoded or the previous round of encryption. Thus, the extraction
    of the keys for the various rounds can be done independently of the encoding round
    for the AES algorithm. Usually, as the key is constant for all blocks, this will
    be done before any encryption begins.
  prefs: []
  type: TYPE_NORMAL
- en: 'AES uses 128-, 192-, or 256-bit keys, although the block size (the size of
    the plain text) is always 128 bits. The number of rounds used changes according
    to the key length chosen: 10, 12, and 14 rounds, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: The plain text is represented as a 4 × 4 matrix of byte data, known as the state
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'An encryption round itself consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: • Substitution—Bytes within the 4 × 4 matrix are swapped with other bytes from
    a lookup table.
  prefs: []
  type: TYPE_NORMAL
- en: • Row rotate left—Rows 1, 2, and 3 are rotated left by one, two, or three positions,
    respectively. Row 0 is unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: • Mix columns—Each column has a step applied to diffuse the values.
  prefs: []
  type: TYPE_NORMAL
- en: • Round key—The data is XOR’d with the appropriate current round key extracted
    from the original key.
  prefs: []
  type: TYPE_NORMAL
- en: The initial round, also known as round zero, consists only of the round key
    operation. The final round drops the mix columns operation. Decryption is simply
    the inverse of the encryption process, starting at the last round and working
    backwards to the start.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to implement the algorithm, we need to look in detail at the five key
    aspects, those just shown plus the extraction of the round keys from the original
    128 bit key.
  prefs: []
  type: TYPE_NORMAL
- en: Substitution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The substitution step swaps every byte in the 4 × 4 data block, the state space,
    with a value from a constant lookup table known as the Rijndael s-box.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '` 0xE7, 0xC8, 0x37, 0x6D, 0x8D, 0xD5, 0x4E, 0xA9, 0x6C, 0x56, 0xF4, 0xEA, 0x65,
    0x7A, 0xAE, 0x08, /∗ B ∗/`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: For each of the 16-byte elements in the state space we have to extract out a
    single hex digit. The first digit, or high nibble of the byte, (0…F), is used
    as row reference. The second digit of the byte, or low nibble, is used as the
    column index. Thus, a value of `0x3E` in the state space would result in a row
    value of 3 and a column value of E. If we look up this in the `s_box` table, we
    get `0xB2`. Thus, the byte `0x3E` in the state space is replaced by `0xB2`. The
    same operation is performed for all the other bytes in the state space.
  prefs: []
  type: TYPE_NORMAL
- en: Row rotate left
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this step, rows 1, 2, and 3 are rotated left by one, two, or three positions,
    respectively. Row 0 is left unchanged. A rotate left operation takes the row and
    shuffles all bytes to the left by one position. The byte at the far left wraps
    around and becomes the byte on the far right. In [Figure 7.11](#F0060) I’ve pulled
    out each row to show how the rotation of the bytes works.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-11-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.11 AES row rotate left.
  prefs: []
  type: TYPE_NORMAL
- en: Mix columns
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Rijndael mix column step is a complex piece of code. It multiples the column
    `r` by a 4 × 4 matrix. The matrix is shown in [Figure 7.12](#F0065).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-12-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.12 Mix columns matrix.
  prefs: []
  type: TYPE_NORMAL
- en: A 1 in the matrix means leave the value unchanged. A 2 indicates multiplication
    by 2\. A 3 indicates a multiplication by 2 plus an XOR with the original value.
    In the 3 case, should the resultant value be larger than `0xFF`, then an additional
    XOR with `0x1B` needs to be performed. This is a simplification of Galois multiplication.
    A typical implementation in C code is shown here ([Wikipedia, Jan. 31, 2012](#BIB1)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This is not the most optimal, but the most likely example implementation you
    will find of this standard algorithm. In the preceding code, the input parameter
    `r` points to a 1 × 4 matrix that is a single column from the state space. It
    is copied to a temporary array `a` for use later. An array `b` is generated that
    holds the multiply by 2 (the `<<1`) operation. The multiply by 3 is actually a
    multiply by 2 followed by an XOR (`^`) operation. Thus, the final step becomes
    a series of XOR operations of the original data in `a` plus the result of the
    matrix multiplication in `b`. See [Figure 7.13](#F0070).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-13-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.13 `Mix columns` with column 0 (repeated for columns 1, 2, and 3).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look a little more at this step later, as it’s one of the more time-consuming
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: Add round key
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The round key is the key extracted from the original cipher key for a given
    round or iteration of the encryption algorithm. It’s in the form of a 4 × 4 matrix
    and is simply XOR’d with the current result.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the round keys
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The AES algorithm uses a number of round keys, one for each round. Generating
    the keys is an iterative process where new keys depend on previous ones. The first
    part of the operation is to take the existing key and copy it as key 0, thus generating
    a 4 × 4 matrix providing the single starting round key.
  prefs: []
  type: TYPE_NORMAL
- en: The next *N* round keys must be constructed one at a time. The first column
    of any round key takes the last column of the previous round key as its starting
    point. The operation for the first column in the new key contains some addition
    operations over and above the standard column generation function.
  prefs: []
  type: TYPE_NORMAL
- en: For the first column of the key only, we need to do a column-based rotate such
    that the values move up the column. The value at the top of the column, row 0,
    moves to row 3\. An identical operation is to rotate the row left on the cipher
    data, but instead of a row, the rotate is over a column. We then again use the
    substitution method and the Rijndael s-box to substitute values as we did for
    the cipher text.
  prefs: []
  type: TYPE_NORMAL
- en: The operation for all elements is then the same. The newly calculated value
    must be XOR’d with the key value at index minus 4\. For columns 1, 2, and 3 we’re
    now done. However, column 0 has an addition operation. The first element of column
    zero is then XOR’d with `0x01`, `0x02`, `0x04`, `0x08`, `0x10`, `0x20`, `0x40`,
    `0x80`, `0x1b`, or `0x36`, the RCON value, depending on the current round ([Figure
    7.14](#F0075)).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-14-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.14 AES round key generation (first column).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the first column of round key 1 becomes the next extracted column. The
    calculation of columns 1, 2, and 3 is simpler ([Figure 7.15](#F0080)). The column
    rotation and XOR with the RCON values is dropped. Thus, we simply have an XOR
    with the column at row minus 4\. At column 4, the pattern repeats.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-15-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.15 AES round key generation (columns 1, 2, and 3).
  prefs: []
  type: TYPE_NORMAL
- en: As the key generation always uses values from the previous key, this means the
    keys need to be generated in sequence. This in turn may form the bottleneck of
    any parallel implementation if many keys are needed. Thankfully for most uses,
    only a single set of keys is required. Thus, this step can be performed prior
    to any encoding or decoding and the keys simply stored in an array. As it’s not
    time consuming for a single key, it can be done on either the CPU or GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Serial implementations of AES
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: AES has been the subject of a lot of study. It was designed to run on 8-, 16-,
    or 32-bit machines without significant processing load. However, as we have seen
    from looking at the algorithm, it’s not a simple algorithm to implement. Let’s
    consider some of the design tradeoffs when thinking about optimizing such an algorithm
    for a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Access size
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The first issue is that it is designed around byte-based access, to support
    8-bit simple processors. All modern processors are at least 32-bit designs. Thus,
    if we use just single byte operations, 75% of the space in the register and the
    potential work goes unused. Clearly with a 32-bit processor, an x86 or a Fermi
    GPU, we need to design a solution such that it uses 32 bits.
  prefs: []
  type: TYPE_NORMAL
- en: We can naturally combine a single row into one 32-bit word. We can also combine
    the entire 4 × 4 matrix into a 16-byte vector (128 bits). Such vectors are supported
    by the Intel AVX (Advanced Vector eXtension) instruction set. The GPU `uint4`
    type would also allow for the GPU to fetch and store this data to/from memory
    in a single instruction. However, unlike Intel’s AVX, the GPU has no per thread
    wide vector instructions other than storing to or from memory.
  prefs: []
  type: TYPE_NORMAL
- en: We have to consider that any encoding of the state or key matrix that is larger
    than a single byte would necessitate bit mask and shift operations if the operation
    needed to be individually applied to a single byte. Providing these were not considerable,
    the benefit of less memory reads/writes, though fetching the data in larger transactions,
    would easily outweigh register-based mask and shift operations.
  prefs: []
  type: TYPE_NORMAL
- en: Memory versus operations tradeoff
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With most algorithms it’s possible to trade an increased memory footprint for
    a decreased execution time. It depends significantly on the speed of memory versus
    the cost and number of arithmetic instructions being traded.
  prefs: []
  type: TYPE_NORMAL
- en: There are implementations of AES that simply expand the operations of the substitution,
    shift rows left, and mix columns operation to a series of lookups. With a 32-bit
    processor, this apparently requires a 4 K constant table and a small number of
    lookup and bitwise operations. Providing the 4 K lookup table remains in the cache,
    the execution time is greatly reduced using such a method on most processors.
    We will, however, implement at least initially the full algorithm before we look
    to this type of optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Hardware acceleration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Intel AES-NI extension to the x86 processor instruction set is available
    on most Intel Sandybridge I5 and I7 processors as well as the Westmere-based I7
    Xeon processors and their successors. The AES-NI instruction set consists of the
    following instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: • AESENC (cipher data, round key)—Standard round of encoding completely in hardware.
  prefs: []
  type: TYPE_NORMAL
- en: • AESENCLAST (cipher data, round key)—Last round of encoding completely in hardware.
  prefs: []
  type: TYPE_NORMAL
- en: • AESKEYGENASSIST (round key, cipher key, round number)—Assist in the generation
    of the round keys.
  prefs: []
  type: TYPE_NORMAL
- en: • ASDEC (cipher data, round key)—Standard round of decryption in hardware.
  prefs: []
  type: TYPE_NORMAL
- en: • ASDECLAST (cipher data, round key)—Last round of decryption in hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the entire AES encryption and decryption process can be done entirely
    in hardware. Special 128-bit xmm1 and xmm2 registers are used to contain the operands
    in single registers. We see that in practice when such AES-NI is used with real
    applications, there is something in the order of a 2× or more performance improvement
    over a nonaccelerated processor (Toms Hardware, “AES-NI Benchmark Results: Bitlocker,
    Everest, And WinZip 14,” [http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption,review-31801-7.html](http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption,review-31801-7.html)).
    Of course with handwritten assembler and optimal scheduling conditions over many
    cores, it’s possible to get significantly more. This, however, gives us a feel
    for the likely benefit of coding such a solution and therefore it seems worth
    the effort.'
  prefs: []
  type: TYPE_NORMAL
- en: An initial kernel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s look at an initial kernel for this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: First, we have the function prototype. Here we pass in a pointer to a cipher
    block as a `uint4` vector type. A single `uint4` vector (four integers) is sufficient
    to hold a single set of 16 bytes, the 128-bit cipher data. Next we have the cipher
    key, which is a set of 10 `uint4` keys. Finally, we have a specifier for the number
    of rounds, which we will replace with a fixed value at some point later. Note
    both the `__host__` and `__device__` qualifiers that allow the function to be
    called from both the CPU and GPU.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Next we extract from the `uint4` vector type the four unsigned integer component
    parts.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We next extract individual bytes from the four words into individual registers.
    Note the use of the `u8` type rather than the base C type, allowing an easy redefinition
    of this type. Note also the `EXTRACT` macro, which is used to allow support for
    both big-endian and little-endian representation of the bytes within the 32-bit
    words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '` w0 = (∗cipher_key)[round_num].w;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We then read a set of four values from the key, again from a `uint4` type into
    four 32-bit values.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: The first round of the key encoding simply uses an XOR operation on the values
    in the columns.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '`   a9 =  s_box_ptr[a9];`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We then enter the main loop of the kernel. We run for `num_rounds` of iterations.
    As we later need the key and the key is to be fetched from memory, we initiate
    the read from memory as early as possible. Next we have the substitution step,
    which simply replaces the existing values with new ones from the `s_box` array
    shown earlier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to rotate rows 1, 2, and 3\. As we have stored one byte per
    register, we cannot simply do a 32-bit rotate. As there is no native support in
    the GPU instruction set for such an operation, this is of little real relevance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '`   a9 = tmp9;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to mix the columns operation, which is done in every round
    except the last one. The previous mix column code shown earlier has had the `c`
    loop unrolled to form the `MIX_COL` macro. Additionally, to control the order
    of the XOR, we implement an `XOR_5`, which is a five-input XOR macro.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: We then implement the XOR operation with the key fetched at the start of the
    loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Finally, the resultant key is combined into a 32-bit value and written back
    to the `uint4` cipher word. At this point we’ve completed all 10 rounds and the
    cipher block is encoded based on the set of 10 round keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'For completeness purposes, the macros used are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '`#define EXTRACT_D1(x) ( ( (x) >> 16uL ) & 0xFFuL )`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Kernel performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So how does such a kernel perform? How do we measure, understand, and predict
    performance? Initially, looking at the disassembled code for a compute 2.x target,
    we see something you might not expect. Declaring the registers as unsigned 8 bits
    results in sections of code to shift and mask data. The extract data macros are
    deliberately written to mask off the bits that are not used, so this is entirely
    unnecessary. In fact, we generate around four times the amount of code if we use
    a `u8` type instead of a `u32` type.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the `u8` definition to a `u32` definition means we *potentially* waste
    a lot of register space, but it eliminates huge numbers of instructions. In practice,
    the GPU implements `u8` registers as `u32` registers, so this doesn’t actually
    cost us anything in terms of register space.
  prefs: []
  type: TYPE_NORMAL
- en: Next we come to the number of registers used. Our initial kernel uses 43 registers,
    which is not altogether too surprising but is somewhat disappointing. If you load
    up the CUDA Occupancy Calculator, found in the “Tools” directory of the SDK, we
    can see that 43 registers will limit us to just a single block per SM of no more
    than 320 threads. This is just 10 active warps and nowhere near the maximum (24
    on compute 1.3 devices, 48 on compute 2.x devices, 64 on compute 3.× devices).
    We need to have more blocks than this, so there is a greater mix of instructions
    for the warp scheduler to select from. There are limits on, for example, the number
    of XOR operations an SM can perform (see [Chapter 9](CHP009.html)) and 10 warps
    will not hide the memory latency.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to achieve the best throughput, we don’t want to execute just a series
    of the same instructions one after another. By having more than one block per
    SM there is a good probability that while one block is performing the XOR section,
    another block may be doing the `s_box` substitution operation. This involves a
    number of address calculations and memory lookups. We need to somehow decrease
    the register count.
  prefs: []
  type: TYPE_NORMAL
- en: The compiler provides a switch for this. How does this perform? We’ll call the
    function with 16 blocks of 256 threads. Thus, we should see the improvement as
    and when we can schedule more blocks per SM. We’ll run this test on a NVIDIA ION
    (compute 1.2)–based laptop, which has two SMs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '` const int idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: As our `encrypt` function is a device function, we need a `global` function
    to call it. The `global` function extracts the appropriate block of cipher data
    and uses the same cipher key for all blocks. This represents what most encoding
    algorithms would do.
  prefs: []
  type: TYPE_NORMAL
- en: We see that for the original case, we get 6.91 ms to encode 512 keys simultaneously
    (two blocks of 256 threads each, one block per SM). Forcing the compiler to use
    just 32 registers should result in two blocks per SM, four blocks in total. Selecting
    24 registers will result in three blocks per SM, six blocks in total. Indeed,
    we see a drop to 4.74 ms when using 32 registers, a huge improvement. However,
    when we try 24 registers, this time increases to 5.24 ms. Why is this?
  prefs: []
  type: TYPE_NORMAL
- en: Asking the compiler to use less registers does not cause them to magically disappear.
    The compiler has a number of strategies it can use. First, it can reload registers
    from memory. This may sound a bit counterintuitive, as we know global memory is
    very slow compared to registers. However, the additional block may bring in another
    set of warps that in turn may hide this memory latency. In the case of moving
    from 256 threads (1 block, 8 warps) to 512 threads (2 blocks, 16 warps), we gain
    significantly in terms of instruction mix and number of potential warps schedulable
    per SM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second strategy is to move registers into other memory types: shared, constant,
    or local memory. If you use the `-v` option during compilation, the compiler tells
    you what amount of each memory type it is using. Shared memory is slower than
    registers. Constant memory is cached, but again slower than registers. Local memory
    is the L1 cache on Fermi (compute 2.x) and global memory on compute 1.x devices.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the compiler can reuse registers if it can correctly identify the scope
    and usage of the registers within a section.
  prefs: []
  type: TYPE_NORMAL
- en: As we push the compiler to use ever fewer registers it eventually spills the
    registers into local memory. Although not too bad on Fermi, performance on our
    compute 1.2 test platform is thus terrible as, in fact, we’re then using global
    memory. The additional gain of a third block is just simply not enough to overcome
    this rather huge penalty. Thus, we see the kernel slow down instead of speed up.
  prefs: []
  type: TYPE_NORMAL
- en: We achieved a 30% execution time reduction simply by setting a compiler switch,
    which is pretty impressive for five minutes of work. However, can we do any better
    by rewriting the C code? What is making the regular compilation take a massive
    43 registers? What can we do to reduce this?
  prefs: []
  type: TYPE_NORMAL
- en: Taking the existing code, we can comment out certain sections. This tells us
    easily what *additional* registers that code section requires. Thus, we start
    by localizing all registers to individual blocks. We can create a new scope level
    in C by simply placing braces (the {} symbols) around a block of code. This should
    allow the scope of a variable or constant to be identified and localized to within
    a section.
  prefs: []
  type: TYPE_NORMAL
- en: 'It turns out the most expensive part of the code is the mix columns section.
    Looking at the code it’s not too surprising. We calculate 16 `b<n>` values based
    on the 16 `a<n>` values, plus an additional 16 `tmp<n>` values. However, these
    are really just sets of four column parameters. The compiler should, when building
    the dependency tree, see it and rearrange the order of execution. Thus, instead
    of 32 additional registers, it needs only 8\. However, it does not do this reordering,
    perhaps because it simply does not model such a large number of parameters efficiently.
    Whatever the cause, it’s using far more registers than it needs to. We can therefore
    rewrite the mix column section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: For simplicity, only the operation on a single column is shown here. This, however,
    moves the usage of the variables closer to the setting of the variable or constant.
    This improves the instruction mix and also reduces the scope of where a variable
    or constant needs to exist. In effect, we make it easier for the compiler to identify
    and reuse these registers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also change the reading of the key values. Previously we’d calculate the
    address calculation for each access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the `cipher_key` pointer is being dereferenced, then indexed by `round_num`,
    with a zero-byte offset for the structure member `w`. This calculation would normally
    be made once and the offset part (`w`, `x`, `y`, or `z`) would then be added.
    To avoid creating a dependency on the next instruction the compiler actually repeats
    this instruction four times, once for each `w<n>` value. As the instruction latency
    is on the order of 20 cycles, this approach produces four answers in quick succession.
    However, it uses more registers than performing the calculation once and then
    adding the offset. As more blocks will bring us significantly more warps that
    in turn hide more latency, this is a good tradeoff. Thus, we replace this section
    of code with a new one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Here we introduce a new pointer parameter that performs the base address calculation
    once. Accessing the members `w`, `x`, `y`, or `z` through the pointer just requires
    a simple addition of literal 0, 4, 8, or 12 to the base address when the compiler
    calculates the address offsets.
  prefs: []
  type: TYPE_NORMAL
- en: Note we also tried simply reading the `uint4` key into a `uint4` local constant.
    Unfortunately, this resulted in the compiler placing the `uint4` constant into
    local memory (`lmem`), which is exactly what we do not want, and perhaps something
    later versions of the compiler may resolve. The LLVM compiler (CUDA 4.1) seems
    to prefer to place vector types into local memory rather than registers.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we moved the definition of `round_num` from the start of the function
    to just before the `while` loop and replaced its usage in round zero with an explicit
    zero index.
  prefs: []
  type: TYPE_NORMAL
- en: These steps brought the kernel register usage down from 43 registers to just
    25 registers and dropped the execution time to just 4.32 ms, somewhat faster than
    the forced register allocation version. Forcing this to just 24 again resulted
    in slower code due to the compiler’s usage of local memory. Unfortunately, we
    really want a maximum of 24 registers, not 25, as this will increase the block
    count and bring in another set of warps, increasing the overall amount of parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s replace
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: This will eliminate the need to hold the formal parameter `num_rounds` in a
    register and allow the compiler to instead use a literal value of 10, the value
    of the `#define` for `NUM_ROUNDS`. Using a literal value serves two purposes.
    First, it allows the comparison of the register holding `num_rounds` with an immediate
    value, rather than a comparison of two registers. Second, it means the bounds
    of the loop are known, which in turn allows the compiler to safely unroll the
    entire loop or sections of the loop as needed.
  prefs: []
  type: TYPE_NORMAL
- en: This indeed allows the compiler to use just 24 registers, the magic boundary
    number we need to potentially schedule another block. The savings are significant,
    although with 256 threads per block we do not bring in any additional blocks.
    Despite this the time for 16 blocks does drop. However, the timing becomes erratic
    and quite variable, with some runs now taking longer than before. We’re now starting
    to see the warps compete with one another. With such a small sample size (16 cipher
    blocks) the results become highly variable from run to run. Therefore, we’ll increase
    the number of cipher blocks to 2048 K and average the results.
  prefs: []
  type: TYPE_NORMAL
- en: The strategy CUDA adopts when allocating registers is to try for the smallest
    number of registers possible. With our transition from 25 to 24 registers, using
    256 threads per block, we can still only schedule two blocks. However, if we halve
    the number of threads per block, we can squeeze in another block of 128 threads.
    Thus, we can run five blocks per SM at 128 threads, 24 registers (640 total).
    This is compared with four blocks at 25 registers per block (512 threads). Does
    this make a difference? Yes, it does (see [Table 7.3](#T0020)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 7.3 Effect of Using Different Numbers of Threads
  prefs: []
  type: TYPE_NORMAL
- en: '| 64 Threads | 128 Threads | 256 Threads |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1150 ms | 1100 ms | 1220 ms |'
  prefs: []
  type: TYPE_TB
- en: '| 100% | 96% | 111% |'
  prefs: []
  type: TYPE_TB
- en: If we use a 64-thread version as a baseline, we hit the maximum limit of eight
    blocks, which in turn limits us to a total of 512 threads. The 128-thread version
    is limited to five blocks, 640 threads in total. The 256-thread version is limited
    to two blocks, again 512 threads total.
  prefs: []
  type: TYPE_NORMAL
- en: You might expect the 64-thread version and the 256-thread version, given they
    both run a total of 512 threads, to take the same time. The 64-thread version
    is faster because it provides a better instruction mix, with different blocks
    performing different parts of the algorithm. The 256-thread version tends to have
    its threads all doing the same thing at the same time. Remember in this compute
    1.2 device there is no L1/L2 cache, so this is simply a comparison of instruction
    and memory throughput. It’s also far easier for the CUDA runtime to get a better
    load balance between the two SMs due to the smaller scheduling unit.
  prefs: []
  type: TYPE_NORMAL
- en: Squeezing in that extra 64 threads by selecting a small number of threads per
    block gains us 120 ms, a 15% improvement over the 256-thread version on this compute
    1.2 device. We can only do this because we are within the 24 register threshold.
  prefs: []
  type: TYPE_NORMAL
- en: With a small laptop ION-based GPU, we’re encoding around 1.8 million cipher
    blocks per second, which is approximately 28 MB/s including transfer times. Excluding
    the transfer times, this approximately doubles. This is the next area to address.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It’s necessary to transfer data to the GPU over the PCI-E data bus. Compared
    to access to memory, this bus is very slow. [Chapter 9](CHP009.html) explores
    in detail PCI-E transfer sizes and the effects of using paged or pinned memory.
    Pinned memory is memory that cannot be paged (swapped) out to disk by the virtual
    memory management of the OS. PCI-E transfer can, in fact, only be done using pinned
    memory, and if the application does not allocate pinned memory, the CUDA driver
    does this in the background for you. Unfortunately, this results in a needless
    copy operation from the regular (paged) memory to or from pinned memory. We can
    of course eliminate this by allocating pinned memory ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the application, we simply replace the following lines when allocating memory
    in the host application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: And at the end, when cleaning up the memory allocation on the host, we replace
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: So how does this affect performance? It reduces our 1100 ms time down to 1070
    ms, a drop of some 30 ms, just a 3% decrease in the execution time. The actual
    gain is *very* dependent on the processor and chipset being used. Typically you
    see anything up to 20% performance gain in transfer time using this method. However,
    the laptop we are using for this test is using an X1 PCI-E 2.0 link. The fact
    that we see a minor but consistent improvement would suggest removing the redundant
    copy is insignificant in comparison to the actual copy time over this rather slow
    link.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the miserable gain pinned memory has brought us on this platform, we
    need to use pinned memory for the next step in the optimization of the transfers.
  prefs: []
  type: TYPE_NORMAL
- en: A single streaming version
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We cover streams in detail in [Chapter 8](CHP008.html), as they are essential
    in using more than one GPU on a problem. We’ll use them here on a single-GPU problem,
    as they allow us to both execute memory transfers and perform kernels at the same
    time. In effect, you must try to overlap the kernel execution with the transfer
    time. If we’re lucky the transfer time is less than or equal to the calculation
    time. Thus, the transfer time is effectively hidden behind the compute time of
    a different stream and becomes free.
  prefs: []
  type: TYPE_NORMAL
- en: Streams are simply virtual work queues that we’ll use here in a relatively simple
    manner. Initially we’ll create a single stream and move from a synchronous operation
    to an asynchronous operation with respect to the CPU. With this approach we will
    likely see a slight improvement due to the decreased synchronization needed for
    an asynchronous operation, but I’d expect this to be minor. Only once you introduce
    multiple streams can you really expect to see any significant speedup.
  prefs: []
  type: TYPE_NORMAL
- en: Stream 0 is the default stream; the stream used if you do not specify one. This
    is a synchronous stream that helps significantly when debugging an application
    but is not the most efficient use of the GPU. Thus, we must first create an alternative
    stream. We then need to push the memory copy, events, and kernel operations into
    the stream.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we need to do is to create an alternative stream. This is done
    with
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Conversely, we need to destroy the stream at the end of the host program once
    we’re finished with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Next the copy and event operations need to have the new stream added. Thus,
    we change
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '`CUDA_CALL(cudaEventRecord(start_round_timer, **aes_async_stream**));`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the newly created stream is used as the last parameter in each of
    the calls. The stream parameter is an optional parameter. Then we need to launch
    the kernel into the correct stream, which we again do by specifying the stream.
    As the stream parameter is actually the fourth parameter, we need to use zero
    as parameter 3\. Parameter 3 is the amount of dynamic shared memory the kernel
    will use. As we are using no dynamically allocated shared memory, we set this
    to zero. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: becomes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: We do the same for the copy back and stop timer event. As the stop timer event
    is at the end of the kernel, we also need to ensure we wait for this event.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: As the kernel, copy, and event operations are now entirely asynchronous it is
    critical that the data returned from the kernel is not used until such time as
    the kernel is actually complete. Forgetting to add such a synchronize operation
    after the final memory copy back to the host is often a cause for failure when
    moving to an asynchronous operation.
  prefs: []
  type: TYPE_NORMAL
- en: How does this change help? Running the test program reveals the time drops from
    1070 ms to just 940 ms, a drop of just over 12% in execution time. This is quite
    significant really, considering all we have done is to remove the implicit synchronization
    steps the CUDA driver was inserting when using stream 0.
  prefs: []
  type: TYPE_NORMAL
- en: How do we compare with the CPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intel provides a special extension to the AVX instruction set called AES-NI.
    This is based on a 128-bit-wide processing of the entire AES key state and key
    expansion. This equates to the `u4` type we’ve been using so far for memory load/stores.
    AES-NI has hardware support for both encode/decode and the expand key operation.
    Therefore, let’s look at how we can make use of this.
  prefs: []
  type: TYPE_NORMAL
- en: Intel provides a AES-NI sample library, which is available at [*http://software.intel.com/en-us/articles/download-the-intel-aesni-sample-library/*](http://software.intel.com/en-us/articles/download-the-intel-aesni-sample-library/).
    The library, once downloaded, needs to be built, as there are no precompiled binary
    libraries to link to. This is still via an old command line interface. Those running
    Microsoft Visual Studio need to run a command `vcvars32.bat`, which sets a number
    of command line environment variables for the command line version. This in practice
    maps to the `vsvars32.bat` file, which actually sets the environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: Once the library is built, you need to add the library search path, and include
    the search path and library to the additional libraries in your Visual Studio
    project.
  prefs: []
  type: TYPE_NORMAL
- en: The Intel version of AES has one key difference to the GPU one we’ve developed
    to date. The original specification of AES lays out data in a column format, so
    A, B, C, and D are located in the same column. The Intel ASE-NI expects this to
    be transposed, so A, B, C, and D are all on the same row. AES-NI also, due to
    Intel’s byte ordering, requires the bytes to be ordered in memory in the reverse
    order compared to the order we have now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we have two choices: either restructure the code to match the Intel AES-NI
    ordering, or perform a transformation on the data to convert one to the other.
    To allow memory blocks to be directly compared on the host, we’ll adapt our current
    solution to match the AES-NI format. As we also need AES-NI support, we’ll move
    all future development onto our Sandybridge-E (Core i7 3930 K @ 3.2 Ghz) platform
    with GTX470 GPUs. Thus, any further timings will no longer be comparable with
    our atom-based ION system used to date for this development.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other major issue we should note at this point is the `uint4` type is encoded
    on the GPU as `x`, `y`, `z`, `w` and not `w`, `x`, `y`, `z`. Both my GPU and CPU
    version gave the same wrong answer, as it was based on the same wrong code. This
    was easily corrected once you understood the rather strange ordering of the `uint4`
    type (this is usually a red, green, blue, alpha representation where `w` is the
    alpha channel). Clearly, we should have based the CPU version on either an existing
    library or used the AES-NI library sooner to have detected such issues. The AES-NI
    code is quite simple, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: The interface for the AES code needs to be a byte-based interface. Here we show
    some sample code used to encode a single block of data `num_cipher_blocks` based
    on a single key. A similar set of code is used for the decode operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '`               const u8 ∗ key,`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: The key expansion operation is implicit in this operation as we pass an unexpanded
    key of just 16 bytes. However, it is done, internally, only once per encrypt/decrypt
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll develop a program that will generate a set of four million random data
    blocks (around 64 MB of data), and encode it using a single key. We’ll then decode
    this data and check that the decoded data is the same as the original. We’ll run
    AES-NI, Serial, and CUDA versions of these operations and cross-check the results
    from each to ensure all implementations agree.
  prefs: []
  type: TYPE_NORMAL
- en: Once the GPU and CPU versions matched the AES-NI library, we were able to see
    just how fast the AES-NI instruction set is. On our Sandybridge-E system, the
    software-based serial expand key and decode block operation took 3880 ms, whereas
    the hardware-enabled AES-NI version took just 20 ms. By comparison, the CUDA version
    took 103 ms excluding any transfer times to or from the device. In fact, the copy
    to and copy from device operations took 27 ms and 26 ms, respectively. Given we’re
    using a GTX470 as our test device, and not a Tesla, we’d not be able to overlap
    both the transfer in and the transfer out as there is only a single memory transfer
    engine enabled in this device. Therefore, the absolute best case we could possibly
    achieve would be to entirely hide the kernel execution time behind one of these
    transfers, effectively eliminating it. However, to do this we’d need a 5× improvement
    in the kernel’s execution time. Let’s look therefore at the decode kernel in its
    revised form to be byte-for-byte compatible with the AES-NI output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '` u32 w1 = key.y;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '`   EXTRACT_WORD_XOR(w1, a4, a5, a6, a7);`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '`  }`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: The function first reads encrypted data and then decodes it into a set of 16
    registers. The decode function is the inverse of the encode function. Therefore,
    we count the rounds down from 10 to 0.
  prefs: []
  type: TYPE_NORMAL
- en: The decode side is more complex than encode, mainly because of the Galois multiplication
    that is used. The multiplication is precalculated into a table. Thus, the simple
    series of XOR operations now needs to perform a number of data-dependent lookups
    into one of four tables, each of which is 1 K bytes in size. This, however, generates
    a poor scattered memory access pattern.
  prefs: []
  type: TYPE_NORMAL
- en: We then rotate the values in the rows and finally perform the `s_box` substitution
    as before. As with the inverted mix column operation, the `s_box` function generates
    a scattered memory read pattern. Finally, a single 128-byte write is used to write
    out the data to global memory.
  prefs: []
  type: TYPE_NORMAL
- en: Another significant problem with this initial implementation is that this to
    uses far too many registers, 44 in total. It’s a complex kernel. We succeed in
    the goal of keeping the computation within registers until the very last moment.
    Forcing this (via the `maxrregcount=42` compiler flag) to 42 registers allows
    the scheduling of one additional block into the SM. This in turn reduces the execution
    time to 97 ms. Forcing register usage down means more spilling to global memory,
    and in this case, we see the memory bandwidth requirements jump by 25%. This suggests
    there is room to improve by reducing the register usage, but it needs to be done
    by other means.
  prefs: []
  type: TYPE_NORMAL
- en: We can achieve the desired effect of allowing more blocks to get scheduled by
    reducing the number of threads per block. Dropping down from 128 threads to 96
    threads per block allows us to schedule the same number of warps as before, but
    with eight blocks instead of six. This drops the execution time to 96 ms. As the
    kernel uses no synchronization points, this is entirely down to the better instruction
    mix the additional blocks bring and also the effects of caching.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the memory view in [Figure 7.16](#F0085) from one of the experiments
    Parallel Nsight can run for use, we see that we have very high L1 cache usage,
    but nonetheless 281 MB is spilling out of this to the L2\. Worse still, 205 MB
    of that is spilling into global memory. The kernel reads and writes to global
    memory so we will have some global memory traffic, but how much should we expect?
    We have 4,195,328 blocks with each block being 16 bytes in size. Therefore, we
    have 67,125,248 or exactly 64 MB of data to read. Equally, we write out a decrypted
    block, so we have 64 MB of data to write out. The statistics for global memory
    are shown for the device as a whole and shows we’re reading/writing a total of
    205MB. Therefore, we are generating 160% of the global memory traffic necessary,
    which in turn is limiting the performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-16-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.16 Initial memory bandwidth view.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, the L1 cache is operating at peak efficiency, but there is 16 K of
    shared memory we’re not using at all. It does not have the coalescing requirements
    global memory has, so it would be a good candidate for a small data region with
    a scattered memory pattern. However, unlike the L1 cache, the shared memory has
    a per-block visibility, which would mean having to duplicate the data for every
    resident block on the SM.
  prefs: []
  type: TYPE_NORMAL
- en: The constant memory cache is not shown in [Figure 7.16](#F0085), but it would
    also be large enough to hold the Galios multiplication (`gmul`) and/or `s_box`
    tables. However, the constant cache has only one 32-bit element bandwidth per
    clock and is designed for the same element being accessed by every thread. Thus,
    the shared memory is a better candidate.
  prefs: []
  type: TYPE_NORMAL
- en: However, let’s first look at the two problem areas, `s_box` and the `gmul` tables.
    Both were declared as 32-bit unsigned types, to avoid huge numbers of instructions
    being added to shift and mask the 32-bit words. Given the memory traffic we’re
    generating, this was probably not a good choice. Changing these to a `u8` type,
    we see the off-chip memory accesses drop from 205 MB to 183 MB and the execution
    time drop from 96 ms to 63 ms. Clearly, this was causing a significant amount
    of overfetch from the global memory and reducing it helps considerably.
  prefs: []
  type: TYPE_NORMAL
- en: With a reduced memory footprint, each `gmul` table is now 256 bytes in size,
    so the four tables fit easily with 1 K. As we can place a maximum of eight blocks
    per SM, 8 K of shared memory is now sufficient to accommodate the `gmul` tables.
  prefs: []
  type: TYPE_NORMAL
- en: Performing this shared memory optimization, however, has a problem. Indeed we
    move 18 GB of memory bandwidth from the L1 cache to the shared memory, and the
    main memory bandwidth drops by 7 MB. However, we have to move 1 K of data at the
    start of each block, as the shared memory is not persistent or shared between
    blocks. The L1 cache, however, is shared between the blocks and is currently doing
    a very good job of dealing with this scattered memory pattern, as the tables are
    entirely resident within the cache. The net improvement of speed for our 8 K of
    shared memory usage is almost zero, so this optimization was removed, leaving
    the tables in the L1 cache instead. Note this would have brought considerable
    improvement on compute 1.x devices, compared to global memory accesses, where
    there are no L1/L2 caches.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at [Figure 7.16](#F0085), did you notice something interesting?
    Did you notice we were using 1.91 GB of local storage? Local storage is the compiler
    spilling registers to the memory system. Prior to compute 2.0 devices this would
    actually go to global memory space. From compute 2.0 onward it gets contained
    within the L1 cache if possible, but can still cause significant unwanted global
    memory traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'When compiling, the `-v` option will display a summary of the register usage
    from the kernel. Anytime you see the following message you have local memory being
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: The main issue here is the `uint4` type being used. In combination with the
    high register usage elsewhere this `uint4` load from global memory is immediately
    being spilled to local memory. A 128-bit `uint4` load was deliberately chosen
    to minimize the number of load transactions to global memory. By spilling it to
    local memory instead of holding in registers, the compiler is unnecessarily polluting
    the caches and causing writes back to global memory.
  prefs: []
  type: TYPE_NORMAL
- en: We can explicitly move this data item into shared memory instead of local memory
    by simply declaring it as an array of `__shared__` and indexing it by `threadIdx.x`.
    As shared memory is a per-block form of local memory, we can move the spilled
    register explicitly into the shared memory. Moving this parameter generates the
    memory view shown in [Figure 7.17](#F0090).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-17-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.17 Memory transfers after using shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Notice how simply moving this data item to shared memory drops the local memory
    usage from 1.91 GB to just 256 MB, and the traffic to global memory from 183 MB
    to 133 MB. Our shared memory traffic is approximately double what it was before
    to the L1, which is largely due to the shared memory bank conflicts. These are
    caused by placing a 128-bit (16-byte) value into a 32-bit (4-byte) shared memory
    system. The compiler, however, still insists on creating a stack frame, much smaller
    than before, but it’s still there. The overall execution time remains stubbornly
    stuck at 63 ms.
  prefs: []
  type: TYPE_NORMAL
- en: To see exactly what parameters are being spilled you have to look at the PTX
    code, the assembly code, generated within a given kernel. Any PTX instructions
    such as `st.local` or `ld.local` are operating on local data. Local data is also
    declared with `local` as a prefix. It turns out the remaining local data is actually
    the parameter data used between the `__global__` caller and the `__device__` function,
    that is,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '` const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'The fact that we have passed a number of parameters to the device function,
    which in turn allows it to be called by a number of global functions and the host
    function, causes the compiler to insert a stack frame. We rarely if ever want
    the compiler to call a stack and instead want it to inline the call to the device
    function, thereby eliminating any need to use a stack. We can do this using the
    __`_forceinline__` directive when declaring the function as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '`}`'
  prefs: []
  type: TYPE_NORMAL
- en: Recompiling the code no longer produces the stack frame message. Due to the
    function now being a consolidated whole, the compiler can much better apply optimization
    techniques to it. The register usage drops to just 33 instead of the forced 42
    registers we were using before to accommodate eight blocks. We can verify local
    memory is no longer being used by looking at the memory overview in [Figure 7.18](#F0095).
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000077f07-18-9780124159334.jpg)'
  prefs: []
  type: TYPE_IMG
- en: FIGURE 7.18 Memory usage after stack elimination.
  prefs: []
  type: TYPE_NORMAL
- en: We can see in [Figure 7.18](#F0095) the local memory traffic now falls to zero.
    What little L2 cache usage there was is eliminated. The global memory usage falls
    by another 5 MB to 128 MB, the magic figure we were expecting the global memory
    bandwidth to be based on for the size of data we’re processing. The execution
    time reduces marginally but still remains at 63 ms.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel makes considerable use of the XOR operation, which is one of the
    instructions that is not available at full rate within the device. Thus, by ensuring
    we keep the maximum number of blocks in the SM, we ensure a good instruction mix
    and that everything doesn’t start backing up behind the units performing the XOR
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: At 96 threads per block with the previous high 42 register count we could schedule
    eight blocks using 24 warps. This is around 50% of the available capacity of the
    SM in terms of the number of warps it could run. However, we can see from looking
    at the Parallel Nsight “Issue Stalls” experiment how much of the SM capacity we’re
    actually using. We stall just 0.01% of the time, which means the SM is already
    almost at peak capacity. Increasing the occupancy figure by increasing the list
    of possible warps, therefore, is unlikely to help significantly. Increasing the
    number of threads from 96 to 128 allows us to increase the number of warps available
    for scheduling from 24 to 28\. This eliminates the remaining fractional stall
    issue and increases the fraction of the time that both warp scheduler have warps
    available, gaining us a 1.5 ms reduction in the timing. This brings the total
    execution time to 61.5 ms.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for running on other GPUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Having now developed a program for a single, modern GPU, how well does i work
    on other GPUs? Often, especially if you are writing commercial applications, your
    program will need to work well on each level of hardware in the marketplace. Although
    programs will run on most GPU generations, you should be aware of what adaptations
    may be required to achieve good performance on that hardware. We’ll look at this
    with the AES program we’ve developed here.
  prefs: []
  type: TYPE_NORMAL
- en: Out first target is the GTX460 card, a compute 2.1 card based on Fermi. Major
    differences are the compute 2.1 architecture (7 SMs × 48 CUDA cores vs. 14 SMs
    × 32 CUDA cores), reduced L2 cache size (512 K vs. 640 K), reduced L1 cache size
    per CUDA core (48 K L1 shared between 48 CUDA cores vs. 48K L1 shared between
    32 CUDA cores), and the reduced memory bandwidth (115 GB/s vs. 134 GB/s).
  prefs: []
  type: TYPE_NORMAL
- en: Based purely on total CUDA core count (336 vs. 448), we’d expect around 75%
    of the performance. However, adjusting for clock speed differences, this gives
    us a little less than 10% performance difference between the two devices. Memory
    bandwidth is 15% less on the GTX460.
  prefs: []
  type: TYPE_NORMAL
- en: For the decrypt function the time actually measured is 100 ms compared with
    61.5ms, which is somewhat disappointing. Looking at the execution profile we see
    that the SMs on the GTX460 are able to clock through more instructions, so the
    ratio of when the data arrives to the compute has changed. We again see a tiny
    amount of stalling in the SMs. With 128 threads per block we manage to get seven
    blocks scheduled (28 warps). If we could just reduce the register usage slightly
    we could execute another block and make better use of the SM. We therefore apply
    the same technique we used in the encode operation and move the inverse mix columns
    operation closer to the decode operation. Thus,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '`  INV_MIX_COLUMN_PTR2(a12, a13, a14, a15,`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: becomes
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: This fusing of the operation allows the register usage to drop to the magic
    31 registers, which in turn allows us to schedule another block, giving a total
    of 32 warps per SM. This compensates for the compute 2.1 devices having a higher
    ratio of compute to load/store units than compute 2.0 devices. We see a small
    drop from 100 ms to 98 ms. However, our compute 2.0 device (the GTX470) was already
    using its compute cores to full capacity. This change, which introduces a few
    more tests, costs us 0.5 ms, bringing us back up to 62 ms on the compute 2.0 device.
    You may sometimes find this, especially with compute 2.0/compute 2.1 devices where
    the balance of execution units within an SM is different.
  prefs: []
  type: TYPE_NORMAL
- en: The second target is the GTX260, a compute 1.3 device. The major difference
    here is the complete lack of L1 and L2 caches. SM architecture is different with
    27 SMs versus 14 SMs, for a total of 216 CUDA cores versus 448 CUDA cores. Memory
    bandwidth is 112 GB/s versus 134 GB/s some 16% less and on par with the GTX460.
  prefs: []
  type: TYPE_NORMAL
- en: The initial run was 650 ms for the decode function, over 10 times slower than
    the GTX470\. Why is this? One of the key reasons is the compute 1.x platform does
    not support a unified addressing mode. Thus, an explicit declaration of intended
    memory usage is needed. In the case of the `gmul` tables, they are generated on
    the device through a small compute kernel. As such, these tables exist in global
    memory. On compute 2.x platforms global memory is cached, whereas on compute 1.x
    platforms you have to explicitly make it cacheable. We can do this in a couple
    of ways.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to specify that the memory used for `gmul` is constant, which
    in turn means we can’t write to it from the device. As we have a copy of the data
    on the host we can either copy it to the device via the `cudaMemcpyToSymbol` call
    or simply declare it on the device as constant memory and initialize it there
    statically. Thus, the code to calculate the `gmul` table was replaced with a simple
    expanded definition of the table lookup. This then resides in the constant cache.
    Rerunning the code we see a drop from 650 ms to 265 ms, a drop in execution time
    of nearly 60%. However, the GTX260 is still a factor of 4.2× slower than the GTX470
    and 2.7× slower than the GTX460.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, an older GT9800 card has approximately half the number of CUDA cores
    of the GTX260 and half the memory bandwidth. As might be expected, we see the
    265 ms approximately double (1.8×) to 478 ms.
  prefs: []
  type: TYPE_NORMAL
- en: The issue with both GTX260 and GT9800 is the organization of the data. Having
    the data match the format used for AES-NI means the data for a single key value
    is laid out sequentially in memory. To achieve much better performance we need
    to organize the memory such that each successive 32-bit value from the key appears
    as a column in memory rather than a row. The typical sequential arrangement that
    is ideal for the CPU is far from ideal for the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual output of our AES encryption/decryption is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '`Encrypt Copy From Device  :  25.428ms`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Notice that with encrypt we’ve managed to get within approximately 2× of the
    AES-NI hardware, and for decrypt approximately within 3×. We’re using here a GTX470,
    which is hardware from the time of the regular Sandybridge CPU, rather than the
    more modern Sandybridge-E device. The regular Sandybridge device’s AES-NI performance
    is approximately half of the Sandybridge-E, which puts us on similar timings.
    The Kepler-based GTX680 would be a representative device to pair with a Sandybridge-E
    CPU. This would bring us in the order of a 2× performance improvement, bringing
    the GPU in line with the hardware-based AES-NI performance.
  prefs: []
  type: TYPE_NORMAL
- en: The issue of what GPUs to support is a tricky one. There are a lot of older
    GPUs in the consumer market, so applications have to work well on these if you
    have a consumer application. Yet in large installations, simply the power bill
    means it makes no sense at all to keep the old GPUs running if they can be replaced
    with newer ones. The introduction of Kepler will hugely accelerate the retirement
    of the older Tesla boards.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to support older hardware, then the best approach is to develop
    on that hardware from day one. You will then have a baseline application that
    will work reasonably well on the later-generation cards. Many of the optimizations
    you’d need to do for these cards would show significantly less benefit on the
    later-generation cards. However, almost all would show *some* benefit, it’s just
    a question of what return you get for the time you invest.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple streams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An example of multistream and multistream/multi-GPU programming is provided
    in [Chapter 8](CHP008.html). We’ll therefore not cover how to implement a streamed
    version of this algorithm. However, we’ll discuss some of the issues you’d need
    to think about to implement one, with this algorithm or a problem of your own.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple streams are useful in that they allow some overlap of kernel execution
    with PCI-E transfers. Their usefulness, however, is seriously hampered by the
    fact that one PCI-E transfer engine is only ever enabled on consumer cards. Only
    the Tesla series cards have both PCI-E transfer engines enabled, allowing for
    simultaneous bidirectional transfers.
  prefs: []
  type: TYPE_NORMAL
- en: We typically want to transfer data to the card, process some data, and then
    transfer the data out of the card. With a single PCI-E transfer engine enabled,
    we have just a single queue for all the memory transfers in the hardware. Despite
    being in separate streams, memory transfer requests feed into a *single* queue
    on Fermi and earlier hardware. Thus, the typical workflow pattern of transfer
    from host to device, invoke kernel, and then transfer from device to host creates
    a stall in the workflow. The transfer out of the device blocks the transfer into
    the device from the next stream. Thus, all streams actually run in series.
  prefs: []
  type: TYPE_NORMAL
- en: The next issue we need to think about when using multiple streams is the resource
    usage. You need *N* sets of host and device memory, where *N* is the number of
    streams you wish to run. When you have multiple GPUs, this makes a lot of sense,
    as each GPU contributes significantly to the overall result. However, with a single-consumer
    GPU the gain is less easy to quantify. It works well only where either the input
    or output of the GPU workload is small in comparison to one another and the total
    transfer time is less than the kernel execution time.
  prefs: []
  type: TYPE_NORMAL
- en: In our application, we transfer in a set of blocks to be encoded in a single
    key set to use for the encoding. We transfer out the encoded blocks. The transfer
    in and transfer out are all but identical in size. The kernel execution time is
    around twice the size of the transfers. This means we have the opportunity to
    hide the input transfer time and only suffer the output transfer time.
  prefs: []
  type: TYPE_NORMAL
- en: A single GPU can support up to 16 hardware streams (32 in Kepler), so it would
    be possible to perform 16 inbound transfers, 16 kernels, and then 16 outbound
    transfers and still be within the bounds of the memory on the device and the host.
    Transfers become more of an issue, as you will see in [Chapter 9](CHP009.html),
    where we introduce more than one GPU into the system. Due to contention for host
    resources, the transfer time itself may become longer the more concurrent transfers
    are in flight over the PCI-E bus.
  prefs: []
  type: TYPE_NORMAL
- en: AES summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There were a number of issues we saw with AES that are worth summarizing here.
  prefs: []
  type: TYPE_NORMAL
- en: • The ideal memory pattern for the CPU and GPU versions are different. Optimizing
    the memory pattern for the GPU would have brought considerable benefits (typically
    at least 2× on Fermi), especially on the earlier GPUs where this is far more critical.
  prefs: []
  type: TYPE_NORMAL
- en: • For compute 1.x devices read-only memory needs to be explicitly declared as
    constant memory, rather than auto-designated by the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: • It may be necessary to reorder or transform the kernel to allow the compiler
    to more easily see optimization opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: • Efficient register usage and count were critical to achieving good performance.
  prefs: []
  type: TYPE_NORMAL
- en: • You can share read-only data between blocks using the L1 cache, whereas holding
    the same read-only data shared memory necessitates *N* copies where *N* is the
    number of resident blocks.
  prefs: []
  type: TYPE_NORMAL
- en: • Complex and thread-divergent algorithms, for example, the `gmul` function
    when decoding, can be replaced by nonthread-divergent memory lookups in the cache
    or shared memory. The cache was added specifically for such data-driven scattered
    memory patterns.
  prefs: []
  type: TYPE_NORMAL
- en: • Check the allocation of variables to registers and eliminate stack or local
    memory usage where possible.
  prefs: []
  type: TYPE_NORMAL
- en: • Always check correctness early in the solution, preferably with code developed
    independently.
  prefs: []
  type: TYPE_NORMAL
- en: • Always look at the *actual* timing of the program. Your mental model of how
    things work will not always be correct and often you will overlook something.
    Always look to the data for what effect each change has.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve looked at a couple of applications of GPU technology, deliberately chosen
    for not being a simple matrix multiply shown in so many other examples of CUDA
    programming. We looked at using GPUs to filter data, which is useful from the
    perspective of searching data for interesting facts and also from a pure signal
    processing perspective. We’ve also looked how to implement AES, a standard encryption
    algorithm on GPUs. Even if you never have to implement this in CUDA, you should
    now understand and feel happy about implementing or using such algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: You should also have picked up on some of the tradeoffs and design points when
    targeting multiple compute levels and how design decisions early on in project
    development can affect the outcome later. Thinking about the usage of registers,
    shared memory, cache, and access patterns to global memory are all key aspects
    of a design that should be understood and worked out before you write a single
    line of code.
  prefs: []
  type: TYPE_NORMAL
- en: One of the biggest issues programmers have today is growing up in a world where
    they are isolated from the hardware on which they are programming. To achieve
    great performance and not just average performance, it pays to understand, and
    understand thoroughly, the environment in which you are developing. Concepts such
    as various levels of memory hierarchy don’t really exist in traditional programming
    languages. The C language was invented back in the early 1970s and only in the
    C11 (as in 2011) standard do we finally see thread and local thread storage start
    to appear. CUDA, and its native language C, follows the principle of trusting
    the programmer. It exposes aspects of the hardware to you, and you should therefore
    consider it your responsibility to understand those features and use them well.
  prefs: []
  type: TYPE_NORMAL
- en: With a few examples now covered, we’ll move on to using multiple GPUs and optimizing
    applications, an area where we can extract massive speedups within a node simply
    by plugging more cards into the PCI-E bus and adapting our applications to be
    multi-GPU aware. The Kepler Tesla K10 product is the first Tesla dual-GPU solution,
    perhaps one of many we may see in the coming years. Multi-GPU programming, after
    CUDA 4.0, is actually not hard, as you’ll see in the subsequent chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 1. What was the main reason why the AES application ran significantly slower
    on the GTX260 and GT9800 cards compared with the GTX460 and GTX470 cards? What
    would you do to address this?
  prefs: []
  type: TYPE_NORMAL
- en: 2. In the AES application, why did changing the `s_box` and `gmul` tables from
    `u32` to `u8` improve performance?
  prefs: []
  type: TYPE_NORMAL
- en: 3. What is thread level parallelism? Does it help, and if so why?
  prefs: []
  type: TYPE_NORMAL
- en: 4. What problems are associated with using atomic operations?
  prefs: []
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 1. The GTX260 and GT9800 cards are compute 1.3 and compute 1.1 cards, respectively.
    As such, they have no level one (L1) or level two (L2) caches as found on the
    compute 2.x cards. In the memory figures shown we were using the L1 cache with
    a 99% hit rate. Going from L1 to global memory means we move from terabytes of
    bandwidth to just the low hundreds of megabytes of bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: The memory coalescing also radically changes. The compute 2.x hardware fetches
    memory in 128-byte cache lines. If the thread fetches a single 128-byte value,
    `uint4` for example, the hardware can service this. On compute 1.x hardware coalescing
    requirements are much stricter.
  prefs: []
  type: TYPE_NORMAL
- en: The `uint4` type as currently compiled is hurting the algorithm. On compute
    2.x hardware a four-word vector load from memory is used followed by a four-word
    vector to shared memory. On the compute 1.x hardware, the CUDA 4.1 compiler generates
    code to load each 32-bit word separately and thus generates four more times the
    traffic in each direction than is necessary. The encrypted cipher data needs to
    be placed into a suitable form for coalescing.
  prefs: []
  type: TYPE_NORMAL
- en: The constant cache is helpful. However, removing the `uint4` type from the shared
    memory, replacing it with register-held `u32` values, and then using the shared
    memory for the `gmul` and `s_box` tables would be more beneficial. You should
    also consider that on older devices, the texture cache can be a worthwhile additional
    resource worth the effort of exploiting.
  prefs: []
  type: TYPE_NORMAL
- en: 2. The s_box and `gmul` tables are accessed with a data-dependent pattern. We
    have a total of four tables, each of which is 256 entries in size. Using a `u8`
    type means we use 5 K of memory, which fits into both the L1 cache and the constant
    cache. Using `u32` values removed a number of `cvt` (convert type) instructions,
    but shifts four times the data from the L1 or constant cache. The extra compute
    overhead is easily worth the cost of not moving so much data. As a `u32` type,
    the caches need to store 20 K of data, easily exceeding the normal 16 K L1 cache
    allocation and the 8 K constant cache working set.
  prefs: []
  type: TYPE_NORMAL
- en: 3. Thread level parallelism exploits the fact that most hardware is pipelined
    and thus able to accept nondependent instructions on successive clocks without
    blocking. A value of four independent items per thread is typically a good value
    to exploit to achieve thread level parallelism, something we look at in [Chapter
    9](CHP009.html).
  prefs: []
  type: TYPE_NORMAL
- en: 4. There are two main issues to consider. First, atomic operations, if oversubscribed,
    cause serialization. Thus, a warp of 32 values writing to the same memory address,
    be it shared or global memory, will serialize. Atomics, at least on Fermi, are
    warp-wide operations. Thus, having each thread in a warp perform an atomic operation
    to independent addressable locations will result in 32 atomic operations without
    serialization.
  prefs: []
  type: TYPE_NORMAL
- en: The second problem is ordering of atomic writes. If all values in a warp write
    to one address, the order of the operation is not defined. You can obviously observe
    the order and it’s likely that this will remain consistent for a given device.
    Another device may, however, work differently. Thus, in using such knowledge,
    you’d be building a failure point into your application.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1\. *Wikipedia, Rijndael Mix Columns*. Available at *[http://en.wikipedia.org/wiki/Rijndael_mix_columns](http://en.wikipedia.org/wiki/Rijndael_mix_columns)*;
    accessed Jan. 31, 2012.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. *Federal Information Processing Standards Publication 197, Advanced Encryption
    Standard (AES)*. Available at *[http://csrc.nist.gov/publications/fips/fips197/fips-197.pdf](http://csrc.nist.gov/publications/fips/fips197/fips-197.pdf)*;
    accessed Feb. 5, 2012.
  prefs: []
  type: TYPE_NORMAL
- en: '3\. *Toms Hardware, AES-NI Benchmark Results: Bitlocker, Everest, and WinZip
    14*. Available at *[http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption,
    review-31801–7.html](http://www.tomshardware.co.uk/clarkdale-aes-ni-encryption_review-31801-7.html)*;
    accessed Apr. 26, 2012.'
  prefs: []
  type: TYPE_NORMAL
