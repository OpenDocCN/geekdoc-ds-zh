["```\n ! Split the world communicator into subgroups of commu, each of which\n  ! contains processes that run on the same node, and which can create a\n  ! shared memory region (via the type MPI_COMM_TYPE_SHARED).\n  ! The call returns a new communicator \"host_comm\", which is created by\n  ! each subgroup. \n  call MPI_COMM_SPLIT_TYPE(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,&\n  MPI_INFO_NULL,  host_comm,ierr)\n  call MPI_COMM_RANK(host_comm,  host_rank,ierr) \n```", "```\n // Split the world communicator into subgroups.\n  MPI_Comm  host_comm;\n  MPI_Comm_split_type(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,  MPI_INFO_NULL,\n  &host_comm);\n  int  host_rank;\n  MPI_Comm_rank(host_comm,  &host_rank); \n```", "```\n myDevice  =  host_rank\n\n  ! Sets the device number and the device type to be used\n  call acc_set_device_num(myDevice,  acc_get_device_type())\n\n  ! Returns the number of devices available on the host\n  numDevice  =  acc_get_num_devices(acc_get_device_type()) \n```", "```\n myDevice  =  host_rank\n\n  ! Sets the device number to use in device constructs by setting the initial value of the default-device-var \n  call omp_set_default_device(myDevice)\n\n  ! Returns the number of devices available for offloading.\n  numDevice  =  omp_get_num_devices() \n```", "```\n int  myDevice  =  host_rank;\n  // Set the device number to use in device constructs.\n  omp_set_default_device(myDevice);\n\n  // Return the number of devices available for offloading.\n  int  numDevice  =  omp_get_num_devices(); \n```", "```\nacc_get_device_num() \n```", "```\nomp_get_device_num() \n```", "```\n ! Initialise MPI communication \n  call MPI_Init(ierr)\n  ! Identify the ID rank (process) \n  call MPI_COMM_RANK(  MPI_COMM_WORLD,  myid,  ierr  )\n  ! Get number of active processes (from 0 to nproc-1) \n  call MPI_COMM_SIZE(  MPI_COMM_WORLD,  nproc,  ierr  )\n\n  ! Split the world communicator into subgroups of commu, each of which\n  ! contains processes that run on the same node, and which can create a\n  ! shared memory region (via the type MPI_COMM_TYPE_SHARED).\n  ! The call returns a new communicator \"host_comm\", which is created by\n  ! each subgroup. \n  call MPI_COMM_SPLIT_TYPE(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,&\n  MPI_INFO_NULL,  host_comm,ierr)\n  call MPI_COMM_RANK(host_comm,  host_rank,ierr)\n\n  ! Gets the node name\n  call MPI_GET_PROCESSOR_NAME(name,  resulten,  ierror)\n\n  myDevice  =  host_rank\n\n  ! Sets the device number and the device type to be used\n  call acc_set_device_num(myDevice,  acc_get_device_type())\n\n  ! Returns the number of devices available on the host\n  numDevice  =  acc_get_num_devices(acc_get_device_type()) \n```", "```\n ! Initialise MPI communication \n  call MPI_Init(ierr)\n  ! Identify the ID rank (process) \n  call MPI_COMM_RANK(  MPI_COMM_WORLD,  myid,  ierr  )\n  ! Get number of active processes (from 0 to nproc-1) \n  call MPI_COMM_SIZE(  MPI_COMM_WORLD,  nproc,  ierr  )\n\n  ! Split the world communicator into subgroups of commu, each of which\n  ! contains processes that run on the same node, and which can create a\n  ! shared memory region (via the type MPI_COMM_TYPE_SHARED).\n  ! The call returns a new communicator \"host_comm\", which is created by\n  ! each subgroup. \n  call MPI_COMM_SPLIT_TYPE(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,&\n  MPI_INFO_NULL,  host_comm,ierr)\n  call MPI_COMM_RANK(host_comm,  host_rank,ierr)\n\n  ! Gets the node name\n  call MPI_GET_PROCESSOR_NAME(name,  resulten,  ierror)\n\n  myDevice  =  host_rank\n\n  ! Sets the device number to use in device constructs by setting the initial value of the default-device-var \n  call omp_set_default_device(myDevice)\n\n  ! Returns the number of devices available for offloading.\n  numDevice  =  omp_get_num_devices() \n```", "```\n // Initialize MPI communication.\n  MPI_Init(&argc,  &argv);\n\n  // Identify the ID rank (process).\n  int  myid,  nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD,  &myid);\n  // Get number of active processes.\n  MPI_Comm_size(MPI_COMM_WORLD,  &nproc);\n\n  // Split the world communicator into subgroups.\n  MPI_Comm  host_comm;\n  MPI_Comm_split_type(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,  MPI_INFO_NULL,\n  &host_comm);\n  int  host_rank;\n  MPI_Comm_rank(host_comm,  &host_rank);\n\n  // Get the node name.\n  int  name_len;\n  char  processor_name[MPI_MAX_PROCESSOR_NAME];\n  MPI_Get_processor_name(processor_name,  &name_len);\n\n  int  myDevice  =  host_rank;\n  // Set the device number to use in device constructs.\n  omp_set_default_device(myDevice);\n\n  // Return the number of devices available for offloading.\n  int  numDevice  =  omp_get_num_devices(); \n```", "```\n !offload f to GPUs\n  !$acc enter data copyin(f)\n\n  !update f: copy f from GPU to CPU\n  !$acc update host(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$acc update device(f) \n```", "```\n !offload f to GPUs\n  !$omp target enter data device(myDevice) map(to:f)\n\n  !update f: copy f from GPU to CPU\n  !$omp target update device(myDevice) from(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$omp target update device(myDevice) to(f) \n```", "```\n// Offload f to GPUs\n#pragma omp target enter data map(to : f[0 : np]) device(myDevice)\n\n  // update f: copy f from GPU to CPU\n#pragma omp target update device(myDevice) from(f)\n\n  if  (myid  <  nproc  -  1)  {\n  MPI_Send(&f[np  -  1],  1,  MPI_DOUBLE,  myid  +  1,  tag,  MPI_COMM_WORLD);\n  }\n  if  (myid  >  0)  {\n  MPI_Recv(&f[0],  1,  MPI_DOUBLE,  myid  -  1,  tag,  MPI_COMM_WORLD,\n  MPI_STATUS_IGNORE);\n  }\n\n// update f: copy f from CPU to GPU\n#pragma omp target update device(myDevice) to(f) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$acc enter data copyin(f)\n\n  !update f: copy f from GPU to CPU\n  !$acc update host(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$acc update device(f)\n\n  !do something .e.g.\n  !$acc kernels\n  f  =  f/2.\n  !$acc end kernels\n\n  SumToT=0d0\n  !$acc parallel loop reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$acc end parallel loop\n\n  !SumToT is by default copied back to the CPU\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n\n  !$acc exit data delete(f) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$omp target enter data device(myDevice) map(to:f)\n\n  !update f: copy f from GPU to CPU\n  !$omp target update device(myDevice) from(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$omp target update device(myDevice) to(f)\n\n  !do something .e.g.\n  !$omp target teams distribute parallel do\n  do k=1,np\n  f(k)  =  f(k)/2.\n  enddo\n  !$omp end target teams distribute parallel do\n\n  SumToT=0d0\n  !$omp target teams distribute parallel do reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$omp end target teams distribute parallel do \n\n  !SumToT is by default copied back to the CPU\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n\n  !$omp target exit data map(delete:f) \n```", "```\n MPI_Scatter(f_send.data(),  np,  MPI_DOUBLE,  f,  np,  MPI_DOUBLE,  0,\n  MPI_COMM_WORLD);\n\n// Offload f to GPUs\n#pragma omp target enter data map(to : f[0 : np]) device(myDevice)\n\n  // update f: copy f from GPU to CPU\n#pragma omp target update device(myDevice) from(f)\n\n  if  (myid  <  nproc  -  1)  {\n  MPI_Send(&f[np  -  1],  1,  MPI_DOUBLE,  myid  +  1,  tag,  MPI_COMM_WORLD);\n  }\n  if  (myid  >  0)  {\n  MPI_Recv(&f[0],  1,  MPI_DOUBLE,  myid  -  1,  tag,  MPI_COMM_WORLD,\n  MPI_STATUS_IGNORE);\n  }\n\n// update f: copy f from CPU to GPU\n#pragma omp target update device(myDevice) to(f)\n\n// Update, operate, and offload data back to GPUs\n#pragma omp target teams distribute parallel for device(myDevice)\n  for  (int  k  =  0;  k  <  np;  ++k)  {\n  f[k]  /=  2.0;\n  }\n\n  double  SumToT  =  0.0;\n\n#pragma omp target teams distribute parallel for reduction(+ : SumToT)         \\\n device(myDevice)\n  for  (int  k  =  0;  k  <  np;  ++k)  {\n  SumToT  +=  f[k];\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE,  &SumToT,  1,  MPI_DOUBLE,  MPI_SUM,  MPI_COMM_WORLD);\n\n#pragma omp target exit data map(delete : f[0 : np]) \n```", "```\n !Device pointer f will be passed to MPI_send & MPI_recv\n  !$acc host_data use_device(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$acc end host_data \n```", "```\n !Device pointer f will be passed to MPI_send & MPI_recv\n  !$omp target data use_device_ptr(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$omp end target data \n```", "```\n#pragma omp target data use_device_ptr(f)\n  {\n  if  (myid  <  nproc  -  1)  {\n  MPI_Send(&f[np  -  1],  1,  MPI_DOUBLE,  myid  +  1,  tag,  MPI_COMM_WORLD);\n  }\n\n  if  (myid  >  0)  {\n  MPI_Recv(&f[0],  1,  MPI_DOUBLE,  myid  -  1,  tag,  MPI_COMM_WORLD,\n  MPI_STATUS_IGNORE);\n  }\n  } \n```", "```\n !$acc data copy(SumToT)\n  !$acc host_data use_device(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$acc end host_data\n  !$acc end data \n```", "```\n !$omp target enter data device(myDevice) map(to:SumToT)\n  !$omp target data use_device_ptr(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$omp end target data\n  !$omp target exit data map(from:SumToT) \n```", "```\n#pragma omp target enter data device(myDevice) map(to : SumToT)\n  double  *SumToTPtr  =  &SumToT;\n#pragma omp target data use_device_ptr(SumToTPtr)\n  {\n  MPI_Allreduce(MPI_IN_PLACE,  SumToTPtr,  1,  MPI_DOUBLE,  MPI_SUM,\n  MPI_COMM_WORLD);\n  }\n#pragma omp target exit data map(from : SumToT) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$acc enter data copyin(f)\n\n  !Device pointer f will be passed to MPI_send & MPI_recv\n  !$acc host_data use_device(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$acc end host_data\n\n  !do something .e.g.\n  !$acc kernels\n  f  =  f/2.\n  !$acc end kernels\n\n  SumToT=0d0\n  !$acc parallel loop reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$acc end parallel loop\n\n  !SumToT is by default copied back to the CPU\n\n  !$acc data copy(SumToT)\n  !$acc host_data use_device(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$acc end host_data\n  !$acc end data\n\n  !$acc exit data delete(f) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$omp target enter data device(myDevice) map(to:f)\n\n  !Device pointer f will be passed to MPI_send & MPI_recv\n  !$omp target data use_device_ptr(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$omp end target data\n\n  !do something .e.g.\n  !$omp target teams distribute parallel do\n  do k=1,np\n  f(k)  =  f(k)/2.\n  enddo\n  !$omp end target teams distribute parallel do\n\n  SumToT=0d0\n  !$omp target teams distribute parallel do reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$omp end target teams distribute parallel do \n\n  !SumToT is by default copied back to the CPU\n\n  !$omp target enter data device(myDevice) map(to:SumToT)\n  !$omp target data use_device_ptr(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$omp end target data\n  !$omp target exit data map(from:SumToT)\n\n  !$omp target exit data map(delete:f) \n```", "```\n call  MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload  f  to  GPUs\n  !$omp  target  enter  data  device(myDevice)  map(to:f)\n\n  !Device  pointer  f  will  be  passed  to  MPI_send  &  MPI_recv\n  !$omp  target  data  use_device_ptr(f)\n  if(myid.lt.nproc-1)  then\n  call  MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n  if(myid.gt.0)  then\n  call  MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$omp  end  target  data\n\n  !do  something  .e.g.\n  !$omp  target  teams  distribute  parallel  do\n  do  k=1,np\n  f(k)  =  f(k)/2.\n  enddo\n  !$omp  end  target  teams  distribute  parallel  do\n\n  SumToT=0d0\n  !$omp  target  teams  distribute  parallel  do  reduction(+:SumToT)\n  do  k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$omp  end  target  teams  distribute  parallel  do  \n\n  !SumToT  is  by  default  copied  back  to  the  CPU\n\n  !$omp  target  enter  data  device(myDevice)  map(to:SumToT)\n  !$omp  target  data  use_device_ptr(SumToT)\n  call  MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$omp  end  target  data\n  !$omp  target  exit  data  map(from:SumToT)\n\n  !$omp  target  exit  data  map(delete:f) \n```", "```\n$ ml  LUMI/24.03\n$ ml  PrgEnv-cray\n$ ml  cray-mpich\n$ ml  rocm\n$ ml  craype-accel-amd-gfx90a \n```", "```\n$ ftn  -hacc  -o  mycode.mpiacc.exe  mycode_mpiacc.f90 \n```", "```\n$ ftn  -homp  -o  mycode.mpiomp.exe  mycode_mpiomp.f90 \n```", "```\n$ CC  -fopenmp  -fopenmp-targets=amdgcn-amd-amdhsa  -Xopenmp-target  -march=gfx90a  -o  mycode.mpiomp.exe  mycode_mpiomp.cpp \n```", "```\n$ export MPICH_GPU_SUPPORT_ENABLED=1 \n```", "```\n$ git  clone  https://github.com/ENCCS/gpu-programming.git\n$ cd  gpu-programming/content/examples/exercise_multipleGPU\n$ ls \n```", "```\n ! Split the world communicator into subgroups of commu, each of which\n  ! contains processes that run on the same node, and which can create a\n  ! shared memory region (via the type MPI_COMM_TYPE_SHARED).\n  ! The call returns a new communicator \"host_comm\", which is created by\n  ! each subgroup. \n  call MPI_COMM_SPLIT_TYPE(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,&\n  MPI_INFO_NULL,  host_comm,ierr)\n  call MPI_COMM_RANK(host_comm,  host_rank,ierr) \n```", "```\n // Split the world communicator into subgroups.\n  MPI_Comm  host_comm;\n  MPI_Comm_split_type(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,  MPI_INFO_NULL,\n  &host_comm);\n  int  host_rank;\n  MPI_Comm_rank(host_comm,  &host_rank); \n```", "```\n myDevice  =  host_rank\n\n  ! Sets the device number and the device type to be used\n  call acc_set_device_num(myDevice,  acc_get_device_type())\n\n  ! Returns the number of devices available on the host\n  numDevice  =  acc_get_num_devices(acc_get_device_type()) \n```", "```\n myDevice  =  host_rank\n\n  ! Sets the device number to use in device constructs by setting the initial value of the default-device-var \n  call omp_set_default_device(myDevice)\n\n  ! Returns the number of devices available for offloading.\n  numDevice  =  omp_get_num_devices() \n```", "```\n int  myDevice  =  host_rank;\n  // Set the device number to use in device constructs.\n  omp_set_default_device(myDevice);\n\n  // Return the number of devices available for offloading.\n  int  numDevice  =  omp_get_num_devices(); \n```", "```\nacc_get_device_num() \n```", "```\nomp_get_device_num() \n```", "```\n ! Initialise MPI communication \n  call MPI_Init(ierr)\n  ! Identify the ID rank (process) \n  call MPI_COMM_RANK(  MPI_COMM_WORLD,  myid,  ierr  )\n  ! Get number of active processes (from 0 to nproc-1) \n  call MPI_COMM_SIZE(  MPI_COMM_WORLD,  nproc,  ierr  )\n\n  ! Split the world communicator into subgroups of commu, each of which\n  ! contains processes that run on the same node, and which can create a\n  ! shared memory region (via the type MPI_COMM_TYPE_SHARED).\n  ! The call returns a new communicator \"host_comm\", which is created by\n  ! each subgroup. \n  call MPI_COMM_SPLIT_TYPE(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,&\n  MPI_INFO_NULL,  host_comm,ierr)\n  call MPI_COMM_RANK(host_comm,  host_rank,ierr)\n\n  ! Gets the node name\n  call MPI_GET_PROCESSOR_NAME(name,  resulten,  ierror)\n\n  myDevice  =  host_rank\n\n  ! Sets the device number and the device type to be used\n  call acc_set_device_num(myDevice,  acc_get_device_type())\n\n  ! Returns the number of devices available on the host\n  numDevice  =  acc_get_num_devices(acc_get_device_type()) \n```", "```\n ! Initialise MPI communication \n  call MPI_Init(ierr)\n  ! Identify the ID rank (process) \n  call MPI_COMM_RANK(  MPI_COMM_WORLD,  myid,  ierr  )\n  ! Get number of active processes (from 0 to nproc-1) \n  call MPI_COMM_SIZE(  MPI_COMM_WORLD,  nproc,  ierr  )\n\n  ! Split the world communicator into subgroups of commu, each of which\n  ! contains processes that run on the same node, and which can create a\n  ! shared memory region (via the type MPI_COMM_TYPE_SHARED).\n  ! The call returns a new communicator \"host_comm\", which is created by\n  ! each subgroup. \n  call MPI_COMM_SPLIT_TYPE(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,&\n  MPI_INFO_NULL,  host_comm,ierr)\n  call MPI_COMM_RANK(host_comm,  host_rank,ierr)\n\n  ! Gets the node name\n  call MPI_GET_PROCESSOR_NAME(name,  resulten,  ierror)\n\n  myDevice  =  host_rank\n\n  ! Sets the device number to use in device constructs by setting the initial value of the default-device-var \n  call omp_set_default_device(myDevice)\n\n  ! Returns the number of devices available for offloading.\n  numDevice  =  omp_get_num_devices() \n```", "```\n // Initialize MPI communication.\n  MPI_Init(&argc,  &argv);\n\n  // Identify the ID rank (process).\n  int  myid,  nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD,  &myid);\n  // Get number of active processes.\n  MPI_Comm_size(MPI_COMM_WORLD,  &nproc);\n\n  // Split the world communicator into subgroups.\n  MPI_Comm  host_comm;\n  MPI_Comm_split_type(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,  MPI_INFO_NULL,\n  &host_comm);\n  int  host_rank;\n  MPI_Comm_rank(host_comm,  &host_rank);\n\n  // Get the node name.\n  int  name_len;\n  char  processor_name[MPI_MAX_PROCESSOR_NAME];\n  MPI_Get_processor_name(processor_name,  &name_len);\n\n  int  myDevice  =  host_rank;\n  // Set the device number to use in device constructs.\n  omp_set_default_device(myDevice);\n\n  // Return the number of devices available for offloading.\n  int  numDevice  =  omp_get_num_devices(); \n```", "```\n !offload f to GPUs\n  !$acc enter data copyin(f)\n\n  !update f: copy f from GPU to CPU\n  !$acc update host(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$acc update device(f) \n```", "```\n !offload f to GPUs\n  !$omp target enter data device(myDevice) map(to:f)\n\n  !update f: copy f from GPU to CPU\n  !$omp target update device(myDevice) from(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$omp target update device(myDevice) to(f) \n```", "```\n// Offload f to GPUs\n#pragma omp target enter data map(to : f[0 : np]) device(myDevice)\n\n  // update f: copy f from GPU to CPU\n#pragma omp target update device(myDevice) from(f)\n\n  if  (myid  <  nproc  -  1)  {\n  MPI_Send(&f[np  -  1],  1,  MPI_DOUBLE,  myid  +  1,  tag,  MPI_COMM_WORLD);\n  }\n  if  (myid  >  0)  {\n  MPI_Recv(&f[0],  1,  MPI_DOUBLE,  myid  -  1,  tag,  MPI_COMM_WORLD,\n  MPI_STATUS_IGNORE);\n  }\n\n// update f: copy f from CPU to GPU\n#pragma omp target update device(myDevice) to(f) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$acc enter data copyin(f)\n\n  !update f: copy f from GPU to CPU\n  !$acc update host(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$acc update device(f)\n\n  !do something .e.g.\n  !$acc kernels\n  f  =  f/2.\n  !$acc end kernels\n\n  SumToT=0d0\n  !$acc parallel loop reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$acc end parallel loop\n\n  !SumToT is by default copied back to the CPU\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n\n  !$acc exit data delete(f) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$omp target enter data device(myDevice) map(to:f)\n\n  !update f: copy f from GPU to CPU\n  !$omp target update device(myDevice) from(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$omp target update device(myDevice) to(f)\n\n  !do something .e.g.\n  !$omp target teams distribute parallel do\n  do k=1,np\n  f(k)  =  f(k)/2.\n  enddo\n  !$omp end target teams distribute parallel do\n\n  SumToT=0d0\n  !$omp target teams distribute parallel do reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$omp end target teams distribute parallel do \n\n  !SumToT is by default copied back to the CPU\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n\n  !$omp target exit data map(delete:f) \n```", "```\n MPI_Scatter(f_send.data(),  np,  MPI_DOUBLE,  f,  np,  MPI_DOUBLE,  0,\n  MPI_COMM_WORLD);\n\n// Offload f to GPUs\n#pragma omp target enter data map(to : f[0 : np]) device(myDevice)\n\n  // update f: copy f from GPU to CPU\n#pragma omp target update device(myDevice) from(f)\n\n  if  (myid  <  nproc  -  1)  {\n  MPI_Send(&f[np  -  1],  1,  MPI_DOUBLE,  myid  +  1,  tag,  MPI_COMM_WORLD);\n  }\n  if  (myid  >  0)  {\n  MPI_Recv(&f[0],  1,  MPI_DOUBLE,  myid  -  1,  tag,  MPI_COMM_WORLD,\n  MPI_STATUS_IGNORE);\n  }\n\n// update f: copy f from CPU to GPU\n#pragma omp target update device(myDevice) to(f)\n\n// Update, operate, and offload data back to GPUs\n#pragma omp target teams distribute parallel for device(myDevice)\n  for  (int  k  =  0;  k  <  np;  ++k)  {\n  f[k]  /=  2.0;\n  }\n\n  double  SumToT  =  0.0;\n\n#pragma omp target teams distribute parallel for reduction(+ : SumToT)         \\\n device(myDevice)\n  for  (int  k  =  0;  k  <  np;  ++k)  {\n  SumToT  +=  f[k];\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE,  &SumToT,  1,  MPI_DOUBLE,  MPI_SUM,  MPI_COMM_WORLD);\n\n#pragma omp target exit data map(delete : f[0 : np]) \n```", "```\n !Device pointer f will be passed to MPI_send & MPI_recv\n  !$acc host_data use_device(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$acc end host_data \n```", "```\n !Device pointer f will be passed to MPI_send & MPI_recv\n  !$omp target data use_device_ptr(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$omp end target data \n```", "```\n#pragma omp target data use_device_ptr(f)\n  {\n  if  (myid  <  nproc  -  1)  {\n  MPI_Send(&f[np  -  1],  1,  MPI_DOUBLE,  myid  +  1,  tag,  MPI_COMM_WORLD);\n  }\n\n  if  (myid  >  0)  {\n  MPI_Recv(&f[0],  1,  MPI_DOUBLE,  myid  -  1,  tag,  MPI_COMM_WORLD,\n  MPI_STATUS_IGNORE);\n  }\n  } \n```", "```\n !$acc data copy(SumToT)\n  !$acc host_data use_device(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$acc end host_data\n  !$acc end data \n```", "```\n !$omp target enter data device(myDevice) map(to:SumToT)\n  !$omp target data use_device_ptr(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$omp end target data\n  !$omp target exit data map(from:SumToT) \n```", "```\n#pragma omp target enter data device(myDevice) map(to : SumToT)\n  double  *SumToTPtr  =  &SumToT;\n#pragma omp target data use_device_ptr(SumToTPtr)\n  {\n  MPI_Allreduce(MPI_IN_PLACE,  SumToTPtr,  1,  MPI_DOUBLE,  MPI_SUM,\n  MPI_COMM_WORLD);\n  }\n#pragma omp target exit data map(from : SumToT) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$acc enter data copyin(f)\n\n  !Device pointer f will be passed to MPI_send & MPI_recv\n  !$acc host_data use_device(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$acc end host_data\n\n  !do something .e.g.\n  !$acc kernels\n  f  =  f/2.\n  !$acc end kernels\n\n  SumToT=0d0\n  !$acc parallel loop reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$acc end parallel loop\n\n  !SumToT is by default copied back to the CPU\n\n  !$acc data copy(SumToT)\n  !$acc host_data use_device(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$acc end host_data\n  !$acc end data\n\n  !$acc exit data delete(f) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$omp target enter data device(myDevice) map(to:f)\n\n  !Device pointer f will be passed to MPI_send & MPI_recv\n  !$omp target data use_device_ptr(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$omp end target data\n\n  !do something .e.g.\n  !$omp target teams distribute parallel do\n  do k=1,np\n  f(k)  =  f(k)/2.\n  enddo\n  !$omp end target teams distribute parallel do\n\n  SumToT=0d0\n  !$omp target teams distribute parallel do reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$omp end target teams distribute parallel do \n\n  !SumToT is by default copied back to the CPU\n\n  !$omp target enter data device(myDevice) map(to:SumToT)\n  !$omp target data use_device_ptr(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$omp end target data\n  !$omp target exit data map(from:SumToT)\n\n  !$omp target exit data map(delete:f) \n```", "```\n call  MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload  f  to  GPUs\n  !$omp  target  enter  data  device(myDevice)  map(to:f)\n\n  !Device  pointer  f  will  be  passed  to  MPI_send  &  MPI_recv\n  !$omp  target  data  use_device_ptr(f)\n  if(myid.lt.nproc-1)  then\n  call  MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n  if(myid.gt.0)  then\n  call  MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$omp  end  target  data\n\n  !do  something  .e.g.\n  !$omp  target  teams  distribute  parallel  do\n  do  k=1,np\n  f(k)  =  f(k)/2.\n  enddo\n  !$omp  end  target  teams  distribute  parallel  do\n\n  SumToT=0d0\n  !$omp  target  teams  distribute  parallel  do  reduction(+:SumToT)\n  do  k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$omp  end  target  teams  distribute  parallel  do  \n\n  !SumToT  is  by  default  copied  back  to  the  CPU\n\n  !$omp  target  enter  data  device(myDevice)  map(to:SumToT)\n  !$omp  target  data  use_device_ptr(SumToT)\n  call  MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$omp  end  target  data\n  !$omp  target  exit  data  map(from:SumToT)\n\n  !$omp  target  exit  data  map(delete:f) \n```", "```\n$ ml  LUMI/24.03\n$ ml  PrgEnv-cray\n$ ml  cray-mpich\n$ ml  rocm\n$ ml  craype-accel-amd-gfx90a \n```", "```\n$ ftn  -hacc  -o  mycode.mpiacc.exe  mycode_mpiacc.f90 \n```", "```\n$ ftn  -homp  -o  mycode.mpiomp.exe  mycode_mpiomp.f90 \n```", "```\n$ CC  -fopenmp  -fopenmp-targets=amdgcn-amd-amdhsa  -Xopenmp-target  -march=gfx90a  -o  mycode.mpiomp.exe  mycode_mpiomp.cpp \n```", "```\n$ export MPICH_GPU_SUPPORT_ENABLED=1 \n```", "```\n$ git  clone  https://github.com/ENCCS/gpu-programming.git\n$ cd  gpu-programming/content/examples/exercise_multipleGPU\n$ ls \n```", "```\n ! Split the world communicator into subgroups of commu, each of which\n  ! contains processes that run on the same node, and which can create a\n  ! shared memory region (via the type MPI_COMM_TYPE_SHARED).\n  ! The call returns a new communicator \"host_comm\", which is created by\n  ! each subgroup. \n  call MPI_COMM_SPLIT_TYPE(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,&\n  MPI_INFO_NULL,  host_comm,ierr)\n  call MPI_COMM_RANK(host_comm,  host_rank,ierr) \n```", "```\n // Split the world communicator into subgroups.\n  MPI_Comm  host_comm;\n  MPI_Comm_split_type(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,  MPI_INFO_NULL,\n  &host_comm);\n  int  host_rank;\n  MPI_Comm_rank(host_comm,  &host_rank); \n```", "```\n myDevice  =  host_rank\n\n  ! Sets the device number and the device type to be used\n  call acc_set_device_num(myDevice,  acc_get_device_type())\n\n  ! Returns the number of devices available on the host\n  numDevice  =  acc_get_num_devices(acc_get_device_type()) \n```", "```\n myDevice  =  host_rank\n\n  ! Sets the device number to use in device constructs by setting the initial value of the default-device-var \n  call omp_set_default_device(myDevice)\n\n  ! Returns the number of devices available for offloading.\n  numDevice  =  omp_get_num_devices() \n```", "```\n int  myDevice  =  host_rank;\n  // Set the device number to use in device constructs.\n  omp_set_default_device(myDevice);\n\n  // Return the number of devices available for offloading.\n  int  numDevice  =  omp_get_num_devices(); \n```", "```\nacc_get_device_num() \n```", "```\nomp_get_device_num() \n```", "```\n ! Initialise MPI communication \n  call MPI_Init(ierr)\n  ! Identify the ID rank (process) \n  call MPI_COMM_RANK(  MPI_COMM_WORLD,  myid,  ierr  )\n  ! Get number of active processes (from 0 to nproc-1) \n  call MPI_COMM_SIZE(  MPI_COMM_WORLD,  nproc,  ierr  )\n\n  ! Split the world communicator into subgroups of commu, each of which\n  ! contains processes that run on the same node, and which can create a\n  ! shared memory region (via the type MPI_COMM_TYPE_SHARED).\n  ! The call returns a new communicator \"host_comm\", which is created by\n  ! each subgroup. \n  call MPI_COMM_SPLIT_TYPE(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,&\n  MPI_INFO_NULL,  host_comm,ierr)\n  call MPI_COMM_RANK(host_comm,  host_rank,ierr)\n\n  ! Gets the node name\n  call MPI_GET_PROCESSOR_NAME(name,  resulten,  ierror)\n\n  myDevice  =  host_rank\n\n  ! Sets the device number and the device type to be used\n  call acc_set_device_num(myDevice,  acc_get_device_type())\n\n  ! Returns the number of devices available on the host\n  numDevice  =  acc_get_num_devices(acc_get_device_type()) \n```", "```\n ! Initialise MPI communication \n  call MPI_Init(ierr)\n  ! Identify the ID rank (process) \n  call MPI_COMM_RANK(  MPI_COMM_WORLD,  myid,  ierr  )\n  ! Get number of active processes (from 0 to nproc-1) \n  call MPI_COMM_SIZE(  MPI_COMM_WORLD,  nproc,  ierr  )\n\n  ! Split the world communicator into subgroups of commu, each of which\n  ! contains processes that run on the same node, and which can create a\n  ! shared memory region (via the type MPI_COMM_TYPE_SHARED).\n  ! The call returns a new communicator \"host_comm\", which is created by\n  ! each subgroup. \n  call MPI_COMM_SPLIT_TYPE(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,&\n  MPI_INFO_NULL,  host_comm,ierr)\n  call MPI_COMM_RANK(host_comm,  host_rank,ierr)\n\n  ! Gets the node name\n  call MPI_GET_PROCESSOR_NAME(name,  resulten,  ierror)\n\n  myDevice  =  host_rank\n\n  ! Sets the device number to use in device constructs by setting the initial value of the default-device-var \n  call omp_set_default_device(myDevice)\n\n  ! Returns the number of devices available for offloading.\n  numDevice  =  omp_get_num_devices() \n```", "```\n // Initialize MPI communication.\n  MPI_Init(&argc,  &argv);\n\n  // Identify the ID rank (process).\n  int  myid,  nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD,  &myid);\n  // Get number of active processes.\n  MPI_Comm_size(MPI_COMM_WORLD,  &nproc);\n\n  // Split the world communicator into subgroups.\n  MPI_Comm  host_comm;\n  MPI_Comm_split_type(MPI_COMM_WORLD,  MPI_COMM_TYPE_SHARED,  0,  MPI_INFO_NULL,\n  &host_comm);\n  int  host_rank;\n  MPI_Comm_rank(host_comm,  &host_rank);\n\n  // Get the node name.\n  int  name_len;\n  char  processor_name[MPI_MAX_PROCESSOR_NAME];\n  MPI_Get_processor_name(processor_name,  &name_len);\n\n  int  myDevice  =  host_rank;\n  // Set the device number to use in device constructs.\n  omp_set_default_device(myDevice);\n\n  // Return the number of devices available for offloading.\n  int  numDevice  =  omp_get_num_devices(); \n```", "```\n !offload f to GPUs\n  !$acc enter data copyin(f)\n\n  !update f: copy f from GPU to CPU\n  !$acc update host(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$acc update device(f) \n```", "```\n !offload f to GPUs\n  !$omp target enter data device(myDevice) map(to:f)\n\n  !update f: copy f from GPU to CPU\n  !$omp target update device(myDevice) from(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$omp target update device(myDevice) to(f) \n```", "```\n// Offload f to GPUs\n#pragma omp target enter data map(to : f[0 : np]) device(myDevice)\n\n  // update f: copy f from GPU to CPU\n#pragma omp target update device(myDevice) from(f)\n\n  if  (myid  <  nproc  -  1)  {\n  MPI_Send(&f[np  -  1],  1,  MPI_DOUBLE,  myid  +  1,  tag,  MPI_COMM_WORLD);\n  }\n  if  (myid  >  0)  {\n  MPI_Recv(&f[0],  1,  MPI_DOUBLE,  myid  -  1,  tag,  MPI_COMM_WORLD,\n  MPI_STATUS_IGNORE);\n  }\n\n// update f: copy f from CPU to GPU\n#pragma omp target update device(myDevice) to(f) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$acc enter data copyin(f)\n\n  !update f: copy f from GPU to CPU\n  !$acc update host(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$acc update device(f)\n\n  !do something .e.g.\n  !$acc kernels\n  f  =  f/2.\n  !$acc end kernels\n\n  SumToT=0d0\n  !$acc parallel loop reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$acc end parallel loop\n\n  !SumToT is by default copied back to the CPU\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n\n  !$acc exit data delete(f) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$omp target enter data device(myDevice) map(to:f)\n\n  !update f: copy f from GPU to CPU\n  !$omp target update device(myDevice) from(f)\n\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n\n  !update f: copy f from CPU to GPU\n  !$omp target update device(myDevice) to(f)\n\n  !do something .e.g.\n  !$omp target teams distribute parallel do\n  do k=1,np\n  f(k)  =  f(k)/2.\n  enddo\n  !$omp end target teams distribute parallel do\n\n  SumToT=0d0\n  !$omp target teams distribute parallel do reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$omp end target teams distribute parallel do \n\n  !SumToT is by default copied back to the CPU\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n\n  !$omp target exit data map(delete:f) \n```", "```\n MPI_Scatter(f_send.data(),  np,  MPI_DOUBLE,  f,  np,  MPI_DOUBLE,  0,\n  MPI_COMM_WORLD);\n\n// Offload f to GPUs\n#pragma omp target enter data map(to : f[0 : np]) device(myDevice)\n\n  // update f: copy f from GPU to CPU\n#pragma omp target update device(myDevice) from(f)\n\n  if  (myid  <  nproc  -  1)  {\n  MPI_Send(&f[np  -  1],  1,  MPI_DOUBLE,  myid  +  1,  tag,  MPI_COMM_WORLD);\n  }\n  if  (myid  >  0)  {\n  MPI_Recv(&f[0],  1,  MPI_DOUBLE,  myid  -  1,  tag,  MPI_COMM_WORLD,\n  MPI_STATUS_IGNORE);\n  }\n\n// update f: copy f from CPU to GPU\n#pragma omp target update device(myDevice) to(f)\n\n// Update, operate, and offload data back to GPUs\n#pragma omp target teams distribute parallel for device(myDevice)\n  for  (int  k  =  0;  k  <  np;  ++k)  {\n  f[k]  /=  2.0;\n  }\n\n  double  SumToT  =  0.0;\n\n#pragma omp target teams distribute parallel for reduction(+ : SumToT)         \\\n device(myDevice)\n  for  (int  k  =  0;  k  <  np;  ++k)  {\n  SumToT  +=  f[k];\n  }\n\n  MPI_Allreduce(MPI_IN_PLACE,  &SumToT,  1,  MPI_DOUBLE,  MPI_SUM,  MPI_COMM_WORLD);\n\n#pragma omp target exit data map(delete : f[0 : np]) \n```", "```\n !Device pointer f will be passed to MPI_send & MPI_recv\n  !$acc host_data use_device(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$acc end host_data \n```", "```\n !Device pointer f will be passed to MPI_send & MPI_recv\n  !$omp target data use_device_ptr(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$omp end target data \n```", "```\n#pragma omp target data use_device_ptr(f)\n  {\n  if  (myid  <  nproc  -  1)  {\n  MPI_Send(&f[np  -  1],  1,  MPI_DOUBLE,  myid  +  1,  tag,  MPI_COMM_WORLD);\n  }\n\n  if  (myid  >  0)  {\n  MPI_Recv(&f[0],  1,  MPI_DOUBLE,  myid  -  1,  tag,  MPI_COMM_WORLD,\n  MPI_STATUS_IGNORE);\n  }\n  } \n```", "```\n !$acc data copy(SumToT)\n  !$acc host_data use_device(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$acc end host_data\n  !$acc end data \n```", "```\n !$omp target enter data device(myDevice) map(to:SumToT)\n  !$omp target data use_device_ptr(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$omp end target data\n  !$omp target exit data map(from:SumToT) \n```", "```\n#pragma omp target enter data device(myDevice) map(to : SumToT)\n  double  *SumToTPtr  =  &SumToT;\n#pragma omp target data use_device_ptr(SumToTPtr)\n  {\n  MPI_Allreduce(MPI_IN_PLACE,  SumToTPtr,  1,  MPI_DOUBLE,  MPI_SUM,\n  MPI_COMM_WORLD);\n  }\n#pragma omp target exit data map(from : SumToT) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$acc enter data copyin(f)\n\n  !Device pointer f will be passed to MPI_send & MPI_recv\n  !$acc host_data use_device(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$acc end host_data\n\n  !do something .e.g.\n  !$acc kernels\n  f  =  f/2.\n  !$acc end kernels\n\n  SumToT=0d0\n  !$acc parallel loop reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$acc end parallel loop\n\n  !SumToT is by default copied back to the CPU\n\n  !$acc data copy(SumToT)\n  !$acc host_data use_device(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$acc end host_data\n  !$acc end data\n\n  !$acc exit data delete(f) \n```", "```\n call MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload f to GPUs\n  !$omp target enter data device(myDevice) map(to:f)\n\n  !Device pointer f will be passed to MPI_send & MPI_recv\n  !$omp target data use_device_ptr(f)\n  if(myid.lt.nproc-1)  then\n call MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n if(myid.gt.0)  then\n call MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$omp end target data\n\n  !do something .e.g.\n  !$omp target teams distribute parallel do\n  do k=1,np\n  f(k)  =  f(k)/2.\n  enddo\n  !$omp end target teams distribute parallel do\n\n  SumToT=0d0\n  !$omp target teams distribute parallel do reduction(+:SumToT)\n  do k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$omp end target teams distribute parallel do \n\n  !SumToT is by default copied back to the CPU\n\n  !$omp target enter data device(myDevice) map(to:SumToT)\n  !$omp target data use_device_ptr(SumToT)\n  call MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$omp end target data\n  !$omp target exit data map(from:SumToT)\n\n  !$omp target exit data map(delete:f) \n```", "```\n call  MPI_Scatter(f_send,np,MPI_DOUBLE_PRECISION,f,  np,MPI_DOUBLE_PRECISION,0,MPI_COMM_WORLD,  ierr)\n\n  !offload  f  to  GPUs\n  !$omp  target  enter  data  device(myDevice)  map(to:f)\n\n  !Device  pointer  f  will  be  passed  to  MPI_send  &  MPI_recv\n  !$omp  target  data  use_device_ptr(f)\n  if(myid.lt.nproc-1)  then\n  call  MPI_Send(f(np:np),1,MPI_DOUBLE_PRECISION,myid+1,tag,MPI_COMM_WORLD,  ierr)\n  endif\n\n  if(myid.gt.0)  then\n  call  MPI_Recv(f(1),1,MPI_DOUBLE_PRECISION,myid-1,tag,MPI_COMM_WORLD,  status,ierr)\n  endif\n  !$omp  end  target  data\n\n  !do  something  .e.g.\n  !$omp  target  teams  distribute  parallel  do\n  do  k=1,np\n  f(k)  =  f(k)/2.\n  enddo\n  !$omp  end  target  teams  distribute  parallel  do\n\n  SumToT=0d0\n  !$omp  target  teams  distribute  parallel  do  reduction(+:SumToT)\n  do  k=1,np\n  SumToT  =  SumToT  +  f(k)\n  enddo\n  !$omp  end  target  teams  distribute  parallel  do  \n\n  !SumToT  is  by  default  copied  back  to  the  CPU\n\n  !$omp  target  enter  data  device(myDevice)  map(to:SumToT)\n  !$omp  target  data  use_device_ptr(SumToT)\n  call  MPI_ALLREDUCE(MPI_IN_PLACE,SumToT,1,MPI_DOUBLE_PRECISION,MPI_SUM,MPI_COMM_WORLD,ierr  )\n  !$omp  end  target  data\n  !$omp  target  exit  data  map(from:SumToT)\n\n  !$omp  target  exit  data  map(delete:f) \n```", "```\n$ ml  LUMI/24.03\n$ ml  PrgEnv-cray\n$ ml  cray-mpich\n$ ml  rocm\n$ ml  craype-accel-amd-gfx90a \n```", "```\n$ ftn  -hacc  -o  mycode.mpiacc.exe  mycode_mpiacc.f90 \n```", "```\n$ ftn  -homp  -o  mycode.mpiomp.exe  mycode_mpiomp.f90 \n```", "```\n$ CC  -fopenmp  -fopenmp-targets=amdgcn-amd-amdhsa  -Xopenmp-target  -march=gfx90a  -o  mycode.mpiomp.exe  mycode_mpiomp.cpp \n```", "```\n$ export MPICH_GPU_SUPPORT_ENABLED=1 \n```", "```\n$ git  clone  https://github.com/ENCCS/gpu-programming.git\n$ cd  gpu-programming/content/examples/exercise_multipleGPU\n$ ls \n```"]