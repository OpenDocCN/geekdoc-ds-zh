<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>1.5. Exercises#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>1.5. Exercises#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap01_intro/exercises/roch-mmids-intro-exercises.html">https://mmids-textbook.github.io/chap01_intro/exercises/roch-mmids-intro-exercises.html</a></blockquote>

<section id="warm-up-worksheets">
<h2><span class="section-number">1.5.1. </span>Warm-up worksheets<a class="headerlink" href="#warm-up-worksheets" title="Link to this heading">#</a></h2>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>Section 1.2</strong></p>
<p><strong>E1.2.1</strong> Calculate the Euclidean norm of the vector <span class="math notranslate nohighlight">\(\mathbf{x} = (6, 8)\)</span>.</p>
<p><strong>E1.2.2</strong> Compute the inner product <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{v} \rangle\)</span> for vectors <span class="math notranslate nohighlight">\(\mathbf{u} = (1, 2, 3)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v} = (4, 5, 6)\)</span>.</p>
<p><strong>E1.2.3</strong> Find the transpose of the matrix <span class="math notranslate nohighlight">\(A = \begin{pmatrix}1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6\end{pmatrix}\)</span>.</p>
<p><strong>E1.2.4</strong> Compute the Frobenius norm of the matrix <span class="math notranslate nohighlight">\(A = \begin{pmatrix}1 &amp; 2 \\ 3 &amp; 4\end{pmatrix}\)</span>.</p>
<p><strong>E1.2.5</strong> Verify if the matrix <span class="math notranslate nohighlight">\(A = \begin{pmatrix}2 &amp; 0 \\ 0 &amp; 3\end{pmatrix}\)</span> is symmetric.</p>
<p><strong>E1.2.6</strong> Let <span class="math notranslate nohighlight">\(A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\)</span> and <span class="math notranslate nohighlight">\(B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}\)</span>. Compute <span class="math notranslate nohighlight">\(AB\)</span> and <span class="math notranslate nohighlight">\(BA\)</span>.</p>
<p><strong>E1.2.7</strong> Let <span class="math notranslate nohighlight">\(f(x, y) = x^2 + xy + y^2\)</span>. Compute the partial derivatives <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y}\)</span>.</p>
<p><strong>E1.2.8</strong> Given the function <span class="math notranslate nohighlight">\(g(x, y) = x^2 y + xy^3\)</span>, compute its partial derivatives <span class="math notranslate nohighlight">\(\frac{\partial g}{\partial x}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial g}{\partial y}\)</span>.</p>
<p><strong>E1.2.9</strong> Let <span class="math notranslate nohighlight">\(f(x) = x^3 - 3x^2 + 2x\)</span>. Use <em>Taylor’s Theorem</em> to find a linear approximation of <span class="math notranslate nohighlight">\(f\)</span> around <span class="math notranslate nohighlight">\(a = 1\)</span> with a second-order error term.</p>
<p><strong>E1.2.10</strong> Compute the expectation <span class="math notranslate nohighlight">\(\E[X]\)</span> of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(\P(X = 1) = 0.2\)</span>, <span class="math notranslate nohighlight">\(\P(X = 2) = 0.5\)</span>, and <span class="math notranslate nohighlight">\(\P(X = 3) = 0.3\)</span>.</p>
<p><strong>E1.2.11:</strong> Calculate the variance <span class="math notranslate nohighlight">\(\mathrm{Var}[X]\)</span> of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(\P(X = 1) = 0.4\)</span>, <span class="math notranslate nohighlight">\(\P(X = 2) = 0.6\)</span>, and <span class="math notranslate nohighlight">\(\E[X] = 1.6\)</span>.</p>
<p><strong>E1.2.12</strong> Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be random variables with <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 2\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[Y] = 3\)</span>, <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = 4\)</span>, and <span class="math notranslate nohighlight">\(\mathrm{Var}[Y] = 9\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[2X + Y - 1]\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[2X + Y - 1]\)</span>, assuming <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent.</p>
<p><strong>E1.2.13</strong> Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable with <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 3\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = 4\)</span>. Use <em>Chebyshev’s Inequality</em> to bound <span class="math notranslate nohighlight">\(\P[|X - 3| \geq 4]\)</span>.</p>
<p><strong>E1.2.14</strong> A random variable <span class="math notranslate nohighlight">\(X\)</span> has mean <span class="math notranslate nohighlight">\(\mu = 5\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2 = 4\)</span>. Use <em>Chebyshev’s Inequality</em> to find an upper bound on the probability that <span class="math notranslate nohighlight">\(X\)</span> is more than 3 units away from its mean.</p>
<p><strong>E1.2.15</strong> Given the random vector <span class="math notranslate nohighlight">\(\bX = (X_1, X_2)\)</span> where <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> are independent standard normal random variables, what is the covariance matrix of <span class="math notranslate nohighlight">\(\bX\)</span>?</p>
<p><strong>E1.2.16</strong> Let <span class="math notranslate nohighlight">\(\bX = \begin{pmatrix} X_1 \\ X_2 \end{pmatrix}\)</span> be a random vector with <span class="math notranslate nohighlight">\(\mathbb{E}[X_1] = 1\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[X_2] = 2\)</span>, <span class="math notranslate nohighlight">\(\text{Var}[X_1] = 3\)</span>, <span class="math notranslate nohighlight">\(\text{Var}[X_2] = 4\)</span>, and <span class="math notranslate nohighlight">\(\text{Cov}[X_1, X_2] = 1\)</span>. Write the mean vector <span class="math notranslate nohighlight">\(\bmu_\bX\)</span> and the covariance matrix <span class="math notranslate nohighlight">\(\bSigma_\bX\)</span> of <span class="math notranslate nohighlight">\(\bX\)</span>.</p>
<p><strong>E1.2.17</strong> Let <span class="math notranslate nohighlight">\(\bX = \begin{pmatrix} X_1 \\ X_2 \end{pmatrix}\)</span> be a random vector with mean <span class="math notranslate nohighlight">\(\bmu_\bX = \begin{pmatrix} 1 \\ 2 \end{pmatrix}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\bSigma_\bX = \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 4 \end{pmatrix}\)</span>. Let <span class="math notranslate nohighlight">\(A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[A\bX]\)</span> and <span class="math notranslate nohighlight">\(\text{Cov}[A\bX]\)</span>.</p>
<p><strong>E1.2.18</strong> Let <span class="math notranslate nohighlight">\(X_1, X_2, X_3\)</span> be independent random variables with <span class="math notranslate nohighlight">\(\mathbb{E}[X_1] = 1\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[X_2] = 2\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[X_3] = 3\)</span>, <span class="math notranslate nohighlight">\(\text{Var}[X_1] = 4\)</span>, <span class="math notranslate nohighlight">\(\text{Var}[X_2] = 5\)</span>, and <span class="math notranslate nohighlight">\(\text{Var}[X_3] = 6\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[X_1 + X_2 + X_3]\)</span> and <span class="math notranslate nohighlight">\(\text{Var}[X_1 + X_2 + X_3]\)</span>.</p>
<p><strong>E1.2.19</strong> Let <span class="math notranslate nohighlight">\(A = \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix}\)</span>. Show that <span class="math notranslate nohighlight">\(A\)</span> is positive definite.</p>
<p><strong>Section 1.3</strong></p>
<p><strong>E1.3.1</strong> Given the data points <span class="math notranslate nohighlight">\(\mathbf{x}_1 = (1, 2)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_2 = (-1, 1)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_3 = (0, -1)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{x}_4 = (2, 0)\)</span>, and the partition <span class="math notranslate nohighlight">\(C_1 = \{1, 4\}\)</span>, <span class="math notranslate nohighlight">\(C_2 = \{2, 3\}\)</span>, compute the optimal cluster representatives <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1^*\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_2^*\)</span>.</p>
<p><strong>E1.3.2</strong> For the data points and partition from E1.3.1, compute the <span class="math notranslate nohighlight">\(k\)</span>-means objective value <span class="math notranslate nohighlight">\(\mathcal{G}(C_1, C_2)\)</span>.</p>
<p><strong>E1.3.3</strong> Given the cluster representatives <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1 = (0, 1)\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_2 = (1, -1)\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_3 = (-2, 0)\)</span>, and the data points <span class="math notranslate nohighlight">\(\mathbf{x}_1 = (1, 1)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_2 = (-1, -1)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_3 = (2, -2)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_4 = (-2, 1)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{x}_5 = (0, 0)\)</span>, find the optimal partition <span class="math notranslate nohighlight">\(C_1, C_2, C_3\)</span>.</p>
<p><strong>E1.3.4</strong> For the data points and cluster representatives from E1.3.3, compute the <span class="math notranslate nohighlight">\(k\)</span>-means objective value <span class="math notranslate nohighlight">\(G(C_1, C_2, C_3; \boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \boldsymbol{\mu}_3)\)</span>.</p>
<p><strong>E1.3.5</strong> Show that for any two cluster representatives <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1, \boldsymbol{\mu}_2 \in \mathbb{R}^d\)</span> and any data point <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span>, the inequality <span class="math notranslate nohighlight">\(\|\mathbf{x} - \boldsymbol{\mu}_1\|^2 \leq \|\mathbf{x} - \boldsymbol{\mu}_2\|^2\)</span> is equivalent to <span class="math notranslate nohighlight">\(2(\boldsymbol{\mu}_2 - \boldsymbol{\mu}_1)^T \mathbf{x} \leq \|\boldsymbol{\mu}_2\|^2 - \|\boldsymbol{\mu}_1\|^2\)</span>.</p>
<p><strong>E1.3.6</strong> Given the cluster assignment matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Z = \begin{bmatrix}
   1 &amp; 0 &amp; 0\\
   0 &amp; 1 &amp; 0\\
   0 &amp; 0 &amp; 1\\
   1 &amp; 0 &amp; 0\\
   0 &amp; 1 &amp; 0
   \end{bmatrix}
\end{split}\]</div>
<p>and the cluster representative matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
U = \begin{bmatrix}
   1 &amp; 2\\
   -1 &amp; 0\\
   0 &amp; -2
\end{bmatrix},
\end{split}\]</div>
<p>compute the matrix product <span class="math notranslate nohighlight">\(ZU\)</span>.</p>
<p><strong>E1.3.7</strong> For any two matrices <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span>, prove that <span class="math notranslate nohighlight">\(\|A + B\|_F^2 = \|A\|_F^2 + \|B\|_F^2 + 2 \sum_{i=1}^n \sum_{j=1}^m A_{ij} B_{ij}\)</span>.</p>
<p><strong>E1.3.8</strong> Show that for any matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> and any scalar <span class="math notranslate nohighlight">\(c \in \mathbb{R}\)</span>, it holds that <span class="math notranslate nohighlight">\(\|cA\|_F = |c| \|A\|_F\)</span>.</p>
<p><strong>E1.3.9</strong> Complete the square for the quadratic function <span class="math notranslate nohighlight">\(q(x) = 3x^2 - 6x + 5\)</span>. What is the minimum value of <span class="math notranslate nohighlight">\(q\)</span> and where is it attained?</p>
<p><strong>E1.3.10</strong> Let <span class="math notranslate nohighlight">\(f_1(x) = x^2 + 1\)</span> and <span class="math notranslate nohighlight">\(f_2(y) = 2y^2 - 3\)</span>. Define <span class="math notranslate nohighlight">\(h(x, y) = f_1(x) + f_2(y)\)</span>. Find the global minimum of <span class="math notranslate nohighlight">\(h(x, y)\)</span>.</p>
<p><strong>E1.3.11</strong> Compute the Frobenius norm of the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 2 &amp; -1 \\ 0 &amp; 3 \end{bmatrix}.
\end{split}\]</div>
<p><strong>Section 1.4</strong></p>
<p><strong>E1.4.1</strong> Let <span class="math notranslate nohighlight">\(X_1, \dots, X_d\)</span> be independent random variables uniformly distributed on <span class="math notranslate nohighlight">\([-1/2, 1/2]\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[X_i]\)</span> and <span class="math notranslate nohighlight">\(\text{Var}[X_i]\)</span>.</p>
<p><strong>E1.4.2</strong> Let <span class="math notranslate nohighlight">\(\bX = (X_1, \dots, X_d)\)</span> be a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector where each <span class="math notranslate nohighlight">\(X_i\)</span> is an independent random variable uniformly distributed on <span class="math notranslate nohighlight">\([-1/2, 1/2]\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[\|\bX\|^2]\)</span>.</p>
<p><strong>E1.4.3</strong> Let <span class="math notranslate nohighlight">\(\bX = (X_1, \dots, X_d)\)</span> be a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector where each <span class="math notranslate nohighlight">\(X_i\)</span> is an independent standard normal random variable. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[\|\bX\|^2]\)</span>.</p>
<p><strong>E1.4.4</strong> Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_d\)</span> be independent random variables with <span class="math notranslate nohighlight">\(E[X_i] = \mu\)</span> and <span class="math notranslate nohighlight">\(Var[X_i] = \sigma^2\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>. Find <span class="math notranslate nohighlight">\(\E[\sum_{i=1}^d X_i]\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[\sum_{i=1}^d X_i]\)</span>.</p>
<p><strong>E1.4.5</strong> Given a random variable <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(\E[X] = 1\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = 4\)</span>, bound <span class="math notranslate nohighlight">\(\P[|X - 1| \geq 3]\)</span> using <em>Chebyshev’s Inequality</em>.</p>
<p><strong>E1.4.6</strong> Let <span class="math notranslate nohighlight">\(\bX = (X_1, \ldots, X_d)\)</span> be a random vector with independent components, each following <span class="math notranslate nohighlight">\(U[-\frac{1}{2}, \frac{1}{2}]\)</span>. Compute <span class="math notranslate nohighlight">\(\P[\|\bX\| \leq \frac{1}{2}]\)</span> explicitly for <span class="math notranslate nohighlight">\(d = 1, 2, 3\)</span>. [Hint: Use a geometric arugment.]</p>
<p><strong>E1.4.7</strong> Given a random variable <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(\E[X] = 0\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = 1\)</span>, bound <span class="math notranslate nohighlight">\(\P[|X| \geq 2\sqrt{d}]\)</span> using <em>Chebyshev’s Inequality</em> for <span class="math notranslate nohighlight">\(d = 1, 10, 100\)</span>.</p>
<p><strong>E1.4.8</strong> Let <span class="math notranslate nohighlight">\(\bX = (X_1, \ldots, X_d)\)</span> be a random vector with independent components, each following <span class="math notranslate nohighlight">\(U[-\frac{1}{2}, \frac{1}{2}]\)</span>. Find an upper bound for <span class="math notranslate nohighlight">\(\P[\|\bX\| \geq \frac{1}{2}]\)</span> using <em>Chebyshev’s Inequality</em> for <span class="math notranslate nohighlight">\(d = 1, 10, 100\)</span>.</p>
</section>
<section id="problems">
<h2><span class="section-number">1.5.2. </span>Problems<a class="headerlink" href="#problems" title="Link to this heading">#</a></h2>
<p><strong>1.1</strong> (Adapted from [VLMS]) Recall that for two vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> of the same dimension, their inner product is <span class="math notranslate nohighlight">\(\mathbf{x}^T \mathbf{y}\)</span>. A vector is said to be nonnegative if <em>all</em> of its entries are <span class="math notranslate nohighlight">\(\geq 0\)</span>.</p>
<p>a) Show that the inner product of two nonnegative vectors is necessarily <span class="math notranslate nohighlight">\(\geq 0\)</span>.</p>
<p>b) Suppose the inner product of two nonnegative vectors is <em>zero</em>. What can you say about their sparsity patterns, i.e., which of their entries are zero or nonzero.<span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.2</strong> (Adapted from [VLMS]) A Boolean <span class="math notranslate nohighlight">\(n\)</span>-vector is one for which all entries are either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. Such vectors are used to encode whether each of <span class="math notranslate nohighlight">\(n\)</span> conditions holds, with <span class="math notranslate nohighlight">\(a_i = 1\)</span> meaning that condition <span class="math notranslate nohighlight">\(i\)</span> holds.</p>
<p>a) Another common encoding of the same information uses the two values <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> for the entries. For example the Boolean vector <span class="math notranslate nohighlight">\((0,1,1,0)\)</span> would be written using this alternative encoding as <span class="math notranslate nohighlight">\((-1, 1, 1, -1)\)</span>. Suppose that <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a Boolean vector, and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is a vector encoding the same information using the values <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Express <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> in terms of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> using vector notation. Also, express <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in terms of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> using vector notation.</p>
<p>b) Suppose that <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> are Boolean <span class="math notranslate nohighlight">\(n\)</span>-vectors. Give a simple word description of their squared Euclidean distance <span class="math notranslate nohighlight">\(\|\mathbf{x} - \mathbf{y}\|_2^2\)</span> and justify it mathematically. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.3</strong> (Adapted from [VLMS])  If <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> is a vector, then <span class="math notranslate nohighlight">\(\mathbf{a}_{r:s}\)</span> is the vector of size <span class="math notranslate nohighlight">\(s - r + 1\)</span>, with entries <span class="math notranslate nohighlight">\(a_r,\ldots,a_s\)</span>, i.e., <span class="math notranslate nohighlight">\(\mathbf{a}_{r:s} = (a_r,\ldots,a_s)\)</span>. The vector <span class="math notranslate nohighlight">\(\mathbf{a}_{r:s}\)</span> is called a slice. As a more concrete example, if <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is the <span class="math notranslate nohighlight">\(4\)</span>-vector <span class="math notranslate nohighlight">\((1, -1, 2, 0)\)</span>, the slice <span class="math notranslate nohighlight">\(\mathbf{z}_{2:3} = (-1, 2)\)</span>. Suppose the <span class="math notranslate nohighlight">\(T\)</span>-vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> represents a time series or signal. The quantity</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}(\mathbf{x})=(x_1 - x_2)^2 + (x_2 - x_3)^2 + \cdots+(x_{T-1} - x_T)^2,
\]</div>
<p>the sum of the differences of adjacent values of the signal, is called the Dirichlet energy of the signal. The Dirichlet energy is a measure of the roughness or wiggliness of the time series.</p>
<p>a) Express <span class="math notranslate nohighlight">\(\mathcal{D}(\mathbf{x})\)</span> in vector notation using slicing. [<em>Hint:</em> Note the similarity between <span class="math notranslate nohighlight">\(\mathcal{D}(\mathbf{x})\)</span> and the squared Euclidean distance between two vectors.]</p>
<p>b) How small can <span class="math notranslate nohighlight">\(\mathcal{D}(\mathbf{x})\)</span> be? What signals <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> have this minimum value of the Dirichlet energy?</p>
<p>c) Find a signal <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with entries no more than one in absolute value that has the largest possible value of <span class="math notranslate nohighlight">\(\mathcal{D}(\mathbf{x})\)</span>. Give the value of the Dirichlet energy achieved. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.4</strong> (Adapted from [VLMS]) A vector of length <span class="math notranslate nohighlight">\(n\)</span> can represent the number of times each word in a dictionary of <span class="math notranslate nohighlight">\(n\)</span> words appears in a document. For example, <span class="math notranslate nohighlight">\((25,2,0)\)</span> means that the first dictionary word appears <span class="math notranslate nohighlight">\(25\)</span> times, the second one twice, and the third one not at all. Suppose the <span class="math notranslate nohighlight">\(n\)</span>-vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is the word count vector associated with a document and a dictionary of <span class="math notranslate nohighlight">\(n\)</span> words. For simplicity we will assume that all words in the document appear in the dictionary.</p>
<p>a) What is <span class="math notranslate nohighlight">\(\mathbf{1}^T \mathbf{w}\)</span>? Here <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is an all-one vector of the appropriate size.</p>
<p>b) What does <span class="math notranslate nohighlight">\(w_{282} = 0\)</span> mean?</p>
<p>c) Let <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> be the <span class="math notranslate nohighlight">\(n\)</span>-vector that gives the histogram of the word counts, i.e., <span class="math notranslate nohighlight">\(h_i\)</span> is the fraction of the words in the document that are word <span class="math notranslate nohighlight">\(i\)</span>. Use vector notation to express <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> in terms of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>. (You can assume that the document contains at least one word.) <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.5</strong> (Adapted from [VLMS]) Suppose <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> are vectors of the same size. The triangle inequality states that <span class="math notranslate nohighlight">\(\|\mathbf{a} + \mathbf{b}\|_2 \leq \|\mathbf{a}\|_2 + \|\mathbf{b}\|_2\)</span>. Show that we also have <span class="math notranslate nohighlight">\(\|\mathbf{a} + \mathbf{b}\|_2 \geq \|\mathbf{a}\|_2 - \|\mathbf{b}\|_2\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.6</strong> (Adapted from [VLMS]) Verify that the following identities hold for any two vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> of the same size.</p>
<p>a) <span class="math notranslate nohighlight">\((\mathbf{a}+\mathbf{b})^T(\mathbf{a} - \mathbf{b})=\|\mathbf{a}\|_2^2 - \|\mathbf{b}\|_2^2\)</span>.</p>
<p>b) <span class="math notranslate nohighlight">\(\|\mathbf{a} + \mathbf{b}\|_2^2 + \|\mathbf{a} - \mathbf{b}\|_2^2 = 2(\|\mathbf{a}\|_2^2 + \|\mathbf{b}\|_2^2)\)</span>. This is called the <em>Parallelogram Law</em>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.7</strong> (Adapted from [VLMS]) The root-mean-square (RMS) value of an <span class="math notranslate nohighlight">\(n\)</span>-vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathrm{rms}(\mathbf{x}) 
= \sqrt{\frac{x_1^2 + \cdots + x_n^2}{n}}
= \frac{\|\mathbf{x}\|_2}{\sqrt{n}}.
\]</div>
<p>a) Show that at least one entry of a vector has absolute value at least as large as the RMS value of the vector.</p>
<p>b) For an <span class="math notranslate nohighlight">\(n\)</span>-vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, let <span class="math notranslate nohighlight">\(\mathrm{avg}(\mathbf{x}) = \mathbf{1}^T \mathbf{x}/n\)</span> and</p>
<div class="math notranslate nohighlight">
\[
\mathrm{std}(\mathbf{x}) = \frac{\|\mathbf{x} - \mathrm{avg}(\mathbf{x}) \mathbf{1}\|_2}{\sqrt{n}}.
\]</div>
<p>Establish the identity</p>
<div class="math notranslate nohighlight">
\[
\mathrm{rms}(\mathbf{x})^2 = \mathrm{avg}(\mathbf{x})^2 + \mathrm{std}(\mathbf{x})^2.
\]</div>
<p>c) Use (b) to show that <span class="math notranslate nohighlight">\(|\mathrm{avg}(\mathbf{x})| \leq \mathrm{rms}(\mathbf{x})\)</span>.</p>
<p>d) Use (b) to show that <span class="math notranslate nohighlight">\(\mathrm{std}(\mathbf{x}) \leq \mathrm{rms}(\mathbf{x})\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.8</strong> (Adapted from [VLMS]) Suppose that the vectors <span class="math notranslate nohighlight">\(\mathbf{x}_1,\ldots,\mathbf{x}_N\)</span> are clustered using <span class="math notranslate nohighlight">\(k\)</span>-means, with group representatives <span class="math notranslate nohighlight">\(\mathbf{z}_1,\ldots,\mathbf{z}_k\)</span>.</p>
<p>a) Suppose the original vectors <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> are nonnegative, i.e., their entries are nonnegative. Explain why the representatives <span class="math notranslate nohighlight">\(\mathbf{z}_j\)</span> are also nonnegative.</p>
<p>b) Suppose the original vectors <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> represent proportions, i.e., their entries are nonnegative and sum to one. Explain why the representatives <span class="math notranslate nohighlight">\(\mathbf{z}_j\)</span> also represent proportions, i.e., their entries are nonnegative and sum to one.</p>
<p>c) Suppose the original vectors <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> are Boolean, i.e., their entries are either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. Give an interpretation of <span class="math notranslate nohighlight">\((\mathbf{z}_j)_i\)</span>, the <span class="math notranslate nohighlight">\(i\)</span>-th entry of the <span class="math notranslate nohighlight">\(j\)</span>-th group representative. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.9</strong> (Adapted from [VLMS]) Clustering a collection of vectors into <span class="math notranslate nohighlight">\(k = 2\)</span> groups is called <span class="math notranslate nohighlight">\(2\)</span>-way partitioning, since we are partitioning the vectors into <span class="math notranslate nohighlight">\(2\)</span> groups, with index sets <span class="math notranslate nohighlight">\(G_1\)</span> and <span class="math notranslate nohighlight">\(G_2\)</span>. Suppose we run <span class="math notranslate nohighlight">\(k\)</span>-means with <span class="math notranslate nohighlight">\(k = 2\)</span> on the <span class="math notranslate nohighlight">\(n\)</span>-vectors <span class="math notranslate nohighlight">\(\mathbf{x}_1,\ldots,\mathbf{x}_N\)</span> . Show that there is a nonzero vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and a scalar <span class="math notranslate nohighlight">\(v\)</span> that satisfy</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^T \mathbf{x}_i + v \geq 0, \forall i \in G_1, \quad \mathbf{w}^T \mathbf{x}_i + v \leq 0, \forall i \in G_2.
\]</div>
<p>In other words, the affine function <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + v\)</span> is greater than or equal to zero on the first group, and less than or equal to zero on the second group. This is called linear separation of the two groups. [<em>Hint:</em> Consider the function <span class="math notranslate nohighlight">\(\|\mathbf{x} - \mathbf{z}_1\|_2^2 - \|\mathbf{x} - \mathbf{z}_2\|_2^2\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{z}_2\)</span> are the group representatives.] <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.10</strong> (Adapted from [ASV]) Let <span class="math notranslate nohighlight">\(0 &lt; p \leq 1\)</span>. A random variable <span class="math notranslate nohighlight">\(X\)</span> has the geometric distribution with success parameter <span class="math notranslate nohighlight">\(p\)</span> if the possible values of <span class="math notranslate nohighlight">\(X\)</span> are <span class="math notranslate nohighlight">\(\{1, 2, 3, \ldots\}\)</span> and <span class="math notranslate nohighlight">\(X\)</span> satisfies <span class="math notranslate nohighlight">\(\mathbb{P}(X = k) = (1 - p)^{k - 1}p\)</span> for positive integers <span class="math notranslate nohighlight">\(k\)</span>. Its mean is <span class="math notranslate nohighlight">\(1/p\)</span> and its variance is <span class="math notranslate nohighlight">\((1 - p)/p^2\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a geometric random variable with parameter <span class="math notranslate nohighlight">\(p = 1/6\)</span>.</p>
<p>a) Use <em>Markov’s Inequality</em> to find an upper bound for <span class="math notranslate nohighlight">\(\mathbb{P}(X \geq 16)\)</span>.</p>
<p>b) Use <em>Chebyshev’s Inequality</em> to find an upper bound for <span class="math notranslate nohighlight">\(\mathbb{P}(X \geq 16)\)</span>.</p>
<p>c) Use the sum of a geometric series to explicitly compute the probability <span class="math notranslate nohighlight">\(\mathbb{P}(X \geq 16)\)</span> and compare with the upper bounds you derived. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.11</strong> (Adapted from [ASV]) Let <span class="math notranslate nohighlight">\(0 &lt; \lambda &lt; +\infty\)</span>. A random variable <span class="math notranslate nohighlight">\(X\)</span> has the exponential distribution with parameter <span class="math notranslate nohighlight">\(\lambda\)</span> if <span class="math notranslate nohighlight">\(X\)</span> has density function
<span class="math notranslate nohighlight">\(f(x) = \lambda e^{-\lambda x}\)</span>, for <span class="math notranslate nohighlight">\(x \geq 0\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise. Its mean is <span class="math notranslate nohighlight">\(1/\lambda\)</span> and its variance is <span class="math notranslate nohighlight">\(1/\lambda^2\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be an exponential random variable with parameter <span class="math notranslate nohighlight">\(\lambda = 1/2\)</span>.</p>
<p>a) Use <em>Markov’s Inequality</em> to find an upper bound for <span class="math notranslate nohighlight">\(\mathbb{P}(X &gt; 6)\)</span>.</p>
<p>b) Use <em>Chebyshev’s Inequality</em> to find an upper bound for <span class="math notranslate nohighlight">\(\mathbb{P}(X &gt; 6)\)</span>.</p>
<p>c) Use an integral to explicitly compute the probability <span class="math notranslate nohighlight">\(\mathbb{P}(X &gt; 6)\)</span> and compare with the upper bounds you derived. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.12</strong> (Adapted from [ASV]) Suppose that <span class="math notranslate nohighlight">\(X\)</span> is a nonnegative random variable with <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 10\)</span>.</p>
<p>a) Use <em>Markov’s Inequality</em> to give an upper bound on the probability that <span class="math notranslate nohighlight">\(X\)</span> is larger than <span class="math notranslate nohighlight">\(15\)</span>.</p>
<p>b) Suppose that we also know that <span class="math notranslate nohighlight">\(\mathrm{Var}(X) = 3\)</span>. Use <em>Chebyshev’s Inequality</em> to give a better upper bound on
<span class="math notranslate nohighlight">\(\mathbb{P}(X &gt; 15)\)</span> than in part (a).</p>
<p>c) Suppose that <span class="math notranslate nohighlight">\(Y_1,Y_2,\ldots,Y_{300}\)</span> are i.i.d. random variables with the same distribution as <span class="math notranslate nohighlight">\(X\)</span>, so that, in particular <span class="math notranslate nohighlight">\(\mathbb{E}(Y_i) = 10\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}(Y_i) = 3\)</span>. Use <em>Chebyshev’s Inequality</em> to give an upper bound on the
probability that <span class="math notranslate nohighlight">\(\sum_{i=1}^{300} Y_i\)</span> is larger than <span class="math notranslate nohighlight">\(3060\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.13</strong> (Adapted from [ASV]) Suppose that we have i.i.d. random variables <span class="math notranslate nohighlight">\(X_1, X_2, X_3,\ldots\)</span> with finite mean <span class="math notranslate nohighlight">\(\mathbb{E}[X_1] = \mu\)</span> and variance <span class="math notranslate nohighlight">\(\mathrm{Var}(X_1) = \sigma^2\)</span>. Let <span class="math notranslate nohighlight">\(S_n = X_1 +\cdots+X_n\)</span>. Prove that for any fixed <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> and <span class="math notranslate nohighlight">\(1/2 &lt; \alpha &lt; 1\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\lim_{n \to +\infty} \mathbb{P}\left[\left|\frac{S_n - n\mu}{n^\alpha}\right| &lt; \varepsilon\right] = 1.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.14</strong> (Adapted from [ASV]) By mimicking the proof of <em>Weak Law of Large Numbers</em>, prove the following variant. Suppose that we have random variables <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> each with finite mean <span class="math notranslate nohighlight">\(\mathbb{E}[X_i] = \mu\)</span> and variance <span class="math notranslate nohighlight">\(\mathrm{Var}(X_i) = \sigma^2\)</span>. Suppose further that <span class="math notranslate nohighlight">\(\mathrm{Cov}(X_i,X_j) = 0\)</span> whenever <span class="math notranslate nohighlight">\(|i - j| \geq 2\)</span> and that there is a constant <span class="math notranslate nohighlight">\(c &gt; 0\)</span> so that <span class="math notranslate nohighlight">\(|\mathrm{Cov}(X_i, X_{i+1})| &lt; c\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>. Let <span class="math notranslate nohighlight">\(S_n =X_1 +\cdots+X_n\)</span>. Then for any fixed <span class="math notranslate nohighlight">\(\varepsilon&gt;0\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\lim_{n\to +\infty} \mathbb{P}\left[\left|\frac{S_n}{n} - \mu\right|\right]&lt;\varepsilon=1.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.15</strong> (Adapted from [ASV]) By mimicking the proof of <em>Chebyshev’s Inequality</em>, prove the following variant. Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable with a finite mean <span class="math notranslate nohighlight">\(\mu\)</span> and for which <span class="math notranslate nohighlight">\(\mathbb{E}[\exp(s(X - \mu)] &lt; +\infty\)</span> for some <span class="math notranslate nohighlight">\(s &gt; 0\)</span>. Then we have for <span class="math notranslate nohighlight">\(c &gt; 0\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X \geq \mu + c) \leq \frac{\mathbb{E}[\exp(s(X - \mu)]}{e^{s c}}.
\]</div>
<p>Justify your answer carefully. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.16</strong> (Adapted from [ASV]) Recall that the cumulative distribution function (CDF) of a real-valued random variable <span class="math notranslate nohighlight">\(Z\)</span> is the function <span class="math notranslate nohighlight">\(F_Z(z) = \mathbb{P}[Z \leq z]\)</span> for all <span class="math notranslate nohighlight">\(z \in \mathbb{R}\)</span>. Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> be independent random variables with the same cumulative distribution function <span class="math notranslate nohighlight">\(F\)</span>. Denote the minimum and the maximum by</p>
<div class="math notranslate nohighlight">
\[
Z = \min(X_1,X_2,\ldots,X_n) \quad\text{and}\quad W = \max(X_1,X_2,\ldots,X_n). 
\]</div>
<p>Find the cumulative distribution functions <span class="math notranslate nohighlight">\(F_Z\)</span> and <span class="math notranslate nohighlight">\(F_W\)</span> of <span class="math notranslate nohighlight">\(Z\)</span> and <span class="math notranslate nohighlight">\(W\)</span> respectively. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.17</strong> (Adapted from [ASV]) Suppose that <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> are i.i.d. random variables with Exponential distribution with parameter <span class="math notranslate nohighlight">\(\lambda = 1\)</span> (see Exercise 1.11 above), and let <span class="math notranslate nohighlight">\(M_n = \max(X_1,\ldots,X_n)\)</span>. Show that for any <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\lim_{n \to +\infty} \mathbb{P}(M_n - \ln n \leq x) = \exp(-e^{-x}).
\]</div>
<p>The right hand side is the cumulative distribution function of the Gumbel distribution, an example of an extreme value distribution. [<em>Hint:</em> Use Exercise 1.16 to compute <span class="math notranslate nohighlight">\(\mathbb{P}(M_n \leq \ln n + x)\)</span> explicitly, and then evaluate the limit as <span class="math notranslate nohighlight">\(n \to +\infty\)</span>.] <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.18</strong> (Adapted from [ASV]) Let <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> be two disjoint events. Under what condition are they independent? <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.19</strong> (Adapted from [ASV]) We choose a number from the set <span class="math notranslate nohighlight">\(\{10, 11, 12, \ldots , 99\}\)</span> uniformly at random.</p>
<p>a) Let <span class="math notranslate nohighlight">\(X\)</span> be the first digit and <span class="math notranslate nohighlight">\(Y\)</span> the second digit of the chosen number. Show that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent random variables.</p>
<p>b) Let <span class="math notranslate nohighlight">\(X\)</span> be the first digit of the chosen number and <span class="math notranslate nohighlight">\(Z\)</span> the sum of the two digits. Show that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> are not independent.</p>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.20</strong> (Adapted from [ASV]) Suppose that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> have joint probability density function <span class="math notranslate nohighlight">\(f(x,y) = 2e^{-(x+2y)}\)</span> for <span class="math notranslate nohighlight">\(x &gt; 0, y &gt; 0\)</span> and <span class="math notranslate nohighlight">\(0\)</span> elsewhere. Show that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent random variables and provide their marginal distributions. [<em>Hint:</em> Recall the exponential distribution from Problem 1.11.] <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.21</strong> (Adapted from [ASV]) Suppose that <span class="math notranslate nohighlight">\(X,Y\)</span> have joint probability density function</p>
<div class="math notranslate nohighlight">
\[
f(x,y) = c\exp\left[-\frac{x^2}{2} - \frac{(x - y)^2}{2}\right],
\]</div>
<p>for <span class="math notranslate nohighlight">\(x,y \in \mathbb{R}^2\)</span>, for some constant <span class="math notranslate nohighlight">\(c &gt; 0\)</span>.</p>
<p>a) Find the value of the constant <span class="math notranslate nohighlight">\(c\)</span>. [<em>Hint:</em> The order of integration matters. You can do this without doing complicated integrals. Specifically, recall that for any <span class="math notranslate nohighlight">\(\mu \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\sigma &gt; 0\)</span>, it holds that</p>
<div class="math notranslate nohighlight">
\[
\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(z - \mu)^2}{2\sigma^2}\right) \mathrm{d} z= 1.
\]</div>
<p>]</p>
<p>b) Find the marginal density functions of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. [<em>Hint:</em> You can do this without doing complicated integrals. Complete the square.]</p>
<p>c) Determine whether <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent. Justify your answer. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.22</strong> (Adapted from [ASV]) Let <span class="math notranslate nohighlight">\(p(x, y)\)</span> be the joint probability mass function of <span class="math notranslate nohighlight">\((X, Y)\)</span>. Assume that there are two functions <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> such that <span class="math notranslate nohighlight">\(p(x,y) = a(x)b(y)\)</span> for all possible values <span class="math notranslate nohighlight">\(x\)</span> of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(y\)</span> of <span class="math notranslate nohighlight">\(Y\)</span>. Show that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent. Do <em>not</em> assume that <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are probability mass functions. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.23</strong> (Adapted from [BHK]) a) Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be independent random variables with uniform distribution in <span class="math notranslate nohighlight">\([-1/2 , 1/2]\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[X]\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[X^2]\)</span>, <span class="math notranslate nohighlight">\(\mathrm{Var}[X^2]\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[X - Y]\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[XY]\)</span>, and <span class="math notranslate nohighlight">\(\mathbb{E}[(X - Y)^2]\)</span>.</p>
<p>b) What is the expected squared Euclidean distance between two points generated at random inside the unit d-dimensional cube <span class="math notranslate nohighlight">\(\mathcal{C} = [-1/2,1/2]^d\)</span>? <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.24</strong> (Adapted from [BHK]) a) For an arbitrary <span class="math notranslate nohighlight">\(a &gt; 0\)</span> give a probability distribution for a nonnegative random variable <span class="math notranslate nohighlight">\(X\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[X \geq a] = \frac{\mathbb{E}[X]}{a}.
\]</div>
<p>b) Show that for any <span class="math notranslate nohighlight">\(c &gt; 0\)</span> there exists a distribution for which <em>Chebyshev’s Inequality</em> is tight, that is,
$<span class="math notranslate nohighlight">\(
\mathbb{P}[|X - \mathbb{E}[X]| \geq c] = \frac{\mathrm{Var}[X]}{c^2}.
\)</span>$</p>
<p>[<em>Hint:</em> Choose a distribution symmetric about <span class="math notranslate nohighlight">\(0\)</span>.] <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.25</strong> (Adapted from [BHK]) Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> be i.i.d. random variables with
mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Let</p>
<div class="math notranslate nohighlight">
\[
\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i,
\]</div>
<p>be the sample mean. Suppose one estimates the variance using the sample mean as follows</p>
<div class="math notranslate nohighlight">
\[
S^2_n = \frac{1}{n} \sum_{i=1}^n \left(X_i - \overline{X}_n\right)^2.
\]</div>
<p>Calculate <span class="math notranslate nohighlight">\(\mathbb{E}(S^2_n)\)</span>. [<em>Hint:</em> Replace <span class="math notranslate nohighlight">\(X_i - \overline{X}_n\)</span> with <span class="math notranslate nohighlight">\((X_i - \mu) - (\overline{X}_n - \mu)\)</span>.] <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.26</strong> Let <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> have derivatives at <span class="math notranslate nohighlight">\(x\)</span> and let <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> be constants. Prove from the definition of the derivative that</p>
<div class="math notranslate nohighlight">
\[
[\alpha f(x) + \beta g(x)]' = \alpha f'(x) + \beta g'(x).
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.27</strong> Let <span class="math notranslate nohighlight">\(Z_1 \sim \mathrm{N}(\mu_1, \sigma_1^2)\)</span> and <span class="math notranslate nohighlight">\(Z_2 \sim \mathrm{N}(\mu_2, \sigma_2^2)\)</span> be independent Normal variables with mean <span class="math notranslate nohighlight">\(\mu_1\)</span>, <span class="math notranslate nohighlight">\(\mu_2\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_1^2\)</span> and <span class="math notranslate nohighlight">\(\sigma_2^2\)</span> respectively. Recall that <span class="math notranslate nohighlight">\(Z_1 + Z_2\)</span> is still Normal.</p>
<p>a) What are the mean and variance of <span class="math notranslate nohighlight">\(Z_1 + Z_2\)</span>?</p>
<p>b) Let <span class="math notranslate nohighlight">\(\mathbf{X}_1, \mathbf{X}_2, \mathbf{Y}_1\)</span> be independent spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with mean <span class="math notranslate nohighlight">\(-w \mathbf{e}_1\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{Y}_2\)</span> be an indenpedent spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian with mean <span class="math notranslate nohighlight">\(w \mathbf{e}_1\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>.  Compute <span class="math notranslate nohighlight">\(\mathbb{E}[\|\mathbf{X}_1 - \mathbf{X}_2\|^2]\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}[\|\mathbf{Y}_1 - \mathbf{Y}_2\|^2]\)</span> explicitly. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.28</strong> Suppose we are given <span class="math notranslate nohighlight">\(n\)</span> vectors <span class="math notranslate nohighlight">\(\mathbf{x}_1,\ldots,\mathbf{x}_n\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> and a partition <span class="math notranslate nohighlight">\(C_1, \ldots, C_k \subseteq [n]\)</span>. Let <span class="math notranslate nohighlight">\(n_i = |C_i|\)</span> be the size of cluster <span class="math notranslate nohighlight">\(C_i\)</span> and let</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\mu}_i^* = \frac{1}{n_i} \sum_{j\in C_i} \mathbf{x}_j
\]</div>
<p>be the centroid of <span class="math notranslate nohighlight">\(C_i\)</span>, for <span class="math notranslate nohighlight">\(i=1,\ldots,k\)</span>.</p>
<p>a) Show that</p>
<div class="math notranslate nohighlight">
\[
\sum_{j \in C_i} \|\mathbf{x}_j - \boldsymbol{\mu}_i^*\|^2 
= \left(\sum_{j \in C_i} \|\mathbf{x}_j\|^2\right) - n_i\|\boldsymbol{\mu}_i^*\|^2.
\]</div>
<p>b) Show that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\|\boldsymbol{\mu}_i^*\|^2 = \frac{1}{n_i^2}\left(\sum_{j \in C_i} \|\mathbf{x}_j\|^2 
+ \sum_{\substack{j, \ell \in C_i\\j \neq \ell}} \mathbf{x}_j^T\mathbf{x}_\ell\right).
\end{split}\]</div>
<p>c) Show that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\sum_{\substack{j, \ell \in C_i\\j \neq \ell}} \|\mathbf{x}_j - \mathbf{x}_\ell\|^2
= 2(n_i-1)\sum_{j \in C_i} \|\mathbf{x}_j\|^2 
- 2 \sum_{\substack{j, \ell \in C_i\\j \neq \ell}} \mathbf{x}_j^T\mathbf{x}_\ell.
\end{split}\]</div>
<p>d) Combine a), b), c) to prove that minimizing the <span class="math notranslate nohighlight">\(k\)</span>-means objective <span class="math notranslate nohighlight">\(\mathcal{G}(C_1,\ldots,C_k)\)</span> over all partitions <span class="math notranslate nohighlight">\(C_1, \ldots, C_k\)</span> of <span class="math notranslate nohighlight">\([n]\)</span> is equivalent to minimizing instead</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^k \frac{1}{2 |C_i|} \sum_{j,\ell \in C_i} \|\mathbf{x}_j - \mathbf{x}_\ell\|^2.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.29</strong> Suppose the rows of <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> are given by the transposes of <span class="math notranslate nohighlight">\(\mathbf{r}_1,\ldots,\mathbf{r}_n \in \mathbb{R}^m\)</span> and the columns of <span class="math notranslate nohighlight">\(A\)</span> are given by <span class="math notranslate nohighlight">\(\mathbf{c}_1,\ldots,\mathbf{c}_m \in \mathbb{R}^n\)</span>. Give expressions for the elements of <span class="math notranslate nohighlight">\(A^T A\)</span> and <span class="math notranslate nohighlight">\(A A^T\)</span> in terms of these vectors. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.30</strong> Give a proof of the <em>Positive Semidefiniteness of the Covariance</em> using the matrix form of <span class="math notranslate nohighlight">\(\bSigma_\bX\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.31</strong> Use the <em>Cauchy-Schwarz Inequality</em> to prove that the correlation coefficient lies in <span class="math notranslate nohighlight">\([-1,1]\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.32</strong> Let <span class="math notranslate nohighlight">\(f(x,y) = g(x) + h(y)\)</span> where <span class="math notranslate nohighlight">\(x, y \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(g,h\)</span> are real-valued continuously differentiable functions. Compute the gradient of <span class="math notranslate nohighlight">\(f\)</span> in terms of the derivatives of <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.33</strong> Let <span class="math notranslate nohighlight">\(A = [a]\)</span> be a <span class="math notranslate nohighlight">\(1\times 1\)</span> positive definite matrix. Show that <span class="math notranslate nohighlight">\(a &gt; 0\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.34</strong> Show that the diagonal elements of a positive definite matrix are necessarily positive. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.35</strong> Let <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span> and let <span class="math notranslate nohighlight">\(c \in \mathbb{R}\)</span>. Show that</p>
<p>a) <span class="math notranslate nohighlight">\((A + B)^T = A^T + B^T\)</span></p>
<p>b) <span class="math notranslate nohighlight">\((c A)^T = c A^T\)</span></p>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
</section>
&#13;

<h2><span class="section-number">1.5.1. </span>Warm-up worksheets<a class="headerlink" href="#warm-up-worksheets" title="Link to this heading">#</a></h2>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>Section 1.2</strong></p>
<p><strong>E1.2.1</strong> Calculate the Euclidean norm of the vector <span class="math notranslate nohighlight">\(\mathbf{x} = (6, 8)\)</span>.</p>
<p><strong>E1.2.2</strong> Compute the inner product <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{v} \rangle\)</span> for vectors <span class="math notranslate nohighlight">\(\mathbf{u} = (1, 2, 3)\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v} = (4, 5, 6)\)</span>.</p>
<p><strong>E1.2.3</strong> Find the transpose of the matrix <span class="math notranslate nohighlight">\(A = \begin{pmatrix}1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6\end{pmatrix}\)</span>.</p>
<p><strong>E1.2.4</strong> Compute the Frobenius norm of the matrix <span class="math notranslate nohighlight">\(A = \begin{pmatrix}1 &amp; 2 \\ 3 &amp; 4\end{pmatrix}\)</span>.</p>
<p><strong>E1.2.5</strong> Verify if the matrix <span class="math notranslate nohighlight">\(A = \begin{pmatrix}2 &amp; 0 \\ 0 &amp; 3\end{pmatrix}\)</span> is symmetric.</p>
<p><strong>E1.2.6</strong> Let <span class="math notranslate nohighlight">\(A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\)</span> and <span class="math notranslate nohighlight">\(B = \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{pmatrix}\)</span>. Compute <span class="math notranslate nohighlight">\(AB\)</span> and <span class="math notranslate nohighlight">\(BA\)</span>.</p>
<p><strong>E1.2.7</strong> Let <span class="math notranslate nohighlight">\(f(x, y) = x^2 + xy + y^2\)</span>. Compute the partial derivatives <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y}\)</span>.</p>
<p><strong>E1.2.8</strong> Given the function <span class="math notranslate nohighlight">\(g(x, y) = x^2 y + xy^3\)</span>, compute its partial derivatives <span class="math notranslate nohighlight">\(\frac{\partial g}{\partial x}\)</span> and <span class="math notranslate nohighlight">\(\frac{\partial g}{\partial y}\)</span>.</p>
<p><strong>E1.2.9</strong> Let <span class="math notranslate nohighlight">\(f(x) = x^3 - 3x^2 + 2x\)</span>. Use <em>Taylor’s Theorem</em> to find a linear approximation of <span class="math notranslate nohighlight">\(f\)</span> around <span class="math notranslate nohighlight">\(a = 1\)</span> with a second-order error term.</p>
<p><strong>E1.2.10</strong> Compute the expectation <span class="math notranslate nohighlight">\(\E[X]\)</span> of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(\P(X = 1) = 0.2\)</span>, <span class="math notranslate nohighlight">\(\P(X = 2) = 0.5\)</span>, and <span class="math notranslate nohighlight">\(\P(X = 3) = 0.3\)</span>.</p>
<p><strong>E1.2.11:</strong> Calculate the variance <span class="math notranslate nohighlight">\(\mathrm{Var}[X]\)</span> of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(\P(X = 1) = 0.4\)</span>, <span class="math notranslate nohighlight">\(\P(X = 2) = 0.6\)</span>, and <span class="math notranslate nohighlight">\(\E[X] = 1.6\)</span>.</p>
<p><strong>E1.2.12</strong> Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be random variables with <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 2\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[Y] = 3\)</span>, <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = 4\)</span>, and <span class="math notranslate nohighlight">\(\mathrm{Var}[Y] = 9\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[2X + Y - 1]\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[2X + Y - 1]\)</span>, assuming <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent.</p>
<p><strong>E1.2.13</strong> Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable with <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 3\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = 4\)</span>. Use <em>Chebyshev’s Inequality</em> to bound <span class="math notranslate nohighlight">\(\P[|X - 3| \geq 4]\)</span>.</p>
<p><strong>E1.2.14</strong> A random variable <span class="math notranslate nohighlight">\(X\)</span> has mean <span class="math notranslate nohighlight">\(\mu = 5\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2 = 4\)</span>. Use <em>Chebyshev’s Inequality</em> to find an upper bound on the probability that <span class="math notranslate nohighlight">\(X\)</span> is more than 3 units away from its mean.</p>
<p><strong>E1.2.15</strong> Given the random vector <span class="math notranslate nohighlight">\(\bX = (X_1, X_2)\)</span> where <span class="math notranslate nohighlight">\(X_1\)</span> and <span class="math notranslate nohighlight">\(X_2\)</span> are independent standard normal random variables, what is the covariance matrix of <span class="math notranslate nohighlight">\(\bX\)</span>?</p>
<p><strong>E1.2.16</strong> Let <span class="math notranslate nohighlight">\(\bX = \begin{pmatrix} X_1 \\ X_2 \end{pmatrix}\)</span> be a random vector with <span class="math notranslate nohighlight">\(\mathbb{E}[X_1] = 1\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[X_2] = 2\)</span>, <span class="math notranslate nohighlight">\(\text{Var}[X_1] = 3\)</span>, <span class="math notranslate nohighlight">\(\text{Var}[X_2] = 4\)</span>, and <span class="math notranslate nohighlight">\(\text{Cov}[X_1, X_2] = 1\)</span>. Write the mean vector <span class="math notranslate nohighlight">\(\bmu_\bX\)</span> and the covariance matrix <span class="math notranslate nohighlight">\(\bSigma_\bX\)</span> of <span class="math notranslate nohighlight">\(\bX\)</span>.</p>
<p><strong>E1.2.17</strong> Let <span class="math notranslate nohighlight">\(\bX = \begin{pmatrix} X_1 \\ X_2 \end{pmatrix}\)</span> be a random vector with mean <span class="math notranslate nohighlight">\(\bmu_\bX = \begin{pmatrix} 1 \\ 2 \end{pmatrix}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\bSigma_\bX = \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 4 \end{pmatrix}\)</span>. Let <span class="math notranslate nohighlight">\(A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[A\bX]\)</span> and <span class="math notranslate nohighlight">\(\text{Cov}[A\bX]\)</span>.</p>
<p><strong>E1.2.18</strong> Let <span class="math notranslate nohighlight">\(X_1, X_2, X_3\)</span> be independent random variables with <span class="math notranslate nohighlight">\(\mathbb{E}[X_1] = 1\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[X_2] = 2\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[X_3] = 3\)</span>, <span class="math notranslate nohighlight">\(\text{Var}[X_1] = 4\)</span>, <span class="math notranslate nohighlight">\(\text{Var}[X_2] = 5\)</span>, and <span class="math notranslate nohighlight">\(\text{Var}[X_3] = 6\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[X_1 + X_2 + X_3]\)</span> and <span class="math notranslate nohighlight">\(\text{Var}[X_1 + X_2 + X_3]\)</span>.</p>
<p><strong>E1.2.19</strong> Let <span class="math notranslate nohighlight">\(A = \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix}\)</span>. Show that <span class="math notranslate nohighlight">\(A\)</span> is positive definite.</p>
<p><strong>Section 1.3</strong></p>
<p><strong>E1.3.1</strong> Given the data points <span class="math notranslate nohighlight">\(\mathbf{x}_1 = (1, 2)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_2 = (-1, 1)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_3 = (0, -1)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{x}_4 = (2, 0)\)</span>, and the partition <span class="math notranslate nohighlight">\(C_1 = \{1, 4\}\)</span>, <span class="math notranslate nohighlight">\(C_2 = \{2, 3\}\)</span>, compute the optimal cluster representatives <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1^*\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_2^*\)</span>.</p>
<p><strong>E1.3.2</strong> For the data points and partition from E1.3.1, compute the <span class="math notranslate nohighlight">\(k\)</span>-means objective value <span class="math notranslate nohighlight">\(\mathcal{G}(C_1, C_2)\)</span>.</p>
<p><strong>E1.3.3</strong> Given the cluster representatives <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1 = (0, 1)\)</span>, <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_2 = (1, -1)\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_3 = (-2, 0)\)</span>, and the data points <span class="math notranslate nohighlight">\(\mathbf{x}_1 = (1, 1)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_2 = (-1, -1)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_3 = (2, -2)\)</span>, <span class="math notranslate nohighlight">\(\mathbf{x}_4 = (-2, 1)\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{x}_5 = (0, 0)\)</span>, find the optimal partition <span class="math notranslate nohighlight">\(C_1, C_2, C_3\)</span>.</p>
<p><strong>E1.3.4</strong> For the data points and cluster representatives from E1.3.3, compute the <span class="math notranslate nohighlight">\(k\)</span>-means objective value <span class="math notranslate nohighlight">\(G(C_1, C_2, C_3; \boldsymbol{\mu}_1, \boldsymbol{\mu}_2, \boldsymbol{\mu}_3)\)</span>.</p>
<p><strong>E1.3.5</strong> Show that for any two cluster representatives <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_1, \boldsymbol{\mu}_2 \in \mathbb{R}^d\)</span> and any data point <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span>, the inequality <span class="math notranslate nohighlight">\(\|\mathbf{x} - \boldsymbol{\mu}_1\|^2 \leq \|\mathbf{x} - \boldsymbol{\mu}_2\|^2\)</span> is equivalent to <span class="math notranslate nohighlight">\(2(\boldsymbol{\mu}_2 - \boldsymbol{\mu}_1)^T \mathbf{x} \leq \|\boldsymbol{\mu}_2\|^2 - \|\boldsymbol{\mu}_1\|^2\)</span>.</p>
<p><strong>E1.3.6</strong> Given the cluster assignment matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
Z = \begin{bmatrix}
   1 &amp; 0 &amp; 0\\
   0 &amp; 1 &amp; 0\\
   0 &amp; 0 &amp; 1\\
   1 &amp; 0 &amp; 0\\
   0 &amp; 1 &amp; 0
   \end{bmatrix}
\end{split}\]</div>
<p>and the cluster representative matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
U = \begin{bmatrix}
   1 &amp; 2\\
   -1 &amp; 0\\
   0 &amp; -2
\end{bmatrix},
\end{split}\]</div>
<p>compute the matrix product <span class="math notranslate nohighlight">\(ZU\)</span>.</p>
<p><strong>E1.3.7</strong> For any two matrices <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span>, prove that <span class="math notranslate nohighlight">\(\|A + B\|_F^2 = \|A\|_F^2 + \|B\|_F^2 + 2 \sum_{i=1}^n \sum_{j=1}^m A_{ij} B_{ij}\)</span>.</p>
<p><strong>E1.3.8</strong> Show that for any matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> and any scalar <span class="math notranslate nohighlight">\(c \in \mathbb{R}\)</span>, it holds that <span class="math notranslate nohighlight">\(\|cA\|_F = |c| \|A\|_F\)</span>.</p>
<p><strong>E1.3.9</strong> Complete the square for the quadratic function <span class="math notranslate nohighlight">\(q(x) = 3x^2 - 6x + 5\)</span>. What is the minimum value of <span class="math notranslate nohighlight">\(q\)</span> and where is it attained?</p>
<p><strong>E1.3.10</strong> Let <span class="math notranslate nohighlight">\(f_1(x) = x^2 + 1\)</span> and <span class="math notranslate nohighlight">\(f_2(y) = 2y^2 - 3\)</span>. Define <span class="math notranslate nohighlight">\(h(x, y) = f_1(x) + f_2(y)\)</span>. Find the global minimum of <span class="math notranslate nohighlight">\(h(x, y)\)</span>.</p>
<p><strong>E1.3.11</strong> Compute the Frobenius norm of the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A = \begin{bmatrix} 2 &amp; -1 \\ 0 &amp; 3 \end{bmatrix}.
\end{split}\]</div>
<p><strong>Section 1.4</strong></p>
<p><strong>E1.4.1</strong> Let <span class="math notranslate nohighlight">\(X_1, \dots, X_d\)</span> be independent random variables uniformly distributed on <span class="math notranslate nohighlight">\([-1/2, 1/2]\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[X_i]\)</span> and <span class="math notranslate nohighlight">\(\text{Var}[X_i]\)</span>.</p>
<p><strong>E1.4.2</strong> Let <span class="math notranslate nohighlight">\(\bX = (X_1, \dots, X_d)\)</span> be a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector where each <span class="math notranslate nohighlight">\(X_i\)</span> is an independent random variable uniformly distributed on <span class="math notranslate nohighlight">\([-1/2, 1/2]\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[\|\bX\|^2]\)</span>.</p>
<p><strong>E1.4.3</strong> Let <span class="math notranslate nohighlight">\(\bX = (X_1, \dots, X_d)\)</span> be a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector where each <span class="math notranslate nohighlight">\(X_i\)</span> is an independent standard normal random variable. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[\|\bX\|^2]\)</span>.</p>
<p><strong>E1.4.4</strong> Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots, X_d\)</span> be independent random variables with <span class="math notranslate nohighlight">\(E[X_i] = \mu\)</span> and <span class="math notranslate nohighlight">\(Var[X_i] = \sigma^2\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>. Find <span class="math notranslate nohighlight">\(\E[\sum_{i=1}^d X_i]\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[\sum_{i=1}^d X_i]\)</span>.</p>
<p><strong>E1.4.5</strong> Given a random variable <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(\E[X] = 1\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = 4\)</span>, bound <span class="math notranslate nohighlight">\(\P[|X - 1| \geq 3]\)</span> using <em>Chebyshev’s Inequality</em>.</p>
<p><strong>E1.4.6</strong> Let <span class="math notranslate nohighlight">\(\bX = (X_1, \ldots, X_d)\)</span> be a random vector with independent components, each following <span class="math notranslate nohighlight">\(U[-\frac{1}{2}, \frac{1}{2}]\)</span>. Compute <span class="math notranslate nohighlight">\(\P[\|\bX\| \leq \frac{1}{2}]\)</span> explicitly for <span class="math notranslate nohighlight">\(d = 1, 2, 3\)</span>. [Hint: Use a geometric arugment.]</p>
<p><strong>E1.4.7</strong> Given a random variable <span class="math notranslate nohighlight">\(X\)</span> with <span class="math notranslate nohighlight">\(\E[X] = 0\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}[X] = 1\)</span>, bound <span class="math notranslate nohighlight">\(\P[|X| \geq 2\sqrt{d}]\)</span> using <em>Chebyshev’s Inequality</em> for <span class="math notranslate nohighlight">\(d = 1, 10, 100\)</span>.</p>
<p><strong>E1.4.8</strong> Let <span class="math notranslate nohighlight">\(\bX = (X_1, \ldots, X_d)\)</span> be a random vector with independent components, each following <span class="math notranslate nohighlight">\(U[-\frac{1}{2}, \frac{1}{2}]\)</span>. Find an upper bound for <span class="math notranslate nohighlight">\(\P[\|\bX\| \geq \frac{1}{2}]\)</span> using <em>Chebyshev’s Inequality</em> for <span class="math notranslate nohighlight">\(d = 1, 10, 100\)</span>.</p>
&#13;

<h2><span class="section-number">1.5.2. </span>Problems<a class="headerlink" href="#problems" title="Link to this heading">#</a></h2>
<p><strong>1.1</strong> (Adapted from [VLMS]) Recall that for two vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> of the same dimension, their inner product is <span class="math notranslate nohighlight">\(\mathbf{x}^T \mathbf{y}\)</span>. A vector is said to be nonnegative if <em>all</em> of its entries are <span class="math notranslate nohighlight">\(\geq 0\)</span>.</p>
<p>a) Show that the inner product of two nonnegative vectors is necessarily <span class="math notranslate nohighlight">\(\geq 0\)</span>.</p>
<p>b) Suppose the inner product of two nonnegative vectors is <em>zero</em>. What can you say about their sparsity patterns, i.e., which of their entries are zero or nonzero.<span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.2</strong> (Adapted from [VLMS]) A Boolean <span class="math notranslate nohighlight">\(n\)</span>-vector is one for which all entries are either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. Such vectors are used to encode whether each of <span class="math notranslate nohighlight">\(n\)</span> conditions holds, with <span class="math notranslate nohighlight">\(a_i = 1\)</span> meaning that condition <span class="math notranslate nohighlight">\(i\)</span> holds.</p>
<p>a) Another common encoding of the same information uses the two values <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span> for the entries. For example the Boolean vector <span class="math notranslate nohighlight">\((0,1,1,0)\)</span> would be written using this alternative encoding as <span class="math notranslate nohighlight">\((-1, 1, 1, -1)\)</span>. Suppose that <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a Boolean vector, and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is a vector encoding the same information using the values <span class="math notranslate nohighlight">\(-1\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. Express <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> in terms of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> using vector notation. Also, express <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> in terms of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> using vector notation.</p>
<p>b) Suppose that <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> are Boolean <span class="math notranslate nohighlight">\(n\)</span>-vectors. Give a simple word description of their squared Euclidean distance <span class="math notranslate nohighlight">\(\|\mathbf{x} - \mathbf{y}\|_2^2\)</span> and justify it mathematically. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.3</strong> (Adapted from [VLMS])  If <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> is a vector, then <span class="math notranslate nohighlight">\(\mathbf{a}_{r:s}\)</span> is the vector of size <span class="math notranslate nohighlight">\(s - r + 1\)</span>, with entries <span class="math notranslate nohighlight">\(a_r,\ldots,a_s\)</span>, i.e., <span class="math notranslate nohighlight">\(\mathbf{a}_{r:s} = (a_r,\ldots,a_s)\)</span>. The vector <span class="math notranslate nohighlight">\(\mathbf{a}_{r:s}\)</span> is called a slice. As a more concrete example, if <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is the <span class="math notranslate nohighlight">\(4\)</span>-vector <span class="math notranslate nohighlight">\((1, -1, 2, 0)\)</span>, the slice <span class="math notranslate nohighlight">\(\mathbf{z}_{2:3} = (-1, 2)\)</span>. Suppose the <span class="math notranslate nohighlight">\(T\)</span>-vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> represents a time series or signal. The quantity</p>
<div class="math notranslate nohighlight">
\[
\mathcal{D}(\mathbf{x})=(x_1 - x_2)^2 + (x_2 - x_3)^2 + \cdots+(x_{T-1} - x_T)^2,
\]</div>
<p>the sum of the differences of adjacent values of the signal, is called the Dirichlet energy of the signal. The Dirichlet energy is a measure of the roughness or wiggliness of the time series.</p>
<p>a) Express <span class="math notranslate nohighlight">\(\mathcal{D}(\mathbf{x})\)</span> in vector notation using slicing. [<em>Hint:</em> Note the similarity between <span class="math notranslate nohighlight">\(\mathcal{D}(\mathbf{x})\)</span> and the squared Euclidean distance between two vectors.]</p>
<p>b) How small can <span class="math notranslate nohighlight">\(\mathcal{D}(\mathbf{x})\)</span> be? What signals <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> have this minimum value of the Dirichlet energy?</p>
<p>c) Find a signal <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> with entries no more than one in absolute value that has the largest possible value of <span class="math notranslate nohighlight">\(\mathcal{D}(\mathbf{x})\)</span>. Give the value of the Dirichlet energy achieved. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.4</strong> (Adapted from [VLMS]) A vector of length <span class="math notranslate nohighlight">\(n\)</span> can represent the number of times each word in a dictionary of <span class="math notranslate nohighlight">\(n\)</span> words appears in a document. For example, <span class="math notranslate nohighlight">\((25,2,0)\)</span> means that the first dictionary word appears <span class="math notranslate nohighlight">\(25\)</span> times, the second one twice, and the third one not at all. Suppose the <span class="math notranslate nohighlight">\(n\)</span>-vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is the word count vector associated with a document and a dictionary of <span class="math notranslate nohighlight">\(n\)</span> words. For simplicity we will assume that all words in the document appear in the dictionary.</p>
<p>a) What is <span class="math notranslate nohighlight">\(\mathbf{1}^T \mathbf{w}\)</span>? Here <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is an all-one vector of the appropriate size.</p>
<p>b) What does <span class="math notranslate nohighlight">\(w_{282} = 0\)</span> mean?</p>
<p>c) Let <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> be the <span class="math notranslate nohighlight">\(n\)</span>-vector that gives the histogram of the word counts, i.e., <span class="math notranslate nohighlight">\(h_i\)</span> is the fraction of the words in the document that are word <span class="math notranslate nohighlight">\(i\)</span>. Use vector notation to express <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> in terms of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>. (You can assume that the document contains at least one word.) <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.5</strong> (Adapted from [VLMS]) Suppose <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> are vectors of the same size. The triangle inequality states that <span class="math notranslate nohighlight">\(\|\mathbf{a} + \mathbf{b}\|_2 \leq \|\mathbf{a}\|_2 + \|\mathbf{b}\|_2\)</span>. Show that we also have <span class="math notranslate nohighlight">\(\|\mathbf{a} + \mathbf{b}\|_2 \geq \|\mathbf{a}\|_2 - \|\mathbf{b}\|_2\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.6</strong> (Adapted from [VLMS]) Verify that the following identities hold for any two vectors <span class="math notranslate nohighlight">\(\mathbf{a}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> of the same size.</p>
<p>a) <span class="math notranslate nohighlight">\((\mathbf{a}+\mathbf{b})^T(\mathbf{a} - \mathbf{b})=\|\mathbf{a}\|_2^2 - \|\mathbf{b}\|_2^2\)</span>.</p>
<p>b) <span class="math notranslate nohighlight">\(\|\mathbf{a} + \mathbf{b}\|_2^2 + \|\mathbf{a} - \mathbf{b}\|_2^2 = 2(\|\mathbf{a}\|_2^2 + \|\mathbf{b}\|_2^2)\)</span>. This is called the <em>Parallelogram Law</em>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.7</strong> (Adapted from [VLMS]) The root-mean-square (RMS) value of an <span class="math notranslate nohighlight">\(n\)</span>-vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathrm{rms}(\mathbf{x}) 
= \sqrt{\frac{x_1^2 + \cdots + x_n^2}{n}}
= \frac{\|\mathbf{x}\|_2}{\sqrt{n}}.
\]</div>
<p>a) Show that at least one entry of a vector has absolute value at least as large as the RMS value of the vector.</p>
<p>b) For an <span class="math notranslate nohighlight">\(n\)</span>-vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, let <span class="math notranslate nohighlight">\(\mathrm{avg}(\mathbf{x}) = \mathbf{1}^T \mathbf{x}/n\)</span> and</p>
<div class="math notranslate nohighlight">
\[
\mathrm{std}(\mathbf{x}) = \frac{\|\mathbf{x} - \mathrm{avg}(\mathbf{x}) \mathbf{1}\|_2}{\sqrt{n}}.
\]</div>
<p>Establish the identity</p>
<div class="math notranslate nohighlight">
\[
\mathrm{rms}(\mathbf{x})^2 = \mathrm{avg}(\mathbf{x})^2 + \mathrm{std}(\mathbf{x})^2.
\]</div>
<p>c) Use (b) to show that <span class="math notranslate nohighlight">\(|\mathrm{avg}(\mathbf{x})| \leq \mathrm{rms}(\mathbf{x})\)</span>.</p>
<p>d) Use (b) to show that <span class="math notranslate nohighlight">\(\mathrm{std}(\mathbf{x}) \leq \mathrm{rms}(\mathbf{x})\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.8</strong> (Adapted from [VLMS]) Suppose that the vectors <span class="math notranslate nohighlight">\(\mathbf{x}_1,\ldots,\mathbf{x}_N\)</span> are clustered using <span class="math notranslate nohighlight">\(k\)</span>-means, with group representatives <span class="math notranslate nohighlight">\(\mathbf{z}_1,\ldots,\mathbf{z}_k\)</span>.</p>
<p>a) Suppose the original vectors <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> are nonnegative, i.e., their entries are nonnegative. Explain why the representatives <span class="math notranslate nohighlight">\(\mathbf{z}_j\)</span> are also nonnegative.</p>
<p>b) Suppose the original vectors <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> represent proportions, i.e., their entries are nonnegative and sum to one. Explain why the representatives <span class="math notranslate nohighlight">\(\mathbf{z}_j\)</span> also represent proportions, i.e., their entries are nonnegative and sum to one.</p>
<p>c) Suppose the original vectors <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> are Boolean, i.e., their entries are either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>. Give an interpretation of <span class="math notranslate nohighlight">\((\mathbf{z}_j)_i\)</span>, the <span class="math notranslate nohighlight">\(i\)</span>-th entry of the <span class="math notranslate nohighlight">\(j\)</span>-th group representative. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.9</strong> (Adapted from [VLMS]) Clustering a collection of vectors into <span class="math notranslate nohighlight">\(k = 2\)</span> groups is called <span class="math notranslate nohighlight">\(2\)</span>-way partitioning, since we are partitioning the vectors into <span class="math notranslate nohighlight">\(2\)</span> groups, with index sets <span class="math notranslate nohighlight">\(G_1\)</span> and <span class="math notranslate nohighlight">\(G_2\)</span>. Suppose we run <span class="math notranslate nohighlight">\(k\)</span>-means with <span class="math notranslate nohighlight">\(k = 2\)</span> on the <span class="math notranslate nohighlight">\(n\)</span>-vectors <span class="math notranslate nohighlight">\(\mathbf{x}_1,\ldots,\mathbf{x}_N\)</span> . Show that there is a nonzero vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and a scalar <span class="math notranslate nohighlight">\(v\)</span> that satisfy</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^T \mathbf{x}_i + v \geq 0, \forall i \in G_1, \quad \mathbf{w}^T \mathbf{x}_i + v \leq 0, \forall i \in G_2.
\]</div>
<p>In other words, the affine function <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \mathbf{w}^T \mathbf{x} + v\)</span> is greater than or equal to zero on the first group, and less than or equal to zero on the second group. This is called linear separation of the two groups. [<em>Hint:</em> Consider the function <span class="math notranslate nohighlight">\(\|\mathbf{x} - \mathbf{z}_1\|_2^2 - \|\mathbf{x} - \mathbf{z}_2\|_2^2\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{z}_1\)</span> and <span class="math notranslate nohighlight">\(\mathbf{z}_2\)</span> are the group representatives.] <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.10</strong> (Adapted from [ASV]) Let <span class="math notranslate nohighlight">\(0 &lt; p \leq 1\)</span>. A random variable <span class="math notranslate nohighlight">\(X\)</span> has the geometric distribution with success parameter <span class="math notranslate nohighlight">\(p\)</span> if the possible values of <span class="math notranslate nohighlight">\(X\)</span> are <span class="math notranslate nohighlight">\(\{1, 2, 3, \ldots\}\)</span> and <span class="math notranslate nohighlight">\(X\)</span> satisfies <span class="math notranslate nohighlight">\(\mathbb{P}(X = k) = (1 - p)^{k - 1}p\)</span> for positive integers <span class="math notranslate nohighlight">\(k\)</span>. Its mean is <span class="math notranslate nohighlight">\(1/p\)</span> and its variance is <span class="math notranslate nohighlight">\((1 - p)/p^2\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a geometric random variable with parameter <span class="math notranslate nohighlight">\(p = 1/6\)</span>.</p>
<p>a) Use <em>Markov’s Inequality</em> to find an upper bound for <span class="math notranslate nohighlight">\(\mathbb{P}(X \geq 16)\)</span>.</p>
<p>b) Use <em>Chebyshev’s Inequality</em> to find an upper bound for <span class="math notranslate nohighlight">\(\mathbb{P}(X \geq 16)\)</span>.</p>
<p>c) Use the sum of a geometric series to explicitly compute the probability <span class="math notranslate nohighlight">\(\mathbb{P}(X \geq 16)\)</span> and compare with the upper bounds you derived. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.11</strong> (Adapted from [ASV]) Let <span class="math notranslate nohighlight">\(0 &lt; \lambda &lt; +\infty\)</span>. A random variable <span class="math notranslate nohighlight">\(X\)</span> has the exponential distribution with parameter <span class="math notranslate nohighlight">\(\lambda\)</span> if <span class="math notranslate nohighlight">\(X\)</span> has density function
<span class="math notranslate nohighlight">\(f(x) = \lambda e^{-\lambda x}\)</span>, for <span class="math notranslate nohighlight">\(x \geq 0\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise. Its mean is <span class="math notranslate nohighlight">\(1/\lambda\)</span> and its variance is <span class="math notranslate nohighlight">\(1/\lambda^2\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be an exponential random variable with parameter <span class="math notranslate nohighlight">\(\lambda = 1/2\)</span>.</p>
<p>a) Use <em>Markov’s Inequality</em> to find an upper bound for <span class="math notranslate nohighlight">\(\mathbb{P}(X &gt; 6)\)</span>.</p>
<p>b) Use <em>Chebyshev’s Inequality</em> to find an upper bound for <span class="math notranslate nohighlight">\(\mathbb{P}(X &gt; 6)\)</span>.</p>
<p>c) Use an integral to explicitly compute the probability <span class="math notranslate nohighlight">\(\mathbb{P}(X &gt; 6)\)</span> and compare with the upper bounds you derived. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.12</strong> (Adapted from [ASV]) Suppose that <span class="math notranslate nohighlight">\(X\)</span> is a nonnegative random variable with <span class="math notranslate nohighlight">\(\mathbb{E}[X] = 10\)</span>.</p>
<p>a) Use <em>Markov’s Inequality</em> to give an upper bound on the probability that <span class="math notranslate nohighlight">\(X\)</span> is larger than <span class="math notranslate nohighlight">\(15\)</span>.</p>
<p>b) Suppose that we also know that <span class="math notranslate nohighlight">\(\mathrm{Var}(X) = 3\)</span>. Use <em>Chebyshev’s Inequality</em> to give a better upper bound on
<span class="math notranslate nohighlight">\(\mathbb{P}(X &gt; 15)\)</span> than in part (a).</p>
<p>c) Suppose that <span class="math notranslate nohighlight">\(Y_1,Y_2,\ldots,Y_{300}\)</span> are i.i.d. random variables with the same distribution as <span class="math notranslate nohighlight">\(X\)</span>, so that, in particular <span class="math notranslate nohighlight">\(\mathbb{E}(Y_i) = 10\)</span> and <span class="math notranslate nohighlight">\(\mathrm{Var}(Y_i) = 3\)</span>. Use <em>Chebyshev’s Inequality</em> to give an upper bound on the
probability that <span class="math notranslate nohighlight">\(\sum_{i=1}^{300} Y_i\)</span> is larger than <span class="math notranslate nohighlight">\(3060\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.13</strong> (Adapted from [ASV]) Suppose that we have i.i.d. random variables <span class="math notranslate nohighlight">\(X_1, X_2, X_3,\ldots\)</span> with finite mean <span class="math notranslate nohighlight">\(\mathbb{E}[X_1] = \mu\)</span> and variance <span class="math notranslate nohighlight">\(\mathrm{Var}(X_1) = \sigma^2\)</span>. Let <span class="math notranslate nohighlight">\(S_n = X_1 +\cdots+X_n\)</span>. Prove that for any fixed <span class="math notranslate nohighlight">\(\varepsilon &gt; 0\)</span> and <span class="math notranslate nohighlight">\(1/2 &lt; \alpha &lt; 1\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\lim_{n \to +\infty} \mathbb{P}\left[\left|\frac{S_n - n\mu}{n^\alpha}\right| &lt; \varepsilon\right] = 1.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.14</strong> (Adapted from [ASV]) By mimicking the proof of <em>Weak Law of Large Numbers</em>, prove the following variant. Suppose that we have random variables <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> each with finite mean <span class="math notranslate nohighlight">\(\mathbb{E}[X_i] = \mu\)</span> and variance <span class="math notranslate nohighlight">\(\mathrm{Var}(X_i) = \sigma^2\)</span>. Suppose further that <span class="math notranslate nohighlight">\(\mathrm{Cov}(X_i,X_j) = 0\)</span> whenever <span class="math notranslate nohighlight">\(|i - j| \geq 2\)</span> and that there is a constant <span class="math notranslate nohighlight">\(c &gt; 0\)</span> so that <span class="math notranslate nohighlight">\(|\mathrm{Cov}(X_i, X_{i+1})| &lt; c\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>. Let <span class="math notranslate nohighlight">\(S_n =X_1 +\cdots+X_n\)</span>. Then for any fixed <span class="math notranslate nohighlight">\(\varepsilon&gt;0\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\lim_{n\to +\infty} \mathbb{P}\left[\left|\frac{S_n}{n} - \mu\right|\right]&lt;\varepsilon=1.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.15</strong> (Adapted from [ASV]) By mimicking the proof of <em>Chebyshev’s Inequality</em>, prove the following variant. Let <span class="math notranslate nohighlight">\(X\)</span> be a random variable with a finite mean <span class="math notranslate nohighlight">\(\mu\)</span> and for which <span class="math notranslate nohighlight">\(\mathbb{E}[\exp(s(X - \mu)] &lt; +\infty\)</span> for some <span class="math notranslate nohighlight">\(s &gt; 0\)</span>. Then we have for <span class="math notranslate nohighlight">\(c &gt; 0\)</span></p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(X \geq \mu + c) \leq \frac{\mathbb{E}[\exp(s(X - \mu)]}{e^{s c}}.
\]</div>
<p>Justify your answer carefully. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.16</strong> (Adapted from [ASV]) Recall that the cumulative distribution function (CDF) of a real-valued random variable <span class="math notranslate nohighlight">\(Z\)</span> is the function <span class="math notranslate nohighlight">\(F_Z(z) = \mathbb{P}[Z \leq z]\)</span> for all <span class="math notranslate nohighlight">\(z \in \mathbb{R}\)</span>. Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> be independent random variables with the same cumulative distribution function <span class="math notranslate nohighlight">\(F\)</span>. Denote the minimum and the maximum by</p>
<div class="math notranslate nohighlight">
\[
Z = \min(X_1,X_2,\ldots,X_n) \quad\text{and}\quad W = \max(X_1,X_2,\ldots,X_n). 
\]</div>
<p>Find the cumulative distribution functions <span class="math notranslate nohighlight">\(F_Z\)</span> and <span class="math notranslate nohighlight">\(F_W\)</span> of <span class="math notranslate nohighlight">\(Z\)</span> and <span class="math notranslate nohighlight">\(W\)</span> respectively. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.17</strong> (Adapted from [ASV]) Suppose that <span class="math notranslate nohighlight">\(X_1, X_2, \ldots\)</span> are i.i.d. random variables with Exponential distribution with parameter <span class="math notranslate nohighlight">\(\lambda = 1\)</span> (see Exercise 1.11 above), and let <span class="math notranslate nohighlight">\(M_n = \max(X_1,\ldots,X_n)\)</span>. Show that for any <span class="math notranslate nohighlight">\(x \in \mathbb{R}\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\lim_{n \to +\infty} \mathbb{P}(M_n - \ln n \leq x) = \exp(-e^{-x}).
\]</div>
<p>The right hand side is the cumulative distribution function of the Gumbel distribution, an example of an extreme value distribution. [<em>Hint:</em> Use Exercise 1.16 to compute <span class="math notranslate nohighlight">\(\mathbb{P}(M_n \leq \ln n + x)\)</span> explicitly, and then evaluate the limit as <span class="math notranslate nohighlight">\(n \to +\infty\)</span>.] <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.18</strong> (Adapted from [ASV]) Let <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> be two disjoint events. Under what condition are they independent? <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.19</strong> (Adapted from [ASV]) We choose a number from the set <span class="math notranslate nohighlight">\(\{10, 11, 12, \ldots , 99\}\)</span> uniformly at random.</p>
<p>a) Let <span class="math notranslate nohighlight">\(X\)</span> be the first digit and <span class="math notranslate nohighlight">\(Y\)</span> the second digit of the chosen number. Show that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent random variables.</p>
<p>b) Let <span class="math notranslate nohighlight">\(X\)</span> be the first digit of the chosen number and <span class="math notranslate nohighlight">\(Z\)</span> the sum of the two digits. Show that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> are not independent.</p>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.20</strong> (Adapted from [ASV]) Suppose that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> have joint probability density function <span class="math notranslate nohighlight">\(f(x,y) = 2e^{-(x+2y)}\)</span> for <span class="math notranslate nohighlight">\(x &gt; 0, y &gt; 0\)</span> and <span class="math notranslate nohighlight">\(0\)</span> elsewhere. Show that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent random variables and provide their marginal distributions. [<em>Hint:</em> Recall the exponential distribution from Problem 1.11.] <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.21</strong> (Adapted from [ASV]) Suppose that <span class="math notranslate nohighlight">\(X,Y\)</span> have joint probability density function</p>
<div class="math notranslate nohighlight">
\[
f(x,y) = c\exp\left[-\frac{x^2}{2} - \frac{(x - y)^2}{2}\right],
\]</div>
<p>for <span class="math notranslate nohighlight">\(x,y \in \mathbb{R}^2\)</span>, for some constant <span class="math notranslate nohighlight">\(c &gt; 0\)</span>.</p>
<p>a) Find the value of the constant <span class="math notranslate nohighlight">\(c\)</span>. [<em>Hint:</em> The order of integration matters. You can do this without doing complicated integrals. Specifically, recall that for any <span class="math notranslate nohighlight">\(\mu \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\sigma &gt; 0\)</span>, it holds that</p>
<div class="math notranslate nohighlight">
\[
\int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{(z - \mu)^2}{2\sigma^2}\right) \mathrm{d} z= 1.
\]</div>
<p>]</p>
<p>b) Find the marginal density functions of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>. [<em>Hint:</em> You can do this without doing complicated integrals. Complete the square.]</p>
<p>c) Determine whether <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent. Justify your answer. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.22</strong> (Adapted from [ASV]) Let <span class="math notranslate nohighlight">\(p(x, y)\)</span> be the joint probability mass function of <span class="math notranslate nohighlight">\((X, Y)\)</span>. Assume that there are two functions <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> such that <span class="math notranslate nohighlight">\(p(x,y) = a(x)b(y)\)</span> for all possible values <span class="math notranslate nohighlight">\(x\)</span> of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(y\)</span> of <span class="math notranslate nohighlight">\(Y\)</span>. Show that <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are independent. Do <em>not</em> assume that <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span> are probability mass functions. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.23</strong> (Adapted from [BHK]) a) Let <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> be independent random variables with uniform distribution in <span class="math notranslate nohighlight">\([-1/2 , 1/2]\)</span>. Compute <span class="math notranslate nohighlight">\(\mathbb{E}[X]\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[X^2]\)</span>, <span class="math notranslate nohighlight">\(\mathrm{Var}[X^2]\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[X - Y]\)</span>, <span class="math notranslate nohighlight">\(\mathbb{E}[XY]\)</span>, and <span class="math notranslate nohighlight">\(\mathbb{E}[(X - Y)^2]\)</span>.</p>
<p>b) What is the expected squared Euclidean distance between two points generated at random inside the unit d-dimensional cube <span class="math notranslate nohighlight">\(\mathcal{C} = [-1/2,1/2]^d\)</span>? <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.24</strong> (Adapted from [BHK]) a) For an arbitrary <span class="math notranslate nohighlight">\(a &gt; 0\)</span> give a probability distribution for a nonnegative random variable <span class="math notranslate nohighlight">\(X\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[X \geq a] = \frac{\mathbb{E}[X]}{a}.
\]</div>
<p>b) Show that for any <span class="math notranslate nohighlight">\(c &gt; 0\)</span> there exists a distribution for which <em>Chebyshev’s Inequality</em> is tight, that is,
$<span class="math notranslate nohighlight">\(
\mathbb{P}[|X - \mathbb{E}[X]| \geq c] = \frac{\mathrm{Var}[X]}{c^2}.
\)</span>$</p>
<p>[<em>Hint:</em> Choose a distribution symmetric about <span class="math notranslate nohighlight">\(0\)</span>.] <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.25</strong> (Adapted from [BHK]) Let <span class="math notranslate nohighlight">\(X_1, X_2, \ldots , X_n\)</span> be i.i.d. random variables with
mean <span class="math notranslate nohighlight">\(\mu\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2\)</span>. Let</p>
<div class="math notranslate nohighlight">
\[
\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i,
\]</div>
<p>be the sample mean. Suppose one estimates the variance using the sample mean as follows</p>
<div class="math notranslate nohighlight">
\[
S^2_n = \frac{1}{n} \sum_{i=1}^n \left(X_i - \overline{X}_n\right)^2.
\]</div>
<p>Calculate <span class="math notranslate nohighlight">\(\mathbb{E}(S^2_n)\)</span>. [<em>Hint:</em> Replace <span class="math notranslate nohighlight">\(X_i - \overline{X}_n\)</span> with <span class="math notranslate nohighlight">\((X_i - \mu) - (\overline{X}_n - \mu)\)</span>.] <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.26</strong> Let <span class="math notranslate nohighlight">\(f\)</span> and <span class="math notranslate nohighlight">\(g\)</span> have derivatives at <span class="math notranslate nohighlight">\(x\)</span> and let <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> be constants. Prove from the definition of the derivative that</p>
<div class="math notranslate nohighlight">
\[
[\alpha f(x) + \beta g(x)]' = \alpha f'(x) + \beta g'(x).
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.27</strong> Let <span class="math notranslate nohighlight">\(Z_1 \sim \mathrm{N}(\mu_1, \sigma_1^2)\)</span> and <span class="math notranslate nohighlight">\(Z_2 \sim \mathrm{N}(\mu_2, \sigma_2^2)\)</span> be independent Normal variables with mean <span class="math notranslate nohighlight">\(\mu_1\)</span>, <span class="math notranslate nohighlight">\(\mu_2\)</span> and variance <span class="math notranslate nohighlight">\(\sigma_1^2\)</span> and <span class="math notranslate nohighlight">\(\sigma_2^2\)</span> respectively. Recall that <span class="math notranslate nohighlight">\(Z_1 + Z_2\)</span> is still Normal.</p>
<p>a) What are the mean and variance of <span class="math notranslate nohighlight">\(Z_1 + Z_2\)</span>?</p>
<p>b) Let <span class="math notranslate nohighlight">\(\mathbf{X}_1, \mathbf{X}_2, \mathbf{Y}_1\)</span> be independent spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with mean <span class="math notranslate nohighlight">\(-w \mathbf{e}_1\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{Y}_2\)</span> be an indenpedent spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian with mean <span class="math notranslate nohighlight">\(w \mathbf{e}_1\)</span> and variance <span class="math notranslate nohighlight">\(1\)</span>.  Compute <span class="math notranslate nohighlight">\(\mathbb{E}[\|\mathbf{X}_1 - \mathbf{X}_2\|^2]\)</span> and <span class="math notranslate nohighlight">\(\mathbb{E}[\|\mathbf{Y}_1 - \mathbf{Y}_2\|^2]\)</span> explicitly. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.28</strong> Suppose we are given <span class="math notranslate nohighlight">\(n\)</span> vectors <span class="math notranslate nohighlight">\(\mathbf{x}_1,\ldots,\mathbf{x}_n\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> and a partition <span class="math notranslate nohighlight">\(C_1, \ldots, C_k \subseteq [n]\)</span>. Let <span class="math notranslate nohighlight">\(n_i = |C_i|\)</span> be the size of cluster <span class="math notranslate nohighlight">\(C_i\)</span> and let</p>
<div class="math notranslate nohighlight">
\[
\boldsymbol{\mu}_i^* = \frac{1}{n_i} \sum_{j\in C_i} \mathbf{x}_j
\]</div>
<p>be the centroid of <span class="math notranslate nohighlight">\(C_i\)</span>, for <span class="math notranslate nohighlight">\(i=1,\ldots,k\)</span>.</p>
<p>a) Show that</p>
<div class="math notranslate nohighlight">
\[
\sum_{j \in C_i} \|\mathbf{x}_j - \boldsymbol{\mu}_i^*\|^2 
= \left(\sum_{j \in C_i} \|\mathbf{x}_j\|^2\right) - n_i\|\boldsymbol{\mu}_i^*\|^2.
\]</div>
<p>b) Show that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\|\boldsymbol{\mu}_i^*\|^2 = \frac{1}{n_i^2}\left(\sum_{j \in C_i} \|\mathbf{x}_j\|^2 
+ \sum_{\substack{j, \ell \in C_i\\j \neq \ell}} \mathbf{x}_j^T\mathbf{x}_\ell\right).
\end{split}\]</div>
<p>c) Show that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\sum_{\substack{j, \ell \in C_i\\j \neq \ell}} \|\mathbf{x}_j - \mathbf{x}_\ell\|^2
= 2(n_i-1)\sum_{j \in C_i} \|\mathbf{x}_j\|^2 
- 2 \sum_{\substack{j, \ell \in C_i\\j \neq \ell}} \mathbf{x}_j^T\mathbf{x}_\ell.
\end{split}\]</div>
<p>d) Combine a), b), c) to prove that minimizing the <span class="math notranslate nohighlight">\(k\)</span>-means objective <span class="math notranslate nohighlight">\(\mathcal{G}(C_1,\ldots,C_k)\)</span> over all partitions <span class="math notranslate nohighlight">\(C_1, \ldots, C_k\)</span> of <span class="math notranslate nohighlight">\([n]\)</span> is equivalent to minimizing instead</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^k \frac{1}{2 |C_i|} \sum_{j,\ell \in C_i} \|\mathbf{x}_j - \mathbf{x}_\ell\|^2.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.29</strong> Suppose the rows of <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> are given by the transposes of <span class="math notranslate nohighlight">\(\mathbf{r}_1,\ldots,\mathbf{r}_n \in \mathbb{R}^m\)</span> and the columns of <span class="math notranslate nohighlight">\(A\)</span> are given by <span class="math notranslate nohighlight">\(\mathbf{c}_1,\ldots,\mathbf{c}_m \in \mathbb{R}^n\)</span>. Give expressions for the elements of <span class="math notranslate nohighlight">\(A^T A\)</span> and <span class="math notranslate nohighlight">\(A A^T\)</span> in terms of these vectors. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.30</strong> Give a proof of the <em>Positive Semidefiniteness of the Covariance</em> using the matrix form of <span class="math notranslate nohighlight">\(\bSigma_\bX\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.31</strong> Use the <em>Cauchy-Schwarz Inequality</em> to prove that the correlation coefficient lies in <span class="math notranslate nohighlight">\([-1,1]\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.32</strong> Let <span class="math notranslate nohighlight">\(f(x,y) = g(x) + h(y)\)</span> where <span class="math notranslate nohighlight">\(x, y \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(g,h\)</span> are real-valued continuously differentiable functions. Compute the gradient of <span class="math notranslate nohighlight">\(f\)</span> in terms of the derivatives of <span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.33</strong> Let <span class="math notranslate nohighlight">\(A = [a]\)</span> be a <span class="math notranslate nohighlight">\(1\times 1\)</span> positive definite matrix. Show that <span class="math notranslate nohighlight">\(a &gt; 0\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.34</strong> Show that the diagonal elements of a positive definite matrix are necessarily positive. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>1.35</strong> Let <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span> and let <span class="math notranslate nohighlight">\(c \in \mathbb{R}\)</span>. Show that</p>
<p>a) <span class="math notranslate nohighlight">\((A + B)^T = A^T + B^T\)</span></p>
<p>b) <span class="math notranslate nohighlight">\((c A)^T = c A^T\)</span></p>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
    
</body>
</html>