["```py\nx1 = torch.tensor(1.0, requires_grad=True)\nx2 = torch.tensor(2.0, requires_grad=True) \n```", "```py\nf = 3 * (x1 ** 2) + x2 + torch.exp(x1 * x2)\n\nf.backward()\n\nprint(x1.grad)  # df/dx1\nprint(x2.grad)  # df/dx2 \n```", "```py\ntensor(20.7781)\ntensor(8.3891) \n```", "```py\nz = torch.tensor([1., 2., 3.], requires_grad=True)\n\ng = torch.sum(z ** 2)\ng.backward()\n\nprint(z.grad)  # gradient is (2 z_1, 2 z_2, 2 z_3) \n```", "```py\ntensor([2., 4., 6.]) \n```", "```py\nX = torch.randn(3, 2)  # Random dataset (features)\ny = torch.tensor([[1., 0., 1.]])  # Dataset (labels)\ntheta = torch.ones(2, 1, requires_grad=True)  # Parameter assignment\n\npredict = X @ theta  # Classifier with parameter vector theta\nloss = torch.sum((predict - y)**2)  # Loss function\nloss.backward()  # Compute gradients\n\nprint(theta.grad)  # gradient of loss \n```", "```py\ntensor([[29.7629],\n        [31.4817]]) \n```", "```py\ndef gd_with_ad(f, x0, alpha=1e-3, niters=int(1e6)):\n    xk = torch.tensor(x0, requires_grad=True, dtype=torch.float)\n\n    for _ in range(niters):\n        value = f(xk)\n        value.backward()\n\n        with torch.no_grad():  \n            xk -= alpha * xk.grad\n\n        xk.grad.zero_()\n\n    return xk.numpy(force=True), f(xk).item() \n```", "```py\ndef f(x):\n    return x**3\n\nprint(gd_with_ad(f, 2, niters=int(1e4))) \n```", "```py\n(array(0.03277362, dtype=float32), 3.5202472645323724e-05) \n```", "```py\nprint(gd_with_ad(f, -2, niters=100)) \n```", "```py\n(array(-4.9335055, dtype=float32), -120.07894897460938) \n```", "```py\nx1 = torch.tensor(1.0, requires_grad=True)\nx2 = torch.tensor(2.0, requires_grad=True) \n```", "```py\nf = 3 * (x1 ** 2) + x2 + torch.exp(x1 * x2)\n\nf.backward()\n\nprint(x1.grad)  # df/dx1\nprint(x2.grad)  # df/dx2 \n```", "```py\ntensor(20.7781)\ntensor(8.3891) \n```", "```py\nz = torch.tensor([1., 2., 3.], requires_grad=True)\n\ng = torch.sum(z ** 2)\ng.backward()\n\nprint(z.grad)  # gradient is (2 z_1, 2 z_2, 2 z_3) \n```", "```py\ntensor([2., 4., 6.]) \n```", "```py\nX = torch.randn(3, 2)  # Random dataset (features)\ny = torch.tensor([[1., 0., 1.]])  # Dataset (labels)\ntheta = torch.ones(2, 1, requires_grad=True)  # Parameter assignment\n\npredict = X @ theta  # Classifier with parameter vector theta\nloss = torch.sum((predict - y)**2)  # Loss function\nloss.backward()  # Compute gradients\n\nprint(theta.grad)  # gradient of loss \n```", "```py\ntensor([[29.7629],\n        [31.4817]]) \n```", "```py\ndef gd_with_ad(f, x0, alpha=1e-3, niters=int(1e6)):\n    xk = torch.tensor(x0, requires_grad=True, dtype=torch.float)\n\n    for _ in range(niters):\n        value = f(xk)\n        value.backward()\n\n        with torch.no_grad():  \n            xk -= alpha * xk.grad\n\n        xk.grad.zero_()\n\n    return xk.numpy(force=True), f(xk).item() \n```", "```py\ndef f(x):\n    return x**3\n\nprint(gd_with_ad(f, 2, niters=int(1e4))) \n```", "```py\n(array(0.03277362, dtype=float32), 3.5202472645323724e-05) \n```", "```py\nprint(gd_with_ad(f, -2, niters=100)) \n```", "```py\n(array(-4.9335055, dtype=float32), -120.07894897460938) \n```"]