<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>8.2. Background: Jacobian, chain rule, and a brief introduction to automatic differentiation#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>8.2. Background: Jacobian, chain rule, and a brief introduction to automatic differentiation#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap08_nn/02_chain/roch-mmids-nn-chain.html">https://mmids-textbook.github.io/chap08_nn/02_chain/roch-mmids-nn-chain.html</a></blockquote>

<p>We introduce the Jacobian of a vector-valued function of several variables as well as the <em>Chain Rule</em> for this more general setting. We also give a brief introduction to automatic differentiation. We begin with some additional matrix algebra.</p>
<section id="more-matrix-algebra-hadamard-and-kronecker-products">
<h2><span class="section-number">8.2.1. </span>More matrix algebra: Hadamard and Kronecker products<a class="headerlink" href="#more-matrix-algebra-hadamard-and-kronecker-products" title="Link to this heading">#</a></h2>
<p>First, we introduce the Hadamard product<span class="math notranslate nohighlight">\(\idx{Hadamard product}\xdi\)</span> and division<span class="math notranslate nohighlight">\(\idx{Hadamard division}\xdi\)</span>. The Hadamard product of two matrices (or vectors) of the same dimension, <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i \in [n], j \in [m]}, B = (b_{i,j})_{i\in [n], j \in [m]} \in \mathbb{R}^{n \times m}\)</span>, is defined as the element-wise product</p>
<div class="math notranslate nohighlight">
\[
A \odot B
= (a_{i,j} b_{i,j})_{i\in [n], j \in [m]}.
\]</div>
<p>Similarly the Hadamard division is defined as the element-wise division</p>
<div class="math notranslate nohighlight">
\[
A \oslash B
= (a_{i,j} / b_{i,j})_{i\in [n], j \in [m]}
\]</div>
<p>where we assume that <span class="math notranslate nohighlight">\(b_{i,j} \neq 0\)</span> for all <span class="math notranslate nohighlight">\(i,j\)</span>.</p>
<p><strong>EXAMPLE:</strong> As an illustrative example,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} \odot
  \begin{bmatrix}
    0 &amp; 5 \\
    6 &amp; 7 \\
  \end{bmatrix}
  =
   \begin{bmatrix}
    1 \times 0 &amp; 2 \times 5\\
    3 \times 6 &amp; 4 \times 7\\
  \end{bmatrix}
  =
   \begin{bmatrix}
    0 &amp; 10\\
    18 &amp; 28\\
  \end{bmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Recall that <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is the all-one vector and that, for <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1,\ldots,x_n) \in \mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(\mathrm{diag}(\mathbf{x}) \in \mathbb{R}^{n \times n}\)</span> is the diagonal matrix with diagonal entries <span class="math notranslate nohighlight">\(x_1,\ldots,x_n\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(Properties of the Hadamard Product)</strong> <span class="math notranslate nohighlight">\(\idx{properties of the Hadamard product}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{a} = (a_1,\ldots,a_n), \mathbf{b} = (b_1,\ldots,b_n), \mathbf{c} = (c_1,\ldots,c_n) \in \mathbb{R}^n\)</span>. Then the following hold:</p>
<p>a) <span class="math notranslate nohighlight">\(\mathrm{diag}(\mathbf{a}) \, \mathbf{b} = \mathrm{diag}(\mathbf{a} \odot \mathbf{b})\)</span>;</p>
<p>b) <span class="math notranslate nohighlight">\(\mathbf{a}^T(\mathbf{b} \odot \mathbf{c}) = \mathbf{1}^T(\mathbf{a} \odot \mathbf{b} \odot \mathbf{c})\)</span></p>
<p>and, provided <span class="math notranslate nohighlight">\(a_i \neq 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>, the following hold as well:</p>
<p>c) <span class="math notranslate nohighlight">\(\mathrm{diag}(\mathbf{a}) \, (\mathbf{b} \oslash \mathbf{a}) = \mathrm{diag}(\mathbf{b})\)</span>;</p>
<p>d) <span class="math notranslate nohighlight">\(\mathbf{a}^T \, (\mathbf{b} \oslash \mathbf{a}) = \mathbf{1}^T \mathbf{b}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> a) The product of a diagonal matrix and a vector produces a new vector whose original coordinates are multiplied by the corresponding diagonal entry. That proves the first claim.</p>
<p>b) We have</p>
<div class="math notranslate nohighlight">
\[
\mathbf{a}^T \, (\mathbf{b} \odot \mathbf{c})
= \sum_{i=1}^n a_i (b_i c_i)
= \mathbf{1}^T (\mathbf{a} \odot \mathbf{b} \odot \mathbf{c}).
\]</div>
<p>c) and d) follow respectively from a) and b).</p>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Second, we introduce the Kronecker product<span class="math notranslate nohighlight">\(\idx{Kronecker product}\xdi\)</span>. Let <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i \in [n], j \in [m]} \in \mathbb{R}^{n \times m}\)</span> and <span class="math notranslate nohighlight">\(B = (b_{i,j})_{i \in [p], j \in [q]} \in \mathbb{R}^{p \times q}\)</span> be arbitrary matrices. Their Kronecker product, denoted <span class="math notranslate nohighlight">\(A \otimes B \in \mathbb{R}^{np \times mq}\)</span>, is the following matrix in block form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = 
\begin{pmatrix}
  a_{1,1} B &amp; \cdots &amp; a_{1,m} B \\
             \vdots &amp; \ddots &amp;           \vdots \\
  a_{n,1} B &amp; \cdots &amp; a_{n,m} B
\end{pmatrix}.
\end{split}\]</div>
<p><strong>EXAMPLE:</strong> Here is a simple illustrative example from <a class="reference external" href="https://en.wikipedia.org/wiki/Kronecker_product#Examples">Wikipedia</a>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} \otimes
  \begin{bmatrix}
    0 &amp; 5 \\
    6 &amp; 7 \\
  \end{bmatrix} =
  \begin{bmatrix}
    1 \begin{bmatrix}
      0 &amp; 5 \\
      6 &amp; 7 \\
    \end{bmatrix} &amp; 
    2 \begin{bmatrix}
      0 &amp; 5 \\
      6 &amp; 7 \\
    \end{bmatrix} \\
    3 \begin{bmatrix}
      0 &amp; 5 \\
      6 &amp; 7 \\
    \end{bmatrix} &amp; 
    4 \begin{bmatrix}
      0 &amp; 5 \\
      6 &amp; 7 \\
    \end{bmatrix} \\
  \end{bmatrix} =
  \begin{bmatrix}
     0 &amp;  5 &amp;  0 &amp; 10 \\
     6 &amp;  7 &amp; 12 &amp; 14 \\
     0 &amp; 15 &amp;  0 &amp; 20 \\
    18 &amp; 21 &amp; 24 &amp; 28
  \end{bmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Outer product)</strong> <span class="math notranslate nohighlight">\(\idx{outer product}\xdi\)</span> Here is another example we have encoutered previously, the outer product of two vectors <span class="math notranslate nohighlight">\(\mathbf{u} = (u_1,\ldots,u_n) \in \mathbb{R}^n\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v} = (v_1,\ldots, v_m) \in \mathbb{R}^m\)</span>. Recall that the outer product is defined in block form as the <span class="math notranslate nohighlight">\(n \times m\)</span> matrix</p>
<div class="math notranslate nohighlight">
\[
\mathbf{u} \mathbf{v}^T 
= \begin{pmatrix}
v_1 \mathbf{u} &amp; \cdots &amp; v_m \mathbf{u}
\end{pmatrix}
= \mathbf{v}^T \otimes \mathbf{u}.
\]</div>
<p>Equivalently,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{u} \mathbf{v}^T 
= \begin{pmatrix}
u_1 \mathbf{v}^T\\ \vdots\\ u_n \mathbf{v}^T
\end{pmatrix}
= \mathbf{u} \otimes \mathbf{v}^T.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(continued)</strong> In the previous example the Kronecker product turned out to be commutative (i.e., we had <span class="math notranslate nohighlight">\(\mathbf{v}^T \otimes \mathbf{u} = \mathbf{u} \otimes \mathbf{v}^T\)</span>). This is not the case in general. Going back to the first example above, note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   \begin{bmatrix}
    0 &amp; 5 \\
    6 &amp; 7 \\
  \end{bmatrix}
  \otimes
   \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix}
 =
  \begin{bmatrix}
    0 \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} &amp; 
    5 \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} \\
    6 \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} &amp; 
    7 \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} \\
  \end{bmatrix} =
  \begin{bmatrix}
     0 &amp;  0 &amp;  5 &amp; 10 \\
     0 &amp;  0 &amp; 15 &amp; 20 \\
     6 &amp; 12 &amp;  7 &amp; 14 \\
    18 &amp; 24 &amp; 21 &amp; 28
  \end{bmatrix}.
\end{split}\]</div>
<p>You can check that this is different from what we obtained in the opposite order. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>The proof of the following lemma is left as an exercise.</p>
<p><strong>LEMMA</strong> <strong>(Properties of the Kronecker Product)</strong> <span class="math notranslate nohighlight">\(\idx{properties of the Kronecker Product}\xdi\)</span> The Kronecker product has the following properties:</p>
<p>a) If <span class="math notranslate nohighlight">\(B, C\)</span> are matrices of the same dimension</p>
<div class="math notranslate nohighlight">
\[
A \otimes (B + C) = A \otimes B + A \otimes C
\quad \text{and}\quad
(B + C) \otimes A  = B \otimes A + C \otimes A.
\]</div>
<p>b) If <span class="math notranslate nohighlight">\(A, B, C, D\)</span> are matrices of such size that one can form the matrix products <span class="math notranslate nohighlight">\(AC\)</span> and <span class="math notranslate nohighlight">\(BD\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
(A \otimes B)\,(C \otimes D)
= (AC) \otimes (BD).
\]</div>
<p>c) If <span class="math notranslate nohighlight">\(A, C\)</span> are matrices of the same dimension and <span class="math notranslate nohighlight">\(B, D\)</span> are matrices of the same dimension, then</p>
<div class="math notranslate nohighlight">
\[
(A \otimes B)\odot(C \otimes D)
= (A\odot C) \otimes (B\odot D).
\]</div>
<p>d) If <span class="math notranslate nohighlight">\(A,B\)</span> are invertible, then</p>
<div class="math notranslate nohighlight">
\[
(A \otimes B)^{-1} 
= A^{-1} \otimes B^{-1}.
\]</div>
<p>e) The transpose of <span class="math notranslate nohighlight">\(A \otimes B\)</span> is</p>
<div class="math notranslate nohighlight">
\[
(A \otimes B)^T 
= A^T \otimes B^T.
\]</div>
<p>f) If <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is a column vector and <span class="math notranslate nohighlight">\(A, B\)</span> are matrices of such size that one can form the matrix product <span class="math notranslate nohighlight">\(AB\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{u} \otimes A) B
= \mathbf{u} \otimes (AB)
\quad\text{and}\quad
(A \otimes \mathbf{u}) B
= (AB) \otimes \mathbf{u}.
\]</div>
<p>Similarly,</p>
<div class="math notranslate nohighlight">
\[
A \,(\mathbf{u}^T \otimes B)
= \mathbf{u}^T \otimes (AB)
\quad\text{and}\quad
A \,(B \otimes \mathbf{u}^T)
= (AB) \otimes \mathbf{u}^T.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
</section>
<section id="jacobian">
<h2><span class="section-number">8.2.2. </span>Jacobian<a class="headerlink" href="#jacobian" title="Link to this heading">#</a></h2>
<p>Recall that the derivative of a function of a real variable is the rate of change of the function with respect to the change in the variable. A different way to put this is that <span class="math notranslate nohighlight">\(f'(x)\)</span> is the slope of the tangent line to <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>. Formally, one can approximate <span class="math notranslate nohighlight">\(f(x)\)</span> by a linear function in the neighborhood of <span class="math notranslate nohighlight">\(x\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[
f(x + h)
= f(x)
+ f'(x) h 
+ r(h),
\]</div>
<p>where <span class="math notranslate nohighlight">\(r(h)\)</span> is negligible compared to <span class="math notranslate nohighlight">\(h\)</span> in the sense that</p>
<div class="math notranslate nohighlight">
\[
\lim_{h\to 0} \frac{r(h)}{h} = 0.
\]</div>
<p>Indeed, define</p>
<div class="math notranslate nohighlight">
\[
r(h)
= 
f(x + h)
- f(x)
- f'(x) h.
\]</div>
<p>Then by definition of the derivative</p>
<div class="math notranslate nohighlight">
\[
\lim_{h\to 0} \frac{r(h)}{h} 
= \lim_{h\to 0} \frac{f(x + h)
- f(x)
- f'(x) h}{h} 
= \lim_{h\to 0} \left[\frac{f(x + h)
- f(x)}{h} - f'(x) \right]
= 0.
\]</div>
<p>For vector-valued functions, we have the following generalization. Let <span class="math notranslate nohighlight">\(\mathbf{f} = (f_1, \ldots, f_m)  : D \to \mathbb{R}^m\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x} \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. We say that <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> is diffentiable<span class="math notranslate nohighlight">\(\idx{diffentiable}\xdi\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> if there exists a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times d}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x}+\mathbf{h})
= \mathbf{f}(\mathbf{x}) + A \mathbf{h} + \mathbf{r}(\mathbf{h})
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\lim_{\mathbf{h} \to 0} \frac{\|\mathbf{r}(\mathbf{h})\|_2}{\|\mathbf{h}\|_2} = 0.
\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{f}'(\mathbf{x}) = A\)</span>
is called the differential<span class="math notranslate nohighlight">\(\idx{differential}\xdi\)</span> of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, and we see that the affine map <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) + A \mathbf{h}\)</span> provides an approximation of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> in the neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>We will not derive the full theory here. In the case where each component of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> has continuous partial derivatives in a neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then the differential exists and is equal to the Jacobian, as defined next.</p>
<p><strong>DEFINITION</strong> <strong>(Jacobian)</strong> <span class="math notranslate nohighlight">\(\idx{Jacobian}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{f} = (f_1, \ldots, f_m)  : D \to \mathbb{R}^m\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span> where <span class="math notranslate nohighlight">\(\frac{\partial f_j (\mathbf{x}_0)}{\partial x_i}\)</span> exists for all <span class="math notranslate nohighlight">\(i, j\)</span>. The Jacobian of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> is the <span class="math notranslate nohighlight">\(m \times d\)</span> matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{f}}(\mathbf{x}_0)
= \begin{pmatrix}
\frac{\partial f_1 (\mathbf{x}_0)}{\partial x_1}
&amp;  \ldots &amp; \frac{\partial f_1 (\mathbf{x}_0)}{\partial x_d}\\
\vdots &amp; \ddots &amp; \vdots\\
\frac{\partial f_m (\mathbf{x}_0)}{\partial x_1}
&amp;  \ldots &amp; \frac{\partial f_m (\mathbf{x}_0)}{\partial x_d}
\end{pmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>THEOREM</strong> <strong>(Differential and Jacobian)</strong> <span class="math notranslate nohighlight">\(\idx{differential and Jacobian theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{f} = (f_1, \ldots, f_m)  : D \to \mathbb{R}^m\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Assume that <span class="math notranslate nohighlight">\(\frac{\partial f_j (\mathbf{x}_0)}{\partial x_i}\)</span> exists and is continous is an open ball around <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> for all <span class="math notranslate nohighlight">\(i, j\)</span>. Then the differential at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> is equal to the Jacobian of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>Recall that for any <span class="math notranslate nohighlight">\(A, B\)</span> for which <span class="math notranslate nohighlight">\(AB\)</span> is well-defined it holds that <span class="math notranslate nohighlight">\(\|A B \|_F \leq \|A\|_F \|B\|_F\)</span>. This applies in particular when <span class="math notranslate nohighlight">\(B\)</span> is a column vector, in which case <span class="math notranslate nohighlight">\(\|B\|_F\)</span> is its Euclidean norm.</p>
<p><em>Proof:</em> By the <em>Mean Value Theorem</em>, for each <span class="math notranslate nohighlight">\(i\)</span>, there is <span class="math notranslate nohighlight">\(\xi_{\mathbf{h},i} \in (0,1)\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
f_i(\mathbf{x}_0+\mathbf{h})
= f_i(\mathbf{x}_0) + \nabla f_i(\mathbf{x}_0 + \xi_{\mathbf{h},i} \mathbf{h})^T \mathbf{h}. 
\]</div>
<p>Define</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\tilde{J}(\mathbf{h})
= \begin{pmatrix}
\frac{\partial f_1 (\mathbf{x}_0 + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_1}
&amp;  \ldots &amp; \frac{\partial f_1 (\mathbf{x}_0 + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_d}\\
\vdots &amp; \ddots &amp; \vdots\\
\frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial x_1}
&amp;  \ldots &amp; \frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial x_d}
\end{pmatrix}.
\end{split}\]</div>
<p>Hence we have</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x}_0+\mathbf{h})
- \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
= \tilde{J}(\mathbf{h}) \,\mathbf{h} - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
= \left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}.
\]</div>
<p>Taking a limit as <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span>, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\lim_{\mathbf{h} \to 0} \frac{\|\mathbf{f}(\mathbf{x}_0+\mathbf{h})
- \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2} 
&amp;= \lim_{\mathbf{h} \to 0} \frac{\|\left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\
&amp;\leq \lim_{\mathbf{h} \to 0} \frac{\|\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\|_F \|\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\
&amp;= 0,
\end{align*}\]</div>
<p>by continuity of the partial derivatives. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>EXAMPLE:</strong> An example of a vector-valued function is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{g}(x_1,x_2)
=
\begin{pmatrix}
g_1(x_1,x_2)\\
g_2(x_1,x_2)\\
g_3(x_1,x_2)
\end{pmatrix}
=
\begin{pmatrix}
3 x_1^2\\
x_2\\
x_1 x_2
\end{pmatrix}.
\end{split}\]</div>
<p>Its Jacobian is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(x_1, x_2)
= \begin{pmatrix}
\frac{\partial g_1 (x_1, x_2)}{\partial x_1}
&amp;  \frac{\partial g_1 (x_1, x_2)}{\partial x_2}\\
\frac{\partial g_2 (x_1, x_2)}{\partial x_1}
&amp;  \frac{\partial g_2 (x_1, x_2)}{\partial x_2}\\
\frac{\partial g_3 (x_1, x_2)}{\partial x_1}
&amp;  \frac{\partial g_3 (x_1, x_2)}{\partial x_2}
\end{pmatrix}
= \begin{pmatrix}
6 x_1
&amp;  0\\
0
&amp;  1\\
x_2
&amp;  x_1
\end{pmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Gradient and Jacobian)</strong> For a continuously differentiable real-valued function <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span>, the Jacobian reduces to the row vector</p>
<div class="math notranslate nohighlight">
\[
J_{f}(\mathbf{x}_0)
= \left(\frac{\partial f (\mathbf{x}_0)}{\partial x_1},  
\ldots, 
\frac{\partial f (\mathbf{x}_0)}{\partial x_d}\right)^T
= \nabla f(\mathbf{x}_0)^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}_0)\)</span> is the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Hessian and Jacobian)</strong> For a twice continuously differentiable real-valued function <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span>, the Jacobian of its gradient is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\nabla f}(\mathbf{x}_0)
= \begin{pmatrix}
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_1^2} 
&amp; \cdots 
&amp; \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d \partial x_1}\\
\vdots &amp; \ddots &amp; \vdots\\
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_1 \partial x_d} 
&amp; \cdots 
&amp; \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d^2}
\end{pmatrix},
\end{split}\]</div>
<p>that is, the Hessian (tranposed, but that makes no difference; why?) of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Parametric Curve and Jacobian)</strong> Consider the parametric curve <span class="math notranslate nohighlight">\(\mathbf{g}(t) = (g_1(t), \ldots, g_d(t)) \in \mathbb{R}^d\)</span> for <span class="math notranslate nohighlight">\(t\)</span> in some closed interval of <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Assume that <span class="math notranslate nohighlight">\(\mathbf{g}(t)\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(t\)</span>, that is, each of its component is.</p>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(t)
= \begin{pmatrix}
g_1'(t)\\
\vdots\\
g_m'(t)
\end{pmatrix}
= \mathbf{g}'(t).
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Affine Map)</strong> Let <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i,j} \in \mathbb{R}^{m \times d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} = (b_1,\ldots,b_m) \in \mathbb{R}^{m}\)</span>. Define the vector-valued function <span class="math notranslate nohighlight">\(\mathbf{f} = (f_1, \ldots, f_m)  : \mathbb{R}^d \to \mathbb{R}^m\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x})
= A \mathbf{x} + \mathbf{b}.
\]</div>
<p>This is an affine map. Note in particular that, in the case <span class="math notranslate nohighlight">\(\mathbf{b} = \mathbf{0}\)</span> of a linear map,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x} + \mathbf{y}) 
= A(\mathbf{x} + \mathbf{y}) 
= A\mathbf{x} + A\mathbf{y} 
= \mathbf{f}(\mathbf{x}) + \mathbf{f}(\mathbf{y}).
\]</div>
<p>Denote the rows of <span class="math notranslate nohighlight">\(A\)</span> by <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_1^T,\ldots,\boldsymbol{\alpha}_m^T\)</span>.</p>
<p>We compute the Jacobian of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Note that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f_i (\mathbf{x})}{\partial x_j}
&amp;= \frac{\partial}{\partial x_j}[\boldsymbol{\alpha}_i^T \mathbf{x} + b_i]\\
&amp;= \frac{\partial}{\partial x_j}\left[\sum_{\ell=1}^m a_{i,\ell} x_{\ell} + b_i\right]\\
&amp;= a_{i,j}.
\end{align*}\]</div>
<p>So</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{f}}(\mathbf{x}) = A.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>The following important example is a less straightforward application of the Jacobian.</p>
<p>It will be useful to introduce the vectorization <span class="math notranslate nohighlight">\(\mathrm{vec}(A) \in \mathbb{R}^{nm}\)</span> of a matrix <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)</span> as the vector</p>
<div class="math notranslate nohighlight">
\[
\mathrm{vec}(A)
= (a_{1,1},\ldots,a_{n,1},a_{1,2},\ldots,a_{n,2},\ldots,a_{1,m},\ldots,a_{n,m}).
\]</div>
<p>That is, it is obtained by stacking the columns of <span class="math notranslate nohighlight">\(A\)</span> on top of each other.</p>
<p><strong>EXAMPLE:</strong> <strong>(Jacobian of a Linear Map with Respect to its Matrix)</strong> We take a different tack on the previous example. In data science applications, it will be useful to compute the Jacobian of a linear map <span class="math notranslate nohighlight">\(X \mathbf{z}\)</span> – with respect to the matrix <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times m}\)</span>. Specifically, for a fixed <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^{m}\)</span>, letting <span class="math notranslate nohighlight">\((\mathbf{x}^{(i)})^T\)</span> be the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(X\)</span> we define the function</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{f}(\mathbf{x})
= X \mathbf{z}
= \begin{pmatrix}
(\mathbf{x}^{(1)})^T\\
\vdots\\
(\mathbf{x}^{(n)})^T
\end{pmatrix}
\mathbf{z}
= \begin{pmatrix}
(\mathbf{x}^{(1)})^T \mathbf{z} \\
\vdots\\
(\mathbf{x}^{(n)})^T \mathbf{z}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})\)</span>.</p>
<p>To compute the Jacobian, let us look at its columns that correspond to the variables in <span class="math notranslate nohighlight">\(\mathbf{x}^{(k)}\)</span>, that is, columns <span class="math notranslate nohighlight">\(\alpha_k = (k-1) m + 1\)</span> to <span class="math notranslate nohighlight">\(\beta_k = k m\)</span>. Note that only the <span class="math notranslate nohighlight">\(k\)</span>-th component of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> depends on <span class="math notranslate nohighlight">\(\mathbf{x}^{(k)}\)</span>, so the rows <span class="math notranslate nohighlight">\(\neq k\)</span> of <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x})\)</span> are <span class="math notranslate nohighlight">\(0\)</span> for the corresponding columns.</p>
<p>Row <span class="math notranslate nohighlight">\(k\)</span> on the other hand is <span class="math notranslate nohighlight">\(\mathbf{z}^T\)</span> from our previous formula for the gradient of an affine map. Hence one way to write the columns <span class="math notranslate nohighlight">\(\alpha_k\)</span> to <span class="math notranslate nohighlight">\(\beta_k\)</span> of <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x})\)</span> is <span class="math notranslate nohighlight">\(\mathbf{e}_k \mathbf{z}^T\)</span>, where here <span class="math notranslate nohighlight">\(\mathbf{e}_k \in \mathbb{R}^{n}\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-th standard basis vector of <span class="math notranslate nohighlight">\(\mathbb{R}^{n}\)</span>.</p>
<p>So <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x})\)</span> can be written in block form as</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{f}}(\mathbf{x})
= \begin{pmatrix}
\mathbf{e}_1 \mathbf{z}^T
&amp; \cdots &amp; \mathbf{e}_{n}\mathbf{z}^T
\end{pmatrix}
= I_{n\times n} \otimes \mathbf{z}^T
=: \mathbb{B}_{n}[\mathbf{z}],
\]</div>
<p>where the last equality is a definition. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>We will need one more wrinkle.</p>
<p><strong>EXAMPLE:</strong> <strong>(Jacobian of a Linear Map with Respect to its Input and Matrix)</strong> Consider again the linear map <span class="math notranslate nohighlight">\(X \mathbf{z}\)</span> – this time as a function of <em>both</em> the matrix <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times m}\)</span> and the vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^{m}\)</span>. That is, letting again <span class="math notranslate nohighlight">\((\mathbf{x}^{(i)})^T\)</span> be the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(X\)</span>, we define the function</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{g}(\mathbf{z}, \mathbf{x})
= X \mathbf{z}
= \begin{pmatrix}
(\mathbf{x}^{(1)})^T\\
\vdots\\
(\mathbf{x}^{(n)})^T
\end{pmatrix}
\mathbf{z}
= \begin{pmatrix}
(\mathbf{x}^{(1)})^T \mathbf{z} \\
\vdots\\
(\mathbf{x}^{(n)})^T \mathbf{z}
\end{pmatrix}
\end{split}\]</div>
<p>where as before <span class="math notranslate nohighlight">\(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})\)</span>.</p>
<p>To compute the Jacobian, we think of it as a block matrix and use the two previous examples. The columns of <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\)</span> corresponding to the variables in <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, that is, columns <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(m\)</span>, are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X
=
\begin{pmatrix}
(\mathbf{x}^{(1)})^T\\
\vdots\\
(\mathbf{x}^{(n)})^T
\end{pmatrix}
=: \mathbb{A}_{n}[\mathbf{x}].
\end{split}\]</div>
<p>The columns of <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\)</span> corresponding to the variables in <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, that is, columns <span class="math notranslate nohighlight">\(m + 1\)</span> to <span class="math notranslate nohighlight">\(m + nm\)</span>, are the matrix <span class="math notranslate nohighlight">\(\mathbb{B}_{n}[\mathbf{z}]\)</span>. Note that, in both <span class="math notranslate nohighlight">\(\mathbb{A}_{n}[\mathbf{x}]\)</span> and <span class="math notranslate nohighlight">\(\mathbb{B}_{n}[\mathbf{z}]\)</span>, the subscript <span class="math notranslate nohighlight">\(n\)</span> indicates the number of rows of the matrix. The number of columns is determined by <span class="math notranslate nohighlight">\(n\)</span> and the size of the input vector:</p>
<ul class="simple">
<li><p>the length of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> divided by <span class="math notranslate nohighlight">\(n\)</span> for <span class="math notranslate nohighlight">\(\mathbb{A}_{n}[\mathbf{x}]\)</span>;</p></li>
<li><p>the length of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> multiplied by <span class="math notranslate nohighlight">\(n\)</span> for <span class="math notranslate nohighlight">\(\mathbb{B}_{n}[\mathbf{z}]\)</span>.</p></li>
</ul>
<p>So <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\)</span> can be written in block form as</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})
= \begin{pmatrix}
\mathbb{A}_{n}[\mathbf{x}] &amp;
\mathbb{B}_{n}[\mathbf{z}]
\end{pmatrix}.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Elementwise Function)</strong> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span>, with <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}\)</span>, be a continuously differentiable real-valued function of a single variable. For <span class="math notranslate nohighlight">\(n \geq 2\)</span>, consider applying <span class="math notranslate nohighlight">\(f\)</span> to each entry of a vector <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>, that is, let <span class="math notranslate nohighlight">\(\mathbf{f} : D^n \to \mathbb{R}^n\)</span> with</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x})
= (f_1(\mathbf{x}), \ldots, f_n(\mathbf{x}))
= (f(x_1), \ldots, f(x_n)).
\]</div>
<p>The Jacobian of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> can be computed from <span class="math notranslate nohighlight">\(f'\)</span>, the derivative of the single-variable case. Indeed, letting <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1,\ldots,x_n)\)</span> be such that <span class="math notranslate nohighlight">\(x_i\)</span> is an interior point of <span class="math notranslate nohighlight">\(D\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f_j(\mathbf{x})}{\partial x_j} 
= f'(x_j),
\]</div>
<p>while for <span class="math notranslate nohighlight">\(\ell \neq j\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f_\ell(\mathbf{x})}{\partial x_j} 
=0,
\]</div>
<p>as <span class="math notranslate nohighlight">\(f_\ell(\mathbf{x})\)</span> does not in fact depend on <span class="math notranslate nohighlight">\(x_j\)</span>. In other words, the <span class="math notranslate nohighlight">\(j\)</span>-th column of the Jacobian is <span class="math notranslate nohighlight">\(f'(x_j) \,\mathbf{e}_j\)</span>, where again <span class="math notranslate nohighlight">\(\mathbf{e}_{j}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th standard basis vector in <span class="math notranslate nohighlight">\(\mathbb{R}^{n}\)</span>.</p>
<p>So <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x})\)</span> is the diagonal matrix with diagonal entries <span class="math notranslate nohighlight">\(f'(x_j)\)</span>, <span class="math notranslate nohighlight">\(j=1, \ldots, n\)</span>, which we denote by</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{f}}(\mathbf{x})
= \mathrm{diag}(f'(x_1),\ldots,f'(x_n)).
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
</section>
<section id="generalization-of-the-chain-rule">
<h2><span class="section-number">8.2.3. </span>Generalization of the Chain Rule<a class="headerlink" href="#generalization-of-the-chain-rule" title="Link to this heading">#</a></h2>
<p>As we have seen, functions are often obtained from the composition of simpler ones. We will use the vector notation <span class="math notranslate nohighlight">\(\mathbf{h} = \mathbf{g} \circ \mathbf{f}\)</span> for the function <span class="math notranslate nohighlight">\(\mathbf{h}(\mathbf{x}) = \mathbf{g} (\mathbf{f} (\mathbf{x}))\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(Composition of Continuous Functions)</strong> <span class="math notranslate nohighlight">\(\idx{composition of continuous functions lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{f} : D_1 \to \mathbb{R}^m\)</span>, where <span class="math notranslate nohighlight">\(D_1 \subseteq \mathbb{R}^d\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{g} : D_2 \to \mathbb{R}^p\)</span>, where <span class="math notranslate nohighlight">\(D_2 \subseteq \mathbb{R}^m\)</span>. Assume that <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> is continuous at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> and that <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is continuous <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}_0)\)</span>. Then <span class="math notranslate nohighlight">\(\mathbf{g} \circ \mathbf{f}\)</span> is continuous at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p>The <em>Chain Rule</em> gives a formula for the Jacobian of a composition.</p>
<p><strong>THEOREM</strong> <strong>(Chain Rule)</strong> <span class="math notranslate nohighlight">\(\idx{chain rule}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{f} : D_1 \to \mathbb{R}^m\)</span>, where <span class="math notranslate nohighlight">\(D_1 \subseteq \mathbb{R}^d\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{g} : D_2 \to \mathbb{R}^p\)</span>, where <span class="math notranslate nohighlight">\(D_2 \subseteq \mathbb{R}^m\)</span>. Assume that <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_1\)</span>, and that <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}_0)\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0)
= J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
\,J_{\mathbf{f}}(\mathbf{x}_0)
\]</div>
<p>as a product of matrices. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>Intuitively, the Jacobian provides a linear approximation of the function in the neighborhood of a point. The composition of linear maps corresponds to the product of the associated matrices. Similarly, the Jacobian of a composition is the product of the Jacobians.</p>
<p><em>Proof:</em> To avoid confusion, we think of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> as being functions of variables with different names, specifically,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x})
= (f_1(x_1,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_d))
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\mathbf{g}(\mathbf{y})
= (g_1(y_1,\ldots,y_m),\ldots,g_p(y_1,\ldots,y_m)).
\]</div>
<p>We apply the <em>Chain Rule</em> for a real-valued function over a parametric vector curve. That is, we think of</p>
<div class="math notranslate nohighlight">
\[
h_i(\mathbf{x})
= g_i(\mathbf{f}(\mathbf{x}))
= g_i(f_1(x_1,\ldots,x_j,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_j,\ldots,x_d))
\]</div>
<p>as <em>a function of <span class="math notranslate nohighlight">\(x_j\)</span> only</em> with all other <span class="math notranslate nohighlight">\(x_i\)</span>s fixed.</p>
<p>We get that</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial h_i(\mathbf{x}_0)}{\partial x_j}
= \sum_{k=1}^m \frac{\partial g_i(\mathbf{f}(\mathbf{x}_0))}
{\partial y_k} \frac{\partial f_k(\mathbf{x}_0)}{\partial x_j}
\]</div>
<p>where, as before, the notation <span class="math notranslate nohighlight">\(\frac{\partial g_i}
{\partial y_k}\)</span> indicates the partial derivative of <span class="math notranslate nohighlight">\(g_i\)</span> with respect to its <span class="math notranslate nohighlight">\(k\)</span>-th component. In matrix form, the claim follows. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Affine Map continued)</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^{m}\)</span>. Define again the vector-valued function <span class="math notranslate nohighlight">\(\mathbf{f}  : \mathbb{R}^d \to \mathbb{R}^m\)</span> as
<span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\)</span>.
In addition, for <span class="math notranslate nohighlight">\(C \in \mathbb{R}^{p \times m}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{d} \in \mathbb{R}^{p}\)</span>, define <span class="math notranslate nohighlight">\(\mathbf{g}  : \mathbb{R}^m \to \mathbb{R}^p\)</span> as
<span class="math notranslate nohighlight">\(\mathbf{g}(\mathbf{y}) = C \mathbf{y} + \mathbf{d}\)</span>.</p>
<p>Then</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x})
= J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}))
\,J_{\mathbf{f}}(\mathbf{x})
= C A,
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span>.</p>
<p>This is consistent with the observation that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{g} \circ \mathbf{f} (\mathbf{x})
= \mathbf{g} (\mathbf{f} (\mathbf{x}))
= C( A\mathbf{x} + \mathbf{b} ) + \mathbf{d}
= CA \mathbf{x} + (C\mathbf{b} + \mathbf{d}).
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> Suppose we want to compute the gradient of the function</p>
<div class="math notranslate nohighlight">
\[
f(x_1, x_2)
= 3 x_1^2 
+ x_2
+ \exp(x_1 x_2).
\]</div>
<p>We could apply the <em>Chain Rule</em> directly, but to illustrate the perspective that is coming up, we think of <span class="math notranslate nohighlight">\(f\)</span> as a composition of “simpler” vector-valued functions. Specifically, let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{g}(x_1,x_2)
=
\begin{pmatrix}
3 x_1^2\\
x_2\\
x_1 x_2
\end{pmatrix}
\qquad
h(y_1,y_2,y_3)
=
y_1 + y_2 + \exp(y_3).
\end{split}\]</div>
<p>Then <span class="math notranslate nohighlight">\(f(x_1, x_2) = h(\mathbf{g}(x_1, x_2)) = h \circ \mathbf{g}(x_1, x_2)\)</span>.</p>
<p>By the <em>Chain Rule</em>, we can compute the gradient of <span class="math notranslate nohighlight">\(f\)</span> by first computing the Jacobians of <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> and <span class="math notranslate nohighlight">\(h\)</span>. We have already computed the Jacobian of <span class="math notranslate nohighlight">\(\mathbf{g}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(x_1, x_2)
= \begin{pmatrix}
6 x_1
&amp;  0\\
0
&amp;  1\\
x_2
&amp;  x_1
\end{pmatrix}.
\end{split}\]</div>
<p>The Jacobian of <span class="math notranslate nohighlight">\(h\)</span> is</p>
<div class="math notranslate nohighlight">
\[
J_h(y_1, y_2, y_3)
=
\begin{pmatrix}
\frac{\partial h(y_1, y_2, y_3)}{\partial y_1}
&amp;  \frac{\partial h(y_1, y_2, y_3)}{\partial y_2}
&amp; \frac{\partial h(y_1, y_2, y_3)}{\partial y_3}
\end{pmatrix}
=
\begin{pmatrix}
1
&amp; 1
&amp; \exp(y_3)
\end{pmatrix}.
\]</div>
<p>The <em>Chain Rule</em> stipulates that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(x_1, x_2)^T
&amp;= J_f(x_1, x_2)\\
&amp;= J_h(\mathbf{g}(x_1,x_2)) \, J_{\mathbf{g}}(x_1, x_2)\\
&amp;= \begin{pmatrix}
1
&amp; 1
&amp; \exp(g_3(x_1, x_2))
\end{pmatrix}
\begin{pmatrix}
6 x_1
&amp;  0\\
0
&amp;  1\\
x_2
&amp;  x_1
\end{pmatrix}\\
&amp;= \begin{pmatrix}
1
&amp; 1
&amp; \exp(x_1 x_2)
\end{pmatrix}
\begin{pmatrix}
6 x_1
&amp;  0\\
0
&amp;  1\\
x_2
&amp;  x_1
\end{pmatrix}\\
&amp;= \begin{pmatrix}
6 x_1 + x_2 \exp(x_1 x_2)
&amp; 1 + x_1 \exp(x_1 x_2)
\end{pmatrix}.
\end{align*}\]</div>
<p>You can check directly (i.e., without the composition) that this is indeed the correct gradient (transposed).</p>
<p>Alternatively, it is instructive to “expand” the <em>Chain Rule</em> as we did in its proof. Specifically,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f (x_1, x_2)}{\partial x_1} 
&amp;= \sum_{i=1}^3 \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1, x_2)}{\partial x_1}\\
&amp;= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1} \frac{\partial g_1 (x_1, x_2)}{\partial x_1}
+ \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_1}
+ \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_1}\\
&amp;= 1 \cdot 6x_1 + 1 \cdot 0 + \exp(g_3(x_1, x_2)) \cdot x_2\\
&amp;= 6 x_1 + x_2 \exp(x_1 x_2).
\end{align*}\]</div>
<p>Note that this corresponds to multiplying <span class="math notranslate nohighlight">\(J_h(\mathbf{g}(x_1,x_2))\)</span> by the first column of <span class="math notranslate nohighlight">\(J_{\mathbf{g}}(x_1, x_2)\)</span>.</p>
<p>Similarly</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f (x_1, x_2)}{\partial x_2} 
&amp;= \sum_{i=1}^3 \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1, x_2)}{\partial x_2}\\
&amp;= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1} \frac{\partial g_1 (x_1, x_2)}{\partial x_2}
+ \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_2}
+ \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_2}\\
&amp;= 1 \cdot 0 + 1 \cdot 1 + \exp(g_3(x_1, x_2)) \cdot x_1\\
&amp;= 1 + x_1 \exp(x_1 x_2).
\end{align*}\]</div>
<p>This corresponds to multiplying <span class="math notranslate nohighlight">\(J_h(\mathbf{g}(x_1,x_2))\)</span> by the second column of <span class="math notranslate nohighlight">\(J_{\mathbf{g}}(x_1, x_2)\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The Jacobian determinant has important applications in change of variables for multivariable integrals. Ask your favorite AI chatbot to explain this application and provide an example of using the Jacobian determinant in a change of variables for a double integral. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
</section>
<section id="brief-introduction-to-automatic-differentiation-in-pytorch">
<h2><span class="section-number">8.2.4. </span>Brief introduction to automatic differentiation in PyTorch<a class="headerlink" href="#brief-introduction-to-automatic-differentiation-in-pytorch" title="Link to this heading">#</a></h2>
<p>We illustrate the use of <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a><span class="math notranslate nohighlight">\(\idx{automatic differentiation}\xdi\)</span> to compute gradients in PyTorch.</p>
<p>Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">Wikipedia</a>:</p>
<blockquote>
<div><p>In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation or computational differentiation, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program. Automatic differentiation is distinct from symbolic differentiation and numerical differentiation (the method of finite differences). Symbolic differentiation can lead to inefficient code and faces the difficulty of converting a computer program into a single expression, while numerical differentiation can introduce round-off errors in the discretization process and cancellation.</p>
</div></blockquote>
<p><strong>Automatic differentiation in PyTorch</strong> We will use <a class="reference external" href="https://pytorch.org/tutorials/">PyTorch</a>. It uses <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html">tensors</a><span class="math notranslate nohighlight">\(\idx{tensor}\xdi\)</span>, which in many ways behave similarly to NumPy arrays. See <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">here</a> for a quick introduction. We first initialize the tensors. Here each tensor corresponds to a single real variable. With the option <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad"><code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code></a>, we indicate that these are variables with respect to which a gradient will be taken later. We initialize the tensors at the values where the derivatives will be computed. If derivatives need to be computed at different values, we need to repeat this process. The function <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html"><code class="docutils literal notranslate"><span class="pre">.backward()</span></code></a> computes the gradient using backpropagation, to which we will return later. The partial derivatives are accessed with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html"><code class="docutils literal notranslate"><span class="pre">.grad</span></code></a>.</p>
<p><strong>NUMERICAL CORNER:</strong> This is better understood through an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We define the function. Note that we use
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.exp.html"><code class="docutils literal notranslate"><span class="pre">torch.exp</span></code></a>, the PyTorch implementation of the (element-wise) exponential function. Moreover, as in NumPy, PyTorch allows the use of <code class="docutils literal notranslate"><span class="pre">**</span></code> for <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.pow.html">taking a power</a>. <a class="reference external" href="https://pytorch.org/docs/stable/name_inference.html">Here</a> is a list of operations on tensors in PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">f</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span><span class="p">)</span>

<span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># df/dx1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># df/dx2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor(20.7781)
tensor(8.3891)
</pre></div>
</div>
</div>
</div>
<p>The input parameters can also be vectors, which allows to consider functions of large numbers of variables. Here we use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"><code class="docutils literal notranslate"><span class="pre">torch.sum</span></code></a> for taking a sum of the arguments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">z</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># gradient is (2 z_1, 2 z_2, 2 z_3)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([2., 4., 6.])
</pre></div>
</div>
</div>
</div>
<p>Here is another typical example in a data science context.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Random dataset (features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>  <span class="c1"># Dataset (labels)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Parameter assignment</span>

<span class="n">predict</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">theta</span>  <span class="c1"># Classifier with parameter vector theta</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">predict</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Loss function</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Compute gradients</span>

<span class="nb">print</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># gradient of loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([[29.7629],
        [31.4817]])
</pre></div>
</div>
</div>
</div>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot to explain how to compute a second derivative using PyTorch (it’s bit tricky). Ask for code that you can apply to the previous examples. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb">Open In Colab</a>) <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Implementing gradient descent in PyTorch</strong> Rather than explicitly specifying the gradient function, we could use PyTorch to compute it automatically. This is done next. Note that the descent update is done within <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html"><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">torch.no_grad()</span></code></a>, which ensures that the update operation itself is not tracked for gradient computation. Here the input <code class="docutils literal notranslate"><span class="pre">x0</span></code> as well as the output <code class="docutils literal notranslate"><span class="pre">xk.numpy(force=True)</span></code> are NumPy arrays. The function <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html"><code class="docutils literal notranslate"><span class="pre">torch.Tensor.numpy()</span></code></a> converts a PyTorch tensor to a NumPy array (see the documentation for an explanation of the <code class="docutils literal notranslate"><span class="pre">force=True</span></code> option). Also, quoting ChatGPT:</p>
<blockquote>
<div><p>In the given code, <code class="docutils literal notranslate"><span class="pre">.item()</span></code> is used to extract the scalar value from a tensor. In PyTorch, when you perform operations on tensors, you get back tensors as results, even if the result is a single scalar value. <code class="docutils literal notranslate"><span class="pre">.item()</span></code> is used to extract this scalar value from the tensor.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">gd_with_ad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">)):</span>
    <span class="n">xk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span>
        <span class="n">value</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  
            <span class="n">xk</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">xk</span><span class="o">.</span><span class="n">grad</span>

        <span class="n">xk</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">xk</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">f</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We revisit a previous example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span>

<span class="nb">print</span><span class="p">(</span><span class="n">gd_with_ad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(array(0.03277362, dtype=float32), 3.5202472645323724e-05)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">gd_with_ad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(array(-4.9335055, dtype=float32), -120.07894897460938)
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The section briefly mentions that automatic differentiation is distinct from symbolic differentiation and numerical differentiation. Ask your favorite AI chatbot to explain in more detail the differences between these three methods of computing derivatives. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> and <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{p \times q}\)</span>. What are the dimensions of the Kronecker product <span class="math notranslate nohighlight">\(A \otimes B\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(n \times m\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(p \times q\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(np \times mq\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(nq \times mp\)</span></p>
<p><strong>2</strong> If <span class="math notranslate nohighlight">\(f: \mathbb{R}^d \rightarrow \mathbb{R}^m\)</span> is a continuously differentiable function, what is its Jacobian <span class="math notranslate nohighlight">\(J_f(x_0)\)</span> at an interior point <span class="math notranslate nohighlight">\(x_0\)</span> of its domain?</p>
<p>a) A scalar representing the rate of change of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>b) A vector in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> representing the direction of steepest ascent of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>c) An <span class="math notranslate nohighlight">\(m \times d\)</span> matrix of partial derivatives of the component functions of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>d) The Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p><strong>3</strong> In the context of the Chain Rule, if <span class="math notranslate nohighlight">\(f: \mathbb{R}^2 \to \mathbb{R}^3\)</span> and <span class="math notranslate nohighlight">\(g: \mathbb{R}^3 \to \mathbb{R}\)</span>, what is the dimension of the Jacobian matrix <span class="math notranslate nohighlight">\(J_{g \circ f}(x)\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(3 \times 2\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(1 \times 3\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(2 \times 3\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(1 \times 2\)</span></p>
<p><strong>4</strong> Let <span class="math notranslate nohighlight">\(\mathbf{f} : D_1 \to \mathbb{R}^m\)</span>, where <span class="math notranslate nohighlight">\(D_1 \subseteq \mathbb{R}^d\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{g} : D_2 \to \mathbb{R}^p\)</span>, where <span class="math notranslate nohighlight">\(D_2 \subseteq \mathbb{R}^m\)</span>. Assume that <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_1\)</span>, and that <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}_0)\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_2\)</span>. Which of the following is correct according to the Chain Rule?</p>
<p>a) <span class="math notranslate nohighlight">\(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{x}_0) \, J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0)) \, J_{\mathbf{f}}(\mathbf{x}_0)\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0)) \, J_{\mathbf{g}}(\mathbf{x}_0)\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{x}_0) \, J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0))\)</span></p>
<p><strong>5</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^{m}\)</span>. Define the vector-valued function <span class="math notranslate nohighlight">\(\mathbf{f}  : \mathbb{R}^d \to \mathbb{R}^m\)</span> as <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\)</span>. What is the Jacobian of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x}_0) = A^T\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x}_0) = A \mathbf{x}_0 + \mathbf{b}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x}_0) = A\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x}_0) = \mathbf{b}\)</span></p>
<p>Answer for 1: c. Justification: The text defines the Kronecker product as a matrix in block form with dimensions <span class="math notranslate nohighlight">\(np \times mq\)</span>.</p>
<p>Answer for 2: c. Justification: The text defines the Jacobian of a vector-valued function as a matrix of partial derivatives.</p>
<p>Answer for 3: d. Justification: The composition <span class="math notranslate nohighlight">\(g \circ f\)</span> maps <span class="math notranslate nohighlight">\(\mathbb{R}^2 \to \mathbb{R}\)</span>, hence the Jacobian matrix <span class="math notranslate nohighlight">\(J_{g \circ f}(x)\)</span> is <span class="math notranslate nohighlight">\(1 \times 2\)</span>.</p>
<p>Answer for 4: b. Justification: The text states: “The Chain Rule gives a formula for the Jacobian of a composition. […] Assume that <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_1\)</span>, and that <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}_0)\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0)) \,J_{\mathbf{f}}(\mathbf{x}_0)
\]</div>
<p>as a product of matrices.”</p>
<p>Answer for 5: c. Justification: The text states: “Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} = (b_1,\ldots,b_m) \in \mathbb{R}^{m}\)</span>. Define the vector-valued function <span class="math notranslate nohighlight">\(\mathbf{f} = (f_1, \ldots, f_m)  : \mathbb{R}^d \to \mathbb{R}^m\)</span> as <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\)</span>. […] So <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x}) = A.\)</span>”</p>
</section>
&#13;

<h2><span class="section-number">8.2.1. </span>More matrix algebra: Hadamard and Kronecker products<a class="headerlink" href="#more-matrix-algebra-hadamard-and-kronecker-products" title="Link to this heading">#</a></h2>
<p>First, we introduce the Hadamard product<span class="math notranslate nohighlight">\(\idx{Hadamard product}\xdi\)</span> and division<span class="math notranslate nohighlight">\(\idx{Hadamard division}\xdi\)</span>. The Hadamard product of two matrices (or vectors) of the same dimension, <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i \in [n], j \in [m]}, B = (b_{i,j})_{i\in [n], j \in [m]} \in \mathbb{R}^{n \times m}\)</span>, is defined as the element-wise product</p>
<div class="math notranslate nohighlight">
\[
A \odot B
= (a_{i,j} b_{i,j})_{i\in [n], j \in [m]}.
\]</div>
<p>Similarly the Hadamard division is defined as the element-wise division</p>
<div class="math notranslate nohighlight">
\[
A \oslash B
= (a_{i,j} / b_{i,j})_{i\in [n], j \in [m]}
\]</div>
<p>where we assume that <span class="math notranslate nohighlight">\(b_{i,j} \neq 0\)</span> for all <span class="math notranslate nohighlight">\(i,j\)</span>.</p>
<p><strong>EXAMPLE:</strong> As an illustrative example,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} \odot
  \begin{bmatrix}
    0 &amp; 5 \\
    6 &amp; 7 \\
  \end{bmatrix}
  =
   \begin{bmatrix}
    1 \times 0 &amp; 2 \times 5\\
    3 \times 6 &amp; 4 \times 7\\
  \end{bmatrix}
  =
   \begin{bmatrix}
    0 &amp; 10\\
    18 &amp; 28\\
  \end{bmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Recall that <span class="math notranslate nohighlight">\(\mathbf{1}\)</span> is the all-one vector and that, for <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1,\ldots,x_n) \in \mathbb{R}^n\)</span>, <span class="math notranslate nohighlight">\(\mathrm{diag}(\mathbf{x}) \in \mathbb{R}^{n \times n}\)</span> is the diagonal matrix with diagonal entries <span class="math notranslate nohighlight">\(x_1,\ldots,x_n\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(Properties of the Hadamard Product)</strong> <span class="math notranslate nohighlight">\(\idx{properties of the Hadamard product}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{a} = (a_1,\ldots,a_n), \mathbf{b} = (b_1,\ldots,b_n), \mathbf{c} = (c_1,\ldots,c_n) \in \mathbb{R}^n\)</span>. Then the following hold:</p>
<p>a) <span class="math notranslate nohighlight">\(\mathrm{diag}(\mathbf{a}) \, \mathbf{b} = \mathrm{diag}(\mathbf{a} \odot \mathbf{b})\)</span>;</p>
<p>b) <span class="math notranslate nohighlight">\(\mathbf{a}^T(\mathbf{b} \odot \mathbf{c}) = \mathbf{1}^T(\mathbf{a} \odot \mathbf{b} \odot \mathbf{c})\)</span></p>
<p>and, provided <span class="math notranslate nohighlight">\(a_i \neq 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>, the following hold as well:</p>
<p>c) <span class="math notranslate nohighlight">\(\mathrm{diag}(\mathbf{a}) \, (\mathbf{b} \oslash \mathbf{a}) = \mathrm{diag}(\mathbf{b})\)</span>;</p>
<p>d) <span class="math notranslate nohighlight">\(\mathbf{a}^T \, (\mathbf{b} \oslash \mathbf{a}) = \mathbf{1}^T \mathbf{b}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> a) The product of a diagonal matrix and a vector produces a new vector whose original coordinates are multiplied by the corresponding diagonal entry. That proves the first claim.</p>
<p>b) We have</p>
<div class="math notranslate nohighlight">
\[
\mathbf{a}^T \, (\mathbf{b} \odot \mathbf{c})
= \sum_{i=1}^n a_i (b_i c_i)
= \mathbf{1}^T (\mathbf{a} \odot \mathbf{b} \odot \mathbf{c}).
\]</div>
<p>c) and d) follow respectively from a) and b).</p>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Second, we introduce the Kronecker product<span class="math notranslate nohighlight">\(\idx{Kronecker product}\xdi\)</span>. Let <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i \in [n], j \in [m]} \in \mathbb{R}^{n \times m}\)</span> and <span class="math notranslate nohighlight">\(B = (b_{i,j})_{i \in [p], j \in [q]} \in \mathbb{R}^{p \times q}\)</span> be arbitrary matrices. Their Kronecker product, denoted <span class="math notranslate nohighlight">\(A \otimes B \in \mathbb{R}^{np \times mq}\)</span>, is the following matrix in block form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A \otimes B = 
\begin{pmatrix}
  a_{1,1} B &amp; \cdots &amp; a_{1,m} B \\
             \vdots &amp; \ddots &amp;           \vdots \\
  a_{n,1} B &amp; \cdots &amp; a_{n,m} B
\end{pmatrix}.
\end{split}\]</div>
<p><strong>EXAMPLE:</strong> Here is a simple illustrative example from <a class="reference external" href="https://en.wikipedia.org/wiki/Kronecker_product#Examples">Wikipedia</a>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} \otimes
  \begin{bmatrix}
    0 &amp; 5 \\
    6 &amp; 7 \\
  \end{bmatrix} =
  \begin{bmatrix}
    1 \begin{bmatrix}
      0 &amp; 5 \\
      6 &amp; 7 \\
    \end{bmatrix} &amp; 
    2 \begin{bmatrix}
      0 &amp; 5 \\
      6 &amp; 7 \\
    \end{bmatrix} \\
    3 \begin{bmatrix}
      0 &amp; 5 \\
      6 &amp; 7 \\
    \end{bmatrix} &amp; 
    4 \begin{bmatrix}
      0 &amp; 5 \\
      6 &amp; 7 \\
    \end{bmatrix} \\
  \end{bmatrix} =
  \begin{bmatrix}
     0 &amp;  5 &amp;  0 &amp; 10 \\
     6 &amp;  7 &amp; 12 &amp; 14 \\
     0 &amp; 15 &amp;  0 &amp; 20 \\
    18 &amp; 21 &amp; 24 &amp; 28
  \end{bmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Outer product)</strong> <span class="math notranslate nohighlight">\(\idx{outer product}\xdi\)</span> Here is another example we have encoutered previously, the outer product of two vectors <span class="math notranslate nohighlight">\(\mathbf{u} = (u_1,\ldots,u_n) \in \mathbb{R}^n\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v} = (v_1,\ldots, v_m) \in \mathbb{R}^m\)</span>. Recall that the outer product is defined in block form as the <span class="math notranslate nohighlight">\(n \times m\)</span> matrix</p>
<div class="math notranslate nohighlight">
\[
\mathbf{u} \mathbf{v}^T 
= \begin{pmatrix}
v_1 \mathbf{u} &amp; \cdots &amp; v_m \mathbf{u}
\end{pmatrix}
= \mathbf{v}^T \otimes \mathbf{u}.
\]</div>
<p>Equivalently,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{u} \mathbf{v}^T 
= \begin{pmatrix}
u_1 \mathbf{v}^T\\ \vdots\\ u_n \mathbf{v}^T
\end{pmatrix}
= \mathbf{u} \otimes \mathbf{v}^T.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(continued)</strong> In the previous example the Kronecker product turned out to be commutative (i.e., we had <span class="math notranslate nohighlight">\(\mathbf{v}^T \otimes \mathbf{u} = \mathbf{u} \otimes \mathbf{v}^T\)</span>). This is not the case in general. Going back to the first example above, note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
   \begin{bmatrix}
    0 &amp; 5 \\
    6 &amp; 7 \\
  \end{bmatrix}
  \otimes
   \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix}
 =
  \begin{bmatrix}
    0 \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} &amp; 
    5 \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} \\
    6 \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} &amp; 
    7 \begin{bmatrix}
    1 &amp; 2 \\
    3 &amp; 4 \\
  \end{bmatrix} \\
  \end{bmatrix} =
  \begin{bmatrix}
     0 &amp;  0 &amp;  5 &amp; 10 \\
     0 &amp;  0 &amp; 15 &amp; 20 \\
     6 &amp; 12 &amp;  7 &amp; 14 \\
    18 &amp; 24 &amp; 21 &amp; 28
  \end{bmatrix}.
\end{split}\]</div>
<p>You can check that this is different from what we obtained in the opposite order. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>The proof of the following lemma is left as an exercise.</p>
<p><strong>LEMMA</strong> <strong>(Properties of the Kronecker Product)</strong> <span class="math notranslate nohighlight">\(\idx{properties of the Kronecker Product}\xdi\)</span> The Kronecker product has the following properties:</p>
<p>a) If <span class="math notranslate nohighlight">\(B, C\)</span> are matrices of the same dimension</p>
<div class="math notranslate nohighlight">
\[
A \otimes (B + C) = A \otimes B + A \otimes C
\quad \text{and}\quad
(B + C) \otimes A  = B \otimes A + C \otimes A.
\]</div>
<p>b) If <span class="math notranslate nohighlight">\(A, B, C, D\)</span> are matrices of such size that one can form the matrix products <span class="math notranslate nohighlight">\(AC\)</span> and <span class="math notranslate nohighlight">\(BD\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
(A \otimes B)\,(C \otimes D)
= (AC) \otimes (BD).
\]</div>
<p>c) If <span class="math notranslate nohighlight">\(A, C\)</span> are matrices of the same dimension and <span class="math notranslate nohighlight">\(B, D\)</span> are matrices of the same dimension, then</p>
<div class="math notranslate nohighlight">
\[
(A \otimes B)\odot(C \otimes D)
= (A\odot C) \otimes (B\odot D).
\]</div>
<p>d) If <span class="math notranslate nohighlight">\(A,B\)</span> are invertible, then</p>
<div class="math notranslate nohighlight">
\[
(A \otimes B)^{-1} 
= A^{-1} \otimes B^{-1}.
\]</div>
<p>e) The transpose of <span class="math notranslate nohighlight">\(A \otimes B\)</span> is</p>
<div class="math notranslate nohighlight">
\[
(A \otimes B)^T 
= A^T \otimes B^T.
\]</div>
<p>f) If <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is a column vector and <span class="math notranslate nohighlight">\(A, B\)</span> are matrices of such size that one can form the matrix product <span class="math notranslate nohighlight">\(AB\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
(\mathbf{u} \otimes A) B
= \mathbf{u} \otimes (AB)
\quad\text{and}\quad
(A \otimes \mathbf{u}) B
= (AB) \otimes \mathbf{u}.
\]</div>
<p>Similarly,</p>
<div class="math notranslate nohighlight">
\[
A \,(\mathbf{u}^T \otimes B)
= \mathbf{u}^T \otimes (AB)
\quad\text{and}\quad
A \,(B \otimes \mathbf{u}^T)
= (AB) \otimes \mathbf{u}^T.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
&#13;

<h2><span class="section-number">8.2.2. </span>Jacobian<a class="headerlink" href="#jacobian" title="Link to this heading">#</a></h2>
<p>Recall that the derivative of a function of a real variable is the rate of change of the function with respect to the change in the variable. A different way to put this is that <span class="math notranslate nohighlight">\(f'(x)\)</span> is the slope of the tangent line to <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>. Formally, one can approximate <span class="math notranslate nohighlight">\(f(x)\)</span> by a linear function in the neighborhood of <span class="math notranslate nohighlight">\(x\)</span> as follows</p>
<div class="math notranslate nohighlight">
\[
f(x + h)
= f(x)
+ f'(x) h 
+ r(h),
\]</div>
<p>where <span class="math notranslate nohighlight">\(r(h)\)</span> is negligible compared to <span class="math notranslate nohighlight">\(h\)</span> in the sense that</p>
<div class="math notranslate nohighlight">
\[
\lim_{h\to 0} \frac{r(h)}{h} = 0.
\]</div>
<p>Indeed, define</p>
<div class="math notranslate nohighlight">
\[
r(h)
= 
f(x + h)
- f(x)
- f'(x) h.
\]</div>
<p>Then by definition of the derivative</p>
<div class="math notranslate nohighlight">
\[
\lim_{h\to 0} \frac{r(h)}{h} 
= \lim_{h\to 0} \frac{f(x + h)
- f(x)
- f'(x) h}{h} 
= \lim_{h\to 0} \left[\frac{f(x + h)
- f(x)}{h} - f'(x) \right]
= 0.
\]</div>
<p>For vector-valued functions, we have the following generalization. Let <span class="math notranslate nohighlight">\(\mathbf{f} = (f_1, \ldots, f_m)  : D \to \mathbb{R}^m\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x} \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. We say that <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> is diffentiable<span class="math notranslate nohighlight">\(\idx{diffentiable}\xdi\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> if there exists a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times d}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x}+\mathbf{h})
= \mathbf{f}(\mathbf{x}) + A \mathbf{h} + \mathbf{r}(\mathbf{h})
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\lim_{\mathbf{h} \to 0} \frac{\|\mathbf{r}(\mathbf{h})\|_2}{\|\mathbf{h}\|_2} = 0.
\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{f}'(\mathbf{x}) = A\)</span>
is called the differential<span class="math notranslate nohighlight">\(\idx{differential}\xdi\)</span> of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, and we see that the affine map <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) + A \mathbf{h}\)</span> provides an approximation of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> in the neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p>
<p>We will not derive the full theory here. In the case where each component of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> has continuous partial derivatives in a neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, then the differential exists and is equal to the Jacobian, as defined next.</p>
<p><strong>DEFINITION</strong> <strong>(Jacobian)</strong> <span class="math notranslate nohighlight">\(\idx{Jacobian}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{f} = (f_1, \ldots, f_m)  : D \to \mathbb{R}^m\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span> where <span class="math notranslate nohighlight">\(\frac{\partial f_j (\mathbf{x}_0)}{\partial x_i}\)</span> exists for all <span class="math notranslate nohighlight">\(i, j\)</span>. The Jacobian of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> is the <span class="math notranslate nohighlight">\(m \times d\)</span> matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{f}}(\mathbf{x}_0)
= \begin{pmatrix}
\frac{\partial f_1 (\mathbf{x}_0)}{\partial x_1}
&amp;  \ldots &amp; \frac{\partial f_1 (\mathbf{x}_0)}{\partial x_d}\\
\vdots &amp; \ddots &amp; \vdots\\
\frac{\partial f_m (\mathbf{x}_0)}{\partial x_1}
&amp;  \ldots &amp; \frac{\partial f_m (\mathbf{x}_0)}{\partial x_d}
\end{pmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p><strong>THEOREM</strong> <strong>(Differential and Jacobian)</strong> <span class="math notranslate nohighlight">\(\idx{differential and Jacobian theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{f} = (f_1, \ldots, f_m)  : D \to \mathbb{R}^m\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Assume that <span class="math notranslate nohighlight">\(\frac{\partial f_j (\mathbf{x}_0)}{\partial x_i}\)</span> exists and is continous is an open ball around <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> for all <span class="math notranslate nohighlight">\(i, j\)</span>. Then the differential at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> is equal to the Jacobian of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>Recall that for any <span class="math notranslate nohighlight">\(A, B\)</span> for which <span class="math notranslate nohighlight">\(AB\)</span> is well-defined it holds that <span class="math notranslate nohighlight">\(\|A B \|_F \leq \|A\|_F \|B\|_F\)</span>. This applies in particular when <span class="math notranslate nohighlight">\(B\)</span> is a column vector, in which case <span class="math notranslate nohighlight">\(\|B\|_F\)</span> is its Euclidean norm.</p>
<p><em>Proof:</em> By the <em>Mean Value Theorem</em>, for each <span class="math notranslate nohighlight">\(i\)</span>, there is <span class="math notranslate nohighlight">\(\xi_{\mathbf{h},i} \in (0,1)\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
f_i(\mathbf{x}_0+\mathbf{h})
= f_i(\mathbf{x}_0) + \nabla f_i(\mathbf{x}_0 + \xi_{\mathbf{h},i} \mathbf{h})^T \mathbf{h}. 
\]</div>
<p>Define</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\tilde{J}(\mathbf{h})
= \begin{pmatrix}
\frac{\partial f_1 (\mathbf{x}_0 + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_1}
&amp;  \ldots &amp; \frac{\partial f_1 (\mathbf{x}_0 + \xi_{\mathbf{h},1} \mathbf{h})}{\partial x_d}\\
\vdots &amp; \ddots &amp; \vdots\\
\frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial x_1}
&amp;  \ldots &amp; \frac{\partial f_m (\mathbf{x}_0 + \xi_{\mathbf{h},m} \mathbf{h})}{\partial x_d}
\end{pmatrix}.
\end{split}\]</div>
<p>Hence we have</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x}_0+\mathbf{h})
- \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
= \tilde{J}(\mathbf{h}) \,\mathbf{h} - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}
= \left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}.
\]</div>
<p>Taking a limit as <span class="math notranslate nohighlight">\(\mathbf{h}\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span>, we get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\lim_{\mathbf{h} \to 0} \frac{\|\mathbf{f}(\mathbf{x}_0+\mathbf{h})
- \mathbf{f}(\mathbf{x}_0) - J_{\mathbf{f}}(\mathbf{x}_0)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2} 
&amp;= \lim_{\mathbf{h} \to 0} \frac{\|\left(\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\right)\,\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\
&amp;\leq \lim_{\mathbf{h} \to 0} \frac{\|\tilde{J}(\mathbf{h}) - J_{\mathbf{f}}(\mathbf{x}_0)\|_F \|\mathbf{h}\|_2}{\|\mathbf{h}\|_2}\\
&amp;= 0,
\end{align*}\]</div>
<p>by continuity of the partial derivatives. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>EXAMPLE:</strong> An example of a vector-valued function is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{g}(x_1,x_2)
=
\begin{pmatrix}
g_1(x_1,x_2)\\
g_2(x_1,x_2)\\
g_3(x_1,x_2)
\end{pmatrix}
=
\begin{pmatrix}
3 x_1^2\\
x_2\\
x_1 x_2
\end{pmatrix}.
\end{split}\]</div>
<p>Its Jacobian is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(x_1, x_2)
= \begin{pmatrix}
\frac{\partial g_1 (x_1, x_2)}{\partial x_1}
&amp;  \frac{\partial g_1 (x_1, x_2)}{\partial x_2}\\
\frac{\partial g_2 (x_1, x_2)}{\partial x_1}
&amp;  \frac{\partial g_2 (x_1, x_2)}{\partial x_2}\\
\frac{\partial g_3 (x_1, x_2)}{\partial x_1}
&amp;  \frac{\partial g_3 (x_1, x_2)}{\partial x_2}
\end{pmatrix}
= \begin{pmatrix}
6 x_1
&amp;  0\\
0
&amp;  1\\
x_2
&amp;  x_1
\end{pmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Gradient and Jacobian)</strong> For a continuously differentiable real-valued function <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span>, the Jacobian reduces to the row vector</p>
<div class="math notranslate nohighlight">
\[
J_{f}(\mathbf{x}_0)
= \left(\frac{\partial f (\mathbf{x}_0)}{\partial x_1},  
\ldots, 
\frac{\partial f (\mathbf{x}_0)}{\partial x_d}\right)^T
= \nabla f(\mathbf{x}_0)^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}_0)\)</span> is the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Hessian and Jacobian)</strong> For a twice continuously differentiable real-valued function <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span>, the Jacobian of its gradient is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\nabla f}(\mathbf{x}_0)
= \begin{pmatrix}
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_1^2} 
&amp; \cdots 
&amp; \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d \partial x_1}\\
\vdots &amp; \ddots &amp; \vdots\\
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_1 \partial x_d} 
&amp; \cdots 
&amp; \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d^2}
\end{pmatrix},
\end{split}\]</div>
<p>that is, the Hessian (tranposed, but that makes no difference; why?) of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Parametric Curve and Jacobian)</strong> Consider the parametric curve <span class="math notranslate nohighlight">\(\mathbf{g}(t) = (g_1(t), \ldots, g_d(t)) \in \mathbb{R}^d\)</span> for <span class="math notranslate nohighlight">\(t\)</span> in some closed interval of <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. Assume that <span class="math notranslate nohighlight">\(\mathbf{g}(t)\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(t\)</span>, that is, each of its component is.</p>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(t)
= \begin{pmatrix}
g_1'(t)\\
\vdots\\
g_m'(t)
\end{pmatrix}
= \mathbf{g}'(t).
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Affine Map)</strong> Let <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i,j} \in \mathbb{R}^{m \times d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} = (b_1,\ldots,b_m) \in \mathbb{R}^{m}\)</span>. Define the vector-valued function <span class="math notranslate nohighlight">\(\mathbf{f} = (f_1, \ldots, f_m)  : \mathbb{R}^d \to \mathbb{R}^m\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x})
= A \mathbf{x} + \mathbf{b}.
\]</div>
<p>This is an affine map. Note in particular that, in the case <span class="math notranslate nohighlight">\(\mathbf{b} = \mathbf{0}\)</span> of a linear map,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x} + \mathbf{y}) 
= A(\mathbf{x} + \mathbf{y}) 
= A\mathbf{x} + A\mathbf{y} 
= \mathbf{f}(\mathbf{x}) + \mathbf{f}(\mathbf{y}).
\]</div>
<p>Denote the rows of <span class="math notranslate nohighlight">\(A\)</span> by <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_1^T,\ldots,\boldsymbol{\alpha}_m^T\)</span>.</p>
<p>We compute the Jacobian of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. Note that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f_i (\mathbf{x})}{\partial x_j}
&amp;= \frac{\partial}{\partial x_j}[\boldsymbol{\alpha}_i^T \mathbf{x} + b_i]\\
&amp;= \frac{\partial}{\partial x_j}\left[\sum_{\ell=1}^m a_{i,\ell} x_{\ell} + b_i\right]\\
&amp;= a_{i,j}.
\end{align*}\]</div>
<p>So</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{f}}(\mathbf{x}) = A.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>The following important example is a less straightforward application of the Jacobian.</p>
<p>It will be useful to introduce the vectorization <span class="math notranslate nohighlight">\(\mathrm{vec}(A) \in \mathbb{R}^{nm}\)</span> of a matrix <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)</span> as the vector</p>
<div class="math notranslate nohighlight">
\[
\mathrm{vec}(A)
= (a_{1,1},\ldots,a_{n,1},a_{1,2},\ldots,a_{n,2},\ldots,a_{1,m},\ldots,a_{n,m}).
\]</div>
<p>That is, it is obtained by stacking the columns of <span class="math notranslate nohighlight">\(A\)</span> on top of each other.</p>
<p><strong>EXAMPLE:</strong> <strong>(Jacobian of a Linear Map with Respect to its Matrix)</strong> We take a different tack on the previous example. In data science applications, it will be useful to compute the Jacobian of a linear map <span class="math notranslate nohighlight">\(X \mathbf{z}\)</span> – with respect to the matrix <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times m}\)</span>. Specifically, for a fixed <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^{m}\)</span>, letting <span class="math notranslate nohighlight">\((\mathbf{x}^{(i)})^T\)</span> be the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(X\)</span> we define the function</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{f}(\mathbf{x})
= X \mathbf{z}
= \begin{pmatrix}
(\mathbf{x}^{(1)})^T\\
\vdots\\
(\mathbf{x}^{(n)})^T
\end{pmatrix}
\mathbf{z}
= \begin{pmatrix}
(\mathbf{x}^{(1)})^T \mathbf{z} \\
\vdots\\
(\mathbf{x}^{(n)})^T \mathbf{z}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})\)</span>.</p>
<p>To compute the Jacobian, let us look at its columns that correspond to the variables in <span class="math notranslate nohighlight">\(\mathbf{x}^{(k)}\)</span>, that is, columns <span class="math notranslate nohighlight">\(\alpha_k = (k-1) m + 1\)</span> to <span class="math notranslate nohighlight">\(\beta_k = k m\)</span>. Note that only the <span class="math notranslate nohighlight">\(k\)</span>-th component of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> depends on <span class="math notranslate nohighlight">\(\mathbf{x}^{(k)}\)</span>, so the rows <span class="math notranslate nohighlight">\(\neq k\)</span> of <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x})\)</span> are <span class="math notranslate nohighlight">\(0\)</span> for the corresponding columns.</p>
<p>Row <span class="math notranslate nohighlight">\(k\)</span> on the other hand is <span class="math notranslate nohighlight">\(\mathbf{z}^T\)</span> from our previous formula for the gradient of an affine map. Hence one way to write the columns <span class="math notranslate nohighlight">\(\alpha_k\)</span> to <span class="math notranslate nohighlight">\(\beta_k\)</span> of <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x})\)</span> is <span class="math notranslate nohighlight">\(\mathbf{e}_k \mathbf{z}^T\)</span>, where here <span class="math notranslate nohighlight">\(\mathbf{e}_k \in \mathbb{R}^{n}\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-th standard basis vector of <span class="math notranslate nohighlight">\(\mathbb{R}^{n}\)</span>.</p>
<p>So <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x})\)</span> can be written in block form as</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{f}}(\mathbf{x})
= \begin{pmatrix}
\mathbf{e}_1 \mathbf{z}^T
&amp; \cdots &amp; \mathbf{e}_{n}\mathbf{z}^T
\end{pmatrix}
= I_{n\times n} \otimes \mathbf{z}^T
=: \mathbb{B}_{n}[\mathbf{z}],
\]</div>
<p>where the last equality is a definition. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>We will need one more wrinkle.</p>
<p><strong>EXAMPLE:</strong> <strong>(Jacobian of a Linear Map with Respect to its Input and Matrix)</strong> Consider again the linear map <span class="math notranslate nohighlight">\(X \mathbf{z}\)</span> – this time as a function of <em>both</em> the matrix <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times m}\)</span> and the vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^{m}\)</span>. That is, letting again <span class="math notranslate nohighlight">\((\mathbf{x}^{(i)})^T\)</span> be the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(X\)</span>, we define the function</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{g}(\mathbf{z}, \mathbf{x})
= X \mathbf{z}
= \begin{pmatrix}
(\mathbf{x}^{(1)})^T\\
\vdots\\
(\mathbf{x}^{(n)})^T
\end{pmatrix}
\mathbf{z}
= \begin{pmatrix}
(\mathbf{x}^{(1)})^T \mathbf{z} \\
\vdots\\
(\mathbf{x}^{(n)})^T \mathbf{z}
\end{pmatrix}
\end{split}\]</div>
<p>where as before <span class="math notranslate nohighlight">\(\mathbf{x} = \mathrm{vec}(X^T) = (\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(n)})\)</span>.</p>
<p>To compute the Jacobian, we think of it as a block matrix and use the two previous examples. The columns of <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\)</span> corresponding to the variables in <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>, that is, columns <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(m\)</span>, are</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X
=
\begin{pmatrix}
(\mathbf{x}^{(1)})^T\\
\vdots\\
(\mathbf{x}^{(n)})^T
\end{pmatrix}
=: \mathbb{A}_{n}[\mathbf{x}].
\end{split}\]</div>
<p>The columns of <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\)</span> corresponding to the variables in <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, that is, columns <span class="math notranslate nohighlight">\(m + 1\)</span> to <span class="math notranslate nohighlight">\(m + nm\)</span>, are the matrix <span class="math notranslate nohighlight">\(\mathbb{B}_{n}[\mathbf{z}]\)</span>. Note that, in both <span class="math notranslate nohighlight">\(\mathbb{A}_{n}[\mathbf{x}]\)</span> and <span class="math notranslate nohighlight">\(\mathbb{B}_{n}[\mathbf{z}]\)</span>, the subscript <span class="math notranslate nohighlight">\(n\)</span> indicates the number of rows of the matrix. The number of columns is determined by <span class="math notranslate nohighlight">\(n\)</span> and the size of the input vector:</p>
<ul class="simple">
<li><p>the length of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> divided by <span class="math notranslate nohighlight">\(n\)</span> for <span class="math notranslate nohighlight">\(\mathbb{A}_{n}[\mathbf{x}]\)</span>;</p></li>
<li><p>the length of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> multiplied by <span class="math notranslate nohighlight">\(n\)</span> for <span class="math notranslate nohighlight">\(\mathbb{B}_{n}[\mathbf{z}]\)</span>.</p></li>
</ul>
<p>So <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})\)</span> can be written in block form as</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{f}}(\mathbf{z}, \mathbf{x})
= \begin{pmatrix}
\mathbb{A}_{n}[\mathbf{x}] &amp;
\mathbb{B}_{n}[\mathbf{z}]
\end{pmatrix}.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Elementwise Function)</strong> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span>, with <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}\)</span>, be a continuously differentiable real-valued function of a single variable. For <span class="math notranslate nohighlight">\(n \geq 2\)</span>, consider applying <span class="math notranslate nohighlight">\(f\)</span> to each entry of a vector <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>, that is, let <span class="math notranslate nohighlight">\(\mathbf{f} : D^n \to \mathbb{R}^n\)</span> with</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x})
= (f_1(\mathbf{x}), \ldots, f_n(\mathbf{x}))
= (f(x_1), \ldots, f(x_n)).
\]</div>
<p>The Jacobian of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> can be computed from <span class="math notranslate nohighlight">\(f'\)</span>, the derivative of the single-variable case. Indeed, letting <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1,\ldots,x_n)\)</span> be such that <span class="math notranslate nohighlight">\(x_i\)</span> is an interior point of <span class="math notranslate nohighlight">\(D\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f_j(\mathbf{x})}{\partial x_j} 
= f'(x_j),
\]</div>
<p>while for <span class="math notranslate nohighlight">\(\ell \neq j\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f_\ell(\mathbf{x})}{\partial x_j} 
=0,
\]</div>
<p>as <span class="math notranslate nohighlight">\(f_\ell(\mathbf{x})\)</span> does not in fact depend on <span class="math notranslate nohighlight">\(x_j\)</span>. In other words, the <span class="math notranslate nohighlight">\(j\)</span>-th column of the Jacobian is <span class="math notranslate nohighlight">\(f'(x_j) \,\mathbf{e}_j\)</span>, where again <span class="math notranslate nohighlight">\(\mathbf{e}_{j}\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th standard basis vector in <span class="math notranslate nohighlight">\(\mathbb{R}^{n}\)</span>.</p>
<p>So <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x})\)</span> is the diagonal matrix with diagonal entries <span class="math notranslate nohighlight">\(f'(x_j)\)</span>, <span class="math notranslate nohighlight">\(j=1, \ldots, n\)</span>, which we denote by</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{f}}(\mathbf{x})
= \mathrm{diag}(f'(x_1),\ldots,f'(x_n)).
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
&#13;

<h2><span class="section-number">8.2.3. </span>Generalization of the Chain Rule<a class="headerlink" href="#generalization-of-the-chain-rule" title="Link to this heading">#</a></h2>
<p>As we have seen, functions are often obtained from the composition of simpler ones. We will use the vector notation <span class="math notranslate nohighlight">\(\mathbf{h} = \mathbf{g} \circ \mathbf{f}\)</span> for the function <span class="math notranslate nohighlight">\(\mathbf{h}(\mathbf{x}) = \mathbf{g} (\mathbf{f} (\mathbf{x}))\)</span>.</p>
<p><strong>LEMMA</strong> <strong>(Composition of Continuous Functions)</strong> <span class="math notranslate nohighlight">\(\idx{composition of continuous functions lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{f} : D_1 \to \mathbb{R}^m\)</span>, where <span class="math notranslate nohighlight">\(D_1 \subseteq \mathbb{R}^d\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{g} : D_2 \to \mathbb{R}^p\)</span>, where <span class="math notranslate nohighlight">\(D_2 \subseteq \mathbb{R}^m\)</span>. Assume that <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> is continuous at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> and that <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is continuous <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}_0)\)</span>. Then <span class="math notranslate nohighlight">\(\mathbf{g} \circ \mathbf{f}\)</span> is continuous at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p>The <em>Chain Rule</em> gives a formula for the Jacobian of a composition.</p>
<p><strong>THEOREM</strong> <strong>(Chain Rule)</strong> <span class="math notranslate nohighlight">\(\idx{chain rule}\xdi\)</span> Let <span class="math notranslate nohighlight">\(\mathbf{f} : D_1 \to \mathbb{R}^m\)</span>, where <span class="math notranslate nohighlight">\(D_1 \subseteq \mathbb{R}^d\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{g} : D_2 \to \mathbb{R}^p\)</span>, where <span class="math notranslate nohighlight">\(D_2 \subseteq \mathbb{R}^m\)</span>. Assume that <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_1\)</span>, and that <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}_0)\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0)
= J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))
\,J_{\mathbf{f}}(\mathbf{x}_0)
\]</div>
<p>as a product of matrices. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>Intuitively, the Jacobian provides a linear approximation of the function in the neighborhood of a point. The composition of linear maps corresponds to the product of the associated matrices. Similarly, the Jacobian of a composition is the product of the Jacobians.</p>
<p><em>Proof:</em> To avoid confusion, we think of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> as being functions of variables with different names, specifically,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{f}(\mathbf{x})
= (f_1(x_1,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_d))
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\mathbf{g}(\mathbf{y})
= (g_1(y_1,\ldots,y_m),\ldots,g_p(y_1,\ldots,y_m)).
\]</div>
<p>We apply the <em>Chain Rule</em> for a real-valued function over a parametric vector curve. That is, we think of</p>
<div class="math notranslate nohighlight">
\[
h_i(\mathbf{x})
= g_i(\mathbf{f}(\mathbf{x}))
= g_i(f_1(x_1,\ldots,x_j,\ldots,x_d),\ldots,f_m(x_1,\ldots,x_j,\ldots,x_d))
\]</div>
<p>as <em>a function of <span class="math notranslate nohighlight">\(x_j\)</span> only</em> with all other <span class="math notranslate nohighlight">\(x_i\)</span>s fixed.</p>
<p>We get that</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial h_i(\mathbf{x}_0)}{\partial x_j}
= \sum_{k=1}^m \frac{\partial g_i(\mathbf{f}(\mathbf{x}_0))}
{\partial y_k} \frac{\partial f_k(\mathbf{x}_0)}{\partial x_j}
\]</div>
<p>where, as before, the notation <span class="math notranslate nohighlight">\(\frac{\partial g_i}
{\partial y_k}\)</span> indicates the partial derivative of <span class="math notranslate nohighlight">\(g_i\)</span> with respect to its <span class="math notranslate nohighlight">\(k\)</span>-th component. In matrix form, the claim follows. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>EXAMPLE:</strong> <strong>(Affine Map continued)</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^{m}\)</span>. Define again the vector-valued function <span class="math notranslate nohighlight">\(\mathbf{f}  : \mathbb{R}^d \to \mathbb{R}^m\)</span> as
<span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\)</span>.
In addition, for <span class="math notranslate nohighlight">\(C \in \mathbb{R}^{p \times m}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{d} \in \mathbb{R}^{p}\)</span>, define <span class="math notranslate nohighlight">\(\mathbf{g}  : \mathbb{R}^m \to \mathbb{R}^p\)</span> as
<span class="math notranslate nohighlight">\(\mathbf{g}(\mathbf{y}) = C \mathbf{y} + \mathbf{d}\)</span>.</p>
<p>Then</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x})
= J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}))
\,J_{\mathbf{f}}(\mathbf{x})
= C A,
\]</div>
<p>for all <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span>.</p>
<p>This is consistent with the observation that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{g} \circ \mathbf{f} (\mathbf{x})
= \mathbf{g} (\mathbf{f} (\mathbf{x}))
= C( A\mathbf{x} + \mathbf{b} ) + \mathbf{d}
= CA \mathbf{x} + (C\mathbf{b} + \mathbf{d}).
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> Suppose we want to compute the gradient of the function</p>
<div class="math notranslate nohighlight">
\[
f(x_1, x_2)
= 3 x_1^2 
+ x_2
+ \exp(x_1 x_2).
\]</div>
<p>We could apply the <em>Chain Rule</em> directly, but to illustrate the perspective that is coming up, we think of <span class="math notranslate nohighlight">\(f\)</span> as a composition of “simpler” vector-valued functions. Specifically, let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{g}(x_1,x_2)
=
\begin{pmatrix}
3 x_1^2\\
x_2\\
x_1 x_2
\end{pmatrix}
\qquad
h(y_1,y_2,y_3)
=
y_1 + y_2 + \exp(y_3).
\end{split}\]</div>
<p>Then <span class="math notranslate nohighlight">\(f(x_1, x_2) = h(\mathbf{g}(x_1, x_2)) = h \circ \mathbf{g}(x_1, x_2)\)</span>.</p>
<p>By the <em>Chain Rule</em>, we can compute the gradient of <span class="math notranslate nohighlight">\(f\)</span> by first computing the Jacobians of <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> and <span class="math notranslate nohighlight">\(h\)</span>. We have already computed the Jacobian of <span class="math notranslate nohighlight">\(\mathbf{g}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_{\mathbf{g}}(x_1, x_2)
= \begin{pmatrix}
6 x_1
&amp;  0\\
0
&amp;  1\\
x_2
&amp;  x_1
\end{pmatrix}.
\end{split}\]</div>
<p>The Jacobian of <span class="math notranslate nohighlight">\(h\)</span> is</p>
<div class="math notranslate nohighlight">
\[
J_h(y_1, y_2, y_3)
=
\begin{pmatrix}
\frac{\partial h(y_1, y_2, y_3)}{\partial y_1}
&amp;  \frac{\partial h(y_1, y_2, y_3)}{\partial y_2}
&amp; \frac{\partial h(y_1, y_2, y_3)}{\partial y_3}
\end{pmatrix}
=
\begin{pmatrix}
1
&amp; 1
&amp; \exp(y_3)
\end{pmatrix}.
\]</div>
<p>The <em>Chain Rule</em> stipulates that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla f(x_1, x_2)^T
&amp;= J_f(x_1, x_2)\\
&amp;= J_h(\mathbf{g}(x_1,x_2)) \, J_{\mathbf{g}}(x_1, x_2)\\
&amp;= \begin{pmatrix}
1
&amp; 1
&amp; \exp(g_3(x_1, x_2))
\end{pmatrix}
\begin{pmatrix}
6 x_1
&amp;  0\\
0
&amp;  1\\
x_2
&amp;  x_1
\end{pmatrix}\\
&amp;= \begin{pmatrix}
1
&amp; 1
&amp; \exp(x_1 x_2)
\end{pmatrix}
\begin{pmatrix}
6 x_1
&amp;  0\\
0
&amp;  1\\
x_2
&amp;  x_1
\end{pmatrix}\\
&amp;= \begin{pmatrix}
6 x_1 + x_2 \exp(x_1 x_2)
&amp; 1 + x_1 \exp(x_1 x_2)
\end{pmatrix}.
\end{align*}\]</div>
<p>You can check directly (i.e., without the composition) that this is indeed the correct gradient (transposed).</p>
<p>Alternatively, it is instructive to “expand” the <em>Chain Rule</em> as we did in its proof. Specifically,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f (x_1, x_2)}{\partial x_1} 
&amp;= \sum_{i=1}^3 \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1, x_2)}{\partial x_1}\\
&amp;= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1} \frac{\partial g_1 (x_1, x_2)}{\partial x_1}
+ \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_1}
+ \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_1}\\
&amp;= 1 \cdot 6x_1 + 1 \cdot 0 + \exp(g_3(x_1, x_2)) \cdot x_2\\
&amp;= 6 x_1 + x_2 \exp(x_1 x_2).
\end{align*}\]</div>
<p>Note that this corresponds to multiplying <span class="math notranslate nohighlight">\(J_h(\mathbf{g}(x_1,x_2))\)</span> by the first column of <span class="math notranslate nohighlight">\(J_{\mathbf{g}}(x_1, x_2)\)</span>.</p>
<p>Similarly</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial f (x_1, x_2)}{\partial x_2} 
&amp;= \sum_{i=1}^3 \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_i} \frac{\partial g_i (x_1, x_2)}{\partial x_2}\\
&amp;= \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_1} \frac{\partial g_1 (x_1, x_2)}{\partial x_2}
+ \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_2} \frac{\partial g_2 (x_1, x_2)}{\partial x_2}
+ \frac{\partial h(\mathbf{g}(x_1,x_2))}{\partial y_3} \frac{\partial g_3 (x_1, x_2)}{\partial x_2}\\
&amp;= 1 \cdot 0 + 1 \cdot 1 + \exp(g_3(x_1, x_2)) \cdot x_1\\
&amp;= 1 + x_1 \exp(x_1 x_2).
\end{align*}\]</div>
<p>This corresponds to multiplying <span class="math notranslate nohighlight">\(J_h(\mathbf{g}(x_1,x_2))\)</span> by the second column of <span class="math notranslate nohighlight">\(J_{\mathbf{g}}(x_1, x_2)\)</span>. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The Jacobian determinant has important applications in change of variables for multivariable integrals. Ask your favorite AI chatbot to explain this application and provide an example of using the Jacobian determinant in a change of variables for a double integral. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
&#13;

<h2><span class="section-number">8.2.4. </span>Brief introduction to automatic differentiation in PyTorch<a class="headerlink" href="#brief-introduction-to-automatic-differentiation-in-pytorch" title="Link to this heading">#</a></h2>
<p>We illustrate the use of <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic differentiation</a><span class="math notranslate nohighlight">\(\idx{automatic differentiation}\xdi\)</span> to compute gradients in PyTorch.</p>
<p>Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Automatic_differentiation">Wikipedia</a>:</p>
<blockquote>
<div><p>In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation or computational differentiation, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program. Automatic differentiation is distinct from symbolic differentiation and numerical differentiation (the method of finite differences). Symbolic differentiation can lead to inefficient code and faces the difficulty of converting a computer program into a single expression, while numerical differentiation can introduce round-off errors in the discretization process and cancellation.</p>
</div></blockquote>
<p><strong>Automatic differentiation in PyTorch</strong> We will use <a class="reference external" href="https://pytorch.org/tutorials/">PyTorch</a>. It uses <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html">tensors</a><span class="math notranslate nohighlight">\(\idx{tensor}\xdi\)</span>, which in many ways behave similarly to NumPy arrays. See <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">here</a> for a quick introduction. We first initialize the tensors. Here each tensor corresponds to a single real variable. With the option <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad"><code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code></a>, we indicate that these are variables with respect to which a gradient will be taken later. We initialize the tensors at the values where the derivatives will be computed. If derivatives need to be computed at different values, we need to repeat this process. The function <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html"><code class="docutils literal notranslate"><span class="pre">.backward()</span></code></a> computes the gradient using backpropagation, to which we will return later. The partial derivatives are accessed with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html"><code class="docutils literal notranslate"><span class="pre">.grad</span></code></a>.</p>
<p><strong>NUMERICAL CORNER:</strong> This is better understood through an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">x1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">x2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We define the function. Note that we use
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.exp.html"><code class="docutils literal notranslate"><span class="pre">torch.exp</span></code></a>, the PyTorch implementation of the (element-wise) exponential function. Moreover, as in NumPy, PyTorch allows the use of <code class="docutils literal notranslate"><span class="pre">**</span></code> for <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.pow.html">taking a power</a>. <a class="reference external" href="https://pytorch.org/docs/stable/name_inference.html">Here</a> is a list of operations on tensors in PyTorch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">f</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="p">(</span><span class="n">x1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">x2</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x1</span> <span class="o">*</span> <span class="n">x2</span><span class="p">)</span>

<span class="n">f</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x1</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># df/dx1</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x2</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># df/dx2</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor(20.7781)
tensor(8.3891)
</pre></div>
</div>
</div>
</div>
<p>The input parameters can also be vectors, which allows to consider functions of large numbers of variables. Here we use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"><code class="docutils literal notranslate"><span class="pre">torch.sum</span></code></a> for taking a sum of the arguments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">z</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># gradient is (2 z_1, 2 z_2, 2 z_3)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([2., 4., 6.])
</pre></div>
</div>
</div>
</div>
<p>Here is another typical example in a data science context.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># Random dataset (features)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>  <span class="c1"># Dataset (labels)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># Parameter assignment</span>

<span class="n">predict</span> <span class="o">=</span> <span class="n">X</span> <span class="o">@</span> <span class="n">theta</span>  <span class="c1"># Classifier with parameter vector theta</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">predict</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># Loss function</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>  <span class="c1"># Compute gradients</span>

<span class="nb">print</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span>  <span class="c1"># gradient of loss</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>tensor([[29.7629],
        [31.4817]])
</pre></div>
</div>
</div>
</div>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot to explain how to compute a second derivative using PyTorch (it’s bit tricky). Ask for code that you can apply to the previous examples. (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb">Open In Colab</a>) <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Implementing gradient descent in PyTorch</strong> Rather than explicitly specifying the gradient function, we could use PyTorch to compute it automatically. This is done next. Note that the descent update is done within <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html"><code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">torch.no_grad()</span></code></a>, which ensures that the update operation itself is not tracked for gradient computation. Here the input <code class="docutils literal notranslate"><span class="pre">x0</span></code> as well as the output <code class="docutils literal notranslate"><span class="pre">xk.numpy(force=True)</span></code> are NumPy arrays. The function <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html"><code class="docutils literal notranslate"><span class="pre">torch.Tensor.numpy()</span></code></a> converts a PyTorch tensor to a NumPy array (see the documentation for an explanation of the <code class="docutils literal notranslate"><span class="pre">force=True</span></code> option). Also, quoting ChatGPT:</p>
<blockquote>
<div><p>In the given code, <code class="docutils literal notranslate"><span class="pre">.item()</span></code> is used to extract the scalar value from a tensor. In PyTorch, when you perform operations on tensors, you get back tensors as results, even if the result is a single scalar value. <code class="docutils literal notranslate"><span class="pre">.item()</span></code> is used to extract this scalar value from the tensor.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">gd_with_ad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">)):</span>
    <span class="n">xk</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span>
        <span class="n">value</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>  
            <span class="n">xk</span> <span class="o">-=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">xk</span><span class="o">.</span><span class="n">grad</span>

        <span class="n">xk</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">xk</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">f</span><span class="p">(</span><span class="n">xk</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We revisit a previous example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span>

<span class="nb">print</span><span class="p">(</span><span class="n">gd_with_ad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e4</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(array(0.03277362, dtype=float32), 3.5202472645323724e-05)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">gd_with_ad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>(array(-4.9335055, dtype=float32), -120.07894897460938)
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> The section briefly mentions that automatic differentiation is distinct from symbolic differentiation and numerical differentiation. Ask your favorite AI chatbot to explain in more detail the differences between these three methods of computing derivatives. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> and <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{p \times q}\)</span>. What are the dimensions of the Kronecker product <span class="math notranslate nohighlight">\(A \otimes B\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(n \times m\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(p \times q\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(np \times mq\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(nq \times mp\)</span></p>
<p><strong>2</strong> If <span class="math notranslate nohighlight">\(f: \mathbb{R}^d \rightarrow \mathbb{R}^m\)</span> is a continuously differentiable function, what is its Jacobian <span class="math notranslate nohighlight">\(J_f(x_0)\)</span> at an interior point <span class="math notranslate nohighlight">\(x_0\)</span> of its domain?</p>
<p>a) A scalar representing the rate of change of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>b) A vector in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> representing the direction of steepest ascent of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>c) An <span class="math notranslate nohighlight">\(m \times d\)</span> matrix of partial derivatives of the component functions of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>d) The Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p><strong>3</strong> In the context of the Chain Rule, if <span class="math notranslate nohighlight">\(f: \mathbb{R}^2 \to \mathbb{R}^3\)</span> and <span class="math notranslate nohighlight">\(g: \mathbb{R}^3 \to \mathbb{R}\)</span>, what is the dimension of the Jacobian matrix <span class="math notranslate nohighlight">\(J_{g \circ f}(x)\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(3 \times 2\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(1 \times 3\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(2 \times 3\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(1 \times 2\)</span></p>
<p><strong>4</strong> Let <span class="math notranslate nohighlight">\(\mathbf{f} : D_1 \to \mathbb{R}^m\)</span>, where <span class="math notranslate nohighlight">\(D_1 \subseteq \mathbb{R}^d\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{g} : D_2 \to \mathbb{R}^p\)</span>, where <span class="math notranslate nohighlight">\(D_2 \subseteq \mathbb{R}^m\)</span>. Assume that <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_1\)</span>, and that <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}_0)\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_2\)</span>. Which of the following is correct according to the Chain Rule?</p>
<p>a) <span class="math notranslate nohighlight">\(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{x}_0) \, J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0))\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0)) \, J_{\mathbf{f}}(\mathbf{x}_0)\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0)) \, J_{\mathbf{g}}(\mathbf{x}_0)\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{x}_0) \, J_{\mathbf{f}}(\mathbf{g}(\mathbf{x}_0))\)</span></p>
<p><strong>5</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^{m}\)</span>. Define the vector-valued function <span class="math notranslate nohighlight">\(\mathbf{f}  : \mathbb{R}^d \to \mathbb{R}^m\)</span> as <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\)</span>. What is the Jacobian of <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x}_0) = A^T\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x}_0) = A \mathbf{x}_0 + \mathbf{b}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x}_0) = A\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x}_0) = \mathbf{b}\)</span></p>
<p>Answer for 1: c. Justification: The text defines the Kronecker product as a matrix in block form with dimensions <span class="math notranslate nohighlight">\(np \times mq\)</span>.</p>
<p>Answer for 2: c. Justification: The text defines the Jacobian of a vector-valued function as a matrix of partial derivatives.</p>
<p>Answer for 3: d. Justification: The composition <span class="math notranslate nohighlight">\(g \circ f\)</span> maps <span class="math notranslate nohighlight">\(\mathbb{R}^2 \to \mathbb{R}\)</span>, hence the Jacobian matrix <span class="math notranslate nohighlight">\(J_{g \circ f}(x)\)</span> is <span class="math notranslate nohighlight">\(1 \times 2\)</span>.</p>
<p>Answer for 4: b. Justification: The text states: “The Chain Rule gives a formula for the Jacobian of a composition. […] Assume that <span class="math notranslate nohighlight">\(\mathbf{f}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_1\)</span>, and that <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}_0)\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
J_{\mathbf{g} \circ \mathbf{f}}(\mathbf{x}_0) = J_{\mathbf{g}}(\mathbf{f}(\mathbf{x}_0)) \,J_{\mathbf{f}}(\mathbf{x}_0)
\]</div>
<p>as a product of matrices.”</p>
<p>Answer for 5: c. Justification: The text states: “Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{m \times d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} = (b_1,\ldots,b_m) \in \mathbb{R}^{m}\)</span>. Define the vector-valued function <span class="math notranslate nohighlight">\(\mathbf{f} = (f_1, \ldots, f_m)  : \mathbb{R}^d \to \mathbb{R}^m\)</span> as <span class="math notranslate nohighlight">\(\mathbf{f}(\mathbf{x}) = A \mathbf{x} + \mathbf{b}\)</span>. […] So <span class="math notranslate nohighlight">\(J_{\mathbf{f}}(\mathbf{x}) = A.\)</span>”</p>
    
</body>
</html>