<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Cache Lines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Cache Lines</h1>
<blockquote>原文：<a href="https://en.algorithmica.org/hpc/cpu-cache/cache-lines/">https://en.algorithmica.org/hpc/cpu-cache/cache-lines/</a></blockquote><div id="search"><input id="search-bar" type="search" placeholder="Search this book…" oninput="search()"/><div id="search-count"/><div id="search-results"/></div><header><div class="info"/></header><article><p>The basic units of data transfer in the CPU cache system are not individual bits and bytes, but <em>cache lines</em>. On most architectures, the size of a cache line is 64 bytes, meaning that all memory is divided in blocks of 64 bytes, and whenever you request (read or write) a single byte, you are also fetching all its 63 cache line neighbors whether your want them or not.</p><p>To demonstrate this, we add a “step” parameter to our <a href="../bandwidth">incrementing loop</a>. Now we only touch every $D$-th element:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="n">D</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">++</span><span class="p">;</span>
</span></span></code></pre></div><p>If we run it with $D=1$ and $D=16$, we can observe something interesting:</p><p><figure><img src="../Images/3142c6c323ee109d005888386857b077.png" data-original-src="https://en.algorithmica.org/hpc/cpu-cache/img/strided.svg"/><figcaption>Performance is normalized by the total time to run benchmark, not the total number of elements incremented</figcaption></figure></p><p>As the problem size grows, the graphs of the two loops meet, despite one doing 16 times less work than the other. This is because, in terms of cache lines, we are fetching the exact same memory in both loops, and the fact that the strided loop only needs one-sixteenth of it is irrelevant.</p><p>When the array fits into the L1 cache, the strided version completes faster — although not 16 but just two times as fast. This is because it only needs to do half the work: it only executes a single <code>inc DWORD PTR [rdx]</code> instruction for every 16 elements, while the original loop needed two 8-element <a href="/hpc/simd">vector instructions</a> to process the same 16 elements. Both computations are bottlenecked by writing the result back: Zen 2 can only write one word per cycle — regardless of whether it is composed of one integer or eight.</p><p>When we change the step parameter to 8, the graphs equalize, as we now also need two increments and two write-backs per every 16 elements:</p><p><figure><img src="../Images/254c995eba2d87ec3ed1d324103d9a0b.png" data-original-src="https://en.algorithmica.org/hpc/cpu-cache/img/strided2.svg"/><figcaption/></figure></p><p>We can use this effect to minimize cache sharing in our <a href="../latency">latency benchmark</a> to measure it more precisely. We need to <em>pad</em> the indices of a permutation so that each of them lies in its own cache line:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="k">struct</span> <span class="nc">padded_int</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">val</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">padding</span><span class="p">[</span><span class="mi">15</span><span class="p">];</span>
</span></span><span class="line"><span class="cl"><span class="p">};</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">padded_int</span> <span class="n">q</span><span class="p">[</span><span class="n">N</span> <span class="o">/</span> <span class="mi">16</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">// constructing a cycle from a random permutation
</span></span></span><span class="line"><span class="cl"><span class="c1">// ...
</span></span></span><span class="line"><span class="cl"><span class="c1"/>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">N</span> <span class="o">/</span> <span class="mi">16</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">k</span><span class="p">].</span><span class="n">val</span><span class="p">;</span>
</span></span></code></pre></div><p>Now, each index is much more likely to be kicked out of the cache by the time we loop around and request it again:</p><p><figure><img src="../Images/0862c8d945214b8e8534c114937b9fa4.png" data-original-src="https://en.algorithmica.org/hpc/cpu-cache/img/permutation-padded.svg"/><figcaption/></figure></p><p>The important practical lesson when designing and analyzing memory-bound algorithms is to count the number of cache lines accessed and not just the total count of memory reads and writes.</p></article><div class="nextprev"><div class="left"><a href="https://en.algorithmica.org/hpc/cpu-cache/latency/" id="prev-article">← Memory Latency</a></div><div class="right"><a href="https://en.algorithmica.org/hpc/cpu-cache/sharing/" id="next-article">Memory Sharing →</a></div></div>    
</body>
</html>