- en: 7.8\. Online supplementary materials#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap07_rwmc/supp/roch-mmids-rwmc-supp.html](https://mmids-textbook.github.io/chap07_rwmc/supp/roch-mmids-rwmc-supp.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 7.8.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_rwmc_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_rwmc_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_rwmc_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 7.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-rwmc-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-rwmc-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.1** We need to check that all entries of \(P\) are non-negative and
    that all rows sum to one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-negativity: All entries of \(P\) are clearly non-negative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Row sums: Row 1: \(0.2 + 0.5 + 0.3 = 1\) Row 2: \(0.4 + 0.1 + 0.5 = 1\) Row
    3: \(0.6 + 0.3 + 0.1 = 1\)'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, \(P\) is a stochastic matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.3**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[X_2 = 2] &= (\boldsymbol{\mu} P^2)_2 \\ &= (0.2,
    0.3, 0.5)^T \begin{pmatrix} 0.44 & 0.31 & 0.25 \\ 0.44 & 0.37 & 0.19 \\ 0.36 &
    0.33 & 0.31 \end{pmatrix}_2 \\ &= 0.2 \cdot 0.31 + 0.3 \cdot 0.37 + 0.5 \cdot
    0.33 \\ &= 0.338. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.5**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The transition graph has a directed edge from state \(i\) to state \(j\) if
    and only if \(p_{i,j} > 0\), as stated in the definition.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.7**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[X_2 = 2 | X_0 = 3] &= (P^2)_{3,2} \\ &= \begin{pmatrix}
    0.1 & 0.4 & 0.5 \\ 0.2 & 0.6 & 0.2 \\ 0.3 & 0.3 & 0.4 \end{pmatrix}^2_{3,2} \\
    &= \begin{pmatrix} 0.29 & 0.38 & 0.33 \\ 0.26 & 0.44 & 0.30 \\ 0.27 & 0.36 & 0.37
    \end{pmatrix}_{3,2} \\ &= 0.36. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.9** The probability is given by the entry \((P^2)_{0,1}\), where \(P^2\)
    is the matrix product \(P \times P\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P^2 = \begin{pmatrix} 1/3 & 2/3 \\ 1/2 & 1/2 \end{pmatrix} \begin{pmatrix}
    1/3 & 2/3 \\ 1/2 & 1/2 \end{pmatrix} = \begin{pmatrix} 7/18 & 11/18 \\ 5/12 &
    7/12 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the probability is \(\boxed{11/18}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.11** The marginal distribution at time 2 is given by \(\boldsymbol{\mu}
    P^2\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P^2 = \begin{pmatrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/3 & 1/3
    & 1/3 \end{pmatrix} \begin{pmatrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/3 & 1/3 &
    1/3 \end{pmatrix} = \begin{pmatrix} 5/12 & 1/6 & 5/12 \\ 0 & 1 & 0 \\ 4/9 & 4/9
    & 1/9 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mu P^2 = (1/4, 1/2, 1/4)^T \begin{pmatrix} 5/12 & 1/6 & 5/12
    \\ 0 & 1 & 0 \\ 4/9 & 4/9 & 1/9 \end{pmatrix} = \boxed{(13/36, 19/36, 4/36)^T}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.13** The distribution at time 1 is given by \(\boldsymbol{\mu} P\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \boldsymbol{\mu} P = (1/3, 2/3)^T \begin{pmatrix}1/2 & 1/2 \\
    1 & 0\end{pmatrix} = \boxed{(2/3, 1/3)^T} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.15** The chain alternates deterministically between states 1 and 2\.
    Therefore, starting in state 1, it will return to state 1 after exactly \(\boxed{2}\)
    steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.1** Yes, the matrix is irreducible. The corresponding transition graph
    is a cycle, which is strongly connected.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.3** We need to check if \(\boldsymbol{\pi} P = \boldsymbol{\pi}\). Indeed,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} (0.6, 0.4)^T \begin{pmatrix} 0.4 & 0.6 \\ 0.7 & 0.3 \end{pmatrix}
    = (0.6 \cdot 0.4 + 0.4 \cdot 0.7, 0.6 \cdot 0.6 + 0.4 \cdot 0.3)^T = (0.52, 0.48)^T
    \neq (0.6, 0.4)^T, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so \(\pi\) is not a stationary distribution of the Markov chain.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.5** Let \(\boldsymbol{\pi} = (\pi_1, \pi_2)^T\) be a stationary distribution.
    Then, we need to solve the system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 \cdot 0.5 + \pi_2 \cdot 0.5 &= \pi_1 \\ \pi_1 + \pi_2
    &= 1 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The first equation simplifies to \(\pi_1 = \pi_2\), and substituting this into
    the second equation yields \(2\pi_1 = 1\), or \(\pi_1 = \pi_2 = 0.5\). Therefore,
    \(\boldsymbol{\pi} = (0.5, 0.5)^T\) is a stationary distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.7** To verify that \(\boldsymbol{\pi}\) is a stationary distribution,
    we need to check if \(\boldsymbol{\pi} P = \boldsymbol{\pi}\). Let’s perform the
    matrix multiplication step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\boldsymbol{\pi} P = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})^T \begin{pmatrix}
    0.4 & 0.3 & 0.3 \\ 0.2 & 0.5 & 0.3 \\ 0.4 & 0.2 & 0.4 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= (\frac{1}{3} \cdot 0.4 + \frac{1}{3} \cdot 0.2 + \frac{1}{3} \cdot 0.4,
    \frac{1}{3} \cdot 0.3 + \frac{1}{3} \cdot 0.5 + \frac{1}{3} \cdot 0.2, \frac{1}{3}
    \cdot 0.3 + \frac{1}{3} \cdot 0.3 + \frac{1}{3} \cdot 0.4)^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= (\frac{0.4 + 0.2 + 0.4}{3}, \frac{0.3 + 0.5 + 0.2}{3}, \frac{0.3 + 0.3 +
    0.4}{3})^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= \boldsymbol{\pi}\)
  prefs: []
  type: TYPE_NORMAL
- en: The result is equal to \(\pi = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\). Therefore,
    the uniform distribution \(\pi\) is indeed a stationary distribution for the matrix
    \(P\). Note that the transition matrix \(P\) is doubly stochastic because each
    row and each column sums to 1\. This property ensures that the uniform distribution
    is always a stationary distribution for doubly stochastic matrices, as mentioned
    in the text.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.9** Let \(\mathbf{1} = (1, 1, \ldots, 1)\) be the column vector of all
    ones. For any stochastic matrix \(P\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P \mathbf{1} = \begin{pmatrix} p_{1,1} & \cdots & p_{1,n} \\
    \vdots & \ddots & \vdots \\ p_{n,1} & \cdots & p_{n,n} \end{pmatrix} \begin{pmatrix}
    1 \\ \vdots \\ 1 \end{pmatrix} = \begin{pmatrix} \sum_{j=1}^n p_{1,j} \\ \vdots
    \\ \sum_{j=1}^n p_{n,j} \end{pmatrix} = \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}
    = \mathbf{1}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: because each row of a stochastic matrix sums to 1\. Therefore, \(\mathbf{1}\)
    is a right eigenvector of \(P\) with eigenvalue 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.11** Let \(\boldsymbol{\pi} = (\pi_1, \pi_2, \pi_3)^T\) be the stationary
    distribution. We need to solve the system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Write out the system of equations based on \(\boldsymbol{\pi} P = \boldsymbol{\pi}\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 \cdot 0.7 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.6 &= \pi_1
    \\ \pi_1 \cdot 0.2 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.1 &= \pi_2 \\ \pi_1 \cdot
    0.1 + \pi_2 \cdot 0.2 + \pi_3 \cdot 0.3 &= \pi_3 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Rearrange the equations to have zero on the right-hand side.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 \cdot 0.7 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.6 - \pi_1
    &= 0 \\ \pi_1 \cdot 0.2 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.1 - \pi_2 &= 0 \\ \pi_1
    \cdot 0.1 + \pi_2 \cdot 0.2 + \pi_3 \cdot 0.3 - \pi_3 &= 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Simplify the equations.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.3\pi_1 + 0.4\pi_2 + 0.6\pi_3 &= 0 \\ 0.2\pi_1 - 0.6\pi_2
    + 0.1\pi_3 &= 0 \\ 0.1\pi_1 + 0.2\pi_2 - 0.7\pi_3 &= 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Use the condition \(\pi_1 + \pi_2 + \pi_3 = 1\) to eliminate one variable,
    say \(\pi_3\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_3 = 1 - \pi_1 - \pi_2 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Substitute the expression for \(\pi_3\) into the simplified equations
    from Step 3.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.3\pi_1 + 0.4\pi_2 + 0.6(1 - \pi_1 - \pi_2) &= 0 \\ 0.2\pi_1
    - 0.6\pi_2 + 0.1(1 - \pi_1 - \pi_2) &= 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Simplify the equations further.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.9\pi_1 - 0.2\pi_2 &= -0.6 \\ 0.1\pi_1 - 0.7\pi_2 &= -0.1
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 7: Solve the linear system of two equations with two unknowns using any
    suitable method (e.g., substitution, elimination, or matrix inversion).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the substitution method: From the second equation, express \(\pi_1\)
    in terms of \(\pi_2\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_1 = 7\pi_2 - 1 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Substitute this into the first equation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.9(7\pi_2 - 1) - 0.2\pi_2 &= -0.6 \\ -6.3\pi_2 + 0.9 - 0.2\pi_2
    &= -0.6 \\ -6.5\pi_2 &= -1.5 \\ \pi_2 &= \frac{3}{13} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, substitute \(\pi_2 = \frac{3}{13}\) back into the expression for \(\pi_1\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 &= 7 \cdot \frac{3}{13} - 1 \\ &= \frac{21}{13} - \frac{13}{13}
    \\ &= \frac{8}{13} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, use \(\pi_1 + \pi_2 + \pi_3 = 1\) to find \(\pi_3\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_3 &= 1 - \pi_1 - \pi_2 \\ &= 1 - \frac{8}{13} - \frac{3}{13}
    \\ &= \frac{2}{13} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the stationary distribution is \((\frac{8}{13}, \frac{3}{13}, \frac{2}{13})^T\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.4.1** Given the transition matrix of a Markov chain:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0.2 & 0.8 \\ 0.6 & 0.4 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: determine if the chain is lazy.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.4.3** Given a Markov chain with transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0.4 & 0.6 \\ 0.7 & 0.3 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and initial distribution \(\boldsymbol{\mu} = (0.2, 0.8)^T\), compute \(\lim_{t
    \to \infty} \boldsymbol{\mu} P^t\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.1** The degree matrix \(D\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{pmatrix} 2 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0
    & 3 & 0 \\ 0 & 0 & 0 & 2 \\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The degrees are computed by summing the entries in each row of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.3** A stochastic matrix has rows that sum to 1\. The rows of \(P\) sum
    to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{2} + \frac{1}{2} = 1, \quad \frac{1}{3} + \frac{1}{3} + \frac{1}{3}
    = 1, \quad \frac{1}{3} + \frac{1}{3} + \frac{1}{3} = 1, \quad \frac{1}{2} + \frac{1}{2}
    = 1\. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, \(P\) is stochastic.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.5**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = D^{-1}A = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0
    \\ 0 & 0 & 1/2 & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 0 & 1 & 0 &
    0 \\ 0 & 0 & 1 & 0 \\ 1 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \end{pmatrix} = \begin{pmatrix}
    0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 1/2 & 0 & 0 & 1/2 \\ 0 & 0 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.7** First, compute the transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = D^{-1}A = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1/2 & 0 &
    0 \\ 0 & 0 & 1/2 & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 0 & 1 & 0
    & 0 \\ 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 0 & 0 & 1 & 0 \end{pmatrix} = \begin{pmatrix}
    0 & 1 & 0 & 0 \\ 1/2 & 0 & 1/2 & 0 \\ 0 & 1/2 & 0 & 1/2 \\ 0 & 0 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, compute the modified transition matrix with the damping factor:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \alpha P + (1-\alpha)\frac{1}{4}\mathbf{1}\mathbf{1}^T =
    0.8P + 0.2\begin{pmatrix} 1/4 & 1/4 & 1/4 & 1/4 \\ 1/4 & 1/4 & 1/4 & 1/4 \\ 1/4
    & 1/4 & 1/4 & 1/4 \\ 1/4 & 1/4 & 1/4 & 1/4 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.9** First, compute the transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 \\
    1/2 & 0 & 0 & 1/2 & 0 \\ 1/2 & 0 & 0 & 0 & 1/2 \\ 0 & 0 & 0 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, compute the modified transition matrix with the damping factor:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = 0.9P + 0.1\begin{pmatrix} 1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\
    1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\ 1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\ 1/5 & 1/5 & 1/5
    & 1/5 & 1/5 \\ 1/5 & 1/5 & 1/5 & 1/5 & 1/5 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.11** The new adjacency matrix \(A''\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A' = \begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 1 & 0
    & 0 & 1 \\ 0 & 0 & 0 & 1 \\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: A self-loop is added to vertex 4.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.13** The modified transition matrix \(Q\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q = \alpha P + (1 - \alpha) \frac{1}{n} \mathbf{1}\mathbf{1}^T = 0.85 P +
    0.15 \frac{1}{4} \mathbf{1}\mathbf{1}^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the values from \(P\) and \(\mathbf{1}\mathbf{1}^T\), we get:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \begin{pmatrix} 0.0375 & 0.8875 & 0.0375 & 0.0375 \\ 0.0375
    & 0.0375 & 0.8875 & 0.0375 \\ 0.4625 & 0.0375 & 0.0375 & 0.4625 \\ 0.0375 & 0.0375
    & 0.0375 & 0.8875 \\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.1** The acceptance probability is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min\left\{1, \frac{\pi_1}{\pi_2}\frac{Q(1, 2)}{Q(2, 1)}\right\} = \min\left\{1,
    \frac{0.1}{0.2}\frac{0.5}{0.5}\right\} = \frac{1}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.3** Since the proposal chain is symmetric, the acceptance probability
    simplifies to $\( \min\left\{1, \frac{\pi_2}{\pi_1}\right\} = \min\left\{1, \frac{0.2}{0.1}\right\}
    = 1. \)$'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.5** The acceptance probability is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min \left\{ 1, \frac{\pi(y)}{\pi(x)} \frac{Q(y, x)}{Q(x, y)} \right\} =
    \min \left\{ 1, \frac{e^{-9/2}}{e^{-2}} \right\} = e^{-5/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.7** The conditional probability is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_1^v(1|\mathbf{v}_{-1}, \mathbf{h}) = \sigma\left(\sum_{j=1}^2 w_{1,j}h_j
    + b_1\right) = \sigma(1\cdot 1 + (-1)\cdot 0 + 1) = \sigma(2) \approx 0.881. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.9** The energy is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathcal{E}(\mathbf{v}, \mathbf{h}) = -\mathbf{v}^T W \mathbf{h}
    = -\begin{pmatrix} 1 & 0 \end{pmatrix} \begin{pmatrix} 1 & -2 \\ 3 & 0 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = 1. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.11** The conditional mean for the hidden units is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[h_j | \mathbf{v}] = \sigma \left( \sum_{i} W_{ij} v_i + c_j \right).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(h_1\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[h_1 | \mathbf{v}] = \sigma (0.5 \cdot 1 + 0.3 \cdot 0 - 0.6 \cdot
    1 + 0.1) = \sigma (0.5 - 0.6 + 0.1) = \sigma (0) = 0.5. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(h_2\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[h_2 | \mathbf{v}] = \sigma (-0.2 \cdot 1 + 0.8 \cdot 0 + 0.1 \cdot
    1 - 0.3) = \sigma (-0.2 + 0.1 - 0.3) = \sigma (-0.4) = \frac{1}{1 + e^{0.4}} \approx
    0.4013. \]
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define a discrete-time Markov chain and its state space, initial distribution,
    and transition probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify Markov chains in real-world scenarios, such as weather patterns or
    random walks on graphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct the transition matrix for a time-homogeneous Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct the transition graph of a Markov chain from its transition matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the transition matrix of a Markov chain is a stochastic matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Markov property to calculate the probability of specific sequences
    of events in a Markov chain using the distribution of a sample path.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the marginal distribution of a Markov chain at a specific time using
    matrix powers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use simulation to generate sample paths of a Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a stationary distribution of a finite-state, discrete-time, time-homogeneous
    Markov chain and express the defining condition in matrix form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the relationship between stationary distributions and left eigenvectors
    of the transition matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine whether a given probability distribution is a stationary distribution
    for a given Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify irreducible Markov chains and explain their significance in the context
    of stationary distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the main theorem on the existence and uniqueness of a stationary distribution
    for an irreducible Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the stationary distribution of a Markov chain numerically using either
    eigenvalue methods or by solving a linear system of equations, including the “Replace
    an Equation” method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the concepts of aperiodicity and weak laziness for finite-space discrete-time
    Markov chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Convergence to Equilibrium Theorem and the Ergodic Theorem for irreducible
    Markov chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the Ergodic Theorem by simulating a Markov chain and comparing the frequency
    of visits to each state with the stationary distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of coupling and its role in proving the Convergence to Equilibrium
    Theorem for irreducible, weakly lazy Markov chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define random walks on directed and undirected graphs, and express their transition
    matrices in terms of the adjacency matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the stationary distribution of a random walk on an undirected graph
    and explain its relation to degree centrality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of reversibility and its connection to the stationary distribution
    of a random walk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the PageRank algorithm and its interpretation as a modified random
    walk on a directed graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the PageRank vector by finding the stationary distribution of the modified
    random walk using power iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the PageRank algorithm to real-world datasets to identify central nodes
    in a network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of Personalized PageRank and how it differs from the standard
    PageRank algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify the PageRank algorithm to implement Personalized PageRank and interpret
    the results based on the chosen teleportation distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the concept of Markov Chain Monte Carlo (MCMC) and its application
    in sampling from complex probability distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the Metropolis-Hastings algorithm, including the proposal distribution
    and acceptance-rejection steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate acceptance probabilities in the Metropolis-Hastings algorithm for
    a given target and proposal distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the correctness of the Metropolis-Hastings algorithm by showing that the
    resulting Markov chain is irreducible and reversible with respect to the target
    distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the Gibbs sampling algorithm for a given probabilistic model, such
    as a Restricted Boltzmann Machine (RBM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the conditional independence properties of RBMs and their role in Gibbs
    sampling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.2\. Additional sections[#](#additional-sections "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.8.2.1\. Random walk on a weighted graph[#](#random-walk-on-a-weighted-graph
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous definitions extend naturally to the weighted case. Again we allow
    loops, i.e., self-weights \(w_{i,i} > 0\). For a weighted graph \(G\), recall
    that the degree of a vertex is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \delta(i) = \sum_{j} w_{i,j}, \]
  prefs: []
  type: TYPE_NORMAL
- en: which includes the self-weight \(w_{i,i}\), and where we use the convention
    that \(w_{i,j} = 0\) if \(\{i,j\} \notin E\). Recall also that \(w_{i,j} = w_{j,i}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Random Walk on a Weighted Graph)** Let \(G = (V,E,w)\) be
    a weighted graph with positive edge weights. Assume all vertices have a positive
    degree. A random walk on \(G\) is a time-homogeneous Markov chain \((X_t)_{t \geq
    0}\) with state space \(\mathcal{S} = V\) and transition probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{w_{i,j}}{\sum_{k} w_{i,k}},
    \qquad \forall i,j \in V. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Once again, it is easily seen that the transition matrix of random walk on \(G\)
    satisfying the conditions of the definition above is \( P = D^{-1} A, \) where
    \(D = \mathrm{diag}(A \mathbf{1})\) is the degree matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(A Weighted Graph)** Here is another example. Consider the following
    adjacency matrix on \(5\) vertices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: It is indeed a symmetric matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We define a graph from its adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To draw it, we first define edge labels by creating a dictionary that assigns
    to each edge (as a tuple) its weight. Here `G.edges.data('weight')` (see [`G.edges`](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.edges.html))
    iterates through the edges `(u,v)` and includes their weight as the third entry
    of the tuple `(u,v,w)`. Then we use the function [`networkx.draw_networkx_edge_labels()`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx_edge_labels.html#networkx.drawing.nx_pylab.draw_networkx_edge_labels)
    to add the weights as edge labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/696132b74026ec3eb91e20e630468fd57042dca1726e5be92d5b48b5435bc3dd.png](../Images/bcf13dbbd96da2dcf4b07ed3618e6860.png)'
  prefs: []
  type: TYPE_IMG
- en: The transition matrix of the random walk on this graph can be computed using
    the lemma above. We first compute the degree matrix, then apply the formula.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This is indeed a stochastic matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Irreducibility in Undirected Case)** Let \(G = (V,E,w)\) be a
    graph with positive edge weights. Assume all vertices have a positive degree.
    Random walk on \(G\) is irreducible if and only if \(G\) is connected. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Stationary Distribution on a Graph)** Let \(G = (V,E,w)\) be
    a graph with positive edge weights. Assume further that \(G\) is connected. Then
    the unique stationary distribution of random walk on \(G\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in
    V. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(A Weighted Graph, continued)** Going back to our weighted graph
    example, we use the previous theorem to compute the stationary distribution. Note
    that the graph is indeed connected so the stationary distribution is unique. We
    have already computed the degrees.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We compute \(\sum_{i \in V} \delta(i)\) next.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the stationary distribution is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We check stationarity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: A random walk on a weighted undirected graph is reversible. Vice versa, it turns
    out that any reversible chain can be seen as a random walk on an appropriately
    defined weighted undirected graph. See the exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.2.2\. Spectral techniques for random walks on graphs[#](#spectral-techniques-for-random-walks-on-graphs
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we use techniques from spectral graph theory to analyze random
    walks on graphs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Applying the spectral theorem via the normalized Laplacian** We have seen
    how to compute the unique stationary distribution \(\bpi\) of random walk on a
    connected weighted (undirected) graph. Recall that \(\bpi\) is a (left) eigenvector
    of \(P\) with eigenvalue \(1\) (i.e., \(\bpi P = \bpi\)). In general, however,
    the matrix \(P\) is *not* symmetric in this case (see the previous example) -
    even though the adjacency matrix is. So we cannot apply the spectral theorem to
    get the rest of the eigenvectors - if they even exist. But, remarkably, a symmetric
    matrix with the a closely related spectral decomposition is hiding in the background.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the normalized Laplacian of a weighted graph \(G = (V,E,w)\) with
    adjacency matrix \(A\) and degree matrix \(D\) is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = I - D^{-1/2} A D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in the weighted case, the degree is defined as \(\delta(i) = \sum_{j:\{i,j\}
    \in E} w_{i,j}\). Because it is symmetric and positive semi-definite, we can write
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = \sum_{i=1}^n \eta_i \mathbf{z}_i \mathbf{z}_i^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\mathbf{z}_i\)s are orthonormal eigenvectors of \(\mathcal{L}\)
    and the eigenvalues satisfy \(0 \leq \eta_1 \leq \eta_2 \leq \cdots \leq \eta_n\).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, \(D^{1/2} \mathbf{1}\) is an eigenvector of \(\mathcal{L}\) with eigenvalue
    \(0\). So \(\eta_1 = 0\) and we set
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathbf{z}_1)_i = \left(\frac{D^{1/2} \mathbf{1}}{\|D^{1/2} \mathbf{1}\|_2}\right)_i
    = \sqrt{\frac{\delta(i)}{\sum_{i\in V} \delta(i)}}, \quad \forall i \in [n], \]
  prefs: []
  type: TYPE_NORMAL
- en: which makes \(\mathbf{z}_1\) into a unit norm vector.
  prefs: []
  type: TYPE_NORMAL
- en: We return to the eigenvectors of \(P\). When a matrix \(A \in \mathbb{R}^{n
    \times n}\) is diagonalizable, it has an eigendecomposition of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = Q \Lambda Q^{-1}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Lambda\) is a diagonal matrix whose diagonal entries are the eigenvalues
    of \(A\). The columns of \(Q\) are the eigenvectors of \(A\) and they form a basis
    of \(\mathbb{R}^n\). Unlike the symmetric case, however, the eigenvectors need
    not be orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Eigendecomposition of Random Walk on a Graph)** Let \(G = (V,E,w)\)
    be a graph with positive edge weights and no isolated vertex, and with degree
    matrix \(D\). Let \(P \in \mathbb{R}^{n \times n}\) be the transition matrix of
    random walk on \(G\). Let \(\mathbf{z}_1,\ldots,\mathbf{z}_n \in \mathbb{R}^n\)
    and \(0 \leq \eta_1 \leq \cdots \eta_n \leq 2\) be the eigenvectors and eigenvalues
    of the normalized Laplacian. Then \(P\) has the following eigendecomposition'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P &= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where the columns of \(Z\) are the \(\mathbf{z}_i\)s and \(H\) is a diagonal
    matrix with the \(\eta_i\)s on its diagonal. This can also be written as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P &= \mathbf{1}\bpi + \sum_{i=2}^n \lambda_i D^{-1/2} \mathbf{z}_i
    \mathbf{z}_i^T D^{1/2}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \frac{\delta(i)}{\sum_{j\in V} \delta(j)} \quad \text{and} \quad
    \lambda_i = 1- \eta_i, \qquad i =1,\ldots,n. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We write \(\mathcal{L}\) in terms of \(P\). Recall that \(P = D^{-1}
    A\). Hence'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = I - D^{1/2} P D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Rearranging this becomes
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = I - D^{-1/2} \mathcal{L} D^{1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence for all \(i=1,\ldots,n\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P (D^{-1/2} \mathbf{z}_i) &= (I - D^{-1/2} \mathcal{L} D^{1/2})\,(D^{-1/2}
    \mathbf{z}_i)\\ &= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \mathcal{L} D^{1/2} (D^{-1/2}
    \mathbf{z}_i)\\ &= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \mathcal{L} \mathbf{z}_i\\
    &= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \eta_i \mathbf{z}_i\\ &= (1 - \eta_i) (D^{-1/2}
    \mathbf{z}_i). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(P\) is a transition matrix, all its eigenvalues are bounded in absolute
    value by \(1\). So \(|1-\eta_i|\leq 1\), which implies \(0 \leq \eta_i \leq 2\).
  prefs: []
  type: TYPE_NORMAL
- en: We also note that
  prefs: []
  type: TYPE_NORMAL
- en: \[ (D^{-1/2} Z) (D^{1/2} Z)^T = D^{-1/2} Z Z^T D^{1/2} = D^{-1/2} D^{1/2} =
    I, \]
  prefs: []
  type: TYPE_NORMAL
- en: by the orthonormality of the eigenvectors of the normalized Laplacian. So the
    columns of \(D^{-1/2} Z\), i.e, \(D^{-1/2} \mathbf{z}_i\) for \(i=1,\ldots,n\),
    are linearly independent and
  prefs: []
  type: TYPE_NORMAL
- en: \[ (D^{-1/2} Z)^{-1} = (D^{1/2} Z)^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: That gives the first claim.
  prefs: []
  type: TYPE_NORMAL
- en: To get the second claim, we first note that (for similar calculations, see the
    definition of an SVD)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P &= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1}\\ &= (D^{-1/2} Z)(I
    - H) (D^{1/2} Z)^T\\ &= D^{-1/2} Z (I - H) Z^T D^{1/2}\\ &= \sum_{i=1}^n \lambda_i
    D^{-1/2} \mathbf{z}_i \mathbf{z}_i^T D^{1/2}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\lambda_i = 1- \eta_i\).
  prefs: []
  type: TYPE_NORMAL
- en: We then use the expression for \(\mathbf{z}_1\) above. We have
  prefs: []
  type: TYPE_NORMAL
- en: \[ D^{-1/2} \mathbf{z}_1 = D^{-1/2} \frac{D^{1/2}\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}
    = \frac{\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: while
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}_i^T D^{1/2} = (D^{1/2} \mathbf{z}_1)^T = \left(D^{1/2} \frac{D^{1/2}\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T
    = \left(\frac{D \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} D^{-1/2} \mathbf{z}_1 \mathbf{z}_1^T D^{1/2} &= \frac{\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}
    \left(\frac{D \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T\\ &= \mathbf{1} \left(\frac{D
    \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2^2}\right)^T\\ &= \mathbf{1} \left(\frac{D
    \mathbf{1}}{\|D \mathbf{1}\|_1}\right)^T\\ &= \mathbf{1} \bpi. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the second claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: If \(G\) is connected and \(w_{i,i} > 0\) for all \(i\), then the chain is irreducible
    and lazy. In that case, there is a unique eigenvalue \(1\) and \(-1\) is not an
    eigenvalue, so we must have \(0 < \eta_2 \leq \cdots \leq \eta_n < 2\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Limit theorems revisited** The *Convergence to Equilibrium Theorem* implies
    that in the irreducible, aperiodic case'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bmu P^t \to \bpi, \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(t \to +\infty\), for any initial distribution \(\bmu\) and the unique stationary
    distribution \(\bpi\). Here we give a simpler proof for random walk on a graph
    (or more generally a reversible chain), with the added bonus of a convergence
    rate. This follows from the same argument we used in the *Power Iteration Lemma*.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Spectral Gap)** Let \(G = (V,E,w)\) be a graph with positive
    edge weights and no isolated vertex. Let \(P \in \mathbb{R}^{n \times n}\) be
    the transition matrix of random walk on \(G\). The absolute spectral gap of \(G\)
    is defined as \(\gamma_{\star} = 1 - \lambda_{\star}\) where'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{\star} = \max\{|\lambda|\,:\, \text{$\lambda$ is an eigenvalue of
    $P$, $\lambda \neq 1$} \}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Convergence to Equilibrium: Reversible Case)** Let \(G = (V,E,w)\)
    be a connected graph with positive edge weights and \(w_{x,x} > 0\) for all \(x
    \in V\). Let \(P \in \mathbb{R}^{n \times n}\) be the transition matrix of random
    walk on \(G\) and \(\bpi\) its unique stationary distribution. Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bmu P^t \to \bpi, \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(t \to +\infty\) for any initial distribution \(\bmu\). Moreover,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left|P^t_{x,y} - \pi_y\right| \leq \gamma_\star^t \sqrt{\frac{\bar{\delta}}{\underline{\delta}}},
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bar{\delta} = \max_x \delta(x)\), \(\underline{\delta} = \min_x \delta(x)\)
    and \(\gamma_\star\) is the absolute spectral gap. \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Similarly to the *Power Iteration Lemma*, using the *Eigendecomposition
    of Random Walk on a Graph* we get'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P^2 &= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1}(D^{-1/2} Z)(I
    - H) (D^{-1/2} Z)^{-1}\\ &= (D^{-1/2} Z)(I - H)^2 (D^{-1/2} Z)^{-1}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By induction,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P^t &= (D^{-1/2} Z)(I - H)^t (D^{-1/2} Z)^{-1}\\ &= \sum_{i=1}^n
    \lambda_i^t (D^{-1/2} \mathbf{z}_i) (D^{1/2} \mathbf{z}_i)^T\\ &= \mathbf{1} \bpi
    + \sum_{i=2}^n \lambda_i^t (D^{-1/2} \mathbf{z}_i) (D^{1/2} \mathbf{z}_i)^T, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by calculations similar to the proof of the *Eigendecomposition of Random Walk
    on a Graph*.
  prefs: []
  type: TYPE_NORMAL
- en: In the irreducible, lazy case, for \(i=2,\ldots,n\), \(\lambda_i^t \to 0\) as
    \(t \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, \(|\lambda_i| \leq (1-\gamma_\star)\) for all \(i=2,\ldots,n\). Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|P^t_{x,y} - \pi_y\right| &= \sum_{i=2}^n \lambda_i^t
    \delta(x)^{-1/2}(\mathbf{z}_i)_x (\mathbf{z}_i)_y \delta(y)^{1/2}\\ &\leq (1-\gamma_\star)^t
    \sqrt{\frac{\delta(y)}{\delta(x)}} \sum_{i=2}^n |(\mathbf{z}_i)_x (\mathbf{z}_i)_y|.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We then use *Cauchy-Schwarz* and the fact that \(Z Z^T = I\) (as \(Z\) is an
    orthogonal matrix), which implies \(\sum_{i=1}^n (\mathbf{z}_i)_x^2 = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: We get that the above is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq (1-\gamma_\star)^t \sqrt{\frac{\delta(y)}{\delta(x)}}
    \sum_{i=2}^n (\mathbf{z}_i)_x^2 \sum_{i=2}^n (\mathbf{z}_i)_y^2\\ &\leq (1-\gamma_\star)^t
    \sqrt{\frac{\delta(y)}{\delta(x)}}\\ &\leq (1-\gamma_\star)^t \sqrt{\frac{\bar{\delta}}{\underline{\delta}}}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 'We record an immediate corollary that will be useful next. Let \(f : V \to
    \mathbb{R}\) be a function over the vertices. Define the (column) vector \(\mathbf{f}
    = (f(1),\ldots,f(n))^T\) and note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi \mathbf{f} = \sum_{x \in V} \pi_x f(x). \]
  prefs: []
  type: TYPE_NORMAL
- en: It will be convenient to use to \(\ell_\infty\)-norm. For a vector \(\mathbf{x}
    = (x_1,\ldots,x_n)^T\), we let \(\|\mathbf{x}\|_\infty = \max_{i \in [n]} |x_i|\).
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** For any initial distribution \(\bmu\) and any \(t\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\,\E[f(X_t)] - \bpi \mathbf{f}\,\right| \leq (1-\gamma_\star)^t
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Time Marginals Theorem*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\,\E[f(X_t)] - \bpi \mathbf{f}\,\right| &= \left|\,\sum_{x}
    \sum_y \mu_x (P^t)_{x,y} f(y) - \sum_{y} \pi_y f(y)\,\right|. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(\sum_{x} \mu_x = 1\), the right-hand side is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \left|\,\sum_{x} \sum_y \mu_x (P^t)_{x,y} f(y) - \sum_x
    \sum_{y} \mu_x \pi_y f(y)\,\right|\\ &\leq \sum_{x} \mu_x \sum_y \left| (P^t)_{x,y}
    - \pi_y \right| |f(y)|, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by the triangle inequality.
  prefs: []
  type: TYPE_NORMAL
- en: Now by the theorem this is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq \sum_{x} \mu_x \sum_y (1-\gamma_\star)^t \frac{\pi_y}{\pi_{\min{}}}|f(y)|\\
    &= (1-\gamma_\star)^t \frac{1}{\pi_{\min{}}} \sum_{x} \mu_x \sum_y \pi_y |f(y)|\\
    &\leq (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We also prove a version of the *Ergodic Theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Ergodic Theorem: Reversible Case)** Let \(G = (V,E,w)\) be a
    connected graph with positive edge weights and \(w_{x,x} > 0\) for all \(x \in
    V\). Let \(P \in \mathbb{R}^{n \times n}\) be the transition matrix of random
    walk on \(G\) and \(\bpi\) its unique stationary distribution. Let \(f : V \to
    \mathbb{R}\) be a function over the vertices. Then for any initial distribution
    \(\bmu\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{T} \sum_{t=1}^T f(X_{t}) \to \sum_{x \in V} \pi_x f(x), \]
  prefs: []
  type: TYPE_NORMAL
- en: in probability as \(T \to +\infty\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use *Chebyshev’s Inequality*. By the *Convergence Theorem:
    Reversible Case*, the expectation converges to the limit. To bound the variance,
    we use the *Eigendecomposition of Random Walk on a Graph*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We use *Chebyshev’s Inequality*, similarly to our proof of the *Weak
    Law of Large Numbers*. However, unlike that case, here the terms in the sum are
    not independent are require some finessing. Define again the (column) vector \(\mathbf{f}
    = (f(1),\ldots,f(n))\). Then the limit can be written as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{x \in V} \pi_x f(x) = \bpi \mathbf{f}. \]
  prefs: []
  type: TYPE_NORMAL
- en: By the corollary, the expectation of the sum can be bounded as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\,\E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] -
    \bpi \mathbf{f}\,\right| &\leq \frac{1}{T} \sum_{t=1}^T\left|\E[f(X_{t})] - \bpi
    \mathbf{f}\right|\\ &\leq \frac{1}{T} \sum_{t=1}^T (1-\gamma_\star)^t \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty\\ &\leq \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty \frac{1}{T}
    \sum_{t=0}^{+\infty} (1-\gamma_\star)^t\\ &= \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty
    \gamma_\star^{-1}\frac{1}{T} \to 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: Next we bound the variance of the sum. By the *Variance of a Sum*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{Var}\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] = \frac{1}{T^2}
    \sum_{t=1}^T \mathrm{Var}[f(X_{t})] + \frac{2}{T^2} \sum_{1 \leq s < t\leq T}
    \mathrm{Cov}[f(X_{s}),f(X_{t})]. \]
  prefs: []
  type: TYPE_NORMAL
- en: We bound the variance and covariance separately using the *Eigendecomposition
    of Random Walk on a Graph*.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain convergence, a trivial bound on the variance suffices. Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 \leq \mathrm{Var}[f(X_{t})] \leq \E[f(X_{t})^2] \leq \|\mathbf{f}\|_\infty^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 \leq \frac{1}{T^2} \sum_{t=1}^T \mathrm{Var}[f(X_{t})] \leq \frac{T \|\mathbf{f}\|_\infty^2}{T^2}
    \to 0, \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: Bounding the covariance requires a more delicate argument. Fix \(1 \leq s <
    t\leq T\). The trick is to condition on \(X_s\) and use the *Markov Property*.
    By definition of the covariance and the *Law of Total Expectation*,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\mathrm{Cov}[f(X_{s}),f(X_{t})]\\ &= \E\left[(f(X_{s}) - \E[f(X_{s})])
    (f(X_{t}) - \E[f(X_{t})])\right]\\ &= \sum_{x} \E\left[(f(X_{s}) - \E[f(X_{s})])
    (f(X_{t}) - \E[f(X_{t})])\,\middle|\,X_s = x\right]\,\P[X_s = x]\\ &= \sum_{x}
    \E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right](f(x) - \E[f(X_{s})])
    \,\P[X_s = x]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We now use the time-homogeneity of the chain to note that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\\
    &= \E\left[f(X_{t})\,\middle|\,X_0 = x\right] - \E[f(X_{t})]\\ &= \E\left[f(X_{t-s})\,\middle|\,X_0
    = x\right] - \E[f(X_{t})]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We now use the corollary. This expression in absolute value is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\left|\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\right|\\
    &= \left|\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right] - \E[f(X_{t})]\right|\\
    &= \left|(\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right] - \bpi \mathbf{f}) - (\E[f(X_{t})]
    - \bpi \mathbf{f})\right|\\ &\leq \left|\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right]
    - \bpi \mathbf{f}\right| + \left|\E[f(X_{t})] - \bpi \mathbf{f}\right|\\ &\leq
    (1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Plugging back above,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\left|\mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\ &\leq \sum_{x}
    \left|\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\right| \left|f(x)
    - \E[f(X_{s})]\right| \,\P[X_s = x]\\ &\leq \sum_{x} ((1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty)
    \left|f(x) - \E[f(X_{s})]\right| \,\P[X_s = x]\\ &\leq ((1-\gamma_\star)^{t-s}
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty) \sum_{x} 2 \|\mathbf{f}\|_\infty\P[X_s = x]\\ &\leq 4 (1-\gamma_\star)^{t-s}
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \((1-\gamma_\star)^t \leq (1-\gamma_\star)^{t-s}\) since
    \((1-\gamma_\star) < 1\) and \(t -s \leq t\).
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the sum over the covariances, the previous bound gives
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\left|\frac{2}{T^2} \sum_{1 \leq s < t\leq T} \mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\
    &\leq \frac{2}{T^2} \sum_{1 \leq s < t\leq T} \left|\mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\
    &\leq \frac{2}{T^2} \sum_{1 \leq s < t\leq T} 4 (1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the sum we make the change of variable \(h = t - s\) to get that
    the previous expression is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq 4 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2\frac{2}{T^2}
    \sum_{1 \leq s \leq T} \sum_{h=1}^{T-s} (1-\gamma_\star)^{h}\\ &\leq 4 \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty^2\frac{2}{T^2} \sum_{1 \leq s \leq T} \sum_{h=0}^{+\infty}
    (1-\gamma_\star)^{h}\\ &= 4 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2\frac{2}{T^2}
    \sum_{1 \leq s \leq T} \frac{1}{\gamma_\star}\\ &= 8 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T} \to 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: We have shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathrm{Var}\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right]
    \leq \|\mathbf{f}\|_\infty^2 \frac{1}{T} + 8 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T} \leq 9 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: For any \(\varepsilon > 0\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \bpi \mathbf{f}\,\right|
    \geq \varepsilon \right]\\ &= \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t})
    - \E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] + \left(\E\left[\frac{1}{T}
    \sum_{t=1}^T f(X_{t})\right] - \bpi \mathbf{f} \right)\,\right| \geq \varepsilon
    \right]\\ &\leq \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \E\left[\frac{1}{T}
    \sum_{t=1}^T f(X_{t})\right]\,\right| + \left|\,\E\left[\frac{1}{T} \sum_{t=1}^T
    f(X_{t})\right] - \bpi \mathbf{f} \,\right| \geq \varepsilon \right]\\ &\leq \P\left[\left|\,\frac{1}{T}
    \sum_{t=1}^T f(X_{t}) - \E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right]\,\right|
    \geq \varepsilon - \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty \gamma_\star^{-1}\frac{1}{T}\right].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We can now apply *Chebyshev* to get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \bpi \mathbf{f}\,\right|
    \geq \varepsilon \right] &\leq \frac{9 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T}}{(\varepsilon - \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty
    \gamma_\star^{-1}\frac{1}{T})^2} \to 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.3\. Additional proofs[#](#additional-proofs "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Proof of the convergence theorem** We prove the convergence to equilibrium
    theorem in the irreducible, lazy case. Throughout this section, \((X_t)_{t \geq
    0}\) is an irreducible, lazy Markov chain on state space \(\mathcal{S} = [n]\)
    with transition matrix \(P = (p_{i,j})_{i,j=1}^n\), initial distribution \(\bmu
    = (\mu_1,\ldots,\mu_n)\) and unique stationary distribution \(\bpi = (\pi_1,\ldots,\pi_n)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We give a probabilistic proof of the *Convergence to Equilibrium Theorem*.
    The proof uses a clever idea: coupling. Separately from \((X_t)_{t \geq 0}\),
    we consider an independent Markov chain \((Y_t)_{t \geq 0}\) with the same transition
    matrix but initial distribution \(\bpi\). By the definition of stationarity,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[Y_t = i] = \pi_i, \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(i\) and all \(t\). Hence it suffices to show that
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\P[X_t = i] - \pi_i| = |\P[X_t = i] - \P[Y_t = i]| \to 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(t \to +\infty\) for all \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1: Showing the joint process is Markov.* We observe first that the joint
    process \((X_0,Y_0),(X_1,Y_1),\ldots\) is itself a Markov chain! Let’s just check
    the definition. By the independence of \((X_t)\) and \((Y_t)\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[(X_t,Y_t) = (x_t,y_t)\,|\,(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1}),\ldots,(X_0,Y_0)
    = (x_0,y_0)]\\ &=\frac{\P[(X_t,Y_t) = (x_t,y_t),(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1}),\ldots,(X_0,Y_0)
    = (x_0,y_0)]} {\P[(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1}),\ldots,(X_0,Y_0) = (x_0,y_0)]}\\
    &=\frac{\P[X_t = x_t,X_{t-1} = x_{t-1},\ldots,X_0 = x_0]\,\P[Y_t = y_t,Y_{t-1}
    = y_{t-1},\ldots,Y_0 = y_0]} {\P[X_{t-1} = x_{t-1},\ldots,X_0 = x_0]\,\P[Y_{t-1}
    = y_{t-1},\ldots,Y_0 = y_0]}\\ &=\frac{\P[X_t = x_t,X_{t-1} = x_{t-1},\ldots,X_0
    = x_0]} {\P[X_{t-1} = x_{t-1},\ldots,X_0 = x_0]} \frac{\P[Y_t = y_t,Y_{t-1} =
    y_{t-1},\ldots,Y_0 = y_0]} {\P[Y_{t-1} = y_{t-1},\ldots,Y_0 = y_0]}\\ &=\P[X_t
    = x_t\,|\,X_{t-1} = x_{t-1}, \ldots,X_0 = x_0] \,\P[Y_t = y_t\,|\,Y_{t-1} = y_{t-1},
    \ldots,Y_0 = y_0]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Now we use the fact that each is separately a Markov chain to simplify this
    last expression
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}] \,\P[Y_t = y_t\,|\,Y_{t-1}
    = y_{t-1}]\\ &= \frac{\P[X_t = x_t,X_{t-1} = x_{t-1}]}{\P[X_{t-1} = x_{t-1}]}
    \frac{\P[Y_t = y_t,Y_{t-1} = y_{t-1}]}{\P[Y_{t-1} = y_{t-1}]}\\ &= \frac{\P[X_t
    = x_t,X_{t-1} = x_{t-1}]\,\P[Y_t = y_t,Y_{t-1} = y_{t-1}]}{\P[X_{t-1} = x_{t-1}]\,\P[Y_{t-1}
    = y_{t-1}]}\\ &= \frac{\P[(X_t,Y_t) = (x_t,y_t),(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1})]}{\P[(X_{t-1},Y_{t-1})
    = (x_{t-1},y_{t-1})]}\\ &= \P[(X_t,Y_t) = (x_t,y_t)\,|\,(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1})].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the claim.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2: Waiting until the marginal processes meet.* The idea of the coupling
    argument is to consider the first time \(T\) that \(X_T = Y_T\). Note that \(T\)
    is a random time. But it is a special kind of random time often referred to as
    a stopping time. That is, the event \(\{T=s\}\) only depends on the trajectory
    of the joint chain \((X_t,Y_t)\) up to time \(s\). Specifically,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \{T=s\} = \left\{ ((X_0,Y_0),\ldots,(X_{s-1},Y_{s-1})) \in \mathcal{N}^2_{s-1},
    X_s = Y_s \right\} \]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{N}^2_{s-1} = \{ ((x_0,y_0),\ldots,(x_{s-1},y_{s-1})) \in [\mathcal{S}\times\mathcal{S}]^{s}\,:\,x_i
    \neq y_i, \forall 0 \leq i \leq s-1 \}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here is a remarkable observation. The distributions of \(X_s\) and \(Y_s\) are
    the same after the coupling time \(T\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Distribution after Coupling)** For all \(s\) and all \(i\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_s = i, T \leq s] = \P[Y_s = i, T \leq s]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We sum over the possible values of \(T\) and \(X_T=Y_T\), and use
    the multiplication rule'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X_s = i, T \leq s]\\ &= \sum_{j=1}^n \sum_{h=0}^s \P[X_s
    = i, T=h, X_h = j]\\ &= \sum_{j=1}^n \sum_{h=0}^s \P[X_s = i \,|\, T=h, X_h =
    j] \,\P[T=h, X_h = j]\\ &= \sum_{j=1}^n \sum_{h=0}^s \P[X_s = i \,|\, ((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1}))
    \in \mathcal{N}^2_{h-1}, (X_h, Y_h) = (j,j)]\\ &\qquad\qquad\qquad\qquad \times
    \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) =
    (j,j)]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Using the Markov property for the joint process, in the form stated in *Exercise
    3.36*, we get that this last expression is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \sum_{j=1}^n \sum_{h=0}^s \P[X_s = i \,|\, (X_h, Y_h) =
    (j,j)]\\ &\qquad\qquad\times \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1},
    (X_h, Y_h) = (j,j)]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the independence of the marginal processes and the fact that they have the
    same transition matrix, this is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \sum_{j=1}^n \sum_{h=0}^s \P[X_s = i \,|\, X_h = j]\\ &\qquad\qquad\times
    \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) =
    (j,j)]\\ &= \sum_{j=1}^n \sum_{h=0}^s \P[Y_s = i \,|\, Y_h = j]\\ &\qquad\qquad\times
    \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) =
    (j,j)]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Arguing backwards gives \(\P[Y_s = i, T \leq s]\) and concludes the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3: Bounding how long it takes for the marginal processes to meet.* Since'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_t = i] = \P[X_t = i, T \leq t] + \P[X_t = i, T > t] \]
  prefs: []
  type: TYPE_NORMAL
- en: and similarly for \(\P[Y_t = i]\), using the previous lemma we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &|\P[X_t = i] - \P[Y_t = i]|\\ &= |\P[X_t = i, T > t] - \P[Y_t
    = i, T > t]|\\ &\leq \P[X_t = i, T > t] + \P[Y_t = i, T > t]\\ &\leq 2 \P[T >
    t]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So it remains to show that \(\P[T > t]\) goes to \(0\) as \(t \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: We note that \(\P[T > t]\) is non-increasing as a function of \(t\). Indeed,
    for \(h > 0\), we have the implication
  prefs: []
  type: TYPE_NORMAL
- en: \[ \{T > t+h\} \subseteq \{T > t\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: so by the monotonicity of probabilities
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[T > t+h] \leq \P[T > t]. \]
  prefs: []
  type: TYPE_NORMAL
- en: So it remains to prove the following lemma.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Tail of Coupling Time)** There is a \(0 < \beta < 1\) and a positive
    integer \(m\) such that, for all positive integers \(k\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[T > k m] \leq \beta^{k m}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Recall that the state space of the marginal processes is \([n]\),
    so the joint process lives in \([n]\times [n]\). Since the event \(\{(X_m, Y_m)
    = (1,1)\}\) implies \(\{T \leq m\}\), to bound \(\P[T > m]\) we note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[T > m] = 1 - \P[T \leq m] \leq 1 - \P[(X_m, Y_m) = (1,1)]. \]
  prefs: []
  type: TYPE_NORMAL
- en: To bound the probability on right-hand side, we construct a path of length \(m\)
    in the transition graph of the joint process from any state to \((1,1)\).
  prefs: []
  type: TYPE_NORMAL
- en: For \(i \in [n]\), let \(\mathcal{P}_i = (z_{i,0},\ldots,z_{i,\ell_i})\) be
    the shortest path in the transition graph of \((X_t)_{t \geq 0}\) from \(i\) to
    \(1\), where \(z_{i,0} = i\) and \(z_{i,\ell_i} = 1\). By irreducibility there
    exists such a path for any \(i\). Here \(\ell_i\) is the length of \(\mathcal{P}_i\),
    and we define
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell^* = \max_{i \neq 1} \ell_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: We make all the paths above the same length \(\ell^*\) by padding them with
    \(1\)s. That is, we define \(\mathcal{P}_i^* = (z_{i,0},\ldots,z_{i,\ell_i},1,\ldots,1)\)
    such that this path now has length \(\ell^*\). This remains a path in the transition
    graph of \((X_t)_{t \geq 0}\) because the chain is lazy.
  prefs: []
  type: TYPE_NORMAL
- en: Now, for any pair of states \((i,j) \in [n] \times [n]\), consider the path
    of length \(m := 2 \ell^*\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{Q}^*_{(i,j)} = ((z_{i,0},j),\ldots,(z_{i,\ell_i},j),(1,j),\ldots,(1,j),(1,z_{j,0}),\ldots,(1,z_{i,\ell_i}),(1,1),\ldots,(1,1)).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, we leave the second component at \(j\) while running through path
    \(\mathcal{P}_i^*\) on the first component, then we leave the first component
    at \(1\) while running through path \(\mathcal{P}_j^*\) on the second component.
    Path \(\mathcal{Q}^*_{(i,j)}\) is a valid path in the transition graph of the
    joint chain. Again, we are using that the marginal processes are lazy.
  prefs: []
  type: TYPE_NORMAL
- en: Denote by \(\mathcal{Q}^*_{(i,j)}[r]\) be the \(r\)-th state in path \(\mathcal{Q}^*_{(i,j)}\).
    Going back to bounding \(\P[(X_m, Y_m) = (1,1)]\), we define \(\beta\) as follows
  prefs: []
  type: TYPE_NORMAL
- en: '\[\begin{align*} &\P[(X_m, Y_m) = (1,1)\,|\,(X_0, Y_0) = (i,j)]\\ &\geq \min_{(i,j)}
    \P[(X_r, Y_r) = \mathcal{Q}^*_{(i,j)}[r], \forall r=0,\ldots,m \,|\,(X_0, Y_0)
    = (i_0,j_0)]\\ &=: 1-\beta \in (0,1). \end{align*}\]'
  prefs: []
  type: TYPE_NORMAL
- en: By the law of total probability,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[(X_m, Y_m) = (1,1)]\\ &= \sum_{(i,j) \in [n]\times [n]}\P[(X_m,
    Y_m) = (1,1)\,|\,(X_0, Y_0) = (i,j)]\, \P[(X_0, Y_0) = (i,j)]\\ &= \sum_{(i,j)
    \in [n]\times [n]}\P[(X_m, Y_m) = (1,1)\,|\,(X_0, Y_0) = (i,j)]\, \mu_i \pi_j\\
    &\geq \sum_{(i,j) \in [n]\times [n]} \P[(X_r, Y_r) = \mathcal{Q}^*_{(i,j)}[r],
    \forall r=0,\ldots,m \,|\,(X_0, Y_0) = (i,j)]\, \mu_i \pi_j\\ &\geq (1-\beta)
    \sum_{(i,j) \in [n]\times [n]} \mu_i \pi_j\\ &= 1-\beta. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[T > m] \leq \beta. \]
  prefs: []
  type: TYPE_NORMAL
- en: Because
  prefs: []
  type: TYPE_NORMAL
- en: \[ \{T > 2m\} \subseteq \{T > m\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: it holds that by the multiplication rule that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[T > 2m] &= \P[\{T > 2m\}\cap\{T > m\}]\\ &= \P[T > 2m\,|\,
    T > m]\,\P[T > m]\\ &\leq \P[T > 2m\,|\, T > m]\,\beta. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Summing over all state pairs at time \(m\), we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[T > 2m\,|\, T > m]\\ &= \sum_{(i,j) \in [n]\times [n]}
    \P[T > 2m\,|\, T > m, (X_m,Y_m) = (i,j)] \,\P[(X_m,Y_m) = (i,j) \,|\, T > m].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Arguing as above, note that for \(i \neq j\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[T \leq 2m\,|\, T > m, (X_m,Y_m) = (i,j)]\\ &\geq \P[(X_{2m},Y_{2m})
    = (1,1) \,|\, T > m, (X_m,Y_m) = (i,j)]\\ &= \P[(X_{2m},Y_{2m}) = (1,1) \,|\,
    (X_m,Y_m) = (i,j), ((X_0,Y_0),\ldots,(X_{m-1},Y_{m-1})) \in \mathcal{N}^2_{m-1}]\\
    &= \P[(X_{2m},Y_{2m}) = (1,1) \,|\, (X_m,Y_m) = (i,j)]\\ &= \P[(X_{m},Y_{m}) =
    (1,1) \,|\, (X_0,Y_0) = (i,j)]\\ &\geq 1-\beta, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by the Markov property and the time-homogeneity of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Plugging above and noting that \(\P[(X_m,Y_m) = (i,j) \,|\, T > m] = 0\) when
    \(i = j\), we get that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[T > 2m\,|\, T > m]\\ &\geq \sum_{\substack{(i,j) \in [n]\times
    [n]\\i \neq j}} \beta \,\P[(X_m,Y_m) = (i,j) \,|\, T > m]\\ &= \beta. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So we have proved that \(\P[T > 2m] \leq \beta^2\). Proceeding similarly by
    induction gives the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_rwmc_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_rwmc_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_rwmc_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 7.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-rwmc-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-rwmc-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.1** We need to check that all entries of \(P\) are non-negative and
    that all rows sum to one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-negativity: All entries of \(P\) are clearly non-negative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Row sums: Row 1: \(0.2 + 0.5 + 0.3 = 1\) Row 2: \(0.4 + 0.1 + 0.5 = 1\) Row
    3: \(0.6 + 0.3 + 0.1 = 1\)'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, \(P\) is a stochastic matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.3**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[X_2 = 2] &= (\boldsymbol{\mu} P^2)_2 \\ &= (0.2,
    0.3, 0.5)^T \begin{pmatrix} 0.44 & 0.31 & 0.25 \\ 0.44 & 0.37 & 0.19 \\ 0.36 &
    0.33 & 0.31 \end{pmatrix}_2 \\ &= 0.2 \cdot 0.31 + 0.3 \cdot 0.37 + 0.5 \cdot
    0.33 \\ &= 0.338. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.5**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The transition graph has a directed edge from state \(i\) to state \(j\) if
    and only if \(p_{i,j} > 0\), as stated in the definition.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.7**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[X_2 = 2 | X_0 = 3] &= (P^2)_{3,2} \\ &= \begin{pmatrix}
    0.1 & 0.4 & 0.5 \\ 0.2 & 0.6 & 0.2 \\ 0.3 & 0.3 & 0.4 \end{pmatrix}^2_{3,2} \\
    &= \begin{pmatrix} 0.29 & 0.38 & 0.33 \\ 0.26 & 0.44 & 0.30 \\ 0.27 & 0.36 & 0.37
    \end{pmatrix}_{3,2} \\ &= 0.36. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.9** The probability is given by the entry \((P^2)_{0,1}\), where \(P^2\)
    is the matrix product \(P \times P\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P^2 = \begin{pmatrix} 1/3 & 2/3 \\ 1/2 & 1/2 \end{pmatrix} \begin{pmatrix}
    1/3 & 2/3 \\ 1/2 & 1/2 \end{pmatrix} = \begin{pmatrix} 7/18 & 11/18 \\ 5/12 &
    7/12 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the probability is \(\boxed{11/18}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.11** The marginal distribution at time 2 is given by \(\boldsymbol{\mu}
    P^2\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P^2 = \begin{pmatrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/3 & 1/3
    & 1/3 \end{pmatrix} \begin{pmatrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/3 & 1/3 &
    1/3 \end{pmatrix} = \begin{pmatrix} 5/12 & 1/6 & 5/12 \\ 0 & 1 & 0 \\ 4/9 & 4/9
    & 1/9 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mu P^2 = (1/4, 1/2, 1/4)^T \begin{pmatrix} 5/12 & 1/6 & 5/12
    \\ 0 & 1 & 0 \\ 4/9 & 4/9 & 1/9 \end{pmatrix} = \boxed{(13/36, 19/36, 4/36)^T}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.13** The distribution at time 1 is given by \(\boldsymbol{\mu} P\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \boldsymbol{\mu} P = (1/3, 2/3)^T \begin{pmatrix}1/2 & 1/2 \\
    1 & 0\end{pmatrix} = \boxed{(2/3, 1/3)^T} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.15** The chain alternates deterministically between states 1 and 2\.
    Therefore, starting in state 1, it will return to state 1 after exactly \(\boxed{2}\)
    steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.1** Yes, the matrix is irreducible. The corresponding transition graph
    is a cycle, which is strongly connected.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.3** We need to check if \(\boldsymbol{\pi} P = \boldsymbol{\pi}\). Indeed,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} (0.6, 0.4)^T \begin{pmatrix} 0.4 & 0.6 \\ 0.7 & 0.3 \end{pmatrix}
    = (0.6 \cdot 0.4 + 0.4 \cdot 0.7, 0.6 \cdot 0.6 + 0.4 \cdot 0.3)^T = (0.52, 0.48)^T
    \neq (0.6, 0.4)^T, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so \(\pi\) is not a stationary distribution of the Markov chain.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.5** Let \(\boldsymbol{\pi} = (\pi_1, \pi_2)^T\) be a stationary distribution.
    Then, we need to solve the system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 \cdot 0.5 + \pi_2 \cdot 0.5 &= \pi_1 \\ \pi_1 + \pi_2
    &= 1 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The first equation simplifies to \(\pi_1 = \pi_2\), and substituting this into
    the second equation yields \(2\pi_1 = 1\), or \(\pi_1 = \pi_2 = 0.5\). Therefore,
    \(\boldsymbol{\pi} = (0.5, 0.5)^T\) is a stationary distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.7** To verify that \(\boldsymbol{\pi}\) is a stationary distribution,
    we need to check if \(\boldsymbol{\pi} P = \boldsymbol{\pi}\). Let’s perform the
    matrix multiplication step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\boldsymbol{\pi} P = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})^T \begin{pmatrix}
    0.4 & 0.3 & 0.3 \\ 0.2 & 0.5 & 0.3 \\ 0.4 & 0.2 & 0.4 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= (\frac{1}{3} \cdot 0.4 + \frac{1}{3} \cdot 0.2 + \frac{1}{3} \cdot 0.4,
    \frac{1}{3} \cdot 0.3 + \frac{1}{3} \cdot 0.5 + \frac{1}{3} \cdot 0.2, \frac{1}{3}
    \cdot 0.3 + \frac{1}{3} \cdot 0.3 + \frac{1}{3} \cdot 0.4)^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= (\frac{0.4 + 0.2 + 0.4}{3}, \frac{0.3 + 0.5 + 0.2}{3}, \frac{0.3 + 0.3 +
    0.4}{3})^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= \boldsymbol{\pi}\)
  prefs: []
  type: TYPE_NORMAL
- en: The result is equal to \(\pi = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\). Therefore,
    the uniform distribution \(\pi\) is indeed a stationary distribution for the matrix
    \(P\). Note that the transition matrix \(P\) is doubly stochastic because each
    row and each column sums to 1\. This property ensures that the uniform distribution
    is always a stationary distribution for doubly stochastic matrices, as mentioned
    in the text.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.9** Let \(\mathbf{1} = (1, 1, \ldots, 1)\) be the column vector of all
    ones. For any stochastic matrix \(P\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P \mathbf{1} = \begin{pmatrix} p_{1,1} & \cdots & p_{1,n} \\
    \vdots & \ddots & \vdots \\ p_{n,1} & \cdots & p_{n,n} \end{pmatrix} \begin{pmatrix}
    1 \\ \vdots \\ 1 \end{pmatrix} = \begin{pmatrix} \sum_{j=1}^n p_{1,j} \\ \vdots
    \\ \sum_{j=1}^n p_{n,j} \end{pmatrix} = \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}
    = \mathbf{1}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: because each row of a stochastic matrix sums to 1\. Therefore, \(\mathbf{1}\)
    is a right eigenvector of \(P\) with eigenvalue 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.11** Let \(\boldsymbol{\pi} = (\pi_1, \pi_2, \pi_3)^T\) be the stationary
    distribution. We need to solve the system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Write out the system of equations based on \(\boldsymbol{\pi} P = \boldsymbol{\pi}\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 \cdot 0.7 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.6 &= \pi_1
    \\ \pi_1 \cdot 0.2 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.1 &= \pi_2 \\ \pi_1 \cdot
    0.1 + \pi_2 \cdot 0.2 + \pi_3 \cdot 0.3 &= \pi_3 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Rearrange the equations to have zero on the right-hand side.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 \cdot 0.7 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.6 - \pi_1
    &= 0 \\ \pi_1 \cdot 0.2 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.1 - \pi_2 &= 0 \\ \pi_1
    \cdot 0.1 + \pi_2 \cdot 0.2 + \pi_3 \cdot 0.3 - \pi_3 &= 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Simplify the equations.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.3\pi_1 + 0.4\pi_2 + 0.6\pi_3 &= 0 \\ 0.2\pi_1 - 0.6\pi_2
    + 0.1\pi_3 &= 0 \\ 0.1\pi_1 + 0.2\pi_2 - 0.7\pi_3 &= 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Use the condition \(\pi_1 + \pi_2 + \pi_3 = 1\) to eliminate one variable,
    say \(\pi_3\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_3 = 1 - \pi_1 - \pi_2 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Substitute the expression for \(\pi_3\) into the simplified equations
    from Step 3.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.3\pi_1 + 0.4\pi_2 + 0.6(1 - \pi_1 - \pi_2) &= 0 \\ 0.2\pi_1
    - 0.6\pi_2 + 0.1(1 - \pi_1 - \pi_2) &= 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Simplify the equations further.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.9\pi_1 - 0.2\pi_2 &= -0.6 \\ 0.1\pi_1 - 0.7\pi_2 &= -0.1
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 7: Solve the linear system of two equations with two unknowns using any
    suitable method (e.g., substitution, elimination, or matrix inversion).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the substitution method: From the second equation, express \(\pi_1\)
    in terms of \(\pi_2\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_1 = 7\pi_2 - 1 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Substitute this into the first equation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.9(7\pi_2 - 1) - 0.2\pi_2 &= -0.6 \\ -6.3\pi_2 + 0.9 - 0.2\pi_2
    &= -0.6 \\ -6.5\pi_2 &= -1.5 \\ \pi_2 &= \frac{3}{13} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, substitute \(\pi_2 = \frac{3}{13}\) back into the expression for \(\pi_1\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 &= 7 \cdot \frac{3}{13} - 1 \\ &= \frac{21}{13} - \frac{13}{13}
    \\ &= \frac{8}{13} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, use \(\pi_1 + \pi_2 + \pi_3 = 1\) to find \(\pi_3\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_3 &= 1 - \pi_1 - \pi_2 \\ &= 1 - \frac{8}{13} - \frac{3}{13}
    \\ &= \frac{2}{13} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the stationary distribution is \((\frac{8}{13}, \frac{3}{13}, \frac{2}{13})^T\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.4.1** Given the transition matrix of a Markov chain:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0.2 & 0.8 \\ 0.6 & 0.4 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: determine if the chain is lazy.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.4.3** Given a Markov chain with transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0.4 & 0.6 \\ 0.7 & 0.3 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and initial distribution \(\boldsymbol{\mu} = (0.2, 0.8)^T\), compute \(\lim_{t
    \to \infty} \boldsymbol{\mu} P^t\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.1** The degree matrix \(D\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{pmatrix} 2 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0
    & 3 & 0 \\ 0 & 0 & 0 & 2 \\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The degrees are computed by summing the entries in each row of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.3** A stochastic matrix has rows that sum to 1\. The rows of \(P\) sum
    to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{2} + \frac{1}{2} = 1, \quad \frac{1}{3} + \frac{1}{3} + \frac{1}{3}
    = 1, \quad \frac{1}{3} + \frac{1}{3} + \frac{1}{3} = 1, \quad \frac{1}{2} + \frac{1}{2}
    = 1\. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, \(P\) is stochastic.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.5**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = D^{-1}A = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0
    \\ 0 & 0 & 1/2 & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 0 & 1 & 0 &
    0 \\ 0 & 0 & 1 & 0 \\ 1 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \end{pmatrix} = \begin{pmatrix}
    0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 1/2 & 0 & 0 & 1/2 \\ 0 & 0 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.7** First, compute the transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = D^{-1}A = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1/2 & 0 &
    0 \\ 0 & 0 & 1/2 & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 0 & 1 & 0
    & 0 \\ 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 0 & 0 & 1 & 0 \end{pmatrix} = \begin{pmatrix}
    0 & 1 & 0 & 0 \\ 1/2 & 0 & 1/2 & 0 \\ 0 & 1/2 & 0 & 1/2 \\ 0 & 0 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, compute the modified transition matrix with the damping factor:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \alpha P + (1-\alpha)\frac{1}{4}\mathbf{1}\mathbf{1}^T =
    0.8P + 0.2\begin{pmatrix} 1/4 & 1/4 & 1/4 & 1/4 \\ 1/4 & 1/4 & 1/4 & 1/4 \\ 1/4
    & 1/4 & 1/4 & 1/4 \\ 1/4 & 1/4 & 1/4 & 1/4 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.9** First, compute the transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 \\
    1/2 & 0 & 0 & 1/2 & 0 \\ 1/2 & 0 & 0 & 0 & 1/2 \\ 0 & 0 & 0 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, compute the modified transition matrix with the damping factor:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = 0.9P + 0.1\begin{pmatrix} 1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\
    1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\ 1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\ 1/5 & 1/5 & 1/5
    & 1/5 & 1/5 \\ 1/5 & 1/5 & 1/5 & 1/5 & 1/5 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.11** The new adjacency matrix \(A''\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A' = \begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 1 & 0
    & 0 & 1 \\ 0 & 0 & 0 & 1 \\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: A self-loop is added to vertex 4.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.13** The modified transition matrix \(Q\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q = \alpha P + (1 - \alpha) \frac{1}{n} \mathbf{1}\mathbf{1}^T = 0.85 P +
    0.15 \frac{1}{4} \mathbf{1}\mathbf{1}^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the values from \(P\) and \(\mathbf{1}\mathbf{1}^T\), we get:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \begin{pmatrix} 0.0375 & 0.8875 & 0.0375 & 0.0375 \\ 0.0375
    & 0.0375 & 0.8875 & 0.0375 \\ 0.4625 & 0.0375 & 0.0375 & 0.4625 \\ 0.0375 & 0.0375
    & 0.0375 & 0.8875 \\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.1** The acceptance probability is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min\left\{1, \frac{\pi_1}{\pi_2}\frac{Q(1, 2)}{Q(2, 1)}\right\} = \min\left\{1,
    \frac{0.1}{0.2}\frac{0.5}{0.5}\right\} = \frac{1}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.3** Since the proposal chain is symmetric, the acceptance probability
    simplifies to $\( \min\left\{1, \frac{\pi_2}{\pi_1}\right\} = \min\left\{1, \frac{0.2}{0.1}\right\}
    = 1. \)$'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.5** The acceptance probability is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min \left\{ 1, \frac{\pi(y)}{\pi(x)} \frac{Q(y, x)}{Q(x, y)} \right\} =
    \min \left\{ 1, \frac{e^{-9/2}}{e^{-2}} \right\} = e^{-5/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.7** The conditional probability is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_1^v(1|\mathbf{v}_{-1}, \mathbf{h}) = \sigma\left(\sum_{j=1}^2 w_{1,j}h_j
    + b_1\right) = \sigma(1\cdot 1 + (-1)\cdot 0 + 1) = \sigma(2) \approx 0.881. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.9** The energy is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathcal{E}(\mathbf{v}, \mathbf{h}) = -\mathbf{v}^T W \mathbf{h}
    = -\begin{pmatrix} 1 & 0 \end{pmatrix} \begin{pmatrix} 1 & -2 \\ 3 & 0 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = 1. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.11** The conditional mean for the hidden units is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[h_j | \mathbf{v}] = \sigma \left( \sum_{i} W_{ij} v_i + c_j \right).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(h_1\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[h_1 | \mathbf{v}] = \sigma (0.5 \cdot 1 + 0.3 \cdot 0 - 0.6 \cdot
    1 + 0.1) = \sigma (0.5 - 0.6 + 0.1) = \sigma (0) = 0.5. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(h_2\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[h_2 | \mathbf{v}] = \sigma (-0.2 \cdot 1 + 0.8 \cdot 0 + 0.1 \cdot
    1 - 0.3) = \sigma (-0.2 + 0.1 - 0.3) = \sigma (-0.4) = \frac{1}{1 + e^{0.4}} \approx
    0.4013. \]
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define a discrete-time Markov chain and its state space, initial distribution,
    and transition probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify Markov chains in real-world scenarios, such as weather patterns or
    random walks on graphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct the transition matrix for a time-homogeneous Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct the transition graph of a Markov chain from its transition matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the transition matrix of a Markov chain is a stochastic matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Markov property to calculate the probability of specific sequences
    of events in a Markov chain using the distribution of a sample path.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the marginal distribution of a Markov chain at a specific time using
    matrix powers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use simulation to generate sample paths of a Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a stationary distribution of a finite-state, discrete-time, time-homogeneous
    Markov chain and express the defining condition in matrix form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the relationship between stationary distributions and left eigenvectors
    of the transition matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine whether a given probability distribution is a stationary distribution
    for a given Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify irreducible Markov chains and explain their significance in the context
    of stationary distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the main theorem on the existence and uniqueness of a stationary distribution
    for an irreducible Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the stationary distribution of a Markov chain numerically using either
    eigenvalue methods or by solving a linear system of equations, including the “Replace
    an Equation” method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the concepts of aperiodicity and weak laziness for finite-space discrete-time
    Markov chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Convergence to Equilibrium Theorem and the Ergodic Theorem for irreducible
    Markov chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the Ergodic Theorem by simulating a Markov chain and comparing the frequency
    of visits to each state with the stationary distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of coupling and its role in proving the Convergence to Equilibrium
    Theorem for irreducible, weakly lazy Markov chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define random walks on directed and undirected graphs, and express their transition
    matrices in terms of the adjacency matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the stationary distribution of a random walk on an undirected graph
    and explain its relation to degree centrality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of reversibility and its connection to the stationary distribution
    of a random walk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the PageRank algorithm and its interpretation as a modified random
    walk on a directed graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the PageRank vector by finding the stationary distribution of the modified
    random walk using power iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the PageRank algorithm to real-world datasets to identify central nodes
    in a network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of Personalized PageRank and how it differs from the standard
    PageRank algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify the PageRank algorithm to implement Personalized PageRank and interpret
    the results based on the chosen teleportation distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the concept of Markov Chain Monte Carlo (MCMC) and its application
    in sampling from complex probability distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the Metropolis-Hastings algorithm, including the proposal distribution
    and acceptance-rejection steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate acceptance probabilities in the Metropolis-Hastings algorithm for
    a given target and proposal distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the correctness of the Metropolis-Hastings algorithm by showing that the
    resulting Markov chain is irreducible and reversible with respect to the target
    distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the Gibbs sampling algorithm for a given probabilistic model, such
    as a Restricted Boltzmann Machine (RBM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the conditional independence properties of RBMs and their role in Gibbs
    sampling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_rwmc_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_rwmc_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_rwmc_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.8.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 7.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 7.6](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_7_6.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.8.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-rwmc-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-rwmc-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7.8.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.1** We need to check that all entries of \(P\) are non-negative and
    that all rows sum to one.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-negativity: All entries of \(P\) are clearly non-negative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Row sums: Row 1: \(0.2 + 0.5 + 0.3 = 1\) Row 2: \(0.4 + 0.1 + 0.5 = 1\) Row
    3: \(0.6 + 0.3 + 0.1 = 1\)'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, \(P\) is a stochastic matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.3**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[X_2 = 2] &= (\boldsymbol{\mu} P^2)_2 \\ &= (0.2,
    0.3, 0.5)^T \begin{pmatrix} 0.44 & 0.31 & 0.25 \\ 0.44 & 0.37 & 0.19 \\ 0.36 &
    0.33 & 0.31 \end{pmatrix}_2 \\ &= 0.2 \cdot 0.31 + 0.3 \cdot 0.37 + 0.5 \cdot
    0.33 \\ &= 0.338. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.5**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The transition graph has a directed edge from state \(i\) to state \(j\) if
    and only if \(p_{i,j} > 0\), as stated in the definition.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.7**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[X_2 = 2 | X_0 = 3] &= (P^2)_{3,2} \\ &= \begin{pmatrix}
    0.1 & 0.4 & 0.5 \\ 0.2 & 0.6 & 0.2 \\ 0.3 & 0.3 & 0.4 \end{pmatrix}^2_{3,2} \\
    &= \begin{pmatrix} 0.29 & 0.38 & 0.33 \\ 0.26 & 0.44 & 0.30 \\ 0.27 & 0.36 & 0.37
    \end{pmatrix}_{3,2} \\ &= 0.36. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.9** The probability is given by the entry \((P^2)_{0,1}\), where \(P^2\)
    is the matrix product \(P \times P\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P^2 = \begin{pmatrix} 1/3 & 2/3 \\ 1/2 & 1/2 \end{pmatrix} \begin{pmatrix}
    1/3 & 2/3 \\ 1/2 & 1/2 \end{pmatrix} = \begin{pmatrix} 7/18 & 11/18 \\ 5/12 &
    7/12 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the probability is \(\boxed{11/18}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.11** The marginal distribution at time 2 is given by \(\boldsymbol{\mu}
    P^2\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P^2 = \begin{pmatrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/3 & 1/3
    & 1/3 \end{pmatrix} \begin{pmatrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/3 & 1/3 &
    1/3 \end{pmatrix} = \begin{pmatrix} 5/12 & 1/6 & 5/12 \\ 0 & 1 & 0 \\ 4/9 & 4/9
    & 1/9 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mu P^2 = (1/4, 1/2, 1/4)^T \begin{pmatrix} 5/12 & 1/6 & 5/12
    \\ 0 & 1 & 0 \\ 4/9 & 4/9 & 1/9 \end{pmatrix} = \boxed{(13/36, 19/36, 4/36)^T}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.13** The distribution at time 1 is given by \(\boldsymbol{\mu} P\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \boldsymbol{\mu} P = (1/3, 2/3)^T \begin{pmatrix}1/2 & 1/2 \\
    1 & 0\end{pmatrix} = \boxed{(2/3, 1/3)^T} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.2.15** The chain alternates deterministically between states 1 and 2\.
    Therefore, starting in state 1, it will return to state 1 after exactly \(\boxed{2}\)
    steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.1** Yes, the matrix is irreducible. The corresponding transition graph
    is a cycle, which is strongly connected.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.3** We need to check if \(\boldsymbol{\pi} P = \boldsymbol{\pi}\). Indeed,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} (0.6, 0.4)^T \begin{pmatrix} 0.4 & 0.6 \\ 0.7 & 0.3 \end{pmatrix}
    = (0.6 \cdot 0.4 + 0.4 \cdot 0.7, 0.6 \cdot 0.6 + 0.4 \cdot 0.3)^T = (0.52, 0.48)^T
    \neq (0.6, 0.4)^T, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: so \(\pi\) is not a stationary distribution of the Markov chain.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.5** Let \(\boldsymbol{\pi} = (\pi_1, \pi_2)^T\) be a stationary distribution.
    Then, we need to solve the system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 \cdot 0.5 + \pi_2 \cdot 0.5 &= \pi_1 \\ \pi_1 + \pi_2
    &= 1 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The first equation simplifies to \(\pi_1 = \pi_2\), and substituting this into
    the second equation yields \(2\pi_1 = 1\), or \(\pi_1 = \pi_2 = 0.5\). Therefore,
    \(\boldsymbol{\pi} = (0.5, 0.5)^T\) is a stationary distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.7** To verify that \(\boldsymbol{\pi}\) is a stationary distribution,
    we need to check if \(\boldsymbol{\pi} P = \boldsymbol{\pi}\). Let’s perform the
    matrix multiplication step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\boldsymbol{\pi} P = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})^T \begin{pmatrix}
    0.4 & 0.3 & 0.3 \\ 0.2 & 0.5 & 0.3 \\ 0.4 & 0.2 & 0.4 \end{pmatrix}\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= (\frac{1}{3} \cdot 0.4 + \frac{1}{3} \cdot 0.2 + \frac{1}{3} \cdot 0.4,
    \frac{1}{3} \cdot 0.3 + \frac{1}{3} \cdot 0.5 + \frac{1}{3} \cdot 0.2, \frac{1}{3}
    \cdot 0.3 + \frac{1}{3} \cdot 0.3 + \frac{1}{3} \cdot 0.4)^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= (\frac{0.4 + 0.2 + 0.4}{3}, \frac{0.3 + 0.5 + 0.2}{3}, \frac{0.3 + 0.3 +
    0.4}{3})^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})^T\)
  prefs: []
  type: TYPE_NORMAL
- en: \(= \boldsymbol{\pi}\)
  prefs: []
  type: TYPE_NORMAL
- en: The result is equal to \(\pi = (\frac{1}{3}, \frac{1}{3}, \frac{1}{3})\). Therefore,
    the uniform distribution \(\pi\) is indeed a stationary distribution for the matrix
    \(P\). Note that the transition matrix \(P\) is doubly stochastic because each
    row and each column sums to 1\. This property ensures that the uniform distribution
    is always a stationary distribution for doubly stochastic matrices, as mentioned
    in the text.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.9** Let \(\mathbf{1} = (1, 1, \ldots, 1)\) be the column vector of all
    ones. For any stochastic matrix \(P\), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P \mathbf{1} = \begin{pmatrix} p_{1,1} & \cdots & p_{1,n} \\
    \vdots & \ddots & \vdots \\ p_{n,1} & \cdots & p_{n,n} \end{pmatrix} \begin{pmatrix}
    1 \\ \vdots \\ 1 \end{pmatrix} = \begin{pmatrix} \sum_{j=1}^n p_{1,j} \\ \vdots
    \\ \sum_{j=1}^n p_{n,j} \end{pmatrix} = \begin{pmatrix} 1 \\ \vdots \\ 1 \end{pmatrix}
    = \mathbf{1}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: because each row of a stochastic matrix sums to 1\. Therefore, \(\mathbf{1}\)
    is a right eigenvector of \(P\) with eigenvalue 1.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.3.11** Let \(\boldsymbol{\pi} = (\pi_1, \pi_2, \pi_3)^T\) be the stationary
    distribution. We need to solve the system of equations:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: Write out the system of equations based on \(\boldsymbol{\pi} P = \boldsymbol{\pi}\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 \cdot 0.7 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.6 &= \pi_1
    \\ \pi_1 \cdot 0.2 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.1 &= \pi_2 \\ \pi_1 \cdot
    0.1 + \pi_2 \cdot 0.2 + \pi_3 \cdot 0.3 &= \pi_3 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Rearrange the equations to have zero on the right-hand side.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 \cdot 0.7 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.6 - \pi_1
    &= 0 \\ \pi_1 \cdot 0.2 + \pi_2 \cdot 0.4 + \pi_3 \cdot 0.1 - \pi_2 &= 0 \\ \pi_1
    \cdot 0.1 + \pi_2 \cdot 0.2 + \pi_3 \cdot 0.3 - \pi_3 &= 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: Simplify the equations.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.3\pi_1 + 0.4\pi_2 + 0.6\pi_3 &= 0 \\ 0.2\pi_1 - 0.6\pi_2
    + 0.1\pi_3 &= 0 \\ 0.1\pi_1 + 0.2\pi_2 - 0.7\pi_3 &= 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: Use the condition \(\pi_1 + \pi_2 + \pi_3 = 1\) to eliminate one variable,
    say \(\pi_3\).'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_3 = 1 - \pi_1 - \pi_2 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: Substitute the expression for \(\pi_3\) into the simplified equations
    from Step 3.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.3\pi_1 + 0.4\pi_2 + 0.6(1 - \pi_1 - \pi_2) &= 0 \\ 0.2\pi_1
    - 0.6\pi_2 + 0.1(1 - \pi_1 - \pi_2) &= 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 6: Simplify the equations further.'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.9\pi_1 - 0.2\pi_2 &= -0.6 \\ 0.1\pi_1 - 0.7\pi_2 &= -0.1
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 7: Solve the linear system of two equations with two unknowns using any
    suitable method (e.g., substitution, elimination, or matrix inversion).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the substitution method: From the second equation, express \(\pi_1\)
    in terms of \(\pi_2\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_1 = 7\pi_2 - 1 \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Substitute this into the first equation:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} -0.9(7\pi_2 - 1) - 0.2\pi_2 &= -0.6 \\ -6.3\pi_2 + 0.9 - 0.2\pi_2
    &= -0.6 \\ -6.5\pi_2 &= -1.5 \\ \pi_2 &= \frac{3}{13} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, substitute \(\pi_2 = \frac{3}{13}\) back into the expression for \(\pi_1\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_1 &= 7 \cdot \frac{3}{13} - 1 \\ &= \frac{21}{13} - \frac{13}{13}
    \\ &= \frac{8}{13} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, use \(\pi_1 + \pi_2 + \pi_3 = 1\) to find \(\pi_3\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \pi_3 &= 1 - \pi_1 - \pi_2 \\ &= 1 - \frac{8}{13} - \frac{3}{13}
    \\ &= \frac{2}{13} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the stationary distribution is \((\frac{8}{13}, \frac{3}{13}, \frac{2}{13})^T\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.4.1** Given the transition matrix of a Markov chain:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0.2 & 0.8 \\ 0.6 & 0.4 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: determine if the chain is lazy.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.4.3** Given a Markov chain with transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0.4 & 0.6 \\ 0.7 & 0.3 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and initial distribution \(\boldsymbol{\mu} = (0.2, 0.8)^T\), compute \(\lim_{t
    \to \infty} \boldsymbol{\mu} P^t\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.1** The degree matrix \(D\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} D = \begin{pmatrix} 2 & 0 & 0 & 0 \\ 0 & 3 & 0 & 0 \\ 0 & 0
    & 3 & 0 \\ 0 & 0 & 0 & 2 \\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The degrees are computed by summing the entries in each row of \(A\).
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.3** A stochastic matrix has rows that sum to 1\. The rows of \(P\) sum
    to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{2} + \frac{1}{2} = 1, \quad \frac{1}{3} + \frac{1}{3} + \frac{1}{3}
    = 1, \quad \frac{1}{3} + \frac{1}{3} + \frac{1}{3} = 1, \quad \frac{1}{2} + \frac{1}{2}
    = 1\. \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, \(P\) is stochastic.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.5**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = D^{-1}A = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0
    \\ 0 & 0 & 1/2 & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 0 & 1 & 0 &
    0 \\ 0 & 0 & 1 & 0 \\ 1 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0 \end{pmatrix} = \begin{pmatrix}
    0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 1/2 & 0 & 0 & 1/2 \\ 0 & 0 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.7** First, compute the transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = D^{-1}A = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 0 & 1/2 & 0 &
    0 \\ 0 & 0 & 1/2 & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 0 & 1 & 0
    & 0 \\ 1 & 0 & 1 & 0 \\ 0 & 1 & 0 & 1 \\ 0 & 0 & 1 & 0 \end{pmatrix} = \begin{pmatrix}
    0 & 1 & 0 & 0 \\ 1/2 & 0 & 1/2 & 0 \\ 0 & 1/2 & 0 & 1/2 \\ 0 & 0 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, compute the modified transition matrix with the damping factor:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \alpha P + (1-\alpha)\frac{1}{4}\mathbf{1}\mathbf{1}^T =
    0.8P + 0.2\begin{pmatrix} 1/4 & 1/4 & 1/4 & 1/4 \\ 1/4 & 1/4 & 1/4 & 1/4 \\ 1/4
    & 1/4 & 1/4 & 1/4 \\ 1/4 & 1/4 & 1/4 & 1/4 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.9** First, compute the transition matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0 & 1 & 0 & 0 & 0 \\ 0 & 0 & 1 & 0 & 0 \\
    1/2 & 0 & 0 & 1/2 & 0 \\ 1/2 & 0 & 0 & 0 & 1/2 \\ 0 & 0 & 0 & 1 & 0 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, compute the modified transition matrix with the damping factor:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = 0.9P + 0.1\begin{pmatrix} 1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\
    1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\ 1/5 & 1/5 & 1/5 & 1/5 & 1/5 \\ 1/5 & 1/5 & 1/5
    & 1/5 & 1/5 \\ 1/5 & 1/5 & 1/5 & 1/5 & 1/5 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.11** The new adjacency matrix \(A''\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A' = \begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 1 & 0 \\ 1 & 0
    & 0 & 1 \\ 0 & 0 & 0 & 1 \\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: A self-loop is added to vertex 4.
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.5.13** The modified transition matrix \(Q\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ Q = \alpha P + (1 - \alpha) \frac{1}{n} \mathbf{1}\mathbf{1}^T = 0.85 P +
    0.15 \frac{1}{4} \mathbf{1}\mathbf{1}^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the values from \(P\) and \(\mathbf{1}\mathbf{1}^T\), we get:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} Q = \begin{pmatrix} 0.0375 & 0.8875 & 0.0375 & 0.0375 \\ 0.0375
    & 0.0375 & 0.8875 & 0.0375 \\ 0.4625 & 0.0375 & 0.0375 & 0.4625 \\ 0.0375 & 0.0375
    & 0.0375 & 0.8875 \\ \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.1** The acceptance probability is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min\left\{1, \frac{\pi_1}{\pi_2}\frac{Q(1, 2)}{Q(2, 1)}\right\} = \min\left\{1,
    \frac{0.1}{0.2}\frac{0.5}{0.5}\right\} = \frac{1}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.3** Since the proposal chain is symmetric, the acceptance probability
    simplifies to $\( \min\left\{1, \frac{\pi_2}{\pi_1}\right\} = \min\left\{1, \frac{0.2}{0.1}\right\}
    = 1. \)$'
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.5** The acceptance probability is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min \left\{ 1, \frac{\pi(y)}{\pi(x)} \frac{Q(y, x)}{Q(x, y)} \right\} =
    \min \left\{ 1, \frac{e^{-9/2}}{e^{-2}} \right\} = e^{-5/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.7** The conditional probability is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_1^v(1|\mathbf{v}_{-1}, \mathbf{h}) = \sigma\left(\sum_{j=1}^2 w_{1,j}h_j
    + b_1\right) = \sigma(1\cdot 1 + (-1)\cdot 0 + 1) = \sigma(2) \approx 0.881. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.9** The energy is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathcal{E}(\mathbf{v}, \mathbf{h}) = -\mathbf{v}^T W \mathbf{h}
    = -\begin{pmatrix} 1 & 0 \end{pmatrix} \begin{pmatrix} 1 & -2 \\ 3 & 0 \end{pmatrix}
    \begin{pmatrix} 1 \\ 1 \end{pmatrix} = 1. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E7.6.11** The conditional mean for the hidden units is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[h_j | \mathbf{v}] = \sigma \left( \sum_{i} W_{ij} v_i + c_j \right).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(h_1\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[h_1 | \mathbf{v}] = \sigma (0.5 \cdot 1 + 0.3 \cdot 0 - 0.6 \cdot
    1 + 0.1) = \sigma (0.5 - 0.6 + 0.1) = \sigma (0) = 0.5. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For \(h_2\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[h_2 | \mathbf{v}] = \sigma (-0.2 \cdot 1 + 0.8 \cdot 0 + 0.1 \cdot
    1 - 0.3) = \sigma (-0.2 + 0.1 - 0.3) = \sigma (-0.4) = \frac{1}{1 + e^{0.4}} \approx
    0.4013. \]
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define a discrete-time Markov chain and its state space, initial distribution,
    and transition probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify Markov chains in real-world scenarios, such as weather patterns or
    random walks on graphs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct the transition matrix for a time-homogeneous Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct the transition graph of a Markov chain from its transition matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that the transition matrix of a Markov chain is a stochastic matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Markov property to calculate the probability of specific sequences
    of events in a Markov chain using the distribution of a sample path.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the marginal distribution of a Markov chain at a specific time using
    matrix powers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use simulation to generate sample paths of a Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define a stationary distribution of a finite-state, discrete-time, time-homogeneous
    Markov chain and express the defining condition in matrix form.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the relationship between stationary distributions and left eigenvectors
    of the transition matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine whether a given probability distribution is a stationary distribution
    for a given Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify irreducible Markov chains and explain their significance in the context
    of stationary distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the main theorem on the existence and uniqueness of a stationary distribution
    for an irreducible Markov chain.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the stationary distribution of a Markov chain numerically using either
    eigenvalue methods or by solving a linear system of equations, including the “Replace
    an Equation” method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the concepts of aperiodicity and weak laziness for finite-space discrete-time
    Markov chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: State the Convergence to Equilibrium Theorem and the Ergodic Theorem for irreducible
    Markov chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verify the Ergodic Theorem by simulating a Markov chain and comparing the frequency
    of visits to each state with the stationary distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of coupling and its role in proving the Convergence to Equilibrium
    Theorem for irreducible, weakly lazy Markov chains.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define random walks on directed and undirected graphs, and express their transition
    matrices in terms of the adjacency matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the stationary distribution of a random walk on an undirected graph
    and explain its relation to degree centrality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of reversibility and its connection to the stationary distribution
    of a random walk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the PageRank algorithm and its interpretation as a modified random
    walk on a directed graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the PageRank vector by finding the stationary distribution of the modified
    random walk using power iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the PageRank algorithm to real-world datasets to identify central nodes
    in a network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of Personalized PageRank and how it differs from the standard
    PageRank algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify the PageRank algorithm to implement Personalized PageRank and interpret
    the results based on the chosen teleportation distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the concept of Markov Chain Monte Carlo (MCMC) and its application
    in sampling from complex probability distributions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the Metropolis-Hastings algorithm, including the proposal distribution
    and acceptance-rejection steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate acceptance probabilities in the Metropolis-Hastings algorithm for
    a given target and proposal distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove the correctness of the Metropolis-Hastings algorithm by showing that the
    resulting Markov chain is irreducible and reversible with respect to the target
    distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the Gibbs sampling algorithm for a given probabilistic model, such
    as a Restricted Boltzmann Machine (RBM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the conditional independence properties of RBMs and their role in Gibbs
    sampling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.2\. Additional sections[#](#additional-sections "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 7.8.2.1\. Random walk on a weighted graph[#](#random-walk-on-a-weighted-graph
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous definitions extend naturally to the weighted case. Again we allow
    loops, i.e., self-weights \(w_{i,i} > 0\). For a weighted graph \(G\), recall
    that the degree of a vertex is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \delta(i) = \sum_{j} w_{i,j}, \]
  prefs: []
  type: TYPE_NORMAL
- en: which includes the self-weight \(w_{i,i}\), and where we use the convention
    that \(w_{i,j} = 0\) if \(\{i,j\} \notin E\). Recall also that \(w_{i,j} = w_{j,i}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Random Walk on a Weighted Graph)** Let \(G = (V,E,w)\) be
    a weighted graph with positive edge weights. Assume all vertices have a positive
    degree. A random walk on \(G\) is a time-homogeneous Markov chain \((X_t)_{t \geq
    0}\) with state space \(\mathcal{S} = V\) and transition probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{w_{i,j}}{\sum_{k} w_{i,k}},
    \qquad \forall i,j \in V. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Once again, it is easily seen that the transition matrix of random walk on \(G\)
    satisfying the conditions of the definition above is \( P = D^{-1} A, \) where
    \(D = \mathrm{diag}(A \mathbf{1})\) is the degree matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(A Weighted Graph)** Here is another example. Consider the following
    adjacency matrix on \(5\) vertices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: It is indeed a symmetric matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We define a graph from its adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: To draw it, we first define edge labels by creating a dictionary that assigns
    to each edge (as a tuple) its weight. Here `G.edges.data('weight')` (see [`G.edges`](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.edges.html))
    iterates through the edges `(u,v)` and includes their weight as the third entry
    of the tuple `(u,v,w)`. Then we use the function [`networkx.draw_networkx_edge_labels()`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx_edge_labels.html#networkx.drawing.nx_pylab.draw_networkx_edge_labels)
    to add the weights as edge labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/696132b74026ec3eb91e20e630468fd57042dca1726e5be92d5b48b5435bc3dd.png](../Images/bcf13dbbd96da2dcf4b07ed3618e6860.png)'
  prefs: []
  type: TYPE_IMG
- en: The transition matrix of the random walk on this graph can be computed using
    the lemma above. We first compute the degree matrix, then apply the formula.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This is indeed a stochastic matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Irreducibility in Undirected Case)** Let \(G = (V,E,w)\) be a
    graph with positive edge weights. Assume all vertices have a positive degree.
    Random walk on \(G\) is irreducible if and only if \(G\) is connected. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Stationary Distribution on a Graph)** Let \(G = (V,E,w)\) be
    a graph with positive edge weights. Assume further that \(G\) is connected. Then
    the unique stationary distribution of random walk on \(G\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in
    V. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(A Weighted Graph, continued)** Going back to our weighted graph
    example, we use the previous theorem to compute the stationary distribution. Note
    that the graph is indeed connected so the stationary distribution is unique. We
    have already computed the degrees.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We compute \(\sum_{i \in V} \delta(i)\) next.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the stationary distribution is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We check stationarity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: A random walk on a weighted undirected graph is reversible. Vice versa, it turns
    out that any reversible chain can be seen as a random walk on an appropriately
    defined weighted undirected graph. See the exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.2.2\. Spectral techniques for random walks on graphs[#](#spectral-techniques-for-random-walks-on-graphs
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we use techniques from spectral graph theory to analyze random
    walks on graphs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Applying the spectral theorem via the normalized Laplacian** We have seen
    how to compute the unique stationary distribution \(\bpi\) of random walk on a
    connected weighted (undirected) graph. Recall that \(\bpi\) is a (left) eigenvector
    of \(P\) with eigenvalue \(1\) (i.e., \(\bpi P = \bpi\)). In general, however,
    the matrix \(P\) is *not* symmetric in this case (see the previous example) -
    even though the adjacency matrix is. So we cannot apply the spectral theorem to
    get the rest of the eigenvectors - if they even exist. But, remarkably, a symmetric
    matrix with the a closely related spectral decomposition is hiding in the background.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the normalized Laplacian of a weighted graph \(G = (V,E,w)\) with
    adjacency matrix \(A\) and degree matrix \(D\) is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = I - D^{-1/2} A D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in the weighted case, the degree is defined as \(\delta(i) = \sum_{j:\{i,j\}
    \in E} w_{i,j}\). Because it is symmetric and positive semi-definite, we can write
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = \sum_{i=1}^n \eta_i \mathbf{z}_i \mathbf{z}_i^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\mathbf{z}_i\)s are orthonormal eigenvectors of \(\mathcal{L}\)
    and the eigenvalues satisfy \(0 \leq \eta_1 \leq \eta_2 \leq \cdots \leq \eta_n\).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, \(D^{1/2} \mathbf{1}\) is an eigenvector of \(\mathcal{L}\) with eigenvalue
    \(0\). So \(\eta_1 = 0\) and we set
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathbf{z}_1)_i = \left(\frac{D^{1/2} \mathbf{1}}{\|D^{1/2} \mathbf{1}\|_2}\right)_i
    = \sqrt{\frac{\delta(i)}{\sum_{i\in V} \delta(i)}}, \quad \forall i \in [n], \]
  prefs: []
  type: TYPE_NORMAL
- en: which makes \(\mathbf{z}_1\) into a unit norm vector.
  prefs: []
  type: TYPE_NORMAL
- en: We return to the eigenvectors of \(P\). When a matrix \(A \in \mathbb{R}^{n
    \times n}\) is diagonalizable, it has an eigendecomposition of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = Q \Lambda Q^{-1}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Lambda\) is a diagonal matrix whose diagonal entries are the eigenvalues
    of \(A\). The columns of \(Q\) are the eigenvectors of \(A\) and they form a basis
    of \(\mathbb{R}^n\). Unlike the symmetric case, however, the eigenvectors need
    not be orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Eigendecomposition of Random Walk on a Graph)** Let \(G = (V,E,w)\)
    be a graph with positive edge weights and no isolated vertex, and with degree
    matrix \(D\). Let \(P \in \mathbb{R}^{n \times n}\) be the transition matrix of
    random walk on \(G\). Let \(\mathbf{z}_1,\ldots,\mathbf{z}_n \in \mathbb{R}^n\)
    and \(0 \leq \eta_1 \leq \cdots \eta_n \leq 2\) be the eigenvectors and eigenvalues
    of the normalized Laplacian. Then \(P\) has the following eigendecomposition'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P &= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where the columns of \(Z\) are the \(\mathbf{z}_i\)s and \(H\) is a diagonal
    matrix with the \(\eta_i\)s on its diagonal. This can also be written as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P &= \mathbf{1}\bpi + \sum_{i=2}^n \lambda_i D^{-1/2} \mathbf{z}_i
    \mathbf{z}_i^T D^{1/2}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \frac{\delta(i)}{\sum_{j\in V} \delta(j)} \quad \text{and} \quad
    \lambda_i = 1- \eta_i, \qquad i =1,\ldots,n. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We write \(\mathcal{L}\) in terms of \(P\). Recall that \(P = D^{-1}
    A\). Hence'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = I - D^{1/2} P D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Rearranging this becomes
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = I - D^{-1/2} \mathcal{L} D^{1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence for all \(i=1,\ldots,n\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P (D^{-1/2} \mathbf{z}_i) &= (I - D^{-1/2} \mathcal{L} D^{1/2})\,(D^{-1/2}
    \mathbf{z}_i)\\ &= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \mathcal{L} D^{1/2} (D^{-1/2}
    \mathbf{z}_i)\\ &= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \mathcal{L} \mathbf{z}_i\\
    &= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \eta_i \mathbf{z}_i\\ &= (1 - \eta_i) (D^{-1/2}
    \mathbf{z}_i). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(P\) is a transition matrix, all its eigenvalues are bounded in absolute
    value by \(1\). So \(|1-\eta_i|\leq 1\), which implies \(0 \leq \eta_i \leq 2\).
  prefs: []
  type: TYPE_NORMAL
- en: We also note that
  prefs: []
  type: TYPE_NORMAL
- en: \[ (D^{-1/2} Z) (D^{1/2} Z)^T = D^{-1/2} Z Z^T D^{1/2} = D^{-1/2} D^{1/2} =
    I, \]
  prefs: []
  type: TYPE_NORMAL
- en: by the orthonormality of the eigenvectors of the normalized Laplacian. So the
    columns of \(D^{-1/2} Z\), i.e, \(D^{-1/2} \mathbf{z}_i\) for \(i=1,\ldots,n\),
    are linearly independent and
  prefs: []
  type: TYPE_NORMAL
- en: \[ (D^{-1/2} Z)^{-1} = (D^{1/2} Z)^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: That gives the first claim.
  prefs: []
  type: TYPE_NORMAL
- en: To get the second claim, we first note that (for similar calculations, see the
    definition of an SVD)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P &= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1}\\ &= (D^{-1/2} Z)(I
    - H) (D^{1/2} Z)^T\\ &= D^{-1/2} Z (I - H) Z^T D^{1/2}\\ &= \sum_{i=1}^n \lambda_i
    D^{-1/2} \mathbf{z}_i \mathbf{z}_i^T D^{1/2}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\lambda_i = 1- \eta_i\).
  prefs: []
  type: TYPE_NORMAL
- en: We then use the expression for \(\mathbf{z}_1\) above. We have
  prefs: []
  type: TYPE_NORMAL
- en: \[ D^{-1/2} \mathbf{z}_1 = D^{-1/2} \frac{D^{1/2}\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}
    = \frac{\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: while
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}_i^T D^{1/2} = (D^{1/2} \mathbf{z}_1)^T = \left(D^{1/2} \frac{D^{1/2}\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T
    = \left(\frac{D \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} D^{-1/2} \mathbf{z}_1 \mathbf{z}_1^T D^{1/2} &= \frac{\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}
    \left(\frac{D \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T\\ &= \mathbf{1} \left(\frac{D
    \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2^2}\right)^T\\ &= \mathbf{1} \left(\frac{D
    \mathbf{1}}{\|D \mathbf{1}\|_1}\right)^T\\ &= \mathbf{1} \bpi. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the second claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: If \(G\) is connected and \(w_{i,i} > 0\) for all \(i\), then the chain is irreducible
    and lazy. In that case, there is a unique eigenvalue \(1\) and \(-1\) is not an
    eigenvalue, so we must have \(0 < \eta_2 \leq \cdots \leq \eta_n < 2\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Limit theorems revisited** The *Convergence to Equilibrium Theorem* implies
    that in the irreducible, aperiodic case'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bmu P^t \to \bpi, \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(t \to +\infty\), for any initial distribution \(\bmu\) and the unique stationary
    distribution \(\bpi\). Here we give a simpler proof for random walk on a graph
    (or more generally a reversible chain), with the added bonus of a convergence
    rate. This follows from the same argument we used in the *Power Iteration Lemma*.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Spectral Gap)** Let \(G = (V,E,w)\) be a graph with positive
    edge weights and no isolated vertex. Let \(P \in \mathbb{R}^{n \times n}\) be
    the transition matrix of random walk on \(G\). The absolute spectral gap of \(G\)
    is defined as \(\gamma_{\star} = 1 - \lambda_{\star}\) where'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{\star} = \max\{|\lambda|\,:\, \text{$\lambda$ is an eigenvalue of
    $P$, $\lambda \neq 1$} \}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Convergence to Equilibrium: Reversible Case)** Let \(G = (V,E,w)\)
    be a connected graph with positive edge weights and \(w_{x,x} > 0\) for all \(x
    \in V\). Let \(P \in \mathbb{R}^{n \times n}\) be the transition matrix of random
    walk on \(G\) and \(\bpi\) its unique stationary distribution. Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bmu P^t \to \bpi, \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(t \to +\infty\) for any initial distribution \(\bmu\). Moreover,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left|P^t_{x,y} - \pi_y\right| \leq \gamma_\star^t \sqrt{\frac{\bar{\delta}}{\underline{\delta}}},
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bar{\delta} = \max_x \delta(x)\), \(\underline{\delta} = \min_x \delta(x)\)
    and \(\gamma_\star\) is the absolute spectral gap. \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Similarly to the *Power Iteration Lemma*, using the *Eigendecomposition
    of Random Walk on a Graph* we get'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P^2 &= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1}(D^{-1/2} Z)(I
    - H) (D^{-1/2} Z)^{-1}\\ &= (D^{-1/2} Z)(I - H)^2 (D^{-1/2} Z)^{-1}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By induction,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P^t &= (D^{-1/2} Z)(I - H)^t (D^{-1/2} Z)^{-1}\\ &= \sum_{i=1}^n
    \lambda_i^t (D^{-1/2} \mathbf{z}_i) (D^{1/2} \mathbf{z}_i)^T\\ &= \mathbf{1} \bpi
    + \sum_{i=2}^n \lambda_i^t (D^{-1/2} \mathbf{z}_i) (D^{1/2} \mathbf{z}_i)^T, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by calculations similar to the proof of the *Eigendecomposition of Random Walk
    on a Graph*.
  prefs: []
  type: TYPE_NORMAL
- en: In the irreducible, lazy case, for \(i=2,\ldots,n\), \(\lambda_i^t \to 0\) as
    \(t \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, \(|\lambda_i| \leq (1-\gamma_\star)\) for all \(i=2,\ldots,n\). Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|P^t_{x,y} - \pi_y\right| &= \sum_{i=2}^n \lambda_i^t
    \delta(x)^{-1/2}(\mathbf{z}_i)_x (\mathbf{z}_i)_y \delta(y)^{1/2}\\ &\leq (1-\gamma_\star)^t
    \sqrt{\frac{\delta(y)}{\delta(x)}} \sum_{i=2}^n |(\mathbf{z}_i)_x (\mathbf{z}_i)_y|.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We then use *Cauchy-Schwarz* and the fact that \(Z Z^T = I\) (as \(Z\) is an
    orthogonal matrix), which implies \(\sum_{i=1}^n (\mathbf{z}_i)_x^2 = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: We get that the above is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq (1-\gamma_\star)^t \sqrt{\frac{\delta(y)}{\delta(x)}}
    \sum_{i=2}^n (\mathbf{z}_i)_x^2 \sum_{i=2}^n (\mathbf{z}_i)_y^2\\ &\leq (1-\gamma_\star)^t
    \sqrt{\frac{\delta(y)}{\delta(x)}}\\ &\leq (1-\gamma_\star)^t \sqrt{\frac{\bar{\delta}}{\underline{\delta}}}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 'We record an immediate corollary that will be useful next. Let \(f : V \to
    \mathbb{R}\) be a function over the vertices. Define the (column) vector \(\mathbf{f}
    = (f(1),\ldots,f(n))^T\) and note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi \mathbf{f} = \sum_{x \in V} \pi_x f(x). \]
  prefs: []
  type: TYPE_NORMAL
- en: It will be convenient to use to \(\ell_\infty\)-norm. For a vector \(\mathbf{x}
    = (x_1,\ldots,x_n)^T\), we let \(\|\mathbf{x}\|_\infty = \max_{i \in [n]} |x_i|\).
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** For any initial distribution \(\bmu\) and any \(t\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\,\E[f(X_t)] - \bpi \mathbf{f}\,\right| \leq (1-\gamma_\star)^t
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Time Marginals Theorem*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\,\E[f(X_t)] - \bpi \mathbf{f}\,\right| &= \left|\,\sum_{x}
    \sum_y \mu_x (P^t)_{x,y} f(y) - \sum_{y} \pi_y f(y)\,\right|. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(\sum_{x} \mu_x = 1\), the right-hand side is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \left|\,\sum_{x} \sum_y \mu_x (P^t)_{x,y} f(y) - \sum_x
    \sum_{y} \mu_x \pi_y f(y)\,\right|\\ &\leq \sum_{x} \mu_x \sum_y \left| (P^t)_{x,y}
    - \pi_y \right| |f(y)|, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by the triangle inequality.
  prefs: []
  type: TYPE_NORMAL
- en: Now by the theorem this is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq \sum_{x} \mu_x \sum_y (1-\gamma_\star)^t \frac{\pi_y}{\pi_{\min{}}}|f(y)|\\
    &= (1-\gamma_\star)^t \frac{1}{\pi_{\min{}}} \sum_{x} \mu_x \sum_y \pi_y |f(y)|\\
    &\leq (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We also prove a version of the *Ergodic Theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Ergodic Theorem: Reversible Case)** Let \(G = (V,E,w)\) be a
    connected graph with positive edge weights and \(w_{x,x} > 0\) for all \(x \in
    V\). Let \(P \in \mathbb{R}^{n \times n}\) be the transition matrix of random
    walk on \(G\) and \(\bpi\) its unique stationary distribution. Let \(f : V \to
    \mathbb{R}\) be a function over the vertices. Then for any initial distribution
    \(\bmu\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{T} \sum_{t=1}^T f(X_{t}) \to \sum_{x \in V} \pi_x f(x), \]
  prefs: []
  type: TYPE_NORMAL
- en: in probability as \(T \to +\infty\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use *Chebyshev’s Inequality*. By the *Convergence Theorem:
    Reversible Case*, the expectation converges to the limit. To bound the variance,
    we use the *Eigendecomposition of Random Walk on a Graph*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We use *Chebyshev’s Inequality*, similarly to our proof of the *Weak
    Law of Large Numbers*. However, unlike that case, here the terms in the sum are
    not independent are require some finessing. Define again the (column) vector \(\mathbf{f}
    = (f(1),\ldots,f(n))\). Then the limit can be written as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{x \in V} \pi_x f(x) = \bpi \mathbf{f}. \]
  prefs: []
  type: TYPE_NORMAL
- en: By the corollary, the expectation of the sum can be bounded as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\,\E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] -
    \bpi \mathbf{f}\,\right| &\leq \frac{1}{T} \sum_{t=1}^T\left|\E[f(X_{t})] - \bpi
    \mathbf{f}\right|\\ &\leq \frac{1}{T} \sum_{t=1}^T (1-\gamma_\star)^t \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty\\ &\leq \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty \frac{1}{T}
    \sum_{t=0}^{+\infty} (1-\gamma_\star)^t\\ &= \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty
    \gamma_\star^{-1}\frac{1}{T} \to 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: Next we bound the variance of the sum. By the *Variance of a Sum*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{Var}\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] = \frac{1}{T^2}
    \sum_{t=1}^T \mathrm{Var}[f(X_{t})] + \frac{2}{T^2} \sum_{1 \leq s < t\leq T}
    \mathrm{Cov}[f(X_{s}),f(X_{t})]. \]
  prefs: []
  type: TYPE_NORMAL
- en: We bound the variance and covariance separately using the *Eigendecomposition
    of Random Walk on a Graph*.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain convergence, a trivial bound on the variance suffices. Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 \leq \mathrm{Var}[f(X_{t})] \leq \E[f(X_{t})^2] \leq \|\mathbf{f}\|_\infty^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 \leq \frac{1}{T^2} \sum_{t=1}^T \mathrm{Var}[f(X_{t})] \leq \frac{T \|\mathbf{f}\|_\infty^2}{T^2}
    \to 0, \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: Bounding the covariance requires a more delicate argument. Fix \(1 \leq s <
    t\leq T\). The trick is to condition on \(X_s\) and use the *Markov Property*.
    By definition of the covariance and the *Law of Total Expectation*,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\mathrm{Cov}[f(X_{s}),f(X_{t})]\\ &= \E\left[(f(X_{s}) - \E[f(X_{s})])
    (f(X_{t}) - \E[f(X_{t})])\right]\\ &= \sum_{x} \E\left[(f(X_{s}) - \E[f(X_{s})])
    (f(X_{t}) - \E[f(X_{t})])\,\middle|\,X_s = x\right]\,\P[X_s = x]\\ &= \sum_{x}
    \E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right](f(x) - \E[f(X_{s})])
    \,\P[X_s = x]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We now use the time-homogeneity of the chain to note that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\\
    &= \E\left[f(X_{t})\,\middle|\,X_0 = x\right] - \E[f(X_{t})]\\ &= \E\left[f(X_{t-s})\,\middle|\,X_0
    = x\right] - \E[f(X_{t})]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We now use the corollary. This expression in absolute value is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\left|\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\right|\\
    &= \left|\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right] - \E[f(X_{t})]\right|\\
    &= \left|(\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right] - \bpi \mathbf{f}) - (\E[f(X_{t})]
    - \bpi \mathbf{f})\right|\\ &\leq \left|\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right]
    - \bpi \mathbf{f}\right| + \left|\E[f(X_{t})] - \bpi \mathbf{f}\right|\\ &\leq
    (1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Plugging back above,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\left|\mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\ &\leq \sum_{x}
    \left|\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\right| \left|f(x)
    - \E[f(X_{s})]\right| \,\P[X_s = x]\\ &\leq \sum_{x} ((1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty)
    \left|f(x) - \E[f(X_{s})]\right| \,\P[X_s = x]\\ &\leq ((1-\gamma_\star)^{t-s}
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty) \sum_{x} 2 \|\mathbf{f}\|_\infty\P[X_s = x]\\ &\leq 4 (1-\gamma_\star)^{t-s}
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \((1-\gamma_\star)^t \leq (1-\gamma_\star)^{t-s}\) since
    \((1-\gamma_\star) < 1\) and \(t -s \leq t\).
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the sum over the covariances, the previous bound gives
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\left|\frac{2}{T^2} \sum_{1 \leq s < t\leq T} \mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\
    &\leq \frac{2}{T^2} \sum_{1 \leq s < t\leq T} \left|\mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\
    &\leq \frac{2}{T^2} \sum_{1 \leq s < t\leq T} 4 (1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the sum we make the change of variable \(h = t - s\) to get that
    the previous expression is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq 4 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2\frac{2}{T^2}
    \sum_{1 \leq s \leq T} \sum_{h=1}^{T-s} (1-\gamma_\star)^{h}\\ &\leq 4 \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty^2\frac{2}{T^2} \sum_{1 \leq s \leq T} \sum_{h=0}^{+\infty}
    (1-\gamma_\star)^{h}\\ &= 4 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2\frac{2}{T^2}
    \sum_{1 \leq s \leq T} \frac{1}{\gamma_\star}\\ &= 8 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T} \to 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: We have shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathrm{Var}\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right]
    \leq \|\mathbf{f}\|_\infty^2 \frac{1}{T} + 8 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T} \leq 9 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: For any \(\varepsilon > 0\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \bpi \mathbf{f}\,\right|
    \geq \varepsilon \right]\\ &= \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t})
    - \E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] + \left(\E\left[\frac{1}{T}
    \sum_{t=1}^T f(X_{t})\right] - \bpi \mathbf{f} \right)\,\right| \geq \varepsilon
    \right]\\ &\leq \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \E\left[\frac{1}{T}
    \sum_{t=1}^T f(X_{t})\right]\,\right| + \left|\,\E\left[\frac{1}{T} \sum_{t=1}^T
    f(X_{t})\right] - \bpi \mathbf{f} \,\right| \geq \varepsilon \right]\\ &\leq \P\left[\left|\,\frac{1}{T}
    \sum_{t=1}^T f(X_{t}) - \E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right]\,\right|
    \geq \varepsilon - \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty \gamma_\star^{-1}\frac{1}{T}\right].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We can now apply *Chebyshev* to get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \bpi \mathbf{f}\,\right|
    \geq \varepsilon \right] &\leq \frac{9 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T}}{(\varepsilon - \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty
    \gamma_\star^{-1}\frac{1}{T})^2} \to 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.2.1\. Random walk on a weighted graph[#](#random-walk-on-a-weighted-graph
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous definitions extend naturally to the weighted case. Again we allow
    loops, i.e., self-weights \(w_{i,i} > 0\). For a weighted graph \(G\), recall
    that the degree of a vertex is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \delta(i) = \sum_{j} w_{i,j}, \]
  prefs: []
  type: TYPE_NORMAL
- en: which includes the self-weight \(w_{i,i}\), and where we use the convention
    that \(w_{i,j} = 0\) if \(\{i,j\} \notin E\). Recall also that \(w_{i,j} = w_{j,i}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Random Walk on a Weighted Graph)** Let \(G = (V,E,w)\) be
    a weighted graph with positive edge weights. Assume all vertices have a positive
    degree. A random walk on \(G\) is a time-homogeneous Markov chain \((X_t)_{t \geq
    0}\) with state space \(\mathcal{S} = V\) and transition probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{i,j} = \P[X_{t+1} = j\,|\,X_{t} = i] = \frac{w_{i,j}}{\sum_{k} w_{i,k}},
    \qquad \forall i,j \in V. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: Once again, it is easily seen that the transition matrix of random walk on \(G\)
    satisfying the conditions of the definition above is \( P = D^{-1} A, \) where
    \(D = \mathrm{diag}(A \mathbf{1})\) is the degree matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(A Weighted Graph)** Here is another example. Consider the following
    adjacency matrix on \(5\) vertices.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: It is indeed a symmetric matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: We define a graph from its adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: To draw it, we first define edge labels by creating a dictionary that assigns
    to each edge (as a tuple) its weight. Here `G.edges.data('weight')` (see [`G.edges`](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.edges.html))
    iterates through the edges `(u,v)` and includes their weight as the third entry
    of the tuple `(u,v,w)`. Then we use the function [`networkx.draw_networkx_edge_labels()`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx_edge_labels.html#networkx.drawing.nx_pylab.draw_networkx_edge_labels)
    to add the weights as edge labels.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/696132b74026ec3eb91e20e630468fd57042dca1726e5be92d5b48b5435bc3dd.png](../Images/bcf13dbbd96da2dcf4b07ed3618e6860.png)'
  prefs: []
  type: TYPE_IMG
- en: The transition matrix of the random walk on this graph can be computed using
    the lemma above. We first compute the degree matrix, then apply the formula.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This is indeed a stochastic matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Irreducibility in Undirected Case)** Let \(G = (V,E,w)\) be a
    graph with positive edge weights. Assume all vertices have a positive degree.
    Random walk on \(G\) is irreducible if and only if \(G\) is connected. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Stationary Distribution on a Graph)** Let \(G = (V,E,w)\) be
    a graph with positive edge weights. Assume further that \(G\) is connected. Then
    the unique stationary distribution of random walk on \(G\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \frac{\delta(i)}{\sum_{i \in V} \delta(i)}, \qquad \forall i \in
    V. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(A Weighted Graph, continued)** Going back to our weighted graph
    example, we use the previous theorem to compute the stationary distribution. Note
    that the graph is indeed connected so the stationary distribution is unique. We
    have already computed the degrees.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: We compute \(\sum_{i \in V} \delta(i)\) next.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the stationary distribution is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We check stationarity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: A random walk on a weighted undirected graph is reversible. Vice versa, it turns
    out that any reversible chain can be seen as a random walk on an appropriately
    defined weighted undirected graph. See the exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.2.2\. Spectral techniques for random walks on graphs[#](#spectral-techniques-for-random-walks-on-graphs
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we use techniques from spectral graph theory to analyze random
    walks on graphs.
  prefs: []
  type: TYPE_NORMAL
- en: '**Applying the spectral theorem via the normalized Laplacian** We have seen
    how to compute the unique stationary distribution \(\bpi\) of random walk on a
    connected weighted (undirected) graph. Recall that \(\bpi\) is a (left) eigenvector
    of \(P\) with eigenvalue \(1\) (i.e., \(\bpi P = \bpi\)). In general, however,
    the matrix \(P\) is *not* symmetric in this case (see the previous example) -
    even though the adjacency matrix is. So we cannot apply the spectral theorem to
    get the rest of the eigenvectors - if they even exist. But, remarkably, a symmetric
    matrix with the a closely related spectral decomposition is hiding in the background.'
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the normalized Laplacian of a weighted graph \(G = (V,E,w)\) with
    adjacency matrix \(A\) and degree matrix \(D\) is defined as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = I - D^{-1/2} A D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that in the weighted case, the degree is defined as \(\delta(i) = \sum_{j:\{i,j\}
    \in E} w_{i,j}\). Because it is symmetric and positive semi-definite, we can write
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = \sum_{i=1}^n \eta_i \mathbf{z}_i \mathbf{z}_i^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where the \(\mathbf{z}_i\)s are orthonormal eigenvectors of \(\mathcal{L}\)
    and the eigenvalues satisfy \(0 \leq \eta_1 \leq \eta_2 \leq \cdots \leq \eta_n\).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, \(D^{1/2} \mathbf{1}\) is an eigenvector of \(\mathcal{L}\) with eigenvalue
    \(0\). So \(\eta_1 = 0\) and we set
  prefs: []
  type: TYPE_NORMAL
- en: \[ (\mathbf{z}_1)_i = \left(\frac{D^{1/2} \mathbf{1}}{\|D^{1/2} \mathbf{1}\|_2}\right)_i
    = \sqrt{\frac{\delta(i)}{\sum_{i\in V} \delta(i)}}, \quad \forall i \in [n], \]
  prefs: []
  type: TYPE_NORMAL
- en: which makes \(\mathbf{z}_1\) into a unit norm vector.
  prefs: []
  type: TYPE_NORMAL
- en: We return to the eigenvectors of \(P\). When a matrix \(A \in \mathbb{R}^{n
    \times n}\) is diagonalizable, it has an eigendecomposition of the form
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = Q \Lambda Q^{-1}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\Lambda\) is a diagonal matrix whose diagonal entries are the eigenvalues
    of \(A\). The columns of \(Q\) are the eigenvectors of \(A\) and they form a basis
    of \(\mathbb{R}^n\). Unlike the symmetric case, however, the eigenvectors need
    not be orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Eigendecomposition of Random Walk on a Graph)** Let \(G = (V,E,w)\)
    be a graph with positive edge weights and no isolated vertex, and with degree
    matrix \(D\). Let \(P \in \mathbb{R}^{n \times n}\) be the transition matrix of
    random walk on \(G\). Let \(\mathbf{z}_1,\ldots,\mathbf{z}_n \in \mathbb{R}^n\)
    and \(0 \leq \eta_1 \leq \cdots \eta_n \leq 2\) be the eigenvectors and eigenvalues
    of the normalized Laplacian. Then \(P\) has the following eigendecomposition'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P &= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where the columns of \(Z\) are the \(\mathbf{z}_i\)s and \(H\) is a diagonal
    matrix with the \(\eta_i\)s on its diagonal. This can also be written as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P &= \mathbf{1}\bpi + \sum_{i=2}^n \lambda_i D^{-1/2} \mathbf{z}_i
    \mathbf{z}_i^T D^{1/2}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_i = \frac{\delta(i)}{\sum_{j\in V} \delta(j)} \quad \text{and} \quad
    \lambda_i = 1- \eta_i, \qquad i =1,\ldots,n. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We write \(\mathcal{L}\) in terms of \(P\). Recall that \(P = D^{-1}
    A\). Hence'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L} = I - D^{1/2} P D^{-1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Rearranging this becomes
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = I - D^{-1/2} \mathcal{L} D^{1/2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence for all \(i=1,\ldots,n\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P (D^{-1/2} \mathbf{z}_i) &= (I - D^{-1/2} \mathcal{L} D^{1/2})\,(D^{-1/2}
    \mathbf{z}_i)\\ &= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \mathcal{L} D^{1/2} (D^{-1/2}
    \mathbf{z}_i)\\ &= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \mathcal{L} \mathbf{z}_i\\
    &= (D^{-1/2} \mathbf{z}_i) - D^{-1/2} \eta_i \mathbf{z}_i\\ &= (1 - \eta_i) (D^{-1/2}
    \mathbf{z}_i). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(P\) is a transition matrix, all its eigenvalues are bounded in absolute
    value by \(1\). So \(|1-\eta_i|\leq 1\), which implies \(0 \leq \eta_i \leq 2\).
  prefs: []
  type: TYPE_NORMAL
- en: We also note that
  prefs: []
  type: TYPE_NORMAL
- en: \[ (D^{-1/2} Z) (D^{1/2} Z)^T = D^{-1/2} Z Z^T D^{1/2} = D^{-1/2} D^{1/2} =
    I, \]
  prefs: []
  type: TYPE_NORMAL
- en: by the orthonormality of the eigenvectors of the normalized Laplacian. So the
    columns of \(D^{-1/2} Z\), i.e, \(D^{-1/2} \mathbf{z}_i\) for \(i=1,\ldots,n\),
    are linearly independent and
  prefs: []
  type: TYPE_NORMAL
- en: \[ (D^{-1/2} Z)^{-1} = (D^{1/2} Z)^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: That gives the first claim.
  prefs: []
  type: TYPE_NORMAL
- en: To get the second claim, we first note that (for similar calculations, see the
    definition of an SVD)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P &= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1}\\ &= (D^{-1/2} Z)(I
    - H) (D^{1/2} Z)^T\\ &= D^{-1/2} Z (I - H) Z^T D^{1/2}\\ &= \sum_{i=1}^n \lambda_i
    D^{-1/2} \mathbf{z}_i \mathbf{z}_i^T D^{1/2}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\lambda_i = 1- \eta_i\).
  prefs: []
  type: TYPE_NORMAL
- en: We then use the expression for \(\mathbf{z}_1\) above. We have
  prefs: []
  type: TYPE_NORMAL
- en: \[ D^{-1/2} \mathbf{z}_1 = D^{-1/2} \frac{D^{1/2}\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}
    = \frac{\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: while
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}_i^T D^{1/2} = (D^{1/2} \mathbf{z}_1)^T = \left(D^{1/2} \frac{D^{1/2}\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T
    = \left(\frac{D \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} D^{-1/2} \mathbf{z}_1 \mathbf{z}_1^T D^{1/2} &= \frac{\mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}
    \left(\frac{D \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2}\right)^T\\ &= \mathbf{1} \left(\frac{D
    \mathbf{1}}{\|D^{1/2}\mathbf{1}\|_2^2}\right)^T\\ &= \mathbf{1} \left(\frac{D
    \mathbf{1}}{\|D \mathbf{1}\|_1}\right)^T\\ &= \mathbf{1} \bpi. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the second claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: If \(G\) is connected and \(w_{i,i} > 0\) for all \(i\), then the chain is irreducible
    and lazy. In that case, there is a unique eigenvalue \(1\) and \(-1\) is not an
    eigenvalue, so we must have \(0 < \eta_2 \leq \cdots \leq \eta_n < 2\).
  prefs: []
  type: TYPE_NORMAL
- en: '**Limit theorems revisited** The *Convergence to Equilibrium Theorem* implies
    that in the irreducible, aperiodic case'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bmu P^t \to \bpi, \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(t \to +\infty\), for any initial distribution \(\bmu\) and the unique stationary
    distribution \(\bpi\). Here we give a simpler proof for random walk on a graph
    (or more generally a reversible chain), with the added bonus of a convergence
    rate. This follows from the same argument we used in the *Power Iteration Lemma*.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Spectral Gap)** Let \(G = (V,E,w)\) be a graph with positive
    edge weights and no isolated vertex. Let \(P \in \mathbb{R}^{n \times n}\) be
    the transition matrix of random walk on \(G\). The absolute spectral gap of \(G\)
    is defined as \(\gamma_{\star} = 1 - \lambda_{\star}\) where'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{\star} = \max\{|\lambda|\,:\, \text{$\lambda$ is an eigenvalue of
    $P$, $\lambda \neq 1$} \}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Convergence to Equilibrium: Reversible Case)** Let \(G = (V,E,w)\)
    be a connected graph with positive edge weights and \(w_{x,x} > 0\) for all \(x
    \in V\). Let \(P \in \mathbb{R}^{n \times n}\) be the transition matrix of random
    walk on \(G\) and \(\bpi\) its unique stationary distribution. Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bmu P^t \to \bpi, \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(t \to +\infty\) for any initial distribution \(\bmu\). Moreover,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \left|P^t_{x,y} - \pi_y\right| \leq \gamma_\star^t \sqrt{\frac{\bar{\delta}}{\underline{\delta}}},
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bar{\delta} = \max_x \delta(x)\), \(\underline{\delta} = \min_x \delta(x)\)
    and \(\gamma_\star\) is the absolute spectral gap. \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Similarly to the *Power Iteration Lemma*, using the *Eigendecomposition
    of Random Walk on a Graph* we get'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P^2 &= (D^{-1/2} Z)(I - H) (D^{-1/2} Z)^{-1}(D^{-1/2} Z)(I
    - H) (D^{-1/2} Z)^{-1}\\ &= (D^{-1/2} Z)(I - H)^2 (D^{-1/2} Z)^{-1}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By induction,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} P^t &= (D^{-1/2} Z)(I - H)^t (D^{-1/2} Z)^{-1}\\ &= \sum_{i=1}^n
    \lambda_i^t (D^{-1/2} \mathbf{z}_i) (D^{1/2} \mathbf{z}_i)^T\\ &= \mathbf{1} \bpi
    + \sum_{i=2}^n \lambda_i^t (D^{-1/2} \mathbf{z}_i) (D^{1/2} \mathbf{z}_i)^T, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by calculations similar to the proof of the *Eigendecomposition of Random Walk
    on a Graph*.
  prefs: []
  type: TYPE_NORMAL
- en: In the irreducible, lazy case, for \(i=2,\ldots,n\), \(\lambda_i^t \to 0\) as
    \(t \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, \(|\lambda_i| \leq (1-\gamma_\star)\) for all \(i=2,\ldots,n\). Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|P^t_{x,y} - \pi_y\right| &= \sum_{i=2}^n \lambda_i^t
    \delta(x)^{-1/2}(\mathbf{z}_i)_x (\mathbf{z}_i)_y \delta(y)^{1/2}\\ &\leq (1-\gamma_\star)^t
    \sqrt{\frac{\delta(y)}{\delta(x)}} \sum_{i=2}^n |(\mathbf{z}_i)_x (\mathbf{z}_i)_y|.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We then use *Cauchy-Schwarz* and the fact that \(Z Z^T = I\) (as \(Z\) is an
    orthogonal matrix), which implies \(\sum_{i=1}^n (\mathbf{z}_i)_x^2 = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: We get that the above is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq (1-\gamma_\star)^t \sqrt{\frac{\delta(y)}{\delta(x)}}
    \sum_{i=2}^n (\mathbf{z}_i)_x^2 \sum_{i=2}^n (\mathbf{z}_i)_y^2\\ &\leq (1-\gamma_\star)^t
    \sqrt{\frac{\delta(y)}{\delta(x)}}\\ &\leq (1-\gamma_\star)^t \sqrt{\frac{\bar{\delta}}{\underline{\delta}}}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 'We record an immediate corollary that will be useful next. Let \(f : V \to
    \mathbb{R}\) be a function over the vertices. Define the (column) vector \(\mathbf{f}
    = (f(1),\ldots,f(n))^T\) and note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi \mathbf{f} = \sum_{x \in V} \pi_x f(x). \]
  prefs: []
  type: TYPE_NORMAL
- en: It will be convenient to use to \(\ell_\infty\)-norm. For a vector \(\mathbf{x}
    = (x_1,\ldots,x_n)^T\), we let \(\|\mathbf{x}\|_\infty = \max_{i \in [n]} |x_i|\).
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** For any initial distribution \(\bmu\) and any \(t\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\,\E[f(X_t)] - \bpi \mathbf{f}\,\right| \leq (1-\gamma_\star)^t
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By the *Time Marginals Theorem*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\,\E[f(X_t)] - \bpi \mathbf{f}\,\right| &= \left|\,\sum_{x}
    \sum_y \mu_x (P^t)_{x,y} f(y) - \sum_{y} \pi_y f(y)\,\right|. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Because \(\sum_{x} \mu_x = 1\), the right-hand side is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \left|\,\sum_{x} \sum_y \mu_x (P^t)_{x,y} f(y) - \sum_x
    \sum_{y} \mu_x \pi_y f(y)\,\right|\\ &\leq \sum_{x} \mu_x \sum_y \left| (P^t)_{x,y}
    - \pi_y \right| |f(y)|, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by the triangle inequality.
  prefs: []
  type: TYPE_NORMAL
- en: Now by the theorem this is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq \sum_{x} \mu_x \sum_y (1-\gamma_\star)^t \frac{\pi_y}{\pi_{\min{}}}|f(y)|\\
    &= (1-\gamma_\star)^t \frac{1}{\pi_{\min{}}} \sum_{x} \mu_x \sum_y \pi_y |f(y)|\\
    &\leq (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We also prove a version of the *Ergodic Theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Ergodic Theorem: Reversible Case)** Let \(G = (V,E,w)\) be a
    connected graph with positive edge weights and \(w_{x,x} > 0\) for all \(x \in
    V\). Let \(P \in \mathbb{R}^{n \times n}\) be the transition matrix of random
    walk on \(G\) and \(\bpi\) its unique stationary distribution. Let \(f : V \to
    \mathbb{R}\) be a function over the vertices. Then for any initial distribution
    \(\bmu\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{T} \sum_{t=1}^T f(X_{t}) \to \sum_{x \in V} \pi_x f(x), \]
  prefs: []
  type: TYPE_NORMAL
- en: in probability as \(T \to +\infty\). \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use *Chebyshev’s Inequality*. By the *Convergence Theorem:
    Reversible Case*, the expectation converges to the limit. To bound the variance,
    we use the *Eigendecomposition of Random Walk on a Graph*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We use *Chebyshev’s Inequality*, similarly to our proof of the *Weak
    Law of Large Numbers*. However, unlike that case, here the terms in the sum are
    not independent are require some finessing. Define again the (column) vector \(\mathbf{f}
    = (f(1),\ldots,f(n))\). Then the limit can be written as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{x \in V} \pi_x f(x) = \bpi \mathbf{f}. \]
  prefs: []
  type: TYPE_NORMAL
- en: By the corollary, the expectation of the sum can be bounded as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \left|\,\E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] -
    \bpi \mathbf{f}\,\right| &\leq \frac{1}{T} \sum_{t=1}^T\left|\E[f(X_{t})] - \bpi
    \mathbf{f}\right|\\ &\leq \frac{1}{T} \sum_{t=1}^T (1-\gamma_\star)^t \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty\\ &\leq \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty \frac{1}{T}
    \sum_{t=0}^{+\infty} (1-\gamma_\star)^t\\ &= \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty
    \gamma_\star^{-1}\frac{1}{T} \to 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: Next we bound the variance of the sum. By the *Variance of a Sum*,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{Var}\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] = \frac{1}{T^2}
    \sum_{t=1}^T \mathrm{Var}[f(X_{t})] + \frac{2}{T^2} \sum_{1 \leq s < t\leq T}
    \mathrm{Cov}[f(X_{s}),f(X_{t})]. \]
  prefs: []
  type: TYPE_NORMAL
- en: We bound the variance and covariance separately using the *Eigendecomposition
    of Random Walk on a Graph*.
  prefs: []
  type: TYPE_NORMAL
- en: To obtain convergence, a trivial bound on the variance suffices. Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 \leq \mathrm{Var}[f(X_{t})] \leq \E[f(X_{t})^2] \leq \|\mathbf{f}\|_\infty^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 \leq \frac{1}{T^2} \sum_{t=1}^T \mathrm{Var}[f(X_{t})] \leq \frac{T \|\mathbf{f}\|_\infty^2}{T^2}
    \to 0, \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: Bounding the covariance requires a more delicate argument. Fix \(1 \leq s <
    t\leq T\). The trick is to condition on \(X_s\) and use the *Markov Property*.
    By definition of the covariance and the *Law of Total Expectation*,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\mathrm{Cov}[f(X_{s}),f(X_{t})]\\ &= \E\left[(f(X_{s}) - \E[f(X_{s})])
    (f(X_{t}) - \E[f(X_{t})])\right]\\ &= \sum_{x} \E\left[(f(X_{s}) - \E[f(X_{s})])
    (f(X_{t}) - \E[f(X_{t})])\,\middle|\,X_s = x\right]\,\P[X_s = x]\\ &= \sum_{x}
    \E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right](f(x) - \E[f(X_{s})])
    \,\P[X_s = x]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We now use the time-homogeneity of the chain to note that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\\
    &= \E\left[f(X_{t})\,\middle|\,X_0 = x\right] - \E[f(X_{t})]\\ &= \E\left[f(X_{t-s})\,\middle|\,X_0
    = x\right] - \E[f(X_{t})]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We now use the corollary. This expression in absolute value is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\left|\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\right|\\
    &= \left|\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right] - \E[f(X_{t})]\right|\\
    &= \left|(\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right] - \bpi \mathbf{f}) - (\E[f(X_{t})]
    - \bpi \mathbf{f})\right|\\ &\leq \left|\E\left[f(X_{t-s})\,\middle|\,X_0 = x\right]
    - \bpi \mathbf{f}\right| + \left|\E[f(X_{t})] - \bpi \mathbf{f}\right|\\ &\leq
    (1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Plugging back above,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\left|\mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\ &\leq \sum_{x}
    \left|\E\left[f(X_{t}) - \E[f(X_{t})]\,\middle|\,X_s = x\right]\right| \left|f(x)
    - \E[f(X_{s})]\right| \,\P[X_s = x]\\ &\leq \sum_{x} ((1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty)
    \left|f(x) - \E[f(X_{s})]\right| \,\P[X_s = x]\\ &\leq ((1-\gamma_\star)^{t-s}
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty + (1-\gamma_\star)^t \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty) \sum_{x} 2 \|\mathbf{f}\|_\infty\P[X_s = x]\\ &\leq 4 (1-\gamma_\star)^{t-s}
    \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \((1-\gamma_\star)^t \leq (1-\gamma_\star)^{t-s}\) since
    \((1-\gamma_\star) < 1\) and \(t -s \leq t\).
  prefs: []
  type: TYPE_NORMAL
- en: Returning to the sum over the covariances, the previous bound gives
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\left|\frac{2}{T^2} \sum_{1 \leq s < t\leq T} \mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\
    &\leq \frac{2}{T^2} \sum_{1 \leq s < t\leq T} \left|\mathrm{Cov}[f(X_{s}),f(X_{t})]\right|\\
    &\leq \frac{2}{T^2} \sum_{1 \leq s < t\leq T} 4 (1-\gamma_\star)^{t-s} \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the sum we make the change of variable \(h = t - s\) to get that
    the previous expression is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\leq 4 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2\frac{2}{T^2}
    \sum_{1 \leq s \leq T} \sum_{h=1}^{T-s} (1-\gamma_\star)^{h}\\ &\leq 4 \pi_{\min{}}^{-1}
    \|\mathbf{f}\|_\infty^2\frac{2}{T^2} \sum_{1 \leq s \leq T} \sum_{h=0}^{+\infty}
    (1-\gamma_\star)^{h}\\ &= 4 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2\frac{2}{T^2}
    \sum_{1 \leq s \leq T} \frac{1}{\gamma_\star}\\ &= 8 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T} \to 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: We have shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathrm{Var}\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right]
    \leq \|\mathbf{f}\|_\infty^2 \frac{1}{T} + 8 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T} \leq 9 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: For any \(\varepsilon > 0\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \bpi \mathbf{f}\,\right|
    \geq \varepsilon \right]\\ &= \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t})
    - \E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right] + \left(\E\left[\frac{1}{T}
    \sum_{t=1}^T f(X_{t})\right] - \bpi \mathbf{f} \right)\,\right| \geq \varepsilon
    \right]\\ &\leq \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \E\left[\frac{1}{T}
    \sum_{t=1}^T f(X_{t})\right]\,\right| + \left|\,\E\left[\frac{1}{T} \sum_{t=1}^T
    f(X_{t})\right] - \bpi \mathbf{f} \,\right| \geq \varepsilon \right]\\ &\leq \P\left[\left|\,\frac{1}{T}
    \sum_{t=1}^T f(X_{t}) - \E\left[\frac{1}{T} \sum_{t=1}^T f(X_{t})\right]\,\right|
    \geq \varepsilon - \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty \gamma_\star^{-1}\frac{1}{T}\right].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We can now apply *Chebyshev* to get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P\left[\left|\,\frac{1}{T} \sum_{t=1}^T f(X_{t}) - \bpi \mathbf{f}\,\right|
    \geq \varepsilon \right] &\leq \frac{9 \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty^2
    \gamma_\star^{-1} \frac{1}{T}}{(\varepsilon - \pi_{\min{}}^{-1} \|\mathbf{f}\|_\infty
    \gamma_\star^{-1}\frac{1}{T})^2} \to 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(T \to +\infty\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.8.3\. Additional proofs[#](#additional-proofs "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Proof of the convergence theorem** We prove the convergence to equilibrium
    theorem in the irreducible, lazy case. Throughout this section, \((X_t)_{t \geq
    0}\) is an irreducible, lazy Markov chain on state space \(\mathcal{S} = [n]\)
    with transition matrix \(P = (p_{i,j})_{i,j=1}^n\), initial distribution \(\bmu
    = (\mu_1,\ldots,\mu_n)\) and unique stationary distribution \(\bpi = (\pi_1,\ldots,\pi_n)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We give a probabilistic proof of the *Convergence to Equilibrium Theorem*.
    The proof uses a clever idea: coupling. Separately from \((X_t)_{t \geq 0}\),
    we consider an independent Markov chain \((Y_t)_{t \geq 0}\) with the same transition
    matrix but initial distribution \(\bpi\). By the definition of stationarity,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[Y_t = i] = \pi_i, \]
  prefs: []
  type: TYPE_NORMAL
- en: for all \(i\) and all \(t\). Hence it suffices to show that
  prefs: []
  type: TYPE_NORMAL
- en: \[ |\P[X_t = i] - \pi_i| = |\P[X_t = i] - \P[Y_t = i]| \to 0 \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(t \to +\infty\) for all \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1: Showing the joint process is Markov.* We observe first that the joint
    process \((X_0,Y_0),(X_1,Y_1),\ldots\) is itself a Markov chain! Let’s just check
    the definition. By the independence of \((X_t)\) and \((Y_t)\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[(X_t,Y_t) = (x_t,y_t)\,|\,(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1}),\ldots,(X_0,Y_0)
    = (x_0,y_0)]\\ &=\frac{\P[(X_t,Y_t) = (x_t,y_t),(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1}),\ldots,(X_0,Y_0)
    = (x_0,y_0)]} {\P[(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1}),\ldots,(X_0,Y_0) = (x_0,y_0)]}\\
    &=\frac{\P[X_t = x_t,X_{t-1} = x_{t-1},\ldots,X_0 = x_0]\,\P[Y_t = y_t,Y_{t-1}
    = y_{t-1},\ldots,Y_0 = y_0]} {\P[X_{t-1} = x_{t-1},\ldots,X_0 = x_0]\,\P[Y_{t-1}
    = y_{t-1},\ldots,Y_0 = y_0]}\\ &=\frac{\P[X_t = x_t,X_{t-1} = x_{t-1},\ldots,X_0
    = x_0]} {\P[X_{t-1} = x_{t-1},\ldots,X_0 = x_0]} \frac{\P[Y_t = y_t,Y_{t-1} =
    y_{t-1},\ldots,Y_0 = y_0]} {\P[Y_{t-1} = y_{t-1},\ldots,Y_0 = y_0]}\\ &=\P[X_t
    = x_t\,|\,X_{t-1} = x_{t-1}, \ldots,X_0 = x_0] \,\P[Y_t = y_t\,|\,Y_{t-1} = y_{t-1},
    \ldots,Y_0 = y_0]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Now we use the fact that each is separately a Markov chain to simplify this
    last expression
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}] \,\P[Y_t = y_t\,|\,Y_{t-1}
    = y_{t-1}]\\ &= \frac{\P[X_t = x_t,X_{t-1} = x_{t-1}]}{\P[X_{t-1} = x_{t-1}]}
    \frac{\P[Y_t = y_t,Y_{t-1} = y_{t-1}]}{\P[Y_{t-1} = y_{t-1}]}\\ &= \frac{\P[X_t
    = x_t,X_{t-1} = x_{t-1}]\,\P[Y_t = y_t,Y_{t-1} = y_{t-1}]}{\P[X_{t-1} = x_{t-1}]\,\P[Y_{t-1}
    = y_{t-1}]}\\ &= \frac{\P[(X_t,Y_t) = (x_t,y_t),(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1})]}{\P[(X_{t-1},Y_{t-1})
    = (x_{t-1},y_{t-1})]}\\ &= \P[(X_t,Y_t) = (x_t,y_t)\,|\,(X_{t-1},Y_{t-1}) = (x_{t-1},y_{t-1})].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the claim.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 2: Waiting until the marginal processes meet.* The idea of the coupling
    argument is to consider the first time \(T\) that \(X_T = Y_T\). Note that \(T\)
    is a random time. But it is a special kind of random time often referred to as
    a stopping time. That is, the event \(\{T=s\}\) only depends on the trajectory
    of the joint chain \((X_t,Y_t)\) up to time \(s\). Specifically,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \{T=s\} = \left\{ ((X_0,Y_0),\ldots,(X_{s-1},Y_{s-1})) \in \mathcal{N}^2_{s-1},
    X_s = Y_s \right\} \]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{N}^2_{s-1} = \{ ((x_0,y_0),\ldots,(x_{s-1},y_{s-1})) \in [\mathcal{S}\times\mathcal{S}]^{s}\,:\,x_i
    \neq y_i, \forall 0 \leq i \leq s-1 \}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Here is a remarkable observation. The distributions of \(X_s\) and \(Y_s\) are
    the same after the coupling time \(T\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Distribution after Coupling)** For all \(s\) and all \(i\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_s = i, T \leq s] = \P[Y_s = i, T \leq s]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We sum over the possible values of \(T\) and \(X_T=Y_T\), and use
    the multiplication rule'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X_s = i, T \leq s]\\ &= \sum_{j=1}^n \sum_{h=0}^s \P[X_s
    = i, T=h, X_h = j]\\ &= \sum_{j=1}^n \sum_{h=0}^s \P[X_s = i \,|\, T=h, X_h =
    j] \,\P[T=h, X_h = j]\\ &= \sum_{j=1}^n \sum_{h=0}^s \P[X_s = i \,|\, ((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1}))
    \in \mathcal{N}^2_{h-1}, (X_h, Y_h) = (j,j)]\\ &\qquad\qquad\qquad\qquad \times
    \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) =
    (j,j)]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Using the Markov property for the joint process, in the form stated in *Exercise
    3.36*, we get that this last expression is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \sum_{j=1}^n \sum_{h=0}^s \P[X_s = i \,|\, (X_h, Y_h) =
    (j,j)]\\ &\qquad\qquad\times \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1},
    (X_h, Y_h) = (j,j)]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the independence of the marginal processes and the fact that they have the
    same transition matrix, this is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \sum_{j=1}^n \sum_{h=0}^s \P[X_s = i \,|\, X_h = j]\\ &\qquad\qquad\times
    \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) =
    (j,j)]\\ &= \sum_{j=1}^n \sum_{h=0}^s \P[Y_s = i \,|\, Y_h = j]\\ &\qquad\qquad\times
    \P[((X_0,Y_0),\ldots,(X_{h-1},Y_{h-1})) \in \mathcal{N}^2_{h-1}, (X_h, Y_h) =
    (j,j)]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Arguing backwards gives \(\P[Y_s = i, T \leq s]\) and concludes the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 3: Bounding how long it takes for the marginal processes to meet.* Since'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_t = i] = \P[X_t = i, T \leq t] + \P[X_t = i, T > t] \]
  prefs: []
  type: TYPE_NORMAL
- en: and similarly for \(\P[Y_t = i]\), using the previous lemma we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &|\P[X_t = i] - \P[Y_t = i]|\\ &= |\P[X_t = i, T > t] - \P[Y_t
    = i, T > t]|\\ &\leq \P[X_t = i, T > t] + \P[Y_t = i, T > t]\\ &\leq 2 \P[T >
    t]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So it remains to show that \(\P[T > t]\) goes to \(0\) as \(t \to +\infty\).
  prefs: []
  type: TYPE_NORMAL
- en: We note that \(\P[T > t]\) is non-increasing as a function of \(t\). Indeed,
    for \(h > 0\), we have the implication
  prefs: []
  type: TYPE_NORMAL
- en: \[ \{T > t+h\} \subseteq \{T > t\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: so by the monotonicity of probabilities
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[T > t+h] \leq \P[T > t]. \]
  prefs: []
  type: TYPE_NORMAL
- en: So it remains to prove the following lemma.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Tail of Coupling Time)** There is a \(0 < \beta < 1\) and a positive
    integer \(m\) such that, for all positive integers \(k\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[T > k m] \leq \beta^{k m}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Recall that the state space of the marginal processes is \([n]\),
    so the joint process lives in \([n]\times [n]\). Since the event \(\{(X_m, Y_m)
    = (1,1)\}\) implies \(\{T \leq m\}\), to bound \(\P[T > m]\) we note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[T > m] = 1 - \P[T \leq m] \leq 1 - \P[(X_m, Y_m) = (1,1)]. \]
  prefs: []
  type: TYPE_NORMAL
- en: To bound the probability on right-hand side, we construct a path of length \(m\)
    in the transition graph of the joint process from any state to \((1,1)\).
  prefs: []
  type: TYPE_NORMAL
- en: For \(i \in [n]\), let \(\mathcal{P}_i = (z_{i,0},\ldots,z_{i,\ell_i})\) be
    the shortest path in the transition graph of \((X_t)_{t \geq 0}\) from \(i\) to
    \(1\), where \(z_{i,0} = i\) and \(z_{i,\ell_i} = 1\). By irreducibility there
    exists such a path for any \(i\). Here \(\ell_i\) is the length of \(\mathcal{P}_i\),
    and we define
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell^* = \max_{i \neq 1} \ell_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: We make all the paths above the same length \(\ell^*\) by padding them with
    \(1\)s. That is, we define \(\mathcal{P}_i^* = (z_{i,0},\ldots,z_{i,\ell_i},1,\ldots,1)\)
    such that this path now has length \(\ell^*\). This remains a path in the transition
    graph of \((X_t)_{t \geq 0}\) because the chain is lazy.
  prefs: []
  type: TYPE_NORMAL
- en: Now, for any pair of states \((i,j) \in [n] \times [n]\), consider the path
    of length \(m := 2 \ell^*\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{Q}^*_{(i,j)} = ((z_{i,0},j),\ldots,(z_{i,\ell_i},j),(1,j),\ldots,(1,j),(1,z_{j,0}),\ldots,(1,z_{i,\ell_i}),(1,1),\ldots,(1,1)).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: In words, we leave the second component at \(j\) while running through path
    \(\mathcal{P}_i^*\) on the first component, then we leave the first component
    at \(1\) while running through path \(\mathcal{P}_j^*\) on the second component.
    Path \(\mathcal{Q}^*_{(i,j)}\) is a valid path in the transition graph of the
    joint chain. Again, we are using that the marginal processes are lazy.
  prefs: []
  type: TYPE_NORMAL
- en: Denote by \(\mathcal{Q}^*_{(i,j)}[r]\) be the \(r\)-th state in path \(\mathcal{Q}^*_{(i,j)}\).
    Going back to bounding \(\P[(X_m, Y_m) = (1,1)]\), we define \(\beta\) as follows
  prefs: []
  type: TYPE_NORMAL
- en: '\[\begin{align*} &\P[(X_m, Y_m) = (1,1)\,|\,(X_0, Y_0) = (i,j)]\\ &\geq \min_{(i,j)}
    \P[(X_r, Y_r) = \mathcal{Q}^*_{(i,j)}[r], \forall r=0,\ldots,m \,|\,(X_0, Y_0)
    = (i_0,j_0)]\\ &=: 1-\beta \in (0,1). \end{align*}\]'
  prefs: []
  type: TYPE_NORMAL
- en: By the law of total probability,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[(X_m, Y_m) = (1,1)]\\ &= \sum_{(i,j) \in [n]\times [n]}\P[(X_m,
    Y_m) = (1,1)\,|\,(X_0, Y_0) = (i,j)]\, \P[(X_0, Y_0) = (i,j)]\\ &= \sum_{(i,j)
    \in [n]\times [n]}\P[(X_m, Y_m) = (1,1)\,|\,(X_0, Y_0) = (i,j)]\, \mu_i \pi_j\\
    &\geq \sum_{(i,j) \in [n]\times [n]} \P[(X_r, Y_r) = \mathcal{Q}^*_{(i,j)}[r],
    \forall r=0,\ldots,m \,|\,(X_0, Y_0) = (i,j)]\, \mu_i \pi_j\\ &\geq (1-\beta)
    \sum_{(i,j) \in [n]\times [n]} \mu_i \pi_j\\ &= 1-\beta. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[T > m] \leq \beta. \]
  prefs: []
  type: TYPE_NORMAL
- en: Because
  prefs: []
  type: TYPE_NORMAL
- en: \[ \{T > 2m\} \subseteq \{T > m\}, \]
  prefs: []
  type: TYPE_NORMAL
- en: it holds that by the multiplication rule that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[T > 2m] &= \P[\{T > 2m\}\cap\{T > m\}]\\ &= \P[T > 2m\,|\,
    T > m]\,\P[T > m]\\ &\leq \P[T > 2m\,|\, T > m]\,\beta. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Summing over all state pairs at time \(m\), we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[T > 2m\,|\, T > m]\\ &= \sum_{(i,j) \in [n]\times [n]}
    \P[T > 2m\,|\, T > m, (X_m,Y_m) = (i,j)] \,\P[(X_m,Y_m) = (i,j) \,|\, T > m].
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Arguing as above, note that for \(i \neq j\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[T \leq 2m\,|\, T > m, (X_m,Y_m) = (i,j)]\\ &\geq \P[(X_{2m},Y_{2m})
    = (1,1) \,|\, T > m, (X_m,Y_m) = (i,j)]\\ &= \P[(X_{2m},Y_{2m}) = (1,1) \,|\,
    (X_m,Y_m) = (i,j), ((X_0,Y_0),\ldots,(X_{m-1},Y_{m-1})) \in \mathcal{N}^2_{m-1}]\\
    &= \P[(X_{2m},Y_{2m}) = (1,1) \,|\, (X_m,Y_m) = (i,j)]\\ &= \P[(X_{m},Y_{m}) =
    (1,1) \,|\, (X_0,Y_0) = (i,j)]\\ &\geq 1-\beta, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: by the Markov property and the time-homogeneity of the process.
  prefs: []
  type: TYPE_NORMAL
- en: Plugging above and noting that \(\P[(X_m,Y_m) = (i,j) \,|\, T > m] = 0\) when
    \(i = j\), we get that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[T > 2m\,|\, T > m]\\ &\geq \sum_{\substack{(i,j) \in [n]\times
    [n]\\i \neq j}} \beta \,\P[(X_m,Y_m) = (i,j) \,|\, T > m]\\ &= \beta. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So we have proved that \(\P[T > 2m] \leq \beta^2\). Proceeding similarly by
    induction gives the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
