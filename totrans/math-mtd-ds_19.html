<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>3.2. Background: review of differentiable functions of several variables#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>3.2. Background: review of differentiable functions of several variables#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap03_opt/02_several/roch-mmids-opt-several.html">https://mmids-textbook.github.io/chap03_opt/02_several/roch-mmids-opt-several.html</a></blockquote>

<p>We review the differential calculus of several variables. We highlight a few key results that will play an important role: the <em>Chain Rule</em> and the <em>Mean Value Theorem</em>.</p>
<section id="gradient">
<h2><span class="section-number">3.2.1. </span>Gradient<a class="headerlink" href="#gradient" title="Link to this heading">#</a></h2>
<p>Recall the definition of the gradient.</p>
<p><strong>DEFINITION</strong> <strong>(Gradient)</strong> <span class="math notranslate nohighlight">\(\idx{gradient}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Assume <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. The (column) vector</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x}_0) 
= \left(\frac{\partial f (\mathbf{x}_0)}{\partial x_1}, \ldots, \frac{\partial f (\mathbf{x}_0)}{\partial x_d}\right)
\]</div>
<p>is called the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Note that the gradient is itself a function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. In fact, unlike <span class="math notranslate nohighlight">\(f\)</span>, it is a vector-valued function.</p>
<p><img alt="Gradient as a function (with help from ChatGPT; adapted from (Source))" src="../Images/4779b42631c18d22c237a9d0faeb1d4e.png" data-original-src="https://mmids-textbook.github.io/_images/quiver.png"/></p>
<p><strong>EXAMPLE:</strong> Consider the affine function</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x})
= \mathbf{q}^T \mathbf{x} + r
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_d), \mathbf{q} = (q_1, \ldots, q_d) \in \mathbb{R}^d\)</span>. The partial derivatives of the linear term are given by</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial x_i}
[\mathbf{q}^T \mathbf{x}]
= \frac{\partial}{\partial x_i}
\left[\sum_{j=1}^d q_j x_j
\right]
= \frac{\partial}{\partial x_i}
\left[q_i x_i
\right]
= q_i.
\]</div>
<p>So the gradient of <span class="math notranslate nohighlight">\(f\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x})
= \mathbf{q}.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> Consider the quadratic function</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x})
= \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x} + r,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_d), \mathbf{q} = (q_1, \ldots, q_d) \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(P \in \mathbb{R}^{d \times d}\)</span>. The partial derivatives of the quadratic term are given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial}{\partial x_i}
[\mathbf{x}^T P \mathbf{x}]
&amp;= \frac{\partial}{\partial x_i}
\left[\sum_{j, k=1}^d P_{jk} x_j x_k
\right]\\
&amp;= \frac{\partial}{\partial x_i}
\left[P_{ii} x_i^2 
+ \sum_{j=1, j\neq i}^d P_{ji} x_j x_i
+ \sum_{k=1, k\neq i}^d P_{ik} x_i x_k
\right],
\end{align*}\]</div>
<p>where we used that all terms not including <span class="math notranslate nohighlight">\(x_i\)</span> have partial derivative <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>This last expression is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;= 2 P_{ii} x_i
+ \sum_{j=1, j\neq i}^d P_{ji} x_j
+ \sum_{k=1, k\neq i}^d P_{ik} x_k\\
&amp;= \sum_{j=1}^d [P^T]_{ij} x_j + \sum_{k=1}^d [P]_{ik} x_k\\
&amp;= ([P^T + P]\mathbf{x})_i.
\end{align*}\]</div>
<p>So the gradient of <span class="math notranslate nohighlight">\(f\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x})
= \frac{1}{2}[P + P^T] \,\mathbf{x}
+ \mathbf{q}.
\]</div>
<p>If <span class="math notranslate nohighlight">\(P\)</span> is symmetric, this further simplifies to
<span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}) = P \,\mathbf{x} + \mathbf{q}\)</span>.
<span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>It will be useful to compute the derivative of a function <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> of several variables along a parametric curve <span class="math notranslate nohighlight">\(\mathbf{g}(t) = (g_1(t), \ldots, g_d(t)) \in \mathbb{R}^d\)</span> for <span class="math notranslate nohighlight">\(t\)</span> in some closed interval of <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. The following result is a special case of an important fact. We will use the following notation <span class="math notranslate nohighlight">\(\mathbf{g}'(t) = (g_1'(t), \ldots, g_m'(t))\)</span>, where <span class="math notranslate nohighlight">\(g_i'\)</span> is the derivative of <span class="math notranslate nohighlight">\(g_i\)</span>. We say that <span class="math notranslate nohighlight">\(\mathbf{g}(t)\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(t = t_0\)</span> if each of its component is.</p>
<p><strong>EXAMPLE:</strong> <strong>(Parametric Line)</strong> The straight line between <span class="math notranslate nohighlight">\(\mathbf{x}_0 = (x_{0,1},\ldots,x_{0,d})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_1 = (x_{1,1},\ldots,x_{1,d})\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> can be parametrized as</p>
<div class="math notranslate nohighlight">
\[
\mathbf{g}(t) = \mathbf{x}_0 + t (\mathbf{x}_1 - \mathbf{x}_0),
\]</div>
<p>where <span class="math notranslate nohighlight">\(t\)</span> goes from <span class="math notranslate nohighlight">\(0\)</span> (at which <span class="math notranslate nohighlight">\(\mathbf{g}(0) = \mathbf{x}_0\)</span>) to <span class="math notranslate nohighlight">\(1\)</span> (at which <span class="math notranslate nohighlight">\(\mathbf{g}(1) = \mathbf{x}_1\)</span>).</p>
<p>Then</p>
<div class="math notranslate nohighlight">
\[
g_i'(t) 
= \frac{\mathrm{d}}{\mathrm{d}t} [x_{0,i} + t (x_{1,i} - x_{0,i})]
= x_{1,i} - x_{0,i},
\]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{g}'(t) = \mathbf{x}_1 - \mathbf{x}_0.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Recall the <em>Chain Rule</em> in the single-variable case. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule">Wikipedia</a>:</p>
<blockquote>
<div><p>The simplest form of the chain rule is for real-valued functions of one real variable. It states that if <span class="math notranslate nohighlight">\(g\)</span> is a function that is differentiable at a point <span class="math notranslate nohighlight">\(c\)</span> (i.e. the derivative <span class="math notranslate nohighlight">\(g'(c)\)</span> exists) and <span class="math notranslate nohighlight">\(f\)</span> is a function that is differentiable at <span class="math notranslate nohighlight">\(g(c)\)</span>, then the composite function <span class="math notranslate nohighlight">\({\displaystyle f\circ g}\)</span> is differentiable at <span class="math notranslate nohighlight">\(c\)</span>, and the derivative is <span class="math notranslate nohighlight">\({\displaystyle (f\circ g)'(c)=f'(g(c))\cdot g'(c)}\)</span>.</p>
</div></blockquote>
<p>Here is a straightforward generalization of the <em>Chain Rule</em>.</p>
<p><strong>THEOREM</strong> <strong>(Chain Rule)</strong> <span class="math notranslate nohighlight">\(\idx{chain rule}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D_1 \to \mathbb{R}\)</span>, where <span class="math notranslate nohighlight">\(D_1 \subseteq \mathbb{R}\)</span>, and let <span class="math notranslate nohighlight">\(g : D_2 \to \mathbb{R}\)</span>, where <span class="math notranslate nohighlight">\(D_2 \subseteq \mathbb{R}^d\)</span>. Assume that <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(g(\mathbf{x}_0)\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_1\)</span>, and that <span class="math notranslate nohighlight">\(g\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\nabla (f\circ g) (\mathbf{x}_0)
= f'(g(\mathbf{x}_0))
\nabla g(\mathbf{x}_0).
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> We apply the <em>Chain Rule</em> for functions of one variable to the partial derivatives. For all <span class="math notranslate nohighlight">\(i\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial x_i}f (g(\mathbf{x}_0))
= f'(g(\mathbf{x}_0)) \frac{\partial}{\partial x_i} g(\mathbf{x}_0).
\]</div>
<p>Collecting the partial derivatives in a vector gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Here is a different generalization of the <em>Chain Rule</em>. Again the composition <span class="math notranslate nohighlight">\(f \circ \mathbf{g}\)</span> denotes the function <span class="math notranslate nohighlight">\(f \circ \mathbf{g}(t) = f (\mathbf{g}(t))\)</span>.</p>
<p><strong>THEOREM</strong> <strong>(Chain Rule)</strong> Let <span class="math notranslate nohighlight">\(f : D_1 \to \mathbb{R}\)</span>, where <span class="math notranslate nohighlight">\(D_1 \subseteq \mathbb{R}^d\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{g} : D_2 \to \mathbb{R}^d\)</span>, where <span class="math notranslate nohighlight">\(D_2 \subseteq \mathbb{R}\)</span>. Assume that <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{g}(t_0)\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_1\)</span>, and that <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(t_0\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
(f\circ \mathbf{g})'(t_0)
= \nabla f (\mathbf{g}(t_0))^T
\mathbf{g}'(t_0).
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> To simplify the notation, suppose that <span class="math notranslate nohighlight">\(f\)</span> is a real-valued function of <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_d)\)</span> whose components are themselves functions of <span class="math notranslate nohighlight">\(t \in \mathbb{R}\)</span>. Assume <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}(t)\)</span>. To compute the <a class="reference external" href="https://en.wikipedia.org/wiki/Total_derivative">total derivative</a><span class="math notranslate nohighlight">\(\idx{total derivative}\xdi\)</span> <span class="math notranslate nohighlight">\(\frac{\mathrm{d} f(t)}{\mathrm{d} t}\)</span>, let <span class="math notranslate nohighlight">\(\Delta x_k = x_k(t + \Delta t) - x_k(t)\)</span>, <span class="math notranslate nohighlight">\(x_k = x_k(t)\)</span> and</p>
<div class="math notranslate nohighlight">
\[
\Delta f 
= 
f(x_1 + \Delta x_1, \ldots, x_d + \Delta x_d)
-
f(x_1, \ldots, x_d).
\]</div>
<p>We seek to compute the limit <span class="math notranslate nohighlight">\(\lim_{\Delta t \to 0} \frac{\Delta f}{\Delta t}\)</span>. To relate this limit to partial derivatives of <span class="math notranslate nohighlight">\(f\)</span>, we re-write <span class="math notranslate nohighlight">\(\Delta f\)</span> as a telescoping sum where each term involves variation of a single variable <span class="math notranslate nohighlight">\(x_k\)</span>. That is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\Delta f
= 
&amp; [f(x_1 + \Delta x_1, \ldots, x_d + \Delta x_d)
-
f(x_1, x_2 + \Delta x_2, \ldots, x_d + \Delta x_d)]\\
&amp;+ [f(x_1, x_2 + \Delta x_2, \ldots, x_d + \Delta x_d)
-
f(x_1, x_2, x_3 + \Delta x_3, \ldots, x_d + \Delta x_d)]  \\
&amp;+ \cdots + 
[f(x_1, \cdots, x_{d-1}, x_d + \Delta x_d)
-
f(x_1, \ldots, x_d)].
\end{align*}\]</div>
<p>Applying the <em>Mean Value Theorem</em> to each term gives</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\Delta f
= 
&amp; \Delta x_1 
\frac{\partial f(x_1 + \theta_1 \Delta x_1, x_2 + \Delta x_2, \ldots, x_d + \Delta x_d)}
{\partial x_1}\\
&amp;+ \Delta x_2 
\frac{\partial f(x_1, x_2 + \theta_2 \Delta x_2, x_3 + \Delta x_3, \ldots, x_d + \Delta x_d)}
{\partial x_2}\\
&amp;+ \cdots + \Delta x_d
\frac{\partial f(x_1, \cdots, x_{d-1}, x_d + \theta_d \Delta x_d)}
{\partial x_d}
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(0 &lt; \theta_k &lt; 1\)</span> for <span class="math notranslate nohighlight">\(k=1,\ldots,d\)</span>. Dividing by <span class="math notranslate nohighlight">\(\Delta t\)</span>, taking the limit <span class="math notranslate nohighlight">\(\Delta t \to 0\)</span> and using the fact that <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable, we get</p>
<div class="math notranslate nohighlight">
\[
\frac{\mathrm{d} f (t)}{\mathrm{d} t}
= \sum_{k=1}^d \frac{\partial f(\mathbf{x}(t))}
{\partial x_k} \frac{\mathrm{d} x_k(t)}{\mathrm{d} t}.
\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>As a first application of the <em>Chain Rule</em>, we generalize the <em>Mean Value Theorem</em> to the case of several variables. We will use this result later to prove a multivariable Taylor expansion result that will play a central role in this chapter.</p>
<p><strong>THEOREM</strong> <strong>(Mean Value)</strong> <span class="math notranslate nohighlight">\(\idx{mean value theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> and <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> be such that <span class="math notranslate nohighlight">\(B_\delta(\mathbf{x}_0) \subseteq D\)</span>. If <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable on <span class="math notranslate nohighlight">\(B_\delta(\mathbf{x}_0)\)</span>, then for any <span class="math notranslate nohighlight">\(\mathbf{x} \in B_\delta(\mathbf{x}_0)\)</span></p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x})
= f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0 + \xi \mathbf{p})^T \mathbf{p} 
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\xi \in (0,1)\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{p} = \mathbf{x} - \mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>One way to think of the <em>Mean Value Theorem</em> is as a <span class="math notranslate nohighlight">\(0\)</span>-th order Taylor expansion. It says that, when <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is close to <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, the value <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> is close to <span class="math notranslate nohighlight">\(f(\mathbf{x}_0)\)</span> in a way that can be controlled in terms of the gradient in the neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. From this point of view, the term <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}_0 + \xi \mathbf{p})^T \mathbf{p}\)</span> is called the Lagrange remainder.</p>
<p><em>Proof idea:</em> We apply the single-variable result and the <em>Chain Rule</em>.</p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(\phi(t) = f(\boldsymbol{\alpha}(t))\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}(t) = \mathbf{x}_0 + t \mathbf{p}\)</span>. Observe that <span class="math notranslate nohighlight">\(\phi(0) = f(\mathbf{x}_0)\)</span> and <span class="math notranslate nohighlight">\(\phi(1) = f(\mathbf{x})\)</span>. By the <em>Chain Rule</em> and the parametric line example,</p>
<div class="math notranslate nohighlight">
\[
\phi'(t)
= \nabla f(\boldsymbol{\alpha}(t))^T \boldsymbol{\alpha}'(t)
= \nabla f(\boldsymbol{\alpha}(t))^T \mathbf{p}
= \nabla f(\mathbf{x}_0 + t \mathbf{p})^T \mathbf{p}.
\]</div>
<p>In particular, <span class="math notranslate nohighlight">\(\phi\)</span> has a continuous first derivative on <span class="math notranslate nohighlight">\([0,1]\)</span>. By the <em>Mean Value Theorem</em> in the single-variable case</p>
<div class="math notranslate nohighlight">
\[
\phi(t)
= \phi(0) + t \phi'(\xi)
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\xi \in (0,t)\)</span>. Plugging in the expressions for <span class="math notranslate nohighlight">\(\phi(0)\)</span> and <span class="math notranslate nohighlight">\(\phi'(\xi)\)</span> and taking <span class="math notranslate nohighlight">\(t=1\)</span> gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
</section>
<section id="second-order-derivatives">
<h2><span class="section-number">3.2.2. </span>Second-order derivatives<a class="headerlink" href="#second-order-derivatives" title="Link to this heading">#</a></h2>
<p>One can also define higher-order derivatives. We start with the single-variable case, where <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> with <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(x_0 \in D\)</span> is an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Note that, if <span class="math notranslate nohighlight">\(f'\)</span> exists in <span class="math notranslate nohighlight">\(D\)</span>, then it is itself a function of <span class="math notranslate nohighlight">\(x\)</span>. Then the second derivative at <span class="math notranslate nohighlight">\(x_0\)</span> is</p>
<div class="math notranslate nohighlight">
\[
f''(x_0)
= \frac{\mathrm{d}^2 f(x_0)}{\mathrm{d} x^2}
= \lim_{h \to 0} \frac{f'(x_0 + h) - f'(x_0)}{h}
\]</div>
<p>provided the limit exists.</p>
<p>In the several variable case, we have the following:</p>
<p><strong>DEFINITION</strong> <strong>(Second Partial Derivatives and Hessian)</strong> <span class="math notranslate nohighlight">\(\idx{second partial derivatives}\xdi\)</span> <span class="math notranslate nohighlight">\(\idx{Hessian}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Assume that <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable in an open ball around <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. Then <span class="math notranslate nohighlight">\(\partial f(\mathbf{x})/\partial x_i\)</span> is itself a function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and its partial derivative with respect to <span class="math notranslate nohighlight">\(x_j\)</span>, if it exists, is denoted by</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_j \partial x_i}
= \lim_{h \to 0} \frac{\frac{\partial f}{\partial x_i}(\mathbf{x}_0 + h \mathbf{e}_j) - \frac{\partial f}{\partial x_i}(\mathbf{x}_0)}{h}.
\]</div>
<p>To simplify the notation, we write this as <span class="math notranslate nohighlight">\(\partial^2 f(\mathbf{x}_0)/\partial x_i^2\)</span> when <span class="math notranslate nohighlight">\(j = i\)</span>. If <span class="math notranslate nohighlight">\(\partial^2 f(\mathbf{x})/\partial x_j \partial x_i\)</span> and <span class="math notranslate nohighlight">\(\partial^2 f(\mathbf{x})/\partial x_i^2\)</span> exist and are continuous in an open ball around <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> for all <span class="math notranslate nohighlight">\(i, j\)</span>, we say that <span class="math notranslate nohighlight">\(f\)</span> is twice continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>.</p>
<p>The matrix of second derivatives is called the Hessian and is denoted by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(\mathbf{x}_0)
= \begin{pmatrix}
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_1^2} 
&amp; \cdots 
&amp; \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d \partial x_1}\\
\vdots &amp; \ddots &amp; \vdots\\
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_1 \partial x_d} 
&amp; \cdots 
&amp; \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d^2}
\end{pmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Like <span class="math notranslate nohighlight">\(f\)</span> and the gradient <span class="math notranslate nohighlight">\(\nabla f\)</span>, the Hessian <span class="math notranslate nohighlight">\(\mathbf{H}_f\)</span> is a function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. It is a matrix-valued function however.</p>
<p>When <span class="math notranslate nohighlight">\(f\)</span> is twice continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, its Hessian is a symmetric matrix.</p>
<p><strong>THEOREM</strong> <strong>(Symmetry of the Hessian)</strong> <span class="math notranslate nohighlight">\(\idx{symmetry of the Hessian}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Assume that <span class="math notranslate nohighlight">\(f\)</span> is twice continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. Then for all <span class="math notranslate nohighlight">\(i \neq j\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_j \partial x_i}
= \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_i \partial x_j}.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> Two applications of the <em>Mean Value Theorem</em> show that the limits can be interchanged.</p>
<p><em>Proof:</em> By definition of the partial derivative,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_j \partial x_i}
&amp;= \lim_{h_j \to 0} \frac{\frac{\partial f}{\partial x_i}(\mathbf{x}_0 + h_j \mathbf{e}_j) - \frac{\partial f}{\partial x_i}(\mathbf{x}_0)}{h_j}\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} \frac{1}{h_j h_i}
\left\{
[f(\mathbf{x}_0 + h_j \mathbf{e}_j + h_i \mathbf{e}_i)
- f(\mathbf{x}_0 + h_j \mathbf{e}_j)]
- [f(\mathbf{x}_0 + h_i \mathbf{e}_i)
- f(\mathbf{x}_0)]
\right\}\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} \frac{1}{h_i}
\left\{
\frac{[f(\mathbf{x}_0 + h_i \mathbf{e}_i + h_j \mathbf{e}_j)
- f(\mathbf{x}_0 + h_i \mathbf{e}_i)]
- [f(\mathbf{x}_0 + h_j \mathbf{e}_j)
- f(\mathbf{x}_0)]}{h_j}
\right\}\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} \frac{1}{h_i}
\left\{\frac{\partial}{\partial x_j}[f(\mathbf{x}_0 + h_i \mathbf{e}_i + \theta_j h_j \mathbf{e}_j)
- f(\mathbf{x}_0 + \theta_j h_j \mathbf{e}_j)]
\right\}\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} \frac{1}{h_i}
\left\{\frac{\partial f}{\partial x_j}(\mathbf{x}_0 + h_i \mathbf{e}_i + \theta_j h_j \mathbf{e}_j)
- \frac{\partial f}{\partial x_j}(\mathbf{x}_0 + \theta_j h_j \mathbf{e}_j)
\right\}
\end{align*}\]</div>
<p>for some <span class="math notranslate nohighlight">\(\theta_j \in (0,1)\)</span>. Note that, on the third line, we rearranged the terms and, on the fourth line, we applied the <em>Mean Value Theorem</em> to <span class="math notranslate nohighlight">\(f(\mathbf{x}_0 + h_i \mathbf{e}_i + h_j \mathbf{e}_j) - f(\mathbf{x}_0 + h_j \mathbf{e}_j)\)</span> as a continuously differentiable function of <span class="math notranslate nohighlight">\(h_j\)</span>.</p>
<p>Because <span class="math notranslate nohighlight">\(\partial f/\partial x_j\)</span> is continuously differentiable in an open ball around <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, a second application of the <em>Mean Value Theorem</em> gives for some <span class="math notranslate nohighlight">\(\theta_i \in (0,1)\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\lim_{h_j \to 0} \lim_{h_i \to 0} \frac{1}{h_i}
\left\{\frac{\partial f}{\partial x_j}(\mathbf{x}_0 + h_i \mathbf{e}_i + \theta_j h_j \mathbf{e}_j)
- \frac{\partial f}{\partial x_j}(\mathbf{x}_0 + \theta_j h_j \mathbf{e}_j)
\right\}\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} 
\frac{\partial}{\partial x_i}\left[\frac{\partial f}{\partial x_j}(\mathbf{x}_0 + \theta_j h_j \mathbf{e}_j + \theta_i h_i \mathbf{e}_i)\right]\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} \frac{\partial^2 f(\mathbf{x}_0 + \theta_j h_j \mathbf{e}_j + \theta_i h_i \mathbf{e}_i)}{\partial x_i \partial x_j}.
\end{align*}\]</div>
<p>The claim then follows from the continuity of <span class="math notranslate nohighlight">\(\partial^2 f/\partial x_i \partial x_j\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>EXAMPLE:</strong> Consider the quadratic function</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x})
= \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x} + r.
\]</div>
<p>Recall that the gradient of <span class="math notranslate nohighlight">\(f\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x})
= \frac{1}{2}[P + P^T] \,\mathbf{x}
+ \mathbf{q}.
\]</div>
<p>To simplify the calculation, let <span class="math notranslate nohighlight">\(B = \frac{1}{2}[P + P^T]\)</span> and denote the rows of <span class="math notranslate nohighlight">\(B\)</span> by <span class="math notranslate nohighlight">\(\mathbf{b}_1^T, \ldots,\mathbf{b}_d^T\)</span>.</p>
<p>Each component of <span class="math notranslate nohighlight">\(\nabla f\)</span> is an affine function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, specifically,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f (\mathbf{x})}{\partial x_i}
= \mathbf{b}_i^T \mathbf{x} + q_i. 
\]</div>
<p>Row <span class="math notranslate nohighlight">\(i\)</span> of the Hessian is simply the gradient transposed of <span class="math notranslate nohighlight">\(\frac{\partial f (\mathbf{x})}{\partial x_i}\)</span> which, by our previous results, is</p>
<div class="math notranslate nohighlight">
\[
\left(\nabla \frac{\partial f (\mathbf{x})}{\partial x_i}\right)^T
= \mathbf{b}_i^T.
\]</div>
<p>Putting this together we get</p>
<div class="math notranslate nohighlight">
\[
\mathbf{H}_f(\mathbf{x}) = \frac{1}{2}[P + P^T].
\]</div>
<p>Observe that this is indeed a symmetric matrix. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> What does it mean for a function <span class="math notranslate nohighlight">\(f\)</span> to be continuously differentiable at <span class="math notranslate nohighlight">\(x_0\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(f\)</span> is continuous at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>b) All partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> exist at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>c) All partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> exist and are continuous in an open ball around <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>d) The gradient of <span class="math notranslate nohighlight">\(f\)</span> is zero at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p><strong>2</strong> What is the gradient of a function <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span>, where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span>, at a point <span class="math notranslate nohighlight">\(x_0 \in D\)</span>?</p>
<p>a) The rate of change of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span></p>
<p>b) The vector of all second partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span></p>
<p>c) The vector of all first partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span></p>
<p>d) The matrix of all second partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span></p>
<p><strong>3</strong> Which of the following statements is true about the Hessian matrix of a twice continuously differentiable function?</p>
<p>a) It is always a diagonal matrix.</p>
<p>b) It is always a symmetric matrix.</p>
<p>c) It is always an invertible matrix.</p>
<p>d) It is always a positive definite matrix.</p>
<p><strong>4</strong> Let <span class="math notranslate nohighlight">\(f(x, y, z) = x^2 + y^2 - z^2\)</span>. What is the Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\begin{pmatrix} 2 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; -2 \end{pmatrix}\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\begin{pmatrix} 2x &amp; 2y &amp; -2z \end{pmatrix}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}\)</span></p>
<p><strong>5</strong> What is the Hessian matrix of the quadratic function <span class="math notranslate nohighlight">\(f(x) = \frac{1}{2}x^TPx + q^Tx + r\)</span>, where <span class="math notranslate nohighlight">\(P \in \mathbb{R}^{d \times d}\)</span> and <span class="math notranslate nohighlight">\(q \in \mathbb{R}^d\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(H_f(x) = P\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(H_f(x) = P^T\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(H_f(x) = \frac{1}{2}[P + P^T]\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(H_f(x) = [P + P^T]\)</span></p>
<p>Answer for 1: c. Justification: The text states, “If <span class="math notranslate nohighlight">\(f\)</span> exists and is continuous in an open ball around <span class="math notranslate nohighlight">\(x_0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>, then we say that <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(x_0\)</span>.”</p>
<p>Answer for 2: c. Justification: From the text: “The (column) vector <span class="math notranslate nohighlight">\(\nabla f(x_0) = ( \frac{\partial f(x_0)}{\partial x_1}, \ldots, \frac{\partial f(x_0)}{\partial x_d})\)</span> is called the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>.”</p>
<p>Answer for 3: b). Justification: The text states: “When <span class="math notranslate nohighlight">\(f\)</span> is twice continuously differentiable at <span class="math notranslate nohighlight">\(x_0\)</span>, its Hessian is a symmetric matrix.”</p>
<p>Answer for 4: a). Justification: The Hessian is the matrix of second partial derivatives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} \frac{\partial^2 f}{\partial x^2} &amp; \frac{\partial^2 f}{\partial x \partial y} &amp; \frac{\partial^2 f}{\partial x \partial z} \\ \frac{\partial^2 f}{\partial y \partial x} &amp; \frac{\partial^2 f}{\partial y^2} &amp; \frac{\partial^2 f}{\partial y \partial z} \\ \frac{\partial^2 f}{\partial z \partial x} &amp; \frac{\partial^2 f}{\partial z \partial y} &amp; \frac{\partial^2 f}{\partial z^2} \end{pmatrix} = \begin{pmatrix} 2 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; -2 \end{pmatrix}
\end{split}\]</div>
<p>Answer for 5: c. Justification: The text shows that the Hessian of the quadratic function is <span class="math notranslate nohighlight">\(H_f(x) = \frac{1}{2}[P + P^T]\)</span>.</p>
</section>
&#13;

<h2><span class="section-number">3.2.1. </span>Gradient<a class="headerlink" href="#gradient" title="Link to this heading">#</a></h2>
<p>Recall the definition of the gradient.</p>
<p><strong>DEFINITION</strong> <strong>(Gradient)</strong> <span class="math notranslate nohighlight">\(\idx{gradient}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Assume <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. The (column) vector</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x}_0) 
= \left(\frac{\partial f (\mathbf{x}_0)}{\partial x_1}, \ldots, \frac{\partial f (\mathbf{x}_0)}{\partial x_d}\right)
\]</div>
<p>is called the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Note that the gradient is itself a function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. In fact, unlike <span class="math notranslate nohighlight">\(f\)</span>, it is a vector-valued function.</p>
<p><img alt="Gradient as a function (with help from ChatGPT; adapted from (Source))" src="../Images/4779b42631c18d22c237a9d0faeb1d4e.png" data-original-src="https://mmids-textbook.github.io/_images/quiver.png"/></p>
<p><strong>EXAMPLE:</strong> Consider the affine function</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x})
= \mathbf{q}^T \mathbf{x} + r
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_d), \mathbf{q} = (q_1, \ldots, q_d) \in \mathbb{R}^d\)</span>. The partial derivatives of the linear term are given by</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial x_i}
[\mathbf{q}^T \mathbf{x}]
= \frac{\partial}{\partial x_i}
\left[\sum_{j=1}^d q_j x_j
\right]
= \frac{\partial}{\partial x_i}
\left[q_i x_i
\right]
= q_i.
\]</div>
<p>So the gradient of <span class="math notranslate nohighlight">\(f\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x})
= \mathbf{q}.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><strong>EXAMPLE:</strong> Consider the quadratic function</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x})
= \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x} + r,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_d), \mathbf{q} = (q_1, \ldots, q_d) \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(P \in \mathbb{R}^{d \times d}\)</span>. The partial derivatives of the quadratic term are given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial}{\partial x_i}
[\mathbf{x}^T P \mathbf{x}]
&amp;= \frac{\partial}{\partial x_i}
\left[\sum_{j, k=1}^d P_{jk} x_j x_k
\right]\\
&amp;= \frac{\partial}{\partial x_i}
\left[P_{ii} x_i^2 
+ \sum_{j=1, j\neq i}^d P_{ji} x_j x_i
+ \sum_{k=1, k\neq i}^d P_{ik} x_i x_k
\right],
\end{align*}\]</div>
<p>where we used that all terms not including <span class="math notranslate nohighlight">\(x_i\)</span> have partial derivative <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>This last expression is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;= 2 P_{ii} x_i
+ \sum_{j=1, j\neq i}^d P_{ji} x_j
+ \sum_{k=1, k\neq i}^d P_{ik} x_k\\
&amp;= \sum_{j=1}^d [P^T]_{ij} x_j + \sum_{k=1}^d [P]_{ik} x_k\\
&amp;= ([P^T + P]\mathbf{x})_i.
\end{align*}\]</div>
<p>So the gradient of <span class="math notranslate nohighlight">\(f\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x})
= \frac{1}{2}[P + P^T] \,\mathbf{x}
+ \mathbf{q}.
\]</div>
<p>If <span class="math notranslate nohighlight">\(P\)</span> is symmetric, this further simplifies to
<span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}) = P \,\mathbf{x} + \mathbf{q}\)</span>.
<span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>It will be useful to compute the derivative of a function <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> of several variables along a parametric curve <span class="math notranslate nohighlight">\(\mathbf{g}(t) = (g_1(t), \ldots, g_d(t)) \in \mathbb{R}^d\)</span> for <span class="math notranslate nohighlight">\(t\)</span> in some closed interval of <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. The following result is a special case of an important fact. We will use the following notation <span class="math notranslate nohighlight">\(\mathbf{g}'(t) = (g_1'(t), \ldots, g_m'(t))\)</span>, where <span class="math notranslate nohighlight">\(g_i'\)</span> is the derivative of <span class="math notranslate nohighlight">\(g_i\)</span>. We say that <span class="math notranslate nohighlight">\(\mathbf{g}(t)\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(t = t_0\)</span> if each of its component is.</p>
<p><strong>EXAMPLE:</strong> <strong>(Parametric Line)</strong> The straight line between <span class="math notranslate nohighlight">\(\mathbf{x}_0 = (x_{0,1},\ldots,x_{0,d})\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x}_1 = (x_{1,1},\ldots,x_{1,d})\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> can be parametrized as</p>
<div class="math notranslate nohighlight">
\[
\mathbf{g}(t) = \mathbf{x}_0 + t (\mathbf{x}_1 - \mathbf{x}_0),
\]</div>
<p>where <span class="math notranslate nohighlight">\(t\)</span> goes from <span class="math notranslate nohighlight">\(0\)</span> (at which <span class="math notranslate nohighlight">\(\mathbf{g}(0) = \mathbf{x}_0\)</span>) to <span class="math notranslate nohighlight">\(1\)</span> (at which <span class="math notranslate nohighlight">\(\mathbf{g}(1) = \mathbf{x}_1\)</span>).</p>
<p>Then</p>
<div class="math notranslate nohighlight">
\[
g_i'(t) 
= \frac{\mathrm{d}}{\mathrm{d}t} [x_{0,i} + t (x_{1,i} - x_{0,i})]
= x_{1,i} - x_{0,i},
\]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[
\mathbf{g}'(t) = \mathbf{x}_1 - \mathbf{x}_0.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>Recall the <em>Chain Rule</em> in the single-variable case. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Chain_rule">Wikipedia</a>:</p>
<blockquote>
<div><p>The simplest form of the chain rule is for real-valued functions of one real variable. It states that if <span class="math notranslate nohighlight">\(g\)</span> is a function that is differentiable at a point <span class="math notranslate nohighlight">\(c\)</span> (i.e. the derivative <span class="math notranslate nohighlight">\(g'(c)\)</span> exists) and <span class="math notranslate nohighlight">\(f\)</span> is a function that is differentiable at <span class="math notranslate nohighlight">\(g(c)\)</span>, then the composite function <span class="math notranslate nohighlight">\({\displaystyle f\circ g}\)</span> is differentiable at <span class="math notranslate nohighlight">\(c\)</span>, and the derivative is <span class="math notranslate nohighlight">\({\displaystyle (f\circ g)'(c)=f'(g(c))\cdot g'(c)}\)</span>.</p>
</div></blockquote>
<p>Here is a straightforward generalization of the <em>Chain Rule</em>.</p>
<p><strong>THEOREM</strong> <strong>(Chain Rule)</strong> <span class="math notranslate nohighlight">\(\idx{chain rule}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D_1 \to \mathbb{R}\)</span>, where <span class="math notranslate nohighlight">\(D_1 \subseteq \mathbb{R}\)</span>, and let <span class="math notranslate nohighlight">\(g : D_2 \to \mathbb{R}\)</span>, where <span class="math notranslate nohighlight">\(D_2 \subseteq \mathbb{R}^d\)</span>. Assume that <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(g(\mathbf{x}_0)\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_1\)</span>, and that <span class="math notranslate nohighlight">\(g\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\nabla (f\circ g) (\mathbf{x}_0)
= f'(g(\mathbf{x}_0))
\nabla g(\mathbf{x}_0).
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> We apply the <em>Chain Rule</em> for functions of one variable to the partial derivatives. For all <span class="math notranslate nohighlight">\(i\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial}{\partial x_i}f (g(\mathbf{x}_0))
= f'(g(\mathbf{x}_0)) \frac{\partial}{\partial x_i} g(\mathbf{x}_0).
\]</div>
<p>Collecting the partial derivatives in a vector gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Here is a different generalization of the <em>Chain Rule</em>. Again the composition <span class="math notranslate nohighlight">\(f \circ \mathbf{g}\)</span> denotes the function <span class="math notranslate nohighlight">\(f \circ \mathbf{g}(t) = f (\mathbf{g}(t))\)</span>.</p>
<p><strong>THEOREM</strong> <strong>(Chain Rule)</strong> Let <span class="math notranslate nohighlight">\(f : D_1 \to \mathbb{R}\)</span>, where <span class="math notranslate nohighlight">\(D_1 \subseteq \mathbb{R}^d\)</span>, and let <span class="math notranslate nohighlight">\(\mathbf{g} : D_2 \to \mathbb{R}^d\)</span>, where <span class="math notranslate nohighlight">\(D_2 \subseteq \mathbb{R}\)</span>. Assume that <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{g}(t_0)\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_1\)</span>, and that <span class="math notranslate nohighlight">\(\mathbf{g}\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(t_0\)</span>, an interior point of <span class="math notranslate nohighlight">\(D_2\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
(f\circ \mathbf{g})'(t_0)
= \nabla f (\mathbf{g}(t_0))^T
\mathbf{g}'(t_0).
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof:</em> To simplify the notation, suppose that <span class="math notranslate nohighlight">\(f\)</span> is a real-valued function of <span class="math notranslate nohighlight">\(\mathbf{x} = (x_1, \ldots, x_d)\)</span> whose components are themselves functions of <span class="math notranslate nohighlight">\(t \in \mathbb{R}\)</span>. Assume <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}(t)\)</span>. To compute the <a class="reference external" href="https://en.wikipedia.org/wiki/Total_derivative">total derivative</a><span class="math notranslate nohighlight">\(\idx{total derivative}\xdi\)</span> <span class="math notranslate nohighlight">\(\frac{\mathrm{d} f(t)}{\mathrm{d} t}\)</span>, let <span class="math notranslate nohighlight">\(\Delta x_k = x_k(t + \Delta t) - x_k(t)\)</span>, <span class="math notranslate nohighlight">\(x_k = x_k(t)\)</span> and</p>
<div class="math notranslate nohighlight">
\[
\Delta f 
= 
f(x_1 + \Delta x_1, \ldots, x_d + \Delta x_d)
-
f(x_1, \ldots, x_d).
\]</div>
<p>We seek to compute the limit <span class="math notranslate nohighlight">\(\lim_{\Delta t \to 0} \frac{\Delta f}{\Delta t}\)</span>. To relate this limit to partial derivatives of <span class="math notranslate nohighlight">\(f\)</span>, we re-write <span class="math notranslate nohighlight">\(\Delta f\)</span> as a telescoping sum where each term involves variation of a single variable <span class="math notranslate nohighlight">\(x_k\)</span>. That is,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\Delta f
= 
&amp; [f(x_1 + \Delta x_1, \ldots, x_d + \Delta x_d)
-
f(x_1, x_2 + \Delta x_2, \ldots, x_d + \Delta x_d)]\\
&amp;+ [f(x_1, x_2 + \Delta x_2, \ldots, x_d + \Delta x_d)
-
f(x_1, x_2, x_3 + \Delta x_3, \ldots, x_d + \Delta x_d)]  \\
&amp;+ \cdots + 
[f(x_1, \cdots, x_{d-1}, x_d + \Delta x_d)
-
f(x_1, \ldots, x_d)].
\end{align*}\]</div>
<p>Applying the <em>Mean Value Theorem</em> to each term gives</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\Delta f
= 
&amp; \Delta x_1 
\frac{\partial f(x_1 + \theta_1 \Delta x_1, x_2 + \Delta x_2, \ldots, x_d + \Delta x_d)}
{\partial x_1}\\
&amp;+ \Delta x_2 
\frac{\partial f(x_1, x_2 + \theta_2 \Delta x_2, x_3 + \Delta x_3, \ldots, x_d + \Delta x_d)}
{\partial x_2}\\
&amp;+ \cdots + \Delta x_d
\frac{\partial f(x_1, \cdots, x_{d-1}, x_d + \theta_d \Delta x_d)}
{\partial x_d}
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(0 &lt; \theta_k &lt; 1\)</span> for <span class="math notranslate nohighlight">\(k=1,\ldots,d\)</span>. Dividing by <span class="math notranslate nohighlight">\(\Delta t\)</span>, taking the limit <span class="math notranslate nohighlight">\(\Delta t \to 0\)</span> and using the fact that <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable, we get</p>
<div class="math notranslate nohighlight">
\[
\frac{\mathrm{d} f (t)}{\mathrm{d} t}
= \sum_{k=1}^d \frac{\partial f(\mathbf{x}(t))}
{\partial x_k} \frac{\mathrm{d} x_k(t)}{\mathrm{d} t}.
\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p>As a first application of the <em>Chain Rule</em>, we generalize the <em>Mean Value Theorem</em> to the case of several variables. We will use this result later to prove a multivariable Taylor expansion result that will play a central role in this chapter.</p>
<p><strong>THEOREM</strong> <strong>(Mean Value)</strong> <span class="math notranslate nohighlight">\(\idx{mean value theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span>. Let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> and <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> be such that <span class="math notranslate nohighlight">\(B_\delta(\mathbf{x}_0) \subseteq D\)</span>. If <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable on <span class="math notranslate nohighlight">\(B_\delta(\mathbf{x}_0)\)</span>, then for any <span class="math notranslate nohighlight">\(\mathbf{x} \in B_\delta(\mathbf{x}_0)\)</span></p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x})
= f(\mathbf{x}_0) + \nabla f(\mathbf{x}_0 + \xi \mathbf{p})^T \mathbf{p} 
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\xi \in (0,1)\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{p} = \mathbf{x} - \mathbf{x}_0\)</span>. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>One way to think of the <em>Mean Value Theorem</em> is as a <span class="math notranslate nohighlight">\(0\)</span>-th order Taylor expansion. It says that, when <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is close to <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, the value <span class="math notranslate nohighlight">\(f(\mathbf{x})\)</span> is close to <span class="math notranslate nohighlight">\(f(\mathbf{x}_0)\)</span> in a way that can be controlled in terms of the gradient in the neighborhood of <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. From this point of view, the term <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}_0 + \xi \mathbf{p})^T \mathbf{p}\)</span> is called the Lagrange remainder.</p>
<p><em>Proof idea:</em> We apply the single-variable result and the <em>Chain Rule</em>.</p>
<p><em>Proof:</em> Let <span class="math notranslate nohighlight">\(\phi(t) = f(\boldsymbol{\alpha}(t))\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}(t) = \mathbf{x}_0 + t \mathbf{p}\)</span>. Observe that <span class="math notranslate nohighlight">\(\phi(0) = f(\mathbf{x}_0)\)</span> and <span class="math notranslate nohighlight">\(\phi(1) = f(\mathbf{x})\)</span>. By the <em>Chain Rule</em> and the parametric line example,</p>
<div class="math notranslate nohighlight">
\[
\phi'(t)
= \nabla f(\boldsymbol{\alpha}(t))^T \boldsymbol{\alpha}'(t)
= \nabla f(\boldsymbol{\alpha}(t))^T \mathbf{p}
= \nabla f(\mathbf{x}_0 + t \mathbf{p})^T \mathbf{p}.
\]</div>
<p>In particular, <span class="math notranslate nohighlight">\(\phi\)</span> has a continuous first derivative on <span class="math notranslate nohighlight">\([0,1]\)</span>. By the <em>Mean Value Theorem</em> in the single-variable case</p>
<div class="math notranslate nohighlight">
\[
\phi(t)
= \phi(0) + t \phi'(\xi)
\]</div>
<p>for some <span class="math notranslate nohighlight">\(\xi \in (0,t)\)</span>. Plugging in the expressions for <span class="math notranslate nohighlight">\(\phi(0)\)</span> and <span class="math notranslate nohighlight">\(\phi'(\xi)\)</span> and taking <span class="math notranslate nohighlight">\(t=1\)</span> gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
&#13;

<h2><span class="section-number">3.2.2. </span>Second-order derivatives<a class="headerlink" href="#second-order-derivatives" title="Link to this heading">#</a></h2>
<p>One can also define higher-order derivatives. We start with the single-variable case, where <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> with <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(x_0 \in D\)</span> is an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Note that, if <span class="math notranslate nohighlight">\(f'\)</span> exists in <span class="math notranslate nohighlight">\(D\)</span>, then it is itself a function of <span class="math notranslate nohighlight">\(x\)</span>. Then the second derivative at <span class="math notranslate nohighlight">\(x_0\)</span> is</p>
<div class="math notranslate nohighlight">
\[
f''(x_0)
= \frac{\mathrm{d}^2 f(x_0)}{\mathrm{d} x^2}
= \lim_{h \to 0} \frac{f'(x_0 + h) - f'(x_0)}{h}
\]</div>
<p>provided the limit exists.</p>
<p>In the several variable case, we have the following:</p>
<p><strong>DEFINITION</strong> <strong>(Second Partial Derivatives and Hessian)</strong> <span class="math notranslate nohighlight">\(\idx{second partial derivatives}\xdi\)</span> <span class="math notranslate nohighlight">\(\idx{Hessian}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Assume that <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable in an open ball around <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. Then <span class="math notranslate nohighlight">\(\partial f(\mathbf{x})/\partial x_i\)</span> is itself a function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and its partial derivative with respect to <span class="math notranslate nohighlight">\(x_j\)</span>, if it exists, is denoted by</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_j \partial x_i}
= \lim_{h \to 0} \frac{\frac{\partial f}{\partial x_i}(\mathbf{x}_0 + h \mathbf{e}_j) - \frac{\partial f}{\partial x_i}(\mathbf{x}_0)}{h}.
\]</div>
<p>To simplify the notation, we write this as <span class="math notranslate nohighlight">\(\partial^2 f(\mathbf{x}_0)/\partial x_i^2\)</span> when <span class="math notranslate nohighlight">\(j = i\)</span>. If <span class="math notranslate nohighlight">\(\partial^2 f(\mathbf{x})/\partial x_j \partial x_i\)</span> and <span class="math notranslate nohighlight">\(\partial^2 f(\mathbf{x})/\partial x_i^2\)</span> exist and are continuous in an open ball around <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> for all <span class="math notranslate nohighlight">\(i, j\)</span>, we say that <span class="math notranslate nohighlight">\(f\)</span> is twice continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>.</p>
<p>The matrix of second derivatives is called the Hessian and is denoted by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}_f(\mathbf{x}_0)
= \begin{pmatrix}
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_1^2} 
&amp; \cdots 
&amp; \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d \partial x_1}\\
\vdots &amp; \ddots &amp; \vdots\\
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_1 \partial x_d} 
&amp; \cdots 
&amp; \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_d^2}
\end{pmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>Like <span class="math notranslate nohighlight">\(f\)</span> and the gradient <span class="math notranslate nohighlight">\(\nabla f\)</span>, the Hessian <span class="math notranslate nohighlight">\(\mathbf{H}_f\)</span> is a function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. It is a matrix-valued function however.</p>
<p>When <span class="math notranslate nohighlight">\(f\)</span> is twice continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, its Hessian is a symmetric matrix.</p>
<p><strong>THEOREM</strong> <strong>(Symmetry of the Hessian)</strong> <span class="math notranslate nohighlight">\(\idx{symmetry of the Hessian}\xdi\)</span> Let <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span> where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x}_0 \in D\)</span> be an interior point of <span class="math notranslate nohighlight">\(D\)</span>. Assume that <span class="math notranslate nohighlight">\(f\)</span> is twice continuously differentiable at <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. Then for all <span class="math notranslate nohighlight">\(i \neq j\)</span></p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_j \partial x_i}
= \frac{\partial^2 f(\mathbf{x}_0)}{\partial x_i \partial x_j}.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> Two applications of the <em>Mean Value Theorem</em> show that the limits can be interchanged.</p>
<p><em>Proof:</em> By definition of the partial derivative,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial^2 f(\mathbf{x}_0)}{\partial x_j \partial x_i}
&amp;= \lim_{h_j \to 0} \frac{\frac{\partial f}{\partial x_i}(\mathbf{x}_0 + h_j \mathbf{e}_j) - \frac{\partial f}{\partial x_i}(\mathbf{x}_0)}{h_j}\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} \frac{1}{h_j h_i}
\left\{
[f(\mathbf{x}_0 + h_j \mathbf{e}_j + h_i \mathbf{e}_i)
- f(\mathbf{x}_0 + h_j \mathbf{e}_j)]
- [f(\mathbf{x}_0 + h_i \mathbf{e}_i)
- f(\mathbf{x}_0)]
\right\}\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} \frac{1}{h_i}
\left\{
\frac{[f(\mathbf{x}_0 + h_i \mathbf{e}_i + h_j \mathbf{e}_j)
- f(\mathbf{x}_0 + h_i \mathbf{e}_i)]
- [f(\mathbf{x}_0 + h_j \mathbf{e}_j)
- f(\mathbf{x}_0)]}{h_j}
\right\}\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} \frac{1}{h_i}
\left\{\frac{\partial}{\partial x_j}[f(\mathbf{x}_0 + h_i \mathbf{e}_i + \theta_j h_j \mathbf{e}_j)
- f(\mathbf{x}_0 + \theta_j h_j \mathbf{e}_j)]
\right\}\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} \frac{1}{h_i}
\left\{\frac{\partial f}{\partial x_j}(\mathbf{x}_0 + h_i \mathbf{e}_i + \theta_j h_j \mathbf{e}_j)
- \frac{\partial f}{\partial x_j}(\mathbf{x}_0 + \theta_j h_j \mathbf{e}_j)
\right\}
\end{align*}\]</div>
<p>for some <span class="math notranslate nohighlight">\(\theta_j \in (0,1)\)</span>. Note that, on the third line, we rearranged the terms and, on the fourth line, we applied the <em>Mean Value Theorem</em> to <span class="math notranslate nohighlight">\(f(\mathbf{x}_0 + h_i \mathbf{e}_i + h_j \mathbf{e}_j) - f(\mathbf{x}_0 + h_j \mathbf{e}_j)\)</span> as a continuously differentiable function of <span class="math notranslate nohighlight">\(h_j\)</span>.</p>
<p>Because <span class="math notranslate nohighlight">\(\partial f/\partial x_j\)</span> is continuously differentiable in an open ball around <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>, a second application of the <em>Mean Value Theorem</em> gives for some <span class="math notranslate nohighlight">\(\theta_i \in (0,1)\)</span></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
&amp;\lim_{h_j \to 0} \lim_{h_i \to 0} \frac{1}{h_i}
\left\{\frac{\partial f}{\partial x_j}(\mathbf{x}_0 + h_i \mathbf{e}_i + \theta_j h_j \mathbf{e}_j)
- \frac{\partial f}{\partial x_j}(\mathbf{x}_0 + \theta_j h_j \mathbf{e}_j)
\right\}\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} 
\frac{\partial}{\partial x_i}\left[\frac{\partial f}{\partial x_j}(\mathbf{x}_0 + \theta_j h_j \mathbf{e}_j + \theta_i h_i \mathbf{e}_i)\right]\\
&amp;= \lim_{h_j \to 0} \lim_{h_i \to 0} \frac{\partial^2 f(\mathbf{x}_0 + \theta_j h_j \mathbf{e}_j + \theta_i h_i \mathbf{e}_i)}{\partial x_i \partial x_j}.
\end{align*}\]</div>
<p>The claim then follows from the continuity of <span class="math notranslate nohighlight">\(\partial^2 f/\partial x_i \partial x_j\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>EXAMPLE:</strong> Consider the quadratic function</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x})
= \frac{1}{2} \mathbf{x}^T P \mathbf{x} + \mathbf{q}^T \mathbf{x} + r.
\]</div>
<p>Recall that the gradient of <span class="math notranslate nohighlight">\(f\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{x})
= \frac{1}{2}[P + P^T] \,\mathbf{x}
+ \mathbf{q}.
\]</div>
<p>To simplify the calculation, let <span class="math notranslate nohighlight">\(B = \frac{1}{2}[P + P^T]\)</span> and denote the rows of <span class="math notranslate nohighlight">\(B\)</span> by <span class="math notranslate nohighlight">\(\mathbf{b}_1^T, \ldots,\mathbf{b}_d^T\)</span>.</p>
<p>Each component of <span class="math notranslate nohighlight">\(\nabla f\)</span> is an affine function of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, specifically,</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial f (\mathbf{x})}{\partial x_i}
= \mathbf{b}_i^T \mathbf{x} + q_i. 
\]</div>
<p>Row <span class="math notranslate nohighlight">\(i\)</span> of the Hessian is simply the gradient transposed of <span class="math notranslate nohighlight">\(\frac{\partial f (\mathbf{x})}{\partial x_i}\)</span> which, by our previous results, is</p>
<div class="math notranslate nohighlight">
\[
\left(\nabla \frac{\partial f (\mathbf{x})}{\partial x_i}\right)^T
= \mathbf{b}_i^T.
\]</div>
<p>Putting this together we get</p>
<div class="math notranslate nohighlight">
\[
\mathbf{H}_f(\mathbf{x}) = \frac{1}{2}[P + P^T].
\]</div>
<p>Observe that this is indeed a symmetric matrix. <span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> What does it mean for a function <span class="math notranslate nohighlight">\(f\)</span> to be continuously differentiable at <span class="math notranslate nohighlight">\(x_0\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(f\)</span> is continuous at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>b) All partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> exist at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>c) All partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> exist and are continuous in an open ball around <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p>d) The gradient of <span class="math notranslate nohighlight">\(f\)</span> is zero at <span class="math notranslate nohighlight">\(x_0\)</span>.</p>
<p><strong>2</strong> What is the gradient of a function <span class="math notranslate nohighlight">\(f : D \to \mathbb{R}\)</span>, where <span class="math notranslate nohighlight">\(D \subseteq \mathbb{R}^d\)</span>, at a point <span class="math notranslate nohighlight">\(x_0 \in D\)</span>?</p>
<p>a) The rate of change of <span class="math notranslate nohighlight">\(f\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span></p>
<p>b) The vector of all second partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span></p>
<p>c) The vector of all first partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span></p>
<p>d) The matrix of all second partial derivatives of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span></p>
<p><strong>3</strong> Which of the following statements is true about the Hessian matrix of a twice continuously differentiable function?</p>
<p>a) It is always a diagonal matrix.</p>
<p>b) It is always a symmetric matrix.</p>
<p>c) It is always an invertible matrix.</p>
<p>d) It is always a positive definite matrix.</p>
<p><strong>4</strong> Let <span class="math notranslate nohighlight">\(f(x, y, z) = x^2 + y^2 - z^2\)</span>. What is the Hessian matrix of <span class="math notranslate nohighlight">\(f\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\begin{pmatrix} 2 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; -2 \end{pmatrix}\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\begin{pmatrix} 2x &amp; 2y &amp; -2z \end{pmatrix}\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\begin{pmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}\)</span></p>
<p><strong>5</strong> What is the Hessian matrix of the quadratic function <span class="math notranslate nohighlight">\(f(x) = \frac{1}{2}x^TPx + q^Tx + r\)</span>, where <span class="math notranslate nohighlight">\(P \in \mathbb{R}^{d \times d}\)</span> and <span class="math notranslate nohighlight">\(q \in \mathbb{R}^d\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(H_f(x) = P\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(H_f(x) = P^T\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(H_f(x) = \frac{1}{2}[P + P^T]\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(H_f(x) = [P + P^T]\)</span></p>
<p>Answer for 1: c. Justification: The text states, “If <span class="math notranslate nohighlight">\(f\)</span> exists and is continuous in an open ball around <span class="math notranslate nohighlight">\(x_0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>, then we say that <span class="math notranslate nohighlight">\(f\)</span> is continuously differentiable at <span class="math notranslate nohighlight">\(x_0\)</span>.”</p>
<p>Answer for 2: c. Justification: From the text: “The (column) vector <span class="math notranslate nohighlight">\(\nabla f(x_0) = ( \frac{\partial f(x_0)}{\partial x_1}, \ldots, \frac{\partial f(x_0)}{\partial x_d})\)</span> is called the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x_0\)</span>.”</p>
<p>Answer for 3: b). Justification: The text states: “When <span class="math notranslate nohighlight">\(f\)</span> is twice continuously differentiable at <span class="math notranslate nohighlight">\(x_0\)</span>, its Hessian is a symmetric matrix.”</p>
<p>Answer for 4: a). Justification: The Hessian is the matrix of second partial derivatives:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix} \frac{\partial^2 f}{\partial x^2} &amp; \frac{\partial^2 f}{\partial x \partial y} &amp; \frac{\partial^2 f}{\partial x \partial z} \\ \frac{\partial^2 f}{\partial y \partial x} &amp; \frac{\partial^2 f}{\partial y^2} &amp; \frac{\partial^2 f}{\partial y \partial z} \\ \frac{\partial^2 f}{\partial z \partial x} &amp; \frac{\partial^2 f}{\partial z \partial y} &amp; \frac{\partial^2 f}{\partial z^2} \end{pmatrix} = \begin{pmatrix} 2 &amp; 0 &amp; 0 \\ 0 &amp; 2 &amp; 0 \\ 0 &amp; 0 &amp; -2 \end{pmatrix}
\end{split}\]</div>
<p>Answer for 5: c. Justification: The text shows that the Hessian of the quadratic function is <span class="math notranslate nohighlight">\(H_f(x) = \frac{1}{2}[P + P^T]\)</span>.</p>
    
</body>
</html>