- en: 1.4\. Some observations about high-dimensional data#
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1.4\. 高维数据的一些观察#
- en: 原文：[https://mmids-textbook.github.io/chap01_intro/04_highdim/roch-mmids-intro-highdim.html](https://mmids-textbook.github.io/chap01_intro/04_highdim/roch-mmids-intro-highdim.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap01_intro/04_highdim/roch-mmids-intro-highdim.html](https://mmids-textbook.github.io/chap01_intro/04_highdim/roch-mmids-intro-highdim.html)
- en: In this section, we first apply \(k\)-means clustering to a high-dimensional
    example to illustrate the issues that arise in that context. We then discuss some
    surprising phenomena in high dimensions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先将 \(k\)-means 聚类应用于一个高维示例，以说明在该背景下出现的问题。然后我们讨论一些高维中的惊人现象。
- en: 1.4.1\. Clustering in high dimension[#](#clustering-in-high-dimension "Link
    to this heading")
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.1\. 高维空间中的聚类[#](#clustering-in-high-dimension "链接到这个标题")
- en: In this section, we test our implementation of \(k\)-means on a simple simulated
    dataset in high dimension.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们测试了我们在高维简单模拟数据集上实现的 \(k\)-means。
- en: The following function generates \(n\) data points from a mixture of two equally
    likely, spherical \(d\)-dimensional Gaussians with variance \(1\), one with mean
    \(-w\mathbf{e}_1\) and one with mean \(w \mathbf{e}_1\). We use `gmm2spherical`
    from a previous section. It is found in `mmids.py`.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数从两个等可能的、球形的 \(d\)-维高斯混合分布中生成 \(n\) 个数据点，方差为 \(1\)，一个以 \(-w\mathbf{e}_1\)
    为均值，另一个以 \(w \mathbf{e}_1\) 为均值。我们使用来自前一个部分的 `gmm2spherical`。它在 `mmids.py` 中。
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**NUMERICAL CORNER:** We start with \(d=2\).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落:** 我们从 \(d=2\) 开始。'
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s run \(k\)-means on this dataset using \(k=2\). We use `kmeans()` from
    the `mmids.py` file.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在数据集上运行 \(k=2\) 的 \(k\)-means。我们使用来自 `mmids.py` 文件的 `kmeans()` 函数。
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Our default of \(10\) iterations seem to have been enough for the algorithm
    to converge. We can visualize the result by [coloring](https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html)
    the points according to the assignment.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们默认的 \(10\) 次迭代似乎已经足够算法收敛。我们可以通过根据分配给点着色来可视化结果。
- en: '[PRE4]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![../../_images/9aa59ca573eb1a92b3e764e96b1d04efe81f825b67863353e16d5736d4608fb4.png](../Images/aa56b156c7bbd5cc401744d4eec4820d.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/9aa59ca573eb1a92b3e764e96b1d04efe81f825b67863353e16d5736d4608fb4.png](../Images/aa56b156c7bbd5cc401744d4eec4820d.png)'
- en: Let’s see what happens in higher dimension. We repeat our experiment with \(d=1000\).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在更高维度会发生什么。我们重复我们的实验，\(d=1000\)。
- en: '[PRE5]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Again, we observe two clearly delineated clusters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们观察到两个清晰划分的簇。
- en: '[PRE6]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![../../_images/264709bc9d4749da24e2b1bc73c5716f8d150355fba2e13f70a3906c858baa7f.png](../Images/2555114660b70c347ebf0d27a90440a0.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/264709bc9d4749da24e2b1bc73c5716f8d150355fba2e13f70a3906c858baa7f.png](../Images/2555114660b70c347ebf0d27a90440a0.png)'
- en: This dataset is in \(1000\) dimensions, but we’ve plotted the data in only the
    first two dimensions. If we plot in any two dimensions not including the first
    one instead, we see only one cluster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集有 \(1000\) 维，但我们只绘制了前两个维度的数据。如果我们绘制不包括第一个维度的任意两个维度，我们只能看到一个簇。
- en: '[PRE7]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![../../_images/21494a7e3aca56828d3ea264f5b728df4a6ba072eff22b28e743010117504049.png](../Images/2ceed7ae94ba461478dde0058309f68a.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/21494a7e3aca56828d3ea264f5b728df4a6ba072eff22b28e743010117504049.png](../Images/2ceed7ae94ba461478dde0058309f68a.png)'
- en: Let’s see how \(k\)-means fares on this dataset.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 \(k\)-means 在这个数据集上的表现如何。
- en: '[PRE8]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our attempt at clustering does not appear to have been successful.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试的聚类似乎没有成功。
- en: '[PRE10]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![../../_images/dc448ee379a7ca994bcd89a9ef7e80c567cd1e8704627661a871effd41fa9f11.png](../Images/085a8dadcd6ad71b3fc7e6a1a02fe4c0.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/dc448ee379a7ca994bcd89a9ef7e80c567cd1e8704627661a871effd41fa9f11.png](../Images/085a8dadcd6ad71b3fc7e6a1a02fe4c0.png)'
- en: \(\unlhd\)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 'What happened? While the clusters are easy to tease apart *if we know to look
    at the first coordinate only*, in the full space the within-cluster and between-cluster
    distances become harder to distinguish: the noise overwhelms the signal.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么？虽然如果我们只看第一个坐标，簇很容易区分，但在整个空间中，簇内和簇间的距离变得难以区分：噪声压倒了信号。
- en: As the dimension increases, the distributions of intra-cluster and inter-cluster
    distances overlap significantly and become more or less indistinguishable. That
    provides some insights into why clustering may fail here. Note that we used the
    same offset for all simulations. On the other hand, if the separation between
    the clusters is sufficiently large, one would expect clustering to work even in
    high dimension.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 随着维度的增加，簇内和簇间距离的分布显著重叠，变得或多或少无法区分。这为我们提供了关于为什么聚类可能在这里失败的一些见解。请注意，我们为所有模拟使用了相同的偏移量。另一方面，如果簇之间的分离足够大，人们会期望聚类即使在高维度下也能工作。
- en: '![Histograms of within-cluster and between-cluster distances for a sample of
    size  in  (left) and  (right) dimensions with a given offset . As  increases,
    the two distributions become increasingly indistinguishable.](../Images/f5f988f0b8742e52a610ea9e41c65370.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![样本大小为 n 的簇内和簇间距离的直方图，在 (左) 和 (右) 维度上具有给定的偏移量。随着 n 的增加，两个分布变得越来越难以区分。](../Images/f5f988f0b8742e52a610ea9e41c65370.png)'
- en: '**TRY IT!** What precedes (and what follows in the next subsection) is not
    a formal proof that \(k\)-means clustering will be unsuccessful here. The behavior
    of the algorithm is quite complex and depends, in particular, on the initialization
    and the density of points. Here, increasing the number of data points eventually
    leads to a much better performance. Explore this behavior on your own by modifying
    the code. (For some theoretical justifications (beyond this course), see [here](https://arxiv.org/pdf/0912.0086.pdf)
    and [here](http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf).)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**尝试一下！** 以下内容（以及下一小节中的内容）并不是 \(k\)-means 聚类在这里将失败的形式化证明。算法的行为相当复杂，并且特别依赖于初始化和点的密度。在这里，增加数据点的数量最终会导致性能显著提高。通过修改代码来探索这种行为。（有关一些理论依据（超出本课程范围），请参阅[这里](https://arxiv.org/pdf/0912.0086.pdf)和[这里](http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf)。）'
- en: '**CHAT & LEARN** According to Claude, here is how a cat might summarize the
    situation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与学习** 根据克劳德的说法，一只猫可能会这样总结情况：'
- en: '**Figure:** A kitten in space (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：** 太空中的小猫 (*来源：使用 [Midjourney](https://www.midjourney.com/) 制作*) '
- en: '![Kitty in space](../Images/0834716500ee9672d75bf25880399171.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![猫咪在太空](../Images/0834716500ee9672d75bf25880399171.png)'
- en: \(\bowtie\)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: “I iz in high-dimensional space
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: “我在高维空间里”
- en: All dese data points, everywheres
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些数据点，无处不在
- en: But no matter how far I roams
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但无论我走得多远
- en: They all look the sames to me!”
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，它们看起来都一样！”
- en: \(\ddagger\)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \(\ddagger\)
- en: 1.4.2\. Surprising phenomena in high dimension[#](#surprising-phenomena-in-high-dimension
    "Link to this heading")
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.2\. 高维空间中的惊人现象[#](#surprising-phenomena-in-high-dimension "链接到本标题")
- en: a high-dimensional space is a lonely place
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 高维空间是一个孤独的地方
- en: ''
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Bernhard Schölkopf (@bschoelkopf) [August 24, 2014](https://twitter.com/bschoelkopf/status/503554842829549568?ref_src=twsrc%5Etfw)
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — Bernhard Schölkopf (@bschoelkopf) [2014年8月24日](https://twitter.com/bschoelkopf/status/503554842829549568?ref_src=twsrc%5Etfw)
- en: In the previous section, we saw how the contribution from a large number of
    “noisy dimensions” can overwhelm the “signal” in the context of clustering. In
    this section we discuss further properties of high-dimensional space that are
    relevant to data science problems.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了大量“噪声维度”的贡献如何在聚类上下文中压倒“信号”。在本节中，我们将讨论与数据科学问题相关的高维空间的其他属性。
- en: 'Applying *Chebyshev’s Inequality* to sums of independent random variables has
    useful statistical implications: it shows that, with a large enough number of
    samples \(n\), the sample mean is close to the population mean. Hence it allows
    us to infer properties of a population from samples. Interestingly, one can apply
    a similar argument to a different asymptotic regime: the limit of large dimension
    \(d\). But as we will see in this section, the statistical implications are quite
    different.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *切比雪夫不等式* 应用于独立随机变量的和，具有有用的统计意义：它表明，当样本数量 \(n\) 足够大时，样本均值接近总体均值。因此，它允许我们从样本中推断总体的性质。有趣的是，可以将类似的论点应用于不同的渐近区域：大维度
    \(d\) 的极限。但正如我们将在本节中看到的，统计意义相当不同。
- en: 'To start explaining the quote above, we consider a simple experiment. Let \(\mathcal{C}
    = [-1/2,1/2]^d\) be the \(d\)-cube with side lengths \(1\) centered at the origin
    and let \(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\|\leq 1/2\}\)
    be the inscribed \(d\)-ball.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解释上述引言，我们考虑一个简单的实验。设 \(\mathcal{C} = [-1/2,1/2]^d\) 为以原点为中心、边长为 \(1\) 的 \(d\)
    维立方体，设 \(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\|\leq 1/2\}\)
    为内嵌的 \(d\) 维球体。'
- en: Now pick a point \(\mathbf{X}\) uniformly at random in \(\mathcal{C}\). What
    is the probability that it falls in \(\mathcal{B}\)?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在在 \(\mathcal{C}\) 中随机均匀地选择一个点 \(\mathbf{X}\)。它落在 \(\mathcal{B}\) 中的概率是多少？
- en: To generate \(\mathbf{X}\), we pick \(d\) independent random variables \(X_1,
    \ldots, X_d \sim \mathrm{U}[-1/2, 1/2]\), and form the vector \(\mathbf{X} = (X_1,
    \ldots, X_d)\). Indeed, the PDF of \(\mathbf{X}\) is then \(f_{\mathbf{X}}(\mathbf{x})=
    1^d = 1\) if \(\mathbf{x} \in \mathcal{C}\) and \(0\) otherwise.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成 \(\mathbf{X}\)，我们选择 \(d\) 个独立的随机变量 \(X_1, \ldots, X_d \sim \mathrm{U}[-1/2,
    1/2]\)，并形成向量 \(\mathbf{X} = (X_1, \ldots, X_d)\)。实际上，\(\mathbf{X}\) 的概率密度函数 \(f_{\mathbf{X}}(\mathbf{x})=
    1^d = 1\) 如果 \(\mathbf{x} \in \mathcal{C}\) 并且否则为 \(0\)。
- en: The event we are interested in is \(A = \left\{\|\mathbf{X}\| \leq 1/2\right\}\).
    The uniform distribution over the set \(\mathcal{C}\) has the property that \(\mathbb{P}[A]\)
    is the volume of \(\mathcal{B}\) divided by the volume of \(\mathcal{C}\). In
    this case, the volume of \(\mathcal{C}\) is \(1^d = 1\) and the volume of \(\mathcal{B}\)
    has an [explicit formula](https://en.wikipedia.org/wiki/Volume_of_an_n-ball).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的事件是 \(A = \left\{\|\mathbf{X}\| \leq 1/2\right\}\)。在集合 \(\mathcal{C}\)
    上的均匀分布具有这样的性质：\(\mathbb{P}[A]\) 是 \(\mathcal{B}\) 的体积除以 \(\mathcal{C}\) 的体积。在这种情况下，\(\mathcal{C}\)
    的体积是 \(1^d = 1\)，而 \(\mathcal{B}\) 的体积有一个 [显式公式](https://en.wikipedia.org/wiki/Volume_of_an_n-ball)。
- en: 'This leads to the following surprising fact:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下令人惊讶的事实：
- en: '**THEOREM** **(High-dimensional Cube)** \(\idx{high-dimensional cube theorem}\xdi\)
    Let \(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d \,:\, \|\mathbf{x}\|\leq 1/2\}\)
    and \(\mathcal{C} = [-1/2,1/2]^d\). Pick \(\mathbf{X} \sim \mathrm{U}[\mathcal{C}]\).
    Then, as \(d \to +\infty\),'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(高维立方体)** \(\idx{high-dimensional cube theorem}\xdi\) 设 \(\mathcal{B}
    = \{\mathbf{x} \in \mathbb{R}^d \,:\, \|\mathbf{x}\|\leq 1/2\}\) 和 \(\mathcal{C}
    = [-1/2,1/2]^d\)。选择 \(\mathbf{X} \sim \mathrm{U}[\mathcal{C}]\)。那么，当 \(d \to +\infty\)
    时，'
- en: \[ \mathbb{P}[\mathbf{X} \in \mathcal{B}] \to 0. \]
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{P}[\mathbf{X} \in \mathcal{B}] \to 0. \]
- en: \(\sharp\)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: In words, in high dimension if one picks a point at random from the cube, it
    is unlikely to be close to the origin. Instead it is likely to be in the corners.
    A geometric interpretation is that a high-dimensional cube is a bit like a “spiky
    ball.”
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 用话来说，在高维空间中，如果随机从立方体中选取一个点，它不太可能靠近原点。相反，它更有可能位于角落。几何解释是，高维立方体有点像“尖锐的球体。”
- en: '**Figure:** Visualization of a high-dimensional cube as a spiky ball (*Credit:*
    Made with [Midjourney](https://www.midjourney.com/))'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**图:** 高维立方体作为尖锐球体的可视化 (*来源:* 使用 [Midjourney](https://www.midjourney.com/))'
- en: '![Visualization of a high-dimensional cube](../Images/0ed7cc7c9c5326071c245e739ab01a88.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![高维立方体的可视化](../Images/0ed7cc7c9c5326071c245e739ab01a88.png)'
- en: \(\bowtie\)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: We give a proof based on *Chebyshev’s Inequality*. It has the advantage of providing
    some insight into this counter-intuitive phenomenon by linking it to the concentration
    of sums of independent random variables, in this case the squared norm of \(\mathbf{X}\).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于 *切比雪夫不等式* 给出证明。它的优点是通过对独立随机变量和的集中现象进行联系，为这一反直觉现象提供了一些洞察，在这种情况下是 \(\mathbf{X}\)
    的平方范数。
- en: '*Proof idea:* We think of \(\|\mathbf{X}\|^2\) as a sum of independent random
    variables and apply *Chebyshev’s Inequality*. It implies that the norm of \(\mathbf{X}\)
    is concentrated around its mean, which grows like \(\sqrt{d}\). The latter is
    larger than \(1/2\) for \(d\) large.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路:* 我们将 \(\|\mathbf{X}\|^2\) 视为一个独立随机变量的和，并应用 *切比雪夫不等式*。它表明 \(\mathbf{X}\)
    的范数集中在其均值附近，其增长类似于 \(\sqrt{d}\)。对于大的 \(d\)，后者大于 \(1/2\)。'
- en: '*Proof:* To see the relevance of *Chebyshev’s Inequality*, we compute the mean
    and standard deviation of the norm of \(\mathbf{X}\). In fact, because of the
    square root in \(\|\mathbf{X}\|\), computing its expectation is difficult. Instead
    we work with the squared norm'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 为了看到 *切比雪夫不等式* 的相关性，我们计算 \(\mathbf{X}\) 的范数的均值和标准差。实际上，由于 \(\|\mathbf{X}\|\)
    中的平方根，计算其期望是困难的。因此，我们处理平方范数'
- en: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
- en: which has the advantage of being a sum of independent random variables – for
    which the expectation and variance are much easier to compute. Observe further
    that the probability of the event of interest \(\{\|\mathbf{X}\| \leq 1/2\}\)
    can be re-written in terms of \(\|\mathbf{X}\|^2\) as follows
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这具有作为独立随机变量和的优势——对于这些变量的期望和方差更容易计算。进一步观察，感兴趣的事件的概率 \(\{\|\mathbf{X}\| \leq 1/2\}\)
    可以用 \(\|\mathbf{X}\|^2\) 表示如下
- en: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}
    \left[ \|\mathbf{X}\|^2 \leq 1/4 \right]. \end{align*}\]
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}
    \left[ \|\mathbf{X}\|^2 \leq 1/4 \right]. \end{align*}\]
- en: To simplify the notation, we use \(\tilde\mu = \mathbb{E}[X_1^2]\) and \(\tilde\sigma
    = \sqrt{\mathrm{Var}[X_1^2]}\) for the mean and standard deviation of \(X_1^2\)
    respectively. Using linearity of expectation and the fact that the \(X_i\)’s are
    independent, we get
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号，我们使用 \(\tilde\mu = \mathbb{E}[X_1^2]\) 和 \(\tilde\sigma = \sqrt{\mathrm{Var}[X_1^2]}\)
    分别表示 \(X_1^2\) 的均值和标准差。利用期望的线性性和 \(X_i\) 的独立性，我们得到
- en: \[ \mu_{\|\mathbf{X}\|^2} = \mathbb{E}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d
    \mathbb{E}[X_i^2] = d \,\mathbb{E}[X_1^2] = \tilde\mu \, d, \]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mu_{\|\mathbf{X}\|^2} = \mathbb{E}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d
    \mathbb{E}[X_i^2] = d \,\mathbb{E}[X_1^2] = \tilde\mu \, d, \]
- en: and
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathrm{Var}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d \mathrm{Var}[X_i^2]
    = d \,\mathrm{Var}[X_1^2]. \]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{Var}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d \mathrm{Var}[X_i^2]
    = d \,\mathrm{Var}[X_1^2]. \]
- en: Taking a square root, we get an expression for the standard deviation of our
    quantity of interest \(\|\mathbf{X}\|^2\) in terms of the standard deviation of
    \(X_1^2\)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 取平方根，我们得到我们感兴趣的数量 \(\|\mathbf{X}\|^2\) 的标准差的表达式，它是 \(X_1^2\) 的标准差的函数
- en: \[ \sigma_{\|\mathbf{X}\|^2} = \tilde\sigma \, \sqrt{d}. \]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_{\|\mathbf{X}\|^2} = \tilde\sigma \, \sqrt{d}. \]
- en: (Note that we could compute \(\tilde\mu\) and \(\tilde\sigma\) explicitly, but
    it will not be necessary here.)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: （注意，我们可以显式地计算 \(\tilde\mu\) 和 \(\tilde\sigma\)，但在这里将不是必要的。）
- en: We use *Chebyshev’s Inequality* to show that \(\|\mathbf{X}\|^2\) is highly
    likely to be close to its mean \(\tilde\mu \, d\), which is much larger than \(1/4\)
    when \(d\) is large. And that therefore \(\|\mathbf{X}\|^2\) is highly unlikely
    to be smaller than \(1/4\). We give the details next.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 *切比雪夫不等式* 来证明 \(\|\mathbf{X}\|^2\) 很有可能接近其均值 \(\tilde\mu \, d\)，当 \(d\)
    很大时，这个均值远大于 \(1/4\)。因此，\(\|\mathbf{X}\|^2\) 很不可能小于 \(1/4\)。我们将在下文中给出详细说明。
- en: By the one-sided version of *Chebyshev’s Inequality* in terms of the standard
    deviation, we have
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据标准差的单边版本 *切比雪夫不等式*，我们有
- en: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 - \mu_{\|\mathbf{X}\|^2} \leq - \alpha
    \right] \leq \left(\frac{\sigma_{\|\mathbf{X}\|^2}}{\alpha}\right)^2. \]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 - \mu_{\|\mathbf{X}\|^2} \leq - \alpha
    \right] \leq \left(\frac{\sigma_{\|\mathbf{X}\|^2}}{\alpha}\right)^2. \]
- en: That is, using the formulas above and rearranging slightly,
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是，使用上述公式并稍作整理，
- en: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha \right] \leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2. \]
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha \right] \leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2. \]
- en: How do we relate this to the probability of interest \(\mathbb{P}\left[\|\mathbf{X}\|^2
    \leq 1/4\right]\)? Recall that we are free to choose \(\alpha\) in this inequality.
    So simply take \(\alpha\) such that
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将此与感兴趣的概率 \(\mathbb{P}\left[\|\mathbf{X}\|^2 \leq 1/4\right]\) 相关联？回想一下，我们可以自由选择这个不等式中的
    \(\alpha\)。因此，简单地取 \(\alpha\) 使得
- en: \[ \tilde\mu \,d - \alpha = \frac{1}{4}, \]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \tilde\mu \,d - \alpha = \frac{1}{4}, \]
- en: that is, \(\alpha = \tilde\mu \,d - 1/4\). Observe that, once \(d\) is large
    enough, it holds that \(\alpha > 0\).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 即，\(\alpha = \tilde\mu \,d - 1/4\)。观察一下，一旦 \(d\) 足够大，就成立 \(\alpha > 0\)。
- en: Finally, replacing this choice of \(\alpha\) in the inequality above gives
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将这个 \(\alpha\) 的选择代入上述不等式中
- en: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}\left[\|\mathbf{X}\|^2
    \leq 1/4\right]\\ &= \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
    \right]\\ &\leq \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2\\ &\leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\tilde\mu \,d - 1/4}\right)^2. \end{align*}\]
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}\left[\|\mathbf{X}\|^2
    \leq 1/4\right]\\ &= \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
    \right]\\ &\leq \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2\\ &\leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\tilde\mu \,d - 1/4}\right)^2. \end{align*}\]
- en: Critically, \(\tilde\mu\) and \(\tilde\sigma\) do not depend on \(d\). So the
    right-hand side goes to \(0\) as \(d \to +\infty\). Indeed, \(d\) is much larger
    than \(\sqrt{d}\) when \(d\) is large. That proves the claim.\(\square\)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，\(\tilde\mu\) 和 \(\tilde\sigma\) 不依赖于 \(d\)。因此，当 \(d \to +\infty\) 时，右侧趋于
    \(0\)。实际上，当 \(d\) 很大时，\(d\) 远大于 \(\sqrt{d}\)。这证明了该命题。\(\square\)
- en: We will see later in the course that this high-dimensional phenomenon has implications
    for data science problems. It is behind what is referred to as the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\(\idx{curse
    of dimensionality}\xdi\).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在课程的后半部分，我们将看到这一高维现象对数据科学问题有影响。它是所谓的[维度诅咒](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\(\idx{维度诅咒}\xdi\)的背景。
- en: While *Chebyshev’s inequality* correctly implies that \(\mathbb{P}[\mathbf{X}
    \in \mathcal{B}]\) goes to \(0\), it does not give the correct rate of convergence.
    In reality, that probability goes to \(0\) at a much faster rate than \(1/d\).
    Specifically, [it can be shown](https://en.wikipedia.org/wiki/Volume_of_an_n-ball#High_dimensions)
    that \(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\) goes to \(0\) roughly as \(d^{-d/2}\).
    We will not derive (or need) this fact here.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 *切比雪夫不等式* 正确地暗示了 \(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\) 趋向于 \(0\)，但它并没有给出正确的收敛速率。实际上，这个概率以比
    \(1/d\) 更快的速度趋向于 \(0\)。具体来说，[可以证明](https://en.wikipedia.org/wiki/Volume_of_an_n-ball#High_dimensions)
    \(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\) 大约以 \(d^{-d/2}\) 的速率趋向于 \(0\)。我们在这里不会推导（或需要）这个事实。
- en: '**NUMERICAL CORNER:** We can check the theorem in a simulation. Here we pick
    \(n\) points uniformly at random in the \(d\)-cube \(\mathcal{C}\), for a range
    of dimensions up to `dmax`. We then plot the frequency of landing in the inscribed
    \(d\)-ball \(\mathcal{B}\) and see that it rapidly converges to \(0\). Alternatively,
    we could just plot the formula for the volume of \(\mathcal{B}\). But knowing
    how to do simulations is useful in situations where explicit formulas are unavailable
    or intractable. We plot the result up to dimension \(10\).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**：我们可以在模拟中检查定理。在这里，我们在 \(d\)-立方体 \(\mathcal{C}\) 中随机均匀地选择 \(n\) 个点，范围包括直到
    `dmax` 的维度。然后我们绘制落在内切 \(d\)-球 \(\mathcal{B}\) 中的频率，并看到它迅速收敛到 \(0\)。或者，我们也可以只绘制
    \(\mathcal{B}\) 的体积公式。但了解如何进行模拟在显式公式不可用或难以处理的情况下是有用的。我们绘制了直到维度 \(10\) 的结果。'
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![../../_images/d08f6e8ae99d1fc47a60ae906d9afb4fb92b64d9611fda98fd7415d2bc98a67d.png](../Images/c7ed85ca4cdd1727d357fd1139869726.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/d08f6e8ae99d1fc47a60ae906d9afb4fb92b64d9611fda98fd7415d2bc98a67d.png](../Images/c7ed85ca4cdd1727d357fd1139869726.png)'
- en: \(\unlhd\)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(有Claude、Gemini和ChatGPT的帮助)*'
- en: '**1** The volume of the \(d\)-dimensional cube \(C = [-1/2, 1/2]^d\) is:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** \(d\)-维立方体 \(C = [-1/2, 1/2]^d\) 的体积是：'
- en: a) \(1/d\)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(1/d\)
- en: b) \(1/2^d\)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(1/2^d\)
- en: c) 1
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: c) 1
- en: d) \(2^d\)
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(2^d\)
- en: '**2** In a high-dimensional cube \(C = [-1/2, 1/2]^d\), as the dimension \(d\)
    increases, the probability that a randomly chosen point lies within the inscribed
    sphere \(B = \{x \in \mathbb{R}^d : \|x\| \le 1/2\}\):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 在高维立方体 \(C = [-1/2, 1/2]^d\) 中，随着维度 \(d\) 的增加，随机选择一个点落在内切球 \(B = \{x
    \in \mathbb{R}^d : \|x\| \le 1/2\}\) 内的概率：'
- en: a) Approaches 1
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: a) 接近1
- en: b) Approaches 1/2
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: b) 接近1/2
- en: c) Approaches 0
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: c) 接近0
- en: d) Remains constant
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: d) 保持不变
- en: '**3** Which of the following best describes the appearance of a high-dimensional
    cube?'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 以下哪个选项最能描述高维立方体的外观？'
- en: a) A smooth, round ball
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: a) 一个光滑的、圆形的球体
- en: b) A spiky ball with most of its volume concentrated in the corners
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: b) 大部分体积集中在角落的尖锐球体
- en: c) A perfect sphere with uniform volume distribution
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: c) 具有均匀体积分布的完美球体
- en: d) A flat, pancake-like shape
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: d) 一个扁平的、类似饼干的形状
- en: '**4** Which inequality is used to prove the theorem about high-dimensional
    cubes?'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 用来证明高维立方体定理的不等式是哪一个？'
- en: a) Cauchy-Schwarz inequality
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: a) 库西-施瓦茨不等式
- en: b) Triangle inequality
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: b) 三角不等式
- en: c) Markov’s inequality
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: c) 马尔可夫不等式
- en: d) Chebyshev’s inequality
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: d) 切比雪夫不等式
- en: '**5** In the proof of the theorem about high-dimensional cubes, which property
    of the squared norm \(\|X\|^2\) is used?'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在高维立方体定理的证明中，使用了平方范数 \(\|X\|^2\) 的哪个性质？'
- en: a) It is a sum of dependent random variables.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它是相关随机变量的和。
- en: b) It is a sum of independent random variables.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它是独立随机变量的和。
- en: c) It is a product of independent random variables.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它是独立随机变量的乘积。
- en: d) It is a product of dependent random variables.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它是相关随机变量的乘积。
- en: 'Answer for 1: c. Justification: The side length of the cube is 1, and the volume
    of a \(d\)-dimensional cube is the side length raised to the power \(d\).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 1题的答案：c. 证明：立方体的边长为1，\(d\)-维立方体的体积是边长的 \(d\) 次幂。
- en: 'Answer for 2: c. Justification: This is the statement of the theorem “High-dimensional
    Cube” in the text.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 2题的答案：c. 证明：这是文中“高维立方体”定理的陈述。
- en: 'Answer for 3: b. Justification: The text mentions, “A geometric interpretation
    is that a high-dimensional cube is a bit like a ‘spiky ball.’”'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 3题的答案：b. 证明：文中提到，“一个高维立方体有点像‘尖锐的球体’。”
- en: 'Answer for 4: d. Justification: The text explicitly states that Chebyshev’s
    inequality is used in the proof.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 4题的答案：d. 证明：文中明确指出在证明中使用了切比雪夫不等式。
- en: 'Answer for 5: b. Justification: The proof states, “we work with the squared
    norm'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 5题的答案：b. 证明：证明中提到，“我们使用平方范数”
- en: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
- en: which has the advantage of being a sum of independent random variables.”
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这具有作为独立随机变量之和的优势。”
- en: 1.4.1\. Clustering in high dimension[#](#clustering-in-high-dimension "Link
    to this heading")
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.1\. 高维聚类[#](#clustering-in-high-dimension "链接到这个标题")
- en: In this section, we test our implementation of \(k\)-means on a simple simulated
    dataset in high dimension.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们在高维的简单模拟数据集上测试我们的 \(k\)-means 实现。
- en: The following function generates \(n\) data points from a mixture of two equally
    likely, spherical \(d\)-dimensional Gaussians with variance \(1\), one with mean
    \(-w\mathbf{e}_1\) and one with mean \(w \mathbf{e}_1\). We use `gmm2spherical`
    from a previous section. It is found in `mmids.py`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数从两个等可能的球形 \(d\) 维高斯混合分布中生成 \(n\) 个数据点，方差为 \(1\)，一个以 \(-w\mathbf{e}_1\) 为均值，另一个以
    \(w \mathbf{e}_1\) 为均值。我们使用来自前一个部分的 `gmm2spherical`。它在 `mmids.py` 中。
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**NUMERICAL CORNER:** We start with \(d=2\).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们从 \(d=2\) 开始。'
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s run \(k\)-means on this dataset using \(k=2\). We use `kmeans()` from
    the `mmids.py` file.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 \(k=2\) 在这个数据集上运行 \(k\)-means。我们使用来自 `mmids.py` 文件的 `kmeans()` 函数。
- en: '[PRE14]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Our default of \(10\) iterations seem to have been enough for the algorithm
    to converge. We can visualize the result by [coloring](https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html)
    the points according to the assignment.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们默认的 \(10\) 次迭代似乎已经足够算法收敛。我们可以通过根据分配给点着色来可视化结果。
- en: '[PRE16]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![../../_images/9aa59ca573eb1a92b3e764e96b1d04efe81f825b67863353e16d5736d4608fb4.png](../Images/aa56b156c7bbd5cc401744d4eec4820d.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/9aa59ca573eb1a92b3e764e96b1d04efe81f825b67863353e16d5736d4608fb4.png](../Images/aa56b156c7bbd5cc401744d4eec4820d.png)'
- en: Let’s see what happens in higher dimension. We repeat our experiment with \(d=1000\).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在高维度会发生什么。我们重复我们的实验，\(d=1000\)。
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Again, we observe two clearly delineated clusters.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们观察到两个清晰划分的簇。
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![../../_images/264709bc9d4749da24e2b1bc73c5716f8d150355fba2e13f70a3906c858baa7f.png](../Images/2555114660b70c347ebf0d27a90440a0.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/264709bc9d4749da24e2b1bc73c5716f8d150355fba2e13f70a3906c858baa7f.png](../Images/2555114660b70c347ebf0d27a90440a0.png)'
- en: This dataset is in \(1000\) dimensions, but we’ve plotted the data in only the
    first two dimensions. If we plot in any two dimensions not including the first
    one instead, we see only one cluster.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集在 \(1000\) 维，但我们只在前两个维度上绘制了数据。如果我们不在包括第一个维度在内的任意两个维度上绘制，我们只会看到一个簇。
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![../../_images/21494a7e3aca56828d3ea264f5b728df4a6ba072eff22b28e743010117504049.png](../Images/2ceed7ae94ba461478dde0058309f68a.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/21494a7e3aca56828d3ea264f5b728df4a6ba072eff22b28e743010117504049.png](../Images/2ceed7ae94ba461478dde0058309f68a.png)'
- en: Let’s see how \(k\)-means fares on this dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 \(k\)-means 在这个数据集上的表现如何。
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our attempt at clustering does not appear to have been successful.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试的聚类似乎没有成功。
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![../../_images/dc448ee379a7ca994bcd89a9ef7e80c567cd1e8704627661a871effd41fa9f11.png](../Images/085a8dadcd6ad71b3fc7e6a1a02fe4c0.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/dc448ee379a7ca994bcd89a9ef7e80c567cd1e8704627661a871effd41fa9f11.png](../Images/085a8dadcd6ad71b3fc7e6a1a02fe4c0.png)'
- en: \(\unlhd\)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 'What happened? While the clusters are easy to tease apart *if we know to look
    at the first coordinate only*, in the full space the within-cluster and between-cluster
    distances become harder to distinguish: the noise overwhelms the signal.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么？当只查看第一个坐标时，簇很容易区分，但在整个空间中，簇内和簇间的距离变得难以区分：噪声掩盖了信号。
- en: As the dimension increases, the distributions of intra-cluster and inter-cluster
    distances overlap significantly and become more or less indistinguishable. That
    provides some insights into why clustering may fail here. Note that we used the
    same offset for all simulations. On the other hand, if the separation between
    the clusters is sufficiently large, one would expect clustering to work even in
    high dimension.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 随着维度的增加，簇内和簇间距离的分布重叠显著，变得或多或少无法区分。这为我们提供了关于为什么聚类可能在这里失败的一些见解。请注意，我们对所有模拟使用了相同的偏移量。另一方面，如果簇之间的分离足够大，人们会预期聚类即使在高维度下也能工作。
- en: '![Histograms of within-cluster and between-cluster distances for a sample of
    size  in  (left) and  (right) dimensions with a given offset . As  increases,
    the two distributions become increasingly indistinguishable.](../Images/f5f988f0b8742e52a610ea9e41c65370.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![簇内和簇间距离的直方图，样本大小为 \(n\)，在 \(d\) 维（左）和 \(d\) 维（右）的给定偏移量下。随着 \(d\) 的增加，两个分布变得越来越难以区分。](../Images/f5f988f0b8742e52a610ea9e41c65370.png)'
- en: '**TRY IT!** What precedes (and what follows in the next subsection) is not
    a formal proof that \(k\)-means clustering will be unsuccessful here. The behavior
    of the algorithm is quite complex and depends, in particular, on the initialization
    and the density of points. Here, increasing the number of data points eventually
    leads to a much better performance. Explore this behavior on your own by modifying
    the code. (For some theoretical justifications (beyond this course), see [here](https://arxiv.org/pdf/0912.0086.pdf)
    and [here](http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf).)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**尝试一下！** 以下（以及在下一个小节中跟随的）不是 \(k\)-means 聚类在这里将失败的形式证明。算法的行为相当复杂，并且特别依赖于初始化和点的密度。在这里，增加数据点的数量最终会导致性能大幅提升。通过修改代码来探索这种行为。（对于一些理论上的证明（超出本课程），请参阅[这里](https://arxiv.org/pdf/0912.0086.pdf)和[这里](http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf)。）'
- en: '**CHAT & LEARN** According to Claude, here is how a cat might summarize the
    situation:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与学习** 根据Claude，一只猫可能会这样总结情况：'
- en: '**Figure:** A kitten in space (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：太空中的小猫（来源：使用 [Midjourney](https://www.midjourney.com/) 制作**）'
- en: '![Kitty in space](../Images/0834716500ee9672d75bf25880399171.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![猫咪在太空](../Images/0834716500ee9672d75bf25880399171.png)'
- en: \(\bowtie\)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: “I iz in high-dimensional space
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: “我身处高维空间中”
- en: All dese data points, everywheres
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些数据点，无处不在
- en: But no matter how far I roams
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 但无论我走得多远
- en: They all look the sames to me!”
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: “它们对我来说看起来都一样！”
- en: \(\ddagger\)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: \(\ddagger\)
- en: 1.4.2\. Surprising phenomena in high dimension[#](#surprising-phenomena-in-high-dimension
    "Link to this heading")
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4.2\. 高维空间中的惊人现象[#](#surprising-phenomena-in-high-dimension "链接到本标题")
- en: a high-dimensional space is a lonely place
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 高维空间是一个孤独的地方
- en: ''
  id: totrans-166
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Bernhard Schölkopf (@bschoelkopf) [August 24, 2014](https://twitter.com/bschoelkopf/status/503554842829549568?ref_src=twsrc%5Etfw)
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: — Bernhard Schölkopf (@bschoelkopf) [2014年8月24日](https://twitter.com/bschoelkopf/status/503554842829549568?ref_src=twsrc%5Etfw)
- en: In the previous section, we saw how the contribution from a large number of
    “noisy dimensions” can overwhelm the “signal” in the context of clustering. In
    this section we discuss further properties of high-dimensional space that are
    relevant to data science problems.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了大量“噪声维度”的贡献如何在聚类中压倒“信号”。在本节中，我们讨论与数据科学问题相关的高维空间的其他性质。
- en: 'Applying *Chebyshev’s Inequality* to sums of independent random variables has
    useful statistical implications: it shows that, with a large enough number of
    samples \(n\), the sample mean is close to the population mean. Hence it allows
    us to infer properties of a population from samples. Interestingly, one can apply
    a similar argument to a different asymptotic regime: the limit of large dimension
    \(d\). But as we will see in this section, the statistical implications are quite
    different.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 将 *切比雪夫不等式* 应用于独立随机变量的和，具有有用的统计意义：它表明，当样本数量 \(n\) 足够大时，样本均值接近总体均值。因此，它允许我们从样本中推断总体的性质。有趣的是，可以将类似的论点应用于不同的渐近区域：大维度
    \(d\) 的极限。但正如我们将在本节中看到的，统计意义相当不同。
- en: 'To start explaining the quote above, we consider a simple experiment. Let \(\mathcal{C}
    = [-1/2,1/2]^d\) be the \(d\)-cube with side lengths \(1\) centered at the origin
    and let \(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\|\leq 1/2\}\)
    be the inscribed \(d\)-ball.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '要开始解释上面的引言，我们考虑一个简单的实验。设 \(\mathcal{C} = [-1/2,1/2]^d\) 为以原点为中心、边长为 \(1\) 的
    \(d\) 维立方体，设 \(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\|\leq
    1/2\}\) 为内切 \(d\) 维球体。'
- en: Now pick a point \(\mathbf{X}\) uniformly at random in \(\mathcal{C}\). What
    is the probability that it falls in \(\mathcal{B}\)?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在随机选择一个点 \(\mathbf{X}\) 在 \(\mathcal{C}\) 中。它落在 \(\mathcal{B}\) 中的概率是多少？
- en: To generate \(\mathbf{X}\), we pick \(d\) independent random variables \(X_1,
    \ldots, X_d \sim \mathrm{U}[-1/2, 1/2]\), and form the vector \(\mathbf{X} = (X_1,
    \ldots, X_d)\). Indeed, the PDF of \(\mathbf{X}\) is then \(f_{\mathbf{X}}(\mathbf{x})=
    1^d = 1\) if \(\mathbf{x} \in \mathcal{C}\) and \(0\) otherwise.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成 \(\mathbf{X}\)，我们选择 \(d\) 个独立的随机变量 \(X_1, \ldots, X_d \sim \mathrm{U}[-1/2,
    1/2]\)，并形成向量 \(\mathbf{X} = (X_1, \ldots, X_d)\)。实际上，\(\mathbf{X}\) 的概率密度函数 \(f_{\mathbf{X}}(\mathbf{x})=
    1^d = 1\) 当 \(\mathbf{x} \in \mathcal{C}\) 时，否则为 \(0\)。
- en: The event we are interested in is \(A = \left\{\|\mathbf{X}\| \leq 1/2\right\}\).
    The uniform distribution over the set \(\mathcal{C}\) has the property that \(\mathbb{P}[A]\)
    is the volume of \(\mathcal{B}\) divided by the volume of \(\mathcal{C}\). In
    this case, the volume of \(\mathcal{C}\) is \(1^d = 1\) and the volume of \(\mathcal{B}\)
    has an [explicit formula](https://en.wikipedia.org/wiki/Volume_of_an_n-ball).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的事件是 \(A = \left\{\|\mathbf{X}\| \leq 1/2\right\}\)。在集合 \(\mathcal{C}\)
    上的均匀分布具有性质 \(\mathbb{P}[A]\) 是 \(\mathcal{B}\) 的体积除以 \(\mathcal{C}\) 的体积。在这种情况下，\(\mathcal{C}\)
    的体积是 \(1^d = 1\)，而 \(\mathcal{B}\) 的体积有一个 [显式公式](https://en.wikipedia.org/wiki/Volume_of_an_n-ball)。
- en: 'This leads to the following surprising fact:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下令人惊讶的事实：
- en: '**THEOREM** **(High-dimensional Cube)** \(\idx{high-dimensional cube theorem}\xdi\)
    Let \(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d \,:\, \|\mathbf{x}\|\leq 1/2\}\)
    and \(\mathcal{C} = [-1/2,1/2]^d\). Pick \(\mathbf{X} \sim \mathrm{U}[\mathcal{C}]\).
    Then, as \(d \to +\infty\),'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理** **(高维立方体)** \(\idx{高维立方体定理}\xdi\) 设 \(\mathcal{B} = \{\mathbf{x} \in
    \mathbb{R}^d \,:\, \|\mathbf{x}\|\leq 1/2\}\) 和 \(\mathcal{C} = [-1/2,1/2]^d\).
    从 \(\mathrm{U}[\mathcal{C}]\) 中选取 \(\mathbf{X}\)。当 \(d \to +\infty\) 时，'
- en: \[ \mathbb{P}[\mathbf{X} \in \mathcal{B}] \to 0. \]
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{P}[\mathbf{X} \in \mathcal{B}] \to 0. \]
- en: \(\sharp\)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: \(\sharp\)
- en: In words, in high dimension if one picks a point at random from the cube, it
    is unlikely to be close to the origin. Instead it is likely to be in the corners.
    A geometric interpretation is that a high-dimensional cube is a bit like a “spiky
    ball.”
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 用话来说，在高度维度中，如果从立方体中随机选择一个点，它不太可能接近原点。相反，它更有可能位于角落。几何解释是，高维立方体有点像“带刺的球体。”
- en: '**Figure:** Visualization of a high-dimensional cube as a spiky ball (*Credit:*
    Made with [Midjourney](https://www.midjourney.com/))'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**图：** 高维立方体的可视化，呈现为带刺的球体（*来源：使用 [Midjourney](https://www.midjourney.com/)*）'
- en: '![Visualization of a high-dimensional cube](../Images/0ed7cc7c9c5326071c245e739ab01a88.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![高维立方体的可视化](../Images/0ed7cc7c9c5326071c245e739ab01a88.png)'
- en: \(\bowtie\)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: We give a proof based on *Chebyshev’s Inequality*. It has the advantage of providing
    some insight into this counter-intuitive phenomenon by linking it to the concentration
    of sums of independent random variables, in this case the squared norm of \(\mathbf{X}\).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于**切比雪夫不等式**给出一个证明。它具有通过将其与独立随机变量和的集中联系起来，从而提供一些对这种反直觉现象的洞察的优势，在这种情况下是 \(\mathbf{X}\)
    的平方范数。
- en: '*Proof idea:* We think of \(\|\mathbf{X}\|^2\) as a sum of independent random
    variables and apply *Chebyshev’s Inequality*. It implies that the norm of \(\mathbf{X}\)
    is concentrated around its mean, which grows like \(\sqrt{d}\). The latter is
    larger than \(1/2\) for \(d\) large.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路：* 我们将 \(\|\mathbf{X}\|^2\) 视为独立随机变量的和，并应用**切比雪夫不等式**。它表明 \(\mathbf{X}\)
    的范数集中在其均值附近，其增长类似于 \(\sqrt{d}\)。对于大的 \(d\)，后者大于 \(1/2\)。'
- en: '*Proof:* To see the relevance of *Chebyshev’s Inequality*, we compute the mean
    and standard deviation of the norm of \(\mathbf{X}\). In fact, because of the
    square root in \(\|\mathbf{X}\|\), computing its expectation is difficult. Instead
    we work with the squared norm'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 为了看到**切比雪夫不等式**的相关性，我们计算 \(\mathbf{X}\) 的范数的均值和标准差。实际上，由于 \(\|\mathbf{X}\|\)
    中的平方根，计算其期望是困难的。因此，我们处理平方范数'
- en: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
- en: which has the advantage of being a sum of independent random variables – for
    which the expectation and variance are much easier to compute. Observe further
    that the probability of the event of interest \(\{\|\mathbf{X}\| \leq 1/2\}\)
    can be re-written in terms of \(\|\mathbf{X}\|^2\) as follows
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这具有将独立随机变量的和作为优势——对于这些变量的期望和方差更容易计算。进一步观察，感兴趣的事件的概率 \(\{\|\mathbf{X}\| \leq
    1/2\}\) 可以用 \(\|\mathbf{X}\|^2\) 表示如下
- en: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}
    \left[ \|\mathbf{X}\|^2 \leq 1/4 \right]. \end{align*}\]
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}
    \left[ \|\mathbf{X}\|^2 \leq 1/4 \right]. \end{align*}\]
- en: To simplify the notation, we use \(\tilde\mu = \mathbb{E}[X_1^2]\) and \(\tilde\sigma
    = \sqrt{\mathrm{Var}[X_1^2]}\) for the mean and standard deviation of \(X_1^2\)
    respectively. Using linearity of expectation and the fact that the \(X_i\)’s are
    independent, we get
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化符号，我们分别用 \(\tilde\mu = \mathbb{E}[X_1^2]\) 和 \(\tilde\sigma = \sqrt{\mathrm{Var}[X_1^2]}\)
    表示 \(X_1^2\) 的均值和标准差。利用期望的线性性和 \(X_i\) 的独立性，我们得到
- en: \[ \mu_{\|\mathbf{X}\|^2} = \mathbb{E}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d
    \mathbb{E}[X_i^2] = d \,\mathbb{E}[X_1^2] = \tilde\mu \, d, \]
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mu_{\|\mathbf{X}\|^2} = \mathbb{E}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d
    \mathbb{E}[X_i^2] = d \,\mathbb{E}[X_1^2] = \tilde\mu \, d, \]
- en: and
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathrm{Var}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d \mathrm{Var}[X_i^2]
    = d \,\mathrm{Var}[X_1^2]. \]
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathrm{Var}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d \mathrm{Var}[X_i^2]
    = d \,\mathrm{Var}[X_1^2]. \]
- en: Taking a square root, we get an expression for the standard deviation of our
    quantity of interest \(\|\mathbf{X}\|^2\) in terms of the standard deviation of
    \(X_1^2\)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 开平方，我们得到我们感兴趣的数量 \(\|\mathbf{X}\|^2\) 的标准差的表达式，它是 \(X_1^2\) 的标准差的函数
- en: \[ \sigma_{\|\mathbf{X}\|^2} = \tilde\sigma \, \sqrt{d}. \]
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma_{\|\mathbf{X}\|^2} = \tilde\sigma \, \sqrt{d}. \]
- en: (Note that we could compute \(\tilde\mu\) and \(\tilde\sigma\) explicitly, but
    it will not be necessary here.)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: （注意，我们可以显式地计算 \(\tilde\mu\) 和 \(\tilde\sigma\)，但在这里并不必要。）
- en: We use *Chebyshev’s Inequality* to show that \(\|\mathbf{X}\|^2\) is highly
    likely to be close to its mean \(\tilde\mu \, d\), which is much larger than \(1/4\)
    when \(d\) is large. And that therefore \(\|\mathbf{X}\|^2\) is highly unlikely
    to be smaller than \(1/4\). We give the details next.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 *切比雪夫不等式* 来证明 \(\|\mathbf{X}\|^2\) 很可能接近其均值 \(\tilde\mu \, d\)，当 \(d\)
    很大时，这个均值远大于 \(1/4\)。因此，\(\|\mathbf{X}\|^2\) 很不可能小于 \(1/4\)。我们将在下文中给出详细说明。
- en: By the one-sided version of *Chebyshev’s Inequality* in terms of the standard
    deviation, we have
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标准差的单侧版本 *切比雪夫不等式*，我们有
- en: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 - \mu_{\|\mathbf{X}\|^2} \leq - \alpha
    \right] \leq \left(\frac{\sigma_{\|\mathbf{X}\|^2}}{\alpha}\right)^2. \]
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 - \mu_{\|\mathbf{X}\|^2} \leq - \alpha
    \right] \leq \left(\frac{\sigma_{\|\mathbf{X}\|^2}}{\alpha}\right)^2. \]
- en: That is, using the formulas above and rearranging slightly,
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，使用上述公式并稍作整理，
- en: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha \right] \leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2. \]
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha \right] \leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2. \]
- en: How do we relate this to the probability of interest \(\mathbb{P}\left[\|\mathbf{X}\|^2
    \leq 1/4\right]\)? Recall that we are free to choose \(\alpha\) in this inequality.
    So simply take \(\alpha\) such that
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这个结果与感兴趣的概率 \(\mathbb{P}\left[\|\mathbf{X}\|^2 \leq 1/4\right]\) 相关联？回想一下，在这个不等式中我们可以自由选择
    \(\alpha\)。所以只需取 \(\alpha\) 使得
- en: \[ \tilde\mu \,d - \alpha = \frac{1}{4}, \]
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \tilde\mu \,d - \alpha = \frac{1}{4}, \]
- en: that is, \(\alpha = \tilde\mu \,d - 1/4\). Observe that, once \(d\) is large
    enough, it holds that \(\alpha > 0\).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 即，\(\alpha = \tilde\mu \,d - 1/4\)。观察一下，一旦 \(d\) 足够大，就有 \(\alpha > 0\)。
- en: Finally, replacing this choice of \(\alpha\) in the inequality above gives
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将上述不等式中的 \(\alpha\) 替换为这个选择
- en: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}\left[\|\mathbf{X}\|^2
    \leq 1/4\right]\\ &= \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
    \right]\\ &\leq \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2\\ &\leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\tilde\mu \,d - 1/4}\right)^2. \end{align*}\]
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}\left[\|\mathbf{X}\|^2
    \leq 1/4\right]\\ &= \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
    \right]\\ &\leq \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2\\ &\leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\tilde\mu \,d - 1/4}\right)^2. \end{align*}\]
- en: Critically, \(\tilde\mu\) and \(\tilde\sigma\) do not depend on \(d\). So the
    right-hand side goes to \(0\) as \(d \to +\infty\). Indeed, \(d\) is much larger
    than \(\sqrt{d}\) when \(d\) is large. That proves the claim.\(\square\)
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，\(\tilde\mu\) 和 \(\tilde\sigma\) 不依赖于 \(d\)。因此，当 \(d \to +\infty\) 时，右侧趋于
    \(0\)。实际上，当 \(d\) 很大时，\(d\) 远大于 \(\sqrt{d}\)。这证明了我们的断言。\(\square\)
- en: We will see later in the course that this high-dimensional phenomenon has implications
    for data science problems. It is behind what is referred to as the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\(\idx{curse
    of dimensionality}\xdi\).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在课程后面的部分看到，这种高维现象对数据科学问题有影响。它是所谓的 [维度诅咒](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\(\idx{维度诅咒}\xdi\)
    的背后原因。
- en: While *Chebyshev’s inequality* correctly implies that \(\mathbb{P}[\mathbf{X}
    \in \mathcal{B}]\) goes to \(0\), it does not give the correct rate of convergence.
    In reality, that probability goes to \(0\) at a much faster rate than \(1/d\).
    Specifically, [it can be shown](https://en.wikipedia.org/wiki/Volume_of_an_n-ball#High_dimensions)
    that \(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\) goes to \(0\) roughly as \(d^{-d/2}\).
    We will not derive (or need) this fact here.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*切比雪夫不等式*正确地暗示了\(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\)趋近于0，但它并没有给出正确的收敛速率。实际上，这个概率以比\(1/d\)快得多的速度趋近于0。具体来说，[可以证明](https://en.wikipedia.org/wiki/Volume_of_an_n-ball#High_dimensions)
    \(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\)大致以\(d^{-d/2}\)的速度趋近于0。我们在这里不会推导（或需要）这个事实。
- en: '**NUMERICAL CORNER:** We can check the theorem in a simulation. Here we pick
    \(n\) points uniformly at random in the \(d\)-cube \(\mathcal{C}\), for a range
    of dimensions up to `dmax`. We then plot the frequency of landing in the inscribed
    \(d\)-ball \(\mathcal{B}\) and see that it rapidly converges to \(0\). Alternatively,
    we could just plot the formula for the volume of \(\mathcal{B}\). But knowing
    how to do simulations is useful in situations where explicit formulas are unavailable
    or intractable. We plot the result up to dimension \(10\).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们可以在模拟中检查定理。在这里，我们在\(d\)-立方体\(\mathcal{C}\)中随机选择\(n\)个点，范围包括直到`dmax`的维度。然后我们绘制落在内切\(d\)-球体\(\mathcal{B}\)中的频率，并看到它迅速收敛到0。或者，我们也可以只绘制\(\mathcal{B}\)体积的公式。但知道如何进行模拟在显式公式不可用或难以处理的情况下是有用的。我们绘制了直到维度10的结果。'
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![../../_images/d08f6e8ae99d1fc47a60ae906d9afb4fb92b64d9611fda98fd7415d2bc98a67d.png](../Images/c7ed85ca4cdd1727d357fd1139869726.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/d08f6e8ae99d1fc47a60ae906d9afb4fb92b64d9611fda98fd7415d2bc98a67d.png](../Images/c7ed85ca4cdd1727d357fd1139869726.png)'
- en: \(\unlhd\)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude、Gemini和ChatGPT协助)*'
- en: '**1** The volume of the \(d\)-dimensional cube \(C = [-1/2, 1/2]^d\) is:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** \(d\)维立方体\(C = [-1/2, 1/2]^d\)的体积是：'
- en: a) \(1/d\)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(1/d\)
- en: b) \(1/2^d\)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(1/2^d\)
- en: c) 1
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: c) 1
- en: d) \(2^d\)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(2^d\)
- en: '**2** In a high-dimensional cube \(C = [-1/2, 1/2]^d\), as the dimension \(d\)
    increases, the probability that a randomly chosen point lies within the inscribed
    sphere \(B = \{x \in \mathbb{R}^d : \|x\| \le 1/2\}\):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 在高维立方体\(C = [-1/2, 1/2]^d\)中，随着维度\(d\)的增加，随机选择一个点落在内切球体\(B = \{x \in
    \mathbb{R}^d : \|x\| \le 1/2\}\)内的概率：'
- en: a) Approaches 1
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: a) 方法1
- en: b) Approaches 1/2
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: b) 趋近于1/2
- en: c) Approaches 0
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: c) 方法0
- en: d) Remains constant
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: d) 保持不变
- en: '**3** Which of the following best describes the appearance of a high-dimensional
    cube?'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 以下哪个最好地描述了高维立方体的外观？'
- en: a) A smooth, round ball
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: a) 一个光滑、圆形的球体
- en: b) A spiky ball with most of its volume concentrated in the corners
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: b) 大部分体积集中在角落的尖锐球体
- en: c) A perfect sphere with uniform volume distribution
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: c) 具有均匀体积分布的完美球体
- en: d) A flat, pancake-like shape
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: d) 一种扁平、类似薄饼的形状
- en: '**4** Which inequality is used to prove the theorem about high-dimensional
    cubes?'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 用来证明高维立方体定理的不等式是哪个？'
- en: a) Cauchy-Schwarz inequality
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: a) 库希不等式
- en: b) Triangle inequality
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: b) 三角不等式
- en: c) Markov’s inequality
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: c) 马尔可夫不等式
- en: d) Chebyshev’s inequality
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: d) 切比雪夫不等式
- en: '**5** In the proof of the theorem about high-dimensional cubes, which property
    of the squared norm \(\|X\|^2\) is used?'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在高维立方体定理的证明中，使用了平方范数\(\|X\|^2\)的哪个性质？'
- en: a) It is a sum of dependent random variables.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它是相关随机变量的和。
- en: b) It is a sum of independent random variables.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它是独立随机变量的和。
- en: c) It is a product of independent random variables.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它是独立随机变量的乘积。
- en: d) It is a product of dependent random variables.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它是相关随机变量的乘积。
- en: 'Answer for 1: c. Justification: The side length of the cube is 1, and the volume
    of a \(d\)-dimensional cube is the side length raised to the power \(d\).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 答案1：c. 理由：立方体的边长为1，\(d\)维立方体的体积是边长的\(d\)次幂。
- en: 'Answer for 2: c. Justification: This is the statement of the theorem “High-dimensional
    Cube” in the text.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 答案2：c. 理由：这是文本中“高维立方体”定理的陈述。
- en: 'Answer for 3: b. Justification: The text mentions, “A geometric interpretation
    is that a high-dimensional cube is a bit like a ‘spiky ball.’”'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 答案3：b. 理由：文本提到，“一个几何解释是高维立方体有点像‘尖锐球体’。”
- en: 'Answer for 4: d. Justification: The text explicitly states that Chebyshev’s
    inequality is used in the proof.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 答案4：d. 理由：文本明确指出在证明中使用了切比雪夫不等式。
- en: 'Answer for 5: b. Justification: The proof states, “we work with the squared
    norm'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 答案5：b. 理由：证明中提到，“我们处理平方范数”
- en: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
- en: which has the advantage of being a sum of independent random variables.”
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 具有作为独立随机变量之和的优势。”
