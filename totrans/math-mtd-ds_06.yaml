- en: 1.4\. Some observations about high-dimensional data#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap01_intro/04_highdim/roch-mmids-intro-highdim.html](https://mmids-textbook.github.io/chap01_intro/04_highdim/roch-mmids-intro-highdim.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this section, we first apply \(k\)-means clustering to a high-dimensional
    example to illustrate the issues that arise in that context. We then discuss some
    surprising phenomena in high dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.1\. Clustering in high dimension[#](#clustering-in-high-dimension "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we test our implementation of \(k\)-means on a simple simulated
    dataset in high dimension.
  prefs: []
  type: TYPE_NORMAL
- en: The following function generates \(n\) data points from a mixture of two equally
    likely, spherical \(d\)-dimensional Gaussians with variance \(1\), one with mean
    \(-w\mathbf{e}_1\) and one with mean \(w \mathbf{e}_1\). We use `gmm2spherical`
    from a previous section. It is found in `mmids.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We start with \(d=2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Let’s run \(k\)-means on this dataset using \(k=2\). We use `kmeans()` from
    the `mmids.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Our default of \(10\) iterations seem to have been enough for the algorithm
    to converge. We can visualize the result by [coloring](https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html)
    the points according to the assignment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/9aa59ca573eb1a92b3e764e96b1d04efe81f825b67863353e16d5736d4608fb4.png](../Images/aa56b156c7bbd5cc401744d4eec4820d.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s see what happens in higher dimension. We repeat our experiment with \(d=1000\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Again, we observe two clearly delineated clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/264709bc9d4749da24e2b1bc73c5716f8d150355fba2e13f70a3906c858baa7f.png](../Images/2555114660b70c347ebf0d27a90440a0.png)'
  prefs: []
  type: TYPE_IMG
- en: This dataset is in \(1000\) dimensions, but we’ve plotted the data in only the
    first two dimensions. If we plot in any two dimensions not including the first
    one instead, we see only one cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/21494a7e3aca56828d3ea264f5b728df4a6ba072eff22b28e743010117504049.png](../Images/2ceed7ae94ba461478dde0058309f68a.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s see how \(k\)-means fares on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Our attempt at clustering does not appear to have been successful.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/dc448ee379a7ca994bcd89a9ef7e80c567cd1e8704627661a871effd41fa9f11.png](../Images/085a8dadcd6ad71b3fc7e6a1a02fe4c0.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 'What happened? While the clusters are easy to tease apart *if we know to look
    at the first coordinate only*, in the full space the within-cluster and between-cluster
    distances become harder to distinguish: the noise overwhelms the signal.'
  prefs: []
  type: TYPE_NORMAL
- en: As the dimension increases, the distributions of intra-cluster and inter-cluster
    distances overlap significantly and become more or less indistinguishable. That
    provides some insights into why clustering may fail here. Note that we used the
    same offset for all simulations. On the other hand, if the separation between
    the clusters is sufficiently large, one would expect clustering to work even in
    high dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '![Histograms of within-cluster and between-cluster distances for a sample of
    size  in  (left) and  (right) dimensions with a given offset . As  increases,
    the two distributions become increasingly indistinguishable.](../Images/f5f988f0b8742e52a610ea9e41c65370.png)'
  prefs: []
  type: TYPE_IMG
- en: '**TRY IT!** What precedes (and what follows in the next subsection) is not
    a formal proof that \(k\)-means clustering will be unsuccessful here. The behavior
    of the algorithm is quite complex and depends, in particular, on the initialization
    and the density of points. Here, increasing the number of data points eventually
    leads to a much better performance. Explore this behavior on your own by modifying
    the code. (For some theoretical justifications (beyond this course), see [here](https://arxiv.org/pdf/0912.0086.pdf)
    and [here](http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf).)'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** According to Claude, here is how a cat might summarize the
    situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** A kitten in space (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kitty in space](../Images/0834716500ee9672d75bf25880399171.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: “I iz in high-dimensional space
  prefs: []
  type: TYPE_NORMAL
- en: All dese data points, everywheres
  prefs: []
  type: TYPE_NORMAL
- en: But no matter how far I roams
  prefs: []
  type: TYPE_NORMAL
- en: They all look the sames to me!”
  prefs: []
  type: TYPE_NORMAL
- en: \(\ddagger\)
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.2\. Surprising phenomena in high dimension[#](#surprising-phenomena-in-high-dimension
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: a high-dimensional space is a lonely place
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Bernhard Schölkopf (@bschoelkopf) [August 24, 2014](https://twitter.com/bschoelkopf/status/503554842829549568?ref_src=twsrc%5Etfw)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the previous section, we saw how the contribution from a large number of
    “noisy dimensions” can overwhelm the “signal” in the context of clustering. In
    this section we discuss further properties of high-dimensional space that are
    relevant to data science problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying *Chebyshev’s Inequality* to sums of independent random variables has
    useful statistical implications: it shows that, with a large enough number of
    samples \(n\), the sample mean is close to the population mean. Hence it allows
    us to infer properties of a population from samples. Interestingly, one can apply
    a similar argument to a different asymptotic regime: the limit of large dimension
    \(d\). But as we will see in this section, the statistical implications are quite
    different.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start explaining the quote above, we consider a simple experiment. Let \(\mathcal{C}
    = [-1/2,1/2]^d\) be the \(d\)-cube with side lengths \(1\) centered at the origin
    and let \(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\|\leq 1/2\}\)
    be the inscribed \(d\)-ball.'
  prefs: []
  type: TYPE_NORMAL
- en: Now pick a point \(\mathbf{X}\) uniformly at random in \(\mathcal{C}\). What
    is the probability that it falls in \(\mathcal{B}\)?
  prefs: []
  type: TYPE_NORMAL
- en: To generate \(\mathbf{X}\), we pick \(d\) independent random variables \(X_1,
    \ldots, X_d \sim \mathrm{U}[-1/2, 1/2]\), and form the vector \(\mathbf{X} = (X_1,
    \ldots, X_d)\). Indeed, the PDF of \(\mathbf{X}\) is then \(f_{\mathbf{X}}(\mathbf{x})=
    1^d = 1\) if \(\mathbf{x} \in \mathcal{C}\) and \(0\) otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: The event we are interested in is \(A = \left\{\|\mathbf{X}\| \leq 1/2\right\}\).
    The uniform distribution over the set \(\mathcal{C}\) has the property that \(\mathbb{P}[A]\)
    is the volume of \(\mathcal{B}\) divided by the volume of \(\mathcal{C}\). In
    this case, the volume of \(\mathcal{C}\) is \(1^d = 1\) and the volume of \(\mathcal{B}\)
    has an [explicit formula](https://en.wikipedia.org/wiki/Volume_of_an_n-ball).
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to the following surprising fact:'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(High-dimensional Cube)** \(\idx{high-dimensional cube theorem}\xdi\)
    Let \(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d \,:\, \|\mathbf{x}\|\leq 1/2\}\)
    and \(\mathcal{C} = [-1/2,1/2]^d\). Pick \(\mathbf{X} \sim \mathrm{U}[\mathcal{C}]\).
    Then, as \(d \to +\infty\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[\mathbf{X} \in \mathcal{B}] \to 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: In words, in high dimension if one picks a point at random from the cube, it
    is unlikely to be close to the origin. Instead it is likely to be in the corners.
    A geometric interpretation is that a high-dimensional cube is a bit like a “spiky
    ball.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Visualization of a high-dimensional cube as a spiky ball (*Credit:*
    Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization of a high-dimensional cube](../Images/0ed7cc7c9c5326071c245e739ab01a88.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: We give a proof based on *Chebyshev’s Inequality*. It has the advantage of providing
    some insight into this counter-intuitive phenomenon by linking it to the concentration
    of sums of independent random variables, in this case the squared norm of \(\mathbf{X}\).
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We think of \(\|\mathbf{X}\|^2\) as a sum of independent random
    variables and apply *Chebyshev’s Inequality*. It implies that the norm of \(\mathbf{X}\)
    is concentrated around its mean, which grows like \(\sqrt{d}\). The latter is
    larger than \(1/2\) for \(d\) large.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* To see the relevance of *Chebyshev’s Inequality*, we compute the mean
    and standard deviation of the norm of \(\mathbf{X}\). In fact, because of the
    square root in \(\|\mathbf{X}\|\), computing its expectation is difficult. Instead
    we work with the squared norm'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: which has the advantage of being a sum of independent random variables – for
    which the expectation and variance are much easier to compute. Observe further
    that the probability of the event of interest \(\{\|\mathbf{X}\| \leq 1/2\}\)
    can be re-written in terms of \(\|\mathbf{X}\|^2\) as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}
    \left[ \|\mathbf{X}\|^2 \leq 1/4 \right]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the notation, we use \(\tilde\mu = \mathbb{E}[X_1^2]\) and \(\tilde\sigma
    = \sqrt{\mathrm{Var}[X_1^2]}\) for the mean and standard deviation of \(X_1^2\)
    respectively. Using linearity of expectation and the fact that the \(X_i\)’s are
    independent, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_{\|\mathbf{X}\|^2} = \mathbb{E}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d
    \mathbb{E}[X_i^2] = d \,\mathbb{E}[X_1^2] = \tilde\mu \, d, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{Var}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d \mathrm{Var}[X_i^2]
    = d \,\mathrm{Var}[X_1^2]. \]
  prefs: []
  type: TYPE_NORMAL
- en: Taking a square root, we get an expression for the standard deviation of our
    quantity of interest \(\|\mathbf{X}\|^2\) in terms of the standard deviation of
    \(X_1^2\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma_{\|\mathbf{X}\|^2} = \tilde\sigma \, \sqrt{d}. \]
  prefs: []
  type: TYPE_NORMAL
- en: (Note that we could compute \(\tilde\mu\) and \(\tilde\sigma\) explicitly, but
    it will not be necessary here.)
  prefs: []
  type: TYPE_NORMAL
- en: We use *Chebyshev’s Inequality* to show that \(\|\mathbf{X}\|^2\) is highly
    likely to be close to its mean \(\tilde\mu \, d\), which is much larger than \(1/4\)
    when \(d\) is large. And that therefore \(\|\mathbf{X}\|^2\) is highly unlikely
    to be smaller than \(1/4\). We give the details next.
  prefs: []
  type: TYPE_NORMAL
- en: By the one-sided version of *Chebyshev’s Inequality* in terms of the standard
    deviation, we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 - \mu_{\|\mathbf{X}\|^2} \leq - \alpha
    \right] \leq \left(\frac{\sigma_{\|\mathbf{X}\|^2}}{\alpha}\right)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, using the formulas above and rearranging slightly,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha \right] \leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: How do we relate this to the probability of interest \(\mathbb{P}\left[\|\mathbf{X}\|^2
    \leq 1/4\right]\)? Recall that we are free to choose \(\alpha\) in this inequality.
    So simply take \(\alpha\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \tilde\mu \,d - \alpha = \frac{1}{4}, \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, \(\alpha = \tilde\mu \,d - 1/4\). Observe that, once \(d\) is large
    enough, it holds that \(\alpha > 0\).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, replacing this choice of \(\alpha\) in the inequality above gives
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}\left[\|\mathbf{X}\|^2
    \leq 1/4\right]\\ &= \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
    \right]\\ &\leq \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2\\ &\leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\tilde\mu \,d - 1/4}\right)^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Critically, \(\tilde\mu\) and \(\tilde\sigma\) do not depend on \(d\). So the
    right-hand side goes to \(0\) as \(d \to +\infty\). Indeed, \(d\) is much larger
    than \(\sqrt{d}\) when \(d\) is large. That proves the claim.\(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We will see later in the course that this high-dimensional phenomenon has implications
    for data science problems. It is behind what is referred to as the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\(\idx{curse
    of dimensionality}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: While *Chebyshev’s inequality* correctly implies that \(\mathbb{P}[\mathbf{X}
    \in \mathcal{B}]\) goes to \(0\), it does not give the correct rate of convergence.
    In reality, that probability goes to \(0\) at a much faster rate than \(1/d\).
    Specifically, [it can be shown](https://en.wikipedia.org/wiki/Volume_of_an_n-ball#High_dimensions)
    that \(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\) goes to \(0\) roughly as \(d^{-d/2}\).
    We will not derive (or need) this fact here.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We can check the theorem in a simulation. Here we pick
    \(n\) points uniformly at random in the \(d\)-cube \(\mathcal{C}\), for a range
    of dimensions up to `dmax`. We then plot the frequency of landing in the inscribed
    \(d\)-ball \(\mathcal{B}\) and see that it rapidly converges to \(0\). Alternatively,
    we could just plot the formula for the volume of \(\mathcal{B}\). But knowing
    how to do simulations is useful in situations where explicit formulas are unavailable
    or intractable. We plot the result up to dimension \(10\).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/d08f6e8ae99d1fc47a60ae906d9afb4fb92b64d9611fda98fd7415d2bc98a67d.png](../Images/c7ed85ca4cdd1727d357fd1139869726.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** The volume of the \(d\)-dimensional cube \(C = [-1/2, 1/2]^d\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(1/d\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(1/2^d\)
  prefs: []
  type: TYPE_NORMAL
- en: c) 1
  prefs: []
  type: TYPE_NORMAL
- en: d) \(2^d\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** In a high-dimensional cube \(C = [-1/2, 1/2]^d\), as the dimension \(d\)
    increases, the probability that a randomly chosen point lies within the inscribed
    sphere \(B = \{x \in \mathbb{R}^d : \|x\| \le 1/2\}\):'
  prefs: []
  type: TYPE_NORMAL
- en: a) Approaches 1
  prefs: []
  type: TYPE_NORMAL
- en: b) Approaches 1/2
  prefs: []
  type: TYPE_NORMAL
- en: c) Approaches 0
  prefs: []
  type: TYPE_NORMAL
- en: d) Remains constant
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Which of the following best describes the appearance of a high-dimensional
    cube?'
  prefs: []
  type: TYPE_NORMAL
- en: a) A smooth, round ball
  prefs: []
  type: TYPE_NORMAL
- en: b) A spiky ball with most of its volume concentrated in the corners
  prefs: []
  type: TYPE_NORMAL
- en: c) A perfect sphere with uniform volume distribution
  prefs: []
  type: TYPE_NORMAL
- en: d) A flat, pancake-like shape
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Which inequality is used to prove the theorem about high-dimensional
    cubes?'
  prefs: []
  type: TYPE_NORMAL
- en: a) Cauchy-Schwarz inequality
  prefs: []
  type: TYPE_NORMAL
- en: b) Triangle inequality
  prefs: []
  type: TYPE_NORMAL
- en: c) Markov’s inequality
  prefs: []
  type: TYPE_NORMAL
- en: d) Chebyshev’s inequality
  prefs: []
  type: TYPE_NORMAL
- en: '**5** In the proof of the theorem about high-dimensional cubes, which property
    of the squared norm \(\|X\|^2\) is used?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It is a sum of dependent random variables.
  prefs: []
  type: TYPE_NORMAL
- en: b) It is a sum of independent random variables.
  prefs: []
  type: TYPE_NORMAL
- en: c) It is a product of independent random variables.
  prefs: []
  type: TYPE_NORMAL
- en: d) It is a product of dependent random variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: c. Justification: The side length of the cube is 1, and the volume
    of a \(d\)-dimensional cube is the side length raised to the power \(d\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: c. Justification: This is the statement of the theorem “High-dimensional
    Cube” in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: b. Justification: The text mentions, “A geometric interpretation
    is that a high-dimensional cube is a bit like a ‘spiky ball.’”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: d. Justification: The text explicitly states that Chebyshev’s
    inequality is used in the proof.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The proof states, “we work with the squared
    norm'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: which has the advantage of being a sum of independent random variables.”
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.1\. Clustering in high dimension[#](#clustering-in-high-dimension "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we test our implementation of \(k\)-means on a simple simulated
    dataset in high dimension.
  prefs: []
  type: TYPE_NORMAL
- en: The following function generates \(n\) data points from a mixture of two equally
    likely, spherical \(d\)-dimensional Gaussians with variance \(1\), one with mean
    \(-w\mathbf{e}_1\) and one with mean \(w \mathbf{e}_1\). We use `gmm2spherical`
    from a previous section. It is found in `mmids.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We start with \(d=2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Let’s run \(k\)-means on this dataset using \(k=2\). We use `kmeans()` from
    the `mmids.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Our default of \(10\) iterations seem to have been enough for the algorithm
    to converge. We can visualize the result by [coloring](https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html)
    the points according to the assignment.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/9aa59ca573eb1a92b3e764e96b1d04efe81f825b67863353e16d5736d4608fb4.png](../Images/aa56b156c7bbd5cc401744d4eec4820d.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s see what happens in higher dimension. We repeat our experiment with \(d=1000\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Again, we observe two clearly delineated clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/264709bc9d4749da24e2b1bc73c5716f8d150355fba2e13f70a3906c858baa7f.png](../Images/2555114660b70c347ebf0d27a90440a0.png)'
  prefs: []
  type: TYPE_IMG
- en: This dataset is in \(1000\) dimensions, but we’ve plotted the data in only the
    first two dimensions. If we plot in any two dimensions not including the first
    one instead, we see only one cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/21494a7e3aca56828d3ea264f5b728df4a6ba072eff22b28e743010117504049.png](../Images/2ceed7ae94ba461478dde0058309f68a.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s see how \(k\)-means fares on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Our attempt at clustering does not appear to have been successful.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/dc448ee379a7ca994bcd89a9ef7e80c567cd1e8704627661a871effd41fa9f11.png](../Images/085a8dadcd6ad71b3fc7e6a1a02fe4c0.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 'What happened? While the clusters are easy to tease apart *if we know to look
    at the first coordinate only*, in the full space the within-cluster and between-cluster
    distances become harder to distinguish: the noise overwhelms the signal.'
  prefs: []
  type: TYPE_NORMAL
- en: As the dimension increases, the distributions of intra-cluster and inter-cluster
    distances overlap significantly and become more or less indistinguishable. That
    provides some insights into why clustering may fail here. Note that we used the
    same offset for all simulations. On the other hand, if the separation between
    the clusters is sufficiently large, one would expect clustering to work even in
    high dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '![Histograms of within-cluster and between-cluster distances for a sample of
    size  in  (left) and  (right) dimensions with a given offset . As  increases,
    the two distributions become increasingly indistinguishable.](../Images/f5f988f0b8742e52a610ea9e41c65370.png)'
  prefs: []
  type: TYPE_IMG
- en: '**TRY IT!** What precedes (and what follows in the next subsection) is not
    a formal proof that \(k\)-means clustering will be unsuccessful here. The behavior
    of the algorithm is quite complex and depends, in particular, on the initialization
    and the density of points. Here, increasing the number of data points eventually
    leads to a much better performance. Explore this behavior on your own by modifying
    the code. (For some theoretical justifications (beyond this course), see [here](https://arxiv.org/pdf/0912.0086.pdf)
    and [here](http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf).)'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** According to Claude, here is how a cat might summarize the
    situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** A kitten in space (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Kitty in space](../Images/0834716500ee9672d75bf25880399171.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: “I iz in high-dimensional space
  prefs: []
  type: TYPE_NORMAL
- en: All dese data points, everywheres
  prefs: []
  type: TYPE_NORMAL
- en: But no matter how far I roams
  prefs: []
  type: TYPE_NORMAL
- en: They all look the sames to me!”
  prefs: []
  type: TYPE_NORMAL
- en: \(\ddagger\)
  prefs: []
  type: TYPE_NORMAL
- en: 1.4.2\. Surprising phenomena in high dimension[#](#surprising-phenomena-in-high-dimension
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: a high-dimensional space is a lonely place
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Bernhard Schölkopf (@bschoelkopf) [August 24, 2014](https://twitter.com/bschoelkopf/status/503554842829549568?ref_src=twsrc%5Etfw)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the previous section, we saw how the contribution from a large number of
    “noisy dimensions” can overwhelm the “signal” in the context of clustering. In
    this section we discuss further properties of high-dimensional space that are
    relevant to data science problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying *Chebyshev’s Inequality* to sums of independent random variables has
    useful statistical implications: it shows that, with a large enough number of
    samples \(n\), the sample mean is close to the population mean. Hence it allows
    us to infer properties of a population from samples. Interestingly, one can apply
    a similar argument to a different asymptotic regime: the limit of large dimension
    \(d\). But as we will see in this section, the statistical implications are quite
    different.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start explaining the quote above, we consider a simple experiment. Let \(\mathcal{C}
    = [-1/2,1/2]^d\) be the \(d\)-cube with side lengths \(1\) centered at the origin
    and let \(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\|\leq 1/2\}\)
    be the inscribed \(d\)-ball.'
  prefs: []
  type: TYPE_NORMAL
- en: Now pick a point \(\mathbf{X}\) uniformly at random in \(\mathcal{C}\). What
    is the probability that it falls in \(\mathcal{B}\)?
  prefs: []
  type: TYPE_NORMAL
- en: To generate \(\mathbf{X}\), we pick \(d\) independent random variables \(X_1,
    \ldots, X_d \sim \mathrm{U}[-1/2, 1/2]\), and form the vector \(\mathbf{X} = (X_1,
    \ldots, X_d)\). Indeed, the PDF of \(\mathbf{X}\) is then \(f_{\mathbf{X}}(\mathbf{x})=
    1^d = 1\) if \(\mathbf{x} \in \mathcal{C}\) and \(0\) otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: The event we are interested in is \(A = \left\{\|\mathbf{X}\| \leq 1/2\right\}\).
    The uniform distribution over the set \(\mathcal{C}\) has the property that \(\mathbb{P}[A]\)
    is the volume of \(\mathcal{B}\) divided by the volume of \(\mathcal{C}\). In
    this case, the volume of \(\mathcal{C}\) is \(1^d = 1\) and the volume of \(\mathcal{B}\)
    has an [explicit formula](https://en.wikipedia.org/wiki/Volume_of_an_n-ball).
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to the following surprising fact:'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(High-dimensional Cube)** \(\idx{high-dimensional cube theorem}\xdi\)
    Let \(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d \,:\, \|\mathbf{x}\|\leq 1/2\}\)
    and \(\mathcal{C} = [-1/2,1/2]^d\). Pick \(\mathbf{X} \sim \mathrm{U}[\mathcal{C}]\).
    Then, as \(d \to +\infty\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[\mathbf{X} \in \mathcal{B}] \to 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: In words, in high dimension if one picks a point at random from the cube, it
    is unlikely to be close to the origin. Instead it is likely to be in the corners.
    A geometric interpretation is that a high-dimensional cube is a bit like a “spiky
    ball.”
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Visualization of a high-dimensional cube as a spiky ball (*Credit:*
    Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualization of a high-dimensional cube](../Images/0ed7cc7c9c5326071c245e739ab01a88.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: We give a proof based on *Chebyshev’s Inequality*. It has the advantage of providing
    some insight into this counter-intuitive phenomenon by linking it to the concentration
    of sums of independent random variables, in this case the squared norm of \(\mathbf{X}\).
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We think of \(\|\mathbf{X}\|^2\) as a sum of independent random
    variables and apply *Chebyshev’s Inequality*. It implies that the norm of \(\mathbf{X}\)
    is concentrated around its mean, which grows like \(\sqrt{d}\). The latter is
    larger than \(1/2\) for \(d\) large.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* To see the relevance of *Chebyshev’s Inequality*, we compute the mean
    and standard deviation of the norm of \(\mathbf{X}\). In fact, because of the
    square root in \(\|\mathbf{X}\|\), computing its expectation is difficult. Instead
    we work with the squared norm'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: which has the advantage of being a sum of independent random variables – for
    which the expectation and variance are much easier to compute. Observe further
    that the probability of the event of interest \(\{\|\mathbf{X}\| \leq 1/2\}\)
    can be re-written in terms of \(\|\mathbf{X}\|^2\) as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}
    \left[ \|\mathbf{X}\|^2 \leq 1/4 \right]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To simplify the notation, we use \(\tilde\mu = \mathbb{E}[X_1^2]\) and \(\tilde\sigma
    = \sqrt{\mathrm{Var}[X_1^2]}\) for the mean and standard deviation of \(X_1^2\)
    respectively. Using linearity of expectation and the fact that the \(X_i\)’s are
    independent, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_{\|\mathbf{X}\|^2} = \mathbb{E}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d
    \mathbb{E}[X_i^2] = d \,\mathbb{E}[X_1^2] = \tilde\mu \, d, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{Var}\left[ \|\mathbf{X}\|^2 \right] = \sum_{i=1}^d \mathrm{Var}[X_i^2]
    = d \,\mathrm{Var}[X_1^2]. \]
  prefs: []
  type: TYPE_NORMAL
- en: Taking a square root, we get an expression for the standard deviation of our
    quantity of interest \(\|\mathbf{X}\|^2\) in terms of the standard deviation of
    \(X_1^2\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma_{\|\mathbf{X}\|^2} = \tilde\sigma \, \sqrt{d}. \]
  prefs: []
  type: TYPE_NORMAL
- en: (Note that we could compute \(\tilde\mu\) and \(\tilde\sigma\) explicitly, but
    it will not be necessary here.)
  prefs: []
  type: TYPE_NORMAL
- en: We use *Chebyshev’s Inequality* to show that \(\|\mathbf{X}\|^2\) is highly
    likely to be close to its mean \(\tilde\mu \, d\), which is much larger than \(1/4\)
    when \(d\) is large. And that therefore \(\|\mathbf{X}\|^2\) is highly unlikely
    to be smaller than \(1/4\). We give the details next.
  prefs: []
  type: TYPE_NORMAL
- en: By the one-sided version of *Chebyshev’s Inequality* in terms of the standard
    deviation, we have
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 - \mu_{\|\mathbf{X}\|^2} \leq - \alpha
    \right] \leq \left(\frac{\sigma_{\|\mathbf{X}\|^2}}{\alpha}\right)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, using the formulas above and rearranging slightly,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha \right] \leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2. \]
  prefs: []
  type: TYPE_NORMAL
- en: How do we relate this to the probability of interest \(\mathbb{P}\left[\|\mathbf{X}\|^2
    \leq 1/4\right]\)? Recall that we are free to choose \(\alpha\) in this inequality.
    So simply take \(\alpha\) such that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \tilde\mu \,d - \alpha = \frac{1}{4}, \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, \(\alpha = \tilde\mu \,d - 1/4\). Observe that, once \(d\) is large
    enough, it holds that \(\alpha > 0\).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, replacing this choice of \(\alpha\) in the inequality above gives
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P} \left[ \|\mathbf{X}\| \leq 1/2 \right] &= \mathbb{P}\left[\|\mathbf{X}\|^2
    \leq 1/4\right]\\ &= \mathbb{P}\left[ \|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
    \right]\\ &\leq \left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2\\ &\leq
    \left(\frac{\tilde\sigma \, \sqrt{d}}{\tilde\mu \,d - 1/4}\right)^2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Critically, \(\tilde\mu\) and \(\tilde\sigma\) do not depend on \(d\). So the
    right-hand side goes to \(0\) as \(d \to +\infty\). Indeed, \(d\) is much larger
    than \(\sqrt{d}\) when \(d\) is large. That proves the claim.\(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: We will see later in the course that this high-dimensional phenomenon has implications
    for data science problems. It is behind what is referred to as the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality)\(\idx{curse
    of dimensionality}\xdi\).
  prefs: []
  type: TYPE_NORMAL
- en: While *Chebyshev’s inequality* correctly implies that \(\mathbb{P}[\mathbf{X}
    \in \mathcal{B}]\) goes to \(0\), it does not give the correct rate of convergence.
    In reality, that probability goes to \(0\) at a much faster rate than \(1/d\).
    Specifically, [it can be shown](https://en.wikipedia.org/wiki/Volume_of_an_n-ball#High_dimensions)
    that \(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\) goes to \(0\) roughly as \(d^{-d/2}\).
    We will not derive (or need) this fact here.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We can check the theorem in a simulation. Here we pick
    \(n\) points uniformly at random in the \(d\)-cube \(\mathcal{C}\), for a range
    of dimensions up to `dmax`. We then plot the frequency of landing in the inscribed
    \(d\)-ball \(\mathcal{B}\) and see that it rapidly converges to \(0\). Alternatively,
    we could just plot the formula for the volume of \(\mathcal{B}\). But knowing
    how to do simulations is useful in situations where explicit formulas are unavailable
    or intractable. We plot the result up to dimension \(10\).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/d08f6e8ae99d1fc47a60ae906d9afb4fb92b64d9611fda98fd7415d2bc98a67d.png](../Images/c7ed85ca4cdd1727d357fd1139869726.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** The volume of the \(d\)-dimensional cube \(C = [-1/2, 1/2]^d\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(1/d\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(1/2^d\)
  prefs: []
  type: TYPE_NORMAL
- en: c) 1
  prefs: []
  type: TYPE_NORMAL
- en: d) \(2^d\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** In a high-dimensional cube \(C = [-1/2, 1/2]^d\), as the dimension \(d\)
    increases, the probability that a randomly chosen point lies within the inscribed
    sphere \(B = \{x \in \mathbb{R}^d : \|x\| \le 1/2\}\):'
  prefs: []
  type: TYPE_NORMAL
- en: a) Approaches 1
  prefs: []
  type: TYPE_NORMAL
- en: b) Approaches 1/2
  prefs: []
  type: TYPE_NORMAL
- en: c) Approaches 0
  prefs: []
  type: TYPE_NORMAL
- en: d) Remains constant
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Which of the following best describes the appearance of a high-dimensional
    cube?'
  prefs: []
  type: TYPE_NORMAL
- en: a) A smooth, round ball
  prefs: []
  type: TYPE_NORMAL
- en: b) A spiky ball with most of its volume concentrated in the corners
  prefs: []
  type: TYPE_NORMAL
- en: c) A perfect sphere with uniform volume distribution
  prefs: []
  type: TYPE_NORMAL
- en: d) A flat, pancake-like shape
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Which inequality is used to prove the theorem about high-dimensional
    cubes?'
  prefs: []
  type: TYPE_NORMAL
- en: a) Cauchy-Schwarz inequality
  prefs: []
  type: TYPE_NORMAL
- en: b) Triangle inequality
  prefs: []
  type: TYPE_NORMAL
- en: c) Markov’s inequality
  prefs: []
  type: TYPE_NORMAL
- en: d) Chebyshev’s inequality
  prefs: []
  type: TYPE_NORMAL
- en: '**5** In the proof of the theorem about high-dimensional cubes, which property
    of the squared norm \(\|X\|^2\) is used?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It is a sum of dependent random variables.
  prefs: []
  type: TYPE_NORMAL
- en: b) It is a sum of independent random variables.
  prefs: []
  type: TYPE_NORMAL
- en: c) It is a product of independent random variables.
  prefs: []
  type: TYPE_NORMAL
- en: d) It is a product of dependent random variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: c. Justification: The side length of the cube is 1, and the volume
    of a \(d\)-dimensional cube is the side length raised to the power \(d\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: c. Justification: This is the statement of the theorem “High-dimensional
    Cube” in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: b. Justification: The text mentions, “A geometric interpretation
    is that a high-dimensional cube is a bit like a ‘spiky ball.’”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: d. Justification: The text explicitly states that Chebyshev’s
    inequality is used in the proof.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The proof states, “we work with the squared
    norm'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2, \]
  prefs: []
  type: TYPE_NORMAL
- en: which has the advantage of being a sum of independent random variables.”
  prefs: []
  type: TYPE_NORMAL
