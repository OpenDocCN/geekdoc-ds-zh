<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>What problems fit to GPU?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>What problems fit to GPU?</h1>
<blockquote>原文：<a href="https://enccs.github.io/gpu-programming/3-gpu-problems/">https://enccs.github.io/gpu-programming/3-gpu-problems/</a></blockquote><nav class="wy-nav-top" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"/>
          <a href="../">GPU programming: why, when and how?</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"/></li>
      <li class="breadcrumb-item active">What problems fit to GPU?</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/gpu-programming/blob/main/content/3-gpu-problems.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="what-problems-fit-to-gpu">
<span id="gpu-problems"/>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>What are the strengths and weaknesses of GPUs?</p></li>
<li><p>What makes a particular problem suitable for GPU-porting?</p></li>
<li><p>Why are GPUs so ubiquitous in machine learning applications?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Get a feeling for the type of use cases that GPUs excel at.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>10 min teaching</p></li>
<li><p>10 min exercises</p></li>
</ul>
</div>
<section id="what-are-gpus-good-for">
<h2>What are GPUs good for?</h2>
<p>Answer from <a class="reference external" href="https://scicomp.stackexchange.com/questions/943/what-kinds-of-problems-lend-themselves-well-to-gpu-computing">Stack Exchange</a>:</p>
<blockquote>
<div><p><em>From a metaphorical point of view, the GPU can be seen as a person lying on a bed
of nails. The person lying on top is the data and in the base of each nail there
is a processor, so the nail is actually an arrow pointing from processor to memory.
All nails are in a regular pattern, like a grid. If the body is well spread,
it feels good (performance is good), if the body only touches some spots of the
nail bed, then the pain is bad (bad performance).</em></p>
</div></blockquote>
<p>GPU computing is well-suited to problems that involve large amounts of data parallelism.
Specifically, you can expect good performance on GPUs for:</p>
<ul class="simple">
<li><p><strong>Large-scale matrix and vector operations</strong>: Common in machine learning, scientific computing, and image processing.</p></li>
<li><p><strong>Fourier transforms</strong>: Also common in machine learning, scientific computing, and image processing.</p></li>
<li><p><strong>Monte Carlo simulations</strong>: Used across finance, physics, and other fields to simulate complex systems.</p></li>
<li><p><strong>Molecular dynamics simulations</strong>: Used in chemistry, biochemistry and physics.</p></li>
<li><p><strong>Computational fluid dynamics</strong>: Used in engineering, physics, and other fields.</p></li>
<li><p><strong>Convolutional neural networks</strong> and <strong>computer vision algorithms</strong>.</p></li>
<li><p><strong>Big data analytics</strong>: Clustering, classification, regression, etc.</p></li>
<li><p><strong>Graphics rendering</strong>: Original use-case for GPUs.</p></li>
</ul>
</section>
<section id="what-are-gpus-not-good-for">
<h2>What are GPUs not good for?</h2>
<p>Not all programming problems can efficiently leverage the parallelism offered by GPUs.
Some types of problems that do not fit well on a GPU include:</p>
<ul class="simple">
<li><p><strong>Sequential tasks</strong>: Problems that require a series of dependent steps,
where each step relies on the outcome of the previous step, are not well-suited
for parallel processing. Examples include recursive algorithms, certain dynamic
programming problems, and some graph traversal algorithms.</p></li>
<li><p><strong>Fine-grained branching</strong>: GPUs perform best when the code being executed across
different threads follows a similar control flow. When there is extensive
branching (i.e., many <code class="docutils literal notranslate"><span class="pre">if</span></code> statements) within a kernel or algorithm, performance
may suffer due to the divergence in execution paths among the GPU threads.</p></li>
<li><p><strong>Low arithmetic intensity</strong>: GPUs excel at performing a large number of mathematical
operations quickly. If a problem has low arithmetic intensity (i.e., a low ratio of
arithmetic operations to memory accesses), the GPU may not be able to efficiently utilize
its computational power, leading to underperformance.</p></li>
<li><p><strong>Small data sets</strong>: If the problem involves a small data set that does not require significant
parallelism, using a GPU may not result in noticeable performance gains. In such cases,
the overhead of transferring data between the CPU and GPU, and the time spent initializing the GPU,
may outweigh any potential benefits.</p></li>
<li><p><strong>Limited parallelism</strong>: Some algorithms have inherent limitations on the degree of parallelism that can be
achieved. In these cases, using a GPU may not lead to significant performance improvements.</p></li>
<li><p><strong>Memory-bound problems</strong>: GPUs generally have less memory available compared to CPUs, and their memory bandwidth
can be a limiting factor. If a problem requires a large amount of memory or involves memory-intensive operations,
it may not be well-suited for a GPU.</p></li>
</ul>
</section>
<section id="examples-of-gpu-acceleration">
<h2>Examples of GPU acceleration</h2>
<p>To give a flavor of what type of performance gains we can achieve by porting a calculations to a GPU
(if we’re lucky!), let’s look at a few case examples.</p>
<div class="admonition-effect-of-array-size discussion important admonition" id="discussion-0">
<p class="admonition-title">Effect of array size</p>
<p>Consider the case of matrix multiplication in the Julia language:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span/><span class="k">using</span><span class="w"> </span><span class="n">AMDGPU</span>
<span class="k">using</span><span class="w"> </span><span class="n">BenchmarkTools</span>

<span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="mi">9</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">11</span><span class="p">,</span><span class="w"> </span><span class="mi">12</span><span class="p">]</span>

<span class="k">for</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">N</span>
<span class="w">   </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="o">^</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="o">^</span><span class="n">n</span><span class="p">);</span><span class="w"> </span><span class="n">A_d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ROCArray</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>

<span class="w">   </span><span class="nd">@btime</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="p">;</span>

<span class="w">   </span><span class="nd">@btime</span><span class="w"> </span><span class="k">begin</span>
<span class="w">      </span><span class="o">$</span><span class="n">A_d</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="o">$</span><span class="n">A_d</span><span class="p">;</span>
<span class="w">      </span><span class="n">AMDGPU</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="w">   </span><span class="k">end</span>
<span class="k">end</span>
</pre></div>
</div>
<ul class="simple">
<li><p>How much faster do you think the GPU version is compared to running on a single CPU core?</p></li>
<li><p>Julia automatically parallelises matrix multiplication over available CPU cores. Will the GPU version be faster than running on 64 cores?</p></li>
<li><p>Does the size of the array affect how much the performance improves?</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>Example results from running on LUMI (MI250X AMD GPU, 64-core AMD Trento CPUs):</p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">GPU acceleration for matrix multiply in Julia</span></caption>
<colgroup>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Matrix size</p></th>
<th class="head"><p>1 CPU core</p></th>
<th class="head"><p>64 CPU cores</p></th>
<th class="head"><p>1 GPU</p></th>
<th class="head"><p>GPU speedup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(512, 512)</p></td>
<td><p>5.472 ms</p></td>
<td><p>517.722 μs</p></td>
<td><p>115.805 μs</p></td>
<td><p>~47x / ~5x</p></td>
</tr>
<tr class="row-odd"><td><p>(1024, 1024)</p></td>
<td><p>43.364 ms</p></td>
<td><p>2.929 ms</p></td>
<td><p>173.316 μs</p></td>
<td><p>~250x / ~17x</p></td>
</tr>
<tr class="row-even"><td><p>(2048, 2048)</p></td>
<td><p>344.364 ms</p></td>
<td><p>30.081 ms</p></td>
<td><p>866.348 μs</p></td>
<td><p>~400x / ~35x</p></td>
</tr>
<tr class="row-odd"><td><p>(4096, 4096)</p></td>
<td><p>3.221 s</p></td>
<td><p>159.563 ms</p></td>
<td><p>5.910 ms</p></td>
<td><p>~550x / ~27x</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="electronic-structure-calculations">
<h3>Electronic structure calculations</h3>
<p><a class="reference external" href="https://www.vasp.at/">VASP</a> is a popular software package used for electronic structure calculations. The figures below show the speedup observed in a recent benchmark study on the <a class="reference external" href="https://ieeexplore.ieee.org/document/10820603">VASP Power Profiles on NVIDIA A100 GPUs</a>, which was conducted on the Perlmutter system at NERSC.
An analysis of total energy usage demonstrated that VASP’s power usage varies significantly with different workloads, more so than with parallel concurrency.
Additionally, power capping GPUs to 50% of their Thermal Design Power can be applied to most VASP workloads with less than a 10% performance loss.</p>
<figure class="align-center" id="id2">
<img alt="../_images/vasp_parallel_efficiency.png" src="../Images/0b6eaaa4ff02027d294385d1f2cbaf8a.png" data-original-src="https://enccs.github.io/gpu-programming/_images/vasp_parallel_efficiency.png"/>
<figcaption>
<p><span class="caption-text">Parallel efficiency of VASP on seven test cases representing diverse VASP production workloads and ensuring a comprehensive coverage of various code paths, elements, and problem sizes.</span></p>
</figcaption>
</figure>
<figure class="align-center" id="id3">
<img alt="../_images/vasp_energy_consumption.jpg" src="../Images/6fa5e8533100766ad3399b8e454b4d19.png" data-original-src="https://enccs.github.io/gpu-programming/_images/vasp_energy_consumption.jpg"/>
<figcaption>
<p><span class="caption-text">(Left) Power usage of seven representative VASP workloads. The horizontal axis shows number of nodes used, and vertical axis shows high power mode per node. (Right) Power consumed per GPU when running VASP under four different power caps: 400 W (default), 300 W, 200 W, and 100 W. Horizontal axis shows power caps applied to GPUs, and vertical axis shows high power mode per GPU as a fraction of applied cap. The dashed horizontal line represents applied power cap. Each benchmark was run with a node count optimizing runtime while remaining above 70% parallel efficiency.</span></p>
</figcaption>
</figure>
</section>
<section id="computational-chemistry">
<h3>Computational Chemistry</h3>
<p>A great deal of computational resources are spent in Quantum Chemical calculations which involve
the solution of the Hartree-Fock eigenvalue problem, which requires the diagonalization of the
Fock matrix whose elements are given by:</p>
<div class="math notranslate nohighlight">
\[F_{\alpha \beta} = H^{\textrm{core}}_{\alpha \beta} + \sum_{\gamma \delta}D_{\gamma \delta} \left [ (\alpha \beta|\gamma \delta) - \frac{1}{2} (\alpha \delta|\gamma \beta) \right ],\]</div>
<p>The first term is related to the one electron contributions and the second term is related to the
electron repulsion integrals (ERIs), in parenthesis, weighted by the by the density matrix
<span class="math notranslate nohighlight">\(D_{\gamma \delta}\)</span>. One of the most expensive parts in the solution of the Hartree-Fock equations is the
processing (digestion) of the ERIs, one algorithm to do this task is as follows:</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/algorithms.svg"><img alt="../_images/algorithms.svg" src="../Images/8315333e7eb919a0358c1a5aad382eaf.png" style="width: 200px;" data-original-src="https://enccs.github.io/gpu-programming/_images/algorithms.svg"/>
</a>
<figcaption>
<p><span class="caption-text">Algorithm for processing ERIs [see <a class="reference external" href="https://doi.org/10.1021/acs.jctc.1c00720">JCTC, 17, 7486, (2021)</a> for details]</span></p>
</figcaption>
</figure>
<p>This algorithm is suitable for GPUs as it involves many arithmetic operations. In addition to this,
there are symmetries and properties of the integrals that could be used to rearrange the loops in
an efficient manner that fit GPU architectures.</p>
</section>
<section id="humanities">
<h3>Humanities</h3>
<p>A brief introduction into some of the work that is being done in the humanities that can benefit from utilizing GPUs.</p>
<p><strong>Language models and NLP (natural language processing)</strong></p>
<p>With the recent popularity of ChatGPT, the use of language models has come into the mainstream,
however such models have been used in the humanities many years already. One of the biggest goals of humanities
researchers is working with textual data which has increased exponentially over recent years due to the rise in
social media. Analyzing such textual data to gain insights into questions of sociology, linguistics and various
other fields have become increasingly reliant on using language models. Along with language models,
the need for GPU access has become essential.</p>
<p><strong>Archeology</strong></p>
<p>The field of archeology also makes use of GPUs in their 3D modelling
and rendering work. The biggest problem with archeological sites is that once they are excavated,
they are destroyed, so any researchers who aren’t present at the site, would lose valuable insights into how
it looked when it was found. However, with recent developments in technology and accessibility to high-performance
computing, they are able to generate extremely detailed renderings of the excavation sites which act as a way to
preserve the site for future researchers to gain critical insights and contribute to the research.</p>
<p><strong>Cognitive Science</strong></p>
<p>Techniques such as Markov Chain Monte Carlo (MCMC) sampling have proven to be invaluable in studies that delve into human behavior or population dynamics. MCMC sampling allows researchers to simulate and analyze complex systems by iteratively sampling from a Markov chain, enabling the exploration of high-dimensional parameter spaces. This method is particularly useful when studying human behavior, as it can capture the inherent randomness and interdependencies that characterize social systems. By leveraging MCMC sampling, researchers can gain insights into various aspects of human behavior, such as decision-making, social interactions, and the spread of information or diseases within populations.</p>
<p>By offloading the computational workload to GPUs, researchers can experience substantial speedup in the execution of MCMC algorithms. This speedup allows for more extensive exploration of parameter spaces and facilitates the analysis of larger datasets, leading to more accurate and detailed insights into human behavior or population dynamics. Examples of studies done using these methods can be found at the <a class="reference external" href="https://chc.au.dk/">Center for Humanities Computing Aarhus</a> (CHCAA) and <a class="reference external" href="https://interactingminds.au.dk/">Interacting Minds Centre</a> (IMC) at Aarhus University.</p>
</section>
</section>
<section id="exercises">
<h2>Exercises</h2>
<div class="admonition-discussion exercise important admonition" id="exercise-0">
<p class="admonition-title">Discussion</p>
<ul class="simple">
<li><p>What type of problems have you used GPUs for?</p></li>
<li><p>How large was the performance boost?</p></li>
</ul>
</div>
<div class="admonition-good-and-bad-use-cases-for-gpu-porting exercise important admonition" id="exercise-1">
<p class="admonition-title">Good and bad use cases for GPU porting</p>
<p>Which of the following computational tasks is likely to gain the least performance benefit from being ported to a GPU?</p>
<ol class="arabic simple">
<li><p>Training a large, deep neural network.</p></li>
<li><p>Performing a Monte Carlo simulation with a large number of independent trials.</p></li>
<li><p>Executing an algorithm with heavy use of recursion and frequent branching.</p></li>
<li><p>Processing a large image with a convolutional filter.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>The right answer is option 3. GPUs do not handle recursion and branching as effectively as more
data-heavy algorithms.</p>
</div>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>GPUs excel in processing tasks with high data parallelism, such as large-scale matrix operations, Fourier transforms, and big data analytics.</p></li>
<li><p>GPUs struggle with sequential tasks, problems with extensive control flow divergence, low arithmetic intensity tasks, small data sets, and memory-bound problems.</p></li>
</ul>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../2-gpu-ecosystem/" class="btn btn-neutral float-left" title="The GPU hardware and software ecosystem" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"/> Previous</a>
        <a href="../4-gpu-concepts/" class="btn btn-neutral float-right" title="GPU programming concepts" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"/></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>© Copyright 2023-2024, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    &#13;

<span id="gpu-problems"/>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>What are the strengths and weaknesses of GPUs?</p></li>
<li><p>What makes a particular problem suitable for GPU-porting?</p></li>
<li><p>Why are GPUs so ubiquitous in machine learning applications?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Get a feeling for the type of use cases that GPUs excel at.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>10 min teaching</p></li>
<li><p>10 min exercises</p></li>
</ul>
</div>
<section id="what-are-gpus-good-for">
<h2>What are GPUs good for?</h2>
<p>Answer from <a class="reference external" href="https://scicomp.stackexchange.com/questions/943/what-kinds-of-problems-lend-themselves-well-to-gpu-computing">Stack Exchange</a>:</p>
<blockquote>
<div><p><em>From a metaphorical point of view, the GPU can be seen as a person lying on a bed
of nails. The person lying on top is the data and in the base of each nail there
is a processor, so the nail is actually an arrow pointing from processor to memory.
All nails are in a regular pattern, like a grid. If the body is well spread,
it feels good (performance is good), if the body only touches some spots of the
nail bed, then the pain is bad (bad performance).</em></p>
</div></blockquote>
<p>GPU computing is well-suited to problems that involve large amounts of data parallelism.
Specifically, you can expect good performance on GPUs for:</p>
<ul class="simple">
<li><p><strong>Large-scale matrix and vector operations</strong>: Common in machine learning, scientific computing, and image processing.</p></li>
<li><p><strong>Fourier transforms</strong>: Also common in machine learning, scientific computing, and image processing.</p></li>
<li><p><strong>Monte Carlo simulations</strong>: Used across finance, physics, and other fields to simulate complex systems.</p></li>
<li><p><strong>Molecular dynamics simulations</strong>: Used in chemistry, biochemistry and physics.</p></li>
<li><p><strong>Computational fluid dynamics</strong>: Used in engineering, physics, and other fields.</p></li>
<li><p><strong>Convolutional neural networks</strong> and <strong>computer vision algorithms</strong>.</p></li>
<li><p><strong>Big data analytics</strong>: Clustering, classification, regression, etc.</p></li>
<li><p><strong>Graphics rendering</strong>: Original use-case for GPUs.</p></li>
</ul>
</section>
<section id="what-are-gpus-not-good-for">
<h2>What are GPUs not good for?</h2>
<p>Not all programming problems can efficiently leverage the parallelism offered by GPUs.
Some types of problems that do not fit well on a GPU include:</p>
<ul class="simple">
<li><p><strong>Sequential tasks</strong>: Problems that require a series of dependent steps,
where each step relies on the outcome of the previous step, are not well-suited
for parallel processing. Examples include recursive algorithms, certain dynamic
programming problems, and some graph traversal algorithms.</p></li>
<li><p><strong>Fine-grained branching</strong>: GPUs perform best when the code being executed across
different threads follows a similar control flow. When there is extensive
branching (i.e., many <code class="docutils literal notranslate"><span class="pre">if</span></code> statements) within a kernel or algorithm, performance
may suffer due to the divergence in execution paths among the GPU threads.</p></li>
<li><p><strong>Low arithmetic intensity</strong>: GPUs excel at performing a large number of mathematical
operations quickly. If a problem has low arithmetic intensity (i.e., a low ratio of
arithmetic operations to memory accesses), the GPU may not be able to efficiently utilize
its computational power, leading to underperformance.</p></li>
<li><p><strong>Small data sets</strong>: If the problem involves a small data set that does not require significant
parallelism, using a GPU may not result in noticeable performance gains. In such cases,
the overhead of transferring data between the CPU and GPU, and the time spent initializing the GPU,
may outweigh any potential benefits.</p></li>
<li><p><strong>Limited parallelism</strong>: Some algorithms have inherent limitations on the degree of parallelism that can be
achieved. In these cases, using a GPU may not lead to significant performance improvements.</p></li>
<li><p><strong>Memory-bound problems</strong>: GPUs generally have less memory available compared to CPUs, and their memory bandwidth
can be a limiting factor. If a problem requires a large amount of memory or involves memory-intensive operations,
it may not be well-suited for a GPU.</p></li>
</ul>
</section>
<section id="examples-of-gpu-acceleration">
<h2>Examples of GPU acceleration</h2>
<p>To give a flavor of what type of performance gains we can achieve by porting a calculations to a GPU
(if we’re lucky!), let’s look at a few case examples.</p>
<div class="admonition-effect-of-array-size discussion important admonition" id="discussion-0">
<p class="admonition-title">Effect of array size</p>
<p>Consider the case of matrix multiplication in the Julia language:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span/><span class="k">using</span><span class="w"> </span><span class="n">AMDGPU</span>
<span class="k">using</span><span class="w"> </span><span class="n">BenchmarkTools</span>

<span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="mi">9</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">11</span><span class="p">,</span><span class="w"> </span><span class="mi">12</span><span class="p">]</span>

<span class="k">for</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">N</span>
<span class="w">   </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="o">^</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="o">^</span><span class="n">n</span><span class="p">);</span><span class="w"> </span><span class="n">A_d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ROCArray</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>

<span class="w">   </span><span class="nd">@btime</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="p">;</span>

<span class="w">   </span><span class="nd">@btime</span><span class="w"> </span><span class="k">begin</span>
<span class="w">      </span><span class="o">$</span><span class="n">A_d</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="o">$</span><span class="n">A_d</span><span class="p">;</span>
<span class="w">      </span><span class="n">AMDGPU</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="w">   </span><span class="k">end</span>
<span class="k">end</span>
</pre></div>
</div>
<ul class="simple">
<li><p>How much faster do you think the GPU version is compared to running on a single CPU core?</p></li>
<li><p>Julia automatically parallelises matrix multiplication over available CPU cores. Will the GPU version be faster than running on 64 cores?</p></li>
<li><p>Does the size of the array affect how much the performance improves?</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>Example results from running on LUMI (MI250X AMD GPU, 64-core AMD Trento CPUs):</p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">GPU acceleration for matrix multiply in Julia</span></caption>
<colgroup>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Matrix size</p></th>
<th class="head"><p>1 CPU core</p></th>
<th class="head"><p>64 CPU cores</p></th>
<th class="head"><p>1 GPU</p></th>
<th class="head"><p>GPU speedup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(512, 512)</p></td>
<td><p>5.472 ms</p></td>
<td><p>517.722 μs</p></td>
<td><p>115.805 μs</p></td>
<td><p>~47x / ~5x</p></td>
</tr>
<tr class="row-odd"><td><p>(1024, 1024)</p></td>
<td><p>43.364 ms</p></td>
<td><p>2.929 ms</p></td>
<td><p>173.316 μs</p></td>
<td><p>~250x / ~17x</p></td>
</tr>
<tr class="row-even"><td><p>(2048, 2048)</p></td>
<td><p>344.364 ms</p></td>
<td><p>30.081 ms</p></td>
<td><p>866.348 μs</p></td>
<td><p>~400x / ~35x</p></td>
</tr>
<tr class="row-odd"><td><p>(4096, 4096)</p></td>
<td><p>3.221 s</p></td>
<td><p>159.563 ms</p></td>
<td><p>5.910 ms</p></td>
<td><p>~550x / ~27x</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="electronic-structure-calculations">
<h3>Electronic structure calculations</h3>
<p><a class="reference external" href="https://www.vasp.at/">VASP</a> is a popular software package used for electronic structure calculations. The figures below show the speedup observed in a recent benchmark study on the <a class="reference external" href="https://ieeexplore.ieee.org/document/10820603">VASP Power Profiles on NVIDIA A100 GPUs</a>, which was conducted on the Perlmutter system at NERSC.
An analysis of total energy usage demonstrated that VASP’s power usage varies significantly with different workloads, more so than with parallel concurrency.
Additionally, power capping GPUs to 50% of their Thermal Design Power can be applied to most VASP workloads with less than a 10% performance loss.</p>
<figure class="align-center" id="id2">
<img alt="../_images/vasp_parallel_efficiency.png" src="../Images/0b6eaaa4ff02027d294385d1f2cbaf8a.png" data-original-src="https://enccs.github.io/gpu-programming/_images/vasp_parallel_efficiency.png"/>
<figcaption>
<p><span class="caption-text">Parallel efficiency of VASP on seven test cases representing diverse VASP production workloads and ensuring a comprehensive coverage of various code paths, elements, and problem sizes.</span></p>
</figcaption>
</figure>
<figure class="align-center" id="id3">
<img alt="../_images/vasp_energy_consumption.jpg" src="../Images/6fa5e8533100766ad3399b8e454b4d19.png" data-original-src="https://enccs.github.io/gpu-programming/_images/vasp_energy_consumption.jpg"/>
<figcaption>
<p><span class="caption-text">(Left) Power usage of seven representative VASP workloads. The horizontal axis shows number of nodes used, and vertical axis shows high power mode per node. (Right) Power consumed per GPU when running VASP under four different power caps: 400 W (default), 300 W, 200 W, and 100 W. Horizontal axis shows power caps applied to GPUs, and vertical axis shows high power mode per GPU as a fraction of applied cap. The dashed horizontal line represents applied power cap. Each benchmark was run with a node count optimizing runtime while remaining above 70% parallel efficiency.</span></p>
</figcaption>
</figure>
</section>
<section id="computational-chemistry">
<h3>Computational Chemistry</h3>
<p>A great deal of computational resources are spent in Quantum Chemical calculations which involve
the solution of the Hartree-Fock eigenvalue problem, which requires the diagonalization of the
Fock matrix whose elements are given by:</p>
<div class="math notranslate nohighlight">
\[F_{\alpha \beta} = H^{\textrm{core}}_{\alpha \beta} + \sum_{\gamma \delta}D_{\gamma \delta} \left [ (\alpha \beta|\gamma \delta) - \frac{1}{2} (\alpha \delta|\gamma \beta) \right ],\]</div>
<p>The first term is related to the one electron contributions and the second term is related to the
electron repulsion integrals (ERIs), in parenthesis, weighted by the by the density matrix
<span class="math notranslate nohighlight">\(D_{\gamma \delta}\)</span>. One of the most expensive parts in the solution of the Hartree-Fock equations is the
processing (digestion) of the ERIs, one algorithm to do this task is as follows:</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/algorithms.svg"><img alt="../_images/algorithms.svg" src="../Images/8315333e7eb919a0358c1a5aad382eaf.png" style="width: 200px;" data-original-src="https://enccs.github.io/gpu-programming/_images/algorithms.svg"/>
</a>
<figcaption>
<p><span class="caption-text">Algorithm for processing ERIs [see <a class="reference external" href="https://doi.org/10.1021/acs.jctc.1c00720">JCTC, 17, 7486, (2021)</a> for details]</span></p>
</figcaption>
</figure>
<p>This algorithm is suitable for GPUs as it involves many arithmetic operations. In addition to this,
there are symmetries and properties of the integrals that could be used to rearrange the loops in
an efficient manner that fit GPU architectures.</p>
</section>
<section id="humanities">
<h3>Humanities</h3>
<p>A brief introduction into some of the work that is being done in the humanities that can benefit from utilizing GPUs.</p>
<p><strong>Language models and NLP (natural language processing)</strong></p>
<p>With the recent popularity of ChatGPT, the use of language models has come into the mainstream,
however such models have been used in the humanities many years already. One of the biggest goals of humanities
researchers is working with textual data which has increased exponentially over recent years due to the rise in
social media. Analyzing such textual data to gain insights into questions of sociology, linguistics and various
other fields have become increasingly reliant on using language models. Along with language models,
the need for GPU access has become essential.</p>
<p><strong>Archeology</strong></p>
<p>The field of archeology also makes use of GPUs in their 3D modelling
and rendering work. The biggest problem with archeological sites is that once they are excavated,
they are destroyed, so any researchers who aren’t present at the site, would lose valuable insights into how
it looked when it was found. However, with recent developments in technology and accessibility to high-performance
computing, they are able to generate extremely detailed renderings of the excavation sites which act as a way to
preserve the site for future researchers to gain critical insights and contribute to the research.</p>
<p><strong>Cognitive Science</strong></p>
<p>Techniques such as Markov Chain Monte Carlo (MCMC) sampling have proven to be invaluable in studies that delve into human behavior or population dynamics. MCMC sampling allows researchers to simulate and analyze complex systems by iteratively sampling from a Markov chain, enabling the exploration of high-dimensional parameter spaces. This method is particularly useful when studying human behavior, as it can capture the inherent randomness and interdependencies that characterize social systems. By leveraging MCMC sampling, researchers can gain insights into various aspects of human behavior, such as decision-making, social interactions, and the spread of information or diseases within populations.</p>
<p>By offloading the computational workload to GPUs, researchers can experience substantial speedup in the execution of MCMC algorithms. This speedup allows for more extensive exploration of parameter spaces and facilitates the analysis of larger datasets, leading to more accurate and detailed insights into human behavior or population dynamics. Examples of studies done using these methods can be found at the <a class="reference external" href="https://chc.au.dk/">Center for Humanities Computing Aarhus</a> (CHCAA) and <a class="reference external" href="https://interactingminds.au.dk/">Interacting Minds Centre</a> (IMC) at Aarhus University.</p>
</section>
</section>
<section id="exercises">
<h2>Exercises</h2>
<div class="admonition-discussion exercise important admonition" id="exercise-0">
<p class="admonition-title">Discussion</p>
<ul class="simple">
<li><p>What type of problems have you used GPUs for?</p></li>
<li><p>How large was the performance boost?</p></li>
</ul>
</div>
<div class="admonition-good-and-bad-use-cases-for-gpu-porting exercise important admonition" id="exercise-1">
<p class="admonition-title">Good and bad use cases for GPU porting</p>
<p>Which of the following computational tasks is likely to gain the least performance benefit from being ported to a GPU?</p>
<ol class="arabic simple">
<li><p>Training a large, deep neural network.</p></li>
<li><p>Performing a Monte Carlo simulation with a large number of independent trials.</p></li>
<li><p>Executing an algorithm with heavy use of recursion and frequent branching.</p></li>
<li><p>Processing a large image with a convolutional filter.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>The right answer is option 3. GPUs do not handle recursion and branching as effectively as more
data-heavy algorithms.</p>
</div>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>GPUs excel in processing tasks with high data parallelism, such as large-scale matrix operations, Fourier transforms, and big data analytics.</p></li>
<li><p>GPUs struggle with sequential tasks, problems with extensive control flow divergence, low arithmetic intensity tasks, small data sets, and memory-bound problems.</p></li>
</ul>
</div>
</section>
&#13;

<h2>What are GPUs good for?</h2>
<p>Answer from <a class="reference external" href="https://scicomp.stackexchange.com/questions/943/what-kinds-of-problems-lend-themselves-well-to-gpu-computing">Stack Exchange</a>:</p>
<blockquote>
<div><p><em>From a metaphorical point of view, the GPU can be seen as a person lying on a bed
of nails. The person lying on top is the data and in the base of each nail there
is a processor, so the nail is actually an arrow pointing from processor to memory.
All nails are in a regular pattern, like a grid. If the body is well spread,
it feels good (performance is good), if the body only touches some spots of the
nail bed, then the pain is bad (bad performance).</em></p>
</div></blockquote>
<p>GPU computing is well-suited to problems that involve large amounts of data parallelism.
Specifically, you can expect good performance on GPUs for:</p>
<ul class="simple">
<li><p><strong>Large-scale matrix and vector operations</strong>: Common in machine learning, scientific computing, and image processing.</p></li>
<li><p><strong>Fourier transforms</strong>: Also common in machine learning, scientific computing, and image processing.</p></li>
<li><p><strong>Monte Carlo simulations</strong>: Used across finance, physics, and other fields to simulate complex systems.</p></li>
<li><p><strong>Molecular dynamics simulations</strong>: Used in chemistry, biochemistry and physics.</p></li>
<li><p><strong>Computational fluid dynamics</strong>: Used in engineering, physics, and other fields.</p></li>
<li><p><strong>Convolutional neural networks</strong> and <strong>computer vision algorithms</strong>.</p></li>
<li><p><strong>Big data analytics</strong>: Clustering, classification, regression, etc.</p></li>
<li><p><strong>Graphics rendering</strong>: Original use-case for GPUs.</p></li>
</ul>
&#13;

<h2>What are GPUs not good for?</h2>
<p>Not all programming problems can efficiently leverage the parallelism offered by GPUs.
Some types of problems that do not fit well on a GPU include:</p>
<ul class="simple">
<li><p><strong>Sequential tasks</strong>: Problems that require a series of dependent steps,
where each step relies on the outcome of the previous step, are not well-suited
for parallel processing. Examples include recursive algorithms, certain dynamic
programming problems, and some graph traversal algorithms.</p></li>
<li><p><strong>Fine-grained branching</strong>: GPUs perform best when the code being executed across
different threads follows a similar control flow. When there is extensive
branching (i.e., many <code class="docutils literal notranslate"><span class="pre">if</span></code> statements) within a kernel or algorithm, performance
may suffer due to the divergence in execution paths among the GPU threads.</p></li>
<li><p><strong>Low arithmetic intensity</strong>: GPUs excel at performing a large number of mathematical
operations quickly. If a problem has low arithmetic intensity (i.e., a low ratio of
arithmetic operations to memory accesses), the GPU may not be able to efficiently utilize
its computational power, leading to underperformance.</p></li>
<li><p><strong>Small data sets</strong>: If the problem involves a small data set that does not require significant
parallelism, using a GPU may not result in noticeable performance gains. In such cases,
the overhead of transferring data between the CPU and GPU, and the time spent initializing the GPU,
may outweigh any potential benefits.</p></li>
<li><p><strong>Limited parallelism</strong>: Some algorithms have inherent limitations on the degree of parallelism that can be
achieved. In these cases, using a GPU may not lead to significant performance improvements.</p></li>
<li><p><strong>Memory-bound problems</strong>: GPUs generally have less memory available compared to CPUs, and their memory bandwidth
can be a limiting factor. If a problem requires a large amount of memory or involves memory-intensive operations,
it may not be well-suited for a GPU.</p></li>
</ul>
&#13;

<h2>Examples of GPU acceleration</h2>
<p>To give a flavor of what type of performance gains we can achieve by porting a calculations to a GPU
(if we’re lucky!), let’s look at a few case examples.</p>
<div class="admonition-effect-of-array-size discussion important admonition" id="discussion-0">
<p class="admonition-title">Effect of array size</p>
<p>Consider the case of matrix multiplication in the Julia language:</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span/><span class="k">using</span><span class="w"> </span><span class="n">AMDGPU</span>
<span class="k">using</span><span class="w"> </span><span class="n">BenchmarkTools</span>

<span class="n">N</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="mi">9</span><span class="p">,</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="mi">11</span><span class="p">,</span><span class="w"> </span><span class="mi">12</span><span class="p">]</span>

<span class="k">for</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">N</span>
<span class="w">   </span><span class="n">A</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="o">^</span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="o">^</span><span class="n">n</span><span class="p">);</span><span class="w"> </span><span class="n">A_d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">ROCArray</span><span class="p">(</span><span class="n">A</span><span class="p">);</span>

<span class="w">   </span><span class="nd">@btime</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="o">$</span><span class="n">A</span><span class="p">;</span>

<span class="w">   </span><span class="nd">@btime</span><span class="w"> </span><span class="k">begin</span>
<span class="w">      </span><span class="o">$</span><span class="n">A_d</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="o">$</span><span class="n">A_d</span><span class="p">;</span>
<span class="w">      </span><span class="n">AMDGPU</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>
<span class="w">   </span><span class="k">end</span>
<span class="k">end</span>
</pre></div>
</div>
<ul class="simple">
<li><p>How much faster do you think the GPU version is compared to running on a single CPU core?</p></li>
<li><p>Julia automatically parallelises matrix multiplication over available CPU cores. Will the GPU version be faster than running on 64 cores?</p></li>
<li><p>Does the size of the array affect how much the performance improves?</p></li>
</ul>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>Example results from running on LUMI (MI250X AMD GPU, 64-core AMD Trento CPUs):</p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">GPU acceleration for matrix multiply in Julia</span></caption>
<colgroup>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
<col style="width: 20.0%"/>
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Matrix size</p></th>
<th class="head"><p>1 CPU core</p></th>
<th class="head"><p>64 CPU cores</p></th>
<th class="head"><p>1 GPU</p></th>
<th class="head"><p>GPU speedup</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>(512, 512)</p></td>
<td><p>5.472 ms</p></td>
<td><p>517.722 μs</p></td>
<td><p>115.805 μs</p></td>
<td><p>~47x / ~5x</p></td>
</tr>
<tr class="row-odd"><td><p>(1024, 1024)</p></td>
<td><p>43.364 ms</p></td>
<td><p>2.929 ms</p></td>
<td><p>173.316 μs</p></td>
<td><p>~250x / ~17x</p></td>
</tr>
<tr class="row-even"><td><p>(2048, 2048)</p></td>
<td><p>344.364 ms</p></td>
<td><p>30.081 ms</p></td>
<td><p>866.348 μs</p></td>
<td><p>~400x / ~35x</p></td>
</tr>
<tr class="row-odd"><td><p>(4096, 4096)</p></td>
<td><p>3.221 s</p></td>
<td><p>159.563 ms</p></td>
<td><p>5.910 ms</p></td>
<td><p>~550x / ~27x</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<section id="electronic-structure-calculations">
<h3>Electronic structure calculations</h3>
<p><a class="reference external" href="https://www.vasp.at/">VASP</a> is a popular software package used for electronic structure calculations. The figures below show the speedup observed in a recent benchmark study on the <a class="reference external" href="https://ieeexplore.ieee.org/document/10820603">VASP Power Profiles on NVIDIA A100 GPUs</a>, which was conducted on the Perlmutter system at NERSC.
An analysis of total energy usage demonstrated that VASP’s power usage varies significantly with different workloads, more so than with parallel concurrency.
Additionally, power capping GPUs to 50% of their Thermal Design Power can be applied to most VASP workloads with less than a 10% performance loss.</p>
<figure class="align-center" id="id2">
<img alt="../_images/vasp_parallel_efficiency.png" src="../Images/0b6eaaa4ff02027d294385d1f2cbaf8a.png" data-original-src="https://enccs.github.io/gpu-programming/_images/vasp_parallel_efficiency.png"/>
<figcaption>
<p><span class="caption-text">Parallel efficiency of VASP on seven test cases representing diverse VASP production workloads and ensuring a comprehensive coverage of various code paths, elements, and problem sizes.</span></p>
</figcaption>
</figure>
<figure class="align-center" id="id3">
<img alt="../_images/vasp_energy_consumption.jpg" src="../Images/6fa5e8533100766ad3399b8e454b4d19.png" data-original-src="https://enccs.github.io/gpu-programming/_images/vasp_energy_consumption.jpg"/>
<figcaption>
<p><span class="caption-text">(Left) Power usage of seven representative VASP workloads. The horizontal axis shows number of nodes used, and vertical axis shows high power mode per node. (Right) Power consumed per GPU when running VASP under four different power caps: 400 W (default), 300 W, 200 W, and 100 W. Horizontal axis shows power caps applied to GPUs, and vertical axis shows high power mode per GPU as a fraction of applied cap. The dashed horizontal line represents applied power cap. Each benchmark was run with a node count optimizing runtime while remaining above 70% parallel efficiency.</span></p>
</figcaption>
</figure>
</section>
<section id="computational-chemistry">
<h3>Computational Chemistry</h3>
<p>A great deal of computational resources are spent in Quantum Chemical calculations which involve
the solution of the Hartree-Fock eigenvalue problem, which requires the diagonalization of the
Fock matrix whose elements are given by:</p>
<div class="math notranslate nohighlight">
\[F_{\alpha \beta} = H^{\textrm{core}}_{\alpha \beta} + \sum_{\gamma \delta}D_{\gamma \delta} \left [ (\alpha \beta|\gamma \delta) - \frac{1}{2} (\alpha \delta|\gamma \beta) \right ],\]</div>
<p>The first term is related to the one electron contributions and the second term is related to the
electron repulsion integrals (ERIs), in parenthesis, weighted by the by the density matrix
<span class="math notranslate nohighlight">\(D_{\gamma \delta}\)</span>. One of the most expensive parts in the solution of the Hartree-Fock equations is the
processing (digestion) of the ERIs, one algorithm to do this task is as follows:</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/algorithms.svg"><img alt="../_images/algorithms.svg" src="../Images/8315333e7eb919a0358c1a5aad382eaf.png" style="width: 200px;" data-original-src="https://enccs.github.io/gpu-programming/_images/algorithms.svg"/>
</a>
<figcaption>
<p><span class="caption-text">Algorithm for processing ERIs [see <a class="reference external" href="https://doi.org/10.1021/acs.jctc.1c00720">JCTC, 17, 7486, (2021)</a> for details]</span></p>
</figcaption>
</figure>
<p>This algorithm is suitable for GPUs as it involves many arithmetic operations. In addition to this,
there are symmetries and properties of the integrals that could be used to rearrange the loops in
an efficient manner that fit GPU architectures.</p>
</section>
<section id="humanities">
<h3>Humanities</h3>
<p>A brief introduction into some of the work that is being done in the humanities that can benefit from utilizing GPUs.</p>
<p><strong>Language models and NLP (natural language processing)</strong></p>
<p>With the recent popularity of ChatGPT, the use of language models has come into the mainstream,
however such models have been used in the humanities many years already. One of the biggest goals of humanities
researchers is working with textual data which has increased exponentially over recent years due to the rise in
social media. Analyzing such textual data to gain insights into questions of sociology, linguistics and various
other fields have become increasingly reliant on using language models. Along with language models,
the need for GPU access has become essential.</p>
<p><strong>Archeology</strong></p>
<p>The field of archeology also makes use of GPUs in their 3D modelling
and rendering work. The biggest problem with archeological sites is that once they are excavated,
they are destroyed, so any researchers who aren’t present at the site, would lose valuable insights into how
it looked when it was found. However, with recent developments in technology and accessibility to high-performance
computing, they are able to generate extremely detailed renderings of the excavation sites which act as a way to
preserve the site for future researchers to gain critical insights and contribute to the research.</p>
<p><strong>Cognitive Science</strong></p>
<p>Techniques such as Markov Chain Monte Carlo (MCMC) sampling have proven to be invaluable in studies that delve into human behavior or population dynamics. MCMC sampling allows researchers to simulate and analyze complex systems by iteratively sampling from a Markov chain, enabling the exploration of high-dimensional parameter spaces. This method is particularly useful when studying human behavior, as it can capture the inherent randomness and interdependencies that characterize social systems. By leveraging MCMC sampling, researchers can gain insights into various aspects of human behavior, such as decision-making, social interactions, and the spread of information or diseases within populations.</p>
<p>By offloading the computational workload to GPUs, researchers can experience substantial speedup in the execution of MCMC algorithms. This speedup allows for more extensive exploration of parameter spaces and facilitates the analysis of larger datasets, leading to more accurate and detailed insights into human behavior or population dynamics. Examples of studies done using these methods can be found at the <a class="reference external" href="https://chc.au.dk/">Center for Humanities Computing Aarhus</a> (CHCAA) and <a class="reference external" href="https://interactingminds.au.dk/">Interacting Minds Centre</a> (IMC) at Aarhus University.</p>
</section>
&#13;

<h3>Electronic structure calculations</h3>
<p><a class="reference external" href="https://www.vasp.at/">VASP</a> is a popular software package used for electronic structure calculations. The figures below show the speedup observed in a recent benchmark study on the <a class="reference external" href="https://ieeexplore.ieee.org/document/10820603">VASP Power Profiles on NVIDIA A100 GPUs</a>, which was conducted on the Perlmutter system at NERSC.
An analysis of total energy usage demonstrated that VASP’s power usage varies significantly with different workloads, more so than with parallel concurrency.
Additionally, power capping GPUs to 50% of their Thermal Design Power can be applied to most VASP workloads with less than a 10% performance loss.</p>
<figure class="align-center" id="id2">
<img alt="../_images/vasp_parallel_efficiency.png" src="../Images/0b6eaaa4ff02027d294385d1f2cbaf8a.png" data-original-src="https://enccs.github.io/gpu-programming/_images/vasp_parallel_efficiency.png"/>
<figcaption>
<p><span class="caption-text">Parallel efficiency of VASP on seven test cases representing diverse VASP production workloads and ensuring a comprehensive coverage of various code paths, elements, and problem sizes.</span></p>
</figcaption>
</figure>
<figure class="align-center" id="id3">
<img alt="../_images/vasp_energy_consumption.jpg" src="../Images/6fa5e8533100766ad3399b8e454b4d19.png" data-original-src="https://enccs.github.io/gpu-programming/_images/vasp_energy_consumption.jpg"/>
<figcaption>
<p><span class="caption-text">(Left) Power usage of seven representative VASP workloads. The horizontal axis shows number of nodes used, and vertical axis shows high power mode per node. (Right) Power consumed per GPU when running VASP under four different power caps: 400 W (default), 300 W, 200 W, and 100 W. Horizontal axis shows power caps applied to GPUs, and vertical axis shows high power mode per GPU as a fraction of applied cap. The dashed horizontal line represents applied power cap. Each benchmark was run with a node count optimizing runtime while remaining above 70% parallel efficiency.</span></p>
</figcaption>
</figure>
&#13;

<h3>Computational Chemistry</h3>
<p>A great deal of computational resources are spent in Quantum Chemical calculations which involve
the solution of the Hartree-Fock eigenvalue problem, which requires the diagonalization of the
Fock matrix whose elements are given by:</p>
<div class="math notranslate nohighlight">
\[F_{\alpha \beta} = H^{\textrm{core}}_{\alpha \beta} + \sum_{\gamma \delta}D_{\gamma \delta} \left [ (\alpha \beta|\gamma \delta) - \frac{1}{2} (\alpha \delta|\gamma \beta) \right ],\]</div>
<p>The first term is related to the one electron contributions and the second term is related to the
electron repulsion integrals (ERIs), in parenthesis, weighted by the by the density matrix
<span class="math notranslate nohighlight">\(D_{\gamma \delta}\)</span>. One of the most expensive parts in the solution of the Hartree-Fock equations is the
processing (digestion) of the ERIs, one algorithm to do this task is as follows:</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/algorithms.svg"><img alt="../_images/algorithms.svg" src="../Images/8315333e7eb919a0358c1a5aad382eaf.png" style="width: 200px;" data-original-src="https://enccs.github.io/gpu-programming/_images/algorithms.svg"/>
</a>
<figcaption>
<p><span class="caption-text">Algorithm for processing ERIs [see <a class="reference external" href="https://doi.org/10.1021/acs.jctc.1c00720">JCTC, 17, 7486, (2021)</a> for details]</span></p>
</figcaption>
</figure>
<p>This algorithm is suitable for GPUs as it involves many arithmetic operations. In addition to this,
there are symmetries and properties of the integrals that could be used to rearrange the loops in
an efficient manner that fit GPU architectures.</p>
&#13;

<h3>Humanities</h3>
<p>A brief introduction into some of the work that is being done in the humanities that can benefit from utilizing GPUs.</p>
<p><strong>Language models and NLP (natural language processing)</strong></p>
<p>With the recent popularity of ChatGPT, the use of language models has come into the mainstream,
however such models have been used in the humanities many years already. One of the biggest goals of humanities
researchers is working with textual data which has increased exponentially over recent years due to the rise in
social media. Analyzing such textual data to gain insights into questions of sociology, linguistics and various
other fields have become increasingly reliant on using language models. Along with language models,
the need for GPU access has become essential.</p>
<p><strong>Archeology</strong></p>
<p>The field of archeology also makes use of GPUs in their 3D modelling
and rendering work. The biggest problem with archeological sites is that once they are excavated,
they are destroyed, so any researchers who aren’t present at the site, would lose valuable insights into how
it looked when it was found. However, with recent developments in technology and accessibility to high-performance
computing, they are able to generate extremely detailed renderings of the excavation sites which act as a way to
preserve the site for future researchers to gain critical insights and contribute to the research.</p>
<p><strong>Cognitive Science</strong></p>
<p>Techniques such as Markov Chain Monte Carlo (MCMC) sampling have proven to be invaluable in studies that delve into human behavior or population dynamics. MCMC sampling allows researchers to simulate and analyze complex systems by iteratively sampling from a Markov chain, enabling the exploration of high-dimensional parameter spaces. This method is particularly useful when studying human behavior, as it can capture the inherent randomness and interdependencies that characterize social systems. By leveraging MCMC sampling, researchers can gain insights into various aspects of human behavior, such as decision-making, social interactions, and the spread of information or diseases within populations.</p>
<p>By offloading the computational workload to GPUs, researchers can experience substantial speedup in the execution of MCMC algorithms. This speedup allows for more extensive exploration of parameter spaces and facilitates the analysis of larger datasets, leading to more accurate and detailed insights into human behavior or population dynamics. Examples of studies done using these methods can be found at the <a class="reference external" href="https://chc.au.dk/">Center for Humanities Computing Aarhus</a> (CHCAA) and <a class="reference external" href="https://interactingminds.au.dk/">Interacting Minds Centre</a> (IMC) at Aarhus University.</p>
&#13;

<h2>Exercises</h2>
<div class="admonition-discussion exercise important admonition" id="exercise-0">
<p class="admonition-title">Discussion</p>
<ul class="simple">
<li><p>What type of problems have you used GPUs for?</p></li>
<li><p>How large was the performance boost?</p></li>
</ul>
</div>
<div class="admonition-good-and-bad-use-cases-for-gpu-porting exercise important admonition" id="exercise-1">
<p class="admonition-title">Good and bad use cases for GPU porting</p>
<p>Which of the following computational tasks is likely to gain the least performance benefit from being ported to a GPU?</p>
<ol class="arabic simple">
<li><p>Training a large, deep neural network.</p></li>
<li><p>Performing a Monte Carlo simulation with a large number of independent trials.</p></li>
<li><p>Executing an algorithm with heavy use of recursion and frequent branching.</p></li>
<li><p>Processing a large image with a convolutional filter.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>The right answer is option 3. GPUs do not handle recursion and branching as effectively as more
data-heavy algorithms.</p>
</div>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>GPUs excel in processing tasks with high data parallelism, such as large-scale matrix operations, Fourier transforms, and big data analytics.</p></li>
<li><p>GPUs struggle with sequential tasks, problems with extensive control flow divergence, low arithmetic intensity tasks, small data sets, and memory-bound problems.</p></li>
</ul>
</div>
    
</body>
</html>