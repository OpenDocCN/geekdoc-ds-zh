<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>6.7. Online supplementary materials#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>6.7. Online supplementary materials#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap06_prob/supp/roch-mmids-prob-supp.html">https://mmids-textbook.github.io/chap06_prob/supp/roch-mmids-prob-supp.html</a></blockquote>

<section id="quizzes-solutions-code-etc">
<h2><span class="section-number">6.7.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">6.7.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_prob_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">6.7.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_2.html">Section 6.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_3.html">Section 6.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_4.html">Section 6.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_5.html">Section 6.5</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">6.7.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">6.7.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>E6.2.1</strong> The probability of <span class="math notranslate nohighlight">\(Y\)</span> being in the second category is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = e_2) = \pi_2 = 0.5.
\]</div>
<p><strong>E6.2.3</strong> Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> be i.i.d. Bernoulli<span class="math notranslate nohighlight">\((q^*)\)</span>. The MLE is <span class="math notranslate nohighlight">\(\hat{q}_{\mathrm{MLE}} = \frac{1}{n} \sum_{i=1}^n X_i\)</span>. By the Law of Large Numbers, as <span class="math notranslate nohighlight">\(n \to \infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\hat{q}_{\mathrm{MLE}} \to \mathbb{E}[X_1] = q^*
\]</div>
<p>almost surely.</p>
<p><strong>E6.2.5</strong> The gradient at <span class="math notranslate nohighlight">\(w = 0\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\nabla_w L_4(0; \{(x_i, y_i)\}_{i=1}^4) = -\sum_{i=1}^4 x_i(y_i - \sigma(0)) = -\frac{1}{2}(1 - 2 + 3 - 4) = 1.
\]</div>
<p>The updated parameter after one step of gradient descent is:</p>
<div class="math notranslate nohighlight">
\[
w' = w - \eta \nabla_w L_4(w; \{(x_i, y_i)\}_{i=1}^4) = 0 - 0.1 \cdot 1 = -0.1.
\]</div>
<p><strong>E6.2.7</strong> We must have <span class="math notranslate nohighlight">\(\sum_{x=-1}^1 \mathbb{P}(X=x) = 1\)</span>. This implies</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{Z(\theta)} (h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}) = 1.
\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[
Z(\theta) = h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}.
\]</div>
<p><strong>E6.2.9</strong> The empirical frequency for each category is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_i = \frac{N_i}{n},
\]</div>
<p>where <span class="math notranslate nohighlight">\(N_i\)</span> is the number of times category <span class="math notranslate nohighlight">\(i\)</span> appears in the sample. The counts are:</p>
<div class="math notranslate nohighlight">
\[
N_1 = 1, \quad N_2 = 2, \quad N_3 = 1.
\]</div>
<p>Thus, the empirical frequencies are:</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_1 = \frac{1}{4} = 0.25, \quad \hat{\pi}_2 = \frac{2}{4} = 0.5, \quad \hat{\pi}_3 = \frac{1}{4} = 0.25.
\]</div>
<p><strong>E6.2.11</strong> The log-likelihood for a multivariate Gaussian distribution is given by:</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L}(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{X}) = -\frac{1}{2} \left[ (\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X} - \boldsymbol{\mu}) + \log |\boldsymbol{\Sigma}| + 2 \log(2\pi) \right].
\]</div>
<p>First, compute the inverse and determinant of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Sigma}^{-1} = \frac{1}{3} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix}, \quad |\boldsymbol{\Sigma}| = 3.
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} - \boldsymbol{\mu} = \begin{pmatrix} 1 \\ 3 \end{pmatrix} - \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}.
\end{split}\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
(\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X} - \boldsymbol{\mu}) = \begin{pmatrix} 0 &amp; 1 \end{pmatrix} \frac{1}{3} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \frac{1}{3} \cdot 2 = \frac{2}{3}.
\end{split}\]</div>
<p>So the log-likelihood is:
$<span class="math notranslate nohighlight">\(
\log \mathcal{L} = -\frac{1}{2} \left[ \frac{2}{3} + \log 3 + 2 \log (2\pi) \right] \approx -3.178.
\)</span>$</p>
<p><strong>E6.3.1</strong> <span class="math notranslate nohighlight">\(\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} = \frac{0.2}{0.5} = 0.4.\)</span> This follows from the definition of conditional probability.</p>
<p><strong>E6.3.3</strong> <span class="math notranslate nohighlight">\(\mathbb{E}[X|Y=y] = 1 \cdot 0.3 + 2 \cdot 0.7 = 0.3 + 1.4 = 1.7.\)</span> This is the definition of the conditional expectation for discrete random variables.</p>
<p><strong>E6.3.5</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}[A \mid B \cap C] &amp;= \frac{\mathbb{P}[A \cap (B \cap C)]}{\mathbb{P}[B \cap C]} \\
&amp;= \frac{\mathbb{P}[A \cap B \cap C]}{\mathbb{P}[B \cap C]} \\
&amp;= \frac{0.05}{0.1} \\
&amp;= 0.5.
\end{align*}\]</div>
<p><strong>E6.3.7</strong> If <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(C\)</span> are pairwise independent, then they are also mutually independent. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}[A \cap B \cap C] &amp;= \mathbb{P}[A] \mathbb{P}[B] \mathbb{P}[C] \\
&amp;= 0.8 \cdot 0.6 \cdot 0.5 \\
&amp;= 0.24.
\end{align*}\]</div>
<p><strong>E6.3.9</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(\mathbb{P}[X=x, Z=z])_{x,z} &amp;= \sum_{y} (\mathbb{P}[X=x, Y = y, Z=z])_{x,z} \\
&amp;= \sum_{y} [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = y \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = y])_z \\
&amp;= [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = 0 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 0])_z\\ 
&amp; \quad + [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = 1 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 1])_z \\
&amp;= \begin{pmatrix} 0.3 \cdot 0.2 \cdot 0.5 &amp; 0.3 \cdot 0.2 \cdot 0.5 \\ 0.7 \cdot 0.6 \cdot 0.5 &amp; 0.7 \cdot 0.6 \cdot 0.5 \end{pmatrix} + \begin{pmatrix} 0.3 \cdot 0.8 \cdot 0.1 &amp; 0.3 \cdot 0.8 \cdot 0.9 \\ 0.7 \cdot 0.4 \cdot 0.1 &amp; 0.7 \cdot 0.4 \cdot 0.9 \end{pmatrix} \\
&amp;= \begin{pmatrix} 0.03 &amp; 0.03 \\ 0.21 &amp; 0.21 \end{pmatrix} + \begin{pmatrix} 0.024 &amp; 0.216 \\ 0.028 &amp; 0.252 \end{pmatrix} \\
&amp;= \begin{pmatrix} 0.054 &amp; 0.246 \\ 0.238 &amp; 0.462 \end{pmatrix}.
\end{align*}\]</div>
<p><strong>E6.3.11</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{\pi}_1 &amp;= \frac{N_1}{N_1 + N_2} = \frac{50}{50 + 100} = \frac{1}{3}, \\
\hat{p}_{1,1} &amp;= \frac{N_{1,1}}{N_1} = \frac{10}{50} = 0.2, \\
\hat{p}_{2,1} &amp;= \frac{N_{2,1}}{N_2} = \frac{40}{100} = 0.4.
\end{align*}\]</div>
<p><strong>E6.3.13</strong></p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[X=x, Y=y, Z=z] = \mathbb{P}[X=x]\mathbb{P}[Y=y|X=x]\mathbb{P}[Z=z|X=x].
\]</div>
<p><strong>E6.4.1</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}(X = 1) &amp;= \sum_{i=1}^2 \pi_i p_i \\
&amp;= (0.6)(0.3) + (0.4)(0.8) \\
&amp;= 0.5
\end{align*}\]</div>
<p><strong>E6.4.3</strong> <span class="math notranslate nohighlight">\(\mathbb{P}[X = (1, 0)] = \pi_1 p_{1,1} (1 - p_{1,2}) + \pi_2 p_{2,1} (1 - p_{2,2}) = 0.4 \cdot 0.7 \cdot 0.7 + 0.6 \cdot 0.2 \cdot 0.2 = 0.196 + 0.024 = 0.22\)</span>.</p>
<p><strong>E6.4.5</strong> <span class="math notranslate nohighlight">\(r_{1,i} = \frac{\pi_1 p_{1,1} p_{1,2}}{\pi_1 p_{1,1} p_{1,2} + \pi_2 p_{2,1} p_{2,2}} = \frac{0.5 \cdot 0.8 \cdot 0.2}{0.5 \cdot 0.8 \cdot 0.2 + 0.5 \cdot 0.1 \cdot 0.9} = \frac{0.08}{0.08 + 0.045} \approx 0.64\)</span>, <span class="math notranslate nohighlight">\(r_{2,i} = 1 - r_{1,i} \approx 0.36\)</span>.</p>
<p><strong>E6.4.7</strong> <span class="math notranslate nohighlight">\(\pi_1 = \frac{\eta_1}{n} = \frac{r_{1,1} + r_{1,1}}{2} = \frac{0.8 + 0.8}{2} = 0.8\)</span>, <span class="math notranslate nohighlight">\(\pi_2 = 1 - \pi_1 = 0.2\)</span>.</p>
<p><strong>E6.4.9</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
r_{1,2} &amp;= \frac{\pi_1 \prod_{m=1}^3 p_{1,m}^{x_{2,m}} (1 - p_{1,m})^{1-x_{2,m}}}{\sum_{k=1}^2 \pi_k \prod_{m=1}^3 p_{k,m}^{x_{2,m}} (1 - p_{k,m})^{1-x_{2,m}}} \\
&amp;= \frac{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1}{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1 + (0.6)(0.8)^0(0.2)^1(0.5)^0(0.5)^1} \\
&amp;= \frac{0.032}{0.032 + 0.06} \\
&amp;= \frac{8}{23}
\end{align*}\]</div>
<p><strong>E6.4.11</strong></p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X] = \pi_1 \mu_1 + \pi_2 \mu_2 = 0.5 \times (-1) + 0.5 \times 3 = -0.5 + 1.5 = 1.
\]</div>
<p><strong>E6.4.13</strong></p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = \pi_1 (\sigma_1^2 + \mu_1^2) + \pi_2 (\sigma_2^2 + \mu_2^2) - \left(\pi_1 \mu_1 + \pi_2 \mu_2\right)^2,
\]</div>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = 0.4 (1 + 0^2) + 0.6 (2 + 4^2) - (0.4 \times 0 + 0.6 \times 4)^2 = 0.4 \times 1 + 0.6 \times 18 - 2.4^2,
\]</div>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = 0.4 + 10.8 - 5.76 = 5.44.
\]</div>
<p><strong>E6.5.1</strong></p>
<div class="math notranslate nohighlight">
\[ 
B / B_{11} = B_{22} - B_{12}^T B_{11}^{-1} B_{12} = 3 - 1 \cdot \frac{1}{2} \cdot 1 = \frac{5}{2}, 
\]</div>
<p>using the definition of the Schur complement.</p>
<p><strong>E6.5.3</strong> The Schur complement of <span class="math notranslate nohighlight">\(A_{11}\)</span> in <span class="math notranslate nohighlight">\(A\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A/A_{11} = A_{22} - A_{21}A_{11}^{-1}A_{12} = 7 - \begin{pmatrix}
0 &amp; 6 
\end{pmatrix} \begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4 
\end{pmatrix}^{-1} \begin{pmatrix}
0 \\
5 
\end{pmatrix}.
\end{split}\]</div>
<p>First, compute <span class="math notranslate nohighlight">\(A_{11}^{-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A_{11}^{-1} = \frac{1}{1 \cdot 4 - 2 \cdot 3} \begin{pmatrix}
4 &amp; -2 \\
-3 &amp; 1
\end{pmatrix} = \begin{pmatrix}
-2 &amp; 1 \\
1.5 &amp; -0.5
\end{pmatrix}.
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A/A_{11} = 7 - \begin{pmatrix}
0 &amp; 6 
\end{pmatrix} \begin{pmatrix}
-2 &amp; 1 \\
1.5 &amp; -0.5
\end{pmatrix} \begin{pmatrix}
0 \\
5 
\end{pmatrix} = 7 + (6 \cdot 0.5 \cdot 5) = 22.
\end{split}\]</div>
<p><strong>E6.5.5</strong> The conditional mean of <span class="math notranslate nohighlight">\(X_1\)</span> given <span class="math notranslate nohighlight">\(X_2 = 3\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\mu_{1|2}(3) = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (3 - \bmu_2) = 1 + 1 \cdot \frac{1}{3} (3 - 2) = \frac{4}{3},
\]</div>
<p>using the formula for the conditional mean of multivariate Gaussians.</p>
<p><strong>E6.5.7</strong> The conditional distribution of <span class="math notranslate nohighlight">\(X_1\)</span> given <span class="math notranslate nohighlight">\(X_2 = 1\)</span> is Gaussian with mean <span class="math notranslate nohighlight">\(\mu_{1|2} = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (1 - \mu_2) = \frac{1}{2}\)</span> and variance <span class="math notranslate nohighlight">\(\bSigma_{1|2} = \bSigma_{11} - \bSigma_{12} \bSigma_{22}^{-1} \bSigma_{21} = \frac{7}{2}\)</span>.</p>
<p><strong>E6.5.9</strong> The distribution of <span class="math notranslate nohighlight">\(\bY\)</span> is Gaussian with mean vector <span class="math notranslate nohighlight">\(A\bmu = \begin{pmatrix} -3 \\ -3 \end{pmatrix}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(A\bSigma A^T = \begin{pmatrix} 8 &amp; 1 \\ 1 &amp; 6 \end{pmatrix}\)</span>.</p>
<p><strong>E6.5.11</strong> The mean of <span class="math notranslate nohighlight">\(Y_t\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{E}[Y_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \mathbb{E}[\bX_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = 1,
\end{split}\]</div>
<p>and the variance of <span class="math notranslate nohighlight">\(Y_t\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{Var}[Y_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \mathrm{Cov}[\bX_t] \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = 2,
\end{split}\]</div>
<p>using the properties of linear-Gaussian systems.</p>
<p><strong>E6.5.13</strong> The innovation is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
e_t = Y_t - H \bmu_{\text{pred}} = 3 - \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 3 \\ 1 \end{pmatrix} = 3 - 3 = 0.
\end{split}\]</div>
<p><strong>E6.5.15</strong> The updated state estimate is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bmu_t = \bmu_{\text{pred}} + K_t e_t = \begin{pmatrix} 3 \\ 1 \end{pmatrix} + \begin{pmatrix} \frac{2}{3} \\ \frac{1}{3} \end{pmatrix} \cdot 0 = \begin{pmatrix} 3 \\ 1 \end{pmatrix}.
\end{split}\]</div>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">6.7.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define exponential families and give examples of common probability distributions that belong to this family.</p></li>
<li><p>Derive the maximum likelihood estimator for exponential families and explain its properties.</p></li>
<li><p>Prove that, under certain conditions, the maximum likelihood estimator is statistically consistent.</p></li>
<li><p>Formulate generalized linear models using exponential families and express linear and logistic regression as special cases.</p></li>
<li><p>Compute the gradient and Hessian of the negative log-likelihood for generalized linear models.</p></li>
<li><p>Interpret the moment-matching equations for the maximum likelihood estimator in generalized linear models.</p></li>
<li><p>Apply the multiplication rule, the law of total probability, and Bayes’ rule to solve problems involving conditional probabilities.</p></li>
<li><p>Calculate conditional probability mass functions and conditional expectations for discrete random variables.</p></li>
<li><p>Define conditional independence and express it mathematically in terms of conditional probabilities.</p></li>
<li><p>Differentiate between the fork, chain, and collider configurations in graphical models representing conditional independence relations.</p></li>
<li><p>Derive the joint probability distribution for the Naive Bayes model under the assumption of conditional independence.</p></li>
<li><p>Implement maximum likelihood estimation to fit the parameters of the Naive Bayes model.</p></li>
<li><p>Apply the Naive Bayes model for prediction and evaluate its accuracy.</p></li>
<li><p>Implement Laplace smoothing to address the issue of unseen words in the training data when fitting a Naive Bayes model.</p></li>
<li><p>Apply the Naive Bayes model to perform sentiment analysis on a real-world dataset and interpret the results.</p></li>
<li><p>Define mixtures as convex combinations of distributions and express the probability distribution of a mixture model using the law of total probability.</p></li>
<li><p>Identify examples of mixture models, such as mixtures of multinomials and Gaussian mixture models, and recognize their probability density functions.</p></li>
<li><p>Explain the concept of marginalizing out an unobserved random variable in the context of mixture models.</p></li>
<li><p>Formulate the objective function for parameter estimation in mixtures of multivariate Bernoullis using the negative log-likelihood.</p></li>
<li><p>Describe the majorization-minimization principle and its application in the Expectation-Maximization (EM) algorithm.</p></li>
<li><p>Derive the E-step and M-step updates for the EM algorithm in the context of mixtures of multivariate Bernoullis.</p></li>
<li><p>Implement the EM algorithm for mixtures of multivariate Bernoullis and apply it to a real-world dataset, such as clustering handwritten digits from the MNIST dataset.</p></li>
<li><p>Identify and address numerical issues that may arise during the implementation of the EM algorithm, such as underflow, by applying techniques like the log-sum-exp trick.</p></li>
<li><p>Define block matrices and the Schur complement, and demonstrate their properties through examples and proofs.</p></li>
<li><p>Derive the marginal and conditional distributions of multivariate Gaussians using the properties of block matrices and the Schur complement.</p></li>
<li><p>Describe the linear-Gaussian system model and its components, including the state evolution and observation processes.</p></li>
<li><p>Explain the purpose and key steps of the Kalman filter algorithm, including the prediction and update steps.</p></li>
<li><p>Implement the Kalman filter algorithm in code, given the state evolution and observation models, and the initial state distribution.</p></li>
<li><p>Apply the Kalman filter to a location tracking problem, and interpret the results in terms of the estimated object path and the algorithm’s performance.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
</section>
<section id="additional-sections">
<h2><span class="section-number">6.7.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="sentiment-analysis">
<h3><span class="section-number">6.7.2.1. </span>Sentiment analysis<a class="headerlink" href="#sentiment-analysis" title="Link to this heading">#</a></h3>
<p>As an application of the Naive Bayes model, we consider the task of sentiment analysis, which is a classification problem. We use a dataset available <a class="reference external" href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment">here</a>. Quoting from there:</p>
<blockquote>
<div><p>A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as “late flight” or “rude service”).</p>
</div></blockquote>
<p>We first load a cleaned-up version of the data and look at its summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'twitter-sentiment.csv'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'latin-1'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>time</th>
      <th>user</th>
      <th>sentiment</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2/24/15 11:35</td>
      <td>cairdin</td>
      <td>neutral</td>
      <td>@VirginAmerica What @dhepburn said.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2/24/15 11:15</td>
      <td>jnardino</td>
      <td>positive</td>
      <td>@VirginAmerica plus you've added commercials t...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2/24/15 11:15</td>
      <td>yvonnalynn</td>
      <td>neutral</td>
      <td>@VirginAmerica I didn't today... Must mean I n...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2/24/15 11:15</td>
      <td>jnardino</td>
      <td>negative</td>
      <td>@VirginAmerica it's really aggressive to blast...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2/24/15 11:14</td>
      <td>jnardino</td>
      <td>negative</td>
      <td>@VirginAmerica and it's a really big bad thing...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>14640
</pre></div>
</div>
</div>
</div>
<p>We extract the text information in this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">corpus</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'text'</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0                      @VirginAmerica What @dhepburn said.
1        @VirginAmerica plus you've added commercials t...
2        @VirginAmerica I didn't today... Must mean I n...
3        @VirginAmerica it's really aggressive to blast...
4        @VirginAmerica and it's a really big bad thing...
                               ...                        
14635    @AmericanAir thank you we got on a different f...
14636    @AmericanAir leaving over 20 minutes Late Flig...
14637    @AmericanAir Please bring American Airlines to...
14638    @AmericanAir you have my money, you change my ...
14639    @AmericanAir we have 8 ppl so we need 2 know h...
Name: text, Length: 14640, dtype: object
</pre></div>
</div>
</div>
</div>
<p>Next, we convert our dataset into a matrix by creating a document-term matrix using <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform"><code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text.CountVectorizer</span></code></a>. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Document-term_matrix">Wikipedia</a>:</p>
<blockquote>
<div><p>A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.</p>
</div></blockquote>
<p>By default, it first preprocesses the data. In particular, it lower-cases all words and removes punctuation. A more careful pre-procsseing would also include stemming, although we do not do this here. Regarding the latter, quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Stemming">Wikipedia</a>:</p>
<blockquote>
<div><p>In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. […] A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer. […] A stemmer for English operating on the stem cat should identify such strings as cats, catlike, and catty. A stemming algorithm might also reduce the words fishing, fished, and fisher to the stem fish. The stem need not be a word, for example the Porter algorithm reduces, argue, argued, argues, arguing, and argus to the stem argu.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">count</span><span class="p">[:</span><span class="mi">2</span><span class="p">,])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>  (0, 14376)	1
  (0, 14654)	1
  (0, 4872)	1
  (0, 11739)	1
  (1, 14376)	1
  (1, 10529)	1
  (1, 15047)	1
  (1, 14296)	1
  (1, 2025)	1
  (1, 4095)	1
  (1, 13425)	1
  (1, 13216)	1
  (1, 5733)	1
  (1, 13021)	1
</pre></div>
</div>
</div>
</div>
<p>The list of all terms used can be accessed as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">terms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['00' '000' '000114' ... 'ü_ù__' 'üi' 'ýã']
</pre></div>
</div>
</div>
</div>
<p>Because of our use of the multivariate Bernoulli naive Bayes model, it will be more convenient to work with a variant of the document-term matrix where each word is either present or absent. Note that, in the context of tweet data which are very short documents with likely little word repetition, there is probably not much difference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>  (0, 4872)	1
  (0, 11739)	1
  (0, 14376)	1
  (0, 14654)	1
  (1, 2025)	1
  (1, 4095)	1
  (1, 5733)	1
  (1, 10529)	1
  (1, 13021)	1
  (1, 13216)	1
  (1, 13425)	1
  (1, 14296)	1
  (1, 14376)	1
  (1, 15047)	1
</pre></div>
</div>
</div>
</div>
<p>We also extract the labels (<code class="docutils literal notranslate"><span class="pre">neutral</span></code>, <code class="docutils literal notranslate"><span class="pre">postive</span></code>, <code class="docutils literal notranslate"><span class="pre">negative</span></code>) from the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['neutral' 'positive' 'neutral' ... 'neutral' 'negative' 'neutral']
</pre></div>
</div>
</div>
</div>
<p>We split the data into a training set and a test set using <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"><code class="docutils literal notranslate"><span class="pre">sklearn.model_selection.train_test_split</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">535</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use the Naive Bayes method. We first construct the matrix <span class="math notranslate nohighlight">\(N_{k,m}\)</span> and the vector <span class="math notranslate nohighlight">\(N_k\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">label_set</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'positive'</span><span class="p">,</span> <span class="s1">'negative'</span><span class="p">,</span> <span class="s1">'neutral'</span><span class="p">]</span>
<span class="n">N_km</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">label_set</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">terms</span><span class="p">)))</span>
<span class="n">N_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label_set</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">label_set</span><span class="p">):</span>
    <span class="n">k_rows</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">N_km</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">k_rows</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">N_k</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">k_rows</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Above, the <a class="reference external" href="https://docs.python.org/3/library/functions.html#enumerate"><code class="docutils literal notranslate"><span class="pre">enumerate</span></code></a> function provides both the index and the value for each item in the <code class="docutils literal notranslate"><span class="pre">label_set</span></code> list during the loop. This allows the code to use the label’s value (<code class="docutils literal notranslate"><span class="pre">k</span></code>) and its numerical position (<code class="docutils literal notranslate"><span class="pre">i</span></code>) at the same time.</p>
<p>We are ready to train on the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">nb_fit_table</span><span class="p">(</span><span class="n">N_km</span><span class="p">,</span> <span class="n">N_k</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.16071022 0.62849989 0.21078989]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">p_km</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0.00047192 0.00330345 0.00047192 ... 0.00094384 0.00094384 0.00047192]
 [0.00144858 0.00229358 0.00012071 ... 0.00012071 0.00012071 0.00024143]
 [0.00071968 0.00143937 0.00071968 ... 0.00035984 0.00035984 0.00071968]]
</pre></div>
</div>
</div>
</div>
<p>Next, we plot the vector <span class="math notranslate nohighlight">\(p_{k,m}\)</span> for each label <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span><span class="n">ax2</span><span class="p">,</span><span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e75f50e0059a06f686e844c95d499bf2dbc3ef78367a3fc95c0ed8f7df2202b1.png" src="../Images/dc490453944015ef54f86c78956d449e.png" data-original-src="https://mmids-textbook.github.io/_images/e75f50e0059a06f686e844c95d499bf2dbc3ef78367a3fc95c0ed8f7df2202b1.png"/>
</div>
</div>
<p>We can compute a prediction on the test tweets. For example, for the 5th test tweet:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="mi">4</span><span class="p">,:]</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">label_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>'positive'
</pre></div>
</div>
</div>
</div>
<p>The following computes the overall accuracy over the test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">mmids</span><span class="o">.</span><span class="n">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">label_set</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="n">acc</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">acc</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0.7670765027322405
</pre></div>
</div>
</div>
</div>
<p>To get a better understanding of the differences uncovered by Naive Bayes between the different labels, we identify words that are particularly common in one label, but not on the other. Recall that label <code class="docutils literal notranslate"><span class="pre">1</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">positive</span></code> while label <code class="docutils literal notranslate"><span class="pre">2</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">negative</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pos_terms</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">&gt;</span> <span class="mf">0.02</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">&lt;</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">[</span><span class="n">pos_terms</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['_ù' 'amazing' 'appreciate' 'awesome' 'best' 'crew' 'first' 'flying'
 'good' 'great' 'll' 'love' 'made' 'much' 'new' 'response' 'see' 'thank'
 'thx' 'very' 'well' 'work']
</pre></div>
</div>
</div>
</div>
<p>One notices that many positive words do appear in this list: <code class="docutils literal notranslate"><span class="pre">awesome</span></code>, <code class="docutils literal notranslate"><span class="pre">best</span></code>, <code class="docutils literal notranslate"><span class="pre">great</span></code>, <code class="docutils literal notranslate"><span class="pre">love</span></code>, <code class="docutils literal notranslate"><span class="pre">thank</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">neg_terms</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">&gt;</span> <span class="mf">0.02</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">&lt;</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">[</span><span class="n">neg_terms</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['about' 'after' 'again' 'agent' 'airport' 'am' 'another' 'any' 'bag'
 'bags' 'because' 'by' 'call' 'cancelled' 'change' 'check' 'days' 'delay'
 'delayed' 'did' 'don' 'due' 'even' 'ever' 'flighted' 'flightled' 'go'
 'going' 'has' 'here' 'hold' 'hour' 'hours' 'how' 'hrs' 'if' 'last' 'late'
 'lost' 'luggage' 'make' 'minutes' 'more' 'need' 'never' 'off' 'only' 'or'
 'over' 'people' 'phone' 'really' 'should' 'sitting' 'someone' 'still'
 'take' 'than' 'them' 'then' 'told' 'trying' 've' 'wait' 'waiting' 'want'
 'weather' 'what' 'when' 'why' 'worst']
</pre></div>
</div>
</div>
</div>
<p>This time, we notice: <code class="docutils literal notranslate"><span class="pre">bag</span></code>, <code class="docutils literal notranslate"><span class="pre">cancelled</span></code>, <code class="docutils literal notranslate"><span class="pre">delayed</span></code>, <code class="docutils literal notranslate"><span class="pre">hours</span></code>, <code class="docutils literal notranslate"><span class="pre">phone</span></code>.</p>
<p><strong>CHAT &amp; LEARN</strong> The bag-of-words representation used in the sentiment analysis example is a simple but limited way to represent text data. More advanced representations such as word embeddings and transformer models can capture more semantic information. Ask your favorite AI chatbot to explain these representations and how they can be used for text classification tasks. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
</section>
<section id="kalman-filtering-missing-data">
<h3><span class="section-number">6.7.2.2. </span>Kalman filtering: missing data<a class="headerlink" href="#kalman-filtering-missing-data" title="Link to this heading">#</a></h3>
<p>In Kalman filtering, we can also allow for the possibility that some observations are missing. Imagine for instance losing GPS signal while going through a tunnel. The recursions above are still valid, with the only modification that the <em>Update</em> equations involving <span class="math notranslate nohighlight">\(\bY_t\)</span> are dropped at those times <span class="math notranslate nohighlight">\(t\)</span> where there is no observation. In Numpy, we can use <a class="reference external" href="https://numpy.org/doc/stable/reference/constants.html#numpy.nan"><code class="docutils literal notranslate"><span class="pre">NaN</span></code></a> to indicate the lack of observation. (Alternatively, one can use the <a class="reference external" href="https://numpy.org/doc/stable/reference/maskedarray.generic.html">numpy.ma</a> module.)</p>
<p>We use a same sample path as above, but mask observations at times <span class="math notranslate nohighlight">\(t=10,\ldots,20\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">ss</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">os</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span> 
<span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="n">Q</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span>
<span class="n">R</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">os</span><span class="p">))</span>
<span class="n">init_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">init_Sig</span> <span class="o">=</span> <span class="n">Q</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">lgSamplePath</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">ss</span><span class="p">,</span> <span class="n">os</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">init_mu</span><span class="p">,</span> <span class="n">init_Sig</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
</pre></div>
</div>
</div>
</div>
<p>Here is the sample we are aiming to infer.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'dotted'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png" src="../Images/23e83372ecc5a5cde94ad7f021c21a15.png" data-original-src="https://mmids-textbook.github.io/_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png"/>
</div>
</div>
<p>We modify the recursion accordingly, that is, skip the <em>Update</em> step when there is no observation to use for the update.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">kalmanUpdate</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">mu_prev</span><span class="p">,</span> <span class="n">Sig_prev</span><span class="p">):</span>
    <span class="n">mu_pred</span> <span class="o">=</span> <span class="n">F</span> <span class="o">@</span> <span class="n">mu_prev</span>
    <span class="n">Sig_pred</span> <span class="o">=</span> <span class="n">F</span> <span class="o">@</span> <span class="n">Sig_prev</span> <span class="o">@</span> <span class="n">F</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">Q</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y_t</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y_t</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">mu_pred</span><span class="p">,</span> <span class="n">Sig_pred</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">e_t</span> <span class="o">=</span> <span class="n">y_t</span> <span class="o">-</span> <span class="n">H</span> <span class="o">@</span> <span class="n">mu_pred</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">H</span> <span class="o">@</span> <span class="n">Sig_pred</span> <span class="o">@</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">R</span>
        <span class="n">Sinv</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">Sig_pred</span> <span class="o">@</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Sinv</span>
        <span class="n">mu_new</span> <span class="o">=</span> <span class="n">mu_pred</span> <span class="o">+</span> <span class="n">K</span> <span class="o">@</span> <span class="n">e_t</span>
        <span class="n">Sig_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span> <span class="o">-</span> <span class="n">K</span> <span class="o">@</span> <span class="n">H</span><span class="p">)</span> <span class="o">@</span> <span class="n">Sig_pred</span>
        <span class="k">return</span> <span class="n">mu_new</span><span class="p">,</span> <span class="n">Sig_new</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">init_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">init_Sig</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kalmanFilter</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span> <span class="n">os</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">init_mu</span><span class="p">,</span> <span class="n">init_Sig</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'s'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'dotted'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png" src="../Images/6633cdad5605633dacdc68a6a28fde12.png" data-original-src="https://mmids-textbook.github.io/_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png"/>
</div>
</div>
</section>
<section id="cholesky-decomposition">
<h3><span class="section-number">6.7.2.3. </span>Cholesky decomposition<a class="headerlink" href="#cholesky-decomposition" title="Link to this heading">#</a></h3>
<p>In  this section, we derive an important matrix factorization and apply it to generating multivariate Gaussians. We also revisit the least-squares problem. We begin with the motivation.</p>
<p><strong>Generating multivariate Gaussians</strong> Suppose we want to generate samples from a multivariate Gaussian <span class="math notranslate nohighlight">\(\bX \sim N_d(\bmu, \bSigma)\)</span> with given mean vector <span class="math notranslate nohighlight">\(\bmu \in \mathbb{R}^d\)</span> and positive definite covariance matrix <span class="math notranslate nohighlight">\(\bSigma  \in \mathbb{R}^{d \times d}\)</span>. Of course, in Numpy, we could use <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.multivariate_normal</span></code></a>. But what is behind it? More precisely, suppose we have access to unlimited samples <span class="math notranslate nohighlight">\(U_1, U_2, U_3, etc.\)</span> from uniform random variables in <span class="math notranslate nohighlight">\([0,1]\)</span>. How do we transform them to obtain samples from <span class="math notranslate nohighlight">\(N_d(\bmu, \bSigma)\)</span>.</p>
<p>We start with the simplest case: <span class="math notranslate nohighlight">\(d=1\)</span>, <span class="math notranslate nohighlight">\(\mu = 0\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span>. That is, we first generate a univariate standard Normal. We have seen a recipe for doing this before, the inverse transform sampling method. Specifically, recall that the cumulative distribution function (CDF) of a random variable <span class="math notranslate nohighlight">\(Z\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
F_Z(z) = \mathbb{P}[Z \leq z], \qquad \forall z \in \mathbb{R}.
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> be the interval where <span class="math notranslate nohighlight">\(F_Z(z) \in (0,1)\)</span> and assume that <span class="math notranslate nohighlight">\(F_X\)</span> is strictly increasing on <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span>. Let <span class="math notranslate nohighlight">\(U \sim \mathrm{U}[0,1]\)</span>. Then it can be shown that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[F_X^{-1}(U) \leq z] = F_X(z).
\]</div>
<p>So take <span class="math notranslate nohighlight">\(F_Z = \Phi\)</span>, the CDF of the standard Normal. Then <span class="math notranslate nohighlight">\(Z = \Phi^{-1}(U)\)</span> is <span class="math notranslate nohighlight">\(N(0,1)\)</span>.</p>
<p>How do we generate a <span class="math notranslate nohighlight">\(N(\mu, \sigma^2)\)</span> variable, for arbitrary <span class="math notranslate nohighlight">\(\mu \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>? We use the fact that the linear transformation of Gaussian is still Gaussian. In particular, if <span class="math notranslate nohighlight">\(Z \sim N(0,1)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
X = \mu + \sigma Z
\]</div>
<p>is <span class="math notranslate nohighlight">\(N(\mu, \sigma^2)\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> In Python, <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> can be accessed using <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html"><code class="docutils literal notranslate"><span class="pre">scipy.stats.norm.ppf</span></code></a>. We implement this next (with help from ChatGPT).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">):</span>
    <span class="c1"># Step 1: Generate uniform [0,1] random variables</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    
    <span class="c1"># Step 2: Apply the inverse CDF (ppf) of the standard normal distribution</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma2</span> <span class="o">*</span> <span class="n">Z</span>
</pre></div>
</div>
</div>
</div>
<p>We generate 1000 samples and plot the empirical distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Generate 1000 standard normal samples</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Plot the empirical PDF</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot histogram of the samples with density=True to normalize the histogram</span>
<span class="n">count</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">ignored</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>

<span class="c1"># Plot the theoretical standard normal PDF for comparison</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">'k'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Empirical PDF of Generated Samples'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Density'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png" src="../Images/c4d1f8c3493a1c88b3b5727f55a25bef.png" data-original-src="https://mmids-textbook.github.io/_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> It turns out there is a neat trick to generate <em>two</em> independent samples from <span class="math notranslate nohighlight">\(N(0,1)\)</span> that does not rely on access to <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span>. It is called the Box-Muller transform. Ask your favorite AI chatbot about it. Modify our code above to implement it. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p>We move on to the multivariate case. We proceed similarly as before. First, how do we generate a <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian with mean vector <span class="math notranslate nohighlight">\(\bmu = \mathbf{0}\)</span> and identity covariance matrix <span class="math notranslate nohighlight">\(\bSigma = I_{d \times d}\)</span>? Easy – it has <span class="math notranslate nohighlight">\(d\)</span> independent components, each of which is standard Normal. So letting <span class="math notranslate nohighlight">\(U_1, \ldots, U_d\)</span> be independent uniform <span class="math notranslate nohighlight">\([0,1]\)</span> variables, then</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}
= (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d))
\]</div>
<p>is <span class="math notranslate nohighlight">\(N(\mathbf{0}, I_{d \times d})\)</span>.</p>
<p>We now seek to generate a multivariate Gaussian with arbitrary mean vector <span class="math notranslate nohighlight">\(\bmu\)</span> and positive definite covariance matrix <span class="math notranslate nohighlight">\(\bSigma\)</span>. Again, we use a linear transformation</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}
= \mathbf{a} + A \mathbf{Z}.
\]</div>
<p>What are the right choices for <span class="math notranslate nohighlight">\(a \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span>? We need to match the obtained and desired mean and covariance. We start with the mean. By linearity of expectation,</p>
<div class="math notranslate nohighlight">
\[
\E[\mathbf{X}]
= \E[\mathbf{a} + A \mathbf{Z}]
= \mathbf{a} + A \,\E[\mathbf{Z}]
= \mathbf{a}.
\]</div>
<p>Hence we pick <span class="math notranslate nohighlight">\(\mathbf{a} := \bmu\)</span>.</p>
<p>As for the covariance, using the <em>Covariance of a Linear Transformation</em>, we get</p>
<div class="math notranslate nohighlight">
\[
\cov[\mathbf{X}]
= A \,\cov[\mathbf{Z}] A^T
= A A^T.
\]</div>
<p>Now we have a problem: what is a matrix <span class="math notranslate nohighlight">\(A\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
A A^T = \bSigma?
\]</div>
<p>In some sense, we are looking for a sort of “square root” of the covariance matrix. There are several ways of doing this. The Cholesky decomposition is one of them. We return to generating samples from <span class="math notranslate nohighlight">\(N(\bmu, \bSigma)\)</span> after introducing it.</p>
<p><strong>A matrix factorization</strong> Our key linear-algebraic result of this section is the following. The matrix factorization in the next theorem is called a Cholesky decomposition. It has many <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications">applications</a>.</p>
<p><strong>THEOREM</strong> <strong>(Cholesky Decomposition)</strong> Any positive definite matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times n}\)</span> can be factorized uniquely as</p>
<div class="math notranslate nohighlight">
\[
B = L L^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(L \in \mathbb{R}^{n \times n}\)</span> is a lower triangular matrix with positive entries on the diagonal. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>The proof is provided below. It is based on deriving an algorithm for computing the Cholesky decomposition: we grow <span class="math notranslate nohighlight">\(L\)</span> starting from its top-left corner by successively computing its next row based on the previously constructed submatrix. Note that, because <span class="math notranslate nohighlight">\(L\)</span> is lower triangular, it suffices to compute its elements on and below the diagonal. We first give the algorithm, then establish that it is well-defined.</p>
<p><strong>Figure:</strong> Access pattern (<a class="reference external" href="https://en.wikipedia.org/wiki/File:Chol.gif">Source</a>)</p>
<p><img alt="Access pattern" src="../Images/20de5b823040500b0d04511959484e97.png" data-original-src="https://upload.wikimedia.org/wikipedia/commons/b/be/Chol.gif"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>EXAMPLE:</strong> Before proceeeding with the general method, we give a small example to provide some intuition as to how it operates. We need a positive definite matrix. Consider the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A 
= 
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
0 &amp; -2 &amp; 1\\
0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>It has full column rank (why?). Recall that, in that case, the <span class="math notranslate nohighlight">\(B = A^T A\)</span> is positive definite.</p>
<p>That is, the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B 
=
A^T A 
= 
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
2 &amp; 8 &amp; 0\\
1 &amp; 0 &amp; 3
\end{pmatrix}
\end{split}\]</div>
<p>is positive definite.</p>
<p>Let <span class="math notranslate nohighlight">\(L = (\ell_{i,j})_{i,j=1}^3\)</span> be lower triangular. We seek to solve <span class="math notranslate nohighlight">\(L L^T = B\)</span> for the nonzero entries of <span class="math notranslate nohighlight">\(L\)</span>. Observe that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\ell_{1,1} &amp; 0 &amp; 0\\
\ell_{2,1} &amp; \ell_{2,2} &amp; 0\\
\ell_{3,1} &amp; \ell_{3,2} &amp; \ell_{3,3}
\end{pmatrix}
\begin{pmatrix}
\ell_{1,1} &amp; \ell_{2,1} &amp; \ell_{3,1}\\
0 &amp; \ell_{2,2} &amp; \ell_{3,2}\\
0 &amp; 0 &amp; \ell_{3,3}
\end{pmatrix}
=
\begin{pmatrix}
\ell_{1,1}^2 &amp; \ell_{1,1}\ell_{2,1} &amp; \ell_{1,1}\ell_{3,1}\\
\ell_{1,1}\ell_{2,1} &amp; \ell_{2,1}^2 + \ell_{2,2}^2 &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
\ell_{1,1}\ell_{3,1} &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} &amp; \ell_{3,1}^2 + \ell_{3,2}^2 + \ell_{3,3}
\end{pmatrix}.
\end{split}\]</div>
<p>The system</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\ell_{1,1}^2 &amp; \ell_{1,1}\ell_{2,1} &amp; \ell_{1,1}\ell_{3,1}\\
\ell_{1,1}\ell_{2,1} &amp; \ell_{2,1}^2 + \ell_{2,2}^2 &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
\ell_{1,1}\ell_{3,1} &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} &amp; \ell_{3,1}^2 + \ell_{3,2}^2 + \ell_{3,3}^2
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
2 &amp; 8 &amp; 0\\
1 &amp; 0 &amp; 3
\end{pmatrix}
\end{split}\]</div>
<p>turns out to be fairly simple to solve.</p>
<ol class="arabic simple">
<li><p>From the first entry, we get <span class="math notranslate nohighlight">\(\ell_{1,1} = 1\)</span> (where we took the positive solution to <span class="math notranslate nohighlight">\(\ell_{1,1}^2 = 1\)</span>).</p></li>
<li><p>Given that <span class="math notranslate nohighlight">\(\ell_{1,1}\)</span> is known, entry <span class="math notranslate nohighlight">\(\ell_{2,1}\)</span> is determined from <span class="math notranslate nohighlight">\(\ell_{1,1}\ell_{2,1} =2\)</span> in the first entry of the second row. That is, <span class="math notranslate nohighlight">\(\ell_{2,1} =2\)</span>. Then the second entry of the second row gives <span class="math notranslate nohighlight">\(\ell_{2,2}\)</span> through <span class="math notranslate nohighlight">\(\ell_{2,1}^2 + \ell_{2,2}^2  = 8\)</span>. So <span class="math notranslate nohighlight">\(\ell_{2,2} = 2\)</span> (again we take the positive solution).</p></li>
<li><p>We move to the third row. The first entry gives <span class="math notranslate nohighlight">\(\ell_{3,1} = 1\)</span>, the second entry gives <span class="math notranslate nohighlight">\(\ell_{3,2} = -1\)</span> and finally the third entry leads to <span class="math notranslate nohighlight">\(\ell_{3,3} = 1\)</span>.</p></li>
</ol>
<p>Hence we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L
=
\begin{pmatrix}
\ell_{1,1} &amp; 0 &amp; 0\\
\ell_{2,1} &amp; \ell_{2,2} &amp; 0\\
\ell_{3,1} &amp; \ell_{3,2} &amp; \ell_{3,3}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 0 &amp; 0\\
2 &amp; 2 &amp; 0\\
1 &amp; -1 &amp; 1
\end{pmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>To detail the computation of the Cholesky decomposition <span class="math notranslate nohighlight">\(L L^T\)</span> of <span class="math notranslate nohighlight">\(B\)</span>, we will need some notation. Write <span class="math notranslate nohighlight">\(B = (b_{i,j})_{i,j=1}^n\)</span> and <span class="math notranslate nohighlight">\(L = (\ell_{i,j})_{i,j=1}^n\)</span>. Let <span class="math notranslate nohighlight">\(L_{(k)} = (\ell_{i,j})_{i,j=1}^k\)</span> be the first <span class="math notranslate nohighlight">\(k\)</span> rows and columns of <span class="math notranslate nohighlight">\(L\)</span>, let <span class="math notranslate nohighlight">\(\bflambda_{(k)}^T = (\ell_{k,1},\ldots,\ell_{k,k-1})\)</span> be the row vector corresponding to the first <span class="math notranslate nohighlight">\(k-1\)</span> entries of row <span class="math notranslate nohighlight">\(k\)</span> of <span class="math notranslate nohighlight">\(L\)</span>, and let <span class="math notranslate nohighlight">\(\bfbeta_{(k)}^T = (b_{k,1},\ldots,b_{k,k-1})\)</span> be the row vector corresponding to the first <span class="math notranslate nohighlight">\(k-1\)</span> entries of row <span class="math notranslate nohighlight">\(k\)</span> of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>The strategy is to compute <span class="math notranslate nohighlight">\(L_{(1)}\)</span>, then <span class="math notranslate nohighlight">\(L_{(2)}\)</span>, then <span class="math notranslate nohighlight">\(L_{(3)}\)</span> and so on. With the notation above, <span class="math notranslate nohighlight">\(L_{(j)}\)</span> can be written in block form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L_{(j)}
=
\begin{pmatrix}
L_{(j-1)} &amp; \mathbf{0}\\
\bflambda_{(j)}^T &amp; \ell_{j,j}.
\end{pmatrix}
\end{split}\]</div>
<p>Hence, once <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> is known, in order to compute <span class="math notranslate nohighlight">\(L_{(j)}\)</span> one only needs <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span> and <span class="math notranslate nohighlight">\(\ell_{j,j}\)</span>. We show next that they satisfy easily solvable systems of equations.</p>
<p>We first note that the <span class="math notranslate nohighlight">\((1,1)\)</span> entry of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> implies that</p>
<div class="math notranslate nohighlight">
\[
\ell_{1,1}^2 = b_{1,1}.
\]</div>
<p>So we set</p>
<div class="math notranslate nohighlight">
\[
L_{(1)}
= \ell_{1,1}
= \sqrt{b_{1,1}}.
\]</div>
<p>For this step to be well-defined, it needs to be the case that <span class="math notranslate nohighlight">\(b_{1,1} &gt; 0\)</span>. It is easy to see that it follows from the positive definiteness of <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="math notranslate nohighlight">
\[
0 &lt; \langle \mathbf{e}_1, B \mathbf{e}_1\rangle = \mathbf{e}_1^T B_{\cdot,1} = b_{1,1}.
\]</div>
<p>Proceeding by induction, assume <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> has been constructed. The first <span class="math notranslate nohighlight">\(j-1\)</span> elements of the <span class="math notranslate nohighlight">\(j\)</span>-th row of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> translate into</p>
<div class="math notranslate nohighlight">
\[
L_{j,\cdot} (L^T)_{\cdot,1:j-1} = \bflambda_{(j)}^T L_{(j-1)}^T = \bfbeta_{(j)}^T,
\]</div>
<p>where <span class="math notranslate nohighlight">\((L^T)_{\cdot,1:j-1}\)</span> denotes the first <span class="math notranslate nohighlight">\(j-1\)</span> columns of <span class="math notranslate nohighlight">\(L^T\)</span>. In the first equality above, we used the fact that <span class="math notranslate nohighlight">\(L^T\)</span> is upper triangular. Taking a transpose, the resulting linear system of equations</p>
<div class="math notranslate nohighlight">
\[
L_{(j-1)} \bflambda_{(j)} = \bfbeta_{(j)}
\]</div>
<p>can be solved by forward substitution (since <span class="math notranslate nohighlight">\(\bfbeta_{(j)}\)</span> is part of the input and <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> was previously computed). The fact that this system has a unique solution (more specifically, that the diagonal entries of <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> are strictly positive) is established in the proof of the <em>Cholesky Decomposition Theorem</em>.</p>
<p>The <span class="math notranslate nohighlight">\((j,j)\)</span>-th entry of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> translates into</p>
<div class="math notranslate nohighlight">
\[
L_{j,\cdot} (L^T)_{\cdot,j} = \sum_{k=1}^j \ell_{j,k}^2 = b_{j,j},
\]</div>
<p>where again we used the fact that <span class="math notranslate nohighlight">\(L^T\)</span> is upper triangular. Since <span class="math notranslate nohighlight">\(\ell_{j,1}, \ldots, \ell_{j,j-1}\)</span> are the elements of <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span>, they have already been determined. So we can set</p>
<div class="math notranslate nohighlight">
\[
\ell_{j,j}
= \sqrt{b_{j,j} - \sum_{k=1}^{j-1} \ell_{j,k}^2}.
\]</div>
<p>The fact that we are taking the square root of a positive quantity is established in the proof of the <em>Cholesky Decomposition Theorem</em>. Finally, from <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span>, <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span>, and <span class="math notranslate nohighlight">\(\ell_{j,j}\)</span>, we construct <span class="math notranslate nohighlight">\(L_{(j)}\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement the algorithm above. In our naive implementation, we assume that <span class="math notranslate nohighlight">\(B\)</span> is positive definite, and therefore that all steps are well-defined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">],</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">])</span>
        <span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">L</span> 
</pre></div>
</div>
</div>
</div>
<p>Here is a simple example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[2. 1.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1.41421356 0.        ]
 [0.70710678 1.22474487]]
</pre></div>
</div>
</div>
</div>
<p>We can check that it produces the right factorization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">L</span> <span class="o">@</span> <span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[2. 1.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Proof of Cholesky decomposition theorem</strong> We give a proof of the <em>Cholesky Decomposition Theorem</em>.</p>
<p><em>Proof idea:</em> Assuming by induction that the upper-left corner of the matrix <span class="math notranslate nohighlight">\(B\)</span> has a Cholesky decomposition, one finds equations for the remaining row that can be solved uniquely by the properties established in the previous subsection.</p>
<p><em>Proof:</em> If <span class="math notranslate nohighlight">\(n=1\)</span>, we have shown previously that <span class="math notranslate nohighlight">\(b_{1,1} &gt; 0\)</span>, and hence we can take <span class="math notranslate nohighlight">\(L = [\ell_{1,1}]\)</span> where <span class="math notranslate nohighlight">\(\ell_{1,1} = \sqrt{b_{1,1}}\)</span>. Assuming the result holds for positive definite matrices in <span class="math notranslate nohighlight">\(\mathbb{R}^{(n-1) \times (n-1)}\)</span>, we first re-write <span class="math notranslate nohighlight">\(B = L L^T\)</span> in block form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
B_{11} &amp; \bfbeta_{12}\\
\bfbeta_{12}^T &amp; \beta_{22}
\end{pmatrix}
= 
\begin{pmatrix}
\Lambda_{11} &amp; \mathbf{0}\\
\bflambda_{12}^T &amp; \lambda_{22}
\end{pmatrix}
\begin{pmatrix}
\Lambda_{11}^T &amp; \bflambda_{12}\\
\mathbf{0}^T &amp; \lambda_{22}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(B_{11}, \Lambda_{11} \in \mathbb{R}^{n-1 \times n-1}\)</span>, <span class="math notranslate nohighlight">\(\bfbeta_{12}, \bflambda_{12} \in \mathbb{R}^{n-1}\)</span> and <span class="math notranslate nohighlight">\(\beta_{22}, \lambda_{22} \in \mathbb{R}\)</span>. By block matrix algebra, we get the system</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B_{11} = \Lambda_{11} \Lambda_{11}^T\\
\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\\
\beta_{22} = \bflambda_{12}^T \bflambda_{12} + \lambda_{22}^2.
\end{split}\]</div>
<p>By the <em>Principal Submatrices Lemma</em>, the principal submatrix <span class="math notranslate nohighlight">\(B_{11}\)</span> is positive definite. Hence, by induction, there is a unique lower-triangular matrix <span class="math notranslate nohighlight">\(\Lambda_{11}\)</span> with positive diagonal elements satisfying the first equation. We can then obtain <span class="math notranslate nohighlight">\(\bfbeta_{12}\)</span> from the second equation by forward substitution. And finally we get</p>
<div class="math notranslate nohighlight">
\[
\lambda_{22}
= \sqrt{\beta_{22} - \bflambda_{12}^T \bflambda_{12}}.
\]</div>
<p>We do have to check that the square root above exists. That is, we need to argue that the expression inside the square root is non-negative. In fact, for the claim to go through, we need it to be strictly positive. We notice that the expression inside the square root is in fact the Schur complement of the block <span class="math notranslate nohighlight">\(B_{11}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\beta_{22} - \bflambda_{12}^T \bflambda_{12}
&amp;= \beta_{22} - (\Lambda_{11}^{-1} \bfbeta_{12})^T (\Lambda_{11}^{-1} \bfbeta_{12})\\
&amp;= \beta_{22} - \bfbeta_{12}^T (\Lambda_{11}^{-1})^T \Lambda_{11}^{-1} \bfbeta_{12}\\
&amp;= \beta_{22} - \bfbeta_{12}^T (\Lambda_{11} \Lambda_{11}^T)^{-1} \bfbeta_{12}\\
&amp;= \beta_{22} - \bfbeta_{12}^T (B_{11})^{-1} \bfbeta_{12}
\end{align*}\]</div>
<p>where we used the equation <span class="math notranslate nohighlight">\(\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\)</span> on the first line, the identities <span class="math notranslate nohighlight">\((Q W)^{-1} = W^{-1} Q^{-1}\)</span> and <span class="math notranslate nohighlight">\((Q^T)^{-1} = (Q^{-1})^T\)</span> (see the exercise below) on the third line and the equation <span class="math notranslate nohighlight">\(B_{11} = \Lambda_{11} \Lambda_{11}^T\)</span> on the fourth line. By the <em>Schur Complement Lemma</em>, the Schur complement is positive definite. Because it is a scalar in this case, it is strictly positive (prove it!), which concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Back to multivariate Gaussians</strong> Returning to our motivation, we can generate samples from a <span class="math notranslate nohighlight">\(N(\bmu, \bSigma)\)</span> by first generating</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}
= (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d))
\]</div>
<p>where <span class="math notranslate nohighlight">\(U_1, \ldots, U_d\)</span> are independent uniform <span class="math notranslate nohighlight">\([0,1]\)</span> variables, then setting</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}
= \bmu + L \mathbf{Z},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bSigma = L L^T\)</span> is a Cholesky decomposition of <span class="math notranslate nohighlight">\(\bSigma\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement this method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">generate_multivariate_normal_samples_using_cholesky</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span><span class="p">):</span>

    <span class="c1"># Compute Cholesky decomposition</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">Sig</span><span class="p">)</span>
    
    <span class="c1"># Initialization</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        
            <span class="c1"># Generate standard normal vector</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="n">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Apply the inverse CDF (ppf) of the standard normal distribution</span>
            <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">L</span> <span class="o">@</span> <span class="n">Z</span> 
    
    <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
</div>
<p>We generate some samples as an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">Sig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">generate_multivariate_normal_samples_using_cholesky</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[-0.47926185  1.97223283  2.73780609]
 [-2.69005319 -4.19788834 -0.43130768]
 [ 0.41957285  3.91719212  2.08604427]
 [-2.11532949 -5.34557983  0.69521104]
 [-2.41203356 -1.84032486 -0.82207565]
 [-1.46121329  0.4821332   0.55005982]
 [-0.84981594  0.67074839  0.16360931]
 [-2.19097155 -1.98022929 -1.06365711]
 [-2.75113597 -3.47560492 -0.26607926]
 [ 0.130848    6.07312936 -0.08800829]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Using a Cholesky decomposition to solve the least squares problem</strong> Another application of the Cholesky decomposition is to solving the least squares problem. In this section, we restrict ourselves to the case where <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> has full column rank. By the <em>Least Squares and Positive Semidefiniteness Lemma</em>, we then have that <span class="math notranslate nohighlight">\(A^T A\)</span> is positive definite. By the <em>Cholesky Decomposition Theorem</em>, we can factorize this matrix as <span class="math notranslate nohighlight">\(A^T A = L L^T\)</span> where <span class="math notranslate nohighlight">\(L\)</span> is lower triangular with positive diagonal elements. The normal equations then reduce to</p>
<div class="math notranslate nohighlight">
\[
L L^T \mathbf{x} = A^T \mathbf{b}.
\]</div>
<p>This system can be solved in two steps. We first obtain the solution to</p>
<div class="math notranslate nohighlight">
\[
L \mathbf{z} = A^T \mathbf{b}
\]</div>
<p>by forward substitution. Then we obtain the solution to</p>
<div class="math notranslate nohighlight">
\[
L^T \mathbf{x} = \mathbf{z}
\]</div>
<p>by back-substitution. Note that <span class="math notranslate nohighlight">\(L^T\)</span> is indeed an upper triangular matrix.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement this algorithm below. In our naive implementation, we assume that <span class="math notranslate nohighlight">\(A\)</span> has full column rank, and therefore that all steps are well-defined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">ls_by_chol</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mmids</span><span class="o">.</span><span class="n">backsubs</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>Other applications of the Cholesky decomposition are briefly described <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications">here</a>.</p>
</section>
</section>
&#13;

<h2><span class="section-number">6.7.1. </span>Quizzes, solutions, code, etc.<a class="headerlink" href="#quizzes-solutions-code-etc" title="Link to this heading">#</a></h2>
<section id="just-the-code">
<h3><span class="section-number">6.7.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_prob_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
</section>
<section id="self-assessment-quizzes">
<h3><span class="section-number">6.7.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_2.html">Section 6.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_3.html">Section 6.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_4.html">Section 6.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_5.html">Section 6.5</a></p></li>
</ul>
</section>
<section id="auto-quizzes">
<h3><span class="section-number">6.7.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
</section>
<section id="solutions-to-odd-numbered-warm-up-exercises">
<h3><span class="section-number">6.7.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>E6.2.1</strong> The probability of <span class="math notranslate nohighlight">\(Y\)</span> being in the second category is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = e_2) = \pi_2 = 0.5.
\]</div>
<p><strong>E6.2.3</strong> Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> be i.i.d. Bernoulli<span class="math notranslate nohighlight">\((q^*)\)</span>. The MLE is <span class="math notranslate nohighlight">\(\hat{q}_{\mathrm{MLE}} = \frac{1}{n} \sum_{i=1}^n X_i\)</span>. By the Law of Large Numbers, as <span class="math notranslate nohighlight">\(n \to \infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\hat{q}_{\mathrm{MLE}} \to \mathbb{E}[X_1] = q^*
\]</div>
<p>almost surely.</p>
<p><strong>E6.2.5</strong> The gradient at <span class="math notranslate nohighlight">\(w = 0\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\nabla_w L_4(0; \{(x_i, y_i)\}_{i=1}^4) = -\sum_{i=1}^4 x_i(y_i - \sigma(0)) = -\frac{1}{2}(1 - 2 + 3 - 4) = 1.
\]</div>
<p>The updated parameter after one step of gradient descent is:</p>
<div class="math notranslate nohighlight">
\[
w' = w - \eta \nabla_w L_4(w; \{(x_i, y_i)\}_{i=1}^4) = 0 - 0.1 \cdot 1 = -0.1.
\]</div>
<p><strong>E6.2.7</strong> We must have <span class="math notranslate nohighlight">\(\sum_{x=-1}^1 \mathbb{P}(X=x) = 1\)</span>. This implies</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{Z(\theta)} (h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}) = 1.
\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[
Z(\theta) = h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}.
\]</div>
<p><strong>E6.2.9</strong> The empirical frequency for each category is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_i = \frac{N_i}{n},
\]</div>
<p>where <span class="math notranslate nohighlight">\(N_i\)</span> is the number of times category <span class="math notranslate nohighlight">\(i\)</span> appears in the sample. The counts are:</p>
<div class="math notranslate nohighlight">
\[
N_1 = 1, \quad N_2 = 2, \quad N_3 = 1.
\]</div>
<p>Thus, the empirical frequencies are:</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_1 = \frac{1}{4} = 0.25, \quad \hat{\pi}_2 = \frac{2}{4} = 0.5, \quad \hat{\pi}_3 = \frac{1}{4} = 0.25.
\]</div>
<p><strong>E6.2.11</strong> The log-likelihood for a multivariate Gaussian distribution is given by:</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L}(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{X}) = -\frac{1}{2} \left[ (\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X} - \boldsymbol{\mu}) + \log |\boldsymbol{\Sigma}| + 2 \log(2\pi) \right].
\]</div>
<p>First, compute the inverse and determinant of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Sigma}^{-1} = \frac{1}{3} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix}, \quad |\boldsymbol{\Sigma}| = 3.
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} - \boldsymbol{\mu} = \begin{pmatrix} 1 \\ 3 \end{pmatrix} - \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}.
\end{split}\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
(\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X} - \boldsymbol{\mu}) = \begin{pmatrix} 0 &amp; 1 \end{pmatrix} \frac{1}{3} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \frac{1}{3} \cdot 2 = \frac{2}{3}.
\end{split}\]</div>
<p>So the log-likelihood is:
$<span class="math notranslate nohighlight">\(
\log \mathcal{L} = -\frac{1}{2} \left[ \frac{2}{3} + \log 3 + 2 \log (2\pi) \right] \approx -3.178.
\)</span>$</p>
<p><strong>E6.3.1</strong> <span class="math notranslate nohighlight">\(\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} = \frac{0.2}{0.5} = 0.4.\)</span> This follows from the definition of conditional probability.</p>
<p><strong>E6.3.3</strong> <span class="math notranslate nohighlight">\(\mathbb{E}[X|Y=y] = 1 \cdot 0.3 + 2 \cdot 0.7 = 0.3 + 1.4 = 1.7.\)</span> This is the definition of the conditional expectation for discrete random variables.</p>
<p><strong>E6.3.5</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}[A \mid B \cap C] &amp;= \frac{\mathbb{P}[A \cap (B \cap C)]}{\mathbb{P}[B \cap C]} \\
&amp;= \frac{\mathbb{P}[A \cap B \cap C]}{\mathbb{P}[B \cap C]} \\
&amp;= \frac{0.05}{0.1} \\
&amp;= 0.5.
\end{align*}\]</div>
<p><strong>E6.3.7</strong> If <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(C\)</span> are pairwise independent, then they are also mutually independent. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}[A \cap B \cap C] &amp;= \mathbb{P}[A] \mathbb{P}[B] \mathbb{P}[C] \\
&amp;= 0.8 \cdot 0.6 \cdot 0.5 \\
&amp;= 0.24.
\end{align*}\]</div>
<p><strong>E6.3.9</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(\mathbb{P}[X=x, Z=z])_{x,z} &amp;= \sum_{y} (\mathbb{P}[X=x, Y = y, Z=z])_{x,z} \\
&amp;= \sum_{y} [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = y \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = y])_z \\
&amp;= [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = 0 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 0])_z\\ 
&amp; \quad + [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = 1 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 1])_z \\
&amp;= \begin{pmatrix} 0.3 \cdot 0.2 \cdot 0.5 &amp; 0.3 \cdot 0.2 \cdot 0.5 \\ 0.7 \cdot 0.6 \cdot 0.5 &amp; 0.7 \cdot 0.6 \cdot 0.5 \end{pmatrix} + \begin{pmatrix} 0.3 \cdot 0.8 \cdot 0.1 &amp; 0.3 \cdot 0.8 \cdot 0.9 \\ 0.7 \cdot 0.4 \cdot 0.1 &amp; 0.7 \cdot 0.4 \cdot 0.9 \end{pmatrix} \\
&amp;= \begin{pmatrix} 0.03 &amp; 0.03 \\ 0.21 &amp; 0.21 \end{pmatrix} + \begin{pmatrix} 0.024 &amp; 0.216 \\ 0.028 &amp; 0.252 \end{pmatrix} \\
&amp;= \begin{pmatrix} 0.054 &amp; 0.246 \\ 0.238 &amp; 0.462 \end{pmatrix}.
\end{align*}\]</div>
<p><strong>E6.3.11</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{\pi}_1 &amp;= \frac{N_1}{N_1 + N_2} = \frac{50}{50 + 100} = \frac{1}{3}, \\
\hat{p}_{1,1} &amp;= \frac{N_{1,1}}{N_1} = \frac{10}{50} = 0.2, \\
\hat{p}_{2,1} &amp;= \frac{N_{2,1}}{N_2} = \frac{40}{100} = 0.4.
\end{align*}\]</div>
<p><strong>E6.3.13</strong></p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[X=x, Y=y, Z=z] = \mathbb{P}[X=x]\mathbb{P}[Y=y|X=x]\mathbb{P}[Z=z|X=x].
\]</div>
<p><strong>E6.4.1</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}(X = 1) &amp;= \sum_{i=1}^2 \pi_i p_i \\
&amp;= (0.6)(0.3) + (0.4)(0.8) \\
&amp;= 0.5
\end{align*}\]</div>
<p><strong>E6.4.3</strong> <span class="math notranslate nohighlight">\(\mathbb{P}[X = (1, 0)] = \pi_1 p_{1,1} (1 - p_{1,2}) + \pi_2 p_{2,1} (1 - p_{2,2}) = 0.4 \cdot 0.7 \cdot 0.7 + 0.6 \cdot 0.2 \cdot 0.2 = 0.196 + 0.024 = 0.22\)</span>.</p>
<p><strong>E6.4.5</strong> <span class="math notranslate nohighlight">\(r_{1,i} = \frac{\pi_1 p_{1,1} p_{1,2}}{\pi_1 p_{1,1} p_{1,2} + \pi_2 p_{2,1} p_{2,2}} = \frac{0.5 \cdot 0.8 \cdot 0.2}{0.5 \cdot 0.8 \cdot 0.2 + 0.5 \cdot 0.1 \cdot 0.9} = \frac{0.08}{0.08 + 0.045} \approx 0.64\)</span>, <span class="math notranslate nohighlight">\(r_{2,i} = 1 - r_{1,i} \approx 0.36\)</span>.</p>
<p><strong>E6.4.7</strong> <span class="math notranslate nohighlight">\(\pi_1 = \frac{\eta_1}{n} = \frac{r_{1,1} + r_{1,1}}{2} = \frac{0.8 + 0.8}{2} = 0.8\)</span>, <span class="math notranslate nohighlight">\(\pi_2 = 1 - \pi_1 = 0.2\)</span>.</p>
<p><strong>E6.4.9</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
r_{1,2} &amp;= \frac{\pi_1 \prod_{m=1}^3 p_{1,m}^{x_{2,m}} (1 - p_{1,m})^{1-x_{2,m}}}{\sum_{k=1}^2 \pi_k \prod_{m=1}^3 p_{k,m}^{x_{2,m}} (1 - p_{k,m})^{1-x_{2,m}}} \\
&amp;= \frac{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1}{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1 + (0.6)(0.8)^0(0.2)^1(0.5)^0(0.5)^1} \\
&amp;= \frac{0.032}{0.032 + 0.06} \\
&amp;= \frac{8}{23}
\end{align*}\]</div>
<p><strong>E6.4.11</strong></p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X] = \pi_1 \mu_1 + \pi_2 \mu_2 = 0.5 \times (-1) + 0.5 \times 3 = -0.5 + 1.5 = 1.
\]</div>
<p><strong>E6.4.13</strong></p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = \pi_1 (\sigma_1^2 + \mu_1^2) + \pi_2 (\sigma_2^2 + \mu_2^2) - \left(\pi_1 \mu_1 + \pi_2 \mu_2\right)^2,
\]</div>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = 0.4 (1 + 0^2) + 0.6 (2 + 4^2) - (0.4 \times 0 + 0.6 \times 4)^2 = 0.4 \times 1 + 0.6 \times 18 - 2.4^2,
\]</div>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = 0.4 + 10.8 - 5.76 = 5.44.
\]</div>
<p><strong>E6.5.1</strong></p>
<div class="math notranslate nohighlight">
\[ 
B / B_{11} = B_{22} - B_{12}^T B_{11}^{-1} B_{12} = 3 - 1 \cdot \frac{1}{2} \cdot 1 = \frac{5}{2}, 
\]</div>
<p>using the definition of the Schur complement.</p>
<p><strong>E6.5.3</strong> The Schur complement of <span class="math notranslate nohighlight">\(A_{11}\)</span> in <span class="math notranslate nohighlight">\(A\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A/A_{11} = A_{22} - A_{21}A_{11}^{-1}A_{12} = 7 - \begin{pmatrix}
0 &amp; 6 
\end{pmatrix} \begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4 
\end{pmatrix}^{-1} \begin{pmatrix}
0 \\
5 
\end{pmatrix}.
\end{split}\]</div>
<p>First, compute <span class="math notranslate nohighlight">\(A_{11}^{-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A_{11}^{-1} = \frac{1}{1 \cdot 4 - 2 \cdot 3} \begin{pmatrix}
4 &amp; -2 \\
-3 &amp; 1
\end{pmatrix} = \begin{pmatrix}
-2 &amp; 1 \\
1.5 &amp; -0.5
\end{pmatrix}.
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A/A_{11} = 7 - \begin{pmatrix}
0 &amp; 6 
\end{pmatrix} \begin{pmatrix}
-2 &amp; 1 \\
1.5 &amp; -0.5
\end{pmatrix} \begin{pmatrix}
0 \\
5 
\end{pmatrix} = 7 + (6 \cdot 0.5 \cdot 5) = 22.
\end{split}\]</div>
<p><strong>E6.5.5</strong> The conditional mean of <span class="math notranslate nohighlight">\(X_1\)</span> given <span class="math notranslate nohighlight">\(X_2 = 3\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\mu_{1|2}(3) = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (3 - \bmu_2) = 1 + 1 \cdot \frac{1}{3} (3 - 2) = \frac{4}{3},
\]</div>
<p>using the formula for the conditional mean of multivariate Gaussians.</p>
<p><strong>E6.5.7</strong> The conditional distribution of <span class="math notranslate nohighlight">\(X_1\)</span> given <span class="math notranslate nohighlight">\(X_2 = 1\)</span> is Gaussian with mean <span class="math notranslate nohighlight">\(\mu_{1|2} = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (1 - \mu_2) = \frac{1}{2}\)</span> and variance <span class="math notranslate nohighlight">\(\bSigma_{1|2} = \bSigma_{11} - \bSigma_{12} \bSigma_{22}^{-1} \bSigma_{21} = \frac{7}{2}\)</span>.</p>
<p><strong>E6.5.9</strong> The distribution of <span class="math notranslate nohighlight">\(\bY\)</span> is Gaussian with mean vector <span class="math notranslate nohighlight">\(A\bmu = \begin{pmatrix} -3 \\ -3 \end{pmatrix}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(A\bSigma A^T = \begin{pmatrix} 8 &amp; 1 \\ 1 &amp; 6 \end{pmatrix}\)</span>.</p>
<p><strong>E6.5.11</strong> The mean of <span class="math notranslate nohighlight">\(Y_t\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{E}[Y_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \mathbb{E}[\bX_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = 1,
\end{split}\]</div>
<p>and the variance of <span class="math notranslate nohighlight">\(Y_t\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{Var}[Y_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \mathrm{Cov}[\bX_t] \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = 2,
\end{split}\]</div>
<p>using the properties of linear-Gaussian systems.</p>
<p><strong>E6.5.13</strong> The innovation is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
e_t = Y_t - H \bmu_{\text{pred}} = 3 - \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 3 \\ 1 \end{pmatrix} = 3 - 3 = 0.
\end{split}\]</div>
<p><strong>E6.5.15</strong> The updated state estimate is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bmu_t = \bmu_{\text{pred}} + K_t e_t = \begin{pmatrix} 3 \\ 1 \end{pmatrix} + \begin{pmatrix} \frac{2}{3} \\ \frac{1}{3} \end{pmatrix} \cdot 0 = \begin{pmatrix} 3 \\ 1 \end{pmatrix}.
\end{split}\]</div>
</section>
<section id="learning-outcomes">
<h3><span class="section-number">6.7.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define exponential families and give examples of common probability distributions that belong to this family.</p></li>
<li><p>Derive the maximum likelihood estimator for exponential families and explain its properties.</p></li>
<li><p>Prove that, under certain conditions, the maximum likelihood estimator is statistically consistent.</p></li>
<li><p>Formulate generalized linear models using exponential families and express linear and logistic regression as special cases.</p></li>
<li><p>Compute the gradient and Hessian of the negative log-likelihood for generalized linear models.</p></li>
<li><p>Interpret the moment-matching equations for the maximum likelihood estimator in generalized linear models.</p></li>
<li><p>Apply the multiplication rule, the law of total probability, and Bayes’ rule to solve problems involving conditional probabilities.</p></li>
<li><p>Calculate conditional probability mass functions and conditional expectations for discrete random variables.</p></li>
<li><p>Define conditional independence and express it mathematically in terms of conditional probabilities.</p></li>
<li><p>Differentiate between the fork, chain, and collider configurations in graphical models representing conditional independence relations.</p></li>
<li><p>Derive the joint probability distribution for the Naive Bayes model under the assumption of conditional independence.</p></li>
<li><p>Implement maximum likelihood estimation to fit the parameters of the Naive Bayes model.</p></li>
<li><p>Apply the Naive Bayes model for prediction and evaluate its accuracy.</p></li>
<li><p>Implement Laplace smoothing to address the issue of unseen words in the training data when fitting a Naive Bayes model.</p></li>
<li><p>Apply the Naive Bayes model to perform sentiment analysis on a real-world dataset and interpret the results.</p></li>
<li><p>Define mixtures as convex combinations of distributions and express the probability distribution of a mixture model using the law of total probability.</p></li>
<li><p>Identify examples of mixture models, such as mixtures of multinomials and Gaussian mixture models, and recognize their probability density functions.</p></li>
<li><p>Explain the concept of marginalizing out an unobserved random variable in the context of mixture models.</p></li>
<li><p>Formulate the objective function for parameter estimation in mixtures of multivariate Bernoullis using the negative log-likelihood.</p></li>
<li><p>Describe the majorization-minimization principle and its application in the Expectation-Maximization (EM) algorithm.</p></li>
<li><p>Derive the E-step and M-step updates for the EM algorithm in the context of mixtures of multivariate Bernoullis.</p></li>
<li><p>Implement the EM algorithm for mixtures of multivariate Bernoullis and apply it to a real-world dataset, such as clustering handwritten digits from the MNIST dataset.</p></li>
<li><p>Identify and address numerical issues that may arise during the implementation of the EM algorithm, such as underflow, by applying techniques like the log-sum-exp trick.</p></li>
<li><p>Define block matrices and the Schur complement, and demonstrate their properties through examples and proofs.</p></li>
<li><p>Derive the marginal and conditional distributions of multivariate Gaussians using the properties of block matrices and the Schur complement.</p></li>
<li><p>Describe the linear-Gaussian system model and its components, including the state evolution and observation processes.</p></li>
<li><p>Explain the purpose and key steps of the Kalman filter algorithm, including the prediction and update steps.</p></li>
<li><p>Implement the Kalman filter algorithm in code, given the state evolution and observation models, and the initial state distribution.</p></li>
<li><p>Apply the Kalman filter to a location tracking problem, and interpret the results in terms of the estimated object path and the algorithm’s performance.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
</section>
&#13;

<h3><span class="section-number">6.7.1.1. </span>Just the code<a class="headerlink" href="#just-the-code" title="Link to this heading">#</a></h3>
<p>An interactive Jupyter notebook featuring the code in this chapter can be accessed below (Google Colab recommended). You are encouraged to tinker with it. Some suggested computational exercises are scattered throughout. The notebook is also available as a slideshow.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Notebook</a> (<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb">Open In Colab</a>)</p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_prob_notebook_slides.slides.html">Slideshow</a></p></li>
</ul>
&#13;

<h3><span class="section-number">6.7.1.2. </span>Self-assessment quizzes<a class="headerlink" href="#self-assessment-quizzes" title="Link to this heading">#</a></h3>
<p>A more extensive web version of the self-assessment quizzes is available by following the links below.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_2.html">Section 6.2</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_3.html">Section 6.3</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_4.html">Section 6.4</a></p></li>
<li><p><a class="reference external" href="https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_5.html">Section 6.5</a></p></li>
</ul>
&#13;

<h3><span class="section-number">6.7.1.3. </span>Auto-quizzes<a class="headerlink" href="#auto-quizzes" title="Link to this heading">#</a></h3>
<p>Automatically generated quizzes for this chapter can be accessed here (Google Colab recommended).</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb">Auto-quizzes</a>
(<a class="reference external" href="https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb">Open In Colab</a>)</p></li>
</ul>
&#13;

<h3><span class="section-number">6.7.1.4. </span>Solutions to odd-numbered warm-up exercises<a class="headerlink" href="#solutions-to-odd-numbered-warm-up-exercises" title="Link to this heading">#</a></h3>
<p><em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>E6.2.1</strong> The probability of <span class="math notranslate nohighlight">\(Y\)</span> being in the second category is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}(Y = e_2) = \pi_2 = 0.5.
\]</div>
<p><strong>E6.2.3</strong> Let <span class="math notranslate nohighlight">\(X_1, \ldots, X_n\)</span> be i.i.d. Bernoulli<span class="math notranslate nohighlight">\((q^*)\)</span>. The MLE is <span class="math notranslate nohighlight">\(\hat{q}_{\mathrm{MLE}} = \frac{1}{n} \sum_{i=1}^n X_i\)</span>. By the Law of Large Numbers, as <span class="math notranslate nohighlight">\(n \to \infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\hat{q}_{\mathrm{MLE}} \to \mathbb{E}[X_1] = q^*
\]</div>
<p>almost surely.</p>
<p><strong>E6.2.5</strong> The gradient at <span class="math notranslate nohighlight">\(w = 0\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
\nabla_w L_4(0; \{(x_i, y_i)\}_{i=1}^4) = -\sum_{i=1}^4 x_i(y_i - \sigma(0)) = -\frac{1}{2}(1 - 2 + 3 - 4) = 1.
\]</div>
<p>The updated parameter after one step of gradient descent is:</p>
<div class="math notranslate nohighlight">
\[
w' = w - \eta \nabla_w L_4(w; \{(x_i, y_i)\}_{i=1}^4) = 0 - 0.1 \cdot 1 = -0.1.
\]</div>
<p><strong>E6.2.7</strong> We must have <span class="math notranslate nohighlight">\(\sum_{x=-1}^1 \mathbb{P}(X=x) = 1\)</span>. This implies</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{Z(\theta)} (h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}) = 1.
\]</div>
<p>Hence,</p>
<div class="math notranslate nohighlight">
\[
Z(\theta) = h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}.
\]</div>
<p><strong>E6.2.9</strong> The empirical frequency for each category is given by:</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_i = \frac{N_i}{n},
\]</div>
<p>where <span class="math notranslate nohighlight">\(N_i\)</span> is the number of times category <span class="math notranslate nohighlight">\(i\)</span> appears in the sample. The counts are:</p>
<div class="math notranslate nohighlight">
\[
N_1 = 1, \quad N_2 = 2, \quad N_3 = 1.
\]</div>
<p>Thus, the empirical frequencies are:</p>
<div class="math notranslate nohighlight">
\[
\hat{\pi}_1 = \frac{1}{4} = 0.25, \quad \hat{\pi}_2 = \frac{2}{4} = 0.5, \quad \hat{\pi}_3 = \frac{1}{4} = 0.25.
\]</div>
<p><strong>E6.2.11</strong> The log-likelihood for a multivariate Gaussian distribution is given by:</p>
<div class="math notranslate nohighlight">
\[
\log \mathcal{L}(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{X}) = -\frac{1}{2} \left[ (\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X} - \boldsymbol{\mu}) + \log |\boldsymbol{\Sigma}| + 2 \log(2\pi) \right].
\]</div>
<p>First, compute the inverse and determinant of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\boldsymbol{\Sigma}^{-1} = \frac{1}{3} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix}, \quad |\boldsymbol{\Sigma}| = 3.
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{X} - \boldsymbol{\mu} = \begin{pmatrix} 1 \\ 3 \end{pmatrix} - \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}.
\end{split}\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
(\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X} - \boldsymbol{\mu}) = \begin{pmatrix} 0 &amp; 1 \end{pmatrix} \frac{1}{3} \begin{pmatrix} 2 &amp; -1 \\ -1 &amp; 2 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \frac{1}{3} \cdot 2 = \frac{2}{3}.
\end{split}\]</div>
<p>So the log-likelihood is:
$<span class="math notranslate nohighlight">\(
\log \mathcal{L} = -\frac{1}{2} \left[ \frac{2}{3} + \log 3 + 2 \log (2\pi) \right] \approx -3.178.
\)</span>$</p>
<p><strong>E6.3.1</strong> <span class="math notranslate nohighlight">\(\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} = \frac{0.2}{0.5} = 0.4.\)</span> This follows from the definition of conditional probability.</p>
<p><strong>E6.3.3</strong> <span class="math notranslate nohighlight">\(\mathbb{E}[X|Y=y] = 1 \cdot 0.3 + 2 \cdot 0.7 = 0.3 + 1.4 = 1.7.\)</span> This is the definition of the conditional expectation for discrete random variables.</p>
<p><strong>E6.3.5</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}[A \mid B \cap C] &amp;= \frac{\mathbb{P}[A \cap (B \cap C)]}{\mathbb{P}[B \cap C]} \\
&amp;= \frac{\mathbb{P}[A \cap B \cap C]}{\mathbb{P}[B \cap C]} \\
&amp;= \frac{0.05}{0.1} \\
&amp;= 0.5.
\end{align*}\]</div>
<p><strong>E6.3.7</strong> If <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, and <span class="math notranslate nohighlight">\(C\)</span> are pairwise independent, then they are also mutually independent. Therefore,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}[A \cap B \cap C] &amp;= \mathbb{P}[A] \mathbb{P}[B] \mathbb{P}[C] \\
&amp;= 0.8 \cdot 0.6 \cdot 0.5 \\
&amp;= 0.24.
\end{align*}\]</div>
<p><strong>E6.3.9</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(\mathbb{P}[X=x, Z=z])_{x,z} &amp;= \sum_{y} (\mathbb{P}[X=x, Y = y, Z=z])_{x,z} \\
&amp;= \sum_{y} [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = y \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = y])_z \\
&amp;= [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = 0 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 0])_z\\ 
&amp; \quad + [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = 1 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 1])_z \\
&amp;= \begin{pmatrix} 0.3 \cdot 0.2 \cdot 0.5 &amp; 0.3 \cdot 0.2 \cdot 0.5 \\ 0.7 \cdot 0.6 \cdot 0.5 &amp; 0.7 \cdot 0.6 \cdot 0.5 \end{pmatrix} + \begin{pmatrix} 0.3 \cdot 0.8 \cdot 0.1 &amp; 0.3 \cdot 0.8 \cdot 0.9 \\ 0.7 \cdot 0.4 \cdot 0.1 &amp; 0.7 \cdot 0.4 \cdot 0.9 \end{pmatrix} \\
&amp;= \begin{pmatrix} 0.03 &amp; 0.03 \\ 0.21 &amp; 0.21 \end{pmatrix} + \begin{pmatrix} 0.024 &amp; 0.216 \\ 0.028 &amp; 0.252 \end{pmatrix} \\
&amp;= \begin{pmatrix} 0.054 &amp; 0.246 \\ 0.238 &amp; 0.462 \end{pmatrix}.
\end{align*}\]</div>
<p><strong>E6.3.11</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{\pi}_1 &amp;= \frac{N_1}{N_1 + N_2} = \frac{50}{50 + 100} = \frac{1}{3}, \\
\hat{p}_{1,1} &amp;= \frac{N_{1,1}}{N_1} = \frac{10}{50} = 0.2, \\
\hat{p}_{2,1} &amp;= \frac{N_{2,1}}{N_2} = \frac{40}{100} = 0.4.
\end{align*}\]</div>
<p><strong>E6.3.13</strong></p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[X=x, Y=y, Z=z] = \mathbb{P}[X=x]\mathbb{P}[Y=y|X=x]\mathbb{P}[Z=z|X=x].
\]</div>
<p><strong>E6.4.1</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}(X = 1) &amp;= \sum_{i=1}^2 \pi_i p_i \\
&amp;= (0.6)(0.3) + (0.4)(0.8) \\
&amp;= 0.5
\end{align*}\]</div>
<p><strong>E6.4.3</strong> <span class="math notranslate nohighlight">\(\mathbb{P}[X = (1, 0)] = \pi_1 p_{1,1} (1 - p_{1,2}) + \pi_2 p_{2,1} (1 - p_{2,2}) = 0.4 \cdot 0.7 \cdot 0.7 + 0.6 \cdot 0.2 \cdot 0.2 = 0.196 + 0.024 = 0.22\)</span>.</p>
<p><strong>E6.4.5</strong> <span class="math notranslate nohighlight">\(r_{1,i} = \frac{\pi_1 p_{1,1} p_{1,2}}{\pi_1 p_{1,1} p_{1,2} + \pi_2 p_{2,1} p_{2,2}} = \frac{0.5 \cdot 0.8 \cdot 0.2}{0.5 \cdot 0.8 \cdot 0.2 + 0.5 \cdot 0.1 \cdot 0.9} = \frac{0.08}{0.08 + 0.045} \approx 0.64\)</span>, <span class="math notranslate nohighlight">\(r_{2,i} = 1 - r_{1,i} \approx 0.36\)</span>.</p>
<p><strong>E6.4.7</strong> <span class="math notranslate nohighlight">\(\pi_1 = \frac{\eta_1}{n} = \frac{r_{1,1} + r_{1,1}}{2} = \frac{0.8 + 0.8}{2} = 0.8\)</span>, <span class="math notranslate nohighlight">\(\pi_2 = 1 - \pi_1 = 0.2\)</span>.</p>
<p><strong>E6.4.9</strong></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
r_{1,2} &amp;= \frac{\pi_1 \prod_{m=1}^3 p_{1,m}^{x_{2,m}} (1 - p_{1,m})^{1-x_{2,m}}}{\sum_{k=1}^2 \pi_k \prod_{m=1}^3 p_{k,m}^{x_{2,m}} (1 - p_{k,m})^{1-x_{2,m}}} \\
&amp;= \frac{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1}{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1 + (0.6)(0.8)^0(0.2)^1(0.5)^0(0.5)^1} \\
&amp;= \frac{0.032}{0.032 + 0.06} \\
&amp;= \frac{8}{23}
\end{align*}\]</div>
<p><strong>E6.4.11</strong></p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X] = \pi_1 \mu_1 + \pi_2 \mu_2 = 0.5 \times (-1) + 0.5 \times 3 = -0.5 + 1.5 = 1.
\]</div>
<p><strong>E6.4.13</strong></p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = \pi_1 (\sigma_1^2 + \mu_1^2) + \pi_2 (\sigma_2^2 + \mu_2^2) - \left(\pi_1 \mu_1 + \pi_2 \mu_2\right)^2,
\]</div>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = 0.4 (1 + 0^2) + 0.6 (2 + 4^2) - (0.4 \times 0 + 0.6 \times 4)^2 = 0.4 \times 1 + 0.6 \times 18 - 2.4^2,
\]</div>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}(X) = 0.4 + 10.8 - 5.76 = 5.44.
\]</div>
<p><strong>E6.5.1</strong></p>
<div class="math notranslate nohighlight">
\[ 
B / B_{11} = B_{22} - B_{12}^T B_{11}^{-1} B_{12} = 3 - 1 \cdot \frac{1}{2} \cdot 1 = \frac{5}{2}, 
\]</div>
<p>using the definition of the Schur complement.</p>
<p><strong>E6.5.3</strong> The Schur complement of <span class="math notranslate nohighlight">\(A_{11}\)</span> in <span class="math notranslate nohighlight">\(A\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A/A_{11} = A_{22} - A_{21}A_{11}^{-1}A_{12} = 7 - \begin{pmatrix}
0 &amp; 6 
\end{pmatrix} \begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4 
\end{pmatrix}^{-1} \begin{pmatrix}
0 \\
5 
\end{pmatrix}.
\end{split}\]</div>
<p>First, compute <span class="math notranslate nohighlight">\(A_{11}^{-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A_{11}^{-1} = \frac{1}{1 \cdot 4 - 2 \cdot 3} \begin{pmatrix}
4 &amp; -2 \\
-3 &amp; 1
\end{pmatrix} = \begin{pmatrix}
-2 &amp; 1 \\
1.5 &amp; -0.5
\end{pmatrix}.
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A/A_{11} = 7 - \begin{pmatrix}
0 &amp; 6 
\end{pmatrix} \begin{pmatrix}
-2 &amp; 1 \\
1.5 &amp; -0.5
\end{pmatrix} \begin{pmatrix}
0 \\
5 
\end{pmatrix} = 7 + (6 \cdot 0.5 \cdot 5) = 22.
\end{split}\]</div>
<p><strong>E6.5.5</strong> The conditional mean of <span class="math notranslate nohighlight">\(X_1\)</span> given <span class="math notranslate nohighlight">\(X_2 = 3\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\mu_{1|2}(3) = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (3 - \bmu_2) = 1 + 1 \cdot \frac{1}{3} (3 - 2) = \frac{4}{3},
\]</div>
<p>using the formula for the conditional mean of multivariate Gaussians.</p>
<p><strong>E6.5.7</strong> The conditional distribution of <span class="math notranslate nohighlight">\(X_1\)</span> given <span class="math notranslate nohighlight">\(X_2 = 1\)</span> is Gaussian with mean <span class="math notranslate nohighlight">\(\mu_{1|2} = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (1 - \mu_2) = \frac{1}{2}\)</span> and variance <span class="math notranslate nohighlight">\(\bSigma_{1|2} = \bSigma_{11} - \bSigma_{12} \bSigma_{22}^{-1} \bSigma_{21} = \frac{7}{2}\)</span>.</p>
<p><strong>E6.5.9</strong> The distribution of <span class="math notranslate nohighlight">\(\bY\)</span> is Gaussian with mean vector <span class="math notranslate nohighlight">\(A\bmu = \begin{pmatrix} -3 \\ -3 \end{pmatrix}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(A\bSigma A^T = \begin{pmatrix} 8 &amp; 1 \\ 1 &amp; 6 \end{pmatrix}\)</span>.</p>
<p><strong>E6.5.11</strong> The mean of <span class="math notranslate nohighlight">\(Y_t\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbb{E}[Y_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \mathbb{E}[\bX_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = 1,
\end{split}\]</div>
<p>and the variance of <span class="math notranslate nohighlight">\(Y_t\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{Var}[Y_t] = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \mathrm{Cov}[\bX_t] \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = 2,
\end{split}\]</div>
<p>using the properties of linear-Gaussian systems.</p>
<p><strong>E6.5.13</strong> The innovation is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
e_t = Y_t - H \bmu_{\text{pred}} = 3 - \begin{pmatrix} 1 &amp; 0 \end{pmatrix} \begin{pmatrix} 3 \\ 1 \end{pmatrix} = 3 - 3 = 0.
\end{split}\]</div>
<p><strong>E6.5.15</strong> The updated state estimate is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\bmu_t = \bmu_{\text{pred}} + K_t e_t = \begin{pmatrix} 3 \\ 1 \end{pmatrix} + \begin{pmatrix} \frac{2}{3} \\ \frac{1}{3} \end{pmatrix} \cdot 0 = \begin{pmatrix} 3 \\ 1 \end{pmatrix}.
\end{split}\]</div>
&#13;

<h3><span class="section-number">6.7.1.5. </span>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define exponential families and give examples of common probability distributions that belong to this family.</p></li>
<li><p>Derive the maximum likelihood estimator for exponential families and explain its properties.</p></li>
<li><p>Prove that, under certain conditions, the maximum likelihood estimator is statistically consistent.</p></li>
<li><p>Formulate generalized linear models using exponential families and express linear and logistic regression as special cases.</p></li>
<li><p>Compute the gradient and Hessian of the negative log-likelihood for generalized linear models.</p></li>
<li><p>Interpret the moment-matching equations for the maximum likelihood estimator in generalized linear models.</p></li>
<li><p>Apply the multiplication rule, the law of total probability, and Bayes’ rule to solve problems involving conditional probabilities.</p></li>
<li><p>Calculate conditional probability mass functions and conditional expectations for discrete random variables.</p></li>
<li><p>Define conditional independence and express it mathematically in terms of conditional probabilities.</p></li>
<li><p>Differentiate between the fork, chain, and collider configurations in graphical models representing conditional independence relations.</p></li>
<li><p>Derive the joint probability distribution for the Naive Bayes model under the assumption of conditional independence.</p></li>
<li><p>Implement maximum likelihood estimation to fit the parameters of the Naive Bayes model.</p></li>
<li><p>Apply the Naive Bayes model for prediction and evaluate its accuracy.</p></li>
<li><p>Implement Laplace smoothing to address the issue of unseen words in the training data when fitting a Naive Bayes model.</p></li>
<li><p>Apply the Naive Bayes model to perform sentiment analysis on a real-world dataset and interpret the results.</p></li>
<li><p>Define mixtures as convex combinations of distributions and express the probability distribution of a mixture model using the law of total probability.</p></li>
<li><p>Identify examples of mixture models, such as mixtures of multinomials and Gaussian mixture models, and recognize their probability density functions.</p></li>
<li><p>Explain the concept of marginalizing out an unobserved random variable in the context of mixture models.</p></li>
<li><p>Formulate the objective function for parameter estimation in mixtures of multivariate Bernoullis using the negative log-likelihood.</p></li>
<li><p>Describe the majorization-minimization principle and its application in the Expectation-Maximization (EM) algorithm.</p></li>
<li><p>Derive the E-step and M-step updates for the EM algorithm in the context of mixtures of multivariate Bernoullis.</p></li>
<li><p>Implement the EM algorithm for mixtures of multivariate Bernoullis and apply it to a real-world dataset, such as clustering handwritten digits from the MNIST dataset.</p></li>
<li><p>Identify and address numerical issues that may arise during the implementation of the EM algorithm, such as underflow, by applying techniques like the log-sum-exp trick.</p></li>
<li><p>Define block matrices and the Schur complement, and demonstrate their properties through examples and proofs.</p></li>
<li><p>Derive the marginal and conditional distributions of multivariate Gaussians using the properties of block matrices and the Schur complement.</p></li>
<li><p>Describe the linear-Gaussian system model and its components, including the state evolution and observation processes.</p></li>
<li><p>Explain the purpose and key steps of the Kalman filter algorithm, including the prediction and update steps.</p></li>
<li><p>Implement the Kalman filter algorithm in code, given the state evolution and observation models, and the initial state distribution.</p></li>
<li><p>Apply the Kalman filter to a location tracking problem, and interpret the results in terms of the estimated object path and the algorithm’s performance.</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(\aleph\)</span></p>
&#13;

<h2><span class="section-number">6.7.2. </span>Additional sections<a class="headerlink" href="#additional-sections" title="Link to this heading">#</a></h2>
<section id="sentiment-analysis">
<h3><span class="section-number">6.7.2.1. </span>Sentiment analysis<a class="headerlink" href="#sentiment-analysis" title="Link to this heading">#</a></h3>
<p>As an application of the Naive Bayes model, we consider the task of sentiment analysis, which is a classification problem. We use a dataset available <a class="reference external" href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment">here</a>. Quoting from there:</p>
<blockquote>
<div><p>A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as “late flight” or “rude service”).</p>
</div></blockquote>
<p>We first load a cleaned-up version of the data and look at its summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'twitter-sentiment.csv'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'latin-1'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>time</th>
      <th>user</th>
      <th>sentiment</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2/24/15 11:35</td>
      <td>cairdin</td>
      <td>neutral</td>
      <td>@VirginAmerica What @dhepburn said.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2/24/15 11:15</td>
      <td>jnardino</td>
      <td>positive</td>
      <td>@VirginAmerica plus you've added commercials t...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2/24/15 11:15</td>
      <td>yvonnalynn</td>
      <td>neutral</td>
      <td>@VirginAmerica I didn't today... Must mean I n...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2/24/15 11:15</td>
      <td>jnardino</td>
      <td>negative</td>
      <td>@VirginAmerica it's really aggressive to blast...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2/24/15 11:14</td>
      <td>jnardino</td>
      <td>negative</td>
      <td>@VirginAmerica and it's a really big bad thing...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>14640
</pre></div>
</div>
</div>
</div>
<p>We extract the text information in this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">corpus</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'text'</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0                      @VirginAmerica What @dhepburn said.
1        @VirginAmerica plus you've added commercials t...
2        @VirginAmerica I didn't today... Must mean I n...
3        @VirginAmerica it's really aggressive to blast...
4        @VirginAmerica and it's a really big bad thing...
                               ...                        
14635    @AmericanAir thank you we got on a different f...
14636    @AmericanAir leaving over 20 minutes Late Flig...
14637    @AmericanAir Please bring American Airlines to...
14638    @AmericanAir you have my money, you change my ...
14639    @AmericanAir we have 8 ppl so we need 2 know h...
Name: text, Length: 14640, dtype: object
</pre></div>
</div>
</div>
</div>
<p>Next, we convert our dataset into a matrix by creating a document-term matrix using <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform"><code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text.CountVectorizer</span></code></a>. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Document-term_matrix">Wikipedia</a>:</p>
<blockquote>
<div><p>A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.</p>
</div></blockquote>
<p>By default, it first preprocesses the data. In particular, it lower-cases all words and removes punctuation. A more careful pre-procsseing would also include stemming, although we do not do this here. Regarding the latter, quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Stemming">Wikipedia</a>:</p>
<blockquote>
<div><p>In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. […] A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer. […] A stemmer for English operating on the stem cat should identify such strings as cats, catlike, and catty. A stemming algorithm might also reduce the words fishing, fished, and fisher to the stem fish. The stem need not be a word, for example the Porter algorithm reduces, argue, argued, argues, arguing, and argus to the stem argu.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">count</span><span class="p">[:</span><span class="mi">2</span><span class="p">,])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>  (0, 14376)	1
  (0, 14654)	1
  (0, 4872)	1
  (0, 11739)	1
  (1, 14376)	1
  (1, 10529)	1
  (1, 15047)	1
  (1, 14296)	1
  (1, 2025)	1
  (1, 4095)	1
  (1, 13425)	1
  (1, 13216)	1
  (1, 5733)	1
  (1, 13021)	1
</pre></div>
</div>
</div>
</div>
<p>The list of all terms used can be accessed as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">terms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['00' '000' '000114' ... 'ü_ù__' 'üi' 'ýã']
</pre></div>
</div>
</div>
</div>
<p>Because of our use of the multivariate Bernoulli naive Bayes model, it will be more convenient to work with a variant of the document-term matrix where each word is either present or absent. Note that, in the context of tweet data which are very short documents with likely little word repetition, there is probably not much difference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>  (0, 4872)	1
  (0, 11739)	1
  (0, 14376)	1
  (0, 14654)	1
  (1, 2025)	1
  (1, 4095)	1
  (1, 5733)	1
  (1, 10529)	1
  (1, 13021)	1
  (1, 13216)	1
  (1, 13425)	1
  (1, 14296)	1
  (1, 14376)	1
  (1, 15047)	1
</pre></div>
</div>
</div>
</div>
<p>We also extract the labels (<code class="docutils literal notranslate"><span class="pre">neutral</span></code>, <code class="docutils literal notranslate"><span class="pre">postive</span></code>, <code class="docutils literal notranslate"><span class="pre">negative</span></code>) from the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['neutral' 'positive' 'neutral' ... 'neutral' 'negative' 'neutral']
</pre></div>
</div>
</div>
</div>
<p>We split the data into a training set and a test set using <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"><code class="docutils literal notranslate"><span class="pre">sklearn.model_selection.train_test_split</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">535</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use the Naive Bayes method. We first construct the matrix <span class="math notranslate nohighlight">\(N_{k,m}\)</span> and the vector <span class="math notranslate nohighlight">\(N_k\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">label_set</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'positive'</span><span class="p">,</span> <span class="s1">'negative'</span><span class="p">,</span> <span class="s1">'neutral'</span><span class="p">]</span>
<span class="n">N_km</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">label_set</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">terms</span><span class="p">)))</span>
<span class="n">N_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label_set</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">label_set</span><span class="p">):</span>
    <span class="n">k_rows</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">N_km</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">k_rows</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">N_k</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">k_rows</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Above, the <a class="reference external" href="https://docs.python.org/3/library/functions.html#enumerate"><code class="docutils literal notranslate"><span class="pre">enumerate</span></code></a> function provides both the index and the value for each item in the <code class="docutils literal notranslate"><span class="pre">label_set</span></code> list during the loop. This allows the code to use the label’s value (<code class="docutils literal notranslate"><span class="pre">k</span></code>) and its numerical position (<code class="docutils literal notranslate"><span class="pre">i</span></code>) at the same time.</p>
<p>We are ready to train on the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">nb_fit_table</span><span class="p">(</span><span class="n">N_km</span><span class="p">,</span> <span class="n">N_k</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.16071022 0.62849989 0.21078989]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">p_km</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0.00047192 0.00330345 0.00047192 ... 0.00094384 0.00094384 0.00047192]
 [0.00144858 0.00229358 0.00012071 ... 0.00012071 0.00012071 0.00024143]
 [0.00071968 0.00143937 0.00071968 ... 0.00035984 0.00035984 0.00071968]]
</pre></div>
</div>
</div>
</div>
<p>Next, we plot the vector <span class="math notranslate nohighlight">\(p_{k,m}\)</span> for each label <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span><span class="n">ax2</span><span class="p">,</span><span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e75f50e0059a06f686e844c95d499bf2dbc3ef78367a3fc95c0ed8f7df2202b1.png" src="../Images/dc490453944015ef54f86c78956d449e.png" data-original-src="https://mmids-textbook.github.io/_images/e75f50e0059a06f686e844c95d499bf2dbc3ef78367a3fc95c0ed8f7df2202b1.png"/>
</div>
</div>
<p>We can compute a prediction on the test tweets. For example, for the 5th test tweet:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="mi">4</span><span class="p">,:]</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">label_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>'positive'
</pre></div>
</div>
</div>
</div>
<p>The following computes the overall accuracy over the test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">mmids</span><span class="o">.</span><span class="n">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">label_set</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="n">acc</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">acc</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0.7670765027322405
</pre></div>
</div>
</div>
</div>
<p>To get a better understanding of the differences uncovered by Naive Bayes between the different labels, we identify words that are particularly common in one label, but not on the other. Recall that label <code class="docutils literal notranslate"><span class="pre">1</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">positive</span></code> while label <code class="docutils literal notranslate"><span class="pre">2</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">negative</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pos_terms</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">&gt;</span> <span class="mf">0.02</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">&lt;</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">[</span><span class="n">pos_terms</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['_ù' 'amazing' 'appreciate' 'awesome' 'best' 'crew' 'first' 'flying'
 'good' 'great' 'll' 'love' 'made' 'much' 'new' 'response' 'see' 'thank'
 'thx' 'very' 'well' 'work']
</pre></div>
</div>
</div>
</div>
<p>One notices that many positive words do appear in this list: <code class="docutils literal notranslate"><span class="pre">awesome</span></code>, <code class="docutils literal notranslate"><span class="pre">best</span></code>, <code class="docutils literal notranslate"><span class="pre">great</span></code>, <code class="docutils literal notranslate"><span class="pre">love</span></code>, <code class="docutils literal notranslate"><span class="pre">thank</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">neg_terms</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">&gt;</span> <span class="mf">0.02</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">&lt;</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">[</span><span class="n">neg_terms</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['about' 'after' 'again' 'agent' 'airport' 'am' 'another' 'any' 'bag'
 'bags' 'because' 'by' 'call' 'cancelled' 'change' 'check' 'days' 'delay'
 'delayed' 'did' 'don' 'due' 'even' 'ever' 'flighted' 'flightled' 'go'
 'going' 'has' 'here' 'hold' 'hour' 'hours' 'how' 'hrs' 'if' 'last' 'late'
 'lost' 'luggage' 'make' 'minutes' 'more' 'need' 'never' 'off' 'only' 'or'
 'over' 'people' 'phone' 'really' 'should' 'sitting' 'someone' 'still'
 'take' 'than' 'them' 'then' 'told' 'trying' 've' 'wait' 'waiting' 'want'
 'weather' 'what' 'when' 'why' 'worst']
</pre></div>
</div>
</div>
</div>
<p>This time, we notice: <code class="docutils literal notranslate"><span class="pre">bag</span></code>, <code class="docutils literal notranslate"><span class="pre">cancelled</span></code>, <code class="docutils literal notranslate"><span class="pre">delayed</span></code>, <code class="docutils literal notranslate"><span class="pre">hours</span></code>, <code class="docutils literal notranslate"><span class="pre">phone</span></code>.</p>
<p><strong>CHAT &amp; LEARN</strong> The bag-of-words representation used in the sentiment analysis example is a simple but limited way to represent text data. More advanced representations such as word embeddings and transformer models can capture more semantic information. Ask your favorite AI chatbot to explain these representations and how they can be used for text classification tasks. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
</section>
<section id="kalman-filtering-missing-data">
<h3><span class="section-number">6.7.2.2. </span>Kalman filtering: missing data<a class="headerlink" href="#kalman-filtering-missing-data" title="Link to this heading">#</a></h3>
<p>In Kalman filtering, we can also allow for the possibility that some observations are missing. Imagine for instance losing GPS signal while going through a tunnel. The recursions above are still valid, with the only modification that the <em>Update</em> equations involving <span class="math notranslate nohighlight">\(\bY_t\)</span> are dropped at those times <span class="math notranslate nohighlight">\(t\)</span> where there is no observation. In Numpy, we can use <a class="reference external" href="https://numpy.org/doc/stable/reference/constants.html#numpy.nan"><code class="docutils literal notranslate"><span class="pre">NaN</span></code></a> to indicate the lack of observation. (Alternatively, one can use the <a class="reference external" href="https://numpy.org/doc/stable/reference/maskedarray.generic.html">numpy.ma</a> module.)</p>
<p>We use a same sample path as above, but mask observations at times <span class="math notranslate nohighlight">\(t=10,\ldots,20\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">ss</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">os</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span> 
<span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="n">Q</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span>
<span class="n">R</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">os</span><span class="p">))</span>
<span class="n">init_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">init_Sig</span> <span class="o">=</span> <span class="n">Q</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">lgSamplePath</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">ss</span><span class="p">,</span> <span class="n">os</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">init_mu</span><span class="p">,</span> <span class="n">init_Sig</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
</pre></div>
</div>
</div>
</div>
<p>Here is the sample we are aiming to infer.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'dotted'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png" src="../Images/23e83372ecc5a5cde94ad7f021c21a15.png" data-original-src="https://mmids-textbook.github.io/_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png"/>
</div>
</div>
<p>We modify the recursion accordingly, that is, skip the <em>Update</em> step when there is no observation to use for the update.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">kalmanUpdate</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">mu_prev</span><span class="p">,</span> <span class="n">Sig_prev</span><span class="p">):</span>
    <span class="n">mu_pred</span> <span class="o">=</span> <span class="n">F</span> <span class="o">@</span> <span class="n">mu_prev</span>
    <span class="n">Sig_pred</span> <span class="o">=</span> <span class="n">F</span> <span class="o">@</span> <span class="n">Sig_prev</span> <span class="o">@</span> <span class="n">F</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">Q</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y_t</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y_t</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">mu_pred</span><span class="p">,</span> <span class="n">Sig_pred</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">e_t</span> <span class="o">=</span> <span class="n">y_t</span> <span class="o">-</span> <span class="n">H</span> <span class="o">@</span> <span class="n">mu_pred</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">H</span> <span class="o">@</span> <span class="n">Sig_pred</span> <span class="o">@</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">R</span>
        <span class="n">Sinv</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">Sig_pred</span> <span class="o">@</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Sinv</span>
        <span class="n">mu_new</span> <span class="o">=</span> <span class="n">mu_pred</span> <span class="o">+</span> <span class="n">K</span> <span class="o">@</span> <span class="n">e_t</span>
        <span class="n">Sig_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span> <span class="o">-</span> <span class="n">K</span> <span class="o">@</span> <span class="n">H</span><span class="p">)</span> <span class="o">@</span> <span class="n">Sig_pred</span>
        <span class="k">return</span> <span class="n">mu_new</span><span class="p">,</span> <span class="n">Sig_new</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">init_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">init_Sig</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kalmanFilter</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span> <span class="n">os</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">init_mu</span><span class="p">,</span> <span class="n">init_Sig</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'s'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'dotted'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png" src="../Images/6633cdad5605633dacdc68a6a28fde12.png" data-original-src="https://mmids-textbook.github.io/_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png"/>
</div>
</div>
</section>
<section id="cholesky-decomposition">
<h3><span class="section-number">6.7.2.3. </span>Cholesky decomposition<a class="headerlink" href="#cholesky-decomposition" title="Link to this heading">#</a></h3>
<p>In  this section, we derive an important matrix factorization and apply it to generating multivariate Gaussians. We also revisit the least-squares problem. We begin with the motivation.</p>
<p><strong>Generating multivariate Gaussians</strong> Suppose we want to generate samples from a multivariate Gaussian <span class="math notranslate nohighlight">\(\bX \sim N_d(\bmu, \bSigma)\)</span> with given mean vector <span class="math notranslate nohighlight">\(\bmu \in \mathbb{R}^d\)</span> and positive definite covariance matrix <span class="math notranslate nohighlight">\(\bSigma  \in \mathbb{R}^{d \times d}\)</span>. Of course, in Numpy, we could use <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.multivariate_normal</span></code></a>. But what is behind it? More precisely, suppose we have access to unlimited samples <span class="math notranslate nohighlight">\(U_1, U_2, U_3, etc.\)</span> from uniform random variables in <span class="math notranslate nohighlight">\([0,1]\)</span>. How do we transform them to obtain samples from <span class="math notranslate nohighlight">\(N_d(\bmu, \bSigma)\)</span>.</p>
<p>We start with the simplest case: <span class="math notranslate nohighlight">\(d=1\)</span>, <span class="math notranslate nohighlight">\(\mu = 0\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span>. That is, we first generate a univariate standard Normal. We have seen a recipe for doing this before, the inverse transform sampling method. Specifically, recall that the cumulative distribution function (CDF) of a random variable <span class="math notranslate nohighlight">\(Z\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
F_Z(z) = \mathbb{P}[Z \leq z], \qquad \forall z \in \mathbb{R}.
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> be the interval where <span class="math notranslate nohighlight">\(F_Z(z) \in (0,1)\)</span> and assume that <span class="math notranslate nohighlight">\(F_X\)</span> is strictly increasing on <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span>. Let <span class="math notranslate nohighlight">\(U \sim \mathrm{U}[0,1]\)</span>. Then it can be shown that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[F_X^{-1}(U) \leq z] = F_X(z).
\]</div>
<p>So take <span class="math notranslate nohighlight">\(F_Z = \Phi\)</span>, the CDF of the standard Normal. Then <span class="math notranslate nohighlight">\(Z = \Phi^{-1}(U)\)</span> is <span class="math notranslate nohighlight">\(N(0,1)\)</span>.</p>
<p>How do we generate a <span class="math notranslate nohighlight">\(N(\mu, \sigma^2)\)</span> variable, for arbitrary <span class="math notranslate nohighlight">\(\mu \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>? We use the fact that the linear transformation of Gaussian is still Gaussian. In particular, if <span class="math notranslate nohighlight">\(Z \sim N(0,1)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
X = \mu + \sigma Z
\]</div>
<p>is <span class="math notranslate nohighlight">\(N(\mu, \sigma^2)\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> In Python, <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> can be accessed using <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html"><code class="docutils literal notranslate"><span class="pre">scipy.stats.norm.ppf</span></code></a>. We implement this next (with help from ChatGPT).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">):</span>
    <span class="c1"># Step 1: Generate uniform [0,1] random variables</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    
    <span class="c1"># Step 2: Apply the inverse CDF (ppf) of the standard normal distribution</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma2</span> <span class="o">*</span> <span class="n">Z</span>
</pre></div>
</div>
</div>
</div>
<p>We generate 1000 samples and plot the empirical distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Generate 1000 standard normal samples</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Plot the empirical PDF</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot histogram of the samples with density=True to normalize the histogram</span>
<span class="n">count</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">ignored</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>

<span class="c1"># Plot the theoretical standard normal PDF for comparison</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">'k'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Empirical PDF of Generated Samples'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Density'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png" src="../Images/c4d1f8c3493a1c88b3b5727f55a25bef.png" data-original-src="https://mmids-textbook.github.io/_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> It turns out there is a neat trick to generate <em>two</em> independent samples from <span class="math notranslate nohighlight">\(N(0,1)\)</span> that does not rely on access to <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span>. It is called the Box-Muller transform. Ask your favorite AI chatbot about it. Modify our code above to implement it. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p>We move on to the multivariate case. We proceed similarly as before. First, how do we generate a <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian with mean vector <span class="math notranslate nohighlight">\(\bmu = \mathbf{0}\)</span> and identity covariance matrix <span class="math notranslate nohighlight">\(\bSigma = I_{d \times d}\)</span>? Easy – it has <span class="math notranslate nohighlight">\(d\)</span> independent components, each of which is standard Normal. So letting <span class="math notranslate nohighlight">\(U_1, \ldots, U_d\)</span> be independent uniform <span class="math notranslate nohighlight">\([0,1]\)</span> variables, then</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}
= (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d))
\]</div>
<p>is <span class="math notranslate nohighlight">\(N(\mathbf{0}, I_{d \times d})\)</span>.</p>
<p>We now seek to generate a multivariate Gaussian with arbitrary mean vector <span class="math notranslate nohighlight">\(\bmu\)</span> and positive definite covariance matrix <span class="math notranslate nohighlight">\(\bSigma\)</span>. Again, we use a linear transformation</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}
= \mathbf{a} + A \mathbf{Z}.
\]</div>
<p>What are the right choices for <span class="math notranslate nohighlight">\(a \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span>? We need to match the obtained and desired mean and covariance. We start with the mean. By linearity of expectation,</p>
<div class="math notranslate nohighlight">
\[
\E[\mathbf{X}]
= \E[\mathbf{a} + A \mathbf{Z}]
= \mathbf{a} + A \,\E[\mathbf{Z}]
= \mathbf{a}.
\]</div>
<p>Hence we pick <span class="math notranslate nohighlight">\(\mathbf{a} := \bmu\)</span>.</p>
<p>As for the covariance, using the <em>Covariance of a Linear Transformation</em>, we get</p>
<div class="math notranslate nohighlight">
\[
\cov[\mathbf{X}]
= A \,\cov[\mathbf{Z}] A^T
= A A^T.
\]</div>
<p>Now we have a problem: what is a matrix <span class="math notranslate nohighlight">\(A\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
A A^T = \bSigma?
\]</div>
<p>In some sense, we are looking for a sort of “square root” of the covariance matrix. There are several ways of doing this. The Cholesky decomposition is one of them. We return to generating samples from <span class="math notranslate nohighlight">\(N(\bmu, \bSigma)\)</span> after introducing it.</p>
<p><strong>A matrix factorization</strong> Our key linear-algebraic result of this section is the following. The matrix factorization in the next theorem is called a Cholesky decomposition. It has many <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications">applications</a>.</p>
<p><strong>THEOREM</strong> <strong>(Cholesky Decomposition)</strong> Any positive definite matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times n}\)</span> can be factorized uniquely as</p>
<div class="math notranslate nohighlight">
\[
B = L L^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(L \in \mathbb{R}^{n \times n}\)</span> is a lower triangular matrix with positive entries on the diagonal. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>The proof is provided below. It is based on deriving an algorithm for computing the Cholesky decomposition: we grow <span class="math notranslate nohighlight">\(L\)</span> starting from its top-left corner by successively computing its next row based on the previously constructed submatrix. Note that, because <span class="math notranslate nohighlight">\(L\)</span> is lower triangular, it suffices to compute its elements on and below the diagonal. We first give the algorithm, then establish that it is well-defined.</p>
<p><strong>Figure:</strong> Access pattern (<a class="reference external" href="https://en.wikipedia.org/wiki/File:Chol.gif">Source</a>)</p>
<p><img alt="Access pattern" src="../Images/20de5b823040500b0d04511959484e97.png" data-original-src="https://upload.wikimedia.org/wikipedia/commons/b/be/Chol.gif"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>EXAMPLE:</strong> Before proceeeding with the general method, we give a small example to provide some intuition as to how it operates. We need a positive definite matrix. Consider the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A 
= 
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
0 &amp; -2 &amp; 1\\
0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>It has full column rank (why?). Recall that, in that case, the <span class="math notranslate nohighlight">\(B = A^T A\)</span> is positive definite.</p>
<p>That is, the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B 
=
A^T A 
= 
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
2 &amp; 8 &amp; 0\\
1 &amp; 0 &amp; 3
\end{pmatrix}
\end{split}\]</div>
<p>is positive definite.</p>
<p>Let <span class="math notranslate nohighlight">\(L = (\ell_{i,j})_{i,j=1}^3\)</span> be lower triangular. We seek to solve <span class="math notranslate nohighlight">\(L L^T = B\)</span> for the nonzero entries of <span class="math notranslate nohighlight">\(L\)</span>. Observe that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\ell_{1,1} &amp; 0 &amp; 0\\
\ell_{2,1} &amp; \ell_{2,2} &amp; 0\\
\ell_{3,1} &amp; \ell_{3,2} &amp; \ell_{3,3}
\end{pmatrix}
\begin{pmatrix}
\ell_{1,1} &amp; \ell_{2,1} &amp; \ell_{3,1}\\
0 &amp; \ell_{2,2} &amp; \ell_{3,2}\\
0 &amp; 0 &amp; \ell_{3,3}
\end{pmatrix}
=
\begin{pmatrix}
\ell_{1,1}^2 &amp; \ell_{1,1}\ell_{2,1} &amp; \ell_{1,1}\ell_{3,1}\\
\ell_{1,1}\ell_{2,1} &amp; \ell_{2,1}^2 + \ell_{2,2}^2 &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
\ell_{1,1}\ell_{3,1} &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} &amp; \ell_{3,1}^2 + \ell_{3,2}^2 + \ell_{3,3}
\end{pmatrix}.
\end{split}\]</div>
<p>The system</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\ell_{1,1}^2 &amp; \ell_{1,1}\ell_{2,1} &amp; \ell_{1,1}\ell_{3,1}\\
\ell_{1,1}\ell_{2,1} &amp; \ell_{2,1}^2 + \ell_{2,2}^2 &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
\ell_{1,1}\ell_{3,1} &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} &amp; \ell_{3,1}^2 + \ell_{3,2}^2 + \ell_{3,3}^2
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
2 &amp; 8 &amp; 0\\
1 &amp; 0 &amp; 3
\end{pmatrix}
\end{split}\]</div>
<p>turns out to be fairly simple to solve.</p>
<ol class="arabic simple">
<li><p>From the first entry, we get <span class="math notranslate nohighlight">\(\ell_{1,1} = 1\)</span> (where we took the positive solution to <span class="math notranslate nohighlight">\(\ell_{1,1}^2 = 1\)</span>).</p></li>
<li><p>Given that <span class="math notranslate nohighlight">\(\ell_{1,1}\)</span> is known, entry <span class="math notranslate nohighlight">\(\ell_{2,1}\)</span> is determined from <span class="math notranslate nohighlight">\(\ell_{1,1}\ell_{2,1} =2\)</span> in the first entry of the second row. That is, <span class="math notranslate nohighlight">\(\ell_{2,1} =2\)</span>. Then the second entry of the second row gives <span class="math notranslate nohighlight">\(\ell_{2,2}\)</span> through <span class="math notranslate nohighlight">\(\ell_{2,1}^2 + \ell_{2,2}^2  = 8\)</span>. So <span class="math notranslate nohighlight">\(\ell_{2,2} = 2\)</span> (again we take the positive solution).</p></li>
<li><p>We move to the third row. The first entry gives <span class="math notranslate nohighlight">\(\ell_{3,1} = 1\)</span>, the second entry gives <span class="math notranslate nohighlight">\(\ell_{3,2} = -1\)</span> and finally the third entry leads to <span class="math notranslate nohighlight">\(\ell_{3,3} = 1\)</span>.</p></li>
</ol>
<p>Hence we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L
=
\begin{pmatrix}
\ell_{1,1} &amp; 0 &amp; 0\\
\ell_{2,1} &amp; \ell_{2,2} &amp; 0\\
\ell_{3,1} &amp; \ell_{3,2} &amp; \ell_{3,3}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 0 &amp; 0\\
2 &amp; 2 &amp; 0\\
1 &amp; -1 &amp; 1
\end{pmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>To detail the computation of the Cholesky decomposition <span class="math notranslate nohighlight">\(L L^T\)</span> of <span class="math notranslate nohighlight">\(B\)</span>, we will need some notation. Write <span class="math notranslate nohighlight">\(B = (b_{i,j})_{i,j=1}^n\)</span> and <span class="math notranslate nohighlight">\(L = (\ell_{i,j})_{i,j=1}^n\)</span>. Let <span class="math notranslate nohighlight">\(L_{(k)} = (\ell_{i,j})_{i,j=1}^k\)</span> be the first <span class="math notranslate nohighlight">\(k\)</span> rows and columns of <span class="math notranslate nohighlight">\(L\)</span>, let <span class="math notranslate nohighlight">\(\bflambda_{(k)}^T = (\ell_{k,1},\ldots,\ell_{k,k-1})\)</span> be the row vector corresponding to the first <span class="math notranslate nohighlight">\(k-1\)</span> entries of row <span class="math notranslate nohighlight">\(k\)</span> of <span class="math notranslate nohighlight">\(L\)</span>, and let <span class="math notranslate nohighlight">\(\bfbeta_{(k)}^T = (b_{k,1},\ldots,b_{k,k-1})\)</span> be the row vector corresponding to the first <span class="math notranslate nohighlight">\(k-1\)</span> entries of row <span class="math notranslate nohighlight">\(k\)</span> of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>The strategy is to compute <span class="math notranslate nohighlight">\(L_{(1)}\)</span>, then <span class="math notranslate nohighlight">\(L_{(2)}\)</span>, then <span class="math notranslate nohighlight">\(L_{(3)}\)</span> and so on. With the notation above, <span class="math notranslate nohighlight">\(L_{(j)}\)</span> can be written in block form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L_{(j)}
=
\begin{pmatrix}
L_{(j-1)} &amp; \mathbf{0}\\
\bflambda_{(j)}^T &amp; \ell_{j,j}.
\end{pmatrix}
\end{split}\]</div>
<p>Hence, once <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> is known, in order to compute <span class="math notranslate nohighlight">\(L_{(j)}\)</span> one only needs <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span> and <span class="math notranslate nohighlight">\(\ell_{j,j}\)</span>. We show next that they satisfy easily solvable systems of equations.</p>
<p>We first note that the <span class="math notranslate nohighlight">\((1,1)\)</span> entry of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> implies that</p>
<div class="math notranslate nohighlight">
\[
\ell_{1,1}^2 = b_{1,1}.
\]</div>
<p>So we set</p>
<div class="math notranslate nohighlight">
\[
L_{(1)}
= \ell_{1,1}
= \sqrt{b_{1,1}}.
\]</div>
<p>For this step to be well-defined, it needs to be the case that <span class="math notranslate nohighlight">\(b_{1,1} &gt; 0\)</span>. It is easy to see that it follows from the positive definiteness of <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="math notranslate nohighlight">
\[
0 &lt; \langle \mathbf{e}_1, B \mathbf{e}_1\rangle = \mathbf{e}_1^T B_{\cdot,1} = b_{1,1}.
\]</div>
<p>Proceeding by induction, assume <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> has been constructed. The first <span class="math notranslate nohighlight">\(j-1\)</span> elements of the <span class="math notranslate nohighlight">\(j\)</span>-th row of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> translate into</p>
<div class="math notranslate nohighlight">
\[
L_{j,\cdot} (L^T)_{\cdot,1:j-1} = \bflambda_{(j)}^T L_{(j-1)}^T = \bfbeta_{(j)}^T,
\]</div>
<p>where <span class="math notranslate nohighlight">\((L^T)_{\cdot,1:j-1}\)</span> denotes the first <span class="math notranslate nohighlight">\(j-1\)</span> columns of <span class="math notranslate nohighlight">\(L^T\)</span>. In the first equality above, we used the fact that <span class="math notranslate nohighlight">\(L^T\)</span> is upper triangular. Taking a transpose, the resulting linear system of equations</p>
<div class="math notranslate nohighlight">
\[
L_{(j-1)} \bflambda_{(j)} = \bfbeta_{(j)}
\]</div>
<p>can be solved by forward substitution (since <span class="math notranslate nohighlight">\(\bfbeta_{(j)}\)</span> is part of the input and <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> was previously computed). The fact that this system has a unique solution (more specifically, that the diagonal entries of <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> are strictly positive) is established in the proof of the <em>Cholesky Decomposition Theorem</em>.</p>
<p>The <span class="math notranslate nohighlight">\((j,j)\)</span>-th entry of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> translates into</p>
<div class="math notranslate nohighlight">
\[
L_{j,\cdot} (L^T)_{\cdot,j} = \sum_{k=1}^j \ell_{j,k}^2 = b_{j,j},
\]</div>
<p>where again we used the fact that <span class="math notranslate nohighlight">\(L^T\)</span> is upper triangular. Since <span class="math notranslate nohighlight">\(\ell_{j,1}, \ldots, \ell_{j,j-1}\)</span> are the elements of <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span>, they have already been determined. So we can set</p>
<div class="math notranslate nohighlight">
\[
\ell_{j,j}
= \sqrt{b_{j,j} - \sum_{k=1}^{j-1} \ell_{j,k}^2}.
\]</div>
<p>The fact that we are taking the square root of a positive quantity is established in the proof of the <em>Cholesky Decomposition Theorem</em>. Finally, from <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span>, <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span>, and <span class="math notranslate nohighlight">\(\ell_{j,j}\)</span>, we construct <span class="math notranslate nohighlight">\(L_{(j)}\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement the algorithm above. In our naive implementation, we assume that <span class="math notranslate nohighlight">\(B\)</span> is positive definite, and therefore that all steps are well-defined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">],</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">])</span>
        <span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">L</span> 
</pre></div>
</div>
</div>
</div>
<p>Here is a simple example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[2. 1.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1.41421356 0.        ]
 [0.70710678 1.22474487]]
</pre></div>
</div>
</div>
</div>
<p>We can check that it produces the right factorization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">L</span> <span class="o">@</span> <span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[2. 1.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Proof of Cholesky decomposition theorem</strong> We give a proof of the <em>Cholesky Decomposition Theorem</em>.</p>
<p><em>Proof idea:</em> Assuming by induction that the upper-left corner of the matrix <span class="math notranslate nohighlight">\(B\)</span> has a Cholesky decomposition, one finds equations for the remaining row that can be solved uniquely by the properties established in the previous subsection.</p>
<p><em>Proof:</em> If <span class="math notranslate nohighlight">\(n=1\)</span>, we have shown previously that <span class="math notranslate nohighlight">\(b_{1,1} &gt; 0\)</span>, and hence we can take <span class="math notranslate nohighlight">\(L = [\ell_{1,1}]\)</span> where <span class="math notranslate nohighlight">\(\ell_{1,1} = \sqrt{b_{1,1}}\)</span>. Assuming the result holds for positive definite matrices in <span class="math notranslate nohighlight">\(\mathbb{R}^{(n-1) \times (n-1)}\)</span>, we first re-write <span class="math notranslate nohighlight">\(B = L L^T\)</span> in block form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
B_{11} &amp; \bfbeta_{12}\\
\bfbeta_{12}^T &amp; \beta_{22}
\end{pmatrix}
= 
\begin{pmatrix}
\Lambda_{11} &amp; \mathbf{0}\\
\bflambda_{12}^T &amp; \lambda_{22}
\end{pmatrix}
\begin{pmatrix}
\Lambda_{11}^T &amp; \bflambda_{12}\\
\mathbf{0}^T &amp; \lambda_{22}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(B_{11}, \Lambda_{11} \in \mathbb{R}^{n-1 \times n-1}\)</span>, <span class="math notranslate nohighlight">\(\bfbeta_{12}, \bflambda_{12} \in \mathbb{R}^{n-1}\)</span> and <span class="math notranslate nohighlight">\(\beta_{22}, \lambda_{22} \in \mathbb{R}\)</span>. By block matrix algebra, we get the system</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B_{11} = \Lambda_{11} \Lambda_{11}^T\\
\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\\
\beta_{22} = \bflambda_{12}^T \bflambda_{12} + \lambda_{22}^2.
\end{split}\]</div>
<p>By the <em>Principal Submatrices Lemma</em>, the principal submatrix <span class="math notranslate nohighlight">\(B_{11}\)</span> is positive definite. Hence, by induction, there is a unique lower-triangular matrix <span class="math notranslate nohighlight">\(\Lambda_{11}\)</span> with positive diagonal elements satisfying the first equation. We can then obtain <span class="math notranslate nohighlight">\(\bfbeta_{12}\)</span> from the second equation by forward substitution. And finally we get</p>
<div class="math notranslate nohighlight">
\[
\lambda_{22}
= \sqrt{\beta_{22} - \bflambda_{12}^T \bflambda_{12}}.
\]</div>
<p>We do have to check that the square root above exists. That is, we need to argue that the expression inside the square root is non-negative. In fact, for the claim to go through, we need it to be strictly positive. We notice that the expression inside the square root is in fact the Schur complement of the block <span class="math notranslate nohighlight">\(B_{11}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\beta_{22} - \bflambda_{12}^T \bflambda_{12}
&amp;= \beta_{22} - (\Lambda_{11}^{-1} \bfbeta_{12})^T (\Lambda_{11}^{-1} \bfbeta_{12})\\
&amp;= \beta_{22} - \bfbeta_{12}^T (\Lambda_{11}^{-1})^T \Lambda_{11}^{-1} \bfbeta_{12}\\
&amp;= \beta_{22} - \bfbeta_{12}^T (\Lambda_{11} \Lambda_{11}^T)^{-1} \bfbeta_{12}\\
&amp;= \beta_{22} - \bfbeta_{12}^T (B_{11})^{-1} \bfbeta_{12}
\end{align*}\]</div>
<p>where we used the equation <span class="math notranslate nohighlight">\(\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\)</span> on the first line, the identities <span class="math notranslate nohighlight">\((Q W)^{-1} = W^{-1} Q^{-1}\)</span> and <span class="math notranslate nohighlight">\((Q^T)^{-1} = (Q^{-1})^T\)</span> (see the exercise below) on the third line and the equation <span class="math notranslate nohighlight">\(B_{11} = \Lambda_{11} \Lambda_{11}^T\)</span> on the fourth line. By the <em>Schur Complement Lemma</em>, the Schur complement is positive definite. Because it is a scalar in this case, it is strictly positive (prove it!), which concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Back to multivariate Gaussians</strong> Returning to our motivation, we can generate samples from a <span class="math notranslate nohighlight">\(N(\bmu, \bSigma)\)</span> by first generating</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}
= (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d))
\]</div>
<p>where <span class="math notranslate nohighlight">\(U_1, \ldots, U_d\)</span> are independent uniform <span class="math notranslate nohighlight">\([0,1]\)</span> variables, then setting</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}
= \bmu + L \mathbf{Z},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bSigma = L L^T\)</span> is a Cholesky decomposition of <span class="math notranslate nohighlight">\(\bSigma\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement this method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">generate_multivariate_normal_samples_using_cholesky</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span><span class="p">):</span>

    <span class="c1"># Compute Cholesky decomposition</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">Sig</span><span class="p">)</span>
    
    <span class="c1"># Initialization</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        
            <span class="c1"># Generate standard normal vector</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="n">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Apply the inverse CDF (ppf) of the standard normal distribution</span>
            <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">L</span> <span class="o">@</span> <span class="n">Z</span> 
    
    <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
</div>
<p>We generate some samples as an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">Sig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">generate_multivariate_normal_samples_using_cholesky</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[-0.47926185  1.97223283  2.73780609]
 [-2.69005319 -4.19788834 -0.43130768]
 [ 0.41957285  3.91719212  2.08604427]
 [-2.11532949 -5.34557983  0.69521104]
 [-2.41203356 -1.84032486 -0.82207565]
 [-1.46121329  0.4821332   0.55005982]
 [-0.84981594  0.67074839  0.16360931]
 [-2.19097155 -1.98022929 -1.06365711]
 [-2.75113597 -3.47560492 -0.26607926]
 [ 0.130848    6.07312936 -0.08800829]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Using a Cholesky decomposition to solve the least squares problem</strong> Another application of the Cholesky decomposition is to solving the least squares problem. In this section, we restrict ourselves to the case where <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> has full column rank. By the <em>Least Squares and Positive Semidefiniteness Lemma</em>, we then have that <span class="math notranslate nohighlight">\(A^T A\)</span> is positive definite. By the <em>Cholesky Decomposition Theorem</em>, we can factorize this matrix as <span class="math notranslate nohighlight">\(A^T A = L L^T\)</span> where <span class="math notranslate nohighlight">\(L\)</span> is lower triangular with positive diagonal elements. The normal equations then reduce to</p>
<div class="math notranslate nohighlight">
\[
L L^T \mathbf{x} = A^T \mathbf{b}.
\]</div>
<p>This system can be solved in two steps. We first obtain the solution to</p>
<div class="math notranslate nohighlight">
\[
L \mathbf{z} = A^T \mathbf{b}
\]</div>
<p>by forward substitution. Then we obtain the solution to</p>
<div class="math notranslate nohighlight">
\[
L^T \mathbf{x} = \mathbf{z}
\]</div>
<p>by back-substitution. Note that <span class="math notranslate nohighlight">\(L^T\)</span> is indeed an upper triangular matrix.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement this algorithm below. In our naive implementation, we assume that <span class="math notranslate nohighlight">\(A\)</span> has full column rank, and therefore that all steps are well-defined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">ls_by_chol</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mmids</span><span class="o">.</span><span class="n">backsubs</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>Other applications of the Cholesky decomposition are briefly described <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications">here</a>.</p>
</section>
&#13;

<h3><span class="section-number">6.7.2.1. </span>Sentiment analysis<a class="headerlink" href="#sentiment-analysis" title="Link to this heading">#</a></h3>
<p>As an application of the Naive Bayes model, we consider the task of sentiment analysis, which is a classification problem. We use a dataset available <a class="reference external" href="https://www.kaggle.com/crowdflower/twitter-airline-sentiment">here</a>. Quoting from there:</p>
<blockquote>
<div><p>A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as “late flight” or “rude service”).</p>
</div></blockquote>
<p>We first load a cleaned-up version of the data and look at its summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'twitter-sentiment.csv'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s1">'latin-1'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>time</th>
      <th>user</th>
      <th>sentiment</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2/24/15 11:35</td>
      <td>cairdin</td>
      <td>neutral</td>
      <td>@VirginAmerica What @dhepburn said.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2/24/15 11:15</td>
      <td>jnardino</td>
      <td>positive</td>
      <td>@VirginAmerica plus you've added commercials t...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2/24/15 11:15</td>
      <td>yvonnalynn</td>
      <td>neutral</td>
      <td>@VirginAmerica I didn't today... Must mean I n...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2/24/15 11:15</td>
      <td>jnardino</td>
      <td>negative</td>
      <td>@VirginAmerica it's really aggressive to blast...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2/24/15 11:14</td>
      <td>jnardino</td>
      <td>negative</td>
      <td>@VirginAmerica and it's a really big bad thing...</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>14640
</pre></div>
</div>
</div>
</div>
<p>We extract the text information in this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">corpus</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'text'</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0                      @VirginAmerica What @dhepburn said.
1        @VirginAmerica plus you've added commercials t...
2        @VirginAmerica I didn't today... Must mean I n...
3        @VirginAmerica it's really aggressive to blast...
4        @VirginAmerica and it's a really big bad thing...
                               ...                        
14635    @AmericanAir thank you we got on a different f...
14636    @AmericanAir leaving over 20 minutes Late Flig...
14637    @AmericanAir Please bring American Airlines to...
14638    @AmericanAir you have my money, you change my ...
14639    @AmericanAir we have 8 ppl so we need 2 know h...
Name: text, Length: 14640, dtype: object
</pre></div>
</div>
</div>
</div>
<p>Next, we convert our dataset into a matrix by creating a document-term matrix using <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform"><code class="docutils literal notranslate"><span class="pre">sklearn.feature_extraction.text.CountVectorizer</span></code></a>. Quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Document-term_matrix">Wikipedia</a>:</p>
<blockquote>
<div><p>A document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms.</p>
</div></blockquote>
<p>By default, it first preprocesses the data. In particular, it lower-cases all words and removes punctuation. A more careful pre-procsseing would also include stemming, although we do not do this here. Regarding the latter, quoting <a class="reference external" href="https://en.wikipedia.org/wiki/Stemming">Wikipedia</a>:</p>
<blockquote>
<div><p>In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. […] A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer. […] A stemmer for English operating on the stem cat should identify such strings as cats, catlike, and catty. A stemming algorithm might also reduce the words fishing, fished, and fisher to the stem fish. The stem need not be a word, for example the Porter algorithm reduces, argue, argued, argues, arguing, and argus to the stem argu.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span>
<span class="n">count</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">count</span><span class="p">[:</span><span class="mi">2</span><span class="p">,])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>  (0, 14376)	1
  (0, 14654)	1
  (0, 4872)	1
  (0, 11739)	1
  (1, 14376)	1
  (1, 10529)	1
  (1, 15047)	1
  (1, 14296)	1
  (1, 2025)	1
  (1, 4095)	1
  (1, 13425)	1
  (1, 13216)	1
  (1, 5733)	1
  (1, 13021)	1
</pre></div>
</div>
</div>
</div>
<p>The list of all terms used can be accessed as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">terms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">vectorizer</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['00' '000' '000114' ... 'ü_ù__' 'üi' 'ýã']
</pre></div>
</div>
</div>
</div>
<p>Because of our use of the multivariate Bernoulli naive Bayes model, it will be more convenient to work with a variant of the document-term matrix where each word is either present or absent. Note that, in the context of tweet data which are very short documents with likely little word repetition, there is probably not much difference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>  (0, 4872)	1
  (0, 11739)	1
  (0, 14376)	1
  (0, 14654)	1
  (1, 2025)	1
  (1, 4095)	1
  (1, 5733)	1
  (1, 10529)	1
  (1, 13021)	1
  (1, 13216)	1
  (1, 13425)	1
  (1, 14296)	1
  (1, 14376)	1
  (1, 15047)	1
</pre></div>
</div>
</div>
</div>
<p>We also extract the labels (<code class="docutils literal notranslate"><span class="pre">neutral</span></code>, <code class="docutils literal notranslate"><span class="pre">postive</span></code>, <code class="docutils literal notranslate"><span class="pre">negative</span></code>) from the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'sentiment'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['neutral' 'positive' 'neutral' ... 'neutral' 'negative' 'neutral']
</pre></div>
</div>
</div>
</div>
<p>We split the data into a training set and a test set using <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"><code class="docutils literal notranslate"><span class="pre">sklearn.model_selection.train_test_split</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">535</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We use the Naive Bayes method. We first construct the matrix <span class="math notranslate nohighlight">\(N_{k,m}\)</span> and the vector <span class="math notranslate nohighlight">\(N_k\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">label_set</span> <span class="o">=</span> <span class="p">[</span><span class="s1">'positive'</span><span class="p">,</span> <span class="s1">'negative'</span><span class="p">,</span> <span class="s1">'neutral'</span><span class="p">]</span>
<span class="n">N_km</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">label_set</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">terms</span><span class="p">)))</span>
<span class="n">N_k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">label_set</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">label_set</span><span class="p">):</span>
    <span class="n">k_rows</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">N_km</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">k_rows</span><span class="p">,</span> <span class="p">:]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">N_k</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">k_rows</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Above, the <a class="reference external" href="https://docs.python.org/3/library/functions.html#enumerate"><code class="docutils literal notranslate"><span class="pre">enumerate</span></code></a> function provides both the index and the value for each item in the <code class="docutils literal notranslate"><span class="pre">label_set</span></code> list during the loop. This allows the code to use the label’s value (<code class="docutils literal notranslate"><span class="pre">k</span></code>) and its numerical position (<code class="docutils literal notranslate"><span class="pre">i</span></code>) at the same time.</p>
<p>We are ready to train on the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">nb_fit_table</span><span class="p">(</span><span class="n">N_km</span><span class="p">,</span> <span class="n">N_k</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pi_k</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[0.16071022 0.62849989 0.21078989]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">p_km</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[0.00047192 0.00330345 0.00047192 ... 0.00094384 0.00094384 0.00047192]
 [0.00144858 0.00229358 0.00012071 ... 0.00012071 0.00012071 0.00024143]
 [0.00071968 0.00143937 0.00071968 ... 0.00035984 0.00035984 0.00071968]]
</pre></div>
</div>
</div>
</div>
<p>Next, we plot the vector <span class="math notranslate nohighlight">\(p_{k,m}\)</span> for each label <span class="math notranslate nohighlight">\(k\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span><span class="n">ax2</span><span class="p">,</span><span class="n">ax3</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">ax3</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">2</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/e75f50e0059a06f686e844c95d499bf2dbc3ef78367a3fc95c0ed8f7df2202b1.png" src="../Images/dc490453944015ef54f86c78956d449e.png" data-original-src="https://mmids-textbook.github.io/_images/e75f50e0059a06f686e844c95d499bf2dbc3ef78367a3fc95c0ed8f7df2202b1.png"/>
</div>
</div>
<p>We can compute a prediction on the test tweets. For example, for the 5th test tweet:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mmids</span><span class="o">.</span><span class="n">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="mi">4</span><span class="p">,:]</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">label_set</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>'positive'
</pre></div>
</div>
</div>
</div>
<p>The following computes the overall accuracy over the test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">acc</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">mmids</span><span class="o">.</span><span class="n">nb_predict</span><span class="p">(</span><span class="n">pi_k</span><span class="p">,</span> <span class="n">p_km</span><span class="p">,</span> <span class="n">X_test</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">label_set</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="n">acc</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="n">acc</span><span class="o">/</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0.7670765027322405
</pre></div>
</div>
</div>
</div>
<p>To get a better understanding of the differences uncovered by Naive Bayes between the different labels, we identify words that are particularly common in one label, but not on the other. Recall that label <code class="docutils literal notranslate"><span class="pre">1</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">positive</span></code> while label <code class="docutils literal notranslate"><span class="pre">2</span></code> corresponds to <code class="docutils literal notranslate"><span class="pre">negative</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">pos_terms</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">&gt;</span> <span class="mf">0.02</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">&lt;</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">[</span><span class="n">pos_terms</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['_ù' 'amazing' 'appreciate' 'awesome' 'best' 'crew' 'first' 'flying'
 'good' 'great' 'll' 'love' 'made' 'much' 'new' 'response' 'see' 'thank'
 'thx' 'very' 'well' 'work']
</pre></div>
</div>
</div>
</div>
<p>One notices that many positive words do appear in this list: <code class="docutils literal notranslate"><span class="pre">awesome</span></code>, <code class="docutils literal notranslate"><span class="pre">best</span></code>, <code class="docutils literal notranslate"><span class="pre">great</span></code>, <code class="docutils literal notranslate"><span class="pre">love</span></code>, <code class="docutils literal notranslate"><span class="pre">thank</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">neg_terms</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">1</span><span class="p">,:]</span> <span class="o">&gt;</span> <span class="mf">0.02</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">p_km</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">&lt;</span> <span class="mf">0.02</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">terms</span><span class="p">[</span><span class="n">neg_terms</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>['about' 'after' 'again' 'agent' 'airport' 'am' 'another' 'any' 'bag'
 'bags' 'because' 'by' 'call' 'cancelled' 'change' 'check' 'days' 'delay'
 'delayed' 'did' 'don' 'due' 'even' 'ever' 'flighted' 'flightled' 'go'
 'going' 'has' 'here' 'hold' 'hour' 'hours' 'how' 'hrs' 'if' 'last' 'late'
 'lost' 'luggage' 'make' 'minutes' 'more' 'need' 'never' 'off' 'only' 'or'
 'over' 'people' 'phone' 'really' 'should' 'sitting' 'someone' 'still'
 'take' 'than' 'them' 'then' 'told' 'trying' 've' 'wait' 'waiting' 'want'
 'weather' 'what' 'when' 'why' 'worst']
</pre></div>
</div>
</div>
</div>
<p>This time, we notice: <code class="docutils literal notranslate"><span class="pre">bag</span></code>, <code class="docutils literal notranslate"><span class="pre">cancelled</span></code>, <code class="docutils literal notranslate"><span class="pre">delayed</span></code>, <code class="docutils literal notranslate"><span class="pre">hours</span></code>, <code class="docutils literal notranslate"><span class="pre">phone</span></code>.</p>
<p><strong>CHAT &amp; LEARN</strong> The bag-of-words representation used in the sentiment analysis example is a simple but limited way to represent text data. More advanced representations such as word embeddings and transformer models can capture more semantic information. Ask your favorite AI chatbot to explain these representations and how they can be used for text classification tasks. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
&#13;

<h3><span class="section-number">6.7.2.2. </span>Kalman filtering: missing data<a class="headerlink" href="#kalman-filtering-missing-data" title="Link to this heading">#</a></h3>
<p>In Kalman filtering, we can also allow for the possibility that some observations are missing. Imagine for instance losing GPS signal while going through a tunnel. The recursions above are still valid, with the only modification that the <em>Update</em> equations involving <span class="math notranslate nohighlight">\(\bY_t\)</span> are dropped at those times <span class="math notranslate nohighlight">\(t\)</span> where there is no observation. In Numpy, we can use <a class="reference external" href="https://numpy.org/doc/stable/reference/constants.html#numpy.nan"><code class="docutils literal notranslate"><span class="pre">NaN</span></code></a> to indicate the lack of observation. (Alternatively, one can use the <a class="reference external" href="https://numpy.org/doc/stable/reference/maskedarray.generic.html">numpy.ma</a> module.)</p>
<p>We use a same sample path as above, but mask observations at times <span class="math notranslate nohighlight">\(t=10,\ldots,20\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">ss</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">os</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span> 
<span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="n">Q</span> <span class="o">=</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span>
<span class="n">R</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">os</span><span class="p">))</span>
<span class="n">init_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">init_Sig</span> <span class="o">=</span> <span class="n">Q</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">lgSamplePath</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">ss</span><span class="p">,</span> <span class="n">os</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">init_mu</span><span class="p">,</span> <span class="n">init_Sig</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">):</span>
    <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
    <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span>
</pre></div>
</div>
</div>
</div>
<p>Here is the sample we are aiming to infer.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'dotted'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png" src="../Images/23e83372ecc5a5cde94ad7f021c21a15.png" data-original-src="https://mmids-textbook.github.io/_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png"/>
</div>
</div>
<p>We modify the recursion accordingly, that is, skip the <em>Update</em> step when there is no observation to use for the update.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">kalmanUpdate</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">mu_prev</span><span class="p">,</span> <span class="n">Sig_prev</span><span class="p">):</span>
    <span class="n">mu_pred</span> <span class="o">=</span> <span class="n">F</span> <span class="o">@</span> <span class="n">mu_prev</span>
    <span class="n">Sig_pred</span> <span class="o">=</span> <span class="n">F</span> <span class="o">@</span> <span class="n">Sig_prev</span> <span class="o">@</span> <span class="n">F</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">Q</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y_t</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y_t</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">return</span> <span class="n">mu_pred</span><span class="p">,</span> <span class="n">Sig_pred</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">e_t</span> <span class="o">=</span> <span class="n">y_t</span> <span class="o">-</span> <span class="n">H</span> <span class="o">@</span> <span class="n">mu_pred</span>
        <span class="n">S</span> <span class="o">=</span> <span class="n">H</span> <span class="o">@</span> <span class="n">Sig_pred</span> <span class="o">@</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">R</span>
        <span class="n">Sinv</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">Sig_pred</span> <span class="o">@</span> <span class="n">H</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">Sinv</span>
        <span class="n">mu_new</span> <span class="o">=</span> <span class="n">mu_pred</span> <span class="o">+</span> <span class="n">K</span> <span class="o">@</span> <span class="n">e_t</span>
        <span class="n">Sig_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span> <span class="o">-</span> <span class="n">K</span> <span class="o">@</span> <span class="n">H</span><span class="p">)</span> <span class="o">@</span> <span class="n">Sig_pred</span>
        <span class="k">return</span> <span class="n">mu_new</span><span class="p">,</span> <span class="n">Sig_new</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">init_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">init_Sig</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">ss</span><span class="p">))</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kalmanFilter</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span> <span class="n">os</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">init_mu</span><span class="p">,</span> <span class="n">init_Sig</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mu</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">mu</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">'s'</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'r'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:],</span> <span class="n">c</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">'dotted'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">,:])</span><span class="o">+</span><span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png" src="../Images/6633cdad5605633dacdc68a6a28fde12.png" data-original-src="https://mmids-textbook.github.io/_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png"/>
</div>
</div>
&#13;

<h3><span class="section-number">6.7.2.3. </span>Cholesky decomposition<a class="headerlink" href="#cholesky-decomposition" title="Link to this heading">#</a></h3>
<p>In  this section, we derive an important matrix factorization and apply it to generating multivariate Gaussians. We also revisit the least-squares problem. We begin with the motivation.</p>
<p><strong>Generating multivariate Gaussians</strong> Suppose we want to generate samples from a multivariate Gaussian <span class="math notranslate nohighlight">\(\bX \sim N_d(\bmu, \bSigma)\)</span> with given mean vector <span class="math notranslate nohighlight">\(\bmu \in \mathbb{R}^d\)</span> and positive definite covariance matrix <span class="math notranslate nohighlight">\(\bSigma  \in \mathbb{R}^{d \times d}\)</span>. Of course, in Numpy, we could use <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html"><code class="docutils literal notranslate"><span class="pre">numpy.random.Generator.multivariate_normal</span></code></a>. But what is behind it? More precisely, suppose we have access to unlimited samples <span class="math notranslate nohighlight">\(U_1, U_2, U_3, etc.\)</span> from uniform random variables in <span class="math notranslate nohighlight">\([0,1]\)</span>. How do we transform them to obtain samples from <span class="math notranslate nohighlight">\(N_d(\bmu, \bSigma)\)</span>.</p>
<p>We start with the simplest case: <span class="math notranslate nohighlight">\(d=1\)</span>, <span class="math notranslate nohighlight">\(\mu = 0\)</span>, and <span class="math notranslate nohighlight">\(\sigma^2 = 1\)</span>. That is, we first generate a univariate standard Normal. We have seen a recipe for doing this before, the inverse transform sampling method. Specifically, recall that the cumulative distribution function (CDF) of a random variable <span class="math notranslate nohighlight">\(Z\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
F_Z(z) = \mathbb{P}[Z \leq z], \qquad \forall z \in \mathbb{R}.
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> be the interval where <span class="math notranslate nohighlight">\(F_Z(z) \in (0,1)\)</span> and assume that <span class="math notranslate nohighlight">\(F_X\)</span> is strictly increasing on <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span>. Let <span class="math notranslate nohighlight">\(U \sim \mathrm{U}[0,1]\)</span>. Then it can be shown that</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[F_X^{-1}(U) \leq z] = F_X(z).
\]</div>
<p>So take <span class="math notranslate nohighlight">\(F_Z = \Phi\)</span>, the CDF of the standard Normal. Then <span class="math notranslate nohighlight">\(Z = \Phi^{-1}(U)\)</span> is <span class="math notranslate nohighlight">\(N(0,1)\)</span>.</p>
<p>How do we generate a <span class="math notranslate nohighlight">\(N(\mu, \sigma^2)\)</span> variable, for arbitrary <span class="math notranslate nohighlight">\(\mu \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>? We use the fact that the linear transformation of Gaussian is still Gaussian. In particular, if <span class="math notranslate nohighlight">\(Z \sim N(0,1)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
X = \mu + \sigma Z
\]</div>
<p>is <span class="math notranslate nohighlight">\(N(\mu, \sigma^2)\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> In Python, <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span> can be accessed using <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html"><code class="docutils literal notranslate"><span class="pre">scipy.stats.norm.ppf</span></code></a>. We implement this next (with help from ChatGPT).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>

<span class="k">def</span> <span class="nf">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">):</span>
    <span class="c1"># Step 1: Generate uniform [0,1] random variables</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    
    <span class="c1"># Step 2: Apply the inverse CDF (ppf) of the standard normal distribution</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">U</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma2</span> <span class="o">*</span> <span class="n">Z</span>
</pre></div>
</div>
</div>
</div>
<p>We generate 1000 samples and plot the empirical distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Generate 1000 standard normal samples</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="c1"># Plot the empirical PDF</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plot histogram of the samples with density=True to normalize the histogram</span>
<span class="n">count</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">ignored</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">'g'</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">'black'</span><span class="p">)</span>

<span class="c1"># Plot the theoretical standard normal PDF for comparison</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="s1">'k'</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">'Empirical PDF of Generated Samples'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'Value'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'Density'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png" src="../Images/c4d1f8c3493a1c88b3b5727f55a25bef.png" data-original-src="https://mmids-textbook.github.io/_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> It turns out there is a neat trick to generate <em>two</em> independent samples from <span class="math notranslate nohighlight">\(N(0,1)\)</span> that does not rely on access to <span class="math notranslate nohighlight">\(\Phi^{-1}\)</span>. It is called the Box-Muller transform. Ask your favorite AI chatbot about it. Modify our code above to implement it. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p>We move on to the multivariate case. We proceed similarly as before. First, how do we generate a <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussian with mean vector <span class="math notranslate nohighlight">\(\bmu = \mathbf{0}\)</span> and identity covariance matrix <span class="math notranslate nohighlight">\(\bSigma = I_{d \times d}\)</span>? Easy – it has <span class="math notranslate nohighlight">\(d\)</span> independent components, each of which is standard Normal. So letting <span class="math notranslate nohighlight">\(U_1, \ldots, U_d\)</span> be independent uniform <span class="math notranslate nohighlight">\([0,1]\)</span> variables, then</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}
= (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d))
\]</div>
<p>is <span class="math notranslate nohighlight">\(N(\mathbf{0}, I_{d \times d})\)</span>.</p>
<p>We now seek to generate a multivariate Gaussian with arbitrary mean vector <span class="math notranslate nohighlight">\(\bmu\)</span> and positive definite covariance matrix <span class="math notranslate nohighlight">\(\bSigma\)</span>. Again, we use a linear transformation</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}
= \mathbf{a} + A \mathbf{Z}.
\]</div>
<p>What are the right choices for <span class="math notranslate nohighlight">\(a \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span>? We need to match the obtained and desired mean and covariance. We start with the mean. By linearity of expectation,</p>
<div class="math notranslate nohighlight">
\[
\E[\mathbf{X}]
= \E[\mathbf{a} + A \mathbf{Z}]
= \mathbf{a} + A \,\E[\mathbf{Z}]
= \mathbf{a}.
\]</div>
<p>Hence we pick <span class="math notranslate nohighlight">\(\mathbf{a} := \bmu\)</span>.</p>
<p>As for the covariance, using the <em>Covariance of a Linear Transformation</em>, we get</p>
<div class="math notranslate nohighlight">
\[
\cov[\mathbf{X}]
= A \,\cov[\mathbf{Z}] A^T
= A A^T.
\]</div>
<p>Now we have a problem: what is a matrix <span class="math notranslate nohighlight">\(A\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
A A^T = \bSigma?
\]</div>
<p>In some sense, we are looking for a sort of “square root” of the covariance matrix. There are several ways of doing this. The Cholesky decomposition is one of them. We return to generating samples from <span class="math notranslate nohighlight">\(N(\bmu, \bSigma)\)</span> after introducing it.</p>
<p><strong>A matrix factorization</strong> Our key linear-algebraic result of this section is the following. The matrix factorization in the next theorem is called a Cholesky decomposition. It has many <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications">applications</a>.</p>
<p><strong>THEOREM</strong> <strong>(Cholesky Decomposition)</strong> Any positive definite matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times n}\)</span> can be factorized uniquely as</p>
<div class="math notranslate nohighlight">
\[
B = L L^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(L \in \mathbb{R}^{n \times n}\)</span> is a lower triangular matrix with positive entries on the diagonal. <span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>The proof is provided below. It is based on deriving an algorithm for computing the Cholesky decomposition: we grow <span class="math notranslate nohighlight">\(L\)</span> starting from its top-left corner by successively computing its next row based on the previously constructed submatrix. Note that, because <span class="math notranslate nohighlight">\(L\)</span> is lower triangular, it suffices to compute its elements on and below the diagonal. We first give the algorithm, then establish that it is well-defined.</p>
<p><strong>Figure:</strong> Access pattern (<a class="reference external" href="https://en.wikipedia.org/wiki/File:Chol.gif">Source</a>)</p>
<p><img alt="Access pattern" src="../Images/20de5b823040500b0d04511959484e97.png" data-original-src="https://upload.wikimedia.org/wikipedia/commons/b/be/Chol.gif"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p><strong>EXAMPLE:</strong> Before proceeeding with the general method, we give a small example to provide some intuition as to how it operates. We need a positive definite matrix. Consider the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A 
= 
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
0 &amp; -2 &amp; 1\\
0 &amp; 0 &amp; 1\\
0 &amp; 0 &amp; 0
\end{pmatrix}.
\end{split}\]</div>
<p>It has full column rank (why?). Recall that, in that case, the <span class="math notranslate nohighlight">\(B = A^T A\)</span> is positive definite.</p>
<p>That is, the matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B 
=
A^T A 
= 
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
2 &amp; 8 &amp; 0\\
1 &amp; 0 &amp; 3
\end{pmatrix}
\end{split}\]</div>
<p>is positive definite.</p>
<p>Let <span class="math notranslate nohighlight">\(L = (\ell_{i,j})_{i,j=1}^3\)</span> be lower triangular. We seek to solve <span class="math notranslate nohighlight">\(L L^T = B\)</span> for the nonzero entries of <span class="math notranslate nohighlight">\(L\)</span>. Observe that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\ell_{1,1} &amp; 0 &amp; 0\\
\ell_{2,1} &amp; \ell_{2,2} &amp; 0\\
\ell_{3,1} &amp; \ell_{3,2} &amp; \ell_{3,3}
\end{pmatrix}
\begin{pmatrix}
\ell_{1,1} &amp; \ell_{2,1} &amp; \ell_{3,1}\\
0 &amp; \ell_{2,2} &amp; \ell_{3,2}\\
0 &amp; 0 &amp; \ell_{3,3}
\end{pmatrix}
=
\begin{pmatrix}
\ell_{1,1}^2 &amp; \ell_{1,1}\ell_{2,1} &amp; \ell_{1,1}\ell_{3,1}\\
\ell_{1,1}\ell_{2,1} &amp; \ell_{2,1}^2 + \ell_{2,2}^2 &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
\ell_{1,1}\ell_{3,1} &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} &amp; \ell_{3,1}^2 + \ell_{3,2}^2 + \ell_{3,3}
\end{pmatrix}.
\end{split}\]</div>
<p>The system</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
\ell_{1,1}^2 &amp; \ell_{1,1}\ell_{2,1} &amp; \ell_{1,1}\ell_{3,1}\\
\ell_{1,1}\ell_{2,1} &amp; \ell_{2,1}^2 + \ell_{2,2}^2 &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
\ell_{1,1}\ell_{3,1} &amp; \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} &amp; \ell_{3,1}^2 + \ell_{3,2}^2 + \ell_{3,3}^2
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 2 &amp; 1\\
2 &amp; 8 &amp; 0\\
1 &amp; 0 &amp; 3
\end{pmatrix}
\end{split}\]</div>
<p>turns out to be fairly simple to solve.</p>
<ol class="arabic simple">
<li><p>From the first entry, we get <span class="math notranslate nohighlight">\(\ell_{1,1} = 1\)</span> (where we took the positive solution to <span class="math notranslate nohighlight">\(\ell_{1,1}^2 = 1\)</span>).</p></li>
<li><p>Given that <span class="math notranslate nohighlight">\(\ell_{1,1}\)</span> is known, entry <span class="math notranslate nohighlight">\(\ell_{2,1}\)</span> is determined from <span class="math notranslate nohighlight">\(\ell_{1,1}\ell_{2,1} =2\)</span> in the first entry of the second row. That is, <span class="math notranslate nohighlight">\(\ell_{2,1} =2\)</span>. Then the second entry of the second row gives <span class="math notranslate nohighlight">\(\ell_{2,2}\)</span> through <span class="math notranslate nohighlight">\(\ell_{2,1}^2 + \ell_{2,2}^2  = 8\)</span>. So <span class="math notranslate nohighlight">\(\ell_{2,2} = 2\)</span> (again we take the positive solution).</p></li>
<li><p>We move to the third row. The first entry gives <span class="math notranslate nohighlight">\(\ell_{3,1} = 1\)</span>, the second entry gives <span class="math notranslate nohighlight">\(\ell_{3,2} = -1\)</span> and finally the third entry leads to <span class="math notranslate nohighlight">\(\ell_{3,3} = 1\)</span>.</p></li>
</ol>
<p>Hence we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L
=
\begin{pmatrix}
\ell_{1,1} &amp; 0 &amp; 0\\
\ell_{2,1} &amp; \ell_{2,2} &amp; 0\\
\ell_{3,1} &amp; \ell_{3,2} &amp; \ell_{3,3}
\end{pmatrix}
=
\begin{pmatrix}
1 &amp; 0 &amp; 0\\
2 &amp; 2 &amp; 0\\
1 &amp; -1 &amp; 1
\end{pmatrix}.
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>To detail the computation of the Cholesky decomposition <span class="math notranslate nohighlight">\(L L^T\)</span> of <span class="math notranslate nohighlight">\(B\)</span>, we will need some notation. Write <span class="math notranslate nohighlight">\(B = (b_{i,j})_{i,j=1}^n\)</span> and <span class="math notranslate nohighlight">\(L = (\ell_{i,j})_{i,j=1}^n\)</span>. Let <span class="math notranslate nohighlight">\(L_{(k)} = (\ell_{i,j})_{i,j=1}^k\)</span> be the first <span class="math notranslate nohighlight">\(k\)</span> rows and columns of <span class="math notranslate nohighlight">\(L\)</span>, let <span class="math notranslate nohighlight">\(\bflambda_{(k)}^T = (\ell_{k,1},\ldots,\ell_{k,k-1})\)</span> be the row vector corresponding to the first <span class="math notranslate nohighlight">\(k-1\)</span> entries of row <span class="math notranslate nohighlight">\(k\)</span> of <span class="math notranslate nohighlight">\(L\)</span>, and let <span class="math notranslate nohighlight">\(\bfbeta_{(k)}^T = (b_{k,1},\ldots,b_{k,k-1})\)</span> be the row vector corresponding to the first <span class="math notranslate nohighlight">\(k-1\)</span> entries of row <span class="math notranslate nohighlight">\(k\)</span> of <span class="math notranslate nohighlight">\(B\)</span>.</p>
<p>The strategy is to compute <span class="math notranslate nohighlight">\(L_{(1)}\)</span>, then <span class="math notranslate nohighlight">\(L_{(2)}\)</span>, then <span class="math notranslate nohighlight">\(L_{(3)}\)</span> and so on. With the notation above, <span class="math notranslate nohighlight">\(L_{(j)}\)</span> can be written in block form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L_{(j)}
=
\begin{pmatrix}
L_{(j-1)} &amp; \mathbf{0}\\
\bflambda_{(j)}^T &amp; \ell_{j,j}.
\end{pmatrix}
\end{split}\]</div>
<p>Hence, once <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> is known, in order to compute <span class="math notranslate nohighlight">\(L_{(j)}\)</span> one only needs <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span> and <span class="math notranslate nohighlight">\(\ell_{j,j}\)</span>. We show next that they satisfy easily solvable systems of equations.</p>
<p>We first note that the <span class="math notranslate nohighlight">\((1,1)\)</span> entry of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> implies that</p>
<div class="math notranslate nohighlight">
\[
\ell_{1,1}^2 = b_{1,1}.
\]</div>
<p>So we set</p>
<div class="math notranslate nohighlight">
\[
L_{(1)}
= \ell_{1,1}
= \sqrt{b_{1,1}}.
\]</div>
<p>For this step to be well-defined, it needs to be the case that <span class="math notranslate nohighlight">\(b_{1,1} &gt; 0\)</span>. It is easy to see that it follows from the positive definiteness of <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="math notranslate nohighlight">
\[
0 &lt; \langle \mathbf{e}_1, B \mathbf{e}_1\rangle = \mathbf{e}_1^T B_{\cdot,1} = b_{1,1}.
\]</div>
<p>Proceeding by induction, assume <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> has been constructed. The first <span class="math notranslate nohighlight">\(j-1\)</span> elements of the <span class="math notranslate nohighlight">\(j\)</span>-th row of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> translate into</p>
<div class="math notranslate nohighlight">
\[
L_{j,\cdot} (L^T)_{\cdot,1:j-1} = \bflambda_{(j)}^T L_{(j-1)}^T = \bfbeta_{(j)}^T,
\]</div>
<p>where <span class="math notranslate nohighlight">\((L^T)_{\cdot,1:j-1}\)</span> denotes the first <span class="math notranslate nohighlight">\(j-1\)</span> columns of <span class="math notranslate nohighlight">\(L^T\)</span>. In the first equality above, we used the fact that <span class="math notranslate nohighlight">\(L^T\)</span> is upper triangular. Taking a transpose, the resulting linear system of equations</p>
<div class="math notranslate nohighlight">
\[
L_{(j-1)} \bflambda_{(j)} = \bfbeta_{(j)}
\]</div>
<p>can be solved by forward substitution (since <span class="math notranslate nohighlight">\(\bfbeta_{(j)}\)</span> is part of the input and <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> was previously computed). The fact that this system has a unique solution (more specifically, that the diagonal entries of <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span> are strictly positive) is established in the proof of the <em>Cholesky Decomposition Theorem</em>.</p>
<p>The <span class="math notranslate nohighlight">\((j,j)\)</span>-th entry of the matrix equation <span class="math notranslate nohighlight">\(L L^T = B\)</span> translates into</p>
<div class="math notranslate nohighlight">
\[
L_{j,\cdot} (L^T)_{\cdot,j} = \sum_{k=1}^j \ell_{j,k}^2 = b_{j,j},
\]</div>
<p>where again we used the fact that <span class="math notranslate nohighlight">\(L^T\)</span> is upper triangular. Since <span class="math notranslate nohighlight">\(\ell_{j,1}, \ldots, \ell_{j,j-1}\)</span> are the elements of <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span>, they have already been determined. So we can set</p>
<div class="math notranslate nohighlight">
\[
\ell_{j,j}
= \sqrt{b_{j,j} - \sum_{k=1}^{j-1} \ell_{j,k}^2}.
\]</div>
<p>The fact that we are taking the square root of a positive quantity is established in the proof of the <em>Cholesky Decomposition Theorem</em>. Finally, from <span class="math notranslate nohighlight">\(L_{(j-1)}\)</span>, <span class="math notranslate nohighlight">\(\bflambda_{(j)}\)</span>, and <span class="math notranslate nohighlight">\(\ell_{j,j}\)</span>, we construct <span class="math notranslate nohighlight">\(L_{(j)}\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement the algorithm above. In our naive implementation, we assume that <span class="math notranslate nohighlight">\(B\)</span> is positive definite, and therefore that all steps are well-defined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
    <span class="n">L</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">],</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">])</span>
        <span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">B</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">L</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="n">j</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">L</span> 
</pre></div>
</div>
</div>
</div>
<p>Here is a simple example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[2. 1.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1.41421356 0.        ]
 [0.70710678 1.22474487]]
</pre></div>
</div>
</div>
</div>
<p>We can check that it produces the right factorization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">L</span> <span class="o">@</span> <span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[2. 1.]
 [1. 2.]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Proof of Cholesky decomposition theorem</strong> We give a proof of the <em>Cholesky Decomposition Theorem</em>.</p>
<p><em>Proof idea:</em> Assuming by induction that the upper-left corner of the matrix <span class="math notranslate nohighlight">\(B\)</span> has a Cholesky decomposition, one finds equations for the remaining row that can be solved uniquely by the properties established in the previous subsection.</p>
<p><em>Proof:</em> If <span class="math notranslate nohighlight">\(n=1\)</span>, we have shown previously that <span class="math notranslate nohighlight">\(b_{1,1} &gt; 0\)</span>, and hence we can take <span class="math notranslate nohighlight">\(L = [\ell_{1,1}]\)</span> where <span class="math notranslate nohighlight">\(\ell_{1,1} = \sqrt{b_{1,1}}\)</span>. Assuming the result holds for positive definite matrices in <span class="math notranslate nohighlight">\(\mathbb{R}^{(n-1) \times (n-1)}\)</span>, we first re-write <span class="math notranslate nohighlight">\(B = L L^T\)</span> in block form</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{pmatrix}
B_{11} &amp; \bfbeta_{12}\\
\bfbeta_{12}^T &amp; \beta_{22}
\end{pmatrix}
= 
\begin{pmatrix}
\Lambda_{11} &amp; \mathbf{0}\\
\bflambda_{12}^T &amp; \lambda_{22}
\end{pmatrix}
\begin{pmatrix}
\Lambda_{11}^T &amp; \bflambda_{12}\\
\mathbf{0}^T &amp; \lambda_{22}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(B_{11}, \Lambda_{11} \in \mathbb{R}^{n-1 \times n-1}\)</span>, <span class="math notranslate nohighlight">\(\bfbeta_{12}, \bflambda_{12} \in \mathbb{R}^{n-1}\)</span> and <span class="math notranslate nohighlight">\(\beta_{22}, \lambda_{22} \in \mathbb{R}\)</span>. By block matrix algebra, we get the system</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B_{11} = \Lambda_{11} \Lambda_{11}^T\\
\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\\
\beta_{22} = \bflambda_{12}^T \bflambda_{12} + \lambda_{22}^2.
\end{split}\]</div>
<p>By the <em>Principal Submatrices Lemma</em>, the principal submatrix <span class="math notranslate nohighlight">\(B_{11}\)</span> is positive definite. Hence, by induction, there is a unique lower-triangular matrix <span class="math notranslate nohighlight">\(\Lambda_{11}\)</span> with positive diagonal elements satisfying the first equation. We can then obtain <span class="math notranslate nohighlight">\(\bfbeta_{12}\)</span> from the second equation by forward substitution. And finally we get</p>
<div class="math notranslate nohighlight">
\[
\lambda_{22}
= \sqrt{\beta_{22} - \bflambda_{12}^T \bflambda_{12}}.
\]</div>
<p>We do have to check that the square root above exists. That is, we need to argue that the expression inside the square root is non-negative. In fact, for the claim to go through, we need it to be strictly positive. We notice that the expression inside the square root is in fact the Schur complement of the block <span class="math notranslate nohighlight">\(B_{11}\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\beta_{22} - \bflambda_{12}^T \bflambda_{12}
&amp;= \beta_{22} - (\Lambda_{11}^{-1} \bfbeta_{12})^T (\Lambda_{11}^{-1} \bfbeta_{12})\\
&amp;= \beta_{22} - \bfbeta_{12}^T (\Lambda_{11}^{-1})^T \Lambda_{11}^{-1} \bfbeta_{12}\\
&amp;= \beta_{22} - \bfbeta_{12}^T (\Lambda_{11} \Lambda_{11}^T)^{-1} \bfbeta_{12}\\
&amp;= \beta_{22} - \bfbeta_{12}^T (B_{11})^{-1} \bfbeta_{12}
\end{align*}\]</div>
<p>where we used the equation <span class="math notranslate nohighlight">\(\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\)</span> on the first line, the identities <span class="math notranslate nohighlight">\((Q W)^{-1} = W^{-1} Q^{-1}\)</span> and <span class="math notranslate nohighlight">\((Q^T)^{-1} = (Q^{-1})^T\)</span> (see the exercise below) on the third line and the equation <span class="math notranslate nohighlight">\(B_{11} = \Lambda_{11} \Lambda_{11}^T\)</span> on the fourth line. By the <em>Schur Complement Lemma</em>, the Schur complement is positive definite. Because it is a scalar in this case, it is strictly positive (prove it!), which concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>Back to multivariate Gaussians</strong> Returning to our motivation, we can generate samples from a <span class="math notranslate nohighlight">\(N(\bmu, \bSigma)\)</span> by first generating</p>
<div class="math notranslate nohighlight">
\[
\mathbf{Z}
= (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d))
\]</div>
<p>where <span class="math notranslate nohighlight">\(U_1, \ldots, U_d\)</span> are independent uniform <span class="math notranslate nohighlight">\([0,1]\)</span> variables, then setting</p>
<div class="math notranslate nohighlight">
\[
\mathbf{X}
= \bmu + L \mathbf{Z},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bSigma = L L^T\)</span> is a Cholesky decomposition of <span class="math notranslate nohighlight">\(\bSigma\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement this method.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">generate_multivariate_normal_samples_using_cholesky</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span><span class="p">):</span>

    <span class="c1"># Compute Cholesky decomposition</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">Sig</span><span class="p">)</span>
    
    <span class="c1"># Initialization</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="n">d</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        
            <span class="c1"># Generate standard normal vector</span>
            <span class="n">Z</span> <span class="o">=</span> <span class="n">generate_standard_normal_samples_using_inverse_cdf</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Apply the inverse CDF (ppf) of the standard normal distribution</span>
            <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">L</span> <span class="o">@</span> <span class="n">Z</span> 
    
    <span class="k">return</span> <span class="n">X</span>
</pre></div>
</div>
</div>
</div>
<p>We generate some samples as an example.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">0.</span><span class="p">,</span><span class="mf">1.</span><span class="p">])</span>
<span class="n">Sig</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">generate_multivariate_normal_samples_using_cholesky</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">Sig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[-0.47926185  1.97223283  2.73780609]
 [-2.69005319 -4.19788834 -0.43130768]
 [ 0.41957285  3.91719212  2.08604427]
 [-2.11532949 -5.34557983  0.69521104]
 [-2.41203356 -1.84032486 -0.82207565]
 [-1.46121329  0.4821332   0.55005982]
 [-0.84981594  0.67074839  0.16360931]
 [-2.19097155 -1.98022929 -1.06365711]
 [-2.75113597 -3.47560492 -0.26607926]
 [ 0.130848    6.07312936 -0.08800829]]
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Using a Cholesky decomposition to solve the least squares problem</strong> Another application of the Cholesky decomposition is to solving the least squares problem. In this section, we restrict ourselves to the case where <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> has full column rank. By the <em>Least Squares and Positive Semidefiniteness Lemma</em>, we then have that <span class="math notranslate nohighlight">\(A^T A\)</span> is positive definite. By the <em>Cholesky Decomposition Theorem</em>, we can factorize this matrix as <span class="math notranslate nohighlight">\(A^T A = L L^T\)</span> where <span class="math notranslate nohighlight">\(L\)</span> is lower triangular with positive diagonal elements. The normal equations then reduce to</p>
<div class="math notranslate nohighlight">
\[
L L^T \mathbf{x} = A^T \mathbf{b}.
\]</div>
<p>This system can be solved in two steps. We first obtain the solution to</p>
<div class="math notranslate nohighlight">
\[
L \mathbf{z} = A^T \mathbf{b}
\]</div>
<p>by forward substitution. Then we obtain the solution to</p>
<div class="math notranslate nohighlight">
\[
L^T \mathbf{x} = \mathbf{z}
\]</div>
<p>by back-substitution. Note that <span class="math notranslate nohighlight">\(L^T\)</span> is indeed an upper triangular matrix.</p>
<p><strong>NUMERICAL CORNER:</strong> We implement this algorithm below. In our naive implementation, we assume that <span class="math notranslate nohighlight">\(A\)</span> has full column rank, and therefore that all steps are well-defined.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">ls_by_chol</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">cholesky</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">A</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">forwardsubs</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mmids</span><span class="o">.</span><span class="n">backsubs</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>Other applications of the Cholesky decomposition are briefly described <a class="reference external" href="https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications">here</a>.</p>
    
</body>
</html>