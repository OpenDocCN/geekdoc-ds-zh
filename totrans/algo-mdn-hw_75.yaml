- en: Prefix Sum with SIMD
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前缀和与SIMD
- en: 原文：[https://en.algorithmica.org/hpc/algorithms/prefix/](https://en.algorithmica.org/hpc/algorithms/prefix/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://en.algorithmica.org/hpc/algorithms/prefix/](https://en.algorithmica.org/hpc/algorithms/prefix/)
- en: 'The *prefix sum*, also known as *cumulative sum*, *inclusive scan*, or simply
    *scan*, is a sequence of numbers $b_i$ generated from another sequence $a_i$ using
    the following rule: $$ \begin{aligned} b_0 &= a_0 \\ b_1 &= a_0 + a_1 \\ b_2 &=
    a_0 + a_1 + a_2 \\ &\ldots \end{aligned} $$'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*前缀和*，也称为*累积和*、*包含扫描*或简单地称为*扫描*，是从另一个序列$a_i$生成另一个序列$b_i$的数字序列，使用以下规则：$$ \begin{aligned}
    b_0 &= a_0 \\ b_1 &= a_0 + a_1 \\ b_2 &= a_0 + a_1 + a_2 \\ &\ldots \end{aligned}
    $$'
- en: In other words, the $k$-th element of the output sequence is the sum of the
    first $k$ elements of the input sequence.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，输出序列的第$k$个元素是输入序列前$k$个元素的和。
- en: Prefix sum is a very important primitive in many algorithms, especially in the
    context of parallel algorithms, where its computation scales almost perfectly
    with the number of processors. Unfortunately, it is much harder to speed up with
    SIMD parallelism on a single CPU core, but we will try it nonetheless — and derive
    an algorithm that is ~2.5x faster than the baseline scalar implementation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀和是许多算法中一个非常重要的基本操作，尤其是在并行算法的背景下，其计算量几乎可以完美地与处理器的数量成比例。不幸的是，在单个CPU核心上使用SIMD并行性来加速它要困难得多，但我们仍然会尝试——并推导出一个比基线标量实现快2.5倍的算法。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/prefix/#baseline)Baseline'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/prefix/#baseline)基线'
- en: 'For our baseline, we could just invoke `std::partial_sum` from the STL, but
    for clarity, we will implement it manually. We create an array of integers and
    then sequentially add the previous element to the current one:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的基线，我们可以直接从STL调用`std::partial_sum`，但为了清晰起见，我们将手动实现它。我们创建一个整数数组，然后逐个将前一个元素加到当前元素上：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It seems like we need two reads, an add, and a write on each iteration, but
    of course, the compiler optimizes the extra read away and uses a register as the
    accumulator:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们似乎需要在每次迭代中进行两次读取、一次加法和一次写入，但当然，编译器会优化额外的读取并使用寄存器作为累加器：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After [unrolling](/hpc/architecture/loops) the loop, just two instructions
    effectively remain: the fused read-add and the write-back of the result. Theoretically,
    these should work at 2 GFLOPS (1 element per CPU cycle, by the virtue of [superscalar
    processing](/hpc/pipelining)), but since the memory system has to constantly [switch](/hpc/cpu-cache/bandwidth#directional-access)
    between reading and writing, the actual performance is between 1.2 and 1.6 GFLOPS,
    depending on the array size.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在[展开](/hpc/architecture/loops)循环后，只剩下两条指令：融合的读取-加法和结果的写回。理论上，这些应该以2 GFLOPS（每个CPU周期处理1个元素，得益于[超标量处理](/hpc/pipelining)）的速度运行，但由于内存系统必须不断在读取和写入之间切换，实际性能在1.2到1.6
    GFLOPS之间，具体取决于数组大小。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/prefix/#vectorization)Vectorization'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/prefix/#vectorization)向量化'
- en: One way to implement a parallel prefix sum algorithm is to split the array into
    small blocks, independently calculate *local* prefix sums on them, and then do
    a second pass where we adjust the computed values in each block by adding the
    sum of all previous elements to them.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 实现并行前缀和算法的一种方法是将数组分成小块，独立地计算它们上的*局部*前缀和，然后在第二次遍历中，我们将每个块中计算出的值通过添加所有先前元素的总和来调整。
- en: '![](../Images/2cf252498fc7bffaf0f5925689608ce1.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2cf252498fc7bffaf0f5925689608ce1.png)'
- en: This allows processing each block in parallel — both during the computation
    of the local prefix sums and the accumulation phase — so you usually split the
    array into as many blocks as you have processors. But since we are only allowed
    to use one CPU core, and [non-sequential memory access](/hpc/simd/moving#non-contiguous-load)
    in SIMD doesn’t work well, we are not going to do that. Instead, we will use a
    fixed block size equal to the size of a SIMD lane and calculate prefix sums within
    a register.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许并行处理每个块——在计算局部前缀和以及累积阶段——因此你通常将数组分成与处理器数量一样多的块。但由于我们只能使用一个CPU核心，并且SIMD中的[非连续内存访问](/hpc/simd/moving#non-contiguous-load)效果不佳，我们不会这样做。相反，我们将使用与SIMD通道大小相等的固定块大小，并在寄存器内计算前缀和。
- en: 'Now, to compute these prefix sums locally, we are going to use another parallel
    prefix sum method that is generally inefficient (the total work is $O(n \log n)$
    instead of linear) but is good enough for the case when the data is already in
    a SIMD register. The idea is to perform $\log n$ iterations where on $k$-th iteration,
    we add $a_{i - 2^k}$ to $a_i$ for all applicable $i$:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了在本地计算这些前缀和，我们将使用另一种并行前缀和算法，它通常效率不高（总工作量为 $O(n \log n)$ 而不是线性），但对于数据已经位于SIMD寄存器中的情况来说足够好了。思路是在
    $\log n$ 次迭代中执行，在第 $k$ 次迭代中，将 $a_{i - 2^k}$ 添加到所有适用的 $a_i$：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can prove that this algorithm works by induction: if on $k$-th iteration
    every element $a_i$ is equal to the sum of the $(i - 2^k, i]$ segment of the original
    array, then after adding $a_{i - 2^k}$ to it, it will be equal to the sum of $(i
    - 2^{k+1}, i]$. After $O(\log n)$ iterations, the array will turn into its prefix
    sum.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过归纳法证明这个算法是有效的：如果在第 $k$ 次迭代中每个元素 $a_i$ 都等于原始数组中 $(i - 2^k, i]$ 区间的和，那么在将其添加到
    $a_{i - 2^k}$ 后，它将等于 $(i - 2^{k+1}, i]$ 区间的和。经过 $O(\log n)$ 次迭代后，数组将变成其前缀和。
- en: 'To implement it in SIMD, we could use [permutations](/hpc/simd/shuffling) to
    place $i$-th element against $(i-2^k)$-th, but they are too slow. Instead, we
    will use the `sll` (“shift lanes left”) instruction that does exactly that and
    also replaces the unmatched elements with zeros:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在单指令多数据（SIMD）实现中，我们可以使用[置换](/hpc/simd/shuffling)来将第 $i$ 个元素与第 $(i-2^k)$ 个元素对齐，但它们太慢了。相反，我们将使用
    `sll`（“左移位”）指令，它正好做这件事，并且还将不匹配的元素替换为零：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Unfortunately, the 256-bit version of this instruction performs this byte shift
    independently within two 128-bit lanes, which is typical to AVX:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这个指令的256位版本在两个128位通道内独立地执行字节移位，这是AVX的典型做法：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We still can use it to compute 4-element prefix sums twice as fast, but we’ll
    have to switch to 128-bit SSE when accumulating. Let’s write a handy function
    that computes a local prefix sum end-to-end:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然可以使用它以两倍的速度计算4元素的前缀和，但在累积时将不得不切换到128位的SSE。让我们编写一个方便的函数，从头到尾计算局部前缀和：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, for the accumulate phase, we will create another handy function that similarly
    takes the pointer to a 4-element block and also the 4-element vector of the previous
    prefix sum. The job of this function is to add this prefix sum vector to the block
    and update it so that it can be passed on to the next block (by broadcasting the
    last element of the block before the addition):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于累积阶段，我们将创建另一个方便的函数，它同样接受4元素块的指针以及前缀和的4元素向量。这个函数的职责是将这个前缀和向量添加到块中，并更新它，以便可以传递给下一个块（通过在加法之前广播块之前的最后一个元素）：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With `prefix` and `accumulate` implemented, the only thing left is to glue
    together our two-pass algorithm:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 `prefix` 和 `accumulate` 的实现，剩下要做的就是将我们的两遍算法粘合在一起：
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The algorithm already performs slightly more than twice as fast as the scalar
    implementation but becomes slower for large arrays that fall out of the L3 cache
    — roughly at half the [two-way RAM bandwidth](/hpc/cpu-cache/bandwidth) as we
    are reading the entire array twice.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法已经比标量实现快了略多于两倍，但对于超出L3缓存的较大数组来说会变慢——大约是两倍[双向RAM带宽](/hpc/cpu-cache/bandwidth)，因为我们正在读取整个数组两次。
- en: '![](../Images/9dc6b3de77d876eb1f9f89f03b30851b.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9dc6b3de77d876eb1f9f89f03b30851b.png)'
- en: 'Another interesting data point: if we only execute the `prefix` phase, the
    performance would be ~8.1 GFLOPS. The `accumulate` phase is slightly slower at
    ~5.8 GFLOPS. Sanity check: the total performance should be $\frac{1}{ \frac{1}{5.8}
    + \frac{1}{8.1} } \approx 3.4$.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有趣的数据点：如果我们只执行 `prefix` 阶段，性能将是 ~8.1 GFLOPS。`accumulate` 阶段略慢，约为 ~5.8 GFLOPS。合理性检查：总性能应该是
    $\frac{1}{ \frac{1}{5.8} + \frac{1}{8.1} } \approx 3.4$。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/prefix/#blocking)Blocking'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/prefix/#blocking)分块'
- en: 'So, we have a memory bandwidth problem for large arrays. We can avoid re-fetching
    the entire array from RAM if we split it into blocks that fit in the cache and
    process them separately. All we need to pass to the next block is the sum of the
    previous ones, so we can design a `local_prefix` function with an interface similar
    to `accumulate`:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于大型数组，我们有一个内存带宽问题。如果我们将其分成适合缓存的块并分别处理它们，就可以避免从RAM中重新检索整个数组。我们只需要传递给下一个块的先前块的求和，因此我们可以设计一个与
    `accumulate` 接口类似的 `local_prefix` 函数：
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: (We have to make sure that $N$ is a multiple of $B$, but we are going to ignore
    such implementation details for now.)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: （我们必须确保 $N$ 是 $B$ 的倍数，但我们现在将忽略这样的实现细节。）
- en: 'The blocked version performs considerably better, and not just for when the
    array is in the RAM:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 块版本的性能明显更好，而不仅仅是当数组在RAM中时：
- en: '![](../Images/04ea3bb5b1264e449ccb79041a9a51d1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04ea3bb5b1264e449ccb79041a9a51d1.png)'
- en: The speedup in the RAM case compared to the non-blocked implementation is only
    ~1.5 and not 2\. This is because the memory controller is sitting idle while we
    iterate over the cached block for the second time instead of fetching the next
    one — the [hardware prefetcher](/hpc/cpu-cache/prefetching) isn’t advanced enough
    to detect this pattern.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与非阻塞实现相比，在RAM情况下的加速仅为~1.5，而不是2。这是因为当我们在第二次迭代缓存块而不是获取下一个块时，内存控制器处于空闲状态——[硬件预取器](/hpc/cpu-cache/prefetching)还不够先进，无法检测到这种模式。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/prefix/#continuous-loads)Continuous
    Loads'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/prefix/#continuous-loads)连续加载'
- en: There are several ways to solve this under-utilization problem. The obvious
    one is to use [software prefetching](/hpc/cpu-cache/prefetching) to explicitly
    request the next block while we are still processing the current one.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这种低利用率问题的方法有好几种。最明显的方法是在处理当前块的同时，使用[软件预取](/hpc/cpu-cache/prefetching)来显式请求下一个块。
- en: 'It is better to add prefetching to the `accumulate` phase because it is slower
    and less memory-intensive than `prefix`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最好在`accumulate`阶段添加预取，因为它比`prefix`慢且内存密集度更低：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The performance slightly decreases for in-cache arrays, but approaches closer
    to 2 GFLOPS for the in-RAM ones:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于缓存中的数组，性能略有下降，但对于RAM中的数组，接近2 GFLOPS：
- en: '![](../Images/660b709c4751cff3fccebc2575c447d6.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/660b709c4751cff3fccebc2575c447d6.png)'
- en: 'Another approach is to do *interleaving* of the two phases. Instead of separating
    and alternating between them in large blocks, we can execute the two phases concurrently,
    with the `accumulate` phase lagging behind by a fixed number of iterations — similar
    to the [CPU pipeline](/hpc/pipelining):'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是进行两个阶段的*交错*。而不是在大块中分离并交替它们，我们可以同时执行这两个阶段，其中`accumulate`阶段落后于固定数量的迭代——类似于[CPU流水线](/hpc/pipelining)：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This has more benefits: the loop progresses at a constant speed, reducing the
    pressure on the memory system, and the scheduler sees the instructions of both
    subroutines, allowing it to be more efficient at assigning instruction to execution
    ports — sort of like hyper-threading, but in code.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这带来了更多的好处：循环以恒定速度进行，减轻了对内存系统的压力，调度器可以看到两个子例程的指令，这使得它能够更有效地分配指令到执行端口——有点像超线程，但体现在代码中。
- en: 'For these reasons, the performance improves even on small arrays:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，即使是小型数组，性能也会提高：
- en: '![](../Images/9027dc82ab7c397ad1f86497a596f7fa.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9027dc82ab7c397ad1f86497a596f7fa.png)'
- en: 'And finally, it doesn’t seem that we are bottlenecked by the [memory read port](/hpc/pipelining/tables/)
    or the [decode width](/hpc/architecture/layout/#cpu-front-end), so we can add
    prefetching for free, which improves the performance even more:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，似乎我们并没有因为[内存读端口](/hpc/pipelining/tables/)或[解码宽度](/hpc/architecture/layout/#cpu-front-end)而成为瓶颈，因此我们可以免费添加预取，这进一步提高了性能：
- en: '![](../Images/0cea93494021fa1fd7aab651b8f9248b.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/0cea93494021fa1fd7aab651b8f9248b.png)'
- en: The total speedup we were able to achieve is between $\frac{4.2}{1.5} \approx
    2.8$ for small arrays and $\frac{2.1}{1.2} \approx 1.75$ for large arrays.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够实现的总加速在小型数组中为$\frac{4.2}{1.5} \approx 2.8$，在大型数组中为$\frac{2.1}{1.2} \approx
    1.75$。
- en: The speedup may be higher for lower-precision data compared to the scalar code,
    as it is pretty much limited to executing one iteration per cycle regardless of
    the operand size, but it is still sort of “meh” when compared to some [other SIMD-based
    algorithms](../argmin). This is largely because there isn’t a full-register byte
    shift in AVX that would allow the `accumulate` stage to proceed twice as fast,
    let alone a dedicated prefix sum instruction.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与标量代码相比，对于低精度数据，加速可能更高，因为它基本上限制在每个周期执行一次迭代，无论操作数的大小如何，但与一些[其他基于SIMD的算法](../argmin)相比，这仍然有点“一般”。这主要是因为AVX中没有完整的寄存器字节移位，这会允许`accumulate`阶段以两倍的速度进行，更不用说专门的累加指令了。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/prefix/#other-relevant-work)Other
    Relevant Work'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/prefix/#other-relevant-work)其他相关工作'
- en: You can read [this paper from Columbia](http://www.adms-conf.org/2020-camera-ready/ADMS20_05.pdf)
    that focuses on the multi-core setting and AVX-512 (which [sort of](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#ig_expand=3037,4870,6715,4845,3853,90,7307,5993,2692,6946,6949,5456,6938,5456,1021,3007,514,518,7253,7183,3892,5135,5260,3915,4027,3873,7401,4376,4229,151,2324,2310,2324,591,4075,6130,4875,6385,5259,6385,6250,1395,7253,6452,7492,4669,4669,7253,1039,1029,4669,4707,7253,7242,848,879,848,7251,4275,879,874,849,833,6046,7250,4870,4872,4875,849,849,5144,4875,4787,4787,4787,3016,3018,5227,7359,7335,7392,4787,5259,5230,5230,5223,6438,488,483,6165,6570,6554,289,6792,6554,5230,6385,5260,5259,289,288,3037,3009,590,604,633,5230,5259,6554,6554,5259,6547,6554,3841,5214,5229,5260,5259,7335,5259,519,1029,515,3009,3009,3013,3011,515,6527,652,6527,6554,288&text=_mm512_alignr_epi32&techs=AVX_512)
    has a fast 512-bit register byte shift) and [this StackOverflow question](https://stackoverflow.com/questions/10587598/simd-prefix-sum-on-intel-cpu)
    for a more general discussion.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以阅读[哥伦比亚大学这篇论文](http://www.adms-conf.org/2020-camera-ready/ADMS20_05.pdf)，它专注于多核设置和AVX-512（[某种程度上](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#ig_expand=3037,4870,6715,4845,3853,90,7307,5993,2692,6946,6949,5456,6938,5456,1021,3007,514,518,7253,7183,3892,5135,5260,3915,4027,3873,7401,4376,4229,151,2324,2310,2324,591,4075,6130,4875,6385,5259,6385,6250,1395,7253,6452,7492,4669,4669,7253,1039,1029,4669,4707,7253,7242,848,879,848,7251,4275,879,874,849,833,6046,7250,4870,4872,4875,849,849,5144,4875,4787,4787,4787,3016,3018,5227,7359,7335,7392,4787,5259,5230,5230,5223,6438,488,483,6165,6570,6554,289,6792,6554,5230,6385,5260,5259,289,288,3037,3009,590,604,633,5230,5259,6554,6554,5259,6547,6554,3841,5214,5229,5260,5259,7335,5259,519,1029,515,3009,3009,3013,3011,515,6527,652,6527,6554,288&text=_mm512_alignr_epi32&techs=AVX_512)具有快速的512位寄存器字节移位）以及[这个StackOverflow问题](https://stackoverflow.com/questions/10587598/simd-prefix-sum-on-intel-cpu)以获得更广泛的讨论。
- en: Most of what I’ve described in this article was already known. To the best of
    my knowledge, my contribution here is the interleaving technique, which is responsible
    for a modest ~20% performance increase. There probably are ways to improve it
    further, but not by a lot.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本文所描述的大部分内容已经为人所知。据我所知，我的贡献在于交错技术，这为性能带来了约20%的提升。可能还有方法可以进一步改进它，但提升不会很大。
- en: There is also this professor at CMU, [Guy Blelloch](https://www.cs.cmu.edu/~blelloch/),
    who [advocated](https://www.cs.cmu.edu/~blelloch/papers/sc90.pdf) for a dedicated
    prefix sum hardware back in the 90s when [vector processors](https://en.wikipedia.org/wiki/Vector_processor)
    were still a thing. Prefix sums are very important for parallel applications,
    and the hardware is becoming increasingly more parallel, so maybe, in the future,
    the CPU manufacturers will revitalize this idea and make prefix sum calculations
    slightly easier. [← Argmin with SIMD](https://en.algorithmica.org/hpc/algorithms/argmin/)[Matrix
    Multiplication →](https://en.algorithmica.org/hpc/algorithms/matmul/)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在卡内基梅隆大学（CMU）还有这样一位教授，[Guy Blelloch](https://www.cs.cmu.edu/~blelloch/)，他在20世纪90年代，当[向量处理器](https://en.wikipedia.org/wiki/Vector_processor)仍然存在的时候，[倡导](https://www.cs.cmu.edu/~blelloch/papers/sc90.pdf)了一种专门的求和硬件。求和对于并行应用非常重要，而硬件正变得越来越并行，所以也许在未来，CPU制造商将重新激活这个想法，使得求和计算稍微容易一些。[←
    使用SIMD进行Argmin](https://en.algorithmica.org/hpc/algorithms/argmin/)[矩阵乘法 →](https://en.algorithmica.org/hpc/algorithms/matmul/)
