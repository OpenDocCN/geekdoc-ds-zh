- en: 4.4\. Power iteration#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/04_power/roch-mmids-svd-power.html](https://mmids-textbook.github.io/chap04_svd/04_power/roch-mmids-svd-power.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: There is in general [no exact method](https://math.stackexchange.com/questions/2582300/what-does-the-author-mean-by-no-method-exists-for-exactly-computing-the-eigenva)
    for computing SVDs. Instead we must rely on iterative methods, that is, methods
    that progressively approach the solution. We describe in this section the power
    iteration method. This method is behind an effective numerical approach for computing
    SVDs.
  prefs: []
  type: TYPE_NORMAL
- en: The focus here is on numerical methods and we will not spend much time computing
    SVDs by hand. But note that the connection between the SVD and the spectral decomposition
    of \(A^T A\) can be used for this purpose on small examples.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1\. Key lemma[#](#key-lemma "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now derive the main idea behind an algorithm to compute singular vectors.
    Let \(U \Sigma V^T\) be a (compact) SVD of \(A\). Because of the orthogonality
    of \(U\) and \(V\), the powers of \(A^T A\) have a simple representation. Indeed
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T
    = V \Sigma^T \Sigma V^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Note that this formula is closely related to our previously uncovered connection
    between the SVD and the spectral decomposition of \(A^T A\) – although it is not
    quite a spectral decomposition of \(A^T A\) since \(V\) is not orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating,
  prefs: []
  type: TYPE_NORMAL
- en: \[ B^2 = (V \Sigma^T \Sigma V^T) (V \Sigma^T \Sigma V^T) = V (\Sigma^T \Sigma)^2
    V^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: and, for general \(k\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ B^{k} = V (\Sigma^T \Sigma)^{k} V^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, defining
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \widetilde{\Sigma} = \Sigma^T \Sigma = \begin{pmatrix} \sigma_1^2
    & 0 & \cdots & 0\\ 0 & \sigma_2^2 & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \cdots & \sigma_r^2 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: we see that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \widetilde{\Sigma}^k = \begin{pmatrix} \sigma_1^{2k} & 0 & \cdots
    & 0\\ 0 & \sigma_2^{2k} & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\ 0 &
    0 & \cdots & \sigma_r^{2k} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: When \(\sigma_1 > \sigma_2, \ldots, \sigma_r\), which is typically the case
    with real datasets, we get that \(\sigma_1^{2k} \gg \sigma_2^{2k}, \ldots, \sigma_r^{2k}\)
    when \(k\) is large. Then, we get the approximation
  prefs: []
  type: TYPE_NORMAL
- en: \[ B^{k} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T \approx \sigma_1^{2k}
    \mathbf{v}_1 \mathbf{v}_1^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we arrive at:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Power Iteration)** \(\idx{power iteration lemma}\xdi\) Let \(A
    \in \mathbb{R}^{n\times m}\) be a matrix and let \(U \Sigma V^T\) be a (compact)
    SVD of \(A\) such that \(\sigma_1 > \sigma_2 > 0\). Define \(B = A^T A\) and assume
    that \(\mathbf{x} \in \mathbb{R}^m\) is a vector satisfying \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle > 0\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \mathbf{v}_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(k \to +\infty\). If instead \(\langle \mathbf{v}_1, \mathbf{x} \rangle
    < 0\), then the limit is \(- \mathbf{v}_1\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use the approximation above and divide by the norm to get
    a unit norm vector in the direction of \(\mathbf{v}_1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B^{k}\mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T
    \mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x}) \mathbf{v}_j.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} &= \sum_{j=1}^r
    \mathbf{v}_j \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k} \mathbf{x}\|}\\
    &= \mathbf{v}_1 \left\{\frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|}\right\} + \sum_{j=2}^r \mathbf{v}_j \left\{\frac{\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})} {\|B^{k} \mathbf{x}\|}\right\}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: This goes to \(\mathbf{v}_1\) as \(k\to +\infty\) if the expression in the first
    curly brackets goes to \(1\) and the one in the second curly brackets goes to
    \(0\). We prove this in the next claim.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** As \(k\to +\infty\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k} \mathbf{x}\|} \to
    1 \qquad \text{and} \qquad \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|} \to 0, \ j = 2,\ldots,r. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Because the \(\mathbf{v}_j\)s are an orthonormal basis,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|B^{k}\mathbf{x}\|^2 = \sum_{j=1}^r \left[\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})\right]^2 = \sum_{j=1}^r \sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So, as \(k\to +\infty\), using the fact that \(\mathbf{v}_1^T \mathbf{x} = \langle
    \mathbf{v}_1, \mathbf{x} \rangle \neq 0\) by assumption
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\|B^{k}\mathbf{x}\|^2}{\sigma_1^{4k} (\mathbf{v}_1^T
    \mathbf{x})^2} &= 1 + \sum_{j=2}^r \frac{\sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2}{\sigma_1^{4k}
    (\mathbf{v}_1^T \mathbf{x})^2}\\ &= 1 + \sum_{j=2}^r \left(\frac{\sigma_j}{\sigma_1}\right)^{4k}
    \frac{(\mathbf{v}_j^T \mathbf{x})^2}{(\mathbf{v}_1^T \mathbf{x})^2}\\ &\to 1,
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: since \(\sigma_j < \sigma_1\) for all \(j =2,\ldots,r\). That implies the first
    part of the claim by taking a square root and using \(\langle \mathbf{v}_1, \mathbf{x}
    \rangle > 0\). The second part of the claim follows essentially from the same
    argument. \(\square\) \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** We revisit the example'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We previously compute its SVD and found that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This time we use the *Power Iteration Lemma*. Here
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = A^T A = \begin{pmatrix} 2 & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Taking powers of this matrix is easy
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B^k = \begin{pmatrix} 2^k & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Let’s choose an arbitrary initial vector \(\mathbf{x}\), say \((-1, 2)\). Then
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B^k \mathbf{x} = \begin{pmatrix} -2^k\\ 0 \end{pmatrix} \quad
    \text{and} \quad \|B^k \mathbf{x}\| = 2^k. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \begin{pmatrix}
    -1\\ 0 \end{pmatrix} = - \mathbf{v}_1, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(k \to +\infty\). In fact, in this case, convergence occurs after one step.
    \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The argument leading to the *Power Iteration Lemma* also holds more generally
    for the eigenvectors of positive semidefinite matrices. Let \(A\) be a symmetric,
    positive semidefinite matrix in \(\mathbb{R}^{d \times d}\). By the *Spectral
    Theorem*, it has an eigenvector decomposition
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where further \(0 \leq \lambda_d \leq \cdots \leq \lambda_1\) by the *Characterization
    of Positive Semidefiniteness*. Because of the orthogonality of \(Q\), the powers
    of \(A\) have a simple representation. The square gives
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^2 = (Q \Lambda Q^T) (Q \Lambda Q^T) = Q \Lambda^2 Q^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Repeating, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{k} = Q \Lambda^{k} Q^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Power Iteration)** \(\idx{power iteration lemma}\xdi\) Let \(A\)
    be a symmetric, positive semindefinite matrix in \(\mathbb{R}^{d \times d}\) with
    eigenvector decomposition \(A= Q \Lambda Q^T\) where the eigenvalues satisfy \(0
    \leq \lambda_d \leq \cdots \leq \lambda_2 < \lambda_1\). Assume that \(\mathbf{x}
    \in \mathbb{R}^d\) is a vector such that \(\langle \mathbf{q}_1, \mathbf{x} \rangle
    > 0\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{A^{k} \mathbf{x}}{\|A^{k} \mathbf{x}\|} \to \mathbf{q}_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(k \to +\infty\). If instead \(\langle \mathbf{q}_1, \mathbf{x} \rangle
    < 0\), then the limit is \(- \mathbf{q}_1\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: The proof is similar to the case of singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2\. Computing the top singular vector[#](#computing-the-top-singular-vector
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Power iteration gives us a way to compute \(\mathbf{v}_1\) – at least approximately
    if we use a large enough \(k\). But how do we find an appropriate vector \(\mathbf{x}\),
    as required by the *Power Iteration Lemma*? It turns out that a random vector
    will do. For instance, let \(\mathbf{X}\) be an \(m\)-dimensional spherical Gaussian
    with mean \(0\) and variance \(1\). Then, \(\mathbb{P}[\langle \mathbf{v}_1, \mathbf{X}
    \rangle = 0] = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: We implement the algorithm suggested by the *Power Iteration Lemma*. That is,
    we compute \(B^{k} \mathbf{x}\), then normalize it. To obtain the corresponding
    singular value and left singular vector, we use that \(\sigma_1 = \|A \mathbf{v}_1\|\)
    and \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We will apply it to our previous two-cluster example.
    The necessary functions are in [mmids.py](https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py),
    which is available on the [GitHub of the book](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/99720e27a1ef3efd196db79640dfdfe1c3d2d70b319ef77d803ab4437bbb954a.png](../Images/e66fa4144461a2ee2155a730c0c7df77.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s compute the top singular vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is approximately \(-\mathbf{e}_1\). We get roughly the same answer (possibly
    up to sign) from Python’s [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Recall that, when we applied \(k\)-means clustering to this example with \(d=1000\)
    dimension, we obtained a very poor clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/7a49cf383a34072dd30fa0e2fed2db76615fa2692ba83fbdee9d0a6a770b3940.png](../Images/e2ab190c5a5c17937d5e2285a9c20f4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s try again, but after projecting on the top singular vector. Recall that
    this corresponds to finding the best one-dimensional approximating subspace. The
    projection can be computed using the truncated SVD \(Z= U_{(1)} \Sigma_{(1)} V_{(1)}^T\).
    We can interpret the rows of \(U_{(1)} \Sigma_{(1)}\) as the coefficients of each
    data point in the basis \(\mathbf{v}_1\). We will work in that basis. We need
    one small hack: because our implementation of \(k\)-means clustering expects data
    points in at least \(2\) dimension, we add a column of \(0\)’s.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/f2b508b6781d21eee9a679ee87894a523e040face9ef4d99ccd09cd38dab5959.png](../Images/d0d37edca421b5badf55590966b6d81a.png)'
  prefs: []
  type: TYPE_IMG
- en: There is a small – yet noticeable – gap around 0\. We run \(k\)-means clustering
    on the projected data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]</details> ![../../_images/b1c80c35b20bcf5c314f991485089bce5ec85beb6ae17ada744dd8f3b8a04ddc.png](../Images/5ffbd56fc722edd92c57dc038954354a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Much better. We give a more formal explanation of this outcome in a subsequent
    section. In essence, quoting [BHK, Section 7.5.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[…] let’s understand the central advantage of doing the projection to [the
    top \(k\) right singular vectors]. It is simply that for any reasonable (unknown)
    clustering of data points, the projection brings data points closer to their cluster
    centers.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Finally, looking at the top right singular vector (or its first ten entries
    for lack of space), we see that it does align quite well (but not perfectly) with
    the first dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** There are other methods to compute the SVD. Ask your favorite
    AI chatbot about randomized algorithms for the SVD. What are their advantages
    in terms of computational efficiency for large matrices? \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** In the power iteration lemma for the positive semidefinite case, what
    happens when the initial vector \(\mathbf{x}\) satisfies \(\langle \mathbf{q}_1,
    \mathbf{x} \rangle < 0\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The iteration converges to \(\mathbf{q}_1\).
  prefs: []
  type: TYPE_NORMAL
- en: b) The iteration converges to \(-\mathbf{q}_1\).
  prefs: []
  type: TYPE_NORMAL
- en: c) The iteration does not converge.
  prefs: []
  type: TYPE_NORMAL
- en: d) The iteration converges to a random eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** In the power iteration lemma for the SVD case, what is the convergence
    result for a random vector \(\mathbf{x}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\mathbf{u}_1\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\mathbf{v}_1\) or
    \(-\mathbf{v}_1\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\sigma_1\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) does not converge.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Suppose you apply the power iteration method to a matrix \(A\) and obtain
    a vector \(\mathbf{v}\). How can you compute the corresponding singular value
    \(\sigma\) and left singular vector \(\mathbf{u}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\sigma = \|A\mathbf{v}\|\) and \(\mathbf{u} = A\mathbf{v}/\sigma\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\sigma = \|A^T\mathbf{v}\|\) and \(\mathbf{u} = A^T\mathbf{v}/\sigma\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\sigma = \|\mathbf{v}\|\) and \(\mathbf{u} = \mathbf{v}/\sigma\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\sigma = 1\) and \(\mathbf{u} = A\mathbf{v}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** What is required for the initial vector \(\mathbf{x}\) in the power iteration
    method to ensure convergence to the top eigenvector?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{x}\) must be a zero vector.
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{x}\) must be orthogonal to the top eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq 0\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbf{x}\) must be the top eigenvector itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** What does the truncated SVD \(Z = U_{(2)} \Sigma_{(2)} V_{(2)}^T\) correspond
    to? [Uses Section 4.8.2.1.]'
  prefs: []
  type: TYPE_NORMAL
- en: a) The best one-dimensional approximating subspace
  prefs: []
  type: TYPE_NORMAL
- en: b) The best two-dimensional approximating subspace
  prefs: []
  type: TYPE_NORMAL
- en: c) The projection of the data onto the top singular vector
  prefs: []
  type: TYPE_NORMAL
- en: d) The projection of the data onto the top two singular vectors
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The lemma states that if \(\langle \mathbf{q}_1,
    \mathbf{x} \rangle < 0\), then the limit of \(A^k \mathbf{x} / \|A^k \mathbf{x}\|\)
    is \(-\mathbf{q}_1\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The lemma states that if \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle > 0\), then \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges
    to \(\mathbf{v}_1\), and if \(\langle \mathbf{v}_1, \mathbf{x} \rangle < 0\),
    then the limit is \(-\mathbf{v}_1\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: a. Justification: The text provides these formulas in the “Numerical
    Corner” section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: c. Justification: The key lemma states that convergence is ensured
    if \(\mathbf{x}\) is such that \(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq
    0\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: d. Justification: The text states that “projecting on the top
    two singular vectors… corresponds to finding the best two-dimensional approximating
    subspace. The projection can be computed using the truncated SVD \(Z = U_{(2)}
    \Sigma_{(2)} V_{(2)}^T\).”'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.1\. Key lemma[#](#key-lemma "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now derive the main idea behind an algorithm to compute singular vectors.
    Let \(U \Sigma V^T\) be a (compact) SVD of \(A\). Because of the orthogonality
    of \(U\) and \(V\), the powers of \(A^T A\) have a simple representation. Indeed
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T
    = V \Sigma^T \Sigma V^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Note that this formula is closely related to our previously uncovered connection
    between the SVD and the spectral decomposition of \(A^T A\) – although it is not
    quite a spectral decomposition of \(A^T A\) since \(V\) is not orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating,
  prefs: []
  type: TYPE_NORMAL
- en: \[ B^2 = (V \Sigma^T \Sigma V^T) (V \Sigma^T \Sigma V^T) = V (\Sigma^T \Sigma)^2
    V^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: and, for general \(k\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ B^{k} = V (\Sigma^T \Sigma)^{k} V^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, defining
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \widetilde{\Sigma} = \Sigma^T \Sigma = \begin{pmatrix} \sigma_1^2
    & 0 & \cdots & 0\\ 0 & \sigma_2^2 & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \cdots & \sigma_r^2 \end{pmatrix}, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: we see that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \widetilde{\Sigma}^k = \begin{pmatrix} \sigma_1^{2k} & 0 & \cdots
    & 0\\ 0 & \sigma_2^{2k} & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\ 0 &
    0 & \cdots & \sigma_r^{2k} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: When \(\sigma_1 > \sigma_2, \ldots, \sigma_r\), which is typically the case
    with real datasets, we get that \(\sigma_1^{2k} \gg \sigma_2^{2k}, \ldots, \sigma_r^{2k}\)
    when \(k\) is large. Then, we get the approximation
  prefs: []
  type: TYPE_NORMAL
- en: \[ B^{k} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T \approx \sigma_1^{2k}
    \mathbf{v}_1 \mathbf{v}_1^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we arrive at:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Power Iteration)** \(\idx{power iteration lemma}\xdi\) Let \(A
    \in \mathbb{R}^{n\times m}\) be a matrix and let \(U \Sigma V^T\) be a (compact)
    SVD of \(A\) such that \(\sigma_1 > \sigma_2 > 0\). Define \(B = A^T A\) and assume
    that \(\mathbf{x} \in \mathbb{R}^m\) is a vector satisfying \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle > 0\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \mathbf{v}_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(k \to +\infty\). If instead \(\langle \mathbf{v}_1, \mathbf{x} \rangle
    < 0\), then the limit is \(- \mathbf{v}_1\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use the approximation above and divide by the norm to get
    a unit norm vector in the direction of \(\mathbf{v}_1\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We have'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B^{k}\mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T
    \mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x}) \mathbf{v}_j.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} &= \sum_{j=1}^r
    \mathbf{v}_j \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k} \mathbf{x}\|}\\
    &= \mathbf{v}_1 \left\{\frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|}\right\} + \sum_{j=2}^r \mathbf{v}_j \left\{\frac{\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})} {\|B^{k} \mathbf{x}\|}\right\}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: This goes to \(\mathbf{v}_1\) as \(k\to +\infty\) if the expression in the first
    curly brackets goes to \(1\) and the one in the second curly brackets goes to
    \(0\). We prove this in the next claim.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** As \(k\to +\infty\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k} \mathbf{x}\|} \to
    1 \qquad \text{and} \qquad \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|} \to 0, \ j = 2,\ldots,r. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Because the \(\mathbf{v}_j\)s are an orthonormal basis,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \|B^{k}\mathbf{x}\|^2 = \sum_{j=1}^r \left[\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})\right]^2 = \sum_{j=1}^r \sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: So, as \(k\to +\infty\), using the fact that \(\mathbf{v}_1^T \mathbf{x} = \langle
    \mathbf{v}_1, \mathbf{x} \rangle \neq 0\) by assumption
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\|B^{k}\mathbf{x}\|^2}{\sigma_1^{4k} (\mathbf{v}_1^T
    \mathbf{x})^2} &= 1 + \sum_{j=2}^r \frac{\sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2}{\sigma_1^{4k}
    (\mathbf{v}_1^T \mathbf{x})^2}\\ &= 1 + \sum_{j=2}^r \left(\frac{\sigma_j}{\sigma_1}\right)^{4k}
    \frac{(\mathbf{v}_j^T \mathbf{x})^2}{(\mathbf{v}_1^T \mathbf{x})^2}\\ &\to 1,
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: since \(\sigma_j < \sigma_1\) for all \(j =2,\ldots,r\). That implies the first
    part of the claim by taking a square root and using \(\langle \mathbf{v}_1, \mathbf{x}
    \rangle > 0\). The second part of the claim follows essentially from the same
    argument. \(\square\) \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** We revisit the example'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We previously compute its SVD and found that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: This time we use the *Power Iteration Lemma*. Here
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = A^T A = \begin{pmatrix} 2 & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Taking powers of this matrix is easy
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B^k = \begin{pmatrix} 2^k & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Let’s choose an arbitrary initial vector \(\mathbf{x}\), say \((-1, 2)\). Then
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B^k \mathbf{x} = \begin{pmatrix} -2^k\\ 0 \end{pmatrix} \quad
    \text{and} \quad \|B^k \mathbf{x}\| = 2^k. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: So
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \begin{pmatrix}
    -1\\ 0 \end{pmatrix} = - \mathbf{v}_1, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: as \(k \to +\infty\). In fact, in this case, convergence occurs after one step.
    \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The argument leading to the *Power Iteration Lemma* also holds more generally
    for the eigenvectors of positive semidefinite matrices. Let \(A\) be a symmetric,
    positive semidefinite matrix in \(\mathbb{R}^{d \times d}\). By the *Spectral
    Theorem*, it has an eigenvector decomposition
  prefs: []
  type: TYPE_NORMAL
- en: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where further \(0 \leq \lambda_d \leq \cdots \leq \lambda_1\) by the *Characterization
    of Positive Semidefiniteness*. Because of the orthogonality of \(Q\), the powers
    of \(A\) have a simple representation. The square gives
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^2 = (Q \Lambda Q^T) (Q \Lambda Q^T) = Q \Lambda^2 Q^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: Repeating, we obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[ A^{k} = Q \Lambda^{k} Q^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Power Iteration)** \(\idx{power iteration lemma}\xdi\) Let \(A\)
    be a symmetric, positive semindefinite matrix in \(\mathbb{R}^{d \times d}\) with
    eigenvector decomposition \(A= Q \Lambda Q^T\) where the eigenvalues satisfy \(0
    \leq \lambda_d \leq \cdots \leq \lambda_2 < \lambda_1\). Assume that \(\mathbf{x}
    \in \mathbb{R}^d\) is a vector such that \(\langle \mathbf{q}_1, \mathbf{x} \rangle
    > 0\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{A^{k} \mathbf{x}}{\|A^{k} \mathbf{x}\|} \to \mathbf{q}_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: as \(k \to +\infty\). If instead \(\langle \mathbf{q}_1, \mathbf{x} \rangle
    < 0\), then the limit is \(- \mathbf{q}_1\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: The proof is similar to the case of singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2\. Computing the top singular vector[#](#computing-the-top-singular-vector
    "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Power iteration gives us a way to compute \(\mathbf{v}_1\) – at least approximately
    if we use a large enough \(k\). But how do we find an appropriate vector \(\mathbf{x}\),
    as required by the *Power Iteration Lemma*? It turns out that a random vector
    will do. For instance, let \(\mathbf{X}\) be an \(m\)-dimensional spherical Gaussian
    with mean \(0\) and variance \(1\). Then, \(\mathbb{P}[\langle \mathbf{v}_1, \mathbf{X}
    \rangle = 0] = 0\).
  prefs: []
  type: TYPE_NORMAL
- en: We implement the algorithm suggested by the *Power Iteration Lemma*. That is,
    we compute \(B^{k} \mathbf{x}\), then normalize it. To obtain the corresponding
    singular value and left singular vector, we use that \(\sigma_1 = \|A \mathbf{v}_1\|\)
    and \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We will apply it to our previous two-cluster example.
    The necessary functions are in [mmids.py](https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py),
    which is available on the [GitHub of the book](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/99720e27a1ef3efd196db79640dfdfe1c3d2d70b319ef77d803ab4437bbb954a.png](../Images/e66fa4144461a2ee2155a730c0c7df77.png)'
  prefs: []
  type: TYPE_IMG
- en: Let’s compute the top singular vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is approximately \(-\mathbf{e}_1\). We get roughly the same answer (possibly
    up to sign) from Python’s [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Recall that, when we applied \(k\)-means clustering to this example with \(d=1000\)
    dimension, we obtained a very poor clustering.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/7a49cf383a34072dd30fa0e2fed2db76615fa2692ba83fbdee9d0a6a770b3940.png](../Images/e2ab190c5a5c17937d5e2285a9c20f4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s try again, but after projecting on the top singular vector. Recall that
    this corresponds to finding the best one-dimensional approximating subspace. The
    projection can be computed using the truncated SVD \(Z= U_{(1)} \Sigma_{(1)} V_{(1)}^T\).
    We can interpret the rows of \(U_{(1)} \Sigma_{(1)}\) as the coefficients of each
    data point in the basis \(\mathbf{v}_1\). We will work in that basis. We need
    one small hack: because our implementation of \(k\)-means clustering expects data
    points in at least \(2\) dimension, we add a column of \(0\)’s.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/f2b508b6781d21eee9a679ee87894a523e040face9ef4d99ccd09cd38dab5959.png](../Images/d0d37edca421b5badf55590966b6d81a.png)'
  prefs: []
  type: TYPE_IMG
- en: There is a small – yet noticeable – gap around 0\. We run \(k\)-means clustering
    on the projected data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]</details> ![../../_images/b1c80c35b20bcf5c314f991485089bce5ec85beb6ae17ada744dd8f3b8a04ddc.png](../Images/5ffbd56fc722edd92c57dc038954354a.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Much better. We give a more formal explanation of this outcome in a subsequent
    section. In essence, quoting [BHK, Section 7.5.1]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[…] let’s understand the central advantage of doing the projection to [the
    top \(k\) right singular vectors]. It is simply that for any reasonable (unknown)
    clustering of data points, the projection brings data points closer to their cluster
    centers.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Finally, looking at the top right singular vector (or its first ten entries
    for lack of space), we see that it does align quite well (but not perfectly) with
    the first dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** There are other methods to compute the SVD. Ask your favorite
    AI chatbot about randomized algorithms for the SVD. What are their advantages
    in terms of computational efficiency for large matrices? \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** In the power iteration lemma for the positive semidefinite case, what
    happens when the initial vector \(\mathbf{x}\) satisfies \(\langle \mathbf{q}_1,
    \mathbf{x} \rangle < 0\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The iteration converges to \(\mathbf{q}_1\).
  prefs: []
  type: TYPE_NORMAL
- en: b) The iteration converges to \(-\mathbf{q}_1\).
  prefs: []
  type: TYPE_NORMAL
- en: c) The iteration does not converge.
  prefs: []
  type: TYPE_NORMAL
- en: d) The iteration converges to a random eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** In the power iteration lemma for the SVD case, what is the convergence
    result for a random vector \(\mathbf{x}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\mathbf{u}_1\).
  prefs: []
  type: TYPE_NORMAL
- en: b) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\mathbf{v}_1\) or
    \(-\mathbf{v}_1\).
  prefs: []
  type: TYPE_NORMAL
- en: c) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\sigma_1\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) does not converge.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Suppose you apply the power iteration method to a matrix \(A\) and obtain
    a vector \(\mathbf{v}\). How can you compute the corresponding singular value
    \(\sigma\) and left singular vector \(\mathbf{u}\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\sigma = \|A\mathbf{v}\|\) and \(\mathbf{u} = A\mathbf{v}/\sigma\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\sigma = \|A^T\mathbf{v}\|\) and \(\mathbf{u} = A^T\mathbf{v}/\sigma\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\sigma = \|\mathbf{v}\|\) and \(\mathbf{u} = \mathbf{v}/\sigma\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\sigma = 1\) and \(\mathbf{u} = A\mathbf{v}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** What is required for the initial vector \(\mathbf{x}\) in the power iteration
    method to ensure convergence to the top eigenvector?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{x}\) must be a zero vector.
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{x}\) must be orthogonal to the top eigenvector.
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq 0\).
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbf{x}\) must be the top eigenvector itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** What does the truncated SVD \(Z = U_{(2)} \Sigma_{(2)} V_{(2)}^T\) correspond
    to? [Uses Section 4.8.2.1.]'
  prefs: []
  type: TYPE_NORMAL
- en: a) The best one-dimensional approximating subspace
  prefs: []
  type: TYPE_NORMAL
- en: b) The best two-dimensional approximating subspace
  prefs: []
  type: TYPE_NORMAL
- en: c) The projection of the data onto the top singular vector
  prefs: []
  type: TYPE_NORMAL
- en: d) The projection of the data onto the top two singular vectors
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The lemma states that if \(\langle \mathbf{q}_1,
    \mathbf{x} \rangle < 0\), then the limit of \(A^k \mathbf{x} / \|A^k \mathbf{x}\|\)
    is \(-\mathbf{q}_1\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The lemma states that if \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle > 0\), then \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges
    to \(\mathbf{v}_1\), and if \(\langle \mathbf{v}_1, \mathbf{x} \rangle < 0\),
    then the limit is \(-\mathbf{v}_1\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: a. Justification: The text provides these formulas in the “Numerical
    Corner” section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: c. Justification: The key lemma states that convergence is ensured
    if \(\mathbf{x}\) is such that \(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq
    0\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: d. Justification: The text states that “projecting on the top
    two singular vectors… corresponds to finding the best two-dimensional approximating
    subspace. The projection can be computed using the truncated SVD \(Z = U_{(2)}
    \Sigma_{(2)} V_{(2)}^T\).”'
  prefs: []
  type: TYPE_NORMAL
