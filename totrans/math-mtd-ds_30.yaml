- en: 4.4\. Power iteration#
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4.4\. 力迭代#
- en: 原文：[https://mmids-textbook.github.io/chap04_svd/04_power/roch-mmids-svd-power.html](https://mmids-textbook.github.io/chap04_svd/04_power/roch-mmids-svd-power.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap04_svd/04_power/roch-mmids-svd-power.html](https://mmids-textbook.github.io/chap04_svd/04_power/roch-mmids-svd-power.html)
- en: There is in general [no exact method](https://math.stackexchange.com/questions/2582300/what-does-the-author-mean-by-no-method-exists-for-exactly-computing-the-eigenva)
    for computing SVDs. Instead we must rely on iterative methods, that is, methods
    that progressively approach the solution. We describe in this section the power
    iteration method. This method is behind an effective numerical approach for computing
    SVDs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，[没有精确的方法](https://math.stackexchange.com/questions/2582300/what-does-the-author-mean-by-no-method-exists-for-exactly-computing-the-eigenva)来计算奇异值分解。相反，我们必须依赖于迭代方法，即逐步接近解的方法。在本节中，我们描述了力迭代方法。这种方法是计算奇异值分解的有效数值方法背后的方法。
- en: The focus here is on numerical methods and we will not spend much time computing
    SVDs by hand. But note that the connection between the SVD and the spectral decomposition
    of \(A^T A\) can be used for this purpose on small examples.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这里关注的是数值方法，我们不会花太多时间手动计算奇异值分解。但请注意，SVD与 \(A^T A\) 的谱分解之间的联系可以用于小例子的这个目的。
- en: 4.4.1\. Key lemma[#](#key-lemma "Link to this heading")
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4.1\. 关键引理[#](#key-lemma "链接到这个标题")
- en: We now derive the main idea behind an algorithm to compute singular vectors.
    Let \(U \Sigma V^T\) be a (compact) SVD of \(A\). Because of the orthogonality
    of \(U\) and \(V\), the powers of \(A^T A\) have a simple representation. Indeed
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在推导出计算奇异向量的算法背后的主要思想。设 \(U \Sigma V^T\) 是 \(A\) 的一个（紧凑）奇异值分解。由于 \(U\) 和 \(V\)
    的正交性，\(A^T A\) 的幂有简单的表示。确实
- en: \[ B = A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T
    = V \Sigma^T \Sigma V^T. \]
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B = A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T
    = V \Sigma^T \Sigma V^T. \]
- en: Note that this formula is closely related to our previously uncovered connection
    between the SVD and the spectral decomposition of \(A^T A\) – although it is not
    quite a spectral decomposition of \(A^T A\) since \(V\) is not orthogonal.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个公式与我们之前未发现的SVD与 \(A^T A\) 的谱分解之间的联系密切相关——尽管它并不是 \(A^T A\) 的谱分解，因为 \(V\)
    不是正交的。
- en: Iterating,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代，
- en: \[ B^2 = (V \Sigma^T \Sigma V^T) (V \Sigma^T \Sigma V^T) = V (\Sigma^T \Sigma)^2
    V^T, \]
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B^2 = (V \Sigma^T \Sigma V^T) (V \Sigma^T \Sigma V^T) = V (\Sigma^T \Sigma)^2
    V^T, \]
- en: and, for general \(k\),
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，对于一般的 \(k\),
- en: \[ B^{k} = V (\Sigma^T \Sigma)^{k} V^T. \]
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B^{k} = V (\Sigma^T \Sigma)^{k} V^T. \]
- en: Hence, defining
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，定义
- en: \[\begin{split} \widetilde{\Sigma} = \Sigma^T \Sigma = \begin{pmatrix} \sigma_1^2
    & 0 & \cdots & 0\\ 0 & \sigma_2^2 & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \cdots & \sigma_r^2 \end{pmatrix}, \end{split}\]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \widetilde{\Sigma} = \Sigma^T \Sigma = \begin{pmatrix} \sigma_1^2
    & 0 & \cdots & 0\\ 0 & \sigma_2^2 & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \cdots & \sigma_r^2 \end{pmatrix}, \end{split}\]
- en: we see that
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到
- en: \[\begin{split} \widetilde{\Sigma}^k = \begin{pmatrix} \sigma_1^{2k} & 0 & \cdots
    & 0\\ 0 & \sigma_2^{2k} & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\ 0 &
    0 & \cdots & \sigma_r^{2k} \end{pmatrix}. \end{split}\]
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \widetilde{\Sigma}^k = \begin{pmatrix} \sigma_1^{2k} & 0 & \cdots
    & 0\\ 0 & \sigma_2^{2k} & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\ 0 &
    0 & \cdots & \sigma_r^{2k} \end{pmatrix}. \end{split}\]
- en: When \(\sigma_1 > \sigma_2, \ldots, \sigma_r\), which is typically the case
    with real datasets, we get that \(\sigma_1^{2k} \gg \sigma_2^{2k}, \ldots, \sigma_r^{2k}\)
    when \(k\) is large. Then, we get the approximation
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(\sigma_1 > \sigma_2, \ldots, \sigma_r\) 时，这在现实数据集中通常是情况，当 \(k\) 很大时，我们得到
    \(\sigma_1^{2k} \gg \sigma_2^{2k}, \ldots, \sigma_r^{2k}\)。然后，我们得到近似
- en: \[ B^{k} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T \approx \sigma_1^{2k}
    \mathbf{v}_1 \mathbf{v}_1^T. \]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B^{k} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T \approx \sigma_1^{2k}
    \mathbf{v}_1 \mathbf{v}_1^T. \]
- en: 'Finally, we arrive at:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到：
- en: '**LEMMA** **(Power Iteration)** \(\idx{power iteration lemma}\xdi\) Let \(A
    \in \mathbb{R}^{n\times m}\) be a matrix and let \(U \Sigma V^T\) be a (compact)
    SVD of \(A\) such that \(\sigma_1 > \sigma_2 > 0\). Define \(B = A^T A\) and assume
    that \(\mathbf{x} \in \mathbb{R}^m\) is a vector satisfying \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle > 0\). Then'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(力迭代)** \(\idx{power iteration lemma}\xdi\) 设 \(A \in \mathbb{R}^{n\times
    m}\) 是一个矩阵，并且设 \(U \Sigma V^T\) 是 \(A\) 的一个（紧凑）奇异值分解，使得 \(\sigma_1 > \sigma_2
    > 0\)。定义 \(B = A^T A\) 并假设 \(\mathbf{x} \in \mathbb{R}^m\) 是一个向量，满足 \(\langle
    \mathbf{v}_1, \mathbf{x} \rangle > 0\)。那么'
- en: \[ \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \mathbf{v}_1 \]
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \mathbf{v}_1 \]
- en: as \(k \to +\infty\). If instead \(\langle \mathbf{v}_1, \mathbf{x} \rangle
    < 0\), then the limit is \(- \mathbf{v}_1\). \(\flat\)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(k \to +\infty\)。如果 \(\langle \mathbf{v}_1, \mathbf{x} \rangle < 0\)，则极限是
    \(- \mathbf{v}_1\)。\(\flat\)
- en: '*Proof idea:* We use the approximation above and divide by the norm to get
    a unit norm vector in the direction of \(\mathbf{v}_1\).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路*: 我们使用上述近似并除以范数，以获得一个单位范数向量，其方向为 \(\mathbf{v}_1\)。'
- en: '*Proof:* We have'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*: 我们有'
- en: \[ B^{k}\mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T
    \mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x}) \mathbf{v}_j.
    \]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B^{k}\mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T
    \mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x}) \mathbf{v}_j.
    \]
- en: So
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以
- en: \[\begin{align*} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} &= \sum_{j=1}^r
    \mathbf{v}_j \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k} \mathbf{x}\|}\\
    &= \mathbf{v}_1 \left\{\frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|}\right\} + \sum_{j=2}^r \mathbf{v}_j \left\{\frac{\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})} {\|B^{k} \mathbf{x}\|}\right\}. \end{align*}\]
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} &= \sum_{j=1}^r
    \mathbf{v}_j \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k} \mathbf{x}\|}\\
    &= \mathbf{v}_1 \left\{\frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|}\right\} + \sum_{j=2}^r \mathbf{v}_j \left\{\frac{\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})} {\|B^{k} \mathbf{x}\|}\right\}. \end{align*}\]
- en: This goes to \(\mathbf{v}_1\) as \(k\to +\infty\) if the expression in the first
    curly brackets goes to \(1\) and the one in the second curly brackets goes to
    \(0\). We prove this in the next claim.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(k\to +\infty\) 时，如果第一个括号中的表达式趋于 \(1\)，第二个括号中的表达式趋于 \(0\)，则该表达式趋于 \(\mathbf{v}_1\)。我们将在下一个引理中证明这一点。
- en: '**LEMMA** As \(k\to +\infty\),'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理:** 当 \(k\to +\infty\) 时，'
- en: \[ \frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k} \mathbf{x}\|} \to
    1 \qquad \text{and} \qquad \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|} \to 0, \ j = 2,\ldots,r. \]
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k} \mathbf{x}\|} \to
    1 \qquad \text{and} \qquad \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|} \to 0, \ j = 2,\ldots,r. \]
- en: \(\flat\)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* Because the \(\mathbf{v}_j\)s are an orthonormal basis,'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*: 因为 \(\mathbf{v}_j\) 是一个正交归一基，'
- en: \[ \|B^{k}\mathbf{x}\|^2 = \sum_{j=1}^r \left[\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})\right]^2 = \sum_{j=1}^r \sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2.
    \]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|B^{k}\mathbf{x}\|^2 = \sum_{j=1}^r \left[\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})\right]^2 = \sum_{j=1}^r \sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2.
    \]
- en: So, as \(k\to +\infty\), using the fact that \(\mathbf{v}_1^T \mathbf{x} = \langle
    \mathbf{v}_1, \mathbf{x} \rangle \neq 0\) by assumption
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 \(k\to +\infty\) 时，利用 \(\mathbf{v}_1^T \mathbf{x} = \langle \mathbf{v}_1,
    \mathbf{x} \rangle \neq 0\) 这一假设
- en: \[\begin{align*} \frac{\|B^{k}\mathbf{x}\|^2}{\sigma_1^{4k} (\mathbf{v}_1^T
    \mathbf{x})^2} &= 1 + \sum_{j=2}^r \frac{\sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2}{\sigma_1^{4k}
    (\mathbf{v}_1^T \mathbf{x})^2}\\ &= 1 + \sum_{j=2}^r \left(\frac{\sigma_j}{\sigma_1}\right)^{4k}
    \frac{(\mathbf{v}_j^T \mathbf{x})^2}{(\mathbf{v}_1^T \mathbf{x})^2}\\ &\to 1,
    \end{align*}\]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\|B^{k}\mathbf{x}\|^2}{\sigma_1^{4k} (\mathbf{v}_1^T
    \mathbf{x})^2} &= 1 + \sum_{j=2}^r \frac{\sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2}{\sigma_1^{4k}
    (\mathbf{v}_1^T \mathbf{x})^2}\\ &= 1 + \sum_{j=2}^r \left(\frac{\sigma_j}{\sigma_1}\right)^{4k}
    \frac{(\mathbf{v}_j^T \mathbf{x})^2}{(\mathbf{v}_1^T \mathbf{x})^2}\\ &\to 1,
    \end{align*}\]
- en: since \(\sigma_j < \sigma_1\) for all \(j =2,\ldots,r\). That implies the first
    part of the claim by taking a square root and using \(\langle \mathbf{v}_1, \mathbf{x}
    \rangle > 0\). The second part of the claim follows essentially from the same
    argument. \(\square\) \(\square\)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因为对于所有 \(j =2,\ldots,r\)，\(\sigma_j < \sigma_1\)。这意味着通过取平方根并使用 \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle > 0\)，可以得出断言的第一部分。第二部分基本上是从相同的论证中得出的。\(\square\) \(\square\)
- en: '**EXAMPLE:** We revisit the example'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例:** 我们重新审视这个例子'
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
- en: We previously compute its SVD and found that
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前计算了它的奇异值分解（SVD）并发现
- en: \[\begin{split} \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}. \end{split}\]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}. \end{split}\]
- en: This time we use the *Power Iteration Lemma*. Here
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们使用 *幂迭代引理*。这里
- en: \[\begin{split} B = A^T A = \begin{pmatrix} 2 & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} B = A^T A = \begin{pmatrix} 2 & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
- en: Taking powers of this matrix is easy
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这个矩阵的幂是容易的
- en: \[\begin{split} B^k = \begin{pmatrix} 2^k & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} B^k = \begin{pmatrix} 2^k & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
- en: Let’s choose an arbitrary initial vector \(\mathbf{x}\), say \((-1, 2)\). Then
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择一个任意的初始向量 \(\mathbf{x}\)，比如说 \((-1, 2)\)。然后
- en: \[\begin{split} B^k \mathbf{x} = \begin{pmatrix} -2^k\\ 0 \end{pmatrix} \quad
    \text{and} \quad \|B^k \mathbf{x}\| = 2^k. \end{split}\]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} B^k \mathbf{x} = \begin{pmatrix} -2^k\\ 0 \end{pmatrix} \quad
    \text{和} \quad \|B^k \mathbf{x}\| = 2^k. \end{split}\]
- en: So
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: \[\begin{split} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \begin{pmatrix}
    -1\\ 0 \end{pmatrix} = - \mathbf{v}_1, \end{split}\]
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \begin{pmatrix}
    -1\\ 0 \end{pmatrix} = - \mathbf{v}_1, \end{split}\]
- en: as \(k \to +\infty\). In fact, in this case, convergence occurs after one step.
    \(\lhd\)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(k \to +\infty\)。实际上，在这种情况下，收敛发生在一步之后。\(\lhd\)
- en: The argument leading to the *Power Iteration Lemma* also holds more generally
    for the eigenvectors of positive semidefinite matrices. Let \(A\) be a symmetric,
    positive semidefinite matrix in \(\mathbb{R}^{d \times d}\). By the *Spectral
    Theorem*, it has an eigenvector decomposition
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 导致 **幂迭代引理** 的论据也普遍适用于正半定矩阵的特征向量。设 \(A\) 是一个在 \(\mathbb{R}^{d \times d}\) 中的对称、正半定矩阵。根据
    **谱定理**，它有一个特征向量分解
- en: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
- en: where further \(0 \leq \lambda_d \leq \cdots \leq \lambda_1\) by the *Characterization
    of Positive Semidefiniteness*. Because of the orthogonality of \(Q\), the powers
    of \(A\) have a simple representation. The square gives
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，进一步地，由于 **正半定性的特征**，有 \(0 \leq \lambda_d \leq \cdots \leq \lambda_1\)。由于
    \(Q\) 的正交性，\(A\) 的幂有一个简单的表示。平方给出
- en: \[ A^2 = (Q \Lambda Q^T) (Q \Lambda Q^T) = Q \Lambda^2 Q^T. \]
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^2 = (Q \Lambda Q^T) (Q \Lambda Q^T) = Q \Lambda^2 Q^T. \]
- en: Repeating, we obtain
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 重复，我们得到
- en: \[ A^{k} = Q \Lambda^{k} Q^T. \]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{k} = Q \Lambda^{k} Q^T. \]
- en: 'This leads to the following:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下结果：
- en: '**LEMMA** **(Power Iteration)** \(\idx{power iteration lemma}\xdi\) Let \(A\)
    be a symmetric, positive semindefinite matrix in \(\mathbb{R}^{d \times d}\) with
    eigenvector decomposition \(A= Q \Lambda Q^T\) where the eigenvalues satisfy \(0
    \leq \lambda_d \leq \cdots \leq \lambda_2 < \lambda_1\). Assume that \(\mathbf{x}
    \in \mathbb{R}^d\) is a vector such that \(\langle \mathbf{q}_1, \mathbf{x} \rangle
    > 0\). Then'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **（幂迭代法）** \(\idx{power iteration lemma}\xdi\) 设 \(A\) 是一个在 \(\mathbb{R}^{d
    \times d}\) 中的对称、正半定矩阵，其特征向量分解为 \(A= Q \Lambda Q^T\)，其中特征值满足 \(0 \leq \lambda_d
    \leq \cdots \leq \lambda_2 < \lambda_1\)。假设 \(\mathbf{x} \in \mathbb{R}^d\) 是一个向量，使得
    \(\langle \mathbf{q}_1, \mathbf{x} \rangle > 0\)。那么'
- en: \[ \frac{A^{k} \mathbf{x}}{\|A^{k} \mathbf{x}\|} \to \mathbf{q}_1 \]
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{A^{k} \mathbf{x}}{\|A^{k} \mathbf{x}\|} \to \mathbf{q}_1 \]
- en: as \(k \to +\infty\). If instead \(\langle \mathbf{q}_1, \mathbf{x} \rangle
    < 0\), then the limit is \(- \mathbf{q}_1\). \(\flat\)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(k \to +\infty\)。如果 \(\langle \mathbf{q}_1, \mathbf{x} \rangle < 0\)，则极限是
    \(- \mathbf{q}_1\)。\(\flat\)
- en: The proof is similar to the case of singular vectors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 证明与奇异向量的情况类似。
- en: 4.4.2\. Computing the top singular vector[#](#computing-the-top-singular-vector
    "Link to this heading")
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4.2\. 计算最高奇异向量[#](#computing-the-top-singular-vector "链接到本标题")
- en: Power iteration gives us a way to compute \(\mathbf{v}_1\) – at least approximately
    if we use a large enough \(k\). But how do we find an appropriate vector \(\mathbf{x}\),
    as required by the *Power Iteration Lemma*? It turns out that a random vector
    will do. For instance, let \(\mathbf{X}\) be an \(m\)-dimensional spherical Gaussian
    with mean \(0\) and variance \(1\). Then, \(\mathbb{P}[\langle \mathbf{v}_1, \mathbf{X}
    \rangle = 0] = 0\).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 幂迭代法为我们提供了一种计算 \(\mathbf{v}_1\) 的方法——至少如果我们使用足够大的 \(k\)，可以得到一个近似值。但如何找到符合 **幂迭代引理**
    要求的适当向量 \(\mathbf{x}\)？结果是一个随机向量就可以。例如，设 \(\mathbf{X}\) 是一个 \(m\) 维球面高斯，均值为 \(0\)，方差为
    \(1\)。那么，\(\mathbb{P}[\langle \mathbf{v}_1, \mathbf{X} \rangle = 0] = 0\)。
- en: We implement the algorithm suggested by the *Power Iteration Lemma*. That is,
    we compute \(B^{k} \mathbf{x}\), then normalize it. To obtain the corresponding
    singular value and left singular vector, we use that \(\sigma_1 = \|A \mathbf{v}_1\|\)
    and \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1\).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了由 **幂迭代引理** 提出的算法。也就是说，我们计算 \(B^{k} \mathbf{x}\)，然后对其进行归一化。为了获得相应的奇异值和左奇异向量，我们使用
    \(\sigma_1 = \|A \mathbf{v}_1\|\) 和 \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1\)。
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**NUMERICAL CORNER:** We will apply it to our previous two-cluster example.
    The necessary functions are in [mmids.py](https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py),
    which is available on the [GitHub of the book](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**：我们将将其应用于我们之前提到的两个簇示例。必要的函数在 [mmids.py](https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py)
    中，该文件可在 [本书的GitHub](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main)
    上找到。'
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![../../_images/99720e27a1ef3efd196db79640dfdfe1c3d2d70b319ef77d803ab4437bbb954a.png](../Images/e66fa4144461a2ee2155a730c0c7df77.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片链接](../Images/e66fa4144461a2ee2155a730c0c7df77.png)'
- en: Let’s compute the top singular vector.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算最大的奇异向量。
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This is approximately \(-\mathbf{e}_1\). We get roughly the same answer (possibly
    up to sign) from Python’s [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)
    function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这大约是 \(-\mathbf{e}_1\)。我们从 Python 的 `numpy.linalg.svd` 函数（[链接](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)）中得到大致相同的答案（可能包括符号）。
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Recall that, when we applied \(k\)-means clustering to this example with \(d=1000\)
    dimension, we obtained a very poor clustering.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，当我们将 \(k\)-means 聚类应用于 \(d=1000\) 维的示例时，我们得到了一个非常差的聚类结果。
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![../../_images/7a49cf383a34072dd30fa0e2fed2db76615fa2692ba83fbdee9d0a6a770b3940.png](../Images/e2ab190c5a5c17937d5e2285a9c20f4c.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片链接](../Images/e2ab190c5a5c17937d5e2285a9c20f4c.png)'
- en: 'Let’s try again, but after projecting on the top singular vector. Recall that
    this corresponds to finding the best one-dimensional approximating subspace. The
    projection can be computed using the truncated SVD \(Z= U_{(1)} \Sigma_{(1)} V_{(1)}^T\).
    We can interpret the rows of \(U_{(1)} \Sigma_{(1)}\) as the coefficients of each
    data point in the basis \(\mathbf{v}_1\). We will work in that basis. We need
    one small hack: because our implementation of \(k\)-means clustering expects data
    points in at least \(2\) dimension, we add a column of \(0\)’s.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次尝试，但这次在投影到最大的奇异向量之后。回想一下，这对应于找到最佳的一维逼近子空间。投影可以使用截断奇异值分解 \(Z= U_{(1)} \Sigma_{(1)}
    V_{(1)}^T\) 来计算。我们可以将 \(U_{(1)} \Sigma_{(1)}\) 的行解释为每个数据点在基 \(\mathbf{v}_1\) 中的系数。我们将在这个基上工作。我们需要一个小技巧：因为我们的
    \(k\)-means 聚类实现期望数据点至少在 \(2\) 维，所以我们添加了一列 \(0\)。
- en: '[PRE9]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![../../_images/f2b508b6781d21eee9a679ee87894a523e040face9ef4d99ccd09cd38dab5959.png](../Images/d0d37edca421b5badf55590966b6d81a.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片链接](../Images/d0d37edca421b5badf55590966b6d81a.png)'
- en: There is a small – yet noticeable – gap around 0\. We run \(k\)-means clustering
    on the projected data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 0 附近有一个小而明显的间隙。我们在投影数据上运行 \(k\)-means 聚类。
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源代码
    隐藏代码单元格源代码</summary>
- en: '[PRE12]</details> ![../../_images/b1c80c35b20bcf5c314f991485089bce5ec85beb6ae17ada744dd8f3b8a04ddc.png](../Images/5ffbd56fc722edd92c57dc038954354a.png)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE12][详情] ![图片链接](../Images/5ffbd56fc722edd92c57dc038954354a.png)'
- en: 'Much better. We give a more formal explanation of this outcome in a subsequent
    section. In essence, quoting [BHK, Section 7.5.1]:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 好多了。我们将在后续部分更正式地解释这一结果。本质上，引用 [BHK，第 7.5.1 节]：
- en: '[…] let’s understand the central advantage of doing the projection to [the
    top \(k\) right singular vectors]. It is simply that for any reasonable (unknown)
    clustering of data points, the projection brings data points closer to their cluster
    centers.'
  id: totrans-87
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[…] 让我们理解进行投影到 [前 \(k\) 个右奇异向量] 的中心优势。简单地说，对于任何合理（未知）的数据点聚类，投影将数据点带到其簇中心附近。'
- en: Finally, looking at the top right singular vector (or its first ten entries
    for lack of space), we see that it does align quite well (but not perfectly) with
    the first dimension.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，查看最大的右上奇异向量（或由于空间不足而只查看其前十个条目），我们看到它与第一个维度相当一致（但不是完美一致）。
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: \(\unlhd\)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** There are other methods to compute the SVD. Ask your favorite
    AI chatbot about randomized algorithms for the SVD. What are their advantages
    in terms of computational efficiency for large matrices? \(\ddagger\)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 计算奇异值分解（SVD）还有其他方法。向您最喜欢的 AI 聊天机器人询问 SVD 的随机算法。它们在处理大型矩阵时在计算效率方面的优势是什么？\(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由 Claude、Gemini 和 ChatGPT 协助)*'
- en: '**1** In the power iteration lemma for the positive semidefinite case, what
    happens when the initial vector \(\mathbf{x}\) satisfies \(\langle \mathbf{q}_1,
    \mathbf{x} \rangle < 0\)?'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在正半定情况下的幂迭代引理中，如果初始向量 \(\mathbf{x}\) 满足 \(\langle \mathbf{q}_1, \mathbf{x}
    \rangle < 0\)，会发生什么？'
- en: a) The iteration converges to \(\mathbf{q}_1\).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: a) 迭代收敛到 \(\mathbf{q}_1\)。
- en: b) The iteration converges to \(-\mathbf{q}_1\).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: b) 迭代收敛到 \(-\mathbf{q}_1\)。
- en: c) The iteration does not converge.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: c) 迭代没有收敛。
- en: d) The iteration converges to a random eigenvector.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: d) 迭代收敛到一个随机特征向量。
- en: '**2** In the power iteration lemma for the SVD case, what is the convergence
    result for a random vector \(\mathbf{x}\)?'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 在 SVD 情况下的幂迭代引理中，随机向量 \(\mathbf{x}\) 的收敛结果是什么？'
- en: a) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\mathbf{u}_1\).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) 收敛到 \(\mathbf{u}_1\)。
- en: b) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\mathbf{v}_1\) or
    \(-\mathbf{v}_1\).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) 收敛到 \(\mathbf{v}_1\) 或 \(-\mathbf{v}_1\)。
- en: c) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\sigma_1\).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: c) \((B^k \mathbf{x}) / \|B^k \mathbf{x}\|\) 收敛到 \(\sigma_1\)。
- en: d) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) does not converge.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: d) \((B^k \mathbf{x}) / \|B^k \mathbf{x}\|\) 不收敛。
- en: '**3** Suppose you apply the power iteration method to a matrix \(A\) and obtain
    a vector \(\mathbf{v}\). How can you compute the corresponding singular value
    \(\sigma\) and left singular vector \(\mathbf{u}\)?'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 假设你对矩阵 \(A\) 应用幂迭代法并得到一个向量 \(\mathbf{v}\)。你如何计算相应的奇异值 \(\sigma\) 和左奇异向量
    \(\mathbf{u}\)?'
- en: a) \(\sigma = \|A\mathbf{v}\|\) and \(\mathbf{u} = A\mathbf{v}/\sigma\)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\sigma = \|A\mathbf{v}\|\) 和 \(\mathbf{u} = A\mathbf{v}/\sigma\)
- en: b) \(\sigma = \|A^T\mathbf{v}\|\) and \(\mathbf{u} = A^T\mathbf{v}/\sigma\)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\sigma = \|A^T\mathbf{v}\|\) 和 \(\mathbf{u} = A^T\mathbf{v}/\sigma\)
- en: c) \(\sigma = \|\mathbf{v}\|\) and \(\mathbf{u} = \mathbf{v}/\sigma\)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\sigma = \|\mathbf{v}\|\) 和 \(\mathbf{u} = \mathbf{v}/\sigma\)
- en: d) \(\sigma = 1\) and \(\mathbf{u} = A\mathbf{v}\)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\sigma = 1\) 和 \(\mathbf{u} = A\mathbf{v}\)
- en: '**4** What is required for the initial vector \(\mathbf{x}\) in the power iteration
    method to ensure convergence to the top eigenvector?'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在幂迭代法中，初始向量 \(\mathbf{x}\) 需要满足什么条件才能确保收敛到最大的特征向量？'
- en: a) \(\mathbf{x}\) must be a zero vector.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathbf{x}\) 必须是零向量。
- en: b) \(\mathbf{x}\) must be orthogonal to the top eigenvector.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathbf{x}\) 必须与最大的特征向量正交。
- en: c) \(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq 0\).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq 0\)。
- en: d) \(\mathbf{x}\) must be the top eigenvector itself.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathbf{x}\) 必须是最大的特征向量本身。
- en: '**5** What does the truncated SVD \(Z = U_{(2)} \Sigma_{(2)} V_{(2)}^T\) correspond
    to? [Uses Section 4.8.2.1.]'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 截断 SVD \(Z = U_{(2)} \Sigma_{(2)} V_{(2)}^T\) 对应的是什么？[使用第 4.8.2.1 节]'
- en: a) The best one-dimensional approximating subspace
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: a) 最佳一维逼近子空间
- en: b) The best two-dimensional approximating subspace
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: b) 最佳二维逼近子空间
- en: c) The projection of the data onto the top singular vector
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: c) 数据在最大奇异向量上的投影
- en: d) The projection of the data onto the top two singular vectors
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: d) 数据在最大两个奇异向量上的投影
- en: 'Answer for 1: b. Justification: The lemma states that if \(\langle \mathbf{q}_1,
    \mathbf{x} \rangle < 0\), then the limit of \(A^k \mathbf{x} / \|A^k \mathbf{x}\|\)
    is \(-\mathbf{q}_1\).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 1 的答案：b. 证明：引理表明，如果 \(\langle \mathbf{q}_1, \mathbf{x} \rangle < 0\)，则 \(A^k
    \mathbf{x} / \|A^k \mathbf{x}\|\) 的极限是 \(-\mathbf{q}_1\)。
- en: 'Answer for 2: b. Justification: The lemma states that if \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle > 0\), then \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges
    to \(\mathbf{v}_1\), and if \(\langle \mathbf{v}_1, \mathbf{x} \rangle < 0\),
    then the limit is \(-\mathbf{v}_1\).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 2 的答案：b. 证明：引理表明，如果 \(\langle \mathbf{v}_1, \mathbf{x} \rangle > 0\)，则 \(B^k
    \mathbf{x} / \|B^k \mathbf{x}\|\) 收敛到 \(\mathbf{v}_1\)，如果 \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle < 0\)，则极限是 \(-\mathbf{v}_1\)。
- en: 'Answer for 3: a. Justification: The text provides these formulas in the “Numerical
    Corner” section.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 3 的答案：a. 证明：文本在“数值角落”部分提供了这些公式。
- en: 'Answer for 4: c. Justification: The key lemma states that convergence is ensured
    if \(\mathbf{x}\) is such that \(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq
    0\).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 4 的答案：c. 证明：关键引理表明，如果 \(\mathbf{x}\) 满足 \(\langle \mathbf{q}_1, \mathbf{x} \rangle
    \neq 0\)，则可以保证收敛。
- en: 'Answer for 5: d. Justification: The text states that “projecting on the top
    two singular vectors… corresponds to finding the best two-dimensional approximating
    subspace. The projection can be computed using the truncated SVD \(Z = U_{(2)}
    \Sigma_{(2)} V_{(2)}^T\).”'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 5 的答案：d. 证明：文本中提到，“在最大的两个奇异向量上投影……对应于找到最佳二维逼近子空间。投影可以使用截断 SVD \(Z = U_{(2)}
    \Sigma_{(2)} V_{(2)}^T\) 来计算。”
- en: 4.4.1\. Key lemma[#](#key-lemma "Link to this heading")
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4.1\. 关键引理[#](#key-lemma "链接到这个标题")
- en: We now derive the main idea behind an algorithm to compute singular vectors.
    Let \(U \Sigma V^T\) be a (compact) SVD of \(A\). Because of the orthogonality
    of \(U\) and \(V\), the powers of \(A^T A\) have a simple representation. Indeed
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在推导出计算奇异向量的算法背后的主要思想。设 \(U \Sigma V^T\) 是 \(A\) 的（紧凑）奇异值分解。由于 \(U\) 和 \(V\)
    的正交性，\(A^T A\) 的幂有简单的表示。确实
- en: \[ B = A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T
    = V \Sigma^T \Sigma V^T. \]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B = A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T
    = V \Sigma^T \Sigma V^T. \]
- en: Note that this formula is closely related to our previously uncovered connection
    between the SVD and the spectral decomposition of \(A^T A\) – although it is not
    quite a spectral decomposition of \(A^T A\) since \(V\) is not orthogonal.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个公式与我们之前未发现的SVD与 \(A^T A\) 的谱分解之间的联系密切相关——尽管它并不是 \(A^T A\) 的谱分解，因为 \(V\)
    不是正交的。
- en: Iterating,
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代，
- en: \[ B^2 = (V \Sigma^T \Sigma V^T) (V \Sigma^T \Sigma V^T) = V (\Sigma^T \Sigma)^2
    V^T, \]
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B^2 = (V \Sigma^T \Sigma V^T) (V \Sigma^T \Sigma V^T) = V (\Sigma^T \Sigma)^2
    V^T, \]
- en: and, for general \(k\),
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一般的 \(k\)，
- en: \[ B^{k} = V (\Sigma^T \Sigma)^{k} V^T. \]
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B^{k} = V (\Sigma^T \Sigma)^{k} V^T. \]
- en: Hence, defining
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，定义
- en: \[\begin{split} \widetilde{\Sigma} = \Sigma^T \Sigma = \begin{pmatrix} \sigma_1^2
    & 0 & \cdots & 0\\ 0 & \sigma_2^2 & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \cdots & \sigma_r^2 \end{pmatrix}, \end{split}\]
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \widetilde{\Sigma} = \Sigma^T \Sigma = \begin{pmatrix} \sigma_1^2
    & 0 & \cdots & 0\\ 0 & \sigma_2^2 & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \cdots & \sigma_r^2 \end{pmatrix}, \end{split}\]
- en: we see that
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，
- en: \[\begin{split} \widetilde{\Sigma}^k = \begin{pmatrix} \sigma_1^{2k} & 0 & \cdots
    & 0\\ 0 & \sigma_2^{2k} & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\ 0 &
    0 & \cdots & \sigma_r^{2k} \end{pmatrix}. \end{split}\]
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \widetilde{\Sigma}^k = \begin{pmatrix} \sigma_1^{2k} & 0 & \cdots
    & 0\\ 0 & \sigma_2^{2k} & \cdots & 0\\ \vdots & \vdots & \ddots & \vdots\\ 0 &
    0 & \cdots & \sigma_r^{2k} \end{pmatrix}. \end{split}\]
- en: When \(\sigma_1 > \sigma_2, \ldots, \sigma_r\), which is typically the case
    with real datasets, we get that \(\sigma_1^{2k} \gg \sigma_2^{2k}, \ldots, \sigma_r^{2k}\)
    when \(k\) is large. Then, we get the approximation
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(\sigma_1 > \sigma_2, \ldots, \sigma_r\) 时，这在实数据集中通常是情况，当 \(k\) 很大时，我们得到
    \(\sigma_1^{2k} \gg \sigma_2^{2k}, \ldots, \sigma_r^{2k}\)。然后，我们得到近似
- en: \[ B^{k} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T \approx \sigma_1^{2k}
    \mathbf{v}_1 \mathbf{v}_1^T. \]
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B^{k} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T \approx \sigma_1^{2k}
    \mathbf{v}_1 \mathbf{v}_1^T. \]
- en: 'Finally, we arrive at:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到：
- en: '**LEMMA** **(Power Iteration)** \(\idx{power iteration lemma}\xdi\) Let \(A
    \in \mathbb{R}^{n\times m}\) be a matrix and let \(U \Sigma V^T\) be a (compact)
    SVD of \(A\) such that \(\sigma_1 > \sigma_2 > 0\). Define \(B = A^T A\) and assume
    that \(\mathbf{x} \in \mathbb{R}^m\) is a vector satisfying \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle > 0\). Then'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(幂迭代)** \(\idx{power iteration lemma}\xdi\) 设 \(A \in \mathbb{R}^{n\times
    m}\) 是一个矩阵，并且设 \(U \Sigma V^T\) 是 \(A\) 的一个（紧凑的）奇异值分解，使得 \(\sigma_1 > \sigma_2
    > 0\)。定义 \(B = A^T A\) 并假设 \(\mathbf{x} \in \mathbb{R}^m\) 是一个向量，满足 \(\langle
    \mathbf{v}_1, \mathbf{x} \rangle > 0\)。那么'
- en: \[ \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \mathbf{v}_1 \]
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \mathbf{v}_1 \]
- en: as \(k \to +\infty\). If instead \(\langle \mathbf{v}_1, \mathbf{x} \rangle
    < 0\), then the limit is \(- \mathbf{v}_1\). \(\flat\)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(k \to +\infty\) 时。如果 \(\langle \mathbf{v}_1, \mathbf{x} \rangle < 0\)，则极限为
    \(- \mathbf{v}_1\)。 \(\flat\)
- en: '*Proof idea:* We use the approximation above and divide by the norm to get
    a unit norm vector in the direction of \(\mathbf{v}_1\).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明思路：* 我们使用上述近似并除以范数，得到一个方向为 \(\mathbf{v}_1\) 的单位范数向量。'
- en: '*Proof:* We have'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明：* 我们有'
- en: \[ B^{k}\mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T
    \mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x}) \mathbf{v}_j.
    \]
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: \[ B^{k}\mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} \mathbf{v}_j \mathbf{v}_j^T
    \mathbf{x} = \sum_{j=1}^r \sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x}) \mathbf{v}_j.
    \]
- en: So
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 所以
- en: \[\begin{align*} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} &= \sum_{j=1}^r
    \mathbf{v}_j \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k} \mathbf{x}\|}\\
    &= \mathbf{v}_1 \left\{\frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|}\right\} + \sum_{j=2}^r \mathbf{v}_j \left\{\frac{\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})} {\|B^{k} \mathbf{x}\|}\right\}. \end{align*}\]
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} &= \sum_{j=1}^r
    \mathbf{v}_j \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k} \mathbf{x}\|}\\
    &= \mathbf{v}_1 \left\{\frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|}\right\} + \sum_{j=2}^r \mathbf{v}_j \left\{\frac{\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})} {\|B^{k} \mathbf{x}\|}\right\}. \end{align*}\]
- en: This goes to \(\mathbf{v}_1\) as \(k\to +\infty\) if the expression in the first
    curly brackets goes to \(1\) and the one in the second curly brackets goes to
    \(0\). We prove this in the next claim.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当第一个括号中的表达式趋近于1，第二个括号中的表达式趋近于0时，随着 \(k\to +\infty\)，这趋向于 \(\mathbf{v}_1\)。我们将在下一个命题中证明这一点。
- en: '**LEMMA** As \(k\to +\infty\),'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** 当 \(k\to +\infty\) 时，'
- en: \[ \frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k} \mathbf{x}\|} \to
    1 \qquad \text{and} \qquad \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|} \to 0, \ j = 2,\ldots,r. \]
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{\sigma_1^{2k} (\mathbf{v}_1^T \mathbf{x})} {\|B^{k} \mathbf{x}\|} \to
    1 \qquad \text{和} \qquad \frac{\sigma_j^{2k} (\mathbf{v}_j^T \mathbf{x})} {\|B^{k}
    \mathbf{x}\|} \to 0, \ j = 2,\ldots,r. \]
- en: \(\flat\)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* Because the \(\mathbf{v}_j\)s are an orthonormal basis,'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**证明：** 因为 \(\mathbf{v}_j\) 是一个正交基，'
- en: \[ \|B^{k}\mathbf{x}\|^2 = \sum_{j=1}^r \left[\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})\right]^2 = \sum_{j=1}^r \sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2.
    \]
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \|B^{k}\mathbf{x}\|^2 = \sum_{j=1}^r \left[\sigma_j^{2k} (\mathbf{v}_j^T
    \mathbf{x})\right]^2 = \sum_{j=1}^r \sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2.
    \]
- en: So, as \(k\to +\infty\), using the fact that \(\mathbf{v}_1^T \mathbf{x} = \langle
    \mathbf{v}_1, \mathbf{x} \rangle \neq 0\) by assumption
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 \(k\to +\infty\) 时，利用假设 \(\mathbf{v}_1^T \mathbf{x} = \langle \mathbf{v}_1,
    \mathbf{x} \rangle \neq 0\)
- en: \[\begin{align*} \frac{\|B^{k}\mathbf{x}\|^2}{\sigma_1^{4k} (\mathbf{v}_1^T
    \mathbf{x})^2} &= 1 + \sum_{j=2}^r \frac{\sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2}{\sigma_1^{4k}
    (\mathbf{v}_1^T \mathbf{x})^2}\\ &= 1 + \sum_{j=2}^r \left(\frac{\sigma_j}{\sigma_1}\right)^{4k}
    \frac{(\mathbf{v}_j^T \mathbf{x})^2}{(\mathbf{v}_1^T \mathbf{x})^2}\\ &\to 1,
    \end{align*}\]
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\|B^{k}\mathbf{x}\|^2}{\sigma_1^{4k} (\mathbf{v}_1^T
    \mathbf{x})^2} &= 1 + \sum_{j=2}^r \frac{\sigma_j^{4k} (\mathbf{v}_j^T \mathbf{x})^2}{\sigma_1^{4k}
    (\mathbf{v}_1^T \mathbf{x})^2}\\ &= 1 + \sum_{j=2}^r \left(\frac{\sigma_j}{\sigma_1}\right)^{4k}
    \frac{(\mathbf{v}_j^T \mathbf{x})^2}{(\mathbf{v}_1^T \mathbf{x})^2}\\ &\to 1,
    \end{align*}\]
- en: since \(\sigma_j < \sigma_1\) for all \(j =2,\ldots,r\). That implies the first
    part of the claim by taking a square root and using \(\langle \mathbf{v}_1, \mathbf{x}
    \rangle > 0\). The second part of the claim follows essentially from the same
    argument. \(\square\) \(\square\)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因为对于所有 \(j =2,\ldots,r\)，\(\sigma_j < \sigma_1\)。这表明通过取平方根并使用 \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle > 0\)，可以得出断言的第一部分。断言的第二部分基本上是从相同的论证中得出的。\(\square\) \(\square\)
- en: '**EXAMPLE:** We revisit the example'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**示例：** 我们重新审视这个例子'
- en: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} A = \begin{pmatrix} 1 & 0\\ -1 & 0 \end{pmatrix}. \end{split}\]
- en: We previously compute its SVD and found that
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前计算了它的奇异值分解（SVD）并发现
- en: \[\begin{split} \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}. \end{split}\]
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathbf{v}_1 = \begin{pmatrix} 1\\ 0 \end{pmatrix}. \end{split}\]
- en: This time we use the *Power Iteration Lemma*. Here
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们使用**幂迭代引理**。在这里
- en: \[\begin{split} B = A^T A = \begin{pmatrix} 2 & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} B = A^T A = \begin{pmatrix} 2 & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
- en: Taking powers of this matrix is easy
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这个矩阵的幂是容易的
- en: \[\begin{split} B^k = \begin{pmatrix} 2^k & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} B^k = \begin{pmatrix} 2^k & 0\\ 0 & 0 \end{pmatrix}. \end{split}\]
- en: Let’s choose an arbitrary initial vector \(\mathbf{x}\), say \((-1, 2)\). Then
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择一个任意的初始向量 \(\mathbf{x}\)，比如说 \((-1, 2)\)。然后
- en: \[\begin{split} B^k \mathbf{x} = \begin{pmatrix} -2^k\\ 0 \end{pmatrix} \quad
    \text{and} \quad \|B^k \mathbf{x}\| = 2^k. \end{split}\]
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} B^k \mathbf{x} = \begin{pmatrix} -2^k\\ 0 \end{pmatrix} \quad
    \text{and} \quad \|B^k \mathbf{x}\| = 2^k. \end{split}\]
- en: So
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: \[\begin{split} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \begin{pmatrix}
    -1\\ 0 \end{pmatrix} = - \mathbf{v}_1, \end{split}\]
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \frac{B^{k} \mathbf{x}}{\|B^{k} \mathbf{x}\|} \to \begin{pmatrix}
    -1\\ 0 \end{pmatrix} = - \mathbf{v}_1, \end{split}\]
- en: as \(k \to +\infty\). In fact, in this case, convergence occurs after one step.
    \(\lhd\)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 \(k \to +\infty\)。事实上，在这种情况下，收敛发生在第一步之后。\(\lhd\)
- en: The argument leading to the *Power Iteration Lemma* also holds more generally
    for the eigenvectors of positive semidefinite matrices. Let \(A\) be a symmetric,
    positive semidefinite matrix in \(\mathbb{R}^{d \times d}\). By the *Spectral
    Theorem*, it has an eigenvector decomposition
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 导致**幂迭代引理**的论据也普遍适用于正半定矩阵的特征向量。设 \(A\) 是 \(\mathbb{R}^{d \times d}\) 中的一个对称、正半定矩阵。根据**谱定理**，它有一个特征向量分解
- en: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A = Q \Lambda Q^T = \sum_{i=1}^d \lambda_i \mathbf{q}_i \mathbf{q}_i^T \]
- en: where further \(0 \leq \lambda_d \leq \cdots \leq \lambda_1\) by the *Characterization
    of Positive Semidefiniteness*. Because of the orthogonality of \(Q\), the powers
    of \(A\) have a simple representation. The square gives
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，进一步地，由于**正半定性的特征**，有 \(0 \leq \lambda_d \leq \cdots \leq \lambda_1\)。由于 \(Q\)
    的正交性，\(A\) 的幂有一个简单的表示。平方给出
- en: \[ A^2 = (Q \Lambda Q^T) (Q \Lambda Q^T) = Q \Lambda^2 Q^T. \]
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^2 = (Q \Lambda Q^T) (Q \Lambda Q^T) = Q \Lambda^2 Q^T. \]
- en: Repeating, we obtain
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 重复进行，我们得到
- en: \[ A^{k} = Q \Lambda^{k} Q^T. \]
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: \[ A^{k} = Q \Lambda^{k} Q^T. \]
- en: 'This leads to the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下结果：
- en: '**LEMMA** **(Power Iteration)** \(\idx{power iteration lemma}\xdi\) Let \(A\)
    be a symmetric, positive semindefinite matrix in \(\mathbb{R}^{d \times d}\) with
    eigenvector decomposition \(A= Q \Lambda Q^T\) where the eigenvalues satisfy \(0
    \leq \lambda_d \leq \cdots \leq \lambda_2 < \lambda_1\). Assume that \(\mathbf{x}
    \in \mathbb{R}^d\) is a vector such that \(\langle \mathbf{q}_1, \mathbf{x} \rangle
    > 0\). Then'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(幂迭代)** \(\idx{power iteration lemma}\xdi\) 设 \(A\) 是一个在 \(\mathbb{R}^{d
    \times d}\) 中的对称、正半定矩阵，其特征向量分解为 \(A= Q \Lambda Q^T\)，其中特征值满足 \(0 \leq \lambda_d
    \leq \cdots \leq \lambda_2 < \lambda_1\)。假设 \(\mathbf{x} \in \mathbb{R}^d\) 是一个向量，使得
    \(\langle \mathbf{q}_1, \mathbf{x} \rangle > 0\)。那么'
- en: \[ \frac{A^{k} \mathbf{x}}{\|A^{k} \mathbf{x}\|} \to \mathbf{q}_1 \]
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \frac{A^{k} \mathbf{x}}{\|A^{k} \mathbf{x}\|} \to \mathbf{q}_1 \]
- en: as \(k \to +\infty\). If instead \(\langle \mathbf{q}_1, \mathbf{x} \rangle
    < 0\), then the limit is \(- \mathbf{q}_1\). \(\flat\)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 \(k \to +\infty\)。如果 \(\langle \mathbf{q}_1, \mathbf{x} \rangle < 0\)，则极限是
    \(- \mathbf{q}_1\)。 \(\flat\)
- en: The proof is similar to the case of singular vectors.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 证明与奇异向量的情况类似。
- en: 4.4.2\. Computing the top singular vector[#](#computing-the-top-singular-vector
    "Link to this heading")
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4.2. 计算最大的奇异向量[#](#computing-the-top-singular-vector "链接到这个标题")
- en: Power iteration gives us a way to compute \(\mathbf{v}_1\) – at least approximately
    if we use a large enough \(k\). But how do we find an appropriate vector \(\mathbf{x}\),
    as required by the *Power Iteration Lemma*? It turns out that a random vector
    will do. For instance, let \(\mathbf{X}\) be an \(m\)-dimensional spherical Gaussian
    with mean \(0\) and variance \(1\). Then, \(\mathbb{P}[\langle \mathbf{v}_1, \mathbf{X}
    \rangle = 0] = 0\).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 幂迭代为我们提供了一种计算 \(\mathbf{v}_1\) 的方法——至少如果我们使用足够大的 \(k\)，可以得到一个近似值。但是，我们如何找到符合
    *幂迭代引理* 要求的适当向量 \(\mathbf{x}\) 呢？实际上，一个随机向量就可以。例如，设 \(\mathbf{X}\) 是一个 \(m\)-维球面高斯分布，均值为
    \(0\)，方差为 \(1\)。那么，\(\mathbb{P}[\langle \mathbf{v}_1, \mathbf{X} \rangle = 0]
    = 0\)。
- en: We implement the algorithm suggested by the *Power Iteration Lemma*. That is,
    we compute \(B^{k} \mathbf{x}\), then normalize it. To obtain the corresponding
    singular value and left singular vector, we use that \(\sigma_1 = \|A \mathbf{v}_1\|\)
    and \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1\).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了由 *幂迭代引理* 提出的算法。也就是说，我们计算 \(B^{k} \mathbf{x}\)，然后对其进行归一化。为了获得相应的奇异值和左奇异向量，我们使用
    \(\sigma_1 = \|A \mathbf{v}_1\|\) 和 \(\mathbf{u}_1 = A \mathbf{v}_1/\sigma_1\)。
- en: '[PRE15]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**NUMERICAL CORNER:** We will apply it to our previous two-cluster example.
    The necessary functions are in [mmids.py](https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py),
    which is available on the [GitHub of the book](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落:** 我们将把它应用到我们之前的两个簇的例子中。必要的函数在 [mmids.py](https://raw.githubusercontent.com/MMiDS-textbook/MMiDS-textbook.github.io/main/utils/mmids.py)
    中，该文件可在书的 [GitHub](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/tree/main)
    上找到。'
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![../../_images/99720e27a1ef3efd196db79640dfdfe1c3d2d70b319ef77d803ab4437bbb954a.png](../Images/e66fa4144461a2ee2155a730c0c7df77.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片链接](../Images/e66fa4144461a2ee2155a730c0c7df77.png)'
- en: Let’s compute the top singular vector.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算最大的奇异向量。
- en: '[PRE17]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is approximately \(-\mathbf{e}_1\). We get roughly the same answer (possibly
    up to sign) from Python’s [`numpy.linalg.svd`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)
    function.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这大约是 \(-\mathbf{e}_1\)。我们从 Python 的 `numpy.linalg.svd` 函数（[链接](https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html)）中得到的答案（可能包括符号）大致相同。
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Recall that, when we applied \(k\)-means clustering to this example with \(d=1000\)
    dimension, we obtained a very poor clustering.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，当我们用 \(d=1000\) 维度的 \(k\)-means 聚类算法处理这个例子时，我们得到了一个非常差的聚类结果。
- en: '[PRE21]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![../../_images/7a49cf383a34072dd30fa0e2fed2db76615fa2692ba83fbdee9d0a6a770b3940.png](../Images/e2ab190c5a5c17937d5e2285a9c20f4c.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片链接](../Images/e2ab190c5a5c17937d5e2285a9c20f4c.png)'
- en: 'Let’s try again, but after projecting on the top singular vector. Recall that
    this corresponds to finding the best one-dimensional approximating subspace. The
    projection can be computed using the truncated SVD \(Z= U_{(1)} \Sigma_{(1)} V_{(1)}^T\).
    We can interpret the rows of \(U_{(1)} \Sigma_{(1)}\) as the coefficients of each
    data point in the basis \(\mathbf{v}_1\). We will work in that basis. We need
    one small hack: because our implementation of \(k\)-means clustering expects data
    points in at least \(2\) dimension, we add a column of \(0\)’s.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次尝试，但这次在投影到最高奇异向量之后。回想一下，这对应于找到最佳一维逼近子空间。投影可以使用截断奇异值分解 \(Z= U_{(1)} \Sigma_{(1)}
    V_{(1)}^T\) 来计算。我们可以将 \(U_{(1)} \Sigma_{(1)}\) 的行解释为每个数据点在基 \(\mathbf{v}_1\) 中的系数。我们将在这个基上工作。我们需要一个小技巧：因为我们的
    \(k\)-means 聚类实现期望数据点至少在 \(2\) 维，所以我们添加了一列0。
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![../../_images/f2b508b6781d21eee9a679ee87894a523e040face9ef4d99ccd09cd38dab5959.png](../Images/d0d37edca421b5badf55590966b6d81a.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/f2b508b6781d21eee9a679ee87894a523e040face9ef4d99ccd09cd38dab5959.png](../Images/d0d37edca421b5badf55590966b6d81a.png)'
- en: There is a small – yet noticeable – gap around 0\. We run \(k\)-means clustering
    on the projected data.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在0附近有一个小——但明显——的间隙。我们在投影数据上运行 \(k\)-means 聚类。
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: <details class="hide above-input"><summary aria-label="Toggle hidden content">显示代码单元格源
    隐藏代码单元格源</summary>
- en: '[PRE27]</details> ![../../_images/b1c80c35b20bcf5c314f991485089bce5ec85beb6ae17ada744dd8f3b8a04ddc.png](../Images/5ffbd56fc722edd92c57dc038954354a.png)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE27]</details> ![../../_images/b1c80c35b20bcf5c314f991485089bce5ec85beb6ae17ada744dd8f3b8a04ddc.png](../Images/5ffbd56fc722edd92c57dc038954354a.png)'
- en: 'Much better. We give a more formal explanation of this outcome in a subsequent
    section. In essence, quoting [BHK, Section 7.5.1]:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 更好了。我们将在后续章节中给出这个结果的更正式解释。本质上，引用[BHK，第7.5.1节]：
- en: '[…] let’s understand the central advantage of doing the projection to [the
    top \(k\) right singular vectors]. It is simply that for any reasonable (unknown)
    clustering of data points, the projection brings data points closer to their cluster
    centers.'
  id: totrans-207
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[…] 让我们理解将投影做到[前 \(k\) 个右奇异向量]的中心优势。简单地说，对于任何合理的（未知）数据点聚类，投影会使数据点更接近它们的簇中心。'
- en: Finally, looking at the top right singular vector (or its first ten entries
    for lack of space), we see that it does align quite well (but not perfectly) with
    the first dimension.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，查看右上奇异向量（或由于空间不足而只查看其前十个条目），我们看到它与第一维相当一致（但不是完美一致）。
- en: '[PRE28]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: \(\unlhd\)
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** There are other methods to compute the SVD. Ask your favorite
    AI chatbot about randomized algorithms for the SVD. What are their advantages
    in terms of computational efficiency for large matrices? \(\ddagger\)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 计算奇异值分解（SVD）还有其他方法。向你的心仪AI聊天机器人询问SVD的随机算法。它们在处理大型矩阵时在计算效率方面的优势是什么？
    \(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude、Gemini和ChatGPT协助)*'
- en: '**1** In the power iteration lemma for the positive semidefinite case, what
    happens when the initial vector \(\mathbf{x}\) satisfies \(\langle \mathbf{q}_1,
    \mathbf{x} \rangle < 0\)?'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在正半定情况的幂迭代引理中，当初始向量 \(\mathbf{x}\) 满足 \(\langle \mathbf{q}_1, \mathbf{x}
    \rangle < 0\) 时会发生什么？'
- en: a) The iteration converges to \(\mathbf{q}_1\).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: a) 迭代收敛到 \(\mathbf{q}_1\).
- en: b) The iteration converges to \(-\mathbf{q}_1\).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: b) 迭代收敛到 \(-\mathbf{q}_1\).
- en: c) The iteration does not converge.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: c) 迭代没有收敛。
- en: d) The iteration converges to a random eigenvector.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: d) 迭代收敛到一个随机特征向量。
- en: '**2** In the power iteration lemma for the SVD case, what is the convergence
    result for a random vector \(\mathbf{x}\)?'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 在SVD情况的幂迭代引理中，对于随机向量 \(\mathbf{x}\) 的收敛结果是什么？'
- en: a) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\mathbf{u}_1\).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\frac{B^k \mathbf{x}}{\|B^k \mathbf{x}\|}\) 收敛到 \(\mathbf{u}_1\).
- en: b) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\mathbf{v}_1\) or
    \(-\mathbf{v}_1\).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\frac{B^k \mathbf{x}}{\|B^k \mathbf{x}\|}\) 收敛到 \(\mathbf{v}_1\) 或 \(-\mathbf{v}_1\).
- en: c) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges to \(\sigma_1\).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\frac{B^k \mathbf{x}}{\|B^k \mathbf{x}\|}\) 收敛到 \(\sigma_1\).
- en: d) \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) does not converge.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\frac{B^k \mathbf{x}}{\|B^k \mathbf{x}\|}\) 没有收敛。
- en: '**3** Suppose you apply the power iteration method to a matrix \(A\) and obtain
    a vector \(\mathbf{v}\). How can you compute the corresponding singular value
    \(\sigma\) and left singular vector \(\mathbf{u}\)?'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 假设你将幂迭代方法应用于矩阵 \(A\) 并得到向量 \(\mathbf{v}\)。你如何计算相应的奇异值 \(\sigma\) 和左奇异向量
    \(\mathbf{u}\)？'
- en: a) \(\sigma = \|A\mathbf{v}\|\) and \(\mathbf{u} = A\mathbf{v}/\sigma\)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\sigma = \|A\mathbf{v}\|\) 和 \(\mathbf{u} = A\mathbf{v}/\sigma\)
- en: b) \(\sigma = \|A^T\mathbf{v}\|\) and \(\mathbf{u} = A^T\mathbf{v}/\sigma\)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\sigma = \|A^T\mathbf{v}\|\) 和 \(\mathbf{u} = A^T\mathbf{v}/\sigma\)
- en: c) \(\sigma = \|\mathbf{v}\|\) and \(\mathbf{u} = \mathbf{v}/\sigma\)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\sigma = \|\mathbf{v}\|\) 和 \(\mathbf{u} = \mathbf{v}/\sigma\)
- en: d) \(\sigma = 1\) and \(\mathbf{u} = A\mathbf{v}\)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\sigma = 1\) 和 \(\mathbf{u} = A\mathbf{v}\)
- en: '**4** What is required for the initial vector \(\mathbf{x}\) in the power iteration
    method to ensure convergence to the top eigenvector?'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在幂迭代法中，为了确保初始向量 \(\mathbf{x}\) 收敛到最大的特征向量，需要满足什么条件？'
- en: a) \(\mathbf{x}\) must be a zero vector.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathbf{x}\) 必须是零向量。
- en: b) \(\mathbf{x}\) must be orthogonal to the top eigenvector.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathbf{x}\) 必须与最大的特征向量正交。
- en: c) \(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq 0\).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq 0\)
- en: d) \(\mathbf{x}\) must be the top eigenvector itself.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathbf{x}\) 必须是最大的特征向量本身。
- en: '**5** What does the truncated SVD \(Z = U_{(2)} \Sigma_{(2)} V_{(2)}^T\) correspond
    to? [Uses Section 4.8.2.1.]'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 截断奇异值分解 \(Z = U_{(2)} \Sigma_{(2)} V_{(2)}^T\) 对应的是什么？[使用第 4.8.2.1 节。]'
- en: a) The best one-dimensional approximating subspace
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: a) 最佳一维逼近子空间
- en: b) The best two-dimensional approximating subspace
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: b) 最佳二维逼近子空间
- en: c) The projection of the data onto the top singular vector
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: c) 将数据投影到最大的奇异向量上
- en: d) The projection of the data onto the top two singular vectors
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: d) 将数据投影到前两个奇异向量上
- en: 'Answer for 1: b. Justification: The lemma states that if \(\langle \mathbf{q}_1,
    \mathbf{x} \rangle < 0\), then the limit of \(A^k \mathbf{x} / \|A^k \mathbf{x}\|\)
    is \(-\mathbf{q}_1\).'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 1 的答案：b. 理由：引理表明，如果 \(\langle \mathbf{q}_1, \mathbf{x} \rangle < 0\)，那么 \(A^k
    \mathbf{x} / \|A^k \mathbf{x}\|\) 的极限是 \(-\mathbf{q}_1\)。
- en: 'Answer for 2: b. Justification: The lemma states that if \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle > 0\), then \(B^k \mathbf{x} / \|B^k \mathbf{x}\|\) converges
    to \(\mathbf{v}_1\), and if \(\langle \mathbf{v}_1, \mathbf{x} \rangle < 0\),
    then the limit is \(-\mathbf{v}_1\).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 2 的答案：b. 理由：引理表明，如果 \(\langle \mathbf{v}_1, \mathbf{x} \rangle > 0\)，那么 \(B^k
    \mathbf{x} / \|B^k \mathbf{x}\|\) 收敛到 \(\mathbf{v}_1\)，如果 \(\langle \mathbf{v}_1,
    \mathbf{x} \rangle < 0\)，则极限是 \(-\mathbf{v}_1\)。
- en: 'Answer for 3: a. Justification: The text provides these formulas in the “Numerical
    Corner” section.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 3 的答案：a. 理由：文本在“数值角落”部分提供了这些公式。
- en: 'Answer for 4: c. Justification: The key lemma states that convergence is ensured
    if \(\mathbf{x}\) is such that \(\langle \mathbf{q}_1, \mathbf{x} \rangle \neq
    0\).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 4 的答案：c. 理由：关键引理表明，如果 \(\mathbf{x}\) 满足 \(\langle \mathbf{q}_1, \mathbf{x} \rangle
    \neq 0\)，则可以保证收敛。
- en: 'Answer for 5: d. Justification: The text states that “projecting on the top
    two singular vectors… corresponds to finding the best two-dimensional approximating
    subspace. The projection can be computed using the truncated SVD \(Z = U_{(2)}
    \Sigma_{(2)} V_{(2)}^T\).”'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 5 的答案：d. 理由：文本指出，“投影到前两个奇异向量……对应于找到最佳二维逼近子空间。投影可以使用截断奇异值分解 \(Z = U_{(2)} \Sigma_{(2)}
    V_{(2)}^T\) 来计算。”
