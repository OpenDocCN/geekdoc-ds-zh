["```cpp\nextern int a,c,d;\nextern const int b;\nextern const int e;\n```", "```cpp\nvoid some_func_with_dependencies(void)\n{\n a = b ∗ 100;\n c = b ∗ 1000;\n d = (a + c) ∗ e;\n```", "```cpp\nextern int a,c,d,f,g,h,i,j;\nextern const int b;\nextern const int e;\n```", "```cpp\nvoid some_func_with_dependencies(void)\n{\n a = b ∗ 100;\n c = b ∗ 1000;\n```", "```cpp\n f = b ∗ 101;\n g = b ∗ 1001;\n```", "```cpp\n d = (a + c) ∗ e;\n h = (f + g) ∗ e;\n```", "```cpp\n i = d ∗ 10;\n j = h ∗ 10;\n}\n```", "```cpp\nvoid loop_fusion_example_unfused(void)\n{\n unsigned int i,j;\n```", "```cpp\n a = 0;\n for (i=0; i<100; i++)  /∗ 100 iterations ∗/\n {\n  a + = b ∗ c ∗ i;\n }\n```", "```cpp\n d = 0;\n for (j=0; j<200; j++)  /∗ 200 iterations ∗/\n {\n  d += e ∗ f ∗ j;\n }\n}\n```", "```cpp\nvoid loop_fusion_example_fused_01(void)\n{\n unsigned int i;   /∗ Notice j is eliminated ∗/\n```", "```cpp\n a = 0;\n d = 0;\n for (i=0; i<100; i++)  /∗ 100 iterations ∗/\n {\n  a += b ∗ c ∗ i;\n  d += e ∗ f ∗ i;\n }\n```", "```cpp\n for (i=100; i<200; i++)  /∗ 100 iterations ∗/\n {\n  d += e ∗ f ∗ i;\n }\n}\n```", "```cpp\nvoid loop_fusion_example_fused_02(void)\n{\n unsigned int i;   /∗ Notice j is eliminated ∗/\n```", "```cpp\n a = 0;\n d = 0;\n for (i=0; i<100; i++)  /∗ 100 iterations ∗/\n {\n  a += b ∗ c ∗ i;\n  d += e ∗ f ∗ i;\n  d += e ∗ f ∗ (i∗2);\n }\n```", "```cpp\n#define MSG_SIZE 4096\ntypedef struct\n{\n u16 header;\n u32 msg_data[MSG_SIZE];\n} MY_STRUCT_T;\n```", "```cpp\nint tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\ndata[tid] = a[tid] ∗ b[tid];\n```", "```cpp\nint tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\ndata[tid] = a[tid] ∗ b[tid];\ndata[tid+1] = a[tid+1] ∗ b[tid+1];\n```", "```cpp\nint tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\nint a_0 = a[tid];\nint b_0 = b[tid];\nint a_1 = a[tid+1];\nint b_1 = b[tid+1];\ndata[tid] = a_0 ∗ b_0;\ndata[tid+1] = a_1 ∗ b_1;\n```", "```cpp\nint tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\nint2 a_vect = a[tid];\nint2 b_vect = b[tid];\n```", "```cpp\ncudaFuncSetCacheConfig(cache_prefer, kernel_name);\n```", "```cpp\nkernel<<<num_blocks, num_threads, smem_size>>>(a,b,c);\n```", "```cpp\nextern volatile __shared__ int s_data[];\n```", "```cpp\n__global__ my_kernel(const int ∗ a,\n       const int ∗ b,\n       const int num_elem_a,\n       const int num_elem_b)\n{\n const int tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n // Copy arrays ‘a’ and ‘b’ to shared memory\n s_data[tid] = a[tid];\n s_data[num_elem_a + tid] = b[tid];\n```", "```cpp\n// Wait for all threads\n__syncthreads();\n```", "```cpp\n // Process s_data[0] to s_data[(num_elem_a-1)] – a\n// Process s_data[num_elem_a] to s_data[num_elem_a + (num_elem_b-1)] – array ‘b’\n}\n```", "```cpp\ncudaError_t cudaHostAlloc (void ∗∗ host_pointer, size_t size, unsigned int flags)\n```", "```cpp\n__global__ void kernel_copy(u32 ∗ const gpu_data,\n          const u32 ∗ const host_data,\n          const u32 num_elements)\n{\n  const u32 idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n  const u32 idy = (blockIdx.y ∗ blockDim.y) + threadIdx.y;\n  const u32 tid = ((gridDim.x ∗ blockDim.x) ∗ idy) + idx;\n```", "```cpp\n  if (tid < num_elements)\n   gpu_data[tid] = host_data[tid];\n}\n```", "```cpp\n// Enable host mapping to device memory\nCUDA_CALL(cudaSetDeviceFlags(cudaDeviceMapHost));\n```", "```cpp\nstruct cudaDeviceProp device_prop;\nCUDA_CALL(cudaGetDeviceProperties(&device_prop, device_num));\nzero_copy_supported = device_prop.canMapHostMemory;\n```", "```cpp\n// Allocate zero copy pinned memory\nCUDA_CALL(cudaHostAlloc((void ∗∗) &host_data_to_device, size_in_bytes, cudaHostAllocWriteCombined | cudaHostAllocMapped));\n```", "```cpp\n// Convert to a GPU host pointer\nCUDA_CALL(cudaHostGetDevicePointer( &dev_host_data_to_device, host_data_to_device, 0));\n```", "```cpp\n// Free pinned memory\nCUDA_CALL(cudaFreeHost(host_data_to_device));\n```", "```cpp\nconst int num_elements = (size_in_bytes / sizeof(u32));\nconst int num_threads = 256;\nconst int num_grid = 64;\nconst int num_blocks = (num_elements + (num_threads-1)) / num_threads;\nint num_blocks_per_grid;\n```", "```cpp\n// Split blocks into grid\nif (num_blocks > num_grid)\n  num_blocks_per_grid = num_blocks / num_grid;\nelse\n  num_blocks_per_grid = 1;\n```", "```cpp\n// Run the kernel\nkernel_copy<<<blocks, num_threads>>>(gpu_data, dev_host_data_to_device, num_elements);\n```", "```cpp\nvoid memcpy_test_zero_to_from(const int device_num,\n          const size_t size_in_bytes,\n          TIMER_T ∗ const kernel_time,\n          const u32 num_runs,\n          const bool pinned)\n{\n  char device_prefix[256];\n  int major, minor;\n  int zero_copy_supported;\n```", "```cpp\n  // Init\n```", "```cpp\n  // Enable host mapping to device memory\n  CUDA_CALL(cudaSetDeviceFlags(cudaDeviceMapHost));\n```", "```cpp\n  // Get the device properties\n  get_device_props(device_prefix, device_num, &major,\n       &minor, &zero_copy_supported);\n```", "```cpp\n  // Exit if zero copy not supported and is requested\n  if (zero_copy_supported == 0)\n  {\n   printf(\"%s Error Zero Copy not supported\", device_prefix);\n   wait_exit(1);\n  }\n```", "```cpp\n  // Select the specified device\n  CUDA_CALL(cudaSetDevice(device_num));\n```", "```cpp\n  printf(\"%s Running Memcpy Test to device using\",\n        device_prefix);\n```", "```cpp\n  if (pinned)\n     printf(\" locked memory\");\n  else\n   printf(\" unlocked memory\");\n```", "```cpp\n  printf(\" %lu K\", size_in_bytes / 1024);\n```", "```cpp\n  (∗kernel_time) = 0;\n```", "```cpp\n  init_device_timer();\n```", "```cpp\n  // Allocate data space on GPU\n  u32 ∗ gpu_data;\n  CUDA_CALL(cudaMalloc((void∗∗)&gpu_data,\n```", "```cpp\nu32 ∗ dev_host_data_to_device;\n  u32 ∗ dev_host_data_from_device;\n```", "```cpp\n  // Allocate data space on host\n  u32 ∗ host_data_to_device;\n  u32 ∗ host_data_from_device;\n```", "```cpp\n  if (pinned)\n  {\n   // Allocate zero copy pinned memory\n   CUDA_CALL(cudaHostAlloc((void ∗∗) &host_data_to_device, size_in_bytes, cudaHostAllocWriteCombined | cudaHostAllocMapped));\n```", "```cpp\n   CUDA_CALL(cudaHostAlloc((void ∗∗) &host_data_from_device, size_in_bytes, cudaHostAllocDefault | cudaHostAllocMapped));\n  }\n  else\n  {\n   host_data_to_device = (u32 ∗) malloc(size_in_bytes);\n   host_data_from_device = (u32 ∗) malloc(size_in_bytes);\n  }\n```", "```cpp\n  // Convert to a GPU host pointer\n  CUDA_CALL(cudaHostGetDevicePointer(&dev_host_data_to_device, host_data_to_device, 0));\n```", "```cpp\n  CUDA_CALL(cudaHostGetDevicePointer(&dev_host_data_from_device, host_data_from_device, 0));\n```", "```cpp\n  // If the host allocation did not result in\n  // an out of memory error\n  if ( (host_data_to_device != NULL) &&\n   (host_data_from_device != NULL) )\n  {\n   const int num_elements = (size_in_bytes / sizeof(u32));\n   const int num_threads = 256;\n   const int num_grid = 64;\n   const int num_blocks = (num_elements + (num_threads-1)) / num_threads;\n   int num_blocks_per_grid;\n```", "```cpp\n   // Split blocks into grid\n   if (num_blocks > num_grid)\n    num_blocks_per_grid = num_blocks / num_grid;\n   else\n    num_blocks_per_grid = 1;\n```", "```cpp\n   dim3 blocks(num_grid, num_blocks_per_grid);\n```", "```cpp\n   for (u32 test=0; test < num_runs+1; test++)\n```", "```cpp\n    // Add in all but first test run\n    if (test != 0)\n      start_device_timer();\n```", "```cpp\n    // Run the kernel\n    kernel_copy<<<blocks, num_threads>>>(dev_host_data_to_device, dev_host_data_to_device, num_elements);\n```", "```cpp\n    // Wait for device to complete all work\n    CUDA_CALL(cudaDeviceSynchronize());\n```", "```cpp\n    // Check for kernel errors\n    cuda_error_check(device_prefix, \" calling kernel kernel_copy\");\n```", "```cpp\n    // Add in all but first test run\n    if (test != 0)\n      (∗kernel_time) += stop_device_timer();\n   }\n```", "```cpp\n   // Average over number of test runs\n   (∗kernel_time) /= num_runs;\n```", "```cpp\n   if (pinned)\n   {\n    // Free pinned memory\n    CUDA_CALL(cudaFreeHost(host_data_to_device));\n    CUDA_CALL(cudaFreeHost(host_data_from_device));\n   }\n   else\n   {\n    // Free regular paged memory\n    free(host_data_to_device);\n    free(host_data_from_device);\n   }\n  }\n```", "```cpp\n  CUDA_CALL(cudaFree(gpu_data));\n  destroy_device_timer();\n```", "```cpp\n  // Free up the device\n  CUDA_CALL(cudaDeviceReset());\n```", "```cpp\n  printf(\" KERNEL:%.2f ms\", (∗kernel_time));\n```", "```cpp\n  const float one_mb = (1024 ∗ 1024);\n  const float kernel_time_for_one_mb = (∗kernel_time) ∗ (one_mb / size_in_bytes);\n```", "```cpp\n  const float MB_per_sec = ((1000.0F / kernel_time_for_one_mb) ∗ 2.0F);\n```", "```cpp\n  printf(\" KERNEL:%.0f MB/s\", MB_per_sec );\n}\n```", "```cpp\nGTX 470: 8 bytes x 1 K (1x4x32)  0.060 ms, 489 MB/s\nGTX 470: 8 bytes x 2 K (1x8x32)  0.059 ms, 988 MB/s\nGTX 470: 8 bytes x 4 K (1x16x32)  0.060 ms, 1969 MB/s\nGTX 470: 8 bytes x 8 K (1x32x32)  0.059 ms, 3948 MB/s\nGTX 470: 8 bytes x 16 K (1x32x64)  0.059 ms, 7927 MB/s\nGTX 470: 8 bytes x 32 K (1x64x64)  0.061 ms, 15444 MB/s\nGTX 470: 8 bytes x 64 K (1x64x128)  0.065 ms, 28779 MB/s\nGTX 470: 8 bytes x 128 K (1x64x256)  0.074 ms, 50468 MB/s\nGTX 470: 8 bytes x 256 K (1x128x256) 0.090 ms, 83053 MB/s\nGTX 470: 8 bytes x 512 K (1x256x256) 0.153 ms, 98147 MB/s\nGTX 470: 8 bytes x 1 M (1x512x256) 0.30 ms, 98508 MB/s\nGTX 470: 8 bytes x 2 M (1x1024x256) 0.56 ms, 105950 MB/s\nGTX 470: 8 bytes x 4 M (1x2048x256) 1.10 ms, 108888 MB/s\nGTX 470: 8 bytes x 8 M (1x4096x256) 2.19 ms, 112215 MB/s\nGTX 470: 8 bytes x 16 M (1x8192x256) 4.26 ms, 112655 MB/s\nGTX 470: 8 bytes x 32 M (1x16384x256) 8.48 ms, 113085 MB/s\nGTX 470: 8 bytes x 64 M (1x32768x256) 16.9 ms, 113001 MB/s\nGTX 470: 8 bytes x 128 M (2x32768x256) 33.9 ms, 112978 MB/s\nGTX 470: 8 bytes x 256 M (4x32768x256) 67.7 ms, 113279 MB/s\n```", "```cpp\n// Create a new stream on the device\ncudaStream_t stream;\nCUDA_CALL(cudaStreamCreate(&stream));\n```", "```cpp\n#define MAX_NUM_TESTS 16\ncudaEvent_t kernel_start[MAX_NUM_TESTS];\ncudaEvent_t kernel_stop[MAX_NUM_TESTS];\n```", "```cpp\nfor (u32 test=0; test < MAX_NUM_TESTS; test++)\n{\n  CUDA_CALL(cudaEventCreate(&kernel_start[test]));\n  CUDA_CALL(cudaEventCreate(&kernel_stop[test]));\n}\n```", "```cpp\n// Start event\nCUDA_CALL(cudaEventRecord(kernel_start[test],stream));\n```", "```cpp\nkernel_copy_single<data_T><<<num_blocks, num_threads, dynamic_shared_memory_usage, stream>>>(s_data_in, s_data_out, num_elements);\n```", "```cpp\n// Stop event\nCUDA_CALL(cudaEventRecord(kernel_stop[test],stream));\n```", "```cpp\n// Extract the total time\nfor (u32 test=0; test < MAX_NUM_TESTS; test++)\n{\n  float delta;\n```", "```cpp\n  // Wait for the event to complete\n  CUDA_CALL(cudaEventSynchronize(kernel_stop[test]));\n```", "```cpp\n  // Get the time difference\n  CUDA_CALL(cudaEventElapsedTime(&delta, kernel_start[test], kernel_stop[test]));\n```", "```cpp\n  kernel_time += delta;\n}\n```", "```cpp\n// Free up all events\nfor (u32 test=0; test < MAX_NUM_TESTS; test++)\n{\n  CUDA_CALL(cudaEventDestroy(kernel_start[test]));\n  CUDA_CALL(cudaEventDestroy(kernel_stop[test]));\n}\n```", "```cpp\nif ( cudaEventQuery( memcpy_to_stop[device_num][complete_count_in_stop[device_num]] ) == cudaSuccess)\n{\n  TIMER_T delta = 0.0F;\n```", "```cpp\n  CUDA_CALL( cudaEventElapsedTime( &delta, memcpy_to_start[device_num][0], memcpy_to_stop[device_num][complete_count_in_stop[device_num]] ));\n```", "```cpp\n  printf(\"%sMemcpy to device test %d completed %.2f ms\", device_prefix[device_num], complete_count_in_stop[device_num], delta);\n```", "```cpp\n  complete_count_in_stop[device_num]++;\n  event_completed = true;\n}\n```", "```cpp\n// Fetch the lower 32 bits of the clock (pre compute 2.x)\nunsigned int clock;\nasm(\"mov.u32 %0, %%clock ;\" : \"=r\"(clock));\n```", "```cpp\n// Fetch the clock (req. compute 2.x)\nunsigned long int clock64;\nasm(\"mov.u64 %0, %%clock64 ;\" : \"=r\"(clock64));\n```", "```cpp\nC[z] = A[y] ∗ B[x];\n```", "```cpp\nC[idx_C].x = A[idx_A].x ∗ B[idx_b].x;\nC[idx_C].y = A[idx_A].y ∗ B[idx_b].y;\nC[idx_C].z = A[idx_A].z ∗ B[idx_b].z;\nC[idx_C].w = A[idx_A].w ∗ B[idx_b].w;\n```", "```cpp\nC[idx_C] = A[idx_A] ∗ B[idx_b];\n```", "```cpp\narray_element_address = index ∗ element size\n```", "```cpp\n{\n  int i;\n  int a[4];\n```", "```cpp\n  for (i=0;i<4;i++)\n   a[i]=i;\n}\n```", "```cpp\nvs.\n```", "```cpp\n{\n  int i;\n```", "```cpp\n  int a[4];\n  int ∗_ptr = a;\n```", "```cpp\n  for (i=0; i<4; i++)\n   ∗_ptr++ = i;\n}\n```", "```cpp\nfor (int j=0;j<100;j++)\n{\n  for (int i=0; i<100; i++)\n  {\n   const int b = j ∗ 200;\n   q[i]= b;\n  }\n}\n```", "```cpp\nfor (int j=0;j<100;j++)\n{\n  const int b = j ∗ 200;\n```", "```cpp\n  for (int i=0; i<100; i++)\n  {\n   q[i]= b;\n  }\n}\n```", "```cpp\nint b;\n```", "```cpp\nvoid some_func(void)\n{\n  for (int j=0;j<100;j++)\n  {\n   for (int i=0; i<100; i++)\n   {\n    b = j ∗ 200;\n    q[i]= b;\n   }\n  }\n```", "```cpp\n{\n  for (i=0;i<100;i++)\n   q[i]=i;\n}\n```", "```cpp\n{\n  for (i=0;i<25;i+=4)\n   q[i]=i;\n   q[i+1]=i+1;\n   q[i+2]=i+2;\n   q[i+3]=i+3;\n```", "```cpp\nconst int a = b[base + i] ∗ c[base + i];\n```", "```cpp\nconst int a = b[NUM_ELEMENTS-1] ∗ c[NUM_ELEMENTS-1];\n```", "```cpp\nif (threadIdx.x < 32)\n{\n  if (threadIdx.x < 16)\n  {\n   if (threadIdx.x < 8)\n    func_a1();\n   else\n    func_a2();\n  }\n  else\n  {\n   func_b();\n  }\n}\n```", "```cpp\n// All threads follow the same path\n__global__ void cuda_test_kernel(\n  u32 ∗ const a,\n```", "```cpp\n  const u32 ∗ const c,\n  const u32 num_elements)\n{\n  const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n  if (tid < num_elements)\n  {\n   for (u32 iter=0; iter<MAX_ITER; iter++)\n   {\n    a[tid] += b[tid] ∗ c[tid];\n   }\n  }\n}\n```", "```cpp\n// Thread diverge by half warps\n__global__ void cuda_test_kernel_branched_half(\n  u32 ∗ const a,\n  const u32 ∗ const b,\n  const u32 ∗ const c,\n  const u32 num_elements)\n{\n  const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n  if (tid < num_elements)\n  {\n   for (u32 iter=0; iter<MAX_ITER; iter++)\n   {\n    if (threadIdx.x < 16)\n      a[tid] += b[tid] ∗ c[tid];\n    else\n      a[tid] -= b[tid] ∗ c[tid];\n   }\n  }\n}\n```", "```cpp\n// Thread diverge into one quarter group\n__global__ void cuda_test_kernel_branched_quarter(\n  u32 ∗ const a,\n  const u32 ∗ const b,\n  const u32 ∗ const c,\n  const u32 num_elements)\n{\n  const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n  if (tid < num_elements)\n  {\n   for (u32 iter=0; iter<MAX_ITER; iter++)\n```", "```cpp\n    if (threadIdx.x < 16)\n    {\n      if (threadIdx.x < 8)\n      {\n       a[tid] += b[tid] ∗ c[tid];\n      }\n      else\n      {\n       a[tid] -= b[tid] ∗ c[tid];\n      }\n    }\n    else\n    {\n      a[tid] += b[tid] ∗ c[tid];\n    }\n   }\n  }\n}\n```", "```cpp\n// Thread diverge into one eighth group\n__global__ void cuda_test_kernel_branched_eighth(\n  u32 ∗ const a,\n  const u32 ∗ const b,\n  const u32 ∗ const c,\n  const u32 num_elements)\n{\n  const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n  if (tid < num_elements)\n  {\n   for (u32 iter=0; iter<MAX_ITER; iter++)\n   {\n    if (threadIdx.x < 16)\n    {\n      if (threadIdx.x < 8)\n      {\n       if (threadIdx.x < 4)\n        a[tid] += b[tid] ∗ c[tid];\n       else\n        a[tid] -= b[tid] ∗ c[tid];\n      }\n      else\n      {\n       if (threadIdx.x >= 8)\n        a[tid] += b[tid] ∗ c[tid];\n       else\n        a[tid] -= b[tid] ∗ c[tid];\n```", "```cpp\n    }\n    else\n    {\n      a[tid] += b[tid] ∗ c[tid];\n    }\n   }\n  }\n}\n```", "```cpp\nID:0 GeForce GTX 470:Running 32768 blocks of 32 threads to calculate 1048576 elements\nID:0 GeForce GTX 470:All threads : 27.05 ms (100%)\nID:0 GeForce GTX 470:Half warps : 32.59 ms (121%)\nID:0 GeForce GTX 470:Quarter warps: 72.14 ms (267%)\nID:0 GeForce GTX 470:Eighth warps : 108.06 ms (400%)\n```", "```cpp\nID:1 GeForce 9800 GT:Running 32768 blocks of 32 threads to calculate 1048576 elements\nID:1 GeForce 9800 GT:All threads : 240.67 ms (100%)\nID:1 GeForce 9800 GT:Half warps : 241.33 ms (100%)\nID:1 GeForce 9800 GT:Quarter warps: 252.77 ms (105%)\nID:1 GeForce 9800 GT:Eighth warps : 285.49 ms (119%)\n```", "```cpp\nID:2 GeForce GTX 260:Running 32768 blocks of 32 threads to calculate 1048576 elements\nID:2 GeForce GTX 260:All threads : 120.36 ms (100%)\nID:2 GeForce GTX 260:Half warps : 122.44 ms (102%)\nID:2 GeForce GTX 260:Quarter warps: 149.60 ms (124%)\nID:2 GeForce GTX 260:Eighth warps : 174.50 ms (145%)\n```", "```cpp\nID:3 GeForce GTX 460:Running 32768 blocks of 32 threads to calculate 1048576 elements\nID:3 GeForce GTX 460:All threads : 43.16 ms (100%)\nID:3 GeForce GTX 460:Half warps : 57.49 ms (133%)\nID:3 GeForce GTX 460:Quarter warps: 127.68 ms (296%)\nID:3 GeForce GTX 460:Eighth warps : 190.85 ms (442%)\n```", "```cpp\n__global__ void add_prefix_sum_total_kernel(\n  u32 ∗ const prefix_idx,\n  const u32 ∗ const total_count)\n{\n  const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n  prefix_idx[tid] += total_count[blockIdx.x];\n}\n```", "```cpp\n.entry _Z27add_prefix_sum_total_kernelPjPKj(\n .param .u64 _Z27add_prefix_sum_total_kernelPjPKj_param_0,\n .param .u64 _Z27add_prefix_sum_total_kernelPjPKj_param_1\n)\n```", "```cpp\n .reg .s32  %r<10>;\n .reg .s64  %rl<9>;\n```", "```cpp\n ld.param.u64  %rl1, [_Z27add_prefix_sum_total_kernelPjPKj_param_0];\n ld.param.u64  %rl2, [_Z27add_prefix_sum_total_kernelPjPKj_param_1];\n cvta.to.global.u64  %rl3, %rl1;\n .loc 2 923 1\n mov.u32  %r1, %ntid.x;\n mov.u32  %r2, %ctaid.x;\n mov.u32  %r3, %tid.x;\n mad.lo.s32  %r4, %r1, %r2, %r3;\n cvta.to.global.u64  %rl4, %rl2;\n .loc 2 925 1\n mul.wide.u32  %rl5, %r2, 4;\n add.s64  %rl6, %rl4, %rl5;\n ldu.global.u32  %r5, [%rl6];\n .loc 2 925 1\n mul.wide.u32  %rl7, %r4, 4;\n add.s64  %rl8, %rl3, %rl7;\n ld.global.u32  %r6, [%rl8];\n add.s32  %r8, %r6, %r5;\n st.global.u32  [%rl8], %r8;\n .loc 2 926 2\n ret;\n}\n```", "```cpp\nFunction : _Z27add_prefix_sum_total_kernelPjPKj\n```", "```cpp\n/∗0000∗/ MOV R1, c [0x1] [0x100];\n/∗0008∗/ S2R R0, SR_CTAid_X;\n/∗0010∗/ S2R R2, SR_Tid_X;\n/∗0018∗/ MOV32I R6, 0x4;\n/∗0020∗/ IMAD R2, R0, c [0x0] [0x8], R2;\n/∗0028∗/ IMUL.U32.U32.HI R3, R2, 0x4;\n/∗0030∗/ IMAD.U32.U32 R4.CC, R2, R6, c [0x0] [0x20];\n/∗0038∗/ IADD.X R5, R3, c [0x0] [0x24];\n/∗0040∗/ IMAD.U32.U32 R6.CC, R0, R6, c [0x0] [0x28];\n/∗0048∗/ IMUL.U32.U32.HI R0, R0, 0x4;\n/∗0050∗/ IADD.X R7, R0, c [0x0] [0x2c];\n/∗0058∗/ LD_LDU.E.E.32.32 R2, R0, [R4], [R6+0x0];\n/∗0060∗/ IADD R0, R2, R0;\n/∗0068∗/ ST.E [R4], R0;\n/∗0070∗/ EXIT;\n```", "```cpp\n<instruction> <target_reg> <source_reg1> <source_reg2>\n```", "```cpp\n  // 0..127 (warps 0..3)\n  if (threadIdx.x < 128)\n0x0002caa0 [3393] mov.u32  %r30, %tid.x;\n0x0002caa0     S2R R0, SR_Tid_X;\n0x0002caa8     MOV R0, R0; \n0x0002cab0 [3395] setp.lt.u32  %p7, %r30, 128;\n0x0002cab0     ISETP.LT.U32.AND P0, pt, R0, 0x80, pt;\n0x0002cab8 [3397] not.pred  %p8, %p7;\n0x0002cab8     PSETP.AND.AND P0, pt, pt, pt, !P0;\n0x0002cac0 [3399] @%p8 bra  BB16_13;\n0x0002cac0     NOP CC.T; \n0x0002cac8     SSY 0x858; \n0x0002cad0     @P0 BRA 0x850; # Target=0x0002cb50\n  {\n  // Accumulate into a register and then write out\n  local_result += ∗(smem_ptr+128);\n0x0002cad8 [3403] ld.u64  %rl28, [%rl7+1024];\n0x0002cad8     IADD R8.CC, R2, 0x400;\n0x0002cae0     IADD.X R9, R3, RZ;\n0x0002cae8     MOV R10, R8;\n```", "```cpp\n0x0002caf8     LD.E.64 R8, [R10];\n0x0002cb00 [3405] add.s64  %rl42, %rl42, %rl28;\n0x0002cb00     IADD R6.CC, R6, R8;\n0x0002cb08     IADD.X R7, R7, R9;\n```", "```cpp\nptxas info : Compiling entry function ’_Z14functionTest’ for ’sm_20’\nptxas info : Function properties for _Z14functionTest\n40 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\nptxas info : Used 26 registers, 8+0 bytes lmem, 80 bytes cmem[0], 144 bytes cmem[2], 52 bytes cmem[16]\n```", "```cpp\nvoid odd_even_sort_cpu(u32 ∗ const data,\n        const u32 num_elem)\n{\n u32 offset = 0; // Start off with even, then odd\n u32 num_swaps; // Keep track of the number of swaps\n u32 run = 0;  // Keep track of the number of iterations\n```", "```cpp\n printf(\"\\nSorting %u elements using odd/even sort on cpu\\n\", num_elem);\n print_array(run, data, num_elem);\n```", "```cpp\n do\n {\n  run++;\n  num_swaps = 0; // Reset number of swaps each iteration\n```", "```cpp\n  // Iterate over 0..num_elements OR\n  // 1..(num_elements-1) in steps of two\n  for (u32 i=offset; i<(num_elem-offset); i+=2)\n  {\n   // Read values into registers\n   const u32 d0 = data[i];\n   const u32 d1 = data[i+1];\n\n   // Compare registers\n   if ( d0 > d1 )\n   {\n    // Swap values if needed\n    data[i] = d1;\n    data[i+1] = d0;\n```", "```cpp\n    // Keep track that we did a swap\n    num_swaps++;\n   }\n  }\n```", "```cpp\n  // Switch from even to odd, or odd to even\n  if (offset == 0)\n   offset = 1;\n```", "```cpp\n   offset = 0;\n```", "```cpp\n  // If something swapped then print the array\n  if (num_swaps > 0)\n   print_array(run, data, num_elem);\n\n // While elements are still being swapped\n } while (num_swaps != 0);\n}\n```", "```cpp\nRun 000: 15 14 13 12 11 10 09 08 07 06 05 04 03 02 01 00\nRun 001: 14 15 12 13 10 11 08 09 06 07 04 05 02 03 00 01\nRun 002: 14 12 15 10 13 08 11 06 09 04 07 02 05 00 03 01\nRun 003: 12 14 10 15 08 13 06 11 04 09 02 07 00 05 01 03\nRun 004: 12 10 14 08 15 06 13 04 11 02 09 00 07 01 05 03\nRun 005: 10 12 08 14 06 15 04 13 02 11 00 09 01 07 03 05\nRun 006: 10 08 12 06 14 04 15 02 13 00 11 01 09 03 07 05\nRun 007: 08 10 06 12 04 14 02 15 00 13 01 11 03 09 05 07\nRun 008: 08 06 10 04 12 02 14 00 15 01 13 03 11 05 09 07\nRun 009: 06 08 04 10 02 12 00 14 01 15 03 13 05 11 07 09\nRun 010: 06 04 08 02 10 00 12 01 14 03 15 05 13 07 11 09\nRun 011: 04 06 02 08 00 10 01 12 03 14 05 15 07 13 09 11\nRun 012: 04 02 06 00 08 01 10 03 12 05 14 07 15 09 13 11\nRun 013: 02 04 00 06 01 08 03 10 05 12 07 14 09 15 11 13\nRun 014: 02 00 04 01 06 03 08 05 10 07 12 09 14 11 15 13\nRun 015: 00 02 01 04 03 06 05 08 07 10 09 12 11 14 13 15\nRun 016: 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15\n```", "```cpp\n__global__ void odd_even_sort_gpu_kernel_gmem(\n  u32 ∗ const data,\n  const u32 num_elem)\n{\n  const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n  u32 tid_idx;\n  u32 offset = 0; // Start off with even, then odd\n```", "```cpp\n  // Calculation maximum index for a given block\n  // Last block it is number of elements minus one\n  // Other blocks to end of block minus one\n  const u32 tid_idx_max = min( (((blockIdx.x+1) ∗ (blockDim.x∗2))-1), (num_elem-1) );\n```", "```cpp\n  do\n  {\n   // Reset number of swaps\n   num_swaps = 0;\n```", "```cpp\n   // Work out index of data\n   tid_idx = (tid ∗ 2) + offset;\n```", "```cpp\n   // If no array or block overrun\n   if (tid_idx < tid_idx_max)\n   {\n    // Read values into registers\n    const u32 d0 = data[tid_idx];\n    const u32 d1 = data[tid_idx+1];\n```", "```cpp\n    // Compare registers\n    if ( d0 > d1 )\n    {\n      // Swap values if needed\n      data[tid_idx] = d1;\n      data[tid_idx+1] = d0;\n```", "```cpp\n      // Keep track that we did a swap\n      num_swaps++;\n    }\n   }\n```", "```cpp\n   // Switch from even to off, or odd to even\n   if (offset == 0)\n    offset = 1;\n   else\n    offset = 0;\n```", "```cpp\n  } while (__syncthreads_count(num_swaps) != 0);\n}\n```", "```cpp\n// Calculation maximum index for a given block\n// Last block it is number of elements minus one\n// Other blocks to end of block minus one\nconst u32 tid_idx_max = min( (((blockIdx.x+1) ∗ (blockDim.x∗2))-1), (num_elem-1) );\n```", "```cpp\n} while (__syncthreads_count(num_swaps) != 0);\n```", "```cpp\n// Host function - copy to / from and invoke kernel\n__host__ void odd_even_sort_gpu_gmem(\n  u32 ∗ const data_cpu,\n  const u32 num_elem)\n{\n  const u32 size_in_bytes = (num_elem ∗ sizeof(u32));\n  u32 ∗ data_gpu;\n```", "```cpp\n  // Allocate memory on the device\n  CUDA_CALL(cudaMalloc((void ∗∗) &data_gpu,\n         size_in_bytes));\n```", "```cpp\n  // Copy data to GPU\n  CUDA_CALL(cudaMemcpy(data_gpu, data_cpu, size_in_bytes, cudaMemcpyHostToDevice));\n```", "```cpp\n  // Use blocks of 256 threads\n  const u32 num_threads = 256;\n  const u32 num_blocks = (((num_elem/2) + (num_threads-1)) / num_threads);\n```", "```cpp\n  printf(\"\\nInvoking Odd Even sort with %d blocks of\n```", "```cpp\n    num_threads, (num_elem / 2));\n```", "```cpp\n  // Invoke the kernel\n  odd_even_sort_gpu_kernel_gmem<<<num_blocks, num_threads>>>(data_gpu, num_elem);\n```", "```cpp\n  cuda_error_check( \"Error Invoking kernel\",\n        \"odd_even_sort_gpu_kernel_gmem\");\n```", "```cpp\n  // Copy back to CPU memory space\n  CUDA_CALL(cudaMemcpy(data_cpu, data_gpu, size_in_bytes, cudaMemcpyDeviceToHost));\n```", "```cpp\n  // Free memory on the device\n  CUDA_CALL(cudaFree(data_gpu));\n```", "```cpp\n  print_array(0, data_cpu, num_elem);\n}\n```", "```cpp\nInvoking Odd Even sort with 1 blocks of 8 threads (8 active)\nRun 000: 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15\nTest passed\n```", "```cpp\nInvoking Odd Even sort with 2 blocks of 4 threads (8 active)\nRun 000: 08 09 10 11 12 13 14 15 00 01 02 03 04 05 06 07\nTest failed\n```", "```cpp\n// Every thread does atomic add to the same\n// address in GMEM\n__global__ void reduce_gmem(const u32 ∗ const data,\n          u64 ∗ const result,\n          const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n if (tid < num_elements)\n  atomicAdd(result, (u64) data[tid]);\n}\n```", "```cpp\nProcessing 48 MB of data, 12M elements\nID:0 GeForce GTX 470:GMEM   passed Time 197.84 ms\nID:3 GeForce GTX 460:GMEM   passed Time 164.28 ms\n```", "```cpp\n// Every thread does atomic add to the same\n```", "```cpp\n__global__ void reduce_gmem_ILP2(const uint2 ∗ const data,\n           u64 ∗ const result,\n           const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n if (tid < (num_elements>>1))\n {\n  uint2 element = data[tid];\n```", "```cpp\n  const u64 add_value = ((u64)element.x) + \n          ((u64)element.y);\n```", "```cpp\n  atomicAdd(result, add_value);\n }\n}\n```", "```cpp\n// Every thread does atomic add to the same\n// address in GMEM\n__global__ void reduce_gmem_ILP4(const uint4 ∗ const data,\n           u64 ∗ const result,\n           const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n if (tid < (num_elements>>2))\n {\n  uint4 element = data[tid];\n```", "```cpp\n  const u64 add_value = ((u64)element.x) + \n          ((u64)element.y) + \n          ((u64)element.z) + \n          ((u64)element.w);\n```", "```cpp\n  atomicAdd(result, add_value);\n }\n}\n```", "```cpp\nID:0 GeForce GTX 470:GMEM ILP2  passed Time 98.96 ms\nID:3 GeForce GTX 460:GMEM ILP2  passed Time 82.37 ms\n```", "```cpp\nID:0 GeForce GTX 470:GMEM ILP4  passed Time 49.53 ms\nID:3 GeForce GTX 460:GMEM ILP4  passed Time 41.47 ms\n```", "```cpp\nconst u64 add_value = ((u64)element.x) + element.y + element.z + element.w;\n```", "```cpp\nconst u64 add_value = ((u64)element.x) + ((u64)element.y) + ((u64)element.z) + ((u64)element.w);\n```", "```cpp\n// Every thread does atomic add to the same\n// address in GMEM\n__global__ void reduce_gmem_ILP8(const uint4 ∗ const data,\n           u64 ∗ const result,\n           const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n if (tid < (num_elements>>3))\n {\n  const u32 idx = (tid ∗ 2);\n```", "```cpp\n  uint4 element = data[idx];\n```", "```cpp\n      ((u64)element.y) + \n      ((u64)element.z) + \n      ((u64)element.w);\n```", "```cpp\n  element = data[idx+1];\n  value += ((u64)element.x) + \n     ((u64)element.y) + \n     ((u64)element.z) + \n     ((u64)element.w);\n```", "```cpp\n  atomicAdd(result, value);\n }\n}\n```", "```cpp\n// Every thread does atomic add to the same\n// address in GMEM\n__global__ void reduce_gmem_ILP16(const uint4 ∗ const data,\n           u64 ∗ const result,\n           const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n if (tid < (num_elements>>4))\n {\n  const u32 idx = (tid ∗ 4);\n```", "```cpp\n  uint4 element = data[idx];\n  u64 value = ((u64)element.x) + \n      ((u64)element.y) + \n      ((u64)element.z) + \n      ((u64)element.w);\n```", "```cpp\n  element = data[idx+1];\n  value += ((u64)element.x) + \n     ((u64)element.y) + \n     ((u64)element.z) + \n     ((u64)element.w);\n```", "```cpp\n  element = data[idx+2];\n  value += ((u64)element.x) + \n     ((u64)element.y) + \n     ((u64)element.z) + \n     ((u64)element.w);\n```", "```cpp\n  element = data[idx+3];\n  value += ((u64)element.x) + \n```", "```cpp\n     ((u64)element.z) + \n     ((u64)element.w);\n```", "```cpp\n  atomicAdd(result, value);\n }\n}\n```", "```cpp\n// Every thread does atomic add to the same\n// address in GMEM\n__global__ void reduce_gmem_ILP32(const uint4 ∗ const data,\n           u64 ∗ const result,\n           const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n if (tid < (num_elements>>5))\n {\n  const u32 idx = (tid ∗ 8);\n```", "```cpp\n  uint4 element = data[idx];\n  u64 value = ((u64)element.x) + \n      ((u64)element.y) + \n      ((u64)element.z) + \n      ((u64)element.w);\n```", "```cpp\n  element = data[idx+1];\n  value += ((u64)element.x) + \n     ((u64)element.y) + \n     ((u64)element.z) + \n     ((u64)element.w);\n```", "```cpp\n  element = data[idx+2];\n  value += ((u64)element.x) + \n     ((u64)element.y) + \n     ((u64)element.z) + \n     ((u64)element.w);\n```", "```cpp\n  element = data[idx+3];\n  value += ((u64)element.x) + \n     ((u64)element.y) + \n     ((u64)element.z) + \n     ((u64)element.w);\n```", "```cpp\n  element = data[idx+4];\n  value += ((u64)element.x) + \n     ((u64)element.y) + \n     ((u64)element.z) + \n```", "```cpp\n  element = data[idx+5];\n  value += ((u64)element.x) + \n     ((u64)element.y) + \n     ((u64)element.z) + \n     ((u64)element.w);\n```", "```cpp\n  element = data[idx+6];\n  value += ((u64)element.x) + \n     ((u64)element.y) + \n     ((u64)element.z) + \n     ((u64)element.w);\n```", "```cpp\n  element = data[idx+7];\n  value += ((u64)element.x) + \n     ((u64)element.y) + \n     ((u64)element.z) + \n     ((u64)element.w);\n```", "```cpp\n  atomicAdd(result, value);\n }\n}\n```", "```cpp\nID:0 GeForce GTX 470:GMEM ILP8  passed Time 24.83 ms\nID:3 GeForce GTX 460:GMEM ILP8  passed Time 20.97 ms\n```", "```cpp\nID:0 GeForce GTX 470:GMEM ILP16 passed Time 12.49 ms\nID:3 GeForce GTX 460:GMEM ILP16 passed Time 10.75 ms\n```", "```cpp\nID:0 GeForce GTX 470:GMEM ILP32 passed Time 13.18 ms\nID:3 GeForce GTX 460:GMEM ILP32 passed Time 15.94 ms\n```", "```cpp\n// Every thread does atomic add to the same\n// address in GMEM after internal accumulation\n__global__ void reduce_gmem_loop(const u32 ∗ const data,\n           u64 ∗ const result,\n           const u32 num_elements)\n{\n // Divide the num. elements by the number of blocks launched\n // ( 4096 elements / 256 threads) / 16 blocks = 1 iter\n // ( 8192 elements / 256 threads) / 16 blocks = 2 iter\n // (16384 elements / 256 threads) / 16 blocks = 4 iter\n // (32768 elements / 256 threads) / 16 blocks = 8 iter\n const u32 num_elements_per_block = ((num_elements / blockDim.x) / gridDim.x);\n```", "```cpp\n const u32 increment = (blockDim.x ∗ gridDim.x);\n```", "```cpp\n // Work out the initial index\n u32 idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n // Accumulate into this register parameter\n u64 local_result = 0;\n```", "```cpp\n // Loop N times depending on the number of\n // blocks launched\n for (u32 i=0; i<num_elements_per_block; i++)\n {\n  // If still within bounds, add into result\n  if (idx < num_elements)\n   local_result += (data[idx]);\n```", "```cpp\n  // Move to the next element in the list\n  idx += increment;\n }\n```", "```cpp\n // Add the final result to the GMEM accumulator\n atomicAdd(result, local_result);\n```", "```cpp\nID:0 GeForce GTX 470:GMEM loop1 49152 passed Time 197.82 ms\nID:0 GeForce GTX 470:GMEM loop1 24576 passed Time 98.96 ms\nID:0 GeForce GTX 470:GMEM loop1 12288 passed Time 49.56 ms\nID:0 GeForce GTX 470:GMEM loop1 6144 passed Time 24.83 ms\nID:0 GeForce GTX 470:GMEM loop1 3072 passed Time 12.48 ms\nID:0 GeForce GTX 470:GMEM loop1 1536 passed Time 6.33 ms\nID:0 GeForce GTX 470:GMEM loop1 768 passed Time 3.35 ms\nID:0 GeForce GTX 470:GMEM loop1 384 passed Time 2.26 ms\nID:0 GeForce GTX 470:GMEM loop1 192 passed Time 1.92 ms\nID:0 GeForce GTX 470:GMEM loop1 96 passed Time 1.87 ms\nID:0 GeForce GTX 470:GMEM loop1 64 passed Time 1.48 ms\nID:0 GeForce GTX 470:GMEM loop1 48 passed Time 1.50 ms\nID:0 GeForce GTX 470:GMEM loop1 32 passed Time 1.75 ms\nID:0 GeForce GTX 470:GMEM loop1 16 passed Time 2.98 ms\n```", "```cpp\nID:3 GeForce GTX 460:GMEM loop1 49152 passed Time 164.25 ms\nID:3 GeForce GTX 460:GMEM loop1 24576 passed Time 82.45 ms\nID:3 GeForce GTX 460:GMEM loop1 12288 passed Time 41.52 ms\nID:3 GeForce GTX 460:GMEM loop1 6144 passed Time 21.01 ms\nID:3 GeForce GTX 460:GMEM loop1 3072 passed Time 10.77 ms\nID:3 GeForce GTX 460:GMEM loop1 1536 passed Time 5.60 ms\nID:3 GeForce GTX 460:GMEM loop1 768 passed Time 3.16 ms\nID:3 GeForce GTX 460:GMEM loop1 384 passed Time 2.51 ms\nID:3 GeForce GTX 460:GMEM loop1 192 passed Time 2.19 ms\nID:3 GeForce GTX 460:GMEM loop1 96 passed Time 2.12 ms\nID:3 GeForce GTX 460:GMEM loop1 64 passed Time 2.05 ms\nID:3 GeForce GTX 460:GMEM loop1 48 passed Time 2.41 ms\nID:3 GeForce GTX 460:GMEM loop1 32 passed Time 1.96 ms\nID:3 GeForce GTX 460:GMEM loop1 16 passed Time 2.70 ms\n```", "```cpp\n// Every thread does atomic add to the same\n// address in GMEM after internal accumulation\n__launch_bounds__(256)\n__global__ void reduce_gmem_loop_ILP2(\n const uint2 ∗ const data,\n u64 ∗ const result,\n const u32 num_elements)\n{\n const u32 num_elements_per_block = (( (num_elements / 2) / blockDim.x) / gridDim.x);\n const u32 increment = (blockDim.x ∗ gridDim.x);\n```", "```cpp\n // Work out the initial index\n u32 idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n // Accumulate into this register parameter\n u64 local_result = 0;\n```", "```cpp\n // Loop N times depending on the number\n // of blocks launched\n for (u32 i=0; i<num_elements_per_block; i++)\n {\n  // If still within bounds, add into result\n  if (idx < (num_elements>>1))\n  {\n    const uint2 elem = data[idx];\n```", "```cpp\n    local_result += ((u64)elem.x) + ((u64)elem.y);\n```", "```cpp\n    // Move to the next element in the list\n    idx += increment;\n  }\n }\n```", "```cpp\n // Add the final result to the GMEM accumulator\n atomicAdd(result, local_result);\n}\n```", "```cpp\n// Every thread does atomic add to the same\n// address in GMEM after internal accumulation\n__launch_bounds__(256)\n__global__ void reduce_gmem_loop_ILP4(\n```", "```cpp\n u64 ∗ const result,\n const u32 num_elements)\n{\n const u32 num_elements_per_block = (( (num_elements/4) / blockDim.x) / gridDim.x);\n const u32 increment = (blockDim.x ∗ gridDim.x);\n```", "```cpp\n // Work out the initial index\n u32 idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n // Accumulate into this register parameter\n u64 local_result = 0;\n```", "```cpp\n // Loop N times depending on the number\n // of blocks launched\n for (u32 i=0; i<num_elements_per_block; i++)\n {\n  // If still within bounds, add into result\n  if (idx < (num_elements>>2))\n  {\n    const uint4 elem = data[idx];\n```", "```cpp\n    local_result += ((u64)elem.x) + ((u64)elem.y);\n    local_result += ((u64)elem.z) + ((u64)elem.w);\n```", "```cpp\n    // Move to the next element in the list\n    idx += increment;\n  }\n }\n```", "```cpp\n // Add the final result to the GMEM accumulator\n atomicAdd(result, local_result);\n}\n```", "```cpp\nID:0 GeForce GTX 470:GMEM loop1 64 passed Time 1.48 ms\nID:3 GeForce GTX 460:GMEM loop1 64 passed Time 2.05 ms\nID:0 GeForce GTX 470:GMEM loop2 64 passed Time 1.16 ms\nID:3 GeForce GTX 460:GMEM loop2 64 passed Time 1.49 ms\nID:0 GeForce GTX 470:GMEM loop4 64 passed Time 1.14 ms\nID:3 GeForce GTX 460:GMEM loop4 64 passed Time 1.38 ms\n```", "```cpp\n__global__ void reduce_gmem_loop_block(\n const uint4 ∗ const data,\n u64 ∗ const result,\n const u32 num_elements)\n{\n const u32 num_elements_per_block = (( (num_elements/4) / blockDim.x) / gridDim.x);\n const u32 increment = (blockDim.x ∗ gridDim.x);\n const u32 num_u4_elements = (num_elements>>2);\n```", "```cpp\n // Work out the initial index\n u32 idx = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n // Accumulate into this register parameter\n u64 local_result = 0;\n```", "```cpp\n // Loop N times depending on the\n // number of blocks launched\n for (u32 i=0; i<num_elements_per_block; i++)\n {\n  // If still within bounds, add into result\n  if (idx < num_u4_elements)\n  {\n    const uint4 elem = data[idx];\n```", "```cpp\n    local_result += ((u64)elem.x) + ((u64)elem.y);\n    local_result += ((u64)elem.z) + ((u64)elem.w);\n```", "```cpp\n    // Move to the next element in the list\n    idx += increment;\n  }\n }\n```", "```cpp\n const u32 num_half_warps = blockDim.x >> 4;\n const u32 half_warp = threadIdx.x >> 4;\n```", "```cpp\n // Have first N threads clear the half warps\n if (threadIdx.x < num_half_warps)\n  intra_half_warp_reduce[threadIdx.x] = 0;\n```", "```cpp\n // Wait for threads to zero SMEM\n __syncthreads();\n```", "```cpp\n // Reduce first by half warp into SMEM\n // 256 -> 16 (32 banks)\n atomicAdd( &intra_half_warp_reduce[half_warp],\n     local_result );\n```", "```cpp\n // Wait for all threads to complete\n __syncthreads();\n```", "```cpp\n // Write up to 16 values out to GMEM\n if (threadIdx.x < num_half_warps)\n  atomicAdd(result,\n      intra_half_warp_reduce[threadIdx.x]);\n```", "```cpp\nID:0 GeForce GTX 470:GMEM loopB 64 passed Time 0.93 ms\nID:0 GeForce GTX 470:GMEM loopC 64 passed Time 0.93 ms\n```", "```cpp\nID:3 GeForce GTX 460:GMEM loopB 64 passed Time 1.34 ms\nID:3 GeForce GTX 460:GMEM loopC 64 passed Time 1.33 ms\n```", "```cpp\nID:0 GeForce GTX 470:GMEM loopC 6144 passed Time 2.42 ms\nID:0 GeForce GTX 470:GMEM loopC 3072 passed Time 1.54 ms\nID:0 GeForce GTX 470:GMEM loopC 1536 passed Time 1.11 ms\nID:0 GeForce GTX 470:GMEM loopC 768 passed Time 0.89 ms\nID:0 GeForce GTX 470:GMEM loopC 384 passed Time 0.80 ms\nID:0 GeForce GTX 470:GMEM loopC 192 passed Time 0.82 ms\nID:0 GeForce GTX 470:GMEM loopC 96 passed Time 0.83 ms\nID:0 GeForce GTX 470:GMEM loopC 64 passed Time 0.77 ms\n```", "```cpp\nID:0 GeForce GTX 470:GMEM loopC 32 passed Time 0.95 ms\nID:0 GeForce GTX 470:GMEM loopC 16 passed Time 1.40 ms\nID:3 GeForce GTX 460:GMEM loopC 6144 passed Time 3.53 ms\nID:3 GeForce GTX 460:GMEM loopC 3072 passed Time 2.04 ms\nID:3 GeForce GTX 460:GMEM loopC 1536 passed Time 1.41 ms\nID:3 GeForce GTX 460:GMEM loopC 768 passed Time 1.11 ms\nID:3 GeForce GTX 460:GMEM loopC 384 passed Time 0.97 ms\nID:3 GeForce GTX 460:GMEM loopC 192 passed Time 0.92 ms\nID:3 GeForce GTX 460:GMEM loopC 96 passed Time 0.91 ms\nID:3 GeForce GTX 460:GMEM loopC 64 passed Time 0.95 ms\nID:3 GeForce GTX 460:GMEM loopC 48 passed Time 1.00 ms\nID:3 GeForce GTX 460:GMEM loopC 32 passed Time 1.02 ms\nID:3 GeForce GTX 460:GMEM loopC 16 passed Time 1.29 ms\n```", "```cpp\n1>ptxas info  : Used 18 registers, 1032+0 bytes smem, 52 bytes cmem[0]\n1>ptxas info  : Compiling entry function ’_Z27reduce_gmem_loop_block_256tPK5uint4Pyj’ for ’sm_20’\n1>ptxas info  : Function properties for _Z27reduce_gmem_loop_block_256tPK5uint4Pyj\n1>  16 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\n```", "```cpp\nID:0 GeForce GTX 470:GMEM loopD 384 passed Time 0.68 ms\nID:0 GeForce GTX 470:GMEM loopD 192 passed Time 0.72 ms\nID:0 GeForce GTX 470:GMEM loopD 96 passed Time 0.73 ms\n```", "```cpp\nID:3 GeForce GTX 460:GMEM loopD 384 passed Time 0.85 ms\nID:3 GeForce GTX 460:GMEM loopD 192 passed Time 0.81 ms\nID:3 GeForce GTX 460:GMEM loopD 96 passed Time 0.80 ms\n```", "```cpp\n// Write initial result to smem – 256 threads\nsmem_data[threadIdx.x] = local_result;\n__syncthreads();\n```", "```cpp\n// 0..127\nif (threadIdx.x < 128)\n smem_data[threadIdx.x] += smem_data[(threadIdx.x)+128];\n__syncthreads();\n```", "```cpp\n// 0..63\nif (threadIdx.x < 64)\n smem_data[threadIdx.x] += smem_data[(threadIdx.x)+64];\n__syncthreads();\n```", "```cpp\n// 0..31 - A single warp\nif (threadIdx.x < 32)\n{\n smem_data[threadIdx.x] += smem_data[(threadIdx.x)+32];  // 0..31\n smem_data[threadIdx.x] += smem_data[(threadIdx.x)+16];  // 0..15\n smem_data[threadIdx.x] += smem_data[(threadIdx.x)+8];  // 0..7\n smem_data[threadIdx.x] += smem_data[(threadIdx.x)+4];  // 0..3\n smem_data[threadIdx.x] += smem_data[(threadIdx.x)+2];  // 0..1\n```", "```cpp\n // Have thread zero write out the result to GMEM\n if (threadIdx.x == 0)\n  atomicAdd(result, smem_data[0] + smem_data[1]);\n}\n```", "```cpp\nID:0 GeForce GTX 470:GMEM loopE 384 passed Time 0.64 ms\nID:3 GeForce GTX 460:GMEM loopE 192 passed Time 0.79 ms\n```", "```cpp\n// Create a pointer to the smem data area\nu64 ∗ const smem_ptr = &smem_data[(threadIdx.x)];\n```", "```cpp\n// Store results - 128..255 (warps 4..7)\nif (threadIdx.x >= 128)\n{\n ∗(smem_ptr) = local_result;\n}\n__syncthreads();\n```", "```cpp\n// 0..127 (warps 0..3)\nif (threadIdx.x < 128)\n{\n // Accumulate into a register and then write out\n local_result += ∗(smem_ptr+128);\n```", "```cpp\n if (threadIdx.x >= 64)   // Warps 2 and 3\n  ∗smem_ptr = local_result;\n}\n__syncthreads();\n```", "```cpp\n// 0..63 (warps 0 and 1)\nif (threadIdx.x < 64)\n{\n // Accumulate into a register and then write out\n```", "```cpp\n ∗smem_ptr = local_result;\n```", "```cpp\n if (threadIdx.x >= 32)   // Warp 1\n  ∗smem_ptr = local_result;\n}\n__syncthreads();\n```", "```cpp\n// 0..31 - A single warp\nif (threadIdx.x < 32)\n{\n local_result += ∗(smem_ptr+32);\n ∗(smem_ptr) = local_result;\n```", "```cpp\n local_result += ∗(smem_ptr+16);\n ∗(smem_ptr) = local_result;\n```", "```cpp\n local_result += ∗(smem_ptr+8);\n ∗(smem_ptr) = local_result;\n```", "```cpp\n local_result += ∗(smem_ptr+4);\n ∗(smem_ptr) = local_result;\n```", "```cpp\n local_result += ∗(smem_ptr+2);\n ∗(smem_ptr) = local_result;\n```", "```cpp\n local_result += ∗(smem_ptr+1);\n```", "```cpp\n // Have thread zero write out the result to GMEM\n if (threadIdx.x == 0)\n  atomicAdd(result, local_result );\n}\n```", "```cpp\nID:0 GeForce GTX 470:GMEM loopE 384 passed Time 0.62 ms\nID:3 GeForce GTX 460:GMEM loopE 192 passed Time 0.77 ms\n```", "```cpp\nu64 reduce_cpu_serial(const u32 ∗ data,\n        const u32 num_elements)\n{\n  u64 result = 0;\n```", "```cpp\n  for (u32 i=0; i< num_elements; i++)\n   result += data[i];\n```", "```cpp\n  return result;\n}\n```", "```cpp\nu64 reduce_cpu_parallel(const u32 ∗ data,\n        const int num_elements)\n{\n  u64 result = 0;\n  int i=0;\n```", "```cpp\n#pragma omp paralle l for reduction(+:result)\n  for (i=0; i< num_elements; i++)\n   result += data[i];\n```", "```cpp\n  return result;\n}\n```", "```cpp\nif (top left corner cell)\nelse if (top right corner cell)\nelse if (bottom right corner cell)\nelse if (bottom left corner cell)\nelse if (top row)\n```", "```cpp\nelse if (bottom row)\nelse if (left row)\nelse (must be centre element)\n```", "```cpp\n#define PI (3.14)\n```", "```cpp\n#define PI (3.14F)\n```", "```cpp\nsetp.eq.s32  %p16, %r295, 1;\n@%p16 bra  BB9_31;\n```", "```cpp\nif (local_idx >= 12)\nlocal_idx = 0;\n```", "```cpp\nsetp.gt.u32 %p18, %r136, 11;\nselp.b32  %r295, 0, %r136, %p18;\n```", "```cpp\n// Fetch the test data element\nswitch(local_idx)\n{\n  case 0: elem = local_elem_00; break;\n  case 1: elem = local_elem_01; break;\n  case 2: elem = local_elem_02; break;\n  case 3: elem = local_elem_03; break;\n```", "```cpp\n  case 4: elem = local_elem_04; break;\n  case 5: elem = local_elem_05; break;\n  case 6: elem = local_elem_06; break;\n  case 7: elem = local_elem_07; break;\n```", "```cpp\n  case 8: elem = local_elem_08; break;\n  case 9: elem = local_elem_09; break;\n  case 10: elem = local_elem_10; break;\n  case 11: elem = local_elem_11; break;\n```", "```cpp\n  case 12: elem = local_elem_12; break;\n  case 13: elem = local_elem_13; break;\n  case 14: elem = local_elem_14; break;\n  case 15: elem = local_elem_15; break;\n}\n```", "```cpp\ncudaError_t cudaGetDeviceCount(int ∗ count);\n```", "```cpp\ncudaError_t cudaGetDeviceProperties (struct cudaDeviceProp ∗ prop, int device);\n```"]