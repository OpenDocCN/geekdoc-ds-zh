- en: GPU programming concepts
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPU编程概念
- en: 原文：[https://enccs.github.io/gpu-programming/4-gpu-concepts/](https://enccs.github.io/gpu-programming/4-gpu-concepts/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://enccs.github.io/gpu-programming/4-gpu-concepts/](https://enccs.github.io/gpu-programming/4-gpu-concepts/)
- en: '*[GPU programming: why, when and how?](../)* **   GPU programming concepts'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*[GPU编程：为什么、何时以及如何？](../)* **   GPU编程概念'
- en: '[Edit on GitHub](https://github.com/ENCCS/gpu-programming/blob/main/content/4-gpu-concepts.rst)'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在GitHub上编辑](https://github.com/ENCCS/gpu-programming/blob/main/content/4-gpu-concepts.rst)'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Questions
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 问题
- en: What types of parallel computing is possible?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能有哪些类型的并行计算？
- en: How does data parallelism differ from task parallelism, and how are they utilized
    in parallel computing?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据并行性与任务并行性有何不同，它们在并行计算中是如何被利用的？
- en: How is the work parallelized and executed on GPUs?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作是如何在GPU上并行化和执行的？
- en: What are general considerations for an efficient code running on GPUs?
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GPU上高效运行的代码有哪些一般性考虑？
- en: Objectives
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Understand parallel computing principles and architectures.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解并行计算原理和架构。
- en: Differentiate data parallelism from task parallelism.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分数据并行性与任务并行性。
- en: Learn the GPU execution model.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习GPU执行模型。
- en: Parallelize and execute work on GPUs.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GPU上并行化和执行工作。
- en: Develop efficient GPU code for high performance.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发高效的GPU代码以实现高性能。
- en: Instructor note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 教师备注
- en: 25 min teaching
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 25分钟教学
- en: 0 min exercises
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0分钟练习
- en: Different types of parallelism
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同的并行类型
- en: Distributed- vs. Shared-Memory Architecture
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式与共享内存架构
- en: Most of computing problems are not trivially parallelizable, which means that
    the subtasks need to have access from time to time to some of the results computed
    by other subtasks. The way subtasks exchange needed information depends on the
    available hardware.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数计算问题都不是简单可并行的，这意味着子任务需要不时地访问其他子任务计算的一些结果。子任务交换所需信息的方式取决于可用的硬件。
- en: '![../_images/distributed_vs_shared.png](../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/distributed_vs_shared.png](../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png)'
- en: Distributed- vs shared-memory parallel computing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式与共享内存并行计算。
- en: In a distributed memory environment each processing unit operates independently
    from the others. It has its own memory and it **cannot** access the memory in
    other nodes. The communication is done via network and each computing unit runs
    a separate copy of the operating system. In a shared memory machine all processing
    units have access to the memory and can read or modify the variables within.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式内存环境中，每个处理单元独立于其他处理单元运行。它有自己的内存，并且**不能**访问其他节点的内存。通信通过网络进行，每个计算单元运行操作系统的单独副本。在共享内存机器中，所有处理单元都可以访问内存，并可以读取或修改变量。
- en: Processes and Threads
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进程和线程
- en: The type of environment (distributed- or shared-memory) determines the programming
    model. There are two types of parallelism possible, process based and thread based.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 环境类型（分布式或共享内存）决定了编程模型。可能的并行性有两种，基于进程的和基于线程的。
- en: '![../_images/processes-threads.png](../Images/94b62a3205925cbbdac5058209021754.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/processes-threads.png](../Images/94b62a3205925cbbdac5058209021754.png)'
- en: For distributed memory machines, a process-based parallel programming model
    is employed. The processes are independent execution units which have their *own
    memory* address spaces. They are created when the parallel program is started
    and they are only terminated at the end. The communication between them is done
    explicitly via message passing like MPI.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分布式内存机器，采用基于进程的并行编程模型。进程是独立的执行单元，它们有自己的内存地址空间。它们在并行程序启动时创建，并在结束时终止。它们之间的通信通过显式的消息传递（如MPI）进行。
- en: On the shared memory architectures it is possible to use a thread based parallelism.
    The threads are light execution units and can be created and destroyed at a relatively
    small cost. The threads have their own state information, but they *share* the
    *same memory* address space. When needed the communication is done though the
    shared memory.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存架构上，可以使用基于线程的并行性。线程是轻量级的执行单元，可以以相对较小的成本创建和销毁。线程有自己的状态信息，但它们*共享*相同的内存地址空间。当需要时，通信通过共享内存进行。
- en: Both approaches have their advantages and disadvantages. Distributed machines
    are relatively cheap to build and they have an “infinite “ capacity. In principle
    one could add more and more computing units. In practice the more computing units
    are used the more time consuming is the communication. The shared memory systems
    can achieve good performance and the programming model is quite simple. However
    they are limited by the memory capacity and by the access speed. In addition in
    the shared parallel model it is much easier to create race conditions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都有其优点和缺点。分布式机器相对便宜，并且具有“无限”的容量。原则上可以添加更多的计算单元。在实践中，使用的计算单元越多，通信就越耗时。共享内存系统可以实现良好的性能，编程模型相当简单。然而，它们受限于内存容量和访问速度。此外，在共享并行模型中，创建竞态条件要容易得多。
- en: Exposing parallelism
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 暴露并行性
- en: There are two types of parallelism that can be explored. The data parallelism
    is when the data can be distributed across computational units that can run in
    parallel. The units process the data by applying the same or very similar operation
    to different data elements. A common example is applying a blur filter to an image
    — the same function is applied to all the pixels on an image. This parallelism
    is natural for the GPU, where the same instruction set is executed in multiple
    threads.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可以探索两种类型的并行。数据并行是指数据可以分布在可以并行运行的计算单元上。单元通过将相同的或非常相似的操作应用于不同的数据元素来处理数据。一个常见的例子是对图像应用模糊滤镜——相同的函数应用于图像上的所有像素。这种并行对于GPU来说是自然的，因为在多个线程中执行相同的指令集。
- en: '[![../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png](../Images/df3318bcb375513cb4f8169dcb4efdbc.png)](../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png](../Images/df3318bcb375513cb4f8169dcb4efdbc.png)'
- en: Data parallelism and task parallelism. The data parallelism is when the same
    operation applies to multiple data (e.g. multiple elements of an array are transformed).
    The task parallelism implies that there are more than one independent task that,
    in principle, can be executed in parallel.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行和任务并行。数据并行是指相同的操作应用于多个数据（例如，数组的多个元素被转换）。任务并行意味着存在多个独立的任务，原则上可以并行执行。
- en: Data parallelism can usually be explored by the GPUs quite easily. The most
    basic approach would be finding a loop over many data elements and converting
    it into a GPU kernel. If the number of elements in the data set is fairly large
    (tens or hundred of thousands elements), the GPU should perform quite well. Although
    it would be odd to expect absolute maximum performance from such a naive approach,
    it is often the one to take. Getting absolute maximum out of the data parallelism
    requires good understanding of how GPU works.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行通常可以通过GPU轻松探索。最基本的方法是找到一个覆盖许多数据元素的循环，并将其转换为GPU内核。如果数据集中的元素数量相当大（数十或数百万个元素），GPU应该表现相当出色。尽管从这种天真方法中期望绝对最大性能可能有些奇怪，但它通常是采取的方法。从数据并行中获得绝对最大性能需要很好地理解GPU的工作原理。
- en: 'Another type of parallelism is a task parallelism. This is when an application
    consists of more than one task that requiring to perform different operations
    with (the same or) different data. An example of task parallelism is cooking:
    slicing vegetables and grilling are very different tasks and can be done at the
    same time. Note that the tasks can consume totally different resources, which
    also can be explored.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种类型的并行是任务并行。这是指一个应用程序由多个任务组成，这些任务需要执行不同的操作（使用相同的或不同的数据）。任务并行的一个例子是烹饪：切菜和烤肉是非常不同的任务，可以同时进行。请注意，任务可以消耗完全不同的资源，这也可以被探索。
- en: In short
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之
- en: Computing problems can be parallelized in distributed memory or shared memory
    architectures.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算问题可以在分布式内存或共享内存架构中并行化。
- en: In distributed memory, each unit operates independently, with no direct memory
    access between nodes.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式内存中，每个单元独立操作，节点之间没有直接的内存访问。
- en: In shared memory, units have access to the same memory and can communicate through
    shared variables.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在共享内存中，单元可以访问相同的内存，并通过共享变量进行通信。
- en: Parallel programming can be process-based (distributed memory) or thread-based
    (shared memory).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行编程可以是基于进程的（分布式内存）或基于线程的（共享内存）。
- en: Process-based parallelism uses independent processes with separate memory spaces
    and explicit message passing.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于进程的并行使用具有独立内存空间和显式消息传递的独立进程。
- en: Thread-based parallelism uses lightweight threads that share the same memory
    space and communicate through shared memory.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于线程的并行使用共享相同内存空间的轻量级线程，并通过共享内存进行通信。
- en: Data parallelism distributes data across computational units, processing them
    with the same or similar operations.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据并行将数据分布到计算单元中，使用相同的或类似的操作进行处理。
- en: Task parallelism involves multiple independent tasks that perform different
    operations on the same or different data.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务并行涉及多个独立任务，这些任务在相同或不同的数据上执行不同的操作。
- en: Task parallelism involves executing different tasks concurrently, leveraging
    different resources.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务并行涉及并发执行不同的任务，利用不同的资源。
- en: GPU Execution Model
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU 执行模型
- en: In order to obtain maximum performance it is important to understand how GPUs
    execute the programs. As mentioned before a CPU is a flexible device oriented
    towards general purpose usage. It’s fast and versatile, designed to run operating
    systems and various, very different types of applications. It has lots of features,
    such as better control logic, caches and cache coherence, that are not related
    to pure computing. CPUs optimize the execution by trying to achieve low latency
    via heavy caching and branch prediction.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳性能，了解 GPU 如何执行程序非常重要。如前所述，CPU 是一种灵活的通用设备。它速度快，功能多样，旨在运行操作系统和多种非常不同的应用程序。它具有许多功能，如更好的控制逻辑、缓存和缓存一致性，这些功能与纯计算无关。CPU
    通过大量缓存和分支预测来尝试降低延迟以优化执行。
- en: '[![../_images/cpu-gpu-highway.png](../Images/2be579a867209f9ee644b6264269be37.png)](../_images/cpu-gpu-highway.png)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/cpu-gpu-highway.png](../Images/2be579a867209f9ee644b6264269be37.png)'
- en: Cars and roads analogy for the CPU and GPU behavior. The compact road is analogous
    to the CPU (low latency, low throughput) and the broader road is analogous to
    the GPU (high latency, high throughput).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 用汽车和道路来类比 CPU 和 GPU 的行为。紧凑的道路相当于 CPU（低延迟，低吞吐量），而宽阔的道路相当于 GPU（高延迟，高吞吐量）。
- en: In contrast the GPUs contain a relatively small amount of transistors dedicated
    to control and caching, and a much larger fraction of transistors dedicated to
    the mathematical operations. Since the cores in a GPU are designed just for 3D
    graphics, they can be made much simpler and there can be a very larger number
    of cores. The current GPUs contain thousands of CUDA cores. Performance in GPUs
    is obtain by having a very high degree of parallelism. Lots of threads are launched
    in parallel. For good performance there should be at least several times more
    than the number of CUDA cores. GPU threads are much lighter than the usual CPU
    threads and they have very little penalty for context switching. This way when
    some threads are performing some memory operations (reading or writing) others
    execute instructions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，GPU 中有相对较少的晶体管用于控制和缓存，而有更大比例的晶体管用于数学运算。由于 GPU 内核的设计仅针对 3D 图形，因此它们可以设计得非常简单，并且可以有非常多的核心。当前的
    GPU 包含数千个 CUDA 核心。GPU 的性能是通过非常高的并行度获得的。并行启动了大量的线程。为了获得良好的性能，至少应该比 CUDA 核心的数量多几倍。GPU
    线程比常规 CPU 线程轻得多，并且它们在上下文切换上几乎没有惩罚。这样，当一些线程执行某些内存操作（读取或写入）时，其他线程执行指令。
- en: CUDA Threads, Warps, Blocks
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUDA 线程、战群、块
- en: In order to understand the GPU execution model let’s look at the so called axpy
    operation. On a single CPU core this operation would be executed in a serial manner
    in a for/do loop going over each element on the array, id, and computing y[id]=y[id]+a*x[id].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解 GPU 执行模型，让我们看看所谓的 axpy 操作。在单个 CPU 内核上，这个操作将以串行方式在一个 for/do 循环中执行，遍历数组中的每个元素
    id，并计算 y[id]=y[id]+a*x[id]。
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In order to perform the some operation on a GPU the program launches a function
    called *kernel*, which is executed simultaneously by tens of thousands of threads
    that can be run on GPU cores parallelly.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 GPU 上执行某些操作，程序会启动一个名为 *kernel* 的函数，该函数由成千上万的线程同时执行，这些线程可以并行地运行在 GPU 内核上。
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The programmers control how many instances of ker_axpy_ are created and they
    have to make sure that all the elements are processed and also that no out of
    bounds accessed are happening.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员控制 ker_axpy_ 实例的创建数量，并必须确保所有元素都得到处理，并且没有越界访问发生。
- en: GPU threads are much lighter than the usual CPU threads and they have very little
    penalty for context switching. By “over-subscribing” the GPU there are threads
    that are performing some memory operations (reading or writing), while others
    execute instructions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: GPU线程比常规CPU线程轻得多，它们在上下文切换方面几乎没有惩罚。通过“过度订阅”GPU，有一些线程正在执行一些内存操作（读取或写入），而其他线程则执行指令。
- en: '[![../_images/THREAD_CORE.png](../Images/5d5381fb2a0059b880639b60b35a9e5e.png)](../_images/THREAD_CORE.png)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/THREAD_CORE.png](../Images/5d5381fb2a0059b880639b60b35a9e5e.png)'
- en: Every thread is associated with a particular intrinsic index which can be used
    to calculate and access memory locations in an array. Each thread has its context
    and set of private variables. All threads have access to the global GPU memory,
    but there is no general way to synchronize when executing a kernel. If some threads
    need data from the global memory which was modified by other threads the code
    would have to be split in several kernels because only at the completion of a
    kernel it is ensured that the writing to the global memory was completed.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程都与一个特定的内在索引相关联，可以用来计算和访问数组中的内存位置。每个线程都有自己的上下文和私有变量集。所有线程都可以访问全局GPU内存，但在执行内核时没有一般的方法来同步。如果一些线程需要从全局内存中获取其他线程修改过的数据，代码就必须分成几个内核，因为只有在内核完成时才能确保全局内存的写入已完成。
- en: Apart from being much light weighted there are more differences between GPU
    threads and CPU threads. GPU threads are grouped together in groups called warps.
    This done at hardware level.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了重量较轻之外，GPU线程和CPU线程之间还有更多差异。GPU线程被分组在一起，称为warp。这是在硬件级别完成的。
- en: '[![../_images/WARP_SMTU.png](../Images/3bce412cee380c39c9c2d4125dbd029f.png)](../_images/WARP_SMTU.png)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/WARP_SMTU.png](../Images/3bce412cee380c39c9c2d4125dbd029f.png)'
- en: All memory accesses to the GPU memory are as a group in blocks of specific sizes
    (32B, 64B, 128B etc.). To obtain good performance the CUDA threads in the same
    warp need to access elements of the data which are adjacent in the memory. This
    is called *coalesced* memory access.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所有对GPU内存的内存访问都是以特定大小的块（32B、64B、128B等）为组进行的。为了获得良好的性能，同一warp中的CUDA线程需要访问内存中相邻的数据元素。这被称为*归约*内存访问。
- en: On some architectures, all members of a warp have to execute the same instruction,
    the so-called “lock-step” execution. This is done to achieve higher performance,
    but there are some drawbacks. If an **if** statement is present inside a warp
    will cause the warp to be executed more than once, one time for each branch. When
    different threads within a single warp take different execution paths based on
    a conditional statement (if), both branches are executed sequentially, with some
    threads being active while others are inactive. On architectures without lock-step
    execution, such as NVIDIA Volta / Turing (e.g., GeForce 16xx-series) or newer,
    warp divergence is less costly.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些架构中，warp中的所有成员必须执行相同的指令，这被称为“锁步”执行。这样做是为了实现更高的性能，但也有一些缺点。如果一个**if**语句在warp内部存在，会导致warp执行多次，每次分支执行一次。当单个warp内的不同线程根据条件语句（if）采取不同的执行路径时，两个分支会顺序执行，一些线程处于活动状态，而其他线程处于非活动状态。在无锁步执行架构上，例如NVIDIA
    Volta / Turing（例如，GeForce 16xx系列）或更新的版本，warp发散的成本较低。
- en: There is another level in the GPU threads hierarchy. The threads are grouped
    together in so called blocks. Each block is assigned to one Streaming Multiprocessor
    (SMP) unit. A SMP contains one or more SIMT (single instruction multiple threads)
    units, schedulers, and very fast on-chip memory. Some of this on-chip memory can
    be used in the programs, this is called shared memory. The shared memory can be
    used to “cache” data that is used by more than one thread, thus avoiding multiple
    reads from the global memory. It can also be used to avoid memory accesses which
    are not efficient. For example in a matrix transpose operation, we have two memory
    operations per element and only can be coalesced. In the first step a tile of
    the matrix is saved read a coalesced manner in the shared memory. After all the
    reads of the block are done the tile can be locally transposed (which is very
    fast) and then written to the destination matrix in a coalesced manner as well.
    Shared memory can also be used to perform block-level reductions and similar collective
    operations. All threads can be synchronized at block level. Furthermore when the
    shared memory is written in order to ensure that all threads have completed the
    operation the synchronization is compulsory to ensure correctness of the program.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU线程层次结构中还有另一个级别。线程被分组在一起，称为块。每个块被分配给一个流多处理器（SMP）单元。一个SMP包含一个或多个SIMT（单指令多线程）单元、调度器和非常快速的片上内存。其中一些片上内存可以在程序中使用，这被称为共享内存。共享内存可以用来“缓存”多个线程使用的内存，从而避免从全局内存进行多次读取。它也可以用来避免低效的内存访问。例如，在矩阵转置操作中，我们每个元素有两个内存操作，并且只能合并。在第一步中，矩阵的块以合并的方式保存在共享内存中。在完成块的读取后，块可以局部转置（这非常快），然后以合并的方式写入目标矩阵。共享内存还可以用来执行块级别的归约和类似的集体操作。所有线程都可以在块级别进行同步。此外，当共享内存被写入以确保所有线程都完成了操作时，同步是强制性的，以确保程序的正确性。
- en: '[![../_images/BLOCK_SMP.png](../Images/ef0f46b8c50be0dbedbce258b377a92e.png)](../_images/BLOCK_SMP.png)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '![BLOCK_SMP.png](../Images/ef0f46b8c50be0dbedbce258b377a92e.png)'
- en: Finally, a block of threads can not be split among SMPs. For performance blocks
    should have more than one warp. The more warps are active on an SMP the better
    is hidden the latency associated with the memory operations. If the resources
    are sufficient, due to fast context switching, an SMP can have more than one block
    active in the same time. However these blocks can not share data with each other
    via the on-chip memory.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，线程块不能在SMP之间分割。为了提高性能，块应该有多个warp。在SMP上活跃的warp越多，与内存操作相关的延迟隐藏得越好。如果资源足够，由于快速的上下文切换，SMP可以在同一时间激活多个块。然而，这些块不能通过片上内存相互共享数据。
- en: To summarize this section. In order to take advantage of GPUs the algorithms
    must allow the division of work in many small subtasks which can be executed in
    the same time. The computations are offloaded to GPUs, by launching tens of thousands
    of threads all executing the same function, *kernel*, each thread working on different
    part of the problem. The threads are executed in groups called *blocks*, each
    block being assigned to a SMP. Furthermore the threads of a block are divided
    in *warps*, each executed by SIMT unit. All threads in a warp execute the same
    instructions and all memory accesses are done collectively at warp level. The
    threads can synchronize and share data only at block level. Depending on the architecture,
    some data sharing can be done as well at warp level.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本节内容。为了利用GPU，算法必须允许将工作划分为许多小任务，这些任务可以同时执行。通过启动成千上万的线程，所有线程执行相同的函数，即内核，每个线程处理问题的不同部分，将计算卸载到GPU上。线程以称为块的形式执行，每个块被分配给一个SMP。此外，块中的线程被分为*warps*，每个由SIMT单元执行。Warp中的所有线程执行相同的指令，所有内存访问都是在warp级别集体进行的。线程只能在块级别同步和共享数据。根据架构，一些数据共享也可以在warp级别进行。
- en: In order to hide latencies it is recommended to “over-subscribe” the GPU. There
    should be many more blocks than SMPs present on the device. Also in order to ensure
    a good occupancy of the CUDA cores there should be more warps active on a given
    SMP than SIMT units. This way while some warps of threads are idle waiting for
    some memory operations to complete, others use the CUDA cores, thus ensuring a
    high occupancy of the GPU.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了隐藏延迟，建议对GPU进行“过度订阅”。设备上应该有比现有的SMPs更多的块。此外，为了确保CUDA核心的良好占用率，给定SMP上应该有比SIMT单元更多的活动warp。这样，当一些warp的线程空闲等待某些内存操作完成时，其他线程则使用CUDA核心，从而确保GPU的高占用率。
- en: In addition to this there are some architecture-specific features of which the
    developers can take advantage. Warp-level operations are primitives provided by
    the GPU architecture to allow for efficient communication and synchronization
    within a warp. They allow threads within a warp to exchange data efficiently,
    without the need for explicit synchronization. These warp-level operations, combined
    with the organization of threads into blocks and clusters, make it possible to
    implement complex algorithms and achieve high performance on the GPU. The cooperative
    groups feature introduced in recent versions of CUDA provides even finer-grained
    control over thread execution, allowing for even more efficient processing by
    giving more flexibility to the thread hierarchy. Cooperative groups allow threads
    within a block to organize themselves into smaller groups, called cooperative
    groups, and to synchronize their execution and share data within the group.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些架构特定的功能，开发者可以利用这些功能。Warp级别的操作是GPU架构提供的原语，用于在warp内部实现高效的通信和同步。它们允许warp内的线程高效地交换数据，无需显式同步。这些warp级别的操作，结合线程组织成块和集群，使得在GPU上实现复杂算法和达到高性能成为可能。CUDA最近版本中引入的协作组功能提供了对线程执行的更细粒度控制，通过赋予线程层次结构更多灵活性，从而实现更高效的处理。协作组允许块内的线程组织成更小的组，称为协作组，并在组内同步执行和共享数据。
- en: Below there is an example of how the threads in a grid can be associated with
    specific elements of an array
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，说明网格中的线程如何与数组的特定元素关联。
- en: '[![../_images/Indexing.png](../Images/fccb3e8f6b92e4b53a0805282127878f.png)](../_images/Indexing.png)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![../_images/Indexing.png](../Images/fccb3e8f6b92e4b53a0805282127878f.png)](../_images/Indexing.png)'
- en: The thread marked by orange color is part of a grid of threads size 4096\. The
    threads are grouped in blocks of size 256\. The “orange” thread has index 3 in
    the block 2 and the global calculated index 515.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 被橙色标记的线程是大小为4096的线程网格的一部分。线程被分组成大小为256的块。该“橙色”线程在块2中的索引为3，全局计算索引为515。
- en: For a vector addition example this would be used as follow `c[index]=a[index]+b[index]`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于向量加法示例，这将按照以下方式使用：`c[index]=a[index]+b[index]`。
- en: In short
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之
- en: GPUs have a different execution model compared to CPUs, with a focus on parallelism
    and mathematical operations.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与CPU相比，GPU具有不同的执行模型，侧重于并行性和数学运算。
- en: GPUs consist of thousands of lightweight threads that can be executed simultaneously
    on GPU cores.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU由成千上万的轻量级线程组成，这些线程可以在GPU核心上同时执行。
- en: Threads are organized into warps, and warps are grouped into blocks assigned
    to streaming multiprocessors (SMPs).
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程被组织成warp，warp被分组成块，分配给流多处理器（SMPs）。
- en: GPUs achieve performance through high degrees of parallelism and efficient memory
    access.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU通过高程度的并行性和高效的内存访问来实现性能。
- en: Shared memory can be used to cache data and improve memory access efficiency
    within a block.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存可以用来缓存数据并提高块内的内存访问效率。
- en: Synchronization and data sharing are limited to the block level, with some possible
    sharing at the warp level depending on the architecture.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步和数据共享仅限于块级别，根据架构，在warp级别上可能有一些共享。
- en: Over-subscribing the GPU and maximizing warp and block occupancy help hide latencies
    and improve performance.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对GPU进行过度订阅和最大化warp和块占用率有助于隐藏延迟并提高性能。
- en: Warp-level operations and cooperative groups provide efficient communication
    and synchronization within a warp or block.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Warp级别的操作和协作组提供了在warp或块内部的高效通信和同步。
- en: Thread indexing allows associating threads with specific elements in an array
    for parallel processing.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程索引允许将线程与数组中的特定元素关联起来，以实现并行处理。
- en: Terminology
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语
- en: 'At the moment there are three major GPU producers: NVIDIA, Intel, and AMD.
    While the basic concept behind GPUs is pretty similar they use different names
    for the various parts. Furthermore there are software environments for GPU programming,
    some from the producers and some from external groups all having different naming
    as well. Below there is a short compilation of the some terms used across different
    platforms and software environments.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有三个主要的GPU生产商：NVIDIA、Intel和AMD。虽然GPU背后的基本概念相当相似，但它们为不同的部分使用不同的名称。此外，还有用于GPU编程的软件环境，有些来自生产商，有些来自外部团体，所有这些都有不同的命名。以下是对不同平台和软件环境中使用的某些术语的简要汇编。
- en: Software mapping naming
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 软件映射命名
- en: '| CUDA | HIP | OpenCL | SYCL |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | HIP | OpenCL | SYCL |'
- en: '| --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| grid of threads | NDRange |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 线程网格 | NDRange |'
- en: '| block | work-group |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 工作组 |'
- en: '| warp | wavefront | sub-group |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| warp | 波前 | 子组 |'
- en: '| thread | work-item |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 线程 | 工作项 |'
- en: '| registers | private memory |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 寄存器 | 私有内存 |'
- en: '| shared memory | local data share | local memory |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 共享内存 | 本地数据共享 | 本地内存 |'
- en: '| threadIdx.{x,y,z} | get_local_id({0,1,2}) | nd_item::get_local({2,1,0}) [[1]](#syclindex)
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 线程索引.{x,y,z} | 获取本地ID({0,1,2}) | nd_item::get_local({2,1,0}) [[1]](#syclindex)
    |'
- en: '| blockIdx.{x,y,z} | get_group_id({0,1,2}) | nd_item::get_group({2,1,0}) [[1]](#syclindex)
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 块索引.{x,y,z} | 获取组ID({0,1,2}) | nd_item::get_group({2,1,0}) [[1]](#syclindex)
    |'
- en: '| blockDim.{x,y,z} | get_local_size({0,1,2}) | nd_item::get_local_range({2,1,0})
    [[1]](#syclindex) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| blockDim.{x,y,z} | 获取本地大小({0,1,2}) | nd_item::get_local_range({2,1,0}) [[1]](#syclindex)
    |'
- en: Exercises
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: What are threads in the context of shared memory architectures?
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存架构的上下文中，什么是线程？
- en: Independent execution units with their own memory address spaces
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有自己内存地址空间的独立执行单元
- en: Light execution units with shared memory address spaces
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有共享内存地址空间的轻量级执行单元
- en: Communication devices between separate memory units
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 介于不同内存单元之间的通信设备
- en: Programming models for distributed memory machines
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分布式内存机器的编程模型
- en: Solution
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *b) Light execution units with shared memory address spaces*'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*b) 拥有共享内存地址空间的轻量级执行单元*
- en: What is data parallelism?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是数据并行？
- en: Distributing data across computational units that run in parallel, applying
    the same or similar operations to different data elements.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在并行运行的计算单元之间分配数据，对不同的数据元素应用相同或类似的操作。
- en: Distributing tasks across computational units that run in parallel, applying
    different operations to the same data elements.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在并行运行的计算单元之间分配任务，对相同的数据元素应用不同的操作。
- en: Distributing data across computational units that run sequentially, applying
    the same operation to all data elements.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在按顺序运行的计算单元之间分配数据，对所有数据元素应用相同的操作。
- en: Distributing tasks across computational units that run sequentially, applying
    different operations to different data elements.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在按顺序运行的计算单元之间分配任务，对不同的数据元素应用不同的操作。
- en: Solution
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *a) Distributing data across computational units that run in
    parallel, applying the same or similar operations to different data elements.*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*a) 在并行运行的计算单元之间分配数据，对不同的数据元素应用相同或类似的操作。*
- en: What type of parallelism is natural for GPU?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: GPU上自然适合哪种类型的并行？
- en: Task Parallelism
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务并行
- en: Data Parallelism
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据并行
- en: Both data and task parallelism
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据和任务并行
- en: Neither data nor task parallelism
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既不是数据并行也不是任务并行
- en: Solution
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *b) Data Parallelism*'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*b) 数据并行*
- en: What is a kernel in the context of GPU execution?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU执行上下文中，什么是内核？
- en: A specific section of the CPU used for memory operations.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CPU上用于内存操作的具体部分。
- en: A specific section of the GPU used for memory operations.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPU上用于内存操作的具体部分。
- en: A type of thread that operates on the GPU.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种在GPU上操作的类型。
- en: A function that is executed simultaneously by tens of thousands of threads on
    GPU cores.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个在GPU核心上由成千上万的线程同时执行的功能。
- en: Solution
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *d) A function that is executed simultaneously by tens of thousands
    of threads on GPU cores.*'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*d) 在GPU核心上由成千上万的线程同时执行的功能。*
- en: What is coalesced memory access?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是归一化内存访问？
- en: It’s when CUDA threads in the same warp access elements of the data which are
    adjacent in the memory.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是指CUDA线程在同一个warp中访问内存中相邻的数据元素。
- en: It’s when CUDA threads in different warps access elements of the data which
    are far in the memory.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是指不同warp中的CUDA线程访问内存中相距很远的数据元素。
- en: It’s when all threads have access to the global GPU memory.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是指所有线程都可以访问全局GPU内存。
- en: It’s when threads in a warp perform different operations.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Correct answer: *a) It’s when CUDA threads in the same warp access elements
    of the data which are adjacent in the memory.*'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: What is the function of shared memory in the context of GPU execution?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: It’s used to store global memory.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s used to store all the threads in a block.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It can be used to “cache” data that is used by more than one thread, avoiding
    multiple reads from the global memory.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It’s used to store all the CUDA cores.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Correct answer: *c) It can be used to “cache” data that is used by more than
    one thread, avoiding multiple reads from the global memory.*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: What is the significance of over-subscribing the GPU?
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: It reduces the overall performance of the GPU.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It ensures that there are more blocks than SMPs present on the device, helping
    to hide latencies and ensure high occupancy of the GPU.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It leads to a memory overflow in the GPU.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It ensures that there are more SMPs than blocks present on the device.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solution
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Correct answer: *b) It ensures that there are more blocks than SMPs present
    on the device, helping to hide latencies and ensure high occupancy of the GPU.*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Keypoints
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Parallel computing can be classified into distributed-memory and shared-memory
    architectures
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two types of parallelism that can be explored are data parallelism and task
    parallelism.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs are a type of shared memory architecture suitable for data parallelism.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPUs have high parallelism, with threads organized into warps and blocks and.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GPU optimization involves coalesced memory access, shared memory usage, and
    high thread and warp occupancy. Additionally, architecture-specific features such
    as warp-level operations and cooperative groups can be leveraged for more efficient
    processing. [Previous](../3-gpu-problems/ "What problems fit to GPU?") [Next](../5-intro-to-gpu-prog-models/
    "Introduction to GPU programming models")
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: © Copyright 2023-2024, The contributors.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Built with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme)
    provided by [Read the Docs](https://readthedocs.org). Questions
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: What types of parallel computing is possible?
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does data parallelism differ from task parallelism, and how are they utilized
    in parallel computing?
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How is the work parallelized and executed on GPUs?
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are general considerations for an efficient code running on GPUs?
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Understand parallel computing principles and architectures.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiate data parallelism from task parallelism.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn the GPU execution model.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelize and execute work on GPUs.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop efficient GPU code for high performance.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 25 min teaching
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 min exercises
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of parallelism
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Distributed- vs. Shared-Memory Architecture
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of computing problems are not trivially parallelizable, which means that
    the subtasks need to have access from time to time to some of the results computed
    by other subtasks. The way subtasks exchange needed information depends on the
    available hardware.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数计算问题都不是简单可并行的，这意味着子任务需要不时地访问其他子任务计算的一些结果。子任务交换所需信息的方式取决于可用的硬件。
- en: '![../_images/distributed_vs_shared.png](../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/distributed_vs_shared.png](../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png)'
- en: Distributed- vs shared-memory parallel computing.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式内存与共享内存并行计算。
- en: In a distributed memory environment each processing unit operates independently
    from the others. It has its own memory and it **cannot** access the memory in
    other nodes. The communication is done via network and each computing unit runs
    a separate copy of the operating system. In a shared memory machine all processing
    units have access to the memory and can read or modify the variables within.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式内存环境中，每个处理单元独立于其他处理单元运行。它有自己的内存，**不能**访问其他节点的内存。通信通过网络进行，每个计算单元运行操作系统的单独副本。在共享内存机器中，所有处理单元都可以访问内存，并可以读取或修改变量。
- en: Processes and Threads
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进程和线程
- en: The type of environment (distributed- or shared-memory) determines the programming
    model. There are two types of parallelism possible, process based and thread based.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 环境类型（分布式或共享内存）决定了编程模型。可能的并行性有两种，基于进程和基于线程。
- en: '![../_images/processes-threads.png](../Images/94b62a3205925cbbdac5058209021754.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/processes-threads.png](../Images/94b62a3205925cbbdac5058209021754.png)'
- en: For distributed memory machines, a process-based parallel programming model
    is employed. The processes are independent execution units which have their *own
    memory* address spaces. They are created when the parallel program is started
    and they are only terminated at the end. The communication between them is done
    explicitly via message passing like MPI.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分布式内存机器，采用基于进程的并行编程模型。进程是独立的执行单元，它们有自己的内存地址空间。它们在并行程序启动时创建，并且在结束时才终止。它们之间的通信通过显式的消息传递（如MPI）进行。
- en: On the shared memory architectures it is possible to use a thread based parallelism.
    The threads are light execution units and can be created and destroyed at a relatively
    small cost. The threads have their own state information, but they *share* the
    *same memory* address space. When needed the communication is done though the
    shared memory.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存架构上，可以使用基于线程的并行性。线程是轻量级的执行单元，可以以相对较小的成本创建和销毁。线程有自己的状态信息，但它们**共享**相同的内存地址空间。需要时，通过共享内存进行通信。
- en: Both approaches have their advantages and disadvantages. Distributed machines
    are relatively cheap to build and they have an “infinite “ capacity. In principle
    one could add more and more computing units. In practice the more computing units
    are used the more time consuming is the communication. The shared memory systems
    can achieve good performance and the programming model is quite simple. However
    they are limited by the memory capacity and by the access speed. In addition in
    the shared parallel model it is much easier to create race conditions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都有其优缺点。分布式机器相对便宜，并且具有“无限”的容量。原则上可以添加越来越多的计算单元。实际上，使用的计算单元越多，通信就越耗时。共享内存系统可以实现良好的性能，编程模型相当简单。然而，它们受限于内存容量和访问速度。此外，在共享并行模型中，创建竞态条件更容易。
- en: Exposing parallelism
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 揭示并行性
- en: There are two types of parallelism that can be explored. The data parallelism
    is when the data can be distributed across computational units that can run in
    parallel. The units process the data by applying the same or very similar operation
    to different data elements. A common example is applying a blur filter to an image
    — the same function is applied to all the pixels on an image. This parallelism
    is natural for the GPU, where the same instruction set is executed in multiple
    threads.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 可以探索两种并行性。数据并行性是指数据可以分布在可以并行运行的计算单元上。这些单元通过将相同的或非常相似的操作应用于不同的数据元素来处理数据。一个常见的例子是对图像应用模糊滤镜——相同的函数应用于图像上的所有像素。这种并行性对于GPU来说很自然，因为在多个线程中执行相同的指令集。
- en: '[![../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png](../Images/df3318bcb375513cb4f8169dcb4efdbc.png)](../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png](../Images/df3318bcb375513cb4f8169dcb4efdbc.png)'
- en: Data parallelism and task parallelism. The data parallelism is when the same
    operation applies to multiple data (e.g. multiple elements of an array are transformed).
    The task parallelism implies that there are more than one independent task that,
    in principle, can be executed in parallel.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行和任务并行。数据并行是指相同的操作应用于多个数据（例如，数组的多个元素被转换）。任务并行意味着存在多个独立的任务，原则上可以并行执行。
- en: Data parallelism can usually be explored by the GPUs quite easily. The most
    basic approach would be finding a loop over many data elements and converting
    it into a GPU kernel. If the number of elements in the data set is fairly large
    (tens or hundred of thousands elements), the GPU should perform quite well. Although
    it would be odd to expect absolute maximum performance from such a naive approach,
    it is often the one to take. Getting absolute maximum out of the data parallelism
    requires good understanding of how GPU works.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行通常可以很容易地由 GPU 探索。最基本的方法是找到一个覆盖许多数据元素的循环，并将其转换为 GPU 内核。如果数据集中的元素数量相当大（数万或数十万个元素），GPU
    应该表现相当出色。尽管从这种天真方法中期望绝对最大性能可能有些奇怪，但它通常是首选的方法。从数据并行中获得绝对最大性能需要很好地理解 GPU 的工作原理。
- en: 'Another type of parallelism is a task parallelism. This is when an application
    consists of more than one task that requiring to perform different operations
    with (the same or) different data. An example of task parallelism is cooking:
    slicing vegetables and grilling are very different tasks and can be done at the
    same time. Note that the tasks can consume totally different resources, which
    also can be explored.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种并行类型是任务并行。这指的是一个应用程序由多个任务组成，这些任务需要执行不同的操作，并且可能使用（相同或）不同的数据。任务并行的例子是烹饪：切菜和烤肉是非常不同的任务，可以同时进行。请注意，任务可以消耗完全不同的资源，这也值得探索。
- en: In short
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之
- en: Computing problems can be parallelized in distributed memory or shared memory
    architectures.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算问题可以在分布式内存或共享内存架构中并行化。
- en: In distributed memory, each unit operates independently, with no direct memory
    access between nodes.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式内存中，每个单元独立操作，节点之间没有直接的内存访问。
- en: In shared memory, units have access to the same memory and can communicate through
    shared variables.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在共享内存中，单元可以访问相同的内存，并通过共享变量进行通信。
- en: Parallel programming can be process-based (distributed memory) or thread-based
    (shared memory).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行编程可以是基于进程的（分布式内存）或基于线程的（共享内存）。
- en: Process-based parallelism uses independent processes with separate memory spaces
    and explicit message passing.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于进程的并行使用具有独立内存空间和显式消息传递的独立进程。
- en: Thread-based parallelism uses lightweight threads that share the same memory
    space and communicate through shared memory.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于线程的并行使用共享相同内存空间并通过共享内存进行通信的轻量级线程。
- en: Data parallelism distributes data across computational units, processing them
    with the same or similar operations.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据并行将数据分布到计算单元，使用相同或类似的操作进行处理。
- en: Task parallelism involves multiple independent tasks that perform different
    operations on the same or different data.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务并行涉及多个独立任务，这些任务在相同或不同的数据上执行不同的操作。
- en: Task parallelism involves executing different tasks concurrently, leveraging
    different resources.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务并行涉及并发执行不同的任务，利用不同的资源。
- en: GPU Execution Model
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU 执行模型
- en: In order to obtain maximum performance it is important to understand how GPUs
    execute the programs. As mentioned before a CPU is a flexible device oriented
    towards general purpose usage. It’s fast and versatile, designed to run operating
    systems and various, very different types of applications. It has lots of features,
    such as better control logic, caches and cache coherence, that are not related
    to pure computing. CPUs optimize the execution by trying to achieve low latency
    via heavy caching and branch prediction.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最大性能，了解 GPU 如何执行程序非常重要。如前所述，CPU 是一种灵活的通用设备，面向通用用途。它速度快，功能多样，旨在运行操作系统和多种非常不同的应用程序。它具有许多功能，例如更好的控制逻辑、缓存和缓存一致性，这些功能与纯计算无关。CPU
    通过大量缓存和分支预测来尝试降低延迟，从而优化执行。
- en: '[![../_images/cpu-gpu-highway.png](../Images/2be579a867209f9ee644b6264269be37.png)](../_images/cpu-gpu-highway.png)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/cpu-gpu-highway.png](../Images/2be579a867209f9ee644b6264269be37.png)'
- en: Cars and roads analogy for the CPU and GPU behavior. The compact road is analogous
    to the CPU (low latency, low throughput) and the broader road is analogous to
    the GPU (high latency, high throughput).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: CPU和GPU行为的汽车和道路类比。紧凑的道路类似于CPU（低延迟，低吞吐量），而宽阔的道路类似于GPU（高延迟，高吞吐量）。
- en: In contrast the GPUs contain a relatively small amount of transistors dedicated
    to control and caching, and a much larger fraction of transistors dedicated to
    the mathematical operations. Since the cores in a GPU are designed just for 3D
    graphics, they can be made much simpler and there can be a very larger number
    of cores. The current GPUs contain thousands of CUDA cores. Performance in GPUs
    is obtain by having a very high degree of parallelism. Lots of threads are launched
    in parallel. For good performance there should be at least several times more
    than the number of CUDA cores. GPU threads are much lighter than the usual CPU
    threads and they have very little penalty for context switching. This way when
    some threads are performing some memory operations (reading or writing) others
    execute instructions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，GPU包含相对较少的晶体管用于控制和缓存，以及大量晶体管用于数学运算。由于GPU的核心仅设计用于3D图形，它们可以设计得非常简单，并且可以有非常多的核心。当前的GPU包含数千个CUDA核心。GPU的性能是通过非常高的并行度获得的。并行启动了大量的线程。为了获得良好的性能，至少应该是CUDA核心数量的几倍。GPU线程比常规CPU线程轻得多，它们在上下文切换上几乎没有惩罚。这样，当一些线程执行一些内存操作（读取或写入）时，其他线程则执行指令。
- en: CUDA Threads, Warps, Blocks
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUDA线程、Warp、Block
- en: In order to understand the GPU execution model let’s look at the so called axpy
    operation. On a single CPU core this operation would be executed in a serial manner
    in a for/do loop going over each element on the array, id, and computing y[id]=y[id]+a*x[id].
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解GPU执行模型，让我们看看所谓的axpy操作。在单个CPU核心上，这个操作将以串行方式在一个for/do循环中执行，遍历数组中的每个元素id，并计算y[id]=y[id]+a*x[id]。
- en: '[PRE2]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In order to perform the some operation on a GPU the program launches a function
    called *kernel*, which is executed simultaneously by tens of thousands of threads
    that can be run on GPU cores parallelly.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在GPU上执行某些操作，程序会启动一个名为*kern*的函数，这个函数由成千上万的线程同时执行，这些线程可以并行地运行在GPU核心上。
- en: '[PRE3]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The programmers control how many instances of ker_axpy_ are created and they
    have to make sure that all the elements are processed and also that no out of
    bounds accessed are happening.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员控制ker_axpy_实例的创建数量，并必须确保所有元素都被处理，并且没有越界访问发生。
- en: GPU threads are much lighter than the usual CPU threads and they have very little
    penalty for context switching. By “over-subscribing” the GPU there are threads
    that are performing some memory operations (reading or writing), while others
    execute instructions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: GPU线程比常规CPU线程轻得多，它们在上下文切换上几乎没有惩罚。通过“过度订阅”GPU，有一些线程正在执行一些内存操作（读取或写入），而其他线程则执行指令。
- en: '[![../_images/THREAD_CORE.png](../Images/5d5381fb2a0059b880639b60b35a9e5e.png)](../_images/THREAD_CORE.png)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/THREAD_CORE.png](../Images/5d5381fb2a0059b880639b60b35a9e5e.png)'
- en: Every thread is associated with a particular intrinsic index which can be used
    to calculate and access memory locations in an array. Each thread has its context
    and set of private variables. All threads have access to the global GPU memory,
    but there is no general way to synchronize when executing a kernel. If some threads
    need data from the global memory which was modified by other threads the code
    would have to be split in several kernels because only at the completion of a
    kernel it is ensured that the writing to the global memory was completed.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程都与一个特定的内在索引相关联，可以用来计算和访问数组中的内存位置。每个线程都有自己的上下文和私有变量集。所有线程都可以访问全局GPU内存，但在执行内核时没有一般的方法来同步。如果某些线程需要从全局内存中获取其他线程修改过的数据，代码就必须分成几个内核，因为只有在内核完成时才能确保全局内存的写入已完成。
- en: Apart from being much light weighted there are more differences between GPU
    threads and CPU threads. GPU threads are grouped together in groups called warps.
    This done at hardware level.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 除了重量轻之外，GPU线程和CPU线程之间还有更多差异。GPU线程被分组在一起，称为warp。这是在硬件级别完成的。
- en: '[![../_images/WARP_SMTU.png](../Images/3bce412cee380c39c9c2d4125dbd029f.png)](../_images/WARP_SMTU.png)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/WARP_SMTU.png](../Images/3bce412cee380c39c9c2d4125dbd029f.png)'
- en: All memory accesses to the GPU memory are as a group in blocks of specific sizes
    (32B, 64B, 128B etc.). To obtain good performance the CUDA threads in the same
    warp need to access elements of the data which are adjacent in the memory. This
    is called *coalesced* memory access.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 所有对GPU内存的内存访问都是以特定大小的块（32B、64B、128B等）为组进行的。为了获得良好的性能，同一warp中的CUDA线程需要访问内存中相邻的数据元素。这被称为*合并*内存访问。
- en: On some architectures, all members of a warp have to execute the same instruction,
    the so-called “lock-step” execution. This is done to achieve higher performance,
    but there are some drawbacks. If an **if** statement is present inside a warp
    will cause the warp to be executed more than once, one time for each branch. When
    different threads within a single warp take different execution paths based on
    a conditional statement (if), both branches are executed sequentially, with some
    threads being active while others are inactive. On architectures without lock-step
    execution, such as NVIDIA Volta / Turing (e.g., GeForce 16xx-series) or newer,
    warp divergence is less costly.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些架构中，warp中的所有成员必须执行相同的指令，即所谓的“锁步”执行。这是为了实现更高的性能，但也有一些缺点。如果一个**if**语句在warp内部，会导致warp被执行多次，每次分支执行一次。当单个warp内的不同线程根据条件语句（if）采取不同的执行路径时，两个分支会顺序执行，一些线程处于活动状态，而其他线程处于非活动状态。在无锁步执行架构中，如NVIDIA
    Volta / Turing（例如，GeForce 16xx系列）或更新的架构中，warp发散的成本较低。
- en: There is another level in the GPU threads hierarchy. The threads are grouped
    together in so called blocks. Each block is assigned to one Streaming Multiprocessor
    (SMP) unit. A SMP contains one or more SIMT (single instruction multiple threads)
    units, schedulers, and very fast on-chip memory. Some of this on-chip memory can
    be used in the programs, this is called shared memory. The shared memory can be
    used to “cache” data that is used by more than one thread, thus avoiding multiple
    reads from the global memory. It can also be used to avoid memory accesses which
    are not efficient. For example in a matrix transpose operation, we have two memory
    operations per element and only can be coalesced. In the first step a tile of
    the matrix is saved read a coalesced manner in the shared memory. After all the
    reads of the block are done the tile can be locally transposed (which is very
    fast) and then written to the destination matrix in a coalesced manner as well.
    Shared memory can also be used to perform block-level reductions and similar collective
    operations. All threads can be synchronized at block level. Furthermore when the
    shared memory is written in order to ensure that all threads have completed the
    operation the synchronization is compulsory to ensure correctness of the program.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU线程层次结构中还有另一个级别。线程被分组在一起，称为块。每个块被分配给一个流式多处理器（SMP）单元。一个SMP包含一个或多个SIMT（单指令多线程）单元、调度器和非常快速的片上内存。其中一些片上内存可以在程序中使用，这被称为共享内存。共享内存可以用来“缓存”多个线程使用的内存，从而避免从全局内存进行多次读取。它也可以用来避免低效的内存访问。例如，在矩阵转置操作中，每个元素有两个内存操作，并且只能合并。在第一步中，矩阵的块以合并的方式保存在共享内存中。在完成块的读取后，块可以局部转置（这非常快），然后以合并的方式写入目标矩阵。共享内存还可以用来执行块级别的归约和类似的集体操作。所有线程都可以在块级别进行同步。此外，当共享内存被写入以确保所有线程都完成了操作时，同步是强制性的，以确保程序的正确性。
- en: '[![../_images/BLOCK_SMP.png](../Images/ef0f46b8c50be0dbedbce258b377a92e.png)](../_images/BLOCK_SMP.png)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/BLOCK_SMP.png](../Images/ef0f46b8c50be0dbedbce258b377a92e.png)'
- en: Finally, a block of threads can not be split among SMPs. For performance blocks
    should have more than one warp. The more warps are active on an SMP the better
    is hidden the latency associated with the memory operations. If the resources
    are sufficient, due to fast context switching, an SMP can have more than one block
    active in the same time. However these blocks can not share data with each other
    via the on-chip memory.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，线程块不能在SMP之间分割。为了提高性能，块应该包含多个warp。在SMP上活跃的warp越多，与内存操作相关的延迟隐藏得越好。如果资源足够，由于快速的上下文切换，一个SMP可以在同一时间激活多个块。然而，这些块不能通过片上内存相互共享数据。
- en: To summarize this section. In order to take advantage of GPUs the algorithms
    must allow the division of work in many small subtasks which can be executed in
    the same time. The computations are offloaded to GPUs, by launching tens of thousands
    of threads all executing the same function, *kernel*, each thread working on different
    part of the problem. The threads are executed in groups called *blocks*, each
    block being assigned to a SMP. Furthermore the threads of a block are divided
    in *warps*, each executed by SIMT unit. All threads in a warp execute the same
    instructions and all memory accesses are done collectively at warp level. The
    threads can synchronize and share data only at block level. Depending on the architecture,
    some data sharing can be done as well at warp level.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本节内容。为了利用GPU，算法必须允许将工作划分为许多小的子任务，这些子任务可以同时执行。通过启动成千上万的线程，所有线程执行相同的函数，即内核，每个线程处理问题的不同部分，将计算卸载到GPU上。线程以称为块的形式执行，每个块被分配给一个SMP。此外，块中的线程被分为*warps*，每个由SIMT单元执行。所有warp中的线程执行相同的指令，所有内存访问都是在warp级别集体完成的。线程只能在块级别同步和共享数据。根据架构，一些数据共享也可以在warp级别进行。
- en: In order to hide latencies it is recommended to “over-subscribe” the GPU. There
    should be many more blocks than SMPs present on the device. Also in order to ensure
    a good occupancy of the CUDA cores there should be more warps active on a given
    SMP than SIMT units. This way while some warps of threads are idle waiting for
    some memory operations to complete, others use the CUDA cores, thus ensuring a
    high occupancy of the GPU.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了隐藏延迟，建议“过度订阅”GPU。设备上应该有比SMP更多的块。此外，为了确保CUDA核心的良好利用率，给定SMP上应该有比SIMT单元更多的活动warp。这样，当一些线程warp空闲等待某些内存操作完成时，其他warp可以使用CUDA核心，从而确保GPU的高利用率。
- en: In addition to this there are some architecture-specific features of which the
    developers can take advantage. Warp-level operations are primitives provided by
    the GPU architecture to allow for efficient communication and synchronization
    within a warp. They allow threads within a warp to exchange data efficiently,
    without the need for explicit synchronization. These warp-level operations, combined
    with the organization of threads into blocks and clusters, make it possible to
    implement complex algorithms and achieve high performance on the GPU. The cooperative
    groups feature introduced in recent versions of CUDA provides even finer-grained
    control over thread execution, allowing for even more efficient processing by
    giving more flexibility to the thread hierarchy. Cooperative groups allow threads
    within a block to organize themselves into smaller groups, called cooperative
    groups, and to synchronize their execution and share data within the group.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些特定于架构的功能，开发者可以利用。Warp级别的操作是GPU架构提供的原语，允许在warp内部进行高效的通信和同步。它们允许warp内的线程高效地交换数据，无需显式同步。这些warp级别的操作，结合线程的块和集群组织，使得在GPU上实现复杂算法并达到高性能成为可能。CUDA最近版本中引入的协作组功能提供了对线程执行的更细粒度控制，通过赋予线程层次结构更多灵活性，从而实现更高效的处理。协作组允许块内的线程组织成更小的组，称为协作组，并在组内同步执行和共享数据。
- en: Below there is an example of how the threads in a grid can be associated with
    specific elements of an array
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，说明网格中的线程如何与数组的特定元素相关联
- en: '[![../_images/Indexing.png](../Images/fccb3e8f6b92e4b53a0805282127878f.png)](../_images/Indexing.png)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/Indexing.png](../Images/fccb3e8f6b92e4b53a0805282127878f.png)'
- en: The thread marked by orange color is part of a grid of threads size 4096\. The
    threads are grouped in blocks of size 256\. The “orange” thread has index 3 in
    the block 2 and the global calculated index 515.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 被橙色标记的线程是大小为4096的线程网格的一部分。线程被分为大小为256的块。橙色线程在块2中的索引为3，全局计算索引为515。
- en: For a vector addition example this would be used as follow `c[index]=a[index]+b[index]`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于向量加法示例，可以使用以下方式`c[index]=a[index]+b[index]`。
- en: In short
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之
- en: GPUs have a different execution model compared to CPUs, with a focus on parallelism
    and mathematical operations.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与CPU相比，GPU具有不同的执行模型，侧重于并行性和数学运算。
- en: GPUs consist of thousands of lightweight threads that can be executed simultaneously
    on GPU cores.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU由成千上万的轻量级线程组成，这些线程可以在GPU核心上同时执行。
- en: Threads are organized into warps, and warps are grouped into blocks assigned
    to streaming multiprocessors (SMPs).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程被组织成warp，warp被分组成块，分配给流多处理器（SMP）。
- en: GPUs achieve performance through high degrees of parallelism and efficient memory
    access.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU通过高程度的并行性和高效的内存访问来实现性能。
- en: Shared memory can be used to cache data and improve memory access efficiency
    within a block.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存可以用来缓存数据并提高块内的内存访问效率。
- en: Synchronization and data sharing are limited to the block level, with some possible
    sharing at the warp level depending on the architecture.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步和数据共享仅限于块级别，根据架构，在某些情况下可能在warp级别进行共享。
- en: Over-subscribing the GPU and maximizing warp and block occupancy help hide latencies
    and improve performance.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度订阅GPU并最大化warp和块占用率有助于隐藏延迟并提高性能。
- en: Warp-level operations and cooperative groups provide efficient communication
    and synchronization within a warp or block.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: warp级别的操作和协作组提供了在warp或块内部的高效通信和同步。
- en: Thread indexing allows associating threads with specific elements in an array
    for parallel processing.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程索引允许将线程与数组中的特定元素关联起来，以实现并行处理。
- en: Terminology
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语
- en: 'At the moment there are three major GPU producers: NVIDIA, Intel, and AMD.
    While the basic concept behind GPUs is pretty similar they use different names
    for the various parts. Furthermore there are software environments for GPU programming,
    some from the producers and some from external groups all having different naming
    as well. Below there is a short compilation of the some terms used across different
    platforms and software environments.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有三个主要的GPU生产商：NVIDIA、Intel和AMD。虽然GPU背后的基本概念相当相似，但它们为各个部分使用不同的名称。此外，还有用于GPU编程的软件环境，有些来自生产商，有些来自外部团体，所有这些都有不同的命名。以下是对不同平台和软件环境中使用的某些术语的简要汇编。
- en: Software mapping naming
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 软件映射命名
- en: '| CUDA | HIP | OpenCL | SYCL |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | HIP | OpenCL | SYCL |'
- en: '| --- | --- | --- | --- |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| grid of threads | NDRange |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 线程网格 | NDRange |'
- en: '| block | work-group |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 工作组 |'
- en: '| warp | wavefront | sub-group |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| warp | 波前 | 子组 |'
- en: '| thread | work-item |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 线程 | 工作项 |'
- en: '| registers | private memory |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 寄存器 | 私有内存 |'
- en: '| shared memory | local data share | local memory |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 共享内存 | 本地数据共享 | 本地内存 |'
- en: '| threadIdx.{x,y,z} | get_local_id({0,1,2}) | nd_item::get_local({2,1,0}) [[1]](#syclindex)
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| threadIdx.{x,y,z} | get_local_id({0,1,2}) | nd_item::get_local({2,1,0}) [[1]](#syclindex)
    |'
- en: '| blockIdx.{x,y,z} | get_group_id({0,1,2}) | nd_item::get_group({2,1,0}) [[1]](#syclindex)
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| blockIdx.{x,y,z} | get_group_id({0,1,2}) | nd_item::get_group({2,1,0}) [[1]](#syclindex)
    |'
- en: '| blockDim.{x,y,z} | get_local_size({0,1,2}) | nd_item::get_local_range({2,1,0})
    [[1]](#syclindex) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| blockDim.{x,y,z} | get_local_size({0,1,2}) | nd_item::get_local_range({2,1,0})
    [[1]](#syclindex) |'
- en: Exercises
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: What are threads in the context of shared memory architectures?
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存架构的上下文中，线程是什么？
- en: Independent execution units with their own memory address spaces
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有自己内存地址空间的独立执行单元
- en: Light execution units with shared memory address spaces
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轻量级执行单元带有共享内存地址空间
- en: Communication devices between separate memory units
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分离内存单元之间的通信设备
- en: Programming models for distributed memory machines
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分布式内存机器的编程模型
- en: Solution
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *b) Light execution units with shared memory address spaces*'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*b) 带有共享内存地址空间的轻量级执行单元*
- en: What is data parallelism?
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是数据并行？
- en: Distributing data across computational units that run in parallel, applying
    the same or similar operations to different data elements.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在并行运行的计算单元之间分配数据，对不同的数据元素应用相同的或类似的操作。
- en: Distributing tasks across computational units that run in parallel, applying
    different operations to the same data elements.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在并行运行的计算单元之间分配任务，对相同的数据元素应用不同的操作。
- en: Distributing data across computational units that run sequentially, applying
    the same operation to all data elements.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在按顺序运行的计算单元之间分配数据，对所有数据元素应用相同的操作。
- en: Distributing tasks across computational units that run sequentially, applying
    different operations to different data elements.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在按顺序运行的计算单元之间分配任务，对不同的数据元素应用不同的操作。
- en: Solution
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *a) Distributing data across computational units that run in
    parallel, applying the same or similar operations to different data elements.*'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*a) 在并行运行的计算单元之间分配数据，对不同的数据元素应用相同的或类似的操作。*
- en: What type of parallelism is natural for GPU?
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPU来说，哪种类型的并行性是自然的？
- en: Task Parallelism
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务并行
- en: Data Parallelism
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据并行
- en: Both data and task parallelism
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据并行和任务并行
- en: Neither data nor task parallelism
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既不是数据并行也不是任务并行
- en: Solution
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *b) Data Parallelism*'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*b) 数据并行*
- en: What is a kernel in the context of GPU execution?
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU执行上下文中，内核是什么？
- en: A specific section of the CPU used for memory operations.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于内存操作的具体CPU部分。
- en: A specific section of the GPU used for memory operations.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用于内存操作的具体GPU部分。
- en: A type of thread that operates on the GPU.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GPU上操作的一种线程类型。
- en: A function that is executed simultaneously by tens of thousands of threads on
    GPU cores.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GPU核心上由成千上万的线程同时执行的功能。
- en: Solution
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *d) A function that is executed simultaneously by tens of thousands
    of threads on GPU cores.*'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*d) 在GPU核心上由成千上万的线程同时执行的功能。*
- en: What is coalesced memory access?
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是归一化内存访问？
- en: It’s when CUDA threads in the same warp access elements of the data which are
    adjacent in the memory.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当CUDA线程在同一个warp中访问内存中相邻的数据元素时。
- en: It’s when CUDA threads in different warps access elements of the data which
    are far in the memory.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当CUDA线程在不同warp中访问内存中相距较远的数据元素时。
- en: It’s when all threads have access to the global GPU memory.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当所有线程都能访问全局GPU内存时。
- en: It’s when threads in a warp perform different operations.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当warp中的线程执行不同的操作时。
- en: Solution
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *a) It’s when CUDA threads in the same warp access elements
    of the data which are adjacent in the memory.*'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*a) 当CUDA线程在同一个warp中访问内存中相邻的数据元素时。*
- en: What is the function of shared memory in the context of GPU execution?
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU执行上下文中，共享内存的功能是什么？
- en: It’s used to store global memory.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它用于存储全局内存。
- en: It’s used to store all the threads in a block.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它用于存储块中的所有线程。
- en: It can be used to “cache” data that is used by more than one thread, avoiding
    multiple reads from the global memory.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它可以用来“缓存”多个线程使用的内存数据，避免从全局内存中进行多次读取。
- en: It’s used to store all the CUDA cores.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它用于存储所有CUDA核心。
- en: Solution
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *c) It can be used to “cache” data that is used by more than
    one thread, avoiding multiple reads from the global memory.*'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*c) 它可以用来“缓存”多个线程使用的内存数据，避免从全局内存中进行多次读取。*
- en: What is the significance of over-subscribing the GPU?
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 过度订阅GPU的意义是什么？
- en: It reduces the overall performance of the GPU.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它降低了GPU的整体性能。
- en: It ensures that there are more blocks than SMPs present on the device, helping
    to hide latencies and ensure high occupancy of the GPU.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它确保设备上存在比SMPs更多的块，有助于隐藏延迟并确保GPU的高占用率。
- en: It leads to a memory overflow in the GPU.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它会导致GPU内存溢出。
- en: It ensures that there are more SMPs than blocks present on the device.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它确保设备上存在比块更多的SMPs。
- en: Solution
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *b) It ensures that there are more blocks than SMPs present
    on the device, helping to hide latencies and ensure high occupancy of the GPU.*'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*b) 它确保设备上存在比SMPs更多的块，有助于隐藏延迟并确保GPU的高占用率。*
- en: Keypoints
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 重点
- en: Parallel computing can be classified into distributed-memory and shared-memory
    architectures
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行计算可以分为分布式内存架构和共享内存架构
- en: Two types of parallelism that can be explored are data parallelism and task
    parallelism.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以探索的两种并行类型是数据并行和任务并行。
- en: GPUs are a type of shared memory architecture suitable for data parallelism.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU是一种适合数据并行的共享内存架构。
- en: GPUs have high parallelism, with threads organized into warps and blocks and.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU具有高并行性，线程被组织成warp和block。
- en: GPU optimization involves coalesced memory access, shared memory usage, and
    high thread and warp occupancy. Additionally, architecture-specific features such
    as warp-level operations and cooperative groups can be leveraged for more efficient
    processing.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU优化涉及归一化内存访问、共享内存使用、高线程和warp占用率。此外，可以利用特定于架构的特性，如warp级操作和协作组，以实现更有效的处理。
- en: Different types of parallelism
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不同的并行类型
- en: Distributed- vs. Shared-Memory Architecture
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式内存架构与共享内存架构
- en: Most of computing problems are not trivially parallelizable, which means that
    the subtasks need to have access from time to time to some of the results computed
    by other subtasks. The way subtasks exchange needed information depends on the
    available hardware.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数计算问题都不是简单可并行的，这意味着子任务需要不时访问其他子任务计算的一些结果。子任务交换所需信息的方式取决于可用的硬件。
- en: '![../_images/distributed_vs_shared.png](../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/distributed_vs_shared.png](../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png)'
- en: Distributed- vs shared-memory parallel computing.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式内存与共享内存并行计算。
- en: In a distributed memory environment each processing unit operates independently
    from the others. It has its own memory and it **cannot** access the memory in
    other nodes. The communication is done via network and each computing unit runs
    a separate copy of the operating system. In a shared memory machine all processing
    units have access to the memory and can read or modify the variables within.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式内存环境中，每个处理单元独立于其他单元运行。它拥有自己的内存，并且**不能**访问其他节点的内存。通信通过网络进行，每个计算单元运行操作系统的独立副本。在共享内存机器中，所有处理单元都可以访问内存，并可以读取或修改其中的变量。
- en: Processes and Threads
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进程与线程
- en: The type of environment (distributed- or shared-memory) determines the programming
    model. There are two types of parallelism possible, process based and thread based.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 环境类型（分布式或共享内存）决定了编程模型。可能的并行性有两种，基于进程的和基于线程的。
- en: '![../_images/processes-threads.png](../Images/94b62a3205925cbbdac5058209021754.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/processes-threads.png](../Images/94b62a3205925cbbdac5058209021754.png)'
- en: For distributed memory machines, a process-based parallel programming model
    is employed. The processes are independent execution units which have their *own
    memory* address spaces. They are created when the parallel program is started
    and they are only terminated at the end. The communication between them is done
    explicitly via message passing like MPI.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分布式内存机器，采用基于进程的并行编程模型。进程是独立的执行单元，它们有自己的内存地址空间。它们在并行程序启动时创建，并在结束时终止。它们之间的通信通过显式的消息传递（如MPI）进行。
- en: On the shared memory architectures it is possible to use a thread based parallelism.
    The threads are light execution units and can be created and destroyed at a relatively
    small cost. The threads have their own state information, but they *share* the
    *same memory* address space. When needed the communication is done though the
    shared memory.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存架构上，可以使用基于线程的并行性。线程是轻量级的执行单元，可以以相对较小的成本创建和销毁。线程有自己的状态信息，但它们**共享**相同的内存地址空间。当需要时，通过共享内存进行通信。
- en: Both approaches have their advantages and disadvantages. Distributed machines
    are relatively cheap to build and they have an “infinite “ capacity. In principle
    one could add more and more computing units. In practice the more computing units
    are used the more time consuming is the communication. The shared memory systems
    can achieve good performance and the programming model is quite simple. However
    they are limited by the memory capacity and by the access speed. In addition in
    the shared parallel model it is much easier to create race conditions.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都有其优缺点。分布式机器相对便宜，并且具有“无限”的容量。原则上可以添加更多的计算单元。实际上，使用的计算单元越多，通信就越耗时。共享内存系统可以实现良好的性能，编程模型相当简单。然而，它们受限于内存容量和访问速度。此外，在共享并行模型中，创建竞态条件要容易得多。
- en: Distributed- vs. Shared-Memory Architecture
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分布式内存与共享内存架构
- en: Most of computing problems are not trivially parallelizable, which means that
    the subtasks need to have access from time to time to some of the results computed
    by other subtasks. The way subtasks exchange needed information depends on the
    available hardware.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数计算问题都不是简单可并行的，这意味着子任务需要不时地访问其他子任务计算的一些结果。子任务交换所需信息的方式取决于可用的硬件。
- en: '![../_images/distributed_vs_shared.png](../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/distributed_vs_shared.png](../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png)'
- en: Distributed- vs shared-memory parallel computing.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式内存与共享内存并行计算。
- en: In a distributed memory environment each processing unit operates independently
    from the others. It has its own memory and it **cannot** access the memory in
    other nodes. The communication is done via network and each computing unit runs
    a separate copy of the operating system. In a shared memory machine all processing
    units have access to the memory and can read or modify the variables within.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式内存环境中，每个处理单元独立于其他单元运行。它拥有自己的内存，并且**不能**访问其他节点的内存。通信通过网络进行，每个计算单元运行操作系统的独立副本。在共享内存机器中，所有处理单元都可以访问内存，并可以读取或修改其中的变量。
- en: Processes and Threads
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进程与线程
- en: The type of environment (distributed- or shared-memory) determines the programming
    model. There are two types of parallelism possible, process based and thread based.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 环境类型（分布式或共享内存）决定了编程模型。可能的并行性类型有两种，基于进程的和基于线程的。
- en: '![../_images/processes-threads.png](../Images/94b62a3205925cbbdac5058209021754.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/processes-threads.png](../Images/94b62a3205925cbbdac5058209021754.png)'
- en: For distributed memory machines, a process-based parallel programming model
    is employed. The processes are independent execution units which have their *own
    memory* address spaces. They are created when the parallel program is started
    and they are only terminated at the end. The communication between them is done
    explicitly via message passing like MPI.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分布式内存机器，采用基于进程的并行编程模型。进程是独立的执行单元，它们有自己的内存地址空间。它们在并行程序启动时创建，并在结束时终止。它们之间的通信通过消息传递（如MPI）显式完成。
- en: On the shared memory architectures it is possible to use a thread based parallelism.
    The threads are light execution units and can be created and destroyed at a relatively
    small cost. The threads have their own state information, but they *share* the
    *same memory* address space. When needed the communication is done though the
    shared memory.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存架构上，可以使用基于线程的并行性。线程是轻量级的执行单元，可以以相对较小的成本创建和销毁。线程有自己的状态信息，但它们*共享*相同的内存地址空间。需要时，通过共享内存进行通信。
- en: Both approaches have their advantages and disadvantages. Distributed machines
    are relatively cheap to build and they have an “infinite “ capacity. In principle
    one could add more and more computing units. In practice the more computing units
    are used the more time consuming is the communication. The shared memory systems
    can achieve good performance and the programming model is quite simple. However
    they are limited by the memory capacity and by the access speed. In addition in
    the shared parallel model it is much easier to create race conditions.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都有其优缺点。分布式机器相对便宜，并且具有“无限”的容量。原则上可以添加更多和更多的计算单元。在实践中，使用的计算单元越多，通信就越耗时。共享内存系统可以实现良好的性能，编程模型相当简单。然而，它们受限于内存容量和访问速度。此外，在共享并行模型中，创建竞态条件要容易得多。
- en: Exposing parallelism
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 暴露并行性
- en: There are two types of parallelism that can be explored. The data parallelism
    is when the data can be distributed across computational units that can run in
    parallel. The units process the data by applying the same or very similar operation
    to different data elements. A common example is applying a blur filter to an image
    — the same function is applied to all the pixels on an image. This parallelism
    is natural for the GPU, where the same instruction set is executed in multiple
    threads.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 可以探索两种并行性类型。数据并行性是指数据可以分布在可以并行运行的计算单元上。这些单元通过将相同的或非常相似的操作应用于不同的数据元素来处理数据。一个常见的例子是将模糊滤镜应用于图像——相同的函数应用于图像上的所有像素。这种并行性对于GPU来说是自然的，因为在多个线程中执行相同的指令集。
- en: '[![../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png](../Images/df3318bcb375513cb4f8169dcb4efdbc.png)](../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png](../Images/df3318bcb375513cb4f8169dcb4efdbc.png)'
- en: Data parallelism and task parallelism. The data parallelism is when the same
    operation applies to multiple data (e.g. multiple elements of an array are transformed).
    The task parallelism implies that there are more than one independent task that,
    in principle, can be executed in parallel.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行性和任务并行性。数据并行性是指相同的操作应用于多个数据（例如，数组的多个元素被转换）。任务并行性意味着存在多个独立的任务，原则上可以并行执行。
- en: Data parallelism can usually be explored by the GPUs quite easily. The most
    basic approach would be finding a loop over many data elements and converting
    it into a GPU kernel. If the number of elements in the data set is fairly large
    (tens or hundred of thousands elements), the GPU should perform quite well. Although
    it would be odd to expect absolute maximum performance from such a naive approach,
    it is often the one to take. Getting absolute maximum out of the data parallelism
    requires good understanding of how GPU works.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行通常可以很容易地由GPU探索。最基本的方法是找到一个覆盖许多数据元素的循环，并将其转换为GPU内核。如果数据集中的元素数量相当大（数十或数百万个元素），GPU应该表现相当好。尽管从这种天真方法中期望绝对最大性能可能有些奇怪，但这通常是采取的方法。从数据并行中获得绝对最大性能需要很好地理解GPU的工作原理。
- en: 'Another type of parallelism is a task parallelism. This is when an application
    consists of more than one task that requiring to perform different operations
    with (the same or) different data. An example of task parallelism is cooking:
    slicing vegetables and grilling are very different tasks and can be done at the
    same time. Note that the tasks can consume totally different resources, which
    also can be explored.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种类型的并行是任务并行。这是指一个应用程序由多个任务组成，这些任务需要执行不同的操作（使用相同的或不同的数据）。任务并行的例子是烹饪：切菜和烤肉是非常不同的任务，可以同时进行。请注意，任务可以消耗完全不同的资源，这也值得探索。
- en: In short
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之
- en: Computing problems can be parallelized in distributed memory or shared memory
    architectures.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算问题可以在分布式内存或共享内存架构中并行化。
- en: In distributed memory, each unit operates independently, with no direct memory
    access between nodes.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式内存中，每个单元独立操作，节点之间没有直接的内存访问。
- en: In shared memory, units have access to the same memory and can communicate through
    shared variables.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在共享内存中，单元可以访问相同的内存，并通过共享变量进行通信。
- en: Parallel programming can be process-based (distributed memory) or thread-based
    (shared memory).
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行编程可以是基于进程的（分布式内存）或基于线程的（共享内存）。
- en: Process-based parallelism uses independent processes with separate memory spaces
    and explicit message passing.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于进程的并行使用具有独立内存空间和显式消息传递的独立进程。
- en: Thread-based parallelism uses lightweight threads that share the same memory
    space and communicate through shared memory.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于线程的并行使用共享相同内存空间的轻量级线程，并通过共享内存进行通信。
- en: Data parallelism distributes data across computational units, processing them
    with the same or similar operations.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据并行将数据分布在计算单元之间，使用相同的或类似的操作进行处理。
- en: Task parallelism involves multiple independent tasks that perform different
    operations on the same or different data.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务并行涉及多个独立任务，这些任务在相同或不同的数据上执行不同的操作。
- en: Task parallelism involves executing different tasks concurrently, leveraging
    different resources.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务并行涉及同时执行不同的任务，利用不同的资源。
- en: GPU Execution Model
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU执行模型
- en: In order to obtain maximum performance it is important to understand how GPUs
    execute the programs. As mentioned before a CPU is a flexible device oriented
    towards general purpose usage. It’s fast and versatile, designed to run operating
    systems and various, very different types of applications. It has lots of features,
    such as better control logic, caches and cache coherence, that are not related
    to pure computing. CPUs optimize the execution by trying to achieve low latency
    via heavy caching and branch prediction.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳性能，了解GPU如何执行程序非常重要。如前所述，CPU是一种灵活的通用设备。它速度快，功能多样，旨在运行操作系统和多种非常不同的应用程序。它具有许多功能，如更好的控制逻辑、缓存和缓存一致性，这些功能与纯计算无关。CPU通过尝试通过大量缓存和分支预测来实现低延迟来优化执行。
- en: '[![../_images/cpu-gpu-highway.png](../Images/2be579a867209f9ee644b6264269be37.png)](../_images/cpu-gpu-highway.png)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/cpu-gpu-highway.png](../Images/2be579a867209f9ee644b6264269be37.png)'
- en: Cars and roads analogy for the CPU and GPU behavior. The compact road is analogous
    to the CPU (low latency, low throughput) and the broader road is analogous to
    the GPU (high latency, high throughput).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: CPU和GPU行为的汽车和道路类比。紧凑的道路类似于CPU（低延迟，低吞吐量），而宽阔的道路类似于GPU（高延迟，高吞吐量）。
- en: In contrast the GPUs contain a relatively small amount of transistors dedicated
    to control and caching, and a much larger fraction of transistors dedicated to
    the mathematical operations. Since the cores in a GPU are designed just for 3D
    graphics, they can be made much simpler and there can be a very larger number
    of cores. The current GPUs contain thousands of CUDA cores. Performance in GPUs
    is obtain by having a very high degree of parallelism. Lots of threads are launched
    in parallel. For good performance there should be at least several times more
    than the number of CUDA cores. GPU threads are much lighter than the usual CPU
    threads and they have very little penalty for context switching. This way when
    some threads are performing some memory operations (reading or writing) others
    execute instructions.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，GPU中包含的用于控制和缓存的晶体管相对较少，而用于数学运算的晶体管比例则大得多。由于GPU的核心仅设计用于3D图形，因此它们可以设计得更加简单，并且可以拥有大量的核心。当前的GPU包含数千个CUDA核心。GPU的性能是通过实现非常高的并行度来获得的。并行启动了大量的线程。为了获得良好的性能，线程的数量至少应该是CUDA核心数量的几倍。GPU线程比常规CPU线程轻得多，并且它们在上下文切换方面几乎没有惩罚。这样，当一些线程执行一些内存操作（读取或写入）时，其他线程则执行指令。
- en: CUDA Threads, Warps, Blocks
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUDA线程、Warps、Blocks
- en: In order to understand the GPU execution model let’s look at the so called axpy
    operation. On a single CPU core this operation would be executed in a serial manner
    in a for/do loop going over each element on the array, id, and computing y[id]=y[id]+a*x[id].
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解GPU执行模型，让我们看看所谓的axpy操作。在单个CPU核心上，这个操作将以串行方式在一个for/do循环中执行，遍历数组中的每个元素id，并计算y[id]=y[id]+a*x[id]。
- en: '[PRE4]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In order to perform the some operation on a GPU the program launches a function
    called *kernel*, which is executed simultaneously by tens of thousands of threads
    that can be run on GPU cores parallelly.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在GPU上执行某些操作，程序会启动一个名为*内核*的函数，该函数由成千上万的线程同时执行，这些线程可以并行地运行在GPU核心上。
- en: '[PRE5]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The programmers control how many instances of ker_axpy_ are created and they
    have to make sure that all the elements are processed and also that no out of
    bounds accessed are happening.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员控制ker_axpy_实例的创建数量，并必须确保所有元素都得到处理，并且没有越界访问发生。
- en: GPU threads are much lighter than the usual CPU threads and they have very little
    penalty for context switching. By “over-subscribing” the GPU there are threads
    that are performing some memory operations (reading or writing), while others
    execute instructions.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: GPU线程比常规CPU线程轻得多，并且它们在上下文切换方面几乎没有惩罚。通过“过度订阅”GPU，有一些线程正在执行一些内存操作（读取或写入），而其他线程则执行指令。
- en: '[![../_images/THREAD_CORE.png](../Images/5d5381fb2a0059b880639b60b35a9e5e.png)](../_images/THREAD_CORE.png)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/THREAD_CORE.png](../Images/5d5381fb2a0059b880639b60b35a9e5e.png)'
- en: Every thread is associated with a particular intrinsic index which can be used
    to calculate and access memory locations in an array. Each thread has its context
    and set of private variables. All threads have access to the global GPU memory,
    but there is no general way to synchronize when executing a kernel. If some threads
    need data from the global memory which was modified by other threads the code
    would have to be split in several kernels because only at the completion of a
    kernel it is ensured that the writing to the global memory was completed.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程都与一个特定的内在索引相关联，该索引可用于计算和访问数组中的内存位置。每个线程都有自己的上下文和私有变量集。所有线程都可以访问全局GPU内存，但在执行内核时没有一般的方法来同步。如果某些线程需要从其他线程修改的全局内存中获取数据，则代码必须分成几个内核，因为只有在内核完成时才能确保全局内存的写入已完成。
- en: Apart from being much light weighted there are more differences between GPU
    threads and CPU threads. GPU threads are grouped together in groups called warps.
    This done at hardware level.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 除了轻量级之外，GPU线程和CPU线程之间还有更多差异。GPU线程被分组在一起，称为warps。这是在硬件级别完成的。
- en: '[![../_images/WARP_SMTU.png](../Images/3bce412cee380c39c9c2d4125dbd029f.png)](../_images/WARP_SMTU.png)'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/WARP_SMTU.png](../Images/3bce412cee380c39c9c2d4125dbd029f.png)'
- en: All memory accesses to the GPU memory are as a group in blocks of specific sizes
    (32B, 64B, 128B etc.). To obtain good performance the CUDA threads in the same
    warp need to access elements of the data which are adjacent in the memory. This
    is called *coalesced* memory access.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 所有对GPU内存的内存访问都是以特定大小的块（32B、64B、128B等）为组进行的。为了获得良好的性能，同一warp中的CUDA线程需要访问内存中相邻的数据元素。这被称为*归一化*内存访问。
- en: On some architectures, all members of a warp have to execute the same instruction,
    the so-called “lock-step” execution. This is done to achieve higher performance,
    but there are some drawbacks. If an **if** statement is present inside a warp
    will cause the warp to be executed more than once, one time for each branch. When
    different threads within a single warp take different execution paths based on
    a conditional statement (if), both branches are executed sequentially, with some
    threads being active while others are inactive. On architectures without lock-step
    execution, such as NVIDIA Volta / Turing (e.g., GeForce 16xx-series) or newer,
    warp divergence is less costly.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些架构中，warp中的所有成员必须执行相同的指令，即所谓的“锁步”执行。这是为了实现更高的性能，但也有一些缺点。如果warp内部存在一个**if**语句，会导致warp执行多次，每次分支执行一次。当单个warp内的不同线程根据条件语句（if）采取不同的执行路径时，两个分支会顺序执行，一些线程处于活动状态，而其他线程处于非活动状态。在没有锁步执行的架构中，例如NVIDIA
    Volta / Turing（例如，GeForce 16xx系列）或更新的架构，warp发散的成本较低。
- en: There is another level in the GPU threads hierarchy. The threads are grouped
    together in so called blocks. Each block is assigned to one Streaming Multiprocessor
    (SMP) unit. A SMP contains one or more SIMT (single instruction multiple threads)
    units, schedulers, and very fast on-chip memory. Some of this on-chip memory can
    be used in the programs, this is called shared memory. The shared memory can be
    used to “cache” data that is used by more than one thread, thus avoiding multiple
    reads from the global memory. It can also be used to avoid memory accesses which
    are not efficient. For example in a matrix transpose operation, we have two memory
    operations per element and only can be coalesced. In the first step a tile of
    the matrix is saved read a coalesced manner in the shared memory. After all the
    reads of the block are done the tile can be locally transposed (which is very
    fast) and then written to the destination matrix in a coalesced manner as well.
    Shared memory can also be used to perform block-level reductions and similar collective
    operations. All threads can be synchronized at block level. Furthermore when the
    shared memory is written in order to ensure that all threads have completed the
    operation the synchronization is compulsory to ensure correctness of the program.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU线程层次结构中还有另一个级别。线程被分组在一起，称为块。每个块都分配给一个流式多处理器（SMP）单元。一个SMP包含一个或多个SIMT（单指令多线程）单元、调度器和非常快速的片上内存。部分片上内存可以在程序中使用，这被称为共享内存。共享内存可以用来“缓存”多个线程使用的内存，从而避免从全局内存进行多次读取。它也可以用来避免低效的内存访问。例如，在矩阵转置操作中，每个元素有两个内存操作，并且只能合并。在第一步中，矩阵的块以合并的方式保存到共享内存中。当块的读取全部完成后，块可以局部转置（这非常快），然后以合并的方式写入目标矩阵。共享内存还可以用来执行块级别的归约和类似的集体操作。所有线程都可以在块级别进行同步。此外，当共享内存写入时，为了确保所有线程都完成了操作，同步是强制性的，以确保程序的正确性。
- en: '[![../_images/BLOCK_SMP.png](../Images/ef0f46b8c50be0dbedbce258b377a92e.png)](../_images/BLOCK_SMP.png)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '![../_images/BLOCK_SMP.png](../Images/ef0f46b8c50be0dbedbce258b377a92e.png)'
- en: Finally, a block of threads can not be split among SMPs. For performance blocks
    should have more than one warp. The more warps are active on an SMP the better
    is hidden the latency associated with the memory operations. If the resources
    are sufficient, due to fast context switching, an SMP can have more than one block
    active in the same time. However these blocks can not share data with each other
    via the on-chip memory.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个线程块不能在多个对称多处理器（SMPs）之间分割。为了提高性能，线程块应该包含多个warp。在SMP上活跃的warp越多，与内存操作相关的延迟就越不明显。如果资源充足，由于快速的上下文切换，一个SMP可以在同一时间激活多个块。然而，这些块不能通过片上内存相互共享数据。
- en: To summarize this section. In order to take advantage of GPUs the algorithms
    must allow the division of work in many small subtasks which can be executed in
    the same time. The computations are offloaded to GPUs, by launching tens of thousands
    of threads all executing the same function, *kernel*, each thread working on different
    part of the problem. The threads are executed in groups called *blocks*, each
    block being assigned to a SMP. Furthermore the threads of a block are divided
    in *warps*, each executed by SIMT unit. All threads in a warp execute the same
    instructions and all memory accesses are done collectively at warp level. The
    threads can synchronize and share data only at block level. Depending on the architecture,
    some data sharing can be done as well at warp level.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本节内容。为了利用GPU，算法必须允许将工作划分为许多小任务，这些任务可以同时执行。通过启动成千上万的线程，所有线程执行相同的函数，即“内核”，每个线程处理问题的不同部分，将计算卸载到GPU上。线程以称为“块”的组执行。此外，块内的线程被划分为“warp”，每个warp由SIMT单元执行。所有warp中的线程执行相同的指令，所有内存访问都在warp级别集体完成。线程只能在块级别同步和共享数据。根据架构，某些数据共享也可以在warp级别进行。
- en: In order to hide latencies it is recommended to “over-subscribe” the GPU. There
    should be many more blocks than SMPs present on the device. Also in order to ensure
    a good occupancy of the CUDA cores there should be more warps active on a given
    SMP than SIMT units. This way while some warps of threads are idle waiting for
    some memory operations to complete, others use the CUDA cores, thus ensuring a
    high occupancy of the GPU.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 为了隐藏延迟，建议对GPU进行“过度订阅”。设备上应该有比现有的SMPs更多的块。此外，为了确保CUDA核心的良好占用率，给定SMP上应该有比SIMT单元更多的活动warp。这样，当一些warp的线程空闲等待某些内存操作完成时，其他线程可以使用CUDA核心，从而确保GPU的高占用率。
- en: In addition to this there are some architecture-specific features of which the
    developers can take advantage. Warp-level operations are primitives provided by
    the GPU architecture to allow for efficient communication and synchronization
    within a warp. They allow threads within a warp to exchange data efficiently,
    without the need for explicit synchronization. These warp-level operations, combined
    with the organization of threads into blocks and clusters, make it possible to
    implement complex algorithms and achieve high performance on the GPU. The cooperative
    groups feature introduced in recent versions of CUDA provides even finer-grained
    control over thread execution, allowing for even more efficient processing by
    giving more flexibility to the thread hierarchy. Cooperative groups allow threads
    within a block to organize themselves into smaller groups, called cooperative
    groups, and to synchronize their execution and share data within the group.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些特定于架构的特性，开发者可以利用这些特性。Warp级操作是GPU架构提供的原语，允许在warp内部进行高效的通信和同步。它们允许warp内的线程高效地交换数据，无需显式同步。这些warp级操作，结合线程组织成块和集群，使得在GPU上实现复杂算法并达到高性能成为可能。CUDA最近版本中引入的协作组功能提供了对线程执行的更细粒度控制，通过赋予线程层次结构更多灵活性，从而实现更高效的处理。协作组允许块内的线程组织成更小的组，称为协作组，并在组内同步执行和共享数据。
- en: Below there is an example of how the threads in a grid can be associated with
    specific elements of an array
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例，说明如何将网格中的线程与数组的特定元素相关联
- en: '[![../_images/Indexing.png](../Images/fccb3e8f6b92e4b53a0805282127878f.png)](../_images/Indexing.png)'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '[![../_images/Indexing.png](../Images/fccb3e8f6b92e4b53a0805282127878f.png)](../_images/Indexing.png)'
- en: The thread marked by orange color is part of a grid of threads size 4096\. The
    threads are grouped in blocks of size 256\. The “orange” thread has index 3 in
    the block 2 and the global calculated index 515.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 用橙色标记的线程是大小为4096的线程网格的一部分。线程被分组为大小为256的块。该“橙色”线程在块2中的索引为3，全局计算索引为515。
- en: For a vector addition example this would be used as follow `c[index]=a[index]+b[index]`.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 对于向量加法示例，可以使用以下方式 `c[index]=a[index]+b[index]`。
- en: In short
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之
- en: GPUs have a different execution model compared to CPUs, with a focus on parallelism
    and mathematical operations.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与CPU相比，GPU具有不同的执行模型，侧重于并行性和数学运算。
- en: GPUs consist of thousands of lightweight threads that can be executed simultaneously
    on GPU cores.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU由成千上万的轻量级线程组成，这些线程可以在GPU核心上同时执行。
- en: Threads are organized into warps, and warps are grouped into blocks assigned
    to streaming multiprocessors (SMPs).
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程被组织成warp，warp被分组到分配给流式多处理器（SMPs）的块中。
- en: GPUs achieve performance through high degrees of parallelism and efficient memory
    access.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU通过高程度的并行性和高效的内存访问来实现性能。
- en: Shared memory can be used to cache data and improve memory access efficiency
    within a block.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 共享内存可以用来缓存数据并提高块内内存访问效率。
- en: Synchronization and data sharing are limited to the block level, with some possible
    sharing at the warp level depending on the architecture.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同步和数据共享限于块级别，根据架构，warp级别可能有某些共享。
- en: Over-subscribing the GPU and maximizing warp and block occupancy help hide latencies
    and improve performance.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度订阅GPU并最大化warp和块占用率有助于隐藏延迟并提高性能。
- en: Warp-level operations and cooperative groups provide efficient communication
    and synchronization within a warp or block.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: warp级操作和协作组提供了在warp或块内部的高效通信和同步。
- en: Thread indexing allows associating threads with specific elements in an array
    for parallel processing.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线程索引允许将线程与数组中的特定元素关联起来，以实现并行处理。
- en: Terminology
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语
- en: 'At the moment there are three major GPU producers: NVIDIA, Intel, and AMD.
    While the basic concept behind GPUs is pretty similar they use different names
    for the various parts. Furthermore there are software environments for GPU programming,
    some from the producers and some from external groups all having different naming
    as well. Below there is a short compilation of the some terms used across different
    platforms and software environments.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有三个主要的GPU生产商：NVIDIA、Intel和AMD。虽然GPU背后的基本概念相当相似，但它们为不同的部分使用不同的名称。此外，还有用于GPU编程的软件环境，有些来自生产商，有些来自外部团体，所有这些都有不同的命名。以下是对不同平台和软件环境中使用的某些术语的简要汇编。
- en: Software mapping naming
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 软件映射命名
- en: '| CUDA | HIP | OpenCL | SYCL |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | HIP | OpenCL | SYCL |'
- en: '| --- | --- | --- | --- |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| grid of threads | NDRange |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| grid of threads | NDRange |'
- en: '| block | work-group |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| block | work-group |'
- en: '| warp | wavefront | sub-group |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| warp | wavefront | sub-group |'
- en: '| thread | work-item |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| thread | work-item |'
- en: '| registers | private memory |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| registers | private memory |'
- en: '| shared memory | local data share | local memory |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| shared memory | local data share | local memory |'
- en: '| threadIdx.{x,y,z} | get_local_id({0,1,2}) | nd_item::get_local({2,1,0}) [[1]](#syclindex)
    |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| threadIdx.{x,y,z} | get_local_id({0,1,2}) | nd_item::get_local({2,1,0}) [[1]](#syclindex)
    |'
- en: '| blockIdx.{x,y,z} | get_group_id({0,1,2}) | nd_item::get_group({2,1,0}) [[1]](#syclindex)
    |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| blockIdx.{x,y,z} | get_group_id({0,1,2}) | nd_item::get_group({2,1,0}) [[1]](#syclindex)
    |'
- en: '| blockDim.{x,y,z} | get_local_size({0,1,2}) | nd_item::get_local_range({2,1,0})
    [[1]](#syclindex) |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| blockDim.{x,y,z} | get_local_size({0,1,2}) | nd_item::get_local_range({2,1,0})
    [[1]](#syclindex) |'
- en: Exercises
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: What are threads in the context of shared memory architectures?
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享内存架构的上下文中，线程是什么？
- en: Independent execution units with their own memory address spaces
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有自己内存地址空间的独立执行单元
- en: Light execution units with shared memory address spaces
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轻量级执行单元，具有共享内存地址空间
- en: Communication devices between separate memory units
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之间不同内存单元的通信设备
- en: Programming models for distributed memory machines
  id: totrans-403
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分布式内存机器的编程模型
- en: Solution
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *b) Light execution units with shared memory address spaces*'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*b) 轻量级执行单元，具有共享内存地址空间*
- en: What is data parallelism?
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是数据并行性？
- en: Distributing data across computational units that run in parallel, applying
    the same or similar operations to different data elements.
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在并行运行的计算单元上分配数据，对不同的数据元素应用相同的或类似的操作。
- en: Distributing tasks across computational units that run in parallel, applying
    different operations to the same data elements.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在并行运行的计算单元上分配任务，对相同的数据元素应用不同的操作。
- en: Distributing data across computational units that run sequentially, applying
    the same operation to all data elements.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算单元上按顺序分配数据，对所有数据元素应用相同的操作。
- en: Distributing tasks across computational units that run sequentially, applying
    different operations to different data elements.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算单元上按顺序分配任务，对不同的数据元素应用不同的操作。
- en: Solution
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *a) Distributing data across computational units that run in
    parallel, applying the same or similar operations to different data elements.*'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*a) 在并行运行的计算单元上分配数据，对不同的数据元素应用相同的或类似的操作。*
- en: What type of parallelism is natural for GPU?
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPU来说，哪种类型的并行性是自然的？
- en: Task Parallelism
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 任务并行性
- en: Data Parallelism
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据并行性
- en: Both data and task parallelism
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据并行性和任务并行性
- en: Neither data nor task parallelism
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既不是数据并行也不是任务并行
- en: Solution
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *b) Data Parallelism*'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*b) 数据并行性*
- en: What is a kernel in the context of GPU execution?
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU执行上下文中，内核是什么？
- en: A specific section of the CPU used for memory operations.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CPU用于内存操作的一个特定部分。
- en: A specific section of the GPU used for memory operations.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GPU用于内存操作的一个特定部分。
- en: A type of thread that operates on the GPU.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一种在GPU上操作的类型。
- en: A function that is executed simultaneously by tens of thousands of threads on
    GPU cores.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个在GPU核心上由成千上万的线程同时执行的功能。
- en: Solution
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *d) A function that is executed simultaneously by tens of thousands
    of threads on GPU cores.*'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*d) 一个在GPU核心上由成千上万的线程同时执行的功能。*
- en: What is coalesced memory access?
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是归一化内存访问？
- en: It’s when CUDA threads in the same warp access elements of the data which are
    adjacent in the memory.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它发生在同一warp中的CUDA线程访问内存中相邻的数据元素时。
- en: It’s when CUDA threads in different warps access elements of the data which
    are far in the memory.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它发生在不同warp中的CUDA线程访问内存中远离的数据元素时。
- en: It’s when all threads have access to the global GPU memory.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它发生在所有线程都能访问全局GPU内存时。
- en: It’s when threads in a warp perform different operations.
  id: totrans-431
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它发生在warp中的线程执行不同的操作时。
- en: Solution
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *a) It’s when CUDA threads in the same warp access elements
    of the data which are adjacent in the memory.*'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*a) 它发生在同一warp中的CUDA线程访问内存中相邻的数据元素时。*
- en: What is the function of shared memory in the context of GPU execution?
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU执行上下文中，共享内存的功能是什么？
- en: It’s used to store global memory.
  id: totrans-435
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它用于存储全局内存。
- en: It’s used to store all the threads in a block.
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它用于存储块中的所有线程。
- en: It can be used to “cache” data that is used by more than one thread, avoiding
    multiple reads from the global memory.
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它可以用来“缓存”多个线程使用的内存，避免从全局内存中进行多次读取。
- en: It’s used to store all the CUDA cores.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它用于存储所有CUDA核心。
- en: Solution
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *c) It can be used to “cache” data that is used by more than
    one thread, avoiding multiple reads from the global memory.*'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*c) 它可以用来“缓存”多个线程使用的内存，避免从全局内存中进行多次读取。*
- en: What is the significance of over-subscribing the GPU?
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 过度订阅GPU的意义是什么？
- en: It reduces the overall performance of the GPU.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它降低了GPU的整体性能。
- en: It ensures that there are more blocks than SMPs present on the device, helping
    to hide latencies and ensure high occupancy of the GPU.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它确保设备上存在比SMPs更多的块，有助于隐藏延迟并确保GPU的高占用率。
- en: It leads to a memory overflow in the GPU.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它导致GPU内存溢出。
- en: It ensures that there are more SMPs than blocks present on the device.
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它确保设备上存在比块更多的SMPs。
- en: Solution
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'Correct answer: *b) It ensures that there are more blocks than SMPs present
    on the device, helping to hide latencies and ensure high occupancy of the GPU.*'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 正确答案：*b) 它确保设备上存在比SMPs更多的块，有助于隐藏延迟并确保GPU的高占用率。*
- en: Keypoints
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 重点
- en: Parallel computing can be classified into distributed-memory and shared-memory
    architectures
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并行计算可以分为分布式内存和共享内存架构
- en: Two types of parallelism that can be explored are data parallelism and task
    parallelism.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以探索的两种并行类型是数据并行和任务并行。
- en: GPUs are a type of shared memory architecture suitable for data parallelism.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU是一种适合数据并行的共享内存架构。
- en: GPUs have high parallelism, with threads organized into warps and blocks and.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU具有高并行性，线程组织成warp和块。
- en: GPU optimization involves coalesced memory access, shared memory usage, and
    high thread and warp occupancy. Additionally, architecture-specific features such
    as warp-level operations and cooperative groups can be leveraged for more efficient
    processing.*
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU优化涉及归一化内存访问、共享内存使用、高线程和warp占用率。此外，可以利用特定于架构的特性，如warp级操作和协作组，以实现更有效的处理。
