- en: Part I
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一部分
- en: Chapter 1\. Background
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一章 背景
- en: Much ink has been spilled describing the GPU revolution in computing. I have
    read about it with interest because I got involved very early. I was at Microsoft
    in the mid-1990s as development lead for Direct3D when Intel and AMD were introducing
    the first multimedia instruction sets to accelerate floating point computation.
    Intel had already tried (unsuccessfully) to forestall the migration of clock cycles
    for 3D rasterization from their CPUs by working with Microsoft to ship rasterizers
    that used their MMX instruction set. I knew that effort was doomed when we found
    that the MMX rasterizer, running on a yet-to-be-released Pentium 2 processor,
    was half as fast as a humble S3 Virge GX rasterizer that was available for sale.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多文章描述了GPU在计算领域的革命。我对此一直很感兴趣，因为我很早就参与其中。在1990年代中期，我在微软担任Direct3D的开发负责人，当时Intel和AMD正在推出第一个多媒体指令集来加速浮点计算。Intel曾尝试（但未成功）通过与微软合作推出使用其MMX指令集的光栅化器，来阻止3D光栅化的时钟周期从他们的CPU迁移出去。我知道那项努力注定会失败，因为我们发现，运行在尚未发布的Pentium
    2处理器上的MMX光栅化器，速度只有市面上可用的简易S3 Virge GX光栅化器的一半。
- en: For Direct3D 6.0, we worked with CPU vendors to integrate their code into our
    geometry pipeline so developers could transparently benefit from vendor-optimized
    code paths that used new instruction sets from Intel and AMD. Game developers
    embraced the new geometry pipeline, but it did not forestall the continued migration
    of clock cycles from the CPU to the GPU, as the new instruction sets were used
    to generate vertex data for consumption by GPUs’ hardware geometry pipelines.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Direct3D 6.0，我们与CPU厂商合作，将他们的代码集成到我们的几何管线中，以便开发者能够透明地从厂商优化的代码路径中受益，这些代码路径使用了Intel和AMD的新指令集。游戏开发者积极接受了新的几何管线，但这并未阻止时钟周期从CPU迁移到GPU的趋势，因为新的指令集被用来生成供GPU硬件几何管线使用的顶点数据。
- en: 'About this time, the number of transistors on GPUs overtook the number of transistors
    on CPUs. The crossover was in 1997–1998, when the Pentium 2 and the NVIDIA RIVA
    TNT both had transistor counts of about 8M. Subsequently, the Geforce 256 (15M
    transistors), Geforce 2 (28M transistors), and Geforce3 (63M transistors) all
    had more transistors than contemporary CPUs. Additionally, the architectural differences
    between the two devices were becoming clear: Most of the die area for CPUs was
    dedicated to cache, while most of the die area for GPUs was dedicated to logic.
    Intel was able to add significant new instruction set extensions (MMX, SSE, SSE2,
    etc.) with negligible area cost because their CPUs were mostly cache. GPUs were
    designed for parallel throughput processing; their small caches were intended
    more for bandwidth aggregation than for reducing latency.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在这个时候，GPU 上的晶体管数量超越了 CPU 上的晶体管数量。这个交叉点出现在 1997 年至 1998 年，当时奔腾 2 和 NVIDIA RIVA
    TNT 的晶体管数量都约为 800 万。此后，Geforce 256（1500 万晶体管）、Geforce 2（2800 万晶体管）和 Geforce 3（6300
    万晶体管）上的晶体管数量都超过了当时的 CPU。此外，两者之间的架构差异也逐渐显现：大部分 CPU 的芯片区域被用于缓存，而大部分 GPU 的芯片区域则用于逻辑运算。英特尔能够以微乎其微的面积成本添加显著的新指令集扩展（如
    MMX、SSE、SSE2 等），因为它们的 CPU 主要由缓存组成。GPU 则设计用于并行吞吐处理；它们的小缓存更多是为了带宽聚合，而非减少延迟。
- en: 'While companies like ATI and NVIDIA were building GPUs that were faster and
    increasingly capable, CPU vendors continued to drive clock rates higher as Moore’s
    Law enabled both increased transistor budgets and increased clock speeds. The
    first Pentium (c. 1993) had a clock rate of 60MHz, while MMX-enabled Pentiums
    (c. 1997) had clock rates of 200MHz. By the end of the decade, clock rates had
    exceeded 1,000MHz. But shortly thereafter, an important event in the history of
    computing took place: Moore’s Law hit a wall. The transistors would continue to
    shrink, but clock rates could not continue to increase.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然像 ATI 和 NVIDIA 这样的公司在构建更快且功能越来越强大的 GPU，但 CPU 厂商依然在推动时钟频率的提升，随着摩尔定律的推动，晶体管预算和时钟速度都在不断增加。第一代奔腾处理器（大约
    1993 年）时钟频率为 60MHz，而支持 MMX 技术的奔腾处理器（大约 1997 年）时钟频率为 200MHz。到了十年末，时钟频率已经超过了 1,000MHz。但不久之后，计算机历史上发生了一件重要事件：摩尔定律遇到了瓶颈。晶体管将继续缩小，但时钟频率无法再继续提升。
- en: The event was not unexpected. Pat Gelsinger of Intel delivered a keynote at
    the 2001 IEEE Solid-State Circuits Conference and stated that if chips continued
    on their current design path, they would be as hot as nuclear reactors by the
    end of the decade and as hot as the surface of the sun by 2015\. In the future,
    performance would have to come from “simultaneous multithreading” (SMT), possibly
    supported by putting multiple CPU cores on a single chip. Indeed, that is exactly
    what CPU vendors have done; today, it is difficult to almost impossible to find
    a desktop PC with a single-core CPU. But the decades-long free ride enabled by
    Moore’s Law, in which increased clock rates made it possible for applications
    to run faster with little to no effort on the part of software developers, was
    over. Multicore CPUs require multithreaded applications. Only applications that
    benefit from parallelism can expect increased performance from CPUs with a larger
    number of cores.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这一事件并不意外。英特尔的帕特·基尔辛格在 2001 年 IEEE 固态电路会议上发表了主题演讲，并表示，如果芯片继续沿着当前的设计路径发展，到十年末，它们的温度将像核反应堆一样高，而到
    2015 年，它们的温度将像太阳表面一样高。未来，性能将必须依赖于“同时多线程”（SMT），这可能通过将多个 CPU 核心集成到单个芯片上来实现。事实上，这正是
    CPU 厂商所做的；今天，几乎不可能找到配备单核 CPU 的台式电脑。但由摩尔定律所带来的几十年的“免费 ride”，通过提升时钟频率，使得应用程序能够在软件开发者几乎不需要做任何努力的情况下变得更快，这一时期已经结束。多核
    CPU 需要多线程应用程序。只有那些能够从并行计算中受益的应用程序，才能期待从拥有更多核心的 CPU 中获得性能提升。
- en: GPUs were well positioned to take advantage of this new trend in Moore’s Law.
    While CPU applications that had not been authored with parallelism in mind would
    require extensive refactoring (if they could be made parallel at all), graphics
    applications were already formulated in a way that exploited the inherent parallelism
    between independent pixels. For GPUs, increasing performance by increasing the
    number of execution cores was a natural progression. In fact, GPU designers tend
    to prefer more cores over more capable cores. They eschew strategies that CPU
    manufacturers take for granted, like maximizing clock frequency (GPUs had never,
    and still do not, run at clock rates approaching the limits of transistor fabrication),
    speculative execution, branch prediction, and store forwarding. And to prevent
    this ever-more-capable processor from becoming I/O bound, GPU designers integrated
    memory controllers and worked with DRAM manufacturers to enable bandwidths that
    far exceeded the amount of bandwidth available to CPUs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: GPU（图形处理单元）在摩尔定律的这一新趋势中占据了有利位置。虽然没有考虑并行性的CPU应用程序需要进行大量重构（如果它们能够变成并行的话），但图形应用程序已经以利用独立像素之间固有并行性的方式进行设计。对于GPU来说，通过增加执行核心的数量来提高性能是一个自然的进步。事实上，GPU设计师倾向于选择更多的核心，而非更强大的核心。他们摒弃了CPU制造商理所当然采用的策略，比如最大化时钟频率（GPU从未，也仍然不会，在接近晶体管制造极限的时钟频率下运行）、投机执行、分支预测和存储转发。而且，为了防止这个日益强大的处理器变得受限于I/O，GPU设计师集成了内存控制器，并与DRAM制造商合作，启用了远超CPU可用带宽的带宽。
- en: 'But that abundant horsepower was difficult for nongraphics developers to exploit.
    Some adventurous souls used graphics APIs such as Direct3D and OpenGL to subvert
    graphics hardware to perform nongraphics computations. The term *GPGPU* (general-purpose
    GPU programming) was invented to describe this approach, but for the most part,
    the computational potential of GPUs remained untapped until CUDA. Ian Buck, whose
    Brook project at Stanford enabled simplified development of GPGPU applications,
    came to NVIDIA and led development of a new set of development tools that would
    enable nongraphics applications to be authored for GPUs much more easily. The
    result is CUDA: a proprietary toolchain from NVIDIA that enables C programmers
    to write parallel code for GPUs using a few easy-to-use language extensions.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，这种丰富的计算能力对非图形开发者来说难以利用。一些敢于冒险的人使用图形API，如Direct3D和OpenGL，将图形硬件转用于执行非图形计算。*GPGPU*（通用GPU编程）这一术语便是用来描述这种做法的，但在CUDA出现之前，GPU的计算潜力大多未被开发。Ian
    Buck，他在斯坦福大学的Brook项目使GPGPU应用开发变得更加简化，后来加入了NVIDIA并领导了新一套开发工具的开发，这些工具能够让非图形应用程序更容易地为GPU编写。最终的结果便是CUDA：NVIDIA的专有工具链，允许C语言程序员通过几个易于使用的语言扩展来为GPU编写并行代码。
- en: Since its introduction in 2007, CUDA has been well received. Tens of thousands
    of academic papers have been written that use the technology. It has been used
    in commercial software packages as varied as Adobe’s CS5 to Manifold’s GIS (geographic
    information system). For suitable workloads, CUDA-capable GPUs range from 5x to
    400x faster than contemporary CPUs. The sources of these speedups vary. Sometimes
    the GPUs are faster because they have more cores; sometimes because they have
    higher memory bandwidth; and sometimes because the application can take advantage
    of specialized GPU hardware not present in CPUs, like the texture hardware or
    the SFU unit that can perform fast transcendentals. Not all applications can be
    implemented in CUDA. In fact, not all *parallel* applications can be implemented
    in CUDA. But it has been used in a wider variety of applications than any other
    GPU computing technology. I hope this book helps accomplished CUDA developers
    to get the most out of CUDA.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2007 年推出以来，CUDA 一直受到广泛欢迎。成千上万篇使用该技术的学术论文已经被撰写出来。它已经被应用于各种商业软件包，从 Adobe 的 CS5
    到 Manifold 的 GIS（地理信息系统）。对于合适的工作负载，支持 CUDA 的 GPU 比当代 CPU 快 5 到 400 倍。这些加速的来源各不相同。有时
    GPU 更快是因为它们拥有更多的核心；有时是因为它们有更高的内存带宽；有时则是因为应用程序可以利用 CPU 中没有的 GPU 专用硬件，比如纹理硬件或可以执行快速超越运算的
    SFU 单元。并非所有应用程序都可以用 CUDA 实现。事实上，并非所有*并行*应用程序都能用 CUDA 实现。但它已被应用于比其他任何 GPU 计算技术更多样化的应用程序。我希望这本书能帮助成熟的
    CUDA 开发者充分发挥 CUDA 的优势。
- en: 1.1\. Our Approach
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 我们的方法
- en: CUDA is a difficult topic to write about. Parallel programming is complicated
    even without operating system considerations (Windows, Linux, MacOS), platform
    considerations (Tesla and Fermi, integrated and discrete GPUs, multiple GPUs),
    CPU/GPU concurrency considerations, and CUDA-specific considerations, such as
    having to decide between using the CUDA runtime or the driver API. When you add
    in the complexities of how best to structure CUDA kernels, it may seem overwhelming.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 是一个很难写的主题。即使不考虑操作系统（Windows、Linux、MacOS）、平台（Tesla 和 Fermi、集成与独立 GPU、多 GPU）、CPU/GPU
    并发性以及 CUDA 特定的考虑（例如，必须在使用 CUDA 运行时和驱动程序 API 之间做出选择），并行编程本身就已经非常复杂。当你再加上如何最佳构建
    CUDA 内核的复杂性时，这可能会显得让人不知所措。
- en: 'To present this complexity in a manageable way, most topics are explained more
    than once from different perspectives. *What does the texture mapping hardware
    do?* is a different question than *How do I write a kernel that does texture mapping?*
    This book addresses both questions in separate sections. Asynchronous memory copy
    operations can be explained in several different contexts: the interactions between
    software abstractions (for example, that participating host memory must be pinned),
    different hardware implementations, API support for the feature, and optimization
    strategies. Readers sometimes may wish to consult the index and read all of the
    different presentations on a given topic.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以一种可管理的方式呈现这种复杂性，大多数话题都会从不同的角度多次解释。*纹理映射硬件做什么？* 是与 *如何编写执行纹理映射的内核？* 不同的问题。本书在不同的章节中分别解决这两个问题。异步内存复制操作可以在几种不同的背景下解释：软件抽象之间的交互（例如，参与的主机内存必须被固定）、不同的硬件实现、API对该特性的支持以及优化策略。读者有时可能希望查阅索引，阅读给定主题的所有不同呈现方式。
- en: 'Optimization guides are like advice columns: Too often, the guidance is offered
    without enough context to be applied meaningfully, and they often seem to contradict
    themselves. That observation isn’t intended to be pejorative; it’s just a symptom
    of the complexity of the problem. It has been at least 20 years since blanket
    generalizations could be made about CPU optimizations, and GPUs are more complicated
    to program, so it’s unrealistic to expect CUDA optimization advice to be simple.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 优化指南就像建议专栏：很多时候，指导意见没有足够的背景信息，无法有意义地应用，而且它们常常互相矛盾。这个观察并不是贬义的；它只是问题复杂性的一个表现。自从至少20年前可以对CPU优化做出一刀切的概括以来，GPU的编程变得更加复杂，因此，期望CUDA优化建议能够简单化是不现实的。
- en: Additionally, GPU computing is so new that GPU architects, let alone developers,
    are still learning how best to program them. For CUDA developers, the ultimate
    arbiter is usually performance, and performance is usually measured in wall clock
    time! Recommendations on grid and block sizes, how and when to use shared memory,
    how many results to compute per thread, and the implications of occupancy on performance
    should be confirmed empirically by implementing different approaches and measuring
    the performance of each.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，GPU计算如此新颖，以至于GPU架构师，更不用说开发者，仍在学习如何最好地编程。对于CUDA开发者来说，最终的裁定标准通常是性能，而性能通常是通过墙上时钟时间来衡量的！关于网格和块大小、如何以及何时使用共享内存、每个线程计算多少结果以及占用率对性能的影响等建议，应该通过实施不同的方法并测量每种方法的性能来进行经验验证。
- en: 1.2\. Code
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 代码
- en: 'Developers want CUDA code that is illustrative yet not a toy; useful but does
    not require a technical dive into a far-afield topic; and high performance but
    does not obscure the path taken by implementors from their initial port to the
    final version. To that end, this book presents three types of code examples designed
    to address each of those considerations: microbenchmarks, microdemos, and optimization
    journeys.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员希望CUDA代码既能起到说明作用，又不至于成为玩具；有用，但不需要深入探讨一个偏远的技术主题；性能高，但不会掩盖实现者从初步移植到最终版本的过程。为此，本书提供了三种类型的代码示例，旨在解决这些考虑因素：微基准、微演示和优化历程。
- en: 1.2.1\. Microbenchmarks
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.1\. 微基准
- en: Microbenchmarks are designed to illustrate the performance implications of a
    very specific CUDA question, such as how uncoalesced memory transactions degrade
    device memory bandwidth or the amount of time it takes the WDDM driver to perform
    a kernel thunk. They are designed to be compiled standalone and will look familiar
    to many CUDA programmers who’ve already implemented microbenchmarks of their own.
    In a sense, I wrote a set of microbenchmarks to obviate the need for other people
    to do the same.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 微基准旨在说明非常具体的CUDA问题的性能影响，例如不合并的内存事务如何降低设备内存带宽，或者WDDM驱动程序执行内核调用所需的时间。它们设计为可以独立编译，并且对于许多已经实现过自己微基准的CUDA程序员来说会显得非常熟悉。从某种意义上说，我编写了一套微基准，以避免其他人做同样的事情。
- en: 1.2.2\. Microdemos
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.2\. 微演示
- en: Microdemos are small applications designed to shed light on specific questions
    of how the hardware or software behaves. Like microbenchmarks, they are small
    and self-contained, but instead of highlighting a performance question, they highlight
    a question of functionality. For example, the chapter on texturing includes microdemos
    that illustrate how to texture from 1D device memory, how the float→int conversion
    is performed, how different texture addressing modes work, and how the linear
    interpolation performed by texture is affected by the 9-bit weights.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 微演示是小型应用程序，旨在阐明硬件或软件行为的具体问题。与微基准类似，它们是小型且自包含的，但它们突出的不是性能问题，而是功能性问题。例如，关于纹理章节包括了微演示，展示了如何从1D设备内存纹理、如何执行float→int转换、不同的纹理寻址模式如何工作，以及纹理执行的线性插值如何受到9位权重的影响。
- en: Like the microbenchmarks, these microdemos are offered in the spirit in which
    developers probably wanted to write them, or at least have them available. I wrote
    them so you don’t have to!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 和微基准一样，这些微演示是以开发者可能希望编写它们的精神来提供的，或者至少希望它们可用。我编写了这些代码，以便你无需亲自编写！
- en: 1.2.3\. Optimization Journeys
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.2.3\. 优化历程
- en: Many papers on CUDA present their results as a *fait accompli*, perhaps with
    some side comments on tradeoffs between different approaches that were investigated
    before settling on the final approach presented in the paper. Authors often have
    length limits and deadlines that work against presenting more complete treatments
    of their work.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 许多关于CUDA的论文将其结果呈现为*既成事实*，可能会附带一些关于在最终方案确定之前，探索不同方法之间权衡的旁注。作者通常面临篇幅限制和截止日期的压力，这使得他们难以呈现对其工作的更全面阐述。
- en: For some select topics central to the data parallel programming enabled by CUDA,
    this book includes *optimization journeys* in the spirit of Mark Harris’s “Optimizing
    Parallel Reduction in CUDA” presentation that walks the reader through seven increasingly
    complex implementations of increasing performance.^([1](ch01.html#ch01fn1)) The
    topics we’ve chosen to address this way include reduction, parallel prefix sum
    (“scan”), and the N-body problem.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CUDA启用的数据并行编程中的一些核心主题，本书包括了*优化历程*，其精神来源于Mark Harris的“优化CUDA中的并行归约”演示，带领读者逐步体验七种越来越复杂的实现，以提升性能。^([1](ch01.html#ch01fn1))
    我们选择用这种方式探讨的主题包括归约、并行前缀和（“扫描”）以及N体问题。
- en: '[1](ch01.html#ch01fn1a). [http://bit.ly/Z2q37x](http://bit.ly/Z2q37x)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch01.html#ch01fn1a). [http://bit.ly/Z2q37x](http://bit.ly/Z2q37x)'
- en: 1.3\. Administrative Items
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 管理事项
- en: 1.3.1\. Open Source
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.1\. 开源
- en: The source code that accompanies this book is available on [www.cudahandbook.com](http://www.cudahandbook.com),
    and it is open source, copyrighted with the 2-clause BSD license.^([2](ch01.html#ch01fn2))
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本书附带的源代码可在[www.cudahandbook.com](http://www.cudahandbook.com)获取，它是开源的，并且使用2条款BSD许可证进行版权保护。^([2](ch01.html#ch01fn2))
- en: '[2](ch01.html#ch01fn2a). [www.opensource.org/licenses/bsd-license.php](http://www.opensource.org/licenses/bsd-license.php)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch01.html#ch01fn2a). [www.opensource.org/licenses/bsd-license.php](http://www.opensource.org/licenses/bsd-license.php)'
- en: 1.3.2\. CUDA Handbook Library (Chlib)
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.2\. CUDA手册库（Chlib）
- en: The CUDA Handbook Library, located in the `chLib/` directory of the source code,
    contains a portable library with support for timing, threading, driver API utilities,
    and more. They are described in more detail in [Appendix A](app01.html#app01).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 位于源代码的`chLib/`目录中的CUDA手册库包含一个可移植的库，支持计时、线程、驱动程序API工具等。它们在[附录A](app01.html#app01)中有更详细的描述。
- en: 1.3.3\. Coding Style
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.3\. 编码风格
- en: Arguments over brace placement aside, the main feature of the code in this book
    that will engender comment is the `goto`-based error handling mechanism. Functions
    that perform multiple resource allocations (or other operations that might fail,
    and where failure should be propagated to the caller) are structured around an
    Initialize / ErrorCheck / Cleanup idiom, similar to a pattern commonly used in
    Linux kernel code.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 不谈括号位置的争论，本书中最引人注目的代码特性是基于`goto`的错误处理机制。执行多个资源分配（或其他可能失败的操作，并且失败应该传递给调用者）的函数，结构上采用初始化/错误检查/清理模式，这与Linux内核代码中常用的模式类似。
- en: On failure, all cleanup is performed by the same body of code at the end of
    the function. It is important to initialize the resources to guaranteed-invalid
    values at the top of the function, so the cleanup code knows which resources must
    be freed. If a resource allocation or other function fails, the code performs
    a `goto` the cleanup code. `chError.h`, described in [Section A.6](app01.html#app01lev1sec6),
    defines error-handling macros for the CUDA runtime and the driver API that implement
    this idiom.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当出现故障时，所有清理操作将在函数末尾由相同的代码块执行。重要的是，在函数开始时将资源初始化为保证无效的值，以便清理代码知道哪些资源必须释放。如果资源分配或其他函数失败，代码会执行`goto`跳转到清理代码。`chError.h`，在[第A.6节](app01.html#app01lev1sec6)中描述，定义了CUDA运行时和驱动程序API的错误处理宏，这些宏实现了这种习惯用法。
- en: 1.3.4\. CUDA SDK
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1.3.4. CUDA SDK
- en: The SDK is a shared experience for all CUDA developers, so we assume you’ve
    installed the CUDA SDK and that you can build CUDA programs with it. The SDK also
    includes the GLUT (GL Utility Library), a convenient library that enables OpenGL
    applications to target a variety of operating systems from the same code base.
    GLUT is designed to build demo-quality as opposed to production-quality applications,
    but it fits the bill for our needs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: SDK是所有CUDA开发者的共享体验，因此我们假设你已经安装了CUDA SDK，并且能够使用它构建CUDA程序。SDK还包括GLUT（GL实用库），它是一个方便的库，可以使OpenGL应用程序通过相同的代码库针对多种操作系统。GLUT旨在构建演示质量的应用程序，而非生产质量的应用程序，但它完全满足我们的需求。
- en: 1.4\. Road Map
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4. 路线图
- en: The remaining chapters in [Part I](part01.html#part01) provide architectural
    overviews of CUDA hardware and software.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[第一部分](part01.html#part01)中的剩余章节提供了CUDA硬件和软件的架构概述。'
- en: • [Chapter 2](ch02.html#ch02) details both the CUDA hardware platforms and the
    GPUs themselves.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: • [第2章](ch02.html#ch02)详细介绍了CUDA硬件平台和GPU本身。
- en: • [Chapter 3](ch03.html#ch03) similarly covers the CUDA software architecture.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: • [第3章](ch03.html#ch03)同样讨论了CUDA软件架构。
- en: • [Chapter 4](ch04.html#ch04) covers the CUDA software environment, including
    descriptions of CUDA software tools and Amazon’s EC2 environment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: • [第4章](ch04.html#ch04)涵盖了CUDA软件环境，包括CUDA软件工具和亚马逊EC2环境的描述。
- en: In [Part II](part02.html#part02), [Chapters 5](ch05.html#ch05) to [10](ch10.html#ch10)
    cover various aspects of the CUDA programming model in great depth.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二部分](part02.html#part02)中，[第5章](ch05.html#ch05)到[第10章](ch10.html#ch10)深入讨论了CUDA编程模型的各个方面。
- en: • [Chapter 5](ch05.html#ch05) covers memory, including device memory, constant
    memory, shared memory, and texture memory.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: • [第5章](ch05.html#ch05)讨论了内存，包括设备内存、常量内存、共享内存和纹理内存。
- en: • [Chapter 6](ch06.html#ch06) covers streams and events—the mechanisms used
    for “coarse-grained” parallelism between the CPU and GPU, between hardware units
    of the GPU such as copy engines and the streaming multiprocessors, or between
    discrete GPUs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: • [第6章](ch06.html#ch06)介绍了流和事件——用于CPU与GPU之间、GPU硬件单元（如复制引擎与流多处理器之间）或离散GPU之间进行“粗粒度”并行的机制。
- en: • [Chapter 7](ch07.html#ch07) covers kernel execution, including the dynamic
    parallelism feature that is new in SM 3.5 and CUDA 5.0.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: • [第7章](ch07.html#ch07)涵盖了内核执行，包括SM 3.5和CUDA 5.0中新引入的动态并行性特性。
- en: • [Chapter 8](ch08.html#ch08) covers every aspect of streaming multiprocessors.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: • [第8章](ch08.html#ch08)涵盖了流式多处理器的各个方面。
- en: • [Chapter 9](ch09.html#ch09) covers multi-GPU applications, including peer-to-peer
    operations and embarrassingly parallel operations, with N-body as an example.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: • [第9章](ch09.html#ch09)涵盖了多GPU应用，包括点对点操作和非常并行的操作，以N-body为例。
- en: • [Chapter 10](ch10.html#ch10) covers every aspect of CUDA texturing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: • [第10章](ch10.html#ch10)涵盖了CUDA纹理处理的各个方面。
- en: Finally, in [Part III](part03.html#part03), [Chapters 11](ch11.html#ch11) to
    [15](ch15.html#ch15) discuss various targeted CUDA applications.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在[第III部分](part03.html#part03)，[第11章](ch11.html#ch11)到[第15章](ch15.html#ch15)讨论了各种针对CUDA的应用。
- en: • [Chapter 11](ch11.html#ch11) describes bandwidth-bound, streaming workloads
    such as vector-vector multiplication.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: • [第11章](ch11.html#ch11)描述了带宽受限的流式工作负载，例如向量-向量乘法。
- en: • [Chapters 12](ch12.html#ch12) and [13](ch13.html#ch13) describe reduction
    and parallel prefix sum (otherwise known as scan), both important building blocks
    in parallel programming.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: • [第12章](ch12.html#ch12)和[第13章](ch13.html#ch13)描述了归约和并行前缀和（即扫描），这两者是并行编程中的重要构建模块。
- en: • [Chapter 14](ch14.html#ch14) describes N-body, an important family of applications
    with high computational density that derive a particular benefit from GPU computing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: • [第14章](ch14.html#ch14)描述了N-body，这是一个重要的应用类别，具有高计算密度，从GPU计算中获得了特别的好处。
- en: • [Chapter 15](ch15.html#ch15) takes an in-depth look at an image processing
    operation called *normalized cross-correlation* that is used for feature extraction.
    [Chapter 15](ch15.html#ch15) features the only code in the book that uses texturing
    and shared memory together to deliver optimal performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: • [第15章](ch15.html#ch15)深入探讨了一种图像处理操作，称为*归一化互相关*，用于特征提取。[第15章](ch15.html#ch15)展示了本书中唯一使用纹理和共享内存结合来提供最佳性能的代码。
- en: Chapter 2\. Hardware Architecture
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2章 硬件架构
- en: 'This chapter provides more detailed descriptions of CUDA platforms, from the
    system level to the functional units within the GPUs. The first section discusses
    the many different ways that CUDA systems can be built. The second section discusses
    address spaces and how CUDA’s memory model is implemented in hardware and software.
    The third section discusses CPU/GPU interactions, with special attention paid
    to how commands are submitted to the GPU and how CPU/GPU synchronization is performed.
    Finally, the chapter concludes with a high-level description of the GPUs themselves:
    functional units such as copy engines and streaming multiprocessors, with block
    diagrams of the different types of streaming multiprocessors over three generations
    of CUDA-capable hardware.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了关于 CUDA 平台的更详细描述，从系统级别到 GPU 内部的功能单元。第一部分讨论了构建 CUDA 系统的多种方式。第二部分讨论了地址空间以及
    CUDA 的内存模型如何在硬件和软件中实现。第三部分讨论了 CPU/GPU 交互，特别关注如何将命令提交给 GPU，以及如何进行 CPU/GPU 同步。最后，本章以对
    GPU 本身的高级描述结束：功能单元，如拷贝引擎和流式多处理器，并展示了三代支持 CUDA 的硬件中不同类型流式多处理器的框图。
- en: 2.1\. CPU Configurations
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1. CPU 配置
- en: This section describes a variety of CPU/GPU architectures, with some comments
    on how a CUDA developer would approach programming the system differently. We
    examine a variety of CPU configurations, integrated GPUs, and multi-GPU configurations.
    We begin with [Figure 2.1](ch02.html#ch02fig01).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了多种 CPU/GPU 架构，并对 CUDA 开发者如何以不同方式编程系统进行了评论。我们研究了各种 CPU 配置、集成 GPU 和多 GPU
    配置。我们从[图 2.1](ch02.html#ch02fig01)开始。
- en: '![Image](graphics/02fig01.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/02fig01.jpg)'
- en: '*Figure 2.1* CPU/GPU architecture simplified.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.1* CPU/GPU 架构简化图。'
- en: An important element that was omitted from [Figure 2.1](ch02.html#ch02fig01)
    is the “chipset” or “core logic” that connects the CPU to the outside world. Every
    bit of input and output of the system, including disk and network controllers,
    keyboards and mice, USB devices, and, yes, GPUs, goes through the chipset. Until
    recently, chipsets were divided into a “southbridge” that connected most peripherals
    to the system^([1](ch02.html#ch02fn1)) and a “northbridge” that contained the
    graphics bus (the Accelerated Graphics Port, until the PCI Express [PCIe] bus
    displaced it) and a memory controller (“front side bus”) connected to the CPU
    memory.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 2.1](ch02.html#ch02fig01)中省略了一个重要元素，即连接 CPU 和外部世界的“芯片组”或“核心逻辑”。系统的每一位输入和输出，包括磁盘和网络控制器、键盘和鼠标、USB
    设备，当然还有 GPU，都会经过芯片组。直到最近，芯片组被划分为连接大多数外设的“南桥”（southbridge）和包含图形总线（加速图形端口，直到 PCI
    Express [PCIe] 总线取而代之）以及连接到 CPU 内存的内存控制器（“前端总线”）的“北桥”（northbridge）。
- en: '[1](ch02.html#ch02fn1a). For simplicity, the southbridge is omitted from all
    diagrams in this section.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch02.html#ch02fn1a)。为了简化起见，本节中的所有图示均省略了南桥部分。'
- en: Each “lane” in PCI Express 2.0 can theoretically deliver about 500MB/s of bandwidth,
    and the number of lanes for a given peripheral can be 1, 4, 8, or 16\. GPUs require
    the most bandwidth of any peripheral on the platform, so they generally are designed
    to be plugged into 16-lane PCIe slots. With packet overhead, the 8G/s of bandwidth
    for such a connection delivers about 6G/s in practice.^([2](ch02.html#ch02fn2))
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PCI Express 2.0 中，每个“通道”理论上可以提供约 500MB/s 的带宽，给定外设的通道数可以是 1、4、8 或 16。GPU 是平台上带宽需求最大的外设，因此它们通常设计为插入
    16 通道的 PCIe 插槽中。考虑到数据包开销，虽然这种连接的带宽为 8G/s，但实际可用带宽大约为 6G/s。^([2](ch02.html#ch02fn2))
- en: '[2](ch02.html#ch02fn2a). PCI 3.0 delivers about twice as much bandwidth as
    PCIe 2.0.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch02.html#ch02fn2a). PCI 3.0 提供的带宽大约是 PCIe 2.0 的两倍。'
- en: 2.1.1\. Front-Side Bus
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 前端总线
- en: '[Figure 2.2](ch02.html#ch02fig02) adds the northbridge and its memory controller
    to the original simplified diagram. For completeness, [Figure 2.2](ch02.html#ch02fig02)
    also shows the GPU’s integrated memory controller, which is designed under a very
    different set of constraints than the CPU’s memory controller. The GPU must accommodate
    so-called *isochronous* clients, such as video display(s), whose bandwidth requirements
    are fixed and nonnegotiable. The GPU’s memory controller also is designed with
    the GPU’s extreme latency-tolerance and vast memory bandwidth requirements in
    mind. As of this writing, high-end GPUs commonly deliver local GPU memory bandwidths
    well in excess of 100G/s. GPU memory controllers are always integrated with the
    GPU, so they are omitted from the rest of the diagrams in this chapter.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.2](ch02.html#ch02fig02) 在原始简化图中增加了北桥及其内存控制器。为完整起见，[图 2.2](ch02.html#ch02fig02)
    还展示了 GPU 的集成内存控制器，它是在与 CPU 内存控制器完全不同的约束条件下设计的。GPU 必须支持所谓的*同步*客户端，例如视频显示器，它们的带宽需求是固定的且不可协商。GPU
    的内存控制器还特别考虑了 GPU 对极高延迟容忍度和巨大的内存带宽需求的设计。截止目前，顶级 GPU 通常提供超过 100G/s 的本地 GPU 内存带宽。GPU
    内存控制器总是与 GPU 集成，因此在本章其余图示中未显示。'
- en: '![Image](graphics/02fig02.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig02.jpg)'
- en: '*Figure 2.2* CPU/GPU architecture—northbridge.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.2* CPU/GPU 架构—北桥。'
- en: 2.1.2\. Symmetric Multiprocessors
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 对称多处理器
- en: '[Figure 2.3](ch02.html#ch02fig03) shows a system with multiple CPUs in a traditional
    northbridge configuration.^([3](ch02.html#ch02fn3)) Before multicore processors,
    applications had to use multiple threads to take full advantage of the additional
    power of multiple CPUs. The northbridge must ensure that each CPU sees the same
    coherent view of memory, even though every CPU and the northbridge itself all
    contain caches. Since these so-called “symmetric multiprocessor” (SMP) systems
    share a common path to CPU memory, memory accesses exhibit relatively uniform
    performance.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.3](ch02.html#ch02fig03)展示了一个传统北桥配置的多 CPU 系统^([3](ch02.html#ch02fn3))。在多核处理器出现之前，应用程序必须使用多个线程才能充分利用多个
    CPU 的额外计算能力。北桥必须确保每个 CPU 都能看到相同的一致内存视图，尽管每个 CPU 和北桥本身都包含缓存。由于这些所谓的“对称多处理器”（SMP）系统共享到
    CPU 内存的公共路径，内存访问表现出相对一致的性能。'
- en: '[3](ch02.html#ch02fn3a). For reasons that will soon become clear, we offer
    [Figure 2.3](ch02.html#ch02fig03) more for historical reference than because there
    are CUDA-capable computers with this configuration.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch02.html#ch02fn3a)。由于一些即将阐明的原因，我们提供[图 2.3](ch02.html#ch02fig03)更多是为了历史参考，而不是因为有具有这种配置的CUDA计算机。'
- en: '![Image](graphics/02fig03.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig03.jpg)'
- en: '*Figure 2.3* Multiple CPUs (SMP configuration).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.3* 多个 CPU（SMP 配置）。'
- en: 2.1.3\. Nonuniform Memory Access
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. 非统一内存访问
- en: Starting with AMD’s Opteron and Intel’s Nehalem (i7) processors, the memory
    controller in the northbridge was integrated directly into the CPU, as shown in
    [Figure 2.4](ch02.html#ch02fig04). This architectural change improves CPU memory
    performance.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从 AMD 的 Opteron 和 Intel 的 Nehalem (i7) 处理器开始，北桥中的内存控制器被直接集成到 CPU 中，如[图 2.4](ch02.html#ch02fig04)所示。这一架构变更提高了
    CPU 的内存性能。
- en: '![Image](graphics/02fig04.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig04.jpg)'
- en: '*Figure 2.4* CPU with integrated memory controller.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.4* 集成内存控制器的 CPU。'
- en: For developers, the system in [Figure 2.4](ch02.html#ch02fig04) is only slightly
    different from the ones we’ve already discussed. For systems that contain multiple
    CPUs, as shown in [Figure 2.5](ch02.html#ch02fig05), things get more interesting.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开发者，[图 2.4](ch02.html#ch02fig04)中的系统与我们已经讨论的系统仅有细微的差别。对于包含多个 CPU 的系统，如[图 2.5](ch02.html#ch02fig05)所示，情况变得更加有趣。
- en: '![Image](graphics/02fig05.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig05.jpg)'
- en: '*Figure 2.5* Multiple CPUs (NUMA).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.5* 多个 CPU（NUMA）。'
- en: For machine configurations with multiple CPUs,^([4](ch02.html#ch02fn4)) this
    architecture implies that each CPU gets its own pool of memory bandwidth. At the
    same time, because multithreaded operating systems and applications rely on the
    cache coherency enforced by previous CPUs and northbridge configurations, the
    Opteron and Nehalem architectures also introduced HyperTransport (HT) and QuickPath
    Interconnect (QPI), respectively.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有多个 CPU 的机器配置^([4](ch02.html#ch02fn4))，这一架构意味着每个 CPU 都拥有自己的内存带宽池。与此同时，由于多线程操作系统和应用程序依赖于先前的
    CPU 和北桥配置强制执行的缓存一致性，Opteron 和 Nehalem 架构分别引入了 HyperTransport (HT) 和 QuickPath
    Interconnect (QPI)。
- en: '[4](ch02.html#ch02fn4a). On such systems, the CPUs also may be referred to
    as “nodes” or “sockets.”'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch02.html#ch02fn4a)。在这种系统上，CPU 也可能被称为“节点”或“插槽”。'
- en: HT and QPI are point-to-point interconnects that connect CPUs to other CPUs,
    or CPUs to I/O hubs. On systems that incorporate HT/QPI, any CPU can access any
    memory location, but accesses are much faster to “local” memory locations whose
    physical address is in the memory directly attached to the CPU. Nonlocal accesses
    are resolved by using HT/QPI to snoop the caches of other CPUs, evict any cached
    copies of the requested data, and deliver the data to the CPU that performed the
    memory request. In general, the enormous on-chip caches on these CPUs mitigate
    the cost of these nonlocal memory accesses; the requesting CPU can keep the data
    in its own cache hierarchy until the memory is requested by another CPU.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: HT 和 QPI 是点对点互连，它们将 CPU 连接到其他 CPU，或将 CPU 连接到 I/O 中心。在包含 HT/QPI 的系统中，任何 CPU 都可以访问任何内存位置，但访问“本地”内存位置的速度要快得多，这些本地内存的物理地址直接连接到
    CPU。非本地访问通过使用 HT/QPI 来窥探其他 CPU 的缓存，逐出任何缓存的请求数据副本，并将数据传送到执行内存请求的 CPU。一般而言，这些 CPU
    上巨大的片上缓存缓解了这些非本地内存访问的成本；请求的 CPU 可以将数据保留在自己的缓存层级中，直到另一个 CPU 请求该内存。
- en: To help developers work around these performance pitfalls, Windows and Linux
    have introduced APIs to enable applications to steer their allocations toward
    specific CPUs and to set CPU “thread affinities” so the operating system schedules
    threads onto CPUs so most or all of their memory accesses will be local.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助开发者绕过这些性能陷阱，Windows 和 Linux 引入了 API，使应用程序能够将其分配引导到特定的 CPU，并设置 CPU “线程亲和性”，以便操作系统将线程调度到
    CPU 上，使它们的大部分或所有内存访问都是本地的。
- en: 'A determined programmer can use these APIs to write contrived code that exposes
    the performance vulnerabilities of NUMA, but the more common (and insidious!)
    symptom is a slowdown due to “false sharing” where two threads running on different
    CPUs cause a plethora of HT/QPI transactions by accessing memory locations that
    are in the same cache line. So NUMA APIs must be used with caution: Although they
    give programmers the tools to improve performance, they also can make it easy
    for developers to inflict performance problems on themselves.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有决心的程序员可以利用这些 API 编写特殊代码，暴露 NUMA 性能的漏洞，但更常见（且隐蔽！）的症状是由于“虚假共享”导致的性能下降，其中在不同
    CPU 上运行的两个线程通过访问位于同一缓存行的内存位置，导致大量 HT/QPI 事务。因此，NUMA API 必须谨慎使用：尽管它们为程序员提供了提高性能的工具，但也可能使开发者很容易给自己带来性能问题。
- en: One approach to mitigating the performance impact of nonlocal memory accesses
    is to enable *memory interleaving*, in which physical memory is evenly split between
    all CPUs on cache line boundaries.^([5](ch02.html#ch02fn5)) For CUDA, this approach
    works well on systems that are designed exactly as shown in [Figure 2.5](ch02.html#ch02fig05),
    with multiple CPUs in a NUMA configuration connected by a shared I/O hub to the
    GPU(s). Since PCI Express bandwidth is often a bottleneck to overall application
    performance, however, many systems have separate I/O hubs to service more than
    one PCI Express bus, as shown in [Figure 2.6](ch02.html#ch02fig06).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 减轻非本地内存访问性能影响的一种方法是启用*内存交错*，即物理内存在所有 CPU 之间按缓存行边界均匀划分。^([5](ch02.html#ch02fn5))
    对于 CUDA 来说，这种方法在按照 [图 2.5](ch02.html#ch02fig05) 所示的系统中效果良好，该系统在 NUMA 配置下有多个 CPU，并通过共享
    I/O 集线器与 GPU(s) 相连。然而，由于 PCI Express 带宽常常成为整体应用性能的瓶颈，许多系统拥有独立的 I/O 集线器来服务多个 PCI
    Express 总线，如 [图 2.6](ch02.html#ch02fig06) 所示。
- en: '[5](ch02.html#ch02fn5a). A cynic would say this makes all memory accesses “equally
    bad.”'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch02.html#ch02fn5a)。一个愤世嫉俗者可能会说，这让所有的内存访问“同样糟糕”。'
- en: '![Image](graphics/02fig06.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig06.jpg)'
- en: '*Figure 2.6* Multi-CPU (NUMA configuration), multiple buses.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.6* 多核 CPU（NUMA 配置），多个总线。'
- en: In order to run well on such “affinitized” systems, CUDA applications must take
    care to use NUMA APIs to match memory allocations and thread affinities to the
    PCI Express bus attached to a given GPU. Otherwise, memory copies initiated by
    the GPU(s) are nonlocal, and the memory transactions take an extra “hop” over
    the HT/QPI interconnect. Since GPUs demand a huge amount of bandwidth, these DMA
    operations reduce the ability of HT/QPI to serve its primary purpose. Compared
    to false sharing, the performance impact of nonlocal GPU memory copies is a much
    more plausible performance risk for CUDA applications.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在这种“亲和性”系统上良好运行，CUDA 应用程序必须小心使用 NUMA API，将内存分配和线程亲和性与连接到给定 GPU 的 PCI Express
    总线匹配。否则，GPU 发起的内存拷贝将是非本地的，并且内存事务需要通过 HT/QPI 互连多“跳”一次。由于 GPU 对带宽的需求非常大，这些 DMA 操作会减少
    HT/QPI 执行其主要任务的能力。与虚假共享相比，非本地 GPU 内存拷贝对性能的影响是 CUDA 应用程序更为显著的性能风险。
- en: 2.1.4\. PCI Express Integration
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. PCI Express 集成
- en: Intel’s Sandy Bridge class processors take another step toward full system integration
    by integrating the I/O hub into the CPU, as shown in [Figure 2.7](ch02.html#ch02fig07).
    A single Sandy Bridge CPU has up to 40 lanes of PCI Express bandwidth (remember
    that one GPU can use up to 16 lanes, so 40 are enough for more than two full-size
    GPUs).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔的 Sandy Bridge 类处理器通过将 I/O 集线器集成到 CPU 中，迈出了系统集成的又一步，如 [图 2.7](ch02.html#ch02fig07)
    所示。单个 Sandy Bridge CPU 可拥有最多 40 条 PCI Express 带宽（记住，一个 GPU 最多使用 16 条带宽，因此 40 条足够支持两个以上的全尺寸
    GPU）。
- en: '![Image](graphics/02fig07.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig07.jpg)'
- en: '*Figure 2.7* Multi-CPU with integrated PCI Express.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.7* 集成 PCI Express 的多核 CPU。'
- en: 'For CUDA developers, PCI Express integration brings bad news and good news.
    The bad news is that PCI Express traffic is always affinitized. Designers cannot
    build systems like the system in [Figure 2.5](ch02.html#ch02fig05), where a single
    I/O hub serves multiple CPUs; all multi-CPU systems resemble [Figure 2.6](ch02.html#ch02fig06).
    As a result, GPUs associated with different CPUs cannot perform peer-to-peer operations.
    The good news is that the CPU cache can participate in PCI Express bus traffic:
    The CPU can service DMA read requests out of cache, and writes by the GPU are
    posted to the CPU cache.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CUDA开发者而言，PCI Express集成带来了坏消息和好消息。坏消息是PCI Express流量总是具有亲和性。设计者无法像[图 2.5](ch02.html#ch02fig05)中的系统那样，使用单一I/O中心来服务多个CPU；所有多CPU系统都类似于[图
    2.6](ch02.html#ch02fig06)。因此，关联到不同CPU的GPU无法执行点对点操作。好消息是CPU缓存可以参与PCI Express总线流量：CPU可以从缓存中服务DMA读取请求，GPU的写入操作会被提交到CPU缓存。
- en: 2.2\. Integrated GPUs
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 集成GPU
- en: Here, the term *integrated* means “integrated into the chipset.” As [Figure
    2.8](ch02.html#ch02fig08) shows, the memory pool that previously belonged only
    to the CPU is now shared between the CPU and the GPU that is integrated into the
    chipset. Examples of NVIDIA chipsets with CUDA-capable GPUs include the MCP79
    (for laptops and netbooks) and MCP89\. MCP89 is the last and greatest CUDA-capable
    x86 chipset that NVIDIA will manufacture; besides an integrated L3 cache, it has
    3x as many SMs as the MCP7x chipsets.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*集成*的意思是“集成到芯片组中”。如[图 2.8](ch02.html#ch02fig08)所示，之前仅属于CPU的内存池现在与集成到芯片组中的GPU共享。NVIDIA具有CUDA能力的芯片组示例包括MCP79（用于笔记本电脑和上网本）和MCP89。MCP89是NVIDIA将制造的最后一款也是最强大的CUDA能力x86芯片组；除了集成L3缓存外，它还拥有MCP7x芯片组的3倍SM数。
- en: '![Image](graphics/02fig08.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig08.jpg)'
- en: '*Figure 2.8* Integrated GPU.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.8* 集成GPU。'
- en: CUDA’s APIs for mapped pinned memory have special meaning on integrated GPUs.
    These APIs, which map host memory allocations into the address space of CUDA kernels
    so they can be accessed directly, also are known as “zero-copy,” because the memory
    is shared and need not be copied over the bus. In fact, for transfer-bound workloads,
    an integrated GPU can outperform a much larger discrete GPU.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA的映射固定内存API在集成GPU上具有特殊意义。这些API将主机内存分配映射到CUDA内核的地址空间，以便它们可以直接访问，这也被称为“零拷贝”，因为内存是共享的，不需要通过总线进行复制。实际上，对于传输受限的工作负载，集成GPU的性能可以超越更大的独立GPU。
- en: “Write-combined” memory allocations also have significance on integrated GPUs;
    cache snoops to the CPU are inhibited on this memory, which increases GPU performance
    when accessing the memory. Of course, if the CPU reads from the memory, the usual
    performance penalties for WC memory apply.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: “写合并”内存分配对集成GPU也有重要意义；CPU对该内存的缓存窥探会被禁止，这在GPU访问内存时能够提高性能。当然，如果CPU从该内存读取数据，则会出现通常的WC内存性能惩罚。
- en: Integrated GPUs are not mutually exclusive with discrete ones; the MCP7x and
    MCP89 chipsets provide for PCI Express connections ([Figure 2.9](ch02.html#ch02fig09)).
    On such systems, CUDA prefers to run on the discrete GPU(s) because most CUDA
    applications are authored with them in mind. For example, a CUDA application designed
    to run on a single GPU will automatically select the discrete one.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 集成GPU与离散GPU并不互相排斥；MCP7x和MCP89芯片组提供了PCI Express连接（[图 2.9](ch02.html#ch02fig09)）。在这样的系统上，CUDA更倾向于在离散GPU上运行，因为大多数CUDA应用程序都是以离散GPU为设计目标的。例如，设计为在单一GPU上运行的CUDA应用程序将自动选择离散GPU。
- en: '![Image](graphics/02fig09.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig09.jpg)'
- en: '*Figure 2.9* Integrated GPU with discrete GPU(s).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.9* 集成GPU与离散GPU。'
- en: CUDA applications can query whether a GPU is integrated by examining `cudaDeviceProp.integrated`
    or by passing `CU_DEVICE_ATTRIBUTE_INTEGRATED` to `cuDeviceGetAttribute()`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA应用程序可以通过检查`cudaDeviceProp.integrated`或传递`CU_DEVICE_ATTRIBUTE_INTEGRATED`给`cuDeviceGetAttribute()`来查询GPU是否是集成的。
- en: For CUDA, integrated GPUs are not exactly a rarity; millions of computers have
    integrated, CUDA-capable GPUs on board, but they are something of a curiosity,
    and in a few years, they will be an anachronism because NVIDIA has exited the
    x86 chipset business. That said, NVIDIA has announced its intention to ship systems
    on a chip (SOCs) that integrate CUDA-capable GPUs with ARM CPUs, and it is a safe
    bet that zero-copy optimizations will work well on those systems.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CUDA来说，集成GPU并不罕见；数百万台计算机都配备了集成的、支持CUDA的GPU，但它们有些像是一种新奇事物，在几年后它们将成为过时的产物，因为NVIDIA已退出了x86芯片组业务。也就是说，NVIDIA已经宣布计划推出将支持CUDA的GPU与ARM
    CPU集成的系统级芯片（SOC），可以肯定的是，零拷贝优化将在这些系统上表现良好。
- en: 2.3\. Multiple GPUs
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 多个GPU
- en: This section explores the different ways that multiple GPUs can be installed
    in a system and the implications for CUDA developers. For purposes of this discussion,
    we will omit GPU memory from our diagrams. Each GPU is assumed to be connected
    to its own dedicated memory.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了如何在系统中安装多个GPU以及这对CUDA开发者的影响。为了本讨论的目的，我们将省略GPU内存的内容。假设每个GPU都连接到其专用内存。
- en: Around 2004, NVIDIA introduced “SLI” (Scalable Link Interface) technology that
    enables multiple GPUs to deliver higher graphics performance by working in parallel.
    With motherboards that could accommodate multiple GPU boards, end users could
    nearly double their graphics performance by installing two GPUs in their system
    ([Figure 2.10](ch02.html#ch02fig10)). By default, the NVIDIA driver software configures
    these boards to behave as if they were a single, very fast GPU to accelerate graphics
    APIs such as Direct3D and OpenGL. End users who intend to use CUDA must explicitly
    enable it in the Display Control panel on Windows.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在2004年，NVIDIA推出了“SLI”（可扩展链接接口）技术，使多个GPU能够并行工作以提供更高的图形性能。通过可以容纳多个GPU板卡的主板，最终用户可以通过在系统中安装两个GPU来几乎翻倍其图形性能（见[图
    2.10](ch02.html#ch02fig10)）。默认情况下，NVIDIA驱动程序会将这些板卡配置为像单个高速GPU一样工作，以加速图形API，如Direct3D和OpenGL。打算使用CUDA的最终用户必须在Windows的显示控制面板中显式启用它。
- en: '![Image](graphics/02fig10.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig10.jpg)'
- en: '*Figure 2.10* GPUs in multiple slots.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.10* 多插槽中的GPU。'
- en: It also is possible to build GPU boards that hold multiple GPUs ([Figure 2.11](ch02.html#ch02fig11)).
    Examples of such boards include the GeForce 9800GX2 (dual-G92), the GeForce GTX
    295 (dual-GT200), the GeForce GTX 590 (dual-GF110), and the GeForce GTX 690 (dual-GK104).
    The only thing shared by the GPUs on these boards is a bridge chip that enables
    both chips to communicate via PCI Express. They do not share memory resources;
    each GPU has an integrated memory controller that gives full-bandwidth performance
    to the memory connected to that GPU. The GPUs on the board can communicate via
    peer-to-peer memcpy, which will use the bridge chip to bypass the main PCIe fabric.
    In addition, if they are Fermi-class or later GPUs, each GPU can map memory belonging
    to the other GPU into its global address space.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以构建包含多个GPU的GPU板卡（见[图 2.11](ch02.html#ch02fig11)）。这类板卡的示例包括GeForce 9800GX2（双G92）、GeForce
    GTX 295（双GT200）、GeForce GTX 590（双GF110）和GeForce GTX 690（双GK104）。这些板卡上的GPU之间唯一的共享资源是桥接芯片，它使得两个芯片能够通过PCI
    Express进行通信。它们不共享内存资源；每个GPU都有一个集成的内存控制器，能够为连接到该GPU的内存提供全带宽性能。板卡上的GPU可以通过点对点内存复制（peer-to-peer
    memcpy）进行通信，使用桥接芯片绕过主PCIe架构。此外，如果它们是Fermi架构或更新的GPU，每个GPU可以将属于另一个GPU的内存映射到其全局地址空间中。
- en: '![Image](graphics/02fig11.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig11.jpg)'
- en: '*Figure 2.11* Multi-GPU board.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.11* 多GPU板卡。'
- en: SLI is an NVIDIA technology that makes multiple GPUs (usually on the same board,
    as in [Figure 2.11](ch02.html#ch02fig11)) appear as a single, much faster GPU.
    When the graphics application downloads textures or other data, the NVIDIA graphics
    driver broadcasts the data to both GPUs; most rendering commands also are broadcast,
    with small changes to enable each GPU to render its part of the output buffer.
    Since SLI causes the multiple GPUs to appear as a single GPU, and since CUDA applications
    cannot be transparently accelerated like graphics applications, CUDA developers
    generally should disable SLI.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: SLI是NVIDIA的一项技术，使多个GPU（通常在同一块板上，如[图2.11](ch02.html#ch02fig11)所示）看起来像一个更快的单一GPU。当图形应用程序下载纹理或其他数据时，NVIDIA图形驱动程序会将数据广播到两个GPU；大多数渲染命令也会被广播，并进行小的更改以使每个GPU能够渲染其输出缓冲区的一部分。由于SLI使多个GPU看起来像一个单一的GPU，并且由于CUDA应用程序无法像图形应用程序那样透明加速，因此CUDA开发人员通常应禁用SLI。
- en: This board design oversubscribes the PCI Express bandwidth available to the
    GPUs. Since only one PCI Express slot’s worth of bandwidth is available to both
    GPUs on the board, the performance of transfer-limited workloads can suffer. If
    multiple PCI Express slots are available, an end user can install multiple dual-GPU
    boards. [Figure 2.12](ch02.html#ch02fig12) shows a machine with four GPUs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 该板设计超额订阅了可用于GPU的PCI Express带宽。由于板上的两个GPU仅可用一个PCI Express插槽的带宽，因此传输受限工作负载的性能可能会受到影响。如果有多个PCI
    Express插槽可用，最终用户可以安装多个双GPU板。[图2.12](ch02.html#ch02fig12)显示了一台配备四个GPU的机器。
- en: '![Image](graphics/02fig12.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/02fig12.jpg)'
- en: '*Figure 2.12* Multi-GPU boards in multiple slots.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.12* 多个插槽中的多GPU板。'
- en: If there are multiple PCI Express I/O hubs, as with the system in [Figure 2.6](ch02.html#ch02fig06),
    the placement and thread affinity considerations for NUMA systems apply to the
    boards just as they would to single-GPU boards plugged into that configuration.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有多个PCI Express I/O集线器，如[图2.6](ch02.html#ch02fig06)中的系统，NUMA系统的放置和线程亲和性考虑同样适用于这些板，就像它们适用于插入该配置的单GPU板一样。
- en: If the chipset, motherboard, operating system, and driver software can support
    it, even more GPUs can be crammed into the system. Researchers at the University
    of Antwerp caused a stir when they built an 8-GPU system called FASTRA by plugging
    four GeForce 9800GX2’s into a single desktop computer. A similar system built
    on a dual-PCI Express chipset would look like the one in [Figure 2.13](ch02.html#ch02fig13).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果芯片组、主板、操作系统和驱动程序软件能够支持，系统中可以安装更多的GPU。安特卫普大学的研究人员在将四个GeForce 9800GX2插入一台桌面计算机时，构建了一个名为FASTRA的8-GPU系统，引起了轰动。基于双PCI
    Express芯片组构建的类似系统将如[图2.13](ch02.html#ch02fig13)所示。
- en: '![Image](graphics/02fig13.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/02fig13.jpg)'
- en: '*Figure 2.13* Multi-GPU boards, multiple I/O hubs.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.13* 多GPU板，多个I/O集线器。'
- en: As a side note, peer-to-peer memory access (the mapping of other GPUs’ device
    memory, not memcpy) does not work across I/O hubs or, in the case of CPUs such
    as Sandy Bridge that integrate PCI Express, sockets.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 作为附注，点对点内存访问（即其他GPU设备内存的映射，而不是memcpy）无法跨越I/O集线器，或者在集成PCI Express的CPU（如Sandy
    Bridge）的情况下，无法跨越插槽。
- en: 2.4\. Address Spaces in CUDA
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4. CUDA中的地址空间
- en: As every beginning CUDA programmer knows, the address spaces for the CPU and
    GPU are separate. The CPU cannot read or write the GPU’s device memory, and in
    turn, the GPU cannot read or write the CPU’s memory. As a result, the application
    must explicitly copy data to and from the GPU’s memory in order to process it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 每个初学CUDA的程序员都知道，CPU和GPU的地址空间是分开的。CPU无法读取或写入GPU的设备内存，反之，GPU也无法读取或写入CPU的内存。因此，应用程序必须显式地将数据复制到GPU内存并从GPU内存中复制数据，以便进行处理。
- en: The reality is a bit more complicated, and it has gotten even more so as CUDA
    has added new capabilities such as mapped pinned memory and peer-to-peer access.
    This section gives a detailed description of how address spaces work in CUDA,
    starting from first principles.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现实情况要复杂一些，随着CUDA增加了映射固定内存和点对点访问等新功能，情况变得更加复杂。本节将从基本原理出发，详细描述CUDA中地址空间的工作原理。
- en: '2.4.1\. Virtual Addressing: A Brief History'
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1. 虚拟寻址：简史
- en: Virtual address spaces are such a pervasive and successful abstraction that
    most programmers use and benefit from them every day without ever knowing they
    exist. They are an extension of the original insight that it was useful to assign
    consecutive numbers to the memory locations in the computer. The standard unit
    of measure is the *byte*, so, for example, a computer with 64K of memory had memory
    locations 0..65535\. The 16-bit values that specify memory locations are known
    as *addresses*, and the process of computing addresses and operating on the corresponding
    memory locations is collectively known as *addressing*.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟地址空间是一种普遍且成功的抽象，绝大多数程序员每天都在使用并受益于它们，却从未意识到它们的存在。它们是最初的洞察的延伸，即将连续的数字分配给计算机中的内存位置是有用的。标准的计量单位是*字节*，例如，一台具有64K内存的计算机，其内存位置是0..65535。用于指定内存位置的16位值称为*地址*，而计算地址并对相应内存位置进行操作的过程统称为*寻址*。
- en: 'Early computers performed *physical addressing.* They would compute a memory
    location and then read or write the corresponding memory location, as shown in
    [Figure 2.14](ch02.html#ch02fig14). As software grew more complex and computers
    hosting multiple users or running multiple jobs grew more common, it became clear
    that allowing any program to read or write any physical memory location was unacceptable;
    software running on the machine could fatally corrupt other software by writing
    the wrong memory location. Besides the robustness concern, there were also security
    concerns: Software could spy on other software by reading memory locations it
    did not “own.”'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 早期计算机执行*物理寻址*。它们会计算一个内存位置，然后读取或写入相应的内存位置，如[图 2.14](ch02.html#ch02fig14)所示。随着软件变得越来越复杂，以及托管多个用户或运行多个任务的计算机变得越来越普遍，显然允许任何程序读取或写入任何物理内存位置是不可接受的；在机器上运行的软件可能会通过写入错误的内存位置致命地损坏其他软件。除了稳健性问题，还有安全性问题：软件可以通过读取它不“拥有”的内存位置来监视其他软件。
- en: '![Image](graphics/02fig14.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/02fig14.jpg)'
- en: '*Figure 2.14* Simple 16-bit address space.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.14* 简单的16位地址空间。'
- en: As a result, modern computers implement *virtual address spaces.* Each program
    (operating system designers call it a *process*) gets a view of memory similar
    to [Figure 2.14](ch02.html#ch02fig14), but each process gets its own address space.
    They cannot read or write memory belonging to other processes without special
    permission from the operating system. Instead of specifying a physical address,
    the machine instruction specifies a *virtual address* to be translated into a
    physical address by performing a series of lookups into tables that were set up
    by the operating system.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现代计算机实现了*虚拟地址空间*。每个程序（操作系统设计者称之为*进程*）获得一个类似于[图 2.14](ch02.html#ch02fig14)的内存视图，但每个进程都有自己的地址空间。它们不能在没有操作系统特别许可的情况下读取或写入属于其他进程的内存。机器指令指定一个*虚拟地址*，该地址通过执行一系列查找操作在操作系统设置的表中转换为物理地址。
- en: In most systems, the virtual address space is divided into *pages*, which are
    units of addressing that are at least 4096 bytes in size. Instead of referencing
    physical memory directly from the address, the hardware looks up a *page table
    entry* (PTE) that specifies the physical address where the page’s memory resides.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数系统中，虚拟地址空间被划分为*页*，这些是至少为4096字节的寻址单位。硬件不是直接从地址引用物理内存，而是查找一个*页表项*（PTE），该项指定页面内存所在的物理地址。
- en: It should be clear from [Figure 2.15](ch02.html#ch02fig15) that virtual addressing
    enables a contiguous virtual address space to map to discontiguous pages in physical
    memory. Also, when an application attempts to read or write a memory location
    whose page has not been mapped to physical memory, the hardware signals a fault
    that must be handled by the operating system.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 从[图 2.15](ch02.html#ch02fig15)可以清楚地看到，虚拟地址映射使得一个连续的虚拟地址空间能够映射到物理内存中的不连续页面。此外，当一个应用程序尝试读取或写入一个其页面尚未映射到物理内存的内存位置时，硬件会发出一个故障信号，操作系统必须处理该故障。
- en: '![Image](graphics/02fig15.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig15.jpg)'
- en: '*Figure 2.15* Virtual address space.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.15* 虚拟地址空间。'
- en: 'Just a side note: In practice, no hardware implements a single-level page table
    as shown in [Figure 2.15](ch02.html#ch02fig15). At minimum, the address is split
    into at least two indices: an index into a “page directory” of page tables, and
    an index into the page table selected by the first index. The hierarchical design
    reduces the amount of memory needed for the page tables and enables inactive page
    tables to be marked nonresident and swapped to disk, much like inactive pages.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便提一下：实际上，没有硬件实现单级页表，如[图 2.15](ch02.html#ch02fig15)所示。至少，地址被拆分成两个索引：一个索引用于“页目录”中的页表，另一个索引用于通过第一个索引选择的页表。分层设计减少了页表所需的内存量，并使得不活动的页表可以被标记为非驻留并交换到磁盘，就像不活动的页面一样。
- en: Besides a physical memory location, the PTEs contain permissions bits that the
    hardware can validate while doing the address translation. For example, the operating
    system can make pages read-only, and the hardware will signal a fault if the application
    attempts to write the page.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 除了物理内存位置，PTE还包含权限位，硬件在进行地址转换时可以验证这些权限。例如，操作系统可以将页面设置为只读，如果应用程序尝试写入该页面，硬件会发出故障信号。
- en: Operating systems use virtual memory hardware to implement many features.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 操作系统利用虚拟内存硬件实现许多功能。
- en: '• *Lazy allocation*: Large amounts of memory can be “allocated” by setting
    aside PTEs with no physical memory backing them. If the application that requested
    the memory happens to access one of those pages, the OS resolves the fault by
    finding a page of physical memory at that time.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: • *懒惰分配*：可以通过预留没有物理内存支持的PTE来“分配”大量内存。如果请求内存的应用程序访问了其中的某个页面，操作系统会在那时通过找到一页物理内存来解决故障。
- en: '• *Demand paging*: Memory can be copied to disk and the page marked nonresident.
    If the memory is referenced again, the hardware signals a “page fault,” and the
    OS resolves the fault by copying the data to a physical page, fixing up the PTE
    to point there, and resuming execution.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: • *按需分页*：内存可以被复制到磁盘，并且该页面标记为非驻留。如果内存再次被引用，硬件会发出“页面故障”信号，操作系统通过将数据复制到物理页面、修正页面表项（PTE）以指向该页面并恢复执行来解决故障。
- en: '• *Copy-on-write*: Virtual memory can be “copied” by creating a second set
    of PTEs that map to the same physical pages, then marking both sets of PTEs read-only.
    If the hardware catches an attempt to write to one of those pages, the OS can
    copy it to another physical page, mark both PTEs writeable again, and resume execution.
    If the application only writes to a small percentage of pages that were “copied,”
    copy-on-write is a big performance win.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: • *写时复制*：可以通过创建第二组PTE（页表项），这些PTE映射到相同的物理页面，然后将两组PTE标记为只读，从而“复制”虚拟内存。如果硬件检测到尝试写入这些页面之一的行为，操作系统可以将其复制到另一个物理页面，重新标记两组PTE为可写，并恢复执行。如果应用程序只对“复制”的页面中的一小部分进行写操作，写时复制将带来显著的性能提升。
- en: '• *Mapped file I/O*: Files can be mapped into the address space, and page faults
    can be resolved by accessing the file. For applications that perform random access
    on the file, it may be advantageous to delegate the memory management to the highly
    optimized VMM code in the operating system, especially since it is tightly coupled
    to the mass storage drivers.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: • *映射文件I/O*：文件可以映射到地址空间，并且通过访问文件可以解决页面错误。对于执行文件随机访问的应用程序，将内存管理委托给操作系统中高度优化的VMM代码可能是有利的，特别是因为它与大容量存储驱动程序紧密耦合。
- en: 'It is important to understand that address translation is performed on *every*
    memory access performed by the CPU. To make this operation fast, the CPU contains
    special hardware: caches called translation lookaside buffers (TLBs) that hold
    recently translated address ranges, and “page walkers” that resolve cache misses
    in the TLBs by reading the page tables.^([6](ch02.html#ch02fn6)) Modern CPUs also
    include hardware support for “unified address spaces,” where multiple CPUs can
    access one another’s memory efficiently via AMD’s HT (HyperTransport) and Intel’s
    QuickPath Interconnect (QPI). Since these hardware facilities enable CPUs to access
    any memory location in the system using a unified address space, this section
    refers to “the CPU” and the “CPU address space” regardless of how many CPUs are
    in the system.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要理解，地址转换会在CPU执行的*每一个*内存访问中进行。为了使这个操作快速，CPU包含了特殊的硬件：称为转换旁路缓冲区（TLB）的缓存，用于存储最近翻译的地址范围，以及“页面遍历器”，它通过读取页表来解决TLB中的缓存缺失。^([6](ch02.html#ch02fn6))
    现代CPU还包括对“统一地址空间”的硬件支持，其中多个CPU可以通过AMD的HT（超传输）和Intel的快速路径互连（QPI）高效地访问彼此的内存。由于这些硬件设施使得CPU可以使用统一的地址空间访问系统中的任何内存位置，本节将不管系统中有多少个CPU，均称其为“CPU”及“CPU地址空间”。
- en: '[6](ch02.html#ch02fn6a). It is possible to write programs (for both CPUs and
    CUDA) that expose the size and structure of the TLBs and/or the memory overhead
    of the page walkers if they stride through enough memory in a short enough period
    of time.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](ch02.html#ch02fn6a)。可以编写程序（针对CPU和CUDA），暴露TLB的大小和结构以及/或页面遍历器的内存开销，只要它们在短时间内跨越足够的内存。'
- en: '* * *'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Sidebar: Kernel Mode and User Mode'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 侧边栏：内核模式与用户模式
- en: A final point about memory management on CPUs is that the operating system code
    must use memory protections to prevent applications from corrupting the operating
    system’s own data structures—for example, the page tables that control address
    translation. To aid with this memory protection, operating systems have a “privileged”
    mode of execution that they use when performing critical system functions. In
    order to manage low-level hardware resources such as page tables or to program
    hardware registers on peripherals such as the disk or network controller or the
    CUDA GPU, the CPU must be running in *kernel mode*. The unprivileged execution
    mode used by application code is called *user mode*.^([7](ch02.html#ch02fn7))
    Besides code written by the operating system provider, low-level driver code to
    control hardware peripherals also runs in kernel mode. Since mistakes in kernel
    mode code can lead to system stability or security problems, kernel mode code
    is held to a higher quality standard. Also, many operating system services, such
    as mapped file I/O or other memory management facilities listed above, are not
    available in kernel mode.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 CPU 内存管理的最后一点是，操作系统代码必须使用内存保护，以防止应用程序破坏操作系统自身的数据结构——例如，控制地址转换的页表。为了辅助此内存保护，操作系统有一个“特权”执行模式，当它执行关键系统功能时使用此模式。为了管理低级硬件资源，如页表，或编程硬件寄存器，例如磁盘、网络控制器或
    CUDA GPU 的外围设备，CPU 必须在*内核模式*下运行。应用程序代码使用的无特权执行模式被称为*用户模式*。^([7](ch02.html#ch02fn7))
    除了操作系统提供者编写的代码外，控制硬件外围设备的低级驱动程序代码也运行在内核模式下。由于内核模式代码中的错误可能导致系统稳定性或安全问题，因此内核模式代码的质量标准更高。此外，许多操作系统服务，如映射文件
    I/O 或其他内存管理功能，在内核模式下不可用。
- en: '[7](ch02.html#ch02fn7a). The x86-specific terms for *kernel mode* and *user
    mode* are “Ring 0” and “Ring 3,” respectively.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](ch02.html#ch02fn7a). x86 特定的*内核模式*和*用户模式*术语分别是“Ring 0”和“Ring 3”。'
- en: To ensure system stability and security, the interface between user mode and
    kernel mode is carefully regulated. The user mode code must set up a data structure
    in memory and make a special system call that validates the memory and the request
    that is being made. This transition from user mode to kernel mode is known as
    a *kernel thunk*. Kernel thunks are expensive, and their cost sometimes must be
    taken into account by CUDA developers.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保系统的稳定性和安全性，用户模式和内核模式之间的接口受到严格监管。用户模式代码必须在内存中设置一个数据结构，并进行一个特殊的系统调用来验证内存和请求。这个从用户模式到内核模式的转换被称为*内核抽象*。内核抽象代价高昂，有时
    CUDA 开发人员必须考虑其成本。
- en: Every interaction with CUDA hardware by the user mode driver is arbitrated by
    kernel mode code. Often this means having resources allocated on its behalf—not
    only memory but also hardware resources such as the hardware register used by
    the user mode driver to submit work to the hardware.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 用户模式驱动程序与 CUDA 硬件的每次交互都由内核模式代码调解。这通常意味着需要为它分配资源——不仅仅是内存，还有硬件资源，例如用户模式驱动程序用于向硬件提交工作任务的硬件寄存器。
- en: The bulk of CUDA’s driver runs in user mode. For example, in order to allocate
    page-locked system memory (e.g., with the `cudaHostAlloc()` function), the CUDA
    application calls into the user mode CUDA driver, which composes a request to
    the kernel mode CUDA driver and performs a kernel thunk. The kernel mode CUDA
    driver uses a mix of low-level OS services (for example, it may call a system
    service to map GPU hardware registers) and hardware-specific code (for example,
    to program the GPU’s memory management hardware) to satisfy the request.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 驱动程序的大部分运行在用户模式下。例如，为了分配页面锁定的系统内存（例如，使用`cudaHostAlloc()`函数），CUDA 应用程序调用用户模式的
    CUDA 驱动程序，该驱动程序向内核模式的 CUDA 驱动程序发出请求，并执行内核转换。内核模式的 CUDA 驱动程序使用低级操作系统服务（例如，它可能调用系统服务来映射
    GPU 硬件寄存器）和硬件特定代码（例如，编程 GPU 的内存管理硬件）来满足请求。
- en: '* * *'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 2.4.2\. Disjoint Address Spaces
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. 不连续地址空间
- en: On the GPU, CUDA also uses virtual address spaces, although the hardware does
    not support as rich a feature set as do the CPUs. GPUs do enforce memory protections,
    so CUDA programs cannot accidentally read or corrupt other CUDA programs’ memory
    or access memory that hasn’t been mapped for them by the kernel mode driver. But
    GPUs do not support demand paging, so every byte of virtual memory allocated by
    CUDA must be backed by a byte of physical memory. Also, demand paging is the underlying
    hardware mechanism used by operating systems to implement most of the features
    outlined above.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 上，CUDA 也使用虚拟地址空间，尽管硬件不像 CPU 那样支持丰富的功能集。GPU 确实执行内存保护，因此 CUDA 程序无法意外读取或损坏其他
    CUDA 程序的内存，也无法访问内核模式驱动程序尚未为其映射的内存。但是，GPU 不支持按需分页，因此 CUDA 分配的每个虚拟内存字节必须由一个物理内存字节支持。此外，按需分页是操作系统用来实现上述大多数功能的底层硬件机制。
- en: Since each GPU has its own memory and address translation hardware, the CUDA
    address space is separate from the CPU address space where the host code in a
    CUDA application runs. [Figure 2.16](ch02.html#ch02fig16) shows the address space
    architecture for CUDA as of version 1.0, before mapped pinned memory became available.
    The CPU and GPU each had its own address space, mapped with each device’s own
    page tables. The two devices exchanged data via explicit memcpy commands. The
    GPU could allocate *pinned memory*—page-locked memory that had been mapped for
    DMA by the GPU—but pinned memory only made DMA faster; it did not enable CUDA
    kernels to access host memory.^([8](ch02.html#ch02fn8))
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个 GPU 都有自己的内存和地址转换硬件，CUDA 地址空间与运行 CUDA 应用程序主机代码的 CPU 地址空间是分开的。[图 2.16](ch02.html#ch02fig16)
    显示了 CUDA 1.0 版本的地址空间架构，即在映射固定内存可用之前。CPU 和 GPU 各自有自己的地址空间，并使用各自的页表进行映射。两个设备通过显式的
    memcpy 命令交换数据。GPU 可以分配*固定内存*——GPU 为 DMA 映射的页面锁定内存——但固定内存仅仅是加速 DMA；它并不使 CUDA 内核能够访问主机内存。^([8](ch02.html#ch02fn8))
- en: '[8](ch02.html#ch02fn8a). On 32-bit operating systems, CUDA-capable GPUs can
    map pinned memory for memcpy in a 40-bit address space that is outside the CUDA
    address space used by kernels.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[8](ch02.html#ch02fn8a)。在 32 位操作系统上，支持 CUDA 的 GPU 可以在一个 40 位的地址空间中映射固定内存，以便在
    CUDA 内核使用的地址空间之外执行 memcpy 操作。'
- en: '![Image](graphics/02fig16.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig16.jpg)'
- en: '*Figure 2.16* Disjoint address spaces.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.16* 不同的地址空间。'
- en: The CUDA driver tracks pinned memory ranges and automatically accelerates memcpy
    operations that reference them. Asynchronous memcpy calls require pinned memory
    ranges to ensure that the operating system does not unmap or move the physical
    memory before the memcpy is performed.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 驱动程序跟踪固定内存范围，并自动加速引用这些内存的 memcpy 操作。异步 memcpy 调用需要固定内存范围，以确保操作系统在执行 memcpy
    操作之前不会取消映射或移动物理内存。
- en: Not all CUDA applications can allocate the host memory they wish to process
    using CUDA. For example, a CUDA-aware plugin to a large, extensible application
    may want to operate on host memory that was allocated by non-CUDA-aware code.
    To accommodate that use case, CUDA 4.0 added the ability to *register* existing
    host address ranges, which page-locks a virtual address range, maps it for the
    GPU, and adds the address range to the tracking data structure so CUDA knows it
    is pinned. The memory then can be passed to asynchronous memcpy calls or otherwise
    treated as if it were allocated by CUDA.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有 CUDA 应用程序都能分配它们希望通过 CUDA 处理的主机内存。例如，某个大型扩展应用程序的 CUDA 感知插件可能希望在非 CUDA 感知代码分配的主机内存上进行操作。为了适应这种用例，CUDA
    4.0 增加了注册现有主机地址范围的功能，这会将虚拟地址范围页锁定，映射到 GPU 上，并将该地址范围添加到跟踪数据结构中，以便 CUDA 知道它已被固定。然后，可以将内存传递给异步
    memcpy 调用，或以它由 CUDA 分配的方式进行处理。
- en: 2.4.3\. Mapped Pinned Memory
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.3\. 映射固定内存
- en: CUDA 2.2 added a feature called *mapped pinned memory*, shown in [Figure 2.17](ch02.html#ch02fig17).
    Mapped pinned memory is page-locked host memory that has been mapped into the
    CUDA address space, where CUDA kernels can read or write it directly. The page
    tables of both the CPU and the GPU are updated so that both the CPU and the GPU
    have address ranges that point to the same host memory buffer. Since the address
    spaces are different, the GPU pointer(s) to the buffer must be queried using `cuMemHostGetDevicePointer()/cudaHostGetDevicePointer()`.^([9](ch02.html#ch02fn9))
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 2.2 增加了一个名为 *映射固定内存* 的特性，如 [图 2.17](ch02.html#ch02fig17) 所示。映射固定内存是页锁定的主机内存，已经映射到
    CUDA 地址空间，在这里 CUDA 内核可以直接读取或写入它。CPU 和 GPU 的页表都被更新，使得 CPU 和 GPU 都有指向同一主机内存缓冲区的地址范围。由于地址空间不同，因此必须使用
    `cuMemHostGetDevicePointer()/cudaHostGetDevicePointer()` 查询 GPU 指针指向的缓冲区。^([9](ch02.html#ch02fn9))
- en: '[9](ch02.html#ch02fn9a). For multi-GPU configurations, CUDA 2.2 also added
    a feature called “portable” pinned memory that causes the allocation to be mapped
    into every GPU’s address space. But there is no guarantee that `cu(da)HostGetDevicePointer()`
    will return the same value for different GPUs!'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[9](ch02.html#ch02fn9a). 对于多GPU配置，CUDA 2.2还添加了一个名为“可移植”固定内存的功能，导致分配被映射到每个GPU的地址空间。但是不能保证`cu(da)HostGetDevicePointer()`会为不同的GPU返回相同的值！'
- en: '![Image](graphics/02fig17.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig17.jpg)'
- en: '*Figure 2.17* Mapped pinned memory.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.17* 映射的固定内存。'
- en: 2.4.4\. Portable Pinned Memory
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.4\. 可移植固定内存
- en: CUDA 2.2 also enabled a feature called *portable pinned memory*, shown in [Figure
    2.18](ch02.html#ch02fig18). Making pinned memory “portable” causes the CUDA driver
    to map it for *all* GPUs in the system, not just the one whose context is current.
    A separate set of page table entries is created for the CPU and every GPU in the
    system, enabling the corresponding device to translate virtual addresses to the
    underlying physical memory. The host memory range also is added to every active
    CUDA context’s tracking mechanism, so every GPU will recognize the portable allocation
    as pinned.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 2.2 还启用了一个名为*可移植固定内存*的功能，如[图 2.18](ch02.html#ch02fig18)所示。使固定内存“可移植”会导致CUDA驱动程序将其映射到系统中*所有*GPU，而不仅仅是当前上下文的那个GPU。为CPU和系统中的每个GPU创建了一组独立的页表条目，使得相应的设备能够将虚拟地址转换为底层物理内存。主机内存范围也会被添加到每个活动CUDA上下文的跟踪机制中，因此每个GPU都能将可移植分配识别为固定内存。
- en: '![Image](graphics/02fig18.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig18.jpg)'
- en: '*Figure 2.18* Portable, mapped pinned memory.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.18* 可移植的映射固定内存。'
- en: '[Figure 2.18](ch02.html#ch02fig18) likely represents the limit of developer
    tolerance for multiple address spaces. Here, a 2-GPU system has 3 addresses for
    an allocation; a 4-GPU system would have 5 addresses. Although CUDA has fast APIs
    to look up a given CPU address range and pass back the corresponding GPU address
    range, having N+1 addresses on an N-GPU system, all for the same allocation, is
    inconvenient to say the least.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.18](ch02.html#ch02fig18) 可能代表了开发人员对于多个地址空间的容忍度的极限。在这里，一个2个GPU的系统为一个分配提供了3个地址；一个4个GPU的系统则会提供5个地址。尽管CUDA有快速的API可以查找给定的CPU地址范围并返回相应的GPU地址范围，但在一个N个GPU的系统上，拥有N+1个地址，且这些地址都指向同一个分配，至少可以说是非常不便的。'
- en: 2.4.5\. Unified Addressing
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.5\. 统一寻址
- en: Multiple address spaces are required for 32-bit CUDA GPUs, which can only map
    2^(32)=4GiB of address space; since some high-end GPUs have up to 4GiB of device
    memory, they are hard-pressed to address all of device memory and also map any
    pinned memory, let alone use the same address space as the CPU. But on 64-bit
    operating systems with Fermi or later GPUs, a simpler abstraction is possible.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对于32位CUDA GPU，需要多个地址空间，因为它们只能映射2^(32)=4GiB的地址空间；由于一些高端GPU拥有最多4GiB的设备内存，它们很难完全映射所有设备内存，同时还要映射任何固定内存，更不用说使用与CPU相同的地址空间了。但是，在具有Fermi或更高版本GPU的64位操作系统上，可以实现更简化的抽象。
- en: CUDA 4.0 added a feature called *unified virtual addressing* (UVA), shown in
    [Figure 2.19](ch02.html#ch02fig19). When UVA is in force, CUDA allocates memory
    for both CPUs and GPUs from the same virtual address space. The CUDA driver accomplishes
    this by having its initialization routine perform large virtual allocations from
    the CPU address space—allocations that are not backed by physical memory—and then
    mapping GPU allocations into those address ranges. Since x64 CPUs support 48-bit
    virtual address spaces,^([10](ch02.html#ch02fn10)) while CUDA GPUs only support
    40 bits, applications using UVA should make sure CUDA gets initialized early to
    guard against CPU code using virtual address needed by CUDA.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 4.0增加了一个叫做*统一虚拟寻址*（UVA）的功能，如[图2.19](ch02.html#ch02fig19)所示。当UVA生效时，CUDA从同一虚拟地址空间为CPU和GPU分配内存。CUDA驱动程序通过其初始化例程从CPU地址空间进行大规模虚拟分配（这些分配不由物理内存支持），然后将GPU的分配映射到这些地址范围。由于x64
    CPU支持48位虚拟地址空间^[10](ch02.html#ch02fn10)，而CUDA GPU仅支持40位，因此使用UVA的应用程序应确保CUDA尽早初始化，以防CPU代码使用CUDA所需的虚拟地址。
- en: '[10](ch02.html#ch02fn10a). 48 bits of virtual address space = 256 terabytes.
    Future x64 CPUs will support even larger address spaces.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[10](ch02.html#ch02fn10a)。48位虚拟地址空间 = 256TB。未来的x64 CPU将支持更大的地址空间。'
- en: '![Image](graphics/02fig19.jpg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig19.jpg)'
- en: '*Figure 2.19* Unified virtual addressing (UVA).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.19* 统一虚拟寻址（UVA）。'
- en: For mapped pinned allocations, the GPU and CPU pointers are the same. For other
    types of allocation, CUDA can infer the device for which a given allocation was
    performed from the address. As a result, the family of linear memcpy functions
    (`cudaMemcpy()` with a direction specified, `cuMemcpyHtoD()`, `cuMemcpyDtoH()`,
    etc.) have been replaced by simplified `cuMemcpy()` and `cudaMemcpy()` functions
    that do not take a memory direction.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于映射的固定内存分配，GPU和CPU的指针是相同的。对于其他类型的内存分配，CUDA可以从地址推断出给定分配所执行的设备。因此，线性内存拷贝函数（如指定了方向的`cudaMemcpy()`、`cuMemcpyHtoD()`、`cuMemcpyDtoH()`等）已被简化的`cuMemcpy()`和`cudaMemcpy()`函数替代，这些简化版函数不再需要指定内存方向。
- en: UVA is enabled automatically on UVA-capable systems. At the time of this writing,
    UVA is enabled on 64-bit Linux, 64-bit MacOS, and 64-bit Windows when using the
    TCC driver; the WDDM driver does not yet support UVA. When UVA is in effect, all
    pinned allocations performed by CUDA are both mapped and portable. Note that for
    system memory that has been pinned using `cuMemRegisterHost()`, the device pointers
    still must be queried using `cu(da)HostGetDevicePointer()`. Even when UVA is in
    effect, the CPU(s) cannot access device memory. In addition, by default, the GPU(s)
    cannot access one another’s memory.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: UVA在支持UVA的系统上自动启用。在撰写本文时，UVA在64位Linux、64位MacOS和64位Windows上使用TCC驱动程序时启用；WDDM驱动程序尚不支持UVA。当UVA生效时，CUDA执行的所有固定分配既是映射的，也是可移植的。请注意，对于使用`cuMemRegisterHost()`固定的系统内存，设备指针仍需使用`cu(da)HostGetDevicePointer()`查询。即使UVA生效，CPU仍无法访问设备内存。此外，默认情况下，GPU不能访问彼此的内存。
- en: 2.4.6\. Peer-to-Peer Mappings
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.6\. 同伴对等映射
- en: In the final stage of our journey through CUDA’s virtual memory abstractions,
    we discuss peer-to-peer mapping of device memory, shown in [Figure 2.20](ch02.html#ch02fig20).
    Peer-to-peer enables a Fermi-class GPU to read or write memory that resides in
    another Fermi-class GPU. Peer-to-peer mapping is supported only on UVA-enabled
    platforms, and it only works on GPUs that are connected to the same I/O hub. Because
    UVA is always in force when using peer-to-peer, the address ranges for different
    devices do not overlap, and the driver (and runtime) can infer the owning device
    from a pointer value.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索 CUDA 虚拟内存抽象的最后阶段，我们讨论了设备内存的点对点映射，如[图 2.20](ch02.html#ch02fig20)所示。点对点映射使得
    Fermi 类 GPU 可以读取或写入另一个 Fermi 类 GPU 上的内存。点对点映射仅在支持 UVA 的平台上受支持，并且只适用于连接到同一 I/O
    集线器的 GPU。由于在使用点对点时 UVA 始终处于启用状态，不同设备的地址范围不会重叠，驱动程序（及运行时）可以从指针值推断出拥有设备。
- en: '![Image](graphics/02fig20.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig20.jpg)'
- en: '*Figure 2.20* Peer-to-peer.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.20* 点对点。'
- en: Peer-to-peer memory addressing is asymmetric; note that [Figure 2.20](ch02.html#ch02fig20)
    shows an asymmetric mapping in which GPU 1’s allocations are visible to GPU 0,
    but not vice versa. In order for GPUs to see each other’s memory, each GPU must
    explicitly map the other’s memory. The API functions to manage peer-to-peer mappings
    are discussed in [Section 9.2](ch09.html#ch09lev1sec2).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 点对点内存寻址是不对称的；请注意，[图 2.20](ch02.html#ch02fig20) 显示了一个不对称的映射，其中 GPU 1 的分配对 GPU
    0 可见，但反之则不可见。为了让 GPU 互相查看彼此的内存，每个 GPU 必须显式地映射对方的内存。管理点对点映射的 API 函数在[第 9.2 节](ch09.html#ch09lev1sec2)中进行了讨论。
- en: 2.5\. CPU/GPU Interactions
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5. CPU/GPU 交互
- en: This section describes key elements of CPU-GPU interactions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了 CPU 与 GPU 交互的关键要素。
- en: • *Pinned host memory:* CPU memory that the GPU can directly access
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: • *固定主机内存：* CPU 内存，GPU 可以直接访问
- en: • *Command buffers:* the buffers written by the CUDA driver and read by the
    GPU to control its execution
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: • *命令缓冲区：* CUDA 驱动程序写入并由 GPU 读取的缓冲区，用于控制其执行
- en: • *CPU/GPU synchronization:* how the GPU’s progress is tracked by the CPU
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: • *CPU/GPU 同步：* CPU 如何追踪 GPU 的进度
- en: This section describes these facilities at the hardware level, citing APIs only
    as necessary to help the reader understand how they pertain to CUDA development.
    For simplicity, this section uses the CPU/GPU model in [Figure 2.1](ch02.html#ch02fig01),
    setting aside the complexities of multi-CPU or multi-GPU programming.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本节从硬件层面描述这些功能，仅在必要时引用 API，以帮助读者理解它们与 CUDA 开发的关系。为了简化，本文使用了[图 2.1](ch02.html#ch02fig01)中的
    CPU/GPU 模型，暂时忽略了多 CPU 或多 GPU 编程的复杂性。
- en: 2.5.1\. Pinned Host Memory and Command Buffers
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.1. 固定主机内存和命令缓冲区
- en: For obvious reasons, the CPU and GPU are each best at accessing its own memory,
    but the GPU can directly access page-locked CPU memory via direct memory access
    (DMA). Page-locking is a facility used by operating systems to enable hardware
    peripherals to directly access CPU memory, avoiding extraneous copies. The “locked”
    pages have been marked as ineligible for eviction by the operating system, so
    device drivers can program these peripherals to use the pages’ physical addresses
    to access the memory directly. The CPU still can access the memory in question,
    but the memory cannot be moved or paged out to disk.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 出于显而易见的原因，CPU 和 GPU 各自最擅长访问自己的内存，但 GPU 可以通过直接内存访问（DMA）直接访问被页面锁定的 CPU 内存。页面锁定是操作系统提供的一种功能，用于使硬件外设能够直接访问
    CPU 内存，从而避免了不必要的拷贝。“锁定”的页面已被操作系统标记为不可驱逐，因此设备驱动程序可以编程让这些外设使用页面的物理地址直接访问内存。CPU 仍然可以访问相关内存，但内存不能移动或换出到磁盘。
- en: Since the GPU is a distinct device from the CPU, direct memory access also enables
    the GPU to read and write CPU memory independently of, and in parallel with, the
    CPU’s execution. Care must be taken to synchronize between the CPU and GPU to
    avoid race conditions, but for applications that can make productive use of CPU
    clock cycles while the GPU is processing, the performance benefits of concurrent
    execution can be significant.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GPU 是与 CPU 分开的设备，直接内存访问还使 GPU 可以独立于 CPU 执行，且与 CPU 执行并行地读写 CPU 内存。在 CPU 和
    GPU 之间必须小心同步，以避免竞态条件，但对于那些能够在 GPU 处理时有效利用 CPU 时钟周期的应用程序，并行执行的性能优势可能非常显著。
- en: '[Figure 2.21](ch02.html#ch02fig21) depicts a “pinned” buffer that has been
    mapped by the GPU^([11](ch02.html#ch02fn11)) for direct access. CUDA programmers
    are familiar with pinned buffers because CUDA has always given them the ability
    to allocate pinned memory via APIs such as `cudaMallocHost()`. But under the hood,
    one of the main applications for such buffers is to submit commands to the GPU.
    The CPU writes commands into a “command buffer” that the GPU can consume, and
    the GPU simultaneously reads and executes previously written commands. [Figure
    2.22](ch02.html#ch02fig22) shows how the CPU and GPU share this buffer. This diagram
    is simplified because the commands may be hundreds of bytes long, and the buffer
    is big enough to hold several thousand such commands. The “leading edge” of the
    buffer is under construction by the CPU and not yet ready to be read by the GPU.
    The “trailing edge” of the buffer is being read by the GPU. The commands in between
    are ready for the GPU to process when it is ready.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2.21](ch02.html#ch02fig21) 展示了一个由 GPU 映射的“固定”缓冲区，以便直接访问。CUDA 程序员对固定缓冲区很熟悉，因为
    CUDA 一直提供通过 `cudaMallocHost()` 等 API 分配固定内存的能力。但在背后，这种缓冲区的主要应用之一是向 GPU 提交命令。CPU
    将命令写入一个 GPU 可消费的“命令缓冲区”，GPU 同时读取并执行之前写入的命令。[图 2.22](ch02.html#ch02fig22) 展示了 CPU
    和 GPU 如何共享这个缓冲区。这个示意图被简化了，因为命令可能有几百字节长，且缓冲区足够大，能够容纳几千个这样的命令。缓冲区的“前沿”由 CPU 构建，尚未准备好供
    GPU 读取；缓冲区的“尾端”正被 GPU 读取。中间的命令在 GPU 准备好时可以处理。'
- en: '[11](ch02.html#ch02fn11a). Important note: In this context, “mapping” for the
    GPU involves setting up hardware tables that refer to the CPU memory’s physical
    addresses. The memory may or may not be mapped into the address space where it
    can be accessed by CUDA kernels.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[11](ch02.html#ch02fn11a). 重要说明：在这个上下文中，GPU 的“映射”涉及设置硬件表格，指向 CPU 内存的物理地址。内存可能已被映射到可以由
    CUDA 内核访问的地址空间，也可能没有。'
- en: '![Image](graphics/02fig21.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig21.jpg)'
- en: '*Figure 2.21* Pinned buffer.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.21* 固定缓冲区。'
- en: '![Image](graphics/02fig22.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig22.jpg)'
- en: '*Figure 2.22* CPU/GPU command buffer.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.22* CPU/GPU 命令缓冲区。'
- en: Typically, the CUDA driver will reuse command buffer memory because once the
    GPU has finished processing a command, the memory becomes eligible to be written
    again by the CPU. [Figure 2.23](ch02.html#ch02fig23) shows how the CPU can “wrap
    around” the command buffer.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，CUDA 驱动程序会重用命令缓冲区内存，因为一旦 GPU 完成处理一个命令，该内存就可以再次被 CPU 写入。[图 2.23](ch02.html#ch02fig23)
    展示了 CPU 如何“循环”命令缓冲区。
- en: '![Image](graphics/02fig23.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig23.jpg)'
- en: '*Figure 2.23* Command buffer wrap-around.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.23* 命令缓冲区循环。'
- en: Since it takes several thousand CPU clock cycles to launch a CUDA kernel, a
    key use case for CPU/GPU concurrency is simply to prepare more GPU commands while
    the GPU is processing. Applications that are not balanced to keep both the CPU
    and GPU busy may become “CPU bound” or “GPU bound,” as shown in [Figures 2.24](ch02.html#ch02fig24)
    and [2.25](ch02.html#ch02fig25), respectively. In a CPU-bound application, the
    GPU is poised and ready to process the next command as soon as it becomes available;
    in a GPU-bound application, the CPU has completely filled the command buffer and
    must wait for the GPU before writing the next GPU command. Some applications are
    intrinsically CPU-bound or GPU-bound, so CPU- and GPU-boundedness does not necessarily
    indicate a fundamental problem with an application’s structure. Nevertheless,
    knowing whether an application is CPU-bound or GPU-bound can help highlight performance
    opportunities.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 由于启动一个 CUDA 内核需要几千个 CPU 时钟周期，因此 CPU/GPU 并发的一个关键使用场景是，在 GPU 正在处理时，CPU 只是准备更多的
    GPU 命令。那些未能平衡以使 CPU 和 GPU 都保持繁忙的应用程序可能会变得“CPU 绑定”或“GPU 绑定”，分别如[图 2.24](ch02.html#ch02fig24)和[2.25](ch02.html#ch02fig25)所示。在
    CPU 绑定的应用程序中，GPU 准备好并等待处理下一个命令；在 GPU 绑定的应用程序中，CPU 已经完全填充了命令缓冲区，必须等待 GPU 才能写入下一个
    GPU 命令。一些应用程序天生就是 CPU 绑定或 GPU 绑定的，因此 CPU 和 GPU 绑定并不一定表示应用程序结构上有根本问题。然而，知道应用程序是
    CPU 绑定还是 GPU 绑定，有助于突出性能提升的机会。
- en: '![Image](graphics/02fig24.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig24.jpg)'
- en: '*Figure 2.24* GPU-bound application.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.24* GPU 绑定的应用程序。'
- en: '![Image](graphics/02fig25.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig25.jpg)'
- en: '*Figure 2.25* CPU-bound application.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.25* CPU 绑定的应用程序。'
- en: 2.5.2\. CPU/GPU Concurrency
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.2\. CPU/GPU 并发
- en: 'The previous section introduced the coarsest-grained parallelism available
    in CUDA systems: *CPU/GPU concurrency*. All launches of CUDA kernels are asynchronous:
    the CPU requests the launch by writing commands into the command buffer, then
    returns without checking the GPU’s progress. Memory copies optionally also may
    be asynchronous, enabling CPU/GPU concurrency and possibly enabling memory copies
    to be done concurrently with kernel processing.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节介绍了 CUDA 系统中最粗粒度的并行性：*CPU/GPU 并发*。所有的 CUDA 内核启动都是异步的：CPU 通过将命令写入命令缓冲区请求启动，然后返回而不检查
    GPU 的进度。内存复制也可以选择异步进行，这样可以实现 CPU/GPU 并发，并可能使内存复制与内核处理并行执行。
- en: Amdahl’s Law
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 阿姆达尔定律
- en: When CUDA programs are written correctly, the CPU and GPU can fully operate
    in parallel, potentially doubling performance. CPU- or GPU-bound programs do not
    benefit much from CPU/GPU concurrency because the CPU or GPU will limit performance
    even if the other device is operating in parallel. This vague observation can
    be concretely characterized using *Amdahl’s Law*, first articulated in a paper
    by Gene Amdahl in 1967.^([12](ch02.html#ch02fn12)) Amdahl’s Law is often summarized
    as follows.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 当 CUDA 程序编写正确时，CPU 和 GPU 可以完全并行工作，可能将性能提升一倍。受 CPU 或 GPU 限制的程序不会从 CPU/GPU 并发中获益太多，因为即使另一个设备并行工作，CPU
    或 GPU 也会限制性能。这个模糊的观察可以通过 *阿姆达尔定律* 来具体化，这一定律首次由吉恩·阿姆达尔（Gene Amdahl）在 1967 年的论文中提出。^[12](ch02.html#ch02fn12)
    阿姆达尔定律通常总结如下。
- en: '[12](ch02.html#ch02fn12a). [http://bit.ly/13UqBm0](http://bit.ly/13UqBm0)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[12](ch02.html#ch02fn12a). [http://bit.ly/13UqBm0](http://bit.ly/13UqBm0)'
- en: '![Image](graphics/036equ01.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/036equ01.jpg)'
- en: where r[s] + r[p] = 1 and r[s] represents the ratio of the sequential portion.
    This formulation seems awkward when examining small-scale performance opportunities
    such as CPU/GPU concurrency. Rearranging the equation as follows
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 r[s] + r[p] = 1，r[s] 表示顺序部分的比例。当考察诸如 CPU/GPU 并发等小规模性能机会时，这种公式显得有些笨拙。将方程式重新排列如下
- en: '![Image](graphics/036equ02.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/036equ02.jpg)'
- en: clearly shows that the speedup is *N*x if r[p] = 1\. If there is one CPU and
    one GPU (*N* = 2), the maximum speedup from full concurrency is 2x; this is almost
    achievable for balanced workloads such as video transcoding, where the CPU can
    perform serial operations (such as variable-length decoding) in parallel with
    the GPU’s performing parallel operations (such as pixel processing). But for more
    CPU- or GPU-bound applications, this type of concurrency offers limited benefits.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 清晰地显示，当 r[p] = 1 时，速度提升为 *N*x。如果有一个 CPU 和一个 GPU（*N* = 2），那么完全并发下的最大加速是 2x；对于平衡负载（如视频转码），CPU
    可以执行顺序操作（如变长解码），并与 GPU 执行并行操作（如像素处理）并行进行，这几乎可以实现。但是对于更多受 CPU 或 GPU 限制的应用来说，这种并发形式提供的好处有限。
- en: Amdahl’s paper was intended as a cautionary tale for those who believed that
    parallelism would be a panacea for performance problems, and we use it elsewhere
    in this book when discussing intra-GPU concurrency, multi-GPU concurrency, and
    the speedups achievable from porting to CUDA kernels. It can be empowering, though,
    to know which forms of concurrency will not confer any benefit to a given application,
    so developers can spend their time exploring other avenues for increased performance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 阿姆达尔的论文旨在警示那些相信并行性能够解决性能问题的读者，我们在本书的其他部分也会引用它，讨论 GPU 内部并发、多 GPU 并发以及从 CUDA 核心迁移中能获得的加速效果。然而，了解哪些并发形式不会为特定应用带来任何好处，反而可能赋予开发者一种权力，让他们可以将时间用于探索其他提高性能的途径。
- en: Error Handling
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 错误处理
- en: 'CPU/GPU concurrency also has implications for error handling. If the CPU launches
    a dozen kernels and one of them causes a memory fault, the CPU cannot discover
    the fault until it has performed CPU/GPU synchronization (described in the next
    section). Developers can manually perform CPU/GPU synchronization by calling `cudaThreadSynchronize()`
    or `cuCtxSynchronize()`, and other functions such as `cudaFree()` or `cuMemFree()`
    may cause CPU/GPU synchronization to occur as a side effect. The *CUDA C Programming
    Guide* references this behavior by calling out functions that may cause CPU/GPU
    synchronization: “Note that this function may also return error codes from previous,
    asynchronous launches.”'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: CPU/GPU 并发也对错误处理产生影响。如果 CPU 启动了十几个内核，而其中一个导致了内存故障，CPU 直到执行了 CPU/GPU 同步（在下一节中描述）后，才能发现故障。开发者可以通过调用`cudaThreadSynchronize()`或`cuCtxSynchronize()`手动执行
    CPU/GPU 同步，其他函数如`cudaFree()`或`cuMemFree()`也可能会作为副作用触发 CPU/GPU 同步。*CUDA C 编程指南*通过列出可能引起
    CPU/GPU 同步的函数来提到这一行为：“请注意，此函数可能还会返回来自之前异步启动的错误代码。”
- en: As CUDA is currently implemented, if a fault does occur, there is no way to
    know which kernel caused the fault. For debug code, if it’s difficult to isolate
    faults with synchronization, developers can set the `CUDA_LAUNCH_BLOCKING` environment
    variable to force all launches to be synchronous.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 根据当前的 CUDA 实现，如果发生故障，就无法知道是哪个内核引起了故障。对于调试代码，如果通过同步很难隔离故障，开发者可以设置 `CUDA_LAUNCH_BLOCKING`
    环境变量，强制所有启动都同步进行。
- en: CPU/GPU Synchronization
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CPU/GPU 同步
- en: Although most GPU commands used by CUDA involve performing memory copies or
    kernel launches, an important subclass of commands helps the CUDA driver track
    the GPU’s progress in processing the command buffer. Because the application cannot
    know how long a given CUDA kernel may run, the GPU itself must report progress
    to the CPU. [Figure 2.26](ch02.html#ch02fig26) shows both the command buffer and
    the “sync location” (which also resides in pinned host memory) used by the driver
    and GPU to track progress. A monotonically increasing integer value (the “progress
    value”) is maintained by the driver, and every major GPU operation is followed
    by a command to write the new progress value to the shared sync location. In the
    case of [Figure 2.26](ch02.html#ch02fig26), the progress value is 3 until the
    GPU finishes executing the command and writes the value 4 to the sync location.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CUDA 使用的大多数 GPU 命令涉及执行内存复制或内核启动，但一个重要的命令子类帮助 CUDA 驱动程序跟踪 GPU 在处理命令缓冲区中的进度。由于应用程序无法知道给定的
    CUDA 内核可能运行多长时间，GPU 本身必须向 CPU 报告进度。[图 2.26](ch02.html#ch02fig26)展示了驱动程序和 GPU 用来跟踪进度的命令缓冲区和“同步位置”（也位于固定主机内存中）。驱动程序维护一个单调递增的整数值（“进度值”），每个主要的
    GPU 操作后都会有一个命令将新的进度值写入共享的同步位置。在[图 2.26](ch02.html#ch02fig26)中，进度值是 3，直到 GPU 执行完命令并将值
    4 写入同步位置。
- en: '![Image](graphics/02fig26.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/02fig26.jpg)'
- en: '*Figure 2.26* Shared sync value—before.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 2.26* 共享同步值—之前。'
- en: CUDA exposes these hardware capabilities both implicitly and explicitly. Context-wide
    synchronization calls such as `cuCtxSynchronize()` or `cudaThreadSynchronize()`
    simply examine the last sync value requested of the GPU and wait until the sync
    location attains that value. For example, if the command 8 being written by the
    CPU in [Figure 2.27](ch02.html#ch02fig27) were followed by `cuCtxSynchronize()`
    or `cudaThreadSynchronize()`, the driver would wait until the shared sync value
    became greater than or equal to 8.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA以隐式和显式两种方式暴露这些硬件能力。上下文范围内的同步调用，如`cuCtxSynchronize()`或`cudaThreadSynchronize()`，仅仅检查请求GPU的最后同步值，并等待同步位置达到该值。例如，如果CPU在[图2.27](ch02.html#ch02fig27)中写入的命令8后面跟着`cuCtxSynchronize()`或`cudaThreadSynchronize()`，驱动程序将等待共享同步值大于或等于8。
- en: '![Image](graphics/02fig27.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/02fig27.jpg)'
- en: '*Figure 2.27* Shared sync value—after.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.27* 共享同步值—之后。'
- en: CUDA events expose these hardware capabilities more explicitly. `cuEvent-Record()`
    enqueues a command to write a new sync value to a shared sync location, and `cuEventQuery()`
    and `cuEventSynchronize()` examine and wait on the event’s sync value, respectively.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA事件更明确地暴露了这些硬件能力。`cuEvent-Record()`将命令排入队列，以将新的同步值写入共享同步位置，而`cuEventQuery()`和`cuEventSynchronize()`分别检查和等待事件的同步值。
- en: Early versions of CUDA simply polled shared sync locations, repeatedly reading
    the memory until the wait criterion had been achieved, but this approach is expensive
    and only works well when the application doesn’t have to wait long (i.e., the
    sync location doesn’t have to be read many times before exiting because the wait
    criterion has been satisfied). For most applications, interrupt-based schemes
    (exposed by CUDA as “blocking syncs”) are better because they enable the CPU to
    suspend the waiting thread until the GPU signals an interrupt. The driver maps
    the GPU interrupt to a platform-specific thread synchronization primitive, such
    as Win32 events or Linux signals, that can be used to suspend the CPU thread if
    the wait condition is not true when the application starts to wait.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA早期版本仅仅轮询共享同步位置，反复读取内存直到满足等待条件，但这种方法成本高昂，并且仅在应用程序不需要长时间等待时效果良好（即，在退出之前同步位置不需要被读取多次，因为等待条件已经满足）。对于大多数应用程序，基于中断的方案（CUDA称之为“阻塞同步”）更好，因为它们允许CPU挂起等待线程，直到GPU发出中断信号。驱动程序将GPU中断映射到特定平台的线程同步原语，例如Win32事件或Linux信号，这可以在应用程序开始等待时，如果等待条件不成立，则用于挂起CPU线程。
- en: Applications can force the context-wide synchronization to be blocking by specifying
    `CU_CTX_BLOCKING_SYNC` to `cuCtxCreate()` or `cudaDeviceBlockingSync` to `cudaSetDeviceFlags()`.
    It is preferable, however, to use blocking CUDA events (specify `CU_EVENT_BLOCKING_SYNC`
    to `cuEventCreate()` or `cudaEventBlockingSync` to `cudaEventCreate()`), since
    they are more fine-grained and interoperate seamlessly with any type of CUDA context.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以通过向 `cuCtxCreate()` 指定 `CU_CTX_BLOCKING_SYNC` 或向 `cudaSetDeviceFlags()`
    指定 `cudaDeviceBlockingSync` 强制执行上下文范围的同步为阻塞模式。然而，最好使用阻塞的 CUDA 事件（向 `cuEventCreate()`
    指定 `CU_EVENT_BLOCKING_SYNC` 或向 `cudaEventCreate()` 指定 `cudaEventBlockingSync`），因为它们更加细粒度，并能与任何类型的
    CUDA 上下文无缝互操作。
- en: Astute readers may be concerned that the CPU and GPU read and write this shared
    memory location without using atomic operations or other synchronization primitives.
    But since the CPU only reads the shared location, race conditions are not a concern.
    The worst that can happen is the CPU reads a “stale” value that causes it to wait
    a little longer than it would otherwise.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 聪明的读者可能会担心，CPU 和 GPU 在不使用原子操作或其他同步原语的情况下读取和写入这个共享内存位置。但由于 CPU 只是读取共享位置，因此竞争条件不是问题。最糟糕的情况是
    CPU 读取到一个“过时”的值，这会导致它等待比正常情况稍长的时间。
- en: Events and Timestamps
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 事件与时间戳
- en: The host interface has an onboard high-resolution timer, and it can write a
    timestamp at the same time it writes a 32-bit sync value. CUDA uses this hardware
    facility to implement the asynchronous timing features in CUDA events.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 主机接口具有一个内建的高分辨率定时器，并且它可以在写入 32 位同步值的同时写入时间戳。CUDA利用这个硬件设施来实现 CUDA 事件中的异步计时功能。
- en: 2.5.3\. The Host Interface and Intra-GPU Synchronization
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.3. 主机接口与 GPU 内部同步
- en: The GPU may contain multiple engines to enable concurrent kernel processing
    and memory copying. In this case, the driver will write commands that are dispatched
    to different engines that run concurrently. Each engine has its own command buffer
    and shared sync value, and the engine’s progress is tracked as described in [Figures
    2.26](ch02.html#ch02fig26) and [2.27](ch02.html#ch02fig27). [Figure 2.28](ch02.html#ch02fig28)
    shows this situation, with two copy engines and a compute engine operating in
    parallel. The host interface is responsible for reading the commands and dispatching
    them to the appropriate engine. In [Figure 2.28](ch02.html#ch02fig28), a host→device
    memcpy and two dependent operations—a kernel launch and a device→host memcpy—have
    been submitted to the hardware. In terms of CUDA programming abstractions, these
    operations are within the same stream. The stream is like a CPU thread, and the
    kernel launch was submitted to the stream after the memcpy, so the CUDA driver
    must insert GPU commands for intra-GPU synchronization into the command streams
    for the host interface.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: GPU可能包含多个引擎，以启用并发的内核处理和内存复制。在这种情况下，驱动程序将写入命令，并将这些命令分配给不同的引擎，这些引擎并发运行。每个引擎都有自己的命令缓冲区和共享同步值，并且如[图2.26](ch02.html#ch02fig26)和[2.27](ch02.html#ch02fig27)所述，跟踪引擎的进度。[图2.28](ch02.html#ch02fig28)展示了这种情况，其中两个复制引擎和一个计算引擎并行工作。主机接口负责读取命令并将其分配给适当的引擎。在[图2.28](ch02.html#ch02fig28)中，已将主机→设备的内存复制（memcpy）和两个依赖操作——内核启动和设备→主机的内存复制——提交给硬件。在CUDA编程抽象中，这些操作位于同一流中。流就像CPU线程，内核启动是在memcpy之后提交给该流的，因此CUDA驱动程序必须在命令流中插入用于GPU内同步的命令，以便主机接口进行处理。
- en: '![Image](graphics/02fig28.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02fig28.jpg)'
- en: '*Figure 2.28* Intra-GPU synchronization.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.28* GPU内部同步。'
- en: As [Figure 2.28](ch02.html#ch02fig28) shows, the host interface plays a central
    role in coordinating the needed synchronization for streams. When, for example,
    a kernel must not be launched until a needed memcpy is completed, the DMA unit
    can stop giving commands to a given engine until a shared sync location attains
    a certain value. This operation is similar to CPU/GPU synchronization, but the
    GPU is synchronizing different engines within itself.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图2.28](ch02.html#ch02fig28)所示，主机接口在协调流的所需同步方面起着核心作用。例如，当内核必须等到所需的memcpy完成后才能启动时，DMA单元可以停止向给定引擎发送命令，直到共享的同步位置达到某个值。这种操作类似于CPU/GPU同步，但GPU是在自身内部同步不同的引擎。
- en: The software abstraction layered on this hardware mechanism is a CUDA stream.
    CUDA streams are like CPU threads in that operations within a stream are sequential
    and multiple streams are needed for concurrency. Because the command buffer is
    shared between engines, applications must “software-pipeline” their requests in
    different streams. So instead of
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在该硬件机制上层的软件抽象是CUDA流。CUDA流类似于CPU线程，因为流内的操作是顺序的，并且需要多个流才能实现并发。由于命令缓冲区在不同的引擎之间共享，因此应用程序必须在不同的流中“软件流水化”其请求。因此，代替
- en: foreach stream
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 每个流
- en: Memcpy device←host
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Memcpy 设备←主机
- en: Launch kernel
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 启动内核
- en: Memcpy host←device
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: Memcpy 主机←设备
- en: they must implement
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 它们必须实现
- en: foreach stream
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历每个流
- en: Memcpy device←host
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Memcpy 设备←主机
- en: foreach stream
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历每个流
- en: Launch kernel
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 启动内核
- en: foreach stream
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历每个流
- en: Memcpy host←device
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Memcpy 主机←设备
- en: Without the software pipelining, the DMA engine will “break concurrency” by
    synchronizing the engines to preserve each stream’s model of sequential execution.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 没有软件管道化的情况下，DMA 引擎通过同步引擎来保持每个流的顺序执行模型，从而“打破并发性”。
- en: Multiple DMA Engines on Kepler
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Kepler 上的多个 DMA 引擎
- en: The latest Kepler-class hardware from NVIDIA implements a DMA unit per engine,
    obviating the need for applications to software-pipeline their streamed operations.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 最新的 Kepler 系列硬件为每个引擎实现了一个 DMA 单元，避免了应用程序需要对其流操作进行软件管道化。
- en: 2.5.4\. Inter-GPU Synchronization
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.4. 跨 GPU 同步
- en: Since the sync location in [Figures 2.26](ch02.html#ch02fig26) through [2.28](ch02.html#ch02fig28)
    is in host memory, it can be accessed by any of the GPUs in the system. As a result,
    in CUDA 4.0, NVIDIA was able to add inter-GPU synchronization in the form of `cudaStreamWaitEvent()`
    and `cuStreamWaitEvent()`. These API calls cause the driver to insert wait commands
    for the host interface into the current GPU’s command buffer, causing the GPU
    to wait until the given event’s sync value has been written. Starting with CUDA
    4.0, the event may or may not be signaled by the same GPU that is doing the wait.
    Streams have been promoted from being able to synchronize execution between hardware
    units on a single GPU to being able to synchronize execution between GPUs.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 由于[图 2.26](ch02.html#ch02fig26)到[2.28](ch02.html#ch02fig28)中的同步位置位于主机内存中，因此系统中的任何
    GPU 都可以访问它。因此，在 CUDA 4.0 中，NVIDIA 通过`cudaStreamWaitEvent()`和`cuStreamWaitEvent()`以跨
    GPU 同步的形式增加了这一功能。这些 API 调用使驱动程序在当前 GPU 的命令缓冲区中插入等待主机接口的命令，从而使 GPU 等待，直到给定事件的同步值被写入。从
    CUDA 4.0 开始，事件可能会由进行等待操作的同一 GPU 或其他 GPU 发出信号。流从能够在单个 GPU 上同步硬件单元之间的执行，提升到能够同步
    GPU 之间的执行。
- en: 2.6\. GPU Architecture
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6. GPU 架构
- en: Three distinct GPU architectures can run CUDA.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 三种不同的 GPU 架构可以运行 CUDA。
- en: • Tesla hardware debuted in 2006, in the GeForce 8800 GTX (G80).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: • Tesla 硬件于 2006 年首次亮相，搭载 GeForce 8800 GTX（G80）。
- en: • Fermi hardware debuted in 2010, in the GeForce GTX 480 (GF100).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: • Fermi 硬件于 2010 年首次亮相，搭载 GeForce GTX 480（GF100）。
- en: • Kepler hardware debuted in 2012, in the GeForce GTX 680 (GK104).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: • Kepler 硬件于 2012 年首次亮相，搭载 GeForce GTX 680（GK104）。
- en: The GF100/GK104 nomenclature refers to the ASIC that implements the GPU. The
    “K” and “F” in GK104 and GF100 refer to Kepler and Fermi, respectively.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: GF100/GK104 命名法指的是实现 GPU 的 ASIC。GK104 和 GF100 中的 “K” 和 “F” 分别指代 Kepler 和 Fermi
    架构。
- en: 'The Tesla and Fermi families followed an NVIDIA tradition in which they would
    first ship the huge, high-end flagship chip that would win benchmarks. These chips
    were expensive because NVIDIA’s manufacturing costs are closely related to the
    number of transistors (and thus the amount of die area) required to build the
    ASIC. The first large “win” chips would then be followed by smaller chips: half-size
    for the mid-range, quarter-size for the low end, and so on.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Tesla和Fermi系列继承了NVIDIA的传统：首先发布巨大的高端旗舰芯片，这些芯片在基准测试中取得胜利。这些芯片价格昂贵，因为NVIDIA的制造成本与制造ASIC所需的晶体管数量（因此与芯片面积）密切相关。首批大型“胜利”芯片之后，紧接着发布更小的芯片：中端为半尺寸，低端为四分之一尺寸，依此类推。
- en: In a departure from that tradition, NVIDIA’s first Kepler-class chip is targeted
    at the midrange; their “win” chip shipped months after the first Kepler-class
    chips became available. GK104 has 3.5B transistors, while GK110 has 7.1B transistors.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统不同，NVIDIA的第一款Kepler架构芯片面向中端市场；他们的“胜利”芯片在第一批Kepler架构芯片上市几个月后才发货。GK104拥有35亿个晶体管，而GK110拥有71亿个晶体管。
- en: 2.6.1\. Overview
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.1\. 概述
- en: CUDA’s simplified view of the GPU includes the following.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA对GPU的简化视图包括以下内容。
- en: • A host interface that connects the GPU to the PCI Express bus
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: • 一个连接GPU与PCI Express总线的主机接口
- en: • 0 to 2 copy engines
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: • 0到2个复制引擎
- en: • A DRAM interface that connects the GPU to its device memory
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: • 一个连接GPU与其设备内存的DRAM接口
- en: • Some number of TPCs or GPCs (texture processing clusters or graphics processing
    clusters), each of which contains caches and some number of streaming multiprocessors
    (SMs)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: • 一些TPC或GPC（纹理处理集群或图形处理集群），每个集群包含缓存和一定数量的流式多处理器（SMs）
- en: The architectural papers cited at the end of this chapter give the full story
    on GPU functionality in CUDA-capable GPUs, including graphics-specific functionality
    like antialiased rendering support.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 本章末尾引用的架构论文提供了有关CUDA支持GPU功能的完整故事，包括诸如抗锯齿渲染支持等图形特定功能。
- en: Host Interface
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 主机接口
- en: The host interface implements the functionality described in the previous section.
    It reads GPU commands (such as memcpy and kernel launch commands) and dispatches
    them to the appropriate hardware units, and it also implements the facilities
    for synchronization between the CPU and GPU, between different engines on the
    GPU, and between different GPUs. In CUDA, the host interface’s functionality primarily
    is exposed via the Stream and Event APIs (see [Chapter 6](ch06.html#ch06)).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 主机接口实现了上一节描述的功能。它读取GPU命令（如memcpy和内核启动命令）并将其分派给适当的硬件单元，同时还实现了CPU和GPU之间、GPU上不同引擎之间以及不同GPU之间的同步机制。在CUDA中，主机接口的功能主要通过Stream和Event
    API暴露（见[第6章](ch06.html#ch06)）。
- en: Copy Engine(s)
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 复制引擎
- en: Copy engines can perform host↔device memory transfers while the SMs are doing
    computations. The earliest CUDA hardware did not have any copy engines; subsequent
    versions of the hardware included a copy engine that could transfer linear device
    memory (but not CUDA arrays), and the most recent CUDA hardware includes up to
    two copy engines that can convert between CUDA arrays and linear memory while
    saturating the PCI Express bus.^([13](ch02.html#ch02fn13))
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[13](ch02.html#ch02fn13a). More than two copy engines doesn’t really make sense,
    since each engine can saturate one of the two directions of PCI Express.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: DRAM Interface
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The GPU-wide DRAM interface, which supports bandwidths in excess of 100 GB/s,
    includes hardware to coalesce memory requests. More recent CUDA hardware has more
    sophisticated DRAM interfaces. The earliest (SM 1.x) hardware had onerous coalescing
    requirements, requiring addresses to be contiguous and 64-, 128-, or 256-byte
    aligned (depending on the operand size). Starting with SM 1.2 (the GT200 or GeForce
    GTX 280), addresses could be coalesced based on locality, regardless of address
    alignment. Fermi-class hardware (SM 2.0 and higher) has a write-through L2 cache
    that provides the benefits of the SM 1.2 coalescing hardware and additionally
    improves performance when data is reused.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: TPCs and GPCs
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: TPCs and GPCs are units of hardware that exist between the full GPU and the
    streaming multiprocessors that perform CUDA computation. Tesla-class hardware
    groups the SMs in “TPCs” (texture processing clusters) that contain texturing
    hardware support (in particular, a texture cache) and two or three streaming multiprocessors,
    described below. Fermi-class hardware groups the SMs in “GPCs” (graphics processing
    clusters) that each contain a raster unit and four SMs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, CUDA developers need not concern themselves with TPCs or
    GPCs because streaming multiprocessors are the most important unit of abstraction
    for computational hardware.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Contrasting Tesla and Fermi
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 对比特斯拉与费米
- en: The first generation of CUDA-capable GPUs was code-named Tesla, and the second,
    Fermi. These were confidential code names during development, but NVIDIA decided
    to use them as external product names to describe the first two generations of
    CUDA-capable GPU. To add to the confusion, NVIDIA chose the name “Tesla” to describe
    the server-class boards used to build compute clusters out of CUDA machines.^([14](ch02.html#ch02fn14))
    To distinguish between the expensive server-class Tesla boards and the architectural
    families, this book refers to the architectural families as “Tesla-class hardware,”
    “Fermi-class hardware,” and “Kepler-class hardware.”
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 第一代支持CUDA的GPU代号为特斯拉，第二代为费米。这些在开发过程中是保密的代号，但NVIDIA决定将它们用作外部产品名称，以描述第一代和第二代支持CUDA的GPU。更令人困惑的是，NVIDIA还使用“特斯拉”这一名称来描述用于构建CUDA计算机集群的服务器级显卡。^([14](ch02.html#ch02fn14))
    为了区分高价的服务器级特斯拉显卡和架构系列，本书将架构系列称为“特斯拉级硬件”、“费米级硬件”和“凯普勒级硬件”。
- en: '[14](ch02.html#ch02fn14a). Of course, when the Tesla brand name was chosen,
    Fermi-class hardware did not exist. The marketing department told us engineers
    that it was just a coincidence that the architectural codename and the brand name
    were both “Tesla”!'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[14](ch02.html#ch02fn14a)。当然，当选择“特斯拉”这个品牌名称时，费米级硬件尚不存在。市场部门告诉我们工程师，这只是一个巧合，架构代号和品牌名称都叫“特斯拉”！'
- en: '* * *'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: All of the differences between Tesla-class hardware and Fermi-class hardware
    also apply to Kepler.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉级硬件与费米级硬件之间的所有差异同样适用于凯普勒。
- en: '* * *'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Early Tesla-class hardware is subject to onerous performance penalties (up to
    6x) when running code that performs uncoalesced memory transactions. Later implementations
    of Tesla-class hardware, starting with the GeForce GTX 280, decreased the penalty
    for uncoalesced transactions to about 2x. Tesla-class hardware also has performance
    counters that enable developers to measure how many memory transactions are uncoalesced.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的特斯拉级硬件在运行执行未合并内存事务的代码时，会面临严重的性能惩罚（最高可达6倍）。而从GeForce GTX 280开始，后续的特斯拉级硬件将未合并事务的惩罚降低到了约2倍。特斯拉级硬件还配备了性能计数器，允许开发者衡量有多少内存事务是未合并的。
- en: Tesla-class hardware only included a 24-bit integer multiplier, so developers
    must use intrinsics such as `__mul24()` for best performance. Full 32-bit multiplication
    (i.e., the native operator * in CUDA) is emulated with a small instruction sequence.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉级硬件只包含24位整数乘法器，因此开发者必须使用像`__mul24()`这样的内建函数来获得最佳性能。完整的32位乘法（即CUDA中的原生操作符*）是通过一小段指令序列模拟实现的。
- en: Tesla-class hardware initialized shared memory to zero, while Fermi-class hardware
    leaves it uninitialized. For applications using the driver API, one subtle side
    effect of this behavior change is that applications that used `cuParamSeti()`
    to pass pointer parameters on 64-bit platforms do not work correctly on Fermi.
    Since parameters were passed in shared memory on Tesla class hardware, the uninitialized
    top half of the parameter would become the most significant 32 bits of the 64-bit
    pointer.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Double-precision support was introduced with SM 1.3 on the GT200, the second-generation
    “win” chip of the Tesla family.^([15](ch02.html#ch02fn15)) At the time, the feature
    was considered speculative, so it was implemented in an area-efficient manner
    that could be added and subtracted from the hardware with whatever ratio of double-to-single
    performance NVIDIA desired (in the case of GT200, this ratio was 1:8). Fermi integrated
    double-precision support much more tightly and at higher performance.^([16](ch02.html#ch02fn16))
    Finally, for graphics applications, Tesla-class hardware was the first DirectX
    10-capable hardware.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[15](ch02.html#ch02fn15a). In fact, the only difference between SM 1.2 and
    SM 1.3 is that SM 1.3 supports double precision.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[16](ch02.html#ch02fn16a). In SM 3.x, NVIDIA has decoupled double precision
    floating-point performance from the SM version, so GK104 has poor double precision
    performance and GK110 has excellent double precision performance.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Fermi-class hardware is much more capable than Tesla-class hardware. It supports
    64-bit addressing; it added L1 and L2 cache hardware; it added a full 32-bit integer
    multiply instruction and new instructions specifically to support the Scan primitive;
    it added surface load/store operations so CUDA kernels could read and write CUDA
    arrays without using the texture hardware; it was the first family of GPUs to
    feature multiple copy engines; and it improved support for C++ code, such as virtual
    functions.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Fermi-class hardware does not include the performance counters needed to track
    uncoalesced memory transactions. Also, because it does not include a 24-bit multiplier,
    Fermi-class hardware may incur a small performance penalty when running code that
    uses the 24-bit multiplication intrinsics. On Fermi, using operator * for multiplication
    is the fast path.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi类硬件不包括用于跟踪未合并内存事务的性能计数器。此外，由于它不包括24位乘法器，因此在运行使用24位乘法内建函数的代码时，Fermi类硬件可能会出现小的性能损失。在Fermi上，使用*操作符进行乘法是快速路径。
- en: For graphics applications, Fermi-class hardware can run DirectX 11\. [Table
    2.1](ch02.html#ch02tab01) summarizes the differences between Tesla- and Fermi-class
    hardware.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图形应用程序，Fermi类硬件可以运行DirectX 11\. [表2.1](ch02.html#ch02tab01)总结了Tesla类和Fermi类硬件之间的差异。
- en: '![Image](graphics/02tab01.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/02tab01.jpg)'
- en: '*Table 2.1* Differences between Tesla- and Fermi-Class Hardware'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '*表2.1* Tesla类和Fermi类硬件之间的差异'
- en: Texturing Niceties
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 纹理处理细节
- en: A subtle difference between Tesla- and Fermi-class hardware is that on Tesla-class
    hardware, the instructions to perform texturing overwrite the input register vector
    with the output. On Fermi-class hardware, the input and output register vectors
    can be different. As a result, Tesla-class hardware may have extra instructions
    to move the texture coordinates into the input registers where they will be overwritten.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: Tesla类和Fermi类硬件之间的一个微妙区别是，在Tesla类硬件上，执行纹理处理的指令会用输出覆盖输入寄存器向量。而在Fermi类硬件上，输入和输出寄存器向量可以是不同的。因此，Tesla类硬件可能需要额外的指令将纹理坐标移动到输入寄存器中，在那里它们将被覆盖。
- en: Another subtle difference between Tesla- and Fermi-class hardware is that when
    texturing from 1D CUDA arrays, Fermi-class hardware emulates this functionality
    using 2D textures with the second coordinate always set to 0.0\. Since this emulation
    only costs an extra register and very few extra instructions, the difference will
    be noticed by very few applications.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: Tesla和Fermi类硬件之间的另一个微妙区别是，当从1D CUDA数组进行纹理处理时，Fermi类硬件使用2D纹理进行此功能的仿真，并且第二个坐标始终设置为0.0\。由于这种仿真只需要额外的一个寄存器和很少的额外指令，因此只有极少数应用程序会注意到这个差异。
- en: 2.6.2\. Streaming Multiprocessors
  id: totrans-300
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.6.2\. 流式多处理器
- en: The workhorse of the GPU is the streaming multiprocessor, or SM. As mentioned
    in the previous section, each TPC in SM 1.x hardware contains 2 or 3 SMs, and
    each GPC in SM 2.x hardware contains 4 SMs. The very first CUDA-capable GPU, the
    G80 or GeForce 8800 GTX, contained 8 TPCs; at 2 SMs per TPC, that is a total of
    16 SMs. The next big CUDA-capable GPU, the GT200 or GeForce GTX 280, increased
    the number of SMs/TPC to 3 and contained 10 TPCs, for a total of 30 SMs.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: GPU的工作马是流式多处理器（SM）。正如上一节所提到的，SM 1.x硬件中的每个TPC包含2或3个SM，而SM 2.x硬件中的每个GPC包含4个SM。第一个支持CUDA的GPU——G80或GeForce
    8800 GTX，包含8个TPC；每个TPC有2个SM，总共有16个SM。下一个大规模支持CUDA的GPU——GT200或GeForce GTX 280，将每个TPC的SM数量增加到3，并包含10个TPC，总共30个SM。
- en: The number of SMs in a CUDA GPU may range from 2 to several dozen, and each
    SM contains
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: • Execution units to perform 32-bit integer and single- and double-precision
    floating-point arithmetic
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: • Special function units (SFUs) to compute single-precision approximations of
    log/exp, sin/cos, and rcp/rsqrt
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: • A warp scheduler to coordinate instruction dispatch to the execution units
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: • A constant cache to broadcast data to the SMs
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory for data interchange between threads
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: • Dedicated hardware for texture mapping
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2.29](ch02.html#ch02fig29) shows a Tesla-class streaming multiprocessor
    (SM 1.x). It contains 8 streaming processors that support 32-bit integer and single-precision
    floating-point arithmetic. The first CUDA hardware did not support double precision
    at all, but starting with GT200, the SMs may include one double-precision floating-point
    unit.^([17](ch02.html#ch02fn17))'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[17](ch02.html#ch02fn17a). GT200 added a few instructions as well as double
    precision (such as shared memory atomics), so the GT200 instruction set without
    double precision is SM 1.2 and with double precision is SM 1.3.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig29.jpg)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.29* Streaming multiprocessor 1.x.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2.30](ch02.html#ch02fig30) shows a Fermi-class streaming multiprocessor
    (SM 2.0). Unlike Tesla-class hardware, which implemented double-precision floating-point
    support separately, each Fermi-class SM has full double-precision support. The
    double-precision instructions execute slower than single precision, but since
    the ratio is more favorable than the 8:1 ratio of Tesla-class hardware, overall
    double-precision performance is much higher.^([18](ch02.html#ch02fn18))'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[18](ch02.html#ch02fn18a). For Kepler-class hardware, NVIDIA can tune floating-point
    performance to the target market of the GPU.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig30.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.30* SM 2.0 (Fermi).'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2.31](ch02.html#ch02fig31) shows an updated Fermi-class streaming multiprocessor
    (SM 2.1) that may be found in, for example, the GF104 chip. For higher performance,
    NVIDIA chose to increase the number of streaming processors per SM to 48\. The
    SFU-to-SM ratio is increased from 1:8 to 1:6.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig31.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.31* SM 2.1 (Fermi).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2.32](ch02.html#ch02fig32) shows the most recent (as of this writing)
    streaming multiprocessor design, featured in the newest Kepler-class hardware
    from NVIDIA. This design is so different from previous generations that NVIDIA
    calls it “SMX” (next-generation SM). The number of cores is increased by a factor
    of 6 to 192, and each SMX is much larger than analogous SMs in previous-generation
    GPUs. The largest Fermi GPU, GF110, had about 3 billion transistors containing
    16 SMs; the GK104 has 3.5 billion transistors and much higher performance but
    only 8 SMX’s. For area savings and power efficiency reasons, NVIDIA greatly increased
    the resources per SM, with the conspicuous exception of the shared memory/L1 cache.
    Like Fermi’s SMs, each Kepler SMX has 64K of cache that can be partitioned as
    48K L1/16K shared or 48K shared/16K L1\. The main implication for CUDA developers
    is that on Kepler, developers have even more incentive to keep data in registers
    (as opposed to L1 cache or shared memory) than on previous architectures.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig32.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.32* SM 3.0 (SMX).'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 2.7\. Further Reading
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NVIDIA has white papers on their Web site that describe the Fermi and Kepler
    architectures in detail. The following white paper describes Fermi.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: The Next Generation of NVIDIA GeForce GPU [www.nvidia.com/object/GTX_400_architecture.html](http://www.nvidia.com/object/GTX_400_architecture.html)
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: The following white paper describes the Kepler architecture and its implementation
    in the NVIDIA GeForce GTX 680 (GK104).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[www.geforce.com/Active/en_US/en_US/pdf/GeForce-GTX-680-Whitepaper-FINAL.pdf](http://www.geforce.com/Active/en_US/en_US/pdf/GeForce-GTX-680-Whitepaper-FINAL.pdf)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA engineers also have published several architectural papers that give
    more detailed descriptions of the various CUDA-capable GPUs.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: 'Lindholm, E., J. Nickolls, S. Oberman, and J. Montrym. NVIDIA Tesla: A unified
    graphics and computing architecture. *IEEE Micro* 28 (2), March–April 2008, pp.
    39–55.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Wittenbrink, C., E. Kilgariff, and A. Prabhu. Fermi GF100 GPU architecture.
    *IEEE Micro* 31 (2), March–April 2011, pp. 50–59.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Wong et al. used CUDA to develop microbenchmarks and clarify some aspects of
    Tesla-class hardware architecture.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Wong, H., M. Papadopoulou, M. Sadooghi-Alvandi, and A. Moshovos. Demystifying
    GPU microarchitecture through microbenchmarking. 2010 IEEE International Symposium
    on Performance Analysis of Systems and Software (IPSASS), March 28–30, 2010, pp.
    235–246.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3\. Software Architecture
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter provides an overview of the CUDA software architecture. [Chapter
    2](ch02.html#ch02) gave an overview of the hardware platform and how it interacts
    with CUDA, and we’ll start this chapter with a description of the software platforms
    and operating environments supported by CUDA. Next, each software abstraction
    in CUDA is briefly described, from devices and contexts to modules and kernels
    to memory. This section may refer back to [Chapter 2](ch02.html#ch02) when describing
    how certain software abstractions are supported by the hardware. Finally, we spend
    some time contrasting the CUDA runtime and driver API and examining how CUDA source
    code is translated into microcode that operates on the GPU. Please remember that
    this chapter is just an overview. Most of the topics covered are described in
    more detail in later chapters.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Software Layers
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 3.1](ch03.html#ch03fig01) shows the different layers of software in
    a CUDA application, from the application itself to the CUDA driver that operates
    the GPU hardware. All of the software except the kernel mode driver operate in
    the target operating system’s unprivileged user mode. Under the security models
    of modern multitasking operating systems, user mode is “untrusted,” and the hardware
    and operating system software must take measures to strictly partition applications
    from one another. In the case of CUDA, that means host and device memory allocated
    by one CUDA program cannot be accessed by other CUDA programs. The only exceptions
    happen when these programs specifically request memory sharing, which must be
    provided by the kernel mode driver.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03fig01.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3.1* Software layers in CUDA.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: CUDA libraries, such as cuBLAS, are built on top of the CUDA runtime or driver
    API. The CUDA runtime is the library targeted by CUDA’s integrated C++/GPU toolchain.
    When the `nvcc` compiler splits `.cu` files into host and device portions, the
    host portion contains automatically generated calls to the CUDA runtime to facilitate
    operations such as the kernel launches invoked by `nvcc`’s special triple-angle
    bracket `<<< >>>` syntax.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA driver API, exported directly by CUDA’s user mode driver, is the lowest-level
    API available to CUDA apps. The driver API calls into the user mode driver, which
    may in turn call the kernel mode driver to perform operations such as memory allocation.
    Functions in the driver API and CUDA runtime generally start with `cu*()` and
    `cuda*()`, respectively. Many functions, such as `cudaEventElapsedTime(),` are
    essentially identical, with the only difference being in the prefix.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. CUDA Runtime and Driver
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CUDA runtime (often abbreviated CUDART) is the library used by the language
    integration features of CUDA. Each version of the CUDA toolchain has its own specific
    version of the CUDA runtime, and programs built with that toolchain will automatically
    link against the corresponding version of the runtime. A program will not run
    correctly unless the correct version of CUDART is available in the path.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA driver is designed to be backward compatible, supporting all programs
    written against its version of CUDA, or older ones. It exports a low-level “driver
    API” (in `cuda.h`) that enables developers to closely manage resources and the
    timing of initialization. The driver version may be queried by calling `cuDriverGetVersion().`
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuDriverGetVersion(int *driverVersion);
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: This function passes back a decimal value that gives the version of CUDA supported
    by the driver—for example, 3010 for CUDA 3.1 and 5000 for CUDA 5.0.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3.1](ch03.html#ch03tab01) summarizes the features that correspond to
    the version number passed back by `cuDriverGetVersion()`. For CUDA runtime applications,
    this information is given by the `major` and `minor` members of the `cudaDeviceProp`
    structure as described in [Section 3.2.2](ch03.html#ch03lev2sec5).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab01.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.1* CUDA Driver Features'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime requires that the installed driver have a version greater than
    or equal to the version of CUDA supported by the runtime. If the driver version
    is older than the runtime version, the CUDA application will fail to initialize
    with the error `cudaErrorInsufficientDriver` (35). CUDA 5.0 introduced the *device
    runtime*, a subset of the CUDA runtime that can be invoked from CUDA kernels.
    A detailed description of the device runtime is given in [Chapter 7](ch07.html#ch07).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Driver Models
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Other than Windows Vista and subsequent releases of Windows, all of the operating
    systems that CUDA runs—Linux, MacOS, and Windows XP—access the hardware with *user
    mode client drivers*. These drivers sidestep the requirement, common to all modern
    operating systems, that hardware resources be manipulated by kernel code. Modern
    hardware such as GPUs can finesse that requirement by mapping certain hardware
    registers—such as the hardware register used to submit work to the hardware—into
    user mode. Since user mode code is not trusted by the operating system, the hardware
    must contain protections against rogue writes to the user mode hardware registers.
    The goal is to prevent user mode code from prompting the hardware to use its direct
    memory access (DMA) facilities to read or write memory that it should not (such
    as the operating system’s kernel code!).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Hardware designers protect against memory corruption by introducing a level
    of indirection into the command stream available to user mode software so DMA
    operations can only be initiated on memory that previously was validated and mapped
    by kernel code; in turn, driver developers must carefully validate their kernel
    code to ensure that it only gives access to memory that should be made available.
    The end result is a driver that can operate at peak efficiency by submitting work
    to the hardware without having to incur the expense of a kernel transition.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: Many operations, such as memory allocation, still require kernel mode transitions
    because editing the GPU’s page tables can only be done in kernel mode. In this
    case, the user mode driver may take steps to reduce the number of kernel mode
    transitions—for example, the CUDA memory allocator tries to satisfy memory allocation
    requests out of a pool.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Unified Virtual Addressing
  id: totrans-354
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Unified virtual addressing, described in detail in [Section 2.4.5](ch02.html#ch02lev2sec9),
    is available on 64-bit Linux, 64-bit XPDDM, and MacOS. On these platforms, it
    is made available transparently. As of this writing, UVA is not available on WDDM.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Windows Display Driver Model
  id: totrans-356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For Windows Vista, Microsoft introduced a new desktop presentation model in
    which the screen output was composed in a back buffer and page-flipped, like a
    video game. The new “Windows Desktop Manager” (WDM) made more extensive use of
    GPUs than Windows had previously, so Microsoft decided it would be best to revise
    the GPU driver model in conjunction with the presentation model. The resulting
    Windows Display Driver Model (WDDM) is now the default driver model on Windows
    Vista and subsequent versions. The term XPDDM was created to refer to the driver
    model used for GPUs on previous versions of Windows.^([1](ch03.html#ch03fn1))
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch03.html#ch03fn1a). Tesla boards (CUDA-capable boards that do not have
    a display output) can use XPDDM on Windows, called the Tesla Compute Cluster (TCC)
    driver, and can be toggled with the `nvidia-smi` tool.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: As far as CUDA is concerned, these are the two major changes made by WDDM.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** WDDM does not permit hardware registers to be mapped into user mode.
    Hardware commands—even commands to kick off DMA operations—must be invoked by
    kernel code. The user→kernel transition is too expensive for the user mode driver
    to submit each command as it arrives, so instead the user mode driver buffers
    commands for later submission.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** Since WDDM was built to enable many applications to use a GPU concurrently,
    and GPUs do not support demand paging, WDDM includes facilities to emulate paging
    on a “memory object” basis. For graphics applications, memory objects may be render
    targets, Z buffers, or textures; for CUDA, memory objects include global memory
    and CUDA arrays. Since the driver must set up access to CUDA arrays before each
    kernel invocation, CUDA arrays can be swapped by WDDM. For global memory, which
    resides in a linear address space (where pointers can be stored), every memory
    object for a given CUDA context must be resident in order for a CUDA kernel to
    launch.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: The main effect of WDDM due to number 1 above is that work requested of CUDA,
    such as kernel launches or asynchronous memcpy operations, generally is not submitted
    to the hardware immediately.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'The accepted idiom to force pending work to be submitted is to query the NULL
    stream: `cudaStreamQuery(0)` or `cuStreamQuery(NULL)`. If there is no pending
    work, these calls will return quickly. If any work is pending, it will be submitted,
    and since the call is asynchronous, execution may be returned to the caller before
    the hardware has finished processing. On non-WDDM platforms, querying the NULL
    stream is always fast.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: The main effect of WDDM due to number 2 above is that CUDA’s control of memory
    allocation is much less concrete. On user mode client drivers, successful memory
    allocations mean that the memory has been allocated and is no longer available
    to any other operating system client (such as a game or other CUDA application
    that may be running). On WDDM, if there are applications competing for time on
    the same GPU, Windows can and will swap memory objects out in order to enable
    each application to run. The Windows operating system tries to make this as efficient
    as possible, but as with all paging, having it *never* happen is much faster than
    having it *ever* happen.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: Timeout Detection and Recovery
  id: totrans-365
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Because Windows uses the GPU to interact with users, it is important that compute
    applications not take inordinate amounts of GPU time. Under WDDM, Windows enforces
    a timeout (default of 2 seconds) that, if it should elapse, will cause a dialog
    box that says “Display driver stopped responding and has recovered,” and the display
    driver is restarted. If this happens, all work in the CUDA context is lost. See
    [http://bit.ly/16mG0dX](http://bit.ly/16mG0dX).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Tesla Compute Cluster Driver
  id: totrans-367
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For compute applications that do not need WDDM, NVIDIA provides the Tesla Compute
    Cluster (TCC) driver, available only for Tesla-class boards. The TCC driver is
    a user mode client driver, so it does not require a kernel thunk to submit work
    to the hardware. The TCC driver may be enabled and disabled using the `nvidia-smi`
    tool.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3\. `nvcc`, PTX, and Microcode
  id: totrans-369
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`nvcc` is the compiler driver used by CUDA developers to turn source code into
    functional CUDA applications. It can perform many functions, from as complex as
    compiling, linking, and executing a sample program in one command (a usage encouraged
    by many of the sample programs in this book) to a simple targeted compilation
    of a GPU-only `.cu` file.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3.2](ch03.html#ch03fig02) shows the two recommended workflows for using
    `nvcc` for CUDA runtime and driver API applications, respectively. For applications
    larger than the most trivial size, `nvcc` is best used strictly for purposes of
    compiling CUDA code and wrapping CUDA functionality into code that is callable
    from other tools. This is due to `nvcc`’s limitations.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: • `nvcc` only works with a specific set of compilers. Many CUDA developers never
    notice because their compiler of choice happens to be in the set of supported
    compilers. But in production software development, the amount of CUDA code tends
    to be minuscule compared to the amount of other code, and the presence or absence
    of CUDA support may not be the dominant factor in deciding which compiler to use.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: • `nvcc` makes changes to the compile environment that may not be compatible
    with the build environment for the bulk of the application.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: • `nvcc` “pollutes” the namespace with nonstandard built-in types (e.g., `int2`)
    and intrinsic names (e.g., `__popc()`). Only in recent versions of CUDA have the
    intrinsics symbols become optional and can be used by including the appropriate
    `sm_*_intrinsics.h` header.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03fig02.jpg)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3.2* `nvcc` workflows.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: For CUDA runtime applications, `nvcc` embeds GPU code into string literals in
    the output executable. If the `--fatbin` option is specified, the executable will
    automatically load suitable microcode for the target GPU or, if no microcode is
    available, have the driver automatically compile the PTX into microcode.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '`nvcc` and PTX'
  id: totrans-378
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: PTX (“Parallel Thread eXecution”) is the intermediate representation of compiled
    GPU code that can be compiled into native GPU microcode. It is the mechanism that
    enables CUDA applications to be “future-proof” against instruction set innovations
    by NVIDIA—as long as the PTX for a given CUDA kernel is available, the CUDA driver
    can translate it into microcode for whichever GPU the application happens to be
    running on (even if the GPU was not available when the code was written).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: PTX can be compiled into GPU microcode both “offline” and “online.” Offline
    compilation refers to building software that will be executed by some computer
    in the future. For example, [Figure 3.2](ch03.html#ch03fig02) highlights the offline
    portions of the CUDA compilation process. Online compilation, otherwise known
    as “just-in-time” compilation, refers to compiling intermediate code (such as
    PTX) for the computer running the application for immediate execution.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '`nvcc` can compile PTX offline by invoking the PTX assembler `ptxas`, which
    compiles PTX into the native microcode for a specific version of GPU. The resulting
    microcode is emitted into a CUDA binary called a “cubin” (pronounced like “Cuban”).
    Cubin files can be disassembled with `cuobjdump --dump-sass;` this will dump the
    SASS mnemonics for the GPU-specific microcode.^([2](ch03.html#ch03fn2))'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch03.html#ch03fn2a). Examining the SASS code is a key strategy to help
    drive optimization.'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: PTX also can be compiled online (JITted) by the CUDA driver. Online compilation
    happens automatically when running CUDART applications that were built with the
    `--fatbin` option (which is the default). `.cubin` and PTX representations of
    every kernel are included in the executable, and if it is run on hardware that
    doesn’t support any of the .cubin representations, the driver compiles the PTX
    version. The driver caches these compiled kernels on disk, since compiling PTX
    can be time consuming.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Finally, PTX can be generated at runtime and compiled explicitly by the driver
    by calling `cuModuleLoadEx()`. The driver API does not automate any of the embedding
    or loading of GPU microcode. Both `.cubin` and `.ptx` files can be given to `cuModuleLoadEx()`;
    if a `.cubin` is not suitable for the target GPU architecture, an error will be
    returned. A reasonable strategy for driver API developers is to compile and embed
    PTX, and they should always JIT-compile it onto the GPU with `cuModuleLoadEx()`,
    relying on the driver to cache the compiled microcode.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Devices and Initialization
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Devices correspond to physical GPUs. When CUDA is initialized (either explicitly
    by calling the driver API’s `cuInit()` function or implicitly by calling a CUDA
    runtime function), the CUDA driver enumerates the available devices and creates
    a global data structure that contains their names and immutable capabilities such
    as the amount of device memory and maximum clock rate.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: For some platforms, NVIDIA includes a tool that can set policies with respect
    to specific devices. The `nvidia-smi` tool sets the policy with respect to a given
    GPU. For example, `nvidia-smi` can be used to enable and disable ECC (error correction)
    on a given GPU. `nvidia-smi` also can be used to control the number of CUDA contexts
    that can be created on a given device. These are the possible modes.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '• Default: Multiple CUDA contexts may be created on the device.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '• “Exclusive” mode: One CUDA context may be created on the device.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '• “Prohibited”: No CUDA context may be created on the device.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: If a device is enumerated but you are not able to create a context on that device,
    it is likely the device is in “prohibited” mode or in “exclusive” mode and another
    CUDA context already has been created on that device.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Device Count
  id: totrans-392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The application can discover how many CUDA devices are available by calling
    `cuDeviceGetCount()` or `cudaGetDeviceCount()`. Devices can then be referenced
    by an index in the range [0..*DeviceCount*-1]. The driver API requires applications
    to call `cuDeviceGet()` to map the device index to a device handle (`CUdevice`).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Device Attributes
  id: totrans-394
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Driver API applications can query the name of a device by calling `cuDeviceGetName()`
    and query the amount of global memory by calling `cuDevice-TotalMem()`. The major
    and minor compute capabilities of the device (i.e., the SM version, such as 2.0
    for the first Fermi-capable GPUs) can be queried by calling `cuDeviceComputeCapability(``)`.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: CUDA runtime applications can call `cudaGetDeviceProperties()`, which will pass
    back a structure containing the name and properties of the device. [Table 3.2](ch03.html#ch03tab02)
    gives the descriptions of the members of `cudaDeviceProp`, the structure passed
    back by `cudaGetDeviceProperties(``)`.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab02.jpg)![Image](graphics/03tab02a.jpg)![Image](graphics/03tab02b.jpg)![Image](graphics/03tab02c.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.2.* `cudaDeviceProp` Members'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: The driver API’s function for querying device attributes, `cuDeviceGetAttribute()`,
    can pass back one attribute at a time, depending on the `CUdevice_attribute` parameter.
    In CUDA 5.0, the CUDA runtime added the same function in the form of `cudaDeviceGetAttribute()`,
    presumably because the structure-based interface was too cumbersome to run on
    the device.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. When CUDA Is Not Present
  id: totrans-400
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CUDA runtime can run on machines that cannot run CUDA or that do not have
    CUDA installed; if `cudaGetDeviceCount()` returns `cudaSuccess` and a nonzero
    device count, CUDA is available.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: When using the driver API, executables that link directly against `nvcuda.dll`
    (Windows) or `libcuda.so` (Linux) will not load unless the driver binary is available.
    For driver API applications that require CUDA, trying to launch an application
    that was linked directly against the driver will result in an error such as in
    [Figure 3.3](ch03.html#ch03fig03).
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03fig03.jpg)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3.3* Error when CUDA is not present (Windows).'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: For those applications that must run with or without CUDA, the CUDA SDK provides
    a set of header files and a C source file that wrap the driver API such that the
    application can check for CUDA without having the operating system signal an exception.
    These files, in the `dynlink` subdirectory `<SDKRoot>/C/common/inc/dynlink`, can
    be included in lieu of the core CUDA files. They interpose an intermediate set
    of functions that lazily load the CUDA libraries if CUDA is available.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s compare two programs that use the driver API to initialize
    CUDA and write the name of each device in the system. [Listing 3.1](ch03.html#ch03lis01)
    gives `init_hardcoded.cpp`, a file that can be compiled against the CUDA SDK with
    the following command line.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: nvcc –oinit_hardcoded –I ../chLib init_hardcoded.cpp -lcuda
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Using `nvcc` to compile a C++ file that doesn’t include any GPU code is just
    a convenient way to pick up the CUDA headers. The `–oinit_hardcoded` at the beginning
    specifies the root name of the output executable. The `-lcuda` at the end causes
    `nvcc` to link against the driver API’s library; without it, the build will fail
    with link errors. This program hard-links against the CUDA driver API, so it will
    fail on systems that don’t have CUDA installed.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 3.1.* Initialization (hard-coded).'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch03_images.html#p03lis01a)'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: /*
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '* init_hardcoded.cpp'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '*'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '#include <stdio.h>'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '#include <cuda.h>'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '#include <chError.h>'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: int
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: main()
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: CUresult status;
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: int numDevices;
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuInit( 0 ) );
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGetCount( &numDevices ) );
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%d devices detected:\n", numDevices );
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < numDevices; i++ ) {
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: char szName[256];
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: CUdevice device;
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGet( &device, i ) );
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGetName( szName, 255, device ) );
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: printf( "\t%s\n", szName );
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: return 0;
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: 'fprintf( stderr, "CUDA failure code: 0x%x\n", status );'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: return 1;
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 3.2](ch03.html#ch03lis02) gives a program that will work on systems
    without CUDA. As you can see, the source code is identical except for a few lines
    of code.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '#include <cuda.h>'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: is replaced by
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch03_images.html#p065pro01a)'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '#include "cuda_drvapi_dynlink.c"'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '#include "dynlink/cuda_drvapi_dynlink.h"'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: and the `cuInit()` call has been changed to specify a CUDA version.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuInit(0) );
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: is replaced by
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuInit( 0, 4010 ) );
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Here, passing `4010` as the second parameter requests CUDA 4.1, and the function
    will fail if the system doesn’t include that level of functionality.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: Note that you could compile and link `cuda_drvapi_dynlink.c` into the application
    separately instead of `#include`’ing it into a single source file. The header
    file and C file work together to interpose a set of wrapper functions onto the
    driver API. The header uses the preprocessor to rename the driver API functions
    to wrapper functions declared in `cuda_drvapi_dynlink.h` (e.g., calls to `cuCtxCreate()`
    become calls to `tcuCtxCreate()`). On CUDA-capable systems, the driver DLL is
    loaded dynamically, and the wrapper functions call pointers-to-function that are
    obtained from the driver DLL during initialization. On non-CUDA-capable systems,
    or if the driver does not support the request CUDA version, the initialization
    function returns an error.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 3.2.* Initialization (dynlink).'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch03_images.html#p03lis02a)'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: /*
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '* init_dynlink.cpp'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: '*'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '#include <stdio.h>'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '#include "dynlink/cuda_drvapi_dynlink.h"'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
- en: '#include <chError.h>'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: int
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: main()
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
- en: CUresult status;
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: int numDevices;
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuInit( 0, 4010 ) );
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGetCount( &numDevices ) );
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%d devices detected:\n", numDevices );
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < numDevices; i++ ) {
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
- en: char szName[256];
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: CUdevice device;
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGet( &device, i ) );
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGetName( szName, 255, device ) );
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: printf( "\t%s\n", szName );
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: return 0;
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: 'fprintf( stderr, "CUDA failure code: 0x%x\n", status );'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: return 1;
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: CUDA-Only DLLs
  id: totrans-483
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For Windows developers, another way to build CUDA applications that can run
    on non-CUDA-capable systems is as follows.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** Move the CUDA-specific code into a DLL.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** Call `LoadLibrary()` explicitly to load the DLL.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '**3.** Enclose the `LoadLibrary()` call in a `__try/__except` clause to catch
    the exception if CUDA is not present.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Contexts
  id: totrans-488
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contexts are analogous to processes on CPUs. With few exceptions, they are containers
    that manage the lifetimes of all other objects in CUDA, including the following.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: • All memory allocations (including linear device memory, host memory, and CUDA
    arrays)
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: • Modules
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA streams
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA events
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: • Texture and surface references
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: • Device memory for kernels that use local memory
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
- en: • Internal resources for debugging, profiling, and synchronization
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: • The pinned staging buffers used for pageable memcpy
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime does not provide direct access to CUDA contexts. It performs
    context creation through *deferred initialization.* Every CUDART library call
    or kernel invocation checks whether a CUDA context is current and, if necessary,
    creates a CUDA context (using the state previously set by calls such as `cudaSetDevice()`,
    `cudaSetDeviceFlags()`, `cudaGLSetGLDevice()`, etc.).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: Many applications prefer to explicitly control the timing of this deferred initialization.
    To force CUDART to initialize without any other side effects, call
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree(0);
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: CUDA runtime applications can access the current-context stack (described below)
    via the driver API.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
- en: For functions that specify per-context state in the driver API, the CUDA runtime
    conflates contexts and devices. Instead of `cuCtxSynchronize()`, the CUDA runtime
    has `cudaDeviceSynchronize()`; instead of `cuCtxSetCacheConfig()`, the CUDA runtime
    has `cudaDeviceSetCacheConfig()`.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: Current Context
  id: totrans-503
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of the current-context stack, the CUDA runtime provides the `cudaSetDevice()`
    function, which sets the current context for the calling thread. A device can
    be current to more than one CPU thread at a time.^([3](ch03.html#ch03fn3))
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch03.html#ch03fn3a). Early versions of CUDA prohibited contexts from being
    current to more than one thread at a time because the driver was not thread-safe.
    Now the driver implements the needed synchronization—even when applications call
    synchronous functions such as `cudaDeviceSynchronize()`.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Lifetime and Scoping
  id: totrans-506
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All of the resources allocated in association with a CUDA context are destroyed
    when the context is destroyed. With few exceptions, the resources created for
    a given CUDA context may not be used with any other CUDA context. This restriction
    applies not only to memory but also to objects such as CUDA streams and CUDA events.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Preallocation of Resources
  id: totrans-508
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA tries to avoid “lazy allocation,” where resources are allocated as needed
    to avoid failing operations for lack of resources. For example, pageable memory
    copies cannot fail with an out-of-memory condition because the pinned staging
    buffers needed to perform pageable memory copies are allocated at context creation
    time. If CUDA is not able to allocate these buffers, the context creation fails.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: There are some isolated cases where CUDA does not preallocate all the resources
    that it might need for a given operation. The amount of memory needed to hold
    local memory for a kernel launch can be prohibitive, so CUDA does not preallocate
    the maximum theoretical amount needed. As a result, a kernel launch may fail if
    it needs more local memory than the default allocated by CUDA for the context.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Address Space
  id: totrans-511
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Besides objects that are automatically destroyed (“cleaned up”) when the context
    is destroyed, the key abstraction embodied in a context is its *address space*:
    the private set of virtual memory addresses that it can use to allocate linear
    device memory or to map pinned host memory. These addresses are unique per context.
    The same address for different contexts may or may not be valid and certainly
    will not resolve to the same memory location unless special provisions are made.
    The address space of a CUDA context is separate and distinct from the CPU address
    space used by CUDA host code. In fact, unlike shared-memory multi-CPU systems,
    CUDA contexts on multi-GPU configurations do not share an address space. When
    UVA (unified virtual addressing) is in effect, the CPU and GPU(s) share the same
    address space, in that any given allocation has a unique address within the process,
    but the CPUs and GPUs can only read or write each other’s memory under special
    circumstances, such as mapped pinned memory (see [Section 5.1.3](ch05.html#ch05lev2sec3))
    or peer-to-peer memory (see [Section 9.2.2](ch09.html#ch09lev2sec2)).'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.4\. Current Context Stack
  id: totrans-513
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most CUDA entry points do not take a context parameter. Instead, they operate
    on the “current context,” which is stored in a thread-local storage (TLS) handle
    in the CPU thread. In the driver API, each CPU thread has a stack of current contexts;
    creating a context pushes the new context onto the stack.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: The current-context stack has three main applications.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: • Single-threaded applications can drive multiple GPU contexts.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: • Libraries can create and manage their own CUDA contexts without interfering
    with their callers’ CUDA contexts.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: • Libraries can be agnostic with respect to which CPU thread calls into the
    CUDA-aware library.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: The original motivation for the current-context stack to CUDA was to enable
    a single-threaded CUDA application to drive multiple CUDA contexts. After creating
    and initializing each CUDA context, the application can pop it off the current-context
    stack, making it a “floating” context. Since only one CUDA context at a time may
    be current to a CPU thread, a single-threaded CUDA application drives multiple
    contexts by pushing and popping the contexts in turn, keeping all but one of the
    contexts “floating” at any given time.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
- en: On most driver architectures, pushing and popping a CUDA context is inexpensive
    enough that a single-threaded application can keep multiple GPUs busy. On WDDM
    (Windows Display Driver Model) drivers, which run only on Windows Vista and later,
    popping the current context is only fast if there are no GPU commands pending.
    If there are commands pending, the driver will incur a kernel thunk to submit
    the commands before popping the CUDA context.^([4](ch03.html#ch03fn4))
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch03.html#ch03fn4a). This expense isn’t unique to the driver API or the
    current-context stack. Calling `cudaSetDevice()` to switch devices when commands
    are pending also will cause a kernel thunk on WDDM.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of the current-context stack is the ability to drive a given
    CUDA context from different CPU threads. Applications using the driver API can
    “migrate” a CUDA context to other CPU threads by popping the context with `cuCtxPopCurrent()`,
    then calling `cuCtxPushCurrent()` from another thread. Libraries can use this
    functionality to create CUDA contexts without the knowledge or involvement of
    their callers. For example, a CUDA-aware plugin library could create its own CUDA
    context on initialization, then pop it and keep it floating except when called
    by the main application. The floating context enables the library to be completely
    agnostic about which CPU thread is used to call into it. When used in this way,
    the containment enforced by CUDA contexts is a mixed blessing. On the one hand,
    the floating context’s memory cannot be polluted by spurious writes by third-party
    CUDA kernels, but on the other hand, the library can only operate on CUDA resources
    that it allocated.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: Attaching and Detaching Contexts
  id: totrans-523
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Until CUDA 4.0, every CUDA context had a “usage count” set to 1 when the context
    was created. The functions `cuCtxAttach()` and `cuCtxDetach()` incremented and
    decremented the usage count, respectively.^([5](ch03.html#ch03fn5)) The usage
    count was intended to enable libraries to “attach” to CUDA contexts created by
    the application into which the library was linked. This way, the application and
    its libraries could interoperate via a CUDA context that was created by the application.^([6](ch03.html#ch03fn6))
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch03.html#ch03fn5a). Until the `cuCtxDestroy()` function was added in CUDA
    2.2, CUDA contexts were destroyed by calling `cuCtxDetach()`.'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch03.html#ch03fn6a). In retrospect, it would have been wiser for NVIDIA
    to leave reference-counting to higher-level software layers than the driver API.'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: If a CUDA context is already current when CUDART is first invoked, it attaches
    the CUDA context instead of creating a new one. The CUDA runtime did not provide
    access to the usage count of a context. As of CUDA 4.0, the usage count is deprecated,
    and `cuCtxAttach()/cuCtxDetach()` do not have any side effects.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.5\. Context State
  id: totrans-528
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `cuCtxSetLimit()` and `cuCtxGetLimit()` functions configure limits related
    to CPU-like functionality: in-kernel `malloc()` and `printf()`. The `cuCtxSetCacheConfig()`
    specifies the preferred cache configuration to use when launching kernels (whether
    to allocate 16K or 48K to shared memory and L1 cache). This is a hint, since any
    kernel that uses more than 16K of shared memory needs the configuration setting
    with 48K of shared memory. Additionally, the context state can be overridden by
    a kernel-specific state (`cuFuncSetCacheConfig()`). These states have context
    scope (in other words, they are not specified for each kernel launch) because
    they are expensive to change.'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Modules and Functions
  id: totrans-530
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modules are collections of code and related data that are loaded together, analogous
    to DLLs on Windows or DSOs on Linux. Like CUDA contexts, the CUDA runtime does
    not explicitly support modules; they are available only in the CUDA driver API.^([7](ch03.html#ch03fn7))
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch03.html#ch03fn7a). If CUDA adds the oft-requested ability to JIT from
    source code (as OpenCL can), NVIDIA may see fit to expose modules to the CUDA
    runtime.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
- en: CUDA does not have an intermediate structure analogous to object files that
    can be synthesized into a CUDA module. Instead, `nvcc` directly emits files that
    can be loaded as CUDA modules.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
- en: • `.cubin` files that target specific SM versions
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
- en: • `.ptx` files that can be compiled onto the hardware by the driver
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
- en: This data needn’t be sent to end users in the form of these files. CUDA includes
    APIs to load modules as NULL-terminated strings that can be embedded in executable
    resources or elsewhere.^([8](ch03.html#ch03fn8))
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch03.html#ch03fn8a). The `cuModuleLoadDataEx()` function is described in
    detail in [Section 4.2](ch04.html#ch04lev1sec2).'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
- en: Once a CUDA module is loaded, the application can query for the resources contained
    in it.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
- en: • Globals
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
- en: • Functions (kernels)
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
- en: • Texture references
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
- en: 'One important note: All of these resources are created when the module is loaded,
    so the query functions cannot fail due to a lack of resources.'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
- en: Like contexts, the CUDA runtime hides the existence and management of modules.
    All modules are loaded at the same time CUDART is initialized. For applications
    with large amounts of GPU code, the ability to explicitly manage residency by
    loading and unloading modules is one of the principal reasons to use the driver
    API instead of the CUDA runtime.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
- en: Modules are built by invoking `nvcc`, which can emit different types of modules,
    depending on the command line parameters, as summarized in [Table 3.3](ch03.html#ch03tab03).
    Since cubins have been compiled to a specific GPU architecture, they do not have
    to be compiled “just in time” and are faster to load. But they are neither backward
    compatible (e.g., cubins compiled onto SM 2.x cannot run on SM 1.x architectures)
    nor forward compatible (e.g., cubins compiled onto SM 2.x architectures will not
    run on SM 3.x architectures). As a result, only applications with *a priori* knowledge
    of their target GPU architectures (and thus cubin versions) can use cubins without
    also embedding PTX versions of the same modules to use as backup.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab03.jpg)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.3* nvcc Module Types'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
- en: PTX is the intermediate language used as a source for the driver’s just-in-time
    compilation. Because this compilation can take a significant amount of time, the
    driver saves compiled modules and reuses them for a given PTX module, provided
    the hardware and driver have not changed. If the driver or hardware changes, all
    PTX modules must be recompiled.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
- en: With fatbins, the CUDA runtime automates the process of using a suitable cubin,
    if available, and compiling PTX otherwise. The different versions are embedded
    as strings in the host C++ code emitted by `nvcc`. Applications using the driver
    API have the advantage of finer-grained control over modules. For example, they
    can be embedded as resources in the executable, encrypted, or generated at runtime,
    but the process of using cubins if available and compiling PTX otherwise must
    be implemented explicitly.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a module is loaded, the application can query for the resources contained
    in it: globals, functions (kernels), and texture references. One important note:
    All of these resources are created when the module is loaded, so the query functions
    (summarized in [Table 3.4](ch03.html#ch03tab04)) cannot fail due to a lack of
    resources.'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab04.jpg)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.4* Module Query Functions'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Kernels (Functions)
  id: totrans-552
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kernels are highlighted by the `__global__` keyword in `.cu` files. When using
    the CUDA runtime, they can be invoked in-line with the triple-angle-bracket `<<<
    >>>` syntax. [Chapter 7](ch07.html#ch07) gives a detailed description of how kernels
    can be invoked and how they execute on the GPU.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
- en: The GPU executable code of the module comes in the form of kernels that are
    invoked with the language integration features of the CUDA runtime `(<<< >>>`
    syntax) or the `cuLaunchKernel()` function in the driver API. At the time of this
    writing, CUDA does not do any dynamic residency management of the executable code
    in CUDA modules. When a module is loaded, *all* of the kernels are loaded into
    device memory.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a module is loaded, kernels may be queried with `cuModuleGetFunction()`;
    the kernel’s attributes can be queried with `cuFuncGetAttribute()`; and the kernel
    may be launched with `cuLaunchKernel()`. `cuLaunchKernel()` rendered a whole slew
    of API entry points obsolete: Functions such as `cuFuncSetBlockShape()` specified
    the block size to use the next time a given kernel was launched; functions such
    as `cuParamSetv()` specified the parameters to pass the next time a given kernel
    was launched; and `cuLaunch()`, `cuLaunchGrid()`, and `cuLaunchGridAsync()` launched
    a kernel using the previously set state. These APIs were inefficient because it
    took so many calls to set up a kernel launch and because parameters such as block
    size are best specified atomically with the request to launch the kernel.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
- en: The `cuFuncGetAttribute()` function may be used to query specific attributes
    of a function, such as
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
- en: • The maximum number of threads per block
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
- en: • The amount of statically allocated shared memory
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
- en: • The size of user-allocated constant memory
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
- en: • The amount of local memory used by each function
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
- en: • The number of registers used by each thread of the function
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
- en: • The virtual (PTX) and binary architecture versions for which the function
    was compiled
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
- en: When using the driver API, it is usually a good idea to use `extern "C"` to
    inhibit the default name-mangling behavior of C++. Otherwise, you have to specify
    the mangled name to `cuModuleGetFunction()`.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  id: totrans-564
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As executables that were built with the CUDA runtime are loaded, they create
    global data structures in host memory that describe the CUDA resources to be allocated
    when a CUDA device is created. Once a CUDA device is initialized, these globals
    are used to create the CUDA resources all at once. Because these globals are shared
    process-wide by the CUDA runtime, it is not possible to incrementally load and
    unload CUDA modules using the CUDA runtime.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
- en: Because of the way the CUDA runtime is integrated with the C++ language, kernels
    and symbols should be specified by name (i.e., not with a string literal) to API
    functions such as `cudaFuncGetAttributes()` and `cudaMemcpyToSymbol()`.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
- en: Cache Configuration
  id: totrans-567
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In Fermi-class architectures, the streaming multiprocessors have L1 caches
    that can be split as 16K shared/48K L1 cache or 48K shared/16K L1 cache.^([9](ch03.html#ch03fn9))
    Initially, CUDA allowed the cache configuration to be specified on a per-kernel
    basis, using `cudaFuncSetCacheConfig()` in the CUDA runtime or `cuFuncSetCacheConfig()`
    in the driver API. Later, this state was moved to be more global: `cuCtxSetCacheConfig()/cudaDeviceSetCacheConfig()`
    specifies the default cache configuration.'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
- en: '[9](ch03.html#ch03fn9a). SM 3.x added the ability to split the cache evenly
    (32K/32K) between L1 and shared memory.'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
- en: 3.6\. Device Memory
  id: totrans-570
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Device memory (or linear device memory) resides in the CUDA address space and
    may be accessed by CUDA kernels via normal C/C++ pointer and array dereferencing
    operations. Most GPUs have a dedicated pool of device memory that is directly
    attached to the GPU and accessed by an integrated memory controller.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
- en: CUDA hardware does not support demand paging, so all memory allocations are
    backed by actual physical memory. Unlike CPU applications, which can allocate
    more virtual memory than there is physical memory in the system, CUDA’s memory
    allocation facilities fail when the physical memory is exhausted. The details
    of how to allocate, free, and access device memory are given in [Section 5.2](ch05.html#ch05lev1sec2).
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  id: totrans-573
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: CUDA runtime applications may query the total amount of device memory available
    on a given device by calling `cudaGetDeviceProperties()` and examining `cudaDeviceProp::totalGlobalMem`.
    `cudaMalloc()` and `cudaFree()` allocate and free device memory, respectively.
    `cudaMallocPitch()` allocates pitched memory; `cudaFree()` may be used to free
    it. `cudaMalloc3D()` performs a 3D allocation of pitched memory.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  id: totrans-575
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Driver API applications may query the total amount of device memory available
    on a given device by calling `cuDeviceTotalMem()`. Alternatively, the `cuMemGetInfo()`
    function may be used to query the amount of free device memory as well as the
    total. `cuMemGetInfo()` can only be called when a CUDA context is current to the
    CPU thread. `cuMemAlloc()` and `cuMemFree()` allocate and free device memory,
    respectively. `cuMemAllocPitch()` allocates pitched memory; `cuMemFree()` may
    be used to free it.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
- en: 3.7\. Streams and Events
  id: totrans-577
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In CUDA, streams and events were added to enable host↔device memory copies to
    be performed concurrently with kernel execution. Later versions of CUDA expanded
    streams’ capabilities to support execution of multiple kernels concurrently on
    the same GPU and to support concurrent execution between multiple GPUs.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
- en: CUDA streams are used to manage concurrency between execution units with coarse
    granularity.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
- en: • The GPU and the CPU
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
- en: • The copy engine(s) that can perform DMA while the SMs are processing
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
- en: • The streaming multiprocessors (SMs)
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
- en: • Kernels that are intended to run concurrently
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
- en: • Separate GPUs that are executing concurrently
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
- en: The operations requested in a given stream are performed sequentially. In a
    sense, CUDA streams are like CPU threads in that operations within a CUDA stream
    are performed in order.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.1\. Software Pipelining
  id: totrans-586
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because there is only one DMA engine serving the various coarse-grained hardware
    resources in the GPU, applications must “software-pipeline” the operations performed
    on multiple streams. Otherwise, the DMA engine will “break concurrency” by enforcing
    synchronization within the stream between different engines. A detailed description
    of how to take full advantage of CUDA streams using software pipelining is given
    in [Section 6.5](ch06.html#ch06lev1sec5), and more examples are given in [Chapter
    11](ch11.html#ch11).
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
- en: The Kepler architecture reduced the need to software-pipeline streamed operations
    and, with NVIDIA’s Hyper-Q technology (first available with SM 3.5), virtually
    eliminated the need for software pipelining.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.2\. Stream Callbacks
  id: totrans-589
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA 5.0 introduced another mechanism for CPU/GPU synchronization that complements
    the existing mechanisms, which focus on enabling CPU threads to wait until streams
    are idle or events have been recorded. Stream callbacks are functions provided
    by the application, registered with CUDA, and later called by CUDA when the stream
    has reached the point at which `cuStreamAddCallback()` was called.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
- en: Stream execution is suspended for the duration of the stream callback, so for
    performance reasons, developers should be careful to make sure other streams are
    available to process during the callback.
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.3\. The NULL Stream
  id: totrans-592
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Any of the asynchronous memcpy functions may be called with NULL as the stream
    parameter, and the memcpy will not be initiated until all preceding operations
    on the GPU have been completed; in effect, the NULL stream is a join of all the
    engines on the GPU. Additionally, all streamed memcpy functions are *asynchronous*,
    potentially returning control to the application before the memcpy has been performed.
    The NULL stream is most useful for facilitating CPU/GPU concurrency in applications
    that have no need for the intra-GPU concurrency facilitated by multiple streams.
    Once a streamed operation has been initiated with the NULL stream, the application
    must use synchronization functions such as `cuCtxSynchronize()` or `cudaThreadSynchronize()`
    to ensure that the operation has been completed before proceeding. But the application
    may request many such operations before performing the synchronization. For example,
    the application may perform
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
- en: • an asynchronous host→device memcpy
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
- en: • one or more kernel launches
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
- en: • an asynchronous device→host memcpy
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
- en: before synchronizing with the context. The `cuCtxSynchronize()` or `cudaThreadSynchronize()`
    call returns after the GPU has performed the last-requested operation. This idiom
    is especially useful when performing smaller memcpy’s or launching kernels that
    will not run for long. The CUDA driver takes valuable CPU time to write commands
    to the GPU, and overlapping that CPU execution with the GPU’s processing of the
    commands can improve performance.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
- en: '*Note:* Even in CUDA 1.0, kernel launches were asynchronous; the NULL stream
    is implicitly specified to any kernel launch in which no stream is explicitly
    specified.'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.4\. Events
  id: totrans-599
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA events present another mechanism for synchronization. Introduced at the
    same time as CUDA streams, “recording” CUDA events is a way for applications to
    track progress within a CUDA stream. All CUDA events work by writing a shared
    sync memory location when all preceding operations in the CUDA stream have been
    performed.^([10](ch03.html#ch03fn10)) *Querying* the CUDA event causes the driver
    to peek at this memory location and report whether the event has been recorded;
    *synchronizing* with the CUDA event causes the driver to wait until the event
    has been recorded.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
- en: '[10](ch03.html#ch03fn10a). Specifying the NULL stream to `cuEventRecord()`
    or `cudaEventRecord()` means the event will not be recorded until the GPU has
    processed *all* preceding operations.'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, CUDA events also can write a timestamp derived from a high-resolution
    timer in the hardware. Event-based timing can be more robust than CPU-based timing,
    especially for smaller operations, because it is not subject to spurious unrelated
    events (such as page faults or network traffic) that may affect wall-clock timing
    by the CPU. Wall-clock times are definitive because they are a better approximation
    of what the end users sees, so CUDA events are best used for performance tuning
    during product development.^([11](ch03.html#ch03fn11))
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
- en: '[11](ch03.html#ch03fn11a). Additionally, CUDA events that can be used for timing
    cannot be used for certain other operations; more recent versions of CUDA allow
    developers to opt out of the timing feature to enable the CUDA event to be used,
    for example, for interdevice synchronization.'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
- en: 'Timing using CUDA events is best performed in conjunction with the NULL stream.
    This rule of thumb is motivated by reasons similar to the reasons RDTSC (Read
    TimeStamp Counter) is a serializing instruction on the CPU: Just as the CPU is
    a superscalar processor that can execute many instructions at once, the GPU can
    be operating on multiple streams at the same time. Without explicit serialization,
    a timing operation may inadvertently include operations that were not intended
    to be timed or may exclude operations that were supposed to be timed. As with
    RDTSC, the trick is to bracket the CUDA event recordings with enough work that
    the overhead of performing the timing itself is negligible.'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
- en: CUDA events optionally can cause an interrupt to be signaled by the hardware,
    enabling the driver to perform a so-called “blocking” wait. Blocking waits suspend
    the waiting CPU thread, saving CPU clock cycles and power while the driver waits
    for the GPU. Before blocking waits became available, CUDA developers commonly
    complained that the CUDA driver burned a whole CPU core waiting for the GPU by
    polling a memory location. At the same time, blocking waits may take longer due
    to the overhead of handling the interrupt, so latency-sensitive applications may
    still wish to use the default polling behavior.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
- en: 3.8\. Host Memory
  id: totrans-606
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: “Host” memory is CPU memory—the stuff we all were managing with `malloc()/free()`
    and `new[]/delete[]` for years before anyone had heard of CUDA. On all operating
    systems that run CUDA, host memory is *virtualized*; memory protections enforced
    by hardware are in place to protect CPU processes from reading or writing each
    other’s memory without special provisions.^([12](ch03.html#ch03fn12)) “Pages”
    of memory, usually 4K or 8K in size, can be relocated without changing their virtual
    address; in particular, they can be swapped to disk, effectively enabling the
    computer to have more virtual memory than physical memory. When a page is marked
    “nonresident,” an attempt to access the page will signal a “page fault” to the
    operating system, which will prompt the operating system to find a physical page
    available to copy the data from disk and resume execution with the virtual page
    pointing to the new physical location.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
- en: '[12](ch03.html#ch03fn12a). Examples of APIs that facilitate interprocess sharing
    include `MapViewOfFile()` on Windows or `mmap()` on Linux.'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
- en: The operating system component that manages virtual memory is called the “virtual
    memory manager” or VMM. Among other things, the VMM monitors memory activity and
    uses heuristics to decide when to “evict” pages to disk and resolves the page
    faults that happen when evicted pages are referenced.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
- en: 'The VMM provides services to hardware drivers to facilitate direct access of
    host memory by hardware. In modern computers, many peripherals, including disk
    controllers, network controllers, and GPUs, can read or write host memory using
    a facility known as “direct memory access” or DMA. DMA gives two performance benefits:
    It avoids a data copy^([13](ch03.html#ch03fn13)) and enables the hardware to operate
    concurrently with the CPU. A tertiary benefit is that hardware may achieve better
    bus performance over DMA.'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
- en: '[13](ch03.html#ch03fn13a). This extra copy is more obvious to developers using
    GPUs, whose target peripheral can consume much more bandwidth than more pedestrian
    devices like those for disk or network controllers. Whatever the type of peripheral,
    without DMA, the driver must use the CPU to copy data to or from special hardware
    buffers.'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate DMA, operating system VMMs provide a service called “page-locking.”
    Memory that is page-locked has been marked by the VMM as ineligible for eviction,
    so its physical address cannot change. Once memory is page-locked, drivers can
    program their DMA hardware to reference the physical addresses of the memory.
    This hardware setup is a separate and distinct operation from the page-locking
    itself. Because page-locking makes the underlying physical memory unavailable
    for other uses by the operating system, page-locking too much memory can adversely
    affect performance.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
- en: Memory that is not page-locked is known as “pageable.” Memory that is page-locked
    is sometimes known as “pinned” memory, since its physical address cannot be changed
    by the operating system (it has been pinned in place).
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.1\. Pinned Host Memory
  id: totrans-614
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: “Pinned” host memory is allocated by CUDA with the functions `cuMemHostAlloc()
    / cudaHostAlloc()`. This memory is page-locked and set up for DMA by the current
    CUDA context.^([14](ch03.html#ch03fn14))
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
- en: '[14](ch03.html#ch03fn14a). CUDA developers often ask if there is any difference
    between page-locked memory and CUDA’s “pinned” memory. There is! Pinned memory
    allocated or registered by CUDA is mapped for direct access by the GPU(s); ordinary
    page-locked memory is not.'
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
- en: CUDA tracks the memory ranges allocated in this way and automatically accelerates
    memcpy operations that reference pinned memory. Asynchronous memcpy operations
    only work on pinned memory. Applications can determine whether a given host memory
    address range is pinned using the `cuMemHostGetFlags()` function.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
- en: In the context of operating system documentation, the terms *page-locked* and
    *pinned* are synonymous, but for CUDA purposes, it may be easier to think of “pinned”
    memory as host memory that has been page-locked and mapped for access by the hardware.
    “Page-locking” refers only to the operating system mechanism for marking host
    memory pages as ineligible for eviction.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.2\. Portable Pinned Memory
  id: totrans-619
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Portable* pinned memory is mapped for all CUDA contexts after being page-locked.
    The underlying mechanism for this operation is complicated: When a portable pinned
    allocation is performed, it is mapped into all CUDA contexts before returning.
    Additionally, whenever a CUDA context is created, all portable pinned memory allocations
    are mapped into the new CUDA context before returning. For either portable memory
    allocation or context creation, any failure to perform these mappings will cause
    the allocation or context creation to fail. Happily, as of CUDA 4.0, if UVA (unified
    virtual addressing) is in force, *all* pinned allocations are portable.'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.3\. Mapped Pinned Memory
  id: totrans-621
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Mapped* pinned memory is mapped into the address space of the CUDA context,
    so kernels may read or write the memory. By default, pinned memory is not mapped
    into the CUDA address space, so it cannot be corrupted by spurious writes by a
    kernel. For integrated GPUs, mapped pinned memory enables “zero copy”: Since the
    host (CPU) and device (GPU) share the same memory pool, they can exchange data
    without explicit copies.'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
- en: For discrete GPUs, mapped pinned memory enables host memory to be read or written
    directly by kernels. For small amounts of data, this has the benefit of eliminating
    the overhead of explicit memory copy commands. Mapped pinned memory can be especially
    beneficial for writes, since there is no latency to cover. As of CUDA 4.0, if
    UVA (unified virtual addressing) is in effect, *all* pinned allocations are mapped.
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.4\. Host Memory Registration
  id: totrans-624
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since developers (especially library developers) don’t always get to allocate
    memory they want to access, CUDA 4.0 added the ability to “register” existing
    virtual address ranges for use by CUDA. The `cuMemHostRegister()/cudaHostRegister()`
    functions take a virtual address range and page-locks and maps it for the current
    GPU (or for all GPUs, if `CU_MEMHOSTREGISTER_PORTABLE` or `cudaHostRegisterPortable`
    is specified). Host memory registration has a perverse relationship with UVA (unified
    virtual addressing), in that any address range eligible for registration must
    not have been included in the virtual address ranges reserved for UVA purposes
    when the CUDA driver was initialized.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
- en: 3.9\. CUDA Arrays and Texturing
  id: totrans-626
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CUDA arrays are allocated from the same pool of physical memory as device memory,
    but they have an opaque layout that is optimized for 2D and 3D locality. The graphics
    drivers use these layouts to hold textures; by decoupling the indexing from the
    addressing, the hardware can operate on 2D or 3D blocks of elements instead of
    1D rows. For applications that exhibit sparse access patterns, especially patterns
    with dimensional locality (for example, computer vision applications), CUDA arrays
    are a clear win. For applications with regular access patterns, especially those
    with little to no reuse or whose reuse can be explicitly managed by the application
    in shared memory, device pointers are the obvious choice.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
- en: Some applications, such as image processing applications, fall into a gray area
    where the choice between device pointers and CUDA arrays is not obvious. All other
    things being equal, device memory is probably preferable to CUDA arrays, but the
    following considerations may be used to help in the decision-making process.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA arrays do not consume CUDA address space.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
- en: • On WDDM drivers (Windows Vista and later), the system can automatically manage
    the residence of CUDA arrays.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA arrays can reside only in device memory, and the GPU can convert between
    the two representations while transferring the data across the bus. For some applications,
    keeping a pitch representation in host memory and a CUDA array representation
    in device memory is the best approach.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
- en: 3.9.1\. Texture References
  id: totrans-632
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Texture references* are objects that CUDA uses to set up the texturing hardware
    to “interpret” the contents of underlying memory.^([15](ch03.html#ch03fn15)) Part
    of the reason this level of indirection exists is because it is valid to have
    multiple texture references referencing the same memory with different attributes.'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
- en: '[15](ch03.html#ch03fn15a). Before CUDA 3.2, texture references were the only
    way to read from CUDA arrays, other than explicit memcpy. Today, surface references
    may be used to write to CUDA arrays as well as to read from them.'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
- en: A texture reference’s attributes may be *immutable*—that is, specified at compile
    time and not subject to change without causing the application to behave incorrectly—or
    *mutable*—that is, where the application may change the texture’s behavior in
    ways that are not visible to the compiler ([Table 3.5](ch03.html#ch03tab05)).
    For example, the dimensionality of the texture (1D, 2D, or 3D) is immutable, since
    it must be known by the compiler to take the correct number of input parameters
    and emit the correct machine instruction. In contrast, the filtering and addressing
    modes are mutable, since they implicitly change the application’s behavior without
    any knowledge or involvement from the compiler.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab05.jpg)'
  id: totrans-636
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.5* Mutable and Immutable Texture Attributes'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime (language integration) and the CUDA driver API deal with texture
    references very differently. In both cases, a texture reference is declared by
    invoking a template called `texture`.
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
- en: texture<Type, Dimension, ReadMode> Name;
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
- en: where *Type* is the type of the elements in the memory being read by the texture,
    *Dimension* is the dimension of the texture (1, 2, or 3), and *ReadMode* specifies
    whether integer-valued texture types should be converted to normalized floating
    point when read by the texture reference.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
- en: The texture reference must be *bound* to underlying memory before it can be
    used. The hardware is better optimized to texture from CUDA arrays, but in the
    following cases, applications benefit from texturing from device memory.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
- en: • It enlists the texture cache, which serves as a bandwidth aggregator.
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
- en: • It enables applications to work around coalescing restrictions.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
- en: • It avoids superfluous copies when reading from memory that is otherwise best
    written via device memory. For example, a video codec may wish to emit frames
    into device memory, yet read from them via texture.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
- en: Once the texture reference is bound to underlying memory, CUDA kernels may read
    the memory by invoking `tex* intrinsics`, such as `tex1D()`, given in [Table 3.6](ch03.html#ch03tab06).
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab06.jpg)'
  id: totrans-646
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.6* Texture Intrinsics'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
- en: '*Note:* There are no coherency guarantees between texture reads and writes
    performed via global load/store or surface load/store. As a result, CUDA kernels
    must take care not to texture from memory that also is being accessed by other
    means.'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  id: totrans-649
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To bind memory to a texture, applications must call one of the functions in
    [Table 3.7](ch03.html#ch03tab07). CUDA runtime applications can modify mutable
    attributes of the texture reference by directly assigning structure members.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch03_images.html#p084pro01a)'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
- en: texture<float, 1, cudaReadModeElementType> tex1;
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
- en: tex1.filterMode = cudaFilterModeLinear; // enable linear filtering
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
- en: tex1.normalized = true; // texture coordinates will be normalized
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab07.jpg)'
  id: totrans-656
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.7* Functions to Bind Device Memory to Textures'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
- en: Assigning to these structure members has an immediate effect; there is no need
    to rebind the texture.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  id: totrans-659
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since there is a stricter partition between CPU code and GPU code when using
    the driver API, any texture references declared in a CUDA module must be queried
    via `cuModuleGetTexRef()`, which passes back a `CUtexref`. Unlike the CUDA runtime,
    the texture reference then must be initialized with *all* of the correct attributes—both
    mutable and immutable—because the compiler does not encode the immutable attributes
    of the texture reference into the CUDA module. [Table 3.8](ch03.html#ch03tab08)
    summarizes the driver API functions that can be used to bind a texture reference
    to memory.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab08.jpg)'
  id: totrans-661
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.8* Driver API Functions to Bind Memory to Textures'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
- en: 3.9.2\. Surface References
  id: totrans-663
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Surface references*, a more recent addition to CUDA not available on Tesla-class
    GPUs, enable CUDA kernels to read and write CUDA arrays via the surface load/store
    intrinsics. Their primary purpose is to enable CUDA kernels to write CUDA arrays
    directly. Before surface load/store became available, kernels had to write to
    device memory and then perform a device→array memcpy to copy and convert the output
    into a CUDA array.'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
- en: Compared to texture references, which can transform everything from the input
    coordinates to the output format, depending on how they are set up, surface references
    expose a vanilla, bitwise interface to read and write the contents of the CUDA
    array.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder why CUDA did not implement surface load/store intrinsics that
    operated directly on CUDA arrays (as OpenCL did). The reason is to be future-proof
    to surface load/store operations that convert to the underlying representation
    in a more sophisticated way, such as enabling samples to be “splatted” into the
    CUDA array with fractional coordinates, or interoperating with an antialiased
    graphics surface. For now, CUDA developers will have to make do implementing such
    operations in software.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
- en: 3.10\. Graphics Interoperability
  id: totrans-667
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The graphics interoperability (or “graphics interop”) family of functions enables
    CUDA to read and write memory belonging to the OpenGL or Direct3D APIs. If applications
    could attain acceptable performance by sharing data via host memory, there would
    be no need for these APIs. But with local memory bandwidth that can exceed 140G/s
    and PCI Express bandwidth that rarely exceeds 6G/s in practice, it is important
    to give applications the opportunity to keep data on the GPU when possible. Using
    the graphics interop APIs, CUDA kernels can write data into images and textures
    that are then incorporated into graphical output to be performed by OpenGL or
    Direct3D.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
- en: Because the graphics and CUDA drivers must coordinate under the hood to enable
    interoperability, applications must signal their intention to perform graphics
    interop early. In particular, the CUDA context must be notified that it will be
    interoperating with a given API by calling special context creation APIs such
    as `cuD3D10CtxCreate()` or `cudaGLSetDevice()`.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
- en: The coordination between drivers also motivated resource-sharing between graphics
    APIs and CUDA to occur in two steps.
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** *Registration:* a potentially expensive operation that signals the developer’s
    intent to share the resources to the underlying drivers, possibly prompting them
    to move and/or lock down the resources in question'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** *Mapping:* a lightweight operation that is expected to occur at high
    frequency'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
- en: In early versions of CUDA, the APIs for graphics interoperability with all four
    graphics APIs (OpenGL, Direct3D 9, Direct3D 10, and Direct3D 11) were strictly
    separate. For example, for Direct3D 9 interoperability, the following functions
    would be used in conjunction with one another.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
- en: • `cuD3D9RegisterResource()/cudaD3D9RegisterResource()`
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
- en: • `cuD3D9MapResources()/cudaD3D9MapResources()`
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
- en: • `cuD3D9UnmapResources()/cudaD3D9UnmapResources()`
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
- en: • `cuD3D9UnregisterResource()/cudaD3D9UnregisterResource()`
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
- en: Because the underlying hardware capabilities are the same, regardless of the
    API used to access them, many of these functions were merged in CUDA 3.2\. The
    registration functions remain API-specific, since they require API-specific bindings,
    but the functions to map, unmap, and unregister resources were made common. The
    CUDA 3.2 APIs corresponding to the above are as follows.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
- en: • `cuD3D9RegisterResource()/cudaD3D9RegisterResource()`
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
- en: • `cuGraphicsMapResources()/cudaGraphicsMapResources()`
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
- en: • `cuGraphicsUnmapResources()/cudaGraphicsUnmapResources()`
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
- en: • `cuGraphicsUnregisterResource()/cudaGraphicsUnregisterResource()`
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
- en: The interoperability APIs for Direct3D 10 are the same, except the developer
    must use `cuD3D10RegisterResource()/cudaD3D10RegisterResource()` instead of the
    `cuD3D9*` variants.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
- en: CUDA 3.2 also added the ability to access textures from graphics APIs in the
    form of CUDA arrays. In Direct3D, textures are just a different type of “resource”
    and may be referenced by `IDirect3DResource9 *` (or `IDirect3DResource10 *`, etc.).
    In OpenGL, a separate function `cuGraphicsGLRegisterImage()` is provided.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
- en: 3.11\. The CUDA Runtime and CUDA Driver API
  id: totrans-685
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The CUDA runtime (“CUDART”) facilitates the language integration that makes
    CUDA so easy to program out of the gate. By automatically taking care of tasks
    such as initializing contexts and loading modules, and especially by enabling
    kernel invocation to be done in-line with other C++ code, CUDART lets developers
    focus on getting their code working quickly. A handful of CUDA abstractions, such
    as CUDA modules, are not accessible via CUDART.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the driver API exposes all CUDA abstractions and enables them to
    be manipulated by developers as needed for the application. The driver API does
    not provide any performance benefit. Instead, it enables explicit resource management
    for applications that need it, like large-scale commercial applications with plug-in
    architectures.
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
- en: The driver API is not noticeably faster than the CUDA runtime. If you are looking
    to improve performance in your CUDA application, look elsewhere.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
- en: Most CUDA features are available to both CUDART and the driver API, but a few
    are exclusive to one or the other. [Table 3.9](ch03.html#ch03tab09) summarizes
    the differences.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab09.jpg)![Image](graphics/03tab09a.jpg)'
  id: totrans-692
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.9.* CUDA Runtime versus Driver API Features'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
- en: Between the two APIs, operations like memcpy tend to be functionally identical,
    but the interfaces can be quite different. The stream APIs are almost identical.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/089tab01.jpg)'
  id: totrans-695
  prefs: []
  type: TYPE_IMG
- en: The event APIs have minor differences, with CUDART providing a separate `cudaEventCreateWithFlags()`
    function if the developer wants to specify a flags word (needed to create a blocking
    event).
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/089tab02.jpg)'
  id: totrans-697
  prefs: []
  type: TYPE_IMG
- en: The memcpy functions are the family where the interfaces are the most different,
    despite identical underlying functionality. CUDA supports three variants of memory—host,
    device, and CUDA array—which are all permutations of participating memory types,
    and 1D, 2D, or 3D memcpy. So the memcpy functions must contain either a large
    family of different functions or a small number of functions that support many
    types of memcpy.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest memcpy’s in CUDA copy between host and device memory, but even
    those function interfaces are different: CUDART uses `void *` for the types of
    both host and device pointers and a single memcpy function with a direction parameter,
    while the driver API uses `void *` for host memory, `CUdeviceptr` for device memory,
    and three separate functions (`cuMemcpyHtoD()`, `cuMemcpyDtoH()`, and `cuMemcpyDtoD()`)
    for the different memcpy directions. Here are equivalent CUDART and driver API
    formulations of the three permutations of host↔device memcpy.'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/090tab01.jpg)'
  id: totrans-700
  prefs: []
  type: TYPE_IMG
- en: For 2D and 3D memcpy’s, the driver API implements a handful of functions that
    take a descriptor struct and support all permutations of memcpy, including lower-dimension
    memcpy’s. For example, if desired, `cuMemcpy3D()` can be used to perform a 1D
    host→device memcpy instead of `cuMemcpyHtoD()`.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch03_images.html#p091pro01a)'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_MEMCPY3D cp = {0};
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
- en: cp.dstMemoryType = CU_MEMORYTYPE_DEVICE;
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
- en: cp.dstDevice = dptr;
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
- en: cp.srcMemoryType = CU_MEMORYTYPE_HOST;
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
- en: cp.srcHost = host;
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
- en: cp.WidthInBytes = bytes;
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
- en: cp.Height = cp.Depth = 1;
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
- en: status = cuMemcpy3D( &cp );
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
- en: CUDART uses a combination of descriptor structs for more complicated memcpy’s
    (e.g., `cudaMemcpy3D()`), while using different functions to cover the different
    memory types. Like `cuMemcpy3D()`, CUDART’s `cudaMemcpy3D()` function takes a
    descriptor struct that can describe any permutation of memcpy, including interdimensional
    memcpy’s (e.g., performing a 1D copy to or from the row of a 2D CUDA array, or
    copying 2D CUDA arrays to or from slices of 3D CUDA arrays). Its descriptor struct
    is slightly different in that it embeds other structures; the two APIs’ 3D memcpy
    structures are compared side-by-side in [Table 3.10](ch03.html#ch03tab10).
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab10.jpg)'
  id: totrans-712
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.10* 3D Memcpy Structures'
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
- en: Usage of both 3D memcpy functions is similar. They are designed to be zero-initialized,
    and developers set the members needed for a given operation. For example, performing
    a host→3D array copy may be done as follows.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/091tab01.jpg)'
  id: totrans-715
  prefs: []
  type: TYPE_IMG
- en: For a 3D copy that covers the entire CUDA array, the source and destination
    offsets are set to 0 by the first line and don’t have to be referenced again.
    Unlike parameters to a function, the code only needs to reference the parameters
    needed by the copy, and if the program must perform more than one similar copy
    (e.g., to populate more than one CUDA array or device memory region), the descriptor
    struct can be reused.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4\. Software Environment
  id: totrans-717
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter gives an overview of the CUDA development tools and the software
    environments that can host CUDA applications. Sections are devoted to the various
    tools in the NVIDIA toolkit.
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
- en: • `nvcc:` the CUDA compiler driver
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
- en: • `ptxas:` the PTX assembler
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
- en: • `cuobjdump:` the CUDA object file dump utility
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
- en: • `nvidia-smi:` the NVIDIA System Management Interface
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 4.5](ch04.html#ch04lev1sec5) describes Amazon’s EC2 (Elastic Compute
    Cloud) service and how to use it to access GPU-capable servers over the Internet.
    This chapter is intended more as a reference than as a tutorial. Example usages
    are given in [Part III](part03.html#part03) of this book.'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. `nvcc`—CUDA Compiler Driver
  id: totrans-724
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`nvcc` is the compiler driver CUDA developers use to translate source code
    into functional CUDA applications. It can perform many functions, from as simple
    as a targeted compilation of a GPU-only `.cu` file to as complex as compiling,
    linking, and executing a sample program in one command (a usage encouraged by
    many of the sample programs in this book).'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
- en: As a compiler driver, `nvcc` does nothing more than set up a build environment
    and spawn a combination of native tools (such as the C compiler installed on the
    system) and CUDA-specific command-line tools (such as `ptxas`) to build the CUDA
    code. It implements many sensible default behaviors that can be overridden by
    command-line options; its exact behavior depends on which “compile trajectory”
    is requested by the main command-line option.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 4.1](ch04.html#ch04tab01) lists the file extensions understood by `nvcc`
    and the default behavior implemented for them. (*Note:* Some intermediate file
    types, like the `.i/.ii` files that contain host code generated by CUDA’s front
    end, are omitted here.) [Table 4.2](ch04.html#ch04tab02) lists the compilation
    stage options and corresponding compile trajectory. [Table 4.3](ch04.html#ch04tab03)
    lists `nvcc` options that affect the environment, such as paths to include directories.
    [Table 4.4](ch04.html#ch04tab04) lists `nvcc` options that affect the output,
    such as whether to include debugging information. [Table 4.5](ch04.html#ch04tab05)
    lists “passthrough” options that enable `nvcc` to pass options to the tools that
    it invokes, such as `ptxas`. [Table 4.6](ch04.html#ch04tab06) lists `nvcc` options
    that aren’t easily categorized, such as the `–keep` option that instructs `nvcc`
    not to delete the temporary files it created.'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab01.jpg)'
  id: totrans-728
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.1* Extensions for `nvcc` Input Files'
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab02.jpg)'
  id: totrans-730
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.2* Compilation Trajectories'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab03.jpg)![Image](graphics/04tab03a.jpg)'
  id: totrans-732
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.3* `nvcc` Options (Environment)'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab04.jpg)'
  id: totrans-734
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.4* Options for Specifying Behavior of Compiler/Linker'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab05.jpg)'
  id: totrans-736
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.5* `nvcc` Options for Passthrough'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab06.jpg)![Image](graphics/04tab06a.jpg)'
  id: totrans-738
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.6* Miscellaneous `nvcc` Options'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 4.7](ch04.html#ch04tab07) lists `nvcc` options related to code generation.
    The `--gpu-architecture` and `--gpu-code` options are especially confusing. The
    former controls which *virtual* GPU architecture to compile for (i.e., which version
    of PTX to emit), while the latter controls which *actual* GPU architecture to
    compile for (i.e., which version of SM microcode to emit). The `--gpu-code` option
    must specify SM versions that are at least as high as the versions specified to
    `--gpu-architecture`.'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab07.jpg)![Image](graphics/04tab07a.jpg)'
  id: totrans-741
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.7.* `nvcc` Options for Code Generation'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: The `--export-dir` option specifies a directory where all device code images
    will be copied. It is intended as a device code repository that can be inspected
    by the CUDA driver when the application is running (in which case the directory
    should be in the `CUDA_DEVCODE_PATH` environment variable). The repository can
    be either a directory or a ZIP file. In either case, CUDA will maintain a directory
    structure to facilitate code lookup by the CUDA driver. If a filename is specified
    but does not exist, a directory structure (not a ZIP file) will be created at
    that location.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. `ptxas`—the PTX Assembler
  id: totrans-744
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ptxas`, the tool that compiles PTX into GPU-specific microcode, occupies a
    unique place in the CUDA ecosystem in that NVIDIA makes it available both in the
    offline tools (which developers compile into applications) and as part of the
    driver, enabling so-called “online” or “just-in-time” (JIT) compilation (which
    occurs at runtime).'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
- en: When compiling offline, `ptxas` generally is invoked by `nvcc` if any actual
    GPU architectures are specified with the `--gpu-code` command-line option. In
    that case, command-line options (summarized in [Table 4.8](ch04.html#ch04tab08))
    can be passed to `ptxas` via the `-Xptxas` command-line option to `nvcc`.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab08.jpg)![Image](graphics/04tab08a.jpg)![Image](graphics/04tab08b.jpg)'
  id: totrans-747
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.8.* Command-Line Options for `ptxas`'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
- en: Developers also can load PTX code dynamically by invoking `cuModuleLoadDataEx()`,
    as follows.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuModuleLoadDataEx (
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
- en: CUmodule *module,
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
- en: const void *image,
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int numOptions,
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
- en: CUjit_option *options,
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
- en: void **optionValues
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
- en: );
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
- en: '`cuModuleLoadDataEx()` takes a pointer image and loads the corresponding module
    into the current context. The pointer may be obtained by mapping a `cubin` or
    `PTX` or `fatbin` file, passing a `cubin` or `PTX` or `fatbin` file as a NULL-terminated
    text string, or incorporating a `cubin` or `fatbin` object into the executable
    resources and using operating system calls such as Windows `FindResource()` to
    obtain the pointer. Options are passed as an array via options, and any corresponding
    parameters are passed in `optionValues`. The number of total options is specified
    by `numOptions`. Any outputs will be returned via `optionValues`. Supported options
    are given in [Table 4.9](ch04.html#ch04tab09).'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab09.jpg)![Image](graphics/04tab09a.jpg)'
  id: totrans-758
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.9.* Options for `cuModuleLoadDataEx()`'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. `cuobjdump`
  id: totrans-760
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`cuobjdump` is the utility that may be used to examine the binaries generated
    by CUDA. In particular, it is useful for examining the microcode generated by
    `nvcc.` Specify the `--cubin` parameter to `nvcc` to generate a `.cubin` file,
    and then use'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
- en: cuobjdump --dump-sass <filename.cubin>
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
- en: to dump the disassembled microcode from the `.cubin` file. The complete list
    of command-line options for `cuobjdump` is given in [Table 4.10](ch04.html#ch04tab10).
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab10.jpg)![Image](graphics/04tab10a.jpg)'
  id: totrans-764
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.10* `cuobjdump` Command-Line Options'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. `nvidia-smi`
  id: totrans-766
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`nvidia-smi`, the NVIDIA System Management Interface, is used to manage the
    environment in which Tesla-class NVIDIA GPU boards operate. It can report GPU
    status and control aspects of GPU execution, such as whether ECC is enabled and
    how many CUDA contexts can be created on a given GPU.'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
- en: When `nvidia-smi` is invoked with the `-–help (-h)` option, it generates a usage
    message that, besides giving a brief description of its purpose and command-line
    options, also gives a list of supported products. Tesla- and Quadro-branded GPUs
    are fully supported, while GeForce-branded GPUs get limited support.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
- en: Many of the GPU boards supported by `nvidia-smi` include multiple GPUs; `nvidia-smi`
    refers to these boards as *units*. Some operations, such as toggling the status
    of an LED (light emitting diode), are available only on a per-unit basis.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
- en: '`nvidia-smi` has several modes of operation. If no other command-line parameters
    are given, it lists a summary of available GPUs that can be refined by the command-line
    options in [Table 4.11](ch04.html#ch04tab11). Otherwise, the other command-line
    options that are available include the following.'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
- en: '• List: The `--list-gpus (-L)` option displays a list of available GPUs and
    their UUIDs. Additional options to refine the listing are summarized in [Table
    4.11](ch04.html#ch04tab11).'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab11.jpg)'
  id: totrans-772
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.11* `nvidia-smi` List Options'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
- en: '• Query: The `--query (-q)` option displays GPU or unit information. Additional
    options to refine the query are summarized in [Table 4.12](ch04.html#ch04tab12).'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab12.jpg)'
  id: totrans-775
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.12* `nvidia-smi` Query Options'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
- en: '• Document Type Definition (DTD): The `--dtd` option produces the Document
    Type Definition for the XML-formatted output of `nvidia-smi`. The `--filename
    (-f)` option optionally specifies an output file; the `--unit (-u)` option causes
    the DTD for GPU boards (as opposed to GPUs) to be written.'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
- en: '• Device modification: The options specified in [Table 4.13](ch04.html#ch04tab13)
    may be used to set the persistent state of the GPU, such as whether ECC (error
    correction) is enabled.'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab13.jpg)![Image](graphics/04tab13a.jpg)'
  id: totrans-779
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.13* `nvidia-smi` Device Modification Options'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
- en: '• Unit modification: The `--toggle-led` option (`-t`) may be set to `0/GREEN`
    or `1/AMBER`. The `--id (-i)` option can be used to target a specific unit.'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Amazon Web Services
  id: totrans-782
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Amazon Web Services is the preeminent vendor of “infrastructure as a service”
    (IAAS) cloud computing services. Their Web services enable customers to allocate
    storage, transfer data to and from their data centers, and run servers in their
    data centers. In turn, customers are charged for the privilege on an a la carte
    basis: per byte of storage, per byte transferred, or per instance-hour of server
    time. On the one hand, customers can access potentially unlimited compute resources
    without having to invest in their own infrastructure, and on the other hand, they
    need only pay for the resources they use. Due to the flexibility and the cloud’s
    ability to accommodate rapidly increasing demand (say, if an independent game
    developer’s game “goes viral”), cloud computing is rapidly increasing in popularity.'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
- en: A full description of the features of AWS and how to use them is outside the
    scope of this book. Here we cover some salient features for those who are interested
    in test-driving CUDA-capable virtual machines.
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
- en: • S3 (Simple Storage Service) objects can be uploaded and downloaded.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
- en: • EC2 (Elastic Compute Cloud) instances can be launched, rebooted, and terminated.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
- en: • EBS (Elastic Block Storage) volumes can be created, copied, attached to EC2
    instances, and destroyed.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
- en: • It features security groups, which are analogous to firewalls for EC2 instances.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
- en: • It features key pairs, which are used for authentication.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
- en: All of the functionality of Amazon Web Services is accessible via the AWS Management
    Console, accessible via aws.amazon.com. The AWS Management Console can do many
    tasks not listed above, but the preceding handful of operations are all we’ll
    need in this book.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1\. Command-Line Tools
  id: totrans-791
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The AWS command-line tools can be downloaded from `[http://aws.amazon.com/developertools](http://aws.amazon.com/developertools)`.
    Look for “Amazon EC2 API Tools.” These tools can be used out of the box on Linux
    machines; Windows users can install Cygwin. Once installed, you can use commands
    such as `ec2-run-instances` to launch EC2 instances, `ec2-describe-instances`
    to give a list of running instances, or `ec2-terminate-instances` to terminate
    a list of instances. Anything that can be done in the Management Console also
    can be done using a command-line tool.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2\. EC2 and Virtualization
  id: totrans-793
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EC2, the “Elastic Compute Cloud,” is the member of the AWS family that enables
    customers to “rent” a CUDA-capable server for a period of time and be charged
    only for the time the server was in use. These virtual computers, which look to
    the customer like standalone servers, are called *instances*. Customers can use
    EC2’s Web services to launch, reboot, and terminate instances according to their
    need for the instances’ computing resources.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
- en: One of the enabling technologies for EC2 is *virtualization*, which enables
    a single server to host multiple “guest” operating systems concurrently. A single
    server in the EC2 fleet potentially can host several customers’ running instances,
    improving the economies of scale and driving down costs. Different instance types
    have different characteristics and pricing. They may have different amounts of
    RAM, CPU power,^([1](ch04.html#ch04fn1)) local storage, and I/O performance, and
    the on-demand pricing may range from $0.085 to $2.40 per instance-hour. As of
    this writing, the CUDA-capable `cg1.4xlarge` instance type costs $2.10 per instance-hour
    and has the following characteristics.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch04.html#ch04fn1a). The CPU capabilities are measured in EC2 Compute Units
    (ECUs). As of this writing, the ECUs available from a given instance range from
    1 (in the `m1.small` instance type) to 88.5 (in the `cc2.8xlarge` instance type).'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
- en: • 23 GB of RAM
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
- en: • 33.5 ECUs (two quad-core Intel Xeon X5570 “Nehalem” CPUs)
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
- en: • 1690 GB of instance storage
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
- en: • 64-bit platform
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
- en: Since `cg1.4xlarge` is a member of the “cluster” instance family, only a single
    instance will run on a given server; also, it is plugged into a much higher bandwidth
    network than other EC2 instance types to enable cluster computing for parallel
    workloads.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.3\. Key Pairs
  id: totrans-802
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Access to EC2 instances is facilitated by *key pairs*. The term refers to the
    central concept in public key cryptography that the authentication is performed
    using a private key (available only to those who are authorized) and a public
    key that can be freely shared.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
- en: 'When a key pair is created, the private key is downloaded in the form of a
    `.pem` file. There are two reasons to keep careful track of a `.pem` file after
    creating a key pair: First, anyone with access to the `.pem` file can use it to
    gain access to your EC2 computing resources, and second, there is no way to obtain
    new copies of the private key! Amazon is not in the key retention business, so
    once the private key is downloaded, it is yours to keep track of.'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 4.1](ch04.html#ch04lis01) gives an example `.pem` file. The format
    is convenient because it has anchor lines (the `“BEGIN RSA PRIVATE KEY”/ “END
    RSA PRIVATE KEY”`) and is “7-bit clean” (i.e., only uses ASCII text characters),
    so it can be emailed, copied-and-pasted into text fields, appended to files such
    as `~/.ssh/authorized_keys` to enable password-less login, or published in books.
    The name for a given key pair is specified when launching an EC2 instance; in
    turn, the corresponding private key file is used to gain access to that instance.
    To see more specifically how the `.pem` file is used to access EC2 instances,
    see the sections on Linux and Windows below.'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 4.1.* Example `.pem` file.'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch04_images.html#p04lis01a)'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
- en: '-----BEGIN RSA PRIVATE KEY-----'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
- en: MIIEowIBAAKCAQEA2mHaXk9tTZqN7ZiUWoxhcSHjVCbHmn1SKamXqOKdLDfmqducvVkAlB1cjIz/
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
- en: NcwIHk0TxbnEPEDyPPHg8RYGya34evswzBUCOIcilbVIpVCyaTyzo4k0WKPW8znXJzQpxr/OHzzu
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
- en: tAvlq95HGoBobuGM5kaDSlkugOmTUXFKxZ4ZN1rm2kUo21N2m9jrkDDq4qTMFxuYW0H0AXeHOfNF
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
- en: ImroUCN2udTWOjpdgIPCgYEzz3Cssd9QIzDyadw+wbkTYq7eeqTNKULs4/gmLIAw+EXKE2/seyBL
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
- en: leQeK11j1TFhDCjYRfghp0ecv4UnpAtiO6nNzod7aTAR1bXqJXbSqwIDAQABAoIBAAh2umvlUCst
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
- en: zkpjG3zW6//ifFkKl7nZGZIbzJDzF3xbPklfBZghFvCmoquf21ROcBIckqObK4vaSIksJrexTtoK
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
- en: MBM0IRQHzGo8co6y0/n0QrXpcFzqOGknEHGk0D3ou6XEUUzMo8O+okwi9UaFq4aAn2FdYkFDa5X7
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
- en: d4Y0id1WzPcVurOSrnFNkWl4GRu+pluD2bmSmb7RUxQWGbP7bf98EyhpdugOdO7R3yOCcdaaGg0L
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
- en: hdTlwJ3jCP9dmnk7NqApRzkv7R1sXzOnU2v3b9+WpF0g6wCeM2eUuK1IY3BPl0Pg+Q4xU0jpRSr0
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
- en: vLDt8fUcIdH4PXTKua1NxsBA1uECgYEA72wC3BmL7HMIgf33yvK+/yA1z6AsAvIIAlCHJOi9sihT
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
- en: XF6dnfaJ6d12oCj1RUqG9e9Y3cW1YjgcdqQBk5F8M6bPuIfzOctM/urd1ryWZ3ddSxgBaLEO1h4c
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
- en: 3/cQWGGvaMPpDSAihs2d/CnnlVoQGiQrlWxDGzIHzu8RRV43fKcCgYEA6YDkj6kzlx4cuQwwsPVb
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
- en: IfdtP6WrHe+Ro724ka3Ry+4xFPcarXj5yl5/aPHNpdPPCfR+uYNjBiTD90w+duV8LtBxJoF+i/lt
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
- en: Mui4116xXMBaMGQfFMS0u2+z3aZI8MXZF8gGDIrI9VVfpDCi2RNKaT7KhfraZ8VzZsdAqDO8Zl0C
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
- en: gYEAvVq3iEvMFl2ERQsPhzslQ7G93U/Yfxvcqbf2qoJIRTcPduZ90gjCWmwE/fZmxT6ELs31grBz
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
- en: HBM0r8BWXteZW2B6uH8NJpBbfOFUQhk0+u+0oUeDFcGy8jUusRM9oijgCgOntfHMXMESSfT6a2yn
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
- en: f4VL0wmkqUWQV2FMT4iMadECgYATFUGYrA9XTlKynNht3d9wyzPWe8ecTrPsWdj3rujybaj9OaSo
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
- en: gLaJX2eyP/C6mLDW83BX4PD6045ga46/UMnxWX+l0fdxoRTXkEVq9IYyOlYklkoj/F944gwlFS3o
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
- en: 34J6exJjfAQoaK3EUWU9sGHocAVFJdcrm+tufuI93NyM0QKBgB+koBIkJG8u0f19oW1dhUWERsuo
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
- en: poXZ9Kh/GvJ9u5DUwv6F+hCGRotdBFhjuwKNTbutdzElxDMNHKoy/rhiqgcneMUmyHh/F0U4sOWl
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
- en: XqqMD2QfKXBAU0ttviPbsmm0dbjzTTd3FO1qx2K90T3u9GEUdWYqMxOyZjUoLyNr+Tar
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
- en: '-----END RSA PRIVATE KEY-----'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.4\. Availability Zones (AZs) and Regions
  id: totrans-833
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AWS provides its services in separate Availability Zones (AZs) that are carefully
    segregated from one another, with the intention of preventing outages affecting
    one Availability Zone from affecting any other Availability Zone. For CUDA developers,
    the main consideration to bear in mind is that instances, EBS volumes, and other
    resources must be in the same AZ to interoperate.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.5\. S3
  id: totrans-835
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: S3 (Simple Storage Service) is designed to be a reliable way to store data for
    later retrieval. The “objects” (basically files) are stored in a hierarchical
    layout consisting of “buckets” at the top of the hierarchy, with optional intervening
    “folders.”
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
- en: Besides storage and retrieval (“PUT” and “GET,” respectively), S3 includes the
    following features.
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
- en: • Permissions control. By default, S3 objects are accessible only to the owner
    of the S3 account, but they may be made public or permissions can be granted to
    specific AWS accounts.
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
- en: • Objects may be encrypted.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
- en: • Metadata may be associated with an S3 object, such as the language of a text
    file or whether the object is an image.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
- en: '• Logging: Operations performed on S3 objects can be logged (and the logs are
    stored as more S3 objects).'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
- en: '• Reduced Redundancy: S3 objects can be stored with a lower reliability factor,
    for a lower price.'
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
- en: '• Notifications: Automatic notifications can be set up, to, for example, let
    the customer know if loss of a Reduced Redundancy object is detected.'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
- en: '• Object lifetime management: Objects can be scheduled to be deleted automatically
    after a specified period of time.'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
- en: Many other AWS services use S3 as a persistent data store; for example, snapshots
    of AMIs and EBS volumes are stored in S3.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.6\. EBS
  id: totrans-846
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EBS (Elastic Block Storage) consists of network-based storage that can be allocated
    and attached and detached to running instances. AWS customers also can “snapshot”
    EBS volumes, creating templates for new EBS volumes.
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
- en: EC2 instances often have a root EBS volume that contains the operating system
    and driver software. If more storage is desired, you can create and attach an
    EBS volume and mount it within the guest operating system.^([2](ch04.html#ch04fn2))
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch04.html#ch04fn2a). You may have to change the OS configuration if you
    want the EBS volume to be available to instances launched from a derivative AMI;
    see the “[Linux on EC2](ch04.html#ch04lev2sec8)” or “[Windows on EC2](ch04.html#ch04lev2sec9)”
    sections in this chapter.'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.7\. AMIs
  id: totrans-850
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Amazon Machine Images (AMIs) are descriptions of what an EC2 instance would
    “look like” once launched, including the operating system and the number and contents
    of attached EBS volumes. Most EC2 customers start with a “stock” AMI provided
    by Amazon, modify it to their satisfaction, and then take a snapshot of the AMI
    so they can launch more instances with the same setup.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
- en: When an instance is launched, EC2 will take a few minutes to muster the requested
    resources and boot the virtual machine. Once the instance is running, you can
    query its IP address and access it over the Internet using the private key whose
    name was specified at instance launch time.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
- en: The external IP address of the instance is incorporated into the DNS name. For
    example, a `cg1.4xlarge` instance might be named
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
- en: ec2-70-17-16-35.compute-1.amazonaws.com
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
- en: and the external IP address of that machine is `70.17.16.35.`^([3](ch04.html#ch04fn3))
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch04.html#ch04fn3a). IP addresses have been changed to protect the innocent.'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
- en: EC2 instances also have internal IP addresses that can be used for intracluster
    communication. If, for example, you launch a cluster of instances that need to
    communicate using software such as the Message Passing Interface (MPI), use the
    internal IP addresses.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.8\. Linux on EC2
  id: totrans-858
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EC2 supports many different flavors of Linux, including an Amazon-branded flavor
    (“Amazon Linux”) that is derived from Red Hat. Once an instance is launched, it
    may be accessed via `ssh` using the key pair that was used to launch the instance.
    Using the IP address above and the `Example.pem` file in [Listing 4.1](ch04.html#ch04lis01),
    we might type
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
- en: ssh –i Example.pem ec2-user@70.17.16.35
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
- en: '(The root username varies with the flavor of Linux: `ec2-user` is the root
    username for Amazon Linux, while CentOS uses `root` and Ubuntu uses `ubuntu`.)'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
- en: Once logged in, the machine is all yours! You can add users and set their passwords,
    set up SSH for password-less login, install needed software (such as the CUDA
    toolchain), attach more EBS volumes, and set up the ephemeral disks. You can then
    snapshot an AMI to be able to launch more instances that look exactly like the
    one you’ve set up.
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
- en: EBS
  id: totrans-863
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: EBS (Elastic Block Storage) volumes are easy to create, either from a blank
    volume or by making a live copy of a snapshot. Once created, the EBS volume may
    be attached to an instance, where it will appear as a device (such as `/dev/sdf`
    or, on more recent Linux kernels, `/dev/xvdf`). When the EBS volume is first attached,
    it is just a raw block storage device that must be formatted before use using
    a command such as `mkfs.ext3`. Once formatted, the drive may be mounted to a directory.
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
- en: mount <Device> <Directory>
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you want to snapshot an AMI and for the drive to be visible on instances
    launched using the derivative AMI, edit `/etc/fstab` to include the volume. When
    creating an EBS volume to attach to a running instance, make sure to create it
    in the same Availability Zone (e.g., us-east-1b) as the instance.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
- en: Ephemeral Storage
  id: totrans-867
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Many EC2 instance types, including `cg1.4xlarge`, have local hard disks associated
    with them. These disks, when available, are used strictly for scratch local storage;
    unlike EBS or S3, no erasure encoding or other technologies are employed to make
    the disks appear more reliable. To emphasize this reduced reliability, the disks
    are referred to as *ephemeral storage*.
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
- en: To make ephemeral disks available, specify the “-b” option to ec2-run-instances—for
    example,
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch04_images.html#p115pro01a)'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
- en: ec2-run-instances –t cg1.4xlarge –k nwiltEC2 –b /dev/sdb=ephemeral0
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
- en: /dev/sdc=ephemeral1
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
- en: Like EBS volumes, ephemerals must be formatted (e.g., mkfs.ext3) and mounted
    before they can be used, and they must have `fstab` entries in order to reappear
    when the instance is rebooted.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
- en: User Data
  id: totrans-874
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: User data may be specified to an instance, either at launch time or while an
    instance is running (in which case the instance must be rebooted). The user data
    then may be queried at
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
- en: '[http://169.254.169.254/latest/user-data](http://169.254.169.254/latest/user-data)'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.9\. Windows on EC2
  id: totrans-877
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Windows instances are accessed in a slightly different way than Linux instances.
    Once launched, customers must use their private key file to retrieve the password
    for the EC2 instance’s Administrator account. You can either specify your `.pem`
    file or copy-and-paste its contents into the AWS Management Console (shown in
    [Figure 4.1](ch04.html#ch04fig01)).
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04fig01.jpg)'
  id: totrans-879
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4.1* AWS Windows password retrieval.'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
- en: By default, this password-generation behavior is only in force on “stock” AMIs
    from AWS. If you “snapshot” one of these AMIs, they will retain whatever passwords
    were present on the machine when the snapshot was taken. To create a new Windows
    AMI that generates a random password upon launch, run the “EC2 Config Service”
    tool (available in the Start menu), click the “Bundle” tab, and click the button
    that says “Run Sysprep and Shutdown Now” ([Figure 4.2](ch04.html#ch04fig02)).
    After clicking this button, any AMI created against it will generate a random
    password, like the stock Windows AMIs.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04fig02.jpg)'
  id: totrans-882
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4.2* Sysprep for Windows on EC2.'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
- en: Ephemeral Storage
  id: totrans-884
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In order for ephemeral storage to be useable by a Windows instance, you must
    specify the –b option to `ec2-run-instances`, as follows.
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch04_images.html#p116pro01a)'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
- en: ec2-run-instances –t cg1.4xlarge –k nwiltEC2 –b /dev/sdb=ephemeral0
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
- en: /dev/sdc=ephemeral1
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
- en: User Data
  id: totrans-889
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: User data may be specified to an instance, either at launch time or while an
    instance is running (in which case the instance must be rebooted). The user data
    then may be queried at
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
- en: '[http://169.254.169.254/latest/user-data](http://169.254.169.254/latest/user-data)'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
