- en: Part I
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 1\. Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Much ink has been spilled describing the GPU revolution in computing. I have
    read about it with interest because I got involved very early. I was at Microsoft
    in the mid-1990s as development lead for Direct3D when Intel and AMD were introducing
    the first multimedia instruction sets to accelerate floating point computation.
    Intel had already tried (unsuccessfully) to forestall the migration of clock cycles
    for 3D rasterization from their CPUs by working with Microsoft to ship rasterizers
    that used their MMX instruction set. I knew that effort was doomed when we found
    that the MMX rasterizer, running on a yet-to-be-released Pentium 2 processor,
    was half as fast as a humble S3 Virge GX rasterizer that was available for sale.
  prefs: []
  type: TYPE_NORMAL
- en: For Direct3D 6.0, we worked with CPU vendors to integrate their code into our
    geometry pipeline so developers could transparently benefit from vendor-optimized
    code paths that used new instruction sets from Intel and AMD. Game developers
    embraced the new geometry pipeline, but it did not forestall the continued migration
    of clock cycles from the CPU to the GPU, as the new instruction sets were used
    to generate vertex data for consumption by GPUs’ hardware geometry pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'About this time, the number of transistors on GPUs overtook the number of transistors
    on CPUs. The crossover was in 1997–1998, when the Pentium 2 and the NVIDIA RIVA
    TNT both had transistor counts of about 8M. Subsequently, the Geforce 256 (15M
    transistors), Geforce 2 (28M transistors), and Geforce3 (63M transistors) all
    had more transistors than contemporary CPUs. Additionally, the architectural differences
    between the two devices were becoming clear: Most of the die area for CPUs was
    dedicated to cache, while most of the die area for GPUs was dedicated to logic.
    Intel was able to add significant new instruction set extensions (MMX, SSE, SSE2,
    etc.) with negligible area cost because their CPUs were mostly cache. GPUs were
    designed for parallel throughput processing; their small caches were intended
    more for bandwidth aggregation than for reducing latency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While companies like ATI and NVIDIA were building GPUs that were faster and
    increasingly capable, CPU vendors continued to drive clock rates higher as Moore’s
    Law enabled both increased transistor budgets and increased clock speeds. The
    first Pentium (c. 1993) had a clock rate of 60MHz, while MMX-enabled Pentiums
    (c. 1997) had clock rates of 200MHz. By the end of the decade, clock rates had
    exceeded 1,000MHz. But shortly thereafter, an important event in the history of
    computing took place: Moore’s Law hit a wall. The transistors would continue to
    shrink, but clock rates could not continue to increase.'
  prefs: []
  type: TYPE_NORMAL
- en: The event was not unexpected. Pat Gelsinger of Intel delivered a keynote at
    the 2001 IEEE Solid-State Circuits Conference and stated that if chips continued
    on their current design path, they would be as hot as nuclear reactors by the
    end of the decade and as hot as the surface of the sun by 2015\. In the future,
    performance would have to come from “simultaneous multithreading” (SMT), possibly
    supported by putting multiple CPU cores on a single chip. Indeed, that is exactly
    what CPU vendors have done; today, it is difficult to almost impossible to find
    a desktop PC with a single-core CPU. But the decades-long free ride enabled by
    Moore’s Law, in which increased clock rates made it possible for applications
    to run faster with little to no effort on the part of software developers, was
    over. Multicore CPUs require multithreaded applications. Only applications that
    benefit from parallelism can expect increased performance from CPUs with a larger
    number of cores.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs were well positioned to take advantage of this new trend in Moore’s Law.
    While CPU applications that had not been authored with parallelism in mind would
    require extensive refactoring (if they could be made parallel at all), graphics
    applications were already formulated in a way that exploited the inherent parallelism
    between independent pixels. For GPUs, increasing performance by increasing the
    number of execution cores was a natural progression. In fact, GPU designers tend
    to prefer more cores over more capable cores. They eschew strategies that CPU
    manufacturers take for granted, like maximizing clock frequency (GPUs had never,
    and still do not, run at clock rates approaching the limits of transistor fabrication),
    speculative execution, branch prediction, and store forwarding. And to prevent
    this ever-more-capable processor from becoming I/O bound, GPU designers integrated
    memory controllers and worked with DRAM manufacturers to enable bandwidths that
    far exceeded the amount of bandwidth available to CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'But that abundant horsepower was difficult for nongraphics developers to exploit.
    Some adventurous souls used graphics APIs such as Direct3D and OpenGL to subvert
    graphics hardware to perform nongraphics computations. The term *GPGPU* (general-purpose
    GPU programming) was invented to describe this approach, but for the most part,
    the computational potential of GPUs remained untapped until CUDA. Ian Buck, whose
    Brook project at Stanford enabled simplified development of GPGPU applications,
    came to NVIDIA and led development of a new set of development tools that would
    enable nongraphics applications to be authored for GPUs much more easily. The
    result is CUDA: a proprietary toolchain from NVIDIA that enables C programmers
    to write parallel code for GPUs using a few easy-to-use language extensions.'
  prefs: []
  type: TYPE_NORMAL
- en: Since its introduction in 2007, CUDA has been well received. Tens of thousands
    of academic papers have been written that use the technology. It has been used
    in commercial software packages as varied as Adobe’s CS5 to Manifold’s GIS (geographic
    information system). For suitable workloads, CUDA-capable GPUs range from 5x to
    400x faster than contemporary CPUs. The sources of these speedups vary. Sometimes
    the GPUs are faster because they have more cores; sometimes because they have
    higher memory bandwidth; and sometimes because the application can take advantage
    of specialized GPU hardware not present in CPUs, like the texture hardware or
    the SFU unit that can perform fast transcendentals. Not all applications can be
    implemented in CUDA. In fact, not all *parallel* applications can be implemented
    in CUDA. But it has been used in a wider variety of applications than any other
    GPU computing technology. I hope this book helps accomplished CUDA developers
    to get the most out of CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Our Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CUDA is a difficult topic to write about. Parallel programming is complicated
    even without operating system considerations (Windows, Linux, MacOS), platform
    considerations (Tesla and Fermi, integrated and discrete GPUs, multiple GPUs),
    CPU/GPU concurrency considerations, and CUDA-specific considerations, such as
    having to decide between using the CUDA runtime or the driver API. When you add
    in the complexities of how best to structure CUDA kernels, it may seem overwhelming.
  prefs: []
  type: TYPE_NORMAL
- en: 'To present this complexity in a manageable way, most topics are explained more
    than once from different perspectives. *What does the texture mapping hardware
    do?* is a different question than *How do I write a kernel that does texture mapping?*
    This book addresses both questions in separate sections. Asynchronous memory copy
    operations can be explained in several different contexts: the interactions between
    software abstractions (for example, that participating host memory must be pinned),
    different hardware implementations, API support for the feature, and optimization
    strategies. Readers sometimes may wish to consult the index and read all of the
    different presentations on a given topic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimization guides are like advice columns: Too often, the guidance is offered
    without enough context to be applied meaningfully, and they often seem to contradict
    themselves. That observation isn’t intended to be pejorative; it’s just a symptom
    of the complexity of the problem. It has been at least 20 years since blanket
    generalizations could be made about CPU optimizations, and GPUs are more complicated
    to program, so it’s unrealistic to expect CUDA optimization advice to be simple.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, GPU computing is so new that GPU architects, let alone developers,
    are still learning how best to program them. For CUDA developers, the ultimate
    arbiter is usually performance, and performance is usually measured in wall clock
    time! Recommendations on grid and block sizes, how and when to use shared memory,
    how many results to compute per thread, and the implications of occupancy on performance
    should be confirmed empirically by implementing different approaches and measuring
    the performance of each.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Developers want CUDA code that is illustrative yet not a toy; useful but does
    not require a technical dive into a far-afield topic; and high performance but
    does not obscure the path taken by implementors from their initial port to the
    final version. To that end, this book presents three types of code examples designed
    to address each of those considerations: microbenchmarks, microdemos, and optimization
    journeys.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1\. Microbenchmarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Microbenchmarks are designed to illustrate the performance implications of a
    very specific CUDA question, such as how uncoalesced memory transactions degrade
    device memory bandwidth or the amount of time it takes the WDDM driver to perform
    a kernel thunk. They are designed to be compiled standalone and will look familiar
    to many CUDA programmers who’ve already implemented microbenchmarks of their own.
    In a sense, I wrote a set of microbenchmarks to obviate the need for other people
    to do the same.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2\. Microdemos
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Microdemos are small applications designed to shed light on specific questions
    of how the hardware or software behaves. Like microbenchmarks, they are small
    and self-contained, but instead of highlighting a performance question, they highlight
    a question of functionality. For example, the chapter on texturing includes microdemos
    that illustrate how to texture from 1D device memory, how the float→int conversion
    is performed, how different texture addressing modes work, and how the linear
    interpolation performed by texture is affected by the 9-bit weights.
  prefs: []
  type: TYPE_NORMAL
- en: Like the microbenchmarks, these microdemos are offered in the spirit in which
    developers probably wanted to write them, or at least have them available. I wrote
    them so you don’t have to!
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3\. Optimization Journeys
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many papers on CUDA present their results as a *fait accompli*, perhaps with
    some side comments on tradeoffs between different approaches that were investigated
    before settling on the final approach presented in the paper. Authors often have
    length limits and deadlines that work against presenting more complete treatments
    of their work.
  prefs: []
  type: TYPE_NORMAL
- en: For some select topics central to the data parallel programming enabled by CUDA,
    this book includes *optimization journeys* in the spirit of Mark Harris’s “Optimizing
    Parallel Reduction in CUDA” presentation that walks the reader through seven increasingly
    complex implementations of increasing performance.^([1](ch01.html#ch01fn1)) The
    topics we’ve chosen to address this way include reduction, parallel prefix sum
    (“scan”), and the N-body problem.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch01.html#ch01fn1a). [http://bit.ly/Z2q37x](http://bit.ly/Z2q37x)'
  prefs: []
  type: TYPE_NORMAL
- en: 1.3\. Administrative Items
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 1.3.1\. Open Source
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The source code that accompanies this book is available on [www.cudahandbook.com](http://www.cudahandbook.com),
    and it is open source, copyrighted with the 2-clause BSD license.^([2](ch01.html#ch01fn2))
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch01.html#ch01fn2a). [www.opensource.org/licenses/bsd-license.php](http://www.opensource.org/licenses/bsd-license.php)'
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.2\. CUDA Handbook Library (Chlib)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CUDA Handbook Library, located in the `chLib/` directory of the source code,
    contains a portable library with support for timing, threading, driver API utilities,
    and more. They are described in more detail in [Appendix A](app01.html#app01).
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.3\. Coding Style
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Arguments over brace placement aside, the main feature of the code in this book
    that will engender comment is the `goto`-based error handling mechanism. Functions
    that perform multiple resource allocations (or other operations that might fail,
    and where failure should be propagated to the caller) are structured around an
    Initialize / ErrorCheck / Cleanup idiom, similar to a pattern commonly used in
    Linux kernel code.
  prefs: []
  type: TYPE_NORMAL
- en: On failure, all cleanup is performed by the same body of code at the end of
    the function. It is important to initialize the resources to guaranteed-invalid
    values at the top of the function, so the cleanup code knows which resources must
    be freed. If a resource allocation or other function fails, the code performs
    a `goto` the cleanup code. `chError.h`, described in [Section A.6](app01.html#app01lev1sec6),
    defines error-handling macros for the CUDA runtime and the driver API that implement
    this idiom.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3.4\. CUDA SDK
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The SDK is a shared experience for all CUDA developers, so we assume you’ve
    installed the CUDA SDK and that you can build CUDA programs with it. The SDK also
    includes the GLUT (GL Utility Library), a convenient library that enables OpenGL
    applications to target a variety of operating systems from the same code base.
    GLUT is designed to build demo-quality as opposed to production-quality applications,
    but it fits the bill for our needs.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4\. Road Map
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The remaining chapters in [Part I](part01.html#part01) provide architectural
    overviews of CUDA hardware and software.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 2](ch02.html#ch02) details both the CUDA hardware platforms and the
    GPUs themselves.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 3](ch03.html#ch03) similarly covers the CUDA software architecture.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 4](ch04.html#ch04) covers the CUDA software environment, including
    descriptions of CUDA software tools and Amazon’s EC2 environment.
  prefs: []
  type: TYPE_NORMAL
- en: In [Part II](part02.html#part02), [Chapters 5](ch05.html#ch05) to [10](ch10.html#ch10)
    cover various aspects of the CUDA programming model in great depth.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 5](ch05.html#ch05) covers memory, including device memory, constant
    memory, shared memory, and texture memory.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 6](ch06.html#ch06) covers streams and events—the mechanisms used
    for “coarse-grained” parallelism between the CPU and GPU, between hardware units
    of the GPU such as copy engines and the streaming multiprocessors, or between
    discrete GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 7](ch07.html#ch07) covers kernel execution, including the dynamic
    parallelism feature that is new in SM 3.5 and CUDA 5.0.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 8](ch08.html#ch08) covers every aspect of streaming multiprocessors.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 9](ch09.html#ch09) covers multi-GPU applications, including peer-to-peer
    operations and embarrassingly parallel operations, with N-body as an example.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 10](ch10.html#ch10) covers every aspect of CUDA texturing.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in [Part III](part03.html#part03), [Chapters 11](ch11.html#ch11) to
    [15](ch15.html#ch15) discuss various targeted CUDA applications.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 11](ch11.html#ch11) describes bandwidth-bound, streaming workloads
    such as vector-vector multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapters 12](ch12.html#ch12) and [13](ch13.html#ch13) describe reduction
    and parallel prefix sum (otherwise known as scan), both important building blocks
    in parallel programming.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 14](ch14.html#ch14) describes N-body, an important family of applications
    with high computational density that derive a particular benefit from GPU computing.
  prefs: []
  type: TYPE_NORMAL
- en: • [Chapter 15](ch15.html#ch15) takes an in-depth look at an image processing
    operation called *normalized cross-correlation* that is used for feature extraction.
    [Chapter 15](ch15.html#ch15) features the only code in the book that uses texturing
    and shared memory together to deliver optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 2\. Hardware Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter provides more detailed descriptions of CUDA platforms, from the
    system level to the functional units within the GPUs. The first section discusses
    the many different ways that CUDA systems can be built. The second section discusses
    address spaces and how CUDA’s memory model is implemented in hardware and software.
    The third section discusses CPU/GPU interactions, with special attention paid
    to how commands are submitted to the GPU and how CPU/GPU synchronization is performed.
    Finally, the chapter concludes with a high-level description of the GPUs themselves:
    functional units such as copy engines and streaming multiprocessors, with block
    diagrams of the different types of streaming multiprocessors over three generations
    of CUDA-capable hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. CPU Configurations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section describes a variety of CPU/GPU architectures, with some comments
    on how a CUDA developer would approach programming the system differently. We
    examine a variety of CPU configurations, integrated GPUs, and multi-GPU configurations.
    We begin with [Figure 2.1](ch02.html#ch02fig01).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.1* CPU/GPU architecture simplified.'
  prefs: []
  type: TYPE_NORMAL
- en: An important element that was omitted from [Figure 2.1](ch02.html#ch02fig01)
    is the “chipset” or “core logic” that connects the CPU to the outside world. Every
    bit of input and output of the system, including disk and network controllers,
    keyboards and mice, USB devices, and, yes, GPUs, goes through the chipset. Until
    recently, chipsets were divided into a “southbridge” that connected most peripherals
    to the system^([1](ch02.html#ch02fn1)) and a “northbridge” that contained the
    graphics bus (the Accelerated Graphics Port, until the PCI Express [PCIe] bus
    displaced it) and a memory controller (“front side bus”) connected to the CPU
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch02.html#ch02fn1a). For simplicity, the southbridge is omitted from all
    diagrams in this section.'
  prefs: []
  type: TYPE_NORMAL
- en: Each “lane” in PCI Express 2.0 can theoretically deliver about 500MB/s of bandwidth,
    and the number of lanes for a given peripheral can be 1, 4, 8, or 16\. GPUs require
    the most bandwidth of any peripheral on the platform, so they generally are designed
    to be plugged into 16-lane PCIe slots. With packet overhead, the 8G/s of bandwidth
    for such a connection delivers about 6G/s in practice.^([2](ch02.html#ch02fn2))
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch02.html#ch02fn2a). PCI 3.0 delivers about twice as much bandwidth as
    PCIe 2.0.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1\. Front-Side Bus
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 2.2](ch02.html#ch02fig02) adds the northbridge and its memory controller
    to the original simplified diagram. For completeness, [Figure 2.2](ch02.html#ch02fig02)
    also shows the GPU’s integrated memory controller, which is designed under a very
    different set of constraints than the CPU’s memory controller. The GPU must accommodate
    so-called *isochronous* clients, such as video display(s), whose bandwidth requirements
    are fixed and nonnegotiable. The GPU’s memory controller also is designed with
    the GPU’s extreme latency-tolerance and vast memory bandwidth requirements in
    mind. As of this writing, high-end GPUs commonly deliver local GPU memory bandwidths
    well in excess of 100G/s. GPU memory controllers are always integrated with the
    GPU, so they are omitted from the rest of the diagrams in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.2* CPU/GPU architecture—northbridge.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2\. Symmetric Multiprocessors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 2.3](ch02.html#ch02fig03) shows a system with multiple CPUs in a traditional
    northbridge configuration.^([3](ch02.html#ch02fn3)) Before multicore processors,
    applications had to use multiple threads to take full advantage of the additional
    power of multiple CPUs. The northbridge must ensure that each CPU sees the same
    coherent view of memory, even though every CPU and the northbridge itself all
    contain caches. Since these so-called “symmetric multiprocessor” (SMP) systems
    share a common path to CPU memory, memory accesses exhibit relatively uniform
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch02.html#ch02fn3a). For reasons that will soon become clear, we offer
    [Figure 2.3](ch02.html#ch02fig03) more for historical reference than because there
    are CUDA-capable computers with this configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.3* Multiple CPUs (SMP configuration).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3\. Nonuniform Memory Access
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Starting with AMD’s Opteron and Intel’s Nehalem (i7) processors, the memory
    controller in the northbridge was integrated directly into the CPU, as shown in
    [Figure 2.4](ch02.html#ch02fig04). This architectural change improves CPU memory
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.4* CPU with integrated memory controller.'
  prefs: []
  type: TYPE_NORMAL
- en: For developers, the system in [Figure 2.4](ch02.html#ch02fig04) is only slightly
    different from the ones we’ve already discussed. For systems that contain multiple
    CPUs, as shown in [Figure 2.5](ch02.html#ch02fig05), things get more interesting.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.5* Multiple CPUs (NUMA).'
  prefs: []
  type: TYPE_NORMAL
- en: For machine configurations with multiple CPUs,^([4](ch02.html#ch02fn4)) this
    architecture implies that each CPU gets its own pool of memory bandwidth. At the
    same time, because multithreaded operating systems and applications rely on the
    cache coherency enforced by previous CPUs and northbridge configurations, the
    Opteron and Nehalem architectures also introduced HyperTransport (HT) and QuickPath
    Interconnect (QPI), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch02.html#ch02fn4a). On such systems, the CPUs also may be referred to
    as “nodes” or “sockets.”'
  prefs: []
  type: TYPE_NORMAL
- en: HT and QPI are point-to-point interconnects that connect CPUs to other CPUs,
    or CPUs to I/O hubs. On systems that incorporate HT/QPI, any CPU can access any
    memory location, but accesses are much faster to “local” memory locations whose
    physical address is in the memory directly attached to the CPU. Nonlocal accesses
    are resolved by using HT/QPI to snoop the caches of other CPUs, evict any cached
    copies of the requested data, and deliver the data to the CPU that performed the
    memory request. In general, the enormous on-chip caches on these CPUs mitigate
    the cost of these nonlocal memory accesses; the requesting CPU can keep the data
    in its own cache hierarchy until the memory is requested by another CPU.
  prefs: []
  type: TYPE_NORMAL
- en: To help developers work around these performance pitfalls, Windows and Linux
    have introduced APIs to enable applications to steer their allocations toward
    specific CPUs and to set CPU “thread affinities” so the operating system schedules
    threads onto CPUs so most or all of their memory accesses will be local.
  prefs: []
  type: TYPE_NORMAL
- en: 'A determined programmer can use these APIs to write contrived code that exposes
    the performance vulnerabilities of NUMA, but the more common (and insidious!)
    symptom is a slowdown due to “false sharing” where two threads running on different
    CPUs cause a plethora of HT/QPI transactions by accessing memory locations that
    are in the same cache line. So NUMA APIs must be used with caution: Although they
    give programmers the tools to improve performance, they also can make it easy
    for developers to inflict performance problems on themselves.'
  prefs: []
  type: TYPE_NORMAL
- en: One approach to mitigating the performance impact of nonlocal memory accesses
    is to enable *memory interleaving*, in which physical memory is evenly split between
    all CPUs on cache line boundaries.^([5](ch02.html#ch02fn5)) For CUDA, this approach
    works well on systems that are designed exactly as shown in [Figure 2.5](ch02.html#ch02fig05),
    with multiple CPUs in a NUMA configuration connected by a shared I/O hub to the
    GPU(s). Since PCI Express bandwidth is often a bottleneck to overall application
    performance, however, many systems have separate I/O hubs to service more than
    one PCI Express bus, as shown in [Figure 2.6](ch02.html#ch02fig06).
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch02.html#ch02fn5a). A cynic would say this makes all memory accesses “equally
    bad.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.6* Multi-CPU (NUMA configuration), multiple buses.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to run well on such “affinitized” systems, CUDA applications must take
    care to use NUMA APIs to match memory allocations and thread affinities to the
    PCI Express bus attached to a given GPU. Otherwise, memory copies initiated by
    the GPU(s) are nonlocal, and the memory transactions take an extra “hop” over
    the HT/QPI interconnect. Since GPUs demand a huge amount of bandwidth, these DMA
    operations reduce the ability of HT/QPI to serve its primary purpose. Compared
    to false sharing, the performance impact of nonlocal GPU memory copies is a much
    more plausible performance risk for CUDA applications.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4\. PCI Express Integration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Intel’s Sandy Bridge class processors take another step toward full system integration
    by integrating the I/O hub into the CPU, as shown in [Figure 2.7](ch02.html#ch02fig07).
    A single Sandy Bridge CPU has up to 40 lanes of PCI Express bandwidth (remember
    that one GPU can use up to 16 lanes, so 40 are enough for more than two full-size
    GPUs).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.7* Multi-CPU with integrated PCI Express.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For CUDA developers, PCI Express integration brings bad news and good news.
    The bad news is that PCI Express traffic is always affinitized. Designers cannot
    build systems like the system in [Figure 2.5](ch02.html#ch02fig05), where a single
    I/O hub serves multiple CPUs; all multi-CPU systems resemble [Figure 2.6](ch02.html#ch02fig06).
    As a result, GPUs associated with different CPUs cannot perform peer-to-peer operations.
    The good news is that the CPU cache can participate in PCI Express bus traffic:
    The CPU can service DMA read requests out of cache, and writes by the GPU are
    posted to the CPU cache.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Integrated GPUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, the term *integrated* means “integrated into the chipset.” As [Figure
    2.8](ch02.html#ch02fig08) shows, the memory pool that previously belonged only
    to the CPU is now shared between the CPU and the GPU that is integrated into the
    chipset. Examples of NVIDIA chipsets with CUDA-capable GPUs include the MCP79
    (for laptops and netbooks) and MCP89\. MCP89 is the last and greatest CUDA-capable
    x86 chipset that NVIDIA will manufacture; besides an integrated L3 cache, it has
    3x as many SMs as the MCP7x chipsets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.8* Integrated GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA’s APIs for mapped pinned memory have special meaning on integrated GPUs.
    These APIs, which map host memory allocations into the address space of CUDA kernels
    so they can be accessed directly, also are known as “zero-copy,” because the memory
    is shared and need not be copied over the bus. In fact, for transfer-bound workloads,
    an integrated GPU can outperform a much larger discrete GPU.
  prefs: []
  type: TYPE_NORMAL
- en: “Write-combined” memory allocations also have significance on integrated GPUs;
    cache snoops to the CPU are inhibited on this memory, which increases GPU performance
    when accessing the memory. Of course, if the CPU reads from the memory, the usual
    performance penalties for WC memory apply.
  prefs: []
  type: TYPE_NORMAL
- en: Integrated GPUs are not mutually exclusive with discrete ones; the MCP7x and
    MCP89 chipsets provide for PCI Express connections ([Figure 2.9](ch02.html#ch02fig09)).
    On such systems, CUDA prefers to run on the discrete GPU(s) because most CUDA
    applications are authored with them in mind. For example, a CUDA application designed
    to run on a single GPU will automatically select the discrete one.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.9* Integrated GPU with discrete GPU(s).'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA applications can query whether a GPU is integrated by examining `cudaDeviceProp.integrated`
    or by passing `CU_DEVICE_ATTRIBUTE_INTEGRATED` to `cuDeviceGetAttribute()`.
  prefs: []
  type: TYPE_NORMAL
- en: For CUDA, integrated GPUs are not exactly a rarity; millions of computers have
    integrated, CUDA-capable GPUs on board, but they are something of a curiosity,
    and in a few years, they will be an anachronism because NVIDIA has exited the
    x86 chipset business. That said, NVIDIA has announced its intention to ship systems
    on a chip (SOCs) that integrate CUDA-capable GPUs with ARM CPUs, and it is a safe
    bet that zero-copy optimizations will work well on those systems.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Multiple GPUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section explores the different ways that multiple GPUs can be installed
    in a system and the implications for CUDA developers. For purposes of this discussion,
    we will omit GPU memory from our diagrams. Each GPU is assumed to be connected
    to its own dedicated memory.
  prefs: []
  type: TYPE_NORMAL
- en: Around 2004, NVIDIA introduced “SLI” (Scalable Link Interface) technology that
    enables multiple GPUs to deliver higher graphics performance by working in parallel.
    With motherboards that could accommodate multiple GPU boards, end users could
    nearly double their graphics performance by installing two GPUs in their system
    ([Figure 2.10](ch02.html#ch02fig10)). By default, the NVIDIA driver software configures
    these boards to behave as if they were a single, very fast GPU to accelerate graphics
    APIs such as Direct3D and OpenGL. End users who intend to use CUDA must explicitly
    enable it in the Display Control panel on Windows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.10* GPUs in multiple slots.'
  prefs: []
  type: TYPE_NORMAL
- en: It also is possible to build GPU boards that hold multiple GPUs ([Figure 2.11](ch02.html#ch02fig11)).
    Examples of such boards include the GeForce 9800GX2 (dual-G92), the GeForce GTX
    295 (dual-GT200), the GeForce GTX 590 (dual-GF110), and the GeForce GTX 690 (dual-GK104).
    The only thing shared by the GPUs on these boards is a bridge chip that enables
    both chips to communicate via PCI Express. They do not share memory resources;
    each GPU has an integrated memory controller that gives full-bandwidth performance
    to the memory connected to that GPU. The GPUs on the board can communicate via
    peer-to-peer memcpy, which will use the bridge chip to bypass the main PCIe fabric.
    In addition, if they are Fermi-class or later GPUs, each GPU can map memory belonging
    to the other GPU into its global address space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.11* Multi-GPU board.'
  prefs: []
  type: TYPE_NORMAL
- en: SLI is an NVIDIA technology that makes multiple GPUs (usually on the same board,
    as in [Figure 2.11](ch02.html#ch02fig11)) appear as a single, much faster GPU.
    When the graphics application downloads textures or other data, the NVIDIA graphics
    driver broadcasts the data to both GPUs; most rendering commands also are broadcast,
    with small changes to enable each GPU to render its part of the output buffer.
    Since SLI causes the multiple GPUs to appear as a single GPU, and since CUDA applications
    cannot be transparently accelerated like graphics applications, CUDA developers
    generally should disable SLI.
  prefs: []
  type: TYPE_NORMAL
- en: This board design oversubscribes the PCI Express bandwidth available to the
    GPUs. Since only one PCI Express slot’s worth of bandwidth is available to both
    GPUs on the board, the performance of transfer-limited workloads can suffer. If
    multiple PCI Express slots are available, an end user can install multiple dual-GPU
    boards. [Figure 2.12](ch02.html#ch02fig12) shows a machine with four GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.12* Multi-GPU boards in multiple slots.'
  prefs: []
  type: TYPE_NORMAL
- en: If there are multiple PCI Express I/O hubs, as with the system in [Figure 2.6](ch02.html#ch02fig06),
    the placement and thread affinity considerations for NUMA systems apply to the
    boards just as they would to single-GPU boards plugged into that configuration.
  prefs: []
  type: TYPE_NORMAL
- en: If the chipset, motherboard, operating system, and driver software can support
    it, even more GPUs can be crammed into the system. Researchers at the University
    of Antwerp caused a stir when they built an 8-GPU system called FASTRA by plugging
    four GeForce 9800GX2’s into a single desktop computer. A similar system built
    on a dual-PCI Express chipset would look like the one in [Figure 2.13](ch02.html#ch02fig13).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.13* Multi-GPU boards, multiple I/O hubs.'
  prefs: []
  type: TYPE_NORMAL
- en: As a side note, peer-to-peer memory access (the mapping of other GPUs’ device
    memory, not memcpy) does not work across I/O hubs or, in the case of CPUs such
    as Sandy Bridge that integrate PCI Express, sockets.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4\. Address Spaces in CUDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As every beginning CUDA programmer knows, the address spaces for the CPU and
    GPU are separate. The CPU cannot read or write the GPU’s device memory, and in
    turn, the GPU cannot read or write the CPU’s memory. As a result, the application
    must explicitly copy data to and from the GPU’s memory in order to process it.
  prefs: []
  type: TYPE_NORMAL
- en: The reality is a bit more complicated, and it has gotten even more so as CUDA
    has added new capabilities such as mapped pinned memory and peer-to-peer access.
    This section gives a detailed description of how address spaces work in CUDA,
    starting from first principles.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.1\. Virtual Addressing: A Brief History'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Virtual address spaces are such a pervasive and successful abstraction that
    most programmers use and benefit from them every day without ever knowing they
    exist. They are an extension of the original insight that it was useful to assign
    consecutive numbers to the memory locations in the computer. The standard unit
    of measure is the *byte*, so, for example, a computer with 64K of memory had memory
    locations 0..65535\. The 16-bit values that specify memory locations are known
    as *addresses*, and the process of computing addresses and operating on the corresponding
    memory locations is collectively known as *addressing*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Early computers performed *physical addressing.* They would compute a memory
    location and then read or write the corresponding memory location, as shown in
    [Figure 2.14](ch02.html#ch02fig14). As software grew more complex and computers
    hosting multiple users or running multiple jobs grew more common, it became clear
    that allowing any program to read or write any physical memory location was unacceptable;
    software running on the machine could fatally corrupt other software by writing
    the wrong memory location. Besides the robustness concern, there were also security
    concerns: Software could spy on other software by reading memory locations it
    did not “own.”'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.14* Simple 16-bit address space.'
  prefs: []
  type: TYPE_NORMAL
- en: As a result, modern computers implement *virtual address spaces.* Each program
    (operating system designers call it a *process*) gets a view of memory similar
    to [Figure 2.14](ch02.html#ch02fig14), but each process gets its own address space.
    They cannot read or write memory belonging to other processes without special
    permission from the operating system. Instead of specifying a physical address,
    the machine instruction specifies a *virtual address* to be translated into a
    physical address by performing a series of lookups into tables that were set up
    by the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: In most systems, the virtual address space is divided into *pages*, which are
    units of addressing that are at least 4096 bytes in size. Instead of referencing
    physical memory directly from the address, the hardware looks up a *page table
    entry* (PTE) that specifies the physical address where the page’s memory resides.
  prefs: []
  type: TYPE_NORMAL
- en: It should be clear from [Figure 2.15](ch02.html#ch02fig15) that virtual addressing
    enables a contiguous virtual address space to map to discontiguous pages in physical
    memory. Also, when an application attempts to read or write a memory location
    whose page has not been mapped to physical memory, the hardware signals a fault
    that must be handled by the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.15* Virtual address space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just a side note: In practice, no hardware implements a single-level page table
    as shown in [Figure 2.15](ch02.html#ch02fig15). At minimum, the address is split
    into at least two indices: an index into a “page directory” of page tables, and
    an index into the page table selected by the first index. The hierarchical design
    reduces the amount of memory needed for the page tables and enables inactive page
    tables to be marked nonresident and swapped to disk, much like inactive pages.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides a physical memory location, the PTEs contain permissions bits that the
    hardware can validate while doing the address translation. For example, the operating
    system can make pages read-only, and the hardware will signal a fault if the application
    attempts to write the page.
  prefs: []
  type: TYPE_NORMAL
- en: Operating systems use virtual memory hardware to implement many features.
  prefs: []
  type: TYPE_NORMAL
- en: '• *Lazy allocation*: Large amounts of memory can be “allocated” by setting
    aside PTEs with no physical memory backing them. If the application that requested
    the memory happens to access one of those pages, the OS resolves the fault by
    finding a page of physical memory at that time.'
  prefs: []
  type: TYPE_NORMAL
- en: '• *Demand paging*: Memory can be copied to disk and the page marked nonresident.
    If the memory is referenced again, the hardware signals a “page fault,” and the
    OS resolves the fault by copying the data to a physical page, fixing up the PTE
    to point there, and resuming execution.'
  prefs: []
  type: TYPE_NORMAL
- en: '• *Copy-on-write*: Virtual memory can be “copied” by creating a second set
    of PTEs that map to the same physical pages, then marking both sets of PTEs read-only.
    If the hardware catches an attempt to write to one of those pages, the OS can
    copy it to another physical page, mark both PTEs writeable again, and resume execution.
    If the application only writes to a small percentage of pages that were “copied,”
    copy-on-write is a big performance win.'
  prefs: []
  type: TYPE_NORMAL
- en: '• *Mapped file I/O*: Files can be mapped into the address space, and page faults
    can be resolved by accessing the file. For applications that perform random access
    on the file, it may be advantageous to delegate the memory management to the highly
    optimized VMM code in the operating system, especially since it is tightly coupled
    to the mass storage drivers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to understand that address translation is performed on *every*
    memory access performed by the CPU. To make this operation fast, the CPU contains
    special hardware: caches called translation lookaside buffers (TLBs) that hold
    recently translated address ranges, and “page walkers” that resolve cache misses
    in the TLBs by reading the page tables.^([6](ch02.html#ch02fn6)) Modern CPUs also
    include hardware support for “unified address spaces,” where multiple CPUs can
    access one another’s memory efficiently via AMD’s HT (HyperTransport) and Intel’s
    QuickPath Interconnect (QPI). Since these hardware facilities enable CPUs to access
    any memory location in the system using a unified address space, this section
    refers to “the CPU” and the “CPU address space” regardless of how many CPUs are
    in the system.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch02.html#ch02fn6a). It is possible to write programs (for both CPUs and
    CUDA) that expose the size and structure of the TLBs and/or the memory overhead
    of the page walkers if they stride through enough memory in a short enough period
    of time.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sidebar: Kernel Mode and User Mode'
  prefs: []
  type: TYPE_NORMAL
- en: A final point about memory management on CPUs is that the operating system code
    must use memory protections to prevent applications from corrupting the operating
    system’s own data structures—for example, the page tables that control address
    translation. To aid with this memory protection, operating systems have a “privileged”
    mode of execution that they use when performing critical system functions. In
    order to manage low-level hardware resources such as page tables or to program
    hardware registers on peripherals such as the disk or network controller or the
    CUDA GPU, the CPU must be running in *kernel mode*. The unprivileged execution
    mode used by application code is called *user mode*.^([7](ch02.html#ch02fn7))
    Besides code written by the operating system provider, low-level driver code to
    control hardware peripherals also runs in kernel mode. Since mistakes in kernel
    mode code can lead to system stability or security problems, kernel mode code
    is held to a higher quality standard. Also, many operating system services, such
    as mapped file I/O or other memory management facilities listed above, are not
    available in kernel mode.
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch02.html#ch02fn7a). The x86-specific terms for *kernel mode* and *user
    mode* are “Ring 0” and “Ring 3,” respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: To ensure system stability and security, the interface between user mode and
    kernel mode is carefully regulated. The user mode code must set up a data structure
    in memory and make a special system call that validates the memory and the request
    that is being made. This transition from user mode to kernel mode is known as
    a *kernel thunk*. Kernel thunks are expensive, and their cost sometimes must be
    taken into account by CUDA developers.
  prefs: []
  type: TYPE_NORMAL
- en: Every interaction with CUDA hardware by the user mode driver is arbitrated by
    kernel mode code. Often this means having resources allocated on its behalf—not
    only memory but also hardware resources such as the hardware register used by
    the user mode driver to submit work to the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: The bulk of CUDA’s driver runs in user mode. For example, in order to allocate
    page-locked system memory (e.g., with the `cudaHostAlloc()` function), the CUDA
    application calls into the user mode CUDA driver, which composes a request to
    the kernel mode CUDA driver and performs a kernel thunk. The kernel mode CUDA
    driver uses a mix of low-level OS services (for example, it may call a system
    service to map GPU hardware registers) and hardware-specific code (for example,
    to program the GPU’s memory management hardware) to satisfy the request.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2\. Disjoint Address Spaces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On the GPU, CUDA also uses virtual address spaces, although the hardware does
    not support as rich a feature set as do the CPUs. GPUs do enforce memory protections,
    so CUDA programs cannot accidentally read or corrupt other CUDA programs’ memory
    or access memory that hasn’t been mapped for them by the kernel mode driver. But
    GPUs do not support demand paging, so every byte of virtual memory allocated by
    CUDA must be backed by a byte of physical memory. Also, demand paging is the underlying
    hardware mechanism used by operating systems to implement most of the features
    outlined above.
  prefs: []
  type: TYPE_NORMAL
- en: Since each GPU has its own memory and address translation hardware, the CUDA
    address space is separate from the CPU address space where the host code in a
    CUDA application runs. [Figure 2.16](ch02.html#ch02fig16) shows the address space
    architecture for CUDA as of version 1.0, before mapped pinned memory became available.
    The CPU and GPU each had its own address space, mapped with each device’s own
    page tables. The two devices exchanged data via explicit memcpy commands. The
    GPU could allocate *pinned memory*—page-locked memory that had been mapped for
    DMA by the GPU—but pinned memory only made DMA faster; it did not enable CUDA
    kernels to access host memory.^([8](ch02.html#ch02fn8))
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch02.html#ch02fn8a). On 32-bit operating systems, CUDA-capable GPUs can
    map pinned memory for memcpy in a 40-bit address space that is outside the CUDA
    address space used by kernels.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.16* Disjoint address spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA driver tracks pinned memory ranges and automatically accelerates memcpy
    operations that reference them. Asynchronous memcpy calls require pinned memory
    ranges to ensure that the operating system does not unmap or move the physical
    memory before the memcpy is performed.
  prefs: []
  type: TYPE_NORMAL
- en: Not all CUDA applications can allocate the host memory they wish to process
    using CUDA. For example, a CUDA-aware plugin to a large, extensible application
    may want to operate on host memory that was allocated by non-CUDA-aware code.
    To accommodate that use case, CUDA 4.0 added the ability to *register* existing
    host address ranges, which page-locks a virtual address range, maps it for the
    GPU, and adds the address range to the tracking data structure so CUDA knows it
    is pinned. The memory then can be passed to asynchronous memcpy calls or otherwise
    treated as if it were allocated by CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.3\. Mapped Pinned Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA 2.2 added a feature called *mapped pinned memory*, shown in [Figure 2.17](ch02.html#ch02fig17).
    Mapped pinned memory is page-locked host memory that has been mapped into the
    CUDA address space, where CUDA kernels can read or write it directly. The page
    tables of both the CPU and the GPU are updated so that both the CPU and the GPU
    have address ranges that point to the same host memory buffer. Since the address
    spaces are different, the GPU pointer(s) to the buffer must be queried using `cuMemHostGetDevicePointer()/cudaHostGetDevicePointer()`.^([9](ch02.html#ch02fn9))
  prefs: []
  type: TYPE_NORMAL
- en: '[9](ch02.html#ch02fn9a). For multi-GPU configurations, CUDA 2.2 also added
    a feature called “portable” pinned memory that causes the allocation to be mapped
    into every GPU’s address space. But there is no guarantee that `cu(da)HostGetDevicePointer()`
    will return the same value for different GPUs!'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.17* Mapped pinned memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.4\. Portable Pinned Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA 2.2 also enabled a feature called *portable pinned memory*, shown in [Figure
    2.18](ch02.html#ch02fig18). Making pinned memory “portable” causes the CUDA driver
    to map it for *all* GPUs in the system, not just the one whose context is current.
    A separate set of page table entries is created for the CPU and every GPU in the
    system, enabling the corresponding device to translate virtual addresses to the
    underlying physical memory. The host memory range also is added to every active
    CUDA context’s tracking mechanism, so every GPU will recognize the portable allocation
    as pinned.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.18* Portable, mapped pinned memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2.18](ch02.html#ch02fig18) likely represents the limit of developer
    tolerance for multiple address spaces. Here, a 2-GPU system has 3 addresses for
    an allocation; a 4-GPU system would have 5 addresses. Although CUDA has fast APIs
    to look up a given CPU address range and pass back the corresponding GPU address
    range, having N+1 addresses on an N-GPU system, all for the same allocation, is
    inconvenient to say the least.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.5\. Unified Addressing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multiple address spaces are required for 32-bit CUDA GPUs, which can only map
    2^(32)=4GiB of address space; since some high-end GPUs have up to 4GiB of device
    memory, they are hard-pressed to address all of device memory and also map any
    pinned memory, let alone use the same address space as the CPU. But on 64-bit
    operating systems with Fermi or later GPUs, a simpler abstraction is possible.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA 4.0 added a feature called *unified virtual addressing* (UVA), shown in
    [Figure 2.19](ch02.html#ch02fig19). When UVA is in force, CUDA allocates memory
    for both CPUs and GPUs from the same virtual address space. The CUDA driver accomplishes
    this by having its initialization routine perform large virtual allocations from
    the CPU address space—allocations that are not backed by physical memory—and then
    mapping GPU allocations into those address ranges. Since x64 CPUs support 48-bit
    virtual address spaces,^([10](ch02.html#ch02fn10)) while CUDA GPUs only support
    40 bits, applications using UVA should make sure CUDA gets initialized early to
    guard against CPU code using virtual address needed by CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: '[10](ch02.html#ch02fn10a). 48 bits of virtual address space = 256 terabytes.
    Future x64 CPUs will support even larger address spaces.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.19* Unified virtual addressing (UVA).'
  prefs: []
  type: TYPE_NORMAL
- en: For mapped pinned allocations, the GPU and CPU pointers are the same. For other
    types of allocation, CUDA can infer the device for which a given allocation was
    performed from the address. As a result, the family of linear memcpy functions
    (`cudaMemcpy()` with a direction specified, `cuMemcpyHtoD()`, `cuMemcpyDtoH()`,
    etc.) have been replaced by simplified `cuMemcpy()` and `cudaMemcpy()` functions
    that do not take a memory direction.
  prefs: []
  type: TYPE_NORMAL
- en: UVA is enabled automatically on UVA-capable systems. At the time of this writing,
    UVA is enabled on 64-bit Linux, 64-bit MacOS, and 64-bit Windows when using the
    TCC driver; the WDDM driver does not yet support UVA. When UVA is in effect, all
    pinned allocations performed by CUDA are both mapped and portable. Note that for
    system memory that has been pinned using `cuMemRegisterHost()`, the device pointers
    still must be queried using `cu(da)HostGetDevicePointer()`. Even when UVA is in
    effect, the CPU(s) cannot access device memory. In addition, by default, the GPU(s)
    cannot access one another’s memory.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.6\. Peer-to-Peer Mappings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the final stage of our journey through CUDA’s virtual memory abstractions,
    we discuss peer-to-peer mapping of device memory, shown in [Figure 2.20](ch02.html#ch02fig20).
    Peer-to-peer enables a Fermi-class GPU to read or write memory that resides in
    another Fermi-class GPU. Peer-to-peer mapping is supported only on UVA-enabled
    platforms, and it only works on GPUs that are connected to the same I/O hub. Because
    UVA is always in force when using peer-to-peer, the address ranges for different
    devices do not overlap, and the driver (and runtime) can infer the owning device
    from a pointer value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.20* Peer-to-peer.'
  prefs: []
  type: TYPE_NORMAL
- en: Peer-to-peer memory addressing is asymmetric; note that [Figure 2.20](ch02.html#ch02fig20)
    shows an asymmetric mapping in which GPU 1’s allocations are visible to GPU 0,
    but not vice versa. In order for GPUs to see each other’s memory, each GPU must
    explicitly map the other’s memory. The API functions to manage peer-to-peer mappings
    are discussed in [Section 9.2](ch09.html#ch09lev1sec2).
  prefs: []
  type: TYPE_NORMAL
- en: 2.5\. CPU/GPU Interactions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section describes key elements of CPU-GPU interactions.
  prefs: []
  type: TYPE_NORMAL
- en: • *Pinned host memory:* CPU memory that the GPU can directly access
  prefs: []
  type: TYPE_NORMAL
- en: • *Command buffers:* the buffers written by the CUDA driver and read by the
    GPU to control its execution
  prefs: []
  type: TYPE_NORMAL
- en: • *CPU/GPU synchronization:* how the GPU’s progress is tracked by the CPU
  prefs: []
  type: TYPE_NORMAL
- en: This section describes these facilities at the hardware level, citing APIs only
    as necessary to help the reader understand how they pertain to CUDA development.
    For simplicity, this section uses the CPU/GPU model in [Figure 2.1](ch02.html#ch02fig01),
    setting aside the complexities of multi-CPU or multi-GPU programming.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1\. Pinned Host Memory and Command Buffers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For obvious reasons, the CPU and GPU are each best at accessing its own memory,
    but the GPU can directly access page-locked CPU memory via direct memory access
    (DMA). Page-locking is a facility used by operating systems to enable hardware
    peripherals to directly access CPU memory, avoiding extraneous copies. The “locked”
    pages have been marked as ineligible for eviction by the operating system, so
    device drivers can program these peripherals to use the pages’ physical addresses
    to access the memory directly. The CPU still can access the memory in question,
    but the memory cannot be moved or paged out to disk.
  prefs: []
  type: TYPE_NORMAL
- en: Since the GPU is a distinct device from the CPU, direct memory access also enables
    the GPU to read and write CPU memory independently of, and in parallel with, the
    CPU’s execution. Care must be taken to synchronize between the CPU and GPU to
    avoid race conditions, but for applications that can make productive use of CPU
    clock cycles while the GPU is processing, the performance benefits of concurrent
    execution can be significant.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2.21](ch02.html#ch02fig21) depicts a “pinned” buffer that has been
    mapped by the GPU^([11](ch02.html#ch02fn11)) for direct access. CUDA programmers
    are familiar with pinned buffers because CUDA has always given them the ability
    to allocate pinned memory via APIs such as `cudaMallocHost()`. But under the hood,
    one of the main applications for such buffers is to submit commands to the GPU.
    The CPU writes commands into a “command buffer” that the GPU can consume, and
    the GPU simultaneously reads and executes previously written commands. [Figure
    2.22](ch02.html#ch02fig22) shows how the CPU and GPU share this buffer. This diagram
    is simplified because the commands may be hundreds of bytes long, and the buffer
    is big enough to hold several thousand such commands. The “leading edge” of the
    buffer is under construction by the CPU and not yet ready to be read by the GPU.
    The “trailing edge” of the buffer is being read by the GPU. The commands in between
    are ready for the GPU to process when it is ready.'
  prefs: []
  type: TYPE_NORMAL
- en: '[11](ch02.html#ch02fn11a). Important note: In this context, “mapping” for the
    GPU involves setting up hardware tables that refer to the CPU memory’s physical
    addresses. The memory may or may not be mapped into the address space where it
    can be accessed by CUDA kernels.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.21* Pinned buffer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.22* CPU/GPU command buffer.'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the CUDA driver will reuse command buffer memory because once the
    GPU has finished processing a command, the memory becomes eligible to be written
    again by the CPU. [Figure 2.23](ch02.html#ch02fig23) shows how the CPU can “wrap
    around” the command buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.23* Command buffer wrap-around.'
  prefs: []
  type: TYPE_NORMAL
- en: Since it takes several thousand CPU clock cycles to launch a CUDA kernel, a
    key use case for CPU/GPU concurrency is simply to prepare more GPU commands while
    the GPU is processing. Applications that are not balanced to keep both the CPU
    and GPU busy may become “CPU bound” or “GPU bound,” as shown in [Figures 2.24](ch02.html#ch02fig24)
    and [2.25](ch02.html#ch02fig25), respectively. In a CPU-bound application, the
    GPU is poised and ready to process the next command as soon as it becomes available;
    in a GPU-bound application, the CPU has completely filled the command buffer and
    must wait for the GPU before writing the next GPU command. Some applications are
    intrinsically CPU-bound or GPU-bound, so CPU- and GPU-boundedness does not necessarily
    indicate a fundamental problem with an application’s structure. Nevertheless,
    knowing whether an application is CPU-bound or GPU-bound can help highlight performance
    opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.24* GPU-bound application.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.25* CPU-bound application.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2\. CPU/GPU Concurrency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The previous section introduced the coarsest-grained parallelism available
    in CUDA systems: *CPU/GPU concurrency*. All launches of CUDA kernels are asynchronous:
    the CPU requests the launch by writing commands into the command buffer, then
    returns without checking the GPU’s progress. Memory copies optionally also may
    be asynchronous, enabling CPU/GPU concurrency and possibly enabling memory copies
    to be done concurrently with kernel processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl’s Law
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When CUDA programs are written correctly, the CPU and GPU can fully operate
    in parallel, potentially doubling performance. CPU- or GPU-bound programs do not
    benefit much from CPU/GPU concurrency because the CPU or GPU will limit performance
    even if the other device is operating in parallel. This vague observation can
    be concretely characterized using *Amdahl’s Law*, first articulated in a paper
    by Gene Amdahl in 1967.^([12](ch02.html#ch02fn12)) Amdahl’s Law is often summarized
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[12](ch02.html#ch02fn12a). [http://bit.ly/13UqBm0](http://bit.ly/13UqBm0)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/036equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where r[s] + r[p] = 1 and r[s] represents the ratio of the sequential portion.
    This formulation seems awkward when examining small-scale performance opportunities
    such as CPU/GPU concurrency. Rearranging the equation as follows
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/036equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: clearly shows that the speedup is *N*x if r[p] = 1\. If there is one CPU and
    one GPU (*N* = 2), the maximum speedup from full concurrency is 2x; this is almost
    achievable for balanced workloads such as video transcoding, where the CPU can
    perform serial operations (such as variable-length decoding) in parallel with
    the GPU’s performing parallel operations (such as pixel processing). But for more
    CPU- or GPU-bound applications, this type of concurrency offers limited benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Amdahl’s paper was intended as a cautionary tale for those who believed that
    parallelism would be a panacea for performance problems, and we use it elsewhere
    in this book when discussing intra-GPU concurrency, multi-GPU concurrency, and
    the speedups achievable from porting to CUDA kernels. It can be empowering, though,
    to know which forms of concurrency will not confer any benefit to a given application,
    so developers can spend their time exploring other avenues for increased performance.
  prefs: []
  type: TYPE_NORMAL
- en: Error Handling
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'CPU/GPU concurrency also has implications for error handling. If the CPU launches
    a dozen kernels and one of them causes a memory fault, the CPU cannot discover
    the fault until it has performed CPU/GPU synchronization (described in the next
    section). Developers can manually perform CPU/GPU synchronization by calling `cudaThreadSynchronize()`
    or `cuCtxSynchronize()`, and other functions such as `cudaFree()` or `cuMemFree()`
    may cause CPU/GPU synchronization to occur as a side effect. The *CUDA C Programming
    Guide* references this behavior by calling out functions that may cause CPU/GPU
    synchronization: “Note that this function may also return error codes from previous,
    asynchronous launches.”'
  prefs: []
  type: TYPE_NORMAL
- en: As CUDA is currently implemented, if a fault does occur, there is no way to
    know which kernel caused the fault. For debug code, if it’s difficult to isolate
    faults with synchronization, developers can set the `CUDA_LAUNCH_BLOCKING` environment
    variable to force all launches to be synchronous.
  prefs: []
  type: TYPE_NORMAL
- en: CPU/GPU Synchronization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Although most GPU commands used by CUDA involve performing memory copies or
    kernel launches, an important subclass of commands helps the CUDA driver track
    the GPU’s progress in processing the command buffer. Because the application cannot
    know how long a given CUDA kernel may run, the GPU itself must report progress
    to the CPU. [Figure 2.26](ch02.html#ch02fig26) shows both the command buffer and
    the “sync location” (which also resides in pinned host memory) used by the driver
    and GPU to track progress. A monotonically increasing integer value (the “progress
    value”) is maintained by the driver, and every major GPU operation is followed
    by a command to write the new progress value to the shared sync location. In the
    case of [Figure 2.26](ch02.html#ch02fig26), the progress value is 3 until the
    GPU finishes executing the command and writes the value 4 to the sync location.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.26* Shared sync value—before.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA exposes these hardware capabilities both implicitly and explicitly. Context-wide
    synchronization calls such as `cuCtxSynchronize()` or `cudaThreadSynchronize()`
    simply examine the last sync value requested of the GPU and wait until the sync
    location attains that value. For example, if the command 8 being written by the
    CPU in [Figure 2.27](ch02.html#ch02fig27) were followed by `cuCtxSynchronize()`
    or `cudaThreadSynchronize()`, the driver would wait until the shared sync value
    became greater than or equal to 8.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.27* Shared sync value—after.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA events expose these hardware capabilities more explicitly. `cuEvent-Record()`
    enqueues a command to write a new sync value to a shared sync location, and `cuEventQuery()`
    and `cuEventSynchronize()` examine and wait on the event’s sync value, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Early versions of CUDA simply polled shared sync locations, repeatedly reading
    the memory until the wait criterion had been achieved, but this approach is expensive
    and only works well when the application doesn’t have to wait long (i.e., the
    sync location doesn’t have to be read many times before exiting because the wait
    criterion has been satisfied). For most applications, interrupt-based schemes
    (exposed by CUDA as “blocking syncs”) are better because they enable the CPU to
    suspend the waiting thread until the GPU signals an interrupt. The driver maps
    the GPU interrupt to a platform-specific thread synchronization primitive, such
    as Win32 events or Linux signals, that can be used to suspend the CPU thread if
    the wait condition is not true when the application starts to wait.
  prefs: []
  type: TYPE_NORMAL
- en: Applications can force the context-wide synchronization to be blocking by specifying
    `CU_CTX_BLOCKING_SYNC` to `cuCtxCreate()` or `cudaDeviceBlockingSync` to `cudaSetDeviceFlags()`.
    It is preferable, however, to use blocking CUDA events (specify `CU_EVENT_BLOCKING_SYNC`
    to `cuEventCreate()` or `cudaEventBlockingSync` to `cudaEventCreate()`), since
    they are more fine-grained and interoperate seamlessly with any type of CUDA context.
  prefs: []
  type: TYPE_NORMAL
- en: Astute readers may be concerned that the CPU and GPU read and write this shared
    memory location without using atomic operations or other synchronization primitives.
    But since the CPU only reads the shared location, race conditions are not a concern.
    The worst that can happen is the CPU reads a “stale” value that causes it to wait
    a little longer than it would otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Events and Timestamps
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The host interface has an onboard high-resolution timer, and it can write a
    timestamp at the same time it writes a 32-bit sync value. CUDA uses this hardware
    facility to implement the asynchronous timing features in CUDA events.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.3\. The Host Interface and Intra-GPU Synchronization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The GPU may contain multiple engines to enable concurrent kernel processing
    and memory copying. In this case, the driver will write commands that are dispatched
    to different engines that run concurrently. Each engine has its own command buffer
    and shared sync value, and the engine’s progress is tracked as described in [Figures
    2.26](ch02.html#ch02fig26) and [2.27](ch02.html#ch02fig27). [Figure 2.28](ch02.html#ch02fig28)
    shows this situation, with two copy engines and a compute engine operating in
    parallel. The host interface is responsible for reading the commands and dispatching
    them to the appropriate engine. In [Figure 2.28](ch02.html#ch02fig28), a host→device
    memcpy and two dependent operations—a kernel launch and a device→host memcpy—have
    been submitted to the hardware. In terms of CUDA programming abstractions, these
    operations are within the same stream. The stream is like a CPU thread, and the
    kernel launch was submitted to the stream after the memcpy, so the CUDA driver
    must insert GPU commands for intra-GPU synchronization into the command streams
    for the host interface.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.28* Intra-GPU synchronization.'
  prefs: []
  type: TYPE_NORMAL
- en: As [Figure 2.28](ch02.html#ch02fig28) shows, the host interface plays a central
    role in coordinating the needed synchronization for streams. When, for example,
    a kernel must not be launched until a needed memcpy is completed, the DMA unit
    can stop giving commands to a given engine until a shared sync location attains
    a certain value. This operation is similar to CPU/GPU synchronization, but the
    GPU is synchronizing different engines within itself.
  prefs: []
  type: TYPE_NORMAL
- en: The software abstraction layered on this hardware mechanism is a CUDA stream.
    CUDA streams are like CPU threads in that operations within a stream are sequential
    and multiple streams are needed for concurrency. Because the command buffer is
    shared between engines, applications must “software-pipeline” their requests in
    different streams. So instead of
  prefs: []
  type: TYPE_NORMAL
- en: foreach stream
  prefs: []
  type: TYPE_NORMAL
- en: Memcpy device←host
  prefs: []
  type: TYPE_NORMAL
- en: Launch kernel
  prefs: []
  type: TYPE_NORMAL
- en: Memcpy host←device
  prefs: []
  type: TYPE_NORMAL
- en: they must implement
  prefs: []
  type: TYPE_NORMAL
- en: foreach stream
  prefs: []
  type: TYPE_NORMAL
- en: Memcpy device←host
  prefs: []
  type: TYPE_NORMAL
- en: foreach stream
  prefs: []
  type: TYPE_NORMAL
- en: Launch kernel
  prefs: []
  type: TYPE_NORMAL
- en: foreach stream
  prefs: []
  type: TYPE_NORMAL
- en: Memcpy host←device
  prefs: []
  type: TYPE_NORMAL
- en: Without the software pipelining, the DMA engine will “break concurrency” by
    synchronizing the engines to preserve each stream’s model of sequential execution.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple DMA Engines on Kepler
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The latest Kepler-class hardware from NVIDIA implements a DMA unit per engine,
    obviating the need for applications to software-pipeline their streamed operations.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.4\. Inter-GPU Synchronization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since the sync location in [Figures 2.26](ch02.html#ch02fig26) through [2.28](ch02.html#ch02fig28)
    is in host memory, it can be accessed by any of the GPUs in the system. As a result,
    in CUDA 4.0, NVIDIA was able to add inter-GPU synchronization in the form of `cudaStreamWaitEvent()`
    and `cuStreamWaitEvent()`. These API calls cause the driver to insert wait commands
    for the host interface into the current GPU’s command buffer, causing the GPU
    to wait until the given event’s sync value has been written. Starting with CUDA
    4.0, the event may or may not be signaled by the same GPU that is doing the wait.
    Streams have been promoted from being able to synchronize execution between hardware
    units on a single GPU to being able to synchronize execution between GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6\. GPU Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Three distinct GPU architectures can run CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: • Tesla hardware debuted in 2006, in the GeForce 8800 GTX (G80).
  prefs: []
  type: TYPE_NORMAL
- en: • Fermi hardware debuted in 2010, in the GeForce GTX 480 (GF100).
  prefs: []
  type: TYPE_NORMAL
- en: • Kepler hardware debuted in 2012, in the GeForce GTX 680 (GK104).
  prefs: []
  type: TYPE_NORMAL
- en: The GF100/GK104 nomenclature refers to the ASIC that implements the GPU. The
    “K” and “F” in GK104 and GF100 refer to Kepler and Fermi, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Tesla and Fermi families followed an NVIDIA tradition in which they would
    first ship the huge, high-end flagship chip that would win benchmarks. These chips
    were expensive because NVIDIA’s manufacturing costs are closely related to the
    number of transistors (and thus the amount of die area) required to build the
    ASIC. The first large “win” chips would then be followed by smaller chips: half-size
    for the mid-range, quarter-size for the low end, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: In a departure from that tradition, NVIDIA’s first Kepler-class chip is targeted
    at the midrange; their “win” chip shipped months after the first Kepler-class
    chips became available. GK104 has 3.5B transistors, while GK110 has 7.1B transistors.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.1\. Overview
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA’s simplified view of the GPU includes the following.
  prefs: []
  type: TYPE_NORMAL
- en: • A host interface that connects the GPU to the PCI Express bus
  prefs: []
  type: TYPE_NORMAL
- en: • 0 to 2 copy engines
  prefs: []
  type: TYPE_NORMAL
- en: • A DRAM interface that connects the GPU to its device memory
  prefs: []
  type: TYPE_NORMAL
- en: • Some number of TPCs or GPCs (texture processing clusters or graphics processing
    clusters), each of which contains caches and some number of streaming multiprocessors
    (SMs)
  prefs: []
  type: TYPE_NORMAL
- en: The architectural papers cited at the end of this chapter give the full story
    on GPU functionality in CUDA-capable GPUs, including graphics-specific functionality
    like antialiased rendering support.
  prefs: []
  type: TYPE_NORMAL
- en: Host Interface
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The host interface implements the functionality described in the previous section.
    It reads GPU commands (such as memcpy and kernel launch commands) and dispatches
    them to the appropriate hardware units, and it also implements the facilities
    for synchronization between the CPU and GPU, between different engines on the
    GPU, and between different GPUs. In CUDA, the host interface’s functionality primarily
    is exposed via the Stream and Event APIs (see [Chapter 6](ch06.html#ch06)).
  prefs: []
  type: TYPE_NORMAL
- en: Copy Engine(s)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Copy engines can perform host↔device memory transfers while the SMs are doing
    computations. The earliest CUDA hardware did not have any copy engines; subsequent
    versions of the hardware included a copy engine that could transfer linear device
    memory (but not CUDA arrays), and the most recent CUDA hardware includes up to
    two copy engines that can convert between CUDA arrays and linear memory while
    saturating the PCI Express bus.^([13](ch02.html#ch02fn13))
  prefs: []
  type: TYPE_NORMAL
- en: '[13](ch02.html#ch02fn13a). More than two copy engines doesn’t really make sense,
    since each engine can saturate one of the two directions of PCI Express.'
  prefs: []
  type: TYPE_NORMAL
- en: DRAM Interface
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The GPU-wide DRAM interface, which supports bandwidths in excess of 100 GB/s,
    includes hardware to coalesce memory requests. More recent CUDA hardware has more
    sophisticated DRAM interfaces. The earliest (SM 1.x) hardware had onerous coalescing
    requirements, requiring addresses to be contiguous and 64-, 128-, or 256-byte
    aligned (depending on the operand size). Starting with SM 1.2 (the GT200 or GeForce
    GTX 280), addresses could be coalesced based on locality, regardless of address
    alignment. Fermi-class hardware (SM 2.0 and higher) has a write-through L2 cache
    that provides the benefits of the SM 1.2 coalescing hardware and additionally
    improves performance when data is reused.
  prefs: []
  type: TYPE_NORMAL
- en: TPCs and GPCs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: TPCs and GPCs are units of hardware that exist between the full GPU and the
    streaming multiprocessors that perform CUDA computation. Tesla-class hardware
    groups the SMs in “TPCs” (texture processing clusters) that contain texturing
    hardware support (in particular, a texture cache) and two or three streaming multiprocessors,
    described below. Fermi-class hardware groups the SMs in “GPCs” (graphics processing
    clusters) that each contain a raster unit and four SMs.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, CUDA developers need not concern themselves with TPCs or
    GPCs because streaming multiprocessors are the most important unit of abstraction
    for computational hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Contrasting Tesla and Fermi
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The first generation of CUDA-capable GPUs was code-named Tesla, and the second,
    Fermi. These were confidential code names during development, but NVIDIA decided
    to use them as external product names to describe the first two generations of
    CUDA-capable GPU. To add to the confusion, NVIDIA chose the name “Tesla” to describe
    the server-class boards used to build compute clusters out of CUDA machines.^([14](ch02.html#ch02fn14))
    To distinguish between the expensive server-class Tesla boards and the architectural
    families, this book refers to the architectural families as “Tesla-class hardware,”
    “Fermi-class hardware,” and “Kepler-class hardware.”
  prefs: []
  type: TYPE_NORMAL
- en: '[14](ch02.html#ch02fn14a). Of course, when the Tesla brand name was chosen,
    Fermi-class hardware did not exist. The marketing department told us engineers
    that it was just a coincidence that the architectural codename and the brand name
    were both “Tesla”!'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: All of the differences between Tesla-class hardware and Fermi-class hardware
    also apply to Kepler.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Early Tesla-class hardware is subject to onerous performance penalties (up to
    6x) when running code that performs uncoalesced memory transactions. Later implementations
    of Tesla-class hardware, starting with the GeForce GTX 280, decreased the penalty
    for uncoalesced transactions to about 2x. Tesla-class hardware also has performance
    counters that enable developers to measure how many memory transactions are uncoalesced.
  prefs: []
  type: TYPE_NORMAL
- en: Tesla-class hardware only included a 24-bit integer multiplier, so developers
    must use intrinsics such as `__mul24()` for best performance. Full 32-bit multiplication
    (i.e., the native operator * in CUDA) is emulated with a small instruction sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Tesla-class hardware initialized shared memory to zero, while Fermi-class hardware
    leaves it uninitialized. For applications using the driver API, one subtle side
    effect of this behavior change is that applications that used `cuParamSeti()`
    to pass pointer parameters on 64-bit platforms do not work correctly on Fermi.
    Since parameters were passed in shared memory on Tesla class hardware, the uninitialized
    top half of the parameter would become the most significant 32 bits of the 64-bit
    pointer.
  prefs: []
  type: TYPE_NORMAL
- en: Double-precision support was introduced with SM 1.3 on the GT200, the second-generation
    “win” chip of the Tesla family.^([15](ch02.html#ch02fn15)) At the time, the feature
    was considered speculative, so it was implemented in an area-efficient manner
    that could be added and subtracted from the hardware with whatever ratio of double-to-single
    performance NVIDIA desired (in the case of GT200, this ratio was 1:8). Fermi integrated
    double-precision support much more tightly and at higher performance.^([16](ch02.html#ch02fn16))
    Finally, for graphics applications, Tesla-class hardware was the first DirectX
    10-capable hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '[15](ch02.html#ch02fn15a). In fact, the only difference between SM 1.2 and
    SM 1.3 is that SM 1.3 supports double precision.'
  prefs: []
  type: TYPE_NORMAL
- en: '[16](ch02.html#ch02fn16a). In SM 3.x, NVIDIA has decoupled double precision
    floating-point performance from the SM version, so GK104 has poor double precision
    performance and GK110 has excellent double precision performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Fermi-class hardware is much more capable than Tesla-class hardware. It supports
    64-bit addressing; it added L1 and L2 cache hardware; it added a full 32-bit integer
    multiply instruction and new instructions specifically to support the Scan primitive;
    it added surface load/store operations so CUDA kernels could read and write CUDA
    arrays without using the texture hardware; it was the first family of GPUs to
    feature multiple copy engines; and it improved support for C++ code, such as virtual
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Fermi-class hardware does not include the performance counters needed to track
    uncoalesced memory transactions. Also, because it does not include a 24-bit multiplier,
    Fermi-class hardware may incur a small performance penalty when running code that
    uses the 24-bit multiplication intrinsics. On Fermi, using operator * for multiplication
    is the fast path.
  prefs: []
  type: TYPE_NORMAL
- en: For graphics applications, Fermi-class hardware can run DirectX 11\. [Table
    2.1](ch02.html#ch02tab01) summarizes the differences between Tesla- and Fermi-class
    hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 2.1* Differences between Tesla- and Fermi-Class Hardware'
  prefs: []
  type: TYPE_NORMAL
- en: Texturing Niceties
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: A subtle difference between Tesla- and Fermi-class hardware is that on Tesla-class
    hardware, the instructions to perform texturing overwrite the input register vector
    with the output. On Fermi-class hardware, the input and output register vectors
    can be different. As a result, Tesla-class hardware may have extra instructions
    to move the texture coordinates into the input registers where they will be overwritten.
  prefs: []
  type: TYPE_NORMAL
- en: Another subtle difference between Tesla- and Fermi-class hardware is that when
    texturing from 1D CUDA arrays, Fermi-class hardware emulates this functionality
    using 2D textures with the second coordinate always set to 0.0\. Since this emulation
    only costs an extra register and very few extra instructions, the difference will
    be noticed by very few applications.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6.2\. Streaming Multiprocessors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The workhorse of the GPU is the streaming multiprocessor, or SM. As mentioned
    in the previous section, each TPC in SM 1.x hardware contains 2 or 3 SMs, and
    each GPC in SM 2.x hardware contains 4 SMs. The very first CUDA-capable GPU, the
    G80 or GeForce 8800 GTX, contained 8 TPCs; at 2 SMs per TPC, that is a total of
    16 SMs. The next big CUDA-capable GPU, the GT200 or GeForce GTX 280, increased
    the number of SMs/TPC to 3 and contained 10 TPCs, for a total of 30 SMs.
  prefs: []
  type: TYPE_NORMAL
- en: The number of SMs in a CUDA GPU may range from 2 to several dozen, and each
    SM contains
  prefs: []
  type: TYPE_NORMAL
- en: • Execution units to perform 32-bit integer and single- and double-precision
    floating-point arithmetic
  prefs: []
  type: TYPE_NORMAL
- en: • Special function units (SFUs) to compute single-precision approximations of
    log/exp, sin/cos, and rcp/rsqrt
  prefs: []
  type: TYPE_NORMAL
- en: • A warp scheduler to coordinate instruction dispatch to the execution units
  prefs: []
  type: TYPE_NORMAL
- en: • A constant cache to broadcast data to the SMs
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory for data interchange between threads
  prefs: []
  type: TYPE_NORMAL
- en: • Dedicated hardware for texture mapping
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2.29](ch02.html#ch02fig29) shows a Tesla-class streaming multiprocessor
    (SM 1.x). It contains 8 streaming processors that support 32-bit integer and single-precision
    floating-point arithmetic. The first CUDA hardware did not support double precision
    at all, but starting with GT200, the SMs may include one double-precision floating-point
    unit.^([17](ch02.html#ch02fn17))'
  prefs: []
  type: TYPE_NORMAL
- en: '[17](ch02.html#ch02fn17a). GT200 added a few instructions as well as double
    precision (such as shared memory atomics), so the GT200 instruction set without
    double precision is SM 1.2 and with double precision is SM 1.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.29* Streaming multiprocessor 1.x.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2.30](ch02.html#ch02fig30) shows a Fermi-class streaming multiprocessor
    (SM 2.0). Unlike Tesla-class hardware, which implemented double-precision floating-point
    support separately, each Fermi-class SM has full double-precision support. The
    double-precision instructions execute slower than single precision, but since
    the ratio is more favorable than the 8:1 ratio of Tesla-class hardware, overall
    double-precision performance is much higher.^([18](ch02.html#ch02fn18))'
  prefs: []
  type: TYPE_NORMAL
- en: '[18](ch02.html#ch02fn18a). For Kepler-class hardware, NVIDIA can tune floating-point
    performance to the target market of the GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.30* SM 2.0 (Fermi).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2.31](ch02.html#ch02fig31) shows an updated Fermi-class streaming multiprocessor
    (SM 2.1) that may be found in, for example, the GF104 chip. For higher performance,
    NVIDIA chose to increase the number of streaming processors per SM to 48\. The
    SFU-to-SM ratio is increased from 1:8 to 1:6.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.31* SM 2.1 (Fermi).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2.32](ch02.html#ch02fig32) shows the most recent (as of this writing)
    streaming multiprocessor design, featured in the newest Kepler-class hardware
    from NVIDIA. This design is so different from previous generations that NVIDIA
    calls it “SMX” (next-generation SM). The number of cores is increased by a factor
    of 6 to 192, and each SMX is much larger than analogous SMs in previous-generation
    GPUs. The largest Fermi GPU, GF110, had about 3 billion transistors containing
    16 SMs; the GK104 has 3.5 billion transistors and much higher performance but
    only 8 SMX’s. For area savings and power efficiency reasons, NVIDIA greatly increased
    the resources per SM, with the conspicuous exception of the shared memory/L1 cache.
    Like Fermi’s SMs, each Kepler SMX has 64K of cache that can be partitioned as
    48K L1/16K shared or 48K shared/16K L1\. The main implication for CUDA developers
    is that on Kepler, developers have even more incentive to keep data in registers
    (as opposed to L1 cache or shared memory) than on previous architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/02fig32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 2.32* SM 3.0 (SMX).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.7\. Further Reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: NVIDIA has white papers on their Web site that describe the Fermi and Kepler
    architectures in detail. The following white paper describes Fermi.
  prefs: []
  type: TYPE_NORMAL
- en: The Next Generation of NVIDIA GeForce GPU [www.nvidia.com/object/GTX_400_architecture.html](http://www.nvidia.com/object/GTX_400_architecture.html)
  prefs: []
  type: TYPE_NORMAL
- en: The following white paper describes the Kepler architecture and its implementation
    in the NVIDIA GeForce GTX 680 (GK104).
  prefs: []
  type: TYPE_NORMAL
- en: '[www.geforce.com/Active/en_US/en_US/pdf/GeForce-GTX-680-Whitepaper-FINAL.pdf](http://www.geforce.com/Active/en_US/en_US/pdf/GeForce-GTX-680-Whitepaper-FINAL.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA engineers also have published several architectural papers that give
    more detailed descriptions of the various CUDA-capable GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lindholm, E., J. Nickolls, S. Oberman, and J. Montrym. NVIDIA Tesla: A unified
    graphics and computing architecture. *IEEE Micro* 28 (2), March–April 2008, pp.
    39–55.'
  prefs: []
  type: TYPE_NORMAL
- en: Wittenbrink, C., E. Kilgariff, and A. Prabhu. Fermi GF100 GPU architecture.
    *IEEE Micro* 31 (2), March–April 2011, pp. 50–59.
  prefs: []
  type: TYPE_NORMAL
- en: Wong et al. used CUDA to develop microbenchmarks and clarify some aspects of
    Tesla-class hardware architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Wong, H., M. Papadopoulou, M. Sadooghi-Alvandi, and A. Moshovos. Demystifying
    GPU microarchitecture through microbenchmarking. 2010 IEEE International Symposium
    on Performance Analysis of Systems and Software (IPSASS), March 28–30, 2010, pp.
    235–246.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 3\. Software Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter provides an overview of the CUDA software architecture. [Chapter
    2](ch02.html#ch02) gave an overview of the hardware platform and how it interacts
    with CUDA, and we’ll start this chapter with a description of the software platforms
    and operating environments supported by CUDA. Next, each software abstraction
    in CUDA is briefly described, from devices and contexts to modules and kernels
    to memory. This section may refer back to [Chapter 2](ch02.html#ch02) when describing
    how certain software abstractions are supported by the hardware. Finally, we spend
    some time contrasting the CUDA runtime and driver API and examining how CUDA source
    code is translated into microcode that operates on the GPU. Please remember that
    this chapter is just an overview. Most of the topics covered are described in
    more detail in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Software Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 3.1](ch03.html#ch03fig01) shows the different layers of software in
    a CUDA application, from the application itself to the CUDA driver that operates
    the GPU hardware. All of the software except the kernel mode driver operate in
    the target operating system’s unprivileged user mode. Under the security models
    of modern multitasking operating systems, user mode is “untrusted,” and the hardware
    and operating system software must take measures to strictly partition applications
    from one another. In the case of CUDA, that means host and device memory allocated
    by one CUDA program cannot be accessed by other CUDA programs. The only exceptions
    happen when these programs specifically request memory sharing, which must be
    provided by the kernel mode driver.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3.1* Software layers in CUDA.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA libraries, such as cuBLAS, are built on top of the CUDA runtime or driver
    API. The CUDA runtime is the library targeted by CUDA’s integrated C++/GPU toolchain.
    When the `nvcc` compiler splits `.cu` files into host and device portions, the
    host portion contains automatically generated calls to the CUDA runtime to facilitate
    operations such as the kernel launches invoked by `nvcc`’s special triple-angle
    bracket `<<< >>>` syntax.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA driver API, exported directly by CUDA’s user mode driver, is the lowest-level
    API available to CUDA apps. The driver API calls into the user mode driver, which
    may in turn call the kernel mode driver to perform operations such as memory allocation.
    Functions in the driver API and CUDA runtime generally start with `cu*()` and
    `cuda*()`, respectively. Many functions, such as `cudaEventElapsedTime(),` are
    essentially identical, with the only difference being in the prefix.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. CUDA Runtime and Driver
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CUDA runtime (often abbreviated CUDART) is the library used by the language
    integration features of CUDA. Each version of the CUDA toolchain has its own specific
    version of the CUDA runtime, and programs built with that toolchain will automatically
    link against the corresponding version of the runtime. A program will not run
    correctly unless the correct version of CUDART is available in the path.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA driver is designed to be backward compatible, supporting all programs
    written against its version of CUDA, or older ones. It exports a low-level “driver
    API” (in `cuda.h`) that enables developers to closely manage resources and the
    timing of initialization. The driver version may be queried by calling `cuDriverGetVersion().`
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuDriverGetVersion(int *driverVersion);
  prefs: []
  type: TYPE_NORMAL
- en: This function passes back a decimal value that gives the version of CUDA supported
    by the driver—for example, 3010 for CUDA 3.1 and 5000 for CUDA 5.0.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 3.1](ch03.html#ch03tab01) summarizes the features that correspond to
    the version number passed back by `cuDriverGetVersion()`. For CUDA runtime applications,
    this information is given by the `major` and `minor` members of the `cudaDeviceProp`
    structure as described in [Section 3.2.2](ch03.html#ch03lev2sec5).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.1* CUDA Driver Features'
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime requires that the installed driver have a version greater than
    or equal to the version of CUDA supported by the runtime. If the driver version
    is older than the runtime version, the CUDA application will fail to initialize
    with the error `cudaErrorInsufficientDriver` (35). CUDA 5.0 introduced the *device
    runtime*, a subset of the CUDA runtime that can be invoked from CUDA kernels.
    A detailed description of the device runtime is given in [Chapter 7](ch07.html#ch07).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Driver Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Other than Windows Vista and subsequent releases of Windows, all of the operating
    systems that CUDA runs—Linux, MacOS, and Windows XP—access the hardware with *user
    mode client drivers*. These drivers sidestep the requirement, common to all modern
    operating systems, that hardware resources be manipulated by kernel code. Modern
    hardware such as GPUs can finesse that requirement by mapping certain hardware
    registers—such as the hardware register used to submit work to the hardware—into
    user mode. Since user mode code is not trusted by the operating system, the hardware
    must contain protections against rogue writes to the user mode hardware registers.
    The goal is to prevent user mode code from prompting the hardware to use its direct
    memory access (DMA) facilities to read or write memory that it should not (such
    as the operating system’s kernel code!).
  prefs: []
  type: TYPE_NORMAL
- en: Hardware designers protect against memory corruption by introducing a level
    of indirection into the command stream available to user mode software so DMA
    operations can only be initiated on memory that previously was validated and mapped
    by kernel code; in turn, driver developers must carefully validate their kernel
    code to ensure that it only gives access to memory that should be made available.
    The end result is a driver that can operate at peak efficiency by submitting work
    to the hardware without having to incur the expense of a kernel transition.
  prefs: []
  type: TYPE_NORMAL
- en: Many operations, such as memory allocation, still require kernel mode transitions
    because editing the GPU’s page tables can only be done in kernel mode. In this
    case, the user mode driver may take steps to reduce the number of kernel mode
    transitions—for example, the CUDA memory allocator tries to satisfy memory allocation
    requests out of a pool.
  prefs: []
  type: TYPE_NORMAL
- en: Unified Virtual Addressing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Unified virtual addressing, described in detail in [Section 2.4.5](ch02.html#ch02lev2sec9),
    is available on 64-bit Linux, 64-bit XPDDM, and MacOS. On these platforms, it
    is made available transparently. As of this writing, UVA is not available on WDDM.
  prefs: []
  type: TYPE_NORMAL
- en: Windows Display Driver Model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For Windows Vista, Microsoft introduced a new desktop presentation model in
    which the screen output was composed in a back buffer and page-flipped, like a
    video game. The new “Windows Desktop Manager” (WDM) made more extensive use of
    GPUs than Windows had previously, so Microsoft decided it would be best to revise
    the GPU driver model in conjunction with the presentation model. The resulting
    Windows Display Driver Model (WDDM) is now the default driver model on Windows
    Vista and subsequent versions. The term XPDDM was created to refer to the driver
    model used for GPUs on previous versions of Windows.^([1](ch03.html#ch03fn1))
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch03.html#ch03fn1a). Tesla boards (CUDA-capable boards that do not have
    a display output) can use XPDDM on Windows, called the Tesla Compute Cluster (TCC)
    driver, and can be toggled with the `nvidia-smi` tool.'
  prefs: []
  type: TYPE_NORMAL
- en: As far as CUDA is concerned, these are the two major changes made by WDDM.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** WDDM does not permit hardware registers to be mapped into user mode.
    Hardware commands—even commands to kick off DMA operations—must be invoked by
    kernel code. The user→kernel transition is too expensive for the user mode driver
    to submit each command as it arrives, so instead the user mode driver buffers
    commands for later submission.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** Since WDDM was built to enable many applications to use a GPU concurrently,
    and GPUs do not support demand paging, WDDM includes facilities to emulate paging
    on a “memory object” basis. For graphics applications, memory objects may be render
    targets, Z buffers, or textures; for CUDA, memory objects include global memory
    and CUDA arrays. Since the driver must set up access to CUDA arrays before each
    kernel invocation, CUDA arrays can be swapped by WDDM. For global memory, which
    resides in a linear address space (where pointers can be stored), every memory
    object for a given CUDA context must be resident in order for a CUDA kernel to
    launch.'
  prefs: []
  type: TYPE_NORMAL
- en: The main effect of WDDM due to number 1 above is that work requested of CUDA,
    such as kernel launches or asynchronous memcpy operations, generally is not submitted
    to the hardware immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The accepted idiom to force pending work to be submitted is to query the NULL
    stream: `cudaStreamQuery(0)` or `cuStreamQuery(NULL)`. If there is no pending
    work, these calls will return quickly. If any work is pending, it will be submitted,
    and since the call is asynchronous, execution may be returned to the caller before
    the hardware has finished processing. On non-WDDM platforms, querying the NULL
    stream is always fast.'
  prefs: []
  type: TYPE_NORMAL
- en: The main effect of WDDM due to number 2 above is that CUDA’s control of memory
    allocation is much less concrete. On user mode client drivers, successful memory
    allocations mean that the memory has been allocated and is no longer available
    to any other operating system client (such as a game or other CUDA application
    that may be running). On WDDM, if there are applications competing for time on
    the same GPU, Windows can and will swap memory objects out in order to enable
    each application to run. The Windows operating system tries to make this as efficient
    as possible, but as with all paging, having it *never* happen is much faster than
    having it *ever* happen.
  prefs: []
  type: TYPE_NORMAL
- en: Timeout Detection and Recovery
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Because Windows uses the GPU to interact with users, it is important that compute
    applications not take inordinate amounts of GPU time. Under WDDM, Windows enforces
    a timeout (default of 2 seconds) that, if it should elapse, will cause a dialog
    box that says “Display driver stopped responding and has recovered,” and the display
    driver is restarted. If this happens, all work in the CUDA context is lost. See
    [http://bit.ly/16mG0dX](http://bit.ly/16mG0dX).
  prefs: []
  type: TYPE_NORMAL
- en: Tesla Compute Cluster Driver
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For compute applications that do not need WDDM, NVIDIA provides the Tesla Compute
    Cluster (TCC) driver, available only for Tesla-class boards. The TCC driver is
    a user mode client driver, so it does not require a kernel thunk to submit work
    to the hardware. The TCC driver may be enabled and disabled using the `nvidia-smi`
    tool.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3\. `nvcc`, PTX, and Microcode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '`nvcc` is the compiler driver used by CUDA developers to turn source code into
    functional CUDA applications. It can perform many functions, from as complex as
    compiling, linking, and executing a sample program in one command (a usage encouraged
    by many of the sample programs in this book) to a simple targeted compilation
    of a GPU-only `.cu` file.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3.2](ch03.html#ch03fig02) shows the two recommended workflows for using
    `nvcc` for CUDA runtime and driver API applications, respectively. For applications
    larger than the most trivial size, `nvcc` is best used strictly for purposes of
    compiling CUDA code and wrapping CUDA functionality into code that is callable
    from other tools. This is due to `nvcc`’s limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: • `nvcc` only works with a specific set of compilers. Many CUDA developers never
    notice because their compiler of choice happens to be in the set of supported
    compilers. But in production software development, the amount of CUDA code tends
    to be minuscule compared to the amount of other code, and the presence or absence
    of CUDA support may not be the dominant factor in deciding which compiler to use.
  prefs: []
  type: TYPE_NORMAL
- en: • `nvcc` makes changes to the compile environment that may not be compatible
    with the build environment for the bulk of the application.
  prefs: []
  type: TYPE_NORMAL
- en: • `nvcc` “pollutes” the namespace with nonstandard built-in types (e.g., `int2`)
    and intrinsic names (e.g., `__popc()`). Only in recent versions of CUDA have the
    intrinsics symbols become optional and can be used by including the appropriate
    `sm_*_intrinsics.h` header.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3.2* `nvcc` workflows.'
  prefs: []
  type: TYPE_NORMAL
- en: For CUDA runtime applications, `nvcc` embeds GPU code into string literals in
    the output executable. If the `--fatbin` option is specified, the executable will
    automatically load suitable microcode for the target GPU or, if no microcode is
    available, have the driver automatically compile the PTX into microcode.
  prefs: []
  type: TYPE_NORMAL
- en: '`nvcc` and PTX'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: PTX (“Parallel Thread eXecution”) is the intermediate representation of compiled
    GPU code that can be compiled into native GPU microcode. It is the mechanism that
    enables CUDA applications to be “future-proof” against instruction set innovations
    by NVIDIA—as long as the PTX for a given CUDA kernel is available, the CUDA driver
    can translate it into microcode for whichever GPU the application happens to be
    running on (even if the GPU was not available when the code was written).
  prefs: []
  type: TYPE_NORMAL
- en: PTX can be compiled into GPU microcode both “offline” and “online.” Offline
    compilation refers to building software that will be executed by some computer
    in the future. For example, [Figure 3.2](ch03.html#ch03fig02) highlights the offline
    portions of the CUDA compilation process. Online compilation, otherwise known
    as “just-in-time” compilation, refers to compiling intermediate code (such as
    PTX) for the computer running the application for immediate execution.
  prefs: []
  type: TYPE_NORMAL
- en: '`nvcc` can compile PTX offline by invoking the PTX assembler `ptxas`, which
    compiles PTX into the native microcode for a specific version of GPU. The resulting
    microcode is emitted into a CUDA binary called a “cubin” (pronounced like “Cuban”).
    Cubin files can be disassembled with `cuobjdump --dump-sass;` this will dump the
    SASS mnemonics for the GPU-specific microcode.^([2](ch03.html#ch03fn2))'
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch03.html#ch03fn2a). Examining the SASS code is a key strategy to help
    drive optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: PTX also can be compiled online (JITted) by the CUDA driver. Online compilation
    happens automatically when running CUDART applications that were built with the
    `--fatbin` option (which is the default). `.cubin` and PTX representations of
    every kernel are included in the executable, and if it is run on hardware that
    doesn’t support any of the .cubin representations, the driver compiles the PTX
    version. The driver caches these compiled kernels on disk, since compiling PTX
    can be time consuming.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, PTX can be generated at runtime and compiled explicitly by the driver
    by calling `cuModuleLoadEx()`. The driver API does not automate any of the embedding
    or loading of GPU microcode. Both `.cubin` and `.ptx` files can be given to `cuModuleLoadEx()`;
    if a `.cubin` is not suitable for the target GPU architecture, an error will be
    returned. A reasonable strategy for driver API developers is to compile and embed
    PTX, and they should always JIT-compile it onto the GPU with `cuModuleLoadEx()`,
    relying on the driver to cache the compiled microcode.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Devices and Initialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Devices correspond to physical GPUs. When CUDA is initialized (either explicitly
    by calling the driver API’s `cuInit()` function or implicitly by calling a CUDA
    runtime function), the CUDA driver enumerates the available devices and creates
    a global data structure that contains their names and immutable capabilities such
    as the amount of device memory and maximum clock rate.
  prefs: []
  type: TYPE_NORMAL
- en: For some platforms, NVIDIA includes a tool that can set policies with respect
    to specific devices. The `nvidia-smi` tool sets the policy with respect to a given
    GPU. For example, `nvidia-smi` can be used to enable and disable ECC (error correction)
    on a given GPU. `nvidia-smi` also can be used to control the number of CUDA contexts
    that can be created on a given device. These are the possible modes.
  prefs: []
  type: TYPE_NORMAL
- en: '• Default: Multiple CUDA contexts may be created on the device.'
  prefs: []
  type: TYPE_NORMAL
- en: '• “Exclusive” mode: One CUDA context may be created on the device.'
  prefs: []
  type: TYPE_NORMAL
- en: '• “Prohibited”: No CUDA context may be created on the device.'
  prefs: []
  type: TYPE_NORMAL
- en: If a device is enumerated but you are not able to create a context on that device,
    it is likely the device is in “prohibited” mode or in “exclusive” mode and another
    CUDA context already has been created on that device.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Device Count
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The application can discover how many CUDA devices are available by calling
    `cuDeviceGetCount()` or `cudaGetDeviceCount()`. Devices can then be referenced
    by an index in the range [0..*DeviceCount*-1]. The driver API requires applications
    to call `cuDeviceGet()` to map the device index to a device handle (`CUdevice`).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Device Attributes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Driver API applications can query the name of a device by calling `cuDeviceGetName()`
    and query the amount of global memory by calling `cuDevice-TotalMem()`. The major
    and minor compute capabilities of the device (i.e., the SM version, such as 2.0
    for the first Fermi-capable GPUs) can be queried by calling `cuDeviceComputeCapability(``)`.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA runtime applications can call `cudaGetDeviceProperties()`, which will pass
    back a structure containing the name and properties of the device. [Table 3.2](ch03.html#ch03tab02)
    gives the descriptions of the members of `cudaDeviceProp`, the structure passed
    back by `cudaGetDeviceProperties(``)`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab02.jpg)![Image](graphics/03tab02a.jpg)![Image](graphics/03tab02b.jpg)![Image](graphics/03tab02c.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.2.* `cudaDeviceProp` Members'
  prefs: []
  type: TYPE_NORMAL
- en: The driver API’s function for querying device attributes, `cuDeviceGetAttribute()`,
    can pass back one attribute at a time, depending on the `CUdevice_attribute` parameter.
    In CUDA 5.0, the CUDA runtime added the same function in the form of `cudaDeviceGetAttribute()`,
    presumably because the structure-based interface was too cumbersome to run on
    the device.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3\. When CUDA Is Not Present
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CUDA runtime can run on machines that cannot run CUDA or that do not have
    CUDA installed; if `cudaGetDeviceCount()` returns `cudaSuccess` and a nonzero
    device count, CUDA is available.
  prefs: []
  type: TYPE_NORMAL
- en: When using the driver API, executables that link directly against `nvcuda.dll`
    (Windows) or `libcuda.so` (Linux) will not load unless the driver binary is available.
    For driver API applications that require CUDA, trying to launch an application
    that was linked directly against the driver will result in an error such as in
    [Figure 3.3](ch03.html#ch03fig03).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 3.3* Error when CUDA is not present (Windows).'
  prefs: []
  type: TYPE_NORMAL
- en: For those applications that must run with or without CUDA, the CUDA SDK provides
    a set of header files and a C source file that wrap the driver API such that the
    application can check for CUDA without having the operating system signal an exception.
    These files, in the `dynlink` subdirectory `<SDKRoot>/C/common/inc/dynlink`, can
    be included in lieu of the core CUDA files. They interpose an intermediate set
    of functions that lazily load the CUDA libraries if CUDA is available.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s compare two programs that use the driver API to initialize
    CUDA and write the name of each device in the system. [Listing 3.1](ch03.html#ch03lis01)
    gives `init_hardcoded.cpp`, a file that can be compiled against the CUDA SDK with
    the following command line.
  prefs: []
  type: TYPE_NORMAL
- en: nvcc –oinit_hardcoded –I ../chLib init_hardcoded.cpp -lcuda
  prefs: []
  type: TYPE_NORMAL
- en: Using `nvcc` to compile a C++ file that doesn’t include any GPU code is just
    a convenient way to pick up the CUDA headers. The `–oinit_hardcoded` at the beginning
    specifies the root name of the output executable. The `-lcuda` at the end causes
    `nvcc` to link against the driver API’s library; without it, the build will fail
    with link errors. This program hard-links against the CUDA driver API, so it will
    fail on systems that don’t have CUDA installed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 3.1.* Initialization (hard-coded).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch03_images.html#p03lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: /*
  prefs: []
  type: TYPE_NORMAL
- en: '* init_hardcoded.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '*'
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  prefs: []
  type: TYPE_NORMAL
- en: '#include <stdio.h>'
  prefs: []
  type: TYPE_NORMAL
- en: '#include <cuda.h>'
  prefs: []
  type: TYPE_NORMAL
- en: '#include <chError.h>'
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: main()
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult status;
  prefs: []
  type: TYPE_NORMAL
- en: int numDevices;
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuInit( 0 ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGetCount( &numDevices ) );
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%d devices detected:\n", numDevices );
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < numDevices; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: char szName[256];
  prefs: []
  type: TYPE_NORMAL
- en: CUdevice device;
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGet( &device, i ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGetName( szName, 255, device ) );
  prefs: []
  type: TYPE_NORMAL
- en: printf( "\t%s\n", szName );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return 0;
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: 'fprintf( stderr, "CUDA failure code: 0x%x\n", status );'
  prefs: []
  type: TYPE_NORMAL
- en: return 1;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 3.2](ch03.html#ch03lis02) gives a program that will work on systems
    without CUDA. As you can see, the source code is identical except for a few lines
    of code.'
  prefs: []
  type: TYPE_NORMAL
- en: '#include <cuda.h>'
  prefs: []
  type: TYPE_NORMAL
- en: is replaced by
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch03_images.html#p065pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '#include "cuda_drvapi_dynlink.c"'
  prefs: []
  type: TYPE_NORMAL
- en: '#include "dynlink/cuda_drvapi_dynlink.h"'
  prefs: []
  type: TYPE_NORMAL
- en: and the `cuInit()` call has been changed to specify a CUDA version.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuInit(0) );
  prefs: []
  type: TYPE_NORMAL
- en: is replaced by
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuInit( 0, 4010 ) );
  prefs: []
  type: TYPE_NORMAL
- en: Here, passing `4010` as the second parameter requests CUDA 4.1, and the function
    will fail if the system doesn’t include that level of functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you could compile and link `cuda_drvapi_dynlink.c` into the application
    separately instead of `#include`’ing it into a single source file. The header
    file and C file work together to interpose a set of wrapper functions onto the
    driver API. The header uses the preprocessor to rename the driver API functions
    to wrapper functions declared in `cuda_drvapi_dynlink.h` (e.g., calls to `cuCtxCreate()`
    become calls to `tcuCtxCreate()`). On CUDA-capable systems, the driver DLL is
    loaded dynamically, and the wrapper functions call pointers-to-function that are
    obtained from the driver DLL during initialization. On non-CUDA-capable systems,
    or if the driver does not support the request CUDA version, the initialization
    function returns an error.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 3.2.* Initialization (dynlink).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch03_images.html#p03lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: /*
  prefs: []
  type: TYPE_NORMAL
- en: '* init_dynlink.cpp'
  prefs: []
  type: TYPE_NORMAL
- en: '*'
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  prefs: []
  type: TYPE_NORMAL
- en: '#include <stdio.h>'
  prefs: []
  type: TYPE_NORMAL
- en: '#include "dynlink/cuda_drvapi_dynlink.h"'
  prefs: []
  type: TYPE_NORMAL
- en: '#include <chError.h>'
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: main()
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult status;
  prefs: []
  type: TYPE_NORMAL
- en: int numDevices;
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuInit( 0, 4010 ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGetCount( &numDevices ) );
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%d devices detected:\n", numDevices );
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < numDevices; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: char szName[256];
  prefs: []
  type: TYPE_NORMAL
- en: CUdevice device;
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGet( &device, i ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuDeviceGetName( szName, 255, device ) );
  prefs: []
  type: TYPE_NORMAL
- en: printf( "\t%s\n", szName );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return 0;
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: 'fprintf( stderr, "CUDA failure code: 0x%x\n", status );'
  prefs: []
  type: TYPE_NORMAL
- en: return 1;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA-Only DLLs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For Windows developers, another way to build CUDA applications that can run
    on non-CUDA-capable systems is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** Move the CUDA-specific code into a DLL.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** Call `LoadLibrary()` explicitly to load the DLL.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3.** Enclose the `LoadLibrary()` call in a `__try/__except` clause to catch
    the exception if CUDA is not present.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Contexts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Contexts are analogous to processes on CPUs. With few exceptions, they are containers
    that manage the lifetimes of all other objects in CUDA, including the following.
  prefs: []
  type: TYPE_NORMAL
- en: • All memory allocations (including linear device memory, host memory, and CUDA
    arrays)
  prefs: []
  type: TYPE_NORMAL
- en: • Modules
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA streams
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA events
  prefs: []
  type: TYPE_NORMAL
- en: • Texture and surface references
  prefs: []
  type: TYPE_NORMAL
- en: • Device memory for kernels that use local memory
  prefs: []
  type: TYPE_NORMAL
- en: • Internal resources for debugging, profiling, and synchronization
  prefs: []
  type: TYPE_NORMAL
- en: • The pinned staging buffers used for pageable memcpy
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime does not provide direct access to CUDA contexts. It performs
    context creation through *deferred initialization.* Every CUDART library call
    or kernel invocation checks whether a CUDA context is current and, if necessary,
    creates a CUDA context (using the state previously set by calls such as `cudaSetDevice()`,
    `cudaSetDeviceFlags()`, `cudaGLSetGLDevice()`, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: Many applications prefer to explicitly control the timing of this deferred initialization.
    To force CUDART to initialize without any other side effects, call
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree(0);
  prefs: []
  type: TYPE_NORMAL
- en: CUDA runtime applications can access the current-context stack (described below)
    via the driver API.
  prefs: []
  type: TYPE_NORMAL
- en: For functions that specify per-context state in the driver API, the CUDA runtime
    conflates contexts and devices. Instead of `cuCtxSynchronize()`, the CUDA runtime
    has `cudaDeviceSynchronize()`; instead of `cuCtxSetCacheConfig()`, the CUDA runtime
    has `cudaDeviceSetCacheConfig()`.
  prefs: []
  type: TYPE_NORMAL
- en: Current Context
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instead of the current-context stack, the CUDA runtime provides the `cudaSetDevice()`
    function, which sets the current context for the calling thread. A device can
    be current to more than one CPU thread at a time.^([3](ch03.html#ch03fn3))
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch03.html#ch03fn3a). Early versions of CUDA prohibited contexts from being
    current to more than one thread at a time because the driver was not thread-safe.
    Now the driver implements the needed synchronization—even when applications call
    synchronous functions such as `cudaDeviceSynchronize()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Lifetime and Scoping
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All of the resources allocated in association with a CUDA context are destroyed
    when the context is destroyed. With few exceptions, the resources created for
    a given CUDA context may not be used with any other CUDA context. This restriction
    applies not only to memory but also to objects such as CUDA streams and CUDA events.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. Preallocation of Resources
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA tries to avoid “lazy allocation,” where resources are allocated as needed
    to avoid failing operations for lack of resources. For example, pageable memory
    copies cannot fail with an out-of-memory condition because the pinned staging
    buffers needed to perform pageable memory copies are allocated at context creation
    time. If CUDA is not able to allocate these buffers, the context creation fails.
  prefs: []
  type: TYPE_NORMAL
- en: There are some isolated cases where CUDA does not preallocate all the resources
    that it might need for a given operation. The amount of memory needed to hold
    local memory for a kernel launch can be prohibitive, so CUDA does not preallocate
    the maximum theoretical amount needed. As a result, a kernel launch may fail if
    it needs more local memory than the default allocated by CUDA for the context.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. Address Space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Besides objects that are automatically destroyed (“cleaned up”) when the context
    is destroyed, the key abstraction embodied in a context is its *address space*:
    the private set of virtual memory addresses that it can use to allocate linear
    device memory or to map pinned host memory. These addresses are unique per context.
    The same address for different contexts may or may not be valid and certainly
    will not resolve to the same memory location unless special provisions are made.
    The address space of a CUDA context is separate and distinct from the CPU address
    space used by CUDA host code. In fact, unlike shared-memory multi-CPU systems,
    CUDA contexts on multi-GPU configurations do not share an address space. When
    UVA (unified virtual addressing) is in effect, the CPU and GPU(s) share the same
    address space, in that any given allocation has a unique address within the process,
    but the CPUs and GPUs can only read or write each other’s memory under special
    circumstances, such as mapped pinned memory (see [Section 5.1.3](ch05.html#ch05lev2sec3))
    or peer-to-peer memory (see [Section 9.2.2](ch09.html#ch09lev2sec2)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.4\. Current Context Stack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most CUDA entry points do not take a context parameter. Instead, they operate
    on the “current context,” which is stored in a thread-local storage (TLS) handle
    in the CPU thread. In the driver API, each CPU thread has a stack of current contexts;
    creating a context pushes the new context onto the stack.
  prefs: []
  type: TYPE_NORMAL
- en: The current-context stack has three main applications.
  prefs: []
  type: TYPE_NORMAL
- en: • Single-threaded applications can drive multiple GPU contexts.
  prefs: []
  type: TYPE_NORMAL
- en: • Libraries can create and manage their own CUDA contexts without interfering
    with their callers’ CUDA contexts.
  prefs: []
  type: TYPE_NORMAL
- en: • Libraries can be agnostic with respect to which CPU thread calls into the
    CUDA-aware library.
  prefs: []
  type: TYPE_NORMAL
- en: The original motivation for the current-context stack to CUDA was to enable
    a single-threaded CUDA application to drive multiple CUDA contexts. After creating
    and initializing each CUDA context, the application can pop it off the current-context
    stack, making it a “floating” context. Since only one CUDA context at a time may
    be current to a CPU thread, a single-threaded CUDA application drives multiple
    contexts by pushing and popping the contexts in turn, keeping all but one of the
    contexts “floating” at any given time.
  prefs: []
  type: TYPE_NORMAL
- en: On most driver architectures, pushing and popping a CUDA context is inexpensive
    enough that a single-threaded application can keep multiple GPUs busy. On WDDM
    (Windows Display Driver Model) drivers, which run only on Windows Vista and later,
    popping the current context is only fast if there are no GPU commands pending.
    If there are commands pending, the driver will incur a kernel thunk to submit
    the commands before popping the CUDA context.^([4](ch03.html#ch03fn4))
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch03.html#ch03fn4a). This expense isn’t unique to the driver API or the
    current-context stack. Calling `cudaSetDevice()` to switch devices when commands
    are pending also will cause a kernel thunk on WDDM.'
  prefs: []
  type: TYPE_NORMAL
- en: Another benefit of the current-context stack is the ability to drive a given
    CUDA context from different CPU threads. Applications using the driver API can
    “migrate” a CUDA context to other CPU threads by popping the context with `cuCtxPopCurrent()`,
    then calling `cuCtxPushCurrent()` from another thread. Libraries can use this
    functionality to create CUDA contexts without the knowledge or involvement of
    their callers. For example, a CUDA-aware plugin library could create its own CUDA
    context on initialization, then pop it and keep it floating except when called
    by the main application. The floating context enables the library to be completely
    agnostic about which CPU thread is used to call into it. When used in this way,
    the containment enforced by CUDA contexts is a mixed blessing. On the one hand,
    the floating context’s memory cannot be polluted by spurious writes by third-party
    CUDA kernels, but on the other hand, the library can only operate on CUDA resources
    that it allocated.
  prefs: []
  type: TYPE_NORMAL
- en: Attaching and Detaching Contexts
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Until CUDA 4.0, every CUDA context had a “usage count” set to 1 when the context
    was created. The functions `cuCtxAttach()` and `cuCtxDetach()` incremented and
    decremented the usage count, respectively.^([5](ch03.html#ch03fn5)) The usage
    count was intended to enable libraries to “attach” to CUDA contexts created by
    the application into which the library was linked. This way, the application and
    its libraries could interoperate via a CUDA context that was created by the application.^([6](ch03.html#ch03fn6))
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch03.html#ch03fn5a). Until the `cuCtxDestroy()` function was added in CUDA
    2.2, CUDA contexts were destroyed by calling `cuCtxDetach()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch03.html#ch03fn6a). In retrospect, it would have been wiser for NVIDIA
    to leave reference-counting to higher-level software layers than the driver API.'
  prefs: []
  type: TYPE_NORMAL
- en: If a CUDA context is already current when CUDART is first invoked, it attaches
    the CUDA context instead of creating a new one. The CUDA runtime did not provide
    access to the usage count of a context. As of CUDA 4.0, the usage count is deprecated,
    and `cuCtxAttach()/cuCtxDetach()` do not have any side effects.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.5\. Context State
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `cuCtxSetLimit()` and `cuCtxGetLimit()` functions configure limits related
    to CPU-like functionality: in-kernel `malloc()` and `printf()`. The `cuCtxSetCacheConfig()`
    specifies the preferred cache configuration to use when launching kernels (whether
    to allocate 16K or 48K to shared memory and L1 cache). This is a hint, since any
    kernel that uses more than 16K of shared memory needs the configuration setting
    with 48K of shared memory. Additionally, the context state can be overridden by
    a kernel-specific state (`cuFuncSetCacheConfig()`). These states have context
    scope (in other words, they are not specified for each kernel launch) because
    they are expensive to change.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Modules and Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Modules are collections of code and related data that are loaded together, analogous
    to DLLs on Windows or DSOs on Linux. Like CUDA contexts, the CUDA runtime does
    not explicitly support modules; they are available only in the CUDA driver API.^([7](ch03.html#ch03fn7))
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch03.html#ch03fn7a). If CUDA adds the oft-requested ability to JIT from
    source code (as OpenCL can), NVIDIA may see fit to expose modules to the CUDA
    runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA does not have an intermediate structure analogous to object files that
    can be synthesized into a CUDA module. Instead, `nvcc` directly emits files that
    can be loaded as CUDA modules.
  prefs: []
  type: TYPE_NORMAL
- en: • `.cubin` files that target specific SM versions
  prefs: []
  type: TYPE_NORMAL
- en: • `.ptx` files that can be compiled onto the hardware by the driver
  prefs: []
  type: TYPE_NORMAL
- en: This data needn’t be sent to end users in the form of these files. CUDA includes
    APIs to load modules as NULL-terminated strings that can be embedded in executable
    resources or elsewhere.^([8](ch03.html#ch03fn8))
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch03.html#ch03fn8a). The `cuModuleLoadDataEx()` function is described in
    detail in [Section 4.2](ch04.html#ch04lev1sec2).'
  prefs: []
  type: TYPE_NORMAL
- en: Once a CUDA module is loaded, the application can query for the resources contained
    in it.
  prefs: []
  type: TYPE_NORMAL
- en: • Globals
  prefs: []
  type: TYPE_NORMAL
- en: • Functions (kernels)
  prefs: []
  type: TYPE_NORMAL
- en: • Texture references
  prefs: []
  type: TYPE_NORMAL
- en: 'One important note: All of these resources are created when the module is loaded,
    so the query functions cannot fail due to a lack of resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Like contexts, the CUDA runtime hides the existence and management of modules.
    All modules are loaded at the same time CUDART is initialized. For applications
    with large amounts of GPU code, the ability to explicitly manage residency by
    loading and unloading modules is one of the principal reasons to use the driver
    API instead of the CUDA runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Modules are built by invoking `nvcc`, which can emit different types of modules,
    depending on the command line parameters, as summarized in [Table 3.3](ch03.html#ch03tab03).
    Since cubins have been compiled to a specific GPU architecture, they do not have
    to be compiled “just in time” and are faster to load. But they are neither backward
    compatible (e.g., cubins compiled onto SM 2.x cannot run on SM 1.x architectures)
    nor forward compatible (e.g., cubins compiled onto SM 2.x architectures will not
    run on SM 3.x architectures). As a result, only applications with *a priori* knowledge
    of their target GPU architectures (and thus cubin versions) can use cubins without
    also embedding PTX versions of the same modules to use as backup.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.3* nvcc Module Types'
  prefs: []
  type: TYPE_NORMAL
- en: PTX is the intermediate language used as a source for the driver’s just-in-time
    compilation. Because this compilation can take a significant amount of time, the
    driver saves compiled modules and reuses them for a given PTX module, provided
    the hardware and driver have not changed. If the driver or hardware changes, all
    PTX modules must be recompiled.
  prefs: []
  type: TYPE_NORMAL
- en: With fatbins, the CUDA runtime automates the process of using a suitable cubin,
    if available, and compiling PTX otherwise. The different versions are embedded
    as strings in the host C++ code emitted by `nvcc`. Applications using the driver
    API have the advantage of finer-grained control over modules. For example, they
    can be embedded as resources in the executable, encrypted, or generated at runtime,
    but the process of using cubins if available and compiling PTX otherwise must
    be implemented explicitly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a module is loaded, the application can query for the resources contained
    in it: globals, functions (kernels), and texture references. One important note:
    All of these resources are created when the module is loaded, so the query functions
    (summarized in [Table 3.4](ch03.html#ch03tab04)) cannot fail due to a lack of
    resources.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.4* Module Query Functions'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Kernels (Functions)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kernels are highlighted by the `__global__` keyword in `.cu` files. When using
    the CUDA runtime, they can be invoked in-line with the triple-angle-bracket `<<<
    >>>` syntax. [Chapter 7](ch07.html#ch07) gives a detailed description of how kernels
    can be invoked and how they execute on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: The GPU executable code of the module comes in the form of kernels that are
    invoked with the language integration features of the CUDA runtime `(<<< >>>`
    syntax) or the `cuLaunchKernel()` function in the driver API. At the time of this
    writing, CUDA does not do any dynamic residency management of the executable code
    in CUDA modules. When a module is loaded, *all* of the kernels are loaded into
    device memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a module is loaded, kernels may be queried with `cuModuleGetFunction()`;
    the kernel’s attributes can be queried with `cuFuncGetAttribute()`; and the kernel
    may be launched with `cuLaunchKernel()`. `cuLaunchKernel()` rendered a whole slew
    of API entry points obsolete: Functions such as `cuFuncSetBlockShape()` specified
    the block size to use the next time a given kernel was launched; functions such
    as `cuParamSetv()` specified the parameters to pass the next time a given kernel
    was launched; and `cuLaunch()`, `cuLaunchGrid()`, and `cuLaunchGridAsync()` launched
    a kernel using the previously set state. These APIs were inefficient because it
    took so many calls to set up a kernel launch and because parameters such as block
    size are best specified atomically with the request to launch the kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: The `cuFuncGetAttribute()` function may be used to query specific attributes
    of a function, such as
  prefs: []
  type: TYPE_NORMAL
- en: • The maximum number of threads per block
  prefs: []
  type: TYPE_NORMAL
- en: • The amount of statically allocated shared memory
  prefs: []
  type: TYPE_NORMAL
- en: • The size of user-allocated constant memory
  prefs: []
  type: TYPE_NORMAL
- en: • The amount of local memory used by each function
  prefs: []
  type: TYPE_NORMAL
- en: • The number of registers used by each thread of the function
  prefs: []
  type: TYPE_NORMAL
- en: • The virtual (PTX) and binary architecture versions for which the function
    was compiled
  prefs: []
  type: TYPE_NORMAL
- en: When using the driver API, it is usually a good idea to use `extern "C"` to
    inhibit the default name-mangling behavior of C++. Otherwise, you have to specify
    the mangled name to `cuModuleGetFunction()`.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As executables that were built with the CUDA runtime are loaded, they create
    global data structures in host memory that describe the CUDA resources to be allocated
    when a CUDA device is created. Once a CUDA device is initialized, these globals
    are used to create the CUDA resources all at once. Because these globals are shared
    process-wide by the CUDA runtime, it is not possible to incrementally load and
    unload CUDA modules using the CUDA runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the way the CUDA runtime is integrated with the C++ language, kernels
    and symbols should be specified by name (i.e., not with a string literal) to API
    functions such as `cudaFuncGetAttributes()` and `cudaMemcpyToSymbol()`.
  prefs: []
  type: TYPE_NORMAL
- en: Cache Configuration
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In Fermi-class architectures, the streaming multiprocessors have L1 caches
    that can be split as 16K shared/48K L1 cache or 48K shared/16K L1 cache.^([9](ch03.html#ch03fn9))
    Initially, CUDA allowed the cache configuration to be specified on a per-kernel
    basis, using `cudaFuncSetCacheConfig()` in the CUDA runtime or `cuFuncSetCacheConfig()`
    in the driver API. Later, this state was moved to be more global: `cuCtxSetCacheConfig()/cudaDeviceSetCacheConfig()`
    specifies the default cache configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: '[9](ch03.html#ch03fn9a). SM 3.x added the ability to split the cache evenly
    (32K/32K) between L1 and shared memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6\. Device Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Device memory (or linear device memory) resides in the CUDA address space and
    may be accessed by CUDA kernels via normal C/C++ pointer and array dereferencing
    operations. Most GPUs have a dedicated pool of device memory that is directly
    attached to the GPU and accessed by an integrated memory controller.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA hardware does not support demand paging, so all memory allocations are
    backed by actual physical memory. Unlike CPU applications, which can allocate
    more virtual memory than there is physical memory in the system, CUDA’s memory
    allocation facilities fail when the physical memory is exhausted. The details
    of how to allocate, free, and access device memory are given in [Section 5.2](ch05.html#ch05lev1sec2).
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: CUDA runtime applications may query the total amount of device memory available
    on a given device by calling `cudaGetDeviceProperties()` and examining `cudaDeviceProp::totalGlobalMem`.
    `cudaMalloc()` and `cudaFree()` allocate and free device memory, respectively.
    `cudaMallocPitch()` allocates pitched memory; `cudaFree()` may be used to free
    it. `cudaMalloc3D()` performs a 3D allocation of pitched memory.
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Driver API applications may query the total amount of device memory available
    on a given device by calling `cuDeviceTotalMem()`. Alternatively, the `cuMemGetInfo()`
    function may be used to query the amount of free device memory as well as the
    total. `cuMemGetInfo()` can only be called when a CUDA context is current to the
    CPU thread. `cuMemAlloc()` and `cuMemFree()` allocate and free device memory,
    respectively. `cuMemAllocPitch()` allocates pitched memory; `cuMemFree()` may
    be used to free it.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7\. Streams and Events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In CUDA, streams and events were added to enable host↔device memory copies to
    be performed concurrently with kernel execution. Later versions of CUDA expanded
    streams’ capabilities to support execution of multiple kernels concurrently on
    the same GPU and to support concurrent execution between multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA streams are used to manage concurrency between execution units with coarse
    granularity.
  prefs: []
  type: TYPE_NORMAL
- en: • The GPU and the CPU
  prefs: []
  type: TYPE_NORMAL
- en: • The copy engine(s) that can perform DMA while the SMs are processing
  prefs: []
  type: TYPE_NORMAL
- en: • The streaming multiprocessors (SMs)
  prefs: []
  type: TYPE_NORMAL
- en: • Kernels that are intended to run concurrently
  prefs: []
  type: TYPE_NORMAL
- en: • Separate GPUs that are executing concurrently
  prefs: []
  type: TYPE_NORMAL
- en: The operations requested in a given stream are performed sequentially. In a
    sense, CUDA streams are like CPU threads in that operations within a CUDA stream
    are performed in order.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.1\. Software Pipelining
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because there is only one DMA engine serving the various coarse-grained hardware
    resources in the GPU, applications must “software-pipeline” the operations performed
    on multiple streams. Otherwise, the DMA engine will “break concurrency” by enforcing
    synchronization within the stream between different engines. A detailed description
    of how to take full advantage of CUDA streams using software pipelining is given
    in [Section 6.5](ch06.html#ch06lev1sec5), and more examples are given in [Chapter
    11](ch11.html#ch11).
  prefs: []
  type: TYPE_NORMAL
- en: The Kepler architecture reduced the need to software-pipeline streamed operations
    and, with NVIDIA’s Hyper-Q technology (first available with SM 3.5), virtually
    eliminated the need for software pipelining.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.2\. Stream Callbacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA 5.0 introduced another mechanism for CPU/GPU synchronization that complements
    the existing mechanisms, which focus on enabling CPU threads to wait until streams
    are idle or events have been recorded. Stream callbacks are functions provided
    by the application, registered with CUDA, and later called by CUDA when the stream
    has reached the point at which `cuStreamAddCallback()` was called.
  prefs: []
  type: TYPE_NORMAL
- en: Stream execution is suspended for the duration of the stream callback, so for
    performance reasons, developers should be careful to make sure other streams are
    available to process during the callback.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.3\. The NULL Stream
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Any of the asynchronous memcpy functions may be called with NULL as the stream
    parameter, and the memcpy will not be initiated until all preceding operations
    on the GPU have been completed; in effect, the NULL stream is a join of all the
    engines on the GPU. Additionally, all streamed memcpy functions are *asynchronous*,
    potentially returning control to the application before the memcpy has been performed.
    The NULL stream is most useful for facilitating CPU/GPU concurrency in applications
    that have no need for the intra-GPU concurrency facilitated by multiple streams.
    Once a streamed operation has been initiated with the NULL stream, the application
    must use synchronization functions such as `cuCtxSynchronize()` or `cudaThreadSynchronize()`
    to ensure that the operation has been completed before proceeding. But the application
    may request many such operations before performing the synchronization. For example,
    the application may perform
  prefs: []
  type: TYPE_NORMAL
- en: • an asynchronous host→device memcpy
  prefs: []
  type: TYPE_NORMAL
- en: • one or more kernel launches
  prefs: []
  type: TYPE_NORMAL
- en: • an asynchronous device→host memcpy
  prefs: []
  type: TYPE_NORMAL
- en: before synchronizing with the context. The `cuCtxSynchronize()` or `cudaThreadSynchronize()`
    call returns after the GPU has performed the last-requested operation. This idiom
    is especially useful when performing smaller memcpy’s or launching kernels that
    will not run for long. The CUDA driver takes valuable CPU time to write commands
    to the GPU, and overlapping that CPU execution with the GPU’s processing of the
    commands can improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note:* Even in CUDA 1.0, kernel launches were asynchronous; the NULL stream
    is implicitly specified to any kernel launch in which no stream is explicitly
    specified.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.4\. Events
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA events present another mechanism for synchronization. Introduced at the
    same time as CUDA streams, “recording” CUDA events is a way for applications to
    track progress within a CUDA stream. All CUDA events work by writing a shared
    sync memory location when all preceding operations in the CUDA stream have been
    performed.^([10](ch03.html#ch03fn10)) *Querying* the CUDA event causes the driver
    to peek at this memory location and report whether the event has been recorded;
    *synchronizing* with the CUDA event causes the driver to wait until the event
    has been recorded.
  prefs: []
  type: TYPE_NORMAL
- en: '[10](ch03.html#ch03fn10a). Specifying the NULL stream to `cuEventRecord()`
    or `cudaEventRecord()` means the event will not be recorded until the GPU has
    processed *all* preceding operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, CUDA events also can write a timestamp derived from a high-resolution
    timer in the hardware. Event-based timing can be more robust than CPU-based timing,
    especially for smaller operations, because it is not subject to spurious unrelated
    events (such as page faults or network traffic) that may affect wall-clock timing
    by the CPU. Wall-clock times are definitive because they are a better approximation
    of what the end users sees, so CUDA events are best used for performance tuning
    during product development.^([11](ch03.html#ch03fn11))
  prefs: []
  type: TYPE_NORMAL
- en: '[11](ch03.html#ch03fn11a). Additionally, CUDA events that can be used for timing
    cannot be used for certain other operations; more recent versions of CUDA allow
    developers to opt out of the timing feature to enable the CUDA event to be used,
    for example, for interdevice synchronization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Timing using CUDA events is best performed in conjunction with the NULL stream.
    This rule of thumb is motivated by reasons similar to the reasons RDTSC (Read
    TimeStamp Counter) is a serializing instruction on the CPU: Just as the CPU is
    a superscalar processor that can execute many instructions at once, the GPU can
    be operating on multiple streams at the same time. Without explicit serialization,
    a timing operation may inadvertently include operations that were not intended
    to be timed or may exclude operations that were supposed to be timed. As with
    RDTSC, the trick is to bracket the CUDA event recordings with enough work that
    the overhead of performing the timing itself is negligible.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA events optionally can cause an interrupt to be signaled by the hardware,
    enabling the driver to perform a so-called “blocking” wait. Blocking waits suspend
    the waiting CPU thread, saving CPU clock cycles and power while the driver waits
    for the GPU. Before blocking waits became available, CUDA developers commonly
    complained that the CUDA driver burned a whole CPU core waiting for the GPU by
    polling a memory location. At the same time, blocking waits may take longer due
    to the overhead of handling the interrupt, so latency-sensitive applications may
    still wish to use the default polling behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 3.8\. Host Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: “Host” memory is CPU memory—the stuff we all were managing with `malloc()/free()`
    and `new[]/delete[]` for years before anyone had heard of CUDA. On all operating
    systems that run CUDA, host memory is *virtualized*; memory protections enforced
    by hardware are in place to protect CPU processes from reading or writing each
    other’s memory without special provisions.^([12](ch03.html#ch03fn12)) “Pages”
    of memory, usually 4K or 8K in size, can be relocated without changing their virtual
    address; in particular, they can be swapped to disk, effectively enabling the
    computer to have more virtual memory than physical memory. When a page is marked
    “nonresident,” an attempt to access the page will signal a “page fault” to the
    operating system, which will prompt the operating system to find a physical page
    available to copy the data from disk and resume execution with the virtual page
    pointing to the new physical location.
  prefs: []
  type: TYPE_NORMAL
- en: '[12](ch03.html#ch03fn12a). Examples of APIs that facilitate interprocess sharing
    include `MapViewOfFile()` on Windows or `mmap()` on Linux.'
  prefs: []
  type: TYPE_NORMAL
- en: The operating system component that manages virtual memory is called the “virtual
    memory manager” or VMM. Among other things, the VMM monitors memory activity and
    uses heuristics to decide when to “evict” pages to disk and resolves the page
    faults that happen when evicted pages are referenced.
  prefs: []
  type: TYPE_NORMAL
- en: 'The VMM provides services to hardware drivers to facilitate direct access of
    host memory by hardware. In modern computers, many peripherals, including disk
    controllers, network controllers, and GPUs, can read or write host memory using
    a facility known as “direct memory access” or DMA. DMA gives two performance benefits:
    It avoids a data copy^([13](ch03.html#ch03fn13)) and enables the hardware to operate
    concurrently with the CPU. A tertiary benefit is that hardware may achieve better
    bus performance over DMA.'
  prefs: []
  type: TYPE_NORMAL
- en: '[13](ch03.html#ch03fn13a). This extra copy is more obvious to developers using
    GPUs, whose target peripheral can consume much more bandwidth than more pedestrian
    devices like those for disk or network controllers. Whatever the type of peripheral,
    without DMA, the driver must use the CPU to copy data to or from special hardware
    buffers.'
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate DMA, operating system VMMs provide a service called “page-locking.”
    Memory that is page-locked has been marked by the VMM as ineligible for eviction,
    so its physical address cannot change. Once memory is page-locked, drivers can
    program their DMA hardware to reference the physical addresses of the memory.
    This hardware setup is a separate and distinct operation from the page-locking
    itself. Because page-locking makes the underlying physical memory unavailable
    for other uses by the operating system, page-locking too much memory can adversely
    affect performance.
  prefs: []
  type: TYPE_NORMAL
- en: Memory that is not page-locked is known as “pageable.” Memory that is page-locked
    is sometimes known as “pinned” memory, since its physical address cannot be changed
    by the operating system (it has been pinned in place).
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.1\. Pinned Host Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: “Pinned” host memory is allocated by CUDA with the functions `cuMemHostAlloc()
    / cudaHostAlloc()`. This memory is page-locked and set up for DMA by the current
    CUDA context.^([14](ch03.html#ch03fn14))
  prefs: []
  type: TYPE_NORMAL
- en: '[14](ch03.html#ch03fn14a). CUDA developers often ask if there is any difference
    between page-locked memory and CUDA’s “pinned” memory. There is! Pinned memory
    allocated or registered by CUDA is mapped for direct access by the GPU(s); ordinary
    page-locked memory is not.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA tracks the memory ranges allocated in this way and automatically accelerates
    memcpy operations that reference pinned memory. Asynchronous memcpy operations
    only work on pinned memory. Applications can determine whether a given host memory
    address range is pinned using the `cuMemHostGetFlags()` function.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of operating system documentation, the terms *page-locked* and
    *pinned* are synonymous, but for CUDA purposes, it may be easier to think of “pinned”
    memory as host memory that has been page-locked and mapped for access by the hardware.
    “Page-locking” refers only to the operating system mechanism for marking host
    memory pages as ineligible for eviction.
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.2\. Portable Pinned Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Portable* pinned memory is mapped for all CUDA contexts after being page-locked.
    The underlying mechanism for this operation is complicated: When a portable pinned
    allocation is performed, it is mapped into all CUDA contexts before returning.
    Additionally, whenever a CUDA context is created, all portable pinned memory allocations
    are mapped into the new CUDA context before returning. For either portable memory
    allocation or context creation, any failure to perform these mappings will cause
    the allocation or context creation to fail. Happily, as of CUDA 4.0, if UVA (unified
    virtual addressing) is in force, *all* pinned allocations are portable.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.3\. Mapped Pinned Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Mapped* pinned memory is mapped into the address space of the CUDA context,
    so kernels may read or write the memory. By default, pinned memory is not mapped
    into the CUDA address space, so it cannot be corrupted by spurious writes by a
    kernel. For integrated GPUs, mapped pinned memory enables “zero copy”: Since the
    host (CPU) and device (GPU) share the same memory pool, they can exchange data
    without explicit copies.'
  prefs: []
  type: TYPE_NORMAL
- en: For discrete GPUs, mapped pinned memory enables host memory to be read or written
    directly by kernels. For small amounts of data, this has the benefit of eliminating
    the overhead of explicit memory copy commands. Mapped pinned memory can be especially
    beneficial for writes, since there is no latency to cover. As of CUDA 4.0, if
    UVA (unified virtual addressing) is in effect, *all* pinned allocations are mapped.
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.4\. Host Memory Registration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since developers (especially library developers) don’t always get to allocate
    memory they want to access, CUDA 4.0 added the ability to “register” existing
    virtual address ranges for use by CUDA. The `cuMemHostRegister()/cudaHostRegister()`
    functions take a virtual address range and page-locks and maps it for the current
    GPU (or for all GPUs, if `CU_MEMHOSTREGISTER_PORTABLE` or `cudaHostRegisterPortable`
    is specified). Host memory registration has a perverse relationship with UVA (unified
    virtual addressing), in that any address range eligible for registration must
    not have been included in the virtual address ranges reserved for UVA purposes
    when the CUDA driver was initialized.
  prefs: []
  type: TYPE_NORMAL
- en: 3.9\. CUDA Arrays and Texturing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CUDA arrays are allocated from the same pool of physical memory as device memory,
    but they have an opaque layout that is optimized for 2D and 3D locality. The graphics
    drivers use these layouts to hold textures; by decoupling the indexing from the
    addressing, the hardware can operate on 2D or 3D blocks of elements instead of
    1D rows. For applications that exhibit sparse access patterns, especially patterns
    with dimensional locality (for example, computer vision applications), CUDA arrays
    are a clear win. For applications with regular access patterns, especially those
    with little to no reuse or whose reuse can be explicitly managed by the application
    in shared memory, device pointers are the obvious choice.
  prefs: []
  type: TYPE_NORMAL
- en: Some applications, such as image processing applications, fall into a gray area
    where the choice between device pointers and CUDA arrays is not obvious. All other
    things being equal, device memory is probably preferable to CUDA arrays, but the
    following considerations may be used to help in the decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA arrays do not consume CUDA address space.
  prefs: []
  type: TYPE_NORMAL
- en: • On WDDM drivers (Windows Vista and later), the system can automatically manage
    the residence of CUDA arrays.
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA arrays can reside only in device memory, and the GPU can convert between
    the two representations while transferring the data across the bus. For some applications,
    keeping a pitch representation in host memory and a CUDA array representation
    in device memory is the best approach.
  prefs: []
  type: TYPE_NORMAL
- en: 3.9.1\. Texture References
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Texture references* are objects that CUDA uses to set up the texturing hardware
    to “interpret” the contents of underlying memory.^([15](ch03.html#ch03fn15)) Part
    of the reason this level of indirection exists is because it is valid to have
    multiple texture references referencing the same memory with different attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: '[15](ch03.html#ch03fn15a). Before CUDA 3.2, texture references were the only
    way to read from CUDA arrays, other than explicit memcpy. Today, surface references
    may be used to write to CUDA arrays as well as to read from them.'
  prefs: []
  type: TYPE_NORMAL
- en: A texture reference’s attributes may be *immutable*—that is, specified at compile
    time and not subject to change without causing the application to behave incorrectly—or
    *mutable*—that is, where the application may change the texture’s behavior in
    ways that are not visible to the compiler ([Table 3.5](ch03.html#ch03tab05)).
    For example, the dimensionality of the texture (1D, 2D, or 3D) is immutable, since
    it must be known by the compiler to take the correct number of input parameters
    and emit the correct machine instruction. In contrast, the filtering and addressing
    modes are mutable, since they implicitly change the application’s behavior without
    any knowledge or involvement from the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.5* Mutable and Immutable Texture Attributes'
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime (language integration) and the CUDA driver API deal with texture
    references very differently. In both cases, a texture reference is declared by
    invoking a template called `texture`.
  prefs: []
  type: TYPE_NORMAL
- en: texture<Type, Dimension, ReadMode> Name;
  prefs: []
  type: TYPE_NORMAL
- en: where *Type* is the type of the elements in the memory being read by the texture,
    *Dimension* is the dimension of the texture (1, 2, or 3), and *ReadMode* specifies
    whether integer-valued texture types should be converted to normalized floating
    point when read by the texture reference.
  prefs: []
  type: TYPE_NORMAL
- en: The texture reference must be *bound* to underlying memory before it can be
    used. The hardware is better optimized to texture from CUDA arrays, but in the
    following cases, applications benefit from texturing from device memory.
  prefs: []
  type: TYPE_NORMAL
- en: • It enlists the texture cache, which serves as a bandwidth aggregator.
  prefs: []
  type: TYPE_NORMAL
- en: • It enables applications to work around coalescing restrictions.
  prefs: []
  type: TYPE_NORMAL
- en: • It avoids superfluous copies when reading from memory that is otherwise best
    written via device memory. For example, a video codec may wish to emit frames
    into device memory, yet read from them via texture.
  prefs: []
  type: TYPE_NORMAL
- en: Once the texture reference is bound to underlying memory, CUDA kernels may read
    the memory by invoking `tex* intrinsics`, such as `tex1D()`, given in [Table 3.6](ch03.html#ch03tab06).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.6* Texture Intrinsics'
  prefs: []
  type: TYPE_NORMAL
- en: '*Note:* There are no coherency guarantees between texture reads and writes
    performed via global load/store or surface load/store. As a result, CUDA kernels
    must take care not to texture from memory that also is being accessed by other
    means.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To bind memory to a texture, applications must call one of the functions in
    [Table 3.7](ch03.html#ch03tab07). CUDA runtime applications can modify mutable
    attributes of the texture reference by directly assigning structure members.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch03_images.html#p084pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: texture<float, 1, cudaReadModeElementType> tex1;
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: tex1.filterMode = cudaFilterModeLinear; // enable linear filtering
  prefs: []
  type: TYPE_NORMAL
- en: tex1.normalized = true; // texture coordinates will be normalized
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.7* Functions to Bind Device Memory to Textures'
  prefs: []
  type: TYPE_NORMAL
- en: Assigning to these structure members has an immediate effect; there is no need
    to rebind the texture.
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Since there is a stricter partition between CPU code and GPU code when using
    the driver API, any texture references declared in a CUDA module must be queried
    via `cuModuleGetTexRef()`, which passes back a `CUtexref`. Unlike the CUDA runtime,
    the texture reference then must be initialized with *all* of the correct attributes—both
    mutable and immutable—because the compiler does not encode the immutable attributes
    of the texture reference into the CUDA module. [Table 3.8](ch03.html#ch03tab08)
    summarizes the driver API functions that can be used to bind a texture reference
    to memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.8* Driver API Functions to Bind Memory to Textures'
  prefs: []
  type: TYPE_NORMAL
- en: 3.9.2\. Surface References
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Surface references*, a more recent addition to CUDA not available on Tesla-class
    GPUs, enable CUDA kernels to read and write CUDA arrays via the surface load/store
    intrinsics. Their primary purpose is to enable CUDA kernels to write CUDA arrays
    directly. Before surface load/store became available, kernels had to write to
    device memory and then perform a device→array memcpy to copy and convert the output
    into a CUDA array.'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to texture references, which can transform everything from the input
    coordinates to the output format, depending on how they are set up, surface references
    expose a vanilla, bitwise interface to read and write the contents of the CUDA
    array.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder why CUDA did not implement surface load/store intrinsics that
    operated directly on CUDA arrays (as OpenCL did). The reason is to be future-proof
    to surface load/store operations that convert to the underlying representation
    in a more sophisticated way, such as enabling samples to be “splatted” into the
    CUDA array with fractional coordinates, or interoperating with an antialiased
    graphics surface. For now, CUDA developers will have to make do implementing such
    operations in software.
  prefs: []
  type: TYPE_NORMAL
- en: 3.10\. Graphics Interoperability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The graphics interoperability (or “graphics interop”) family of functions enables
    CUDA to read and write memory belonging to the OpenGL or Direct3D APIs. If applications
    could attain acceptable performance by sharing data via host memory, there would
    be no need for these APIs. But with local memory bandwidth that can exceed 140G/s
    and PCI Express bandwidth that rarely exceeds 6G/s in practice, it is important
    to give applications the opportunity to keep data on the GPU when possible. Using
    the graphics interop APIs, CUDA kernels can write data into images and textures
    that are then incorporated into graphical output to be performed by OpenGL or
    Direct3D.
  prefs: []
  type: TYPE_NORMAL
- en: Because the graphics and CUDA drivers must coordinate under the hood to enable
    interoperability, applications must signal their intention to perform graphics
    interop early. In particular, the CUDA context must be notified that it will be
    interoperating with a given API by calling special context creation APIs such
    as `cuD3D10CtxCreate()` or `cudaGLSetDevice()`.
  prefs: []
  type: TYPE_NORMAL
- en: The coordination between drivers also motivated resource-sharing between graphics
    APIs and CUDA to occur in two steps.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** *Registration:* a potentially expensive operation that signals the developer’s
    intent to share the resources to the underlying drivers, possibly prompting them
    to move and/or lock down the resources in question'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** *Mapping:* a lightweight operation that is expected to occur at high
    frequency'
  prefs: []
  type: TYPE_NORMAL
- en: In early versions of CUDA, the APIs for graphics interoperability with all four
    graphics APIs (OpenGL, Direct3D 9, Direct3D 10, and Direct3D 11) were strictly
    separate. For example, for Direct3D 9 interoperability, the following functions
    would be used in conjunction with one another.
  prefs: []
  type: TYPE_NORMAL
- en: • `cuD3D9RegisterResource()/cudaD3D9RegisterResource()`
  prefs: []
  type: TYPE_NORMAL
- en: • `cuD3D9MapResources()/cudaD3D9MapResources()`
  prefs: []
  type: TYPE_NORMAL
- en: • `cuD3D9UnmapResources()/cudaD3D9UnmapResources()`
  prefs: []
  type: TYPE_NORMAL
- en: • `cuD3D9UnregisterResource()/cudaD3D9UnregisterResource()`
  prefs: []
  type: TYPE_NORMAL
- en: Because the underlying hardware capabilities are the same, regardless of the
    API used to access them, many of these functions were merged in CUDA 3.2\. The
    registration functions remain API-specific, since they require API-specific bindings,
    but the functions to map, unmap, and unregister resources were made common. The
    CUDA 3.2 APIs corresponding to the above are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: • `cuD3D9RegisterResource()/cudaD3D9RegisterResource()`
  prefs: []
  type: TYPE_NORMAL
- en: • `cuGraphicsMapResources()/cudaGraphicsMapResources()`
  prefs: []
  type: TYPE_NORMAL
- en: • `cuGraphicsUnmapResources()/cudaGraphicsUnmapResources()`
  prefs: []
  type: TYPE_NORMAL
- en: • `cuGraphicsUnregisterResource()/cudaGraphicsUnregisterResource()`
  prefs: []
  type: TYPE_NORMAL
- en: The interoperability APIs for Direct3D 10 are the same, except the developer
    must use `cuD3D10RegisterResource()/cudaD3D10RegisterResource()` instead of the
    `cuD3D9*` variants.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA 3.2 also added the ability to access textures from graphics APIs in the
    form of CUDA arrays. In Direct3D, textures are just a different type of “resource”
    and may be referenced by `IDirect3DResource9 *` (or `IDirect3DResource10 *`, etc.).
    In OpenGL, a separate function `cuGraphicsGLRegisterImage()` is provided.
  prefs: []
  type: TYPE_NORMAL
- en: 3.11\. The CUDA Runtime and CUDA Driver API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The CUDA runtime (“CUDART”) facilitates the language integration that makes
    CUDA so easy to program out of the gate. By automatically taking care of tasks
    such as initializing contexts and loading modules, and especially by enabling
    kernel invocation to be done in-line with other C++ code, CUDART lets developers
    focus on getting their code working quickly. A handful of CUDA abstractions, such
    as CUDA modules, are not accessible via CUDART.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the driver API exposes all CUDA abstractions and enables them to
    be manipulated by developers as needed for the application. The driver API does
    not provide any performance benefit. Instead, it enables explicit resource management
    for applications that need it, like large-scale commercial applications with plug-in
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The driver API is not noticeably faster than the CUDA runtime. If you are looking
    to improve performance in your CUDA application, look elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Most CUDA features are available to both CUDART and the driver API, but a few
    are exclusive to one or the other. [Table 3.9](ch03.html#ch03tab09) summarizes
    the differences.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab09.jpg)![Image](graphics/03tab09a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.9.* CUDA Runtime versus Driver API Features'
  prefs: []
  type: TYPE_NORMAL
- en: Between the two APIs, operations like memcpy tend to be functionally identical,
    but the interfaces can be quite different. The stream APIs are almost identical.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/089tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The event APIs have minor differences, with CUDART providing a separate `cudaEventCreateWithFlags()`
    function if the developer wants to specify a flags word (needed to create a blocking
    event).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/089tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The memcpy functions are the family where the interfaces are the most different,
    despite identical underlying functionality. CUDA supports three variants of memory—host,
    device, and CUDA array—which are all permutations of participating memory types,
    and 1D, 2D, or 3D memcpy. So the memcpy functions must contain either a large
    family of different functions or a small number of functions that support many
    types of memcpy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest memcpy’s in CUDA copy between host and device memory, but even
    those function interfaces are different: CUDART uses `void *` for the types of
    both host and device pointers and a single memcpy function with a direction parameter,
    while the driver API uses `void *` for host memory, `CUdeviceptr` for device memory,
    and three separate functions (`cuMemcpyHtoD()`, `cuMemcpyDtoH()`, and `cuMemcpyDtoD()`)
    for the different memcpy directions. Here are equivalent CUDART and driver API
    formulations of the three permutations of host↔device memcpy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/090tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For 2D and 3D memcpy’s, the driver API implements a handful of functions that
    take a descriptor struct and support all permutations of memcpy, including lower-dimension
    memcpy’s. For example, if desired, `cuMemcpy3D()` can be used to perform a 1D
    host→device memcpy instead of `cuMemcpyHtoD()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch03_images.html#p091pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_MEMCPY3D cp = {0};
  prefs: []
  type: TYPE_NORMAL
- en: cp.dstMemoryType = CU_MEMORYTYPE_DEVICE;
  prefs: []
  type: TYPE_NORMAL
- en: cp.dstDevice = dptr;
  prefs: []
  type: TYPE_NORMAL
- en: cp.srcMemoryType = CU_MEMORYTYPE_HOST;
  prefs: []
  type: TYPE_NORMAL
- en: cp.srcHost = host;
  prefs: []
  type: TYPE_NORMAL
- en: cp.WidthInBytes = bytes;
  prefs: []
  type: TYPE_NORMAL
- en: cp.Height = cp.Depth = 1;
  prefs: []
  type: TYPE_NORMAL
- en: status = cuMemcpy3D( &cp );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART uses a combination of descriptor structs for more complicated memcpy’s
    (e.g., `cudaMemcpy3D()`), while using different functions to cover the different
    memory types. Like `cuMemcpy3D()`, CUDART’s `cudaMemcpy3D()` function takes a
    descriptor struct that can describe any permutation of memcpy, including interdimensional
    memcpy’s (e.g., performing a 1D copy to or from the row of a 2D CUDA array, or
    copying 2D CUDA arrays to or from slices of 3D CUDA arrays). Its descriptor struct
    is slightly different in that it embeds other structures; the two APIs’ 3D memcpy
    structures are compared side-by-side in [Table 3.10](ch03.html#ch03tab10).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/03tab10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.10* 3D Memcpy Structures'
  prefs: []
  type: TYPE_NORMAL
- en: Usage of both 3D memcpy functions is similar. They are designed to be zero-initialized,
    and developers set the members needed for a given operation. For example, performing
    a host→3D array copy may be done as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/091tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For a 3D copy that covers the entire CUDA array, the source and destination
    offsets are set to 0 by the first line and don’t have to be referenced again.
    Unlike parameters to a function, the code only needs to reference the parameters
    needed by the copy, and if the program must perform more than one similar copy
    (e.g., to populate more than one CUDA array or device memory region), the descriptor
    struct can be reused.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 4\. Software Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter gives an overview of the CUDA development tools and the software
    environments that can host CUDA applications. Sections are devoted to the various
    tools in the NVIDIA toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: • `nvcc:` the CUDA compiler driver
  prefs: []
  type: TYPE_NORMAL
- en: • `ptxas:` the PTX assembler
  prefs: []
  type: TYPE_NORMAL
- en: • `cuobjdump:` the CUDA object file dump utility
  prefs: []
  type: TYPE_NORMAL
- en: • `nvidia-smi:` the NVIDIA System Management Interface
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 4.5](ch04.html#ch04lev1sec5) describes Amazon’s EC2 (Elastic Compute
    Cloud) service and how to use it to access GPU-capable servers over the Internet.
    This chapter is intended more as a reference than as a tutorial. Example usages
    are given in [Part III](part03.html#part03) of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. `nvcc`—CUDA Compiler Driver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`nvcc` is the compiler driver CUDA developers use to translate source code
    into functional CUDA applications. It can perform many functions, from as simple
    as a targeted compilation of a GPU-only `.cu` file to as complex as compiling,
    linking, and executing a sample program in one command (a usage encouraged by
    many of the sample programs in this book).'
  prefs: []
  type: TYPE_NORMAL
- en: As a compiler driver, `nvcc` does nothing more than set up a build environment
    and spawn a combination of native tools (such as the C compiler installed on the
    system) and CUDA-specific command-line tools (such as `ptxas`) to build the CUDA
    code. It implements many sensible default behaviors that can be overridden by
    command-line options; its exact behavior depends on which “compile trajectory”
    is requested by the main command-line option.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 4.1](ch04.html#ch04tab01) lists the file extensions understood by `nvcc`
    and the default behavior implemented for them. (*Note:* Some intermediate file
    types, like the `.i/.ii` files that contain host code generated by CUDA’s front
    end, are omitted here.) [Table 4.2](ch04.html#ch04tab02) lists the compilation
    stage options and corresponding compile trajectory. [Table 4.3](ch04.html#ch04tab03)
    lists `nvcc` options that affect the environment, such as paths to include directories.
    [Table 4.4](ch04.html#ch04tab04) lists `nvcc` options that affect the output,
    such as whether to include debugging information. [Table 4.5](ch04.html#ch04tab05)
    lists “passthrough” options that enable `nvcc` to pass options to the tools that
    it invokes, such as `ptxas`. [Table 4.6](ch04.html#ch04tab06) lists `nvcc` options
    that aren’t easily categorized, such as the `–keep` option that instructs `nvcc`
    not to delete the temporary files it created.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.1* Extensions for `nvcc` Input Files'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.2* Compilation Trajectories'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab03.jpg)![Image](graphics/04tab03a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.3* `nvcc` Options (Environment)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.4* Options for Specifying Behavior of Compiler/Linker'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.5* `nvcc` Options for Passthrough'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab06.jpg)![Image](graphics/04tab06a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.6* Miscellaneous `nvcc` Options'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 4.7](ch04.html#ch04tab07) lists `nvcc` options related to code generation.
    The `--gpu-architecture` and `--gpu-code` options are especially confusing. The
    former controls which *virtual* GPU architecture to compile for (i.e., which version
    of PTX to emit), while the latter controls which *actual* GPU architecture to
    compile for (i.e., which version of SM microcode to emit). The `--gpu-code` option
    must specify SM versions that are at least as high as the versions specified to
    `--gpu-architecture`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab07.jpg)![Image](graphics/04tab07a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.7.* `nvcc` Options for Code Generation'
  prefs: []
  type: TYPE_NORMAL
- en: The `--export-dir` option specifies a directory where all device code images
    will be copied. It is intended as a device code repository that can be inspected
    by the CUDA driver when the application is running (in which case the directory
    should be in the `CUDA_DEVCODE_PATH` environment variable). The repository can
    be either a directory or a ZIP file. In either case, CUDA will maintain a directory
    structure to facilitate code lookup by the CUDA driver. If a filename is specified
    but does not exist, a directory structure (not a ZIP file) will be created at
    that location.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. `ptxas`—the PTX Assembler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`ptxas`, the tool that compiles PTX into GPU-specific microcode, occupies a
    unique place in the CUDA ecosystem in that NVIDIA makes it available both in the
    offline tools (which developers compile into applications) and as part of the
    driver, enabling so-called “online” or “just-in-time” (JIT) compilation (which
    occurs at runtime).'
  prefs: []
  type: TYPE_NORMAL
- en: When compiling offline, `ptxas` generally is invoked by `nvcc` if any actual
    GPU architectures are specified with the `--gpu-code` command-line option. In
    that case, command-line options (summarized in [Table 4.8](ch04.html#ch04tab08))
    can be passed to `ptxas` via the `-Xptxas` command-line option to `nvcc`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab08.jpg)![Image](graphics/04tab08a.jpg)![Image](graphics/04tab08b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.8.* Command-Line Options for `ptxas`'
  prefs: []
  type: TYPE_NORMAL
- en: Developers also can load PTX code dynamically by invoking `cuModuleLoadDataEx()`,
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuModuleLoadDataEx (
  prefs: []
  type: TYPE_NORMAL
- en: CUmodule *module,
  prefs: []
  type: TYPE_NORMAL
- en: const void *image,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int numOptions,
  prefs: []
  type: TYPE_NORMAL
- en: CUjit_option *options,
  prefs: []
  type: TYPE_NORMAL
- en: void **optionValues
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: '`cuModuleLoadDataEx()` takes a pointer image and loads the corresponding module
    into the current context. The pointer may be obtained by mapping a `cubin` or
    `PTX` or `fatbin` file, passing a `cubin` or `PTX` or `fatbin` file as a NULL-terminated
    text string, or incorporating a `cubin` or `fatbin` object into the executable
    resources and using operating system calls such as Windows `FindResource()` to
    obtain the pointer. Options are passed as an array via options, and any corresponding
    parameters are passed in `optionValues`. The number of total options is specified
    by `numOptions`. Any outputs will be returned via `optionValues`. Supported options
    are given in [Table 4.9](ch04.html#ch04tab09).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab09.jpg)![Image](graphics/04tab09a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.9.* Options for `cuModuleLoadDataEx()`'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. `cuobjdump`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`cuobjdump` is the utility that may be used to examine the binaries generated
    by CUDA. In particular, it is useful for examining the microcode generated by
    `nvcc.` Specify the `--cubin` parameter to `nvcc` to generate a `.cubin` file,
    and then use'
  prefs: []
  type: TYPE_NORMAL
- en: cuobjdump --dump-sass <filename.cubin>
  prefs: []
  type: TYPE_NORMAL
- en: to dump the disassembled microcode from the `.cubin` file. The complete list
    of command-line options for `cuobjdump` is given in [Table 4.10](ch04.html#ch04tab10).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab10.jpg)![Image](graphics/04tab10a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.10* `cuobjdump` Command-Line Options'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. `nvidia-smi`
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`nvidia-smi`, the NVIDIA System Management Interface, is used to manage the
    environment in which Tesla-class NVIDIA GPU boards operate. It can report GPU
    status and control aspects of GPU execution, such as whether ECC is enabled and
    how many CUDA contexts can be created on a given GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: When `nvidia-smi` is invoked with the `-–help (-h)` option, it generates a usage
    message that, besides giving a brief description of its purpose and command-line
    options, also gives a list of supported products. Tesla- and Quadro-branded GPUs
    are fully supported, while GeForce-branded GPUs get limited support.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the GPU boards supported by `nvidia-smi` include multiple GPUs; `nvidia-smi`
    refers to these boards as *units*. Some operations, such as toggling the status
    of an LED (light emitting diode), are available only on a per-unit basis.
  prefs: []
  type: TYPE_NORMAL
- en: '`nvidia-smi` has several modes of operation. If no other command-line parameters
    are given, it lists a summary of available GPUs that can be refined by the command-line
    options in [Table 4.11](ch04.html#ch04tab11). Otherwise, the other command-line
    options that are available include the following.'
  prefs: []
  type: TYPE_NORMAL
- en: '• List: The `--list-gpus (-L)` option displays a list of available GPUs and
    their UUIDs. Additional options to refine the listing are summarized in [Table
    4.11](ch04.html#ch04tab11).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.11* `nvidia-smi` List Options'
  prefs: []
  type: TYPE_NORMAL
- en: '• Query: The `--query (-q)` option displays GPU or unit information. Additional
    options to refine the query are summarized in [Table 4.12](ch04.html#ch04tab12).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.12* `nvidia-smi` Query Options'
  prefs: []
  type: TYPE_NORMAL
- en: '• Document Type Definition (DTD): The `--dtd` option produces the Document
    Type Definition for the XML-formatted output of `nvidia-smi`. The `--filename
    (-f)` option optionally specifies an output file; the `--unit (-u)` option causes
    the DTD for GPU boards (as opposed to GPUs) to be written.'
  prefs: []
  type: TYPE_NORMAL
- en: '• Device modification: The options specified in [Table 4.13](ch04.html#ch04tab13)
    may be used to set the persistent state of the GPU, such as whether ECC (error
    correction) is enabled.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04tab13.jpg)![Image](graphics/04tab13a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 4.13* `nvidia-smi` Device Modification Options'
  prefs: []
  type: TYPE_NORMAL
- en: '• Unit modification: The `--toggle-led` option (`-t`) may be set to `0/GREEN`
    or `1/AMBER`. The `--id (-i)` option can be used to target a specific unit.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5\. Amazon Web Services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Amazon Web Services is the preeminent vendor of “infrastructure as a service”
    (IAAS) cloud computing services. Their Web services enable customers to allocate
    storage, transfer data to and from their data centers, and run servers in their
    data centers. In turn, customers are charged for the privilege on an a la carte
    basis: per byte of storage, per byte transferred, or per instance-hour of server
    time. On the one hand, customers can access potentially unlimited compute resources
    without having to invest in their own infrastructure, and on the other hand, they
    need only pay for the resources they use. Due to the flexibility and the cloud’s
    ability to accommodate rapidly increasing demand (say, if an independent game
    developer’s game “goes viral”), cloud computing is rapidly increasing in popularity.'
  prefs: []
  type: TYPE_NORMAL
- en: A full description of the features of AWS and how to use them is outside the
    scope of this book. Here we cover some salient features for those who are interested
    in test-driving CUDA-capable virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: • S3 (Simple Storage Service) objects can be uploaded and downloaded.
  prefs: []
  type: TYPE_NORMAL
- en: • EC2 (Elastic Compute Cloud) instances can be launched, rebooted, and terminated.
  prefs: []
  type: TYPE_NORMAL
- en: • EBS (Elastic Block Storage) volumes can be created, copied, attached to EC2
    instances, and destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: • It features security groups, which are analogous to firewalls for EC2 instances.
  prefs: []
  type: TYPE_NORMAL
- en: • It features key pairs, which are used for authentication.
  prefs: []
  type: TYPE_NORMAL
- en: All of the functionality of Amazon Web Services is accessible via the AWS Management
    Console, accessible via aws.amazon.com. The AWS Management Console can do many
    tasks not listed above, but the preceding handful of operations are all we’ll
    need in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1\. Command-Line Tools
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The AWS command-line tools can be downloaded from `[http://aws.amazon.com/developertools](http://aws.amazon.com/developertools)`.
    Look for “Amazon EC2 API Tools.” These tools can be used out of the box on Linux
    machines; Windows users can install Cygwin. Once installed, you can use commands
    such as `ec2-run-instances` to launch EC2 instances, `ec2-describe-instances`
    to give a list of running instances, or `ec2-terminate-instances` to terminate
    a list of instances. Anything that can be done in the Management Console also
    can be done using a command-line tool.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2\. EC2 and Virtualization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EC2, the “Elastic Compute Cloud,” is the member of the AWS family that enables
    customers to “rent” a CUDA-capable server for a period of time and be charged
    only for the time the server was in use. These virtual computers, which look to
    the customer like standalone servers, are called *instances*. Customers can use
    EC2’s Web services to launch, reboot, and terminate instances according to their
    need for the instances’ computing resources.
  prefs: []
  type: TYPE_NORMAL
- en: One of the enabling technologies for EC2 is *virtualization*, which enables
    a single server to host multiple “guest” operating systems concurrently. A single
    server in the EC2 fleet potentially can host several customers’ running instances,
    improving the economies of scale and driving down costs. Different instance types
    have different characteristics and pricing. They may have different amounts of
    RAM, CPU power,^([1](ch04.html#ch04fn1)) local storage, and I/O performance, and
    the on-demand pricing may range from $0.085 to $2.40 per instance-hour. As of
    this writing, the CUDA-capable `cg1.4xlarge` instance type costs $2.10 per instance-hour
    and has the following characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch04.html#ch04fn1a). The CPU capabilities are measured in EC2 Compute Units
    (ECUs). As of this writing, the ECUs available from a given instance range from
    1 (in the `m1.small` instance type) to 88.5 (in the `cc2.8xlarge` instance type).'
  prefs: []
  type: TYPE_NORMAL
- en: • 23 GB of RAM
  prefs: []
  type: TYPE_NORMAL
- en: • 33.5 ECUs (two quad-core Intel Xeon X5570 “Nehalem” CPUs)
  prefs: []
  type: TYPE_NORMAL
- en: • 1690 GB of instance storage
  prefs: []
  type: TYPE_NORMAL
- en: • 64-bit platform
  prefs: []
  type: TYPE_NORMAL
- en: Since `cg1.4xlarge` is a member of the “cluster” instance family, only a single
    instance will run on a given server; also, it is plugged into a much higher bandwidth
    network than other EC2 instance types to enable cluster computing for parallel
    workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.3\. Key Pairs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Access to EC2 instances is facilitated by *key pairs*. The term refers to the
    central concept in public key cryptography that the authentication is performed
    using a private key (available only to those who are authorized) and a public
    key that can be freely shared.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a key pair is created, the private key is downloaded in the form of a
    `.pem` file. There are two reasons to keep careful track of a `.pem` file after
    creating a key pair: First, anyone with access to the `.pem` file can use it to
    gain access to your EC2 computing resources, and second, there is no way to obtain
    new copies of the private key! Amazon is not in the key retention business, so
    once the private key is downloaded, it is yours to keep track of.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 4.1](ch04.html#ch04lis01) gives an example `.pem` file. The format
    is convenient because it has anchor lines (the `“BEGIN RSA PRIVATE KEY”/ “END
    RSA PRIVATE KEY”`) and is “7-bit clean” (i.e., only uses ASCII text characters),
    so it can be emailed, copied-and-pasted into text fields, appended to files such
    as `~/.ssh/authorized_keys` to enable password-less login, or published in books.
    The name for a given key pair is specified when launching an EC2 instance; in
    turn, the corresponding private key file is used to gain access to that instance.
    To see more specifically how the `.pem` file is used to access EC2 instances,
    see the sections on Linux and Windows below.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 4.1.* Example `.pem` file.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch04_images.html#p04lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '-----BEGIN RSA PRIVATE KEY-----'
  prefs: []
  type: TYPE_NORMAL
- en: MIIEowIBAAKCAQEA2mHaXk9tTZqN7ZiUWoxhcSHjVCbHmn1SKamXqOKdLDfmqducvVkAlB1cjIz/
  prefs: []
  type: TYPE_NORMAL
- en: NcwIHk0TxbnEPEDyPPHg8RYGya34evswzBUCOIcilbVIpVCyaTyzo4k0WKPW8znXJzQpxr/OHzzu
  prefs: []
  type: TYPE_NORMAL
- en: tAvlq95HGoBobuGM5kaDSlkugOmTUXFKxZ4ZN1rm2kUo21N2m9jrkDDq4qTMFxuYW0H0AXeHOfNF
  prefs: []
  type: TYPE_NORMAL
- en: ImroUCN2udTWOjpdgIPCgYEzz3Cssd9QIzDyadw+wbkTYq7eeqTNKULs4/gmLIAw+EXKE2/seyBL
  prefs: []
  type: TYPE_NORMAL
- en: leQeK11j1TFhDCjYRfghp0ecv4UnpAtiO6nNzod7aTAR1bXqJXbSqwIDAQABAoIBAAh2umvlUCst
  prefs: []
  type: TYPE_NORMAL
- en: zkpjG3zW6//ifFkKl7nZGZIbzJDzF3xbPklfBZghFvCmoquf21ROcBIckqObK4vaSIksJrexTtoK
  prefs: []
  type: TYPE_NORMAL
- en: MBM0IRQHzGo8co6y0/n0QrXpcFzqOGknEHGk0D3ou6XEUUzMo8O+okwi9UaFq4aAn2FdYkFDa5X7
  prefs: []
  type: TYPE_NORMAL
- en: d4Y0id1WzPcVurOSrnFNkWl4GRu+pluD2bmSmb7RUxQWGbP7bf98EyhpdugOdO7R3yOCcdaaGg0L
  prefs: []
  type: TYPE_NORMAL
- en: hdTlwJ3jCP9dmnk7NqApRzkv7R1sXzOnU2v3b9+WpF0g6wCeM2eUuK1IY3BPl0Pg+Q4xU0jpRSr0
  prefs: []
  type: TYPE_NORMAL
- en: vLDt8fUcIdH4PXTKua1NxsBA1uECgYEA72wC3BmL7HMIgf33yvK+/yA1z6AsAvIIAlCHJOi9sihT
  prefs: []
  type: TYPE_NORMAL
- en: XF6dnfaJ6d12oCj1RUqG9e9Y3cW1YjgcdqQBk5F8M6bPuIfzOctM/urd1ryWZ3ddSxgBaLEO1h4c
  prefs: []
  type: TYPE_NORMAL
- en: 3/cQWGGvaMPpDSAihs2d/CnnlVoQGiQrlWxDGzIHzu8RRV43fKcCgYEA6YDkj6kzlx4cuQwwsPVb
  prefs: []
  type: TYPE_NORMAL
- en: IfdtP6WrHe+Ro724ka3Ry+4xFPcarXj5yl5/aPHNpdPPCfR+uYNjBiTD90w+duV8LtBxJoF+i/lt
  prefs: []
  type: TYPE_NORMAL
- en: Mui4116xXMBaMGQfFMS0u2+z3aZI8MXZF8gGDIrI9VVfpDCi2RNKaT7KhfraZ8VzZsdAqDO8Zl0C
  prefs: []
  type: TYPE_NORMAL
- en: gYEAvVq3iEvMFl2ERQsPhzslQ7G93U/Yfxvcqbf2qoJIRTcPduZ90gjCWmwE/fZmxT6ELs31grBz
  prefs: []
  type: TYPE_NORMAL
- en: HBM0r8BWXteZW2B6uH8NJpBbfOFUQhk0+u+0oUeDFcGy8jUusRM9oijgCgOntfHMXMESSfT6a2yn
  prefs: []
  type: TYPE_NORMAL
- en: f4VL0wmkqUWQV2FMT4iMadECgYATFUGYrA9XTlKynNht3d9wyzPWe8ecTrPsWdj3rujybaj9OaSo
  prefs: []
  type: TYPE_NORMAL
- en: gLaJX2eyP/C6mLDW83BX4PD6045ga46/UMnxWX+l0fdxoRTXkEVq9IYyOlYklkoj/F944gwlFS3o
  prefs: []
  type: TYPE_NORMAL
- en: 34J6exJjfAQoaK3EUWU9sGHocAVFJdcrm+tufuI93NyM0QKBgB+koBIkJG8u0f19oW1dhUWERsuo
  prefs: []
  type: TYPE_NORMAL
- en: poXZ9Kh/GvJ9u5DUwv6F+hCGRotdBFhjuwKNTbutdzElxDMNHKoy/rhiqgcneMUmyHh/F0U4sOWl
  prefs: []
  type: TYPE_NORMAL
- en: XqqMD2QfKXBAU0ttviPbsmm0dbjzTTd3FO1qx2K90T3u9GEUdWYqMxOyZjUoLyNr+Tar
  prefs: []
  type: TYPE_NORMAL
- en: '-----END RSA PRIVATE KEY-----'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.4\. Availability Zones (AZs) and Regions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AWS provides its services in separate Availability Zones (AZs) that are carefully
    segregated from one another, with the intention of preventing outages affecting
    one Availability Zone from affecting any other Availability Zone. For CUDA developers,
    the main consideration to bear in mind is that instances, EBS volumes, and other
    resources must be in the same AZ to interoperate.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.5\. S3
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: S3 (Simple Storage Service) is designed to be a reliable way to store data for
    later retrieval. The “objects” (basically files) are stored in a hierarchical
    layout consisting of “buckets” at the top of the hierarchy, with optional intervening
    “folders.”
  prefs: []
  type: TYPE_NORMAL
- en: Besides storage and retrieval (“PUT” and “GET,” respectively), S3 includes the
    following features.
  prefs: []
  type: TYPE_NORMAL
- en: • Permissions control. By default, S3 objects are accessible only to the owner
    of the S3 account, but they may be made public or permissions can be granted to
    specific AWS accounts.
  prefs: []
  type: TYPE_NORMAL
- en: • Objects may be encrypted.
  prefs: []
  type: TYPE_NORMAL
- en: • Metadata may be associated with an S3 object, such as the language of a text
    file or whether the object is an image.
  prefs: []
  type: TYPE_NORMAL
- en: '• Logging: Operations performed on S3 objects can be logged (and the logs are
    stored as more S3 objects).'
  prefs: []
  type: TYPE_NORMAL
- en: '• Reduced Redundancy: S3 objects can be stored with a lower reliability factor,
    for a lower price.'
  prefs: []
  type: TYPE_NORMAL
- en: '• Notifications: Automatic notifications can be set up, to, for example, let
    the customer know if loss of a Reduced Redundancy object is detected.'
  prefs: []
  type: TYPE_NORMAL
- en: '• Object lifetime management: Objects can be scheduled to be deleted automatically
    after a specified period of time.'
  prefs: []
  type: TYPE_NORMAL
- en: Many other AWS services use S3 as a persistent data store; for example, snapshots
    of AMIs and EBS volumes are stored in S3.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.6\. EBS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EBS (Elastic Block Storage) consists of network-based storage that can be allocated
    and attached and detached to running instances. AWS customers also can “snapshot”
    EBS volumes, creating templates for new EBS volumes.
  prefs: []
  type: TYPE_NORMAL
- en: EC2 instances often have a root EBS volume that contains the operating system
    and driver software. If more storage is desired, you can create and attach an
    EBS volume and mount it within the guest operating system.^([2](ch04.html#ch04fn2))
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch04.html#ch04fn2a). You may have to change the OS configuration if you
    want the EBS volume to be available to instances launched from a derivative AMI;
    see the “[Linux on EC2](ch04.html#ch04lev2sec8)” or “[Windows on EC2](ch04.html#ch04lev2sec9)”
    sections in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.7\. AMIs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Amazon Machine Images (AMIs) are descriptions of what an EC2 instance would
    “look like” once launched, including the operating system and the number and contents
    of attached EBS volumes. Most EC2 customers start with a “stock” AMI provided
    by Amazon, modify it to their satisfaction, and then take a snapshot of the AMI
    so they can launch more instances with the same setup.
  prefs: []
  type: TYPE_NORMAL
- en: When an instance is launched, EC2 will take a few minutes to muster the requested
    resources and boot the virtual machine. Once the instance is running, you can
    query its IP address and access it over the Internet using the private key whose
    name was specified at instance launch time.
  prefs: []
  type: TYPE_NORMAL
- en: The external IP address of the instance is incorporated into the DNS name. For
    example, a `cg1.4xlarge` instance might be named
  prefs: []
  type: TYPE_NORMAL
- en: ec2-70-17-16-35.compute-1.amazonaws.com
  prefs: []
  type: TYPE_NORMAL
- en: and the external IP address of that machine is `70.17.16.35.`^([3](ch04.html#ch04fn3))
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch04.html#ch04fn3a). IP addresses have been changed to protect the innocent.'
  prefs: []
  type: TYPE_NORMAL
- en: EC2 instances also have internal IP addresses that can be used for intracluster
    communication. If, for example, you launch a cluster of instances that need to
    communicate using software such as the Message Passing Interface (MPI), use the
    internal IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.8\. Linux on EC2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: EC2 supports many different flavors of Linux, including an Amazon-branded flavor
    (“Amazon Linux”) that is derived from Red Hat. Once an instance is launched, it
    may be accessed via `ssh` using the key pair that was used to launch the instance.
    Using the IP address above and the `Example.pem` file in [Listing 4.1](ch04.html#ch04lis01),
    we might type
  prefs: []
  type: TYPE_NORMAL
- en: ssh –i Example.pem ec2-user@70.17.16.35
  prefs: []
  type: TYPE_NORMAL
- en: '(The root username varies with the flavor of Linux: `ec2-user` is the root
    username for Amazon Linux, while CentOS uses `root` and Ubuntu uses `ubuntu`.)'
  prefs: []
  type: TYPE_NORMAL
- en: Once logged in, the machine is all yours! You can add users and set their passwords,
    set up SSH for password-less login, install needed software (such as the CUDA
    toolchain), attach more EBS volumes, and set up the ephemeral disks. You can then
    snapshot an AMI to be able to launch more instances that look exactly like the
    one you’ve set up.
  prefs: []
  type: TYPE_NORMAL
- en: EBS
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: EBS (Elastic Block Storage) volumes are easy to create, either from a blank
    volume or by making a live copy of a snapshot. Once created, the EBS volume may
    be attached to an instance, where it will appear as a device (such as `/dev/sdf`
    or, on more recent Linux kernels, `/dev/xvdf`). When the EBS volume is first attached,
    it is just a raw block storage device that must be formatted before use using
    a command such as `mkfs.ext3`. Once formatted, the drive may be mounted to a directory.
  prefs: []
  type: TYPE_NORMAL
- en: mount <Device> <Directory>
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you want to snapshot an AMI and for the drive to be visible on instances
    launched using the derivative AMI, edit `/etc/fstab` to include the volume. When
    creating an EBS volume to attach to a running instance, make sure to create it
    in the same Availability Zone (e.g., us-east-1b) as the instance.
  prefs: []
  type: TYPE_NORMAL
- en: Ephemeral Storage
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Many EC2 instance types, including `cg1.4xlarge`, have local hard disks associated
    with them. These disks, when available, are used strictly for scratch local storage;
    unlike EBS or S3, no erasure encoding or other technologies are employed to make
    the disks appear more reliable. To emphasize this reduced reliability, the disks
    are referred to as *ephemeral storage*.
  prefs: []
  type: TYPE_NORMAL
- en: To make ephemeral disks available, specify the “-b” option to ec2-run-instances—for
    example,
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch04_images.html#p115pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: ec2-run-instances –t cg1.4xlarge –k nwiltEC2 –b /dev/sdb=ephemeral0
  prefs: []
  type: TYPE_NORMAL
- en: /dev/sdc=ephemeral1
  prefs: []
  type: TYPE_NORMAL
- en: Like EBS volumes, ephemerals must be formatted (e.g., mkfs.ext3) and mounted
    before they can be used, and they must have `fstab` entries in order to reappear
    when the instance is rebooted.
  prefs: []
  type: TYPE_NORMAL
- en: User Data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: User data may be specified to an instance, either at launch time or while an
    instance is running (in which case the instance must be rebooted). The user data
    then may be queried at
  prefs: []
  type: TYPE_NORMAL
- en: '[http://169.254.169.254/latest/user-data](http://169.254.169.254/latest/user-data)'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.9\. Windows on EC2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Windows instances are accessed in a slightly different way than Linux instances.
    Once launched, customers must use their private key file to retrieve the password
    for the EC2 instance’s Administrator account. You can either specify your `.pem`
    file or copy-and-paste its contents into the AWS Management Console (shown in
    [Figure 4.1](ch04.html#ch04fig01)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4.1* AWS Windows password retrieval.'
  prefs: []
  type: TYPE_NORMAL
- en: By default, this password-generation behavior is only in force on “stock” AMIs
    from AWS. If you “snapshot” one of these AMIs, they will retain whatever passwords
    were present on the machine when the snapshot was taken. To create a new Windows
    AMI that generates a random password upon launch, run the “EC2 Config Service”
    tool (available in the Start menu), click the “Bundle” tab, and click the button
    that says “Run Sysprep and Shutdown Now” ([Figure 4.2](ch04.html#ch04fig02)).
    After clicking this button, any AMI created against it will generate a random
    password, like the stock Windows AMIs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/04fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 4.2* Sysprep for Windows on EC2.'
  prefs: []
  type: TYPE_NORMAL
- en: Ephemeral Storage
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In order for ephemeral storage to be useable by a Windows instance, you must
    specify the –b option to `ec2-run-instances`, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch04_images.html#p116pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: ec2-run-instances –t cg1.4xlarge –k nwiltEC2 –b /dev/sdb=ephemeral0
  prefs: []
  type: TYPE_NORMAL
- en: /dev/sdc=ephemeral1
  prefs: []
  type: TYPE_NORMAL
- en: User Data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: User data may be specified to an instance, either at launch time or while an
    instance is running (in which case the instance must be rebooted). The user data
    then may be queried at
  prefs: []
  type: TYPE_NORMAL
- en: '[http://169.254.169.254/latest/user-data](http://169.254.169.254/latest/user-data)'
  prefs: []
  type: TYPE_NORMAL
