- en: '26'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '26'
- en: CLASSIFICATION METHODS
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类方法
- en: The most common application of supervised machine learning is building classification
    models. A **classification model**, or classifier, is used to label an example
    as belonging to one of a finite set of categories. Deciding whether an email message
    is spam, for example, is a classification problem. In the literature, these categories
    are typically called **classes** (hence the name classification). Equivalently,
    we can describe an example as belonging to a class or as having a **label**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习最常见的应用是构建分类模型。**分类模型**或分类器用于将示例标记为属于有限类别集中的一个。例如，判断一封电子邮件是否为垃圾邮件就是一个分类问题。在文献中，这些类别通常称为**类**（因此得名分类）。我们也可以将一个示例描述为属于某个类或具有**标签**。
- en: In **one-class** **learning**, the training set contains examples drawn from
    only one class. The goal is to learn a model that predicts whether an example
    belongs to that class. One-class learning is useful when it is difficult to find
    training examples that lie outside the class. One-class learning is frequently
    used for building anomaly detectors, e.g., detecting previously unseen kinds of
    attacks on a computer network.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在**单类学习**中，训练集仅包含来自一个类的示例。目标是学习一个模型，预测示例是否属于该类。当很难找到不在该类之外的训练示例时，单类学习非常有用。单类学习常用于构建异常检测器，例如，检测计算机网络上以前未见过的攻击类型。
- en: In **two-class learning** (often called **binary classification**), the training
    set contains examples drawn from exactly two classes (typically called positive
    and negative), and the objective is to find a boundary that separates the two
    classes. **Multi-class learning** involves finding boundaries that separate more
    than two classes from each other.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在**二分类学习**（通常称为**二元分类**）中，训练集包含来自正负两个类的示例，目标是找到一个边界将这两个类分开。**多类学习**涉及找到将多个类别彼此分开的边界。
- en: 'In this chapter, we look at two widely used supervised learning methods for
    solving classification problems: k-nearest neighbors and regression. Before we
    do, we address the question of how to evaluate the classifiers produced by these
    methods.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究两种广泛使用的监督学习方法来解决分类问题：k-近邻和回归。在此之前，我们先讨论如何评估这些方法产生的分类器的问题。
- en: The code in this chapter assumes the import statements
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码假设了导入语句
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 26.1 Evaluating Classifiers
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.1 评估分类器
- en: Those of you who read Chapter 20 might recall that part of that chapter addressed
    the question of choosing a degree for a linear regression that would 1) provide
    a reasonably good fit for the available data, and 2) have a reasonable chance
    of making good predictions about as yet unseen data. The same issues arise when
    using supervised machine learning to train a classifier.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读过第20章的朋友可能会记得，该章的一部分讨论了选择线性回归的次数的问题，1) 提供对可用数据的合理拟合，2) 有合理的机会对尚未见过的数据做出良好的预测。使用监督机器学习训练分类器时同样会出现这些问题。
- en: We start by dividing our data into two sets, a training set and a **test set**.
    The training set is used to learn a model, and the test set is used to evaluate
    that model. When we train the classifier, we attempt to minimize **training error**,
    i.e., errors in classifying the examples in the training set, subject to certain
    constraints. The constraints are designed to increase the probability that the
    model will perform reasonably well on as yet unseen data. Let's look at this pictorially.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将数据分为两个集合，一个训练集和一个**测试集**。训练集用于学习模型，测试集用于评估该模型。当我们训练分类器时，我们试图最小化**训练误差**，即在训练集中对示例分类时的错误，前提是满足某些约束。这些约束旨在提高模型在尚未见过的数据上表现良好的概率。让我们用图示来看一下。
- en: The chart on the left of [Figure 26-1](#c26-fig-0001) shows a representation
    of voting patterns for 60 (simulated) American citizens. The x-axis is the distance
    of the voter's home from Boston, Massachusetts. The y-axis is the age of the voter.
    The stars indicate voters who usually vote Democratic, and the triangles voters
    who usually vote Republican. The chart on the right in [Figure 26-1](#c26-fig-0001)
    shows a training set containing a randomly chosen sample of 30 of those voters.
    The solid and dashed lines show two possible boundaries between the two populations.
    For the model based on the solid line, points below the line are classified as
    Democratic voters. For the model based on the dotted line, points to the left
    of the line are classified as Democratic voters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-1](#c26-fig-0001)左侧的图表显示了60位（模拟）美国公民的投票模式。x轴表示选民家距离马萨诸塞州波士顿的距离，y轴表示选民的年龄。星形标记表示通常投票给民主党的选民，三角形标记表示通常投票给共和党的选民。右侧的图表在[图26-1](#c26-fig-0001)中展示了包含随机选择的30位选民的训练集。实线和虚线表示两个群体之间的两个可能边界。基于实线的模型中，线下的点被分类为民主党选民；基于虚线的模型中，线左侧的点被分类为民主党选民。'
- en: '![c26-fig-0001.jpg](../images/c26-fig-0001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0001.jpg](../images/c26-fig-0001.jpg)'
- en: '[Figure 26-1](#c26-fig-0001a) Plots of voter preferences'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-1](#c26-fig-0001a) 选民偏好的图'
- en: Neither boundary separates the training data perfectly. The training errors
    for the two models are shown in the **confusion matrices** in [Figure 26-2](#c26-fig-0002).
    The top-left corner of each shows the number of examples classified as Democratic
    that are actually Democratic, i.e., the true positives. The bottom-left corner
    shows the number of examples classified as Democratic that are actually Republican,
    i.e., the false positives. The righthand column shows the number of false negatives
    on the top and the number of true negatives on the bottom.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个边界都不能完美分隔训练数据。两个模型的训练错误显示在[图26-2](#c26-fig-0002)中的**混淆矩阵**里。每个矩阵的左上角显示被分类为民主党的例子的数量，这些例子实际上也是民主党，即真正的正例。左下角显示被分类为民主党的例子的数量，但这些例子实际上是共和党，即假阳性。右侧列显示了顶部的假阴性数量和底部的真正负例数量。
- en: '![c26-fig-0002.jpg](../images/c26-fig-0002.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0002.jpg](../images/c26-fig-0002.jpg)'
- en: '[Figure 26-2](#c26-fig-0002a) Confusion matrices'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-2](#c26-fig-0002a) 混淆矩阵'
- en: The **accuracy** of each classifier on the training data can be calculated as
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分类器在训练数据上的**准确性**可以计算为
- en: '![c26-fig-5001.jpg](../images/c26-fig-5001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-5001.jpg](../images/c26-fig-5001.jpg)'
- en: In this case, each classifier has an accuracy of `0.7`. Which does a better
    job of fitting the training data? It depends upon whether we are more concerned
    about misclassifying Republicans as Democrats, or vice versa.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每个分类器的准确率为`0.7`。哪个更好地拟合了训练数据？这取决于我们是否更关注将共和党选民错误分类为民主党选民，或反之亦然。
- en: If we are willing to draw a more complex boundary, we can get a classifier that
    does a more accurate job of classifying the training data. The classifier pictured
    in [Figure 26-3](#c26-fig-0004), for example, has an accuracy of about 0.83 on
    the training data, as depicted in the left plot of the figure. However, as we
    saw in our discussion of linear regression in Chapter 20, the more complicated
    the model, the higher the probability that it has been overfit to the training
    data. The righthand plot in [Figure 26-3](#c26-fig-0004) depicts what happens
    if we apply the complex model to the holdout set—the accuracy drops to `0.6`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意画出更复杂的边界，就可以得到一个更准确分类训练数据的分类器。例如，[图26-3](#c26-fig-0004)中所示的分类器在训练数据上的准确率约为0.83，如图的左侧图所示。然而，正如我们在第20章的线性回归讨论中看到的，模型越复杂，就越有可能出现对训练数据的过拟合。[图26-3](#c26-fig-0004)右侧的图展示了如果将复杂模型应用于保留集，会发生什么——准确率下降至`0.6`。
- en: '![c26-fig-0003.jpg](../images/c26-fig-0003.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0003.jpg](../images/c26-fig-0003.jpg)'
- en: '[Figure 26-3](#c26-fig-0004a) A more complex model'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-3](#c26-fig-0004a) 更复杂的模型'
- en: Accuracy is a reasonable way to evaluate a classifier when the two classes are
    of roughly equal size. It is a terrible way to evaluate a classifier when there
    is a large **class imbalance**. Imagine that you are charged with evaluating a
    classifier that predicts whether a person has a potentially fatal disease occurring
    in about `0.1%` of the population to be tested. Accuracy is not a particularly
    useful statistic, since `99.9%` accuracy can be attained by merely declaring all
    patients disease-free. That classifier might seem great to those charged with
    paying for the treatment (nobody would get treated!), but it might not seem so
    great to those worried that they might have the disease.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个类别的大小大致相等时，准确度是一种合理的评估分类器的方法。但在类别严重**不平衡**的情况下，准确度是一种糟糕的评估方式。想象一下，你被指派评估一个分类器，该分类器预测某种潜在致命疾病，该疾病在大约
    `0.1%` 的待测人群中出现。准确度并不是一个特别有用的统计数据，因为仅仅通过宣称所有患者无病，就可以达到 `99.9%` 的准确率。对那些负责支付治疗费用的人来说，该分类器可能看起来很好（没人会接受治疗！），但对于那些担心自己可能患有该疾病的人来说，这个分类器可能看起来就没那么好了。
- en: 'Fortunately, there are statistics about classifiers that shed light when classes
    are imbalanced:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有关分类器的统计数据能够在类别不平衡时提供洞见：
- en: '![c26-fig-5002.jpg](../images/c26-fig-5002.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-5002.jpg](../images/c26-fig-5002.jpg)'
- en: '![c26-fig-5003.jpg](../images/c26-fig-5003.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-5003.jpg](../images/c26-fig-5003.jpg)'
- en: '![c26-fig-5004.jpg](../images/c26-fig-5004.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-5004.jpg](../images/c26-fig-5004.jpg)'
- en: '![c26-fig-5005.jpg](../images/c26-fig-5005.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-5005.jpg](../images/c26-fig-5005.jpg)'
- en: '**Sensitivity** (called **recall** in some fields) is the true positive rate,
    i.e., the proportion of positives that are correctly identified as such. **Specificity**
    is the true negative rate, i.e., the proportion of negatives that are correctly
    identified as such. **Positive predictive** **value** is the probability that
    an example classified as positive is truly positive. **Negative predictive** **value**
    is the probability that an example classified as negative is truly negative.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**灵敏度**（在某些领域称为 **召回率**）是真阳性率，即正确识别为阳性的比例。**特异性**是真阴性率，即正确识别为阴性的比例。**正预测值**是被分类为阳性的示例真正为阳性的概率。**负预测值**是被分类为阴性的示例真正为阴性的概率。'
- en: Implementations of these statistical measures and a function that uses them
    to generate some statistics are in [Figure 26-4](#c26-fig-0009). We will use these
    functions later in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些统计测量的实现以及一个使用它们生成一些统计数据的函数在 [图26-4](#c26-fig-0009) 中。我们将在本章稍后使用这些函数。
- en: '![c26-fig-0004.jpg](../images/c26-fig-0004.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0004.jpg](../images/c26-fig-0004.jpg)'
- en: '[Figure 26-4](#c26-fig-0009a) Functions for evaluating classifiers'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-4](#c26-fig-0009a) 评估分类器的函数'
- en: 26.2 Predicting the Gender of Runners
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.2 预测跑者的性别
- en: Earlier in this book, we used data from the Boston Marathon to illustrate a
    number of statistical concepts. We will now use the same data to illustrate the
    application of various classification methods. The task is to predict the gender
    of a runner given the runner's age and finishing time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书早些时候，我们使用波士顿马拉松的数据来说明许多统计概念。现在我们将使用相同的数据来说明各种分类方法的应用。任务是根据跑者的年龄和完成时间预测其性别。
- en: The function `build_marathon_examples` in [Figure 26-6](#c26-fig-0011) reads
    in the data from a CSV file of the form shown in [Figure 26-5](#c26-fig-0010),
    and then builds a set of examples. Each example is an instance of class `Runner`.
    Each runner has a label (gender) and a feature vector (age and finishing time).
    The only interesting method in `Runner` is `feature_dist`. It returns the Euclidean
    distance between the feature vectors of two runners.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `build_marathon_examples` 在 [图26-6](#c26-fig-0011) 中从如 [图26-5](#c26-fig-0010)
    所示格式的CSV文件中读取数据，然后构建一组示例。每个示例是类 `Runner` 的一个实例。每个跑者都有一个标签（性别）和一个特征向量（年龄和完成时间）。在
    `Runner` 中唯一有趣的方法是 `feature_dist`。它返回两个跑者特征向量之间的欧几里得距离。
- en: '![c26-fig-0005.jpg](../images/c26-fig-0005.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0005.jpg](../images/c26-fig-0005.jpg)'
- en: "[Figure 26-5](#c26-fig-0010a) First few lines of `\uFEFFbm_results2012.csv`"
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-5](#c26-fig-0010a) `bm_results2012.csv` 的前几行'
- en: The next step is to split the examples into a training set and a held-out test
    set. As is frequently done, we use `80%` of the data for training and test on
    the remaining `20%`. This is done using the function `divide_80_20` at the bottom
    of [Figure 26-6](#c26-fig-0011). Notice that we select the training data at random.
    It would have taken less code to simply select the first `80%` of the data, but
    that runs the risk of not being representative of the set as a whole. If the file
    had been sorted by finishing time, for example, we would get a training set biased
    towards the better runners.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将示例分成训练集和保留的测试集。像往常一样，我们使用`80%`的数据进行训练，使用剩下的`20%`进行测试。这是通过[图 26-6](#c26-fig-0011)底部的函数`divide_80_20`完成的。请注意，我们随机选择训练数据。简单地选择数据的前`80%`会减少代码量，但这样做有可能无法代表整个集合。例如，如果文件按完成时间排序，我们将得到一个偏向于更优秀跑者的训练集。
- en: We are now ready to look at different ways of using the training set to build
    a classifier that predicts the gender of a runner. Inspection reveals that `58%`
    of the runners in the training set are male. So, if we guess male all the time,
    we should expect an accuracy of `58%`. Keep this baseline in mind when looking
    at the performance of more sophisticated classification algorithms.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备看看使用训练集构建一个可以预测跑者性别的分类器的不同方法。检查发现，训练集中`58%`的跑者是男性。因此，如果我们总是猜测男性，我们应该期待`58%`的准确率。在查看更复杂分类算法的性能时，请牢记这一基线。
- en: '![c26-fig-0006.jpg](../images/c26-fig-0006.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0006.jpg](../images/c26-fig-0006.jpg)'
- en: '[Figure 26-6](#c26-fig-0011a) Build examples and divide data into training
    and test sets'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-6](#c26-fig-0011a) 构建示例并将数据分为训练集和测试集'
- en: 26.3 K-nearest Neighbors
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.3 K-最近邻
- en: '**K-nearest neighbors** (KNN) is probably the simplest of all classification
    algorithms. The “learned” model is simply the training examples themselves. New
    examples are assigned a label based on how similar they are to examples in the
    training data.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-最近邻**（KNN）可能是所有分类算法中最简单的一种。“学习”模型只是训练示例本身。新的示例会根据它们与训练数据中示例的相似度被分配标签。'
- en: Imagine that you and a friend are strolling through the park and spot a bird.
    You believe that it is a yellow-throated woodpecker, but your friend is pretty
    sure that it is a golden-green woodpecker. You rush home and dig out your cache
    of bird books (or, if you are under `35,` go to your favorite search engine) and
    start looking at labeled pictures of birds. Think of these labeled pictures as
    the training set. None of the pictures is an exact match for the bird you saw,
    so you settle for selecting the five that look the most like the bird you saw
    (the five “nearest neighbors”). The majority of them are photos of a yellow-throated
    woodpecker—you declare victory.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你和朋友在公园散步，发现了一只鸟。你认为它是一只黄喉啄木鸟，但你的朋友则很确定它是一只金绿色啄木鸟。你急忙回家，翻找出你的鸟类书籍（或者，如果你在`35岁`以下，去你喜欢的搜索引擎），开始查看标记好的鸟类图片。把这些标记好的图片看作训练集。没有一张图片完全匹配你看到的那只鸟，因此你选择了看起来最像你看到的鸟的五张（五个“最近邻”）。其中大多数是黄喉啄木鸟的照片——你宣布胜利。
- en: A weakness of KNN (and other) classifiers is that they often give poor results
    when the distribution of examples in the training data is different from that
    in the test data. If the frequency of pictures of bird species in the book is
    the same as the frequency of that species in your neighborhood, KNN will probably
    work well. Suppose, however, that despite the species being equally common in
    your neighborhood, your books contain 30 pictures of yellow-throated woodpeckers
    and only one of a golden-green woodpecker. If a simple majority vote is used to
    determine the classification, the yellow-throated woodpecker will be chosen even
    if the photos don't look much like the bird you saw. This problem can be partially
    mitigated by using a more complicated voting scheme in which the k-nearest neighbors
    are weighted based on their similarity to the example being classified.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: KNN（和其他）分类器的一个弱点是，当训练数据中的示例分布与测试数据中的分布不同时，它们往往会给出较差的结果。如果书中鸟类照片的频率与您所在社区中该物种的频率相同，KNN可能会表现良好。然而，假设尽管这种物种在你所在的社区同样常见，但你的书中有30张黄喉啄木鸟的照片，仅有一张金绿色啄木鸟的照片。如果使用简单的多数投票来确定分类，即使这些照片看起来与您看到的鸟不太相似，黄喉啄木鸟仍然会被选中。通过使用更复杂的投票方案，可以部分缓解这个问题，在这种方案中，k个最近邻会根据它们与待分类示例的相似度加权。
- en: The functions in [Figure 26-7](#c26-fig-0012) implement a k-nearest neighbors
    classifier that predicts the gender of a runner based on the runner's age and
    finishing time. The implementation is brute force. The function `find_k_nearest`
    is linear in the number of examples in `example_set`, since it computes the feature
    distance between `example` and each element in `example_set`. The function `k_nearest_classify`
    uses a simple majority-voting scheme to do the classification. The complexity
    of `k_nearest_classify` is `O(len(training)*len(test_set))`, since it calls the
    function `find_k_nearest` a total of `len(test_set)` times.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-7](#c26-fig-0012) 中的函数实现了一个 k 最近邻分类器，根据跑步者的年龄和完成时间来预测跑步者的性别。该实现是暴力破解。函数
    `find_k_nearest` 在 `example_set` 的示例数量上是线性的，因为它计算了 `example` 和 `example_set` 中每个元素之间的特征距离。函数
    `k_nearest_classify` 使用简单的多数投票机制进行分类。`k_nearest_classify` 的复杂度为 `O(len(training)*len(test_set))`，因为它总共调用
    `find_k_nearest` 函数 `len(test_set)` 次。'
- en: '![c26-fig-0007.jpg](../images/c26-fig-0007.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0007.jpg](../images/c26-fig-0007.jpg)'
- en: '[Figure 26-7](#c26-fig-0012a) Finding the k-nearest neighbors'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-7](#c26-fig-0012a) 查找 k 最近邻'
- en: When the code
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: was run, it printed
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，它打印了
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Should we be pleased that we can predict gender with `65%` accuracy given age
    and finishing time? One way to evaluate a classifier is to compare it to a classifier
    that doesn't even look at age and finishing time. The classifier in [Figure 26-8](#c26-fig-0013)
    first uses the examples in `training` to estimate the probability of a randomly
    chosen example in `test_set` being from class `label`. Using this prior probability,
    it then randomly assigns a label to each example in `test_set`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否应该感到高兴，因为在给定年龄和完成时间的情况下，我们可以以 `65%` 的准确率预测性别？评估分类器的一种方法是将其与一个甚至不考虑年龄和完成时间的分类器进行比较。[图
    26-8](#c26-fig-0013) 中的分类器首先使用 `training` 中的示例来估计在 `test_set` 中随机选择的示例属于 `label`
    类的概率。利用这个先验概率，它然后随机分配一个标签给 `test_set` 中的每个示例。
- en: '![c26-fig-0008.jpg](../images/c26-fig-0008.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0008.jpg](../images/c26-fig-0008.jpg)'
- en: '[Figure 26-8](#c26-fig-0013a) Prevalence-based classifier'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-8](#c26-fig-0013a) 基于普遍性的分类器'
- en: When we test `prevalence_classify` on the same Boston Marathon data on which
    we tested KNN, it prints
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在同样的波士顿马拉松数据上测试 `prevalence_classify` 时，它打印了
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: indicating that we are reaping a considerable advantage from considering age
    and finishing time.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表明我们在考虑年龄和完成时间时获得了相当大的优势。
- en: That advantage has a cost. If you run the code in [Figure 26-7](#c26-fig-0012),
    you will notice that it takes a rather long time to finish. There are `17,233`
    training examples and `4,308` test examples, so there are nearly `75` million
    distances calculated. This raises the question of whether we really need to use
    all of the training examples. Let's see what happens if we simply **downsample**
    the training data by a factor of `10`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这种优势是有代价的。如果你运行 [图 26-7](#c26-fig-0012) 中的代码，你会注意到它需要相当长的时间才能完成。训练示例有 `17,233`
    个，测试示例有 `4,308` 个，因此计算了近 `75` 百万的距离。这引发了一个问题：我们是否真的需要使用所有的训练示例。让我们看看如果我们简单地将训练数据按
    `10` 的比例 **下采样** 会发生什么。
- en: If we run
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'it completes in one-tenth the time, with little change in classification performance:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 它完成所需时间的一半，分类性能几乎没有变化：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In practice, when people apply KNN to large data sets, they often downsample
    the training data. An even more common alternative is to use some sort of fast
    approximate-KNN algorithm.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，当人们将 KNN 应用于大型数据集时，他们常常会对训练数据进行下采样。一个更常见的替代方法是使用某种快速近似 KNN 算法。
- en: In the above experiments, we set `k` to `9`. We did not choose this number for
    its role in science (the number of planets in our solar system),[^(198)](#c26-fn-0001)
    its religious significance (the number of forms of the Hindu goddess Durga), or
    its sociological importance (the number of hitters in a baseball lineup). Instead,
    we learned `k` from the training data by using the code in [Figure 26-9](#c26-fig-0014)
    to search for a good `k`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述实验中，我们将 `k` 设置为 `9`。我们并不是因为科学上的角色（我们太阳系中的行星数量），[^(198)](#c26-fn-0001)它的宗教意义（印度女神杜尔迦的形式数量），或它的社会学重要性（棒球阵容中的击球手数量）而选择这个数字。相反，我们通过使用
    [图 26-9](#c26-fig-0014) 中的代码从训练数据中学习了 `k`，以搜索一个合适的 `k`。
- en: The outer loop tests a sequence of values for `k`. We test only odd values to
    ensure that when the vote is taken in `k_nearest_classify`, there will always
    be a majority for one gender or the other.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 外层循环测试一系列 `k` 的值。我们只测试奇数值，以确保在 `k_nearest_classify` 中投票时，总会有一个性别占多数。
- en: The inner loop tests each value of k using **n-fold cross validation**. In each
    of the `num_folds` iterations of the loop, the original training set is split
    into a new training set/test set pair. We then compute the accuracy of classifying
    the new test set using k-nearest neighbors and the new training set. When we exit
    the inner loop, we calculate the average accuracy of the `num_folds` folds.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 内部循环使用**n折交叉验证**测试每个k值。在循环的每次`num_folds`迭代中，原始训练集被分成新的训练集/测试集对。然后，我们计算使用k近邻和新训练集对新测试集分类的准确度。当我们退出内部循环时，计算`num_folds`折的平均准确度。
- en: When we ran the code, it produced the plot in [Figure 26-10](#c26-fig-0015).
    As we can see, `17` was the value of `k` that led to the best accuracy across
    5 folds. Of course, there is no guarantee that some value larger than `21` might
    not have been even better. However, once `k` reached `9`, the accuracy fluctuated
    over a reasonably narrow range, so we chose to use `9`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，生成了[图 26-10](#c26-fig-0015)中的图。正如我们所见，`17`是导致5折交叉验证中最佳准确度的`k`值。当然，没有保证某个大于`21`的值会更好。然而，一旦`k`达到`9`，准确度就在合理的狭窄范围内波动，因此我们选择使用`9`。
- en: '![c26-fig-0009.jpg](../images/c26-fig-0009.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0009.jpg](../images/c26-fig-0009.jpg)'
- en: '[Figure 26-9](#c26-fig-0014a) Searching for a good k'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-9](#c26-fig-0014a) 寻找合适的k值'
- en: '![c26-fig-0010.jpg](../images/c26-fig-0010.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0010.jpg](../images/c26-fig-0010.jpg)'
- en: '[Figure 26-10](#c26-fig-0015a) Choosing a value for k'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-10](#c26-fig-0015a) 选择k值'
- en: 26.4 Regression-based Classifiers
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.4 基于回归的分类器
- en: In Chapter 20 we used linear regression to build models of data. We can try
    the same thing here and use the training data to build separate models for the
    men and the women. The plot in [Figure 25-11](#c26-fig-0016) was produced by the
    code in [Figure 26-12](#c26-fig-0017).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在第20章中，我们使用线性回归构建了数据模型。我们可以在这里尝试同样的做法，使用训练数据为男性和女性分别构建模型。[图 25-11](#c26-fig-0016)中的图是通过[图
    26-12](#c26-fig-0017)中的代码生成的。
- en: '![c26-fig-0011.jpg](../images/c26-fig-0011.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0011.jpg](../images/c26-fig-0011.jpg)'
- en: '[Figure 26-11](#c26-fig-0016a) Linear regression models for men and women'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-11](#c26-fig-0016a) 男性和女性的线性回归模型'
- en: '![c26-fig-0012.jpg](../images/c26-fig-0012.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0012.jpg](../images/c26-fig-0012.jpg)'
- en: '[Figure 26-12](#c26-fig-0017a) Produce and plot linear regression models'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-12](#c26-fig-0017a) 生成并绘制线性回归模型'
- en: A quick glance at [Figure 26-11](#c26-fig-0016) is enough to see that the linear
    regression models explain only a small amount of the variance in the data.[^(199)](#c26-fn-0002)
    Nevertheless, it is possible to use these models to build a classifier. Each model
    attempts to capture the relationship between age and finishing time. This relationship
    is different for men and women, a fact we can exploit in building a classifier.
    Given an example, we ask whether the relationship between age and finishing time
    is closer to the relationship predicted by the model for male runners (the solid
    line) or to the model for female runners (the dashed line). This idea is implemented
    in [Figure 26-13](#c26-fig-0018).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 看一眼[图 26-11](#c26-fig-0016)就足以看到，线性回归模型仅解释了数据中很小一部分的方差。[^(199)](#c26-fn-0002)
    尽管如此，可以利用这些模型构建分类器。每个模型试图捕捉年龄与完成时间之间的关系。这种关系对男性和女性是不同的，我们可以利用这一点来构建分类器。给定一个示例，我们会问年龄与完成时间之间的关系更接近男性跑者的模型（实线）还是女性跑者的模型（虚线）所预测的关系。这个想法在[图
    26-13](#c26-fig-0018)中得到了实现。
- en: When the code is run, it prints
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当代码运行时，它打印
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The results are better than random, but worse than for KNN.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结果比随机更好，但不如KNN。
- en: '![c26-fig-0013.jpg](../images/c26-fig-0013.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0013.jpg](../images/c26-fig-0013.jpg)'
- en: '[Figure 26-13](#c26-fig-0018a) Using linear regression to build a classifier'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-13](#c26-fig-0018a) 使用线性回归构建分类器'
- en: You might be wondering why we took this indirect approach to using linear regression,
    rather than explicitly building a model using some function of age and time as
    the dependent variable and real numbers (say `0` for female and `1` for male)
    as the dependent variable.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么我们采用这种间接的方法来使用线性回归，而不是明确使用年龄和时间的某种函数作为因变量，使用实际数字（比如`0`代表女性，`1`代表男性）作为自变量。
- en: We could easily build such a model using `polyfit` to map a function of age
    and time to a real number. However, what would it mean to predict that some runner
    is halfway between male and female? Were there some hermaphrodites in the race?
    Perhaps we can interpret the y-axis as the probability that a runner is male.
    Not really. There is not even a guarantee that applying `polyval` to the model
    will return a value between `0` and `1`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地使用`polyfit`构建这样的模型，将年龄和时间的函数映射到一个实数。然而，预测某个跑步者处于男性和女性之间的中间位置意味着什么呢？比赛中有双性人吗？也许我们可以将y轴解释为一个跑步者是男性的概率。其实并不是。甚至没有保证将`polyval`应用于模型会返回一个介于`0`和`1`之间的值。
- en: Fortunately, there is a form of regression, **logistic regression**,[^(200)](#c26-fn-0003)
    designed explicitly for predicting the probability of an event. The Python library
    `sklearn`[^(201)](#c26-fn-0004) provides a good implementation of logistic regression—and
    many other useful functions and classes related to machine learning.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一种回归形式，**逻辑回归**，[^(200)](#c26-fn-0003)专门设计用于预测事件的概率。Python库`sklearn`[^(201)](#c26-fn-0004)提供了良好的逻辑回归实现，以及与机器学习相关的许多其他有用函数和类。
- en: The module `sklearn.linear_model` contains the class `LogisticRegression`. The
    `__init__` method of this class has a large number of parameters that control
    things such as the optimization algorithm used to solve the regression equation.
    They all have default values, and on most occasions, it is fine to stick with
    those.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 模块`sklearn.linear_model`包含类`LogisticRegression`。该类的`__init__`方法有大量参数，用于控制解决回归方程所用的优化算法等。它们都有默认值，在大多数情况下，使用这些默认值是可以的。
- en: The central method of class `LogisticRegression` is `fit`. The method takes
    as arguments two sequences (tuples, lists, or arrays) of the same length. The
    first is a sequence of feature vectors and the second a sequence of the corresponding
    labels. In the literature, these labels are typically called **outcomes**.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`LogisticRegression`类的核心方法是`fit`。该方法接受两个相同长度的序列（元组、列表或数组）作为参数。第一个是特征向量的序列，第二个是相应标签的序列。在文献中，这些标签通常称为**结果**。'
- en: The `fit` method returns an object of type `LogisticRegression` for which coefficients
    have been learned for each feature in the feature vector. These coefficients,
    often called **feature weights**, capture the relationship between the feature
    and the outcome. A positive feature weight suggests a positive correlation between
    the feature and the outcome, and a negative feature weight suggests a negative
    correlation. The absolute magnitude of the weight is related to the strength of
    the correlation.[^(202)](#c26-fn-0005) The values of these weights can be accessed
    using the `coef_` attribute of `LogisticRegression`. Since it is possible to train
    a `LogisticRegression` object on multiple outcomes (called classes in the documentation
    for the package), the value of `coef_` is a sequence in which each element contains
    the sequence of weights associated with a single outcome. So, for example, the
    expression `model.coef_[1][0]` denotes the value of the coefficient of the first
    feature for the second outcome.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`方法返回一个类型为`LogisticRegression`的对象，该对象已为特征向量中的每个特征学习了系数。这些系数通常称为**特征权重**，捕捉了特征与结果之间的关系。正特征权重表明特征与结果之间存在正相关，而负特征权重表明负相关。权重的绝对值与相关性的强度有关。[^(202)](#c26-fn-0005)
    这些权重的值可以通过`LogisticRegression`的`coef_`属性访问。由于可以在多个结果（在包的文档中称为类）上训练`LogisticRegression`对象，因此`coef_`的值是一个序列，其中每个元素包含与单个结果相关的权重序列。例如，表达式`model.coef_[1][0]`表示第二个结果的第一个特征的系数值。'
- en: Once the coefficients have been learned, the method `predict_proba` of the `LogisticRegression`
    class can be used to predict the outcome associated with a feature vector. The
    method `predict_proba` takes a single argument (in addition to `self`), a sequence
    of feature vectors. It returns an array of arrays, one per feature vector. Each
    element in the returned array contains a prediction for the corresponding feature
    vector. The reason that the prediction is an array is that it contains a probability
    for each label used in building `model`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦学习了系数，就可以使用`LogisticRegression`类的`predict_proba`方法来预测与特征向量相关的结果。`predict_proba`方法接受一个参数（除了`self`），即特征向量的序列。它返回一个数组，其中每个特征向量对应一个数组。返回数组中的每个元素包含对应特征向量的预测。预测为数组的原因是它包含构建`model`时所用每个标签的概率。
- en: The code in [Figure 26-14](#c26-fig-0019) contains a simple illustration of
    how this all works. It first creates a list of `100,000` examples, each of which
    has a feature vector of length `3` and is labeled either `'A'`, `'B'`, `'C'`,
    or `‘D'`.The first two feature values for each example are drawn from a Gaussian
    with a standard deviation of `0.5`, but the means vary depending upon the label.
    The value of the third feature is chosen at random, and therefore should not be
    useful in predicting the label. After creating the examples, the code generates
    a logistic regression model, prints the feature weights, and finally the probabilities
    associated with four examples.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![c26-fig-0014.jpg](../images/c26-fig-0014.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: '[Figure 26-14](#c26-fig-0019a) Using `sklearn` to do multi-class logistic regression'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: When we ran the code in [Figure 26-14](#c26-fig-0019), it printed
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let's look first at the feature weights. The first line tells us that the first
    two features have roughly the same weight and are negatively correlated with the
    probability of an example having label `'A'`.[^(203)](#c26-fn-0006) That is, the
    larger the value of the first two features, the less likely that the example is
    of type `'A'`. The third feature, which we expect to have little value in predicting
    the label, has a small value relative to the other two values, indicating that
    it is relatively unimportant. The second line tells us that the probability of
    an example having the label `'B'` is negatively correlated with value of the first
    feature, but positively with the second feature. Again, the third feature has
    a relatively small value. The third and fourth lines are mirror images of the
    first two lines.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the probabilities associated with the four examples. The
    order of the probabilities corresponds to the order of the outcomes in the attribute
    `model.classes_`. As you would hope, when we predict the label associated with
    the feature vector `[0, 0]`, `'A'` has a very high probability and `'D'` a very
    low probability. Similarly, `[2, 2]` has a very high probability for `'D'` and
    a very low one for `'A'`. The probabilities associated with the middle two examples
    are also as expected.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: The example in [Figure 26-15](#c26-fig-0020) is similar to the one in [Figure
    26-14](#c26-fig-0019), except that we create examples of only two classes, `'A'`
    and `'D'`, and don't include the irrelevant third feature.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![c26-fig-0015.jpg](../images/c26-fig-0015.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: '[Figure 26-15](#c26-fig-0020a) Example of two-class logistic regression'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: When we run the code in [Figure 26-15](#c26-fig-0020), it prints
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice that there is only one set of weights in `coef_`. When `fit` is used
    to produce a model for a binary classifier, it only produces weights for one label.
    This is sufficient because once `proba` has calculated the probability of an example
    being in either of the classes, the probability of it being in the other class
    is determined—since the probabilities must add up to `1`. To which of the two
    labels do the weights in `coef_` correspond? Since the weights are positive, they
    must correspond to `'D'`, since we know that the larger the values in the feature
    vector, the more likely the example is of class `'D'`. Traditionally, binary classification
    uses the labels `0` and `1`, and the classifier uses the weights for `1`. In this
    case, `coef_` contains the weights associated with the largest label, as defined
    by the `>` operator for type `str`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`coef_`中只有一组权重。当使用`fit`为二元分类器生成模型时，它只为一个标签生成权重。这是足够的，因为一旦`proba`计算出某个示例属于任一类的概率，就可以确定它属于另一类的概率——因为这两者的概率之和必须为`1`。`coef_`中的权重对应于哪一个标签？由于权重是正的，它们必须对应于`'D'`，因为我们知道特征向量中的值越大，示例越可能属于`'D'`类。传统上，二元分类使用标签`0`和`1`，分类器使用`1`的权重。在这种情况下，`coef_`包含与最大标签相关联的权重，正如`str`类型的`>`运算符所定义的。
- en: 'Let''s return to the Boston Marathon example. The code in [Figure 26-16](#c26-fig-0021)
    uses the `LogisticRegression` class to build and test a model for our Boston Marathon
    data. The function `apply_model` takes four arguments:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到波士顿马拉松的例子。[图26-16](#c26-fig-0021)中的代码使用`LogisticRegression`类为我们的波士顿马拉松数据构建和测试模型。函数`apply_model`接受四个参数：
- en: '`model`: an object of type `LogisticRegression` for which a fit has been constructed'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`：一个`LogisticRegression`类型的对象，它已经构建了一个拟合模型。'
- en: '`test_set`: a sequence of examples. The examples have the same kinds of features
    and labels used in constructing the fit for `model`.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test_set`：一系列示例。这些示例具有与构建`model`拟合模型所使用的特征和标签相同的类型。'
- en: '`label`: The label of the positive class. The confusion matrix information
    returned by `apply_model` is relative to this label.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`label`：正类的标签。`apply_model`返回的混淆矩阵信息是相对于这个标签的。'
- en: '`prob`: the probability threshold to use in deciding which label to assign
    to an example in `test_set`. The default value is `0.5`. Because it is not a constant,
    `apply_model` can be used to investigate the tradeoff between false positives
    and false negatives.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`prob`：用于决定在`test_set`中将哪个标签分配给示例的概率阈值。默认值为`0.5`。由于它不是常量，`apply_model`可以用来研究假阳性和假阴性之间的权衡。'
- en: The implementation of `apply_model` first uses a list comprehension (Section
    5.3.2) to build a list whose elements are the feature vectors of the examples
    in `test_set`. It then calls `model.predict_proba` to get an array of pairs corresponding
    to the prediction for each feature vector. Finally, it compares the prediction
    against the label associated with the example with that feature vector, and keeps
    track of and returns the number of true positives, false positives, true negatives,
    and false negatives.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply_model`的实现首先使用列表推导（第5.3.2节）构建一个列表，其元素是`test_set`中示例的特征向量。然后它调用`model.predict_proba`获取与每个特征向量预测相对应的对的数组。最后，它将预测与与该特征向量相关的示例的标签进行比较，并跟踪并返回真正例、假正例、真负例和假负例的数量。'
- en: When we ran the code, it printed
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行代码时，它打印出：
- en: '[PRE9]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let''s compare these results to what we got when we used KNN:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些结果与我们使用KNN时获得的结果进行比较：
- en: '[PRE10]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The accuracies and positive predictive values are similar, but logistic regression
    has a much higher sensitivity and a much lower specificity. That makes the two
    methods hard to compare. We can address this problem by adjusting the probability
    threshold used by `apply_model` so that it has approximately the same sensitivity
    as KNN. We can find that probability by iterating over values of `prob` until
    we get a sensitivity close to that we got using KNN.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率和正预测值相似，但逻辑回归具有更高的敏感性和更低的特异性。这使得这两种方法难以比较。我们可以通过调整`apply_model`使用的概率阈值，使其具有与KNN大致相同的敏感性，从而解决这个问题。我们可以通过迭代`prob`的值，直到获得接近KNN的敏感性的概率。
- en: If we call `apply_model` with `prob = 0.578` instead of `0.5`, we get the results
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用`prob = 0.578`来调用`apply_model`而不是`0.5`，我们会得到以下结果。
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In other words, the models have similar performance.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这些模型的性能相似。
- en: '![c26-fig-0016.jpg](../images/c26-fig-0016.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0016.jpg](../images/c26-fig-0016.jpg)'
- en: '[Figure 26-16](#c26-fig-0021a) Use logistic regression to predict gender'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-16](#c26-fig-0021a) 使用逻辑回归预测性别'
- en: Since it can be complicated to explore the ramifications of changing the decision
    threshold for a logistic regression model, people often use something called the
    **receiver operating characteristic curve**,[^(204)](#c26-fn-0007) or **ROC curve**,
    to visualize the tradeoff between sensitivity and specificity. The curve plots
    the true positive rate (sensitivity) against the false positive rate (`1` – specificity)
    for multiple decision thresholds.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 由于探索改变逻辑回归模型的决策阈值的影响可能会很复杂，人们常常使用称为**接收器操作特征曲线**，[^(204)](#c26-fn-0007)或**ROC曲线**，来可视化敏感性和特异性之间的权衡。该曲线绘制了多个决策阈值下的真实正例率（敏感性）与假正例率（`1`
    – 特异性）的关系。
- en: ROC curves are often compared to one another by computing the area under the
    curve (**AUROC**, often abbreviated as **AUC**). This area is equal to the probability
    that the model will assign a higher probability of being positive to a randomly
    chosen positive example than to a randomly chosen negative example. This is known
    as the **discrimination** of the model. Keep in mind that discrimination says
    nothing about the accuracy, often called the **calibration**, of the probabilities.
    We could, for example, divide all of the estimated probabilities by `2` without
    changing the discrimination—but it would certainly change the accuracy of the
    estimates.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线通常通过计算曲线下的面积（**AUROC**，常缩写为**AUC**）来彼此比较。该面积等于模型将随机选择的正例分配更高的正概率的概率，相对于随机选择的负例。这被称为模型的**区分能力**。请记住，区分能力并不反映概率的准确性，通常称为**校准**。例如，我们可以将所有估计的概率除以`2`，而不会改变区分能力，但肯定会改变估计的准确性。
- en: The code in [Figure 26-17](#c26-fig-0022) plots the ROC curve for the logistic
    regression classifier as a solid line, [Figure 26-18](#c26-fig-0023). The dotted
    line is the ROC for a random classifier—a classifier that chooses the label randomly.
    We could have computed the AUROC by first interpolating (because we have only
    a discrete number of points) and then integrating the ROC curve, but we got lazy
    and simply called the function `sklearn.metrics.auc`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-17](#c26-fig-0022)中的代码将逻辑回归分类器的ROC曲线绘制为实线，[图26-18](#c26-fig-0023)。虚线是随机分类器的ROC——一个随机选择标签的分类器。我们本可以先插值（因为我们只有离散的点）然后积分ROC曲线来计算AUROC，但我们懒惰地直接调用了函数`sklearn.metrics.auc`。'
- en: '![c26-fig-0017.jpg](../images/c26-fig-0017.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0017.jpg](../images/c26-fig-0017.jpg)'
- en: '[Figure 26-17](#c26-fig-0022a) Construct ROC curve and find AUROC'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-17](#c26-fig-0022a) 构建ROC曲线并找到AUROC'
- en: '![c26-fig-0018.jpg](../images/c26-fig-0018.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0018.jpg](../images/c26-fig-0018.jpg)'
- en: '[Figure 26-18](#c26-fig-0023a) ROC curve and AUROC'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-18](#c26-fig-0023a) ROC曲线和AUROC'
- en: '**Finger exercise**: Write code to plot the ROC curve and compute the AUROC
    when the model built in [Figure 26-16](#c26-fig-0021) is tested on 200 randomly
    chosen competitors. Use that code to investigate the impact of the number of training
    examples (try varying it from `10` to `1010` in increments of `50`) on the AUROC.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '**指尖练习**：编写代码以绘制ROC曲线并计算在200名随机选择的竞争者上测试时所构建模型的AUROC。使用该代码调查训练样本数量对AUROC的影响（尝试从`10`变化到`1010`，增量为`50`）。'
- en: 26.5 Surviving the *Titanic*
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.5 生存于*泰坦尼克号*
- en: On the morning of April 15, 1912, the RMS *Titanic* hit an iceberg and sank
    in the North Atlantic. Of the roughly `1,300` passengers on board, `832` perished
    in the disaster. Many factors contributed to the disaster, including navigational
    error, inadequate lifeboats, and the slow response of a nearby ship. Whether individual
    passengers survived had an element of randomness, but was far from completely
    random. One interesting question is whether it is possible to build a reasonably
    good model for predicting survival using only information from the ship's passenger
    manifest.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在1912年4月15日的早晨，RMS *泰坦尼克号*撞上冰山并在北大西洋沉没。大约有`1,300`名乘客在船上，`832`人在这场灾难中遇难。许多因素导致了这场灾难，包括导航错误、救生艇不足以及附近船只反应缓慢。个别乘客的生存与否有随机因素，但远非完全随机。有一个有趣的问题是，是否可以仅通过船上乘客名单的信息建立一个合理的生存预测模型。
- en: 'In this section, we build a classification model from a CSV file containing
    information for `1046` passengers.[^(205)](#c26-fn-0008) Each line of the file
    contains information about a single passenger: cabin class (1st, 2nd, or 3rd),
    age, gender, whether the passenger survived the disaster, and the passenger''s
    name. The first few lines of the CSV file are'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从一个包含`1046`名乘客信息的CSV文件构建分类模型。[^(205)](#c26-fn-0008) 文件的每一行包含关于单个乘客的信息：舱位等级（1等、2等或3等）、年龄、性别、乘客是否在灾难中幸存以及乘客的姓名。CSV文件的前几行是
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Before building a model, it's probably a good idea take a quick look at the
    data with Pandas. Doing this often provides useful insights into the role various
    features might play in a model. Executing the code
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型之前，快速查看一下数据，可能是个好主意。这样做通常能提供有关各种特征在模型中可能发挥的作用的有用见解。执行代码
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: produces the correlation table
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 生成相关性表
- en: '[PRE14]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Why doesn't `Gender` appear in this table? Because it is not encoded as a number
    in the CSV file. Let's deal with that and see what the correlations look like.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么`Gender`没有出现在这个表中？因为它在CSV文件中没有编码为数字。我们来处理一下，看看相关性是什么样的。
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: produces
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 生成
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The negative correlations of `class` and `Gender` with `Survived` suggest that
    it might indeed be possible to build a predictive model using information in the
    manifest. (Because we have coded males as 1 and females as 0, the negative correlation
    of `Survived` and `Gender` tells us that women are more likely to have survived
    than men. Similarly, the negative correlation with `Class` tells us that it was
    safer to have been in first class.)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`class`和`Gender`与`Survived`之间的负相关性表明，确实有可能利用清单中的信息建立预测模型。（因为我们将男性编码为1，女性编码为0，`Survived`和`Gender`的负相关性告诉我们，女性比男性更可能生存。同样，`Class`的负相关性表明，头等舱的乘客更安全。）'
- en: Now, let's build a model using logistic regression. We chose to use logistic
    regression because
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用逻辑回归构建一个模型。我们选择使用逻辑回归是因为
- en: It is the most commonly used classification method.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是最常用的分类方法。
- en: By examining the weights produced by logistic regression, we can gain some insight
    into why some passengers were more likely to have survived than others.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过检查逻辑回归生成的权重，我们可以获得一些关于为什么某些乘客比其他乘客更可能生存的见解。
- en: '[Figure 26-19](#c26-fig-0024) defines class `Passenger`. The only thing of
    interest in this code is the encoding of cabin class. Though the CSV file encodes
    the cabin class as an integer, it is really shorthand for a category. Cabin classes
    do not behave like numbers, e.g., a first-class cabin plus a second-class cabin
    does not equal a third-class cabin. We encode cabin class using three binary features
    (one per possible cabin class). For each passenger, exactly one of these variables
    is set to `1`, and the other two are set to `0`.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[图26-19](#c26-fig-0024)定义了`Passenger`类。该代码中唯一感兴趣的地方是舱位等级的编码。虽然CSV文件将舱位等级编码为整数，但它实际上是类别的简写。舱位等级不像数字那样运作，例如，一个头等舱加一个二等舱并不等于一个三等舱。我们使用三个二进制特征（每种可能的舱位等级一个）对舱位等级进行编码。对于每位乘客，这三个变量中的一个被设置为`1`，其他两个被设置为`0`。'
- en: This is an example of an issue that frequently arises in machine learning. **Categorical**
    (sometimes called nominal) features are the natural way to describe many things,
    e.g., the home country of a runner. It's easy to replace these by integers, e.g.,
    we could choose a representation for countries based on their ISO 3166-1 numeric
    code,[^(206)](#c26-fn-0009) e.g., 076 for Brazil, 826 for the United Kingdom,
    and 862 for Venezuela. The problem with doing this is that the regression will
    treat these as numerical variables, thus using a nonsensical ordering on the countries
    in which Venezuela would be closer to the UK than it is to Brazil.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是机器学习中经常出现的问题的一个例子。**类别**特征（有时称为名义特征）是描述许多事物的自然方式，例如，跑步者的国家。用整数替换这些特征是很简单的，例如，我们可以根据国家的ISO
    3166-1数字代码来选择表示，例如，巴西为076，英国为826，委内瑞拉为862。这样做的问题在于，回归会将这些视为数值变量，从而对国家施加无意义的排序，导致委内瑞拉距离英国比距离巴西更近。
- en: This problem can be avoided by converting categorical variables to binary variables,
    as we did with cabin class. One potential problem with doing this is that it can
    lead to very long and sparse feature vectors. For example, if a hospital dispenses
    `2000` different drugs, we would convert one categorical variable into `2000`
    binary variables, one for each drug.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将分类变量转换为二元变量可以避免这个问题，就像我们处理舱位类时所做的那样。这样做的一个潜在问题是，它可能导致非常长且稀疏的特征向量。例如，如果一家医院配发
    `2000` 种不同的药物，我们将把一个分类变量转换为 `2000` 个二元变量，每种药物一个。
- en: '[Figure 26-20](#c26-fig-0025) contains code that uses Pandas to read the data
    from a file and build a set of examples from the data about the *Titanic*.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-20](#c26-fig-0025) 包含使用 Pandas 从文件中读取数据并根据 *泰坦尼克号* 数据构建示例集的代码。'
- en: Now that we have the data, we can build a logistic regression model using the
    same code we used to build a model of the Boston Marathon data. However, because
    the data set has a relatively small number of examples, we need to be concerned
    about using the evaluation method we employed earlier. It is entirely possible
    to get an unrepresentative `80-20` split of the data, and then generate misleading
    results.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据，可以使用构建波士顿马拉松数据模型时使用的相同代码构建逻辑回归模型。然而，由于数据集样本相对较少，我们需要关注使用之前采用的评估方法。完全有可能得到一个不具代表性的
    `80-20` 数据划分，然后生成误导性结果。
- en: To ameliorate the risk, we create many `80-20` splits (each split is created
    using the `divide_80_20` function defined in [Figure 26-6](#c26-fig-0011)), build
    and evaluate a classifier for each, and then report mean values and `95%` confidence
    intervals, using the code in [Figure 26-21](#c26-fig-0026) and [Figure 26-22](#c26-fig-0027).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低风险，我们创建了许多 `80-20` 划分（每个划分使用在 [图 26-6](#c26-fig-0011) 中定义的 `divide_80_20`
    函数创建），为每个划分构建和评估一个分类器，然后报告均值和 `95%` 置信区间，使用 [图 26-21](#c26-fig-0026) 和 [图 26-22](#c26-fig-0027)
    中的代码。
- en: '![c26-fig-0019.jpg](../images/c26-fig-0019.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0019.jpg](../images/c26-fig-0019.jpg)'
- en: '[Figure 26-19](#c26-fig-0024a) Class `Passenger`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-19](#c26-fig-0024a) 类 `Passenger`'
- en: '![c26-fig-0020.jpg](../images/c26-fig-0020.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0020.jpg](../images/c26-fig-0020.jpg)'
- en: '[Figure 26-20](#c26-fig-0025a) Read *Titanic* data and build list of examples[^(207)](#c26-fn-0010)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-20](#c26-fig-0025a) 读取 *泰坦尼克号* 数据并构建示例列表[^(207)](#c26-fn-0010)'
- en: '![c26-fig-0021.jpg](../images/c26-fig-0021.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0021.jpg](../images/c26-fig-0021.jpg)'
- en: '[Figure 26-21](#c26-fig-0026a) Test models for *Titanic* survival'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-21](#c26-fig-0026a) *泰坦尼克号* 生存测试模型'
- en: '![c26-fig-0022.jpg](../images/c26-fig-0022.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![c26-fig-0022.jpg](../images/c26-fig-0022.jpg)'
- en: '[Figure 26-22](#c26-fig-0027a) Print statistics about classifiers'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 26-22](#c26-fig-0027a) 打印有关分类器的统计信息'
- en: "The \uFEFFcall `test_models(build_Titanic_examples(), 100, True, False)` printed"
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `test_models(build_Titanic_examples(), 100, True, False)` 打印了
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It appears that this small set of features is sufficient to do a reasonably
    good job of predicting survival. To see why, let's take a look at the weights
    of the various features. We can do that with the call `test_models(build_Titanic_examples(),
    100, False, True)`, which printed
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这小组特征足以很好地预测生存情况。为了了解原因，让我们看看各种特征的权重。我们可以通过调用 `test_models(build_Titanic_examples(),
    100, False, True)` 来做到这一点，它打印了
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: When it comes to surviving a shipwreck, it seems useful to be rich (a first-class
    cabin on the *Titanic* cost the equivalent of more than $70,000 in today's U.S.
    dollars), young, and female.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到船难生存时，似乎拥有财富是有用的（*泰坦尼克号*的一等舱舱位在今天的美国美元中相当于超过$70,000），年轻和女性也是优势。
- en: 26.6 Wrapping Up
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.6 总结
- en: In the last three chapters, we've barely scratched the surface of machine learning.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后三章中，我们几乎只是触及了机器学习的表面。
- en: The same could be said about many of the other topics presented in the second
    part of this book. I've tried to give you a taste of the kind of thinking involved
    in using computation to better understand the world—in the hope that you will
    find ways to pursue the topic on your own.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这同样适用于本书第二部分中介绍的许多其他主题。我试图让你感受到利用计算更好理解世界所涉及的思维方式——希望你能找到独立研究该主题的方法。
- en: 26.7 Terms Introduced in Chapter
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 26.7 章节中引入的术语
- en: classification model
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型
- en: class
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别
- en: label
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签
- en: one-class learning
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单类学习
- en: two-class learning
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两类学习
- en: binary classification
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元分类
- en: multi-class learning
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类学习
- en: test set
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集
- en: training error
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练误差
- en: confusion matrix
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: accuracy
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率
- en: class imbalance
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别不平衡
- en: sensitivity (recall)
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 敏感性（召回率）
- en: specificity (precision)
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特异性（精准度）
- en: positive predictive value (PPV)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正预测值（PPV）
- en: k-nearest neighbors (KNN)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k 最近邻（KNN）
- en: downsample
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下采样
- en: negative predictive value
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阴性预测值
- en: n-fold cross validation
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: n 倍交叉验证
- en: logistic regression
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: outcome
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果
- en: feature weight
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征权重
- en: ROC curve
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC 曲线
- en: AUROC
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUROC
- en: model discrimination
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型区分度
- en: calibration
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 校准
- en: categorical feature
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别特征
