- en: Chapter 12
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 12 章
- en: Common Problems, Causes, and Solutions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见问题、原因与解决方案
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'In this chapter we look at some of the issues that plague CUDA developers and
    how you can avoid or at least mitigate these issues with some relatively simple
    practices. Issues with CUDA programs often fall into one the following categories:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一些困扰 CUDA 开发者的问题，以及如何通过一些相对简单的实践来避免或至少减轻这些问题。CUDA 程序中的问题通常可以归类为以下几种类型：
- en: • Errors of usage of various CUDA directives.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用各种 CUDA 指令时的错误。
- en: • General parallel programming errors.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: • 一般的并行编程错误。
- en: • Algorithmic errors.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: • 算法错误。
- en: Finally, we finish this last chapter with a discussion of where to continue
    your learning. There are many other texts on the subject of CUDA and GPU programming
    in general, as well as a lot of online material. We provide some pointers for
    what to read and where to find it. We also briefly discuss NVIDIA’s professional
    certification program for CUDA developers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在本章结束时讨论如何继续学习。关于 CUDA 和 GPU 编程的书籍有很多，在线资源也非常丰富。我们将提供一些阅读建议和获取资源的途径。同时，我们还会简要讨论
    NVIDIA 针对 CUDA 开发者的专业认证计划。
- en: Errors With CUDA Directives
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 CUDA 指令时的错误
- en: Errors using the CUDA API are by far the most common issue we see with people
    learning CUDA. It is a new API for many, and therefore mistakes in its usage should
    be expected and planned for.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CUDA API 时的错误是我们在学习 CUDA 时最常见的问题。这对于许多人来说是一个新的 API，因此在使用中出现错误是可以预料并且应该有所准备的。
- en: CUDA error handling
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CUDA 错误处理
- en: In [Chapter 4](CHP004.html), we introduced the `CUDA_CALL` macro. All of the
    CUDA API functions return an error code. Anything other than `cudaSuccess` generally
    indicates you did something wrong in calling the API. There are, however, a few
    exceptions, such as `cudaEventQuery`, which returns the event status as opposed
    to an error status.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 4 章](CHP004.html)中，我们介绍了 `CUDA_CALL` 宏。所有 CUDA API 函数都会返回一个错误代码。除非返回 `cudaSuccess`，否则通常表示你在调用
    API 时出了问题。不过，也有一些例外情况，比如 `cudaEventQuery`，它返回的是事件状态，而不是错误状态。
- en: The CUDA API is by nature asynchronous, meaning the error code returned at the
    point of the query, may have happened at some distant point in the past. In practice,
    it will usually be as a result of the call immediately prior to the error being
    detected. You can, of course, force this by synchronizing (i.e., calling the `cudaDeviceSynchronize`
    function) after every API call. While this strategy might be a good one for debugging,
    it’s not something that should be in any release version of the code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA API 本质上是异步的，这意味着在查询时返回的错误代码可能发生在过去的某个远程时刻。实际上，它通常是由于在检测到错误之前的紧接着的调用所导致的。当然，你可以通过在每个
    API 调用后进行同步（即调用 `cudaDeviceSynchronize` 函数）来强制实现这一点。尽管这种策略在调试时可能是个不错的方法，但它不应该出现在任何发布版本的代码中。
- en: Each error code can be turned into a semi-useful error string, rather than a
    number you have to look up in the API documentation. The error string is a somewhat
    helpful first attempt to identify the potential cause of the problem. However,
    it relies on the programmer explicitly checking the return code in the host program.
    It would be better if the CUDA runtime could trap such exceptions and perform
    some error indication, as we do explicitly with the `CUDA_CALL` macro, when running
    the debug version. This would help tremendously in pointing out errors in the
    user’s program, as and when they are introduced. We see some move toward this
    in the CUDA v4.1 SDK.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 每个错误代码都可以转换为一个半有用的错误字符串，而不是一个需要查阅 API 文档的数字。错误字符串是识别问题潜在原因的一个稍微有用的初步尝试。然而，它依赖于程序员在主机程序中显式检查返回代码。如果
    CUDA 运行时能够捕获这些异常并执行一些错误指示，那就更好了，就像我们在运行调试版本时通过 `CUDA_CALL` 宏显式执行的那样。这将极大地帮助指出用户程序中的错误，并在错误发生时及时发现。我们在
    CUDA v4.1 SDK 中看到了一些朝这个方向发展的变化。
- en: The CUDA error handling can be somewhat rudimentary. Most of the time, you’ll
    get a useful error message. However, often you will get a not-so-useful message
    such as `unknown error`, usually after a kernel invocation. This basically means
    your kernel did something it should not have, for example, writing over the end
    of the array in global or shared memory. There are debugging tools and methods
    we cover later in this chapter that help identify this type of problem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 错误处理有些简陋。大多数时候，你会收到一个有用的错误信息。然而，通常你会收到一个不太有用的信息，如 `unknown error`，通常是在内核调用之后。这基本上意味着你的内核做了一些不该做的事情，例如，写入了全局或共享内存数组的末尾。我们将在本章稍后介绍一些调试工具和方法，帮助识别这种类型的问题。
- en: Kernel launching and bounds checking
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内核启动和边界检查
- en: One of the most common failings in CUDA is an array overrun. You should ensure
    all your kernel invocations start with a check to ensure the data they will access,
    both for read and write purposes, is guarded by a conditional. For example,
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 中最常见的错误之一是数组溢出。你应该确保所有的内核调用都以检查开始，确保它们访问的数据（无论是读取还是写入）都有条件保护。例如，
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This conditional takes a marginal amount of time, but will save you a lot of
    debugging effort. You typically see such a problem where you have a number of
    data elements that are not multiples of the thread block size.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个条件语句会消耗少量时间，但可以节省大量调试工作。通常你会在存在多个数据元素且它们不是线程块大小的倍数时，遇到这样的情况。
- en: Suppose we have 256 threads per block and 1024 data elements. This would invoke
    four blocks of 256 threads. Each thread would contribute to the result. Now suppose
    we had 1025 data elements. You would typically have two types of errors here.
    The first is to not invoke a sufficient number of threads, due to using an integer
    division. This will usually truncate the number of blocks needed. Typically people
    write
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们每个块有256个线程，且有1024个数据元素。这将调用四个256线程的块，每个线程都会贡献结果。现在假设我们有1025个数据元素。通常你会遇到两种错误。第一个是由于使用整数除法，未调用足够的线程。这样通常会截断所需的块数。通常人们会写：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will work, but only where the number of elements is an exact multiple of
    the number of threads. In the 1025 elements case we launch 4 × 256 threads, some
    1024 threads in total. The last element remains unprocessed. I’ve also seen, as
    well as other variations, attempts to “get around” this issue. For example,
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这样是可以工作的，但仅在元素数量是线程数的精确倍数时。在1025个元素的情况下，我们启动了4 × 256个线程，总共1024个线程。最后一个元素未被处理。我还看到过，除了其他变体外，有些人试图“绕过”这个问题。例如，
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This does not solve the problem. You cannot have 4.1 blocks. The assignment
    to integer truncates the number to four blocks. The solution is a simple one.
    You write the following instead:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这样并不能解决问题。你不能有4.1个块。分配给整数时会将数字截断为四个块。解决方案很简单。你可以改写为：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will ensure you always allocate enough blocks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这将确保你始终分配足够的块。
- en: The second issue we commonly see then follows. We now invoke five blocks for
    a total of 1280 threads. Without such guarded access to the array within the kernel,
    all but the first thread in block 5 would be accessing an out-of-bounds memory
    location. The CUDA runtime performs little if any runtime checks, such as array
    bounds. You will never see it halt the kernel and display a message such as `array
    overrun in line 252 file kernel.cu`. However, rather than silently fail, which
    is the worst case, it does at least trap the error in some way and then returns
    a message such as `unknown error`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是我们常见的第二个问题。我们现在调用五个块，总共1280个线程。如果在内核中没有对数组的这种受保护访问，除了第一个线程外，块5中的所有线程都会访问越界的内存位置。CUDA运行时几乎不会进行任何运行时检查，如数组边界检查。你永远不会看到它停止内核并显示类似`array
    overrun in line 252 file kernel.cu`的消息。然而，与其默默失败（这是最糟糕的情况），它至少会以某种方式捕获错误，并返回类似`unknown
    error`的消息。
- en: Invalid device handles
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无效的设备句柄
- en: The other type of errors you typically see are incorrect mixing of handles,
    most often pointers. When you allocate memory on the device or on the host, you
    receive a pointer to that memory. However, that pointer comes with an implicit
    requirement that *only the host* may access host pointers and *only the device*
    may access device pointers. There are a few exceptions, such as zero-copy memory,
    where a host pointer can be converted to a device pointer to host memory, but
    even in this case you have a separation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你通常会看到的另一类错误是句柄混用错误，最常见的是指针错误。当你在设备或主机上分配内存时，你会得到指向该内存的指针。然而，这个指针隐含着一个要求，即*只有主机*可以访问主机指针，*只有设备*可以访问设备指针。也有一些例外情况，例如零拷贝内存，在这种情况下，主机指针可以转换为设备指针以访问主机内存，但即便如此，你仍然需要区分它们。
- en: As the pointers are not interchangeable, one might have hoped that device pointers
    would be declared using a different type. This would allow for type-based checks
    on calls to the API to flag such issues at compile time. Unfortunately, a device
    pointer and a host pointer are the same basic type, which means there is no static-type
    checking performed by the compiler.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于指针不可互换，人们可能会希望设备指针使用不同的类型进行声明。这样就可以在调用API时进行基于类型的检查，在编译时标记出这种问题。不幸的是，设备指针和主机指针是相同的基本类型，这意味着编译器并不会执行静态类型检查。
- en: There is, of course, no reason why you could not define such a type. You could
    then develop your own wrapper functions around the API functions that performed
    type checking. Certainly for those who are starting out writing CUDA, this would
    be a tremendous help and perhaps something we’ll see as CUDA develops. The Thrust
    library we looked at in [Chapter 10](CHP010.html) has the concept of a host vector
    and a device vector. It uses C++ function overloading to ensure that the correct
    function is always called for the given data type.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你完全可以定义这样的类型。然后，你可以围绕API函数开发你自己的包装函数，进行类型检查。当然，对于那些刚开始写CUDA的人来说，这将是一个巨大的帮助，或许我们会在CUDA的发展过程中看到这样的功能。我们在[第10章](CHP010.html)中看到的Thrust库就有主机向量和设备向量的概念。它使用C++的函数重载机制，确保针对给定数据类型始终调用正确的函数。
- en: The standard CUDA runtime checks for this type of incorrect mixing of device
    and host pointers, in terms of passing a host pointer to a device function are
    reasonable. The CUDA API checks the pointer’s origin and will generate a runtime
    error if you pass a host pointer to a kernel function without first converting
    it to a device pointer to host memory. However, the same cannot be said for the
    standard C/C++ system libraries. If you call the standard `free` function as opposed
    to the `cudaFree` function with a device pointer, the system libraries will try
    to free that memory on the host, and then will likely crash. The host libraries
    have no concept of a memory space they can’t access.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 标准CUDA运行时检查这种类型的错误，涉及到设备和主机指针的混用问题，在将主机指针传递给设备函数时是否合理。CUDA API会检查指针的来源，如果你在没有先将其转换为设备指针的情况下将主机指针传递给内核函数，它会生成运行时错误。然而，标准的C/C++系统库就不能如此智能。如果你使用设备指针调用标准的`free`函数而不是`cudaFree`函数，系统库将试图在主机上释放该内存，这可能导致崩溃。主机库并不理解它无法访问的内存空间。
- en: The other type of invalid handle comes from the usage of a type before it’s
    been initialized. This is akin to using a variable before assigning it a value.
    For example,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种无效句柄的类型来源于在初始化之前使用某个类型。这就像在为变量赋值之前使用它一样。例如，
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this example we’re missing the call to `cudaStreamCreate` and subsequent
    `cudaStreamDestroy` functions. The create call performs some initialization to
    register the event in the CUDA API. The destroy call releases those resources.
    The correct code is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们缺少调用`cudaStreamCreate`和随后的`cudaStreamDestroy`函数。创建调用执行一些初始化，注册事件到CUDA
    API中。销毁调用则释放这些资源。正确的代码如下：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`cudaStreamDestroy(my_stream);`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaStreamDestroy(my_stream);`'
- en: 'Unfortunately, the CUDA multiple-device model is based on selecting a device
    context prior to performing an operation. A somewhat cleaner interface would have
    been to specify an optional `device_num` parameter in each call, which would default
    to device 0 if not specified. This would then allow the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，CUDA的多设备模型基于在执行操作之前选择一个设备上下文。一个更清晰的接口本应该是在每个调用中指定一个可选的`device_num`参数，如果没有指定，则默认为设备0。这样就可以实现如下功能：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Although this is moving from C to C++, it provides a somewhat cleaner interface,
    as resources would be automatically created with a constructor and destroyed with
    a destructor. You can, of course, easily write such a C++ class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是从C转向C++，但它提供了一个相对更清晰的接口，因为资源将通过构造函数自动创建，并通过析构函数销毁。当然，你可以轻松编写这样的C++类。
- en: Invalid device handles, however, are not simply caused by forgetting to create
    them. They can also be caused by destroying them prior to the device finishing
    usage of them. Try deleting the `cudaStreamSynchronize` call from the original
    code. This will cause the stream in use by the asynchronous kernel to be destroyed
    while the kernel is potentially still running on the device.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，无效的设备句柄并不仅仅是由于忘记创建它们造成的。它们还可能是由于在设备完成对它们的使用之前销毁了它们。尝试从原始代码中删除`cudaStreamSynchronize`调用。这将导致异步内核正在使用的流在内核可能仍在设备上运行时被销毁。
- en: Due to the asynchronous nature of streams, the `cudaStreamDestroy` function
    will not fail. It will return `cudaSuccess`, so it will not even be detected by
    the `CUDA_CALL` macro. In fact, you will not get an error until sometime later,
    from an entirely unrelated call into the CUDA API. One solution to this is to
    embed the `cudaSynchronizeDevice` call into the `CUDA_CALL` macro. This can help
    in identifying the exact cause of the problem. However, be careful not to leave
    this in production code.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于流的异步性质，`cudaStreamDestroy`函数不会失败。它将返回`cudaSuccess`，因此不会被`CUDA_CALL`宏检测到。事实上，直到稍后从与此完全无关的CUDA
    API调用中，你才会遇到错误。解决方法之一是将`cudaSynchronizeDevice`调用嵌入到`CUDA_CALL`宏中，这有助于确定问题的具体原因。然而，要小心不要将其留在生产代码中。
- en: Volatile qualifiers
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 易变修饰符
- en: 'The C “volatile” keyword specifies to the compiler that all references to this
    variable, read or write, must result in a memory reference, and those references
    must be in the order specified in the program. Consider the following code segment:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: C语言中的“volatile”关键字指示编译器，所有对该变量的读写操作都必须导致内存访问，并且这些操作必须按照程序中指定的顺序进行。考虑以下代码段：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here we declare a global variable `a` starting at 0\. Every time we call the
    function it iterates `i` from 0 to 1000 and adds each value to the variable `a`.
    In the nonoptimized version of this code, it’s likely each write of `a` will result
    in a physical memory write. However, this is highly unlikely in the optimized
    code version.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们声明一个从0开始的全局变量`a`。每次调用该函数时，它都会将`i`从0迭代到1000，并将每个值加到变量`a`中。在这个未优化的代码版本中，每次对`a`的写操作很可能会导致物理内存写入。然而，在优化后的代码版本中，这种情况的发生几率极低。
- en: 'The optimizer can apply two approaches here. First, and the most common, would
    be to load the value of `a` into a register at the start of the loop, run the
    loop to the end, and then write the resulting register back to memory as a *single*
    store operation. This is simply an example of the programmer being unaware, or
    not caring, about the cost of memory access. The C code could have been written
    as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器可以在此应用两种方法。首先，也是最常见的，可能是在循环开始时将`a`的值加载到寄存器中，执行循环直到结束，然后将结果寄存器写回内存，作为*单次*存储操作。这只是一个例子，说明程序员可能没有意识到，或者不在乎内存访问的成本。C代码可以如下编写：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is effectively what the compiler will likely replace it with. A somewhat
    more advanced optimizer may be able to unroll the loop, as if it had constant
    boundaries, to a single expression. As that expression would contain `a` plus
    a series of constants, the constants could be reduced to a single constant at
    compile time, eliminating the loop altogether. For example,
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是编译器可能会用来替换它的方式。一种稍微高级的优化器可能能够将循环展开，就像它有常量边界一样，转化为一个单一的表达式。由于该表达式包含`a`加上一系列常量，这些常量可以在编译时被简化为一个单一常量，从而完全消除循环。例如，
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: or
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: While many compilers will unroll loops, I’d not expect many, if any, compilers
    to produce the later, simplified code. However, in theory there is no reason why
    this could not be the case.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然许多编译器会展开循环，但我不认为许多编译器，甚至没有编译器，能够生成后面简化的代码。然而，理论上没有理由不这么做。
- en: Either approach potentially causes problems if some other thread needs to share
    the value of parameter `a` during any intermediate loop iteration. On the GPU
    this shared parameter can be either in shared or global memory. For the most part
    these types of problems are largely hidden from the GPU programmer in that the
    call to `__syncthreads()` causes an implicit flush of any writes to memory in
    both shared and global memory for the *current block*. As most shared memory code
    typically does some action, writes the result, and then synchronizes, the synchronization
    operation also serves to automatically distribute the data between threads.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果其他线程在任何中间循环迭代过程中需要共享参数`a`的值，这两种方法都有可能引发问题。在GPU上，这个共享参数可以位于共享内存或全局内存中。在大多数情况下，这些类型的问题对于GPU程序员来说是隐藏的，因为`__syncthreads()`的调用会隐式地刷新共享内存和全局内存中所有对内存的写入，作用于*当前块*。由于大多数共享内存代码通常会执行一些操作，写入结果，然后同步，同步操作也会自动将数据分发到各个线程。
- en: Problems occur when the programmer takes account of the fact that threads within
    a warp operate in a synchronous manner and thus omits the synchronization primitive.
    You typically see such optimizations when a reduction operation is in use and
    the last 32 values don’t need a synchronization primitive. This is true only in
    the case in which the shared memory is additionally declared as volatile. Otherwise,
    the compiler does not have to write any values at all to shared memory.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 问题出现在程序员考虑到线程在一个warp内以同步方式操作，并因此省略了同步原语时。通常，当使用归约操作且最后32个值不需要同步原语时，你会看到这种优化。只有在共享内存额外声明为volatile的情况下，这种优化才成立。否则，编译器根本不需要将任何值写入共享内存。
- en: 'Shared memory has two purposes: first, to act as a block of local, high-speed,
    per-thread memory, and second, to facilitate interthread communication within
    a block. Only in the latter case does shared memory need to be declared as volatile.
    Thus, the `__shared__` directive does not implicitly declare the parameter as
    volatile since the programmer may not always wish to enforce reads and writes
    when the compiler is able to use a register to optimize out some of these. It
    is perfectly valid practice not to use a `syncthread` call when the threads are
    cooperating within a warp, but you must realize that the shared memory has no
    longer been made coherent to every thread in the warp.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存有两个目的：首先，作为每个线程的局部高速内存块，其次，促进块内线程间的通信。仅在后者情况下，才需要将共享内存声明为volatile。因此，`__shared__`指令不会隐式地将参数声明为volatile，因为程序员可能并不总是希望在编译器能够使用寄存器来优化掉其中一些操作时强制执行读写操作。在线程在一个warp内协作时，不调用`syncthread`是完全有效的做法，但你必须意识到，此时共享内存已不再对warp内的每个线程保持一致性。
- en: When you have interblock communication via global memory, the view each block
    sees of global memory is again not consistent between blocks without explicit
    synchronization. We have the same issue as with shared memory, in that the compiler
    may optimize away intermediate global writes and write only the last one out to
    memory. This can be overcome by using the volatile keyword for access within a
    block. However, CUDA does not specify block execution order, so this does not
    deal with interblock-based dependencies. These are handled in two ways. First,
    and the most common, is the termination and invocation of another kernel. Implicit
    in this is a completion of all pending global memory transactions and a flush
    of all caches. The second method is used where you wish to perform some operation
    within the same kernel invocation. In this instance you need to call the `__threadfence`
    primitive, which simply causes, and waits for, any writes from the calling thread
    to be visible to all affected threads. For shared memory, this equates to the
    threads within the same block, as only these threads can see the shared memory
    allocated to a given block. For global memory, this equates to all threads within
    the device.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当你通过全局内存进行块间通信时，每个块看到的全局内存视图在没有显式同步的情况下，块与块之间并不一致。我们面临的问题与共享内存相同，因为编译器可能会优化掉中间的全局写入，只将最后一次写入保存到内存中。通过在块内使用`volatile`关键字来访问，可以解决这个问题。然而，CUDA并没有指定块的执行顺序，因此这不能解决基于块间依赖的问题。这些问题有两种处理方式。第一种，也是最常见的，是通过终止并调用另一个内核。隐含的含义是完成所有待处理的全局内存事务，并刷新所有缓存。第二种方法用于你希望在同一个内核调用内执行某些操作的场景。在这种情况下，你需要调用`__threadfence`原语，它会简单地触发并等待，确保调用线程的所有写入对所有受影响的线程可见。对于共享内存，这意味着同一块中的线程，因为只有这些线程才能看到分配给特定块的共享内存。对于全局内存，这意味着设备内的所有线程。
- en: Compute level–dependent functions
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算级别相关的函数
- en: The compute 2.x hardware supports many additional functions not present in the
    earlier hardware. The same is true of compute 1.3 devices. If you search through
    the CUDA programming guide it will list various functions as available only on
    certain compute levels. For example, `__syncthreads_count` is a compute 2.0 function.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: compute 2.x 硬件支持许多早期硬件不具备的附加功能。compute 1.3 设备也是如此。如果你查阅 CUDA 编程指南，会发现某些功能仅在特定的计算架构下可用。例如，`__syncthreads_count`
    是一个 compute 2.0 函数。
- en: 'Unfortunately, the default CUDA projects (e.g., the New Project wizard in Visual
    Studio) use CUDA 1.0 support. Thus, when you have a Fermi card installed (a compute
    2.x device) and compile the project using a compute 2.0 directive, the compiler
    rather unhelpfully states the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，默认的 CUDA 项目（例如 Visual Studio 中的新建项目向导）使用的是 CUDA 1.0 支持。因此，当你安装了 Fermi 卡（一个
    compute 2.x 设备）并使用 compute 2.0 指令编译项目时，编译器会非常不友好地给出以下提示：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It doesn’t say this function is supported only under the compute 2.0 architecture.
    This would be at least helpful in helping you identify the problem. It just says
    it’s undefined, which makes most programmers assume they have missed an include
    statement or have done something wrong. Thus, they are sent off in the wrong direction
    searching for a solution.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它并没有说明这个函数仅在 compute 2.0 架构下得到支持。这至少能帮助你识别问题。它只是说明该函数未定义，这让大多数程序员误以为他们遗漏了某个包含声明或者做错了什么。因此，他们会朝错误的方向去寻找解决方案。
- en: 'The issue is simply resolved by setting the GPU architecture level by changing
    the properties of the GPU option of the CUDA runtime, as shown in [Figure 12.1](#F0010).
    This results in the following command line option being added to the compiler
    invocation command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过通过更改 CUDA 运行时中 GPU 选项的属性来设置 GPU 架构级别来简单地解决，正如 [图 12.1](#F0010) 所示。这样会将以下命令行选项添加到编译器调用命令中：
- en: '![image](../images/F000120f12-01-9780124159334.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000120f12-01-9780124159334.jpg)'
- en: FIGURE 12.1 Setting the correct architecture.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 设置正确的架构。
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note that you can set, by default, up to three architectures in the standard
    project created by Visual Studio for CUDA projects. Code can be written for various
    compute levels using the compiler preprocessor. In fact, this is what is being
    used to make higher compute level functions visible.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在 Visual Studio 为 CUDA 项目创建的标准项目中，默认情况下最多可以设置三种架构。你可以使用编译器预处理器为不同的计算架构级别编写代码。实际上，这就是用来使更高计算级别的功能可见的方法。
- en: CUDA defines a preprocessor symbol `__CUDA_ARCH__`, which currently holds the
    value 100, 110, 120, 130, 200, 210, or 300\. Clearly, as future architectures
    are defined these will increase. Thus, you can write
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 定义了一个预处理器符号 `__CUDA_ARCH__`，其当前值为 100、110、120、130、200、210 或 300。显然，随着未来架构的定义，这些值将会增加。因此，你可以编写
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Alternatively, you can write a single function that uses conditional compilation
    only where necessary to either make use of the later compute level functions or
    provide an alternative solution for lower compute level devices.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以编写一个单一的函数，仅在必要时使用条件编译，既能利用较新的计算级别函数，又能为较低计算级别的设备提供替代解决方案。
- en: Many of the compute 2.x functions simplify the programming necessary and therefore
    make development easier. However, most of these later functions can also be implemented
    by lower compute level devices in a slower manner or with slightly more programming.
    By not providing any implementation to provide for backward compatibility, CUDA
    forces programmers to make a choice of either not using the new features, using
    them and excluding those customers with older hardware, or using them and writing
    their own implementation for older hardware.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 许多计算 2.x 函数简化了所需的编程，因此使得开发变得更加容易。然而，这些较新的函数大多数也可以通过较低计算级别的设备以较慢的方式或稍多的编程实现。如果不提供任何实现来支持向后兼容性，CUDA
    就迫使程序员做出选择：要么不使用新功能，要么使用它们并排除那些使用较旧硬件的客户，要么使用它们并为旧硬件编写自己的实现。
- en: Most consumers will expect your software to work on their hardware. They will
    not be impressed with a message telling them to swap out their 9800 GT or GTX260
    for a 400/500/600 series Fermi/Kepler card. Most consumers will have no clue what
    the compute level is anyway and will have purchased the card to play the latest
    version of a particular game.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数消费者会期望你的软件能够在他们的硬件上正常运行。他们不会对提示他们更换 9800 GT 或 GTX260 为 400/500/600 系列 Fermi/Kepler
    显卡的消息感到满意。大多数消费者根本不清楚计算级别是什么，而且他们购买显卡的目的是为了玩最新版本的某个游戏。
- en: If you work in the research or commercial fields, then your hardware is largely
    defined for you by the institution or company. If you have an input into this,
    absolutely choose at least compute 2.x hardware or later, as it is much easier
    to program. You can then largely forget about the evolution of GPUs to date and
    work with a cache-based system far more familiar to most CPU programmers. If you
    have a mix of hardware, as do many of our clients, then you need to think about
    how to achieve the best performance on each generation of hardware and write your
    program accordingly.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从事研究或商业领域工作，那么你的硬件在很大程度上是由机构或公司为你定义的。如果你可以参与选择，务必选择至少是计算 2.x 或更高版本的硬件，因为这将大大简化编程。这样你就可以基本上忽略
    GPU 迭代的历史，使用一种对大多数 CPU 程序员来说更为熟悉的基于缓存的系统。如果你有多种硬件配置，就像我们许多客户一样，那么你需要考虑如何在每一代硬件上实现最佳性能，并据此编写程序。
- en: Device, global, and host functions
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设备、全局和主机函数
- en: In CUDA you have to specify if a function or data item exists on the host (the
    CPU side) or the device (the GPU side) of the PCI-E data bus. Thus, there are
    three specifiers that can be used, as shown in [Table 12.1](#T0010). If you omit
    the specifier, then the CUDA compiler will assume the function exists on the host
    and will only allow you to call it from there. This is an error detected at compile
    time and thus easy to correct. It is possible to specify that a function exists
    both on the host (CPU) and also on the device (GPU) by using both the `__device__`
    and `__host__` specifiers. However, it’s not possible to mix `__global__` and
    `__host__` specifiers.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CUDA 中，你必须指定一个函数或数据项存在于主机（CPU 端）还是设备（GPU 端）的 PCI-E 数据总线上。因此，可以使用三种限定符，如 [表
    12.1](#T0010) 所示。如果你省略限定符，CUDA 编译器将假定该函数存在于主机端，并且只允许从主机调用该函数。这是一个在编译时检测到的错误，因此很容易纠正。通过同时使用
    `__device__` 和 `__host__` 限定符，可以指定函数同时存在于主机（CPU）和设备（GPU）上。然而，不能将 `__global__`
    和 `__host__` 限定符混合使用。
- en: Table 12.1 GPU and Host Functions
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12.1 GPU 和主机函数
- en: '| Specifier | Code Is Located on | May Be Called by |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 限定符 | 代码所在位置 | 可被调用者 |'
- en: '| --- | --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `__device__` | GPU | A global or device function |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `__device__` | GPU | 全局函数或设备函数 |'
- en: '| `__global__` | GPU | A host function using a kernel invocation |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `__global__` | GPU | 使用内核调用的主机函数 |'
- en: '| `__host__` | Host | A regular C function call |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `__host__` | 主机 | 普通 C 函数调用 |'
- en: This dual specification is useful in that it allows you to write common code
    on both the GPU and the CPU. You can abstract what data gets processed by what
    thread to the global function. The global function then calls the device function,
    passing it a pointer to the data it should perform the task on. The host function
    can simply call the device function in a loop to achieve the same functionality.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种双重限定指定方式非常有用，因为它允许你在 GPU 和 CPU 上编写通用代码。你可以将由哪个线程处理的数据抽象到全局函数中。全局函数然后调用设备函数，并传递一个指向它应该处理的数据的指针。主机函数可以简单地通过循环调用设备函数，以实现相同的功能。
- en: In terms of how device and global functions get translated, device functions
    are similar to static functions in C. That is, the CUDA compiler expects to be
    able to see the entire scope of a device function at compile time, not link time.
    This is because device functions, by default, get in-lined into the global function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备函数和全局函数如何被翻译方面，设备函数类似于 C 中的静态函数。也就是说，CUDA 编译器期望在编译时看到设备函数的整个作用域，而不是链接时看到。这是因为设备函数默认会内联到全局函数中。
- en: In-lining is a process where the formal parameters and the call overhead are
    eliminated and every call to the function is expanded as if the body of the called
    function was included at the point of the call. This might lead you to think the
    compiler is wasting code space, as you will have potentially two copies of the
    same device function in the program memory space. However, usually the context
    of the call will allow additional optimization strategies to be used, so although
    the device function is largely duplicated, it may be slightly different in each
    usage.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 内联是一种消除形式参数和调用开销的过程，每次调用函数时，都会像把被调用函数的代码包含在调用点一样展开。这可能会让你觉得编译器浪费了代码空间，因为你可能会在程序内存中有两个相同的设备函数副本。然而，通常调用的上下文会允许使用额外的优化策略，因此，尽管设备函数大体上是重复的，但每次使用时可能略有不同。
- en: The problem this causes for you, the programmer, is that the compiler expects
    one source file. If you want to have two kernel source files (.cu files) that
    share a common device function, then you need to `#include` the .cu source file
    into each caller instead of declaring the usual header file approach and having
    the linker resolve the call. Note that in the CUDA 5.0 release of the SDK, its
    new GPU Library Object Linking feature allows for standard object code generation
    of the device code kernel and even placing this code into static linkable libraries.
    This allows for much better reuse of existing code and somewhat quicker compile
    times.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题对于你，程序员来说，就是编译器期望只有一个源文件。如果你想拥有两个共享公共设备函数的内核源文件（.cu 文件），那么你需要将 `.cu` 源文件
    `#include` 到每个调用者中，而不是采用通常的头文件声明方法，并让链接器解析调用。注意，在 CUDA 5.0 版本的 SDK 中，它的新 GPU 库对象链接功能允许标准的设备代码内核对象代码生成，甚至将这些代码放入静态可链接库中。这大大提高了现有代码的重用性，并且编译时间有所缩短。
- en: Kernels within streams
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流内的内核
- en: Getting an asynchronous operation to work as you intend is actually quite tricky
    since the stream model is not reflected in the actual hardware, at least up to
    compute 2.1 devices. Thus, you might create two streams and fill stream A with
    a number of memory copies and then stream B with a number of memory copies. You
    might expect that as streams A and B are different, the hardware would interleave
    copies from each stream. What happens in practice is the hardware has only a single
    queue and executes commands based on the order in which they were issued. Thus,
    two streams that implement a `copy to` device, `execute` kernel, and `copy from`
    device operation will be run in sequence rather than being overlapped with one
    another.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让异步操作按预期工作其实相当棘手，因为流模型并未在实际硬件中得到体现，至少在计算能力为 2.1 的设备上是这样的。因此，你可能会创建两个流，将流 A 填充一系列内存复制操作，然后将流
    B 填充另一系列内存复制操作。你可能预期，由于流 A 和 B 是不同的，硬件会交替执行来自每个流的复制操作。实际上发生的情况是，硬件只有一个队列，并按发出命令的顺序执行命令。因此，两个实现
    `copy to` 设备、`execute` 内核和 `copy from` 设备操作的流将会顺序执行，而不是彼此重叠。
- en: In consumer hardware up to and including compute 3.0 devices there are just
    two queues—one for the memory copies and one for the kernels. In the memory queue,
    any preceding operation must complete prior to a new operation being issued. This
    makes perfect sense, as a single DMA (direct-memory access) engine can do a single
    transfer at a time. However, this means filling the queues, depth first by stream
    has the effect of serializing the stream operations, which defeats the object
    of using streams, to achieve an enhanced level of concurrent kernel/memory transfers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在包括计算3.0设备的消费者硬件中，只有两个队列——一个用于内存复制，另一个用于kernels。在内存队列中，任何前面的操作必须在新操作被发出之前完成。这是非常合理的，因为单个DMA（直接内存访问）引擎一次只能进行一个传输。然而，这意味着按流的深度优先方式填充队列，会使得流操作序列化，这违背了使用流的目的——实现更高水平的并发kernel/内存传输。
- en: The solution is still to fill the queue depth first, but to exclude the `copy
    back` memory operations from the queue. Thus, the `copy to` and `kernel` operations
    will overlap execution with one another. In a situation where the input data is
    larger than the output data, this works quite well. Once the last kernel in the
    batch has been pushed into the queue, all of the `copy back` operations are then
    pushed into the transfer queue.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案仍然是首先填充队列深度，但要排除队列中的`copy back`内存操作。因此，`copy to`和`kernel`操作将互相重叠执行。在输入数据大于输出数据的情况下，这种方式效果相当好。一旦批次中的最后一个kernel被推入队列，所有的`copy
    back`操作将被推入传输队列。
- en: In Fermi devices based on the GF100/GF110 devices (i.e., GTX470, GTX480, GTX570,
    GTX580, Tesla C2050, C2070, C2075, Tesla M2050/2070) there are two DMA engines.
    However, only the Tesla devices enable this second transfer engine, known as “async
    engine count,” in the driver. Thus, on Fermi Tesla devices, the depth-first approach
    mentioned previously can be improved upon. As we no longer have a single transfer
    queue, we in fact should issue commands to the stream breadth first. This vastly
    simplifies stream handling, as we can effectively forget about the hardware handling
    internally and expect it to work as the logical stream model predicts.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于GF100/GF110设备的Fermi设备（例如GTX470、GTX480、GTX570、GTX580、Tesla C2050、C2070、C2075、Tesla
    M2050/2070）中，有两个DMA引擎。然而，只有Tesla设备在驱动程序中启用了第二个传输引擎，称为“异步引擎计数（async engine count）”。因此，在Fermi
    Tesla设备上，前述的深度优先方法可以得到改进。由于我们不再有单一的传输队列，实际上我们应该按广度优先方式向流发出命令。这大大简化了流的处理，因为我们可以有效地忽略硬件内部的处理，预期它将按逻辑流模型的预测正常工作。
- en: However, do be aware of one optimization in the hardware that can cause issues.
    The hardware will tie successive transfers together in terms of when they complete.
    Thus, launching two memory copies followed by two kernel calls results in both
    the memory copies having to complete before either kernel gets launched. You can
    break this behavior by inserting an event into the stream in between the memory
    copies. Then each copy is handled independently of the ones after it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，必须注意硬件中的一种优化，它可能导致问题。硬件会将连续的传输操作按完成时间串联起来。因此，启动两个内存拷贝，然后进行两个内核调用时，必须等到两个内存拷贝都完成后，任何一个内核才会被启动。你可以通过在内存拷贝之间插入一个事件来打破这种行为。这样，每个拷贝就会独立于其后的拷贝进行处理。
- en: Parallel Programming Issues
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行编程问题
- en: Having gotten over the usage of the API issues, the next pitfalls most CUDA
    developers fall into are some of the more general problems that plague all parallel
    software development. We look in this section at some of these issues and how
    they affect GPU development.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 克服了API使用问题后，接下来大多数CUDA开发者遇到的陷阱是一些困扰所有并行软件开发的通用问题。我们在这一节中探讨这些问题以及它们如何影响GPU开发。
- en: Race hazards
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 竞态风险
- en: In a single-thread application, the problem of producer/consumer is quite easy
    to handle. It’s simply a case of looking at the data flow and seeing if a variable
    was read before anything wrote to it. Many of the better compilers highlight such
    issues. However, even with this assistance, complex code can suffer from this
    issue.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在单线程应用中，生产者/消费者问题相对容易处理。这只是一个查看数据流并检查一个变量在被写入之前是否被读取的简单问题。许多较好的编译器会高亮显示此类问题。然而，即使有了这种帮助，复杂的代码仍然可能受到这个问题的影响。
- en: As soon as you introduce threads into the equation, producer/consumer problems
    become a real headache if not thought about carefully in advance. The threading
    mechanism in most operating systems—and CUDA is no exception—tries to operate
    to achieve the best overall throughput. This usually means threads can run in
    any order and the program must not be sensitive to this ordering.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦引入线程，生产者/消费者问题就会变得非常棘手，如果没有事先仔细考虑。大多数操作系统中的线程机制——CUDA也不例外——尝试实现最佳的总体吞吐量。这通常意味着线程可以以任何顺序运行，程序必须对这种顺序不敏感。
- en: Consider a loop where iteration `i` depends on loop iteration `i-1`. If we simply
    assign a thread to each element of the array and do nothing else, the program
    will work only when the processor executes one thread at a time according to the
    thread ID from low to high thread numbers. Reverse this order or execute more
    than one thread in parallel and the program breaks. However, this is a rather
    simple example and not all programs break. Many run and produce the answer correctly
    sometimes. If you ever find you have a correct answer on some runs, but the wrong
    answer on others, it is likely you have a producer/consumer or race hazard issue.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个循环，其中迭代 `i` 依赖于上一轮迭代 `i-1`。如果我们仅将每个数组元素分配给一个线程，并且不做其他处理，程序只有在处理器按照线程 ID
    从低到高顺序逐个执行线程时才能正常工作。如果反转这个顺序，或者并行执行多个线程，程序就会出错。然而，这只是一个相对简单的例子，并不是所有程序都会出错。许多程序在某些情况下能运行并正确输出答案。如果你发现某些运行时结果正确，而其他运行时结果错误，那么很可能是你遇到了生产者/消费者或竞态风险问题。
- en: A race hazard, as its name implies, occurs when sections of the program “race”
    toward a critical point, such as a memory read/write. Sometimes warp 0 may win
    the race and the result is correct. Other times warp 1 might get delayed and warp
    3 hits the critical section first, producing the wrong answer.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 竞态风险，顾名思义，发生在程序的不同部分“竞速”到达一个关键点，例如内存的读写。有时 warp 0 可能赢得竞赛，结果是正确的。其他时候，warp 1
    可能会被延迟，warp 3 先到达关键区段，从而产生错误的结果。
- en: The major problem with race hazards is they do not always occur. This makes
    debugging them and trying to place a breakpoint on the error difficult. The second
    feature of race hazards is they are extremely sensitive to timing disturbances.
    Thus, adding a breakpoint and single-stepping the code always delays the thread
    being observed. This delay often changes the scheduling pattern of other warps,
    meaning the particular conditions of the wrong answer may never occur.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 竞态风险的主要问题在于它们并不总是发生。这使得调试它们并在错误发生时设置断点变得困难。竞态风险的第二个特点是它们对时序干扰非常敏感。因此，添加断点并单步执行代码总是会延迟被观察的线程。这个延迟常常会改变其他
    warp 的调度模式，这意味着错误答案发生的特定条件可能永远不会出现。
- en: The first question in such a situation is not where in the code is this happening,
    but requires you to take a step backward and look at the larger picture. Consider
    under what circumstances the answer can change. If there is some assumption about
    the ordering of thread or block execution in the design, then we already have
    the cause of the problem. As CUDA does not provide any guarantee of block ordering
    or warp execution ordering, any such assumption means the design is flawed. For
    instance, take a simple sum-based reduction to add all the numbers in a large
    array. If each run produces a different answer, then this is likely because the
    blocks are running in a different order, *which is to be expected*. The order
    should not and must not affect the outcome of the result.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，第一个问题不是代码中发生的具体位置，而是需要你退后一步，从更大的角度看问题。考虑在哪些情况下答案可能会改变。如果设计中有关于线程或块执行顺序的假设，那么我们已经找到了问题的根源。由于CUDA不提供任何关于块顺序或warp执行顺序的保证，任何此类假设都意味着设计存在缺陷。例如，考虑一个简单的基于求和的归约操作来加和一个大数组中的所有数字。如果每次运行都产生不同的结果，那么很可能是因为块的执行顺序不同，*这是可以预期的*。顺序不应该也必须不影响结果的最终输出。
- en: In such an example we can fix the ordering issues by sorting the array and combining
    values from low to high in a defined order. We can and should define an order
    for such problems. However, the actual execution order in the hardware should
    be considered as undefined with known synchronization points.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的示例中，我们可以通过对数组进行排序并按照定义的顺序从低到高组合值来解决排序问题。我们可以并且应该为这类问题定义一个顺序。然而，硬件中的实际执行顺序应该被视为未定义的，但有已知的同步点。
- en: Synchronization
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 同步
- en: '*Synchronization* in CUDA is the term used for sharing of information between
    threads within a block, or between blocks within a grid. A thread can access register
    space or local memory space, both of which are private to the thread. For threads
    to work together on a problem they will often use the on-chip shared memory. We
    saw some examples of this in the reduction problem we looked at earlier.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，*同步*是指线程之间在一个块内，或在一个网格中的块之间共享信息的过程。线程可以访问寄存器空间或本地内存空间，这两者都是线程私有的。为了让线程在问题上协作，它们通常会使用片上共享内存。我们在之前的归约问题中已经看过一些这样的例子。
- en: Threads are grouped into warps of 32 threads. Each warp is an independent schedulable
    unit for the hardware. The SMs themselves have 8, 16, 32, 48, or more CUDA cores
    within them. Thus, they can schedule at any single point in time a number of warps
    and will switch warps to maintain the throughput of the device. This causes us
    some issues in terms of synchronization. Suppose we have 256 threads in a single
    block. This equates to eight warps. On a compute 2.0 device, with 32 CUDA cores,
    two warps will be running at any single time. There are two warps running and
    not one warp because the hardware actually runs two independent halfwarps per
    shader clock (two full warps per GPU clock). Thus, two warps may make some progress
    in the program while others remain idle.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 线程被分组为 32 个线程的 warp。每个 warp 是硬件可独立调度的单元。SMs 本身有 8、16、32、48 或更多的 CUDA 核心。因此，它们可以在任何单一时刻调度多个
    warp，并会切换 warp 以保持设备的吞吐量。这会在同步方面带来一些问题。假设我们在一个块中有 256 个线程。这相当于 8 个 warp。在一个 compute
    2.0 设备上，拥有 32 个 CUDA 核心，任何时刻都会有两个 warp 在运行。之所以是两个 warp，而不是一个 warp，是因为硬件实际上每个着色器时钟周期运行两个独立的半
    warp（每个 GPU 时钟周期运行两个完整的 warp）。因此，两个 warp 可能会在程序中有所进展，而其他 warp 仍然处于空闲状态。
- en: Let’s assume warps 0 and 1 are the ones that are initially selected by the hardware
    to run. The SMs do not use a conventional time-slicing method, but run until the
    warp is blocked or hits a maximum run period. In principle this is all that is
    needed of the scheduler. As soon as warp 0 issues an operation, arithmetic or
    memory, it will stall and the warp will switch. If all warps follow the same path
    this has the effect of pipelining the operations within a block, one warp at a
    time. This in turn allows for extremely efficient execution of the instruction
    stream across *N* warps.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 warp 0 和 warp 1 是硬件最初选择运行的 warp。SMs 并不使用传统的时间切片方法，而是持续运行直到 warp 被阻塞或达到最大运行周期。从原理上讲，调度器只需要做到这一点。一旦
    warp 0 发出一个操作，无论是算术运算还是内存操作，它都会停顿，然后切换到另一个 warp。如果所有 warp 都走相同的路径，那么就相当于在一个块内对操作进行流水线处理，一次处理一个
    warp。这反过来又可以实现跨 *N* 个 warp 高效执行指令流。
- en: However, this arrangement rarely remains for long, as one or more external dependencies
    will cause one warp to get delayed. For example, let’s assume every warp in the
    block reads from global memory. All but the last warp hit the L1 cache. The last
    warp was unlucky and its data is now being fetched from global memory. If we assume
    a 20-clock-cycle instruction latency and a 600-cycle memory latency, the other
    warps will have progressed 30 instructions by the time the memory request is satisfied.
    If the kernel has a loop, then warps 0..6 could be several iterations ahead of
    warp 7.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种安排很少能维持很长时间，因为一个或多个外部依赖会导致某个 warp 被延迟。例如，假设块中的每个 warp 都从全局内存中读取数据。除了最后一个
    warp，其余的 warp 都命中了 L1 缓存。最后一个 warp 不幸地需要从全局内存中获取数据。如果我们假设指令延迟为 20 时钟周期，内存延迟为 600
    时钟周期，那么其他 warp 会在内存请求被满足时已经推进了 30 条指令。如果内核中有一个循环，那么 warp 0 到 warp 6 可能会比 warp
    7 提前几次迭代。
- en: 'Let’s look at an example of this from [Chapter 9](CHP009.html), adding a dataset.
    To do this we add the following sections of code to the start of the loop:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下来自[第 9 章](CHP009.html)的一个示例，添加一个数据集。为此，我们将以下代码部分添加到循环开始的位置：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '` // ( 8192 elements / 256 threads) / 16 blocks = 2 iterations`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '` // ( 8192 元素 / 256 线程) / 16 块 = 2 次迭代`'
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: What we’ve done here is to store into shared memory the internal GPU clock at
    the start of the accumulation, and then again just prior to the synchronization
    operation. The raw data results are shown in [Table 12.2](#T0015). Notice a few
    things from this data. First, the first run through the data takes more time.
    This is because the data is being fetched from memory rather than the cache. Second,
    notice the actual start time varies between the warps. We can see the even and
    odd warps being scheduled within a few clocks of one another, as you might expect.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的，是将 GPU 时钟在累积开始时的内部时钟存储到共享内存中，然后在同步操作之前再次存储。原始数据结果显示在[表 12.2](#T0015)中。从这些数据中可以注意到几个点。首先，第一次处理数据所需的时间更长。这是因为数据是从内存中获取的，而不是从缓存中获取。其次，注意实际的启动时间在不同的
    Warps 之间有所不同。正如你可能预期的那样，我们可以看到偶数和奇数 Warp 在几个时钟周期内被调度在一起。
- en: Table 12.2 Clock Data from Reduction Example
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12.2 来自归约示例的时钟数据
- en: '![image](../images/F000120u12-01-9780124159334.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000120u12-01-9780124159334.jpg)'
- en: However, even so, there is still quite some variation in the start time at this
    very early stage. [Figure 12.2](#F0015) shows a scatter plot of start times for
    a normalized version. Warps are shown along the *X* axis and cycles on the *Y*
    axis. Notice how we see the alternate warp schedulers issue warps into the SM.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即便如此，在这个非常早期的阶段，启动时间仍然存在相当大的差异。[图 12.2](#F0015) 显示了一个归一化版本的启动时间散点图。*X*轴上显示了
    Warp，*Y*轴上显示了周期。注意到我们可以看到交替的 Warp 调度器将 Warp 发射到 SM 中。
- en: '![image](../images/F000120f12-02-9780124159334.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000120f12-02-9780124159334.jpg)'
- en: FIGURE 12.2 Normalized warp start time distribution.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2 归一化的 Warp 启动时间分布。
- en: As we might expect, given the warps are executed out of order, the timing variation
    by the time we hit the synchronization operation is on the order of 4000 clocks.
    Even though warp 1 started after warp 0, it hits the synchronization point just
    over 3000 cycles later ([Figure 12.3](#F0020)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所预期的，由于 Warp 是无序执行的，到我们执行同步操作时，时间差异大约在 4000 个时钟周期的数量级。即使 Warp 1 在 Warp 0
    之后启动，它仍然会在大约 3000 个周期后到达同步点（[图 12.3](#F0020)）。
- en: '![image](../images/F000120f12-03-9780124159334.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000120f12-03-9780124159334.jpg)'
- en: FIGURE 12.3 Normalized warp sync time distribution.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 归一化的 Warp 同步时间分布。
- en: Clearly, we can see that it is impossible to rely on *any* execution order to
    achieve correct operation. Synchronization points are needed at any point where
    the threads within different warps need to exchange data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们可以看到，无法依赖*任何*执行顺序来实现正确操作。在不同 Warp 中的线程需要交换数据的任何时刻，都需要同步点。
- en: 'We see the same issue when we try to exchange data from different blocks:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们尝试交换来自不同块的数据时，我们会看到相同的问题：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here we have dumped the start time and completion time from thread 0 for those
    blocks running on SM 0\. You can see that initially SM 0 gets wide distribution
    of block IDs as the blocks are distributed to many SMs in turn. We’d expect to
    see that pattern continue, as individual blocks are retired from the SM and new
    blocks introduced.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从线程 0 中导出了那些在 SM 0 上运行的块的开始时间和完成时间。你可以看到，最初 SM 0 会广泛分配块 ID，因为这些块依次分配给多个
    SM。我们预计这种模式会继续下去，因为单个块会从 SM 中退役并引入新的块。
- en: In practice, we see the scheduler add large sets of near linear block IDs to
    each SM. This would suggest the block scheduler is allocating new blocks only
    once a certain threshold of free block slots is reached with a given SM. This
    would be beneficial in terms of localizing the cache accesses, which may in turn
    improve the L1 cache hit rate. However, it comes at the cost of potentially reducing
    the number of available warps for scheduling. Thus, we can see that both warps
    and blocks are distributed in time, and therefore it is essential that any thread-
    or block-based cooperation allows for all elements of the calculation to complete.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们看到调度器将大批接近线性的块 ID 添加到每个 SM。这表明块调度器仅在给定 SM 的空闲块槽位达到某个阈值时才分配新的块。这种做法有助于本地化缓存访问，从而可能提高
    L1 缓存命中率。然而，这也可能导致可调度的 warp 数量减少。因此，我们可以看到，warps 和块是按时间分布的，因此任何基于线程或块的协作都必须确保计算的所有元素都能够完成。
- en: For thread synchronization you need to use the `__syncthreads` primitive and
    can make use of on-chip shared memory. For block-based synchronization you write
    the data to global memory and launch a further kernel.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线程同步，你需要使用 `__syncthreads` 原语，并可以利用片上共享内存。对于块级同步，你可以将数据写入全局内存并启动进一步的内核。
- en: One final point that often trips up people with synchronization is that you
    need to remember that *all* threads in a thread block must reach any barrier synchronization
    primitive such as `__syncthreads` or else your kernel will hang. Therefore, be
    careful of using such primitives within an `if` statement or looping construct,
    as such usage may cause the GPU to hang.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常常让人困惑的同步问题是，你需要记住，*线程块*中的*所有*线程必须到达任何同步屏障原语，如 `__syncthreads`，否则你的内核将会挂起。因此，要小心在
    `if` 语句或循环结构中使用这些原语，因为这种用法可能导致 GPU 挂起。
- en: Atomic operations
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原子操作
- en: As you can see in the previous section, you cannot rely on, or make any assumption
    about, ordering to ensure an output is correct. However, neither can you assume
    a read/modify/write operation will be completed synchronously with the other SMs
    within the device. Consider the scenario of SM 0 and SM 1 both performing a read/modify/write.
    They must perform it in series to ensure the correct answer is reached. If SM
    0 and SM 1 both read 10 from a memory address, add 1 to it, and both write 11
    back, one of the increments to the counter has been lost. As the L1 cache is not
    coherent, this is a very real possibility if more than one block writes to the
    same output address within a single kernel call.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一节所示，你不能依赖或假设排序来确保输出正确。然而，你也不能假设读/修改/写操作会与设备内其他 SM 同步完成。考虑 SM 0 和 SM 1 都执行读/修改/写的场景。为了确保正确的结果，它们必须按顺序执行。如果
    SM 0 和 SM 1 都从一个内存地址读取 10，分别加 1 然后写回 11，那么其中一个计数器的增量将丢失。由于 L1 缓存不是一致的，如果多个块在单个内核调用中写入同一个输出地址，这是一个非常现实的可能性。
- en: Atomic operations are used where we have many threads that need to write to
    a common output. They guarantee that the read/write/modify operation will be performed
    as an entire serial operation. They, however, do not guarantee any ordering of
    the read/write/modify operation. Thus, if both SM 0 and SM 1 ask to perform an
    atomic operation on the same address, which SM goes first is not defined.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有许多线程需要写入一个公共输出时，就使用原子操作。它们保证读/写/修改操作会作为整个串行操作执行。然而，它们并不保证读/写/修改操作的顺序。因此，如果
    SM 0 和 SM 1 都请求对同一地址执行原子操作，哪个 SM 先执行是未定义的。
- en: Let’s consider the classic parallel reduction algorithm. It can be viewed as
    a simple tree as shown in [Figure 12.4](#F0025). We have a number of ways to view
    this operation. We could allocate A, B, C, and D to a single thread and have those
    threads do an atomic add to an output storing (A,B) and (C,D). We then drop down
    to two threads, each of which would add the partial result to the final result.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑经典的并行归约算法。它可以看作是一个简单的树，如[图 12.4](#F0025)所示。我们有多种方式来查看这个操作。我们可以将 A、B、C 和
    D 分配给单个线程，让这些线程进行原子加法操作，将 (A,B) 和 (C,D) 存储到输出中。然后我们降级为两个线程，每个线程会将部分结果加到最终结果中。
- en: '![image](../images/F000120f12-04-9780124159334.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000120f12-04-9780124159334.jpg)'
- en: FIGURE 12.4 Classic reduction.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.4 经典归约。
- en: Alternatively, we could start with the second line and use two threads. Thread
    0 would read the contents of A and B and write it as the designated output address.
    Thread 1 would handle the inputs from C and D. Thread 1 would then drop out, leaving
    thread 0 to add the two partial results. Equally, we could reduce the problem
    to a single thread by simply having thread 0 calculate A + B + C + D.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方式是从第二行开始，使用两个线程。线程 0 会读取 A 和 B 的内容，并将其写入指定的输出地址。线程 1 会处理来自 C 和 D 的输入。然后线程
    1 会退出，留下线程 0 来将两个部分结果相加。同样，我们也可以通过让线程 0 计算 A + B + C + D 来将问题归约为单线程。
- en: The first approach works by considering the destination data writing to a common
    output, a scatter operation. The other approaches work by considering the source
    data and gathering it for use in the next stage. The scatter operation, because
    more than one contributor is writing to the output, requires the use of atomic
    operations. The gather approach completely eliminates the use of atomic operations
    and is therefore usually the preferable solution.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法通过将目标数据写入一个公共输出（即散射操作）来工作。其他方法通过考虑源数据并将其收集以供下一阶段使用来工作。散射操作因为有多个参与者向输出写入数据，所以需要使用原子操作。聚合方法完全消除了原子操作的使用，因此通常是更可取的解决方案。
- en: Atomic operations introduce serialization if, in fact, there is more than a
    single thread trying to perform a write at exactly the same time. If the writes
    are distributed in time such that there is no conflicting write, then an atomic
    operation has no significant cost. However, you cannot say with any certainty
    in a complex system that there will be absolutely no two writes happening at any
    single point in time. Therefore, even if the writes are expected to be sparsely
    distributed in time, we need to use atomics to *ensure* this is always the case.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 原子操作引入了序列化问题，特别是当多个线程确实在完全相同的时间尝试进行写入时。如果写入操作在时间上分布得当，确保没有冲突的写入，那么原子操作的开销就不大。然而，在复杂系统中，你无法百分百确定任何时刻不会有两个写入发生。因此，即便预计写入操作在时间上会稀疏分布，我们仍然需要使用原子操作来*确保*这种情况始终成立。
- en: Given we can replace an atomic write with a gather operation, which does not
    need any form of data locking, does it makes sense to use atomics at all? The
    answer in most cases is the gather approach will be quicker. However, this comes
    at a cost.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们可以用一次聚合操作来替代原子写入，而聚合操作不需要任何形式的数据锁定，那么使用原子操作是否还有意义呢？在大多数情况下，答案是聚合方法会更快。然而，这也有一定的代价。
- en: In our reduction example the addition of two numbers is trivial. Given just
    four numbers, we could easily eliminate all threads and have a single thread add
    the four numbers sequentially. This clearly works for trivial amounts of values,
    but what if we have 32 million values that we have to process in some form of
    reduction?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的归约示例中，两个数字的加法是微不足道的。假设只有四个数字，我们可以轻松地消除所有线程，让一个线程按顺序加这四个数字。这对于少量的值显然是有效的，但如果我们有
    3200 万个值需要以某种形式进行归约处理，情况就不一样了。
- en: We saw in the reduction example from [Chapter 9](CHP009.html) that using a single
    thread on a CPU was slower than two threads, which itself was slower than three.
    There is a clear tradeoff here between the amount of work done by a given thread
    and the overall number of threads running. In the CPU case the maximum throughput
    on our AMD Phenom II 905e system was effectively limited to three threads due
    to memory bandwidth issues on the host.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '在[第 9 章](CHP009.html)的归约示例中，我们看到使用单线程在 CPU 上比使用两个线程更慢，而两个线程又比三个线程更慢。这里显然存在一个权衡问题，即给定线程的工作量与运行的线程总数之间的关系。在
    CPU 的例子中，我们的 AMD Phenom II 905e 系统的最大吞吐量实际上被内存带宽问题限制为三个线程。  '
- en: A more modern processor, such as the Sandybridge-E, has higher host memory bandwidth,
    but at the same time, two additional processor cores (six instead of four). Running
    the same OpenMP reduction on a Sandybridge-E I7 3930 K system produces the results
    shown in [Table 12.3](#T0020) and [Figure 12.5](#F0030). Thus, even if we hugely
    increase the memory bandwidth and increase the core count, we see the same issue
    as before. Using more threads on CPU-based architecture produces progressively
    lower returns as we add more and more cores.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '更现代的处理器，如 Sandybridge-E，具有更高的主机内存带宽，但同时也增加了两个处理器核心（从四个增加到六个）。在 Sandybridge-E
    I7 3930 K 系统上运行相同的 OpenMP 归约操作会产生如[表 12.3](#T0020)和[图 12.5](#F0030)所示的结果。因此，即使我们大幅增加内存带宽并增加核心数量，仍然会出现与之前相同的问题。在基于
    CPU 的架构上使用更多线程时，随着核心数量的增加，性能回报逐渐递减。  '
- en: Table 12.3 OpenMP Scaling on Sandybridge-E
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12.3 Sandybridge-E 上的 OpenMP 扩展性  '
- en: '![Image](../images/T000120tabT0020.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/T000120tabT0020.jpg)  '
- en: '![image](../images/F000120f12-05-9780124159334.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/F000120f12-05-9780124159334.jpg)  '
- en: FIGURE 12.5 OpenMP scaling on Sandybridge-E.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12.5 Sandybridge-E 上的 OpenMP 扩展性。  '
- en: Running only two threads would not make use of the hardware. Running 16 million
    threads and killing half of them every reduction round would also not be a good
    approach on a CPU. On a GPU we could adopt this approach since the GPU creates
    a thread pool that gradually moves through the 32 million threads the programmer
    requested. We can, of course, manually create a similar thread pool on the CPU,
    although we have far fewer cores with which we can run threads.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 仅运行两个线程将无法充分利用硬件。每轮归约时运行 1600 万线程并杀死其中一半，在 CPU 上也不是一个好的方法。在 GPU 上，我们可以采用这种方法，因为
    GPU 会创建一个线程池，逐步处理程序员请求的 3200 万线程。当然，我们也可以在 CPU 上手动创建类似的线程池，尽管我们可用的核心数要少得多。
- en: Our approach with the reduction example from [Chapter 9](CHP009.html) is a gather
    operation mixed with the scatter operation. We schedule a number of blocks based
    on a multiple of the number of SMs physically present on the device. We then divide
    the data set into *N* blocks and have each thread gather the necessary data from
    memory to perform a local, on-chip accumulation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第9章](CHP009.html)中使用的规约示例中采用了一个结合了收集操作和分发操作的方式。我们根据设备上物理存在的SM数量的倍数来调度多个块。然后，我们将数据集划分为*N*个块，每个线程从内存中收集必要的数据，执行本地的片上累加。
- en: Each thread is doing a significant amount of work. We can see from the previous
    timing example that the wider data bus and double the number of SMs on the GTX470
    allow it to complete this operation much quicker than the GTX460\. We want to
    ensure we’re using the parallelism present in the device, be it a GPU or CPU,
    to the maximum.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程都在执行大量工作。从前面的计时示例中我们可以看到，GTX470更宽的数据总线和双倍的SM数量使其能够比GTX460更快地完成此操作。我们希望确保最大化地利用设备中存在的并行性，无论是GPU还是CPU。
- en: Having calculated the partial sums on a per-thread basis, the issue then is
    how to combine the partial sums. This is where atomic operations become necessary
    because the accumulated data is private to the thread. Thus, it’s not possible
    to gather this data from another thread without the source thread writing its
    data somewhere.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在按线程计算出部分和之后，问题是如何合并这些部分和。这时原子操作就变得必要了，因为累加的数据是线程私有的。因此，不可能从另一个线程收集数据，除非源线程将其数据写入某个地方。
- en: A typical compute 2.0 GPU has up to 16 SMs, each of which can run up to 48 warps
    of 32 threads each. Thus, we have up to 24.5 K threads active at any point in
    time. Atomic operations can be performed in shared memory (from compute 1.1 devices
    and later) or in global memory. Shared memory atomics are, not surprisingly, significantly
    faster than having to go all the way out of the SM to global memory for global
    memory–based atomics. As we have up to 16 SMs, the shared memory–based atomics
    are 16 times wider than a write to global memory. Therefore, we want to use shared
    memory atomics wherever possible.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一般的计算2.0 GPU最多有16个SM，每个SM可以运行最多48个warp，每个warp包含32个线程。因此，我们在任何时刻最多有24.5K个线程处于活动状态。原子操作可以在共享内存中执行（从计算1.1设备及更高版本开始）或在全局内存中执行。共享内存的原子操作显然比必须从SM进入全局内存执行全局内存原子操作要快得多。由于我们最多有16个SM，因此基于共享内存的原子操作的带宽是写入全局内存的16倍。因此，我们希望尽可能使用共享内存的原子操作。
- en: Atomic functions as a whole are only available on compute 1.1 devices, which
    is basically any device except the early GTX8800 series cards. 32-bit integer
    atomic operations on shared memory became available in compute 1.2 (the 9800 series
    and later). 64-bit integer atomic operations became available in global memory
    from compute 1.2 devices and in shared memory from compute 2.0 devices (the GTX400
    series).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 作为整体，原子函数仅在计算能力1.1的设备上可用，这几乎包括除了早期的GTX8800系列显卡以外的所有设备。共享内存上的32位整数原子操作从计算能力1.2（即9800系列及更高版本）开始支持。64位整数原子操作从计算能力1.2的设备上的全局内存以及计算能力2.0设备（即GTX400系列）的共享内存中可用。
- en: Single-precision, floating point–based atomic operations are available only
    in compute 2.0 and later. Double-precision atomics are not natively supported
    in any current hardware. However, you can implement them via software. The CUDA
    programming guide provides an example of how to do this using the atomic CAS (compare
    and swap) operation.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 基于单精度浮点数的原子操作仅在计算能力2.0及更高版本中可用。双精度原子操作在当前硬件中并不原生支持。不过，你可以通过软件实现它们。CUDA编程指南提供了如何使用原子CAS（比较与交换）操作实现这一点的示例。
- en: Understanding when to use gather operations and when to use scatter operations
    are often key to achieving both correctness and performance. Think about how best
    to structure the design to minimize the use of atomics (scatters) and maximize
    the use of gather operations instead.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 理解何时使用聚集操作（gather operations）以及何时使用分散操作（scatter operations）通常是实现正确性和性能的关键。考虑如何最好地构建设计，以尽量减少原子操作（分散操作）的使用，而最大化聚集操作的使用。
- en: Algorithmic Issues
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 算法问题
- en: The final type of problem programmers hit is a tricky one. The program runs
    and doesn’t produce any errors, but the answer is wrong.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员遇到的最后一种问题是一个棘手的问题。程序运行时没有产生任何错误，但答案却是错误的。
- en: Back-to-back testing
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 连续测试
- en: Testing is something that is key to a programmer being perceived as either someone
    who writes “good code” or someone who throws together something that occasionally
    works. As a professional programmer you should strive to deliver the best-quality
    software you are able to in the timeframe available. How can you achieve this?
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 测试是决定程序员是否被认为是编写“优秀代码”还是偶尔能正常工作的“胡乱拼凑”的关键。作为一名专业程序员，你应该努力在可用的时间框架内交付出最优质的软件。你该如何做到这一点呢？
- en: Back-to-back testing is a technique that acknowledges that it is much harder
    to write code that executes in parallel than a functionally equivalent set of
    code for a serial processor. With this in mind you always develop, in parallel
    to or prior to the CUDA application, a serial implementation of the problem. You
    then run the identical dataset through both sets of code and compare the output.
    Any difference tells you that you may have an issue.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 连续测试（Back-to-back testing）是一种技巧，它承认编写并行执行的代码比为串行处理器编写功能等效的代码要困难得多。考虑到这一点，你总是在CUDA应用程序的并行开发或之前，先开发问题的串行实现。然后，将相同的数据集通过两组代码运行并比较输出。任何不同之处都表明你可能有问题。
- en: Now why do I only say “may” have an issue? The answer is largely down to if
    you are using floating-point (single- or double-precision) numbers or not. The
    issue with floating-point numbers is rounding and precision. Adding a large series
    of random floating-point numbers on a serial CPU from the lowest array value to
    the highest array value will result in a different value than if you were to add
    the same numbers from the highest array index to the lowest array index. Try it
    and see.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么我只说“可能”会有问题呢？答案主要取决于你是否使用浮动点（单精度或双精度）数字。浮动点数字的问题在于舍入和精度。在串行CPU上，从最低数组值加到最高数组值的随机浮动点数字序列的结果，与从最高数组索引加到最低数组索引的结果是不同的。试试看吧。
- en: Now why is this? Single-precision, floating-point numbers use 24 bits to hold
    the mantissa value and 8 bits to hold the exponent. If we add 1.1e+38 to 0.1e−38
    what do you think the result will be? The answer is 1.1e+38\. The tiny value represented
    by 0.1e−38 is too small to be represented in the mantissa part. Over a large set
    of numbers there will be many of these types of issues. Therefore, the order in
    which the numbers are processed becomes important. To preserve accuracy often
    the best way to solve this issue is to sort the set of numbers and add from the
    lowest number to the largest. However, this introduces potentially a significant
    amount of work, in the terms of the sort, for this enhanced precision.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么会这样呢？单精度浮动点数使用24位来表示尾数值，8位来表示指数。如果我们将1.1e+38加到0.1e−38，你认为结果会是什么？答案是1.1e+38。由0.1e−38表示的微小值太小，无法在尾数部分表示。在一大组数字中，类似的问题会很多。因此，数字处理的顺序变得很重要。为了保持精度，通常最好的解决方法是对数字集合进行排序，从最小的数字加到最大的数字。然而，这会引入潜在的大量工作量，尤其是排序部分，以提高精度。
- en: There are also other issues concerning the handling of floating-point values
    in compute 1.x devices, especially with very small numbers around 0, which may
    cause them to handle floating-point numbers in different ways than the same code
    running on the CPU. Thus, it’s often best to compromise and allow a certain threshold
    of error when dealing with floating-point equivalence tests.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算1.x设备中处理浮动点值时，还存在其他问题，特别是当值非常小并接近0时，可能会导致它们以不同于CPU上运行相同代码的方式处理浮动点数。因此，通常最好是妥协，允许在处理浮动点等价性测试时存在一定的误差阈值。
- en: If you have an existing CPU solution, then it is relatively simple to compare
    the results. With integer-based problems the standard C library function `memcmp`
    (memory compare) is quite sufficient to see if there is a difference between two
    sets of outputs. Usually when there is a programming error on the GPU side, the
    results are not just a little different, but greatly different, so it’s easy to
    say this code does or does not work and at which point in the output the difference
    occurs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已有一个现成的 CPU 解决方案，那么比较结果相对简单。对于基于整数的问题，标准的 C 库函数 `memcmp`（内存比较）完全足够用来判断两组输出之间是否有差异。通常，当
    GPU 端出现编程错误时，结果不仅仅是稍微不同，而是差异很大，因此很容易判断这段代码是否有效，并且能够确定差异出现在输出的哪个位置。
- en: More difficult are aspects where the results match up until a certain point.
    Typically this might be the first 256 values. As 256 is often used as a thread
    count, this points to an error in the block index calculation. Only the first
    32 values being correct points to an error in the thread index calculation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 更难的是某些情况，结果在某一时刻之前完全匹配。通常，这可能是前 256 个值。因为 256 通常被用作线程数，这指向了块索引计算中的错误。如果只有前 32
    个值是正确的，这则指示了线程索引计算中的错误。
- en: Without an already existing CPU implementation, you’ll need to write one or
    use someone else’s implementation that you know works. However, actually writing
    your own serial implementation allows you to formulate the problem and understand
    it much better before attempting a parallel implementation. You have to, of course,
    ensure the serial version produces the expected answer before you start the parallel
    work.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有现成的 CPU 实现，你需要自己编写一个，或者使用你知道有效的其他人的实现。然而，实际上编写自己的串行实现可以帮助你在尝试并行实现之前，更好地理解和构思问题。当然，你必须确保串行版本能够生成预期的答案，才可以开始并行工作。
- en: It also provides a useful benchmark to see if using the GPU is providing a good
    speedup. In this evaluation always consider any transfer times for the PCI-E bus.
    As with the reduction example, we could write a reduction algorithm on the GPU
    that runs much faster than its CPU OpenMP equivalent. However, just sending the
    data to the GPU swamped any execution time saving. Be aware the GPU is not always
    the best solution. Having a CPU counterpart can let you evaluate this decision
    easily. The solution should be about maximizing the use of whatever resources
    are available, CPU and GPU.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 它还提供了一个有用的基准，帮助你判断使用 GPU 是否能带来显著的加速。在此评估过程中，始终要考虑 PCI-E 总线的传输时间。就像在归约示例中一样，我们可以在
    GPU 上编写一个归约算法，它运行速度比 CPU 上的 OpenMP 版本快得多。然而，仅仅将数据传输到 GPU 就会使得任何执行时间的节省都被淹没。要意识到
    GPU 并不总是最佳解决方案。拥有一个 CPU 对应实现可以让你轻松评估这一决策。解决方案应该是尽量最大化利用所有可用资源，无论是 CPU 还是 GPU。
- en: Once the back-to-back test is set up, and there are many such examples where
    we do this in the various examples in this book, you can instantly see if you
    introduce an error. As you see this *at the point* you introduce it, it makes
    finding and identifying the error far easier. Combining this with a version control
    system, or simply always making a new backup after every major step, allows you
    to eliminate a lot of hard debugging effort later in the development cycle.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了连续测试，并且在本书中的多个示例中你会看到这样的例子，你可以立即看到引入错误的地方。因为你是在引入错误的*那个时刻*看到它，这使得查找和识别错误变得更加容易。将这一点与版本控制系统结合使用，或者简单地在每个重要步骤后创建一个新的备份，可以让你在开发周期的后期消除大量困难的调试工作。
- en: Memory leaks
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内存泄漏
- en: Memory leaks are a common problem and something that is not just restricted
    to the CPU domain. A memory leak, as its name suggests, is available memory space
    simply leaking away as the program runs. The most common cause of this is where
    a program allocates, or mallocs, memory space but does not free that space later.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 内存泄漏是一个常见的问题，并且不仅仅局限于 CPU 领域。顾名思义，内存泄漏就是在程序运行时，空闲内存空间不断流失。造成这种情况的最常见原因是程序分配（或
    malloc）了内存空间，但之后没有释放这块空间。
- en: If you have ever left a computer on for weeks at a time, sooner or later it
    will start to slow down. Sometime afterwards it will start to display out of memory
    warnings. This is caused by badly written programs that don’t clean up after themselves.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经让计算机连续运行几周，迟早它会开始变慢。再过一段时间，它会开始显示内存不足的警告。这是由于写得不好的程序没有清理自己的资源所导致的。
- en: Explicit memory management is something you are responsible for within CUDA.
    If you allocate memory, you are responsible for deallocating that memory when
    the program completes its task. You are also responsible for not using a device
    handle or pointer that you previously released back to the CUDA runtime.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 显式内存管理是 CUDA 中你需要负责的任务。如果你分配了内存，当程序完成任务时，你就需要负责释放这块内存。你还需要负责避免使用已经释放回 CUDA 运行时的设备句柄或指针。
- en: Several of the CUDA operations, in particular streams and events, require you
    to create an instance of that stream. During that initial creation the CUDA runtime
    may allocate memory internally. Failing to call `cudaStreamDestroy` or `cudaEventDestory`
    means that memory, which may be both on the host and on the GPU, stays allocated.
    Your program may exit, but without the explicit release of this data by the programmer,
    the runtime does not know it should be released.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 CUDA 操作，特别是流和事件，要求你创建该流的实例。在初次创建时，CUDA 运行时可能会在内部分配内存。如果没有调用`cudaStreamDestroy`或`cudaEventDestroy`，那么这块可能位于主机或
    GPU 上的内存就会保持分配状态。你的程序可能会退出，但如果程序员没有显式释放这些数据，运行时并不知道应该释放它们。
- en: A nice catchall for this type of problem is the `cudaResetDevice` call, which
    completely clears all allocations on the device. This should be the last call
    you make before exiting the host program. Even if you have released all the resources
    you think you have allocated, with a program of a reasonable size, you or a colleague
    on the team may have forgotten one or more allocations. It’s a simple and easy
    way to ensure everything is cleaned up.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这种问题的一个有效方法是调用`cudaResetDevice`，该函数会完全清除设备上的所有内存分配。这应该是你在退出主机程序之前调用的最后一个函数。即使你认为已经释放了所有分配的资源，但在程序规模适中的情况下，你或团队中的同事可能忘记了一个或多个内存分配。这是一个简单且有效的方式，确保所有资源都得到清理。
- en: Finally, a very useful tool available for developers, supported on Linux, Windows,
    and Mac, is the `cuda-memcheck` tool. This can be integrated into `cuda-gdb` for
    Linux and Mac users. For Windows users it’s simply run from the command line
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，开发人员可以使用一个非常有用的工具——`cuda-memcheck`工具，它支持Linux、Windows和Mac平台。Linux和Mac用户可以将其集成到`cuda-gdb`中；而Windows用户则可以直接通过命令行运行。
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The program will execute your kernel and print appropriate error messages should
    your kernel contain any of the following issues:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的内核存在以下问题，程序将执行内核并打印相应的错误信息：
- en: • Unspecified launch failures.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: • 未指定的启动失败。
- en: • Out-of-bounds global memory access.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: • 越界的全局内存访问。
- en: • Misaligned global memory access.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: • 未对齐的全局内存访问。
- en: • Certain hardware-detected exceptions on compute 2.x GPUs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: • 在计算能力为2.x的GPU上检测到的某些硬件异常。
- en: • Errors detected by the `cudaGetLastError` API call.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: • 通过`cudaGetLastError` API调用检测到的错误。
- en: It will run on both debug and release versions of the kernels. In the debug
    mode, due to the additional information present in the executable, the source
    line causing the issue in the source can also be identified.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 它将同时在调试版和发布版内核上运行。在调试模式下，由于可执行文件中包含了额外的信息，因此可以定位到引发问题的源代码行。
- en: Long kernels
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长时间运行的内核
- en: Kernels that take a long time to execute can cause a number of problems. One
    of the most noticeable is slow screen updates when the kernel is executing in
    the background on a device also used to display the screen. To run a CUDA kernel
    and at the same time support a display, the GPU must context switch between the
    display updates and the kernel. When the kernels take a short time, the user has
    little perception of this. However, when they become longer, it can become quite
    annoying to the point of the user not using the program.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时间较长的内核可能会引发一些问题。其中最显著的是当内核在后台执行时，屏幕更新变得缓慢，特别是当设备同时用于显示屏时。为了在运行CUDA内核的同时支持显示，GPU必须在显示更新和内核执行之间进行上下文切换。当内核执行时间较短时，用户对此几乎没有感知。然而，当执行时间变长时，可能会变得非常令人烦恼，甚至导致用户放弃使用该程序。
- en: Fermi attempted to address this issue, and users with compute 2.x hardware or
    better suffer far less from this than those with earlier hardware. However, it
    is still noticeable. Thus, if your application is something like BOINC, which
    uses “spare” GPU cycles, then it will likely get switched off by the user—clearly
    not good.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi尝试解决这个问题，拥有2.x及以上计算能力的硬件用户，比使用较早硬件的用户更少遭遇这个问题。然而，这个问题仍然是可察觉的。因此，如果你的应用像BOINC那样使用“空闲”GPU周期，那么用户很可能会将其关闭——显然这是不好的。
- en: The solution to this issue is to ensure you have small kernels in the first
    instance. If you consider the display needs to be updated every 60 ms, this means
    each screen update takes place at approximately 16 ms intervals. You could break
    up your kernel into sections that would fit within this time period. However,
    that would likely mean your overall problem execution time would increase considerably,
    as the GPU would need to continuously switch between the graphics context and
    the CUDA context.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是首先确保你的内核足够小。如果你考虑到显示每60毫秒需要更新一次，这意味着每次屏幕更新大约发生在16毫秒的间隔内。你可以将内核分成适应这个时间段的多个部分。然而，这可能会导致你的整体问题执行时间显著增加，因为GPU需要不断在图形上下文和CUDA上下文之间切换。
- en: There is no easy solution to this particular issue. Lower-powered machines and
    older (compute 1.x) cards suffer badly from trying to execute CUDA and graphics
    workloads if the CUDA workload becomes significant. Just be aware of this and
    test your program on older hardware to ensure it behaves well. Users often prefer
    slightly slower programs if it means they can still use the machine for other
    tasks.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定问题，没有简单的解决方案。如果CUDA工作负载变得非常重要，低功耗机器和旧版（计算1.x）显卡在尝试执行CUDA和图形工作负载时会遭遇很大的性能问题。只需意识到这一点，并在旧硬件上测试你的程序，确保它的行为正常。用户通常更喜欢稍微慢一点的程序，只要这意味着他们可以继续用于其他任务。
- en: Finding and Avoiding Errors
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找和避免错误
- en: How many errors does your GPU program have?
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你的GPU程序有多少错误？
- en: One of the most beneficial development changes we ever made at CudaDeveloper
    was to move to encapsulating all CUDA API calls in the `CUDA_CALL` macro. We looked
    at this in [Chapter 4](CHP004.html) on setting up CUDA. This is an incredibly
    useful way to free yourself of laboriously checking return values, yet see the
    point in a CUDA program where you introduced an error.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在CudaDeveloper所做的最有益的开发变更之一是将所有CUDA API调用封装在`CUDA_CALL`宏中。我们在[第4章](CHP004.html)中讨论了如何设置CUDA。这是一种非常有用的方式，可以帮助你免去费力地检查返回值，同时还能看到你在CUDA程序中引入错误的地方。
- en: If you are not using such a detection mechanism, the number of errors your kernels
    generates is shown in tools such as Parallel Nsight. Unfortunately, they do not
    pinpoint the error for you. They simply tell you the number of errors returned
    from the execution run. Obviously any value other than zero is not good. Trying
    to track down those errors is then troublesome. It’s usually a case of you not
    checking a return value, which is of course bad programming practice. Either the
    function should handle all errors internally or, if it does not, the caller must
    handle them.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用这种检测机制，内核生成的错误数量会显示在像 Parallel Nsight 这样的工具中。不幸的是，它们不会为你定位错误。它们只是告诉你执行过程中返回的错误数量。显然，任何非零值都不好。然后追踪这些错误就会变得麻烦。通常是因为你没有检查返回值，这显然是糟糕的编程习惯。要么函数应该在内部处理所有错误，要么如果没有处理，调用者必须处理它们。
- en: The errors detected by the runtime are the easy issues to fix. Simply using
    the `CUDA_CALL` macro in every CUDA API, along with `cudaGetLastError()` after
    the kernel has completed, will pick up most problems. The back-to-back testing
    against the CPU code will pick up the vast majority of the functional/algorithmic
    errors in any kernel.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时检测到的错误是容易修复的问题。只需在每个 CUDA API 中使用`CUDA_CALL`宏，并在内核完成后调用`cudaGetLastError()`，即可捕捉到大多数问题。与
    CPU 代码进行的背靠背测试将捕捉到大多数内核中的功能性/算法错误。
- en: Tools such as Memcheck and the Memory Checker tool within Parallel Nsight are
    also extremely useful ([Figure 12.6](#F0035)). One of the most common mistakes
    that often leads to “Unknown Error” being returned after a kernel call is out-of-bounds
    memory access. The Memcheck utility we have already covered. However, the Parallel
    Nsight Debugger can also check for out-of-bounds memory access.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Memcheck 和 Parallel Nsight 中的内存检查器工具也是非常有用的（[图 12.6](#F0035)）。其中一个最常见的错误，通常会导致内核调用后返回“未知错误”，就是越界内存访问。我们已经介绍过
    Memcheck 工具。然而，Parallel Nsight 调试器也可以检查越界内存访问。
- en: '![image](../images/F000120f12-06-9780124159334.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000120f12-06-9780124159334.jpg)'
- en: FIGURE 12.6 Enabling CUDA Memory Checker tool by default.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6 默认启用 CUDA 内存检查器工具。
- en: Selecting the Nsight→Options menu allows you to enable the memory checker during
    sessions where Nsight is running as a debugger. If your kernel then writes out
    of bounds, be it in global memory or shared memory, the debugger will break on
    the out-of-bounds access.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 Nsight→选项菜单可以在 Nsight 作为调试器运行的会话期间启用内存检查器。如果你的内核随后发生越界写入，无论是在全局内存还是共享内存中，调试器将在越界访问时中断。
- en: Note, however, this does not work where the out-of-bounds memory access occurs
    on thread local variables, and enabling this feature slows down the overall execution
    time of the kernel. As it’s only enabled when debugging with Parallel Nsight,
    this is usually not an issue.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 但是请注意，当越界内存访问发生在线程本地变量上时，这个功能不起作用，并且启用此功能会导致内核的整体执行时间变慢。由于它仅在使用 Parallel Nsight
    调试时启用，因此通常不会成为问题。
- en: Enabling this option will also provide some useful information about misaligned
    accesses to memory. Misaligned accesses are not errors in the strictest sense,
    but simply point where, if you could make the access aligned, you may considerably
    improve the kernel’s speed. These messages are written to the Nsight Output window,
    which is one of the many output windows selectable by a dropdown box in Microsoft
    Visual Studio. This is the same output window that the compile error messages
    are written to, usually the bottom pane of the three standard windows that open
    in a Visual Studio project.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 启用此选项还会提供一些关于内存访问不对齐的有用信息。内存访问不对齐严格来说不是错误，只是指出如果你能够让访问对齐，可能会显著提高内核的速度。这些信息会被写入到Nsight输出窗口中，这是Microsoft
    Visual Studio中可通过下拉框选择的众多输出窗口之一。这个输出窗口与编译错误信息输出窗口相同，通常位于Visual Studio项目中打开的三个标准窗口的底部窗格。
- en: Divide and conquer
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分而治之
- en: The divide-and-conquer approach is a common approach for debugging and is not
    GPU specific. However, it’s quite effective, which is why we mention it here.
    It is useful where your kernel is causing some exception that is not handled by
    the runtime. This usually means you get an error message and the program stops
    running or, in the worst case, the machine simply hangs.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 分而治之的方法是调试中的一种常见方法，并非GPU特有。然而，它非常有效，这也是我们在这里提到它的原因。这个方法在内核引发了一个运行时未处理的异常时非常有用。这通常意味着你会得到一个错误信息，程序停止运行，或者在最坏的情况下，机器会直接挂起。
- en: The first approach in this sort of problem should be to run through with the
    debugger, stepping over each line at a high level. Sooner or later you will hit
    the call that triggers the crash. Start with the host debugger, ensuring you are
    using the `CUDA_CALL` macro, and see at which point the error occurs. It’s most
    likely it will be the kernel invocation or the first call into the CUDA API after
    the kernel invocation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此类问题的第一种方法应该是通过调试器逐步执行每一行代码，确保以较高的层次进行逐步调试。迟早你会触发导致崩溃的调用。从主机调试器开始，确保使用`CUDA_CALL`宏，并查看错误发生的具体位置。最可能的情况是它会出现在内核调用或者是内核调用后的第一次CUDA
    API调用。
- en: If you identify the issue as within the kernel, switch to a GPU debugger such
    as Parallel Nsight or CUDA-GDB. Then simply repeat the process following a single
    thread through the kernel execution process. This should allow you to see the
    top-level call that triggers the fault. If not, the cause may be a thread other
    than the one you are tracking. Typically the “interesting” threads are threads
    0 and 32 within any given block. Most CUDA kernel errors that are not otherwise
    detected are either to do with interwarp or interblock behavior not working as
    the programmer imagined they would work.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你确认问题出在内核中，可以切换到GPU调试器，比如Parallel Nsight或CUDA-GDB。然后只需按照单线程的方式重复这个过程，跟踪内核执行过程中的一个线程。这应该能让你看到触发故障的顶级调用。如果没有，这个问题可能出现在你正在追踪的线程以外。通常，“有趣”的线程是任何给定块中的线程0和线程32。大多数CUDA内核错误如果没有被其他方式检测出来，通常是与跨warp或跨block的行为不符合程序员的预期有关。
- en: Single step through the code and check that the answer for every calculation
    is what it is expected to be. As soon as you have one wrong answer, you simply
    have to understand why it’s wrong and often the solution is then clear. What you
    are attempting to do is a very high level binary search. By stepping over the
    code until you hit the failure point, you are eliminating a single level of functionality.
    You can then very quickly identify the problem function/code line.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 单步调试代码，并检查每个计算的答案是否符合预期。当你发现有一个错误答案时，只需理解为什么它是错误的，通常解决方案就会变得清晰。你正在做的是一种非常高级的二分查找。通过逐步跳过代码直到遇到故障点，你实际上是在消除某一层功能。然后你可以非常迅速地定位到问题的函数/代码行。
- en: You can also use this approach without a debugger if for whatever reason you
    have no access to such a debugger within your environment or the debugger is in
    some way interfering with the visibility of the problem. Simply place `#if 0`
    and `#endif` preprocessor directives around the code you wish to remove for this
    run. Compile and run the kernel and check the results. When the code runs error
    free, the error is likely to be somewhere within the section that is removed.
    Gradually reduce the size of this section until it breaks again. The point it
    breaks is a clear indicator of the likely source of the issue.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你由于某种原因无法访问调试器，或者调试器干扰了问题的可见性，你也可以在没有调试器的情况下使用这种方法。只需在你希望去除的代码周围加上`#if 0`和`#endif`预处理指令。编译并运行内核，然后检查结果。当代码运行没有错误时，错误可能就在被移除的部分中。逐步缩小这一部分，直到它再次出错。出错的地方是问题来源的明确指示。
- en: 'You may also wish to try the approach of seeing if the program runs with the
    following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以尝试以下方法来检查程序是否运行正常：
- en: • One block of 1 thread.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: • 一个1线程的块。
- en: • One block of 32 threads.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: • 一个32线程的块。
- en: • One block of 64 threads.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: • 一个64线程的块。
- en: • Two blocks of 1 thread.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: • 两个1线程的块。
- en: • Two blocks of 32 threads.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: • 两个32线程的块。
- en: • Two blocks of 64 threads.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: • 两个64线程的块。
- en: • Sixteen blocks of 1 thread.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: • 十六个1线程的块。
- en: • Sixteen blocks of 32 threads.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: • 十六个32线程的块。
- en: • Sixteen blocks of 64 threads.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: • 十六个块，每块64个线程。
- en: If one or more of these tests fail, it tells you there is some interaction of
    either the threads within a warp, threads within a block, or blocks within a kernel
    launch that is causing the issue. It provides a pointer as to what to look for
    in the code.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些测试中的一个或多个失败，它会告诉你是线程在一个warp内部、线程在一个块内部，还是块在内核启动时的交互引发了问题。它提供了一个指针，指示你在代码中应该查找什么。
- en: Assertions and defensive programming
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 断言和防御性编程
- en: Defensive programming is programming that assumes the caller will do something
    wrong. For example, what is wrong with the following code?
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 防御性编程是假设调用者会做错事的编程。例如，下面的代码有什么问题？
- en: '[PRE18]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The code assumes that `malloc` will return a valid pointer to 1024 bytes of
    memory. Given the small amount of memory we’re requesting, it’s unlikely in reality
    to fail. If it fails, `malloc` returns a null pointer. For the code to work correctly,
    the `free()` function also needs to handle null pointers. Thus, the start of the
    free function might be
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 代码假设`malloc`会返回一个有效的指针，指向1024字节的内存。考虑到我们请求的内存量很小，实际上它失败的可能性不大。如果它失败，`malloc`会返回一个空指针。为了让代码正确工作，`free()`函数也需要处理空指针。因此，`free`函数的开始可能是
- en: '[PRE19]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `free()` function needs to consider both receiving a null pointer and also
    an apparently valid pointer. The `NULL` pointer, however, doesn’t point to a valid
    area of allocated memory. Typically, if you call `free()` with a null or an invalid
    pointer, a function that is written defensively will not corrupt the heap storage,
    but will instead do nothing. Defensive programming is about doing nothing erroneous
    in the case of bad inputs to a function.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '`free()`函数需要同时考虑接收到空指针和看似有效的指针。然而，`NULL`指针并不指向一个有效的已分配内存区域。通常情况下，如果你用空指针或无效指针调用`free()`，一个防御性编写的函数不会损坏堆存储，而是会什么也不做。防御性编程是在函数接收到错误输入时，不做任何错误操作。'
- en: However, this has a rather nasty side effect. While the user no longer sees
    the program crash, neither does the test or quality assurance department, or the
    author for that matter. In fact, the program now silently fails, despite the programming
    errors in the caller. If a function has implicit requirements on the bounds or
    range of an input, this should be checked. For example, if a parameter is an index
    into an array, you should absolutely check this value to ensure the array access
    does not generate an out-of-bounds access. This is a question that is often addressed
    incorrectly.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这有一个相当糟糕的副作用。当用户不再看到程序崩溃时，测试部门、质量保证部门，甚至作者也不会看到这个问题。事实上，程序现在悄无声息地失败了，尽管调用者中存在编程错误。如果一个函数对输入的范围或界限有隐含要求，这应该被检查。例如，如果一个参数是数组的索引，你应该绝对检查这个值，以确保数组访问不会导致越界访问。这是一个经常被错误处理的问题。
- en: 'C provides a very useful construct that is rarely used, except by those programmers
    familiar with good software engineering practices—the `assert` directive. When
    a program fails, to have it fail silently is bad practice. It allows bugs to remain
    in the code and go undetected. The idea behind `assert` is the opposite. If there
    is an error with the parameters passed by the caller, there is a programming error.
    The called function should scream about the issue until it’s fixed. Thus, if a
    null pointer is not allowed as one of the input parameters to the function, then
    replace the `if ptr =! NULL` check with the following:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: C 提供了一种非常有用的构造，但很少被使用，除非是那些熟悉良好软件工程实践的程序员——`assert`指令。当程序失败时，默默地失败是不好的做法。这会让程序中的错误得以保留并未被检测到。`assert`背后的理念正好相反。如果调用者传递的参数有错误，那就是编程错误。被调用的函数应该大声喊出这个问题，直到修复为止。因此，如果不允许空指针作为函数的输入参数之一，那么应将`if
    ptr =! NULL`检查替换为以下内容：
- en: '[PRE20]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This means we no longer require an additional indent, plus we document in the
    code the precondition for entry into the function. Always make sure you place
    a comment above the assertion explaining why the assertion is necessary. It will
    likely fail at some point in the future and you want the caller of that function
    to understand as quickly as possible why their call to the function is invalid.
    That caller may very often be yourself, so it’s in your own best interests to
    ensure it is commented.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们不再需要额外的缩进，同时我们在代码中记录了进入函数的前提条件。务必确保在断言上方加上注释，解释为什么断言是必要的。它可能在未来某个时刻失败，你希望调用该函数的人能尽快理解为什么他们的函数调用无效。这个调用者很可能是你自己，所以为了自己的利益，确保有注释是很重要的。
- en: Six months from now you’ll have forgotten why this precondition was necessary.
    You will then have to search around trying to remember why it was needed. It also
    helps prevent future programmers from removing the “incorrect” assertion and therefore
    making the problem “go away” before the upcoming release. Never do this without
    entirely understanding why the assertion was put there in the first place. In
    almost all cases, removing the `assert` check will simply mask an error later
    in the program.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 六个月后，你可能会忘记为什么这个前提条件是必要的。届时，你需要四处寻找，试图回想为什么它是必须的。这也有助于防止未来的程序员移除“错误的”断言，从而在即将发布的版本之前让问题“消失”。在完全理解为什么一开始要放置断言之前，绝不要这么做。在几乎所有的情况下，移除`assert`检查只会在程序后续的地方掩盖错误。
- en: 'When using assertions, be careful not to mix handling of programming errors
    with valid failure conditions. For example, this following code is incorrect:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用断言时，要小心不要把编程错误的处理与有效的失败条件混淆。例如，以下代码是错误的：
- en: '[PRE21]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: It is a valid condition for `malloc` to return a `NULL` pointer. It does so
    when the heap space is exhausted. This is something the programmer should have
    a valid error handling case for, as it’s something that will always happen eventually.
    Assertions should be reserved for handling an invalid condition, such as index
    out of bounds, default switch case when processing enumerations, etc.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`malloc`返回`NULL`指针是一个有效的情况。当堆空间耗尽时，它会这么做。这是程序员应当有有效错误处理的情况，因为最终这种情况总会发生。断言应该保留用于处理无效条件，例如越界索引、处理枚举时的默认`switch`情况等。'
- en: One of the concerns with using defensive programming and assertions is that
    the processor spends time checking conditions that for the most part will always
    be valid. It can do this on each and every function call, loop iteration, etc.,
    depending on how widespread the use of assertions are. The solution to this issue
    is a simple one—to generate two sets of software, a debug version and a release
    version. If you’re already using a package such as Visual Studio this is inherent
    in the default project setup. Older systems, especially non-IDE-based systems,
    may need this to be set up.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用防御性编程和断言时的一个问题是，处理器需要花费时间检查那些大多数情况下总是有效的条件。这可以在每次函数调用、循环迭代等时进行检查，具体取决于断言使用的广泛程度。解决这个问题的简单方法是生成两组软件，一个调试版和一个发布版。如果你已经在使用诸如Visual
    Studio之类的工具，这在默认项目设置中是固有的。旧系统，尤其是非IDE的系统，可能需要手动设置。
- en: Once done, you can simply generate a version of the `assert` macro, `ASSERT`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，你可以简单地生成`assert`宏的一个版本，即`ASSERT`。
- en: '[PRE22]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This simple macro will include the assertion checks only into the debug code,
    the version you and the quality assurance people test alongside the release version.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的宏只会将断言检查包含到调试代码中，即你和质量保证人员在发布版本旁边测试的版本。
- en: As of the CUDA 4.1 release, it’s now also possible to place assertions into
    device code for compute 2.x devices. This was not something that was previously
    possible due to the inability of the GPU to raise such an exception.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 从CUDA 4.1版本开始，现在也可以在计算2.x设备的设备代码中放置断言。这在之前是不可行的，因为GPU无法抛出此类异常。
- en: Debug level and printing
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调试级别和打印
- en: As well as having a single release and debug version, it’s often useful to have
    a debug level that is easily changeable, for example, by setting the value of
    a global variable, `#define`, or other constant. You may also wish to allow for
    setting such a parameter via the command line, for example `-debug=5` to set debug
    level five, etc.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 除了拥有一个单一的发布版和调试版外，通常还需要有一个可以轻松更改的调试级别，例如通过设置全局变量、`#define`或其他常量的值来设置。你也可能希望通过命令行设置这样的参数，例如`-debug=5`来设置调试级别为5，等等。
- en: 'During development, you can add useful information messages to the code, for
    example:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发过程中，你可以向代码中添加有用的信息消息，例如：
- en: '[PRE23]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`extern unsigned int GLOBAL_ERROR_LEVEL;`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`extern unsigned int GLOBAL_ERROR_LEVEL;`'
- en: '[PRE24]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this example, we’ve created five levels of debug messages. Where the debug
    version of the software is not used, these messages are stripped from the executable
    in a way that does not cause compilation errors.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们创建了五个级别的调试信息。如果没有使用调试版本的软件，这些信息会以一种不会导致编译错误的方式从可执行文件中去除。
- en: '[PRE25]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To call the function, you simply place the macro into the code as shown in the
    previous example. This will work fine in host code, but will not work on device
    code without some minor modifications.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 调用这个函数时，你只需将宏像前面示例中那样插入代码中即可。这在主机代码中可以正常工作，但在设备代码中则需要做一些小的修改才能正常工作。
- en: First, you have to be aware of some issues when printing a message within a
    kernel. Kernel level `printf` is only supported for compute 2.x capability. If
    you try to use `printf` in a kernel that is being compiled for compute 1.x devices,
    you will get an error saying you cannot call `printf` from a global or device
    function. This is not strictly true—it’s simply that it’s not supported for compute
    1.x devices and the target architecture must be compute 2.x.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要了解在内核中打印消息时的一些问题。内核级的 `printf` 仅支持 compute 2.x 能力。如果你尝试在为 compute 1.x
    设备编译的内核中使用 `printf`，你会收到一个错误，提示不能从全局函数或设备函数中调用 `printf`。这不完全准确——实际上，这是因为 compute
    1.x 设备不支持 `printf`，目标架构必须是 compute 2.x。
- en: Let’s assume you have a Fermi-level device so the `printf` call is supported.
    Unless you take care not to, the message will be printed from every thread in
    groups of 32, the warp size. Clearly, as you should be launching tens of thousands
    of threads, simply printing a single message may result in 10,000 plus lines scrolling
    off the top of the terminal window. As the `printf` buffer is of a fixed size,
    and wraps, you will lose the earlier output.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个 Fermi 层次的设备，因此支持 `printf` 调用。除非你特别小心，否则消息会从每个线程以 32 个为一组（即 warp 大小）打印出来。显然，由于你应该启动成千上万的线程，仅仅打印一条消息可能会导致
    10,000 多行信息滚动出终端窗口的顶部。由于 `printf` 缓冲区大小是固定的，并且会循环覆盖，你将会丢失早期的输出。
- en: As the lines can also be printed in any order, we cannot take the order of printing
    to represent the order of execution without also some reference to the time to
    confirm exactly when the message originated. Consequently, we need to identify
    the source of each message and timestamp it.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些行可以按任意顺序打印，我们不能仅凭打印顺序来表示执行顺序，除非我们参考时间戳，以确认消息的具体来源时间。因此，我们需要标识每条消息的来源并为其加上时间戳。
- en: The first issue is easily handled, by having one thread in a block or warp print
    the message. By convention this is usually thread 0\. We might also wish to print
    a message from every warp, so again we select only the first thread from each
    warp to print the message. You may also have some other criteria, such as the
    threads that calculate halo regions, etc. A sample set of code is shown here.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题很容易处理，只需让一个线程在一个块或 warp 中打印消息。按照惯例，这通常是线程 0。我们也可能希望从每个 warp 打印一条消息，因此我们再次只选择每个
    warp 中的第一个线程来打印消息。你可能还有其他标准，比如计算 halo 区域的线程等。这里展示了一段示例代码。
- en: '[PRE26]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This simply looks for a specified block ID and prints the block ID, warp number,
    SM we’re executing on, and the raw clock value.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是寻找指定的块 ID，并打印块 ID、warp 编号、我们正在执行的 SM 和原始时钟值。
- en: '[PRE27]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`B:00002, W:00, SM:05, CLK:24076570`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`B:00002, W:00, SM:05, CLK:24076570`'
- en: Here we’re printing the block index, warp ID, SM the warp is executing on, and
    the raw clock value. You can simply redirect this output to a file and then plot
    a scatter graph. As we chose to place the device `printf` at the start of the
    kernel, it shows when each kernel is invoked.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们打印的是块索引、warp ID、warp 正在执行的 SM 以及原始时钟值。你可以简单地将这个输出重定向到一个文件中，然后绘制一个散点图。由于我们选择将设备
    `printf` 放在内核的开始位置，它显示了每个内核何时被调用。
- en: In [Figure 12.7](#F0040), the SMs are shown on the vertical axis with absolute
    clock time on the horizontal axis. We can see all the SMs start at around the
    same time, except a few SMs that start a little later, again all together. We
    then see a mostly random distribution of timestamps as each block prints its details
    at the start of its execution. The distribution depends entirely on the program
    you execute and the time for external resources to become available, global memory
    being the primary example.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 12.7](#F0040)中，SMs 显示在纵轴上，绝对时钟时间显示在横轴上。我们可以看到所有 SMs 几乎同时开始，除了少数几个 SM 开始得稍晚，但它们仍然是一起开始的。接着我们看到每个块在其执行开始时打印其详细信息的时间戳几乎是随机分布的。这个分布完全依赖于你执行的程序以及外部资源可用的时间，全球内存是最主要的例子。
- en: '![image](../images/F000120f12-07-9780124159334.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000120f12-07-9780124159334.jpg)'
- en: FIGURE 12.7 Warp execution by the 14 SMs (GTX470).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7 由 14 个 SM 执行的 warp（GTX470）。
- en: With multiple GPUs or multiple streams, we also have the issue of identification
    of where the message originated. This again can be simply handled by prefixing
    the message with a unique identifier. In several examples we have used a string
    created from the device ID string, `device_prefix`, to do exactly this when using
    multiple GPUs. However, the API for extracting this information is a host-side
    call, not a device-side call. This makes sense as we wouldn’t want 30,000 threads
    each getting the device ID string, as it would be the same for all of them. Therefore,
    what we can do is provide this host-side information via global or constant memory.
    If we have one GPU, one stream, this is not necessary, but any nontrivial programs
    will be using both streams and multiple GPUs where available.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多个GPU或多个流时，我们还面临着识别消息来源的问题。这可以通过在消息前加上唯一标识符来简单处理。在多个示例中，我们使用了由设备ID字符串`device_prefix`生成的字符串来实现这一点，尤其是在使用多个GPU时。然而，提取这些信息的API是一个主机端调用，而不是设备端调用。这是有道理的，因为我们不希望每30,000个线程都获取设备ID字符串，因为它们的值是相同的。因此，我们可以通过全局或常量内存来提供这个主机端的信息。如果只有一个GPU和一个流，这就不必要了，但任何复杂的程序都会在有多个GPU和流的情况下运行。
- en: In the case of multiple GPUs, you will see a noticeable change in the clock
    values. Thus, it’s quite easy to see the output streams are from different devices,
    but which came from device 0, 1, 2, or 3? For identical devices, we can’t say.
    What if these messages originate from different streams on the same device?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个GPU的情况下，你会看到时钟值的明显变化。因此，很容易看出输出流来自不同的设备，但到底是来自设备0、1、2还是3呢？对于相同的设备，我们无法确定。如果这些消息来自同一设备的不同流呢？
- en: Using the absolute TID (thread ID) value is sufficient to identify messages
    for single GPU kernels. However, a combination of device number, TID, and stream
    number is required where either multiple streams and/or devices are used.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 使用绝对TID（线程ID）值就足够识别单个GPU内核的消息。然而，若使用多个流和/或设备时，必须结合设备编号、TID和流编号来识别。
- en: 'The ordering issue is a problem in terms of viewing the output only. You should
    create a prefix in the following form:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 排序问题仅仅是查看输出时的问题。你应该创建一个如下格式的前缀：
- en: '[PRE28]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With this prefix, it’s possible to redirect the output to a file and simple
    sort using a sort that preserves the relative ordering. We then end up with all
    the messages, in order, for each GPU and stream.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个前缀，我们可以将输出重定向到文件，并使用一种可以保持相对顺序的排序方式进行简单排序。然后，我们就能得到每个GPU和流的所有消息，按顺序排列。
- en: Note that although `printf` is an easy way to display information at the host
    end, be aware that it’s creating a 1 MB buffer in GPU memory and transferring
    that buffer back to the host upon certain events.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管`printf`是一种在主机端显示信息的简便方式，但请注意它会在GPU内存中创建一个1MB的缓冲区，并在某些事件发生时将该缓冲区传回主机。
- en: 'Thus, the `printf` output will be seen only under the following conditions:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`printf`输出只有在以下条件下才会显示：
- en: 1. At the start of a *subsequent* kernel launch.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 在*后续*内核启动时。
- en: 2. At the end of a kernel execution if the environment variable `CUDA_LAUNCH_BLOCKING`
    is set (not recommended if using multiple GPUs or streams).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 在内核执行结束时，如果设置了环境变量`CUDA_LAUNCH_BLOCKING`（如果使用多个GPU或流，不推荐这样做）。
- en: 3. As the result of a host side–initiated synchronization point such as synchronizing
    the device, stream, or an event.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 作为主机端发起的同步点的结果，例如同步设备、流或事件。
- en: 4. Blocking versions of `cudaMemcpy`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 阻塞版本的`cudaMemcpy`。
- en: 5. Programmer-initiated device reset (`cudaDeviceReset` or driver `cuCtxDestroy`
    API calls).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 程序员发起的设备重置（`cudaDeviceReset`或驱动程序`cuCtxDestroy` API 调用）。
- en: Thus, in most cases you will see the output printed. If you do not, simply call
    `cudaDeviceReset` prior to exiting the host program or `cudaStreamSynchronize`
    at the end of the set of work from a stream and the missing output should appear.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在大多数情况下，你会看到输出已打印出来。如果没有，简单地在退出主机程序之前调用`cudaDeviceReset`，或者在流的一组工作结束时调用`cudaStreamSynchronize`，缺失的输出应该会出现。
- en: Should you need a larger buffer, this can be set using the `cudaDeviceSetLimit(cudaLimitPrintFifoSize,
    new_size_in_bytes)` API call.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要更大的缓冲区，可以使用`cudaDeviceSetLimit(cudaLimitPrintFifoSize, new_size_in_bytes)`
    API 调用来设置。
- en: Version control
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 版本控制
- en: Version control is a key aspect of any professional software development. It
    does not necessitate using very expensive tools or huge processes that cover who
    can update what. In large projects version control is absolutely essential. However,
    even for single-developer projects, something that may apply to many readers,
    it is important.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制是任何专业软件开发中的一个关键方面。它并不要求使用非常昂贵的工具或庞大的流程来管理谁可以更新什么。在大型项目中，版本控制是绝对必要的。然而，即使是单人开发的项目，对于许多读者来说，版本控制也同样重要。
- en: Consider for a moment that debugging a 30,000-thread program is easy. If you
    laugh at this statement then you realize just how hard a task you are setting
    yourself up for by not versioning your program, either regularly or whenever a
    major point is reached. Programmers are generally a fairly overconfident bunch
    of people and can be sure at the outset that a “simple” change will work without
    problems. However, when it doesn’t quite work to plan, remembering exactly the
    set of changes you made can be difficult. Without a working backup of the program
    it can be difficult if nearly impossible to get back to exactly the working version
    before the changes.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 想一想，调试一个包含30,000个线程的程序是不是很简单。如果你对这个说法感到好笑，那么你就意识到，如果不定期版本控制你的程序，或者在每次到达一个重大节点时进行版本控制，你将给自己设置一个多么困难的任务。程序员通常是一个相当自信的群体，刚开始时他们可能会很确定一个“简单”的修改会毫无问题地工作。然而，当它并没有按照预期工作时，准确记住你所做的所有修改可能会很困难。如果没有程序的工作备份，回到修改前的工作版本几乎是不可能的。
- en: Most programs in the professional world are developed in teams. A colleague
    can be extremely helpful in providing a fresh pair of eyes with which to see a
    problem. If you have a versioned or baselined copy of the working code it makes
    it relatively easy to look simply at the differences and see what is now breaking
    the previously working solution. Without these periodic baselines it’s not easy
    to identify the place where the error might be, and thus instead of a few hundred
    lines of code, you may have to look at a few thousand.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 专业领域中的大多数程序都是由团队开发的。一个同事能提供非常有帮助的“新眼睛”，帮助看清问题。如果你拥有一个版本化或基准化的工作代码副本，那么仅仅查看差异就能相对轻松地看到现在是什么导致了原本正常工作的解决方案出现问题。没有这些定期的基准版本，就很难确定错误可能出现的位置，因此你可能需要查看几千行代码，而不是几百行。
- en: Developing for Future GPUs
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为未来 GPU 开发
- en: Kepler
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kepler
- en: The roadmap that NVIDIA has presented from Fermi and later versions is the Kepler
    GK104 (K10), the Kepler GK110 (K20), and the Maxwell. As of March 2012 the first
    of the Kepler releases was made, the GK104\. This product was aimed squarely at
    the consumer market and lacked some of the features that some aspects of the HPC
    (high-performance computing) market would have liked to see, specifically significant
    dual-precision math support. Kepler GK110 will almost certainly be a far more
    HPC-focused product that will likely end up in some form or another as a consumer
    card. The GK110 is scheduled for release at the end of 2012, but the design is
    already in use internally at NVIDIA for development of the CUDA 5 release that
    will accompany it.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 从 Fermi 及其后续版本展示的路线图包括 Kepler GK104（K10）、Kepler GK110（K20）和 Maxwell。2012
    年 3 月，Kepler 系列的第一个版本发布，即 GK104。该产品完全针对消费者市场，但缺少了一些高性能计算（HPC）市场可能希望看到的特性，特别是对双精度数学运算的支持。Kepler
    GK110 几乎可以肯定会成为一个更加面向 HPC 的产品，并可能以某种形式或另一种形式最终成为消费者卡。GK110 计划在 2012 年底发布，但该设计已经在
    NVIDIA 内部用于开发将伴随其发布的 CUDA 5 版本。
- en: Let’s look briefly at the changes Kepler brings to the table. First and foremost,
    it brings energy efficiency. The Kepler GTX680 has a TDP rating of 195 watts as
    compared with GTX580 at 244 watts. This is just over a 20% reduction in absolute
    power usage of the top-end single-consumer GPU. Looking more closely at the GTX680
    it is actually closer to the GTX560 (GF114) in architecture than the GTX580 (GF110),
    being somewhat like an internally doubled-up version of the GTX560.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要看一下 Kepler 带来的变化。首先，它带来了能效提升。Kepler GTX680 的 TDP（热设计功率）为 195 瓦特，而 GTX580
    为 244 瓦特。这意味着顶级单用户 GPU 的功耗减少了超过 20%。仔细看看 GTX680，它的架构实际上比 GTX580（GF110）更接近 GTX560（GF114），有点像
    GTX560 的内部双倍版。
- en: If, however, we look at power usage in terms of watts per gigaflop, then you
    see the Kepler GK104 outperforming Fermi GF110 by a factor of up to two. NVIDIA’s
    own studies on common consumer games ([NVIDIA, May 18, 2012](#BIB1)) show an average
    of 1.5× better performance per watt. Many of today’s games are highly complex,
    and thus it’s reasonable to expect a comparable power usage profile on compute-based
    applications.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们从每吉浮点运算（gigaflop）所消耗的功率来考虑，那么可以看到 Kepler GK104 的表现比 Fermi GF110 高出最多两倍。NVIDIA
    自己对常见消费者游戏的研究（[NVIDIA, 2012年5月18日](#BIB1)）显示每瓦性能提高了 1.5 倍。今天的许多游戏都非常复杂，因此可以合理预期计算型应用程序也会有类似的功率使用模式。
- en: By being highly selective in terms of binning components, the GTX690 (a dual-GPU
    version of the GTX680) significantly outperforms even the GTX680 in terms of gigaflops
    per watt. A doubling or more in terms of performance per watt is a huge achievement
    on the part of the team at NVIDIA. The GTX690 is the basis of the Tesla K10 range.
    This is the first time a Tesla product will be a dual-GPU solution.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在组件分选方面非常严格，GTX690（GTX680 的双 GPU 版本）在每瓦计算性能上显著超过了 GTX680。每瓦性能翻倍或更多是 NVIDIA
    团队的一项巨大成就。GTX690 是 Tesla K10 系列的基础。这是 Tesla 产品首次采用双 GPU 解决方案。
- en: Although peak global memory bandwidth has remained the same from the GTX580
    to the GTX680, we have now transitioned from PCI-E 2.0 to the PCI-E 3.0 specification.
    Thus, transfers to and from the card under a PCI-E 3.0 motherboard with a PCI-E
    3.0–enabled CPU are double the speed of the PCI-E 2.0 400/500 series cards. This
    doubling of bandwidth should see significant speedups for certain PCI-E-limited
    kernels.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管从 GTX580 到 GTX680，峰值全局内存带宽保持不变，但我们已经从 PCI-E 2.0 过渡到了 PCI-E 3.0 规范。因此，在配备 PCI-E
    3.0 启用 CPU 的 PCI-E 3.0 主板上，卡与卡之间的传输速度是 PCI-E 2.0 400/500 系列卡的两倍。这种带宽的翻倍应能为某些 PCI-E
    限制的内核带来显著的速度提升。
- en: The Kepler GTX680/GTX690 moves us from the compute 2.1 level to the compute
    3.0 level, with the Kepler GK110 being targeted as a compute 3.5 device. A summary
    of the new compute levels is shown in [Table 12.4](#T0025).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Kepler GTX680/GTX690 将计算级别从 2.1 升级到 3.0，Kepler GK110 被定位为计算 3.5 级设备。新的计算级别总结见
    [表 12.4](#T0025)。
- en: Table 12.4 New Compute Levels in Kepler
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12.4 Kepler 中的新计算级别
- en: '![Image](../images/T000120tabT0025.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/T000120tabT0025.jpg)'
- en: ^∗Plus an additional 64 dual-precision units per SM.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗每个 SM 还增加了额外的 64 个双精度单元。
- en: One of the major changes in Kepler was the elimination of the shader clock.
    Prior to Kepler, the GPU ran at a given GPU clock frequency and the shader clock
    was multiplied internally by a factor of 2\. In previous generations, it was the
    shader clock and not the GPU clock that drove the execution of the CUDA cores
    within the device.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Kepler 中的一个主要变化是消除了着色器时钟。在 Kepler 之前，GPU 以给定的 GPU 时钟频率运行，而着色器时钟则内部被乘以 2。在之前的世代中，驱动
    CUDA 核心执行的是着色器时钟，而非 GPU 时钟。
- en: Clock rate is a significant driver of power consumption in any processor design.
    In eliminating the shader clocker, NVIDIA has to lay out double the number of
    CUDA cores per SM to achieve the same throughput. This tradeoff significantly
    reduced overall power consumption and allowed NVIDIA to push the core clock from
    772 MHz all the way up to just over 1 GHz.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: The Kepler GK104 design actually increases the number of CUDA cores by four.
    It doubles the number load/store units (LSUs), special functional units (SFUs),
    instruction dispatchers, and the size of the register file. The shared memory/L1
    cache remains unchanged at 64 KB, but can now be split in a 32 K/32 K in addition
    to the usual 16 K/48 K split.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: This choice is interesting in that a large amount of additional compute power
    has been added. If we look to previous generations, we see the move from the GT200
    (compute 1.3) to the GF110 (compute 2.0) devices from 24 warps per SM to 48 warps
    per SM. The Kepler GK104 design increases the total warp count per SM to 64 and
    the total thread count per SM to 2048.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The GTX680 claims a peak performance of 3 teraflops compared with the claimed
    1.5 teraflops of the GTX580\. This peak performance is based on executing floating-point
    multiply and add (FMAD) operations. Of course, in any real usage there is a huge
    variation in instruction makeup and memory access patterns, which ultimately determine
    real performance levels.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the Kepler GK104 now features dynamic clock adjustment where it
    will ramp down and up the clock according to the current GPU loading. We’ve seen
    this feature for years on the CPU side, which helps significantly in saving power,
    especially when the device itself is not in use.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: In terms of instruction evolution, the major benefit we see is a shuffle instruction
    that allows communication between threads within a single warp. This is a huge
    benefit in that threads within a warp can now cooperate without the need to share
    data via shared memory. The final stages of reduction operations and prefix sum
    can be easily accelerated with such operations. Additional compiler intrinsics
    have become available for hardware-level shift, rotate, and access to the texture
    memory as a simple additional 48 K read-only cache without the overhead of having
    to write texture memory code. Four byte, packed vector instructions (`add, subtract,
    average, abs, min, max`) are also introduced.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在指令演化方面，我们看到的主要好处是引入了洗牌指令，允许在同一warp内的线程之间进行通信。这是一个巨大的优势，因为warp内的线程现在可以在无需通过共享内存共享数据的情况下进行协作。减少操作和前缀和的最终阶段可以通过这种操作轻松加速。额外的编译器内建函数已经可以用于硬件级别的移位、旋转，以及访问纹理内存，作为一个简单的附加48K只读缓存，而无需编写纹理内存代码的开销。还引入了四字节打包向量指令（`add,
    subtract, average, abs, min, max`）。
- en: The Kepler GK110 (K20) has some very attractive features from the compute perspective—the
    technologies NVIDIA refer to as dynamic parallelism, Hyper-Q, and RDMA. It also
    almost doubles the number of SMs per device and adds the missing double-precision
    floating-point units necessary for significant numbers of HPC applications. Initial
    (NVIDIA) figures indicate in excess of 1 teraflop of double-precision performance.
    The memory bus has been increased from 256 bits to 384 bits, which if we see similar
    clocks to the GK104, should result in a memory bandwidth in excess of 250 GB/s.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Kepler GK110（K20）从计算角度来看具有一些非常吸引人的特性——NVIDIA所称的动态并行性、Hyper-Q和RDMA技术。它还几乎将每个设备的SM数量翻倍，并增加了重要的双精度浮点单元，这对于大量HPC应用至关重要。初步（NVIDIA）的数据显示，双精度性能超过1
    teraflop。内存总线从256位增加到384位，如果我们看到类似GK104的时钟频率，应该会导致超过250 GB/s的内存带宽。
- en: The first of these technologies, dynamic parallelism, allows us for the first
    time to easily launch additional work from a GPU kernel. Previously, this was
    implemented by either oversubscribing the thread blocks and leaving some idle
    or by running multiple kernels. The former is wasteful of resources and works
    poorly, especially for large problems. The latter means there are periods where
    the GPU is underutilized and prevents kernels from maintaining data in the high-speed
    shared memory/cache as this memory is not persistent between kernel launches.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术中的第一个，动态并行性，使我们第一次能够轻松地从GPU内核启动额外的工作。以前，这通过过度订阅线程块并让一些线程空闲，或者通过运行多个内核来实现。前者浪费资源，特别是在处理大型问题时效果较差。后者意味着GPU存在低利用率的时期，并且防止内核在高速度共享内存/缓存中保持数据，因为这种内存在内核启动之间并不是持久的。
- en: The second of these technologies is Hyper-Q, which addresses the difference
    between the programmer exposed stream model and how it’s actually implemented
    in the hardware. All streams up to and including Kepler GK104 are implemented
    in the hardware as a single pipe. Thus, a stream of kernels from stream 0 will
    not be intermixed with a stream of kernels from stream 1, despite the programmer
    explicitly specifying, via putting these kernels into separate streams, that they
    are independent work units.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术中的第二个是Hyper-Q，它解决了程序员暴露的流模型与硬件实际实现之间的差异。直到包括Kepler GK104的所有流，都在硬件中作为单一管道实现。因此，尽管程序员通过将这些内核放入不同的流中明确指定它们是独立的工作单元，但来自流0的内核流与来自流1的内核流不会混合。
- en: Hyper-Q breaks this single hardware stream into 32 separate hardware queues.
    Thus, up to 32 streams from perhaps a set of a few hundred programmer-defined
    streams are available to be independently run on the hardware. The main benefit
    of this is in terms of loading the device. With 192 plus cores per SM, the granularity
    of an SM has increased considerably. The resources within an SM can therefore
    be wasted if small kernels are run that only partially load an SM.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Hyper-Q将这个单一的硬件流打破为32个独立的硬件队列。因此，最多可以有32个流，可能来自几百个程序员定义的流，这些流可以独立地在硬件上运行。其主要好处体现在设备负载方面。每个SM有超过192个核心，SM的粒度大大增加。因此，如果运行的小内核仅部分加载SM，SM中的资源可能会浪费。
- en: Finally, RDMA (remote direct memory access) is also an interesting technology.
    NVIDIA has been working with certain vendors, noticeably on the Infiniband side,
    to improve the latency of GPU-to-GPU communications between nodes. Currently,
    the peer-to-peer function supports communication between GPUs within the node
    directly over the PCI-E bus. For cards and OSs supporting this, it avoids the
    need to go indirectly via the CPU memory space.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，RDMA（远程直接内存访问）也是一项有趣的技术。NVIDIA一直在与某些厂商，尤其是在Infiniband方面，合作，以提高节点间GPU之间通信的延迟。目前，点对点功能支持在节点内直接通过PCI-E总线进行GPU之间的通信。对于支持此功能的卡和操作系统，它避免了通过CPU内存空间间接进行通信的需求。
- en: However, to send or receive data from a non-GPU device (e.g., an I/O device
    such as a network card), the best case is a shared area of pinned memory on the
    host. The RDMA feature changes that in that it allows the GPU to talk over the
    PCI-E bus directly to other PCI-E cards, not just NVIDIA GPUs. Currently, this
    is only supported for some Infiniband cards, but it opens up the potential for
    the use of other cards, such as direct data acquisition, FPGAs, RAID controllers,
    and the like, to be able to talk directly to a GPU. This will be an interesting
    technology to watch develop.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要从非GPU设备（例如网络卡等I/O设备）发送或接收数据，最佳情况是主机上一个共享的固定内存区域。RDMA功能改变了这一点，因为它允许GPU通过PCI-E总线直接与其他PCI-E卡通信，不仅仅是NVIDIA的GPU。目前，这仅支持某些Infiniband卡，但它为其他卡的使用开辟了潜力，比如直接数据采集、FPGA、RAID控制器等，可以直接与GPU进行通信。这将是一个值得关注的技术发展。
- en: What to think about
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Developing code that will run many years into the future, or at least be able
    to be run in the future, is always a difficult issue. The more something is tuned
    to be fast on one particular set of hardware, the less portable code will be in
    terms of future development. Thus, one strategy is to ensure any code you develop
    is parameterized so it can easily be adapted for future GPUs.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Often an application will be tailored to a particular architecture. Thus, you
    might have a code section such as the following:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now what happens if a compute 2.2 or compute 3.0 architecture is released? In
    the sample program we’ll drop through to the compute 1.x path (the G80/G92/G200
    series). The users of your program don’t want to replace their Fermi-class GPU
    with a new Kepler card and find your program runs slower or not at all on their
    brand-new graphics card. When writing such code, assume you may also come across
    an unknown computer level and cater for it accordingly.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: With the move from G200 to Fermi there was a transition period, where authors
    had to reissue programs because the number of blocks executed per SM remained
    the same between generations, only the number of threads per block increased.
    If a kernel was already using the maximum number of blocks per SM, which allowed
    for the best instruction mix and thus good performance, no additional blocks got
    scheduled onto the SMs. Thus, the new hardware went unused and the existing software
    did not run any faster on the new hardware.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: The major transition between G200 and Fermi was the need to increase the number
    of threads per block. The maximum number of threads per block, a property that
    can be queried, went from a maximum of 512 to 1024\. At the same time the number
    of resident warps has increased from 24 (compute 1.0/1.1) to 32 (compute 1.2/1.3)
    to 48 (compute 2.0/2.1). Thus, it’s likely in the future we’ll continue to see
    such a trend, with blocks containing larger and larger thread numbers. Kepler
    was the first GPU architecture to also increase the block count per SM, doubling
    it from 8 to 16 blocks. Thus, the optimal number of threads, to schedule the maximum
    number of blocks, shifts back to 2048 threads ÷ 16 blocks = 128 threads per block.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: We can work out the number of warps available from simply querying the number
    of threads and the warp size. The `cudaDeviceProp` structure returns `warpSize`
    and `maxThreadsPerBlock`. Thus, we can call `cudaGetDeviceProperties(&device_props)`
    API and then divide the number of threads per block by the number of warps to
    work out the maximum number of warps on a given GPU.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: This approach would work well for Kepler GK104 and also the upcoming Kepler
    GK110\. However, it does not take account of the changes in the programming model
    that the GK110 will bring. The dynamic parallelism aspect of the GK110, now that
    it’s public, can clearly be planned for. NVIDIA showed some work at GTC (GPU Technology
    Conference), where it claimed this feature alone, primarily the elimination of
    the CPU control overhead, would leads to quite significant speedups on many codes.
    It also leads to greatly simpler forms of recursion, where the recursive part
    can increase the amount of parallelism as the number of nodes expands and contracts
    depending on the data that is encountered.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'One important aspect that you can implement into programs today to run on Kepler
    hardware is the use of the dedicated 48 K read-only texture cache without the
    need to do texture memory programming. This will require only that you declare
    read-only pointers with the C99 standard `___restrict__` keyword, so for example:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In this example by adding this keyword we’re saying that any writes to the parameter
    `out_ptr` will have no effect on the memory region pointed to by `in_ptr`. In
    effect, we’re saying that the two pointers do not alias one another. This will
    cause the reads via `in_ptr` to be cached in the texture cache, giving an additional
    48 K of read-only cache memory. Potentially this could significantly reduce off-chip
    access to global memory and thus significantly improve memory throughput.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: The Hyper-Q logic is also something you should think about in terms of what
    elements of existing kernels can be performed in parallel. For the first time
    task-level parallelism will be truly possible on the GPU. To prepare for this,
    if you currently run a series of kernels, split these into independent streams,
    one stream for every independent task. This will not adversely affect code performance
    when running on your current platform, but will prepare those kernels to execute
    better on Kepler once this feature becomes available.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the new K10 Tesla product is a dual GPU, based on the currently available
    GTX690 consumer card. As with CPUs, if you’re using only a single core you’re
    wasting 50% plus of the available compute capability. Thus, anyone planning to
    install the K10 product will need to move their existing code to support multiple
    GPUs. We covered this in [Chapter 8](CHP008.html). You’ll need to think about
    where the data resides and if any communication between the GPUs will be necessary.
    Moving to a multi-GPU solution today will make the transition much easier and
    provide almost linear scaling for many applications.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Further Resources
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many CUDA resources available on the Internet and through a large
    number of universities worldwide. We run sessions for professionals wishing to
    learn CUDA on an individual or group basis. As such, I try to attend, in person
    or online, as many courses about CUDA as possible each year. As CUDA is a great
    passion of mine, I’ve read every book published to date on this subject. I’d,
    therefore, like to provide some information here about the various CUDA resources
    for anyone wishing to learn more about CUDA. This information is also available
    from our website [*www.learncuda.com*](http://www.learncuda.com), a portal for
    the various CUDA resources available worldwide.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Online courses
  id: totrans-312
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the great successes of CUDA is the commitment from NVIDIA to bring CUDA
    to a wider audience. If we look back in time, there have been many attempts to
    bring parallel programming to the mainstream and many languages designed to enable
    the use of parallel constructs. With the exception of perhaps OpenMP, and to a
    lesser extent MPI, all have failed. This is largely because they never escaped
    the niche group they were created for, did not have a major backer willing to
    invest in training, and were often restricted to a small number of machines owned
    by universities, governments, or corporations.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we start with one of the best resources for CUDA, NVIDIA’s own page on
    training: [*http://developer.nvidia.com/cuda-training*](http://developer.nvidia.com/cuda-training).
    Here you can access a number of recorded lectures from various universities, including:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: ECE-498AL, [*http://courses.engr.illinois.edu/ece498al/*](http://courses.engr.illinois.edu/ece498al/)
    — a course taught by Professor Wen-mei W. Hwu, author of the first major textbook
    on CUDA. Available from the 2010 course are lecture audio recording and slides.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Stanford CS193G, [*http://code.google.com/p/stanford-cs193g-sp2010/*](http://code.google.com/p/stanford-cs193g-sp2010/)
    — a course run by Stanford University based on the ECE-498 course. Includes recorded
    lecture videos available via iTunes. Taught by Jared Hoberock and David Tarjan.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Winsconsin ME964, [*http://sbel.wisc.edu/Courses/ME964/*](http://sbel.wisc.edu/Courses/ME964/)
    — a course on high-performance computing applications in engineering, with links
    to lecture videos and a number of interesting guest lectures. Taught by Dan Negrut.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: EE171 Parallel Computer Architecture, [*http://www.nvidia.com/object/cudau_ucdavis*](http://www.nvidia.com/object/cudau_ucdavis)
    — an excellent course covering data-level parallelism, instruction-level parallelism,
    and thread-level parallelism from the architecture perspective. Taught by John
    Owens, University of California—Davis.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The next major source of online information is the recorded GPU conference archives.
    Usually every year NVIDIA holds a conference in San Jose, California, where they
    are based, called the GPU Technology Conference. These are actually held worldwide
    in various locations. About a month after the conference, the various sessions
    are uploaded to NVIDIA’s GPU technology portal at [*http://www.gputechconf.com/gtcnew/on-demand-gtc.php*](http://www.gputechconf.com/gtcnew/on-demand-gtc.php).
    There are far too many sessions to attend since, like many conferences, sessions
    overlap with one another. You can view almost all of the sessions online going
    back a number of years. Also available are other conferences where NVIDIA has
    recorded sessions.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: The keynotes, especially those by Jen-Hsun Huang, are always very interesting
    to listen to and give a great insight into the future of GPU technology. The keynote
    on the DARPA challenge by Sebastian Thrun shows just how wide the range of CUDA
    applications is, for example, with GPUs being used to autonomously control a car.
    Various talks by Paulius Micikevicius are available focusing on CUDA optimization,
    as well as one talk by Vasily Volkov on occupancy, which is also interesting to
    watch.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: The next major source of online information is the archived webinars provided
    by NVIDIA that can be found at [*http://developer.nvidia.com/gpu-computing-webinars*](http://developer.nvidia.com/gpu-computing-webinars).
    The webinar series is aimed at registered CUDA developers. Registration is free
    and allows you access to the webinars live. Live attendance allows you to ask
    questions and provide feedback on a particular subject of interest. Sometime after
    the webinar is over, the archived versions usually become available. The webinar
    series tends to focus on new innovations in CUDA, the API, and may also have sessions
    on vendor-specific tools.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: There are also many other resources available on CUDA and parallel computing.
    Visit [*www.learncuda.com*](http://www.learncuda.com) for a complete list.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Taught courses
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many universities teach CUDA as part of parallel programming courses. Often
    it is taught alongside OpenMP and MPI, which are the dominant intercore and intranode
    programming models used today. NVIDIA provides a very useful tool to identify
    where CUDA is being taught around the world, so you can find a course near you:
    [*http://research.nvidia.com/content/cuda-courses-map*](http://research.nvidia.com/content/cuda-courses-map).
    As of mid-2012, NVIDIA was listing 500 plus universities around the world teaching
    CUDA.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Books
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are a number of books written that cover CUDA. No single book will cover
    every aspect of CUDA and/or parallel programming. You may wish to read the following
    additional texts:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: • *CUDA by Example* by Jason Sanders
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: • *CUDA Application Design and Development* by Rob Farber
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: • *Programming Massively Parallel Processors* by D. Kirk and Wen-mei W. Hwu
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: • *GPU Computing Gems*, Emerald and Jade Editions, by various authors
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: I’ve ordered these books in terms of how I’d rate them for accessibility to
    new CUDA/GPU programmers. All of these books are highly rated on consumer sites
    such as Amazon, so they are well worth the investment.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA CUDA certification
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The CUDA certification program is a program run by NVIDIA to allow you to demonstrate
    to a potential employer that you have achieved a certain level of competence in
    CUDA. It consists of a number of multiple-choice questions and a number of programming
    assignments that have to be completed within a given timeframe. The syllabus for
    the exam is covered at NVIDIA’s website at [*http://developer.nvidia.com/nvidia-cuda-professional-developer-program-study-guide*](http://developer.nvidia.com/nvidia-cuda-professional-developer-program-study-guide)
    and [*http://developer.nvidia.com/cuda-certification*](http://developer.nvidia.com/cuda-certification).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: The material you need to cover largely overlaps with the *Programming Massively
    Parallel Processors* textbook. The questions are highly programming focused. You
    are expected to have a good knowledge of CUDA, both in terms of being able to
    write a number of CUDA kernels from scratch and understanding what makes for efficient
    and high-performance code. This text you are reading covers many significant aspects
    of the certification exam, but not everything you might be asked. In many areas
    this text goes far beyond what is necessary for the certification. Throughout
    the text there are question and answer sections that require you to think and
    understand the examples provided in the various chapters. It is through working
    with such questions and adapting the examples so that you will gain the most understanding.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: You will also be expected to keep abreast of new developments in CUDA that may
    not necessarily be listed in the syllabus but are covered by other aspects like
    webinars and training provided by NVIDIA.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have finally reached the end of a book that attempts to cover CUDA from
    a practitioner’s perspective. I hope you have learned a significant amount about
    CUDA, GPUs, CPUs, and how to write efficient programs.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: I hope too that your view on GPUs and the use of CUDA is one of excitement.
    The older serial model of programming is dead. Parallel architectures, be it on
    a GPU or a CPU, are the future of computing. You are at a tipping point in history
    where parallel computing is finally gathering enough practitioners and is being
    driven from the computing industry as the only answer to increasing computational
    throughput.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Having to think as programmers in a parallel manner is becoming ever more the
    norm. Our everyday smart phones now have or are moving to dual-core processors.
    Most tablet-based PCs are dual core. Of those PCs used for gaming, the vast majority
    of the home PC market, some 92%, are now multicore machines. Just fewer than 50%
    of those machines are running NVIDIA GPUs ([Steam, April 14, 2012](#BIB2)).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: CUDA has a huge potential to revolutionize parallel processing, both in the
    consumer arena and the business market. You can purchase a top-end consumer Kepler
    graphics card (GeForce GTX680) for around $500 USD. The GPU industry is still
    riding the curve of doubling performance every couple of years and looks set to
    continue this for at least the near future. It’s an exciting time to be someone
    learning to program GPUs.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-341
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1\. NVIDIA, “NVIDIA’s Next Generation Compute Architecture: Kepler GK110.”
    Available at [*http://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf*](http://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf),
    accessed May 18, 2012.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Steam, “Consumer Hardware Survey.” Available at [*http://store.steampowered.com/hwsurvey*](http://store.steampowered.com/hwsurvey),
    accessed April 14, 2012.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
