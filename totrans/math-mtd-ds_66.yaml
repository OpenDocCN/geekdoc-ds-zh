- en: '8.5\. Building blocks of AI 3: neural networks#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap08_nn/05_nn/roch-mmids-nn-nn.html](https://mmids-textbook.github.io/chap08_nn/05_nn/roch-mmids-nn-nn.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Today''s paper shows that it is possible to implement John Von Neumann''s claim:
    "With 4 parameters I can ﬁt an elephant, and with 5 I can make him wiggle his
    trunk"'
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Paper here: [https://t.co/SvVrLuRFNy](https://t.co/SvVrLuRFNy) [pic.twitter.com/VG37439vE7](https://t.co/VG37439vE7)'
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Fermat's Library (@fermatslibrary) [February 20, 2018](https://twitter.com/fermatslibrary/status/965971333422120962?ref_src=twsrc%5Etfw)
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this section, we introduce neural networks. Unlike the previous examples
    we encountered, this one is not convex. Based on the theory we developed in Chapter
    3, finding a local minimizer is the best we can hope for in general from descent
    methods. Yet, in many application settings, stochastic gradient descent (and some
    variants) have proven very effective at computing a good model to fit the data.
    Why that is remains an open question.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: We describe the basic setup and apply it to classification on the Fashion-MNIST
    dataset. As we will see, we will get an improvement over multinomial logistic
    regression (with some help from a different optimization method). We use a particular
    architecture referred as a multilayer perceptron (MLP)\(\idx{multilayer perceptron}\xdi\).
    These are a special class of progressive functions.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.1\. Multilayer perceptron[#](#multilayer-perceptron "Link to this heading")
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each of the main layers of a feedforward neural network\(\idx{neural network}\xdi\)
    has two components, an affine map and a nonlinear activation function\(\idx{activation
    function}\xdi\). For the latter, we restrict ourselves here to the sigmoid function\(\idx{sigmoid}\xdi\)
    (although there are many [other choices of activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: The Jacobian of the elementwise version of the sigmoid function (which we will
    need later on)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n})), \]
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: as a function of several variables can be computed from \(\sigma'\), i.e., the
    derivative of the single-variable case. Indeed, we have seen in a previous example
    that \(J_{\bsigma}(\mathbf{t})\) is the diagonal matrix with diagonal entries
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma'(t_j) = \frac{e^{-t_j}}{(1 + e^{-t_j})^2} = \sigma(t_j) (1 - \sigma(t_j)),
    \qquad j=1, \ldots, n, \]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: which we denote
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma'(\mathbf{t})) = \mathrm{diag}(\bsigma(\mathbf{t})
    \odot (\mathbf{1} - \bsigma(\mathbf{t}))), \]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bsigma'(\mathbf{t}) = (\sigma'(t_1), \ldots,\sigma'(t_{n}))\) and \(\mathbf{1}\)
    is the all-one vector.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'We consider an arbitrary number of layers\(\idx{layer}\xdi\) \(L+2\). As a
    special case of progressive functions\(\idx{progressive function}\xdi\), hidden
    layer\(\idx{hidden layer}\xdi\) \(i\), \(i=1,\ldots,L\), is defined by a continuously
    differentiable function \(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1}, \mathbf{w}_{i-1})\)
    which takes two vector-valued inputs: a vector \(\mathbf{z}_{i-1} \in \mathbb{R}^{n_{i-1}}\)
    fed from the \((i-1)\)-st layer and a vector \(\mathbf{w}_{i-1} \in \mathbb{R}^{r_{i-1}}\)
    of parameters specific to the \(i\)-th layer'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑任意数量的层\(\idx{layer}\xdi\) \(L+2\)。作为渐进函数\(\idx{progressive function}\xdi\)的特殊情况，隐藏层\(\idx{hidden
    layer}\xdi\) \(i\)，\(i=1,\ldots,L\)，由一个连续可微的函数 \(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1},
    \mathbf{w}_{i-1})\) 定义，它接受两个向量值输入：一个来自 \((i-1)\) 层的向量 \(\mathbf{z}_{i-1} \in \mathbb{R}^{n_{i-1}}\)
    和一个特定于第 \(i\) 层的参数向量 \(\mathbf{w}_{i-1} \in \mathbb{R}^{r_{i-1}}\)
- en: '\[ \bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}}
    \to \mathbb{R}^{n_{i}}. \]'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}}
    \to \mathbb{R}^{n_{i}}. \]'
- en: The output \(\mathbf{z}_i\) of \(\bfg_{i-1}\) is a vector in \(\mathbb{R}^{n_{i}}\)
    which is passed to the \((i+1)\)-st layer as input. Each component of \(\bfg_{i-1}\)
    is referred to as a neuron. Here \(r_{i-1} = n_{i} n_{i-1}\) and \(\mathbf{w}_{i-1}
    = (\mathbf{w}^{(1)}_{i-1},\ldots,\mathbf{w}^{(n_{i})}_{i-1})\) are the parameters
    with \(\mathbf{w}^{(k)}_{i-1} \in \mathbb{R}^{n_{i-1}}\) for all \(k\). Specifically,
    \(\bfg_{i-1}\) is given by
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bfg_{i-1}\) 的输出 \(\mathbf{z}_i\) 是一个 \(\mathbb{R}^{n_{i}}\) 中的向量，它作为输入传递到第
    \((i+1)\) 层。\(\bfg_{i-1}\) 的每个组件被称为一个神经元。在这里 \(r_{i-1} = n_{i} n_{i-1}\) 和 \(\mathbf{w}_{i-1}
    = (\mathbf{w}^{(1)}_{i-1},\ldots,\mathbf{w}^{(n_{i})}_{i-1})\) 是参数，其中 \(\mathbf{w}^{(k)}_{i-1}
    \in \mathbb{R}^{n_{i-1}}\) 对所有 \(k\) 都成立。具体来说，\(\bfg_{i-1}\) 由以下给出
- en: \[ \bfg_{i-1}(\mathbf{z}_{i-1},\mathbf{w}_{i-1}) = \bsigma\left(\mathcal{W}_{i-1}
    \mathbf{z}_{i-1}\right) = \left(\sigma\left(\sum_{j=1}^{n_{i-1}} w^{(1)}_{i-1,j}
    z_{i-1,j}\right), \ldots, \sigma\left(\sum_{j=1}^{n_{i-1}} w^{(n_{i})}_{i-1,j}
    z_{i-1,j}\right)\right) \]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfg_{i-1}(\mathbf{z}_{i-1},\mathbf{w}_{i-1}) = \bsigma\left(\mathcal{W}_{i-1}
    \mathbf{z}_{i-1}\right) = \left(\sigma\left(\sum_{j=1}^{n_{i-1}} w^{(1)}_{i-1,j}
    z_{i-1,j}\right), \ldots, \sigma\left(\sum_{j=1}^{n_{i-1}} w^{(n_{i})}_{i-1,j}
    z_{i-1,j}\right)\right) \]
- en: where we define \(\mathcal{W}_{i-1} \in \mathbb{R}^{n_{i} \times n_{i-1}}\)
    as the matrix with rows \((\mathbf{w}_{i-1}^{(1)})^T,\ldots,(\mathbf{w}_{i-1}^{(n_{i})})^T\).
    As we have done previously, to keep the notation simple, we ignore the constant
    term (or “bias variable”\(\idx{bias variable}\xdi\)) in the linear combinations
    of \(\mathbf{z}_{i-1}\). It can be incorporated by adding a \(1\) input to each
    neuron as illustrated below. We will not detail this complication here.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(\mathcal{W}_{i-1} \in \mathbb{R}^{n_{i} \times n_{i-1}}\) 为具有行 \((\mathbf{w}_{i-1}^{(1)})^T,\ldots,(\mathbf{w}_{i-1}^{(n_{i})})^T\)
    的矩阵。像之前一样，为了简化符号，我们忽略了线性组合中的常数项（或“偏差变量”\(\idx{bias variable}\xdi\)）。可以通过向每个神经元添加一个
    \(1\) 输入来包含它，如下所示。我们在此不详细说明这种复杂性。
- en: '**Figure:** Each component of \(g_i\) is referred to as a neuron ([Source](https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png))'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：** \(g_i\) 的每个组件被称为一个神经元 ([来源](https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png))'
- en: '![Neuron](../Images/228200787298b2f894943501d2a42428.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![神经元](../Images/228200787298b2f894943501d2a42428.png)'
- en: \(\bowtie\)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: The input layer\(\idx{input layer}\xdi\) is \(\mathbf{z}_0 := \mathbf{x}\),
    which we refer to as layer \(0\), so that \(n_0 = d\).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层\(\idx{input layer}\xdi\) 是 \(\mathbf{z}_0 := \mathbf{x}\)，我们将其称为层 \(0\)，因此
    \(n_0 = d\)。
- en: Similarly to multinomial logistic regression, layer \(L+1\) (i.e., the output
    layer\(\idx{output layer}\xdi\)) is the softmax function
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与多项式逻辑回归类似，层 \(L+1\)（即输出层\(\idx{output layer}\xdi\)）是 softmax 函数
- en: \[ \hat{\mathbf{y}} := \mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_L \mathbf{z}_{L}), \]
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\mathbf{y}} := \mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_L \mathbf{z}_{L}), \]
- en: but this time we compose it with a linear transformation. We implicitly assume
    that \(\bgamma\) has \(K\) outputs and, in particular, we have that \(n_{L+1}
    = K\).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但这次我们将其与线性变换组合。我们隐含地假设 \(\bgamma\) 有 \(K\) 个输出，特别是我们有 \(n_{L+1} = K\)。
- en: So the output of the classifier with parameters \(\mathbf{w} =(\mathbf{w}_0,\ldots,\mathbf{w}_{L})\)
    on input \(\mathbf{x}\) is
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，具有参数 \(\mathbf{w} =(\mathbf{w}_0,\ldots,\mathbf{w}_{L})\) 的分类器在输入 \(\mathbf{x}\)
    上的输出是
- en: \[ \bfh(\mathbf{w}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}). \]
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfh(\mathbf{w}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}). \]
- en: What makes this an MLP is that, on each layer, all outputs feed into the input
    of the next layer. In graphical terms, the edges between consecutive layers form
    a complete bipartite graph.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使其成为MLP的原因是，在每个层上，所有输出都馈入下一层的输入。在图形术语中，连续层之间的边形成一个完全二分图。
- en: '![Feedforward neural network (with help from Claude; inspired by (Source))](../Images/271cf3797d7e5cd7669d7f17ae49618b.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![前馈神经网络（得益于Claude的帮助；受（来源）启发）](../Images/271cf3797d7e5cd7669d7f17ae49618b.png)'
- en: We again use cross-entropy\(\idx{cross-entropy}\xdi\) as the loss function (although
    there are many [other choices of loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions)).
    That is, we set
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用交叉熵\(\idx{cross-entropy}\xdi\)作为损失函数（尽管有许多[其他损失函数的选择](https://pytorch.org/docs/stable/nn.html#loss-functions)）。也就是说，我们设置
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_i. \]
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_i. \]
- en: Finally,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，
- en: \[ f(\mathbf{w}) = \ell(\mathbf{h}(\mathbf{w})). \]
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{w}) = \ell(\mathbf{h}(\mathbf{w})). \]
- en: 8.5.2\. A first example[#](#a-first-example "Link to this heading")
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5.2\. 第一个示例[#](#a-first-example "链接到这个标题")
- en: Before detailing a general algorithm for computing the gradient, we adapt our
    first progressive example to this setting to illustrate the main ideas. Suppose
    \(d=3\), \(L=1\), \(n_1 = n_2 = 2\), and \(K = 2\), that is, we have one hidden
    layer and our output is 2-dimensional. Fix a data sample \(\mathbf{x} = (x_1,x_2,x_3)
    \in \mathbb{R}^3, \mathbf{y} = (y_1, y_2) \in \mathbb{R}^2\). For \(i=0, 1\),
    we use the notation
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细说明计算梯度的通用算法之前，我们将我们的第一个渐进示例适应到这个设置中，以说明主要思想。假设\(d=3\)，\(L=1\)，\(n_1 = n_2
    = 2\)，\(K = 2\)，即我们有一个隐藏层，我们的输出是二维的。固定一个数据样本\(\mathbf{x} = (x_1,x_2,x_3) \in \mathbb{R}^3,
    \mathbf{y} = (y_1, y_2) \in \mathbb{R}^2\)。对于\(i=0, 1\)，我们使用以下符号
- en: \[\begin{split} \mathcal{W}_{0} = \begin{pmatrix} w_0 & w_1 & w_2\\ w_3 & w_4
    & w_5 \end{pmatrix} \quad \text{and} \quad \mathcal{W}_{1} = \begin{pmatrix} w_6
    & w_7\\ w_8 & w_9 \end{pmatrix} \end{split}\]
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathcal{W}_{0} = \begin{pmatrix} w_0 & w_1 & w_2\\ w_3 & w_4
    & w_5 \end{pmatrix} \quad \text{and} \quad \mathcal{W}_{1} = \begin{pmatrix} w_6
    & w_7\\ w_8 & w_9 \end{pmatrix} \end{split}\]
- en: and let
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - y_1 \log \hat{y}_1
    - y_2 \log \hat{y}_2. \]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - y_1 \log \hat{y}_1
    - y_2 \log \hat{y}_2. \]
- en: The layer functions are as follows
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 层函数如下
- en: \[ \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})
    \quad\text{with}\quad \mathbf{w}_0 = (w_0, w_1, w_2, w_3, w_4, w_5) \]\[ \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1}) \quad\text{with}\quad
    \mathbf{w}_1 = (w_6, w_7, w_8, w_9). \]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})
    \quad\text{with}\quad \mathbf{w}_0 = (w_0, w_1, w_2, w_3, w_4, w_5) \]\[ \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1}) \quad\text{with}\quad
    \mathbf{w}_1 = (w_6, w_7, w_8, w_9). \]
- en: We seek to compute the gradient of
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图计算
- en: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
    &= H(\mathbf{y}, \bgamma(\mathcal{W}_{1} \bsigma(\mathcal{W}_{0} \mathbf{x}))\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right] \end{align*}\]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
    &= H(\mathbf{y}, \bgamma(\mathcal{W}_{1} \bsigma(\mathcal{W}_{0} \mathbf{x}))\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right] \end{align*}\]
- en: where
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在哪里
- en: \[\begin{align*} Z &= \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\\ & \quad + \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)
    + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)). \end{align*}\]
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} Z &= \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\\ & \quad + \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)
    + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)). \end{align*}\]
- en: 'In the forward phase, we compute \(f\) itself and the requisite Jacobians:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向阶段，我们计算\(f\)本身和所需的雅可比矩阵：
- en: \[\begin{align*} &\mathbf{z}_0 := \mathbf{x}\\ & = (x_1, x_2, x_3)\\ &\mathbf{z}_1
    := \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})\\
    &= \begin{pmatrix} \sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)\\ \sigma(w_3 x_1 + w_4
    x_2 + w_5 x_3) \end{pmatrix}\\ &J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := J_{\bsigma}(\mathcal{W}_{0}
    \mathbf{z}_{0}) \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_0] & \mathbb{B}_{2}[\mathbf{z}_0]
    \end{pmatrix}\\ &= \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \begin{pmatrix}
    \mathcal{W}_{0} & I_{2\times 2} \otimes \mathbf{z}_0^T \end{pmatrix}\\ &= \begin{pmatrix}
    \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \mathcal{W}_{0} & \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{z}_{0})) \otimes \mathbf{z}_0^T \end{pmatrix}, \end{align*}\]
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\mathbf{z}_0 := \mathbf{x}\\ & = (x_1, x_2, x_3)\\ &\mathbf{z}_1
    := \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})\\
    &= \begin{pmatrix} \sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)\\ \sigma(w_3 x_1 + w_4
    x_2 + w_5 x_3) \end{pmatrix}\\ &J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := J_{\bsigma}(\mathcal{W}_{0}
    \mathbf{z}_{0}) \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_0] & \mathbb{B}_{2}[\mathbf{z}_0]
    \end{pmatrix}\\ &= \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \begin{pmatrix}
    \mathcal{W}_{0} & I_{2\times 2} \otimes \mathbf{z}_0^T \end{pmatrix}\\ &= \begin{pmatrix}
    \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \mathcal{W}_{0} & \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{z}_{0})) \otimes \mathbf{z}_0^T \end{pmatrix}, \end{align*}\]
- en: where we used the *Chain Rule* to compute the Jacobian of \(J_{\bfg_0}\).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了**链式法则**来计算 \(J_{\bfg_0}\) 的雅可比矩阵。
- en: Then
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[\begin{align*} &\hat{\mathbf{y}} := \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1)
    = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\\ &= \begin{pmatrix} Z^{-1} \exp(w_6\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\\ Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)) \end{pmatrix}\\
    &J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):= J_{\bgamma}(\mathcal{W}_{1} \mathbf{z}_{1})
    \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_1] & \mathbb{B}_{2}[\mathbf{z}_1] \end{pmatrix}\\
    &= [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \begin{pmatrix} \mathcal{W}_{1} & I_{2\times
    2} \otimes \mathbf{z}_1^T \end{pmatrix}\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} & [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \otimes \mathbf{z}_1^T \end{pmatrix},
    \end{align*}\]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\hat{\mathbf{y}} := \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1)
    = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\\ &= \begin{pmatrix} Z^{-1} \exp(w_6\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\\ Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)) \end{pmatrix}\\
    &J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):= J_{\bgamma}(\mathcal{W}_{1} \mathbf{z}_{1})
    \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_1] & \mathbb{B}_{2}[\mathbf{z}_1] \end{pmatrix}\\
    &= [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \begin{pmatrix} \mathcal{W}_{1} & I_{2\times
    2} \otimes \mathbf{z}_1^T \end{pmatrix}\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} & [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \otimes \mathbf{z}_1^T \end{pmatrix},
    \end{align*}\]
- en: where \(Z\) was introduced previously and we used the expression for \(J_{\bgamma}\)
    from the previous subsection.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(Z\) 是之前引入的，我们使用了上一小节中 \(J_{\bgamma}\) 的表达式。
- en: Finally
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后
- en: \[\begin{align*} &f(\mathbf{w}) := \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}})\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\ &J_{\ell}(\hat{\mathbf{y}})
    = - (\mathbf{y} \oslash \hat{\mathbf{y}})^T. \end{align*}\]
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &f(\mathbf{w}) := \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}})\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\ &J_{\ell}(\hat{\mathbf{y}})
    = - (\mathbf{y} \oslash \hat{\mathbf{y}})^T. \end{align*}\]
- en: We first compute the partial derivatives with respect to \(\mathbf{w}_1 = (w_6,
    w_7, w_8, w_9)\). For this step, we think of \(f\) as the composition of \(\ell(\mathbf{z}_2)\)
    as a function of \(\mathbf{z}_2\) and \(\bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \bgamma(\mathcal{W}_1
    \mathbf{z}_1)\) as a function of \(\mathbf{w}_1\). Here \(\mathbf{z}_1\) does
    not depend on \(\mathbf{w}_1\) and therefore can be considered fixed for this
    calculation. By the *Chain Rule* and the *Properties of the Kronecker Product
    (f)*, we get
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算关于\(\mathbf{w}_1 = (w_6, w_7, w_8, w_9)\)的偏导数。对于这一步，我们将\(f\)视为\(\ell(\mathbf{z}_2)\)作为\(\mathbf{z}_2\)的函数以及\(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \bgamma(\mathcal{W}_1 \mathbf{z}_1)\)作为\(\mathbf{w}_1\)的函数的复合。在这里，\(\mathbf{z}_1\)不依赖于\(\mathbf{w}_1\)，因此可以在这个计算中将其视为固定。通过**链式法则**和**Kronecker积的性质**，我们得到
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6}
    & \frac{\partial f(\mathbf{w})}{\partial w_7} & \frac{\partial f(\mathbf{w})}{\partial
    w_8} & \frac{\partial f(\mathbf{w})}{\partial w_9} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \otimes \mathbf{z}_1^T \right\}\\ &= \left\{- (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}
    \mathbf{z}_{1}))^T \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \right\} \otimes \mathbf{z}_1^T\\ &=
    (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T \otimes \mathbf{z}_1^T\\
    &= (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T,
    \end{align*}\]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6}
    & \frac{\partial f(\mathbf{w})}{\partial w_7} & \frac{\partial f(\mathbf{w})}{\partial
    w_8} & \frac{\partial f(\mathbf{w})}{\partial w_9} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \otimes \mathbf{z}_1^T \right\}\\ &= \left\{- (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}
    \mathbf{z}_{1}))^T \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \right\} \otimes \mathbf{z}_1^T\\ &=
    (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T \otimes \mathbf{z}_1^T\\
    &= (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T,
    \end{align*}\]
- en: where we used the *Properties of the Hadamard Product* and the fact that \(\hat{\mathbf{y}}
    = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\) similarly to a calculation we did
    in the multinomial logistic regression setting.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了**Hadamard积的性质**以及\(\hat{\mathbf{y}} = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\)的事实，这与我们在多项式逻辑回归设置中进行的计算类似。
- en: To compute the partial derivatives with respect to \(\mathbf{w}_0 = (w_0, w_1,
    \ldots, w_5)\), we first need to compute partial derivatives with respect to \(\mathbf{z}_1
    = (z_{1,1}, z_{1,2})\) since \(f\) depends on \(\mathbf{w}_0\) through it. For
    this calculation, we think again of \(f\) as the composition \(\ell(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1))\), but this time our focus is on the variables \(\mathbf{z}_1\).
    This is almost identical to the previous calculation, except that we use the block
    of \(J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1)\) corresponding to the partial derivatives
    with respect to \(\mathbf{z}_1\) (i.e., the “\(A\)” block). We obtain
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算关于\(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\)的偏导数，我们首先需要计算关于\(\mathbf{z}_1
    = (z_{1,1}, z_{1,2})\)的偏导数，因为\(f\)通过它依赖于\(\mathbf{w}_0\)。对于这个计算，我们再次将\(f\)视为\(\ell(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1))\)的复合函数，但这次我们的重点是变量\(\mathbf{z}_1\)。这几乎与之前的计算相同，只是我们使用了对应于关于\(\mathbf{z}_1\)的偏导数的\(J_{\bfg_1}(\mathbf{z}_1,
    \mathbf{w}_1)\)的块（即“\(A\)”块）。我们得到
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}
    & \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} \right\}\\ &= - (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}\mathbf{z}_1))^T
    \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \end{align*}\]
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}
    & \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} \right\}\\ &= - (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}\mathbf{z}_1))^T
    \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \end{align*}\]
- en: The vector \(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}, \frac{\partial
    f(\mathbf{w})}{\partial z_{1,2}}\right)\) is called an adjoint.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 \(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}, \frac{\partial f(\mathbf{w})}{\partial
    z_{1,2}}\right)\) 被称为伴随向量。
- en: We now compute the gradient of \(\mathbf{f}\) with respect to \(\mathbf{w}_0
    = (w_0, w_1, \ldots, w_5)\). For this step, we think of \(f\) as the composition
    of \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) as a function of \(\mathbf{z}_1\)
    and \(\bfg_0(\mathbf{z}_0, \mathbf{w}_0)\) as a function of \(\mathbf{w}_0\).
    Here \(\mathbf{w}_1\) and \(\mathbf{z}_0\) do not depend on \(\mathbf{w}_0\) and
    therefore can be considered fixed for this calculation. By the *Chain Rule*
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们计算 \(\mathbf{f}\) 关于 \(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\) 的梯度。为此，我们将
    \(f\) 视为 \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) 作为 \(\mathbf{z}_1\) 的函数和
    \(\bfg_0(\mathbf{z}_0, \mathbf{w}_0)\) 作为 \(\mathbf{w}_0\) 的函数的组合。在这里，\(\mathbf{w}_1\)
    和 \(\mathbf{z}_0\) 不依赖于 \(\mathbf{w}_0\)，因此可以在这个计算中视为固定。根据**链式法则**
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0}
    & \frac{\partial f(\mathbf{w})}{\partial w_1} & \frac{\partial f(\mathbf{w})}{\partial
    w_2} & \frac{\partial f(\mathbf{w})}{\partial w_3} & \frac{\partial f(\mathbf{w})}{\partial
    w_4} & \frac{\partial f(\mathbf{w})}{\partial w_5} \end{pmatrix}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \left[\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))
    \otimes \mathbf{z}_0^T\right]\\ &= [(\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
    \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))] \otimes
    \mathbf{z}_0^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{x}))] \otimes \mathbf{x}^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1}
    \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x}) \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0}
    \mathbf{x})))] \otimes \mathbf{x}^T \end{align*}\]
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0}
    & \frac{\partial f(\mathbf{w})}{\partial w_1} & \frac{\partial f(\mathbf{w})}{\partial
    w_2} & \frac{\partial f(\mathbf{w})}{\partial w_3} & \frac{\partial f(\mathbf{w})}{\partial
    w_4} & \frac{\partial f(\mathbf{w})}{\partial w_5} \end{pmatrix}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \left[\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))
    \otimes \mathbf{z}_0^T\right]\\ &= [(\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
    \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))] \otimes
    \mathbf{z}_0^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{z}_{0}))] \otimes \mathbf{z}_0^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T
    \mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x}) \odot (\mathbf{1}
    - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T \end{align*}\]
- en: where we used *Properties of the Kronecker Product (f)* on the second to last
    line and our expression for the derivative of the sigmoid function on the last
    line.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在倒数第二行中，我们使用了**克罗内克积的性质 (f**)，在最后一行中，我们使用了sigmoid函数的导数表达式。
- en: To sum up,
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，
- en: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x})
    \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T
    & (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T
    \end{pmatrix} \end{align*}\]
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x})
    \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T
    & (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T
    \end{pmatrix} \end{align*}\]
- en: '**NUMERICAL CORNER:** We return to the concrete example from the previous section.
    We re-write the gradient as'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们回到上一节中的具体例子。我们将梯度重新写为'
- en: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\mathbf{z}_2 -
    \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\mathbf{z}_1 \odot (\mathbf{1} - \mathbf{z}_1))]
    \otimes \mathbf{z}_0^T & (\mathbf{z}_2 - \mathbf{y})^T \otimes \mathbf{z}_1^T
    \end{pmatrix}. \end{align*}\]
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\mathbf{z}_2 -
    \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\mathbf{z}_1 \odot (\mathbf{1} - \mathbf{z}_1))]
    \otimes \mathbf{z}_0^T & (\mathbf{z}_2 - \mathbf{y})^T \otimes \mathbf{z}_1^T
    \end{pmatrix}. \end{align*}\]
- en: We will use [`torch.nn.functional.sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html)
    and [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)
    for the sigmoid and softmax functions respectively. We also use [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html)
    for the inner product (i.e., dot product) of two vectors (as tensors) and [`torch.diag`](https://pytorch.org/docs/stable/generated/torch.diag.html)
    for the creation of a diagonal matrix with specified entries on its diagonal.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `torch.nn.functional.sigmoid` 和 `torch.nn.functional.softmax` 分别来计算sigmoid和softmax函数。我们还使用
    `torch.dot` 来计算两个向量（作为张量）的内积（即点积），以及 `torch.diag` 来创建具有指定对角线元素的对角矩阵。
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We compute the gradient \(\nabla f(\mathbf{w})\) using AD.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用AD（自动微分）来计算梯度 \(\nabla f(\mathbf{w})\)。
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We use our formulas to confirm that they match these results.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们的公式来验证它们与这些结果相匹配。
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The results match with the AD output.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与AD输出相匹配。
- en: \(\unlhd\)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 8.5.3\. Computing the gradient[#](#computing-the-gradient "Link to this heading")
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5.3\. 计算梯度[#](#computing-the-gradient "链接到这个标题")
- en: We now detail how to compute the gradient of \(f(\mathbf{w})\) for a general
    MLP. In the forward loop, we first set \(\mathbf{z}_0 := \mathbf{x}\) and then
    we compute for \(i = 0,1,\ldots,L-1\)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们详细说明如何计算一般MLP（多层感知器）的 \(f(\mathbf{w})\) 的梯度。在正向循环中，我们首先设置 \(\mathbf{z}_0
    := \mathbf{x}\)，然后对于 \(i = 0,1,\ldots,L-1\)
- en: \[\begin{align*} \mathbf{z}_{i+1} &:= \bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i).
    \end{align*}\]
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{i+1} &:= \bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i).
    \end{align*}\]
- en: To compute the Jacobian of \(\bfg_i\), we use the *Chain Rule* on the composition
    \(\bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma(\bfk_i(\mathbf{z}_i,\mathbf{w}_i))\)
    where we define \(\bfk_i(\mathbf{z}_i,\mathbf{w}_i) = \mathcal{W}_i \mathbf{z}_i\).
    That is,
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算 \(\bfg_i\) 的雅可比矩阵，我们使用链式法则在复合函数 \(\bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma(\bfk_i(\mathbf{z}_i,\mathbf{w}_i))\)
    上进行计算，其中我们定义 \(\bfk_i(\mathbf{z}_i,\mathbf{w}_i) = \mathcal{W}_i \mathbf{z}_i\)。也就是说，
- en: \[ J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) = J_{\bsigma}\left(\mathcal{W}_i \mathbf{z}_i\right)
    J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i). \]
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) = J_{\bsigma}\left(\mathcal{W}_i \mathbf{z}_i\right)
    J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i). \]
- en: In our analysis of multinomial logistic regression, we computed the Jacobian
    of \(\bfk_i\). We obtained
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对多项式逻辑回归的分析中，我们计算了 \(\bfk_i\) 的雅可比矩阵。我们得到了
- en: \[ J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i) = \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i]
    & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] \end{pmatrix}. \]
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i) = \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i]
    & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] \end{pmatrix}. \]
- en: Recall that
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下
- en: \[ \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] = \mathcal{W}_i. \]
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] = \mathcal{W}_i. \]
- en: and
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_i^T
    & \cdots & \mathbf{e}_{n_{i+1}}\mathbf{z}_i^T \end{pmatrix} = I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T, \]
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_i^T
    & \cdots & \mathbf{e}_{n_{i+1}}\mathbf{z}_i^T \end{pmatrix} = I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T, \]
- en: where here \(\mathbf{e}_{j}\) is the \(j\)-th standard basis vector in \(\mathbb{R}^{n_{i+1}}\).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{e}_{j}\) 是 \(\mathbb{R}^{n_{i+1}}\) 中第 \(j\) 个标准基向量。
- en: From a previous calculation, the Jacobian of
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的计算中，我们得到了雅可比矩阵
- en: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n_{i+1}}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n_{i+1}})), \]
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n_{i+1}}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n_{i+1}})), \]
- en: is
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 是
- en: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma(\mathbf{t}) \odot (\mathbf{1}
    - \bsigma(\mathbf{t}))). \]
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma(\mathbf{t}) \odot (\mathbf{1}
    - \bsigma(\mathbf{t}))). \]
- en: Combining the previous formulas, we get
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 结合前面的公式，我们得到
- en: \[\begin{align*} J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) &= J_{\bsigma}\left(\mathcal{W}_i
    \mathbf{z}_i\right) J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i]
    \end{pmatrix}\\ &=\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    & \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) &= J_{\bsigma}\left(\mathcal{W}_i
    \mathbf{z}_i\right) J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i]
    \end{pmatrix}\\ &=\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    & \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
- en: where we define
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义
- en: \[ \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] = \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \mathcal{W}_i, \]
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] = \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \mathcal{W}_i, \]
- en: and
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[\begin{align*} \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1}
    - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \left(I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T\right)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \otimes \mathbf{z}_i^T, \end{align*}\]
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1}
    - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \left(I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T\right)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \otimes \mathbf{z}_i^T, \end{align*}\]
- en: where we used the *Properties of the Kronecker Product (f)*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了克朗内克积的性质（f）。
- en: For layer \(L+1\) (i.e., the output layer), we have previously computed the
    Jacobian of the softmax function composed with a linear transformation. We get
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第 \(L+1\) 层（即输出层），我们之前已经计算了由线性变换与 softmax 函数组合的雅可比矩阵。我们得到
- en: '\[\begin{align*} &\mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\\
    &= \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ &\begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    := J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L})\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L))
    - \bgamma(\mathcal{W}_{L}\mathbf{z}_L) \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T]
    \mathcal{W}_{L} & [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L)) - \bgamma(\mathcal{W}_{L}\mathbf{z}_L)
    \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T] \otimes \mathbf{z}_L^T \end{pmatrix}\\
    &=: \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]&
    \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix} \end{align*}\]'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\begin{align*} &\mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\\
    &= \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ &\begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    := J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L})\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L))
    - \bgamma(\mathcal{W}_{L}\mathbf{z}_L) \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T]
    \mathcal{W}_{L} & [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L)) - \bgamma(\mathcal{W}_{L}\mathbf{z}_L)
    \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T] \otimes \mathbf{z}_L^T \end{pmatrix}\\
    &=: \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]&
    \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix} \end{align*}\]'
- en: Also, as in the multinomial logistic regression case, the loss and gradient
    of the loss are
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在多项式逻辑回归的情况下，损失和损失梯度是
- en: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1}) = - \sum_{i=1}^K
    y_i \log z_{L+1,i}\\ \mathbf{q}_{L+1} &:= \nabla \ell(\mathbf{z}_{L+1}) = \left(-
    \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right). \end{align*}\]
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1}) = - \sum_{i=1}^K
    y_i \log z_{L+1,i}\\ \mathbf{q}_{L+1} &:= \nabla \ell(\mathbf{z}_{L+1}) = \left(-
    \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right). \end{align*}\]
- en: '*Initialization:* \(\mathbf{z}_0 := \mathbf{x}\)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*初始化:* \(\mathbf{z}_0 := \mathbf{x}\)'
- en: '*Forward loop:* For \(i = 0,1,\ldots,L-1\):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*前向循环:* 对于 \(i = 0,1,\ldots,L-1\):'
- en: \[\begin{align*} \mathbf{z}_{i+1} &:= g_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i \right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
    =\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] &
    \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{i+1} &:= g_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i \right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
    =\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] &
    \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
- en: and
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[\begin{align*} \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ \begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    &:= J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L}) = \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L},
    \mathbf{w}_{L}]& \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix}.
    \end{align*}\]
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ \begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    &:= J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L}) = \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L},
    \mathbf{w}_{L}]& \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix}.
    \end{align*}\]
- en: '*Loss:*'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失:*'
- en: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1})\\ \mathbf{p}_{L+1}
    &:= \nabla {\ell}(\mathbf{z}_{L+1}) = \left(- \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right).
    \end{align*}\]
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1})\\ \mathbf{p}_{L+1}
    &:= \nabla {\ell}(\mathbf{z}_{L+1}) = \left(- \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right).
    \end{align*}\]
- en: '*Backward loop:*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向循环:*'
- en: \[\begin{align*} \mathbf{p}_{L} := A_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{C}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1}\\ \mathbf{q}_{L} := B_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{D}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1} \end{align*}\]
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{p}_{L} := A_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{C}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1}\\ \mathbf{q}_{L} := B_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{D}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1} \end{align*}\]
- en: 'and for \(i = L-1,L-2,\ldots,1,0\):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(i = L-1,L-2,\ldots,1,0\)：
- en: \[\begin{align*} \mathbf{p}_{i} &:= A_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1}\\ \mathbf{q}_{i} &:= B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1} \end{align*}\]
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{p}_{i} &:= A_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1}\\ \mathbf{q}_{i} &:= B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1} \end{align*}\]
- en: '*Output:*'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出:*'
- en: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_{L}).
    \]
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_{L}).
    \]
- en: '**NUMERICAL CORNER:** We implement the training of a neural network in PyTorch.
    We use the Fashion-MNIST dataset again. We first load it again. We also check
    for the availability of GPUs.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们在 PyTorch 中实现了神经网络的训练。我们再次使用 Fashion-MNIST 数据集。我们首先再次加载它。我们还检查了
    GPU 的可用性。'
- en: '[PRE16]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We construct a two-layer model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个两层模型。
- en: '[PRE19]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As we did for multinomial logistic regression, we use the SGD optimizer and
    the cross-entropy loss (which in PyTorch includes the softmax function and expects
    labels to be actual class labels rather than one-hot encoding).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在多项式逻辑回归中所做的那样，我们使用 SGD 优化器和交叉熵损失（在 PyTorch 中包括 softmax 函数，并期望标签是实际类别标签而不是
    one-hot 编码）。
- en: '[PRE20]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We train for 10 epochs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了 10 个周期。
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'On the test data, we get:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上，我们得到：
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Disappointingly, this is significantly less accurate than what we obtained using
    multinomial logistic regression. It turns out that using a different optimizer
    gives much better results.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 令人失望的是，这比我们使用多项式逻辑回归获得的结果差得多。结果是，使用不同的优化器可以得到更好的结果。
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '**CHAT & LEARN** We mentioned that there are many optimizers available in PyTorch
    besides SGD and Adam. Ask your favorite AI chatbot to explain and implement a
    different optimizer, such as Adagrad or RMSprop, for the MLP. Compare the results
    with those obtained using SGD and Adam. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与学习**: 我们提到 PyTorch 中除了 SGD 和 Adam 之外还有许多优化器可用。请你的首选 AI 聊天机器人解释并实现一个不同的优化器，例如
    Adagrad 或 RMSprop，用于 MLP。将结果与使用 SGD 和 Adam 获得的结果进行比较。（[在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))
    \(\ddagger\)'
- en: \(\unlhd\)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** Regularization techniques are often used to prevent overfitting
    in neural networks. Ask your favorite AI chatbot about \(L_1\) and \(L_2\) regularization,
    dropout, and early stopping. Discuss how these techniques can be incorporated
    into the training process and their effects on the learned model. \(\ddagger\)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 正则化技术通常用于防止神经网络过拟合。向你的心仪AI聊天机器人询问\(L_1\)和\(L_2\)正则化、dropout和提前停止。讨论这些技术如何融入训练过程及其对学习模型的影响。
    \(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(在Claude、Gemini和ChatGPT的帮助下)*'
- en: '**1** What is the role of the sigmoid function in a multilayer perceptron (MLP)?'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在多层感知器（MLP）中sigmoid函数的作用是什么？'
- en: a) It is used as the loss function for training the MLP.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它被用作训练MLP的损失函数。
- en: b) It is used as the nonlinear activation function in each layer of the MLP.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它被用作MLP每一层的非线性激活函数。
- en: c) It is used to compute the gradient of the loss function with respect to the
    weights.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它用于计算关于权重的损失函数的梯度。
- en: d) It is used to initialize the weights of the MLP.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它用于初始化MLP的权重。
- en: '**2** In an MLP, what is the purpose of the softmax function in the output
    layer?'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 在MLP中，输出层中的softmax函数的目的是什么？'
- en: a) To introduce nonlinearity into the model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: a) 将非线性引入模型。
- en: b) To normalize the outputs into a probability distribution.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: b) 将输出归一化成一个概率分布。
- en: c) To compute the gradient of the loss function.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: c) 计算损失函数的梯度。
- en: d) To reduce the dimensionality of the output.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: d) 减少输出的维度。
- en: '**3** What is the Jacobian matrix of the elementwise sigmoid function \(\boldsymbol{\sigma}(\mathbf{t})
    = (\sigma(t_1), \dots, \sigma(t_n))\)?'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 元素级sigmoid函数 \(\boldsymbol{\sigma}(\mathbf{t}) = (\sigma(t_1), \dots,
    \sigma(t_n))\) 的雅可比矩阵是什么？'
- en: a) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t}))\)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t}))\)
- en: b) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t}) \odot
    (1-\boldsymbol{\sigma}(\mathbf{t}))\)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t}) \odot
    (1-\boldsymbol{\sigma}(\mathbf{t}))\)
- en: c) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t})
    \odot (1 - \boldsymbol{\sigma}(\mathbf{t})))\)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t})
    \odot (1 - \boldsymbol{\sigma}(\mathbf{t})))\)
- en: d) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t})(1
    - \boldsymbol{\sigma}(\mathbf{t}))^T\)
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t})(1
    - \boldsymbol{\sigma}(\mathbf{t}))^T\)
- en: '**4** In the forward phase of computing the gradient of the loss function in
    an MLP, what is the output of the \(i\)-th hidden layer?'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在计算MLP中损失函数梯度的正向阶段，第\(i\)个隐藏层的输出是什么？'
- en: a) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i)\)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i)\)
- en: b) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \mathbf{z}_i\)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \mathbf{z}_i\)
- en: c) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\gamma}(\mathcal{W}_i
    \mathbf{z}_i)\)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\gamma}(\mathcal{W}_i
    \mathbf{z}_i)\)
- en: d) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \boldsymbol{\sigma}(\mathbf{z}_i)\)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \boldsymbol{\sigma}(\mathbf{z}_i)\)
- en: '**5** What is the output of the backward loop in computing the gradient of
    the loss function in an MLP?'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在计算MLP中损失函数的梯度时，反向循环的输出是什么？'
- en: a) The gradient of the loss function with respect to the activations of each
    layer.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: a) 关于每一层激活的损失函数的梯度。
- en: b) The gradient of the loss function with respect to the weights of each layer.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: b) 关于每一层权重的损失函数的梯度。
- en: c) The updated weights of the MLP.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: c) MLP的更新权重。
- en: d) The loss function value.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: d) 损失函数的值。
- en: 'Answer for 1: b. Justification: The text states that “Each of the main layers
    of a feedforward neural network has two components, an affine map and a nonlinear
    activation function. For the latter, we restrict ourselves here to the sigmoid
    function.”'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 1的答案：b. 理由：文本中提到“前馈神经网络的主要每一层都有两个组成部分，一个是仿射映射，另一个是非线性激活函数。对于后者，我们在这里限制自己使用sigmoid函数。”
- en: 'Answer for 2: b. Justification: The text states that the softmax function is
    used in the output layer to produce a probability distribution over the possible
    classes.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 2的答案：b. 理由：文本中提到softmax函数用于输出层以生成关于可能类别的概率分布。
- en: 'Answer for 3: c. Justification: The text states that the Jacobian of the elementwise
    sigmoid function is:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 答案为 3：c. 理由：文本指出逐元素 sigmoid 函数的雅可比矩阵是：
- en: \[ J_{\boldsymbol{\sigma}}(t) = \mathrm{diag}(\boldsymbol{\sigma}'(t)) = \mathrm{diag}(\boldsymbol{\sigma}(t)
    \odot (1 - \boldsymbol{\sigma}(t))) \]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\boldsymbol{\sigma}}(t) = \mathrm{diag}(\boldsymbol{\sigma}'(t)) = \mathrm{diag}(\boldsymbol{\sigma}(t)
    \odot (1 - \boldsymbol{\sigma}(t))) \]
- en: where \(\odot\) denotes the Hadamard (elementwise) product.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\odot\) 表示 Hadamard（逐元素）乘积。
- en: 'Answer for 4: a. Justification: The text defines the output of the \(i\)-th
    hidden layer as:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 答案为 4：a. 理由：文本定义第 \(i\) 个隐藏层的输出为：
- en: \[ \mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i) \]
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i) \]
- en: where \(\boldsymbol{\sigma}\) is the sigmoid activation function and \(\mathcal{W}_i\)
    is the weight matrix for the \(i\)-th layer.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\boldsymbol{\sigma}\) 是 sigmoid 激活函数，\(\mathcal{W}_i\) 是第 \(i\) 层的权重矩阵。
- en: 'Answer for 5: b. Justification: The text states that the output of the backward
    loop is the gradient of the loss function with respect to the weights of each
    layer:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 答案为 5：b. 理由：文本指出反向循环的输出是相对于每层权重的损失函数的梯度：
- en: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0, \mathbf{q}_1, \dots, \mathbf{q}_L)
    \]
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0, \mathbf{q}_1, \dots, \mathbf{q}_L)
    \]
- en: where \(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,
    \mathbf{w}_i]^T \mathbf{p}_{i+1}\) is the gradient with respect to the weights
    of the \(i\)-th layer.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,
    \mathbf{w}_i]^T \mathbf{p}_{i+1}\) 是相对于第 \(i\) 层权重的梯度。
- en: 8.5.1\. Multilayer perceptron[#](#multilayer-perceptron "Link to this heading")
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5.1\. 多层感知器[#](#multilayer-perceptron "链接到这个标题")
- en: Each of the main layers of a feedforward neural network\(\idx{neural network}\xdi\)
    has two components, an affine map and a nonlinear activation function\(\idx{activation
    function}\xdi\). For the latter, we restrict ourselves here to the sigmoid function\(\idx{sigmoid}\xdi\)
    (although there are many [other choices of activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 前馈神经网络的主要层有两个组成部分，一个仿射映射和一个非线性激活函数\(\idx{激活函数}\xdi\)。对于后者，我们在这里限制自己使用 sigmoid
    函数\(\idx{sigmoid}\xdi\)（尽管有其他许多[激活函数的选择](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)）。
- en: The Jacobian of the elementwise version of the sigmoid function (which we will
    need later on)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid 函数逐元素版本的雅可比矩阵（我们稍后会用到）
- en: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n})), \]
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n})), \]
- en: as a function of several variables can be computed from \(\sigma'\), i.e., the
    derivative of the single-variable case. Indeed, we have seen in a previous example
    that \(J_{\bsigma}(\mathbf{t})\) is the diagonal matrix with diagonal entries
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 作为多个变量的函数可以通过 \(\sigma'\) 来计算，即单变量情况的导数。实际上，我们在先前的例子中已经看到 \(J_{\bsigma}(\mathbf{t})\)
    是一个对角矩阵，其对角元素
- en: \[ \sigma'(t_j) = \frac{e^{-t_j}}{(1 + e^{-t_j})^2} = \sigma(t_j) (1 - \sigma(t_j)),
    \qquad j=1, \ldots, n, \]
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma'(t_j) = \frac{e^{-t_j}}{(1 + e^{-t_j})^2} = \sigma(t_j) (1 - \sigma(t_j)),
    \qquad j=1, \ldots, n, \]
- en: which we denote
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将其表示为
- en: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma'(\mathbf{t})) = \mathrm{diag}(\bsigma(\mathbf{t})
    \odot (\mathbf{1} - \bsigma(\mathbf{t}))), \]
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma'(\mathbf{t})) = \mathrm{diag}(\bsigma(\mathbf{t})
    \odot (\mathbf{1} - \bsigma(\mathbf{t}))), \]
- en: where \(\bsigma'(\mathbf{t}) = (\sigma'(t_1), \ldots,\sigma'(t_{n}))\) and \(\mathbf{1}\)
    is the all-one vector.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\bsigma'(\mathbf{t}) = (\sigma'(t_1), \ldots,\sigma'(t_{n}))\)，\(\mathbf{1}\)
    是全 1 向量。
- en: 'We consider an arbitrary number of layers\(\idx{layer}\xdi\) \(L+2\). As a
    special case of progressive functions\(\idx{progressive function}\xdi\), hidden
    layer\(\idx{hidden layer}\xdi\) \(i\), \(i=1,\ldots,L\), is defined by a continuously
    differentiable function \(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1}, \mathbf{w}_{i-1})\)
    which takes two vector-valued inputs: a vector \(\mathbf{z}_{i-1} \in \mathbb{R}^{n_{i-1}}\)
    fed from the \((i-1)\)-st layer and a vector \(\mathbf{w}_{i-1} \in \mathbb{R}^{r_{i-1}}\)
    of parameters specific to the \(i\)-th layer'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑任意数量的层\(\idx{layer}\xdi\) \(L+2\)。作为渐进函数\(\idx{progressive function}\xdi\)的特殊情况，隐藏层\(\idx{hidden
    layer}\xdi\) \(i\)，\(i=1,\ldots,L\)，由一个连续可微的函数 \(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1},
    \mathbf{w}_{i-1})\) 定义，它接受两个向量值输入：一个来自 \((i-1)\) 层的向量 \(\mathbf{z}_{i-1} \in \mathbb{R}^{n_{i-1}}\)
    和一个属于 \(i\) 层的参数向量 \(\mathbf{w}_{i-1} \in \mathbb{R}^{r_{i-1}}\)。
- en: '\[ \bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}}
    \to \mathbb{R}^{n_{i}}. \]'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '\[ \bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}}
    \to \mathbb{R}^{n_{i}}. \]'
- en: The output \(\mathbf{z}_i\) of \(\bfg_{i-1}\) is a vector in \(\mathbb{R}^{n_{i}}\)
    which is passed to the \((i+1)\)-st layer as input. Each component of \(\bfg_{i-1}\)
    is referred to as a neuron. Here \(r_{i-1} = n_{i} n_{i-1}\) and \(\mathbf{w}_{i-1}
    = (\mathbf{w}^{(1)}_{i-1},\ldots,\mathbf{w}^{(n_{i})}_{i-1})\) are the parameters
    with \(\mathbf{w}^{(k)}_{i-1} \in \mathbb{R}^{n_{i-1}}\) for all \(k\). Specifically,
    \(\bfg_{i-1}\) is given by
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bfg_{i-1}\) 的输出 \(\mathbf{z}_i\) 是 \(\mathbb{R}^{n_{i}}\) 中的一个向量，它作为输入传递到
    \((i+1)\) 层。\(\bfg_{i-1}\) 的每个组成部分被称为神经元。在这里 \(r_{i-1} = n_{i} n_{i-1}\) 且 \(\mathbf{w}_{i-1}
    = (\mathbf{w}^{(1)}_{i-1},\ldots,\mathbf{w}^{(n_{i})}_{i-1})\) 是参数，其中 \(\mathbf{w}^{(k)}_{i-1}
    \in \mathbb{R}^{n_{i-1}}\) 对所有 \(k\) 都成立。特别是，\(\bfg_{i-1}\) 由以下给出
- en: \[ \bfg_{i-1}(\mathbf{z}_{i-1},\mathbf{w}_{i-1}) = \bsigma\left(\mathcal{W}_{i-1}
    \mathbf{z}_{i-1}\right) = \left(\sigma\left(\sum_{j=1}^{n_{i-1}} w^{(1)}_{i-1,j}
    z_{i-1,j}\right), \ldots, \sigma\left(\sum_{j=1}^{n_{i-1}} w^{(n_{i})}_{i-1,j}
    z_{i-1,j}\right)\right) \]
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfg_{i-1}(\mathbf{z}_{i-1},\mathbf{w}_{i-1}) = \bsigma\left(\mathcal{W}_{i-1}
    \mathbf{z}_{i-1}\right) = \left(\sigma\left(\sum_{j=1}^{n_{i-1}} w^{(1)}_{i-1,j}
    z_{i-1,j}\right), \ldots, \sigma\left(\sum_{j=1}^{n_{i-1}} w^{(n_{i})}_{i-1,j}
    z_{i-1,j}\right)\right) \]
- en: where we define \(\mathcal{W}_{i-1} \in \mathbb{R}^{n_{i} \times n_{i-1}}\)
    as the matrix with rows \((\mathbf{w}_{i-1}^{(1)})^T,\ldots,(\mathbf{w}_{i-1}^{(n_{i})})^T\).
    As we have done previously, to keep the notation simple, we ignore the constant
    term (or “bias variable”\(\idx{bias variable}\xdi\)) in the linear combinations
    of \(\mathbf{z}_{i-1}\). It can be incorporated by adding a \(1\) input to each
    neuron as illustrated below. We will not detail this complication here.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们定义 \(\mathcal{W}_{i-1} \in \mathbb{R}^{n_{i} \times n_{i-1}}\) 为具有行 \((\mathbf{w}_{i-1}^{(1)})^T,\ldots,(\mathbf{w}_{i-1}^{(n_{i})})^T\)
    的矩阵。像之前一样，为了简化符号，我们忽略了 \(\mathbf{z}_{i-1}\) 线性组合中的常数项（或“偏变量”\(\idx{bias variable}\xdi\)）。可以通过向每个神经元添加一个
    \(1\) 输入来包含它，如下所示。我们在此不详细说明这种复杂性。
- en: '**Figure:** Each component of \(g_i\) is referred to as a neuron ([Source](https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png))'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**图示：** \(g_i\) 的每个组成部分被称为神经元 ([来源](https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png))'
- en: '![Neuron](../Images/228200787298b2f894943501d2a42428.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![神经元](../Images/228200787298b2f894943501d2a42428.png)'
- en: \(\bowtie\)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: \(\bowtie\)
- en: The input layer\(\idx{input layer}\xdi\) is \(\mathbf{z}_0 := \mathbf{x}\),
    which we refer to as layer \(0\), so that \(n_0 = d\).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层\(\idx{input layer}\xdi\)是 \(\mathbf{z}_0 := \mathbf{x}\)，我们将其称为层 \(0\)，因此
    \(n_0 = d\)。
- en: Similarly to multinomial logistic regression, layer \(L+1\) (i.e., the output
    layer\(\idx{output layer}\xdi\)) is the softmax function
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于多项式逻辑回归，层 \(L+1\)（即输出层\(\idx{output layer}\xdi\)）是 softmax 函数
- en: \[ \hat{\mathbf{y}} := \mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_L \mathbf{z}_{L}), \]
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \hat{\mathbf{y}} := \mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_L \mathbf{z}_{L}), \]
- en: but this time we compose it with a linear transformation. We implicitly assume
    that \(\bgamma\) has \(K\) outputs and, in particular, we have that \(n_{L+1}
    = K\).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 但这次我们通过线性变换来组合它。我们隐含地假设 \(\bgamma\) 有 \(K\) 个输出，特别是我们有 \(n_{L+1} = K\)。
- en: So the output of the classifier with parameters \(\mathbf{w} =(\mathbf{w}_0,\ldots,\mathbf{w}_{L})\)
    on input \(\mathbf{x}\) is
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，具有参数 \(\mathbf{w} =(\mathbf{w}_0,\ldots,\mathbf{w}_{L})\) 的分类器在输入 \(\mathbf{x}\)
    上的输出是
- en: \[ \bfh(\mathbf{w}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}). \]
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfh(\mathbf{w}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}). \]
- en: What makes this an MLP is that, on each layer, all outputs feed into the input
    of the next layer. In graphical terms, the edges between consecutive layers form
    a complete bipartite graph.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得这是一个MLP（多层感知器），因为在每一层，所有输出都馈入下一层的输入。在图形术语中，连续层之间的边形成一个完全二分图。
- en: '![Feedforward neural network (with help from Claude; inspired by (Source))](../Images/271cf3797d7e5cd7669d7f17ae49618b.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![前馈神经网络（得益于Claude的帮助；灵感来源于（来源）)](../Images/271cf3797d7e5cd7669d7f17ae49618b.png)'
- en: We again use cross-entropy\(\idx{cross-entropy}\xdi\) as the loss function (although
    there are many [other choices of loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions)).
    That is, we set
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用交叉熵\(\idx{cross-entropy}\xdi\)作为损失函数（尽管有许多[其他损失函数的选择](https://pytorch.org/docs/stable/nn.html#loss-functions)）。也就是说，我们设置
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_i. \]
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_i. \]
- en: Finally,
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，
- en: \[ f(\mathbf{w}) = \ell(\mathbf{h}(\mathbf{w})). \]
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: \[ f(\mathbf{w}) = \ell(\mathbf{h}(\mathbf{w})). \]
- en: 8.5.2\. A first example[#](#a-first-example "Link to this heading")
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5.2\. 第一个例子[#](#a-first-example "链接到这个标题")
- en: Before detailing a general algorithm for computing the gradient, we adapt our
    first progressive example to this setting to illustrate the main ideas. Suppose
    \(d=3\), \(L=1\), \(n_1 = n_2 = 2\), and \(K = 2\), that is, we have one hidden
    layer and our output is 2-dimensional. Fix a data sample \(\mathbf{x} = (x_1,x_2,x_3)
    \in \mathbb{R}^3, \mathbf{y} = (y_1, y_2) \in \mathbb{R}^2\). For \(i=0, 1\),
    we use the notation
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细说明计算梯度的通用算法之前，我们将我们的第一个渐进示例适应到这个设置中，以说明主要思想。假设\(d=3\)，\(L=1\)，\(n_1 = n_2
    = 2\)，\(K = 2\)，即我们有一个隐藏层，我们的输出是二维的。固定一个数据样本\(\mathbf{x} = (x_1,x_2,x_3) \in \mathbb{R}^3,
    \mathbf{y} = (y_1, y_2) \in \mathbb{R}^2\)。对于\(i=0, 1\)，我们使用以下符号
- en: \[\begin{split} \mathcal{W}_{0} = \begin{pmatrix} w_0 & w_1 & w_2\\ w_3 & w_4
    & w_5 \end{pmatrix} \quad \text{and} \quad \mathcal{W}_{1} = \begin{pmatrix} w_6
    & w_7\\ w_8 & w_9 \end{pmatrix} \end{split}\]
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{split} \mathcal{W}_{0} = \begin{pmatrix} w_0 & w_1 & w_2\\ w_3 & w_4
    & w_5 \end{pmatrix} \quad \text{和} \quad \mathcal{W}_{1} = \begin{pmatrix} w_6
    & w_7\\ w_8 & w_9 \end{pmatrix} \end{split}\]
- en: and let
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 并令
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - y_1 \log \hat{y}_1
    - y_2 \log \hat{y}_2. \]
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - y_1 \log \hat{y}_1
    - y_2 \log \hat{y}_2. \]
- en: The layer functions are as follows
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 层函数如下
- en: \[ \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})
    \quad\text{with}\quad \mathbf{w}_0 = (w_0, w_1, w_2, w_3, w_4, w_5) \]\[ \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1}) \quad\text{with}\quad
    \mathbf{w}_1 = (w_6, w_7, w_8, w_9). \]
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})
    \quad\text{其中}\quad \mathbf{w}_0 = (w_0, w_1, w_2, w_3, w_4, w_5) \]\[ \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1}) \quad\text{其中}\quad \mathbf{w}_1
    = (w_6, w_7, w_8, w_9). \]
- en: We seek to compute the gradient of
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图计算以下梯度的值
- en: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
    &= H(\mathbf{y}, \bgamma(\mathcal{W}_{1} \bsigma(\mathcal{W}_{0} \mathbf{x}))\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right] \end{align*}\]
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
    &= H(\mathbf{y}, \bgamma(\mathcal{W}_{1} \bsigma(\mathcal{W}_{0} \mathbf{x})))\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right] \end{align*}\]
- en: where
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: \[\begin{align*} Z &= \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\\ & \quad + \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)
    + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)). \end{align*}\]
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} Z &= \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\\ & \quad + \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)
    + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)). \end{align*}\]
- en: 'In the forward phase, we compute \(f\) itself and the requisite Jacobians:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在正向阶段，我们计算\(f\)本身以及所需的雅可比矩阵：
- en: \[\begin{align*} &\mathbf{z}_0 := \mathbf{x}\\ & = (x_1, x_2, x_3)\\ &\mathbf{z}_1
    := \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})\\
    &= \begin{pmatrix} \sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)\\ \sigma(w_3 x_1 + w_4
    x_2 + w_5 x_3) \end{pmatrix}\\ &J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := J_{\bsigma}(\mathcal{W}_{0}
    \mathbf{z}_{0}) \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_0] & \mathbb{B}_{2}[\mathbf{z}_0]
    \end{pmatrix}\\ &= \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \begin{pmatrix}
    \mathcal{W}_{0} & I_{2\times 2} \otimes \mathbf{z}_0^T \end{pmatrix}\\ &= \begin{pmatrix}
    \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \mathcal{W}_{0} & \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{z}_{0})) \otimes \mathbf{z}_0^T \end{pmatrix}, \end{align*}\]
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\mathbf{z}_0 := \mathbf{x}\\ & = (x_1, x_2, x_3)\\ &\mathbf{z}_1
    := \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})\\
    &= \begin{pmatrix} \sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)\\ \sigma(w_3 x_1 + w_4
    x_2 + w_5 x_3) \end{pmatrix}\\ &J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := J_{\bsigma}(\mathcal{W}_{0}
    \mathbf{z}_{0}) \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_0] & \mathbb{B}_{2}[\mathbf{z}_0]
    \end{pmatrix}\\ &= \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \begin{pmatrix}
    \mathcal{W}_{0} & I_{2\times 2} \otimes \mathbf{z}_0^T \end{pmatrix}\\ &= \begin{pmatrix}
    \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \mathcal{W}_{0} & \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{z}_{0})) \otimes \mathbf{z}_0^T \end{pmatrix}, \end{align*}\]
- en: where we used the *Chain Rule* to compute the Jacobian of \(J_{\bfg_0}\).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用**链式法则**来计算 \(J_{\bfg_0}\) 的雅可比矩阵。
- en: Then
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: \[\begin{align*} &\hat{\mathbf{y}} := \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1)
    = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\\ &= \begin{pmatrix} Z^{-1} \exp(w_6\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\\ Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)) \end{pmatrix}\\
    &J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):= J_{\bgamma}(\mathcal{W}_{1} \mathbf{z}_{1})
    \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_1] & \mathbb{B}_{2}[\mathbf{z}_1] \end{pmatrix}\\
    &= [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \begin{pmatrix} \mathcal{W}_{1} & I_{2\times
    2} \otimes \mathbf{z}_1^T \end{pmatrix}\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} & [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \otimes \mathbf{z}_1^T \end{pmatrix},
    \end{align*}\]
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\hat{\mathbf{y}} := \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1)
    = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\\ &= \begin{pmatrix} Z^{-1} \exp(w_6\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\\ Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)) \end{pmatrix}\\
    &J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):= J_{\bgamma}(\mathcal{W}_{1} \mathbf{z}_{1})
    \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_1] & \mathbb{B}_{2}[\mathbf{z}_1] \end{pmatrix}\\
    &= [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \begin{pmatrix} \mathcal{W}_{1} & I_{2\times
    2} \otimes \mathbf{z}_1^T \end{pmatrix}\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} & [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \otimes \mathbf{z}_1^T \end{pmatrix},
    \end{align*}\]
- en: where \(Z\) was introduced previously and we used the expression for \(J_{\bgamma}\)
    from the previous subsection.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(Z\) 是之前引入的，我们使用了前一小节中 \(J_{\bgamma}\) 的表达式。
- en: Finally
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最后
- en: \[\begin{align*} &f(\mathbf{w}) := \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}})\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\ &J_{\ell}(\hat{\mathbf{y}})
    = - (\mathbf{y} \oslash \hat{\mathbf{y}})^T. \end{align*}\]
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &f(\mathbf{w}) := \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}})\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\ &J_{\ell}(\hat{\mathbf{y}})
    = - (\mathbf{y} \oslash \hat{\mathbf{y}})^T. \end{align*}\]
- en: We first compute the partial derivatives with respect to \(\mathbf{w}_1 = (w_6,
    w_7, w_8, w_9)\). For this step, we think of \(f\) as the composition of \(\ell(\mathbf{z}_2)\)
    as a function of \(\mathbf{z}_2\) and \(\bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \bgamma(\mathcal{W}_1
    \mathbf{z}_1)\) as a function of \(\mathbf{w}_1\). Here \(\mathbf{z}_1\) does
    not depend on \(\mathbf{w}_1\) and therefore can be considered fixed for this
    calculation. By the *Chain Rule* and the *Properties of the Kronecker Product
    (f)*, we get
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先计算相对于 \(\mathbf{w}_1 = (w_6, w_7, w_8, w_9)\) 的偏导数。对于这一步，我们将 \(f\) 视为 \(\ell(\mathbf{z}_2)\)
    作为 \(\mathbf{z}_2\) 的函数和 \(\bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \bgamma(\mathcal{W}_1
    \mathbf{z}_1)\) 作为 \(\mathbf{w}_1\) 的函数的组合。在这里，\(\mathbf{z}_1\) 不依赖于 \(\mathbf{w}_1\)，因此可以认为在这个计算中是固定的。通过
    **链式法则** 和 **克罗内克积的性质（f**），我们得到
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6}
    & \frac{\partial f(\mathbf{w})}{\partial w_7} & \frac{\partial f(\mathbf{w})}{\partial
    w_8} & \frac{\partial f(\mathbf{w})}{\partial w_9} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \otimes \mathbf{z}_1^T \right\}\\ &= \left\{- (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}
    \mathbf{z}_{1}))^T \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \right\} \otimes \mathbf{z}_1^T\\ &=
    (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T \otimes \mathbf{z}_1^T\\
    &= (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T,
    \end{align*}\]
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6}
    & \frac{\partial f(\mathbf{w})}{\partial w_7} & \frac{\partial f(\mathbf{w})}{\partial
    w_8} & \frac{\partial f(\mathbf{w})}{\partial w_9} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \otimes \mathbf{z}_1^T \right\}\\ &= \left\{- (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}
    \mathbf{z}_{1}))^T \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \right\} \otimes \mathbf{z}_1^T\\ &=
    (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T \otimes \mathbf{z}_1^T\\
    &= (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T,
    \end{align*}\]
- en: where we used the *Properties of the Hadamard Product* and the fact that \(\hat{\mathbf{y}}
    = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\) similarly to a calculation we did
    in the multinomial logistic regression setting.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了 **Hadamard 积的性质** 和 \(\hat{\mathbf{y}} = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\)
    与我们在多项式逻辑回归设置中进行的计算类似的事实。
- en: To compute the partial derivatives with respect to \(\mathbf{w}_0 = (w_0, w_1,
    \ldots, w_5)\), we first need to compute partial derivatives with respect to \(\mathbf{z}_1
    = (z_{1,1}, z_{1,2})\) since \(f\) depends on \(\mathbf{w}_0\) through it. For
    this calculation, we think again of \(f\) as the composition \(\ell(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1))\), but this time our focus is on the variables \(\mathbf{z}_1\).
    This is almost identical to the previous calculation, except that we use the block
    of \(J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1)\) corresponding to the partial derivatives
    with respect to \(\mathbf{z}_1\) (i.e., the “\(A\)” block). We obtain
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算相对于 \(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\) 的偏导数，我们首先需要计算相对于 \(\mathbf{z}_1
    = (z_{1,1}, z_{1,2})\) 的偏导数，因为 \(f\) 通过它依赖于 \(\mathbf{w}_0\)。对于这个计算，我们再次将 \(f\)
    视为 \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) 的组合，但这次我们的重点是变量 \(\mathbf{z}_1\)。这几乎与之前的计算相同，除了我们使用与相对于
    \(\mathbf{z}_1\) 的偏导数相对应的 \(J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1)\) 块（即“\(A\)”块）。我们得到
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}
    & \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} \right\}\\ &= - (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}\mathbf{z}_1))^T
    \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \end{align*}\]
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}
    & \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} \right\}\\ &= - (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}\mathbf{z}_1))^T
    \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \end{align*}\]
- en: The vector \(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}, \frac{\partial
    f(\mathbf{w})}{\partial z_{1,2}}\right)\) is called an adjoint.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 向量\(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}, \frac{\partial f(\mathbf{w})}{\partial
    z_{1,2}}\right)\)被称为伴随向量。
- en: We now compute the gradient of \(\mathbf{f}\) with respect to \(\mathbf{w}_0
    = (w_0, w_1, \ldots, w_5)\). For this step, we think of \(f\) as the composition
    of \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) as a function of \(\mathbf{z}_1\)
    and \(\bfg_0(\mathbf{z}_0, \mathbf{w}_0)\) as a function of \(\mathbf{w}_0\).
    Here \(\mathbf{w}_1\) and \(\mathbf{z}_0\) do not depend on \(\mathbf{w}_0\) and
    therefore can be considered fixed for this calculation. By the *Chain Rule*
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们计算相对于\(\mathbf{w}_0 = (w_0, w_1, \ldots, w_5)\)的\(\mathbf{f}\)的梯度。为此，我们将\(f\)视为\(\ell(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1))\)作为\(\mathbf{z}_1\)的函数和\(\bfg_0(\mathbf{z}_0, \mathbf{w}_0)\)作为\(\mathbf{w}_0\)的函数的组合。在这里，\(\mathbf{w}_1\)和\(\mathbf{z}_0\)不依赖于\(\mathbf{w}_0\)，因此可以在这个计算中视为固定。根据**链式法则**
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0}
    & \frac{\partial f(\mathbf{w})}{\partial w_1} & \frac{\partial f(\mathbf{w})}{\partial
    w_2} & \frac{\partial f(\mathbf{w})}{\partial w_3} & \frac{\partial f(\mathbf{w})}{\partial
    w_4} & \frac{\partial f(\mathbf{w})}{\partial w_5} \end{pmatrix}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \left[\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))
    \otimes \mathbf{z}_0^T\right]\\ &= [(\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
    \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))] \otimes
    \mathbf{z}_0^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{x}))] \otimes \mathbf{x}^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1}
    \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x}) \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0}
    \mathbf{x})))] \otimes \mathbf{x}^T \end{align*}\]
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0}
    & \frac{\partial f(\mathbf{w})}{\partial w_1} & \frac{\partial f(\mathbf{w})}{\partial
    w_2} & \frac{\partial f(\mathbf{w})}{\partial w_3} & \frac{\partial f(\mathbf{w})}{\partial
    w_4} & \frac{\partial f(\mathbf{w})}{\partial w_5} \end{pmatrix}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \left[\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))
    \otimes \mathbf{z}_0^T\right]\\ &= [(\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
    \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))] \otimes
    \mathbf{z}_0^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{x}))] \otimes \mathbf{x}^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1}
    \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x}) \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0}
    \mathbf{x})))] \otimes \mathbf{x}^T \end{align*}\]
- en: where we used *Properties of the Kronecker Product (f)* on the second to last
    line and our expression for the derivative of the sigmoid function on the last
    line.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在倒数第二行我们使用了**克罗内克积的性质 (f**)，并在最后一行给出了sigmoid函数导数的表达式。
- en: To sum up,
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 总结起来，
- en: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x})
    \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T
    & (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T
    \end{pmatrix} \end{align*}\]
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x})
    \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T
    & (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T
    \end{pmatrix} \end{align*}\]
- en: '**NUMERICAL CORNER:** We return to the concrete example from the previous section.
    We re-write the gradient as'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角:** 我们回到上一节的具体例子。我们将梯度重新写为'
- en: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\mathbf{z}_2 -
    \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\mathbf{z}_1 \odot (\mathbf{1} - \mathbf{z}_1))]
    \otimes \mathbf{z}_0^T & (\mathbf{z}_2 - \mathbf{y})^T \otimes \mathbf{z}_1^T
    \end{pmatrix}. \end{align*}\]
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\mathbf{z}_2 -
    \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\mathbf{z}_1 \odot (\mathbf{1} - \mathbf{z}_1))]
    \otimes \mathbf{z}_0^T & (\mathbf{z}_2 - \mathbf{y})^T \otimes \mathbf{z}_1^T
    \end{pmatrix}. \end{align*}\]
- en: We will use [`torch.nn.functional.sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html)
    and [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)
    for the sigmoid and softmax functions respectively. We also use [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html)
    for the inner product (i.e., dot product) of two vectors (as tensors) and [`torch.diag`](https://pytorch.org/docs/stable/generated/torch.diag.html)
    for the creation of a diagonal matrix with specified entries on its diagonal.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `torch.nn.functional.sigmoid` [链接](https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html)
    和 `torch.nn.functional.softmax` [链接](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)
    分别用于sigmoid和softmax函数。我们还使用 `torch.dot` [链接](https://pytorch.org/docs/stable/generated/torch.dot.html)
    用于两个向量（作为张量）的内积（即点积），以及 `torch.diag` [链接](https://pytorch.org/docs/stable/generated/torch.diag.html)
    用于创建具有指定对角线元素的对角矩阵。
- en: '[PRE27]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We compute the gradient \(\nabla f(\mathbf{w})\) using AD.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用AD计算梯度 \(\nabla f(\mathbf{w})\)。
- en: '[PRE35]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We use our formulas to confirm that they match these results.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用我们的公式来确认它们与这些结果相匹配。
- en: '[PRE39]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The results match with the AD output.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与AD输出相匹配。
- en: \(\unlhd\)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: 8.5.3\. Computing the gradient[#](#computing-the-gradient "Link to this heading")
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5.3\. 计算梯度[#](#computing-the-gradient "链接到这个标题")
- en: We now detail how to compute the gradient of \(f(\mathbf{w})\) for a general
    MLP. In the forward loop, we first set \(\mathbf{z}_0 := \mathbf{x}\) and then
    we compute for \(i = 0,1,\ldots,L-1\)
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将详细说明如何计算一般MLP的 \(f(\mathbf{w})\) 的梯度。在正向循环中，我们首先设置 \(\mathbf{z}_0 := \mathbf{x}\)，然后对于
    \(i = 0,1,\ldots,L-1\) 进行计算
- en: \[\begin{align*} \mathbf{z}_{i+1} &:= \bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i).
    \end{align*}\]
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{i+1} &:= \bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i).
    \end{align*}\]
- en: To compute the Jacobian of \(\bfg_i\), we use the *Chain Rule* on the composition
    \(\bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma(\bfk_i(\mathbf{z}_i,\mathbf{w}_i))\)
    where we define \(\bfk_i(\mathbf{z}_i,\mathbf{w}_i) = \mathcal{W}_i \mathbf{z}_i\).
    That is,
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 \(\bfg_i\) 的雅可比矩阵，我们在复合函数 \(\bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma(\bfk_i(\mathbf{z}_i,\mathbf{w}_i))\)
    上使用链式法则，其中我们定义 \(\bfk_i(\mathbf{z}_i,\mathbf{w}_i) = \mathcal{W}_i \mathbf{z}_i\)。也就是说，
- en: \[ J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) = J_{\bsigma}\left(\mathcal{W}_i \mathbf{z}_i\right)
    J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i). \]
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) = J_{\bsigma}\left(\mathcal{W}_i \mathbf{z}_i\right)
    J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i). \]
- en: In our analysis of multinomial logistic regression, we computed the Jacobian
    of \(\bfk_i\). We obtained
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对多项式逻辑回归的分析中，我们计算了 \(\bfk_i\) 的雅可比矩阵。我们得到了
- en: \[ J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i) = \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i]
    & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] \end{pmatrix}. \]
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i) = \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i]
    & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] \end{pmatrix}. \]
- en: Recall that
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下
- en: \[ \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] = \mathcal{W}_i. \]
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] = \mathcal{W}_i. \]
- en: and
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: \[ \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_i^T
    & \cdots & \mathbf{e}_{n_{i+1}}\mathbf{z}_i^T \end{pmatrix} = I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T, \]
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_i^T
    & \cdots & \mathbf{e}_{n_{i+1}}\mathbf{z}_i^T \end{pmatrix} = I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T, \]
- en: where here \(\mathbf{e}_{j}\) is the \(j\)-th standard basis vector in \(\mathbb{R}^{n_{i+1}}\).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，这里的 \(\mathbf{e}_{j}\) 是 \(\mathbb{R}^{n_{i+1}}\) 中的第 \(j\) 个标准基向量。
- en: From a previous calculation, the Jacobian of
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的计算中，我们计算了
- en: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n_{i+1}}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n_{i+1}})), \]
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n_{i+1}}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n_{i+1}})), \]
- en: is
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 是
- en: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma(\mathbf{t}) \odot (\mathbf{1}
    - \bsigma(\mathbf{t}))). \]
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma(\mathbf{t}) \odot (\mathbf{1}
    - \bsigma(\mathbf{t}))). \]
- en: Combining the previous formulas, we get
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 结合前面的公式，我们得到
- en: \[\begin{align*} J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) &= J_{\bsigma}\left(\mathcal{W}_i
    \mathbf{z}_i\right) J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i]
    \end{pmatrix}\\ &=\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    & \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) &= J_{\bsigma}\left(\mathcal{W}_i
    \mathbf{z}_i\right) J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i]
    \end{pmatrix}\\ &=\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    & \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
- en: where we define
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在此我们定义
- en: \[ \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] = \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \mathcal{W}_i, \]
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] = \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \mathcal{W}_i, \]
- en: and
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: \[\begin{align*} \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1}
    - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \left(I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T\right)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \otimes \mathbf{z}_i^T, \end{align*}\]
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1}
    - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \left(I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T\right)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \otimes \mathbf{z}_i^T, \end{align*}\]
- en: where we used the *Properties of the Kronecker Product (f)*.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了**克罗内克积的性质（f）**。
- en: For layer \(L+1\) (i.e., the output layer), we have previously computed the
    Jacobian of the softmax function composed with a linear transformation. We get
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(L+1\) 层（即输出层），我们之前已经计算了由线性变换与softmax函数组合的雅可比矩阵。我们得到
- en: '\[\begin{align*} &\mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\\
    &= \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ &\begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    := J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L})\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L))
    - \bgamma(\mathcal{W}_{L}\mathbf{z}_L) \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T]
    \mathcal{W}_{L} & [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L)) - \bgamma(\mathcal{W}_{L}\mathbf{z}_L)
    \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T] \otimes \mathbf{z}_L^T \end{pmatrix}\\
    &=: \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]&
    \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix} \end{align*}\]'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '\[\begin{align*} &\mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\\
    &= \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ &\begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    := J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L})\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L))
    - \bgamma(\mathcal{W}_{L}\mathbf{z}_L) \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T]
    \mathcal{W}_{L} & [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L)) - \bgamma(\mathcal{W}_{L}\mathbf{z}_L)
    \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T] \otimes \mathbf{z}_L^T \end{pmatrix}\\
    &=: \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]&
    \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix} \end{align*}\]'
- en: Also, as in the multinomial logistic regression case, the loss and gradient
    of the loss are
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如多项式逻辑回归的情况一样，损失和损失梯度是
- en: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1}) = - \sum_{i=1}^K
    y_i \log z_{L+1,i}\\ \mathbf{q}_{L+1} &:= \nabla \ell(\mathbf{z}_{L+1}) = \left(-
    \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right). \end{align*}\]
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1}) = - \sum_{i=1}^K
    y_i \log z_{L+1,i}\\ \mathbf{q}_{L+1} &:= \nabla \ell(\mathbf{z}_{L+1}) = \left(-
    \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right). \end{align*}\]
- en: '*Initialization:* \(\mathbf{z}_0 := \mathbf{x}\)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**初始化：** \(\mathbf{z}_0 := \mathbf{x}\)'
- en: '*Forward loop:* For \(i = 0,1,\ldots,L-1\):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向循环：** 对于 \(i = 0,1,\ldots,L-1\)：'
- en: \[\begin{align*} \mathbf{z}_{i+1} &:= g_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i \right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
    =\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] &
    \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{i+1} &:= g_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i \right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
    =\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] &
    \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
- en: and
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 以及
- en: \[\begin{align*} \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ \begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    &:= J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L}) = \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L},
    \mathbf{w}_{L}]& \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix}.
    \end{align*}\]
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ \begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    &:= J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L}) = \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L},
    \mathbf{w}_{L}]& \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix}.
    \end{align*}\]
- en: '*Loss:*'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失：*'
- en: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1})\\ \mathbf{p}_{L+1}
    &:= \nabla {\ell}(\mathbf{z}_{L+1}) = \left(- \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right).
    \end{align*}\]
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1})\\ \mathbf{p}_{L+1}
    &:= \nabla {\ell}(\mathbf{z}_{L+1}) = \left(- \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right).
    \end{align*}\]
- en: '*Backward loop:*'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*反向循环：*'
- en: \[\begin{align*} \mathbf{p}_{L} := A_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{C}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1}\\ \mathbf{q}_{L} := B_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{D}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1} \end{align*}\]
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{p}_{L} := A_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{C}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1}\\ \mathbf{q}_{L} := B_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{D}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1} \end{align*}\]
- en: 'and for \(i = L-1,L-2,\ldots,1,0\):'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(i = L-1,L-2,\ldots,1,0\)：
- en: \[\begin{align*} \mathbf{p}_{i} &:= A_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1}\\ \mathbf{q}_{i} &:= B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1} \end{align*}\]
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{p}_{i} &:= A_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1}\\ \mathbf{q}_{i} &:= B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1} \end{align*}\]
- en: '*Output:*'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '*输出：*'
- en: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_{L}).
    \]
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_{L}).
    \]
- en: '**NUMERICAL CORNER:** We implement the training of a neural network in PyTorch.
    We use the Fashion-MNIST dataset again. We first load it again. We also check
    for the availability of GPUs.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角落：** 我们在 PyTorch 中实现了神经网络的训练。我们再次使用 Fashion-MNIST 数据集。我们首先再次加载它。我们还检查了
    GPU 的可用性。'
- en: '[PRE43]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We construct a two-layer model.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个双层模型。
- en: '[PRE46]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As we did for multinomial logistic regression, we use the SGD optimizer and
    the cross-entropy loss (which in PyTorch includes the softmax function and expects
    labels to be actual class labels rather than one-hot encoding).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在多项式逻辑回归中所做的那样，我们使用 SGD 优化器和交叉熵损失（在 PyTorch 中包括 softmax 函数，并期望标签是实际类别标签而不是
    one-hot 编码）。
- en: '[PRE47]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We train for 10 epochs.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了 10 个周期。
- en: '[PRE48]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'On the test data, we get:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试数据上，我们得到：
- en: '[PRE49]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Disappointingly, this is significantly less accurate than what we obtained using
    multinomial logistic regression. It turns out that using a different optimizer
    gives much better results.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 令人失望的是，这个结果与我们使用多项式逻辑回归得到的结果相比明显要差得多。结果证明，使用不同的优化器可以得到更好的结果。
- en: '[PRE51]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '**CHAT & LEARN** We mentioned that there are many optimizers available in PyTorch
    besides SGD and Adam. Ask your favorite AI chatbot to explain and implement a
    different optimizer, such as Adagrad or RMSprop, for the MLP. Compare the results
    with those obtained using SGD and Adam. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与学习** 我们提到 PyTorch 中除了 SGD 和 Adam 之外还有许多优化器。请你的首选 AI 聊天机器人解释并实现一个不同的优化器，例如
    Adagrad 或 RMSprop，用于 MLP。将结果与使用 SGD 和 Adam 得到的结果进行比较。([在 Colab 中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))
    \(\ddagger\)'
- en: \(\unlhd\)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '**CHAT & LEARN** Regularization techniques are often used to prevent overfitting
    in neural networks. Ask your favorite AI chatbot about \(L_1\) and \(L_2\) regularization,
    dropout, and early stopping. Discuss how these techniques can be incorporated
    into the training process and their effects on the learned model. \(\ddagger\)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 正则化技术通常用于防止神经网络过拟合。向你的喜欢的AI聊天机器人询问\(L_1\)和\(L_2\)正则化、dropout和提前停止。讨论这些技术如何融入训练过程及其对学习模型的影响。
    \(\ddagger\)'
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude, Gemini和ChatGPT协助)*'
- en: '**1** What is the role of the sigmoid function in a multilayer perceptron (MLP)?'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 在多层感知器（MLP）中sigmoid函数的作用是什么？'
- en: a) It is used as the loss function for training the MLP.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它被用作训练MLP的损失函数。
- en: b) It is used as the nonlinear activation function in each layer of the MLP.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它被用作MLP每一层的非线性激活函数。
- en: c) It is used to compute the gradient of the loss function with respect to the
    weights.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它用于计算相对于权重的损失函数梯度。
- en: d) It is used to initialize the weights of the MLP.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: d) 它用于初始化MLP的权重。
- en: '**2** In an MLP, what is the purpose of the softmax function in the output
    layer?'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 在多层感知器（MLP）中，输出层中的softmax函数的目的是什么？'
- en: a) To introduce nonlinearity into the model.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: a) 为了将非线性引入模型。
- en: b) To normalize the outputs into a probability distribution.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: b) 将输出归一化成一个概率分布。
- en: c) To compute the gradient of the loss function.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: c) 为了计算损失函数的梯度。
- en: d) To reduce the dimensionality of the output.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: d) 为了减少输出的维度。
- en: '**3** What is the Jacobian matrix of the elementwise sigmoid function \(\boldsymbol{\sigma}(\mathbf{t})
    = (\sigma(t_1), \dots, \sigma(t_n))\)?'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 元素级sigmoid函数\(\boldsymbol{\sigma}(\mathbf{t}) = (\sigma(t_1), \dots,
    \sigma(t_n))\)的雅可比矩阵是什么？'
- en: a) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t}))\)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t}))\)
- en: b) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t}) \odot
    (1-\boldsymbol{\sigma}(\mathbf{t}))\)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t}) \odot
    (1-\boldsymbol{\sigma}(\mathbf{t}))\)
- en: c) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t})
    \odot (1 - \boldsymbol{\sigma}(\mathbf{t})))\)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t})
    \odot (1 - \boldsymbol{\sigma}(\mathbf{t})))\)
- en: d) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t})(1
    - \boldsymbol{\sigma}(\mathbf{t}))^T\)
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t})(1
    - \boldsymbol{\sigma}(\mathbf{t}))^T\)
- en: '**4** In the forward phase of computing the gradient of the loss function in
    an MLP, what is the output of the \(i\)-th hidden layer?'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在计算多层感知器中损失函数梯度的正向阶段，第\(i\)个隐藏层的输出是什么？'
- en: a) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i)\)
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i)\)
- en: b) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \mathbf{z}_i\)
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \mathbf{z}_i\)
- en: c) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\gamma}(\mathcal{W}_i
    \mathbf{z}_i)\)
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\gamma}(\mathcal{W}_i
    \mathbf{z}_i)\)
- en: d) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \boldsymbol{\sigma}(\mathbf{z}_i)\)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \boldsymbol{\sigma}(\mathbf{z}_i)\)
- en: '**5** What is the output of the backward loop in computing the gradient of
    the loss function in an MLP?'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在计算多层感知器中损失函数梯度的反向循环中，输出是什么？'
- en: a) The gradient of the loss function with respect to the activations of each
    layer.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: a) 相对于每一层激活的损失函数梯度。
- en: b) The gradient of the loss function with respect to the weights of each layer.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: b) 相对于每一层权重的损失函数梯度。
- en: c) The updated weights of the MLP.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: c) MLP的更新权重。
- en: d) The loss function value.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: d) 损失函数的值。
- en: 'Answer for 1: b. Justification: The text states that “Each of the main layers
    of a feedforward neural network has two components, an affine map and a nonlinear
    activation function. For the latter, we restrict ourselves here to the sigmoid
    function.”'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 答案1：b. 理由：文本说明“前馈神经网络的每个主要层有两个组成部分，一个仿射映射和一个非线性激活函数。对于后者，我们在这里限制自己使用sigmoid函数。”
- en: 'Answer for 2: b. Justification: The text states that the softmax function is
    used in the output layer to produce a probability distribution over the possible
    classes.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 答案2：b. 理由：文本说明softmax函数用于输出层以生成可能的类别的概率分布。
- en: 'Answer for 3: c. Justification: The text states that the Jacobian of the elementwise
    sigmoid function is:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 3题的答案：c. 证明：逐元素sigmoid函数的雅可比矩阵为：
- en: \[ J_{\boldsymbol{\sigma}}(t) = \mathrm{diag}(\boldsymbol{\sigma}'(t)) = \mathrm{diag}(\boldsymbol{\sigma}(t)
    \odot (1 - \boldsymbol{\sigma}(t))) \]
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: \[ J_{\boldsymbol{\sigma}}(t) = \mathrm{diag}(\boldsymbol{\sigma}'(t)) = \mathrm{diag}(\boldsymbol{\sigma}(t)
    \odot (1 - \boldsymbol{\sigma}(t))) \]
- en: where \(\odot\) denotes the Hadamard (elementwise) product.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\odot\) 表示Hadamard（逐元素）乘积。
- en: 'Answer for 4: a. Justification: The text defines the output of the \(i\)-th
    hidden layer as:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 4题的答案：a. 证明：文本定义了第 \(i\) 个隐藏层的输出为：
- en: \[ \mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i) \]
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i) \]
- en: where \(\boldsymbol{\sigma}\) is the sigmoid activation function and \(\mathcal{W}_i\)
    is the weight matrix for the \(i\)-th layer.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\boldsymbol{\sigma}\) 是sigmoid激活函数，\(\mathcal{W}_i\) 是第 \(i\) 层的权重矩阵。
- en: 'Answer for 5: b. Justification: The text states that the output of the backward
    loop is the gradient of the loss function with respect to the weights of each
    layer:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 5题的答案：b. 证明：文本指出反向循环的输出是损失函数相对于每一层权重的梯度：
- en: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0, \mathbf{q}_1, \dots, \mathbf{q}_L)
    \]
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0, \mathbf{q}_1, \dots, \mathbf{q}_L)
    \]
- en: where \(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,
    \mathbf{w}_i]^T \mathbf{p}_{i+1}\) is the gradient with respect to the weights
    of the \(i\)-th layer.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,
    \mathbf{w}_i]^T \mathbf{p}_{i+1}\) 是相对于第 \(i\) 层权重的梯度。
