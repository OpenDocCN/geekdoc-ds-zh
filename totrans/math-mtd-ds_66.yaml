- en: '8.5\. Building blocks of AI 3: neural networks#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap08_nn/05_nn/roch-mmids-nn-nn.html](https://mmids-textbook.github.io/chap08_nn/05_nn/roch-mmids-nn-nn.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Today''s paper shows that it is possible to implement John Von Neumann''s claim:
    "With 4 parameters I can ﬁt an elephant, and with 5 I can make him wiggle his
    trunk"'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Paper here: [https://t.co/SvVrLuRFNy](https://t.co/SvVrLuRFNy) [pic.twitter.com/VG37439vE7](https://t.co/VG37439vE7)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: — Fermat's Library (@fermatslibrary) [February 20, 2018](https://twitter.com/fermatslibrary/status/965971333422120962?ref_src=twsrc%5Etfw)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this section, we introduce neural networks. Unlike the previous examples
    we encountered, this one is not convex. Based on the theory we developed in Chapter
    3, finding a local minimizer is the best we can hope for in general from descent
    methods. Yet, in many application settings, stochastic gradient descent (and some
    variants) have proven very effective at computing a good model to fit the data.
    Why that is remains an open question.
  prefs: []
  type: TYPE_NORMAL
- en: We describe the basic setup and apply it to classification on the Fashion-MNIST
    dataset. As we will see, we will get an improvement over multinomial logistic
    regression (with some help from a different optimization method). We use a particular
    architecture referred as a multilayer perceptron (MLP)\(\idx{multilayer perceptron}\xdi\).
    These are a special class of progressive functions.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.1\. Multilayer perceptron[#](#multilayer-perceptron "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each of the main layers of a feedforward neural network\(\idx{neural network}\xdi\)
    has two components, an affine map and a nonlinear activation function\(\idx{activation
    function}\xdi\). For the latter, we restrict ourselves here to the sigmoid function\(\idx{sigmoid}\xdi\)
    (although there are many [other choices of activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)).
  prefs: []
  type: TYPE_NORMAL
- en: The Jacobian of the elementwise version of the sigmoid function (which we will
    need later on)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n})), \]
  prefs: []
  type: TYPE_NORMAL
- en: as a function of several variables can be computed from \(\sigma'\), i.e., the
    derivative of the single-variable case. Indeed, we have seen in a previous example
    that \(J_{\bsigma}(\mathbf{t})\) is the diagonal matrix with diagonal entries
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma'(t_j) = \frac{e^{-t_j}}{(1 + e^{-t_j})^2} = \sigma(t_j) (1 - \sigma(t_j)),
    \qquad j=1, \ldots, n, \]
  prefs: []
  type: TYPE_NORMAL
- en: which we denote
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma'(\mathbf{t})) = \mathrm{diag}(\bsigma(\mathbf{t})
    \odot (\mathbf{1} - \bsigma(\mathbf{t}))), \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bsigma'(\mathbf{t}) = (\sigma'(t_1), \ldots,\sigma'(t_{n}))\) and \(\mathbf{1}\)
    is the all-one vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'We consider an arbitrary number of layers\(\idx{layer}\xdi\) \(L+2\). As a
    special case of progressive functions\(\idx{progressive function}\xdi\), hidden
    layer\(\idx{hidden layer}\xdi\) \(i\), \(i=1,\ldots,L\), is defined by a continuously
    differentiable function \(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1}, \mathbf{w}_{i-1})\)
    which takes two vector-valued inputs: a vector \(\mathbf{z}_{i-1} \in \mathbb{R}^{n_{i-1}}\)
    fed from the \((i-1)\)-st layer and a vector \(\mathbf{w}_{i-1} \in \mathbb{R}^{r_{i-1}}\)
    of parameters specific to the \(i\)-th layer'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}}
    \to \mathbb{R}^{n_{i}}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: The output \(\mathbf{z}_i\) of \(\bfg_{i-1}\) is a vector in \(\mathbb{R}^{n_{i}}\)
    which is passed to the \((i+1)\)-st layer as input. Each component of \(\bfg_{i-1}\)
    is referred to as a neuron. Here \(r_{i-1} = n_{i} n_{i-1}\) and \(\mathbf{w}_{i-1}
    = (\mathbf{w}^{(1)}_{i-1},\ldots,\mathbf{w}^{(n_{i})}_{i-1})\) are the parameters
    with \(\mathbf{w}^{(k)}_{i-1} \in \mathbb{R}^{n_{i-1}}\) for all \(k\). Specifically,
    \(\bfg_{i-1}\) is given by
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bfg_{i-1}(\mathbf{z}_{i-1},\mathbf{w}_{i-1}) = \bsigma\left(\mathcal{W}_{i-1}
    \mathbf{z}_{i-1}\right) = \left(\sigma\left(\sum_{j=1}^{n_{i-1}} w^{(1)}_{i-1,j}
    z_{i-1,j}\right), \ldots, \sigma\left(\sum_{j=1}^{n_{i-1}} w^{(n_{i})}_{i-1,j}
    z_{i-1,j}\right)\right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where we define \(\mathcal{W}_{i-1} \in \mathbb{R}^{n_{i} \times n_{i-1}}\)
    as the matrix with rows \((\mathbf{w}_{i-1}^{(1)})^T,\ldots,(\mathbf{w}_{i-1}^{(n_{i})})^T\).
    As we have done previously, to keep the notation simple, we ignore the constant
    term (or “bias variable”\(\idx{bias variable}\xdi\)) in the linear combinations
    of \(\mathbf{z}_{i-1}\). It can be incorporated by adding a \(1\) input to each
    neuron as illustrated below. We will not detail this complication here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Each component of \(g_i\) is referred to as a neuron ([Source](https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neuron](../Images/228200787298b2f894943501d2a42428.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: The input layer\(\idx{input layer}\xdi\) is \(\mathbf{z}_0 := \mathbf{x}\),
    which we refer to as layer \(0\), so that \(n_0 = d\).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to multinomial logistic regression, layer \(L+1\) (i.e., the output
    layer\(\idx{output layer}\xdi\)) is the softmax function
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\mathbf{y}} := \mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_L \mathbf{z}_{L}), \]
  prefs: []
  type: TYPE_NORMAL
- en: but this time we compose it with a linear transformation. We implicitly assume
    that \(\bgamma\) has \(K\) outputs and, in particular, we have that \(n_{L+1}
    = K\).
  prefs: []
  type: TYPE_NORMAL
- en: So the output of the classifier with parameters \(\mathbf{w} =(\mathbf{w}_0,\ldots,\mathbf{w}_{L})\)
    on input \(\mathbf{x}\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bfh(\mathbf{w}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}). \]
  prefs: []
  type: TYPE_NORMAL
- en: What makes this an MLP is that, on each layer, all outputs feed into the input
    of the next layer. In graphical terms, the edges between consecutive layers form
    a complete bipartite graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![Feedforward neural network (with help from Claude; inspired by (Source))](../Images/271cf3797d7e5cd7669d7f17ae49618b.png)'
  prefs: []
  type: TYPE_IMG
- en: We again use cross-entropy\(\idx{cross-entropy}\xdi\) as the loss function (although
    there are many [other choices of loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions)).
    That is, we set
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: Finally,
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{w}) = \ell(\mathbf{h}(\mathbf{w})). \]
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.2\. A first example[#](#a-first-example "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before detailing a general algorithm for computing the gradient, we adapt our
    first progressive example to this setting to illustrate the main ideas. Suppose
    \(d=3\), \(L=1\), \(n_1 = n_2 = 2\), and \(K = 2\), that is, we have one hidden
    layer and our output is 2-dimensional. Fix a data sample \(\mathbf{x} = (x_1,x_2,x_3)
    \in \mathbb{R}^3, \mathbf{y} = (y_1, y_2) \in \mathbb{R}^2\). For \(i=0, 1\),
    we use the notation
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathcal{W}_{0} = \begin{pmatrix} w_0 & w_1 & w_2\\ w_3 & w_4
    & w_5 \end{pmatrix} \quad \text{and} \quad \mathcal{W}_{1} = \begin{pmatrix} w_6
    & w_7\\ w_8 & w_9 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - y_1 \log \hat{y}_1
    - y_2 \log \hat{y}_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: The layer functions are as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})
    \quad\text{with}\quad \mathbf{w}_0 = (w_0, w_1, w_2, w_3, w_4, w_5) \]\[ \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1}) \quad\text{with}\quad
    \mathbf{w}_1 = (w_6, w_7, w_8, w_9). \]
  prefs: []
  type: TYPE_NORMAL
- en: We seek to compute the gradient of
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
    &= H(\mathbf{y}, \bgamma(\mathcal{W}_{1} \bsigma(\mathcal{W}_{0} \mathbf{x}))\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right] \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} Z &= \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\\ & \quad + \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)
    + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'In the forward phase, we compute \(f\) itself and the requisite Jacobians:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\mathbf{z}_0 := \mathbf{x}\\ & = (x_1, x_2, x_3)\\ &\mathbf{z}_1
    := \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})\\
    &= \begin{pmatrix} \sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)\\ \sigma(w_3 x_1 + w_4
    x_2 + w_5 x_3) \end{pmatrix}\\ &J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := J_{\bsigma}(\mathcal{W}_{0}
    \mathbf{z}_{0}) \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_0] & \mathbb{B}_{2}[\mathbf{z}_0]
    \end{pmatrix}\\ &= \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \begin{pmatrix}
    \mathcal{W}_{0} & I_{2\times 2} \otimes \mathbf{z}_0^T \end{pmatrix}\\ &= \begin{pmatrix}
    \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \mathcal{W}_{0} & \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{z}_{0})) \otimes \mathbf{z}_0^T \end{pmatrix}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Chain Rule* to compute the Jacobian of \(J_{\bfg_0}\).
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\hat{\mathbf{y}} := \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1)
    = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\\ &= \begin{pmatrix} Z^{-1} \exp(w_6\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\\ Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)) \end{pmatrix}\\
    &J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):= J_{\bgamma}(\mathcal{W}_{1} \mathbf{z}_{1})
    \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_1] & \mathbb{B}_{2}[\mathbf{z}_1] \end{pmatrix}\\
    &= [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \begin{pmatrix} \mathcal{W}_{1} & I_{2\times
    2} \otimes \mathbf{z}_1^T \end{pmatrix}\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} & [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \otimes \mathbf{z}_1^T \end{pmatrix},
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(Z\) was introduced previously and we used the expression for \(J_{\bgamma}\)
    from the previous subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Finally
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &f(\mathbf{w}) := \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}})\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\ &J_{\ell}(\hat{\mathbf{y}})
    = - (\mathbf{y} \oslash \hat{\mathbf{y}})^T. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We first compute the partial derivatives with respect to \(\mathbf{w}_1 = (w_6,
    w_7, w_8, w_9)\). For this step, we think of \(f\) as the composition of \(\ell(\mathbf{z}_2)\)
    as a function of \(\mathbf{z}_2\) and \(\bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \bgamma(\mathcal{W}_1
    \mathbf{z}_1)\) as a function of \(\mathbf{w}_1\). Here \(\mathbf{z}_1\) does
    not depend on \(\mathbf{w}_1\) and therefore can be considered fixed for this
    calculation. By the *Chain Rule* and the *Properties of the Kronecker Product
    (f)*, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6}
    & \frac{\partial f(\mathbf{w})}{\partial w_7} & \frac{\partial f(\mathbf{w})}{\partial
    w_8} & \frac{\partial f(\mathbf{w})}{\partial w_9} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \otimes \mathbf{z}_1^T \right\}\\ &= \left\{- (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}
    \mathbf{z}_{1}))^T \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \right\} \otimes \mathbf{z}_1^T\\ &=
    (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T \otimes \mathbf{z}_1^T\\
    &= (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T,
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Properties of the Hadamard Product* and the fact that \(\hat{\mathbf{y}}
    = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\) similarly to a calculation we did
    in the multinomial logistic regression setting.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the partial derivatives with respect to \(\mathbf{w}_0 = (w_0, w_1,
    \ldots, w_5)\), we first need to compute partial derivatives with respect to \(\mathbf{z}_1
    = (z_{1,1}, z_{1,2})\) since \(f\) depends on \(\mathbf{w}_0\) through it. For
    this calculation, we think again of \(f\) as the composition \(\ell(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1))\), but this time our focus is on the variables \(\mathbf{z}_1\).
    This is almost identical to the previous calculation, except that we use the block
    of \(J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1)\) corresponding to the partial derivatives
    with respect to \(\mathbf{z}_1\) (i.e., the “\(A\)” block). We obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}
    & \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} \right\}\\ &= - (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}\mathbf{z}_1))^T
    \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The vector \(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}, \frac{\partial
    f(\mathbf{w})}{\partial z_{1,2}}\right)\) is called an adjoint.
  prefs: []
  type: TYPE_NORMAL
- en: We now compute the gradient of \(\mathbf{f}\) with respect to \(\mathbf{w}_0
    = (w_0, w_1, \ldots, w_5)\). For this step, we think of \(f\) as the composition
    of \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) as a function of \(\mathbf{z}_1\)
    and \(\bfg_0(\mathbf{z}_0, \mathbf{w}_0)\) as a function of \(\mathbf{w}_0\).
    Here \(\mathbf{w}_1\) and \(\mathbf{z}_0\) do not depend on \(\mathbf{w}_0\) and
    therefore can be considered fixed for this calculation. By the *Chain Rule*
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0}
    & \frac{\partial f(\mathbf{w})}{\partial w_1} & \frac{\partial f(\mathbf{w})}{\partial
    w_2} & \frac{\partial f(\mathbf{w})}{\partial w_3} & \frac{\partial f(\mathbf{w})}{\partial
    w_4} & \frac{\partial f(\mathbf{w})}{\partial w_5} \end{pmatrix}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \left[\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))
    \otimes \mathbf{z}_0^T\right]\\ &= [(\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
    \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))] \otimes
    \mathbf{z}_0^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{x}))] \otimes \mathbf{x}^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1}
    \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x}) \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0}
    \mathbf{x})))] \otimes \mathbf{x}^T \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used *Properties of the Kronecker Product (f)* on the second to last
    line and our expression for the derivative of the sigmoid function on the last
    line.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x})
    \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T
    & (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T
    \end{pmatrix} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We return to the concrete example from the previous section.
    We re-write the gradient as'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\mathbf{z}_2 -
    \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\mathbf{z}_1 \odot (\mathbf{1} - \mathbf{z}_1))]
    \otimes \mathbf{z}_0^T & (\mathbf{z}_2 - \mathbf{y})^T \otimes \mathbf{z}_1^T
    \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We will use [`torch.nn.functional.sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html)
    and [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)
    for the sigmoid and softmax functions respectively. We also use [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html)
    for the inner product (i.e., dot product) of two vectors (as tensors) and [`torch.diag`](https://pytorch.org/docs/stable/generated/torch.diag.html)
    for the creation of a diagonal matrix with specified entries on its diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We compute the gradient \(\nabla f(\mathbf{w})\) using AD.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We use our formulas to confirm that they match these results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The results match with the AD output.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.3\. Computing the gradient[#](#computing-the-gradient "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now detail how to compute the gradient of \(f(\mathbf{w})\) for a general
    MLP. In the forward loop, we first set \(\mathbf{z}_0 := \mathbf{x}\) and then
    we compute for \(i = 0,1,\ldots,L-1\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{i+1} &:= \bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To compute the Jacobian of \(\bfg_i\), we use the *Chain Rule* on the composition
    \(\bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma(\bfk_i(\mathbf{z}_i,\mathbf{w}_i))\)
    where we define \(\bfk_i(\mathbf{z}_i,\mathbf{w}_i) = \mathcal{W}_i \mathbf{z}_i\).
    That is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) = J_{\bsigma}\left(\mathcal{W}_i \mathbf{z}_i\right)
    J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i). \]
  prefs: []
  type: TYPE_NORMAL
- en: In our analysis of multinomial logistic regression, we computed the Jacobian
    of \(\bfk_i\). We obtained
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i) = \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i]
    & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] \end{pmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] = \mathcal{W}_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_i^T
    & \cdots & \mathbf{e}_{n_{i+1}}\mathbf{z}_i^T \end{pmatrix} = I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where here \(\mathbf{e}_{j}\) is the \(j\)-th standard basis vector in \(\mathbb{R}^{n_{i+1}}\).
  prefs: []
  type: TYPE_NORMAL
- en: From a previous calculation, the Jacobian of
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n_{i+1}}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n_{i+1}})), \]
  prefs: []
  type: TYPE_NORMAL
- en: is
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma(\mathbf{t}) \odot (\mathbf{1}
    - \bsigma(\mathbf{t}))). \]
  prefs: []
  type: TYPE_NORMAL
- en: Combining the previous formulas, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) &= J_{\bsigma}\left(\mathcal{W}_i
    \mathbf{z}_i\right) J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i]
    \end{pmatrix}\\ &=\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    & \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we define
  prefs: []
  type: TYPE_NORMAL
- en: \[ \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] = \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \mathcal{W}_i, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1}
    - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \left(I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T\right)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \otimes \mathbf{z}_i^T, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Properties of the Kronecker Product (f)*.
  prefs: []
  type: TYPE_NORMAL
- en: For layer \(L+1\) (i.e., the output layer), we have previously computed the
    Jacobian of the softmax function composed with a linear transformation. We get
  prefs: []
  type: TYPE_NORMAL
- en: '\[\begin{align*} &\mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\\
    &= \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ &\begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    := J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L})\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L))
    - \bgamma(\mathcal{W}_{L}\mathbf{z}_L) \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T]
    \mathcal{W}_{L} & [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L)) - \bgamma(\mathcal{W}_{L}\mathbf{z}_L)
    \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T] \otimes \mathbf{z}_L^T \end{pmatrix}\\
    &=: \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]&
    \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix} \end{align*}\]'
  prefs: []
  type: TYPE_NORMAL
- en: Also, as in the multinomial logistic regression case, the loss and gradient
    of the loss are
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1}) = - \sum_{i=1}^K
    y_i \log z_{L+1,i}\\ \mathbf{q}_{L+1} &:= \nabla \ell(\mathbf{z}_{L+1}) = \left(-
    \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Initialization:* \(\mathbf{z}_0 := \mathbf{x}\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Forward loop:* For \(i = 0,1,\ldots,L-1\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{i+1} &:= g_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i \right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
    =\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] &
    \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ \begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    &:= J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L}) = \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L},
    \mathbf{w}_{L}]& \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1})\\ \mathbf{p}_{L+1}
    &:= \nabla {\ell}(\mathbf{z}_{L+1}) = \left(- \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Backward loop:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{p}_{L} := A_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{C}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1}\\ \mathbf{q}_{L} := B_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{D}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'and for \(i = L-1,L-2,\ldots,1,0\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{p}_{i} &:= A_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1}\\ \mathbf{q}_{i} &:= B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Output:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_{L}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement the training of a neural network in PyTorch.
    We use the Fashion-MNIST dataset again. We first load it again. We also check
    for the availability of GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We construct a two-layer model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As we did for multinomial logistic regression, we use the SGD optimizer and
    the cross-entropy loss (which in PyTorch includes the softmax function and expects
    labels to be actual class labels rather than one-hot encoding).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We train for 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'On the test data, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Disappointingly, this is significantly less accurate than what we obtained using
    multinomial logistic regression. It turns out that using a different optimizer
    gives much better results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**CHAT & LEARN** We mentioned that there are many optimizers available in PyTorch
    besides SGD and Adam. Ask your favorite AI chatbot to explain and implement a
    different optimizer, such as Adagrad or RMSprop, for the MLP. Compare the results
    with those obtained using SGD and Adam. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Regularization techniques are often used to prevent overfitting
    in neural networks. Ask your favorite AI chatbot about \(L_1\) and \(L_2\) regularization,
    dropout, and early stopping. Discuss how these techniques can be incorporated
    into the training process and their effects on the learned model. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** What is the role of the sigmoid function in a multilayer perceptron (MLP)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It is used as the loss function for training the MLP.
  prefs: []
  type: TYPE_NORMAL
- en: b) It is used as the nonlinear activation function in each layer of the MLP.
  prefs: []
  type: TYPE_NORMAL
- en: c) It is used to compute the gradient of the loss function with respect to the
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: d) It is used to initialize the weights of the MLP.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** In an MLP, what is the purpose of the softmax function in the output
    layer?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To introduce nonlinearity into the model.
  prefs: []
  type: TYPE_NORMAL
- en: b) To normalize the outputs into a probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: c) To compute the gradient of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: d) To reduce the dimensionality of the output.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** What is the Jacobian matrix of the elementwise sigmoid function \(\boldsymbol{\sigma}(\mathbf{t})
    = (\sigma(t_1), \dots, \sigma(t_n))\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t}))\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t}) \odot
    (1-\boldsymbol{\sigma}(\mathbf{t}))\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t})
    \odot (1 - \boldsymbol{\sigma}(\mathbf{t})))\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t})(1
    - \boldsymbol{\sigma}(\mathbf{t}))^T\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** In the forward phase of computing the gradient of the loss function in
    an MLP, what is the output of the \(i\)-th hidden layer?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i)\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \mathbf{z}_i\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\gamma}(\mathcal{W}_i
    \mathbf{z}_i)\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \boldsymbol{\sigma}(\mathbf{z}_i)\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** What is the output of the backward loop in computing the gradient of
    the loss function in an MLP?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The gradient of the loss function with respect to the activations of each
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: b) The gradient of the loss function with respect to the weights of each layer.
  prefs: []
  type: TYPE_NORMAL
- en: c) The updated weights of the MLP.
  prefs: []
  type: TYPE_NORMAL
- en: d) The loss function value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states that “Each of the main layers
    of a feedforward neural network has two components, an affine map and a nonlinear
    activation function. For the latter, we restrict ourselves here to the sigmoid
    function.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The text states that the softmax function is
    used in the output layer to produce a probability distribution over the possible
    classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text states that the Jacobian of the elementwise
    sigmoid function is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\boldsymbol{\sigma}}(t) = \mathrm{diag}(\boldsymbol{\sigma}'(t)) = \mathrm{diag}(\boldsymbol{\sigma}(t)
    \odot (1 - \boldsymbol{\sigma}(t))) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\odot\) denotes the Hadamard (elementwise) product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: a. Justification: The text defines the output of the \(i\)-th
    hidden layer as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\boldsymbol{\sigma}\) is the sigmoid activation function and \(\mathcal{W}_i\)
    is the weight matrix for the \(i\)-th layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text states that the output of the backward
    loop is the gradient of the loss function with respect to the weights of each
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0, \mathbf{q}_1, \dots, \mathbf{q}_L)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,
    \mathbf{w}_i]^T \mathbf{p}_{i+1}\) is the gradient with respect to the weights
    of the \(i\)-th layer.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.1\. Multilayer perceptron[#](#multilayer-perceptron "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each of the main layers of a feedforward neural network\(\idx{neural network}\xdi\)
    has two components, an affine map and a nonlinear activation function\(\idx{activation
    function}\xdi\). For the latter, we restrict ourselves here to the sigmoid function\(\idx{sigmoid}\xdi\)
    (although there are many [other choices of activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)).
  prefs: []
  type: TYPE_NORMAL
- en: The Jacobian of the elementwise version of the sigmoid function (which we will
    need later on)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n})), \]
  prefs: []
  type: TYPE_NORMAL
- en: as a function of several variables can be computed from \(\sigma'\), i.e., the
    derivative of the single-variable case. Indeed, we have seen in a previous example
    that \(J_{\bsigma}(\mathbf{t})\) is the diagonal matrix with diagonal entries
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma'(t_j) = \frac{e^{-t_j}}{(1 + e^{-t_j})^2} = \sigma(t_j) (1 - \sigma(t_j)),
    \qquad j=1, \ldots, n, \]
  prefs: []
  type: TYPE_NORMAL
- en: which we denote
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma'(\mathbf{t})) = \mathrm{diag}(\bsigma(\mathbf{t})
    \odot (\mathbf{1} - \bsigma(\mathbf{t}))), \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bsigma'(\mathbf{t}) = (\sigma'(t_1), \ldots,\sigma'(t_{n}))\) and \(\mathbf{1}\)
    is the all-one vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'We consider an arbitrary number of layers\(\idx{layer}\xdi\) \(L+2\). As a
    special case of progressive functions\(\idx{progressive function}\xdi\), hidden
    layer\(\idx{hidden layer}\xdi\) \(i\), \(i=1,\ldots,L\), is defined by a continuously
    differentiable function \(\mathbf{z}_i := \bfg_{i-1}(\mathbf{z}_{i-1}, \mathbf{w}_{i-1})\)
    which takes two vector-valued inputs: a vector \(\mathbf{z}_{i-1} \in \mathbb{R}^{n_{i-1}}\)
    fed from the \((i-1)\)-st layer and a vector \(\mathbf{w}_{i-1} \in \mathbb{R}^{r_{i-1}}\)
    of parameters specific to the \(i\)-th layer'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \bfg_{i-1} = (g_{i-1,1},\ldots,g_{i-1,n_{i}}) : \mathbb{R}^{n_{i-1} + r_{i-1}}
    \to \mathbb{R}^{n_{i}}. \]'
  prefs: []
  type: TYPE_NORMAL
- en: The output \(\mathbf{z}_i\) of \(\bfg_{i-1}\) is a vector in \(\mathbb{R}^{n_{i}}\)
    which is passed to the \((i+1)\)-st layer as input. Each component of \(\bfg_{i-1}\)
    is referred to as a neuron. Here \(r_{i-1} = n_{i} n_{i-1}\) and \(\mathbf{w}_{i-1}
    = (\mathbf{w}^{(1)}_{i-1},\ldots,\mathbf{w}^{(n_{i})}_{i-1})\) are the parameters
    with \(\mathbf{w}^{(k)}_{i-1} \in \mathbb{R}^{n_{i-1}}\) for all \(k\). Specifically,
    \(\bfg_{i-1}\) is given by
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bfg_{i-1}(\mathbf{z}_{i-1},\mathbf{w}_{i-1}) = \bsigma\left(\mathcal{W}_{i-1}
    \mathbf{z}_{i-1}\right) = \left(\sigma\left(\sum_{j=1}^{n_{i-1}} w^{(1)}_{i-1,j}
    z_{i-1,j}\right), \ldots, \sigma\left(\sum_{j=1}^{n_{i-1}} w^{(n_{i})}_{i-1,j}
    z_{i-1,j}\right)\right) \]
  prefs: []
  type: TYPE_NORMAL
- en: where we define \(\mathcal{W}_{i-1} \in \mathbb{R}^{n_{i} \times n_{i-1}}\)
    as the matrix with rows \((\mathbf{w}_{i-1}^{(1)})^T,\ldots,(\mathbf{w}_{i-1}^{(n_{i})})^T\).
    As we have done previously, to keep the notation simple, we ignore the constant
    term (or “bias variable”\(\idx{bias variable}\xdi\)) in the linear combinations
    of \(\mathbf{z}_{i-1}\). It can be incorporated by adding a \(1\) input to each
    neuron as illustrated below. We will not detail this complication here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Each component of \(g_i\) is referred to as a neuron ([Source](https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Neuron](../Images/228200787298b2f894943501d2a42428.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: The input layer\(\idx{input layer}\xdi\) is \(\mathbf{z}_0 := \mathbf{x}\),
    which we refer to as layer \(0\), so that \(n_0 = d\).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to multinomial logistic regression, layer \(L+1\) (i.e., the output
    layer\(\idx{output layer}\xdi\)) is the softmax function
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\mathbf{y}} := \mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_L \mathbf{z}_{L}), \]
  prefs: []
  type: TYPE_NORMAL
- en: but this time we compose it with a linear transformation. We implicitly assume
    that \(\bgamma\) has \(K\) outputs and, in particular, we have that \(n_{L+1}
    = K\).
  prefs: []
  type: TYPE_NORMAL
- en: So the output of the classifier with parameters \(\mathbf{w} =(\mathbf{w}_0,\ldots,\mathbf{w}_{L})\)
    on input \(\mathbf{x}\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bfh(\mathbf{w}) = \bfg_{L}(\bfg_{L-1}(\cdots \bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1),
    \cdots, \mathbf{w}_{L-1}), \mathbf{w}_{L}). \]
  prefs: []
  type: TYPE_NORMAL
- en: What makes this an MLP is that, on each layer, all outputs feed into the input
    of the next layer. In graphical terms, the edges between consecutive layers form
    a complete bipartite graph.
  prefs: []
  type: TYPE_NORMAL
- en: '![Feedforward neural network (with help from Claude; inspired by (Source))](../Images/271cf3797d7e5cd7669d7f17ae49618b.png)'
  prefs: []
  type: TYPE_IMG
- en: We again use cross-entropy\(\idx{cross-entropy}\xdi\) as the loss function (although
    there are many [other choices of loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions)).
    That is, we set
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{i=1}^K
    y_i \log \hat{y}_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: Finally,
  prefs: []
  type: TYPE_NORMAL
- en: \[ f(\mathbf{w}) = \ell(\mathbf{h}(\mathbf{w})). \]
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.2\. A first example[#](#a-first-example "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before detailing a general algorithm for computing the gradient, we adapt our
    first progressive example to this setting to illustrate the main ideas. Suppose
    \(d=3\), \(L=1\), \(n_1 = n_2 = 2\), and \(K = 2\), that is, we have one hidden
    layer and our output is 2-dimensional. Fix a data sample \(\mathbf{x} = (x_1,x_2,x_3)
    \in \mathbb{R}^3, \mathbf{y} = (y_1, y_2) \in \mathbb{R}^2\). For \(i=0, 1\),
    we use the notation
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathcal{W}_{0} = \begin{pmatrix} w_0 & w_1 & w_2\\ w_3 & w_4
    & w_5 \end{pmatrix} \quad \text{and} \quad \mathcal{W}_{1} = \begin{pmatrix} w_6
    & w_7\\ w_8 & w_9 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and let
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}}) = - y_1 \log \hat{y}_1
    - y_2 \log \hat{y}_2. \]
  prefs: []
  type: TYPE_NORMAL
- en: The layer functions are as follows
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})
    \quad\text{with}\quad \mathbf{w}_0 = (w_0, w_1, w_2, w_3, w_4, w_5) \]\[ \bfg_1(\mathbf{z}_1,
    \mathbf{w}_1) = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1}) \quad\text{with}\quad
    \mathbf{w}_1 = (w_6, w_7, w_8, w_9). \]
  prefs: []
  type: TYPE_NORMAL
- en: We seek to compute the gradient of
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} f(\mathbf{w}) &= \ell(\bfg_1(\bfg_0(\mathbf{x},\mathbf{w}_0),\mathbf{w}_1))\\
    &= H(\mathbf{y}, \bgamma(\mathcal{W}_{1} \bsigma(\mathcal{W}_{0} \mathbf{x}))\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right] \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} Z &= \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\\ & \quad + \exp(w_8\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)
    + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'In the forward phase, we compute \(f\) itself and the requisite Jacobians:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\mathbf{z}_0 := \mathbf{x}\\ & = (x_1, x_2, x_3)\\ &\mathbf{z}_1
    := \bfg_0(\mathbf{z}_0, \mathbf{w}_0) = \bsigma(\mathcal{W}_{0} \mathbf{z}_{0})\\
    &= \begin{pmatrix} \sigma(w_0 x_1 + w_1 x_2 + w_2 x_3)\\ \sigma(w_3 x_1 + w_4
    x_2 + w_5 x_3) \end{pmatrix}\\ &J_{\bfg_0}(\mathbf{z}_0, \mathbf{w}_0) := J_{\bsigma}(\mathcal{W}_{0}
    \mathbf{z}_{0}) \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_0] & \mathbb{B}_{2}[\mathbf{z}_0]
    \end{pmatrix}\\ &= \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \begin{pmatrix}
    \mathcal{W}_{0} & I_{2\times 2} \otimes \mathbf{z}_0^T \end{pmatrix}\\ &= \begin{pmatrix}
    \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0})) \mathcal{W}_{0} & \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{z}_{0})) \otimes \mathbf{z}_0^T \end{pmatrix}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Chain Rule* to compute the Jacobian of \(J_{\bfg_0}\).
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\hat{\mathbf{y}} := \mathbf{z}_2 := \bfg_1(\mathbf{z}_1, \mathbf{w}_1)
    = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\\ &= \begin{pmatrix} Z^{-1} \exp(w_6\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\\ Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3)) \end{pmatrix}\\
    &J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1):= J_{\bgamma}(\mathcal{W}_{1} \mathbf{z}_{1})
    \begin{pmatrix} \mathbb{A}_{2}[\mathbf{w}_1] & \mathbb{B}_{2}[\mathbf{z}_1] \end{pmatrix}\\
    &= [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \begin{pmatrix} \mathcal{W}_{1} & I_{2\times
    2} \otimes \mathbf{z}_1^T \end{pmatrix}\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} & [\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \otimes \mathbf{z}_1^T \end{pmatrix},
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(Z\) was introduced previously and we used the expression for \(J_{\bgamma}\)
    from the previous subsection.
  prefs: []
  type: TYPE_NORMAL
- en: Finally
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &f(\mathbf{w}) := \ell(\hat{\mathbf{y}}) = H(\mathbf{y}, \hat{\mathbf{y}})\\
    &= - y_1 \log \left[Z^{-1} \exp(w_6\sigma(w_0 x_1 + w_1 x_2 + w_2 x_3) + w_7\sigma(w_3
    x_1 + w_4 x_2 + w_5 x_3))\right]\\ & \quad - y_2 \log \left[Z^{-1} \exp(w_8\sigma(w_0
    x_1 + w_1 x_2 + w_2 x_3) + w_9\sigma(w_3 x_1 + w_4 x_2 + w_5 x_3))\right]\\ &J_{\ell}(\hat{\mathbf{y}})
    = - (\mathbf{y} \oslash \hat{\mathbf{y}})^T. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We first compute the partial derivatives with respect to \(\mathbf{w}_1 = (w_6,
    w_7, w_8, w_9)\). For this step, we think of \(f\) as the composition of \(\ell(\mathbf{z}_2)\)
    as a function of \(\mathbf{z}_2\) and \(\bfg_1(\mathbf{z}_1, \mathbf{w}_1) = \bgamma(\mathcal{W}_1
    \mathbf{z}_1)\) as a function of \(\mathbf{w}_1\). Here \(\mathbf{z}_1\) does
    not depend on \(\mathbf{w}_1\) and therefore can be considered fixed for this
    calculation. By the *Chain Rule* and the *Properties of the Kronecker Product
    (f)*, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_6}
    & \frac{\partial f(\mathbf{w})}{\partial w_7} & \frac{\partial f(\mathbf{w})}{\partial
    w_8} & \frac{\partial f(\mathbf{w})}{\partial w_9} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \otimes \mathbf{z}_1^T \right\}\\ &= \left\{- (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}
    \mathbf{z}_{1}))^T \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \right\} \otimes \mathbf{z}_1^T\\ &=
    (\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T \otimes \mathbf{z}_1^T\\
    &= (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T,
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Properties of the Hadamard Product* and the fact that \(\hat{\mathbf{y}}
    = \bgamma(\mathcal{W}_{1} \mathbf{z}_{1})\) similarly to a calculation we did
    in the multinomial logistic regression setting.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the partial derivatives with respect to \(\mathbf{w}_0 = (w_0, w_1,
    \ldots, w_5)\), we first need to compute partial derivatives with respect to \(\mathbf{z}_1
    = (z_{1,1}, z_{1,2})\) since \(f\) depends on \(\mathbf{w}_0\) through it. For
    this calculation, we think again of \(f\) as the composition \(\ell(\bfg_1(\mathbf{z}_1,
    \mathbf{w}_1))\), but this time our focus is on the variables \(\mathbf{z}_1\).
    This is almost identical to the previous calculation, except that we use the block
    of \(J_{\bfg_1}(\mathbf{z}_1, \mathbf{w}_1)\) corresponding to the partial derivatives
    with respect to \(\mathbf{z}_1\) (i.e., the “\(A\)” block). We obtain
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}
    & \frac{\partial f(\mathbf{w})}{\partial z_{1,2}} \end{pmatrix}\\ &= - (\mathbf{y}
    \oslash \hat{\mathbf{y}})^T \left\{ \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1))
    - \bgamma(\mathcal{W}_{1}\mathbf{z}_1) \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T]
    \mathcal{W}_{1} \right\}\\ &= - (\mathbf{y} \oslash \bgamma(\mathcal{W}_{1}\mathbf{z}_1))^T
    \,[\mathrm{diag}(\bgamma(\mathcal{W}_{1}\mathbf{z}_1)) - \bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    \, \bgamma(\mathcal{W}_{1}\mathbf{z}_1)^T] \mathcal{W}_{1}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The vector \(\left(\frac{\partial f(\mathbf{w})}{\partial z_{1,1}}, \frac{\partial
    f(\mathbf{w})}{\partial z_{1,2}}\right)\) is called an adjoint.
  prefs: []
  type: TYPE_NORMAL
- en: We now compute the gradient of \(\mathbf{f}\) with respect to \(\mathbf{w}_0
    = (w_0, w_1, \ldots, w_5)\). For this step, we think of \(f\) as the composition
    of \(\ell(\bfg_1(\mathbf{z}_1, \mathbf{w}_1))\) as a function of \(\mathbf{z}_1\)
    and \(\bfg_0(\mathbf{z}_0, \mathbf{w}_0)\) as a function of \(\mathbf{w}_0\).
    Here \(\mathbf{w}_1\) and \(\mathbf{z}_0\) do not depend on \(\mathbf{w}_0\) and
    therefore can be considered fixed for this calculation. By the *Chain Rule*
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\begin{pmatrix}\frac{\partial f(\mathbf{w})}{\partial w_0}
    & \frac{\partial f(\mathbf{w})}{\partial w_1} & \frac{\partial f(\mathbf{w})}{\partial
    w_2} & \frac{\partial f(\mathbf{w})}{\partial w_3} & \frac{\partial f(\mathbf{w})}{\partial
    w_4} & \frac{\partial f(\mathbf{w})}{\partial w_5} \end{pmatrix}\\ &= (\bgamma(\mathcal{W}_{1}\mathbf{z}_1)
    - \mathbf{y})^T \mathcal{W}_{1} \left[\mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))
    \otimes \mathbf{z}_0^T\right]\\ &= [(\bgamma(\mathcal{W}_{1}\mathbf{z}_1) - \mathbf{y})^T
    \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0} \mathbf{z}_{0}))] \otimes
    \mathbf{z}_0^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma'(\mathcal{W}_{0}
    \mathbf{x}))] \otimes \mathbf{x}^T\\ &= [(\hat{\mathbf{y}} - \mathbf{y})^T \mathcal{W}_{1}
    \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x}) \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0}
    \mathbf{x})))] \otimes \mathbf{x}^T \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used *Properties of the Kronecker Product (f)* on the second to last
    line and our expression for the derivative of the sigmoid function on the last
    line.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\hat{\mathbf{y}}
    - \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\bsigma(\mathcal{W}_{0} \mathbf{x})
    \odot (\mathbf{1} - \bsigma(\mathcal{W}_{0} \mathbf{x})))] \otimes \mathbf{x}^T
    & (\hat{\mathbf{y}} - \mathbf{y})^T \otimes \bsigma(\mathcal{W}_{0} \mathbf{x})^T
    \end{pmatrix} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We return to the concrete example from the previous section.
    We re-write the gradient as'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla f(\mathbf{w})^T &= \begin{pmatrix} [(\mathbf{z}_2 -
    \mathbf{y})^T \mathcal{W}_{1} \mathrm{diag}(\mathbf{z}_1 \odot (\mathbf{1} - \mathbf{z}_1))]
    \otimes \mathbf{z}_0^T & (\mathbf{z}_2 - \mathbf{y})^T \otimes \mathbf{z}_1^T
    \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We will use [`torch.nn.functional.sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.functional.sigmoid.html)
    and [`torch.nn.functional.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)
    for the sigmoid and softmax functions respectively. We also use [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html)
    for the inner product (i.e., dot product) of two vectors (as tensors) and [`torch.diag`](https://pytorch.org/docs/stable/generated/torch.diag.html)
    for the creation of a diagonal matrix with specified entries on its diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We compute the gradient \(\nabla f(\mathbf{w})\) using AD.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We use our formulas to confirm that they match these results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The results match with the AD output.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 8.5.3\. Computing the gradient[#](#computing-the-gradient "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now detail how to compute the gradient of \(f(\mathbf{w})\) for a general
    MLP. In the forward loop, we first set \(\mathbf{z}_0 := \mathbf{x}\) and then
    we compute for \(i = 0,1,\ldots,L-1\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{i+1} &:= \bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: To compute the Jacobian of \(\bfg_i\), we use the *Chain Rule* on the composition
    \(\bfg_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma(\bfk_i(\mathbf{z}_i,\mathbf{w}_i))\)
    where we define \(\bfk_i(\mathbf{z}_i,\mathbf{w}_i) = \mathcal{W}_i \mathbf{z}_i\).
    That is,
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) = J_{\bsigma}\left(\mathcal{W}_i \mathbf{z}_i\right)
    J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i). \]
  prefs: []
  type: TYPE_NORMAL
- en: In our analysis of multinomial logistic regression, we computed the Jacobian
    of \(\bfk_i\). We obtained
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i) = \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i]
    & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] \end{pmatrix}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Recall that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] = \mathcal{W}_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{B}_{n_{i+1}}[\mathbf{z}_i] = \begin{pmatrix} \mathbf{e}_1 \mathbf{z}_i^T
    & \cdots & \mathbf{e}_{n_{i+1}}\mathbf{z}_i^T \end{pmatrix} = I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T, \]
  prefs: []
  type: TYPE_NORMAL
- en: where here \(\mathbf{e}_{j}\) is the \(j\)-th standard basis vector in \(\mathbb{R}^{n_{i+1}}\).
  prefs: []
  type: TYPE_NORMAL
- en: From a previous calculation, the Jacobian of
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bsigma(\mathbf{t}) = (\sigma_{1}(\mathbf{t}),\ldots,\sigma_{n_{i+1}}(\mathbf{t}))
    := (\sigma(t_1),\ldots,\sigma(t_{n_{i+1}})), \]
  prefs: []
  type: TYPE_NORMAL
- en: is
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\bsigma}(\mathbf{t}) = \mathrm{diag}(\bsigma(\mathbf{t}) \odot (\mathbf{1}
    - \bsigma(\mathbf{t}))). \]
  prefs: []
  type: TYPE_NORMAL
- en: Combining the previous formulas, we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i) &= J_{\bsigma}\left(\mathcal{W}_i
    \mathbf{z}_i\right) J_{\bfk_i}(\mathbf{z}_i,\mathbf{w}_i)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \begin{pmatrix} \mathbb{A}_{n_{i+1}}[\mathbf{w}_i] & \mathbb{B}_{n_{i+1}}[\mathbf{z}_i]
    \end{pmatrix}\\ &=\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    & \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we define
  prefs: []
  type: TYPE_NORMAL
- en: \[ \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] = \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \mathcal{W}_i, \]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]
    &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i \mathbf{z}_i\right) \odot (\mathbf{1}
    - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right) \left(I_{n_{i+1}\times
    n_{i+1}} \otimes \mathbf{z}_i^T\right)\\ &= \mathrm{diag}\left(\bsigma\left(\mathcal{W}_i
    \mathbf{z}_i\right) \odot (\mathbf{1} - \bsigma\left(\mathcal{W}_i \mathbf{z}_i\right))\right)
    \otimes \mathbf{z}_i^T, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Properties of the Kronecker Product (f)*.
  prefs: []
  type: TYPE_NORMAL
- en: For layer \(L+1\) (i.e., the output layer), we have previously computed the
    Jacobian of the softmax function composed with a linear transformation. We get
  prefs: []
  type: TYPE_NORMAL
- en: '\[\begin{align*} &\mathbf{z}_{L+1} := \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})\\
    &= \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ &\begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    := J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L})\\ &= \begin{pmatrix} [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L))
    - \bgamma(\mathcal{W}_{L}\mathbf{z}_L) \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T]
    \mathcal{W}_{L} & [\mathrm{diag}(\bgamma(\mathcal{W}_{L}\mathbf{z}_L)) - \bgamma(\mathcal{W}_{L}\mathbf{z}_L)
    \, \bgamma(\mathcal{W}_{L}\mathbf{z}_L)^T] \otimes \mathbf{z}_L^T \end{pmatrix}\\
    &=: \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}]&
    \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix} \end{align*}\]'
  prefs: []
  type: TYPE_NORMAL
- en: Also, as in the multinomial logistic regression case, the loss and gradient
    of the loss are
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1}) = - \sum_{i=1}^K
    y_i \log z_{L+1,i}\\ \mathbf{q}_{L+1} &:= \nabla \ell(\mathbf{z}_{L+1}) = \left(-
    \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Initialization:* \(\mathbf{z}_0 := \mathbf{x}\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Forward loop:* For \(i = 0,1,\ldots,L-1\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{i+1} &:= g_i(\mathbf{z}_i,\mathbf{w}_i) = \bsigma\left(\mathcal{W}_i
    \mathbf{z}_i \right)\\ \begin{pmatrix} A_i & B_i \end{pmatrix} &:= J_{\bfg_i}(\mathbf{z}_i,\mathbf{w}_i)
    =\begin{pmatrix} \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] &
    \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i] \end{pmatrix} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{L+1} &:= \bfg_{L}(\mathbf{z}_{L}, \mathbf{w}_{L})
    = \bgamma(\mathcal{W}_{L} \mathbf{z}_{L})\\ \begin{pmatrix} A_{L} & B_{L} \end{pmatrix}
    &:= J_{\bfg_{L}}(\mathbf{z}_{L}, \mathbf{w}_{L}) = \begin{pmatrix} \widetilde{\mathbb{C}}_K[\mathbf{z}_{L},
    \mathbf{w}_{L}]& \widetilde{\mathbb{D}}_K[\mathbf{z}_{L}, \mathbf{w}_{L}] \end{pmatrix}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Loss:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}_{L+2} &:= \ell(\mathbf{z}_{L+1})\\ \mathbf{p}_{L+1}
    &:= \nabla {\ell}(\mathbf{z}_{L+1}) = \left(- \frac{y_1}{z_{L+1,1}},\ldots,- \frac{y_K}{z_{L+1,K}}\right).
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Backward loop:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{p}_{L} := A_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{C}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1}\\ \mathbf{q}_{L} := B_{L}^T \,\mathbf{p}_{L+1} &= \widetilde{\mathbb{D}}_{K}[\mathbf{z}_{L}]^T
    \mathbf{p}_{L+1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'and for \(i = L-1,L-2,\ldots,1,0\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{p}_{i} &:= A_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{A}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1}\\ \mathbf{q}_{i} &:= B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,\mathbf{w}_i]^T
    \mathbf{p}_{i+1} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '*Output:*'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0,\mathbf{q}_1,\ldots,\mathbf{q}_{L}).
    \]
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement the training of a neural network in PyTorch.
    We use the Fashion-MNIST dataset again. We first load it again. We also check
    for the availability of GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We construct a two-layer model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: As we did for multinomial logistic regression, we use the SGD optimizer and
    the cross-entropy loss (which in PyTorch includes the softmax function and expects
    labels to be actual class labels rather than one-hot encoding).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We train for 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'On the test data, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Disappointingly, this is significantly less accurate than what we obtained using
    multinomial logistic regression. It turns out that using a different optimizer
    gives much better results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '**CHAT & LEARN** We mentioned that there are many optimizers available in PyTorch
    besides SGD and Adam. Ask your favorite AI chatbot to explain and implement a
    different optimizer, such as Adagrad or RMSprop, for the MLP. Compare the results
    with those obtained using SGD and Adam. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_nn_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Regularization techniques are often used to prevent overfitting
    in neural networks. Ask your favorite AI chatbot about \(L_1\) and \(L_2\) regularization,
    dropout, and early stopping. Discuss how these techniques can be incorporated
    into the training process and their effects on the learned model. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** What is the role of the sigmoid function in a multilayer perceptron (MLP)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) It is used as the loss function for training the MLP.
  prefs: []
  type: TYPE_NORMAL
- en: b) It is used as the nonlinear activation function in each layer of the MLP.
  prefs: []
  type: TYPE_NORMAL
- en: c) It is used to compute the gradient of the loss function with respect to the
    weights.
  prefs: []
  type: TYPE_NORMAL
- en: d) It is used to initialize the weights of the MLP.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** In an MLP, what is the purpose of the softmax function in the output
    layer?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To introduce nonlinearity into the model.
  prefs: []
  type: TYPE_NORMAL
- en: b) To normalize the outputs into a probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: c) To compute the gradient of the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: d) To reduce the dimensionality of the output.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** What is the Jacobian matrix of the elementwise sigmoid function \(\boldsymbol{\sigma}(\mathbf{t})
    = (\sigma(t_1), \dots, \sigma(t_n))\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t}))\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t}) \odot
    (1-\boldsymbol{\sigma}(\mathbf{t}))\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \mathrm{diag}(\boldsymbol{\sigma}(\mathbf{t})
    \odot (1 - \boldsymbol{\sigma}(\mathbf{t})))\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(J_{\boldsymbol{\sigma}}(\mathbf{t}) = \boldsymbol{\sigma}(\mathbf{t})(1
    - \boldsymbol{\sigma}(\mathbf{t}))^T\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** In the forward phase of computing the gradient of the loss function in
    an MLP, what is the output of the \(i\)-th hidden layer?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i)\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \mathbf{z}_i\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\gamma}(\mathcal{W}_i
    \mathbf{z}_i)\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \mathcal{W}_i
    \boldsymbol{\sigma}(\mathbf{z}_i)\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** What is the output of the backward loop in computing the gradient of
    the loss function in an MLP?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The gradient of the loss function with respect to the activations of each
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: b) The gradient of the loss function with respect to the weights of each layer.
  prefs: []
  type: TYPE_NORMAL
- en: c) The updated weights of the MLP.
  prefs: []
  type: TYPE_NORMAL
- en: d) The loss function value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states that “Each of the main layers
    of a feedforward neural network has two components, an affine map and a nonlinear
    activation function. For the latter, we restrict ourselves here to the sigmoid
    function.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The text states that the softmax function is
    used in the output layer to produce a probability distribution over the possible
    classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text states that the Jacobian of the elementwise
    sigmoid function is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ J_{\boldsymbol{\sigma}}(t) = \mathrm{diag}(\boldsymbol{\sigma}'(t)) = \mathrm{diag}(\boldsymbol{\sigma}(t)
    \odot (1 - \boldsymbol{\sigma}(t))) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\odot\) denotes the Hadamard (elementwise) product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: a. Justification: The text defines the output of the \(i\)-th
    hidden layer as:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}_{i+1} := \mathbf{g}_i(\mathbf{z}_i, \mathbf{w}_i) = \boldsymbol{\sigma}(\mathcal{W}_i
    \mathbf{z}_i) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\boldsymbol{\sigma}\) is the sigmoid activation function and \(\mathcal{W}_i\)
    is the weight matrix for the \(i\)-th layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text states that the output of the backward
    loop is the gradient of the loss function with respect to the weights of each
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla f(\mathbf{w}) = (\mathbf{q}_0, \mathbf{q}_1, \dots, \mathbf{q}_L)
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{q}_i := B_i^T \mathbf{p}_{i+1} = \widetilde{\mathbb{B}}_{n_{i+1}}[\mathbf{z}_i,
    \mathbf{w}_i]^T \mathbf{p}_{i+1}\) is the gradient with respect to the weights
    of the \(i\)-th layer.
  prefs: []
  type: TYPE_NORMAL
