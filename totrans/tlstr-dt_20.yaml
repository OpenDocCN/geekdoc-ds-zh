- en: 10  Store and share
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://tellingstorieswithdata.com/10-store_and_share.html](https://tellingstorieswithdata.com/10-store_and_share.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Preparation](./09-clean_and_prepare.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[10  Store and share](./10-store_and_share.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prerequisites**'
  prefs: []
  type: TYPE_NORMAL
- en: Read *Promoting Open Science Through Research Data Management*, ([Borghi and
    Van Gulick 2022](99-references.html#ref-Borghi2022Promoting))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describes the state of data management, and some strategies for conducting research
    that is more reproducible.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Read *Data Management in Large-Scale Education Research*, ([Lewis 2024](99-references.html#ref-lewiscrystal))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on Chapter 2 “Research Data Management”, which provides an overview of
    data management concerns, workflow, and terminology.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Read *Transparent and reproducible social science research*, ([Christensen,
    Freese, and Miguel 2019](99-references.html#ref-christensen2019transparent))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on Chapter 10 “Data Sharing”, which specifies ways to share data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Read *Datasheets for datasets*, ([Gebru et al. 2021](99-references.html#ref-gebru2021datasheets))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduces the idea of a datasheet.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Read *Data and its (dis)contents: A survey of dataset development and use in
    machine learning research*, ([Paullada et al. 2021](99-references.html#ref-Paullada2021))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Details the state of data in machine learning.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key concepts and skills**'
  prefs: []
  type: TYPE_NORMAL
- en: The FAIR principles provide the foundation from which we consider data sharing
    and storage. These specify that data should be findable, accessible, interoperable,
    and reusable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most important step is the first one, and that is to get the data off our
    local computer, and to then make it accessible by others. After that, we build
    documentation, and datasheets, to make it easier for others to understand and
    use it. Finally, we ideally enable access without our involvement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the same time as wanting to share our datasets as widely as possible, we
    should respect those whose information are contained in them. This means, for
    instance, protecting, to a reasonable extent, and informed by costs and benefits,
    personally identifying information through selective disclosure, hashing, data
    simulation, and differential privacy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, as our data get larger, approaches that were viable when they were
    smaller start to break down. We need to consider efficiency, and explore other
    approaches, formats, and languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software and packages**'
  prefs: []
  type: TYPE_NORMAL
- en: Base R ([R Core Team 2024](99-references.html#ref-citeR))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`arrow` ([Richardson et al. 2023](99-references.html#ref-arrow))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`devtools` ([Wickham et al. 2022](99-references.html#ref-citeDevtools))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`diffpriv` ([Rubinstein and Alda 2017](99-references.html#ref-diffpriv))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fs` ([Hester, Wickham, and Csárdi 2021](99-references.html#ref-fs))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`janitor` ([Firke 2023](99-references.html#ref-janitor))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openssl` ([Ooms 2022](99-references.html#ref-openssl))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tictoc` ([Izrailev 2022](99-references.html#ref-Izrailev2014))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tidyverse` ([Wickham et al. 2019](99-references.html#ref-tidyverse))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tinytable` ([Arel-Bundock 2024](99-references.html#ref-tinytable))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*## 10.1 Introduction'
  prefs: []
  type: TYPE_NORMAL
- en: After we have put together a dataset we must store it appropriately and enable
    easy retrieval both for ourselves and others. There is no completely agreed on
    approach, but there are best standards, and this is an evolving area of research
    ([Lewis 2024](99-references.html#ref-lewiscrystal)). Wicherts, Bakker, and Molenaar
    ([2011](99-references.html#ref-Wicherts2011)) found that a reluctance to share
    data was associated with research papers that had weaker evidence and more potential
    errors. While it is possible to be especially concerned about this—and entire
    careers and disciplines are based on the storage and retrieval of data—to a certain
    extent, the baseline is not onerous. If we can get our dataset off our own computer,
    then we are much of the way there. Further confirming that someone else can retrieve
    and use it, ideally without our involvement, puts us much further than most. Just
    achieving that for our data, models, and code meets the “bronze” standard of Heil
    et al. ([2021](99-references.html#ref-heil2021reproducibility)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The FAIR principles are useful when we come to think more formally about data
    sharing and management. This requires that datasets are ([Wilkinson et al. 2016](99-references.html#ref-wilkinson2016fair)):'
  prefs: []
  type: TYPE_NORMAL
- en: Findable. There is one, unchanging, identifier for the dataset and the dataset
    has high-quality descriptions and explanations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Accessible. Standardized approaches can be used to retrieve the data, and these
    are open and free, possibly with authentication, and their metadata persist even
    if the dataset is removed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interoperable. The dataset and its metadata use a broadly-applicable language
    and vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reusable. There are extensive descriptions of the dataset and the usage conditions
    are made clear along with provenance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One reason for the rise of data science is that humans are at the heart of it.
    And often the data that we are interested in directly concern humans. This means
    that there can be tension between sharing a dataset to facilitate reproducibility
    and maintaining privacy. Medicine developed approaches to this over a long time.
    And out of that we have seen the Health Insurance Portability and Accountability
    Act (HIPAA) in the US, the broader General Data Protection Regulation (GDPR) in
    Europe introduced in 2016, and the California Consumer Privacy Act (CCPA) introduced
    in 2018, among others.
  prefs: []
  type: TYPE_NORMAL
- en: Our concerns in data science tend to be about personally identifying information.
    We have a variety of ways to protect especially private information, such as emails
    and home addresses. For instance, we can hash those variables. Sometimes we may
    simulate data and distribute that instead of sharing the actual dataset. More
    recently, approaches based on differential privacy are being implemented, for
    instance for the US census. The fundamental problem of data privacy is that increased
    privacy reduces the usefulness of a dataset. The trade-off means the appropriate
    decision is nuanced and depends on costs and benefits, and we should be especially
    concerned about differentiated effects on population minorities.
  prefs: []
  type: TYPE_NORMAL
- en: Just because a dataset is FAIR, it is not necessarily an unbiased representation
    of the world. Further, it is not necessarily fair in the everyday way that word
    is used, i.e. impartial and honest ([Lima et al. 2022](99-references.html#ref-deLima2022)).
    FAIR reflects whether a dataset is appropriately available, not whether it is
    appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in this chapter we consider efficiency. As datasets and code bases
    get larger it becomes more difficult to deal with them, especially if we want
    them to be shared. We come to concerns around efficiency, not for its own sake,
    but to enable us to tell stories that could not otherwise be told. This might
    mean moving beyond CSV files to formats with other properties, or even using databases,
    such as Postgres, although even as we do so acknowledging that the simplicity
    of a CSV, as it is text-based which lends itself to human inspection, can be a
    useful feature.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Plan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The storage and retrieval of information is especially connected with libraries,
    in the traditional sense of a collection of books. These have existed since antiquity
    and have well-established protocols for deciding what information to store and
    what to discard, as well as information retrieval. One of the defining aspects
    of libraries is deliberate curation and organization. The use of a cataloging
    system ensures that books on similar topics are located close to each other, and
    there are typically also deliberate plans for ensuring the collection is up to
    date. This enables information storage and retrieval that is appropriate and efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Data science relies heavily on the internet when it comes to storage and retrieval.
    Vannevar Bush, the twentieth century engineer, defined a “memex” in 1945 as a
    device to store books, records, and communications in a way that supplements memory
    ([Bush 1945](99-references.html#ref-vannevarbush)). The key to it was the indexing,
    or linking together, of items. We see this concept echoed just four decades later
    in the proposal by Tim Berners-Lee for hypertext ([Berners-Lee 1989](99-references.html#ref-berners1989information)).
    This led to the World Wide Web and defines the way that resources are identified.
    They are then transported over the internet, using Hypertext Transfer Protocol
    (HTTP).
  prefs: []
  type: TYPE_NORMAL
- en: At its most fundamental, the internet is about storing and retrieving data.
    It is based on making various files on a computer available to others. When we
    consider the storage and retrieval of our datasets we want to especially contemplate
    for how long they should be stored and for whom ([Michener 2015](99-references.html#ref-michener2015ten)).
    For instance, if we want some dataset to be available for a decade, and widely
    available, then it becomes important to store it in open and persistent formats
    ([Hart et al. 2016](99-references.html#ref-hart2016ten)). But if we are just using
    a dataset as part of an intermediate step, and we have the original, unedited
    data and the scripts to create it, then it might be fine to not worry too much
    about such considerations. The evolution of physical storage media has similar
    complicated issues. For instance, datasets and recordings made on media such as
    wax cylinders, magnetic tapes, and proprietary optical disks, now have a variable
    ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: Storing the original, unedited data is important and there are many cases where
    unedited data have revealed or hinted at fraud ([Simonsohn 2013](99-references.html#ref-simonsohn2013just)).
    Shared data also enhances the credibility of our work, by enabling others to verify
    it, and can lead to the generation of new knowledge as others use it to answer
    different questions ([Christensen, Freese, and Miguel 2019](99-references.html#ref-christensen2019transparent)).
    Christensen et al. ([2019](99-references.html#ref-christensen2019study)) suggest
    that research that shares its data may be more highly cited, although Tierney
    and Ram ([2021](99-references.html#ref-Tierney2021)) caution that widespread data
    sharing may require a cultural change.
  prefs: []
  type: TYPE_NORMAL
- en: We should try to invite scrutiny and make it as easy as possible to receive
    criticism. We should try to do this even when it is the difficult choice and results
    in discomfort because that is the only way to contribute to the stock of lasting
    knowledge. For instance, Piller ([2022](99-references.html#ref-pillerblots)) details
    potential fabrication in research about Alzheimer’s disease. In that case, one
    of the issues that researchers face when trying to understand whether the results
    are legitimate is a lack of access to unpublished images.
  prefs: []
  type: TYPE_NORMAL
- en: Data provenance is especially important. This refers to documenting “where a
    piece of data came from and the process by which it arrived in the database” ([Buneman,
    Khanna, and Wang-Chiew 2001, 316](99-references.html#ref-Buneman2001)). Documenting
    and saving the original, unedited dataset, using scripts to manipulate it to create
    the dataset that is analyzed, and sharing all of this—as recommended in this book—goes
    some way to achieving this. In some fields it is common for just a handful of
    databases to be used by many different teams, for instance, in genetics, the UK
    BioBank, and in the life sciences a cloud-based platform called ORCESTRA ([Mammoliti
    et al. 2021](99-references.html#ref-Mammoliti2021)) has been established to help.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Share
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 10.3.1 GitHub
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The easiest place for us to get started with storing a dataset is GitHub because
    that is already built into our workflow. For instance, if we push a dataset to
    a public repository, then our dataset becomes available. One benefit of this is
    that if we have set up our workspace appropriately, then we likely store our original,
    unedited data and the tidy data, as well as the scripts that are needed to transform
    one to the other. We are most of the way to the “bronze” standard of Heil et al.
    ([2021](99-references.html#ref-heil2021reproducibility)) without changing anything.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of how we have stored some data, we can access “raw_data.csv”
    from the [“starter_folder”](https://github.com/RohanAlexander/starter_folder).
    We navigate to the file in GitHub (“inputs” \(\rightarrow\) “data” \(\rightarrow\)
    “raw_data.csv”), and then click “Raw” ([Figure 10.1](#fig-githubraw)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/233218987d1d470d0594eee7bad9da3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Getting the necessary link to be able to read a CSV from a GitHub
    repository'
  prefs: []
  type: TYPE_NORMAL
- en: We can then add that URL as an argument to `read_csv()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE2]*  *While we can store and retrieve a dataset easily in this way, it
    lacks explanation, a formal dictionary, and aspects such as a license that would
    bring it closer to aligning with the FAIR principles. Another practical concern
    is that the maximum file size on GitHub is 100MB, although Git Large File Storage
    (LFS) can be used if needed. And a final concern, for some, is that GitHub is
    owned by Microsoft, a for-profit US technology firm.*  *### 10.3.2 R packages
    for data'
  prefs: []
  type: TYPE_NORMAL
- en: To this point we have largely used R packages for their code, although we have
    seen a few that were focused on sharing data, for instance, `troopdata` and `babynames`
    in [Chapter 5](05-graphs_tables_maps.html). We can build a R package for our dataset
    and then add it to GitHub and potentially eventually CRAN. This will make it easy
    to store and retrieve because we can obtain the dataset by loading the package.
    In contrast to the CSV-based approach, it also means a dataset brings its documentation
    along with it.
  prefs: []
  type: TYPE_NORMAL
- en: This will be the first R package that we build, and so we will jump over a number
    of steps. The key is to just try to get something working. In [Appendix G](26-deploy.html),
    we return to R packages and use them to deploy models. This gives us another chance
    to further develop experience with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, create a new package: “File” \(\rightarrow\) “New project”
    \(\rightarrow\) “New Directory” \(\rightarrow\) “R Package”. Give the package
    a name, such as “favcolordata” and select “Open in new session”. Create a new
    folder called “data”. We will simulate a dataset of people and their favorite
    colors to include in our R package.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '*To this point we have largely been using CSV files for our datasets. To include
    our data in this R package, we save our dataset in a different format, “.rda”,
    using `save()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*Then we create a R file “data.R” in the “R” folder. This file will only contain
    documentation using `roxygen2` comments. These start with `#''`, and we follow
    the documentation for `troopdata` closely.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Finally, add a README that provides a summary of all of this for someone coming
    to the project for the first time. Examples of packages with excellent READMEs
    include [`ggplot2`](https://github.com/tidyverse/ggplot2#readme), [`pointblank`](https://github.com/rich-iannone/pointblank#readme),
    [`modelsummary`](https://github.com/vincentarelbundock/modelsummary#readme), and
    [`janitor`](https://github.com/sfirke/janitor#readme).'
  prefs: []
  type: TYPE_NORMAL
- en: We can now go to the “Build” tab and click “Install and Restart”. After this,
    the package “favcolordata”, will be loaded and the data can be accessed locally
    using “color_data”. If we were to push this package to GitHub, then anyone would
    be able to install the package using `devtools` and use our dataset. Indeed, the
    following should work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '*This has addressed many of the issues that we faced earlier. For instance,
    we have included a README and a data dictionary, of sorts, in terms of the descriptions
    that we added. But if we were to try to put this package onto CRAN, then we might
    face some issues. For instance, the maximum size of a package is 5MB and we would
    quickly come up against that. We have also largely forced users to use R. While
    there are benefits of that, we may like to be more language agnostic ([Tierney
    and Ram 2020](99-references.html#ref-tierney2020realistic)), especially if we
    are concerned about the FAIR principles.'
  prefs: []
  type: TYPE_NORMAL
- en: Wickham ([2022, chap. 8](99-references.html#ref-rpackages)) provides more information
    about including data in R packages.****  ***### 10.3.3 Depositing data
  prefs: []
  type: TYPE_NORMAL
- en: While it is possible that a dataset will be cited if it is available through
    GitHub or a R package, this becomes more likely if the dataset is deposited somewhere.
    There are several reasons for this, but one is that it seems a bit more formal.
    Another is that it is associated with a DOI. [Zenodo](https://zenodo.org) and
    the [Open Science Framework](https://osf.io) (OSF) are two depositories that are
    commonly used. For instance, Carleton ([2021](99-references.html#ref-chris_carleton_2021_4550688))
    uses Zenodo to share the dataset and analysis supporting Carleton, Campbell, and
    Collard ([2021](99-references.html#ref-carleton2021reassessment)), Geuenich et
    al. ([2021b](99-references.html#ref-geuenich_michael_2021_5156049)) use Zenodo
    to share the dataset that underpins Geuenich et al. ([2021a](99-references.html#ref-geuenich2021automated)),
    and Katz and Alexander ([2023a](99-references.html#ref-katzhansard)) use Zenodo
    to share the dataset that underpins Katz and Alexander ([2023b](99-references.html#ref-katz2023digitization)).
    Similarly, Arel-Bundock et al. ([2022](99-references.html#ref-ryansnewpaper))
    use OSF to share code and data.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to use a dataverse, such as the [Harvard Dataverse](https://dataverse.harvard.edu)
    or the [Australian Data Archive](https://ada.edu.au). This is a common requirement
    for journal publications. One nice aspect of this is that we can use `dataverse`
    to retrieve the dataset as part of a reproducible workflow. We have an example
    of this in [Chapter 13](13-ijaglm.html).
  prefs: []
  type: TYPE_NORMAL
- en: In general, these options are free and provide a DOI that can be useful for
    citation purposes. The use of data deposits such as these is a way to offload
    responsibility for the continued hosting of the dataset (which in this case is
    a good thing) and prevent the dataset from being lost. It also establishes a single
    point of truth, which should act to reduce errors ([Byrd et al. 2020](99-references.html#ref-byrd2020responsible)).
    Finally, it makes access to the dataset independent of the original researchers,
    and results in persistent metadata. That all being said, the viability of these
    options rests on their underlying institutions. For instance, Zenodo is operated
    by CERN and many dataverses are operated by universities. These institutions are
    subject to, as we all are, social and political forces.****  ***## 10.4 Data documentation
  prefs: []
  type: TYPE_NORMAL
- en: Dataset documentation has long consisted of a data dictionary. This may be as
    straight-forward a list of the variables, a few sentences of description, and
    ideally a source. [The data dictionary of the ACS](https://www2.census.gov/programs-surveys/acs/tech_docs/pums/data_dict/PUMS_Data_Dictionary_2016-2020.pdf),
    which was introduced in [Chapter 6](06-farm.html), is particularly comprehensive.
    And OSF provides [instructions](https://help.osf.io/article/217-how-to-make-a-data-dictionary)
    for how to make a data dictionary. Given the workflow advocated in this book,
    it might be worthwhile to actually begin putting together a data dictionary as
    part of the simulation step i.e. before even collecting the data. While it would
    need to be updated, it would be another opportunity to think deeply about the
    data situation.
  prefs: []
  type: TYPE_NORMAL
- en: Datasheets ([Gebru et al. 2021](99-references.html#ref-gebru2021datasheets))
    are an increasingly common addition to documentation. If we think of a data dictionary
    as a list of ingredients for a dataset, then we could think of a datasheet as
    basically a nutrition label for datasets. The process of creating them enables
    us to think more carefully about what we will feed our model. More importantly,
    they enable others to better understand what we fed our model. One important task
    is going back and putting together datasheets for datasets that are widely used.
    For instance, researchers went back and wrote a datasheet for “BookCorpus”, which
    is one of the most popular datasets in computer science, and they found that around
    30 per cent of the data were duplicated ([Bandy and Vincent 2021](99-references.html#ref-bandy2021addressing)).
  prefs: []
  type: TYPE_NORMAL
- en: '*Shoulders of giants* *Timnit Gebru is the founder of the Distributed Artificial
    Intelligence Research Institute (DAIR). After earning a PhD in Computer Science
    from Stanford University, Gebru joined Microsoft and then Google. In addition
    to Bandy and Vincent ([2021](99-references.html#ref-bandy2021addressing)), which
    introduced datasheets, one notable paper is Bender et al. ([2021](99-references.html#ref-Bender2021)),
    which discussed the dangers of language models being too large. She has made many
    other substantial contributions to fairness and accountability, especially Buolamwini
    and Gebru ([2018](99-references.html#ref-buolamwini2018gender)), which demonstrated
    racial bias in facial analysis algorithms.*  *Instead of telling us how unhealthy
    various foods are, a datasheet tells us things like:'
  prefs: []
  type: TYPE_NORMAL
- en: Who put the dataset together?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who paid for the dataset to be created?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How complete is the dataset? (Which is, of course, unanswerable, but detailing
    the ways in which it is known to be incomplete is valuable.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which variables are present, and, equally, not present, for particular observations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, a lot of work is done to create a datasheet. In that case, we may
    like to publish and share it on its own, for instance, Biderman, Bicheno, and
    Gao ([2022](99-references.html#ref-biderman2022datasheet)) and Bandy and Vincent
    ([2021](99-references.html#ref-bandy2021addressing)). But typically a datasheet
    might live in an appendix to the paper, for instance Zhang et al. ([2022](99-references.html#ref-zhang2022opt)),
    or be included in a file adjacent to the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: When creating a datasheet for a dataset, especially a dataset that we did not
    put together ourselves, it is possible that the answer to some questions will
    simply be “Unknown”, but we should do what we can to minimize that. The datasheet
    template created by Gebru et al. ([2021](99-references.html#ref-gebru2021datasheets))
    is not the final word. It is possible to improve on it, and add additional detail
    sometimes. For instance, Miceli, Posada, and Yang ([2022](99-references.html#ref-Miceli2022))
    argue for the addition of questions to do with power relations.*  *## 10.5 Personally
    identifying information
  prefs: []
  type: TYPE_NORMAL
- en: By way of background, Christensen, Freese, and Miguel ([2019, 180](99-references.html#ref-christensen2019transparent))
    define a variable as “confidential” if the researchers know who is associated
    with each observation, but the public version of the dataset removes this association.
    A variable is “anonymous” if even the researchers do not know.
  prefs: []
  type: TYPE_NORMAL
- en: 'Personally identifying information (PII) is that which enables us to link an
    observation in our dataset with an actual person. This is a significant concern
    in fields focused on data about people. Email addresses are often PII, as are
    names and addresses. While some variables may not be PII for many respondents,
    it could be PII for some. For instance, consider a survey that is representative
    of the population age distribution. There is not likely to be many respondents
    aged over 100, and so the variable age may then become PII. The same scenario
    applies to income, wealth, and many other variables. One response to this is for
    data to be censored, which was discussed in [Chapter 6](06-farm.html). For instance,
    we may record age between zero and 90, and then group everyone over that into
    “90+”. Another is to construct age-groups: “18-29”, “30-44”, \(\dots\). Notice
    that with both these solutions we have had to trade-off privacy and usefulness.
    More concerningly, a variable may be PII, not by itself, but when combined with
    another variable.'
  prefs: []
  type: TYPE_NORMAL
- en: Our primary concern should be with ensuring that the privacy of our dataset
    is appropriate, given the expectations of the reasonable person. This requires
    weighing costs and benefits. In national security settings there has been considerable
    concern about the over-classification of documents ([Lin 2014](99-references.html#ref-overclassification)).
    The reduced circulation of information because of this may result in unrealized
    benefits. To avoid this in data science, the test of the need to protect a dataset
    needs to be made by the reasonable person weighing up costs and benefits. It is
    easy, but incorrect, to argue that data should not be released unless it is perfectly
    anonymized. The fundamental problem of data privacy implies that such data would
    have limited utility. That approach, possibly motivated by the precautionary principle,
    would be too conservative and could cause considerable loss in terms of unrealized
    benefits.
  prefs: []
  type: TYPE_NORMAL
- en: Randomized response ([Greenberg et al. 1969](99-references.html#ref-randomizedresponse))
    is a clever way to enable anonymity without much overhead. Each respondent flips
    a coin before they answer a question but does not show the researcher the outcome
    of the coin flip. The respondent is instructed to respond truthfully to the question
    if the coin lands on heads, but to always give some particular (but still plausible)
    response if tails. The results of the other options can then be re-weighted to
    enable an estimate, without a researcher ever knowing the truth about any particular
    respondent. This is especially used in association with snowball sampling, discussed
    in [Chapter 6](06-farm.html). One issue with randomized response is that the resulting
    dataset can be only used to answer specific questions. This requires careful planning,
    and the dataset will be of less general value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zook et al. ([2017](99-references.html#ref-zook2017ten)) recommend considering
    whether data even need to be gathered in the first place. For instance, if a phone
    number is not absolutely required then it might be better to not ask for it, rather
    than need to worry about protecting it before data dissemination. GDPR and HIPAA
    are two legal structures that govern data in Europe, and the United States, respectively.
    Due to the influence of these regions, they have a significant effect outside
    those regions also. GDPR concerns data generally, while HIPAA is focused on healthcare.
    GDPR applies to all personal data, which is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: \(\dots\)any information relating to an identified or identifiable natural person
    (“data subject”); an identifiable natural person is one who can be identified,
    directly or indirectly, in particular by reference to an identifier such as a
    name, an identification number, location data, an online identifier or to one
    or more factors specific to the physical, physiological, genetic, mental, economic,
    cultural or social identity of that natural person;
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Council of European Union ([2016](99-references.html#ref-gdpr)), Article 4,
    “Definitions”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: HIPAA refers to the privacy of medical records in the US and codifies the idea
    that the patient should have access to their medical records, and that only the
    patient should be able to authorize access to their medical records ([Annas 2003](99-references.html#ref-annas2003hipaa)).
    HIPAA only applies to certain entities. This means it sets a standard, but coverage
    is inconsistent. For instance, a person’s social media posts about their health
    would generally not be subject to it, nor would knowledge of a person’s location
    and how active they are, even though based on that information we may be able
    to get some idea of their health ([Cohen and Mello 2018](99-references.html#ref-Cohen2018)).
    Such data are hugely valuable ([Ross 2022](99-references.html#ref-ibmdataset)).
  prefs: []
  type: TYPE_NORMAL
- en: There are a variety of ways of protecting PII, while still sharing some data,
    that we will now go through. We focus here initially on what we can do when the
    dataset is considered by itself, which is the main concern. But sometimes the
    combination of several variables, none of which are PII in and of themselves,
    can be PII. For instance, age is unlikely PII by itself, but age combined with
    city, education, and a few other variables could be. One concern is that re-identification
    could occur by combining datasets and this is a potential role for differential
    privacy.
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.1 Hashing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A cryptographic hash is a one-way transformation, such that the same input always
    provides the same output, but given the output, it is not reasonably possible
    to obtain the input. For instance, a function that doubled its input always gives
    the same output, for the same input, but is also easy to reverse, so would not
    work well as a hash. In contrast, the modulo, which for a non-negative number
    is the remainder after division and can be implemented in R using `%%`, would
    be difficult to reverse.
  prefs: []
  type: TYPE_NORMAL
- en: Knuth ([1998, 514](99-references.html#ref-knuth)) relates an interesting etymology
    for “hash”. He first defines “to hash” as relating to chop up or make a mess,
    and then explaining that hashing relates to scrambling the input and using this
    partial information to define the output. A collision is when different inputs
    map to the same output, and one feature of a good hashing algorithm is that collisions
    are reduced. As mentioned, one simple approach is to rely on the modulo operator.
    For instance, if we were interested in ten different groupings for the integers
    1 through to 10, then modulo would enable this. A better approach would be for
    the number of groupings to be a larger number, because this would reduce the number
    of values with the same hash outcome.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, consider some information that we would like to keep private,
    such as names and ages of respondents.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE8]*  *One option for the names would be to use a function that just took
    the first letter of each name. And one option for the ages would be to convert
    them to Roman numerals.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE10]*  *While the approach for the first variable, names, is good because
    the names cannot be backed out, the issue is that as the dataset grows there are
    likely to be lots of “collisions”—situations where different inputs, say “Rohan”
    and “Robert”, both get the same output, in this case “R”. It is the opposite situation
    for the approach for the second variable, ages. In this case, there will never
    be any collisions—“36” will be the only input that ever maps to “XXXVI”. However,
    it is easy to back out the actual data, for anyone who knows roman numerals.'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than write our own hash functions, we can use cryptographic hash functions
    such as `md5()` from `openssl`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE12]*  *We could share either of these transformed variables and be comfortable
    that it would be difficult for someone to use only that information to recover
    the names of our respondents. That is not to say that it is impossible. Knowledge
    of the key, which is the term given to the string used to encrypt the data, would
    allow someone to reverse this. If we made a mistake, such as accidentally pushing
    the original dataset to GitHub then they could be recovered. And it is likely
    that governments and some private companies can reverse the cryptographic hashes
    used here.'
  prefs: []
  type: TYPE_NORMAL
- en: One issue that remains is that anyone can take advantage of the key feature
    of hashes to back out the input. In particular, the same input always gets the
    same output. So they could test various options for inputs. For instance, they
    could themselves try to hash “Rohan”, and then noticing that the hash is the same
    as the one that we published in our dataset, know that data relates to that individual.
    We could try to keep our hashing approach secret, but that is difficult as there
    are only a few that are widely used. One approach is to add a salt that we keep
    secret. This slightly changes the input. For instance, we could add the salt “_is_a_person”
    to all our names and then hash that, although a large random number might be a
    better option. Provided the salt is not shared, then it would be difficult for
    most people to reverse our approach in that way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE14]****  ***### 10.5.2 Simulation'
  prefs: []
  type: TYPE_NORMAL
- en: One common approach to deal with the issue of being unable to share the actual
    data that underpins an analysis, is to use data simulation. We have used data
    simulation throughout this book toward the start of the workflow to help us to
    think more deeply about our dataset. We can use data simulation again at the end,
    to ensure that others cannot access the actual dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The approach is to understand the critical features of the dataset and the appropriate
    distribution. For instance, if our data were the ages of some population, then
    we may want to use the Poisson distribution and experiment with different parameters
    for the rate. Having simulated a dataset, we conduct our analysis using this simulated
    dataset and ensure that the results are broadly similar to when we use the real
    data. We can then release the simulated dataset along with our code.
  prefs: []
  type: TYPE_NORMAL
- en: For more nuanced situations, Koenecke and Varian ([2020](99-references.html#ref-koenecke2020synthetic))
    recommend using the synthetic data vault ([Patki, Wedge, and Veeramachaneni 2016](99-references.html#ref-patki2016synthetic))
    and then the use of Generative Adversarial Networks, such as implemented by Athey
    et al. ([2021](99-references.html#ref-athey2021using)).
  prefs: []
  type: TYPE_NORMAL
- en: 10.5.3 Differential privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Differential privacy is a mathematical definition of privacy ([Dwork and Roth
    2013, 6](99-references.html#ref-Dwork2013)). It is not just one algorithm, it
    is a definition that many algorithms satisfy. Further, there are many definitions
    of privacy, of which differential privacy is just one. The main issue it solves
    is that there are many datasets available. This means there is always the possibility
    that some combination of them could be used to identify respondents even if PII
    were removed from each of these individual datasets. For instance, experience
    with the Netflix prize found that augmenting the available dataset with data from
    IMBD resulted in better predictions, which points to why this would so commonly
    happen. Rather than needing to anticipate how various datasets could be combined
    to re-identify individuals and adjust variables to remove this possibility, a
    dataset that is created using a differentially private approach provides assurances
    that privacy will be maintained.
  prefs: []
  type: TYPE_NORMAL
- en: '*Shoulders of giants* *Cynthia Dwork is the Gordon McKay Professor of Computer
    Science at Harvard University. After earning a PhD in Computer Science from Cornell
    University, she was a Post-Doctoral Research Fellow at MIT and then worked at
    IBM, Compaq, and Microsoft Research where she is a Distinguished Scientist. She
    joined Harvard in 2017\. One of her major contributions is differential privacy
    ([Dwork et al. 2006](99-references.html#ref-dwork2006calibrating)), which has
    become widely used.*  *To motivate the definition, consider a dataset of responses
    and PII that only has one person in it. The release of that dataset, as is, would
    perfectly identify them. At the other end of the scale, consider a dataset that
    does not contain a particular person. The release of that dataset could, in general,
    never be linked to them because they are not in it.[¹](#fn1) Differential privacy,
    then, is about the inclusion or exclusion of particular individuals in a dataset.
    An algorithm is differentially private if the inclusion or exclusion of any particular
    person in a dataset has at most some given factor of an effect on the probability
    of some output ([Oberski and Kreuter 2020](99-references.html#ref-Oberski2020Differential)).
    The fundamental problem of data privacy is that we cannot have completely anonymized
    data that remains useful ([Dwork and Roth 2013, 6](99-references.html#ref-Dwork2013)).
    Instead, we must trade-off utility and privacy.'
  prefs: []
  type: TYPE_NORMAL
- en: A dataset is differentially private to different levels of privacy, based on
    how much it changes when one person’s results are included or excluded. This is
    the key parameter, because at the same time as deciding how much of an individual’s
    information we are prepared to give up, we are deciding how much random noise
    to add, which will impact our output. The choice of this level is a nuanced one
    and should involve consideration of the costs of undesired disclosures, compared
    with the benefits of additional research. For public data that will be released
    under differential privacy, the reasons for the decision should be public because
    of the costs that are being imposed. Indeed, Tang et al. ([2017](99-references.html#ref-differentialprivacyatapple))
    argue that even in the case of private companies that use differential privacy,
    such as Apple, users should have a choice about the level of privacy loss.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a situation in which a professor wants to release the average mark
    for a particular assignment. The professor wants to ensure that despite that information,
    no student can work out the grade that another student got. For instance, consider
    a small class with the following marks.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE16]*  *The professor could announce the exact mean, for instance, “The
    mean for the first problem set was 50.5”. Theoretically, all-but-one student could
    let the others know their mark. It would then be possible for that group to determine
    the mark of the student who did not agree to make their mark public.'
  prefs: []
  type: TYPE_NORMAL
- en: A non-statistical approach would be for the professor to add the word “roughly”.
    For instance, the professor could say “The mean for the first problem set was
    roughly 50.5”. The students could attempt the same strategy, but they would never
    know with certainty. The professor could implement a more statistical approach
    to this by adding noise to the mean.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE18]*  *The professor could then announce this modified mean. This would
    make the students’ plan more difficult. One thing to notice about that approach
    is that it would not work with persistent questioning. For instance, eventually
    the students would be able to back out the distribution of the noise that the
    professor added. One implication is that the professor would need to limit the
    number of queries they answered about the mean of the problem set.'
  prefs: []
  type: TYPE_NORMAL
- en: A differentially private approach is a sophisticated version of this. We can
    implement it using `diffpriv`. This results in a mean that we could announce ([Table 10.1](#tbl-diffprivaexample)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '*Table 10.1: Comparing the actual mean with a differentially private mean'
  prefs: []
  type: TYPE_NORMAL
- en: '| Actual mean | Announceable mean |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 50.5 | 52.5 |'
  prefs: []
  type: TYPE_TB
- en: The implementation of differential privacy is a costs and benefits issue ([Hotz
    et al. 2022](99-references.html#ref-hotz2022balancing); [Kenny et al. 2023](99-references.html#ref-kennetal22)).
    Stronger privacy protection fundamentally must mean less information ([Bowen 2022,
    39](99-references.html#ref-clairemckaybowen)), and this differently affects various
    aspects of society. For instance, Suriyakumar et al. ([2021](99-references.html#ref-Suriyakumar2021))
    found that, in the context of health care, differentially private learning can
    result in models that are disproportionately affected by large demographic groups.
    A variant of differential privacy has recently been implemented by the US census.
    It may have a significant effect on redistricting ([Kenny et al. 2021](99-references.html#ref-kenny2021impact))
    and result in some publicly available data that are unusable in the social sciences
    ([Ruggles et al. 2019](99-references.html#ref-ruggles2019differential)).*******  ****##
    10.6 Data efficiency
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, done is better than perfect, and unnecessary optimization
    is a waste of resources. However, at a certain point, we need to adapt new ways
    of dealing with data, especially as our datasets start to get larger. Here we
    discuss iterating through multiple files, and then turn to the use of Apache Arrow
    and parquet. Another natural step would be the use of SQL, which is covered in
    [Online Appendix C](22-sql_essentials.html).
  prefs: []
  type: TYPE_NORMAL
- en: 10.6.1 Iteration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several ways to become more efficient with our data, especially as
    it becomes larger. The first, and most obvious, is to break larger datasets into
    smaller pieces. For instance, if we have a dataset for a year, then we could break
    it into months, or even days. To enable this, we need a way of quickly reading
    in many different files.
  prefs: []
  type: TYPE_NORMAL
- en: The need to read in multiple files and combine them into the one tibble is a
    surprisingly common task. For instance, it may be that the data for a year, are
    saved into individual CSV files for each month. We can use `purrr` and `fs` to
    do this. To illustrate this situation we will simulate data from the exponential
    distribution using `rexp()`. Such data may reflect, say, comments on a social
    media platform, where the vast majority of comments are made by a tiny minority
    of users. We will use `dir_create()` from `fs` to create a folder, simulate monthly
    data, and save it. We will then illustrate reading it in.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '*Having created our dataset with each month saved to a different CSV, we can
    now read it in. There are a variety of ways to do this. The first step is that
    we need to get a list of all the CSV files in the directory. We use the “glob”
    argument here to specify that we are interested only in the “.csv” files, and
    that could change to whatever files it is that we are interested in.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE22]'
  prefs: []
  type: TYPE_NORMAL
- en: We can pass this list to `read_csv()` and it will read them in and combine them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE24]'
  prefs: []
  type: TYPE_NORMAL
- en: It prints out the first ten days of April, because alphabetically April is the
    first month of the year and so that was the first CSV that was read.
  prefs: []
  type: TYPE_NORMAL
- en: This works well when we have CSV files, but we might not always have CSV files
    and so will need another way, and can use `map_dfr()` to do this. One nice aspect
    of this approach is that we can include the name of the file alongside the observation
    using “.id”. Here we specify that we would like that column to be called “file”,
    but it could be anything.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE26]****  ***### 10.6.2 Apache Arrow'
  prefs: []
  type: TYPE_NORMAL
- en: CSVs are commonly used without much thought in data science. And while CSVs
    are good because they have little overhead and can be manually inspected, this
    also means they are quite minimal. This can lead to issues, for instance class
    is not preserved, and file sizes can become large leading to storage and performance
    issues. There are various alternatives, including Apache Arrow, which stores data
    in columns rather than rows like CSV. We focus on the “.parquet” format from Apache
    Arrow. Like a CSV, parquet is an open standard. The R package, `arrow`, enables
    us to use this format. The use of parquet has the advantage of requiring little
    change from us while delivering significant benefits.
  prefs: []
  type: TYPE_NORMAL
- en: '*Shoulders of giants* *Wes McKinney holds an undergraduate degree in theoretical
    mathematics from MIT. Starting in 2008, while working at AQR Capital Management,
    he developed the Python package, pandas, which has become a cornerstone of data
    science. He later wrote *Python for Data Analysis* ([McKinney [2011] 2022](99-references.html#ref-pythonfordataanalysis)).
    In 2016, with Hadley Wickham, he designed Feather, which was released in 2016\.
    He now works as CTO of Voltron Data, which focuses on the Apache Arrow project.*  *In
    particular, we focus on the benefit of using parquet for data storage, such as
    when we want to save a copy of an analysis dataset that we cleaned and prepared.
    Among other aspects, parquet brings two specific benefits, compared with CSV:'
  prefs: []
  type: TYPE_NORMAL
- en: the file sizes are typically smaller; and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: class is preserved because parquet attaches a schema, which makes dealing with,
    say, dates and factors considerably easier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having loaded `arrow`, we can use parquet files in a similar way to CSV files.
    Anywhere in our code that we used `write_csv()` and `read_csv()` we could alternatively,
    or additionally, use `write_parquet()` and `read_parquet()`, respectively. The
    decision to use parquet needs to consider both costs and benefits, and it is an
    active area of development.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE28]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE30]**  **We can write a parquet file with `write_parquet()` and we can
    read a parquet with `read_parquet()`. We get significant reductions in file size
    when we compare the size of the same datasets saved in each format, especially
    as they get larger ([Table 10.2](#tbl-filesize)). The speed benefits of using
    parquet are most notable for larger datasets. It turns them from being impractical
    to being usable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10.2: Comparing the file sizes, and read and write times, of CSV and
    parquet as the file size increases'
  prefs: []
  type: TYPE_NORMAL
- en: '| Number | CSV size | CSV write time (sec) | CSV read time (sec) | Parquet
    size | Parquet write time (sec) | Parquet read time (sec) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1e+02 | 3,102.72 | 0.01 | 0.26 | 2,713.6 | 0.01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1e+03 | 30,720 | 0.02 | 0.27 | 11,366.4 | 0.01 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 1e+04 | 307,415.04 | 0.02 | 0.3 | 101,969.92 | 0.01 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 1e+05 | 3,072,327.68 | 0.03 | 0.28 | 1,040,885.76 | 0.04 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 1e+06 | 30,712,791.04 | 0.15 | 0.58 | 8,566,865.92 | 0.22 | 0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| 1e+07 | 307,117,424.64 | 1 | 2.95 | 82,952,847.36 | 1.76 | 0.42 |'
  prefs: []
  type: TYPE_TB
- en: '| 1e+08 | 3,070,901,616.64 | 7.65 | 32.89 | 827,137,720.32 | 16.12 | 4.85 |'
  prefs: []
  type: TYPE_TB
- en: Crane, Hazlitt, and Arrow ([2023](99-references.html#ref-arrowcookbook)) provides
    further information about specific tasks, Navarro ([2022](99-references.html#ref-navarro2021getting))
    provides helpful examples of implementation, and Navarro, Keane, and Hazlitt ([2022](99-references.html#ref-navarroworkshop))
    provides an extensive set of materials. There is no settled consensus on whether
    parquet files should be used exclusively for dataset. But it is indisputable that
    the persistence of class alone provides a compelling reason for including them
    in addition to a CSV.
  prefs: []
  type: TYPE_NORMAL
- en: We will use parquet more in the remainder of this book.******  ****## 10.7 Exercises
  prefs: []
  type: TYPE_NORMAL
- en: Practice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(Plan)* Consider the following scenario: *You work for a large news media
    company and focus on subscriber management. Over the course of a year most subscribers
    will never post a comment beneath a news article, but a few post an awful lot.*
    Please sketch what that dataset could look like and then sketch a graph that you
    could build to show all observations.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Simulate)* Please further consider the scenario described and simulate the
    situation. Carefully pick an appropriate distribution. Please include five tests
    based on the simulated data. Submit a link to a GitHub Gist that contains your
    code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Acquire)* Please describe one possible source of such a dataset.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Explore)* Please use `ggplot2` to build the graph that you sketched. Submit
    a link to a GitHub Gist that contains your code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*(Communicate)* Please write two paragraphs about what you did.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Following Wilkinson et al. ([2016](99-references.html#ref-wilkinson2016fair)),
    please discuss the FAIR principles in the context of a dataset that you are familiar
    with (begin with a one-paragraph summary of the dataset, then write one paragraph
    per principle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please create a R package for a simulated dataset, push it to GitHub, and submit
    code to install the package (e.g. `devtools::install_github("RohanAlexander/favcolordata")`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'According to Gebru et al. ([2021](99-references.html#ref-gebru2021datasheets)),
    a datasheet should document a dataset’s (please select all that apply):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: composition.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: recommended uses.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: motivation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: collection process.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Discuss, with the help of examples and references, whether a person’s name is
    PII (please write at least three paragraphs)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using `md5()` what is the hash of “Monica” (pick one)?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 243f63354f4c1cc25d50f6269b844369
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 02df8936eee3d4d2568857ed530671b2
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 09084cc0cda34fd80bfa3cc0ae8fe3dc
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 1b3840b0b70d91c17e70014c8537dbba
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Please save the `penguins` data from from `palmerpenguins` as a CSV file and
    as a Parquet file. How big are they?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 12.5K; 6.04K
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 14.9K; 6.04K
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 14.9K; 5.02K
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 12.5K; 5.02K
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Class activities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Use the [starter folder](https://github.com/RohanAlexander/starter_folder) and
    create a new repo. Add a link to the GitHub repo in the class’s shared Google
    Doc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the following code to a simulation R script, then lint it. What do you think
    about the recommendations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '**   Simulate a dataset with ten million observations and at least five variables,
    one of which must be a date. Save it in both CSV and parquet formats. What is
    the file size difference?'
  prefs: []
  type: TYPE_NORMAL
- en: Discuss datasheets in the context of the dataset that you simulated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pretend you were joining two datasets with `left_join()`. When joining datasets
    it is easy to accidentally duplicate or remove rows. Please add some tests that
    might put your mind at ease.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '**   Modify the following code to show why using “T” instead of “TRUE” should
    generally not be done (hint: assign “T” to “FALSE”)?'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '*[PRE34]*  **   Working with the instructor, pick a chapter from Lewis ([2024](99-references.html#ref-lewiscrystal))
    and create a five-slide summary of the key take-aways from the chapter. Present
    to the class.'
  prefs: []
  type: TYPE_NORMAL
- en: Working with the instructor, make a pull request that fixes some small aspect
    of a work-in-progress book.[²](#fn2) Options include:[³](#fn3) Lewis ([2024](99-references.html#ref-lewiscrystal))
    or Wickham, Çetinkaya-Rundel, and Grolemund ([[2016] 2023](99-references.html#ref-r4ds)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pretend that you are in a small class, and have some results from an assessment
    ([Table 10.3](#tbl-classmarks)). Use the code, but change the seed to generate
    your own dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hash, but do not salt, the names, and then exchange with another group. Can
    they work out what the names are?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuing with the results that you generated, please write code that simulates
    the dataset. You will need to decide which features are important and which are
    not. Note two interesting aspects of this and then share with the class.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Continuing with the results that you generated, please: 1) work out the class
    mean, 2) remove the mark of one student, 3) provide the mean and the off-by-one
    dataset to another group. Can they work out the mark of the student who opted
    not to share?'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, please do the same exercise, but create a differentially private mean.
    What are they able to figure out now?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '*Table 10.3: Simulated students and their mark (out of 100) in a particular
    class paper'
  prefs: []
  type: TYPE_NORMAL
- en: '| Student | Mark |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Bertha | 37 |'
  prefs: []
  type: TYPE_TB
- en: '| Tyler | 32 |'
  prefs: []
  type: TYPE_TB
- en: '| Kevin | 48 |'
  prefs: []
  type: TYPE_TB
- en: '| Ryan | 39 |'
  prefs: []
  type: TYPE_TB
- en: '| Robert | 34 |'
  prefs: []
  type: TYPE_TB
- en: '| Jennifer | 52 |'
  prefs: []
  type: TYPE_TB
- en: '| Donna | 48 |'
  prefs: []
  type: TYPE_TB
- en: '| Karen | 43 |'
  prefs: []
  type: TYPE_TB
- en: '| Emma | 61 |'
  prefs: []
  type: TYPE_TB
- en: '| Arthur | 55 |****  ***### Task'
  prefs: []
  type: TYPE_NORMAL
- en: Please identify a dataset you consider interesting and important, that does
    not have a datasheet ([Gebru et al. 2021](99-references.html#ref-gebru2021datasheets)).
    As a reminder, datasheets accompany datasets and document “motivation, composition,
    collection process, recommended uses,” among other aspects. Please put together
    a datasheet for this dataset. You are welcome to use the template in the [starter
    folder](https://github.com/RohanAlexander/starter_folder).
  prefs: []
  type: TYPE_NORMAL
- en: Use Quarto, and include an appropriate title, author, date, link to a GitHub
    repo, and citations to produce a draft. Following this, please pair with another
    student and exchange your written work. Update it based on their feedback, and
    be sure to acknowledge them by name in your paper. Submit a PDF.
  prefs: []
  type: TYPE_NORMAL
- en: Paper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At about this point the *Dysart* Paper from [Online Appendix F](25-papers.html)
    would be appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Annas, George. 2003\. “HIPAA Regulations: A New Era of Medical-Record Privacy?”
    *New England Journal of Medicine* 348 (15): 1486–90\. [https://doi.org/10.1056/NEJMlim035027](https://doi.org/10.1056/NEJMlim035027).Arel-Bundock,
    Vincent. 2024\. *tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,”
    “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats*. [https://vincentarelbundock.github.io/tinytable/](https://vincentarelbundock.github.io/tinytable/).Arel-Bundock,
    Vincent, Ryan Briggs, Hristos Doucouliagos, Marco Mendoza Aviña, and T. D. Stanley.
    2022\. “Quantitative Political Science Research Is Greatly Underpowered.” [https://osf.io/bzj9y/](https://osf.io/bzj9y/).Athey,
    Susan, Guido Imbens, Jonas Metzger, and Evan Munro. 2021\. “Using Wasserstein
    Generative Adversarial Networks for the Design of Monte Carlo Simulations.” *Journal
    of Econometrics*. [https://doi.org/10.1016/j.jeconom.2020.09.013](https://doi.org/10.1016/j.jeconom.2020.09.013).Bandy,
    John, and Nicholas Vincent. 2021\. “Addressing ‘Documentation Debt’ in Machine
    Learning: A Retrospective Datasheet for BookCorpus.” In *Proceedings of the Neural
    Information Processing Systems Track on Datasets and Benchmarks*, edited by J.
    Vanschoren and S. Yeung. Vol. 1\. [https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/54229abfcfa5649e7003b83dd4755294-Paper-round1.pdf](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/54229abfcfa5649e7003b83dd4755294-Paper-round1.pdf).Bender,
    Emily, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021\.
    “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” In *Proceedings
    of the 2021 ACM Conference on Fairness, Accountability, and Transparency*. ACM.
    [https://doi.org/10.1145/3442188.3445922](https://doi.org/10.1145/3442188.3445922).Berners-Lee,
    Timothy. 1989\. “Information Management: A Proposal.” [https://www.w3.org/History/1989/proposal.html](https://www.w3.org/History/1989/proposal.html).Biderman,
    Stella, Kieran Bicheno, and Leo Gao. 2022\. “Datasheet for the Pile.” [https://arxiv.org/abs/2201.07311](https://arxiv.org/abs/2201.07311).Borghi,
    John, and Ana Van Gulick. 2022\. “Promoting Open Science Through Research Data
    Management.” *Harvard Data Science Review* 4 (3). [https://doi.org/10.1162/99608f92.9497f68e](https://doi.org/10.1162/99608f92.9497f68e).Bowen,
    Claire McKay. 2022\. *Protecting Your Privacy in a Data-Driven World*. 1st ed.
    Chapman; Hall/CRC. [https://doi.org/10.1201/9781003122043](https://doi.org/10.1201/9781003122043).Buneman,
    Peter, Sanjeev Khanna, and Tan Wang-Chiew. 2001\. “Why and Where: A Characterization
    of Data Provenance.” In *Database Theory ICDT 2001*, 316–30\. Springer Berlin
    Heidelberg. [https://doi.org/10.1007/3-540-44503-x_20](https://doi.org/10.1007/3-540-44503-x_20).Buolamwini,
    Joy, and Timnit Gebru. 2018\. “Gender Shades: Intersectional Accuracy Disparities
    in Commercial Gender Classification.” In *Conference on Fairness, Accountability
    and Transparency*, 77–91.Bush, Vannevar. 1945\. “As We May Think.” *The Atlantic
    Monthly*, July. [https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/](https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/).Byrd,
    James Brian, Anna Greene, Deepashree Venkatesh Prasad, Xiaoqian Jiang, and Casey
    Greene. 2020\. “Responsible, Practical Genomic Data Sharing That Accelerates Research.”
    *Nature Reviews Genetics* 21 (10): 615–29\. [https://doi.org/10.1038/s41576-020-0257-5](https://doi.org/10.1038/s41576-020-0257-5).Carleton,
    Chris. 2021\. “wccarleton/conflict-europe: Acce.” Zenodo. [https://doi.org/10.5281/zenodo.4550688](https://doi.org/10.5281/zenodo.4550688).Carleton,
    Chris, Dave Campbell, and Mark Collard. 2021\. “A Reassessment of the Impact of
    Temperature Change on European Conflict During the Second Millennium CE Using
    a Bespoke Bayesian Time-Series Model.” *Climatic Change* 165 (1): 1–16\. [https://doi.org/10.1007/s10584-021-03022-2](https://doi.org/10.1007/s10584-021-03022-2).Christensen,
    Garret, Allan Dafoe, Edward Miguel, Don Moore, and Andrew Rose. 2019\. “A Study
    of the Impact of Data Sharing on Article Citations Using Journal Policies as a
    Natural Experiment.” *PLOS ONE* 14 (12): e0225883\. [https://doi.org/10.1371/journal.pone.0225883](https://doi.org/10.1371/journal.pone.0225883).Christensen,
    Garret, Jeremy Freese, and Edward Miguel. 2019\. *Transparent and Reproducible
    Social Science Research*. California: University of California Press.Cohen, Glenn,
    and Michelle Mello. 2018\. “HIPAA and Protecting Health Information in the 21st
    Century.” *JAMA* 320 (3): 231\. [https://doi.org/10.1001/jama.2018.5630](https://doi.org/10.1001/jama.2018.5630).Council
    of European Union. 2016\. “General Data Protection Regulation 2016/679.” [https://eur-lex.europa.eu/eli/reg/2016/679/oj](https://eur-lex.europa.eu/eli/reg/2016/679/oj).Crane,
    Nicola, Stephanie Hazlitt, and Apache Arrow. 2023\. *Apache Arrow R Cookbook*.
    [https://arrow.apache.org/cookbook/r/](https://arrow.apache.org/cookbook/r/).Dwork,
    Cynthia, Frank McSherry, Kobbi Nissim, and Adam Smith. 2006\. “Calibrating Noise
    to Sensitivity in Private Data Analysis.” In *Theory of Cryptography Conference*,
    265–84\. Springer. [https://doi.org/10.1007/11681878_14](https://doi.org/10.1007/11681878_14).Dwork,
    Cynthia, and Aaron Roth. 2013\. “The Algorithmic Foundations of Differential Privacy.”
    *Foundations and Trends in Theoretical Computer Science* 9 (3-4): 211–407\. [https://doi.org/10.1561/0400000042](https://doi.org/10.1561/0400000042).Firke,
    Sam. 2023\. *janitor: Simple Tools for Examining and Cleaning Dirty Data*. [https://CRAN.R-project.org/package=janitor](https://CRAN.R-project.org/package=janitor).Gebru,
    Timnit, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,
    Hal Daumé III, and Kate Crawford. 2021\. “Datasheets for Datasets.” *Communications
    of the ACM* 64 (12): 86–92\. [https://doi.org/10.1145/3458723](https://doi.org/10.1145/3458723).Geuenich,
    Michael, Jinyu Hou, Sunyun Lee, Shanza Ayub, Hartland Jackson, and Kieran Campbell.
    2021a. “Automated Assignment of Cell Identity from Single-Cell Multiplexed Imaging
    and Proteomic Data.” *Cell Systems* 12 (12): 1173–86\. [https://doi.org/10.1016/j.cels.2021.08.012](https://doi.org/10.1016/j.cels.2021.08.012).———.
    2021b. “Replication Materials: "Automated Assignment of Cell Identity from Single-Cell
    Multiplexed Imaging and Proteomic Data".” [https://doi.org/10.5281/ZENODO.5156049](https://doi.org/10.5281/ZENODO.5156049).Greenberg,
    Bernard, Abdel-Latif Abul-Ela, Walt Simmons, and Daniel Horvitz. 1969\. “The Unrelated
    Question Randomized Response Model: Theoretical Framework.” *Journal of the American
    Statistical Association* 64 (326): 520–39\. [https://doi.org/10.1080/01621459.1969.10500991](https://doi.org/10.1080/01621459.1969.10500991).Hart,
    Edmund, Pauline Barmby, David LeBauer, François Michonneau, Sarah Mount, Patrick
    Mulrooney, Timothée Poisot, Kara Woo, Naupaka Zimmerman, and Jeffrey Hollister.
    2016\. “Ten Simple Rules for Digital Data Storage.” *PLOS Computational Biology*
    12 (10): e1005097\. [https://doi.org/10.1371/journal.pcbi.1005097](https://doi.org/10.1371/journal.pcbi.1005097).Heil,
    Benjamin, Michael Hoffman, Florian Markowetz, Su-In Lee, Casey Greene, and Stephanie
    Hicks. 2021\. “Reproducibility Standards for Machine Learning in the Life Sciences.”
    *Nature Methods* 18 (10): 1132–35\. [https://doi.org/10.1038/s41592-021-01256-7](https://doi.org/10.1038/s41592-021-01256-7).Hester,
    Jim, Hadley Wickham, and Gábor Csárdi. 2021\. *fs: Cross-Platform File System
    Operations Based on “libuv”*. [https://CRAN.R-project.org/package=fs](https://CRAN.R-project.org/package=fs).Hotz,
    Joseph, Christopher Bollinger, Tatiana Komarova, Charles Manski, Robert Moffitt,
    Denis Nekipelov, Aaron Sojourner, and Bruce Spencer. 2022\. “Balancing Data Privacy
    and Usability in the Federal Statistical System.” *Proceedings of the National
    Academy of Sciences* 119 (31): 1–10\. [https://doi.org/10.1073/pnas.2104906119](https://doi.org/10.1073/pnas.2104906119).Izrailev,
    Sergei. 2022\. *tictoc: Functions for Timing R Scripts, as Well as Implementations
    of “Stack” and “List” Structures*. [https://CRAN.R-project.org/package=tictoc](https://CRAN.R-project.org/package=tictoc).Katz,
    Lindsay, and Rohan Alexander. 2023a. “A new, comprehensive database of all proceedings
    of the Australian Parliamentary Debates (1998-2022).” Zenodo. [https://doi.org/10.5281/zenodo.7799678](https://doi.org/10.5281/zenodo.7799678).———.
    2023b. “Digitization of the Australian Parliamentary Debates, 1998–2022.” *Scientific
    Data* 10 (1): 1–14\. [https://doi.org/10.1038/s41597-023-02464-w](https://doi.org/10.1038/s41597-023-02464-w).Kenny,
    Christopher T., Shiro Kuriwaki, Cory McCartan, Evan T. R. Rosenman, Tyler Simko,
    and Kosuke Imai. 2021\. “The use of differential privacy for census data and its
    impact on redistricting: The case of the 2020 U.S. Census.” *Science Advances*
    7 (41). [https://doi.org/10.1126/sciadv.abk3283](https://doi.org/10.1126/sciadv.abk3283).———.
    2023\. “Comment: The Essential Role of Policy Evaluation for the 2020 Census Disclosure
    Avoidance System.” *Harvard Data Science Review*, no. Special Issue 2\. [https://doi.org/10.1162/99608f92.abc2c765](https://doi.org/10.1162/99608f92.abc2c765).Knuth,
    Donald. 1998\. *Art of Computer Programming, Volume 2: Seminumerical Algorithms*.
    2nd ed.Koenecke, Allison, and Hal Varian. 2020\. “Synthetic Data Generation for
    Economists.” [https://arxiv.org/abs/2011.01374](https://arxiv.org/abs/2011.01374).Lewis,
    Crystal. 2024\. *Data Management in Large-Scale Education Research*. 1st ed. Chapman;
    Hall/CRC. [https://datamgmtinedresearch.com/index.html](https://datamgmtinedresearch.com/index.html).Lima,
    Renato de, Oliver Phillips, Alvaro Duque, Sebastian Tello, Stuart Davies, Alexandre
    Adalardo de Oliveira, Sandra Muller, et al. 2022\. “Making Forest Data Fair and
    Open.” *Nature Ecology & Evolution* 6 (April): 656–58\. [https://doi.org/10.1038/s41559-022-01738-7](https://doi.org/10.1038/s41559-022-01738-7).Lin,
    Herbert. 2014\. “A Proposal to Reduce Government Overclassification of Information
    Related to National Security.” *Journal of National Security Law and Policy* 7:
    443–63.Mammoliti, Anthony, Petr Smirnov, Minoru Nakano, Zhaleh Safikhani, Christopher
    Eeles, Heewon Seo, Sisira Kadambat Nair, et al. 2021\. “Orchestrating and Sharing
    Large Multimodal Data for Transparent and Reproducible Research.” *Nature Communications*
    12 (1). [https://doi.org/10.1038/s41467-021-25974-w](https://doi.org/10.1038/s41467-021-25974-w).McKinney,
    Wes. (2011) 2022\. *Python for Data Analysis*. 3rd ed. [https://wesmckinney.com/book/](https://wesmckinney.com/book/).Miceli,
    Milagros, Julian Posada, and Tianling Yang. 2022\. “Studying up Machine Learning
    Data.” *Proceedings of the ACM on Human-Computer Interaction* 6 (January): 1–14\.
    [https://doi.org/10.1145/3492853](https://doi.org/10.1145/3492853).Michener, William.
    2015\. “Ten Simple Rules for Creating a Good Data Management Plan.” *PLOS Computational
    Biology* 11 (10): e1004525\. [https://doi.org/10.1371/journal.pcbi.1004525](https://doi.org/10.1371/journal.pcbi.1004525).Navarro,
    Danielle. 2022\. “Binding Apache Arrow to R,” January. [https://blog.djnavarro.net/posts/2022-01-18%5Fbinding-arrow-to-r/](https://blog.djnavarro.net/posts/2022-01-18%5Fbinding-arrow-to-r/).Navarro,
    Danielle, Jonathan Keane, and Stephanie Hazlitt. 2022\. “Larger-Than-Memory Data
    Workflows with Apache Arrow,” June. [https://arrow-user2022.netlify.app](https://arrow-user2022.netlify.app).Oberski,
    Daniel, and Frauke Kreuter. 2020\. “Differential Privacy and Social Science: An
    Urgent Puzzle.” *Harvard Data Science Review* 2 (1). [https://doi.org/10.1162/99608f92.63a22079](https://doi.org/10.1162/99608f92.63a22079).Ooms,
    Jeroen. 2022\. *openssl: Toolkit for Encryption, Signatures and Certificates Based
    on OpenSSL*. [https://CRAN.R-project.org/package=openssl](https://CRAN.R-project.org/package=openssl).Patki,
    Neha, Roy Wedge, and Kalyan Veeramachaneni. 2016\. “The Synthetic Data Vault.”
    In *2016 IEEE International Conference on Data Science and Advanced Analytics
    (DSAA)*, 399–410\. [https://doi.org/10.1109/DSAA.2016.49](https://doi.org/10.1109/DSAA.2016.49).Paullada,
    Amandalynne, Inioluwa Deborah Raji, Emily Bender, Emily Denton, and Alex Hanna.
    2021\. “Data and Its (Dis)contents: A Survey of Dataset Development and Use in
    Machine Learning Research.” *Patterns* 2 (11): 100336\. [https://doi.org/10.1016/j.patter.2021.100336](https://doi.org/10.1016/j.patter.2021.100336).Piller,
    Charles. 2022\. “Blots on a Field?” *Science* 377 (6604): 358–63\. [https://doi.org/10.1126/science.ade0209](https://doi.org/10.1126/science.ade0209).R
    Core Team. 2024\. *R: A Language and Environment for Statistical Computing*. Vienna,
    Austria: R Foundation for Statistical Computing. [https://www.R-project.org/](https://www.R-project.org/).Richardson,
    Neal, Ian Cook, Nic Crane, Dewey Dunnington, Romain François, Jonathan Keane,
    Dragoș Moldovan-Grünfeld, Jeroen Ooms, and Apache Arrow. 2023\. *arrow: Integration
    to Apache Arrow*. [https://CRAN.R-project.org/package=arrow](https://CRAN.R-project.org/package=arrow).Ross,
    Casey. 2022\. “How a Decades-Old Database Became a Hugely Profitable Dossier on
    the Health of 270 Million Americans.” *Stat*, February. [https://www.statnews.com/2022/02/01/ibm-watson-health-marketscan-data/](https://www.statnews.com/2022/02/01/ibm-watson-health-marketscan-data/).Rubinstein,
    Benjamin, and Francesco Alda. 2017\. “Pain-Free Random Differential Privacy with
    Sensitivity Sampling.” In *34th International Conference on Machine Learning (ICML’2017)*.Ruggles,
    Steven, Catherine Fitch, Diana Magnuson, and Jonathan Schroeder. 2019\. “Differential
    Privacy and Census Data: Implications for Social and Economic Research.” *AEA
    Papers and Proceedings* 109 (May): 403–8\. [https://doi.org/10.1257/pandp.20191107](https://doi.org/10.1257/pandp.20191107).Simonsohn,
    Uri. 2013\. “Just Post It: The Lesson from Two Cases of Fabricated Data Detected
    by Statistics Alone.” *Psychological Science* 24 (10): 1875–88\. [https://doi.org/10.1177/0956797613480366](https://doi.org/10.1177/0956797613480366).Suriyakumar,
    Vinith, Nicolas Papernot, Anna Goldenberg, and Marzyeh Ghassemi. 2021\. “Chasing
    Your Long Tails.” In *Proceedings of the 2021 ACM Conference on Fairness, Accountability,
    and Transparency*. [https://doi.org/10.1145/3442188.3445934](https://doi.org/10.1145/3442188.3445934).Tang,
    Jun, Aleksandra Korolova, Xiaolong Bai, Xueqiang Wang, and Xiaofeng Wang. 2017\.
    “Privacy Loss in Apple’s Implementation of Differential Privacy on MacOS 10.12.”
    arXiv. [https://doi.org/10.48550/arXiv.1709.02753](https://doi.org/10.48550/arXiv.1709.02753).Tierney,
    Nicholas, and Karthik Ram. 2020\. “A Realistic Guide to Making Data Available
    Alongside Code to Improve Reproducibility.” [https://arxiv.org/abs/2002.11626](https://arxiv.org/abs/2002.11626).———.
    2021\. “Common-Sense Approaches to Sharing Tabular Data Alongside Publication.”
    *Patterns* 2 (12): 100368\. [https://doi.org/10.1016/j.patter.2021.100368](https://doi.org/10.1016/j.patter.2021.100368).Wicherts,
    Jelte, Marjan Bakker, and Dylan Molenaar. 2011\. “Willingness to Share Research
    Data Is Related to the Strength of the Evidence and the Quality of Reporting of
    Statistical Results.” *PLOS ONE* 6 (11): e26828\. [https://doi.org/10.1371/journal.pone.0026828](https://doi.org/10.1371/journal.pone.0026828).Wickham,
    Hadley. 2022\. *R Packages*. 2nd ed. O’Reilly Media. [https://r-pkgs.org](https://r-pkgs.org).Wickham,
    Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain
    François, Garrett Grolemund, et al. 2019\. “Welcome to the Tidyverse.” *Journal
    of Open Source Software* 4 (43): 1686\. [https://doi.org/10.21105/joss.01686](https://doi.org/10.21105/joss.01686).Wickham,
    Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. (2016) 2023\. *R for Data
    Science*. 2nd ed. O’Reilly Media. [https://r4ds.hadley.nz](https://r4ds.hadley.nz).Wickham,
    Hadley, Jim Hester, Winston Chang, and Jenny Bryan. 2022\. *devtools: Tools to
    Make Developing R Packages Easier*. [https://CRAN.R-project.org/package=devtools](https://CRAN.R-project.org/package=devtools).Wilkinson,
    Mark, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton,
    Arie Baak, Niklas Blomberg, et al. 2016\. “The FAIR Guiding Principles for Scientific
    Data Management and Stewardship.” *Scientific Data* 3 (1): 1–9\. [https://doi.org/10.1038/sdata.2016.18](https://doi.org/10.1038/sdata.2016.18).Zhang,
    Susan, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
    Dewan, et al. 2022\. “OPT: Open Pre-Trained Transformer Language Models.” arXiv.
    [https://doi.org/10.48550/arXiv.2205.01068](https://doi.org/10.48550/arXiv.2205.01068).Zook,
    Matthew, Solon Barocas, danah boyd, Kate Crawford, Emily Keller, Seeta Peña Gangadharan,
    Alyssa Goodman, et al. 2017\. “Ten Simple Rules for Responsible Big Data Research.”
    *PLOS Computational Biology* 13 (3): e1005399\. [https://doi.org/10.1371/journal.pcbi.1005399](https://doi.org/10.1371/journal.pcbi.1005399).***
    **** * *'
  prefs: []
  type: TYPE_NORMAL
- en: An interesting counterpoint is the recent use, by law enforcement, of DNA databases
    to find suspects. The suspect themselves might not be in the database, but the
    nature of DNA means that some related individuals can nonetheless still be identified.[↩︎](#fnref1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Closely supervise the students, and especially check the pull requests before
    they are made to ensure they are reasonable; we don’t want to annoy people. A
    good option might a fix to a couple of typos or similar.[↩︎](#fnref2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These will need to change each year.[↩︎](#fnref3)****************
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
