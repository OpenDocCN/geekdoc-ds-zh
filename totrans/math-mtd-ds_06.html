<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>1.4. Some observations about high-dimensional data#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>1.4. Some observations about high-dimensional data#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap01_intro/04_highdim/roch-mmids-intro-highdim.html">https://mmids-textbook.github.io/chap01_intro/04_highdim/roch-mmids-intro-highdim.html</a></blockquote>

<p>In this section, we first apply <span class="math notranslate nohighlight">\(k\)</span>-means clustering to a high-dimensional example to illustrate the issues that arise in that context. We then discuss some surprising phenomena in high dimensions.</p>
<section id="clustering-in-high-dimension">
<h2><span class="section-number">1.4.1. </span>Clustering in high dimension<a class="headerlink" href="#clustering-in-high-dimension" title="Link to this heading">#</a></h2>
<p>In this section, we test our implementation of <span class="math notranslate nohighlight">\(k\)</span>-means on a simple simulated dataset in high dimension.</p>
<p>The following function generates <span class="math notranslate nohighlight">\(n\)</span> data points from a mixture of two equally likely, spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with variance <span class="math notranslate nohighlight">\(1\)</span>, one with mean <span class="math notranslate nohighlight">\(-w\mathbf{e}_1\)</span> and one with mean <span class="math notranslate nohighlight">\(w \mathbf{e}_1\)</span>. We use <code class="docutils literal notranslate"><span class="pre">gmm2spherical</span></code> from a previous section. It is found in <code class="docutils literal notranslate"><span class="pre">mmids.py</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">mu0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    <span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(([</span><span class="o">-</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">mmids</span><span class="o">.</span><span class="n">gmm2spherical</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We start with <span class="math notranslate nohighlight">\(d=2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s run <span class="math notranslate nohighlight">\(k\)</span>-means on this dataset using <span class="math notranslate nohighlight">\(k=2\)</span>. We use <code class="docutils literal notranslate"><span class="pre">kmeans()</span></code> from the <code class="docutils literal notranslate"><span class="pre">mmids.py</span></code> file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1044.8267883490312
208.5284166285488
204.02397716710018
204.02397716710018
204.02397716710018
</pre></div>
</div>
</div>
</div>
<p>Our default of <span class="math notranslate nohighlight">\(10\)</span> iterations seem to have been enough for the algorithm to converge. We can visualize the result by <a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html">coloring</a> the points according to the assignment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'brg'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9aa59ca573eb1a92b3e764e96b1d04efe81f825b67863353e16d5736d4608fb4.png" src="../Images/aa56b156c7bbd5cc401744d4eec4820d.png" data-original-src="https://mmids-textbook.github.io/_images/9aa59ca573eb1a92b3e764e96b1d04efe81f825b67863353e16d5736d4608fb4.png"/>
</div>
</div>
<p>Let’s see what happens in higher dimension. We repeat our experiment with <span class="math notranslate nohighlight">\(d=1000\)</span>.</p>
<div class="cell tag_colab-keep docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Again, we observe two clearly delineated clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/264709bc9d4749da24e2b1bc73c5716f8d150355fba2e13f70a3906c858baa7f.png" src="../Images/2555114660b70c347ebf0d27a90440a0.png" data-original-src="https://mmids-textbook.github.io/_images/264709bc9d4749da24e2b1bc73c5716f8d150355fba2e13f70a3906c858baa7f.png"/>
</div>
</div>
<p>This dataset is in <span class="math notranslate nohighlight">\(1000\)</span> dimensions, but we’ve plotted the data in only the first two dimensions. If we plot in any two dimensions not including the first one instead, we see only one cluster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/21494a7e3aca56828d3ea264f5b728df4a6ba072eff22b28e743010117504049.png" src="../Images/2ceed7ae94ba461478dde0058309f68a.png" data-original-src="https://mmids-textbook.github.io/_images/21494a7e3aca56828d3ea264f5b728df4a6ba072eff22b28e743010117504049.png"/>
</div>
</div>
<p>Let’s see how <span class="math notranslate nohighlight">\(k\)</span>-means fares on this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>99518.03165136592
99518.03165136592
99518.03165136592
99518.03165136592
99518.03165136592
</pre></div>
</div>
</div>
</div>
<p>Our attempt at clustering does not appear to have been successful.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'brg'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/dc448ee379a7ca994bcd89a9ef7e80c567cd1e8704627661a871effd41fa9f11.png" src="../Images/085a8dadcd6ad71b3fc7e6a1a02fe4c0.png" data-original-src="https://mmids-textbook.github.io/_images/dc448ee379a7ca994bcd89a9ef7e80c567cd1e8704627661a871effd41fa9f11.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>What happened? While the clusters are easy to tease apart <em>if we know to look at the first coordinate only</em>, in the full space the within-cluster and between-cluster distances become harder to distinguish: the noise overwhelms the signal.</p>
<p>As the dimension increases, the distributions of intra-cluster and inter-cluster distances overlap significantly and become more or less indistinguishable. That provides some insights into why clustering may fail here. Note that we used the same offset for all simulations. On the other hand, if the separation between the clusters is sufficiently large, one would expect clustering to work even in high dimension.</p>
<p><img alt="Histograms of within-cluster and between-cluster distances for a sample of size  in  (left) and  (right) dimensions with a given offset . As  increases, the two distributions become increasingly indistinguishable." src="../Images/f5f988f0b8742e52a610ea9e41c65370.png" data-original-src="https://mmids-textbook.github.io/_images/cluster-distances.png"/></p>
<p><strong>TRY IT!</strong> What precedes (and what follows in the next subsection) is not a formal proof that <span class="math notranslate nohighlight">\(k\)</span>-means clustering will be unsuccessful here. The behavior of the algorithm is quite complex and depends, in particular, on the initialization and the density of points. Here, increasing the number of data points eventually leads to a much better performance. Explore this behavior on your own by modifying the code. (For some theoretical justifications (beyond this course), see <a class="reference external" href="https://arxiv.org/pdf/0912.0086.pdf">here</a> and <a class="reference external" href="http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf">here</a>.)</p>
<p><strong>CHAT &amp; LEARN</strong> According to Claude, here is how a cat might summarize the situation:</p>
<p><strong>Figure:</strong> A kitten in space (<em>Credit:</em> Made with <a class="reference external" href="https://www.midjourney.com/">Midjourney</a>)</p>
<p><img alt="Kitty in space" src="../Images/0834716500ee9672d75bf25880399171.png" data-original-src="https://mmids-textbook.github.io/_images/kitten_astronaut-small.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>“I iz in high-dimensional space</p>
<p>All dese data points, everywheres</p>
<p>But no matter how far I roams</p>
<p>They all look the sames to me!”</p>
<p><span class="math notranslate nohighlight">\(\ddagger\)</span></p>
</section>
<section id="surprising-phenomena-in-high-dimension">
<h2><span class="section-number">1.4.2. </span>Surprising phenomena in high dimension<a class="headerlink" href="#surprising-phenomena-in-high-dimension" title="Link to this heading">#</a></h2>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">a high-dimensional space is a lonely place</p>— Bernhard Schölkopf (@bschoelkopf) <a href="https://twitter.com/bschoelkopf/status/503554842829549568?ref_src=twsrc%5Etfw">August 24, 2014</a></blockquote>  <p>In the previous section, we saw how the contribution from a large number of “noisy dimensions” can overwhelm the “signal” in the context of clustering. In this section we discuss further properties of high-dimensional space that are relevant to data science problems.</p>
<p>Applying <em>Chebyshev’s Inequality</em> to sums of independent random variables has useful statistical implications: it shows that, with a large enough number of samples <span class="math notranslate nohighlight">\(n\)</span>, the sample mean is close to the population mean. Hence it allows us to infer properties of a population from samples. Interestingly, one can apply a similar argument to a different asymptotic regime: the limit of large dimension <span class="math notranslate nohighlight">\(d\)</span>. But as we will see in this section, the statistical implications are quite different.</p>
<p>To start explaining the quote above, we consider a simple experiment. Let <span class="math notranslate nohighlight">\(\mathcal{C} = [-1/2,1/2]^d\)</span> be the <span class="math notranslate nohighlight">\(d\)</span>-cube with side lengths <span class="math notranslate nohighlight">\(1\)</span> centered at the origin and let <span class="math notranslate nohighlight">\(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\|\leq 1/2\}\)</span> be the inscribed <span class="math notranslate nohighlight">\(d\)</span>-ball.</p>
<p>Now pick a point <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> uniformly at random in <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. What is the probability that it falls in <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>?</p>
<p>To generate <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, we pick <span class="math notranslate nohighlight">\(d\)</span> independent random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_d \sim \mathrm{U}[-1/2, 1/2]\)</span>, and form the vector <span class="math notranslate nohighlight">\(\mathbf{X} = (X_1, \ldots, X_d)\)</span>. Indeed, the PDF of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is then <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})= 1^d = 1\)</span> if <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{C}\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p>
<p>The event we are interested in is <span class="math notranslate nohighlight">\(A = \left\{\|\mathbf{X}\| \leq 1/2\right\}\)</span>. The uniform distribution over the set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> has the property that <span class="math notranslate nohighlight">\(\mathbb{P}[A]\)</span> is the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> divided by the volume of <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. In this case, the volume of <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> is <span class="math notranslate nohighlight">\(1^d = 1\)</span> and the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> has an <a class="reference external" href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball">explicit formula</a>.</p>
<p>This leads to the following surprising fact:</p>
<p><strong>THEOREM</strong> <strong>(High-dimensional Cube)</strong> <span class="math notranslate nohighlight">\(\idx{high-dimensional cube theorem}\xdi\)</span> Let
<span class="math notranslate nohighlight">\(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d \,:\, \|\mathbf{x}\|\leq 1/2\}\)</span> and
<span class="math notranslate nohighlight">\(\mathcal{C} = [-1/2,1/2]^d\)</span>. Pick <span class="math notranslate nohighlight">\(\mathbf{X} \sim \mathrm{U}[\mathcal{C}]\)</span>. Then, as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[\mathbf{X} \in \mathcal{B}]
\to 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>In words, in high dimension if one picks a point at random from the cube, it is unlikely to be close to the origin. Instead it is likely to be in the corners. A geometric interpretation is that a high-dimensional cube is a bit like a “spiky ball.”</p>
<p><strong>Figure:</strong> Visualization of a high-dimensional cube as a spiky ball (<em>Credit:</em> Made with <a class="reference external" href="https://www.midjourney.com/">Midjourney</a>)</p>
<p><img alt="Visualization of a high-dimensional cube" src="../Images/0ed7cc7c9c5326071c245e739ab01a88.png" data-original-src="https://mmids-textbook.github.io/_images/ball_with_spikes.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>We give a proof based on <em>Chebyshev’s Inequality</em>. It has the advantage of providing some insight into this counter-intuitive phenomenon by linking it to the concentration of sums of independent random variables, in this case the squared norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p><em>Proof idea:</em> We think of <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> as a sum of independent random variables and apply <em>Chebyshev’s Inequality</em>. It implies that the norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is concentrated around its mean, which grows like <span class="math notranslate nohighlight">\(\sqrt{d}\)</span>. The latter is larger than <span class="math notranslate nohighlight">\(1/2\)</span> for <span class="math notranslate nohighlight">\(d\)</span> large.</p>
<p><em>Proof:</em> To see the relevance of <em>Chebyshev’s Inequality</em>, we compute the mean and standard deviation of the norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. In fact, because of the square root in <span class="math notranslate nohighlight">\(\|\mathbf{X}\|\)</span>, computing its expectation is difficult. Instead we work with the squared norm</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2,
\]</div>
<p>which has the advantage of being a sum of independent random variables – for which the expectation and variance are much easier to compute. Observe further that the probability of the event of interest <span class="math notranslate nohighlight">\(\{\|\mathbf{X}\| \leq 1/2\}\)</span> can be re-written in terms of <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> as follows</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}
\left[
\|\mathbf{X}\| \leq 1/2
\right]
&amp;= 
\mathbb{P}
\left[
\|\mathbf{X}\|^2 \leq 1/4
\right].
\end{align*}\]</div>
<p>To simplify the notation, we use <span class="math notranslate nohighlight">\(\tilde\mu = \mathbb{E}[X_1^2]\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma = \sqrt{\mathrm{Var}[X_1^2]}\)</span> for the mean and standard deviation of <span class="math notranslate nohighlight">\(X_1^2\)</span> respectively. Using linearity of expectation and the fact that the <span class="math notranslate nohighlight">\(X_i\)</span>’s are independent, we get</p>
<div class="math notranslate nohighlight">
\[
\mu_{\|\mathbf{X}\|^2}
= \mathbb{E}\left[
\|\mathbf{X}\|^2
\right]
= \sum_{i=1}^d \mathbb{E}[X_i^2]
= d \,\mathbb{E}[X_1^2]
= \tilde\mu \, d,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}\left[
\|\mathbf{X}\|^2
\right]
= \sum_{i=1}^d \mathrm{Var}[X_i^2]
= d \,\mathrm{Var}[X_1^2].
\]</div>
<p>Taking a square root, we get an expression for the standard deviation of our quantity of interest <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> in terms of the standard deviation of <span class="math notranslate nohighlight">\(X_1^2\)</span></p>
<div class="math notranslate nohighlight">
\[
\sigma_{\|\mathbf{X}\|^2}
= \tilde\sigma \, \sqrt{d}.
\]</div>
<p>(Note that we could compute <span class="math notranslate nohighlight">\(\tilde\mu\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma\)</span> explicitly, but it will not be necessary here.)</p>
<p>We use <em>Chebyshev’s Inequality</em> to show that <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> is highly likely to be close to its mean <span class="math notranslate nohighlight">\(\tilde\mu \, d\)</span>, which is much larger than <span class="math notranslate nohighlight">\(1/4\)</span> when <span class="math notranslate nohighlight">\(d\)</span> is large. And that therefore <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> is highly unlikely to be smaller than <span class="math notranslate nohighlight">\(1/4\)</span>. We give the details next.</p>
<p>By the one-sided version of <em>Chebyshev’s Inequality</em> in terms of the standard deviation, we have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[
\|\mathbf{X}\|^2 - \mu_{\|\mathbf{X}\|^2} \leq - \alpha
\right]
\leq 
\left(\frac{\sigma_{\|\mathbf{X}\|^2}}{\alpha}\right)^2.
\]</div>
<p>That is, using the formulas above and rearranging slightly,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[
\|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
\right]
\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2.
\]</div>
<p>How do we relate this to the probability of interest
<span class="math notranslate nohighlight">\(\mathbb{P}\left[\|\mathbf{X}\|^2 \leq 1/4\right]\)</span>? Recall that we are free to choose <span class="math notranslate nohighlight">\(\alpha\)</span> in this inequality. So simply take <span class="math notranslate nohighlight">\(\alpha\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\tilde\mu \,d - \alpha = \frac{1}{4},
\]</div>
<p>that is, <span class="math notranslate nohighlight">\(\alpha = \tilde\mu \,d - 1/4\)</span>. Observe that, once <span class="math notranslate nohighlight">\(d\)</span> is large enough, it holds that <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>.</p>
<p>Finally, replacing this choice of <span class="math notranslate nohighlight">\(\alpha\)</span> in the inequality above gives</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}
\left[
\|\mathbf{X}\| \leq 1/2
\right]
&amp;= \mathbb{P}\left[\|\mathbf{X}\|^2 \leq 1/4\right]\\
&amp;=
\mathbb{P}\left[
\|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
\right]\\
&amp;\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2\\
&amp;\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\tilde\mu \,d - 1/4}\right)^2.
\end{align*}\]</div>
<p>Critically, <span class="math notranslate nohighlight">\(\tilde\mu\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma\)</span> do not depend on <span class="math notranslate nohighlight">\(d\)</span>. So the right-hand side goes to <span class="math notranslate nohighlight">\(0\)</span> as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>. Indeed, <span class="math notranslate nohighlight">\(d\)</span> is much larger than <span class="math notranslate nohighlight">\(\sqrt{d}\)</span> when <span class="math notranslate nohighlight">\(d\)</span> is large. That proves the claim.<span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We will see later in the course that this high-dimensional phenomenon has implications for data science problems. It is behind what is referred to as the <a class="reference external" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of Dimensionality</a><span class="math notranslate nohighlight">\(\idx{curse of dimensionality}\xdi\)</span>.</p>
<p>While <em>Chebyshev’s inequality</em> correctly implies that <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span>, it does not give the correct rate of convergence. In reality, that probability goes to <span class="math notranslate nohighlight">\(0\)</span> at a much faster rate than <span class="math notranslate nohighlight">\(1/d\)</span>. Specifically, <a class="reference external" href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball#High_dimensions">it can be shown</a> that <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span> roughly as <span class="math notranslate nohighlight">\(d^{-d/2}\)</span>. We will not derive (or need) this fact here.</p>
<p><strong>NUMERICAL CORNER:</strong> We can check the theorem in a simulation. Here we pick <span class="math notranslate nohighlight">\(n\)</span> points uniformly at random in the <span class="math notranslate nohighlight">\(d\)</span>-cube <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>, for a range of dimensions up to <code class="docutils literal notranslate"><span class="pre">dmax</span></code>. We then plot the frequency of landing in the inscribed <span class="math notranslate nohighlight">\(d\)</span>-ball <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> and see that it rapidly converges to <span class="math notranslate nohighlight">\(0\)</span>. Alternatively, we could just plot the formula for the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>. But knowing how to do simulations is useful in situations where explicit formulas are unavailable or intractable. We plot the result up to dimension <span class="math notranslate nohighlight">\(10\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">dmax</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span>

<span class="n">in_ball</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dmax</span><span class="p">)</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dmax</span><span class="p">):</span>
    <span class="n">in_ball</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">dmax</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">in_ball</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d08f6e8ae99d1fc47a60ae906d9afb4fb92b64d9611fda98fd7415d2bc98a67d.png" src="../Images/c7ed85ca4cdd1727d357fd1139869726.png" data-original-src="https://mmids-textbook.github.io/_images/d08f6e8ae99d1fc47a60ae906d9afb4fb92b64d9611fda98fd7415d2bc98a67d.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> The volume of the <span class="math notranslate nohighlight">\(d\)</span>-dimensional cube <span class="math notranslate nohighlight">\(C = [-1/2, 1/2]^d\)</span> is:</p>
<p>a) <span class="math notranslate nohighlight">\(1/d\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(1/2^d\)</span></p>
<p>c) 1</p>
<p>d) <span class="math notranslate nohighlight">\(2^d\)</span></p>
<p><strong>2</strong> In a high-dimensional cube <span class="math notranslate nohighlight">\(C = [-1/2, 1/2]^d\)</span>, as the dimension <span class="math notranslate nohighlight">\(d\)</span> increases, the probability that a randomly chosen point lies within the inscribed sphere <span class="math notranslate nohighlight">\(B = \{x \in \mathbb{R}^d : \|x\| \le 1/2\}\)</span>:</p>
<p>a) Approaches 1</p>
<p>b) Approaches 1/2</p>
<p>c) Approaches 0</p>
<p>d) Remains constant</p>
<p><strong>3</strong> Which of the following best describes the appearance of a high-dimensional cube?</p>
<p>a) A smooth, round ball</p>
<p>b) A spiky ball with most of its volume concentrated in the corners</p>
<p>c) A perfect sphere with uniform volume distribution</p>
<p>d) A flat, pancake-like shape</p>
<p><strong>4</strong> Which inequality is used to prove the theorem about high-dimensional cubes?</p>
<p>a) Cauchy-Schwarz inequality</p>
<p>b) Triangle inequality</p>
<p>c) Markov’s inequality</p>
<p>d) Chebyshev’s inequality</p>
<p><strong>5</strong> In the proof of the theorem about high-dimensional cubes, which property of the squared norm <span class="math notranslate nohighlight">\(\|X\|^2\)</span> is used?</p>
<p>a) It is a sum of dependent random variables.</p>
<p>b) It is a sum of independent random variables.</p>
<p>c) It is a product of independent random variables.</p>
<p>d) It is a product of dependent random variables.</p>
<p>Answer for 1: c. Justification: The side length of the cube is 1, and the volume of a <span class="math notranslate nohighlight">\(d\)</span>-dimensional cube is the side length raised to the power <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>Answer for 2: c. Justification: This is the statement of the theorem “High-dimensional Cube” in the text.</p>
<p>Answer for 3: b. Justification: The text mentions, “A geometric interpretation is that a high-dimensional cube is a bit like a ‘spiky ball.’”</p>
<p>Answer for 4: d. Justification: The text explicitly states that Chebyshev’s inequality is used in the proof.</p>
<p>Answer for 5: b. Justification: The proof states, “we work with the squared norm</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2,
\]</div>
<p>which has the advantage of being a sum of independent random variables.”</p>
</section>
&#13;

<h2><span class="section-number">1.4.1. </span>Clustering in high dimension<a class="headerlink" href="#clustering-in-high-dimension" title="Link to this heading">#</a></h2>
<p>In this section, we test our implementation of <span class="math notranslate nohighlight">\(k\)</span>-means on a simple simulated dataset in high dimension.</p>
<p>The following function generates <span class="math notranslate nohighlight">\(n\)</span> data points from a mixture of two equally likely, spherical <span class="math notranslate nohighlight">\(d\)</span>-dimensional Gaussians with variance <span class="math notranslate nohighlight">\(1\)</span>, one with mean <span class="math notranslate nohighlight">\(-w\mathbf{e}_1\)</span> and one with mean <span class="math notranslate nohighlight">\(w \mathbf{e}_1\)</span>. We use <code class="docutils literal notranslate"><span class="pre">gmm2spherical</span></code> from a previous section. It is found in <code class="docutils literal notranslate"><span class="pre">mmids.py</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">mu0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    <span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">(([</span><span class="o">-</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">mmids</span><span class="o">.</span><span class="n">gmm2spherical</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">mu0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We start with <span class="math notranslate nohighlight">\(d=2\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s run <span class="math notranslate nohighlight">\(k\)</span>-means on this dataset using <span class="math notranslate nohighlight">\(k=2\)</span>. We use <code class="docutils literal notranslate"><span class="pre">kmeans()</span></code> from the <code class="docutils literal notranslate"><span class="pre">mmids.py</span></code> file.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1044.8267883490312
208.5284166285488
204.02397716710018
204.02397716710018
204.02397716710018
</pre></div>
</div>
</div>
</div>
<p>Our default of <span class="math notranslate nohighlight">\(10\)</span> iterations seem to have been enough for the algorithm to converge. We can visualize the result by <a class="reference external" href="https://matplotlib.org/stable/api/_as_gen/matplotlib.lines.Line2D.html">coloring</a> the points according to the assignment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'brg'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/9aa59ca573eb1a92b3e764e96b1d04efe81f825b67863353e16d5736d4608fb4.png" src="../Images/aa56b156c7bbd5cc401744d4eec4820d.png" data-original-src="https://mmids-textbook.github.io/_images/9aa59ca573eb1a92b3e764e96b1d04efe81f825b67863353e16d5736d4608fb4.png"/>
</div>
</div>
<p>Let’s see what happens in higher dimension. We repeat our experiment with <span class="math notranslate nohighlight">\(d=1000\)</span>.</p>
<div class="cell tag_colab-keep docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">two_mixed_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Again, we observe two clearly delineated clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/264709bc9d4749da24e2b1bc73c5716f8d150355fba2e13f70a3906c858baa7f.png" src="../Images/2555114660b70c347ebf0d27a90440a0.png" data-original-src="https://mmids-textbook.github.io/_images/264709bc9d4749da24e2b1bc73c5716f8d150355fba2e13f70a3906c858baa7f.png"/>
</div>
</div>
<p>This dataset is in <span class="math notranslate nohighlight">\(1000\)</span> dimensions, but we’ve plotted the data in only the first two dimensions. If we plot in any two dimensions not including the first one instead, we see only one cluster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">2</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/21494a7e3aca56828d3ea264f5b728df4a6ba072eff22b28e743010117504049.png" src="../Images/2ceed7ae94ba461478dde0058309f68a.png" data-original-src="https://mmids-textbook.github.io/_images/21494a7e3aca56828d3ea264f5b728df4a6ba072eff22b28e743010117504049.png"/>
</div>
</div>
<p>Let’s see how <span class="math notranslate nohighlight">\(k\)</span>-means fares on this dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">assign</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">kmeans</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>99518.03165136592
99518.03165136592
99518.03165136592
99518.03165136592
99518.03165136592
</pre></div>
</div>
</div>
</div>
<p>Our attempt at clustering does not appear to have been successful.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">assign</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">'brg'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/dc448ee379a7ca994bcd89a9ef7e80c567cd1e8704627661a871effd41fa9f11.png" src="../Images/085a8dadcd6ad71b3fc7e6a1a02fe4c0.png" data-original-src="https://mmids-textbook.github.io/_images/dc448ee379a7ca994bcd89a9ef7e80c567cd1e8704627661a871effd41fa9f11.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p>What happened? While the clusters are easy to tease apart <em>if we know to look at the first coordinate only</em>, in the full space the within-cluster and between-cluster distances become harder to distinguish: the noise overwhelms the signal.</p>
<p>As the dimension increases, the distributions of intra-cluster and inter-cluster distances overlap significantly and become more or less indistinguishable. That provides some insights into why clustering may fail here. Note that we used the same offset for all simulations. On the other hand, if the separation between the clusters is sufficiently large, one would expect clustering to work even in high dimension.</p>
<p><img alt="Histograms of within-cluster and between-cluster distances for a sample of size  in  (left) and  (right) dimensions with a given offset . As  increases, the two distributions become increasingly indistinguishable." src="../Images/f5f988f0b8742e52a610ea9e41c65370.png" data-original-src="https://mmids-textbook.github.io/_images/cluster-distances.png"/></p>
<p><strong>TRY IT!</strong> What precedes (and what follows in the next subsection) is not a formal proof that <span class="math notranslate nohighlight">\(k\)</span>-means clustering will be unsuccessful here. The behavior of the algorithm is quite complex and depends, in particular, on the initialization and the density of points. Here, increasing the number of data points eventually leads to a much better performance. Explore this behavior on your own by modifying the code. (For some theoretical justifications (beyond this course), see <a class="reference external" href="https://arxiv.org/pdf/0912.0086.pdf">here</a> and <a class="reference external" href="http://www.stat.yale.edu/~pollard/Papers/Pollard81AS.pdf">here</a>.)</p>
<p><strong>CHAT &amp; LEARN</strong> According to Claude, here is how a cat might summarize the situation:</p>
<p><strong>Figure:</strong> A kitten in space (<em>Credit:</em> Made with <a class="reference external" href="https://www.midjourney.com/">Midjourney</a>)</p>
<p><img alt="Kitty in space" src="../Images/0834716500ee9672d75bf25880399171.png" data-original-src="https://mmids-textbook.github.io/_images/kitten_astronaut-small.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>“I iz in high-dimensional space</p>
<p>All dese data points, everywheres</p>
<p>But no matter how far I roams</p>
<p>They all look the sames to me!”</p>
<p><span class="math notranslate nohighlight">\(\ddagger\)</span></p>
&#13;

<h2><span class="section-number">1.4.2. </span>Surprising phenomena in high dimension<a class="headerlink" href="#surprising-phenomena-in-high-dimension" title="Link to this heading">#</a></h2>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">a high-dimensional space is a lonely place</p>— Bernhard Schölkopf (@bschoelkopf) <a href="https://twitter.com/bschoelkopf/status/503554842829549568?ref_src=twsrc%5Etfw">August 24, 2014</a></blockquote>  <p>In the previous section, we saw how the contribution from a large number of “noisy dimensions” can overwhelm the “signal” in the context of clustering. In this section we discuss further properties of high-dimensional space that are relevant to data science problems.</p>
<p>Applying <em>Chebyshev’s Inequality</em> to sums of independent random variables has useful statistical implications: it shows that, with a large enough number of samples <span class="math notranslate nohighlight">\(n\)</span>, the sample mean is close to the population mean. Hence it allows us to infer properties of a population from samples. Interestingly, one can apply a similar argument to a different asymptotic regime: the limit of large dimension <span class="math notranslate nohighlight">\(d\)</span>. But as we will see in this section, the statistical implications are quite different.</p>
<p>To start explaining the quote above, we consider a simple experiment. Let <span class="math notranslate nohighlight">\(\mathcal{C} = [-1/2,1/2]^d\)</span> be the <span class="math notranslate nohighlight">\(d\)</span>-cube with side lengths <span class="math notranslate nohighlight">\(1\)</span> centered at the origin and let <span class="math notranslate nohighlight">\(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d : \|\mathbf{x}\|\leq 1/2\}\)</span> be the inscribed <span class="math notranslate nohighlight">\(d\)</span>-ball.</p>
<p>Now pick a point <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> uniformly at random in <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. What is the probability that it falls in <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>?</p>
<p>To generate <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, we pick <span class="math notranslate nohighlight">\(d\)</span> independent random variables <span class="math notranslate nohighlight">\(X_1, \ldots, X_d \sim \mathrm{U}[-1/2, 1/2]\)</span>, and form the vector <span class="math notranslate nohighlight">\(\mathbf{X} = (X_1, \ldots, X_d)\)</span>. Indeed, the PDF of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is then <span class="math notranslate nohighlight">\(f_{\mathbf{X}}(\mathbf{x})= 1^d = 1\)</span> if <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathcal{C}\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p>
<p>The event we are interested in is <span class="math notranslate nohighlight">\(A = \left\{\|\mathbf{X}\| \leq 1/2\right\}\)</span>. The uniform distribution over the set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> has the property that <span class="math notranslate nohighlight">\(\mathbb{P}[A]\)</span> is the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> divided by the volume of <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>. In this case, the volume of <span class="math notranslate nohighlight">\(\mathcal{C}\)</span> is <span class="math notranslate nohighlight">\(1^d = 1\)</span> and the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> has an <a class="reference external" href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball">explicit formula</a>.</p>
<p>This leads to the following surprising fact:</p>
<p><strong>THEOREM</strong> <strong>(High-dimensional Cube)</strong> <span class="math notranslate nohighlight">\(\idx{high-dimensional cube theorem}\xdi\)</span> Let
<span class="math notranslate nohighlight">\(\mathcal{B} = \{\mathbf{x} \in \mathbb{R}^d \,:\, \|\mathbf{x}\|\leq 1/2\}\)</span> and
<span class="math notranslate nohighlight">\(\mathcal{C} = [-1/2,1/2]^d\)</span>. Pick <span class="math notranslate nohighlight">\(\mathbf{X} \sim \mathrm{U}[\mathcal{C}]\)</span>. Then, as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[\mathbf{X} \in \mathcal{B}]
\to 0.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>In words, in high dimension if one picks a point at random from the cube, it is unlikely to be close to the origin. Instead it is likely to be in the corners. A geometric interpretation is that a high-dimensional cube is a bit like a “spiky ball.”</p>
<p><strong>Figure:</strong> Visualization of a high-dimensional cube as a spiky ball (<em>Credit:</em> Made with <a class="reference external" href="https://www.midjourney.com/">Midjourney</a>)</p>
<p><img alt="Visualization of a high-dimensional cube" src="../Images/0ed7cc7c9c5326071c245e739ab01a88.png" data-original-src="https://mmids-textbook.github.io/_images/ball_with_spikes.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>We give a proof based on <em>Chebyshev’s Inequality</em>. It has the advantage of providing some insight into this counter-intuitive phenomenon by linking it to the concentration of sums of independent random variables, in this case the squared norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p><em>Proof idea:</em> We think of <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> as a sum of independent random variables and apply <em>Chebyshev’s Inequality</em>. It implies that the norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is concentrated around its mean, which grows like <span class="math notranslate nohighlight">\(\sqrt{d}\)</span>. The latter is larger than <span class="math notranslate nohighlight">\(1/2\)</span> for <span class="math notranslate nohighlight">\(d\)</span> large.</p>
<p><em>Proof:</em> To see the relevance of <em>Chebyshev’s Inequality</em>, we compute the mean and standard deviation of the norm of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. In fact, because of the square root in <span class="math notranslate nohighlight">\(\|\mathbf{X}\|\)</span>, computing its expectation is difficult. Instead we work with the squared norm</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2,
\]</div>
<p>which has the advantage of being a sum of independent random variables – for which the expectation and variance are much easier to compute. Observe further that the probability of the event of interest <span class="math notranslate nohighlight">\(\{\|\mathbf{X}\| \leq 1/2\}\)</span> can be re-written in terms of <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> as follows</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}
\left[
\|\mathbf{X}\| \leq 1/2
\right]
&amp;= 
\mathbb{P}
\left[
\|\mathbf{X}\|^2 \leq 1/4
\right].
\end{align*}\]</div>
<p>To simplify the notation, we use <span class="math notranslate nohighlight">\(\tilde\mu = \mathbb{E}[X_1^2]\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma = \sqrt{\mathrm{Var}[X_1^2]}\)</span> for the mean and standard deviation of <span class="math notranslate nohighlight">\(X_1^2\)</span> respectively. Using linearity of expectation and the fact that the <span class="math notranslate nohighlight">\(X_i\)</span>’s are independent, we get</p>
<div class="math notranslate nohighlight">
\[
\mu_{\|\mathbf{X}\|^2}
= \mathbb{E}\left[
\|\mathbf{X}\|^2
\right]
= \sum_{i=1}^d \mathbb{E}[X_i^2]
= d \,\mathbb{E}[X_1^2]
= \tilde\mu \, d,
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\mathrm{Var}\left[
\|\mathbf{X}\|^2
\right]
= \sum_{i=1}^d \mathrm{Var}[X_i^2]
= d \,\mathrm{Var}[X_1^2].
\]</div>
<p>Taking a square root, we get an expression for the standard deviation of our quantity of interest <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> in terms of the standard deviation of <span class="math notranslate nohighlight">\(X_1^2\)</span></p>
<div class="math notranslate nohighlight">
\[
\sigma_{\|\mathbf{X}\|^2}
= \tilde\sigma \, \sqrt{d}.
\]</div>
<p>(Note that we could compute <span class="math notranslate nohighlight">\(\tilde\mu\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma\)</span> explicitly, but it will not be necessary here.)</p>
<p>We use <em>Chebyshev’s Inequality</em> to show that <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> is highly likely to be close to its mean <span class="math notranslate nohighlight">\(\tilde\mu \, d\)</span>, which is much larger than <span class="math notranslate nohighlight">\(1/4\)</span> when <span class="math notranslate nohighlight">\(d\)</span> is large. And that therefore <span class="math notranslate nohighlight">\(\|\mathbf{X}\|^2\)</span> is highly unlikely to be smaller than <span class="math notranslate nohighlight">\(1/4\)</span>. We give the details next.</p>
<p>By the one-sided version of <em>Chebyshev’s Inequality</em> in terms of the standard deviation, we have</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[
\|\mathbf{X}\|^2 - \mu_{\|\mathbf{X}\|^2} \leq - \alpha
\right]
\leq 
\left(\frac{\sigma_{\|\mathbf{X}\|^2}}{\alpha}\right)^2.
\]</div>
<p>That is, using the formulas above and rearranging slightly,</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}\left[
\|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
\right]
\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2.
\]</div>
<p>How do we relate this to the probability of interest
<span class="math notranslate nohighlight">\(\mathbb{P}\left[\|\mathbf{X}\|^2 \leq 1/4\right]\)</span>? Recall that we are free to choose <span class="math notranslate nohighlight">\(\alpha\)</span> in this inequality. So simply take <span class="math notranslate nohighlight">\(\alpha\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
\tilde\mu \,d - \alpha = \frac{1}{4},
\]</div>
<p>that is, <span class="math notranslate nohighlight">\(\alpha = \tilde\mu \,d - 1/4\)</span>. Observe that, once <span class="math notranslate nohighlight">\(d\)</span> is large enough, it holds that <span class="math notranslate nohighlight">\(\alpha &gt; 0\)</span>.</p>
<p>Finally, replacing this choice of <span class="math notranslate nohighlight">\(\alpha\)</span> in the inequality above gives</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{P}
\left[
\|\mathbf{X}\| \leq 1/2
\right]
&amp;= \mathbb{P}\left[\|\mathbf{X}\|^2 \leq 1/4\right]\\
&amp;=
\mathbb{P}\left[
\|\mathbf{X}\|^2 \leq \tilde\mu \, d - \alpha
\right]\\
&amp;\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\alpha}\right)^2\\
&amp;\leq 
\left(\frac{\tilde\sigma \, \sqrt{d}}{\tilde\mu \,d - 1/4}\right)^2.
\end{align*}\]</div>
<p>Critically, <span class="math notranslate nohighlight">\(\tilde\mu\)</span> and <span class="math notranslate nohighlight">\(\tilde\sigma\)</span> do not depend on <span class="math notranslate nohighlight">\(d\)</span>. So the right-hand side goes to <span class="math notranslate nohighlight">\(0\)</span> as <span class="math notranslate nohighlight">\(d \to +\infty\)</span>. Indeed, <span class="math notranslate nohighlight">\(d\)</span> is much larger than <span class="math notranslate nohighlight">\(\sqrt{d}\)</span> when <span class="math notranslate nohighlight">\(d\)</span> is large. That proves the claim.<span class="math notranslate nohighlight">\(\square\)</span></p>
<p>We will see later in the course that this high-dimensional phenomenon has implications for data science problems. It is behind what is referred to as the <a class="reference external" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of Dimensionality</a><span class="math notranslate nohighlight">\(\idx{curse of dimensionality}\xdi\)</span>.</p>
<p>While <em>Chebyshev’s inequality</em> correctly implies that <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span>, it does not give the correct rate of convergence. In reality, that probability goes to <span class="math notranslate nohighlight">\(0\)</span> at a much faster rate than <span class="math notranslate nohighlight">\(1/d\)</span>. Specifically, <a class="reference external" href="https://en.wikipedia.org/wiki/Volume_of_an_n-ball#High_dimensions">it can be shown</a> that <span class="math notranslate nohighlight">\(\mathbb{P}[\mathbf{X} \in \mathcal{B}]\)</span> goes to <span class="math notranslate nohighlight">\(0\)</span> roughly as <span class="math notranslate nohighlight">\(d^{-d/2}\)</span>. We will not derive (or need) this fact here.</p>
<p><strong>NUMERICAL CORNER:</strong> We can check the theorem in a simulation. Here we pick <span class="math notranslate nohighlight">\(n\)</span> points uniformly at random in the <span class="math notranslate nohighlight">\(d\)</span>-cube <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>, for a range of dimensions up to <code class="docutils literal notranslate"><span class="pre">dmax</span></code>. We then plot the frequency of landing in the inscribed <span class="math notranslate nohighlight">\(d\)</span>-ball <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> and see that it rapidly converges to <span class="math notranslate nohighlight">\(0\)</span>. Alternatively, we could just plot the formula for the volume of <span class="math notranslate nohighlight">\(\mathcal{B}\)</span>. But knowing how to do simulations is useful in situations where explicit formulas are unavailable or intractable. We plot the result up to dimension <span class="math notranslate nohighlight">\(10\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">dmax</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span>

<span class="n">in_ball</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">dmax</span><span class="p">)</span>
<span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dmax</span><span class="p">):</span>
    <span class="n">in_ball</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([(</span><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">dmax</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">in_ball</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/d08f6e8ae99d1fc47a60ae906d9afb4fb92b64d9611fda98fd7415d2bc98a67d.png" src="../Images/c7ed85ca4cdd1727d357fd1139869726.png" data-original-src="https://mmids-textbook.github.io/_images/d08f6e8ae99d1fc47a60ae906d9afb4fb92b64d9611fda98fd7415d2bc98a67d.png"/>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> The volume of the <span class="math notranslate nohighlight">\(d\)</span>-dimensional cube <span class="math notranslate nohighlight">\(C = [-1/2, 1/2]^d\)</span> is:</p>
<p>a) <span class="math notranslate nohighlight">\(1/d\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(1/2^d\)</span></p>
<p>c) 1</p>
<p>d) <span class="math notranslate nohighlight">\(2^d\)</span></p>
<p><strong>2</strong> In a high-dimensional cube <span class="math notranslate nohighlight">\(C = [-1/2, 1/2]^d\)</span>, as the dimension <span class="math notranslate nohighlight">\(d\)</span> increases, the probability that a randomly chosen point lies within the inscribed sphere <span class="math notranslate nohighlight">\(B = \{x \in \mathbb{R}^d : \|x\| \le 1/2\}\)</span>:</p>
<p>a) Approaches 1</p>
<p>b) Approaches 1/2</p>
<p>c) Approaches 0</p>
<p>d) Remains constant</p>
<p><strong>3</strong> Which of the following best describes the appearance of a high-dimensional cube?</p>
<p>a) A smooth, round ball</p>
<p>b) A spiky ball with most of its volume concentrated in the corners</p>
<p>c) A perfect sphere with uniform volume distribution</p>
<p>d) A flat, pancake-like shape</p>
<p><strong>4</strong> Which inequality is used to prove the theorem about high-dimensional cubes?</p>
<p>a) Cauchy-Schwarz inequality</p>
<p>b) Triangle inequality</p>
<p>c) Markov’s inequality</p>
<p>d) Chebyshev’s inequality</p>
<p><strong>5</strong> In the proof of the theorem about high-dimensional cubes, which property of the squared norm <span class="math notranslate nohighlight">\(\|X\|^2\)</span> is used?</p>
<p>a) It is a sum of dependent random variables.</p>
<p>b) It is a sum of independent random variables.</p>
<p>c) It is a product of independent random variables.</p>
<p>d) It is a product of dependent random variables.</p>
<p>Answer for 1: c. Justification: The side length of the cube is 1, and the volume of a <span class="math notranslate nohighlight">\(d\)</span>-dimensional cube is the side length raised to the power <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>Answer for 2: c. Justification: This is the statement of the theorem “High-dimensional Cube” in the text.</p>
<p>Answer for 3: b. Justification: The text mentions, “A geometric interpretation is that a high-dimensional cube is a bit like a ‘spiky ball.’”</p>
<p>Answer for 4: d. Justification: The text explicitly states that Chebyshev’s inequality is used in the proof.</p>
<p>Answer for 5: b. Justification: The proof states, “we work with the squared norm</p>
<div class="math notranslate nohighlight">
\[
\|\mathbf{X}\|^2 = X_1^2 + X_2^2 + \cdots + X_d^2,
\]</div>
<p>which has the advantage of being a sum of independent random variables.”</p>
    
</body>
</html>