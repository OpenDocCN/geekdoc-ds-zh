["```cpp\nfor (i=0; i<31; i++)\n{\n packed_result |= (pack_array[i] << i);\n}\n```", "```cpp\nID:0 GeForce GTX 470:Reg. version faster by: 2.22ms (Reg=0.26ms, GMEM=2.48ms)\nID:1 GeForce 9800 GT:Reg. version faster by: 52.87ms (Reg=9.27ms, GMEM=62.14ms)\nID:2 GeForce GTX 260:Reg. version faster by: 5.00ms (Reg=0.62ms, GMEM=5.63ms)\nID:3 GeForce GTX 460:Reg. version faster by: 1.56ms (Reg=0.34ms, GMEM=1.90ms)\n```", "```cpp\n__global__ void test_gpu_register(u32 ∗ const data, const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n if (tid < num_elements)\n {\n  u32 d_tmp = 0;\n\n  for (int i=0;i<KERNEL_LOOP;i++)\n  {\n   d_tmp |= (packed_array[i] << i);\n```", "```cpp\n\n  data[tid] = d_tmp;\n }\n}\n\n__device__ static u32 d_tmp = 0;\n__global__ void test_gpu_gmem(u32 ∗ const data, const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n if (tid < num_elements)\n {\n  for (int i=0;i<KERNEL_LOOP;i++)\n  {\n   d_tmp |= (packed_array[i] << i);\n  }\n\n  data[tid] = d_tmp;\n }\n}\n```", "```cpp\n__device__ static u32 d_tmp = 0;\n```", "```cpp\n__device__ static u32 d_tmp[NUM_ELEM];\n```", "```cpp\n__global__ void test_gpu_register(u32 ∗ const data, const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n if (tid < num_elements)\n {\n  u32 d_tmp = 0;\n\n  for (int i=0;i<KERNEL_LOOP;i++)\n  {\n   d_tmp |= (packed_array[i] << i);\n  }\n\n  data[tid] = d_tmp;\n }\n}\n```", "```cpp\n .entry _Z18test_gpu_register1Pjj (\n  .param .u64 __cudaparm__Z18test_gpu_register1Pjj_data,\n  .param .u32 __cudaparm__Z18test_gpu_register1Pjj_num_elements)\n{\n .reg .u32 %r<27>;\n .reg .u64 %rd<9>;\n .reg .pred %p<5>;\n // __cuda_local_var_108903_15_non_const_tid = 0\n // __cuda_local_var_108906_13_non_const_d_tmp = 4\n // i = 8\n .loc 16 36 0\n$LDWbegin__Z18test_gpu_register1Pjj:\n$LDWbeginblock_180_1:\n .loc 16 38 0\n mov.u32  %r1, %tid.x;\n mov.u32  %r2, %ctaid.x;\n mov.u32  %r3, %ntid.x;\n mul.lo.u32  %r4, %r2, %r3;\n add.u32  %r5, %r1, %r4;\n mov.s32  %r6, %r5;\n .loc 16 39 0\n ld.param.u32  %r7, [__cudaparm__Z18test_gpu_register1Pjj_num_elements];\n mov.s32  %r8, %r6;\n setp.le.u32  %p1, %r7, %r8;\n @%p1 bra  $L_0_3074;\n$LDWbeginblock_180_3:\n .loc 16 41 0\n mov.u32  %r9, 0;\n mov.s32  %r10, %r9;\n$LDWbeginblock_180_5:\n .loc 16 43 0\n mov.s32  %r11, 0;\n mov.s32  %r12, %r11;\n mov.s32  %r13, %r12;\n mov.u32  %r14, 31;\n setp.gt.s32  %p2, %r13, %r14;\n @%p2 bra  $L_0_3586;\n$L_0_3330:\n .loc 16 45 0\n mov.s32  %r15, %r12;\n cvt.s64.s32  %rd1, %r15;\n cvta.global.u64  %rd2, packed_array;\n add.u64  %rd3, %rd1, %rd2;\n ld.s8  %r16, [%rd3+0];\n mov.s32  %r17, %r12;\n shl.b32  %r18, %r16, %r17;\n mov.s32  %r19, %r10;\n```", "```cpp\n mov.s32  %r10, %r20;\n .loc 16 43 0\n mov.s32  %r21, %r12;\n add.s32  %r22, %r21, 1;\n mov.s32  %r12, %r22;\n$Lt_0_1794:\n mov.s32  %r23, %r12;\n mov.u32  %r24, 31;\n setp.le.s32  %p3, %r23, %r24;\n @%p3 bra  $L_0_3330;\n$L_0_3586:\n$LDWendblock_180_5:\n .loc 16 48 0\n mov.s32  %r25, %r10;\n ld.param.u64  %rd4, [__cudaparm__Z18test_gpu_register1Pjj_data];\n cvt.u64.u32  %rd5, %r6;\n mul.wide.u32  %rd6, %r6, 4;\n add.u64  %rd7, %rd4, %rd6;\n st.global.u32  [%rd7+0], %r25;\n$LDWendblock_180_3:\n$L_0_3074:\n$LDWendblock_180_1:\n .loc 16 50 0\n exit;\n$LDWend__Z18test_gpu_register1Pjj:\n }\n```", "```cpp\n  d_tmp |= (packed_array[i] << i);\n```", "```cpp\n cvt.s64.s32  %rd1, %r15;\n cvta.global.u64  %rd2, packed_array;\n add.u64  %rd3, %rd1, %rd2;\n```", "```cpp\n .entry _Z18test_gpu_register2Pjj (\n  .param .u64 __cudaparm__Z18test_gpu_register2Pjj_data,\n  .param .u32 __cudaparm__Z18test_gpu_register2Pjj_num_elements)\n {\n .reg .u32 %r<104>;\n```", "```cpp\n .reg .pred %p<3>;\n // __cuda_local_var_108919_15_non_const_tid = 0\n .loc 16 52 0\n$LDWbegin__Z18test_gpu_register2Pjj:\n$LDWbeginblock_181_1:\n .loc 16 54 0\n mov.u32  %r1, %tid.x;\n mov.u32  %r2, %ctaid.x;\n mov.u32  %r3, %ntid.x;\n mul.lo.u32  %r4, %r2, %r3;\n add.u32  %r5, %r1, %r4;\n mov.s32  %r6, %r5;\n .loc 16 55 0\n ld.param.u32  %r7, [__cudaparm__Z18test_gpu_register2Pjj_num_elements];\n mov.s32  %r8, %r6;\n setp.le.u32  %p1, %r7, %r8;\n @%p1 bra  $L_1_1282;\n .loc 16 57 0\n ld.global.s8  %r9, [packed_array+0];\n ld.global.s8  %r10, [packed_array+1];\n shl.b32  %r11, %r10, 1;\n or.b32  %r12, %r9, %r11;\n ld.global.s8  %r13, [packed_array+2];\n shl.b32  %r14, %r13, 2;\n or.b32  %r15, %r12, %r14;\n\n[Repeated code for pack_array+3 to packed_array+29 removed for clarity]\n\n ld.global.s8  %r97, [packed_array+30];\n shl.b32  %r98, %r97, 30;\n or.b32  %r99, %r96, %r98;\n ld.global.s8  %r100, [packed_array+31];\n shl.b32  %r101, %r100, 31;\n or.b32  %r102, %r99, %r101;\n ld.param.u64  %rd1, [__cudaparm__Z18test_gpu_register2Pjj_data];\n cvt.u64.u32  %rd2, %r6;\n mul.wide.u32  %rd3, %r6, 4;\n add.u64  %rd4, %rd1, %rd3;\n st.global.u32  [%rd4+0], %r102;\n$L_1_1282:\n$LDWendblock_181_1:\n .loc 16 90 0\n exit;\n$LDWend__Z18test_gpu_register2Pjj:\n }\n```", "```cpp\n{ 122, 10, 2, 1, 2, 22, 12, 9 }\n```", "```cpp\n122 = 01111010\n 10 = 00001010\n 2 = 00000010\n 22 = 00010010\n 12 = 00001100\n```", "```cpp\n0 = { 122, 10, 2, 22, 12 }\n1 = { 9 }\n```", "```cpp\n{ 122, 10, 2, 22, 12, 9 }\n```", "```cpp\n0 = { 12, 9 }\n1 = { 122, 10, 2, 22 }\n```", "```cpp\n{ 12, 9, 122, 10, 2, 22 }\n```", "```cpp\n0 = { 9, 122, 10, 2, 22 }\n1 = { 12 }\n= { 9, 122, 10, 2, 22, 12 }\n```", "```cpp\n__host__ void cpu_sort(u32 ∗ const data,\n                       const u32 num_elements)\n{\n static u32 cpu_tmp_0[NUM_ELEM];\n static u32 cpu_tmp_1[NUM_ELEM];\n\n for (u32 bit=0;bit<32;bit++)\n {\n  u32 base_cnt_0 = 0;\n  u32 base_cnt_1 = 0;\n\n  for (u32 i=0; i<num_elements; i++)\n  {\n   const u32 d = data[i];\n   const u32 bit_mask = (1 << bit);\n\n   if ( (d & bit_mask) > 0 )\n```", "```cpp\n    cpu_tmp_1[base_cnt_1] = d;\n    base_cnt_1++;\n   }\n   else\n   {\n    cpu_tmp_0[base_cnt_0] = d;\n    base_cnt_0++;\n   }\n  }\n\n  // Copy data back to source - first the zero list\n  for (u32 i=0; i<base_cnt_0; i++)\n  {\n   data[i] = cpu_tmp_0[i];\n  }\n\n  // Copy data back to source - then the one list\n  for (u32 i=0; i<base_cnt_1; i++)\n  {\n   data[base_cnt_0+i] = cpu_tmp_1[i];\n  }\n }\n}\n```", "```cpp\n__device__ void radix_sort(u32 ∗ const sort_tmp,\n                           const u32 num_lists,\n                           const u32 num_elements,\n                           const u32 tid,\n                           u32 ∗ const sort_tmp_0,\n                           u32 ∗ const sort_tmp_1)\n{\n // Sort into num_list, lists\n // Apply radix sort on 32 bits of data\n for (u32 bit=0;bit<32;bit++)\n {\n  u32 base_cnt_0 = 0;\n  u32 base_cnt_1 = 0;\n\n  for (u32 i=0; i<num_elements; i+=num_lists)\n  {\n   const u32 elem = sort_tmp[i+tid];\n   const u32 bit_mask = (1 << bit);\n\n   if ( (elem & bit_mask) > 0 )\n   {\n    sort_tmp_1[base_cnt_1+tid] = elem;\n    base_cnt_1+=num_lists;\n   }\n   else\n   {\n    sort_tmp_0[base_cnt_0+tid] = elem;\n    base_cnt_0+=num_lists;\n   }\n  }\n\n  // Copy data back to source - first the zero list\n  for (u32 i=0; i<base_cnt_0; i+=num_lists)\n  {\n   sort_tmp[i+tid] = sort_tmp_0[i+tid];\n  }\n\n  // Copy data back to source - then the one list\n  for (u32 i=0; i<base_cnt_1; i+=num_lists)\n  {\n   sort_tmp[base_cnt_0+i+tid] = sort_tmp_1[i+tid];\n  }\n }\n __syncthreads();\n}\n```", "```cpp\n__device__ void radix_sort2(u32 ∗ const sort_tmp,\n                            const u32 num_lists,\n                            const u32 num_elements,\n       const u32 tid,\n       u32 ∗ const sort_tmp_1)\n{\n // Sort into num_list, lists\n // Apply radix sort on 32 bits of data\n for (u32 bit=0;bit<32;bit++)\n {\n  const u32 bit_mask = (1 << bit);\n  u32 base_cnt_0 = 0;\n  u32 base_cnt_1 = 0;\n\n  for (u32 i=0; i<num_elements; i+=num_lists)\n  {\n   const u32 elem = sort_tmp[i+tid];\n\n   if ( (elem & bit_mask) > 0 )\n   {\n    sort_tmp_1[base_cnt_1+tid] = elem;\n    base_cnt_1+=num_lists;\n   }\n   else\n   {\n    sort_tmp[base_cnt_0+tid] = elem;\n    base_cnt_0+=num_lists;\n   }\n  }\n\n```", "```cpp\n  for (u32 i=0; i<base_cnt_1; i+=num_lists)\n  {\n   sort_tmp[base_cnt_0+i+tid] = sort_tmp_1[i+tid];\n  }\n }\n __syncthreads();\n}\n```", "```cpp\nvoid merge_array(const u32 ∗ const src_array,\n                 u32 ∗ const dest_array,\n                 const u32 num_lists,\n                 const u32 num_elements)\n{\n const u32 num_elements_per_list = (num_elements / num_lists);\n\n u32 list_indexes[MAX_NUM_LISTS];\n\n for (u32 list=0; list < num_lists; list++)\n {\n  list_indexes[list] = 0;\n }\n\n```", "```cpp\n {\n  dest_array[i] = find_min(src_array,\n                           list_indexes,\n                           num_lists,\n                           num_elements_per_list);\n }\n}\n```", "```cpp\nu32 find_min(const u32 ∗ const src_array,\n             u32 ∗ const list_indexes,\n             const u32 num_lists,\n             const u32 num_elements_per_list)\n{\n u32 min_val = 0xFFFFFFFF;\n u32 min_idx = 0;\n\n // Iterate over each of the lists\n for (u32 i=0; i<num_lists; i++)\n {\n  // If the current list has already been emptied\n  // then ignore it\n  if (list_indexes[i] < num_elements_per_list)\n  {\n   const u32 src_idx = i + (list_indexes[i] ∗ num_lists);\n\n   const u32 data = src_array[src_idx];\n\n   if (data <= min_val)\n   {\n     min_val = data;\n     min_idx = i;\n   }\n  }\n }\n\n list_indexes[min_idx]++;\n return min_val;\n}\n```", "```cpp\n__global__ void gpu_sort_array_array(\n u32 ∗ const data,\n const u32 num_lists,\n const u32 num_elements)\n{\n```", "```cpp\n __shared__ u32 sort_tmp[NUM_ELEM];\n __shared__ u32 sort_tmp_1[NUM_ELEM];\n\n copy_data_to_shared(data, sort_tmp, num_lists,\n                     num_elements, tid);\n\n radix_sort2(sort_tmp, num_lists, num_elements,\n             tid, sort_tmp_1);\n\n merge_array6(sort_tmp, data, num_lists,\n              num_elements, tid);\n}\n```", "```cpp\n__device__ void copy_data_to_shared(const u32 ∗ const data,\n                                    u32 ∗ const sort_tmp,\n                                    const u32 num_lists,\n                                    const u32 num_elements,\n                                    const u32 tid)\n{\n // Copy data into temp store\n for (u32 i=0; i<num_elements; i+=num_lists)\n {\n  sort_tmp[i+tid] = data[i+tid];\n }\n __syncthreads();\n}\n```", "```cpp\n1>ptxas info : Function properties for _Z12merge_arrayPKjPjjjj\n1> 40 bytes stack frame, 40 bytes spill stores, 40 bytes spill loads\n```", "```cpp\n                         list_indexes,\n                         num_lists,\n                         num_elements_per_list);\n```", "```cpp\n// Uses a single thread for merge\n__device__ void merge_array1(const u32 ∗ const src_array,\n                             u32 ∗ const dest_array,\n                             const u32 num_lists,\n                             const u32 num_elements,\n                             const u32 tid)\n{\n __shared__ u32 list_indexes[MAX_NUM_LISTS];\n\n // Multiple threads\n list_indexes[tid] = 0;\n __syncthreads();\n\n // Single threaded\n if (tid == 0)\n {\n  const u32 num_elements_per_list = (num_elements / num_lists);\n\n  for (u32 i=0; i<num_elements;i++)\n  {\n   u32 min_val = 0xFFFFFFFF;\n   u32 min_idx = 0;\n\n   // Iterate over each of the lists\n   for (u32 list=0; list<num_lists; list++)\n   {\n    // If the current list has already been\n    // emptied then ignore it\n    if (list_indexes[list] < num_elements_per_list)\n    {\n     const u32 src_idx = list + (list_indexes[list] ∗ num_lists);\n\n```", "```cpp\n     if (data <= min_val)\n     {\n      min_val = data;\n      min_idx = list;\n     }\n    }\n   }\n   list_indexes[min_idx]++;\n   dest_array[i] = min_val;\n  }\n }\n}\n```", "```cpp\n// Uses multiple threads for merge\n// Deals with multiple identical entries in the data\n__device__ void merge_array6(const u32 ∗ const src_array,\n                             u32 ∗ const dest_array,\n                             const u32 num_lists,\n                             const u32 num_elements,\n                             const u32 tid)\n{\n const u32 num_elements_per_list = (num_elements / num_lists);\n\n __shared__ u32 list_indexes[MAX_NUM_LISTS];\n list_indexes[tid] = 0;\n\n```", "```cpp\n __syncthreads();\n\n // Iterate over all elements\n for (u32 i=0; i<num_elements;i++)\n {\n  // Create a value shared with the other threads\n  __shared__ u32 min_val;\n  __shared__ u32 min_tid;\n\n  // Use a temp register for work purposes\n  u32 data;\n\n  // If the current list has not already been\n  // emptied then read from it, else ignore it\n  if (list_indexes[tid] < num_elements_per_list)\n  {\n   // Work out from the list_index, the index into\n   // the linear array\n   const u32 src_idx = tid + (list_indexes[tid] ∗ num_lists);\n\n   // Read the data from the list for the given\n   // thread\n   data = src_array[src_idx];\n  }\n  else\n  {\n   data = 0xFFFFFFFF;\n  }\n\n  // Have thread zero clear the min values\n  if (tid == 0)\n  {\n   // Write a very large value so the first\n   // thread thread wins the min\n   min_val = 0xFFFFFFFF;\n   min_tid = 0xFFFFFFFF;\n  }\n\n  // Wait for all threads\n  __syncthreads();\n\n  // Have every thread try to store it’s value into\n  // min_val. Only the thread with the lowest value\n  // will win\n  atomicMin(&min_val, data);\n\n  // Make sure all threads have taken their turn.\n```", "```cpp\n\n  // If this thread was the one with the minimum\n  if (min_val == data)\n  {\n   // Check for equal values\n   // Lowest tid wins and does the write\n   atomicMin(&min_tid, tid);\n  }\n\n  // Make sure all threads have taken their turn.\n  __syncthreads();\n\n  // If this thread has the lowest tid\n  if (tid == min_tid)\n  {\n   // Incremene the list pointer for this thread\n   list_indexes[tid]++;\n\n   // Store the winning value\n   dest_array[i] = data;\n  }\n }\n}\n```", "```cpp\n// Uses multiple threads for reduction type merge\n__device__ void merge_array5(const u32 ∗ const src_array,\n                             u32 ∗ const dest_array,\n                             const u32 num_lists,\n                             const u32 num_elements,\n                             const u32 tid)\n{\n const u32 num_elements_per_list = (num_elements / num_lists);\n\n __shared__ u32 list_indexes[MAX_NUM_LISTS];\n __shared__ u32 reduction_val[MAX_NUM_LISTS];\n __shared__ u32 reduction_idx[MAX_NUM_LISTS];\n\n // Clear the working sets\n list_indexes[tid] = 0;\n reduction_val[tid] = 0;\n reduction_idx[tid] = 0;\n __syncthreads();\n\n for (u32 i=0; i<num_elements;i++)\n {\n  // We need (num_lists / 2) active threads\n  u32 tid_max = num_lists >> 1;\n\n  u32 data;\n\n  // If the current list has already been\n```", "```cpp\n  if (list_indexes[tid] < num_elements_per_list)\n  {\n   // Work out from the list_index, the index into\n   // the linear array\n   const u32 src_idx = tid + (list_indexes[tid] ∗ num_lists);\n\n   // Read the data from the list for the given\n   // thread\n   data = src_array[src_idx];\n  }\n  else\n  {\n   data = 0xFFFFFFFF;   \n  }\n\n  // Store the current data value and index\n  reduction_val[tid] = data;\n  reduction_idx[tid] = tid;\n\n  // Wait for all threads to copy\n  __syncthreads();\n\n  // Reduce from num_lists to one thread zero\n  while (tid_max != 0)\n  {\n   // Gradually reduce tid_max from\n   // num_lists to zero\n   if (tid < tid_max)\n   {\n    // Calculate the index of the other half\n    const u32 val2_idx = tid + tid_max;\n\n    // Read in the other half\n    const u32 val2 = reduction_val[val2_idx];\n\n    // If this half is bigger\n    if (reduction_val[tid] > val2)\n    {\n     // The store the smaller value\n     reduction_val[tid] = val2;\n     reduction_idx[tid] = reduction_idx[val2_idx];\n    }\n   }\n\n   // Divide tid_max by two\n   tid_max >>= 1;\n\n```", "```cpp\n  }\n\n  if (tid == 0)\n  {\n   // Incremenet the list pointer for this thread\n   list_indexes[reduction_idx[0]]++;\n\n   // Store the winning value\n   dest_array[i] = reduction_val[0];\n  }\n\n  // Wait for tid zero\n  __syncthreads();\n }\n}\n```", "```cpp\n#define REDUCTION_SIZE 8\n#define REDUCTION_SIZE_BIT_SHIFT 3\n#define MAX_ACTIVE_REDUCTIONS ( (MAX_NUM_LISTS) / REDUCTION_SIZE )\n\n// Uses multiple threads for merge\n// Does reduction into a warp and then into a single value\n__device__ void merge_array9(const u32 ∗ const src_array,\n                             u32 ∗ const dest_array,\n                             const u32 num_lists,\n                             const u32 num_elements,\n                             const u32 tid)\n{\n // Read initial value from the list\n u32 data = src_array[tid];\n\n // Shared memory index\n const u32 s_idx = tid >> REDUCTION_SIZE_BIT_SHIFT;\n\n // Calcuate number of 1st stage reductions\n```", "```cpp\n\n const u32 num_elements_per_list = (num_elements / num_lists);\n\n // Declare a number of list pointers and\n // set to the start of the list\n __shared__ u32 list_indexes[MAX_NUM_LISTS];\n list_indexes[tid] = 0;\n\n // Iterate over all elements\n for (u32 i=0; i<num_elements;i++)\n {\n  // Create a value shared with the other threads\n  __shared__ u32 min_val[MAX_ACTIVE_REDUCTIONS];\n  __shared__ u32 min_tid;\n\n  // Have one thread from warp zero clear the\n  // min value\n  if (tid < num_lists)\n  {\n   // Write a very large value so the first\n   // thread thread wins the min\n   min_val[s_idx] = 0xFFFFFFFF;\n   min_tid = 0xFFFFFFFF;\n  }\n\n  // Wait for warp zero to clear min vals\n  __syncthreads();\n\n  // Have every thread try to store it’s value into\n  // min_val for it’s own reduction elements. Only\n  // the thread with the lowest value will win.\n  atomicMin(&min_val[s_idx], data);\n\n  // If we have more than one reduction then\n  // do an additional reduction step\n  if (num_reductions > 0)\n  {\n   // Wait for all threads\n   __syncthreads();\n\n   // Have each thread in warp zero do an\n   // additional min over all the partial\n   // mins to date\n   if ( (tid < num_reductions) )\n   {\n    atomicMin(&min_val[0], min_val[tid]);\n```", "```cpp\n\n   // Make sure all threads have taken their turn.\n   __syncthreads();\n  }\n\n  // If this thread was the one with the minimum\n  if (min_val[0] == data)\n  {\n   // Check for equal values\n   // Lowest tid wins and does the write\n   atomicMin(&min_tid, tid);\n  }\n\n  // Make sure all threads have taken their turn.\n  __syncthreads();\n\n  // If this thread has the lowest tid\n  if (tid == min_tid)\n  {\n   // Incremenet the list pointer for this thread\n   list_indexes[tid]++;\n\n   // Store the winning value\n   dest_array[i] = data;\n\n   // If the current list has not already been\n   // emptied then read from it, else ignore it\n   if (list_indexes[tid] < num_elements_per_list)\n    data = src_array[tid + (list_indexes[tid] ∗ num_lists)];\n   else\n    data = 0xFFFFFFFF;   \n  }\n\n  // Wait for min_tid thread\n  __syncthreads();\n }\n}\n```", "```cpp\nstruct cudaDeviceProp device_prop;\nCUDA_CALL(cudaGetDeviceProperties(&device_prop, device_num));\nprintf(\"\\nSharedMemory: %u\", device_prop.sharedMemPerBlock);\n```", "```cpp\n___constant__ float my_array[1024] = { 0.0F, 1.0F, 1.34F, … };\n```", "```cpp\n#include \"const_common.h\"\n#include \"stdio.h\"\n#include \"conio.h\"\n#include \"assert.h\"\n\n#define CUDA_CALL(x) {const cudaError_t a = (x); if (a != cudaSuccess) { printf(\"\\nCUDA Error: %s (err_num=%d) \\n\", cudaGetErrorString(a), a); cudaDeviceReset(); assert(0);} }\n#define KERNEL_LOOP 65536\n\n__constant__ static const u32 const_data_01 = 0x55555555;\n__constant__ static const u32 const_data_02 = 0x77777777;\n__constant__ static const u32 const_data_03 = 0x33333333;\n__constant__ static const u32 const_data_04 = 0x11111111;\n\n__global__ void const_test_gpu_literal(u32 ∗ const data, const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n if (tid < num_elements)\n {\n  u32 d = 0x55555555;\n\n  for (int i=0;i<KERNEL_LOOP;i++)\n  {\n   d ^= 0x55555555;\n   d |= 0x77777777;\n   d &= 0x33333333;\n   d |= 0x11111111;\n  }\n\n  data[tid] = d;\n }\n}\n\n__global__ void const_test_gpu_const(u32 ∗ const data, const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n if (tid < num_elements)\n {\n  u32 d = const_data_01;\n\n  for (int i=0;i<KERNEL_LOOP;i++)\n  {\n   d ^= const_data_01;\n   d |= const_data_02;\n   d &= const_data_03;\n   d |= const_data_04;\n```", "```cpp\n  data[tid] = d;\n }\n}\n\n__host__ void wait_exit(void)\n{\n char ch;\n\n printf(\"\\nPress any key to exit\");\n ch = getch();\n}\n\n__host__ void cuda_error_check(\n const char ∗ prefix,\n const char ∗ postfix)\n{\n if (cudaPeekAtLastError() != cudaSuccess)\n {\n  printf(\"\\n%s%s%s\", prefix, cudaGetErrorString(cudaGetLastError()), postfix);\n  cudaDeviceReset();\n  wait_exit();\n  exit(1);\n }\n}\n\n__host__ void gpu_kernel(void)\n{\n const u32 num_elements = (128∗1024);\n const u32 num_threads = 256;\n const u32 num_blocks = (num_elements+(num_threads-1)) / num_threads;\n const u32 num_bytes = num_elements ∗ sizeof(u32);\n int max_device_num;\n const int max_runs = 6;\n\n CUDA_CALL(cudaGetDeviceCount(&max_device_num));\n\n for (int device_num=0; device_num < max_device_num; device_num++)\n {\n  CUDA_CALL(cudaSetDevice(device_num));\n\n  for (int num_test=0;num_test < max_runs; num_test++)\n  {\n   u32 ∗ data_gpu;\n   cudaEvent_t kernel_start1, kernel_stop1;\n   cudaEvent_t kernel_start2, kernel_stop2;\n   float delta_time1 = 0.0F, delta_time2=0.0F;\n   struct cudaDeviceProp device_prop;\n```", "```cpp\n\n   CUDA_CALL(cudaMalloc(&data_gpu, num_bytes));\n   CUDA_CALL(cudaEventCreate(&kernel_start1));\n   CUDA_CALL(cudaEventCreate(&kernel_start2));\n   CUDA_CALL(cudaEventCreateWithFlags(&kernel_stop1, cudaEventBlockingSync));\n   CUDA_CALL(cudaEventCreateWithFlags(&kernel_stop2, cudaEventBlockingSync));\n\n   // printf(\"\\nLaunching %u blocks, %u threads\", num_blocks, num_threads);\n   CUDA_CALL(cudaGetDeviceProperties(&device_prop, device_num));\n   sprintf(device_prefix, \"ID:%d %s:\", device_num, device_prop.name);\n\n   // Warm up run\n   // printf(\"\\nLaunching literal kernel warm-up\");\n   const_test_gpu_literal <<<num_blocks, num_threads>>>(data_gpu, num_elements);\n\n   cuda_error_check(\"Error \", \" returned from literal startup kernel\");\n\n   // Do the literal kernel\n   // printf(\"\\nLaunching literal kernel\");\n   CUDA_CALL(cudaEventRecord(kernel_start1,0));\n   const_test_gpu_literal <<<num_blocks, num_threads>>>(data_gpu, num_elements);\n\n   cuda_error_check(\"Error \", \" returned from literal runtime kernel\");\n\n   CUDA_CALL(cudaEventRecord(kernel_stop1,0));\n   CUDA_CALL(cudaEventSynchronize(kernel_stop1));\n   CUDA_CALL(cudaEventElapsedTime(&delta_time1, kernel_start1, kernel_stop1));\n   //  printf(\"\\nLiteral Elapsed time: %.3fms\", delta_time1);\n\n   // Warm up run\n   // printf(\"\\nLaunching constant kernel warm-up\");\n   const_test_gpu_const <<<num_blocks, num_threads>>>(data_gpu, num_elements);\n\n   cuda_error_check(\"Error \", \" returned from constant startup kernel\");\n\n   // Do the constant kernel\n   // printf(\"\\nLaunching constant kernel\");\n   CUDA_CALL(cudaEventRecord(kernel_start2,0));\n\n   const_test_gpu_const <<<num_blocks, num_threads>>>(data_gpu, num_elements);\n\n   cuda_error_check(\"Error \", \" returned from constant runtime kernel\");\n\n   CUDA_CALL(cudaEventRecord(kernel_stop2,0));\n   CUDA_CALL(cudaEventSynchronize(kernel_stop2));\n   CUDA_CALL(cudaEventElapsedTime(&delta_time2, kernel_start2, kernel_stop2));\n```", "```cpp\n   if (delta_time1 > delta_time2)\n    printf(\"\\n%sConstant version is faster by: %.2fms (Const=%.2fms vs. Literal=%.2fms)\", device_prefix, delta_time1-delta_time2, delta_time1, delta_time2);\n   else\n    printf(\"\\n%sLiteral version is faster by: %.2fms (Const=%.2fms vs. Literal=%.2fms)\", device_prefix, delta_time2-delta_time1, delta_time1, delta_time2);\n\n   CUDA_CALL(cudaEventDestroy(kernel_start1));\n   CUDA_CALL(cudaEventDestroy(kernel_start2));\n   CUDA_CALL(cudaEventDestroy(kernel_stop1));\n   CUDA_CALL(cudaEventDestroy(kernel_stop2));\n   CUDA_CALL(cudaFree(data_gpu));\n  }\n\n  CUDA_CALL(cudaDeviceReset());\n  printf(\"\\n\");\n }\n\n wait_exit();\n}\n```", "```cpp\nID:0 GeForce GTX 470:Constant version is faster by: 0.00ms (C=345.23ms, L=345.23ms)\nID:0 GeForce GTX 470:Constant version is faster by: 0.01ms (C=330.95ms, L=330.94ms)\nID:0 GeForce GTX 470:Literal version is faster by: 0.01ms (C=336.60ms, L=336.60ms)\nID:0 GeForce GTX 470:Constant version is faster by: 5.67ms (C=336.60ms, L=330.93ms)\nID:0 GeForce GTX 470:Constant version is faster by: 5.59ms (C=336.60ms, L=331.01ms)\n```", "```cpp\nID:1 GeForce 9800 GT:Literal version is faster by: 4.04ms (C=574.85ms, L=578.89ms)\nID:1 GeForce 9800 GT:Literal version is faster by: 3.55ms (C=578.18ms, L=581.73ms)\nID:1 GeForce 9800 GT:Literal version is faster by: 4.68ms (C=575.85ms, L=580.53ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 5.25ms (C=581.06ms, L=575.81ms)\nID:1 GeForce 9800 GT:Literal version is faster by: 4.01ms (C=572.08ms, L=576.10ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 8.47ms (C=578.40ms, L=569.93ms)\n\nID:2 GeForce GTX 260:Literal version is faster by: 0.27ms (C=348.74ms, L=349.00ms)\nID:2 GeForce GTX 260:Literal version is faster by: 0.26ms (C=348.72ms, L=348.98ms)\nID:2 GeForce GTX 260:Literal version is faster by: 0.26ms (C=348.74ms, L=349.00ms)\nID:2 GeForce GTX 260:Literal version is faster by: 0.26ms (C=348.74ms, L=349.00ms)\nID:2 GeForce GTX 260:Literal version is faster by: 0.13ms (C=348.83ms, L=348.97ms)\nID:2 GeForce GTX 260:Literal version is faster by: 0.27ms (C=348.73ms, L=348.99ms)\n\nID:3 GeForce GTX 460:Literal version is faster by: 0.59ms (C=541.43ms, L=542.02ms)\nID:3 GeForce GTX 460:Literal version is faster by: 0.17ms (C=541.20ms, L=541.37ms)\nID:3 GeForce GTX 460:Constant version is faster by: 0.45ms (C=542.29ms, L=541.83ms)\nID:3 GeForce GTX 460:Constant version is faster by: 0.27ms (C=542.17ms, L=541.89ms)\nID:3 GeForce GTX 460:Constant version is faster by: 1.17ms (C=543.55ms, L=542.38ms)\nID:3 GeForce GTX 460:Constant version is faster by: 0.24ms (C=542.92ms, L=542.68ms)\n```", "```cpp\n__device__ static u32 data_01 = 0x55555555;\n__device__ static u32 data_02 = 0x77777777;\n__device__ static u32 data_03 = 0x33333333;\n__device__ static u32 data_04 = 0x11111111;\n\n__global__ void const_test_gpu_gmem(u32 ∗ const data, const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n if (tid < num_elements)\n {\n  u32 d = 0x55555555;\n\n  for (int i=0;i<KERNEL_LOOP;i++)\n  {\n   d ^= data_01;\n   d |= data_02;\n   d &= data_03;\n   d |= data_04;\n  }\n\n```", "```cpp\n }\n}\n```", "```cpp\nID:0 GeForce GTX 470:Constant version is faster by: 16.68ms (G=37.38ms, C=20.70ms)\nID:0 GeForce GTX 470:Constant version is faster by: 16.45ms (G=37.50ms, C=21.06ms)\nID:0 GeForce GTX 470:Constant version is faster by: 15.71ms (G=37.30ms, C=21.59ms)\nID:0 GeForce GTX 470:Constant version is faster by: 16.66ms (G=37.36ms, C=20.70ms)\nID:0 GeForce GTX 470:Constant version is faster by: 15.84ms (G=36.55ms, C=20.71ms)\nID:0 GeForce GTX 470:Constant version is faster by: 16.33ms (G=37.39ms, C=21.06ms)\n\nID:1 GeForce 9800 GT:Constant version is faster by: 1427.19ms (G=1463.58ms, C=36.39ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 1425.98ms (G=1462.05ms, C=36.07ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 1426.95ms (G=1463.15ms, C=36.20ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 1426.13ms (G=1462.56ms, C=36.44ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 1427.25ms (G=1463.65ms, C=36.40ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 1427.53ms (G=1463.70ms, C=36.17ms)\n\nID:2 GeForce GTX 260:Constant version is faster by: 54.33ms (G=76.13ms, C=21.81ms)\nID:2 GeForce GTX 260:Constant version is faster by: 54.31ms (G=76.11ms, C=21.80ms)\nID:2 GeForce GTX 260:Constant version is faster by: 54.30ms (G=76.10ms, C=21.80ms)\nID:2 GeForce GTX 260:Constant version is faster by: 54.29ms (G=76.12ms, C=21.83ms)\nID:2 GeForce GTX 260:Constant version is faster by: 54.31ms (G=76.12ms, C=21.81ms)\nID:2 GeForce GTX 260:Constant version is faster by: 54.32ms (G=76.13ms, C=21.80ms)\n\nID:3 GeForce GTX 460:Constant version is faster by: 20.87ms (G=54.85ms, C=33.98ms)\nID:3 GeForce GTX 460:Constant version is faster by: 19.64ms (G=53.57ms, C=33.93ms)\nID:3 GeForce GTX 460:Constant version is faster by: 20.87ms (G=54.86ms, C=33.99ms)\nID:3 GeForce GTX 460:Constant version is faster by: 20.81ms (G=54.77ms, C=33.95ms)\nID:3 GeForce GTX 460:Constant version is faster by: 20.99ms (G=54.87ms, C=33.89ms)\nID:3 GeForce GTX 460:Constant version is faster by: 21.02ms (G=54.93ms, C=33.91ms)\n```", "```cpp\n .const .u32 const_data_01 = 1431655765;\n .const .u32 const_data_02 = 2004318071;\n .const .u32 const_data_03 = 858993459;\n .const .u32 const_data_04 = 286331153;\n\n .entry _Z20const_test_gpu_constPjj (\n  .param .u64 __cudaparm__Z20const_test_gpu_constPjj_data,\n  .param .u32 __cudaparm__Z20const_test_gpu_constPjj_num_elements)\n {\n .reg .u32 %r<29>;\n .reg .u64 %rd<6>;\n .reg .pred %p<5>;\n // __cuda_local_var_108907_15_non_const_tid = 0\n // __cuda_local_var_108910_13_non_const_d = 4\n // i = 8\n .loc 16 40 0\n$LDWbegin__Z20const_test_gpu_constPjj:\n$LDWbeginblock_181_1:\n .loc 16 42 0\n mov.u32  %r1, %tid.x;\n mov.u32  %r2, %ctaid.x;\n mov.u32  %r3, %ntid.x;\n mul.lo.u32  %r4, %r2, %r3;\n add.u32  %r5, %r1, %r4;\n mov.s32  %r6, %r5;\n .loc 16 43 0\n ld.param.u32  %r7, [__cudaparm__Z20const_test_gpu_constPjj_num_elements];\n mov.s32  %r8, %r6;\n setp.le.u32  %p1, %r7, %r8;\n @%p1 bra  $L_1_3074;\n$LDWbeginblock_181_3:\n .loc 16 45 0\n mov.u32  %r9, 1431655765;\n mov.s32  %r10, %r9;\n$LDWbeginblock_181_5:\n .loc 16 47 0\n mov.s32  %r11, 0;\n mov.s32  %r12, %r11;\n mov.s32  %r13, %r12;\n mov.u32  %r14, 4095;\n setp.gt.s32  %p2, %r13, %r14;\n @%p2 bra  $L_1_3586;\n$L_1_3330:\n .loc 16 49 0\n mov.s32  %r15, %r10;\n xor.b32  %r16, %r15, 1431655765;\n mov.s32  %r10, %r16;\n```", "```cpp\n mov.s32  %r17, %r10;\n or.b32  %r18, %r17, 2004318071;\n mov.s32  %r10, %r18;\n .loc 16 51 0\n mov.s32  %r19, %r10;\n and.b32  %r20, %r19, 858993459;\n mov.s32  %r10, %r20;\n .loc 16 52 0\n mov.s32  %r21, %r10;\n or.b32  %r22, %r21, 286331153;\n mov.s32  %r10, %r22;\n .loc 16 47 0\n mov.s32  %r23, %r12;\n add.s32  %r24, %r23, 1;\n mov.s32  %r12, %r24;\n$Lt_1_1794:\n mov.s32  %r25, %r12;\n mov.u32  %r26, 4095;\n setp.le.s32  %p3, %r25, %r26;\n @%p3 bra  $L_1_3330;\n$L_1_3586:\n$LDWendblock_181_5:\n .loc 16 55 0\n mov.s32  %r27, %r10;\n ld.param.u64  %rd1, [__cudaparm__Z20const_test_gpu_constPjj_data];\n cvt.u64.u32  %rd2, %r6;\n mul.wide.u32  %rd3, %r6, 4;\n add.u64  %rd4, %rd1, %rd3;\n st.global.u32  [%rd4+0], %r27;\n$LDWendblock_181_3:\n$L_1_3074:\n$LDWendblock_181_1:\n .loc 16 57 0\n exit;\n$LDWend__Z20const_test_gpu_constPjj:\n } // _Z20const_test_gpu_constPjj\n```", "```cpp\nxor.b32  %r16, %r15, 1431655765;\n```", "```cpp\nld.global.u32  %r16, [data_01];\nxor.b32  %r17, %r15, %r16;\n```", "```cpp\n__constant__ static const u32 const_data[4] = { 0x55555555, 0x77777777, 0x33333333, 0x11111111 };\n\n__global__ void const_test_gpu_const(u32 ∗ const data, const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n if (tid < num_elements)\n {\n  u32 d = const_data[0];\n\n  for (int i=0;i<KERNEL_LOOP;i++)\n  {\n   d ^= const_data[0];\n   d |= const_data[1];\n   d &= const_data[2];\n   d |= const_data[3];\n  }\n\n  data[tid] = d;\n }\n}\n```", "```cpp\n ld.const.u32  %r15, [const_data+0];\n mov.s32  %r16, %r10;\n xor.b32  %r17, %r15, %r16;\n mov.s32  %r10, %r17;\n .loc 16 47 0\n ld.const.u32  %r18, [const_data+4];\n mov.s32  %r19, %r10;\n or.b32  %r20, %r18, %r19;\n mov.s32  %r10, %r20;\n```", "```cpp\nID:0 GeForce GTX 470:Constant version is faster by: 0.34ms (G=36.67ms, C=36.32ms)\nID:0 GeForce GTX 470:Constant version is faster by: 1.11ms (G=37.36ms, C=36.25ms)\nID:0 GeForce GTX 470:GMEM  version is faster by: 0.45ms (G=36.62ms, C=37.07ms)\nID:0 GeForce GTX 470:GMEM  version is faster by: 1.21ms (G=35.86ms, C=37.06ms)\nID:0 GeForce GTX 470:GMEM  version is faster by: 0.63ms (G=36.48ms, C=37.11ms)\nID:0 GeForce GTX 470:Constant version is faster by: 0.23ms (G=37.39ms, C=37.16ms)\n\nID:1 GeForce 9800 GT:Constant version is faster by: 1496.41ms (G=1565.96ms, C=69.55ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 1496.72ms (G=1566.42ms, C=69.71ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 1498.14ms (G=1567.78ms, C=69.64ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 1496.12ms (G=1565.81ms, C=69.69ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 1496.91ms (G=1566.61ms, C=69.70ms)\nID:1 GeForce 9800 GT:Constant version is faster by: 1495.76ms (G=1565.49ms, C=69.73ms)\n\nID:2 GeForce GTX 260:Constant version is faster by: 34.21ms (G=76.12ms, C=41.91ms)\nID:2 GeForce GTX 260:Constant version is faster by: 34.22ms (G=76.13ms, C=41.91ms)\nID:2 GeForce GTX 260:Constant version is faster by: 34.19ms (G=76.10ms, C=41.91ms)\nID:2 GeForce GTX 260:Constant version is faster by: 34.20ms (G=76.11ms, C=41.91ms)\nID:2 GeForce GTX 260:Constant version is faster by: 34.21ms (G=76.12ms, C=41.91ms)\nID:2 GeForce GTX 260:Constant version is faster by: 34.20ms (G=76.12ms, C=41.92ms)\n\nID:3 GeForce GTX 460:GMEM  version is faster by: 0.20ms (G=54.18ms, C=54.38ms)\nID:3 GeForce GTX 460:GMEM  version is faster by: 0.17ms (G=54.86ms, C=55.03ms)\nID:3 GeForce GTX 460:GMEM  version is faster by: 0.25ms (G=54.83ms, C=55.07ms)\nID:3 GeForce GTX 460:GMEM  version is faster by: 0.81ms (G=54.24ms, C=55.05ms)\nID:3 GeForce GTX 460:GMEM  version is faster by: 1.51ms (G=53.54ms, C=55.05ms)\nID:3 GeForce GTX 460:Constant version is faster by: 1.14ms (G=54.83ms, C=53.69ms)\n```", "```cpp\n#include \"stdio.h\"\n#include \"conio.h\"\n#include \"assert.h\"\n\ntypedef unsigned short int u16;\ntypedef unsigned int u32;\n\n```", "```cpp\n\n#define KERNEL_LOOP 4096\n\n__constant__ static const u32 const_data_gpu[KERNEL_LOOP];\n__device__ static  u32 gmem_data_gpu[KERNEL_LOOP];\nstatic u32 const_data_host[KERNEL_LOOP];\n\n__global__ void const_test_gpu_gmem(u32 ∗ const data, const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n if (tid < num_elements)\n {\n  u32 d = gmem_data_gpu[0];\n\n  for (int i=0;i<KERNEL_LOOP;i++)\n  {\n   d ^= gmem_data_gpu[i];\n   d |= gmem_data_gpu[i];\n   d &= gmem_data_gpu[i];\n   d |= gmem_data_gpu[i];\n  }\n\n  data[tid] = d;\n }\n}\n\n__global__ void const_test_gpu_const(u32 ∗ const data, const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n if (tid < num_elements)\n {\n  u32 d = const_data_gpu[0];\n\n  for (int i=0;i<KERNEL_LOOP;i++)\n  {\n   d ^= const_data_gpu[i];\n   d |= const_data_gpu[i];\n   d &= const_data_gpu[i];\n   d |= const_data_gpu[i];\n  }\n\n  data[tid] = d;\n }\n}\n\n__host__ void wait_exit(void)\n{\n```", "```cpp\n\n printf(\"\\nPress any key to exit\");\n ch = getch();\n}\n\n__host__ void cuda_error_check(const char ∗ prefix, const char ∗ postfix)\n{\n if (cudaPeekAtLastError() != cudaSuccess)\n {\n  printf(\"\\n%s%s%s\", prefix, cudaGetErrorString(cudaGetLastError()), postfix);\n  cudaDeviceReset();\n  wait_exit();\n  exit(1);\n }\n}\n\n__host__ void generate_rand_data(u32 ∗ host_data_ptr)\n{\n for (u32 i=0; i < KERNEL_LOOP; i++)\n {\n  host_data_ptr[i] = (u32) rand();\n }\n}\n\n__host__ void gpu_kernel(void)\n{\n const u32 num_elements = (128∗1024);\n const u32 num_threads = 256;\n const u32 num_blocks = (num_elements+(num_threads-1)) / num_threads;\n const u32 num_bytes = num_elements ∗ sizeof(u32);\n int max_device_num;\n const int max_runs = 6;\n\n CUDA_CALL(cudaGetDeviceCount(&max_device_num));\n\n for (int device_num=0; device_num < max_device_num; device_num++)\n {\n  CUDA_CALL(cudaSetDevice(device_num));\n\n  u32 ∗ data_gpu;\n  cudaEvent_t kernel_start1, kernel_stop1;\n  cudaEvent_t kernel_start2, kernel_stop2;\n  float delta_time1 = 0.0F, delta_time2=0.0F;\n  struct cudaDeviceProp device_prop;\n  char device_prefix[261];\n\n  CUDA_CALL(cudaMalloc(&data_gpu, num_bytes));\n```", "```cpp\n  CUDA_CALL(cudaEventCreate(&kernel_start2));\n  CUDA_CALL(cudaEventCreateWithFlags(&kernel_stop1, cudaEventBlockingSync));\n  CUDA_CALL(cudaEventCreateWithFlags(&kernel_stop2, cudaEventBlockingSync));\n\n  // printf(\"\\nLaunching %u blocks, %u threads\", num_blocks, num_threads);\n  CUDA_CALL(cudaGetDeviceProperties(&device_prop, device_num));\n  sprintf(device_prefix, \"ID:%d %s:\", device_num, device_prop.name);\n\n  for (int num_test=0;num_test < max_runs; num_test++)\n  {\n   // Generate some random data on the host side\n   // Replace with function to obtain data block from disk, network or other\n   // data source\n   generate_rand_data(const_data_host);\n\n   // Copy host memory to constant memory section in GPU\n   CUDA_CALL(cudaMemcpyToSymbol(const_data_gpu, const_data_host,\n           KERNEL_LOOP ∗ sizeof(u32)));\n   // Warm up run\n   // printf(\"\\nLaunching gmem kernel warm-up\");\n   const_test_gpu_gmem <<<num_blocks, num_threads>>>(data_gpu, num_elements);\n   cuda_error_check(\"Error \", \" returned from gmem startup kernel\");\n\n   // Do the gmem kernel\n   // printf(\"\\nLaunching gmem kernel\");\n   CUDA_CALL(cudaEventRecord(kernel_start1,0));\n\n   const_test_gpu_gmem <<<num_blocks, num_threads>>>(data_gpu, num_elements);\n\n   cuda_error_check(\"Error \", \" returned from gmem runtime kernel\");\n\n   CUDA_CALL(cudaEventRecord(kernel_stop1,0));\n   CUDA_CALL(cudaEventSynchronize(kernel_stop1));\n   CUDA_CALL(cudaEventElapsedTime(&delta_time1, kernel_start1, kernel_stop1));\n   // printf(\"\\nGMEM Elapsed time: %.3fms\", delta_time1);\n\n   // Copy host memory to global memory section in GPU\n   CUDA_CALL(cudaMemcpyToSymbol(gmem_data_gpu, const_data_host,\n           KERNEL_LOOP ∗ sizeof(u32)));\n   // Warm up run\n   // printf(\"\\nLaunching constant kernel warm-up\");\n   const_test_gpu_const <<<num_blocks, num_threads>>>(data_gpu, num_elements);\n\n   cuda_error_check(\"Error \", \" returned from constant startup kernel\");\n\n   // Do the constant kernel\n   // printf(\"\\nLaunching constant kernel\");\n```", "```cpp\n\n   const_test_gpu_const <<<num_blocks, num_threads>>>(data_gpu, num_elements);\n\n   cuda_error_check(\"Error \", \" returned from constant runtime kernel\");\n\n   CUDA_CALL(cudaEventRecord(kernel_stop2,0));\n   CUDA_CALL(cudaEventSynchronize(kernel_stop2));\n   CUDA_CALL(cudaEventElapsedTime(&delta_time2, kernel_start2, kernel_stop2));\n   // printf(\"\\nConst Elapsed time: %.3fms\", delta_time2);\n\n   if (delta_time1 > delta_time2)\n    printf(\"\\n%sConstant version is faster by: %.2fms (G=%.2fms, C=%.2fms)\", device_prefix, delta_time1-delta_time2, delta_time1, delta_time2);\n   else\n    printf(\"\\n%sGMEM  version is faster by: %.2fms (G=%.2fms, C=%.2fms)\", device_prefix, delta_time2-delta_time1, delta_time1, delta_time2);\n  }\n\n  CUDA_CALL(cudaEventDestroy(kernel_start1));\n  CUDA_CALL(cudaEventDestroy(kernel_start2));\n  CUDA_CALL(cudaEventDestroy(kernel_stop1));\n  CUDA_CALL(cudaEventDestroy(kernel_stop2));\n  CUDA_CALL(cudaFree(data_gpu));\n\n  CUDA_CALL(cudaDeviceReset());\n  printf(\"\\n\");\n }\n\n wait_exit();\n}\n```", "```cpp\nextern __host__ cudaError_t CUDARTAPI cudaMallocPitch(void ∗∗devPtr, size_t ∗pitch, size_t width, size_t height);\n```", "```cpp\ntypedef struct\n{\n unsigned int a;\n unsigned int b;\n unsigned int c;\n unsigned int d;\n} MY_TYPE_T;\n\nMY_TYPE_T some_array[1024]; /∗ 1024 ∗ 4 bytes = 4K ∗/\n```", "```cpp\nconst unsigned int value_u32 = some_data[tid];\nconst unsigned char value_01 = (value_u32 & 0x000000FF)  );\nconst unsigned char value_02 = (value_u32 & 0x0000FF00) >> 8 );\nconst unsigned char value_03 = (value_u32 & 0x00FF0000) >> 16 );\nconst unsigned char value_04 = (value_u32 & 0xFF000000) >> 24 );\n```", "```cpp\n// Define the number of elements we’ll use\n#define NUM_ELEMENTS 4096\n\n// Define an interleaved type\n// 16 bytes, 4 bytes per member\ntypedef struct\n{\n u32 a;\n u32 b;\n u32 c;\n u32 d;\n} INTERLEAVED_T;\n\n// Define an array type based on the interleaved structure\ntypedef INTERLEAVED_T INTERLEAVED_ARRAY_T[NUM_ELEMENTS];\n\n// Alternative - structure of arrays\ntypedef u32 ARRAY_MEMBER_T[NUM_ELEMENTS];\n\ntypedef struct\n{\n ARRAY_MEMBER_T a;\n ARRAY_MEMBER_T b;\n ARRAY_MEMBER_T c;\n ARRAY_MEMBER_T d;\n} NON_INTERLEAVED_T;\n```", "```cpp\n__host__ float add_test_non_interleaved_cpu(\n NON_INTERLEAVED_T ∗ const host_dest_ptr,\n const NON_INTERLEAVED_T ∗ const host_src_ptr,\n const u32 iter,\n const u32 num_elements)\n{\n float start_time = get_time();\n\n for (u32 tid = 0; tid < num_elements; tid++)\n {\n  for (u32 i=0; i<iter; i++)\n  {\n   host_dest_ptr->a[tid] += host_src_ptr->a[tid];\n   host_dest_ptr->b[tid] += host_src_ptr->b[tid];\n   host_dest_ptr->c[tid] += host_src_ptr->c[tid];\n   host_dest_ptr->d[tid] += host_src_ptr->d[tid];\n  }\n }\n\n const float delta = get_time() - start_time;\n\n return delta;\n}\n\n__host__ float add_test_interleaved_cpu(\n INTERLEAVED_T ∗ const host_dest_ptr,\n const INTERLEAVED_T ∗ const host_src_ptr,\n const u32 iter,\n const u32 num_elements)\n{\n float start_time = get_time();\n\n for (u32 tid = 0; tid < num_elements; tid++)\n {\n  for (u32 i=0; i<iter; i++)\n  {\n   host_dest_ptr[tid].a += host_src_ptr[tid].a;\n   host_dest_ptr[tid].b += host_src_ptr[tid].b;\n   host_dest_ptr[tid].c += host_src_ptr[tid].c;\n   host_dest_ptr[tid].d += host_src_ptr[tid].d;\n  }\n }\n\n```", "```cpp\n return delta;\n}\n```", "```cpp\n__global__ void add_kernel_interleaved(\n INTERLEAVED_T ∗ const dest_ptr,\n const INTERLEAVED_T ∗ const src_ptr,\n const u32 iter,\n const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n\n if (tid < num_elements)\n {\n  for (u32 i=0; i<iter; i++)\n  {\n   dest_ptr[tid].a += src_ptr[tid].a;\n   dest_ptr[tid].b += src_ptr[tid].b;\n   dest_ptr[tid].c += src_ptr[tid].c;\n   dest_ptr[tid].d += src_ptr[tid].d;\n  }\n }\n}\n\n__global__ void add_kernel_non_interleaved(\n NON_INTERLEAVED_T ∗ const dest_ptr,\n const NON_INTERLEAVED_T ∗ const src_ptr,\n const u32 iter,\n const u32 num_elements)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n\n if (tid < num_elements)\n {\n  for (u32 i=0; i<iter; i++)\n  {\n   dest_ptr->a[tid] += src_ptr->a[tid];\n   dest_ptr->b[tid] += src_ptr->b[tid];\n   dest_ptr->c[tid] += src_ptr->c[tid];\n```", "```cpp\n  }\n }\n}\n```", "```cpp\n__host__ float add_test_interleaved(\n INTERLEAVED_T ∗ const host_dest_ptr,\n const INTERLEAVED_T ∗ const host_src_ptr,\n const u32 iter,\n const u32 num_elements)\n{\n // Set launch params\n const u32 num_threads = 256;\n const u32 num_blocks = (num_elements + (num_threads-1)) / num_threads;\n\n // Allocate memory on the device\n const size_t num_bytes = (sizeof(INTERLEAVED_T) ∗ num_elements);\n INTERLEAVED_T ∗ device_dest_ptr;\n INTERLEAVED_T ∗ device_src_ptr;\n\n CUDA_CALL(cudaMalloc((void ∗∗) &device_src_ptr, num_bytes));\n\n CUDA_CALL(cudaMalloc((void ∗∗) &device_dest_ptr, num_bytes));\n\n // Create a stop and stop event for timing\n cudaEvent_t kernel_start, kernel_stop;\n cudaEventCreate(&kernel_start, 0);\n cudaEventCreate(&kernel_stop, 0);\n\n // Create a non zero stream\n cudaStream_t test_stream;\n CUDA_CALL(cudaStreamCreate(&test_stream));\n\n // Copy src data to GPU\n CUDA_CALL(cudaMemcpy(device_src_ptr, host_src_ptr, num_bytes, cudaMemcpyHostToDevice));\n\n // Push start event ahread of kernel call\n CUDA_CALL(cudaEventRecord(kernel_start, 0));\n\n // Call the GPU kernel\n add_kernel_interleaved<<<num_blocks, num_threads>>>(device_dest_ptr, device_src_ptr, iter, num_elements);\n\n // Push stop event after of kernel call\n CUDA_CALL(cudaEventRecord(kernel_stop, 0));\n```", "```cpp\n // Wait for stop event\n CUDA_CALL(cudaEventSynchronize(kernel_stop));\n\n // Get delta between start and stop,\n // i.e. the kernel execution time\n float delta = 0.0F;\n CUDA_CALL(cudaEventElapsedTime(&delta, kernel_start, kernel_stop));\n\n // Clean up\n CUDA_CALL(cudaFree(device_src_ptr));\n CUDA_CALL(cudaFree(device_dest_ptr));\n CUDA_CALL(cudaEventDestroy(kernel_start));\n CUDA_CALL(cudaEventDestroy(kernel_stop));\n CUDA_CALL(cudaStreamDestroy(test_stream));\n\n return delta;\n}\n```", "```cpp\nRunning Interleaved /  Non Interleaved memory test using 65536 bytes (4096 elements)\nID:0 GeForce GTX 470:  Interleaved time: 181.83ms\nID:0 GeForce GTX 470:  Non Interleaved time: 45.13ms\n\nID:1 GeForce 9800 GT:  Interleaved time: 2689.15ms\nID:1 GeForce 9800 GT:  Non Interleaved time: 234.98ms\n\nID:2 GeForce GTX 260:  Interleaved time: 444.16ms\nID:2 GeForce GTX 260:  Non Interleaved time: 139.35ms\n\nID:3 GeForce GTX 460:  Interleaved time: 199.15ms\nID:3 GeForce GTX 460:  Non Interleaved time: 63.49ms\n\n        CPU (serial):  Interleaved time: 1216.00ms\n        CPU (serial):  Non Interleaved time: 13640.00ms\n```", "```cpp\n000:00000041 001:00000042 002:00000043 003:00000044 004:00000045 005:00000046 006:00000047 007:00000048\n008:00000049 009:0000004a 010:0000004b 011:0000004c 012:0000004d 013:0000004e 014:0000004f 015:00000050\n016:00000051 017:00000052 018:00000053 019:00000054 020:00000055 021:00000056 022:00000057 023:00000058\n024:00000059 025:0000005a 026:0000005b 027:0000005c 028:0000005d 029:0000005e 030:0000005f 031:00000060\n032:00000061 033:00000062 034:00000063 035:00000064 036:00000065 037:00000066 038:00000067 039:00000068\n040:00000069 041:0000006a 042:0000006b 043:0000006c 044:0000006d 045:0000006e 046:0000006f 047:00000070\n048:00000071 049:00000072 050:00000073 051:00000074 052:00000075 053:00000076 054:00000077 055:00000078\n056:00000079 057:0000007a 058:0000007b 059:0000007c 060:0000007d 061:0000007e 062:0000007f 063:00000080\n\n064:00000001 065:00000002 066:00000003 067:00000004 068:00000005 069:00000006 070:00000007 071:00000008\n072:00000009 073:0000000a 074:0000000b 075:0000000c 076:0000000d 077:0000000e 078:0000000f 079:00000010\n080:00000011 081:00000012 082:00000013 083:00000014 084:00000015 085:00000016 086:00000017 087:00000018\n088:00000019 089:0000001a 090:0000001b 091:0000001c 092:0000001d 093:0000001e 094:0000001f 095:00000020\n096:00000021 097:00000022 098:00000023 099:00000024 100:00000025 101:00000026 102:00000027 103:00000028\n104:00000029 105:0000002a 106:0000002b 107:0000002c 108:0000002d 109:0000002e 110:0000002f 111:00000030\n112:00000031 113:00000032 114:00000033 115:00000034 116:00000035 117:00000036 118:00000037 119:00000038\n120:00000039 121:0000003a 122:0000003b 123:0000003c 124:0000003d 125:0000003e 126:0000003f 127:00000040\n```", "```cpp\n__host__ TIMER_T select_samples_cpu(\n```", "```cpp\n const u32 sample_interval,\n const u32 num_elements,\n const u32 ∗ const src_data)\n{\n const TIMER_T start_time = get_time();\n u32 sample_idx = 0;\n\n for (u32 src_idx=0; src_idx<num_elements; src_idx+=sample_interval)\n {\n  sample_data[sample_idx] = src_data[src_idx];\n  sample_idx++;\n }\n\n const TIMER_T end_time = get_time();\n return end_time - start_time;\n}\n```", "```cpp\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n```", "```cpp\n__global__ void select_samples_gpu_kernel(u32 ∗ const sample_data,\n const u32 sample_interval, const u32 ∗ const src_data)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n sample_data[tid] = src_data[tid∗sample_interval];\n}\n\n__host__ TIMER_T select_samples_gpu(\n u32 ∗ const sample_data,\n const u32 sample_interval,\n const u32 num_elements,\n const u32 num_samples,\n const u32 ∗ const src_data,\n const u32 num_threads_per_block,\n const char ∗ prefix)\n{\n // Invoke one block of N threads per sample\n const u32 num_blocks = num_samples / num_threads_per_block;\n```", "```cpp\n // Check for non equal block size\n assert((num_blocks ∗ num_threads_per_block) == num_samples);\n\n start_device_timer();\n\n select_samples_gpu_kernel<<<num_blocks, num_threads_per_block>>>(sample_data, sample_interval, src_data);\n```", "```cpp\n\n const TIMER_T func_time = stop_device_timer();\n\n return func_time;\n}\n```", "```cpp\n__host__ TIMER_T sort_samples_cpu(\n u32 ∗ const sample_data,\n const u32 num_samples)\n{\n const TIMER_T start_time = get_time();\n\n qsort(sample_data, num_samples, sizeof(u32),\n   &compare_func);\n\n const TIMER_T end_time = get_time();\n return end_time - start_time;\n}\n```", "```cpp\n__host__ TIMER_T count_bins_cpu(const u32 num_samples,\n const u32 num_elements,\n const u32 ∗ const src_data,\n const u32 ∗ const sample_data,\n u32 ∗ const bin_count)\n{\n const TIMER_T start_time = get_time();\n```", "```cpp\n {\n  const u32 data_to_find = src_data[src_idx];\n  const u32 idx = bin_search3(sample_data,\n                              data_to_find,\n                              num_samples);\n  bin_count[idx]++;\n }\n const TIMER_T end_time = get_time();\n return end_time - start_time;\n}\n```", "```cpp\n__host__ __device__ u32 bin_search3(\n const u32 ∗ const src_data,\n```", "```cpp\n const u32 num_elements)\n{\n // Take the middle of the two sections\n u32 size = (num_elements >> 1);\n\n u32 start_idx = 0;\n bool found = false;\n\n do\n {\n  const u32 src_idx = (start_idx+size);\n  const u32 test_value = src_data[src_idx];\n\n  if (test_value == search_value)\n   found = true;\n  else\n   if (search_value > test_value)\n    start_idx = (start_idx+size);\n\n  if (found == false)\n   size >>= 1;\n\n } while ( (found == false) && (size != 0) );\n\n return (start_idx + size);\n}\n```", "```cpp\n// Single data point, atomic add to gmem\n__global__ void count_bins_gpu_kernel5(\n const u32 num_samples,\n const u32 ∗ const src_data,\n const u32 ∗ const sample_data,\n u32 ∗ const bin_count)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n\n // Read the sample point\n const u32 data_to_find = src_data[tid];\n\n // Obtain the index of the element in the search list\n const u32 idx = bin_search3(sample_data, data_to_find, num_samples);\n\n atomicAdd(&bin_count[idx],1);\n}\n\n```", "```cpp\n const u32 num_samples,\n const u32 num_elements,\n const u32 ∗ const src_data,\n const u32 ∗ const sample_data,\n u32 ∗ const bin_count,\n const u32 num_threads,\n const char ∗ prefix)\n{\n const u32 num_blocks = num_elements / num_threads;\n\n start_device_timer();\n\n count_bins_gpu_kernel5<<<num_blocks, num_threads>>>(num_samples, src_data, sample_data, bin_count);\n cuda_error_check(prefix, \"Error invoking count_bins_gpu_kernel\");\n\n const TIMER_T func_time = stop_device_timer();\n return func_time;\n}\n```", "```cpp\n__host__ TIMER_T calc_bin_idx_cpu(const u32 num_samples,\n      const u32 ∗ const bin_count,\n      u32 ∗ const dest_bin_idx)\n{\n const TIMER_T start_time = get_time();\n u32 prefix_sum = 0;\n\n for (u32 i=0; i<num_samples; i++)\n {\n  dest_bin_idx[i] = prefix_sum;\n  prefix_sum += bin_count[i];\n }\n\n const TIMER_T end_time = get_time();\n return end_time - start_time;\n}\n```", "```cpp\n__global__ void calc_prefix_sum_kernel(\n const u32 num_samples_per_thread,\n const u32 ∗ const bin_count,\n u32 ∗ const prefix_idx,\n u32 ∗ const block_sum)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n\n const u32 tid_offset = tid ∗ num_samples_per_thread;\n u32 prefix_sum;\n\n if (tid == 0)\n  prefix_sum = 0;\n else\n  prefix_sum = bin_count[tid_offset-1];\n\n for (u32 i=0; i<num_samples_per_thread; i++)\n```", "```cpp\n  prefix_idx[i+tid_offset] = prefix_sum;\n  prefix_sum += bin_count[i+tid_offset];\n }\n\n // Store the block prefix sum as the value from the last element\n block_sum[tid] = prefix_idx[(num_samples_per_thread-1uL)+tid_offset];\n}\n```", "```cpp\n__global__ void add_prefix_sum_total_kernel(\n u32 ∗ const prefix_idx,\n const u32 ∗ const total_count)\n{\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n\n prefix_idx[tid] += total_count[blockIdx.x];\n}\n```", "```cpp\n__global__ void calc_prefix_sum_kernel_single(\n const u32 num_samples,\n const u32 ∗ const bin_count,\n u32 ∗ const dest_bin_idx)\n{\n u32 prefix_sum = 0;\n\n for (u32 i=0; i<num_samples; i++)\n {\n  dest_bin_idx[i] = prefix_sum;\n  prefix_sum += bin_count[i];\n }\n}\n```", "```cpp\n__host__ TIMER_T calc_bin_idx_gpu(\n const u32 num_elements,\n const u32 ∗ const bin_count,\n u32 ∗ const dest_bin_idx,\n const u32 num_threads_per_block,\n u32 num_blocks,\n const char ∗ prefix,\n u32 ∗ const block_sum,\n u32 ∗ const block_sum_prefix)\n{\n start_device_timer();\n\n if (num_elements >= 4096)\n {\n  const u32 num_threads_total = num_threads_per_block\n                                ∗ num_blocks;\n\n  const u32 num_elements_per_thread = num_elements / num_threads_total;\n\n  // Make sure the caller passed arguments which correctly divide the elements to blocks and threads\n  assert( (num_elements_per_thread ∗\n           num_threads_total) == num_elements );\n\n  // First calculate the prefix sum over a block\n  calc_prefix_sum_kernel<<<num_blocks, num_threads_per_block>>>(num_elements_per_thread, bin_count, dest_bin_idx, block_sum);\n\n  cuda_error_check(prefix, \"Error invoking calc_prefix_sum_kernel\");\n\n  // Calculate prefix for the block sums\n  // Single threaded\n  calc_prefix_sum_kernel_single<<<1,1>>>(num_threads_total, block_sum, block_sum_prefix);\n  cuda_error_check(prefix, \"Error invoking calc_prefix_sum_kernel_single\");\n\n  // Add the prefix sums totals back into the original prefix blocks\n  // Switch to N threads per block\n  num_blocks = num_elements /\n               num_elements_per_thread;\n  add_prefix_sum_total_kernel<<<num_blocks, num_elements_per_thread>>>(dest_bin_idx, block_sum_prefix);\n\n  cuda_error_check(prefix, \"add_prefix_sum_total_kernel\");\n }\n else\n {\n```", "```cpp\n  // Single threaded\n  calc_prefix_sum_kernel_single<<<1,1>>>(num_elements, bin_count, dest_bin_idx);\n\n  cuda_error_check(prefix, \"Error invoking calc_prefix_sum_kernel_single\");\n }\n const TIMER_T func_time = stop_device_timer();\n return func_time;\n}\n```", "```cpp\n__host__ TIMER_T sort_to_bins_cpu(\n const u32 num_samples,\n const u32 num_elements,\n const u32 ∗ const src_data,\n const u32 ∗ const sample_data,\n const u32 ∗ const bin_count,\n const u32 ∗ const dest_bin_idx,\n u32 ∗ const dest_data)\n{\n const TIMER_T start_time = get_time();\n u32 dest_bin_idx_tmp[NUM_SAMPLES];\n\n // Copy the dest_bin_idx array to temp storage\n for (u32 bin=0;bin<NUM_SAMPLES;bin++)\n {\n  dest_bin_idx_tmp[bin] = dest_bin_idx[bin];\n }\n\n // Iterate over all source data points\n for (u32 src_idx=0; src_idx<num_elements; src_idx++)\n {\n  // Read the source data\n  const u32 data = src_data[src_idx];\n\n  // Identify the bin in which the source data\n  // should reside\n```", "```cpp\n                              data,\n                              num_samples);\n\n  // Fetch the current index for that bin\n  const u32 dest_idx = dest_bin_idx_tmp[bin];\n\n  // Write the data using the current index\n  // of the correct bin\n  dest_data[dest_idx] = data;\n\n  // Increment the bin index\n  dest_bin_idx_tmp[bin]++;\n }\n\n const TIMER_T end_time = get_time();\n return end_time - start_time;\n}\n```", "```cpp\n__global__ void sort_to_bins_gpu_kernel(\n const u32 num_samples,\n const u32 ∗ const src_data,\n const u32 ∗ const sample_data,\n u32 ∗ const dest_bin_idx_tmp,\n u32 ∗ const dest_data)\n{\n // Calculate the thread we’re using\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n\n // Read the sample point\n const u32 data = src_data[tid];\n\n // Identify the bin in which the\n // source data should reside\n const u32 bin = bin_search3(sample_data,\n```", "```cpp\n                             num_samples);\n\n // Increment the current index for that bin\n const u32 dest_idx = atomicAdd(&dest_bin_idx_tmp[bin],1);\n\n // Write the data using the\n // current index of the correct bin\n dest_data[dest_idx] = data;\n}\n```", "```cpp\n// Increment the bin index\ndest_bin_idx_tmp[bin]++;\n```", "```cpp\n// Increment the current index for that bin\nconst u32 dest_idx = atomicAdd(&dest_bin_idx_tmp[bin],1);\n```", "```cpp\n const u32 num_samples,\n const u32 num_elements,\n u32 ∗ const data,\n const u32 ∗ const sample_data,\n const u32 ∗ const bin_count,\n const u32 ∗ const dest_bin_idx,\n u32 ∗ const sort_tmp,\n const u32 num_threads,\n const char ∗ prefix)\n{\n start_device_timer();\n\n const u32 num_blocks = num_samples / num_threads;\n\n sort_bins_gpu_kernel3<<<num_blocks, num_threads>>>(num_samples, num_elements, data, sample_data, bin_count, dest_bin_idx, sort_tmp);\n\n cuda_error_check(prefix, \"Error invoking sort_bins_gpu_kernel\");\n\n const TIMER_T func_time = stop_device_timer();\n return func_time;\n}\n```", "```cpp\n__global__ void sort_bins_gpu_kernel3(\n const u32 num_samples,\n const u32 num_elements,\n u32 ∗ const data,\n const u32 ∗ const sample_data,\n const u32 ∗ const bin_count,\n const u32 ∗ const dest_bin_idx,\n u32 ∗ const sort_tmp)\n{\n // Calculate the thread we’re using\n const u32 tid = (blockIdx.x ∗ blockDim.x) + threadIdx.x;\n\n if (tid != (num_samples-1))\n  radix_sort(data, dest_bin_idx[tid], dest_bin_idx[tid+1], sort_tmp);\n else  \n  radix_sort(data, dest_bin_idx[tid], num_elements, sort_tmp);\n}\n```", "```cpp\n__device__ void radix_sort(\n u32 ∗ const data,\n const u32 start_idx,\n const u32 end_idx,\n u32 ∗ const sort_tmp_1)\n{\n // Sort into num_list, lists\n // Apply radix sort on 32 bits of data\n for (u32 bit=0;bit<32;bit++)\n {\n  // Mask off all but the bit we’re interested in\n  const u32 bit_mask = (1u << bit);\n\n  // Set up the zero and one counter\n  u32 base_cnt_0 = start_idx;\n  u32 base_cnt_1 = start_idx;\n\n  for (u32 i=start_idx; i<end_idx; i++)\n  {\n   // Fetch the test data element\n   const u32 elem = data[i];\n\n   // If the element is in the one list\n   if ( (elem & bit_mask) > 0u )\n   {\n    // Copy to the one list\n    sort_tmp_1[base_cnt_1++] = elem;\n   }\n   else\n   {\n    // Copy to the zero list (inplace)\n    data[base_cnt_0++] = elem;\n   }\n  }\n\n  // Copy data back to source from the one’s list\n  for (u32 i=start_idx; i<base_cnt_1; i++)\n  {\n   data[base_cnt_0++] = sort_tmp_1[i];\n  }\n }\n}\n```", "```cpp\nID:3 GeForce GTX 460: Test 32 - Selecting 16384 from 1048576 elements using 512 blocks of 32 threads\nSelect Sample Time - CPU: 0.19 GPU:0.03\nSort Sample Time - CPU: 2.13 GPU:125.57\nCount Bins Time - CPU: 157.59 GPU:17.00\nCalc. Bin Idx Time - CPU: 0.03 GPU:0.58\nSort to Bins Time - CPU: 163.81 GPU:16.94\nSort Bins Time  - CPU: 72.06 GPU:64.46\nTotal Time   - CPU: 395.81 GPU:224.59\nQsort Time   - CPU: 185.41 GPU:N/A\n\nID:3 GeForce GTX 460: Test 32 - Selecting 16384 from 1048576 elements using 256 blocks of 64 threads\nSelect Sample Time - CPU: 0.53 GPU:0.03\nSort Sample Time - CPU: 2.06 GPU:125.57\nCount Bins Time - CPU: 157.75 GPU:19.07\nCalc. Bin Idx Time - CPU: 0.13 GPU:0.26\nSort to Bins Time - CPU: 164.09 GPU:19.09\nSort Bins Time  - CPU: 72.31 GPU:62.11\nTotal Time   - CPU: 396.88 GPU:226.13\nQsort Time   - CPU: 184.50 GPU:N/A\n\nID:3 GeForce GTX 460: Test 32 - Selecting 16384 from 1048576 elements using 128 blocks of 128 threads\nSelect Sample Time - CPU: 0.28 GPU:0.03\nSort Sample Time - CPU: 2.09 GPU:125.57\nCount Bins Time - CPU: 157.91 GPU:13.96\nCalc. Bin Idx Time - CPU: 0.09 GPU:0.26\nSort to Bins Time - CPU: 164.22 GPU:14.00\nSort Bins Time  - CPU: 71.19 GPU:91.33\nTotal Time   - CPU: 395.78 GPU:245.16\nQsort Time   - CPU: 185.19 GPU:N/A\n\nID:3 GeForce GTX 460: Test 32 - Selecting 16384 from 1048576 elements using 64 blocks of 256 threads\nSelect Sample Time - CPU: 0.22 GPU:0.03\nSort Sample Time - CPU: 2.00 GPU:125.57\nCount Bins Time - CPU: 158.78 GPU:12.43\nCalc. Bin Idx Time - CPU: 0.13 GPU:0.49\nSort to Bins Time - CPU: 164.38 GPU:12.39\nSort Bins Time  - CPU: 71.16 GPU:84.89\nTotal Time   - CPU: 396.66 GPU:235.80\n```", "```cpp\nID:3 GeForce GTX 460: Test 32 - Selecting 16384 from 1048576 elements using 512 blocks of 32 threads\nSelect Sample Time - CPU: 0.09 GPU:0.09\nSort Sample Time - CPU: 2.09 GPU:2.09\nCount Bins Time - CPU: 157.69 GPU:17.02\nCalc. Bin Idx Time - CPU: 0.09 GPU:0.58\nSort to Bins Time - CPU: 163.78 GPU:16.94\nSort Bins Time  - CPU: 71.97 GPU:64.47\nTotal Time   - CPU: 395.72 GPU:101.19\nQsort Time   - CPU: 184.78 GPU:N/A\n```", "```cpp\nP = 10, 20, 40, 50, 20\nX = 0, 2, 4, 6, 8\n```"]