- en: '7.2\. Background: elements of finite Markov chains#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap07_rwmc/02_mcdefs/roch-mmids-rwmc-mcdefs.html](https://mmids-textbook.github.io/chap07_rwmc/02_mcdefs/roch-mmids-rwmc-mcdefs.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As we mentioned, we are interested in analyzing the behavior of a random walk
    “diffusing” on a graph. Before we develop such techniques, it will be worthwhile
    to cast them in the more general framework of discrete-time Markov chains on a
    finite state space. Indeed Markov chains have many more applications in data science.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1\. Basic definitions[#](#basic-definitions "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A discrete-time Markov chain\(\idx{Markov chain}\xdi\) is a stochastic process\(\idx{stochastic
    process}\xdi\), i.e., a collection of random variables in this case indexed by
    time. We assume that the random variables take values in a common finite state
    space \(\S\). What makes it “Markovian” is that “it forgets the past” in the sense
    that “its future only depends on its present state.” More formally:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Discrete-Time Markov Chain)** The sequence of random variables
    \((X_t)_{t \geq 0} = (X_0, X_1, X_2, \ldots)\) taking values in the finite state
    space \(\S\) is a Markov chain if: for all \(t \geq 1\) and all \(x_0,x_1,\ldots,x_t
    \in \S\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (*)\qquad\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0
    = x_0] = \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}] \]
  prefs: []
  type: TYPE_NORMAL
- en: provided the conditional probabilities are well-defined. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: To be clear, the event in the conditioning is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \{X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0 = x_0\} = \{X_{t-1} = x_{t-1}\}
    \cap \{X_{t-2} = x_{t-2}\} \cap \cdots \cap \{X_0 = x_0\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: It will sometimes be convenient to assume that the common state space \(\S\)
    is of the form \([m] = \{1,\ldots,m\}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Weather Model)** Here is a simple weather model. Every day
    is either \(\mathrm{Dry}\) or \(\mathrm{Wet}\). We model the transitions as Markovian;
    intuitively, we assume that tomorrow’s weather only depends - in a random fashion
    independent of the past - on today’s weather. Say the weather changes with \(25\%\)
    chance. More formally, let \(X_t \in \mathcal{S}\) be the weather on day \(t\)
    with \(\mathcal{S} = \{\mathrm{Dry}, \mathrm{Wet}\}\). Assume that \(X_0 = \mathrm{Dry}\)
    and let \((Z_t)_{t \geq 0}\) be an i.i.d. (i.e., independent, identically distributed)
    sequence of random variables taking values in \(\{\mathrm{Same}, \mathrm{Change}\}\)
    satisfying'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[Z_t = \mathrm{Same}] = 1 - \mathbb{P}[Z_t = \mathrm{Change}] =
    3/4. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then define for all \(t \geq 0\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} X_{t+1} = f(X_t, Z_t) = \begin{cases} X_t & \text{if $Z_t =
    \mathrm{Same}$},\\ \mathrm{Wet} & \text{if $X_t = \mathrm{Dry}$ and $Z_t = \mathrm{Change}$},\\
    \mathrm{Dry} & \text{if $X_t = \mathrm{Wet}$ and $Z_t = \mathrm{Change}$}. \end{cases}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'We claim that \((X_t)_{t \geq 0}\) is a Markov chain. We use two observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 1- By composition,
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_1 = f(X_0, Z_0), \]\[ X_2 = f(X_1,Z_1) = f(f(X_0,Z_0),Z_1), \]\[ X_3 =
    f(X_2,Z_2) = f(f(X_1,Z_1),Z_2) = f(f(f(X_0,Z_0),Z_1),Z_2), \]
  prefs: []
  type: TYPE_NORMAL
- en: and, more generally,
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_t = f(X_{t-1},Z_{t-1}) = f(f(X_{t-2},Z_{t-2}),Z_{t-1}) = f(f(\cdots f(f(X_0,Z_0),Z_1),\cdots),Z_{t-1})
    \]
  prefs: []
  type: TYPE_NORMAL
- en: is a deterministic function of \(X_0 = \mathrm{Dry}\) and \(Z_0,\ldots,Z_{t-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: 2- For any \(x_1,\ldots,x_t \in \S\), there is precisely one value of \(z \in
    \{\mathrm{Same}, \mathrm{Change}\}\) such that \(x_t = f(x_{t-1}, z)\), i.e.,
    if \(x_t = x_{t-1}\) we must have \(z = \mathrm{Same}\) and if \(x_t \neq x_{t-1}\)
    we must have \(z = \mathrm{Change}\).
  prefs: []
  type: TYPE_NORMAL
- en: Fix \(x_0 = \mathrm{Dry}\). For any \(x_1,\ldots,x_t \in \S\), letting \(z\)
    be as in Observation 2,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0
    = x_0]\\ &= \P[f(X_{t-1}, Z_{t-1}) = x_t \,|\, X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0
    = x_0]\\ &= \P[f(x_{t-1}, Z_{t-1}) = x_t \,|\, X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0
    = x_0]\\ &= \P[Z_{t-1} = z \,|\, X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0
    = x_0]\\ &= \P[Z_{t-1} = z], \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(Z_{t-1}\) is independent of \(Z_{t-2},\ldots,Z_0\) and
    \(X_0\) (which is deterministic), and therefore is independent of \(X_{t-1},\ldots,X_0\)
    by Observation 1\. The same argument shows that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}] = \P[Z_{t-1} = z], \]
  prefs: []
  type: TYPE_NORMAL
- en: and that proves the claim.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, one can pick \(X_0\) according to an initial distribution, independently
    from the sequence \((Z_t)_{t \geq 0}\). The argument above can be adapted to this
    case. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Random Walk on the Petersen Graph)** Let \(G = (V,E)\) be the
    Petersen graph. Each vertex \(i\) has degree \(3\), that is, it has three neighbors
    which we denote \(v_{i,1}, v_{i,2}, v_{i,3}\) in some arbitrary order. For instance,
    denoting the vertices by \(1,\ldots, 10\) as above, vertex \(9\) has neighbors
    \(v_{9,1} = 4, v_{9,2} = 6, v_{9,3} = 7\).'
  prefs: []
  type: TYPE_NORMAL
- en: We consider the following random walk on \(G\). We start at \(X_0 = 1\). Then,
    for each \(t\geq 0\), we let \(X_{t+1}\) be a uniformly chosen neighbor of \(X_t\),
    independently of the previous history. That is, we jump at random from neighbor
    to neighbor. Formally, fix \(X_0 = 1\) and let \((Z_t)_{t \geq 0}\) be an i.i.d.
    sequence of random variables taking values in \(\{1,2,3\}\) satisfying
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[Z_t = 1] = \mathbb{P}[Z_t = 2] = \mathbb{P}[Z_t = 3] = 1/3. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then define, for all \(t \geq 0\), \(X_{t+1} = f(X_t, Z_t) = v_{i,Z_t}\) if
    \(X_t = v_i\).
  prefs: []
  type: TYPE_NORMAL
- en: By an argument similar to the previous example, \((X_t)_{t \geq 0}\) is a Markov
    chain. Also as in the previous example, one can pick \(X_0\) according to an initial
    distribution, independently from the sequence \((Z_t)_{t \geq 0}\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various useful generalizations of the condition \((*)\) in the definition
    of a Markov chain. These are all special cases of what is referred to as the *Markov
    Property*\(\idx{Markov property}\xdi\) which can be summarized as: the past and
    the future are independent given the present. We record a version general enough
    for us here. Let \((X_t)_{t \geq 0}\) be a Markov chain on the state space \(\mathcal{S}\).
    For any integer \(h \geq 0\), \(x_{t-1}\in \mathcal{S}\) and subsets \(\mathcal{P}
    \subseteq \mathcal{S}^{t-1}\), \(\mathcal{F} \subseteq \mathcal{S}^{h+1}\) of
    state sequences of length \(t-1\) and \(h+1\) respectively, it holds that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[(X_t,\ldots,X_{t+h}) \in \mathcal{F}\,|\,X_{t-1} = x_{t-1},
    (X_0,\ldots,X_{t-2}) \in \mathcal{P}] &= \P[(X_t,\ldots,X_{t+h}) \in \mathcal{F}\,|\,X_{t-1}
    = x_{t-1}]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: One important implication of the *Markov Property* is that the distribution
    of a sample path, i.e., an event of the form \(\{X_0 = x_0, X_1 = x_1, \ldots,
    X_T = x_T\}\), simplifies considerably.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Distribution of a Sample Path)** \(\idx{distribution of a sample
    path}\xdi\) For any \(x_0, x_1, \ldots, x_T \in \mathcal{S}\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \P[X_0 = x_0] \,\prod_{t=1}^T
    \,\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use the *Multiplication Rule* and the *Markov Property*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We first apply the *Multiplication Rule*'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P\left[\cap_{i=1}^r A_i\right] = \prod_{i=1}^r \P\left[A_i \,\middle|\,
    \cap_{j=1}^{i-1} A_j \right]. \]
  prefs: []
  type: TYPE_NORMAL
- en: with \(A_i = \{X_{i-1} = x_{i-1}\}\) and \(r = T+1\). That gives
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T]\\ &= \P[X_0 =
    x_0] \,\prod_{t=1}^T \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}, \ldots, X_0 = x_0]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then we use the *Markov Property* to simplify each term in the product. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Weather Model, continued)** Going back to the weather model
    from a previous example, fix \(x_0 = \mathrm{Dry}\) and \(x_1,\ldots,x_t \in \S\).
    Then, by the *Distribution of a Sample Path*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \P[X_0 = x_0] \,\prod_{t=1}^T
    \,\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}]. \]
  prefs: []
  type: TYPE_NORMAL
- en: By assumption \(\P[X_0 = x_0] = 1\). Moreover, we have previously shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}] = \P[Z_{t-1} = z_{t-1}], \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(z_{t-1} = \mathrm{Same}\) if \(x_t = x_{t-1}\) and \(z_{t-1} = \mathrm{Change}\)
    if \(x_t \neq x_{t-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: Hence, using the distribution of \(Z_t\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}] = \begin{cases} 3/4 & \text{if
    $x_t = x_{t-1}$},\\ 1/4 & \text{if $x_t \neq x_{t-1}$}. \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Let \(n_T = |\{0 < t \leq T : x_t = x_{t-1}\}|\) be the number of transitions
    without change. Then,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] &= \P[X_0 = x_0]
    \,\prod_{t=1}^T \,\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}]\\ &= \prod_{t=1}^T\P[Z_{t-1}
    = z_{t-1}]\\ &= (3/4)^{n_T} (1/4)^{T - n_T}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: It will be useful later on to observe that the *Distribution of a Sample Path*
    generalizes to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_{s+1} = x_{s+1}, X_{s+2} = x_{s+2}, \ldots, X_T = x_T\,|\,X_s = x_s]
    = \prod_{t=s+1}^T \,\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}]. \]
  prefs: []
  type: TYPE_NORMAL
- en: Based on the *Distribution of a Sample Path*, in order to specify the distribution
    of the process it suffices to specify
  prefs: []
  type: TYPE_NORMAL
- en: the *initial distribution*\(\idx{initial distribution}\xdi\) \(\mu_x := \P[X_0
    = x]\) for all \(x\); and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the *transition probabilities*\(\idx{transition probability}\xdi\) \(\P[X_{t+1}
    = x\,|\,X_{t} = x']\) for all \(t\), \(x\), \(x'\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '7.2.2\. Time-homogeneous case: transition matrix[#](#time-homogeneous-case-transition-matrix
    "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is common to further assume that the process is *time-homogeneous*\(\idx{time-homogeneous
    process}\xdi\), which means that the transition probabilities do not depend on
    \(t\):'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \P[X_{t+1} =x\,|\,X_{t} = x''] = \P[X_1 =x\,|\,X_{0} = x''] =: p_{x'',x},
    \quad \forall t=1,\ldots \]'
  prefs: []
  type: TYPE_NORMAL
- en: where the last equality is a definition. We can then collect the transition
    probabilities into a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Transition Matrix)** \(\idx{transition matrix}\xdi\) The
    matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = (p_{x',x})_{x,x' \in \S} \]
  prefs: []
  type: TYPE_NORMAL
- en: is called the transition matrix of the chain. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: We also let \(\mu_{x} = \P[X_0 = x]\) and we think of \(\bmu = (\mu_{x})_{x
    \in \S}\) as a vector. The convention in Markov chain theory is to think of probability
    distributions such as \(\bmu\) as *row vectors*. We will see later why it simplifies
    the notation somewhat.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Weather Model, continued)** Going back to the weather model,
    let us number the states as follows: \(1 = \mathrm{Dry}\) and \(2 = \mathrm{Wet}\).
    Then the transition matrix is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 3/4 & 1/4\\ 1/4 & 3/4 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Random Walk on the Petersen Graph, continued)** Consider again
    the random walk on the Petersen graph \(G = (V,E)\). We number the vertices \(1,
    2,\ldots, 10\). To compute the transition matrix, we list for each vertex its
    neighbors and put the value \(1/3\) in the corresponding columns. For instance,
    vertex \(1\) has neighbors \(2\), \(5\) and \(6\), so row \(1\) has \(1/3\) in
    columns \(2\), \(5\), and \(6\). And so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We get:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0 & 1/3 & 0 & 0 & 1/3 & 1/3 & 0 & 0 & 0
    & 0\\ 1/3 & 0 & 1/3 & 0 & 0 & 0 & 1/3 & 0 & 0 & 0\\ 0 & 1/3 & 0 & 1/3 & 0 & 0
    & 0 & 1/3 & 0 & 0\\ 0 & 0 & 1/3 & 0 & 1/3 & 0 & 0 & 0 & 1/3 & 0\\ 1/3 & 0 & 0
    & 1/3 & 0 & 0 & 0 & 0 & 0 & 1/3\\ 1/3 & 0 & 0 & 0 & 0 & 0 & 0 & 1/3 & 1/3 & 0\\
    0 & 1/3 & 0 & 0 & 0 & 0 & 0 & 0 & 1/3 & 1/3\\ 0 & 0 & 1/3 & 0 & 0 & 1/3 & 0 &
    0 & 0 & 1/3\\ 0 & 0 & 0 & 1/3 & 0 & 1/3 & 1/3 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 1/3
    & 0 & 1/3 & 1/3 & 0 & 0 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We have already encountered a matrix that encodes the neighbors of each vertex,
    the adjacency matrix. Here we can recover the transition matrix by multiplying
    the adjacency matrix by \(1/3\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Transition matrices have a very special structure.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Transition Matrix is Stochastic)** \(\idx{transition matrix
    is stochastic theorem}\xdi\) The transition matrix \(P\) is a stochastic matrix\(\idx{stochastic
    matrix}\xdi\), that is, all its entries are nonnegative and all its rows sum to
    one. \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Indeed,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{x \in \S} p_{x',x} = \sum_{x \in \S} \P[X_1 = x\,|\,X_{0} = x'] = \P[X_1
    \in \S \,|\,X_{0} = x'] = 1 \]
  prefs: []
  type: TYPE_NORMAL
- en: by the properties of the conditional probability. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: In matrix form, the condition can be stated as \(P \mathbf{1} = \mathbf{1}\),
    where \(\mathbf{1}\) is an all-one vector of the appropriate size.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that any transition matrix is stochastic. Conversely, any stochastic
    matrix is the transition matrix of a Markov chain. That is, we can specify a Markov
    chain by choosing the number of states \(n\), an initial distribution over \(\mathcal{S}
    = [n]\) and a stochastic matrix \(P \in \mathbb{R}^{n\times n}\). Row \(i\) of
    \(P\) stipulates the probability distribution of the next state given that we
    are currently at state \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Robot Vacuum)** Suppose a robot vacuum roams around a large
    mansion with the following rooms: \(1=\mathrm{Study}\), \(2=\mathrm{Hall}\), \(3=\mathrm{Lounge}\),
    \(4=\mathrm{Library}\), \(5=\mathrm{Billiard\ Room}\), \(6=\mathrm{Dining\ Room}\),
    \(7=\mathrm{Conservatory}\), \(8=\mathrm{Ball\ Room}\), \(9=\mathrm{Kitchen}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** A wrench (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Roomba](../Images/208cda012f226f8db1d9e4a27b9cbdc4.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it is done cleaning a room, it moves to another one nearby according to
    the following stochastic matrix (check it is stochastic!):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0 & 0.8 & 0 & 0.2 & 0 & 0 & 0 & 0 & 0\\
    0.3 & 0 & 0.2 & 0 & 0 & 0.5 & 0 & 0 & 0\\ 0 & 0.6 & 0 & 0 & 0 & 0.4 & 0 & 0 &
    0\\ 0.1 & 0.1 & 0 & 0 & 0.8 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0.25 & 0 & 0 & 0.75
    & 0 & 0\\ 0 & 0.15 & 0.15 & 0 & 0 & 0 & 0 & 0.35 & 0.35\\ 0 & 0 & 0 & 0 & 0 &
    0 & 0 & 1 & 0\\ 0 & 0 & 0 & 0 & 0.3 & 0.4 & 0.2 & 0 & 0.1\\ 0 & 0 & 0 & 0 & 0
    & 1 & 0 & 0 & 0 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the initial distribution \(\bmu\) is uniform over the state space and
    let \(X_t\) be the room the vacuum is in at iteration \(t\). Then \((X_t)_{t\geq
    0}\) is a Markov chain. Unlike our previous examples, \(P\) is not symmetric.
    In particular, its rows sum to \(1\) but its columns do not. (Check it!) \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: When both rows and columns sum to \(1\), we say that \(P\) is doubly stochastic.
  prefs: []
  type: TYPE_NORMAL
- en: With the notation just introduced, the distribution of a sample path simplifies
    further to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \mu_{x_0} \prod_{t=1}^T p_{x_{t-1},x_t}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: This formula has a remarkable consequence. The marginal distribution of \(X_s\)
    is a matrix power of \(P\). As usual, we denote by \(P^s\) the \(s\)-th matrix
    power of \(P\). Recall also that \(\bmu\) is a row vector.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Time Marginals)** \(\idx{time marginals theorem}\xdi\) For any
    \(s \geq 1\) and \(x_s \in \S\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_s = x_s] = \left(\bmu P^s\right)_{x_s}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* The idea is to think of \(\P[X_s = x_s]\) as the time \(s\) marginal
    over all trajectories up to time \(s\) – quantities we know how to compute the
    probabilities of. Then we use the *Distribution of a Sample Path* and “pushe the
    sums in.” This is easier seen on a simple case. We do the case \(s=2\) first.'
  prefs: []
  type: TYPE_NORMAL
- en: Summing over all trajectories up to time \(2\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X_2 = x_2]\\ &= \sum_{x_0 \in \S} \sum_{x_{1} \in \S} \P[X_0
    = x_0, X_1 = x_1, X_2 = x_2]\\ &= \sum_{x_0 \in \S} \sum_{x_{1} \in \S} \mu_{x_0}
    p_{x_{0},x_1} p_{x_{1},x_2}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Distribution of a Sample Path*.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing the sum over \(x_1\) in, this becomes
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \sum_{x_0 \in \S} \mu_{x_0} \sum_{x_{1} \in \S} p_{x_{0},x_1}
    p_{x_{1},x_2}\\ &= \sum_{x_0 \in \S} \mu_{x_0} (P^2)_{x_{0},x_2}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we recognized the definition of a matrix product – here \(P^2\). The result
    then follows.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For any \(s\), by definition of a marginal,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_s = x_s] = \sum_{x_0, \ldots, x_{s-1} \in \S} \P[X_0 = x_0, X_1 = x_1,\ldots,X_{s-1}
    = x_{s-1}, X_s = x_s]. \]
  prefs: []
  type: TYPE_NORMAL
- en: Using the *Distribution of a Sample Path* in the time-homogeneous case, this
    evaluates to
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[X_s = x_s] &= \sum_{x_0, \ldots, x_{s-1} \in \S} \mu_{x_0}
    \prod_{t=1}^s p_{x_{t-1},x_t}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The sum can be simplified by pushing the individual sums as far into the summand
    as possible
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\sum_{x_0, \ldots, x_{s-1} \in \S} \mu_{x_0} \prod_{t=1}^{s}
    p_{x_{t-1},x_t}\\ & \quad = \sum_{x_0 \in \S} \mu_{x_0} \sum_{x_{1} \in \S} p_{x_{0},x_{1}}
    \cdots \sum_{x_{s-2} \in \S} p_{x_{s-3},x_{s-2}} \sum_{x_{s-1} \in \S} p_{x_{s-2},x_{s-1}}
    \,p_{x_{s-1},x_s}\\ & \quad = \sum_{x_0 \in \S} \mu_{x_0} \sum_{x_{1} \in \S}
    p_{x_{0},x_{1}} \cdots \sum_{x_{s-2} \in \S} p_{x_{s-3},x_{s-2}} \, \left(P^2\right)_{x_{s-2},
    x_s} \\ & \quad = \sum_{x_0 \in \S} \mu_{x_0} \sum_{x_{1} \in \S} p_{x_{0},x_{1}}
    \cdots \sum_{x_{s-3} \in \S} p_{x_{s-4},x_{s-3}} \, \left(P^3\right)_{x_{s-3},
    x_s} \\ & \quad = \cdots \\ & \quad = \left(\bmu P^s\right)_{x_s}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where on the second line we recognized the innermost sum as a matrix product,
    then proceeded similarly. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: The special case \(\bmu = \mathbf{e}_x^T\) gives that for any \(x, y \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_s = y\,|\,X_0 = x] = (\boldsymbol{\mu} P^s)_y = (\mathbf{e}_x^T P^s)_y
    = (P^s)_{x,y}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Weather Model, continued)** Suppose day \(0\) is \(\mathrm{Dry}\),
    that is, the initial distribution is \(\bmu = (1,0)^T\). What is the probability
    that it is \(\mathrm{Wet}\) on day \(2\)? We apply the formula above to get \(\P[X_2
    = 2] = \left(\bmu P^2\right)_{2}\). Note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bmu P^2 &= (1,0)^T \begin{pmatrix} 3/4 & 1/4\\ 1/4 & 3/4 \end{pmatrix}
    \begin{pmatrix} 3/4 & 1/4\\ 1/4 & 3/4 \end{pmatrix}\\ &= (3/4,1/4)^T \begin{pmatrix}
    3/4 & 1/4\\ 1/4 & 3/4 \end{pmatrix}\\ &= (10/16,6/16)^T\\ &= (5/8,3/8)^T. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So the answer is \(3/8\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: It will be useful later on to observe that the *Time Marginals Theorem* generalizes
    to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_t = x_t\,|\,X_s = x_s] = (P^{t-s})_{x_s,x_t}, \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(s \leq t\).
  prefs: []
  type: TYPE_NORMAL
- en: In the time-homogeneous case, an alternative way to represent a transition matrix
    is with a directed graph showing all possible transitions.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Transition Graph)** \(\idx{transition graph}\xdi\) Let \((X_t)_{t
    \geq 0}\) be a Markov chain over the state space \(\mathcal{S} = [n]\) with transition
    matrix \(P = (p_{i,j})_{i,j=1}^{n}\). The transition graph (or state transition
    diagram) of \((X_t)_{t \geq 0}\) is a directed graph with vertices \([n]\) and
    a directed edge from \(i\) to \(j\) if and only if \(p_{i,j} > 0\). We often associate
    a weight \(p_{i,j}\) to that edge. \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Returning to our *Robot Vacuum Example*, the transition
    graph of the chain can be obtained by thinking of \(P\) as the weighted adjacency
    matrix of the transition graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We define a graph from its adjancency matrix. See [`networkx.from_numpy_array()`](https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_numpy_array.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Drawing edge weights on a directed graph in a readable fashion is not straighforward.
    We will not do this here.
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]</details> ![../../_images/3ed7b79c7b64ae82a443d682f19bec765a062c280a163d9734fd0ce480d6d157.png](../Images/e3496ddf45cc8074ed0952cee5403722.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Once we have specified a transition matrix (and an initial distribution), we
    can simulate the corresponding Markov chain. This is useful to compute (approximately)
    probabilities of complex events through the law of large numbers. Here is some
    code to generate one sample path up to some given time \(T\). We assume that the
    state space is \([n]\). We use [`rng.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html)
    to generate each transition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** Let’s try with our *Robot Vacuum*. We take the initial
    distribution to be the uniform distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For example, we can use a simulation to approximate the expected number of times
    that room \(9\) is visited up to time \(10\). To do this, we run the simulation
    a large number of times (say \(1000\)) and count the average number of visits
    to \(9\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Markov Decision Processes (MDPs) are a framework for modeling
    decision making in situations where outcomes are partly random and partly under
    the control of a decision maker. Ask your favorite AI chatbot to explain the basic
    components of an MDP and how they relate to Markov chains. Discuss some applications
    of MDPs, such as in robotics or game theory. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following is true about the transition matrix \(P\) of a
    Markov chain?'
  prefs: []
  type: TYPE_NORMAL
- en: a) All entries of \(P\) are non-negative, and all columns sum to one.
  prefs: []
  type: TYPE_NORMAL
- en: b) All entries of \(P\) are non-negative, and all rows sum to one.
  prefs: []
  type: TYPE_NORMAL
- en: c) All entries of \(P\) are non-negative, and both rows and columns sum to one.
  prefs: []
  type: TYPE_NORMAL
- en: d) All entries of \(P\) are non-negative, and either rows or columns sum to
    one, but not both.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** What is the *Markov Property*?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The past and future are independent.
  prefs: []
  type: TYPE_NORMAL
- en: b) The past and future are independent given the present.
  prefs: []
  type: TYPE_NORMAL
- en: c) The present and future are independent given the past.
  prefs: []
  type: TYPE_NORMAL
- en: d) The past, present, and future are all independent.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Consider a Markov chain \((X_t)_{t \ge 0}\) on state space \(S\). Which
    of the following equations is a direct consequence of the *Markov Property*?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbb{P}[X_{t+1} = x_{t+1} | X_t = x_t] = \mathbb{P}[X_{t+1} = x_{t+1}]\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbb{P}[X_{t+1} = x_{t+1} | X_t = x_t, X_{t-1} = x_{t-1}] = \mathbb{P}[X_{t+1}
    = x_{t+1} | X_t = x_t]\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbb{P}[X_{t+1} = x_{t+1} | X_t = x_t] = \mathbb{P}[X_{t+1} = x_{t+1}
    | X_0 = x_0]\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbb{P}[X_{t+1} = x_{t+1} | X_t = x_t, X_{t-1} = x_{t-1}] = \mathbb{P}[X_{t+1}
    = x_{t+1}]\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Consider a Markov chain \((X_t)_{t\geq0}\) with transition matrix \(P
    = (p_{i,j})_{i,j}\) and initial distribution \(\mu\). Which of the following is
    true about the distribution of a sample path \((X_0, X_1, \ldots, X_T)\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbb{P}[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \mu_{x_0} \prod_{t=1}^T
    p_{x_{t-1},x_t}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbb{P}[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \mu_{x_0} \sum_{t=1}^T
    p_{x_{t-1},x_t}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbb{P}[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \prod_{t=0}^T \mu_{x_t}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbb{P}[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \sum_{t=0}^T \mu_{x_t}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** In the random walk on the Petersen graph example, if the current state
    is vertex 9, what is the probability of transitioning to vertex 4 in the next
    step?'
  prefs: []
  type: TYPE_NORMAL
- en: a) 0
  prefs: []
  type: TYPE_NORMAL
- en: b) 1/10
  prefs: []
  type: TYPE_NORMAL
- en: c) 1/3
  prefs: []
  type: TYPE_NORMAL
- en: d) 1
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states that “the transition matrix
    \(P\) is a stochastic matrix, that is, all its entries are nonnegative and all
    its rows sum to one.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The text summarizes the *Markov Property* as
    “the past and the future are independent given the present.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: b. Justification: This is a direct statement of the *Markov Property*,
    where the future state \(X_{t+1}\) depends only on the present state \(X_t\) and
    not on the past state \(X_{t-1}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: a. Justification: The text states the *Distribution of a Sample
    Path*:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \mu_{x_0} \prod_{t=1}^T
    p_{x_{t-1},x_t}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: c. Justification: In the Petersen graph, each vertex has 3 neighbors,
    and the random walk chooses one uniformly at random.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1\. Basic definitions[#](#basic-definitions "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A discrete-time Markov chain\(\idx{Markov chain}\xdi\) is a stochastic process\(\idx{stochastic
    process}\xdi\), i.e., a collection of random variables in this case indexed by
    time. We assume that the random variables take values in a common finite state
    space \(\S\). What makes it “Markovian” is that “it forgets the past” in the sense
    that “its future only depends on its present state.” More formally:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Discrete-Time Markov Chain)** The sequence of random variables
    \((X_t)_{t \geq 0} = (X_0, X_1, X_2, \ldots)\) taking values in the finite state
    space \(\S\) is a Markov chain if: for all \(t \geq 1\) and all \(x_0,x_1,\ldots,x_t
    \in \S\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (*)\qquad\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0
    = x_0] = \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}] \]
  prefs: []
  type: TYPE_NORMAL
- en: provided the conditional probabilities are well-defined. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: To be clear, the event in the conditioning is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \{X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0 = x_0\} = \{X_{t-1} = x_{t-1}\}
    \cap \{X_{t-2} = x_{t-2}\} \cap \cdots \cap \{X_0 = x_0\}. \]
  prefs: []
  type: TYPE_NORMAL
- en: It will sometimes be convenient to assume that the common state space \(\S\)
    is of the form \([m] = \{1,\ldots,m\}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Weather Model)** Here is a simple weather model. Every day
    is either \(\mathrm{Dry}\) or \(\mathrm{Wet}\). We model the transitions as Markovian;
    intuitively, we assume that tomorrow’s weather only depends - in a random fashion
    independent of the past - on today’s weather. Say the weather changes with \(25\%\)
    chance. More formally, let \(X_t \in \mathcal{S}\) be the weather on day \(t\)
    with \(\mathcal{S} = \{\mathrm{Dry}, \mathrm{Wet}\}\). Assume that \(X_0 = \mathrm{Dry}\)
    and let \((Z_t)_{t \geq 0}\) be an i.i.d. (i.e., independent, identically distributed)
    sequence of random variables taking values in \(\{\mathrm{Same}, \mathrm{Change}\}\)
    satisfying'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[Z_t = \mathrm{Same}] = 1 - \mathbb{P}[Z_t = \mathrm{Change}] =
    3/4. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then define for all \(t \geq 0\)
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} X_{t+1} = f(X_t, Z_t) = \begin{cases} X_t & \text{if $Z_t =
    \mathrm{Same}$},\\ \mathrm{Wet} & \text{if $X_t = \mathrm{Dry}$ and $Z_t = \mathrm{Change}$},\\
    \mathrm{Dry} & \text{if $X_t = \mathrm{Wet}$ and $Z_t = \mathrm{Change}$}. \end{cases}
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'We claim that \((X_t)_{t \geq 0}\) is a Markov chain. We use two observations:'
  prefs: []
  type: TYPE_NORMAL
- en: 1- By composition,
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_1 = f(X_0, Z_0), \]\[ X_2 = f(X_1,Z_1) = f(f(X_0,Z_0),Z_1), \]\[ X_3 =
    f(X_2,Z_2) = f(f(X_1,Z_1),Z_2) = f(f(f(X_0,Z_0),Z_1),Z_2), \]
  prefs: []
  type: TYPE_NORMAL
- en: and, more generally,
  prefs: []
  type: TYPE_NORMAL
- en: \[ X_t = f(X_{t-1},Z_{t-1}) = f(f(X_{t-2},Z_{t-2}),Z_{t-1}) = f(f(\cdots f(f(X_0,Z_0),Z_1),\cdots),Z_{t-1})
    \]
  prefs: []
  type: TYPE_NORMAL
- en: is a deterministic function of \(X_0 = \mathrm{Dry}\) and \(Z_0,\ldots,Z_{t-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: 2- For any \(x_1,\ldots,x_t \in \S\), there is precisely one value of \(z \in
    \{\mathrm{Same}, \mathrm{Change}\}\) such that \(x_t = f(x_{t-1}, z)\), i.e.,
    if \(x_t = x_{t-1}\) we must have \(z = \mathrm{Same}\) and if \(x_t \neq x_{t-1}\)
    we must have \(z = \mathrm{Change}\).
  prefs: []
  type: TYPE_NORMAL
- en: Fix \(x_0 = \mathrm{Dry}\). For any \(x_1,\ldots,x_t \in \S\), letting \(z\)
    be as in Observation 2,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0
    = x_0]\\ &= \P[f(X_{t-1}, Z_{t-1}) = x_t \,|\, X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0
    = x_0]\\ &= \P[f(x_{t-1}, Z_{t-1}) = x_t \,|\, X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0
    = x_0]\\ &= \P[Z_{t-1} = z \,|\, X_{t-1} = x_{t-1}, X_{t-2} = x_{t-2},\ldots,X_0
    = x_0]\\ &= \P[Z_{t-1} = z], \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(Z_{t-1}\) is independent of \(Z_{t-2},\ldots,Z_0\) and
    \(X_0\) (which is deterministic), and therefore is independent of \(X_{t-1},\ldots,X_0\)
    by Observation 1\. The same argument shows that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}] = \P[Z_{t-1} = z], \]
  prefs: []
  type: TYPE_NORMAL
- en: and that proves the claim.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, one can pick \(X_0\) according to an initial distribution, independently
    from the sequence \((Z_t)_{t \geq 0}\). The argument above can be adapted to this
    case. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Random Walk on the Petersen Graph)** Let \(G = (V,E)\) be the
    Petersen graph. Each vertex \(i\) has degree \(3\), that is, it has three neighbors
    which we denote \(v_{i,1}, v_{i,2}, v_{i,3}\) in some arbitrary order. For instance,
    denoting the vertices by \(1,\ldots, 10\) as above, vertex \(9\) has neighbors
    \(v_{9,1} = 4, v_{9,2} = 6, v_{9,3} = 7\).'
  prefs: []
  type: TYPE_NORMAL
- en: We consider the following random walk on \(G\). We start at \(X_0 = 1\). Then,
    for each \(t\geq 0\), we let \(X_{t+1}\) be a uniformly chosen neighbor of \(X_t\),
    independently of the previous history. That is, we jump at random from neighbor
    to neighbor. Formally, fix \(X_0 = 1\) and let \((Z_t)_{t \geq 0}\) be an i.i.d.
    sequence of random variables taking values in \(\{1,2,3\}\) satisfying
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[Z_t = 1] = \mathbb{P}[Z_t = 2] = \mathbb{P}[Z_t = 3] = 1/3. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then define, for all \(t \geq 0\), \(X_{t+1} = f(X_t, Z_t) = v_{i,Z_t}\) if
    \(X_t = v_i\).
  prefs: []
  type: TYPE_NORMAL
- en: By an argument similar to the previous example, \((X_t)_{t \geq 0}\) is a Markov
    chain. Also as in the previous example, one can pick \(X_0\) according to an initial
    distribution, independently from the sequence \((Z_t)_{t \geq 0}\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various useful generalizations of the condition \((*)\) in the definition
    of a Markov chain. These are all special cases of what is referred to as the *Markov
    Property*\(\idx{Markov property}\xdi\) which can be summarized as: the past and
    the future are independent given the present. We record a version general enough
    for us here. Let \((X_t)_{t \geq 0}\) be a Markov chain on the state space \(\mathcal{S}\).
    For any integer \(h \geq 0\), \(x_{t-1}\in \mathcal{S}\) and subsets \(\mathcal{P}
    \subseteq \mathcal{S}^{t-1}\), \(\mathcal{F} \subseteq \mathcal{S}^{h+1}\) of
    state sequences of length \(t-1\) and \(h+1\) respectively, it holds that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[(X_t,\ldots,X_{t+h}) \in \mathcal{F}\,|\,X_{t-1} = x_{t-1},
    (X_0,\ldots,X_{t-2}) \in \mathcal{P}] &= \P[(X_t,\ldots,X_{t+h}) \in \mathcal{F}\,|\,X_{t-1}
    = x_{t-1}]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: One important implication of the *Markov Property* is that the distribution
    of a sample path, i.e., an event of the form \(\{X_0 = x_0, X_1 = x_1, \ldots,
    X_T = x_T\}\), simplifies considerably.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Distribution of a Sample Path)** \(\idx{distribution of a sample
    path}\xdi\) For any \(x_0, x_1, \ldots, x_T \in \mathcal{S}\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \P[X_0 = x_0] \,\prod_{t=1}^T
    \,\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}]. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We use the *Multiplication Rule* and the *Markov Property*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We first apply the *Multiplication Rule*'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P\left[\cap_{i=1}^r A_i\right] = \prod_{i=1}^r \P\left[A_i \,\middle|\,
    \cap_{j=1}^{i-1} A_j \right]. \]
  prefs: []
  type: TYPE_NORMAL
- en: with \(A_i = \{X_{i-1} = x_{i-1}\}\) and \(r = T+1\). That gives
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T]\\ &= \P[X_0 =
    x_0] \,\prod_{t=1}^T \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}, \ldots, X_0 = x_0]. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then we use the *Markov Property* to simplify each term in the product. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Weather Model, continued)** Going back to the weather model
    from a previous example, fix \(x_0 = \mathrm{Dry}\) and \(x_1,\ldots,x_t \in \S\).
    Then, by the *Distribution of a Sample Path*,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \P[X_0 = x_0] \,\prod_{t=1}^T
    \,\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}]. \]
  prefs: []
  type: TYPE_NORMAL
- en: By assumption \(\P[X_0 = x_0] = 1\). Moreover, we have previously shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}] = \P[Z_{t-1} = z_{t-1}], \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(z_{t-1} = \mathrm{Same}\) if \(x_t = x_{t-1}\) and \(z_{t-1} = \mathrm{Change}\)
    if \(x_t \neq x_{t-1}\).
  prefs: []
  type: TYPE_NORMAL
- en: Hence, using the distribution of \(Z_t\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \P[X_t = x_t\,|\,X_{t-1} = x_{t-1}] = \begin{cases} 3/4 & \text{if
    $x_t = x_{t-1}$},\\ 1/4 & \text{if $x_t \neq x_{t-1}$}. \end{cases} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'Let \(n_T = |\{0 < t \leq T : x_t = x_{t-1}\}|\) be the number of transitions
    without change. Then,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] &= \P[X_0 = x_0]
    \,\prod_{t=1}^T \,\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}]\\ &= \prod_{t=1}^T\P[Z_{t-1}
    = z_{t-1}]\\ &= (3/4)^{n_T} (1/4)^{T - n_T}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: It will be useful later on to observe that the *Distribution of a Sample Path*
    generalizes to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_{s+1} = x_{s+1}, X_{s+2} = x_{s+2}, \ldots, X_T = x_T\,|\,X_s = x_s]
    = \prod_{t=s+1}^T \,\P[X_t = x_t\,|\,X_{t-1} = x_{t-1}]. \]
  prefs: []
  type: TYPE_NORMAL
- en: Based on the *Distribution of a Sample Path*, in order to specify the distribution
    of the process it suffices to specify
  prefs: []
  type: TYPE_NORMAL
- en: the *initial distribution*\(\idx{initial distribution}\xdi\) \(\mu_x := \P[X_0
    = x]\) for all \(x\); and
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: the *transition probabilities*\(\idx{transition probability}\xdi\) \(\P[X_{t+1}
    = x\,|\,X_{t} = x']\) for all \(t\), \(x\), \(x'\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '7.2.2\. Time-homogeneous case: transition matrix[#](#time-homogeneous-case-transition-matrix
    "Link to this heading")'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It is common to further assume that the process is *time-homogeneous*\(\idx{time-homogeneous
    process}\xdi\), which means that the transition probabilities do not depend on
    \(t\):'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ \P[X_{t+1} =x\,|\,X_{t} = x''] = \P[X_1 =x\,|\,X_{0} = x''] =: p_{x'',x},
    \quad \forall t=1,\ldots \]'
  prefs: []
  type: TYPE_NORMAL
- en: where the last equality is a definition. We can then collect the transition
    probabilities into a matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Transition Matrix)** \(\idx{transition matrix}\xdi\) The
    matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[ P = (p_{x',x})_{x,x' \in \S} \]
  prefs: []
  type: TYPE_NORMAL
- en: is called the transition matrix of the chain. \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: We also let \(\mu_{x} = \P[X_0 = x]\) and we think of \(\bmu = (\mu_{x})_{x
    \in \S}\) as a vector. The convention in Markov chain theory is to think of probability
    distributions such as \(\bmu\) as *row vectors*. We will see later why it simplifies
    the notation somewhat.
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Weather Model, continued)** Going back to the weather model,
    let us number the states as follows: \(1 = \mathrm{Dry}\) and \(2 = \mathrm{Wet}\).
    Then the transition matrix is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 3/4 & 1/4\\ 1/4 & 3/4 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Random Walk on the Petersen Graph, continued)** Consider again
    the random walk on the Petersen graph \(G = (V,E)\). We number the vertices \(1,
    2,\ldots, 10\). To compute the transition matrix, we list for each vertex its
    neighbors and put the value \(1/3\) in the corresponding columns. For instance,
    vertex \(1\) has neighbors \(2\), \(5\) and \(6\), so row \(1\) has \(1/3\) in
    columns \(2\), \(5\), and \(6\). And so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We get:'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0 & 1/3 & 0 & 0 & 1/3 & 1/3 & 0 & 0 & 0
    & 0\\ 1/3 & 0 & 1/3 & 0 & 0 & 0 & 1/3 & 0 & 0 & 0\\ 0 & 1/3 & 0 & 1/3 & 0 & 0
    & 0 & 1/3 & 0 & 0\\ 0 & 0 & 1/3 & 0 & 1/3 & 0 & 0 & 0 & 1/3 & 0\\ 1/3 & 0 & 0
    & 1/3 & 0 & 0 & 0 & 0 & 0 & 1/3\\ 1/3 & 0 & 0 & 0 & 0 & 0 & 0 & 1/3 & 1/3 & 0\\
    0 & 1/3 & 0 & 0 & 0 & 0 & 0 & 0 & 1/3 & 1/3\\ 0 & 0 & 1/3 & 0 & 0 & 1/3 & 0 &
    0 & 0 & 1/3\\ 0 & 0 & 0 & 1/3 & 0 & 1/3 & 1/3 & 0 & 0 & 0\\ 0 & 0 & 0 & 0 & 1/3
    & 0 & 1/3 & 1/3 & 0 & 0 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: We have already encountered a matrix that encodes the neighbors of each vertex,
    the adjacency matrix. Here we can recover the transition matrix by multiplying
    the adjacency matrix by \(1/3\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Transition matrices have a very special structure.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Transition Matrix is Stochastic)** \(\idx{transition matrix
    is stochastic theorem}\xdi\) The transition matrix \(P\) is a stochastic matrix\(\idx{stochastic
    matrix}\xdi\), that is, all its entries are nonnegative and all its rows sum to
    one. \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Indeed,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{x \in \S} p_{x',x} = \sum_{x \in \S} \P[X_1 = x\,|\,X_{0} = x'] = \P[X_1
    \in \S \,|\,X_{0} = x'] = 1 \]
  prefs: []
  type: TYPE_NORMAL
- en: by the properties of the conditional probability. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: In matrix form, the condition can be stated as \(P \mathbf{1} = \mathbf{1}\),
    where \(\mathbf{1}\) is an all-one vector of the appropriate size.
  prefs: []
  type: TYPE_NORMAL
- en: We have seen that any transition matrix is stochastic. Conversely, any stochastic
    matrix is the transition matrix of a Markov chain. That is, we can specify a Markov
    chain by choosing the number of states \(n\), an initial distribution over \(\mathcal{S}
    = [n]\) and a stochastic matrix \(P \in \mathbb{R}^{n\times n}\). Row \(i\) of
    \(P\) stipulates the probability distribution of the next state given that we
    are currently at state \(i\).
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Robot Vacuum)** Suppose a robot vacuum roams around a large
    mansion with the following rooms: \(1=\mathrm{Study}\), \(2=\mathrm{Hall}\), \(3=\mathrm{Lounge}\),
    \(4=\mathrm{Library}\), \(5=\mathrm{Billiard\ Room}\), \(6=\mathrm{Dining\ Room}\),
    \(7=\mathrm{Conservatory}\), \(8=\mathrm{Ball\ Room}\), \(9=\mathrm{Kitchen}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** A wrench (*Credit:* Made with [Midjourney](https://www.midjourney.com/))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Roomba](../Images/208cda012f226f8db1d9e4a27b9cbdc4.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it is done cleaning a room, it moves to another one nearby according to
    the following stochastic matrix (check it is stochastic!):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} P = \begin{pmatrix} 0 & 0.8 & 0 & 0.2 & 0 & 0 & 0 & 0 & 0\\
    0.3 & 0 & 0.2 & 0 & 0 & 0.5 & 0 & 0 & 0\\ 0 & 0.6 & 0 & 0 & 0 & 0.4 & 0 & 0 &
    0\\ 0.1 & 0.1 & 0 & 0 & 0.8 & 0 & 0 & 0 & 0\\ 0 & 0 & 0 & 0.25 & 0 & 0 & 0.75
    & 0 & 0\\ 0 & 0.15 & 0.15 & 0 & 0 & 0 & 0 & 0.35 & 0.35\\ 0 & 0 & 0 & 0 & 0 &
    0 & 0 & 1 & 0\\ 0 & 0 & 0 & 0 & 0.3 & 0.4 & 0.2 & 0 & 0.1\\ 0 & 0 & 0 & 0 & 0
    & 1 & 0 & 0 & 0 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the initial distribution \(\bmu\) is uniform over the state space and
    let \(X_t\) be the room the vacuum is in at iteration \(t\). Then \((X_t)_{t\geq
    0}\) is a Markov chain. Unlike our previous examples, \(P\) is not symmetric.
    In particular, its rows sum to \(1\) but its columns do not. (Check it!) \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: When both rows and columns sum to \(1\), we say that \(P\) is doubly stochastic.
  prefs: []
  type: TYPE_NORMAL
- en: With the notation just introduced, the distribution of a sample path simplifies
    further to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \mu_{x_0} \prod_{t=1}^T p_{x_{t-1},x_t}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: This formula has a remarkable consequence. The marginal distribution of \(X_s\)
    is a matrix power of \(P\). As usual, we denote by \(P^s\) the \(s\)-th matrix
    power of \(P\). Recall also that \(\bmu\) is a row vector.
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Time Marginals)** \(\idx{time marginals theorem}\xdi\) For any
    \(s \geq 1\) and \(x_s \in \S\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_s = x_s] = \left(\bmu P^s\right)_{x_s}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* The idea is to think of \(\P[X_s = x_s]\) as the time \(s\) marginal
    over all trajectories up to time \(s\) – quantities we know how to compute the
    probabilities of. Then we use the *Distribution of a Sample Path* and “pushe the
    sums in.” This is easier seen on a simple case. We do the case \(s=2\) first.'
  prefs: []
  type: TYPE_NORMAL
- en: Summing over all trajectories up to time \(2\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X_2 = x_2]\\ &= \sum_{x_0 \in \S} \sum_{x_{1} \in \S} \P[X_0
    = x_0, X_1 = x_1, X_2 = x_2]\\ &= \sum_{x_0 \in \S} \sum_{x_{1} \in \S} \mu_{x_0}
    p_{x_{0},x_1} p_{x_{1},x_2}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Distribution of a Sample Path*.
  prefs: []
  type: TYPE_NORMAL
- en: Pushing the sum over \(x_1\) in, this becomes
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &= \sum_{x_0 \in \S} \mu_{x_0} \sum_{x_{1} \in \S} p_{x_{0},x_1}
    p_{x_{1},x_2}\\ &= \sum_{x_0 \in \S} \mu_{x_0} (P^2)_{x_{0},x_2}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we recognized the definition of a matrix product – here \(P^2\). The result
    then follows.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* For any \(s\), by definition of a marginal,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_s = x_s] = \sum_{x_0, \ldots, x_{s-1} \in \S} \P[X_0 = x_0, X_1 = x_1,\ldots,X_{s-1}
    = x_{s-1}, X_s = x_s]. \]
  prefs: []
  type: TYPE_NORMAL
- en: Using the *Distribution of a Sample Path* in the time-homogeneous case, this
    evaluates to
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \P[X_s = x_s] &= \sum_{x_0, \ldots, x_{s-1} \in \S} \mu_{x_0}
    \prod_{t=1}^s p_{x_{t-1},x_t}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: The sum can be simplified by pushing the individual sums as far into the summand
    as possible
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\sum_{x_0, \ldots, x_{s-1} \in \S} \mu_{x_0} \prod_{t=1}^{s}
    p_{x_{t-1},x_t}\\ & \quad = \sum_{x_0 \in \S} \mu_{x_0} \sum_{x_{1} \in \S} p_{x_{0},x_{1}}
    \cdots \sum_{x_{s-2} \in \S} p_{x_{s-3},x_{s-2}} \sum_{x_{s-1} \in \S} p_{x_{s-2},x_{s-1}}
    \,p_{x_{s-1},x_s}\\ & \quad = \sum_{x_0 \in \S} \mu_{x_0} \sum_{x_{1} \in \S}
    p_{x_{0},x_{1}} \cdots \sum_{x_{s-2} \in \S} p_{x_{s-3},x_{s-2}} \, \left(P^2\right)_{x_{s-2},
    x_s} \\ & \quad = \sum_{x_0 \in \S} \mu_{x_0} \sum_{x_{1} \in \S} p_{x_{0},x_{1}}
    \cdots \sum_{x_{s-3} \in \S} p_{x_{s-4},x_{s-3}} \, \left(P^3\right)_{x_{s-3},
    x_s} \\ & \quad = \cdots \\ & \quad = \left(\bmu P^s\right)_{x_s}, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where on the second line we recognized the innermost sum as a matrix product,
    then proceeded similarly. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: The special case \(\bmu = \mathbf{e}_x^T\) gives that for any \(x, y \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_s = y\,|\,X_0 = x] = (\boldsymbol{\mu} P^s)_y = (\mathbf{e}_x^T P^s)_y
    = (P^s)_{x,y}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Weather Model, continued)** Suppose day \(0\) is \(\mathrm{Dry}\),
    that is, the initial distribution is \(\bmu = (1,0)^T\). What is the probability
    that it is \(\mathrm{Wet}\) on day \(2\)? We apply the formula above to get \(\P[X_2
    = 2] = \left(\bmu P^2\right)_{2}\). Note that'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \bmu P^2 &= (1,0)^T \begin{pmatrix} 3/4 & 1/4\\ 1/4 & 3/4 \end{pmatrix}
    \begin{pmatrix} 3/4 & 1/4\\ 1/4 & 3/4 \end{pmatrix}\\ &= (3/4,1/4)^T \begin{pmatrix}
    3/4 & 1/4\\ 1/4 & 3/4 \end{pmatrix}\\ &= (10/16,6/16)^T\\ &= (5/8,3/8)^T. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: So the answer is \(3/8\). \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: It will be useful later on to observe that the *Time Marginals Theorem* generalizes
    to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_t = x_t\,|\,X_s = x_s] = (P^{t-s})_{x_s,x_t}, \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(s \leq t\).
  prefs: []
  type: TYPE_NORMAL
- en: In the time-homogeneous case, an alternative way to represent a transition matrix
    is with a directed graph showing all possible transitions.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Transition Graph)** \(\idx{transition graph}\xdi\) Let \((X_t)_{t
    \geq 0}\) be a Markov chain over the state space \(\mathcal{S} = [n]\) with transition
    matrix \(P = (p_{i,j})_{i,j=1}^{n}\). The transition graph (or state transition
    diagram) of \((X_t)_{t \geq 0}\) is a directed graph with vertices \([n]\) and
    a directed edge from \(i\) to \(j\) if and only if \(p_{i,j} > 0\). We often associate
    a weight \(p_{i,j}\) to that edge. \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Returning to our *Robot Vacuum Example*, the transition
    graph of the chain can be obtained by thinking of \(P\) as the weighted adjacency
    matrix of the transition graph.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We define a graph from its adjancency matrix. See [`networkx.from_numpy_array()`](https://networkx.org/documentation/stable/reference/generated/networkx.convert_matrix.from_numpy_array.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Drawing edge weights on a directed graph in a readable fashion is not straighforward.
    We will not do this here.
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]</details> ![../../_images/3ed7b79c7b64ae82a443d682f19bec765a062c280a163d9734fd0ce480d6d157.png](../Images/e3496ddf45cc8074ed0952cee5403722.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Once we have specified a transition matrix (and an initial distribution), we
    can simulate the corresponding Markov chain. This is useful to compute (approximately)
    probabilities of complex events through the law of large numbers. Here is some
    code to generate one sample path up to some given time \(T\). We assume that the
    state space is \([n]\). We use [`rng.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html)
    to generate each transition.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** Let’s try with our *Robot Vacuum*. We take the initial
    distribution to be the uniform distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: For example, we can use a simulation to approximate the expected number of times
    that room \(9\) is visited up to time \(10\). To do this, we run the simulation
    a large number of times (say \(1000\)) and count the average number of visits
    to \(9\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Markov Decision Processes (MDPs) are a framework for modeling
    decision making in situations where outcomes are partly random and partly under
    the control of a decision maker. Ask your favorite AI chatbot to explain the basic
    components of an MDP and how they relate to Markov chains. Discuss some applications
    of MDPs, such as in robotics or game theory. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following is true about the transition matrix \(P\) of a
    Markov chain?'
  prefs: []
  type: TYPE_NORMAL
- en: a) All entries of \(P\) are non-negative, and all columns sum to one.
  prefs: []
  type: TYPE_NORMAL
- en: b) All entries of \(P\) are non-negative, and all rows sum to one.
  prefs: []
  type: TYPE_NORMAL
- en: c) All entries of \(P\) are non-negative, and both rows and columns sum to one.
  prefs: []
  type: TYPE_NORMAL
- en: d) All entries of \(P\) are non-negative, and either rows or columns sum to
    one, but not both.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** What is the *Markov Property*?'
  prefs: []
  type: TYPE_NORMAL
- en: a) The past and future are independent.
  prefs: []
  type: TYPE_NORMAL
- en: b) The past and future are independent given the present.
  prefs: []
  type: TYPE_NORMAL
- en: c) The present and future are independent given the past.
  prefs: []
  type: TYPE_NORMAL
- en: d) The past, present, and future are all independent.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Consider a Markov chain \((X_t)_{t \ge 0}\) on state space \(S\). Which
    of the following equations is a direct consequence of the *Markov Property*?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbb{P}[X_{t+1} = x_{t+1} | X_t = x_t] = \mathbb{P}[X_{t+1} = x_{t+1}]\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbb{P}[X_{t+1} = x_{t+1} | X_t = x_t, X_{t-1} = x_{t-1}] = \mathbb{P}[X_{t+1}
    = x_{t+1} | X_t = x_t]\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbb{P}[X_{t+1} = x_{t+1} | X_t = x_t] = \mathbb{P}[X_{t+1} = x_{t+1}
    | X_0 = x_0]\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbb{P}[X_{t+1} = x_{t+1} | X_t = x_t, X_{t-1} = x_{t-1}] = \mathbb{P}[X_{t+1}
    = x_{t+1}]\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Consider a Markov chain \((X_t)_{t\geq0}\) with transition matrix \(P
    = (p_{i,j})_{i,j}\) and initial distribution \(\mu\). Which of the following is
    true about the distribution of a sample path \((X_0, X_1, \ldots, X_T)\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\mathbb{P}[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \mu_{x_0} \prod_{t=1}^T
    p_{x_{t-1},x_t}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\mathbb{P}[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \mu_{x_0} \sum_{t=1}^T
    p_{x_{t-1},x_t}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\mathbb{P}[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \prod_{t=0}^T \mu_{x_t}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\mathbb{P}[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \sum_{t=0}^T \mu_{x_t}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**5** In the random walk on the Petersen graph example, if the current state
    is vertex 9, what is the probability of transitioning to vertex 4 in the next
    step?'
  prefs: []
  type: TYPE_NORMAL
- en: a) 0
  prefs: []
  type: TYPE_NORMAL
- en: b) 1/10
  prefs: []
  type: TYPE_NORMAL
- en: c) 1/3
  prefs: []
  type: TYPE_NORMAL
- en: d) 1
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states that “the transition matrix
    \(P\) is a stochastic matrix, that is, all its entries are nonnegative and all
    its rows sum to one.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The text summarizes the *Markov Property* as
    “the past and the future are independent given the present.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: b. Justification: This is a direct statement of the *Markov Property*,
    where the future state \(X_{t+1}\) depends only on the present state \(X_t\) and
    not on the past state \(X_{t-1}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: a. Justification: The text states the *Distribution of a Sample
    Path*:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[X_0 = x_0, X_1 = x_1, \ldots, X_T = x_T] = \mu_{x_0} \prod_{t=1}^T
    p_{x_{t-1},x_t}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: c. Justification: In the Petersen graph, each vertex has 3 neighbors,
    and the random walk chooses one uniformly at random.'
  prefs: []
  type: TYPE_NORMAL
