<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>8.4. Building blocks of AI 2: stochastic gradient descent#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>8.4. Building blocks of AI 2: stochastic gradient descent#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap08_nn/04_sgd/roch-mmids-nn-sgd.html">https://mmids-textbook.github.io/chap08_nn/04_sgd/roch-mmids-nn-sgd.html</a></blockquote>

<p>Having shown how to compute the gradient, we can now apply gradient descent to fit the data.</p>
<p>To get the full gradient, we consider <span class="math notranslate nohighlight">\(n\)</span> samples <span class="math notranslate nohighlight">\((\mathbf{x}_i,y_i)\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>. At this point, we make the dependence on <span class="math notranslate nohighlight">\((\mathbf{x}_i, y_i)\)</span> explicit. The loss function can be taken as the average of the individual sample contributions, so the gradient is obtained by linearity</p>
<div class="math notranslate nohighlight">
\[
\nabla \left(\frac{1}{n} \sum_{i=1}^n f_{\mathbf{x}_i,y_i}(\mathbf{w})\right)
=  \frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}),
\]</div>
<p>where each term can be computed separately by the procedure above.</p>
<p>We can then apply gradient decent. We start from an arbitrary <span class="math notranslate nohighlight">\(\mathbf{w}^{0}\)</span> and update as follows</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^{t+1} = 
\mathbf{w}^{t} - \alpha_t \left(\frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t})\right).
\]</div>
<p>In a large dataset, computing the sum over all samples may be prohibitively expensive. We present a popular alternative.</p>
<section id="algorithm">
<h2><span class="section-number">8.4.1. </span>Algorithm<a class="headerlink" href="#algorithm" title="Link to this heading">#</a></h2>
<p>In <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a> (SGD)<span class="math notranslate nohighlight">\(\idx{stochastic gradient descent}\xdi\)</span>, a variant of gradient descent, we pick a sample <span class="math notranslate nohighlight">\(I_t\)</span> uniformly at random in <span class="math notranslate nohighlight">\(\{1,\ldots,n\}\)</span> and update as follows</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^{t+1}
= \mathbf{w}^{t} - \alpha_t \nabla f_{\mathbf{x}_{I_t},y_{I_t}}(\mathbf{w}^{t}).
\]</div>
<p>More generally, in the so-called mini-batch version of SGD, we pick instead a uniformly random sub-sample <span class="math notranslate nohighlight">\(\mathcal{B}_t \subseteq \{1,\ldots,n\}\)</span> of size <span class="math notranslate nohighlight">\(b\)</span> without replacement (i.e., all sub-samples of that size have the same probability of being picked)</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^{t+1}
= \mathbf{w}^{t} - \alpha_t \frac{1}{b} \sum_{i\in \mathcal{B}_t} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t}).
\]</div>
<p>The key observation about the two stochastic updates above is that, in expectation, they perform a step of gradient descent. That turns out to be enough and it has computational advantages.</p>
<p><strong>LEMMA</strong> Fix a batch size <span class="math notranslate nohighlight">\(1 \leq b \leq n\)</span> and and an arbitrary vector of parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>. Let <span class="math notranslate nohighlight">\(\mathcal{B} \subseteq \{1,\ldots,n\}\)</span> be a uniformly random sub-sample of size <span class="math notranslate nohighlight">\(b\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\right]
= 
\frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}).
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Because <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is picked uniformly at random (without replacement), for any sub-sample <span class="math notranslate nohighlight">\(B \subseteq \{1,\ldots,n\}\)</span> of size <span class="math notranslate nohighlight">\(b\)</span> without repeats</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[\mathcal{B} = B]
= \frac{1}{\binom{n}{b}}.
\]</div>
<p>So, summing over all such sub-samples, we obtain</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\right]
&amp;= \sum_{B \subseteq \{1,\ldots,n\}} \mathbb{P}[\mathcal{B} = B] \frac{1}{b} \sum_{i\in B} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\
&amp;= \sum_{B \subseteq \{1,\ldots,n\}} \frac{1}{\binom{n}{b}} \frac{1}{b} \sum_{i=1}^n \mathbf{1}\{i \in B\} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\
&amp;= \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}) \frac{1}{b \binom{n}{b}} \sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}.
\end{align*}\]</div>
<p>Computing the internal sum requires a combinatorial argument. Indeed, <span class="math notranslate nohighlight">\(\sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}\)</span> counts the number of ways that <span class="math notranslate nohighlight">\(i\)</span> can be picked in a sub-sample of size <span class="math notranslate nohighlight">\(b\)</span> without repeats. That is <span class="math notranslate nohighlight">\(\binom{n-1}{b-1}\)</span>, which is the number of ways of picking the remaining <span class="math notranslate nohighlight">\(b-1\)</span> elements of <span class="math notranslate nohighlight">\(B\)</span> from the other <span class="math notranslate nohighlight">\(n-1\)</span> possible elements. By definition of the binomial coefficient and the properties of factorials,</p>
<div class="math notranslate nohighlight">
\[
\frac{\binom{n-1}{b-1}}{b \binom{n}{b}}
= \frac{\frac{(n-1)!}{(b-1)! (n-b)!}}{b \frac{n!}{b! (n-b)!}}
= \frac{(n-1)!}{n!} \frac{b!}{b (b-1)!}
= \frac{1}{n}.
\]</div>
<p>Plugging back gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>As a first illustration, we return to logistic regression<span class="math notranslate nohighlight">\(\idx{logistic regression}\xdi\)</span>. Recall that the input data is of the form <span class="math notranslate nohighlight">\(\{(\boldsymbol{\alpha}_i, b_i) : i=1,\ldots, n\}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i = (\alpha_{i,1}, \ldots, \alpha_{i,d}) \in \mathbb{R}^d\)</span> are the features and <span class="math notranslate nohighlight">\(b_i \in \{0,1\}\)</span> is the label. As before we use a matrix representation: <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times d}\)</span> has rows <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i^T\)</span>, <span class="math notranslate nohighlight">\(i = 1,\ldots, n\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\)</span>. We want to solve the minimization problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b})
\]</div>
<p>where the loss is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell(\mathbf{x}; A, \mathbf{b})
&amp;= \frac{1}{n} \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))
- (1-b_i) \log(1- \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\
&amp;= \mathrm{mean}\left(-\mathbf{b} \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1} - \bsigma(A \mathbf{x}))\right).
\end{align*}\]</div>
<p>The gradient was previously computed as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla\ell(\mathbf{x}; A, \mathbf{b})
&amp;= - \frac{1}{n} \sum_{i=1}^n (
b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) 
) \,\boldsymbol{\alpha}_i\\
&amp;= -\frac{1}{n} A^T [\mathbf{b} - \bsigma(A \mathbf{x})].
\end{align*}\]</div>
<p>For the mini-batch version of SGD, we pick a random sub-sample <span class="math notranslate nohighlight">\(\mathcal{B}_t \subseteq \{1,\ldots,n\}\)</span> of size <span class="math notranslate nohighlight">\(B\)</span> and take the step</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{t+1}
= \mathbf{x}^{t} +\beta \frac{1}{B} \sum_{i\in \mathcal{B}_t} (
b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) 
) \,\boldsymbol{\alpha}_i.
\]</div>
<p>We modify our previous code for logistic regression. The only change is to pick a random mini-batch which can be fed to the descent update sub-routine as dataset.</p>
<div class="cell tag_colab-keep docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">desc_update_for_logreg</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">curr_x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">curr_x</span> <span class="o">-</span> <span class="n">beta</span><span class="o">*</span><span class="n">gradient</span>

<span class="k">def</span> <span class="nf">sgd_for_logreg</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> 
                   <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">),</span> <span class="n">batch</span><span class="o">=</span><span class="mi">40</span><span class="p">):</span>
    
    <span class="n">curr_x</span> <span class="o">=</span> <span class="n">init_x</span>
    <span class="n">nsamples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>
        <span class="n">I</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="n">nsamples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">curr_x</span> <span class="o">=</span> <span class="n">desc_update_for_logreg</span><span class="p">(</span>
            <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="n">I</span><span class="p">,:],</span> <span class="n">b</span><span class="p">[</span><span class="n">I</span><span class="p">],</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">curr_x</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We analyze a dataset from [<a class="reference external" href="https://web.stanford.edu/~hastie/ElemStatLearn/">ESL</a>], which can be downloaded <a class="reference external" href="https://web.stanford.edu/~hastie/ElemStatLearn/data.html">here</a>. Quoting [<a class="reference external" href="https://web.stanford.edu/~hastie/ElemStatLearn/">ESL</a>, Section 4.4.2]</p>
<blockquote>
<div><p>The data […] are a subset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried out in three rural areas of the Western Cape, South Africa (Rousseauw et al., 1983). The aim of the study was to establish the intensity of ischemic heart disease risk factors in that high-incidence region. The data represent white males between 15 and 64, and the response variable is the presence or absence of myocardial infarction (MI) at the time of the survey (the overall prevalence of MI was 5.1% in this region). There are 160 cases in our data set, and a sample of 302 controls. These data are described in more detail in Hastie and Tibshirani (1987).</p>
</div></blockquote>
<p>We load the data, which we slightly reformatted and look at a summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'SAHeart.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>sbp</th>
      <th>tobacco</th>
      <th>ldl</th>
      <th>adiposity</th>
      <th>typea</th>
      <th>obesity</th>
      <th>alcohol</th>
      <th>age</th>
      <th>chd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>160.0</td>
      <td>12.00</td>
      <td>5.73</td>
      <td>23.11</td>
      <td>49.0</td>
      <td>25.30</td>
      <td>97.20</td>
      <td>52.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>144.0</td>
      <td>0.01</td>
      <td>4.41</td>
      <td>28.61</td>
      <td>55.0</td>
      <td>28.87</td>
      <td>2.06</td>
      <td>63.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>118.0</td>
      <td>0.08</td>
      <td>3.48</td>
      <td>32.28</td>
      <td>52.0</td>
      <td>29.14</td>
      <td>3.81</td>
      <td>46.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>170.0</td>
      <td>7.50</td>
      <td>6.41</td>
      <td>38.03</td>
      <td>51.0</td>
      <td>31.99</td>
      <td>24.26</td>
      <td>58.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>134.0</td>
      <td>13.60</td>
      <td>3.50</td>
      <td>27.78</td>
      <td>60.0</td>
      <td>25.99</td>
      <td>57.34</td>
      <td>49.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Our goal to predict <code class="docutils literal notranslate"><span class="pre">chd</span></code>, which stands for coronary heart disease, based on the other variables (which are briefly described <a class="reference external" href="https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt">here</a>). We use logistic regression again.</p>
<p>We first construct the data matrices. We only use three of the predictors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">feature</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">'tobacco'</span><span class="p">,</span> <span class="s1">'ldl'</span><span class="p">,</span> <span class="s1">'age'</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1.200e+01 5.730e+00 5.200e+01]
 [1.000e-02 4.410e+00 6.300e+01]
 [8.000e-02 3.480e+00 4.600e+01]
 ...
 [3.000e+00 1.590e+00 5.500e+01]
 [5.400e+00 1.161e+01 4.000e+01]
 [0.000e+00 4.820e+00 4.600e+01]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'chd'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">),</span><span class="mi">1</span><span class="p">)),</span><span class="n">feature</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">label</span>
</pre></div>
</div>
</div>
</div>
<p>We try mini-batch SGD.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">init_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">best_x</span> <span class="o">=</span> <span class="n">sgd_for_logreg</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[-4.06558071  0.07990955  0.18813635  0.04693118]
</pre></div>
</div>
</div>
</div>
<p>The outcome is harder to vizualize. To get a sense of how accurate the result is, we compare our predictions to the true labels. By prediction, let us say that we mean that we predict label <span class="math notranslate nohighlight">\(1\)</span> whenever <span class="math notranslate nohighlight">\(\sigma(\boldsymbol{\alpha}^T \mathbf{x}) &gt; 1/2\)</span>. We try this on the training set. (A better approach would be to split the data into training and testing sets, but we will not do this here.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">logis_acc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">==</span> <span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">logis_acc</span><span class="p">(</span><span class="n">best_x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0.7207792207792207
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
</section>
<section id="example-multinomial-logistic-regression">
<h2><span class="section-number">8.4.2. </span>Example: multinomial logistic regression<a class="headerlink" href="#example-multinomial-logistic-regression" title="Link to this heading">#</a></h2>
<p>We give a concrete example of progressive functions and of the application of backpropagation and SGD.</p>
<p>Recall that a classifier <span class="math notranslate nohighlight">\(h\)</span> takes an input in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> and predicts one of <span class="math notranslate nohighlight">\(K\)</span> possible labels. It will be convenient for reasons that will become clear below to use <a class="reference external" href="https://en.wikipedia.org/wiki/One-hot">one-hot encoding</a><span class="math notranslate nohighlight">\(\idx{one-hot encoding}\xdi\)</span> of the labels. That is, we encode label <span class="math notranslate nohighlight">\(i\)</span> as the <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span>. Here, as usual, <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span> the standard basis of <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span>, i.e., the vector with a <span class="math notranslate nohighlight">\(1\)</span> in entry <span class="math notranslate nohighlight">\(i\)</span> and a <span class="math notranslate nohighlight">\(0\)</span> elsewhere. Furthermore, we allow the output of the classifier to be a probability distribution over the labels <span class="math notranslate nohighlight">\(\{1,\ldots,K\}\)</span>, that is, a vector in</p>
<div class="math notranslate nohighlight">
\[
\Delta_K 
= \left\{
(p_1,\ldots,p_K) \in [0,1]^K \,:\, \sum_{k=1}^K p_k = 1 
\right\}.
\]</div>
<p>Observe that <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span> can itself be thought of as a probability distribution, one that assigns probability one to <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><strong>Background on multinomial logistic regression</strong> We use <a class="reference external" href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">multinomial logistic regression</a><span class="math notranslate nohighlight">\(\idx{multinomial logistic regression}\xdi\)</span> to learn a classifier over <span class="math notranslate nohighlight">\(K\)</span> labels. In multinomial logistic regression, we once again use an affine function of the input data.</p>
<p>This time, we have <span class="math notranslate nohighlight">\(K\)</span> functions that output a score associated to each label. We then transform these scores into a probability distribution over the <span class="math notranslate nohighlight">\(K\)</span> labels. There are many ways of doing this. A standard approach is the <a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a><span class="math notranslate nohighlight">\(\idx{softmax}\xdi\)</span> <span class="math notranslate nohighlight">\(\bgamma = (\gamma_1,\ldots,\gamma_K)\)</span>: for <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^K\)</span></p>
<div class="math notranslate nohighlight">
\[
\gamma_i(\mathbf{z})
= \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}},
\quad i=1,\ldots,K.
\]</div>
<p>To explain the name, observe that the larger inputs are mapped to larger probabilities.</p>
<p>In fact, since a probability distribution must sum to <span class="math notranslate nohighlight">\(1\)</span>, it is determined by the probabilities assigned to the first <span class="math notranslate nohighlight">\(K-1\)</span> labels. In other words, we could drop the score associated to the last label. But the keep the notation simple, we will not do this here.</p>
<p>For each <span class="math notranslate nohighlight">\(k\)</span>, we have a regression function</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^d w^{(k)}_{j} x_{j}
= \mathbf{x}_1^T \mathbf{w}^{(k)},
\quad k=1,\ldots,K
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\)</span> are the parameters with <span class="math notranslate nohighlight">\(\mathbf{w}^{(k)} \in \mathbb{R}^{d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> is the input. A constant term can be included by adding an additional entry <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. As we did in the linear regression case, we assume that this pre-processing has been performed previously. To simplify the notation, we let <span class="math notranslate nohighlight">\(\mathcal{W} \in \mathbb{R}^{K \times d}\)</span> as the matrix with rows <span class="math notranslate nohighlight">\((\mathbf{w}^{(1)})^T,\ldots,(\mathbf{w}^{(K)})^T\)</span>.</p>
<p>The output of the classifier is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\bfh(\mathbf{w})
&amp;= \bgamma\left(\mathcal{W} \mathbf{x}\right),
\end{align*}\]</div>
<p>for <span class="math notranslate nohighlight">\(i=1,\ldots,K\)</span>, where <span class="math notranslate nohighlight">\(\bgamma\)</span> is the softmax function. Note that the latter has no associated parameter.</p>
<p>It remains to define a loss function. To quantify the fit, it is natural to use a notion of distance between probability measures, here between the output <span class="math notranslate nohighlight">\(\mathbf{h}(\mathbf{w}) \in \Delta_K\)</span> and the correct label <span class="math notranslate nohighlight">\(\mathbf{y} \in \{\mathbf{e}_1,\ldots,\mathbf{e}_{K}\} \subseteq \Delta_K\)</span>. There are many such measures. In multinomial logistic regression, we use the Kullback-Leibler divergence, which we have encountered in the context of maximum likelihood estimation. Recall that, for two probability distributions <span class="math notranslate nohighlight">\(\mathbf{p}, \mathbf{q} \in \Delta_K\)</span>, it is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathrm{KL}(\mathbf{p} \| \mathbf{q})
= \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
\]</div>
<p>where it will suffice to restrict ourselves to the case <span class="math notranslate nohighlight">\(\mathbf{q} &gt; \mathbf{0}\)</span> and where we use the convention <span class="math notranslate nohighlight">\(0 \log 0 = 0\)</span> (so that terms with <span class="math notranslate nohighlight">\(p_i = 0\)</span> contribute <span class="math notranslate nohighlight">\(0\)</span> to the sum). Notice that <span class="math notranslate nohighlight">\(\mathbf{p} = \mathbf{q}\)</span> implies <span class="math notranslate nohighlight">\(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) = 0\)</span>. We proved previously that <span class="math notranslate nohighlight">\(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\)</span>, a result known as <em>Gibbs’ inequality</em>.</p>
<p>Going back to the loss function, we use the identity <span class="math notranslate nohighlight">\(\log\frac{\alpha}{\beta} = \log \alpha - \log \beta\)</span> to re-write</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w}))
&amp;= \sum_{i=1}^K y_i \log \frac{y_i}{h_{i}(\mathbf{w})}\\
&amp;= \sum_{i=1}^K y_i \log y_i
- \sum_{i=1}^K y_i \log h_{i}(\mathbf{w}),
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bfh = (h_{1},\ldots,h_{K})\)</span>. Notice that the first term on right-hand side does not depend on <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>. Hence we can ignore it when optimizing <span class="math notranslate nohighlight">\(\mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w}))\)</span>. The remaining term is</p>
<div class="math notranslate nohighlight">
\[
H(\mathbf{y}, \bfh(\mathbf{w}))
= - \sum_{i=1}^K y_i \log h_{i}(\mathbf{w}).
\]</div>
<p>We use it to define our loss function. That is, we set</p>
<div class="math notranslate nohighlight">
\[
\ell(\hat{\mathbf{y}})
= H(\mathbf{y}, \hat{\mathbf{y}})
= - \sum_{i=1}^K y_i \log \hat{y}_{i}.
\]</div>
<p>Finally,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(\mathbf{w})
&amp;= \ell(\bfh(\mathbf{w}))\\
&amp;= H(\mathbf{y}, \bfh(\mathbf{w}))\\
&amp;= H\left(\mathbf{y}, \bgamma\left(\mathcal{W} \mathbf{x}\right)\right)\\
&amp;=  - \sum_{i=1}^K y_i \log\gamma_i\left(\mathcal{W} \mathbf{x}\right).
\end{align*}\]</div>
<p><strong>Computing the gradient</strong> We apply the forward and backpropagation steps from the previous section. We then use the resulting recursions to derive an analytical formula for the gradient.</p>
<p>The forward pass starts with the initialization <span class="math notranslate nohighlight">\(\mathbf{z}_0 := \mathbf{x}\)</span>. The forward layer loop has two steps. Set <span class="math notranslate nohighlight">\(\mathbf{w}_0 = (\mathbf{w}_0^{(1)},\ldots,\mathbf{w}_0^{(K)})\)</span> equal to <span class="math notranslate nohighlight">\(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\)</span>. First we compute</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{1} 
&amp;:= \bfg_0(\mathbf{z}_0,\mathbf{w}_0)
= \mathcal{W}_0 \mathbf{z}_0\\
J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0)
&amp;:=\begin{pmatrix}
A_0 &amp; B_0
\end{pmatrix}
\end{align*}\]</div>
<p>where we defined <span class="math notranslate nohighlight">\(\mathcal{W}_0 \in \mathbb{R}^{K \times d}\)</span> as the matrix with rows <span class="math notranslate nohighlight">\((\mathbf{w}_0^{(1)})^T,\ldots,(\mathbf{w}_0^{(K-1)})^T\)</span>. We have previously computed the Jacobian:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A_0
= \mathbb{A}_{K}[\mathbf{w}_0]
= \mathcal{W}_0
=
\begin{pmatrix}
(\mathbf{w}^{(1)}_0)^T\\
\vdots\\
(\mathbf{w}^{(K)}_0)^T
\end{pmatrix}
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
B_0
= \mathbb{B}_{K}[\mathbf{z}_0]
= I_{K\times K} \otimes \mathbf{z}_0^T
= \begin{pmatrix}
\mathbf{e}_1 \mathbf{z}_0^T
&amp; \cdots &amp; \mathbf{e}_{K}\mathbf{z}_0^T
\end{pmatrix}.
\]</div>
<p>In the second step of the forward layer loop, we compute</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{\mathbf{y}} := \mathbf{z}_2 
&amp;:= \bfg_1(\mathbf{z}_1)
= \bgamma(\mathbf{z}_1)\\
A_1
&amp;:= J_{\bfg_1}(\mathbf{z}_1)
= J_{\bgamma}(\mathbf{z}_1).
\end{align*}\]</div>
<p>So we need to compute the Jacobian of <span class="math notranslate nohighlight">\(\bgamma\)</span>. We divide this computation into two cases.  When <span class="math notranslate nohighlight">\(1 \leq i = j \leq K\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(A_1)_{ii}
&amp;= \frac{\partial}{\partial z_{1,i}} \left[ \gamma_i(\mathbf{z}_1) \right]\\
&amp;= \frac{\partial}{\partial z_{1,i}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K} e^{z_{1,k}}} \right]\\
&amp;= \frac{e^{z_{1,i}}\left(\sum_{k=1}^{K} e^{z_{1,k}}\right) - e^{z_{1,i}}\left(e^{z_{1,i}}\right)}
{\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)^2}\\
&amp;= \gamma_i(\mathbf{z}_1) - \gamma_i(\mathbf{z}_1)^2,
\end{align*}\]</div>
<p>by the <a class="reference external" href="https://en.wikipedia.org/wiki/Quotient_rule">quotient rule</a>.</p>
<p>When <span class="math notranslate nohighlight">\(1 \leq i, j \leq K\)</span> with <span class="math notranslate nohighlight">\(i \neq j\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(A_1)_{ij}
&amp;= \frac{\partial}{\partial z_{1,j}} \left[ \gamma_i(\mathbf{z}_1) \right]\\
&amp;= \frac{\partial}{\partial z_{1,j}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K} e^{z_{1,k}}} \right]\\
&amp;= \frac{- e^{z_{1,i}}\left(e^{z_{1,j}}\right)}
{\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)^2}\\
&amp;= - \gamma_i(\mathbf{z}_1)\gamma_j(\mathbf{z}_1).
\end{align*}\]</div>
<p>In matrix form,</p>
<div class="math notranslate nohighlight">
\[
J_{\bgamma}(\mathbf{z}_1)
= A_1
= \mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T.
\]</div>
<p>The Jacobian of the loss function is</p>
<div class="math notranslate nohighlight">
\[
J_{\ell}(\hat{\mathbf{y}})
= \nabla \left[ - \sum_{i=1}^K y_i \log \hat{y}_{i} \right]^T
= -\left(\frac{y_1}{\hat{y}_{1}}, \ldots, \frac{y_K}{\hat{y}_{K}}\right)^T
= - (\mathbf{y}\oslash\hat{\mathbf{y}})^T,
\]</div>
<p>where recall that <span class="math notranslate nohighlight">\(\oslash\)</span> is the Hadamard division (i.e., element-wise division).</p>
<p>We summarize the whole procedure next.</p>
<p><em>Initialization:</em></p>
<div class="math notranslate nohighlight">
\[\mathbf{z}_0 := \mathbf{x}\]</div>
<p><em>Forward layer loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{1} 
&amp;:= \bfg_0(\mathbf{z}_0, \mathbf{w}_0)
= \mathcal{W}_0 \mathbf{z}_0\\
\begin{pmatrix}
A_0 &amp; B_0
\end{pmatrix}
&amp;:= J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0)
= 
\begin{pmatrix}
\mathbb{A}_{K}[\mathbf{w}_0] &amp; \mathbb{B}_{K}[\mathbf{z}_0]
\end{pmatrix}
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{\mathbf{y}} := \mathbf{z}_2 &amp;:= \bfg_1(\mathbf{z}_1) = \bgamma(\mathbf{z}_1)\\
A_1
&amp;:= J_{\bfg_1}(\mathbf{z}_1)
= \mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T
\end{align*}\]</div>
<p><em>Loss:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
z_3
&amp;:= \ell(\mathbf{z}_2) = - \sum_{i=1}^K y_i \log z_{2,i}\\
\mathbf{p}_2
&amp;:= \nabla {\ell_{\mathbf{y}}}(\mathbf{z}_2)
=  -\left(\frac{y_1}{z_{2,1}}, \ldots, \frac{y_K}{z_{2,K}}\right)
= - \mathbf{y} \oslash \mathbf{z}_2.
\end{align*}\]</div>
<p><em>Backward layer loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{p}_{1} &amp;:= A_1^T \mathbf{p}_{2}
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_{0} &amp;:= B_0^T \mathbf{p}_{1}
\end{align*}\]</div>
<p><em>Output:</em></p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w})
= \mathbf{q}_0,
\]</div>
<p>where recall that <span class="math notranslate nohighlight">\(\mathbf{w} := \mathbf{w}_0\)</span>.</p>
<p>Explicit formulas can be derived from the previous recursion.</p>
<p>We first compute <span class="math notranslate nohighlight">\(\mathbf{p}_1\)</span>. We use the <em>Properties of the Hadamard Product</em>. We get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{p}_1
&amp;= A_1^T \mathbf{p}_{2}\\
&amp;= [\mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T]^T
[- \mathbf{y} \oslash \bgamma(\mathbf{z}_1)]\\
&amp;= - \mathrm{diag}(\bgamma(\mathbf{z}_1)) \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1)) + \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))\\
&amp;= - \mathbf{y} + \bgamma(\mathbf{z}_1) \, \mathbf{1}^T\mathbf{y}\\
&amp;= \bgamma(\mathbf{z}_1) - \mathbf{y},
\end{align*}\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(\sum_{k=1}^{K} y_k = 1\)</span>.</p>
<p>It remains to compute <span class="math notranslate nohighlight">\(\mathbf{q}_0\)</span>. We have by parts (e) and (f) of the <em>Properties of the Kronecker Product</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_{0} 
= B_0^T \mathbf{p}_{1}
&amp;= (I_{K\times K} \otimes \mathbf{z}_0^T)^T
(\bgamma(\mathbf{z}_1) - \mathbf{y})\\
&amp;= ( I_{K\times K} \otimes \mathbf{z}_0)\, (\bgamma(\mathbf{z}_1) - \mathbf{y})\\
&amp;= (\bgamma(\mathbf{z}_1) - \mathbf{y}) \otimes \mathbf{z}_0.
\end{align*}\]</div>
<p>Finally, replacing <span class="math notranslate nohighlight">\(\mathbf{z}_0 = \mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{z}_1 = \mathcal{W} \mathbf{x}\)</span>, the gradient is</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w})
= \mathbf{q}_0 
= (\bgamma\left(\mathcal{W} \mathbf{x}\right) - \mathbf{y}) \otimes \mathbf{x}.
\]</div>
<p>It can be shown that the objective function <span class="math notranslate nohighlight">\(f(\mathbf{w})\)</span> is convex in <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We will use the Fashion-MNIST dataset. This example is inspired by <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">these</a> <a class="reference external" href="https://www.tensorflow.org/tutorials/keras/classification">tutorials</a>. We first check for the availability of GPUs and load the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                      <span class="k">else</span> <span class="p">(</span><span class="s2">"mps"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                            <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Using device:"</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Using device: mps
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'cuda'</span><span class="p">:</span> <span class="c1"># device-specific seeding and settings</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># for multi-GPU</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">elif</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'mps'</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># MPS-specific seeding</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
<span class="n">g</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                               <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                              <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We used <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a>, which provides utilities to load the data in batches for training. We took mini-batches of size <code class="docutils literal notranslate"><span class="pre">BATCH_SIZE</span> <span class="pre">=</span> <span class="pre">32</span></code> and we apply a random permutation of the samples on every pass over the training data (with the option <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>). The function <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.manual_seed.html"><code class="docutils literal notranslate"><span class="pre">torch.manual_seed()</span></code></a> is used to set the global seed for PyTorch operations (e.g., weight initialization). The shuffling in <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> uses its own separate random number generator, which we initialize with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"><code class="docutils literal notranslate"><span class="pre">torch.Generator()</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.manual_seed"><code class="docutils literal notranslate"><span class="pre">manual_seed()</span></code></a>. (You can tell from the fact that <code class="docutils literal notranslate"><span class="pre">seed=42</span></code> that Claude explained that one to me…)</p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot to explain the lines:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span/><span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'cuda'</span><span class="p">:</span> <span class="c1"># device-specific seeding and settings</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># for multi-GPU</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">elif</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'mps'</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># MPS-specific seeding</span>
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p>We implement multinomial logistic regression to learn a classifier for the Fashion-MNIST data. In PyTorch, composition of functions can be achieved with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Sequential</span></code></a>. Our model is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten</span></code></a> layer turns each input image into a vector of size <span class="math notranslate nohighlight">\(784\)</span> (where <span class="math notranslate nohighlight">\(784 = 28^2\)</span> is the number of pixels in each image). After the flattening, we have an affine map from <span class="math notranslate nohighlight">\(\mathbb{R}^{784}\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}^{10}\)</span>. Note that there is no need to pre-process the inputs by adding <span class="math notranslate nohighlight">\(1\)</span>s. A constant term (or “bias variable”) is automatically added by PyTorch (unless one chooses the option <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code class="docutils literal notranslate"><span class="pre">bias=False</span></code></a>). The final output is <span class="math notranslate nohighlight">\(10\)</span>-dimensional.</p>
<p>Finally, we are ready to run an optimization method of our choice on the loss function, which are specified next. There are many <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#algorithms">optimizers</a> available. (See this <a class="reference external" href="https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc">post</a> for a brief explanation of many common optimizers.) Here we use SGD as the optimizer. A quick tutorial is <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html">here</a>. The loss function is the <a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a>, as implemented by <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code></a>, which first takes the softmax and expects the labels to be the actual class labels rather than their one-hot encoding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We implement special functions for training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>    
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">train</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>An epoch is one training iteration where all samples are iterated once (in a randomly shuffled order). In the interest of time, we train for 10 epochs only. But it does better if you train it longer (try it!). On each pass, we compute the output of the current model, use <code class="docutils literal notranslate"><span class="pre">backward()</span></code> to obtain the gradient, and then perform a descent update with <code class="docutils literal notranslate"><span class="pre">step()</span></code>. We also have to reset the gradients first (otherwise they add up by default).</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Because of the issue of <a class="reference external" href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>, we use the <em>test</em> images to assess the performance of the final classifier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>    
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test error: </span><span class="si">{</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">correct</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">size</span><span class="p">))</span><span class="si">:</span><span class="s2">&gt;0.1f</span><span class="si">}</span><span class="s2">% accuracy"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 78.7% accuracy
</pre></div>
</div>
</div>
</div>
<p>To make a prediction, we take a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.functional.softmax</span></code></a> of the output of our model. Recall that it is implicitly included in <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code>, but is not actually part of <code class="docutils literal notranslate"><span class="pre">model</span></code>. (Note that the softmax itself has no parameter.)</p>
<p>As an illustration, we do this for each test image. We use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cat.html"><code class="docutils literal notranslate"><span class="pre">torch.cat</span></code></a> to concatenate a sequence of tensors into a single tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">def</span> <span class="nf">predict_softmax</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">probabilities</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
            
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">predict_softmax</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The result for the first test image is shown below. To make a prediction, we choose the label with the highest probability.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[4.4307188e-04 3.8354204e-04 2.0886613e-03 8.8066678e-04 3.6079765e-03
 1.7791630e-01 1.4651606e-03 2.2466542e-01 4.8245404e-02 5.4030383e-01]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>9
</pre></div>
</div>
</div>
</div>
<p>The truth is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">))</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">: '</span><span class="si">{</span><span class="n">mmids</span><span class="o">.</span><span class="n">FashionMNIST_get_class_name</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">'"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>9: 'Ankle boot'
</pre></div>
</div>
</div>
</div>
<p>Above, <code class="docutils literal notranslate"><span class="pre">next(iter(test_loader))</span></code> loads the first batch of test images. (See <a class="reference external" href="https://docs.python.org/3/tutorial/classes.html#iterators">here</a> for background on iterators in Python.)</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> In stochastic gradient descent (SGD), how is the gradient estimated at each iteration?</p>
<p>a) By computing the gradient over the entire dataset.</p>
<p>b) By using the gradient from the previous iteration.</p>
<p>c) By randomly selecting a subset of sample and computing their gradient.</p>
<p>d) By averaging the gradients of all samples in the dataset.</p>
<p><strong>2</strong> What is the key advantage of using mini-batch SGD over standard SGD?</p>
<p>a) It guarantees faster convergence to the optimal solution.</p>
<p>b) It reduces the variance of the gradient estimate at each iteration.</p>
<p>c) It eliminates the need for computing gradients altogether.</p>
<p>d) It increases the computational cost per iteration.</p>
<p><strong>3</strong> Which of the following statements is true about the update step in stochastic gradient descent?</p>
<p>a) It is always equal to the full gradient descent update.</p>
<p>b) It is always in the opposite direction of the full gradient descent update.</p>
<p>c) It is, on average, equal to the full gradient descent update.</p>
<p>d) It has no relationship to the full gradient descent update.</p>
<p><strong>4</strong> In multinomial logistic regression, what is the role of the softmax function <span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\)</span>?</p>
<p>a) To compute the gradient of the loss function.</p>
<p>b) To normalize the input features.</p>
<p>c) To transform scores into a probability distribution over labels.</p>
<p>d) To update the model parameters during gradient descent.</p>
<p><strong>5</strong> What is the Kullback-Leibler (KL) divergence used for in multinomial logistic regression?</p>
<p>a) To measure the distance between the predicted probabilities and the true labels.</p>
<p>b) To normalize the input features.</p>
<p>c) To update the model parameters during gradient descent.</p>
<p>d) To compute the gradient of the loss function.</p>
<p>Answer for 1: c. Justification: The text states that in SGD, “we pick a sample uniformly at random in <span class="math notranslate nohighlight">\(\{1, ..., n\}\)</span> and update as follows <span class="math notranslate nohighlight">\(\mathbf{w}^{t+1} = \mathbf{w}^t - \alpha_t \nabla f_{\mathbf{x}_{I_t}, y_{I_t}}(\mathbf{w}^t).\)</span>”</p>
<p>Answer for 2: b. Justification: The text implies that mini-batch SGD reduces the variance of the gradient estimate compared to standard SGD, which only uses a single sample.</p>
<p>Answer for 3: c. Justification: The text proves a lemma stating that “in expectation, they [stochastic updates] perform a step of gradient descent.”</p>
<p>Answer for 4: c. Justification: The text defines the softmax function and states that it is used to “transform these scores into a probability distribution over the labels.”</p>
<p>Answer for 5: a. Justification: The text introduces the KL divergence as a “notion of distance between probability measures” and uses it to define the loss function in multinomial logistic regression.</p>
</section>
&#13;

<h2><span class="section-number">8.4.1. </span>Algorithm<a class="headerlink" href="#algorithm" title="Link to this heading">#</a></h2>
<p>In <a class="reference external" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">stochastic gradient descent</a> (SGD)<span class="math notranslate nohighlight">\(\idx{stochastic gradient descent}\xdi\)</span>, a variant of gradient descent, we pick a sample <span class="math notranslate nohighlight">\(I_t\)</span> uniformly at random in <span class="math notranslate nohighlight">\(\{1,\ldots,n\}\)</span> and update as follows</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^{t+1}
= \mathbf{w}^{t} - \alpha_t \nabla f_{\mathbf{x}_{I_t},y_{I_t}}(\mathbf{w}^{t}).
\]</div>
<p>More generally, in the so-called mini-batch version of SGD, we pick instead a uniformly random sub-sample <span class="math notranslate nohighlight">\(\mathcal{B}_t \subseteq \{1,\ldots,n\}\)</span> of size <span class="math notranslate nohighlight">\(b\)</span> without replacement (i.e., all sub-samples of that size have the same probability of being picked)</p>
<div class="math notranslate nohighlight">
\[
\mathbf{w}^{t+1}
= \mathbf{w}^{t} - \alpha_t \frac{1}{b} \sum_{i\in \mathcal{B}_t} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}^{t}).
\]</div>
<p>The key observation about the two stochastic updates above is that, in expectation, they perform a step of gradient descent. That turns out to be enough and it has computational advantages.</p>
<p><strong>LEMMA</strong> Fix a batch size <span class="math notranslate nohighlight">\(1 \leq b \leq n\)</span> and and an arbitrary vector of parameters <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>. Let <span class="math notranslate nohighlight">\(\mathcal{B} \subseteq \{1,\ldots,n\}\)</span> be a uniformly random sub-sample of size <span class="math notranslate nohighlight">\(b\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\right]
= 
\frac{1}{n} \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}).
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> Because <span class="math notranslate nohighlight">\(\mathcal{B}\)</span> is picked uniformly at random (without replacement), for any sub-sample <span class="math notranslate nohighlight">\(B \subseteq \{1,\ldots,n\}\)</span> of size <span class="math notranslate nohighlight">\(b\)</span> without repeats</p>
<div class="math notranslate nohighlight">
\[
\mathbb{P}[\mathcal{B} = B]
= \frac{1}{\binom{n}{b}}.
\]</div>
<p>So, summing over all such sub-samples, we obtain</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbb{E}\left[\frac{1}{b} \sum_{i\in \mathcal{B}} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\right]
&amp;= \sum_{B \subseteq \{1,\ldots,n\}} \mathbb{P}[\mathcal{B} = B] \frac{1}{b} \sum_{i\in B} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\
&amp;= \sum_{B \subseteq \{1,\ldots,n\}} \frac{1}{\binom{n}{b}} \frac{1}{b} \sum_{i=1}^n \mathbf{1}\{i \in B\} \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w})\\
&amp;= \sum_{i=1}^n \nabla f_{\mathbf{x}_i,y_i}(\mathbf{w}) \frac{1}{b \binom{n}{b}} \sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}.
\end{align*}\]</div>
<p>Computing the internal sum requires a combinatorial argument. Indeed, <span class="math notranslate nohighlight">\(\sum_{B \subseteq \{1,\ldots,n\}} \mathbf{1}\{i \in B\}\)</span> counts the number of ways that <span class="math notranslate nohighlight">\(i\)</span> can be picked in a sub-sample of size <span class="math notranslate nohighlight">\(b\)</span> without repeats. That is <span class="math notranslate nohighlight">\(\binom{n-1}{b-1}\)</span>, which is the number of ways of picking the remaining <span class="math notranslate nohighlight">\(b-1\)</span> elements of <span class="math notranslate nohighlight">\(B\)</span> from the other <span class="math notranslate nohighlight">\(n-1\)</span> possible elements. By definition of the binomial coefficient and the properties of factorials,</p>
<div class="math notranslate nohighlight">
\[
\frac{\binom{n-1}{b-1}}{b \binom{n}{b}}
= \frac{\frac{(n-1)!}{(b-1)! (n-b)!}}{b \frac{n!}{b! (n-b)!}}
= \frac{(n-1)!}{n!} \frac{b!}{b (b-1)!}
= \frac{1}{n}.
\]</div>
<p>Plugging back gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>As a first illustration, we return to logistic regression<span class="math notranslate nohighlight">\(\idx{logistic regression}\xdi\)</span>. Recall that the input data is of the form <span class="math notranslate nohighlight">\(\{(\boldsymbol{\alpha}_i, b_i) : i=1,\ldots, n\}\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i = (\alpha_{i,1}, \ldots, \alpha_{i,d}) \in \mathbb{R}^d\)</span> are the features and <span class="math notranslate nohighlight">\(b_i \in \{0,1\}\)</span> is the label. As before we use a matrix representation: <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times d}\)</span> has rows <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i^T\)</span>, <span class="math notranslate nohighlight">\(i = 1,\ldots, n\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\)</span>. We want to solve the minimization problem</p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b})
\]</div>
<p>where the loss is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\ell(\mathbf{x}; A, \mathbf{b})
&amp;= \frac{1}{n} \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))
- (1-b_i) \log(1- \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\
&amp;= \mathrm{mean}\left(-\mathbf{b} \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1} - \bsigma(A \mathbf{x}))\right).
\end{align*}\]</div>
<p>The gradient was previously computed as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla\ell(\mathbf{x}; A, \mathbf{b})
&amp;= - \frac{1}{n} \sum_{i=1}^n (
b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) 
) \,\boldsymbol{\alpha}_i\\
&amp;= -\frac{1}{n} A^T [\mathbf{b} - \bsigma(A \mathbf{x})].
\end{align*}\]</div>
<p>For the mini-batch version of SGD, we pick a random sub-sample <span class="math notranslate nohighlight">\(\mathcal{B}_t \subseteq \{1,\ldots,n\}\)</span> of size <span class="math notranslate nohighlight">\(B\)</span> and take the step</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{t+1}
= \mathbf{x}^{t} +\beta \frac{1}{B} \sum_{i\in \mathcal{B}_t} (
b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) 
) \,\boldsymbol{\alpha}_i.
\]</div>
<p>We modify our previous code for logistic regression. The only change is to pick a random mini-batch which can be fed to the descent update sub-routine as dataset.</p>
<div class="cell tag_colab-keep docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span> 
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">A</span> <span class="o">@</span> <span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="n">b</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">A</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="p">(</span><span class="n">b</span> <span class="o">-</span> <span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">))</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">desc_update_for_logreg</span><span class="p">(</span><span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="n">gradient</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">curr_x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">curr_x</span> <span class="o">-</span> <span class="n">beta</span><span class="o">*</span><span class="n">gradient</span>

<span class="k">def</span> <span class="nf">sgd_for_logreg</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> 
                   <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e5</span><span class="p">),</span> <span class="n">batch</span><span class="o">=</span><span class="mi">40</span><span class="p">):</span>
    
    <span class="n">curr_x</span> <span class="o">=</span> <span class="n">init_x</span>
    <span class="n">nsamples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">niters</span><span class="p">):</span>
        <span class="n">I</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="n">nsamples</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">curr_x</span> <span class="o">=</span> <span class="n">desc_update_for_logreg</span><span class="p">(</span>
            <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">[</span><span class="n">I</span><span class="p">,:],</span> <span class="n">b</span><span class="p">[</span><span class="n">I</span><span class="p">],</span> <span class="n">curr_x</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">curr_x</span>
</pre></div>
</div>
</div>
</div>
<p><strong>NUMERICAL CORNER:</strong> We analyze a dataset from [<a class="reference external" href="https://web.stanford.edu/~hastie/ElemStatLearn/">ESL</a>], which can be downloaded <a class="reference external" href="https://web.stanford.edu/~hastie/ElemStatLearn/data.html">here</a>. Quoting [<a class="reference external" href="https://web.stanford.edu/~hastie/ElemStatLearn/">ESL</a>, Section 4.4.2]</p>
<blockquote>
<div><p>The data […] are a subset of the Coronary Risk-Factor Study (CORIS) baseline survey, carried out in three rural areas of the Western Cape, South Africa (Rousseauw et al., 1983). The aim of the study was to establish the intensity of ischemic heart disease risk factors in that high-incidence region. The data represent white males between 15 and 64, and the response variable is the presence or absence of myocardial infarction (MI) at the time of the survey (the overall prevalence of MI was 5.1% in this region). There are 160 cases in our data set, and a sample of 302 controls. These data are described in more detail in Hastie and Tibshirani (1987).</p>
</div></blockquote>
<p>We load the data, which we slightly reformatted and look at a summary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'SAHeart.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>sbp</th>
      <th>tobacco</th>
      <th>ldl</th>
      <th>adiposity</th>
      <th>typea</th>
      <th>obesity</th>
      <th>alcohol</th>
      <th>age</th>
      <th>chd</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>160.0</td>
      <td>12.00</td>
      <td>5.73</td>
      <td>23.11</td>
      <td>49.0</td>
      <td>25.30</td>
      <td>97.20</td>
      <td>52.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>144.0</td>
      <td>0.01</td>
      <td>4.41</td>
      <td>28.61</td>
      <td>55.0</td>
      <td>28.87</td>
      <td>2.06</td>
      <td>63.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>118.0</td>
      <td>0.08</td>
      <td>3.48</td>
      <td>32.28</td>
      <td>52.0</td>
      <td>29.14</td>
      <td>3.81</td>
      <td>46.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>170.0</td>
      <td>7.50</td>
      <td>6.41</td>
      <td>38.03</td>
      <td>51.0</td>
      <td>31.99</td>
      <td>24.26</td>
      <td>58.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>134.0</td>
      <td>13.60</td>
      <td>3.50</td>
      <td>27.78</td>
      <td>60.0</td>
      <td>25.99</td>
      <td>57.34</td>
      <td>49.0</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Our goal to predict <code class="docutils literal notranslate"><span class="pre">chd</span></code>, which stands for coronary heart disease, based on the other variables (which are briefly described <a class="reference external" href="https://web.stanford.edu/~hastie/ElemStatLearn/datasets/SAheart.info.txt">here</a>). We use logistic regression again.</p>
<p>We first construct the data matrices. We only use three of the predictors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">feature</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s1">'tobacco'</span><span class="p">,</span> <span class="s1">'ldl'</span><span class="p">,</span> <span class="s1">'age'</span><span class="p">]]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1.200e+01 5.730e+00 5.200e+01]
 [1.000e-02 4.410e+00 6.300e+01]
 [8.000e-02 3.480e+00 4.600e+01]
 ...
 [3.000e+00 1.590e+00 5.500e+01]
 [5.400e+00 1.161e+01 4.000e+01]
 [0.000e+00 4.820e+00 4.600e+01]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">label</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'chd'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">label</span><span class="p">),</span><span class="mi">1</span><span class="p">)),</span><span class="n">feature</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">label</span>
</pre></div>
</div>
</div>
</div>
<p>We try mini-batch SGD.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">init_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">best_x</span> <span class="o">=</span> <span class="n">sgd_for_logreg</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">grad_fn</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">niters</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1e6</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">best_x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[-4.06558071  0.07990955  0.18813635  0.04693118]
</pre></div>
</div>
</div>
</div>
<p>The outcome is harder to vizualize. To get a sense of how accurate the result is, we compare our predictions to the true labels. By prediction, let us say that we mean that we predict label <span class="math notranslate nohighlight">\(1\)</span> whenever <span class="math notranslate nohighlight">\(\sigma(\boldsymbol{\alpha}^T \mathbf{x}) &gt; 1/2\)</span>. We try this on the training set. (A better approach would be to split the data into training and testing sets, but we will not do this here.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">logis_acc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">pred_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">==</span> <span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">logis_acc</span><span class="p">(</span><span class="n">best_x</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>0.7207792207792207
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
&#13;

<h2><span class="section-number">8.4.2. </span>Example: multinomial logistic regression<a class="headerlink" href="#example-multinomial-logistic-regression" title="Link to this heading">#</a></h2>
<p>We give a concrete example of progressive functions and of the application of backpropagation and SGD.</p>
<p>Recall that a classifier <span class="math notranslate nohighlight">\(h\)</span> takes an input in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> and predicts one of <span class="math notranslate nohighlight">\(K\)</span> possible labels. It will be convenient for reasons that will become clear below to use <a class="reference external" href="https://en.wikipedia.org/wiki/One-hot">one-hot encoding</a><span class="math notranslate nohighlight">\(\idx{one-hot encoding}\xdi\)</span> of the labels. That is, we encode label <span class="math notranslate nohighlight">\(i\)</span> as the <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span>. Here, as usual, <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span> the standard basis of <span class="math notranslate nohighlight">\(\mathbb{R}^K\)</span>, i.e., the vector with a <span class="math notranslate nohighlight">\(1\)</span> in entry <span class="math notranslate nohighlight">\(i\)</span> and a <span class="math notranslate nohighlight">\(0\)</span> elsewhere. Furthermore, we allow the output of the classifier to be a probability distribution over the labels <span class="math notranslate nohighlight">\(\{1,\ldots,K\}\)</span>, that is, a vector in</p>
<div class="math notranslate nohighlight">
\[
\Delta_K 
= \left\{
(p_1,\ldots,p_K) \in [0,1]^K \,:\, \sum_{k=1}^K p_k = 1 
\right\}.
\]</div>
<p>Observe that <span class="math notranslate nohighlight">\(\mathbf{e}_i\)</span> can itself be thought of as a probability distribution, one that assigns probability one to <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><strong>Background on multinomial logistic regression</strong> We use <a class="reference external" href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">multinomial logistic regression</a><span class="math notranslate nohighlight">\(\idx{multinomial logistic regression}\xdi\)</span> to learn a classifier over <span class="math notranslate nohighlight">\(K\)</span> labels. In multinomial logistic regression, we once again use an affine function of the input data.</p>
<p>This time, we have <span class="math notranslate nohighlight">\(K\)</span> functions that output a score associated to each label. We then transform these scores into a probability distribution over the <span class="math notranslate nohighlight">\(K\)</span> labels. There are many ways of doing this. A standard approach is the <a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">softmax function</a><span class="math notranslate nohighlight">\(\idx{softmax}\xdi\)</span> <span class="math notranslate nohighlight">\(\bgamma = (\gamma_1,\ldots,\gamma_K)\)</span>: for <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^K\)</span></p>
<div class="math notranslate nohighlight">
\[
\gamma_i(\mathbf{z})
= \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}},
\quad i=1,\ldots,K.
\]</div>
<p>To explain the name, observe that the larger inputs are mapped to larger probabilities.</p>
<p>In fact, since a probability distribution must sum to <span class="math notranslate nohighlight">\(1\)</span>, it is determined by the probabilities assigned to the first <span class="math notranslate nohighlight">\(K-1\)</span> labels. In other words, we could drop the score associated to the last label. But the keep the notation simple, we will not do this here.</p>
<p>For each <span class="math notranslate nohighlight">\(k\)</span>, we have a regression function</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^d w^{(k)}_{j} x_{j}
= \mathbf{x}_1^T \mathbf{w}^{(k)},
\quad k=1,\ldots,K
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\)</span> are the parameters with <span class="math notranslate nohighlight">\(\mathbf{w}^{(k)} \in \mathbb{R}^{d}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^d\)</span> is the input. A constant term can be included by adding an additional entry <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. As we did in the linear regression case, we assume that this pre-processing has been performed previously. To simplify the notation, we let <span class="math notranslate nohighlight">\(\mathcal{W} \in \mathbb{R}^{K \times d}\)</span> as the matrix with rows <span class="math notranslate nohighlight">\((\mathbf{w}^{(1)})^T,\ldots,(\mathbf{w}^{(K)})^T\)</span>.</p>
<p>The output of the classifier is</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\bfh(\mathbf{w})
&amp;= \bgamma\left(\mathcal{W} \mathbf{x}\right),
\end{align*}\]</div>
<p>for <span class="math notranslate nohighlight">\(i=1,\ldots,K\)</span>, where <span class="math notranslate nohighlight">\(\bgamma\)</span> is the softmax function. Note that the latter has no associated parameter.</p>
<p>It remains to define a loss function. To quantify the fit, it is natural to use a notion of distance between probability measures, here between the output <span class="math notranslate nohighlight">\(\mathbf{h}(\mathbf{w}) \in \Delta_K\)</span> and the correct label <span class="math notranslate nohighlight">\(\mathbf{y} \in \{\mathbf{e}_1,\ldots,\mathbf{e}_{K}\} \subseteq \Delta_K\)</span>. There are many such measures. In multinomial logistic regression, we use the Kullback-Leibler divergence, which we have encountered in the context of maximum likelihood estimation. Recall that, for two probability distributions <span class="math notranslate nohighlight">\(\mathbf{p}, \mathbf{q} \in \Delta_K\)</span>, it is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathrm{KL}(\mathbf{p} \| \mathbf{q})
= \sum_{i=1}^K p_i \log \frac{p_i}{q_i}
\]</div>
<p>where it will suffice to restrict ourselves to the case <span class="math notranslate nohighlight">\(\mathbf{q} &gt; \mathbf{0}\)</span> and where we use the convention <span class="math notranslate nohighlight">\(0 \log 0 = 0\)</span> (so that terms with <span class="math notranslate nohighlight">\(p_i = 0\)</span> contribute <span class="math notranslate nohighlight">\(0\)</span> to the sum). Notice that <span class="math notranslate nohighlight">\(\mathbf{p} = \mathbf{q}\)</span> implies <span class="math notranslate nohighlight">\(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) = 0\)</span>. We proved previously that <span class="math notranslate nohighlight">\(\mathrm{KL}(\mathbf{p} \| \mathbf{q}) \geq 0\)</span>, a result known as <em>Gibbs’ inequality</em>.</p>
<p>Going back to the loss function, we use the identity <span class="math notranslate nohighlight">\(\log\frac{\alpha}{\beta} = \log \alpha - \log \beta\)</span> to re-write</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w}))
&amp;= \sum_{i=1}^K y_i \log \frac{y_i}{h_{i}(\mathbf{w})}\\
&amp;= \sum_{i=1}^K y_i \log y_i
- \sum_{i=1}^K y_i \log h_{i}(\mathbf{w}),
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\bfh = (h_{1},\ldots,h_{K})\)</span>. Notice that the first term on right-hand side does not depend on <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>. Hence we can ignore it when optimizing <span class="math notranslate nohighlight">\(\mathrm{KL}(\mathbf{y} \| \bfh(\mathbf{w}))\)</span>. The remaining term is</p>
<div class="math notranslate nohighlight">
\[
H(\mathbf{y}, \bfh(\mathbf{w}))
= - \sum_{i=1}^K y_i \log h_{i}(\mathbf{w}).
\]</div>
<p>We use it to define our loss function. That is, we set</p>
<div class="math notranslate nohighlight">
\[
\ell(\hat{\mathbf{y}})
= H(\mathbf{y}, \hat{\mathbf{y}})
= - \sum_{i=1}^K y_i \log \hat{y}_{i}.
\]</div>
<p>Finally,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(\mathbf{w})
&amp;= \ell(\bfh(\mathbf{w}))\\
&amp;= H(\mathbf{y}, \bfh(\mathbf{w}))\\
&amp;= H\left(\mathbf{y}, \bgamma\left(\mathcal{W} \mathbf{x}\right)\right)\\
&amp;=  - \sum_{i=1}^K y_i \log\gamma_i\left(\mathcal{W} \mathbf{x}\right).
\end{align*}\]</div>
<p><strong>Computing the gradient</strong> We apply the forward and backpropagation steps from the previous section. We then use the resulting recursions to derive an analytical formula for the gradient.</p>
<p>The forward pass starts with the initialization <span class="math notranslate nohighlight">\(\mathbf{z}_0 := \mathbf{x}\)</span>. The forward layer loop has two steps. Set <span class="math notranslate nohighlight">\(\mathbf{w}_0 = (\mathbf{w}_0^{(1)},\ldots,\mathbf{w}_0^{(K)})\)</span> equal to <span class="math notranslate nohighlight">\(\mathbf{w} = (\mathbf{w}^{(1)},\ldots,\mathbf{w}^{(K)})\)</span>. First we compute</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{1} 
&amp;:= \bfg_0(\mathbf{z}_0,\mathbf{w}_0)
= \mathcal{W}_0 \mathbf{z}_0\\
J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0)
&amp;:=\begin{pmatrix}
A_0 &amp; B_0
\end{pmatrix}
\end{align*}\]</div>
<p>where we defined <span class="math notranslate nohighlight">\(\mathcal{W}_0 \in \mathbb{R}^{K \times d}\)</span> as the matrix with rows <span class="math notranslate nohighlight">\((\mathbf{w}_0^{(1)})^T,\ldots,(\mathbf{w}_0^{(K-1)})^T\)</span>. We have previously computed the Jacobian:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A_0
= \mathbb{A}_{K}[\mathbf{w}_0]
= \mathcal{W}_0
=
\begin{pmatrix}
(\mathbf{w}^{(1)}_0)^T\\
\vdots\\
(\mathbf{w}^{(K)}_0)^T
\end{pmatrix}
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
B_0
= \mathbb{B}_{K}[\mathbf{z}_0]
= I_{K\times K} \otimes \mathbf{z}_0^T
= \begin{pmatrix}
\mathbf{e}_1 \mathbf{z}_0^T
&amp; \cdots &amp; \mathbf{e}_{K}\mathbf{z}_0^T
\end{pmatrix}.
\]</div>
<p>In the second step of the forward layer loop, we compute</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{\mathbf{y}} := \mathbf{z}_2 
&amp;:= \bfg_1(\mathbf{z}_1)
= \bgamma(\mathbf{z}_1)\\
A_1
&amp;:= J_{\bfg_1}(\mathbf{z}_1)
= J_{\bgamma}(\mathbf{z}_1).
\end{align*}\]</div>
<p>So we need to compute the Jacobian of <span class="math notranslate nohighlight">\(\bgamma\)</span>. We divide this computation into two cases.  When <span class="math notranslate nohighlight">\(1 \leq i = j \leq K\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(A_1)_{ii}
&amp;= \frac{\partial}{\partial z_{1,i}} \left[ \gamma_i(\mathbf{z}_1) \right]\\
&amp;= \frac{\partial}{\partial z_{1,i}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K} e^{z_{1,k}}} \right]\\
&amp;= \frac{e^{z_{1,i}}\left(\sum_{k=1}^{K} e^{z_{1,k}}\right) - e^{z_{1,i}}\left(e^{z_{1,i}}\right)}
{\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)^2}\\
&amp;= \gamma_i(\mathbf{z}_1) - \gamma_i(\mathbf{z}_1)^2,
\end{align*}\]</div>
<p>by the <a class="reference external" href="https://en.wikipedia.org/wiki/Quotient_rule">quotient rule</a>.</p>
<p>When <span class="math notranslate nohighlight">\(1 \leq i, j \leq K\)</span> with <span class="math notranslate nohighlight">\(i \neq j\)</span>,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(A_1)_{ij}
&amp;= \frac{\partial}{\partial z_{1,j}} \left[ \gamma_i(\mathbf{z}_1) \right]\\
&amp;= \frac{\partial}{\partial z_{1,j}} \left[ \frac{e^{z_{1,i}}}{\sum_{k=1}^{K} e^{z_{1,k}}} \right]\\
&amp;= \frac{- e^{z_{1,i}}\left(e^{z_{1,j}}\right)}
{\left(\sum_{k=1}^{K} e^{z_{1,k}}\right)^2}\\
&amp;= - \gamma_i(\mathbf{z}_1)\gamma_j(\mathbf{z}_1).
\end{align*}\]</div>
<p>In matrix form,</p>
<div class="math notranslate nohighlight">
\[
J_{\bgamma}(\mathbf{z}_1)
= A_1
= \mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T.
\]</div>
<p>The Jacobian of the loss function is</p>
<div class="math notranslate nohighlight">
\[
J_{\ell}(\hat{\mathbf{y}})
= \nabla \left[ - \sum_{i=1}^K y_i \log \hat{y}_{i} \right]^T
= -\left(\frac{y_1}{\hat{y}_{1}}, \ldots, \frac{y_K}{\hat{y}_{K}}\right)^T
= - (\mathbf{y}\oslash\hat{\mathbf{y}})^T,
\]</div>
<p>where recall that <span class="math notranslate nohighlight">\(\oslash\)</span> is the Hadamard division (i.e., element-wise division).</p>
<p>We summarize the whole procedure next.</p>
<p><em>Initialization:</em></p>
<div class="math notranslate nohighlight">
\[\mathbf{z}_0 := \mathbf{x}\]</div>
<p><em>Forward layer loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}_{1} 
&amp;:= \bfg_0(\mathbf{z}_0, \mathbf{w}_0)
= \mathcal{W}_0 \mathbf{z}_0\\
\begin{pmatrix}
A_0 &amp; B_0
\end{pmatrix}
&amp;:= J_{\bfg_0}(\mathbf{z}_0,\mathbf{w}_0)
= 
\begin{pmatrix}
\mathbb{A}_{K}[\mathbf{w}_0] &amp; \mathbb{B}_{K}[\mathbf{z}_0]
\end{pmatrix}
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{\mathbf{y}} := \mathbf{z}_2 &amp;:= \bfg_1(\mathbf{z}_1) = \bgamma(\mathbf{z}_1)\\
A_1
&amp;:= J_{\bfg_1}(\mathbf{z}_1)
= \mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T
\end{align*}\]</div>
<p><em>Loss:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
z_3
&amp;:= \ell(\mathbf{z}_2) = - \sum_{i=1}^K y_i \log z_{2,i}\\
\mathbf{p}_2
&amp;:= \nabla {\ell_{\mathbf{y}}}(\mathbf{z}_2)
=  -\left(\frac{y_1}{z_{2,1}}, \ldots, \frac{y_K}{z_{2,K}}\right)
= - \mathbf{y} \oslash \mathbf{z}_2.
\end{align*}\]</div>
<p><em>Backward layer loop:</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{p}_{1} &amp;:= A_1^T \mathbf{p}_{2}
\end{align*}\]</div>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_{0} &amp;:= B_0^T \mathbf{p}_{1}
\end{align*}\]</div>
<p><em>Output:</em></p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w})
= \mathbf{q}_0,
\]</div>
<p>where recall that <span class="math notranslate nohighlight">\(\mathbf{w} := \mathbf{w}_0\)</span>.</p>
<p>Explicit formulas can be derived from the previous recursion.</p>
<p>We first compute <span class="math notranslate nohighlight">\(\mathbf{p}_1\)</span>. We use the <em>Properties of the Hadamard Product</em>. We get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{p}_1
&amp;= A_1^T \mathbf{p}_{2}\\
&amp;= [\mathrm{diag}(\bgamma(\mathbf{z}_1)) - \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T]^T
[- \mathbf{y} \oslash \bgamma(\mathbf{z}_1)]\\
&amp;= - \mathrm{diag}(\bgamma(\mathbf{z}_1)) \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1)) + \bgamma(\mathbf{z}_1) \, \bgamma(\mathbf{z}_1)^T \, (\mathbf{y} \oslash \bgamma(\mathbf{z}_1))\\
&amp;= - \mathbf{y} + \bgamma(\mathbf{z}_1) \, \mathbf{1}^T\mathbf{y}\\
&amp;= \bgamma(\mathbf{z}_1) - \mathbf{y},
\end{align*}\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(\sum_{k=1}^{K} y_k = 1\)</span>.</p>
<p>It remains to compute <span class="math notranslate nohighlight">\(\mathbf{q}_0\)</span>. We have by parts (e) and (f) of the <em>Properties of the Kronecker Product</em></p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{q}_{0} 
= B_0^T \mathbf{p}_{1}
&amp;= (I_{K\times K} \otimes \mathbf{z}_0^T)^T
(\bgamma(\mathbf{z}_1) - \mathbf{y})\\
&amp;= ( I_{K\times K} \otimes \mathbf{z}_0)\, (\bgamma(\mathbf{z}_1) - \mathbf{y})\\
&amp;= (\bgamma(\mathbf{z}_1) - \mathbf{y}) \otimes \mathbf{z}_0.
\end{align*}\]</div>
<p>Finally, replacing <span class="math notranslate nohighlight">\(\mathbf{z}_0 = \mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{z}_1 = \mathcal{W} \mathbf{x}\)</span>, the gradient is</p>
<div class="math notranslate nohighlight">
\[
\nabla f(\mathbf{w})
= \mathbf{q}_0 
= (\bgamma\left(\mathcal{W} \mathbf{x}\right) - \mathbf{y}) \otimes \mathbf{x}.
\]</div>
<p>It can be shown that the objective function <span class="math notranslate nohighlight">\(f(\mathbf{w})\)</span> is convex in <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>.</p>
<p><strong>NUMERICAL CORNER:</strong> We will use the Fashion-MNIST dataset. This example is inspired by <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html">these</a> <a class="reference external" href="https://www.tensorflow.org/tutorials/keras/classification">tutorials</a>. We first check for the availability of GPUs and load the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                      <span class="k">else</span> <span class="p">(</span><span class="s2">"mps"</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> 
                            <span class="k">else</span> <span class="s2">"cpu"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Using device:"</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Using device: mps
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'cuda'</span><span class="p">:</span> <span class="c1"># device-specific seeding and settings</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># for multi-GPU</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">elif</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'mps'</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># MPS-specific seeding</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span>
<span class="n">g</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                               <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s1">'./data'</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                              <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>

<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We used <a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"><code class="docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a>, which provides utilities to load the data in batches for training. We took mini-batches of size <code class="docutils literal notranslate"><span class="pre">BATCH_SIZE</span> <span class="pre">=</span> <span class="pre">32</span></code> and we apply a random permutation of the samples on every pass over the training data (with the option <code class="docutils literal notranslate"><span class="pre">shuffle=True</span></code>). The function <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.manual_seed.html"><code class="docutils literal notranslate"><span class="pre">torch.manual_seed()</span></code></a> is used to set the global seed for PyTorch operations (e.g., weight initialization). The shuffling in <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> uses its own separate random number generator, which we initialize with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"><code class="docutils literal notranslate"><span class="pre">torch.Generator()</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator.manual_seed"><code class="docutils literal notranslate"><span class="pre">manual_seed()</span></code></a>. (You can tell from the fact that <code class="docutils literal notranslate"><span class="pre">seed=42</span></code> that Claude explained that one to me…)</p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot to explain the lines:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span/><span class="k">if</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'cuda'</span><span class="p">:</span> <span class="c1"># device-specific seeding and settings</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># for multi-GPU</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">False</span>
<span class="k">elif</span> <span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">'mps'</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">mps</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>  <span class="c1"># MPS-specific seeding</span>
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p>We implement multinomial logistic regression to learn a classifier for the Fashion-MNIST data. In PyTorch, composition of functions can be achieved with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Sequential</span></code></a>. Our model is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.Flatten</span></code></a> layer turns each input image into a vector of size <span class="math notranslate nohighlight">\(784\)</span> (where <span class="math notranslate nohighlight">\(784 = 28^2\)</span> is the number of pixels in each image). After the flattening, we have an affine map from <span class="math notranslate nohighlight">\(\mathbb{R}^{784}\)</span> to <span class="math notranslate nohighlight">\(\mathbb{R}^{10}\)</span>. Note that there is no need to pre-process the inputs by adding <span class="math notranslate nohighlight">\(1\)</span>s. A constant term (or “bias variable”) is automatically added by PyTorch (unless one chooses the option <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"><code class="docutils literal notranslate"><span class="pre">bias=False</span></code></a>). The final output is <span class="math notranslate nohighlight">\(10\)</span>-dimensional.</p>
<p>Finally, we are ready to run an optimization method of our choice on the loss function, which are specified next. There are many <a class="reference external" href="https://pytorch.org/docs/stable/optim.html#algorithms">optimizers</a> available. (See this <a class="reference external" href="https://hackernoon.com/demystifying-different-variants-of-gradient-descent-optimization-algorithm-19ae9ba2e9bc">post</a> for a brief explanation of many common optimizers.) Here we use SGD as the optimizer. A quick tutorial is <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html">here</a>. The loss function is the <a class="reference external" href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a>, as implemented by <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code></a>, which first takes the softmax and expects the labels to be the actual class labels rather than their one-hot encoding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We implement special functions for training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>    
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">train</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>An epoch is one training iteration where all samples are iterated once (in a randomly shuffled order). In the interest of time, we train for 10 epochs only. But it does better if you train it longer (try it!). On each pass, we compute the output of the current model, use <code class="docutils literal notranslate"><span class="pre">backward()</span></code> to obtain the gradient, and then perform a descent update with <code class="docutils literal notranslate"><span class="pre">step()</span></code>. We also have to reset the gradients first (otherwise they add up by default).</p>
<div class="cell tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">training_loop</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Because of the issue of <a class="reference external" href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>, we use the <em>test</em> images to assess the performance of the final classifier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>    
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"Test error: </span><span class="si">{</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="p">(</span><span class="n">correct</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">size</span><span class="p">))</span><span class="si">:</span><span class="s2">&gt;0.1f</span><span class="si">}</span><span class="s2">% accuracy"</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">test</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>Test error: 78.7% accuracy
</pre></div>
</div>
</div>
</div>
<p>To make a prediction, we take a <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html"><code class="docutils literal notranslate"><span class="pre">torch.nn.functional.softmax</span></code></a> of the output of our model. Recall that it is implicitly included in <code class="docutils literal notranslate"><span class="pre">torch.nn.CrossEntropyLoss</span></code>, but is not actually part of <code class="docutils literal notranslate"><span class="pre">model</span></code>. (Note that the softmax itself has no parameter.)</p>
<p>As an illustration, we do this for each test image. We use <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.cat.html"><code class="docutils literal notranslate"><span class="pre">torch.cat</span></code></a> to concatenate a sequence of tensors into a single tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="k">def</span> <span class="nf">predict_softmax</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">probabilities</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">probabilities</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
            
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">predict_softmax</span><span class="p">(</span><span class="n">test_loader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The result for the first test image is shown below. To make a prediction, we choose the label with the highest probability.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[4.4307188e-04 3.8354204e-04 2.0886613e-03 8.8066678e-04 3.6079765e-03
 1.7791630e-01 1.4651606e-03 2.2466542e-01 4.8245404e-02 5.4030383e-01]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>9
</pre></div>
</div>
</div>
</div>
<p>The truth is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">test_loader</span><span class="p">))</span>
<span class="n">images</span> <span class="o">=</span> <span class="n">images</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">"</span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">: '</span><span class="si">{</span><span class="n">mmids</span><span class="o">.</span><span class="n">FashionMNIST_get_class_name</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">}</span><span class="s2">'"</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>9: 'Ankle boot'
</pre></div>
</div>
</div>
</div>
<p>Above, <code class="docutils literal notranslate"><span class="pre">next(iter(test_loader))</span></code> loads the first batch of test images. (See <a class="reference external" href="https://docs.python.org/3/tutorial/classes.html#iterators">here</a> for background on iterators in Python.)</p>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> In stochastic gradient descent (SGD), how is the gradient estimated at each iteration?</p>
<p>a) By computing the gradient over the entire dataset.</p>
<p>b) By using the gradient from the previous iteration.</p>
<p>c) By randomly selecting a subset of sample and computing their gradient.</p>
<p>d) By averaging the gradients of all samples in the dataset.</p>
<p><strong>2</strong> What is the key advantage of using mini-batch SGD over standard SGD?</p>
<p>a) It guarantees faster convergence to the optimal solution.</p>
<p>b) It reduces the variance of the gradient estimate at each iteration.</p>
<p>c) It eliminates the need for computing gradients altogether.</p>
<p>d) It increases the computational cost per iteration.</p>
<p><strong>3</strong> Which of the following statements is true about the update step in stochastic gradient descent?</p>
<p>a) It is always equal to the full gradient descent update.</p>
<p>b) It is always in the opposite direction of the full gradient descent update.</p>
<p>c) It is, on average, equal to the full gradient descent update.</p>
<p>d) It has no relationship to the full gradient descent update.</p>
<p><strong>4</strong> In multinomial logistic regression, what is the role of the softmax function <span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\)</span>?</p>
<p>a) To compute the gradient of the loss function.</p>
<p>b) To normalize the input features.</p>
<p>c) To transform scores into a probability distribution over labels.</p>
<p>d) To update the model parameters during gradient descent.</p>
<p><strong>5</strong> What is the Kullback-Leibler (KL) divergence used for in multinomial logistic regression?</p>
<p>a) To measure the distance between the predicted probabilities and the true labels.</p>
<p>b) To normalize the input features.</p>
<p>c) To update the model parameters during gradient descent.</p>
<p>d) To compute the gradient of the loss function.</p>
<p>Answer for 1: c. Justification: The text states that in SGD, “we pick a sample uniformly at random in <span class="math notranslate nohighlight">\(\{1, ..., n\}\)</span> and update as follows <span class="math notranslate nohighlight">\(\mathbf{w}^{t+1} = \mathbf{w}^t - \alpha_t \nabla f_{\mathbf{x}_{I_t}, y_{I_t}}(\mathbf{w}^t).\)</span>”</p>
<p>Answer for 2: b. Justification: The text implies that mini-batch SGD reduces the variance of the gradient estimate compared to standard SGD, which only uses a single sample.</p>
<p>Answer for 3: c. Justification: The text proves a lemma stating that “in expectation, they [stochastic updates] perform a step of gradient descent.”</p>
<p>Answer for 4: c. Justification: The text defines the softmax function and states that it is used to “transform these scores into a probability distribution over the labels.”</p>
<p>Answer for 5: a. Justification: The text introduces the KL divergence as a “notion of distance between probability measures” and uses it to define the loss function in multinomial logistic regression.</p>
    
</body>
</html>