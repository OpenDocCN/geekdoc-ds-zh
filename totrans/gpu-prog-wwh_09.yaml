- en: Directive-based models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://enccs.github.io/gpu-programming/6-directive-based-models/](https://enccs.github.io/gpu-programming/6-directive-based-models/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*[GPU programming: why, when and how?](../)* **   Directive-based models'
  prefs: []
  type: TYPE_NORMAL
- en: '[Edit on GitHub](https://github.com/ENCCS/gpu-programming/blob/main/content/6-directive-based-models.rst)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs: []
  type: TYPE_NORMAL
- en: What is OpenACC and OpenMP offloading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to write GPU code using directives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: Understand the process of offloading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the differences between OpenACC and OpenMP offloading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the various levels of parallelism on a GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand what is data movement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 40 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 40 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most common directive-based models for GPU parallel programming are OpenMP
    offloading and OpenACC. The parallelization is done by introducing directives
    in places which are targeted for parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenACC** is known to be more **descriptive**, which means the programmer
    uses directives to tell the compiler how/where to parallelize the code and to
    move the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenMP offloading**, on the other hand, is known to be more **prescriptive**,
    where the programmer uses directives to tell the compiler more explicitly how/where
    to parallelize the code, instead of letting the compiler decide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In OpenMP/OpenACC the compiler directives are specified by using **#pragma**
    in C/C++ or as special comments identified by unique sentinels in Fortran. Compilers
    can ignore the directives if the support for OpenMP/OpenACC is not enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'The compiler directives are used for various purposes: for thread creation,
    workload distribution (work sharing), data-environment management, serializing
    sections of code or for synchronization of work among the threads.'
  prefs: []
  type: TYPE_NORMAL
- en: Execution model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenMP and OpenACC use the fork-join model of parallel execution. The program
    begins as a single thread of execution, the **master** thread. Everything is executed
    sequentially until the first parallel region construct is encountered.
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/threads.png](../Images/a943f27c94a700c9a167e37f1d504a7d.png)'
  prefs: []
  type: TYPE_IMG
- en: When a parallel region is encountered, the master thread creates a group of
    threads, becomes the master of this group of threads, and is assigned the thread
    index 0 within the group. There is an implicit barrier at the end of the parallel
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: Offloading Directives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenACC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In OpenACC, one of the most commonly used directives is `kernels`, which defines
    a region to be transferred into a series of kernels to be executed in sequence
    on a GPU. Work sharing is defined automatically for the separate kernels, but
    tuning prospects are limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `kernels`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The other approach of OpenACC to define parallel regions is to use the `parallel`
    directive. Contrary to the `kernels` directive, the `parallel` directive is more
    explicit and requires more analysis by the programmer. Work sharing has to be
    defined manually using the `loop` directive, and refined tuning is possible to
    achieve. The above example can be re-written as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `parallel loop`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes we can obtain a little more performance by guiding the compiler to
    make specific choices. OpenACC has four levels of parallelism for offloading execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**gang** coarse grain: the iterations are distributed among the gangs'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**worker** fine grain: worker’s threads are activated within gangs and iterations
    are shared among the threads'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**vector** each worker activates its threads working in SIMT fashion and the
    work is shared among the threads'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**seq** the iterations are executed sequentially'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By default, `gang`, `worker` and `vector` parallelism are automatically decided
    and applied by the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: The programmer could add clauses like `num_gangs`, `num_workers` and `vector_length`
    within the parallel region to specify the number of gangs, workers and vector
    length.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal numbers are highly dependent on the GPU architecture and the compiler
    implementation though.
  prefs: []
  type: TYPE_NORMAL
- en: There is no thread synchronization at `gang` level, which means there is a risk
    of race condition.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP Offloading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With OpenMP, the `target` directive is used for device offloading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `target` construct'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Compared to the OpenACC’s `kernels` directive, the `target` directive will
    not parallelise the underlying loop at all. To achieve proper parallelisation,
    one needs to be more prescriptive and specify what one wants. OpenMP offloading
    offers multiple levels of parallelism as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '**teams** coarse grain: creates a league of teams and one master thread in
    each team, but no worksharing among the teams'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**distribute** distributes the iterations across the master threads in the
    teams, but no worksharing among the threads within one team'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**parallel do/for** fine grain: threads are activated within one team and worksharing
    among them'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**SIMD** like the `vector` directive in OpenACC'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The programmer can add clauses like `num_teams` and `thread_limit` to specify
    the number of teams and threads within a team.
  prefs: []
  type: TYPE_NORMAL
- en: Threads in a team can synchronize but no synchronization among the teams.
  prefs: []
  type: TYPE_NORMAL
- en: Since OpenMP 5.0, there is a new `loop` directive available, which has a functionality
    similar to the corresponding one in OpenACC.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Mapping between OpenACC/OpenMP directives and GPU (**HPE implementation**)
  prefs: []
  type: TYPE_NORMAL
- en: '| Nvidia | AMD | Fortran OpenACC/OpenMP | C/C++ OpenMP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Threadblock | Work group | gang/teams | teams |'
  prefs: []
  type: TYPE_TB
- en: '| Warp | Wavefront | worker/simd | parallel for simd |'
  prefs: []
  type: TYPE_TB
- en: '| Thread | Work item | vector/simd | parallel for simd |'
  prefs: []
  type: TYPE_TB
- en: Each compiler supports different levels of parallelism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of gang/team/worker/vector_length can be chosen arbitrarily by the
    user but there are limits defined by the implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum thread/grid/block size can be found via `rocminfo`/`nvaccelinfo`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise: Change the levels of parallelism'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to change the levels of parallelism using clauses.
    First compile and run one of the examples to find out the default number of blocks
    and threads set by the compiler at runtime. To make a change, try adding clauses
    like `num_gangs`, `num_workers`, `vector_length` for OpenACC and `num_teams`,
    `thread_limit` for OpenMP offloading.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to set the environment by executing `export CRAY_ACC_DEBUG=2` at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'How to compile and run the code interactively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Example of a trivially parallelizable vector addition problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Data Movement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Due to distinct memory spaces on host and device, transferring data becomes
    inevitable. New directives are needed to specify how variables are transferred
    from the host to the device data environment. Commonly transferred items consist
    of arrays (array sections), scalars, pointers, and structure elements. Various
    data clauses used for data movement are summarised in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `OpenMP` | `OpenACC` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `map(to:list)` | `copyin(list)` | On entering the region, variables in the
    list are initialized on the device using the original values from the host |'
  prefs: []
  type: TYPE_TB
- en: '| `map(from:list)` | `copyout(list)` | At the end of the target region, the
    values from variables in the list are copied into the original variables on the
    host. On entering the region, the initial value of the variables on the device
    is not initialized |'
  prefs: []
  type: TYPE_TB
- en: '| `map(tofrom:list)` | `copy(list)` | The effect of both a map-to and a map-from
    |'
  prefs: []
  type: TYPE_TB
- en: '| `map(alloc:list)` | `create(list)` | On entering the region, data is allocated
    and uninitialized on the device |'
  prefs: []
  type: TYPE_TB
- en: '| `map(delete:list)` | `delete(list)` | Delete data on the device |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When mapping data arrays or pointers, be careful about the array section notation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In C/C++: array[lower-bound:length]. The notation :N is equivalent to 0:N.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Fortran:array[lower-bound:upper-bound]. The notation :N is equivalent to
    1:N.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data region
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The specific data clause combined with the data directive constitutes the start
    of a data region. How the directives create storage, transfer data, and remove
    storage on the device are classified as two categories: structured data region
    and unstructured data region.'
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A structured data region is convenient for providing persistent data on the
    device which could be used for subsequent GPU directives.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for structured data region
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Unstructured Data Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: However it is inconvenient in real applications to use a structured data region.
    An unstructured data region gives more freedom in creating and deleting data on
    the device at any appropriate point.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for unstructured data region
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data Region
  prefs: []
  type: TYPE_NORMAL
- en: Start and end points within a single subroutine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory exists within the data region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unstructured Data Region
  prefs: []
  type: TYPE_NORMAL
- en: Multiple start and end points across different subroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory exists until explicitly deallocated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes, variables need to be synchronized between the host and the device
    memory, e.g. in order to write out variables on the host for debugging or visualization,
    and it is often used in conjunction with unstructured data regions. To control
    the data transfer direction, a motion-clause must be present.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for update directive
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`update` directive can only be used in host code since data movement must be
    initiated from the host, i.e. it may not appear inside of a compute region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in OpenACC, motion-clause “host” has been deprecated and renamed “self”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise: `update`'
  prefs: []
  type: TYPE_NORMAL
- en: Try to figure out the variable values on host and device at each check point.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: '| check point | x on host | x on device |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| check point1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| check point2 | 10 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| check point3 | 10 | 10 |'
  prefs: []
  type: TYPE_TB
- en: 'Exercise: Adding data mapping clauses'
  prefs: []
  type: TYPE_NORMAL
- en: Add proper data mapping clauses explicitly to the directives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Optimize Data Transfers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Explicitly transfer the data as much as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the amount of data mapping between host and device, get rid of unnecessary
    data transfers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to keep the data environment residing on the device as long as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pros of directive-based frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incremental programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porting of existing software requires less work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Same code can be compiled to CPU and GPU versions easily using compiler flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low learning curve, do not need to know low-level hardware details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good portability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[ENCCS lesson on OpenACC](https://enccs.github.io/openacc/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ENCCS lesson on OpenMP for GPU offloading](https://enccs.github.io/openmp-gpu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: OpenACC and OpenMP-offloading enables you to annotate your code with special
    directives to identify areas to be executed in parallel on a GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both allow to fine-tune the distribution of the work to match architecture characteristics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both allow to control the flow of data to/from the GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The directive-based approaches save time compared to lower-level approaches,
    but you need to be mindful of data movement in particular to obtain good performance.
    [Previous](../5-intro-to-gpu-prog-models/ "Introduction to GPU programming models")
    [Next](../7-non-portable-kernel-models/ "Non-portable kernel-based models")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: © Copyright 2023-2024, The contributors.
  prefs: []
  type: TYPE_NORMAL
- en: Built with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme)
    provided by [Read the Docs](https://readthedocs.org). Questions
  prefs: []
  type: TYPE_NORMAL
- en: What is OpenACC and OpenMP offloading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to write GPU code using directives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: Understand the process of offloading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the differences between OpenACC and OpenMP offloading
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand the various levels of parallelism on a GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand what is data movement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 40 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 40 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most common directive-based models for GPU parallel programming are OpenMP
    offloading and OpenACC. The parallelization is done by introducing directives
    in places which are targeted for parallelization.
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenACC** is known to be more **descriptive**, which means the programmer
    uses directives to tell the compiler how/where to parallelize the code and to
    move the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenMP offloading**, on the other hand, is known to be more **prescriptive**,
    where the programmer uses directives to tell the compiler more explicitly how/where
    to parallelize the code, instead of letting the compiler decide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In OpenMP/OpenACC the compiler directives are specified by using **#pragma**
    in C/C++ or as special comments identified by unique sentinels in Fortran. Compilers
    can ignore the directives if the support for OpenMP/OpenACC is not enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'The compiler directives are used for various purposes: for thread creation,
    workload distribution (work sharing), data-environment management, serializing
    sections of code or for synchronization of work among the threads.'
  prefs: []
  type: TYPE_NORMAL
- en: Execution model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenMP and OpenACC use the fork-join model of parallel execution. The program
    begins as a single thread of execution, the **master** thread. Everything is executed
    sequentially until the first parallel region construct is encountered.
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/threads.png](../Images/a943f27c94a700c9a167e37f1d504a7d.png)'
  prefs: []
  type: TYPE_IMG
- en: When a parallel region is encountered, the master thread creates a group of
    threads, becomes the master of this group of threads, and is assigned the thread
    index 0 within the group. There is an implicit barrier at the end of the parallel
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: Offloading Directives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenACC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In OpenACC, one of the most commonly used directives is `kernels`, which defines
    a region to be transferred into a series of kernels to be executed in sequence
    on a GPU. Work sharing is defined automatically for the separate kernels, but
    tuning prospects are limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `kernels`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The other approach of OpenACC to define parallel regions is to use the `parallel`
    directive. Contrary to the `kernels` directive, the `parallel` directive is more
    explicit and requires more analysis by the programmer. Work sharing has to be
    defined manually using the `loop` directive, and refined tuning is possible to
    achieve. The above example can be re-written as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `parallel loop`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes we can obtain a little more performance by guiding the compiler to
    make specific choices. OpenACC has four levels of parallelism for offloading execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**gang** coarse grain: the iterations are distributed among the gangs'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**worker** fine grain: worker’s threads are activated within gangs and iterations
    are shared among the threads'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**vector** each worker activates its threads working in SIMT fashion and the
    work is shared among the threads'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**seq** the iterations are executed sequentially'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By default, `gang`, `worker` and `vector` parallelism are automatically decided
    and applied by the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: The programmer could add clauses like `num_gangs`, `num_workers` and `vector_length`
    within the parallel region to specify the number of gangs, workers and vector
    length.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal numbers are highly dependent on the GPU architecture and the compiler
    implementation though.
  prefs: []
  type: TYPE_NORMAL
- en: There is no thread synchronization at `gang` level, which means there is a risk
    of race condition.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP Offloading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With OpenMP, the `target` directive is used for device offloading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `target` construct'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Compared to the OpenACC’s `kernels` directive, the `target` directive will
    not parallelise the underlying loop at all. To achieve proper parallelisation,
    one needs to be more prescriptive and specify what one wants. OpenMP offloading
    offers multiple levels of parallelism as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '**teams** coarse grain: creates a league of teams and one master thread in
    each team, but no worksharing among the teams'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**distribute** distributes the iterations across the master threads in the
    teams, but no worksharing among the threads within one team'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**parallel do/for** fine grain: threads are activated within one team and worksharing
    among them'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**SIMD** like the `vector` directive in OpenACC'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The programmer can add clauses like `num_teams` and `thread_limit` to specify
    the number of teams and threads within a team.
  prefs: []
  type: TYPE_NORMAL
- en: Threads in a team can synchronize but no synchronization among the teams.
  prefs: []
  type: TYPE_NORMAL
- en: Since OpenMP 5.0, there is a new `loop` directive available, which has a functionality
    similar to the corresponding one in OpenACC.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Mapping between OpenACC/OpenMP directives and GPU (**HPE implementation**)
  prefs: []
  type: TYPE_NORMAL
- en: '| Nvidia | AMD | Fortran OpenACC/OpenMP | C/C++ OpenMP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Threadblock | Work group | gang/teams | teams |'
  prefs: []
  type: TYPE_TB
- en: '| Warp | Wavefront | worker/simd | parallel for simd |'
  prefs: []
  type: TYPE_TB
- en: '| Thread | Work item | vector/simd | parallel for simd |'
  prefs: []
  type: TYPE_TB
- en: Each compiler supports different levels of parallelism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of gang/team/worker/vector_length can be chosen arbitrarily by the
    user but there are limits defined by the implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum thread/grid/block size can be found via `rocminfo`/`nvaccelinfo`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise: Change the levels of parallelism'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to change the levels of parallelism using clauses.
    First compile and run one of the examples to find out the default number of blocks
    and threads set by the compiler at runtime. To make a change, try adding clauses
    like `num_gangs`, `num_workers`, `vector_length` for OpenACC and `num_teams`,
    `thread_limit` for OpenMP offloading.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to set the environment by executing `export CRAY_ACC_DEBUG=2` at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'How to compile and run the code interactively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Example of a trivially parallelizable vector addition problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Data Movement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Due to distinct memory spaces on host and device, transferring data becomes
    inevitable. New directives are needed to specify how variables are transferred
    from the host to the device data environment. Commonly transferred items consist
    of arrays (array sections), scalars, pointers, and structure elements. Various
    data clauses used for data movement are summarised in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `OpenMP` | `OpenACC` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `map(to:list)` | `copyin(list)` | On entering the region, variables in the
    list are initialized on the device using the original values from the host |'
  prefs: []
  type: TYPE_TB
- en: '| `map(from:list)` | `copyout(list)` | At the end of the target region, the
    values from variables in the list are copied into the original variables on the
    host. On entering the region, the initial value of the variables on the device
    is not initialized |'
  prefs: []
  type: TYPE_TB
- en: '| `map(tofrom:list)` | `copy(list)` | The effect of both a map-to and a map-from
    |'
  prefs: []
  type: TYPE_TB
- en: '| `map(alloc:list)` | `create(list)` | On entering the region, data is allocated
    and uninitialized on the device |'
  prefs: []
  type: TYPE_TB
- en: '| `map(delete:list)` | `delete(list)` | Delete data on the device |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When mapping data arrays or pointers, be careful about the array section notation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In C/C++: array[lower-bound:length]. The notation :N is equivalent to 0:N.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Fortran:array[lower-bound:upper-bound]. The notation :N is equivalent to
    1:N.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data region
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The specific data clause combined with the data directive constitutes the start
    of a data region. How the directives create storage, transfer data, and remove
    storage on the device are classified as two categories: structured data region
    and unstructured data region.'
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A structured data region is convenient for providing persistent data on the
    device which could be used for subsequent GPU directives.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for structured data region
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Unstructured Data Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: However it is inconvenient in real applications to use a structured data region.
    An unstructured data region gives more freedom in creating and deleting data on
    the device at any appropriate point.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for unstructured data region
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data Region
  prefs: []
  type: TYPE_NORMAL
- en: Start and end points within a single subroutine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory exists within the data region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unstructured Data Region
  prefs: []
  type: TYPE_NORMAL
- en: Multiple start and end points across different subroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory exists until explicitly deallocated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes, variables need to be synchronized between the host and the device
    memory, e.g. in order to write out variables on the host for debugging or visualization,
    and it is often used in conjunction with unstructured data regions. To control
    the data transfer direction, a motion-clause must be present.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for update directive
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`update` directive can only be used in host code since data movement must be
    initiated from the host, i.e. it may not appear inside of a compute region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in OpenACC, motion-clause “host” has been deprecated and renamed “self”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise: `update`'
  prefs: []
  type: TYPE_NORMAL
- en: Try to figure out the variable values on host and device at each check point.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: '| check point | x on host | x on device |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| check point1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| check point2 | 10 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| check point3 | 10 | 10 |'
  prefs: []
  type: TYPE_TB
- en: 'Exercise: Adding data mapping clauses'
  prefs: []
  type: TYPE_NORMAL
- en: Add proper data mapping clauses explicitly to the directives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Optimize Data Transfers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Explicitly transfer the data as much as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the amount of data mapping between host and device, get rid of unnecessary
    data transfers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to keep the data environment residing on the device as long as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pros of directive-based frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incremental programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porting of existing software requires less work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Same code can be compiled to CPU and GPU versions easily using compiler flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low learning curve, do not need to know low-level hardware details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good portability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[ENCCS lesson on OpenACC](https://enccs.github.io/openacc/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ENCCS lesson on OpenMP for GPU offloading](https://enccs.github.io/openmp-gpu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: OpenACC and OpenMP-offloading enables you to annotate your code with special
    directives to identify areas to be executed in parallel on a GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both allow to fine-tune the distribution of the work to match architecture characteristics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both allow to control the flow of data to/from the GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The directive-based approaches save time compared to lower-level approaches,
    but you need to be mindful of data movement in particular to obtain good performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Execution model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenMP and OpenACC use the fork-join model of parallel execution. The program
    begins as a single thread of execution, the **master** thread. Everything is executed
    sequentially until the first parallel region construct is encountered.
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/threads.png](../Images/a943f27c94a700c9a167e37f1d504a7d.png)'
  prefs: []
  type: TYPE_IMG
- en: When a parallel region is encountered, the master thread creates a group of
    threads, becomes the master of this group of threads, and is assigned the thread
    index 0 within the group. There is an implicit barrier at the end of the parallel
    regions.
  prefs: []
  type: TYPE_NORMAL
- en: Offloading Directives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenACC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In OpenACC, one of the most commonly used directives is `kernels`, which defines
    a region to be transferred into a series of kernels to be executed in sequence
    on a GPU. Work sharing is defined automatically for the separate kernels, but
    tuning prospects are limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `kernels`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'The other approach of OpenACC to define parallel regions is to use the `parallel`
    directive. Contrary to the `kernels` directive, the `parallel` directive is more
    explicit and requires more analysis by the programmer. Work sharing has to be
    defined manually using the `loop` directive, and refined tuning is possible to
    achieve. The above example can be re-written as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `parallel loop`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes we can obtain a little more performance by guiding the compiler to
    make specific choices. OpenACC has four levels of parallelism for offloading execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**gang** coarse grain: the iterations are distributed among the gangs'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**worker** fine grain: worker’s threads are activated within gangs and iterations
    are shared among the threads'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**vector** each worker activates its threads working in SIMT fashion and the
    work is shared among the threads'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**seq** the iterations are executed sequentially'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By default, `gang`, `worker` and `vector` parallelism are automatically decided
    and applied by the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: The programmer could add clauses like `num_gangs`, `num_workers` and `vector_length`
    within the parallel region to specify the number of gangs, workers and vector
    length.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal numbers are highly dependent on the GPU architecture and the compiler
    implementation though.
  prefs: []
  type: TYPE_NORMAL
- en: There is no thread synchronization at `gang` level, which means there is a risk
    of race condition.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP Offloading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With OpenMP, the `target` directive is used for device offloading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `target` construct'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Compared to the OpenACC’s `kernels` directive, the `target` directive will
    not parallelise the underlying loop at all. To achieve proper parallelisation,
    one needs to be more prescriptive and specify what one wants. OpenMP offloading
    offers multiple levels of parallelism as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '**teams** coarse grain: creates a league of teams and one master thread in
    each team, but no worksharing among the teams'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**distribute** distributes the iterations across the master threads in the
    teams, but no worksharing among the threads within one team'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**parallel do/for** fine grain: threads are activated within one team and worksharing
    among them'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**SIMD** like the `vector` directive in OpenACC'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The programmer can add clauses like `num_teams` and `thread_limit` to specify
    the number of teams and threads within a team.
  prefs: []
  type: TYPE_NORMAL
- en: Threads in a team can synchronize but no synchronization among the teams.
  prefs: []
  type: TYPE_NORMAL
- en: Since OpenMP 5.0, there is a new `loop` directive available, which has a functionality
    similar to the corresponding one in OpenACC.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Mapping between OpenACC/OpenMP directives and GPU (**HPE implementation**)
  prefs: []
  type: TYPE_NORMAL
- en: '| Nvidia | AMD | Fortran OpenACC/OpenMP | C/C++ OpenMP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Threadblock | Work group | gang/teams | teams |'
  prefs: []
  type: TYPE_TB
- en: '| Warp | Wavefront | worker/simd | parallel for simd |'
  prefs: []
  type: TYPE_TB
- en: '| Thread | Work item | vector/simd | parallel for simd |'
  prefs: []
  type: TYPE_TB
- en: Each compiler supports different levels of parallelism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of gang/team/worker/vector_length can be chosen arbitrarily by the
    user but there are limits defined by the implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum thread/grid/block size can be found via `rocminfo`/`nvaccelinfo`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise: Change the levels of parallelism'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to change the levels of parallelism using clauses.
    First compile and run one of the examples to find out the default number of blocks
    and threads set by the compiler at runtime. To make a change, try adding clauses
    like `num_gangs`, `num_workers`, `vector_length` for OpenACC and `num_teams`,
    `thread_limit` for OpenMP offloading.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to set the environment by executing `export CRAY_ACC_DEBUG=2` at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'How to compile and run the code interactively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Example of a trivially parallelizable vector addition problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: OpenACC
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In OpenACC, one of the most commonly used directives is `kernels`, which defines
    a region to be transferred into a series of kernels to be executed in sequence
    on a GPU. Work sharing is defined automatically for the separate kernels, but
    tuning prospects are limited.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `kernels`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'The other approach of OpenACC to define parallel regions is to use the `parallel`
    directive. Contrary to the `kernels` directive, the `parallel` directive is more
    explicit and requires more analysis by the programmer. Work sharing has to be
    defined manually using the `loop` directive, and refined tuning is possible to
    achieve. The above example can be re-written as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `parallel loop`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes we can obtain a little more performance by guiding the compiler to
    make specific choices. OpenACC has four levels of parallelism for offloading execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**gang** coarse grain: the iterations are distributed among the gangs'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**worker** fine grain: worker’s threads are activated within gangs and iterations
    are shared among the threads'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**vector** each worker activates its threads working in SIMT fashion and the
    work is shared among the threads'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**seq** the iterations are executed sequentially'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: By default, `gang`, `worker` and `vector` parallelism are automatically decided
    and applied by the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: The programmer could add clauses like `num_gangs`, `num_workers` and `vector_length`
    within the parallel region to specify the number of gangs, workers and vector
    length.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal numbers are highly dependent on the GPU architecture and the compiler
    implementation though.
  prefs: []
  type: TYPE_NORMAL
- en: There is no thread synchronization at `gang` level, which means there is a risk
    of race condition.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP Offloading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With OpenMP, the `target` directive is used for device offloading.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `target` construct'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'Compared to the OpenACC’s `kernels` directive, the `target` directive will
    not parallelise the underlying loop at all. To achieve proper parallelisation,
    one needs to be more prescriptive and specify what one wants. OpenMP offloading
    offers multiple levels of parallelism as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '**teams** coarse grain: creates a league of teams and one master thread in
    each team, but no worksharing among the teams'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**distribute** distributes the iterations across the master threads in the
    teams, but no worksharing among the threads within one team'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**parallel do/for** fine grain: threads are activated within one team and worksharing
    among them'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: '**SIMD** like the `vector` directive in OpenACC'
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The programmer can add clauses like `num_teams` and `thread_limit` to specify
    the number of teams and threads within a team.
  prefs: []
  type: TYPE_NORMAL
- en: Threads in a team can synchronize but no synchronization among the teams.
  prefs: []
  type: TYPE_NORMAL
- en: Since OpenMP 5.0, there is a new `loop` directive available, which has a functionality
    similar to the corresponding one in OpenACC.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Mapping between OpenACC/OpenMP directives and GPU (**HPE implementation**)
  prefs: []
  type: TYPE_NORMAL
- en: '| Nvidia | AMD | Fortran OpenACC/OpenMP | C/C++ OpenMP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Threadblock | Work group | gang/teams | teams |'
  prefs: []
  type: TYPE_TB
- en: '| Warp | Wavefront | worker/simd | parallel for simd |'
  prefs: []
  type: TYPE_TB
- en: '| Thread | Work item | vector/simd | parallel for simd |'
  prefs: []
  type: TYPE_TB
- en: Each compiler supports different levels of parallelism.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of gang/team/worker/vector_length can be chosen arbitrarily by the
    user but there are limits defined by the implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The maximum thread/grid/block size can be found via `rocminfo`/`nvaccelinfo`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise: Change the levels of parallelism'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to change the levels of parallelism using clauses.
    First compile and run one of the examples to find out the default number of blocks
    and threads set by the compiler at runtime. To make a change, try adding clauses
    like `num_gangs`, `num_workers`, `vector_length` for OpenACC and `num_teams`,
    `thread_limit` for OpenMP offloading.
  prefs: []
  type: TYPE_NORMAL
- en: Remember to set the environment by executing `export CRAY_ACC_DEBUG=2` at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'How to compile and run the code interactively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'Example of a trivially parallelizable vector addition problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: Data Movement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Due to distinct memory spaces on host and device, transferring data becomes
    inevitable. New directives are needed to specify how variables are transferred
    from the host to the device data environment. Commonly transferred items consist
    of arrays (array sections), scalars, pointers, and structure elements. Various
    data clauses used for data movement are summarised in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| `OpenMP` | `OpenACC` |  |'
  prefs: []
  type: TYPE_TB
- en: '| `map(to:list)` | `copyin(list)` | On entering the region, variables in the
    list are initialized on the device using the original values from the host |'
  prefs: []
  type: TYPE_TB
- en: '| `map(from:list)` | `copyout(list)` | At the end of the target region, the
    values from variables in the list are copied into the original variables on the
    host. On entering the region, the initial value of the variables on the device
    is not initialized |'
  prefs: []
  type: TYPE_TB
- en: '| `map(tofrom:list)` | `copy(list)` | The effect of both a map-to and a map-from
    |'
  prefs: []
  type: TYPE_TB
- en: '| `map(alloc:list)` | `create(list)` | On entering the region, data is allocated
    and uninitialized on the device |'
  prefs: []
  type: TYPE_TB
- en: '| `map(delete:list)` | `delete(list)` | Delete data on the device |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'When mapping data arrays or pointers, be careful about the array section notation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In C/C++: array[lower-bound:length]. The notation :N is equivalent to 0:N.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Fortran:array[lower-bound:upper-bound]. The notation :N is equivalent to
    1:N.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data region
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The specific data clause combined with the data directive constitutes the start
    of a data region. How the directives create storage, transfer data, and remove
    storage on the device are classified as two categories: structured data region
    and unstructured data region.'
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A structured data region is convenient for providing persistent data on the
    device which could be used for subsequent GPU directives.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for structured data region
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Unstructured Data Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: However it is inconvenient in real applications to use a structured data region.
    An unstructured data region gives more freedom in creating and deleting data on
    the device at any appropriate point.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for unstructured data region
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data Region
  prefs: []
  type: TYPE_NORMAL
- en: Start and end points within a single subroutine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory exists within the data region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unstructured Data Region
  prefs: []
  type: TYPE_NORMAL
- en: Multiple start and end points across different subroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory exists until explicitly deallocated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes, variables need to be synchronized between the host and the device
    memory, e.g. in order to write out variables on the host for debugging or visualization,
    and it is often used in conjunction with unstructured data regions. To control
    the data transfer direction, a motion-clause must be present.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for update directive
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`update` directive can only be used in host code since data movement must be
    initiated from the host, i.e. it may not appear inside of a compute region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in OpenACC, motion-clause “host” has been deprecated and renamed “self”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise: `update`'
  prefs: []
  type: TYPE_NORMAL
- en: Try to figure out the variable values on host and device at each check point.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: '| check point | x on host | x on device |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| check point1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| check point2 | 10 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| check point3 | 10 | 10 |'
  prefs: []
  type: TYPE_TB
- en: 'Exercise: Adding data mapping clauses'
  prefs: []
  type: TYPE_NORMAL
- en: Add proper data mapping clauses explicitly to the directives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: Optimize Data Transfers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Explicitly transfer the data as much as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the amount of data mapping between host and device, get rid of unnecessary
    data transfers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to keep the data environment residing on the device as long as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data region
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The specific data clause combined with the data directive constitutes the start
    of a data region. How the directives create storage, transfer data, and remove
    storage on the device are classified as two categories: structured data region
    and unstructured data region.'
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A structured data region is convenient for providing persistent data on the
    device which could be used for subsequent GPU directives.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for structured data region
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: Unstructured Data Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: However it is inconvenient in real applications to use a structured data region.
    An unstructured data region gives more freedom in creating and deleting data on
    the device at any appropriate point.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for unstructured data region
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data Region
  prefs: []
  type: TYPE_NORMAL
- en: Start and end points within a single subroutine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory exists within the data region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unstructured Data Region
  prefs: []
  type: TYPE_NORMAL
- en: Multiple start and end points across different subroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory exists until explicitly deallocated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes, variables need to be synchronized between the host and the device
    memory, e.g. in order to write out variables on the host for debugging or visualization,
    and it is often used in conjunction with unstructured data regions. To control
    the data transfer direction, a motion-clause must be present.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for update directive
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`update` directive can only be used in host code since data movement must be
    initiated from the host, i.e. it may not appear inside of a compute region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in OpenACC, motion-clause “host” has been deprecated and renamed “self”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise: `update`'
  prefs: []
  type: TYPE_NORMAL
- en: Try to figure out the variable values on host and device at each check point.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: '| check point | x on host | x on device |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| check point1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| check point2 | 10 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| check point3 | 10 | 10 |'
  prefs: []
  type: TYPE_TB
- en: 'Exercise: Adding data mapping clauses'
  prefs: []
  type: TYPE_NORMAL
- en: Add proper data mapping clauses explicitly to the directives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: Structured Data Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A structured data region is convenient for providing persistent data on the
    device which could be used for subsequent GPU directives.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for structured data region
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: Unstructured Data Region
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: However it is inconvenient in real applications to use a structured data region.
    An unstructured data region gives more freedom in creating and deleting data on
    the device at any appropriate point.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for unstructured data region
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: Structured Data Region
  prefs: []
  type: TYPE_NORMAL
- en: Start and end points within a single subroutine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory exists within the data region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unstructured Data Region
  prefs: []
  type: TYPE_NORMAL
- en: Multiple start and end points across different subroutines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory exists until explicitly deallocated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sometimes, variables need to be synchronized between the host and the device
    memory, e.g. in order to write out variables on the host for debugging or visualization,
    and it is often used in conjunction with unstructured data regions. To control
    the data transfer direction, a motion-clause must be present.
  prefs: []
  type: TYPE_NORMAL
- en: Syntax for update directive
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '`update` directive can only be used in host code since data movement must be
    initiated from the host, i.e. it may not appear inside of a compute region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in OpenACC, motion-clause “host” has been deprecated and renamed “self”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise: `update`'
  prefs: []
  type: TYPE_NORMAL
- en: Try to figure out the variable values on host and device at each check point.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: '| check point | x on host | x on device |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| check point1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| check point2 | 10 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| check point3 | 10 | 10 |'
  prefs: []
  type: TYPE_TB
- en: 'Exercise: Adding data mapping clauses'
  prefs: []
  type: TYPE_NORMAL
- en: Add proper data mapping clauses explicitly to the directives
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: Solution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: Optimize Data Transfers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Explicitly transfer the data as much as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce the amount of data mapping between host and device, get rid of unnecessary
    data transfers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Try to keep the data environment residing on the device as long as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pros of directive-based frameworks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incremental programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Porting of existing software requires less work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Same code can be compiled to CPU and GPU versions easily using compiler flag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low learning curve, do not need to know low-level hardware details
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good portability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[ENCCS lesson on OpenACC](https://enccs.github.io/openacc/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[ENCCS lesson on OpenMP for GPU offloading](https://enccs.github.io/openmp-gpu/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: OpenACC and OpenMP-offloading enables you to annotate your code with special
    directives to identify areas to be executed in parallel on a GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both allow to fine-tune the distribution of the work to match architecture characteristics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both allow to control the flow of data to/from the GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The directive-based approaches save time compared to lower-level approaches,
    but you need to be mindful of data movement in particular to obtain good performance.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
