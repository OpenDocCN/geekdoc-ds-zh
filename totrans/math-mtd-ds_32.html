<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>4.6. Further applications of the SVD: low-rank approximations and ridge regression#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>4.6. Further applications of the SVD: low-rank approximations and ridge regression#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap04_svd/06_further/roch-mmids-svd-further.html">https://mmids-textbook.github.io/chap04_svd/06_further/roch-mmids-svd-further.html</a></blockquote>

<p>In this section, we discuss further properties of the SVD. We first introduce additional matrix norms.</p>
<section id="matrix-norms">
<h2><span class="section-number">4.6.1. </span>Matrix norms<a class="headerlink" href="#matrix-norms" title="Link to this heading">#</a></h2>
<p>Recall that the Frobenius norm<span class="math notranslate nohighlight">\(\idx{Frobenius norm}\xdi\)</span> of an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
\|A\|_F
= \sqrt{\sum_{i=1}^n \sum_{j=1}^m a_{i,j}^2}.
\]</div>
<p>Here we introduce a different notion of matrix norm that has many uses in data science (and beyond).</p>
<p><strong>Induced norm</strong> The Frobenius norm does not directly relate to <span class="math notranslate nohighlight">\(A\)</span> as a representation of a <a class="reference external" href="https://en.wikipedia.org/wiki/Linear_map">linear map</a>. In particular, it is desirable in many contexts to quantify how two matrices differ in terms of how they act on vectors. For instance, one is often interested in bounding quantities of the following form. Let <span class="math notranslate nohighlight">\(B, B' \in \mathbb{R}^{n \times m}\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^m\)</span> be of unit norm. What can be said about <span class="math notranslate nohighlight">\(\|B \mathbf{x} - B' \mathbf{x}\|\)</span>? Intuitively, what we would like is this: if the norm of <span class="math notranslate nohighlight">\(B - B'\)</span> is small then <span class="math notranslate nohighlight">\(B\)</span> is close to <span class="math notranslate nohighlight">\(B'\)</span> as a linear map, that is, the vector norm <span class="math notranslate nohighlight">\(\|B \mathbf{x} - B' \mathbf{x}\|\)</span> is small for any unit vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. The following definition provides us with such a notion. Define the unit sphere <span class="math notranslate nohighlight">\(\mathbb{S}^{m-1} = \{\mathbf{x} \in \mathbb{R}^m\,:\,\|\mathbf{x}\| = 1\}\)</span> in <span class="math notranslate nohighlight">\(m\)</span> dimensions.</p>
<p><strong>DEFINITION</strong> <strong>(<span class="math notranslate nohighlight">\(2\)</span>-Norm)</strong> The <span class="math notranslate nohighlight">\(2\)</span>-norm of a matrix<span class="math notranslate nohighlight">\(\idx{2-norm}\xdi\)</span> <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\|A\|_2
:= \max_{\mathbf{0} \neq \mathbf{x} \in \mathbb{R}^m} \frac{\|A \mathbf{x}\|}{\|\mathbf{x}\|} = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|.
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>The equality in the definition uses the absolute homogeneity of the vector norm. Also the definition implicitly uses the <em>Extreme Value Theorem</em>. In this case, we use the fact that the function <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \|A \mathbf{x}\|\)</span> is continuous and the set <span class="math notranslate nohighlight">\(\mathbb{S}^{m-1}\)</span> is closed and bounded to conclude that there exists <span class="math notranslate nohighlight">\(\mathbf{x}^* \in \mathbb{S}^{m-1}\)</span> such that <span class="math notranslate nohighlight">\(f(\mathbf{x}^*) \geq f(\mathbf{x})\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{S}^{m-1}\)</span>.</p>
<p>The <span class="math notranslate nohighlight">\(2\)</span>-norm of a matrix has many other useful properties. The first four below are what makes it a <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_norm#Preliminaries">norm</a>.</p>
<p><strong>LEMMA</strong> <strong>(Properties of the <span class="math notranslate nohighlight">\(2\)</span>-Norm)</strong> Let <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span> and <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>. The following hold:</p>
<p>a) <span class="math notranslate nohighlight">\(\|A\|_2 \geq 0\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\|A\|_2 = 0\)</span> if and only if <span class="math notranslate nohighlight">\(A = 0\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\|\alpha A\|_2 = |\alpha| \|A\|_2\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\|A + B \|_2 \leq \|A\|_2 + \|B\|_2\)</span></p>
<p>e) <span class="math notranslate nohighlight">\(\|A B \|_2 \leq \|A\|_2 \|B\|_2\)</span>.</p>
<p>f) <span class="math notranslate nohighlight">\(\|A \mathbf{x}\| \leq \|A\|_2 \|\mathbf{x}\|\)</span>, <span class="math notranslate nohighlight">\(\forall \mathbf{0} \neq \mathbf{x} \in \mathbb{R}^m\)</span></p>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> These properties all follow from the definition of the <span class="math notranslate nohighlight">\(2\)</span>-norm and the corresponding properties for the vector norm:</p>
<ul class="simple">
<li><p>Claims a) and f) are immediate from the definition.</p></li>
<li><p>For b) note that <span class="math notranslate nohighlight">\(\|A\|_2 = 0\)</span> implies <span class="math notranslate nohighlight">\(\|A \mathbf{x}\|_2 = 0, \forall \mathbf{x} \in \mathbb{S}^{m-1}\)</span>, so that <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{0}, \forall \mathbf{x} \in \mathbb{S}^{m-1}\)</span>. In particular, <span class="math notranslate nohighlight">\(a_{ij} = \mathbf{e}_i^T A \mathbf{e}_j = 0, \forall i,j\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>For c), d), e), observe that for all <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{S}^{m-1}\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\|\alpha A \mathbf{x}\| = |\alpha| \|A \mathbf{x}\|,
\]</div>
<div class="math notranslate nohighlight">
\[\|(A+B)\mathbf{x}\|
= \|A\mathbf{x} + B\mathbf{x}\| \leq \|A\mathbf{x}\| + \|B\mathbf{x}\|
\leq \|A\|_2 + \|B\|_2
\]</div>
<div class="math notranslate nohighlight">
\[
\|(AB)\mathbf{x}\|
= \|A(B\mathbf{x})\| \leq \|A\|_2 \|B\mathbf{x}\|
\leq \|A\|_2 \|B\|_2.\]</div>
<p>Then apply the definition of <span class="math notranslate nohighlight">\(2\)</span>-norm. For example, for ©,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|\alpha A\|_2 
&amp;= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|\alpha A \mathbf{x}\|\\
&amp;= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} |\alpha| \|A \mathbf{x}\|\\
&amp;= |\alpha| \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|\\
&amp;= |\alpha| \|A\|_2, 
\end{align*}\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(|\alpha|\)</span> does not depend on <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> In NumPy, the Frobenius norm of a matrix can be computed using the default of the function <code class="docutils literal notranslate"><span class="pre">numpy.linalg.norm</span></code> while the induced norm can be computed using the same function with <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html"><code class="docutils literal notranslate"><span class="pre">ord</span></code> parameter set to <code class="docutils literal notranslate"><span class="pre">2</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1. 0.]
 [0. 1.]
 [0. 0.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1.4142135623730951
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1.0
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Matrix norms and SVD</strong> As it turns out, the two notions of matrix norms we have introduced admit simple expressions in terms of the singular values of the matrix.</p>
<p><strong>LEMMA</strong> <strong>(Matrix Norms and Singular Values)</strong> <span class="math notranslate nohighlight">\(\idx{matrix norms and singular values lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix with compact SVD</p>
<div class="math notranslate nohighlight">
\[
A = \sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T
\]</div>
<p>where recall that <span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r &gt; 0\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\|A\|^2_F = \sum_{\ell=1}^r \sigma_\ell^2
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\|A\|^2_2 = \sigma_{1}^2.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> We will use the notation <span class="math notranslate nohighlight">\(\mathbf{v}_\ell = (v_{\ell,1},\ldots,v_{\ell,m})\)</span>. Using that the squared Frobenius norm of <span class="math notranslate nohighlight">\(A\)</span> is the sum of the squared norms of its columns, we have</p>
<div class="math notranslate nohighlight">
\[
\|A\|^2_F
= \left\|\sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T\right\|_F^2
= \sum_{j=1}^m \left\|\sum_{\ell=1}^r \sigma_\ell v_{\ell,j} \mathbf{u}_\ell  \right\|^2.
\]</div>
<p>Because the <span class="math notranslate nohighlight">\(\mathbf{u}_\ell\)</span>’s are orthonormal, this is</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^m \sum_{\ell=1}^r  \sigma_\ell^2 v_{\ell,j}^2
= \sum_{\ell=1}^r  \sigma_\ell^2 \left(\sum_{j=1}^m  v_{\ell,j}^2\right)
= \sum_{\ell=1}^r  \sigma_\ell^2 \|\mathbf{v}_{\ell}\|^2
= \sum_{\ell=1}^r  \sigma_\ell^2,
\]</div>
<p>where we used that the <span class="math notranslate nohighlight">\(\mathbf{v}_\ell\)</span>’s are also orthonormal.</p>
<p>For the second claim, recall that the <span class="math notranslate nohighlight">\(2\)</span>-norm is defined as</p>
<div class="math notranslate nohighlight">
\[
\|A\|_2^2
= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|^2.
\]</div>
<p>We have shown previously that <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> solves this problem. Hence <span class="math notranslate nohighlight">\(\|A\|_2^2 = \|A \mathbf{v}_1\|^2 = \sigma_1^2\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<!--ONLINE

*Proof:* *(Second proof)* We give a second proof of the second claim that does not use the greedy sequence. Because the $\mathbf{u}_j$'s are orthonormal we have 

\begin{align*}
\|A \mathbf{x}\|^2
&= \left\|\sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell (\mathbf{v}_\ell^T \mathbf{x})\right\|^2\\
&= \sum_{\ell=1}^r \sigma_\ell^2 \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2.
\end{align*}

Here $\langle \mathbf{v}_\ell, \mathbf{x} \rangle^2 \geq 0$ for all $\ell$ and further

\begin{align*}
\sum_{\ell=1}^r \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2
&= \left\|\sum_{\ell=1}^r \langle \mathbf{v}_\ell, \mathbf{x} \rangle \mathbf{v}_\ell \right\|^2\\
&= \left\|\mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}\right\|^2,
\end{align*}

since the $\mathbf{v}_\ell$'s form an orthonormal basis of the row space of $A$. By *Pythagoras*, for a unit norm vector $\mathbf{x}$,

\begin{align*}
1 = \left\|\mathbf{x}\right\|^2
&=  \left\|(\mathbf{x} - \mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}) + \mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}\right\|^2\\
&= \left\|\mathbf{x} - \mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}\right\|^2 + \left\|\mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}\right\|^2
\end{align*}

where we used the orthogonality of $\mathbf{x} - \mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}$ and $\mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}$. That implies in particular that, necessarily, $\left\|\mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}\right\|^2 \leq 1$.

On the other hand, $\mathbf{v}_1$ is a unit norm vector such that

$$
\|A \mathbf{v}_1\|^2
= \sum_{\ell=1}^r \sigma_\ell^2 \langle \mathbf{v}_\ell, \mathbf{v}_1 \rangle^2
= \sigma_1^2.
$$

Hence

$$
\max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|^2 = \sigma_1^2.
$$

$\square$

--><!--ONLINE 

**EXAMPLE:** Let $A \in \mathbb{R}^{n \times m}$ be a matrix with compact SVD

$$
A = \sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T
$$

where recall that $\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0$. The proof above can be used to bound the range of possible values for $\|A \mathbf{x}\|$ with $\mathbf{x}$ a unit vector. If $r < m$, then $\mathrm{null}(A)$ contains nonzero vectors and we have

$$
0\leq \|A \mathbf{x}\| \leq \sigma_1
$$

with endpoints achieved.

Suppose now that $r = m$. In that case, $\mathrm{row}(A) = \mathbb{R}^m$ and the right singular vectors $\mathbf{v}_1,\ldots,\mathbf{v}_m$ form an orthonormal basis of $\mathbb{R}^m$. Hence, for any unit vector $\mathbf{x}$,

$$
\sum_{\ell=1}^m \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2
= \left\|\sum_{\ell=1}^m \langle \mathbf{v}_\ell, \mathbf{x} \rangle \mathbf{v}_\ell \right\|^2
= \left\|\mathbf{x}\right\|^2
= 1.
$$

As a result, we have by using $\sigma_n \leq \sigma_\ell$ for all $\ell$ that

\begin{align*}
\|A \mathbf{x}\|^2
&= \sum_{\ell=1}^m \sigma_\ell^2 \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2\\
&\geq \sum_{\ell=1}^m \sigma_m^2 \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2\\
&= \sigma_m^2 \sum_{\ell=1}^m  \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2\\
&= \sigma_m^2.
\end{align*}

We have shown in that case that

$$
\sigma_m \leq \|A \mathbf{x}\| \leq \sigma_1.
$$

The endpoints are achieved by taking $\mathbf{x} = \mathbf{v}_m$ and $\mathbf{x} = \mathbf{v}_1$ respectively.

Put differently,

$$
\min_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\| = \sigma_m
\quad
\text{and}
\quad
\max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\| = \sigma_1.
$$

(If $\sigma_m$ denotes the smallest singular value in the *full SVD*, then the above expressions hold generally.) $\lhd$

--></section>
<section id="low-rank-approximation">
<h2><span class="section-number">4.6.2. </span>Low-rank approximation<a class="headerlink" href="#low-rank-approximation" title="Link to this heading">#</a></h2>
<p>Now that we have defined a notion of distance between matrices, we will consider the problem of finding a good approximation to a matrix <span class="math notranslate nohighlight">\(A\)</span> among all matrices of rank at most <span class="math notranslate nohighlight">\(k\)</span>. We will start with the Frobenius norm, which is easier to work with, and we will show later on that the solution is the same under the induced norm. The solution to this problem will be familiar. In essence, we will re-interpret our solution to the best approximating subspace as a low-rank approximation.</p>
<p><strong>Low-rank approximation in the Frobenius norm</strong> <span class="math notranslate nohighlight">\(\idx{low-rank approximation}\xdi\)</span> From the proof of the <em>Row Rank Equals Column Rank Lemma</em>, it follows that a rank-<span class="math notranslate nohighlight">\(r\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> can be written as a sum of <span class="math notranslate nohighlight">\(r\)</span> rank-<span class="math notranslate nohighlight">\(1\)</span> matrices</p>
<div class="math notranslate nohighlight">
\[
A = 
\sum_{i=1}^r \mathbf{b}_i \mathbf{c}_i^T.
\]</div>
<p>We will now consider the problem of finding a “simpler” approximation to <span class="math notranslate nohighlight">\(A\)</span></p>
<div class="math notranslate nohighlight">
\[
A \approx \sum_{i=1}^k \mathbf{b}'_i (\mathbf{c}'_i)^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(k &lt; r\)</span>. Here we measure the quality of this approximation using a matrix norm.</p>
<p>We are ready to state our key observation. In words, the best rank-<span class="math notranslate nohighlight">\(k\)</span> approximation to <span class="math notranslate nohighlight">\(A\)</span> in Frobenius norm is obtained by projecting the rows of <span class="math notranslate nohighlight">\(A\)</span> onto a linear subspace of dimension <span class="math notranslate nohighlight">\(k\)</span>. We will come back to how one finds the best such subspace below. (<em>Hint:</em> We have already solved this problem.)</p>
<p><strong>LEMMA</strong> <strong>(Projection and Rank-<span class="math notranslate nohighlight">\(k\)</span> Approximation)</strong> <span class="math notranslate nohighlight">\(\idx{projection and rank-k approximation lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)</span>. For any matrix <span class="math notranslate nohighlight">\(B = (b_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)</span> of rank <span class="math notranslate nohighlight">\(k \leq \min\{n,m\}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\|A - B_{\perp}\|_F \leq \|A - B\|_F
\]</div>
<p>where <span class="math notranslate nohighlight">\(B_{\perp} \in \mathbb{R}^{n \times m}\)</span> is the matrix of rank at most <span class="math notranslate nohighlight">\(k\)</span> obtained as follows. Denote row <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(B_{\perp}\)</span> respectively by <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i^T\)</span>, <span class="math notranslate nohighlight">\(\mathbf{b}_{i}^T\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}_{\perp,i}^T\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots, n\)</span>. Set <span class="math notranslate nohighlight">\(\mathbf{b}_{\perp,i}\)</span> to be the orthogonal projection of <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i\)</span> onto <span class="math notranslate nohighlight">\(\mathcal{Z} = \mathrm{span}(\mathbf{b}_1,\ldots,\mathbf{b}_n)\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> The square of the Frobenius norm decomposes as a sum of squared row norms. Each term in the sum is minimized by the orthogonal projection.</p>
<p><em>Proof:</em> By definition of the Frobenius norm, we note that</p>
<div class="math notranslate nohighlight">
\[
\|A - B\|_F^2
= \sum_{i=1}^n \sum_{j=1}^m (a_{i,j} - b_{i,j})^2 
= \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2
\]</div>
<p>and similarly for <span class="math notranslate nohighlight">\(\|A - B_{\perp}\|_F\)</span>. We make two observations:</p>
<ol class="arabic simple">
<li><p>Because the orthogonal projection of <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i\)</span> onto <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> minimizes the distance to <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span>, it follows that term by term <span class="math notranslate nohighlight">\(\|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\| \leq \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|\)</span> so that</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\|A - B_\perp\|_F^2 =
\sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\|^2
\leq \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2
= \|A - B\|_F^2.
\]</div>
<ol class="arabic simple" start="2">
<li><p>Moreover, because the projections satisfy <span class="math notranslate nohighlight">\(\mathbf{b}_{\perp,i} \in \mathcal{Z}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(\mathrm{row}(B_\perp) \subseteq \mathrm{row}(B)\)</span> and, hence, the rank of <span class="math notranslate nohighlight">\(B_\perp\)</span> is at most the rank of <span class="math notranslate nohighlight">\(B\)</span>.</p></li>
</ol>
<p>That concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Recall the approximating subspace problem. That is, think of the rows <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i^T\)</span> of <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> as a collection of <span class="math notranslate nohighlight">\(n\)</span> data points in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>. We are looking for a linear subspace <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> that minimizes <span class="math notranslate nohighlight">\(\sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2\)</span> over all linear subspaces of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> of dimension at most <span class="math notranslate nohighlight">\(k\)</span>. By the <em>Projection and Rank-<span class="math notranslate nohighlight">\(k\)</span> Approximation Lemma</em>, this problem is equivalent to finding a matrix <span class="math notranslate nohighlight">\(B\)</span> that minimizes <span class="math notranslate nohighlight">\(\|A - B\|_F\)</span> among all matrices in <span class="math notranslate nohighlight">\(\mathbb{R}^{n \times m}\)</span> of rank at most <span class="math notranslate nohighlight">\(k\)</span>. Of course we have solved this problem before.</p>
<p>Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix with SVD <span class="math notranslate nohighlight">\(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span>. For <span class="math notranslate nohighlight">\(k &lt; r\)</span>, truncate the sum at the <span class="math notranslate nohighlight">\(k\)</span>-th term <span class="math notranslate nohighlight">\(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span>. The rank of <span class="math notranslate nohighlight">\(A_k\)</span> is exactly <span class="math notranslate nohighlight">\(k\)</span>. Indeed, by construction,</p>
<ol class="arabic simple">
<li><p>the vectors <span class="math notranslate nohighlight">\(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\)</span> are orthonormal, and</p></li>
<li><p>since <span class="math notranslate nohighlight">\(\sigma_j &gt; 0\)</span> for <span class="math notranslate nohighlight">\(j=1,\ldots,k\)</span> and the vectors <span class="math notranslate nohighlight">\(\{\mathbf{v}_j\,:\,j = 1,\ldots,k\}\)</span> are orthonormal, <span class="math notranslate nohighlight">\(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\)</span> spans the column space of <span class="math notranslate nohighlight">\(A_k\)</span>.</p></li>
</ol>
<p>We have shown before that <span class="math notranslate nohighlight">\(A_k\)</span> is the best approximation to <span class="math notranslate nohighlight">\(A\)</span> among matrices of rank at most <span class="math notranslate nohighlight">\(k\)</span> in Frobenius norm. Specifically, the <em>Greedy Finds Best Fit Theorem</em> implies that, for any matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times m}\)</span> of rank at most <span class="math notranslate nohighlight">\(k\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|_F \leq \|A - B\|_F.
\]</div>
<p>This result is known as the <em>Eckart-Young Theorem</em><span class="math notranslate nohighlight">\(\idx{Eckart-Young theorem}\xdi\)</span>. It also holds in the induced <span class="math notranslate nohighlight">\(2\)</span>-norm, as we show next.</p>
<p><strong>Low-rank approximation in the induced norm</strong> We show in this section that the same holds in the induced norm. First, some observations.</p>
<p><strong>LEMMA</strong> <strong>(Matrix Norms and Singular Values: Truncation)</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix with SVD</p>
<div class="math notranslate nohighlight">
\[
A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T
\]</div>
<p>where recall that <span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r &gt; 0\)</span> and let <span class="math notranslate nohighlight">\(A_k\)</span> be the truncation defined above. Then</p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|^2_F = \sum_{j=k+1}^r \sigma_j^2
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|^2_2 = \sigma_{k+1}^2.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> For the first claim, by definition, summing over the columns of <span class="math notranslate nohighlight">\(A - A_k\)</span></p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|^2_F
= \left\|\sum_{j=k+1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right\|_F^2
= \sum_{i=1}^m \left\|\sum_{j=k+1}^r \sigma_j v_{j,i} \mathbf{u}_j  \right\|^2.
\]</div>
<p>Because the <span class="math notranslate nohighlight">\(\mathbf{u}_j\)</span>’s are orthonormal, this is</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^m \sum_{j=k+1}^r  \sigma_j^2 v_{j,i}^2
= \sum_{j=k+1}^r  \sigma_j^2 \left(\sum_{i=1}^m  v_{j,i}^2\right)
= \sum_{j=k+1}^r  \sigma_j^2
\]</div>
<p>where we used that the <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span>’s are also orthonormal.</p>
<p>For the second claim, recall that the induced norm is defined as</p>
<div class="math notranslate nohighlight">
\[
\|B\|_2
= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|B \mathbf{x}\|.
\]</div>
<p>For any <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{S}^{m-1}\)</span></p>
<div class="math notranslate nohighlight">
\[
\left\|(A - A_k)\mathbf{x}\right\|^2 
= \left\| \sum_{j=k+1}^r \sigma_j \mathbf{u}_j (\mathbf{v}_j^T \mathbf{x}) \right\|^2
= \sum_{j=k+1}^r \sigma_j^2 \langle \mathbf{v}_j, \mathbf{x}\rangle^2.
\]</div>
<p>Because the <span class="math notranslate nohighlight">\(\sigma_j\)</span>’s are in decreasing order, this is maximized when
<span class="math notranslate nohighlight">\(\langle \mathbf{v}_j, \mathbf{x}\rangle = 1\)</span> if <span class="math notranslate nohighlight">\(j=k+1\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise. That is, we take <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{v}_{k+1}\)</span> and the norm is then <span class="math notranslate nohighlight">\(\sigma_{k+1}^2\)</span>, as claimed. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>THEOREM</strong> <strong>(Low-Rank Approximation in the Induced Norm)</strong> <span class="math notranslate nohighlight">\(\idx{low-rank approximation in the induced norm theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix with SVD</p>
<div class="math notranslate nohighlight">
\[
A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T
\]</div>
<p>and let <span class="math notranslate nohighlight">\(A_k\)</span> be the truncation defined above with <span class="math notranslate nohighlight">\(k &lt; r\)</span>. For any matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times m}\)</span> of rank at most <span class="math notranslate nohighlight">\(k\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|_2 \leq \|A - B\|_2.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> We know that <span class="math notranslate nohighlight">\(\|A - A_k\|_2^2 = \sigma_{k+1}^2\)</span>. So we want to lower bound <span class="math notranslate nohighlight">\(\|A - B\|_2^2\)</span> by <span class="math notranslate nohighlight">\(\sigma_{k+1}^2\)</span>. For that, we have to find an appropriate <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> for any given <span class="math notranslate nohighlight">\(B\)</span> of rank at most <span class="math notranslate nohighlight">\(k\)</span>. The idea is to take a vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> in the intersection of the null space of <span class="math notranslate nohighlight">\(B\)</span> and the span of the singular vectors <span class="math notranslate nohighlight">\(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}\)</span>. By the former, the squared norm of <span class="math notranslate nohighlight">\((A - B)\mathbf{z}\)</span> is equal to the squared norm of <span class="math notranslate nohighlight">\(A\mathbf{z}\)</span> which lower bounds <span class="math notranslate nohighlight">\(\|A\|_2^2\)</span>. By the latter, <span class="math notranslate nohighlight">\(\|A \mathbf{z}\|^2\)</span> is at least <span class="math notranslate nohighlight">\(\sigma_{k+1}^2\)</span>.</p>
<p><em>Proof:</em> By the <em>Rank-Nullity Theorem</em>, the dimension of <span class="math notranslate nohighlight">\(\mathrm{null}(B)\)</span> is at least <span class="math notranslate nohighlight">\(m-k\)</span> so there is a unit vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> in the intersection</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z} \in \mathrm{null}(B) \cap \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}).
\]</div>
<p>(Prove it!) Then <span class="math notranslate nohighlight">\((A-B)\mathbf{z} = A\mathbf{z}\)</span> since <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathrm{null}(B)\)</span>. Also since  <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1})\)</span>, and therefore orthogonal to <span class="math notranslate nohighlight">\(\mathbf{v}_{k+2},\ldots,\mathbf{v}_r\)</span>, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|(A-B)\mathbf{z}\|^2
&amp;= \|A\mathbf{z}\|^2\\
&amp;= \left\|\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\
&amp;= \left\|\sum_{j=1}^{k+1} \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\
&amp;= \sum_{j=1}^{k+1} \sigma_j^2 \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\
&amp;\geq \sigma_{k+1}^2 \sum_{j=1}^{k+1} \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\
&amp;= \sigma_{k+1}^2.
\end{align*}\]</div>
<p>By the previous lemma, <span class="math notranslate nohighlight">\(\sigma_{k+1}^2 = \|A - A_k\|_2\)</span> and we are done. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>An application: Why project?</strong> We return to <span class="math notranslate nohighlight">\(k\)</span>-means clustering and why projecting to a lower-dimensional subspace can produce better results. We prove a simple inequality that provides some insight. Quoting [BHK, Section 7.5.1]:</p>
<blockquote>
<div><p>[…] let’s understand the central advantage of doing the projection to [the top <span class="math notranslate nohighlight">\(k\)</span> right singular vectors]. It is simply that for any reasonable (unknown) clustering of data points, the projection brings data points closer to their cluster centers.</p>
</div></blockquote>
<p>To elaborate, suppose we have <span class="math notranslate nohighlight">\(n\)</span> data points in <span class="math notranslate nohighlight">\(d\)</span> dimensions in the form of the rows <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i^T\)</span>, <span class="math notranslate nohighlight">\(i=1\ldots, n\)</span>, of matrix <span class="math notranslate nohighlight">\(A \in \mathbb{A}^{n \times d}\)</span>, where we assume that <span class="math notranslate nohighlight">\(n &gt; d\)</span> and that <span class="math notranslate nohighlight">\(A\)</span> has full column rank. Imagine these data points come from an unknown ground-truth <span class="math notranslate nohighlight">\(k\)</span>-clustering assignment <span class="math notranslate nohighlight">\(g(i) \in [k]\)</span>, <span class="math notranslate nohighlight">\(i = 1,\ldots, n\)</span>, with corresponding unknown centers <span class="math notranslate nohighlight">\(\mathbf{c}_j\)</span>, <span class="math notranslate nohighlight">\(j = 1,\ldots, k\)</span>. Let <span class="math notranslate nohighlight">\(C \in \mathbb{R}^{n \times d}\)</span> be the corresponding matrix, that is, row <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(C\)</span> is <span class="math notranslate nohighlight">\(\mathbf{c}_j^T\)</span> if <span class="math notranslate nohighlight">\(g(i) = j\)</span>. The <span class="math notranslate nohighlight">\(k\)</span>-means objective of the true clustering is then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sum_{j\in [k]} \sum_{i:g(i)=j} \|\boldsymbol{\alpha}_i - \mathbf{c}_{j}\|^2
&amp;= \sum_{j\in [k]} \sum_{i:g(i)=j} \sum_{\ell=1}^d (a_{i,\ell} - c_{j,\ell})^2\\
&amp;= \sum_{i=1}^n \sum_{\ell=1}^d (a_{i,\ell} - c_{g(i),\ell})^2\\
&amp;= \|A - C\|_F^2.
\end{align*}\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(A\)</span> has an SVD <span class="math notranslate nohighlight">\(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span> and for <span class="math notranslate nohighlight">\(k &lt; r\)</span> we have the truncation <span class="math notranslate nohighlight">\(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span>. It corresponds to projecting each row of <span class="math notranslate nohighlight">\(A\)</span> onto the linear subspace spanned by the first <span class="math notranslate nohighlight">\(k\)</span> right singular vectors <span class="math notranslate nohighlight">\(\mathbf{v}_1, \ldots, \mathbf{v}_k\)</span>. To see this, note that the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i^T = \sum_{j=1}^r \sigma_j u_{j,i} \mathbf{v}_j^T\)</span> and that, because the <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span>’s are linearly independent and in particular <span class="math notranslate nohighlight">\(\mathbf{v}_1,\ldots,\mathbf{v}_k\)</span> is an orthonormal basis of its span, the projection of <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i\)</span> onto <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\sum_{\ell=1}^k  \mathbf{v}_\ell \left\langle\sum_{j=1}^r \sigma_j u_{j,i} \mathbf{v}_j,\mathbf{v}_\ell\right\rangle
= \sum_{\ell=1}^k \sigma_\ell u_{\ell,i} \mathbf{v}_\ell
\]</div>
<p>which is the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(A_k\)</span>. The <span class="math notranslate nohighlight">\(k\)</span>-means objective of <span class="math notranslate nohighlight">\(A_k\)</span> with respect to the ground-truth centers <span class="math notranslate nohighlight">\(\mathbf{c}_j\)</span>, <span class="math notranslate nohighlight">\(j=1,\ldots,k\)</span>, is <span class="math notranslate nohighlight">\(\|A_k - C\|_F^2\)</span>.</p>
<p>One more observation: the rank of <span class="math notranslate nohighlight">\(C\)</span> is at most <span class="math notranslate nohighlight">\(k\)</span>. Indeed, there are <span class="math notranslate nohighlight">\(k\)</span> different rows in <span class="math notranslate nohighlight">\(C\)</span> so its row rank is <span class="math notranslate nohighlight">\(k\)</span> if these different rows are linearly independent and less than <span class="math notranslate nohighlight">\(k\)</span> otherwise.</p>
<p><strong>THEOREM</strong> <strong>(Why Project)</strong> <span class="math notranslate nohighlight">\(\idx{why project theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{A}^{n \times d}\)</span> be a matrix
and let <span class="math notranslate nohighlight">\(A_k\)</span> be the truncation above. For any matrix <span class="math notranslate nohighlight">\(C \in \mathbb{R}^{n \times d}\)</span> of rank <span class="math notranslate nohighlight">\(\leq k\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\|A_k - C\|_F^2 \leq 8 k \|A - C\|_2^2.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>Observe that we used different matrix norms on the different sides of the inequality. The content of this inequality is the following. The quantity <span class="math notranslate nohighlight">\(\|A_k - C\|_F^2\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-means objective of the projection <span class="math notranslate nohighlight">\(A_k\)</span> with respect to the true centers, that is, the sum of the squared distances to the centers. By the <em>Matrix Norms and Singular Values Lemma</em>, the inequality above gives that</p>
<div class="math notranslate nohighlight">
\[
\|A_k - C\|_F^2 \leq 8 k \sigma_1(A - C)^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_j(A - C)\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th singular value of <span class="math notranslate nohighlight">\(A - C\)</span>. On the other hand, by the same lemma, the <span class="math notranslate nohighlight">\(k\)</span>-means objective of the un-projected data is</p>
<div class="math notranslate nohighlight">
\[
\|A - C\|_F^2 = \sum_{j=1}^{\mathrm{rk}(A-C)} \sigma_j(A - C)^2.
\]</div>
<p>If the rank of <span class="math notranslate nohighlight">\(A-C\)</span> is much larger than <span class="math notranslate nohighlight">\(k\)</span> and the singular values of <span class="math notranslate nohighlight">\(A-C\)</span> decay slowly, then the latter quantity may be much larger. In other words, projecting may bring the data points closer to their true centers, potentially making it easier to cluster them.</p>
<p><em>Proof:</em> <em>(Why Project)</em> We have shown previously that, for any matrices <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span>, the rank of their sum is less or equal than the sum of their ranks, that is, <span class="math notranslate nohighlight">\(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\)</span>. So the rank of the difference <span class="math notranslate nohighlight">\(A_k - C\)</span> is at most the sum of the ranks</p>
<div class="math notranslate nohighlight">
\[
\mathrm{rk}(A_k - C)
\leq \mathrm{rk}(A_k) + \mathrm{rk}(-C) \leq 2 k
\]</div>
<p>where we used that the rank of <span class="math notranslate nohighlight">\(A_k\)</span> is <span class="math notranslate nohighlight">\(k\)</span> and the rank of <span class="math notranslate nohighlight">\(C\)</span> is <span class="math notranslate nohighlight">\(\leq k\)</span> since it has <span class="math notranslate nohighlight">\(k\)</span> distinct rows. By the <em>Matrix Norms and Singular Values Lemma</em>,</p>
<div class="math notranslate nohighlight">
\[
\|A_k - C\|_F^2 \leq 2k \|A_k - C\|_2^2. 
\]</div>
<p>By the triangle inequality for matrix norms,</p>
<div class="math notranslate nohighlight">
\[
\|A_k - C\|_2
\leq \|A_k - A\|_2 + \|A - C\|_2.
\]</div>
<p>By the <em>Low-Rank Approximation in the Induced Norm Theorem</em>,</p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|_2
\leq \|A - C\|_2
\]</div>
<p>since <span class="math notranslate nohighlight">\(C\)</span> has rank at most <span class="math notranslate nohighlight">\(k\)</span>. Putting these three inequalities together,</p>
<div class="math notranslate nohighlight">
\[
\|A_k - C\|_F^2 
\leq 2k (2 \|A - C\|_2)^2
= 8k \|A - C\|_2^2. 
\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We return to our example with the two Gaussian clusters. We use function producing two separate clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">two_separate_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    
    <span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    <span class="n">mu2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="o">-</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    
    <span class="n">X1</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">spherical_gaussian</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">X2</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">spherical_gaussian</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span>
</pre></div>
</div>
</div>
</div>
<p>We first generate the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">two_separate_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>In reality, we cannot compute the matrix norms of <span class="math notranslate nohighlight">\(X-C\)</span> and <span class="math notranslate nohighlight">\(X_k-C\)</span> as the true centers are not known. But, because this is simulated data, we happen to know the truth and we can check the validity of our results in this case. The centers are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">C1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">C2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="o">-</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">C1</span><span class="p">,</span> <span class="n">C2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.svd</span></code></a> function to compute the norms from the formulas in the <em>Matrix Norms and Singular Values Lemma</em>. First, we observe that the singular values of <span class="math notranslate nohighlight">\(X-C\)</span> are decaying slowly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">uc</span><span class="p">,</span> <span class="n">sc</span><span class="p">,</span> <span class="n">vhc</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="o">-</span><span class="n">C</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a4074a56fd16a51eb92a355bbf35c436e52eced8e21bf3bd48d91efacb43deda.png" src="../Images/23b4327417d560c241971d818c1583cb.png" data-original-src="https://mmids-textbook.github.io/_images/a4074a56fd16a51eb92a355bbf35c436e52eced8e21bf3bd48d91efacb43deda.png"/>
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(k\)</span>-means objective with respect to the true centers under the full-dimensional data is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sc</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>200925.669068181
</pre></div>
</div>
</div>
</div>
<p>while the square of the top singular value of <span class="math notranslate nohighlight">\(X-C\)</span> is only:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">sc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>2095.357155856167
</pre></div>
</div>
</div>
</div>
<p>Finally, we compute the <span class="math notranslate nohighlight">\(k\)</span>-means objective with respect to the true centers under the projected one-dimensional data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">vh</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">u</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">vh</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="n">C</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1614.2173799824254
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot about the applications of SVD in recommendation systems. How is it used to predict user preferences? <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot about nonnegative matrix factorization (NMF) and how it compares to SVD. What are the key differences in their constraints and applications? How does NMF handle interpretability in topics like text analysis or image processing? Explore some algorithms used to compute NMF. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
</section>
<section id="ridge-regression">
<h2><span class="section-number">4.6.3. </span>Ridge regression<a class="headerlink" href="#ridge-regression" title="Link to this heading">#</a></h2>
<p>Here we consider what is called Tikhonov regularization<span class="math notranslate nohighlight">\(\idx{Tikhonov regularization}\xdi\)</span>, an idea that turns out to be useful in overdetermined linear systems, particularly when the columns of the matrix <span class="math notranslate nohighlight">\(A\)</span> are linearly dependent or close to linearly dependent (which is sometimes referred to as <a class="reference external" href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a><span class="math notranslate nohighlight">\(\idx{multicollinearity}\xdi\)</span> in statistics). It trades off minimizing the fit to the data versus minimizing the norm of the solution. More precisely, for a parameter <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> to be chosen, we solve<span class="math notranslate nohighlight">\(\idx{ridge regression}\xdi\)</span></p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2.
\]</div>
<p>The second term is referred to as an <span class="math notranslate nohighlight">\(L_2\)</span>-regularizer<span class="math notranslate nohighlight">\(\idx{L2-regularization}\xdi\)</span>. Here <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> with <span class="math notranslate nohighlight">\(n \geq m\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span>.</p>
<p>To solve this optimization problem, we show that the objective function is strongly convex. We then find its unique stationary point. Rewriting the objective in quadratic function form</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(\mathbf{x})
&amp;= \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2\\
&amp;=  \mathbf{x}^T A^T A \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T \mathbf{b}
+ \lambda \mathbf{x}^T \mathbf{x}\\
&amp;= \mathbf{x}^T (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T \mathbf{b}\\
&amp;= \frac{1}{2} \mathbf{x}^T  P \mathbf{x} + \mathbf{q}^T \mathbf{x} + r,
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(P = 2 (A^T A + \lambda I_{m \times m})\)</span> is symmetric, <span class="math notranslate nohighlight">\(\mathbf{q} = - 2 A^T \mathbf{b}\)</span>, and <span class="math notranslate nohighlight">\(r= \mathbf{b}^T \mathbf{b} = \|\mathbf{b}\|^2\)</span>.</p>
<p>As we previously computed, the Hessian of <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(H_f(\mathbf{x})= P\)</span>. Now comes a key observation. The matrix <span class="math notranslate nohighlight">\(P\)</span> is positive definite whenever <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>. Indeed, for any <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^m\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^T [2 (A^T A + \lambda I_{m \times m})] \mathbf{z}
= 2 \|A \mathbf{z}\|_2^2 + 2 \lambda \|\mathbf{z}\|_2^2 &gt; 0.
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mu = 2 \lambda &gt; 0\)</span>. Then <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\mu\)</span>-strongly convex. This holds whether or not the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent.</p>
<p>The stationary points are easily characterized. Recall that the gradient is <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}) = P \mathbf{x} + \mathbf{q}\)</span>. Equating to <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> leads to the system</p>
<div class="math notranslate nohighlight">
\[
2 (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 A^T \mathbf{b} = \mathbf{0},
\]</div>
<p>that is</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{**}
= (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b}.
\]</div>
<p>The matrix in parenthesis is invertible as it is <span class="math notranslate nohighlight">\(1/2\)</span> of the Hessian, which is positive definite.</p>
<p><strong>Connection to SVD</strong> Expressing the solution in terms of a compact SVD <span class="math notranslate nohighlight">\(A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span> provides some insights into how ridge regression works. Suppose that <span class="math notranslate nohighlight">\(A\)</span> has full column rank. That implies that <span class="math notranslate nohighlight">\(V V^T = I_{m \times m}\)</span>. Then observe that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(A^T A + \lambda I_{m \times m})^{-1}
&amp;= (V \Sigma U^T U \Sigma V^T + \lambda I_{m \times m})^{-1}\\
&amp;= (V \Sigma^2 V^T + \lambda I_{m \times m})^{-1}\\
&amp;= (V \Sigma^2 V^T + V \lambda I_{m \times m} V^T)^{-1}\\
&amp;= (V [\Sigma^2 + \lambda I_{m \times m}] V^T)^{-1}\\
&amp;= V (\Sigma^2 + \lambda I_{m \times m})^{-1} V^T.
\end{align*}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{**}
= (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b}
= V (\Sigma^2 + \lambda I_{m \times m})^{-1} V^T V \Sigma U^T \mathbf{b}
= V (\Sigma^2 + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}.
\]</div>
<p>Our predictions are</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A \mathbf{x}^{**}
&amp;= U \Sigma V^T V (\Sigma^2 + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}\\
&amp;= U \Sigma (\Sigma^2 + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}\\
&amp;= \sum_{j=1}^r \mathbf{u}_j \left\{\frac{\sigma_j^2}{\sigma_j^2 + \lambda} \right\} \mathbf{u}_j^T \mathbf{b}.
\end{align*}\]</div>
<p>Note that the terms in curly brackets are <span class="math notranslate nohighlight">\(&lt; 1\)</span> when <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>.</p>
<p>Compare this to the unregularized least squares solution, which is obtained simply by setting <span class="math notranslate nohighlight">\(\lambda = 0\)</span> above,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A \mathbf{x}^*
&amp;= \sum_{j=1}^r \mathbf{u}_j  \mathbf{u}_j^T \mathbf{b}.
\end{align*}\]</div>
<p>The difference is that the regularized solution reduces the contributions from the left singular vectors corresponding to small singular values.</p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Which of the following best describes the Frobenius norm of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span>?</p>
<p>a) The maximum singular value of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>b) The square root of the sum of the squares of all entries in <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>c) The maximum absolute row sum of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>d) The maximum absolute column sum of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><strong>2</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix with SVD <span class="math notranslate nohighlight">\(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span> and let <span class="math notranslate nohighlight">\(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span> be the truncated SVD with <span class="math notranslate nohighlight">\(k &lt; r\)</span>. Which of the following is true about the Frobenius norm of <span class="math notranslate nohighlight">\(A - A_k\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\|A - A_k\|_F^2 = \sum_{j=1}^k \sigma_j^2\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\|A - A_k\|_F^2 = \sigma_k^2\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\|A - A_k\|_F^2 = \sigma_{k+1}^2\)</span></p>
<p><strong>3</strong> The ridge regression problem is formulated as <span class="math notranslate nohighlight">\(\min_{\mathbf{x} \in \mathbb{R}^m} \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2\)</span>. What is the role of the parameter <span class="math notranslate nohighlight">\(\lambda\)</span>?</p>
<p>a) It controls the trade-off between fitting the data and minimizing the norm of the solution.</p>
<p>b) It determines the rank of the matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>c) It is the smallest singular value of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>d) It is the largest singular value of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><strong>4</strong> Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix with compact SVD <span class="math notranslate nohighlight">\(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span>. How does the ridge regression solution <span class="math notranslate nohighlight">\(\mathbf{x}^{**}\)</span> compare to the least squares solution <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbf{x}^{**}\)</span> has larger components along the left singular vectors corresponding to small singular values.</p>
<p>b) <span class="math notranslate nohighlight">\(\mathbf{x}^{**}\)</span> has smaller components along the left singular vectors corresponding to small singular values.</p>
<p>c) <span class="math notranslate nohighlight">\(\mathbf{x}^{**}\)</span> is identical to <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>.</p>
<p>d) None of the above.</p>
<p><strong>5</strong> (<em>Note:</em> Refers to online supplementary materials.) Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> be a square nonsingular matrix with compact SVD <span class="math notranslate nohighlight">\(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span>. Which of the following is true about the induced 2-norm of the inverse <span class="math notranslate nohighlight">\(A^{-1}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\|A^{-1}\|_2 = \sigma_1\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\|A^{-1}\|_2 = \sigma_n\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\|A^{-1}\|_2 = \sigma_1^{-1}\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\|A^{-1}\|_2 = \sigma_n^{-1}\)</span></p>
<p>Answer for 1: b. Justification: The text defines the Frobenius norm of an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> as <span class="math notranslate nohighlight">\(\|A\|_F = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{m} a_{ij}^2}\)</span>.</p>
<p>Answer for 2: b. Justification: The text proves that <span class="math notranslate nohighlight">\(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)</span> in the Matrix Norms and Singular Values: Truncation Lemma.</p>
<p>Answer for 3: a. Justification: The text explains that ridge regression “trades off minimizing the fit to the data versus minimizing the norm of the solution,” and <span class="math notranslate nohighlight">\(\lambda\)</span> is the parameter that controls this trade-off.</p>
<p>Answer for 4: b. Justification: The text notes that the ridge regression solution “reduces the contributions from the left singular vectors corresponding to small singular values.”</p>
<p>Answer for 5: d. Justification: The text shows in an example that for a square nonsingular matrix <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(\|A^{-1}\|_2 = \sigma_n^{-1}\)</span>, where <span class="math notranslate nohighlight">\(\sigma_n\)</span> is the smallest singular value of <span class="math notranslate nohighlight">\(A\)</span>.</p>
</section>
&#13;

<h2><span class="section-number">4.6.1. </span>Matrix norms<a class="headerlink" href="#matrix-norms" title="Link to this heading">#</a></h2>
<p>Recall that the Frobenius norm<span class="math notranslate nohighlight">\(\idx{Frobenius norm}\xdi\)</span> of an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
\|A\|_F
= \sqrt{\sum_{i=1}^n \sum_{j=1}^m a_{i,j}^2}.
\]</div>
<p>Here we introduce a different notion of matrix norm that has many uses in data science (and beyond).</p>
<p><strong>Induced norm</strong> The Frobenius norm does not directly relate to <span class="math notranslate nohighlight">\(A\)</span> as a representation of a <a class="reference external" href="https://en.wikipedia.org/wiki/Linear_map">linear map</a>. In particular, it is desirable in many contexts to quantify how two matrices differ in terms of how they act on vectors. For instance, one is often interested in bounding quantities of the following form. Let <span class="math notranslate nohighlight">\(B, B' \in \mathbb{R}^{n \times m}\)</span> and let <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^m\)</span> be of unit norm. What can be said about <span class="math notranslate nohighlight">\(\|B \mathbf{x} - B' \mathbf{x}\|\)</span>? Intuitively, what we would like is this: if the norm of <span class="math notranslate nohighlight">\(B - B'\)</span> is small then <span class="math notranslate nohighlight">\(B\)</span> is close to <span class="math notranslate nohighlight">\(B'\)</span> as a linear map, that is, the vector norm <span class="math notranslate nohighlight">\(\|B \mathbf{x} - B' \mathbf{x}\|\)</span> is small for any unit vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. The following definition provides us with such a notion. Define the unit sphere <span class="math notranslate nohighlight">\(\mathbb{S}^{m-1} = \{\mathbf{x} \in \mathbb{R}^m\,:\,\|\mathbf{x}\| = 1\}\)</span> in <span class="math notranslate nohighlight">\(m\)</span> dimensions.</p>
<p><strong>DEFINITION</strong> <strong>(<span class="math notranslate nohighlight">\(2\)</span>-Norm)</strong> The <span class="math notranslate nohighlight">\(2\)</span>-norm of a matrix<span class="math notranslate nohighlight">\(\idx{2-norm}\xdi\)</span> <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\|A\|_2
:= \max_{\mathbf{0} \neq \mathbf{x} \in \mathbb{R}^m} \frac{\|A \mathbf{x}\|}{\|\mathbf{x}\|} = \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|.
\]</div>
<p><span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>The equality in the definition uses the absolute homogeneity of the vector norm. Also the definition implicitly uses the <em>Extreme Value Theorem</em>. In this case, we use the fact that the function <span class="math notranslate nohighlight">\(f(\mathbf{x}) = \|A \mathbf{x}\|\)</span> is continuous and the set <span class="math notranslate nohighlight">\(\mathbb{S}^{m-1}\)</span> is closed and bounded to conclude that there exists <span class="math notranslate nohighlight">\(\mathbf{x}^* \in \mathbb{S}^{m-1}\)</span> such that <span class="math notranslate nohighlight">\(f(\mathbf{x}^*) \geq f(\mathbf{x})\)</span> for all <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{S}^{m-1}\)</span>.</p>
<p>The <span class="math notranslate nohighlight">\(2\)</span>-norm of a matrix has many other useful properties. The first four below are what makes it a <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_norm#Preliminaries">norm</a>.</p>
<p><strong>LEMMA</strong> <strong>(Properties of the <span class="math notranslate nohighlight">\(2\)</span>-Norm)</strong> Let <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span> and <span class="math notranslate nohighlight">\(\alpha \in \mathbb{R}\)</span>. The following hold:</p>
<p>a) <span class="math notranslate nohighlight">\(\|A\|_2 \geq 0\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\|A\|_2 = 0\)</span> if and only if <span class="math notranslate nohighlight">\(A = 0\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\|\alpha A\|_2 = |\alpha| \|A\|_2\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\|A + B \|_2 \leq \|A\|_2 + \|B\|_2\)</span></p>
<p>e) <span class="math notranslate nohighlight">\(\|A B \|_2 \leq \|A\|_2 \|B\|_2\)</span>.</p>
<p>f) <span class="math notranslate nohighlight">\(\|A \mathbf{x}\| \leq \|A\|_2 \|\mathbf{x}\|\)</span>, <span class="math notranslate nohighlight">\(\forall \mathbf{0} \neq \mathbf{x} \in \mathbb{R}^m\)</span></p>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> These properties all follow from the definition of the <span class="math notranslate nohighlight">\(2\)</span>-norm and the corresponding properties for the vector norm:</p>
<ul class="simple">
<li><p>Claims a) and f) are immediate from the definition.</p></li>
<li><p>For b) note that <span class="math notranslate nohighlight">\(\|A\|_2 = 0\)</span> implies <span class="math notranslate nohighlight">\(\|A \mathbf{x}\|_2 = 0, \forall \mathbf{x} \in \mathbb{S}^{m-1}\)</span>, so that <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{0}, \forall \mathbf{x} \in \mathbb{S}^{m-1}\)</span>. In particular, <span class="math notranslate nohighlight">\(a_{ij} = \mathbf{e}_i^T A \mathbf{e}_j = 0, \forall i,j\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>For c), d), e), observe that for all <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{S}^{m-1}\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\|\alpha A \mathbf{x}\| = |\alpha| \|A \mathbf{x}\|,
\]</div>
<div class="math notranslate nohighlight">
\[\|(A+B)\mathbf{x}\|
= \|A\mathbf{x} + B\mathbf{x}\| \leq \|A\mathbf{x}\| + \|B\mathbf{x}\|
\leq \|A\|_2 + \|B\|_2
\]</div>
<div class="math notranslate nohighlight">
\[
\|(AB)\mathbf{x}\|
= \|A(B\mathbf{x})\| \leq \|A\|_2 \|B\mathbf{x}\|
\leq \|A\|_2 \|B\|_2.\]</div>
<p>Then apply the definition of <span class="math notranslate nohighlight">\(2\)</span>-norm. For example, for ©,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|\alpha A\|_2 
&amp;= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|\alpha A \mathbf{x}\|\\
&amp;= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} |\alpha| \|A \mathbf{x}\|\\
&amp;= |\alpha| \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|\\
&amp;= |\alpha| \|A\|_2, 
\end{align*}\]</div>
<p>where we used that <span class="math notranslate nohighlight">\(|\alpha|\)</span> does not depend on <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> In NumPy, the Frobenius norm of a matrix can be computed using the default of the function <code class="docutils literal notranslate"><span class="pre">numpy.linalg.norm</span></code> while the induced norm can be computed using the same function with <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html"><code class="docutils literal notranslate"><span class="pre">ord</span></code> parameter set to <code class="docutils literal notranslate"><span class="pre">2</span></code></a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>[[1. 0.]
 [0. 1.]
 [0. 0.]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1.4142135623730951
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">LA</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1.0
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>Matrix norms and SVD</strong> As it turns out, the two notions of matrix norms we have introduced admit simple expressions in terms of the singular values of the matrix.</p>
<p><strong>LEMMA</strong> <strong>(Matrix Norms and Singular Values)</strong> <span class="math notranslate nohighlight">\(\idx{matrix norms and singular values lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix with compact SVD</p>
<div class="math notranslate nohighlight">
\[
A = \sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T
\]</div>
<p>where recall that <span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r &gt; 0\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\|A\|^2_F = \sum_{\ell=1}^r \sigma_\ell^2
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\|A\|^2_2 = \sigma_{1}^2.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> We will use the notation <span class="math notranslate nohighlight">\(\mathbf{v}_\ell = (v_{\ell,1},\ldots,v_{\ell,m})\)</span>. Using that the squared Frobenius norm of <span class="math notranslate nohighlight">\(A\)</span> is the sum of the squared norms of its columns, we have</p>
<div class="math notranslate nohighlight">
\[
\|A\|^2_F
= \left\|\sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T\right\|_F^2
= \sum_{j=1}^m \left\|\sum_{\ell=1}^r \sigma_\ell v_{\ell,j} \mathbf{u}_\ell  \right\|^2.
\]</div>
<p>Because the <span class="math notranslate nohighlight">\(\mathbf{u}_\ell\)</span>’s are orthonormal, this is</p>
<div class="math notranslate nohighlight">
\[
\sum_{j=1}^m \sum_{\ell=1}^r  \sigma_\ell^2 v_{\ell,j}^2
= \sum_{\ell=1}^r  \sigma_\ell^2 \left(\sum_{j=1}^m  v_{\ell,j}^2\right)
= \sum_{\ell=1}^r  \sigma_\ell^2 \|\mathbf{v}_{\ell}\|^2
= \sum_{\ell=1}^r  \sigma_\ell^2,
\]</div>
<p>where we used that the <span class="math notranslate nohighlight">\(\mathbf{v}_\ell\)</span>’s are also orthonormal.</p>
<p>For the second claim, recall that the <span class="math notranslate nohighlight">\(2\)</span>-norm is defined as</p>
<div class="math notranslate nohighlight">
\[
\|A\|_2^2
= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|^2.
\]</div>
<p>We have shown previously that <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> solves this problem. Hence <span class="math notranslate nohighlight">\(\|A\|_2^2 = \|A \mathbf{v}_1\|^2 = \sigma_1^2\)</span>. <span class="math notranslate nohighlight">\(\square\)</span></p>
<!--ONLINE

*Proof:* *(Second proof)* We give a second proof of the second claim that does not use the greedy sequence. Because the $\mathbf{u}_j$'s are orthonormal we have 

\begin{align*}
\|A \mathbf{x}\|^2
&= \left\|\sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell (\mathbf{v}_\ell^T \mathbf{x})\right\|^2\\
&= \sum_{\ell=1}^r \sigma_\ell^2 \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2.
\end{align*}

Here $\langle \mathbf{v}_\ell, \mathbf{x} \rangle^2 \geq 0$ for all $\ell$ and further

\begin{align*}
\sum_{\ell=1}^r \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2
&= \left\|\sum_{\ell=1}^r \langle \mathbf{v}_\ell, \mathbf{x} \rangle \mathbf{v}_\ell \right\|^2\\
&= \left\|\mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}\right\|^2,
\end{align*}

since the $\mathbf{v}_\ell$'s form an orthonormal basis of the row space of $A$. By *Pythagoras*, for a unit norm vector $\mathbf{x}$,

\begin{align*}
1 = \left\|\mathbf{x}\right\|^2
&=  \left\|(\mathbf{x} - \mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}) + \mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}\right\|^2\\
&= \left\|\mathbf{x} - \mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}\right\|^2 + \left\|\mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}\right\|^2
\end{align*}

where we used the orthogonality of $\mathbf{x} - \mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}$ and $\mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}$. That implies in particular that, necessarily, $\left\|\mathrm{proj}_{\mathrm{row}(A)} \mathbf{x}\right\|^2 \leq 1$.

On the other hand, $\mathbf{v}_1$ is a unit norm vector such that

$$
\|A \mathbf{v}_1\|^2
= \sum_{\ell=1}^r \sigma_\ell^2 \langle \mathbf{v}_\ell, \mathbf{v}_1 \rangle^2
= \sigma_1^2.
$$

Hence

$$
\max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\|^2 = \sigma_1^2.
$$

$\square$

--><!--ONLINE 

**EXAMPLE:** Let $A \in \mathbb{R}^{n \times m}$ be a matrix with compact SVD

$$
A = \sum_{\ell=1}^r \sigma_\ell \mathbf{u}_\ell \mathbf{v}_\ell^T
$$

where recall that $\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r > 0$. The proof above can be used to bound the range of possible values for $\|A \mathbf{x}\|$ with $\mathbf{x}$ a unit vector. If $r < m$, then $\mathrm{null}(A)$ contains nonzero vectors and we have

$$
0\leq \|A \mathbf{x}\| \leq \sigma_1
$$

with endpoints achieved.

Suppose now that $r = m$. In that case, $\mathrm{row}(A) = \mathbb{R}^m$ and the right singular vectors $\mathbf{v}_1,\ldots,\mathbf{v}_m$ form an orthonormal basis of $\mathbb{R}^m$. Hence, for any unit vector $\mathbf{x}$,

$$
\sum_{\ell=1}^m \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2
= \left\|\sum_{\ell=1}^m \langle \mathbf{v}_\ell, \mathbf{x} \rangle \mathbf{v}_\ell \right\|^2
= \left\|\mathbf{x}\right\|^2
= 1.
$$

As a result, we have by using $\sigma_n \leq \sigma_\ell$ for all $\ell$ that

\begin{align*}
\|A \mathbf{x}\|^2
&= \sum_{\ell=1}^m \sigma_\ell^2 \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2\\
&\geq \sum_{\ell=1}^m \sigma_m^2 \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2\\
&= \sigma_m^2 \sum_{\ell=1}^m  \langle \mathbf{v}_\ell, \mathbf{x} \rangle^2\\
&= \sigma_m^2.
\end{align*}

We have shown in that case that

$$
\sigma_m \leq \|A \mathbf{x}\| \leq \sigma_1.
$$

The endpoints are achieved by taking $\mathbf{x} = \mathbf{v}_m$ and $\mathbf{x} = \mathbf{v}_1$ respectively.

Put differently,

$$
\min_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\| = \sigma_m
\quad
\text{and}
\quad
\max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|A \mathbf{x}\| = \sigma_1.
$$

(If $\sigma_m$ denotes the smallest singular value in the *full SVD*, then the above expressions hold generally.) $\lhd$

-->&#13;

<h2><span class="section-number">4.6.2. </span>Low-rank approximation<a class="headerlink" href="#low-rank-approximation" title="Link to this heading">#</a></h2>
<p>Now that we have defined a notion of distance between matrices, we will consider the problem of finding a good approximation to a matrix <span class="math notranslate nohighlight">\(A\)</span> among all matrices of rank at most <span class="math notranslate nohighlight">\(k\)</span>. We will start with the Frobenius norm, which is easier to work with, and we will show later on that the solution is the same under the induced norm. The solution to this problem will be familiar. In essence, we will re-interpret our solution to the best approximating subspace as a low-rank approximation.</p>
<p><strong>Low-rank approximation in the Frobenius norm</strong> <span class="math notranslate nohighlight">\(\idx{low-rank approximation}\xdi\)</span> From the proof of the <em>Row Rank Equals Column Rank Lemma</em>, it follows that a rank-<span class="math notranslate nohighlight">\(r\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> can be written as a sum of <span class="math notranslate nohighlight">\(r\)</span> rank-<span class="math notranslate nohighlight">\(1\)</span> matrices</p>
<div class="math notranslate nohighlight">
\[
A = 
\sum_{i=1}^r \mathbf{b}_i \mathbf{c}_i^T.
\]</div>
<p>We will now consider the problem of finding a “simpler” approximation to <span class="math notranslate nohighlight">\(A\)</span></p>
<div class="math notranslate nohighlight">
\[
A \approx \sum_{i=1}^k \mathbf{b}'_i (\mathbf{c}'_i)^T
\]</div>
<p>where <span class="math notranslate nohighlight">\(k &lt; r\)</span>. Here we measure the quality of this approximation using a matrix norm.</p>
<p>We are ready to state our key observation. In words, the best rank-<span class="math notranslate nohighlight">\(k\)</span> approximation to <span class="math notranslate nohighlight">\(A\)</span> in Frobenius norm is obtained by projecting the rows of <span class="math notranslate nohighlight">\(A\)</span> onto a linear subspace of dimension <span class="math notranslate nohighlight">\(k\)</span>. We will come back to how one finds the best such subspace below. (<em>Hint:</em> We have already solved this problem.)</p>
<p><strong>LEMMA</strong> <strong>(Projection and Rank-<span class="math notranslate nohighlight">\(k\)</span> Approximation)</strong> <span class="math notranslate nohighlight">\(\idx{projection and rank-k approximation lemma}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A = (a_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)</span>. For any matrix <span class="math notranslate nohighlight">\(B = (b_{i,j})_{i,j} \in \mathbb{R}^{n \times m}\)</span> of rank <span class="math notranslate nohighlight">\(k \leq \min\{n,m\}\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\|A - B_{\perp}\|_F \leq \|A - B\|_F
\]</div>
<p>where <span class="math notranslate nohighlight">\(B_{\perp} \in \mathbb{R}^{n \times m}\)</span> is the matrix of rank at most <span class="math notranslate nohighlight">\(k\)</span> obtained as follows. Denote row <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(B_{\perp}\)</span> respectively by <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i^T\)</span>, <span class="math notranslate nohighlight">\(\mathbf{b}_{i}^T\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}_{\perp,i}^T\)</span>, <span class="math notranslate nohighlight">\(i=1,\ldots, n\)</span>. Set <span class="math notranslate nohighlight">\(\mathbf{b}_{\perp,i}\)</span> to be the orthogonal projection of <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i\)</span> onto <span class="math notranslate nohighlight">\(\mathcal{Z} = \mathrm{span}(\mathbf{b}_1,\ldots,\mathbf{b}_n)\)</span>. <span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof idea:</em> The square of the Frobenius norm decomposes as a sum of squared row norms. Each term in the sum is minimized by the orthogonal projection.</p>
<p><em>Proof:</em> By definition of the Frobenius norm, we note that</p>
<div class="math notranslate nohighlight">
\[
\|A - B\|_F^2
= \sum_{i=1}^n \sum_{j=1}^m (a_{i,j} - b_{i,j})^2 
= \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2
\]</div>
<p>and similarly for <span class="math notranslate nohighlight">\(\|A - B_{\perp}\|_F\)</span>. We make two observations:</p>
<ol class="arabic simple">
<li><p>Because the orthogonal projection of <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i\)</span> onto <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> minimizes the distance to <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span>, it follows that term by term <span class="math notranslate nohighlight">\(\|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\| \leq \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|\)</span> so that</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\|A - B_\perp\|_F^2 =
\sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{\perp,i}\|^2
\leq \sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathbf{b}_{i}\|^2
= \|A - B\|_F^2.
\]</div>
<ol class="arabic simple" start="2">
<li><p>Moreover, because the projections satisfy <span class="math notranslate nohighlight">\(\mathbf{b}_{\perp,i} \in \mathcal{Z}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\(\mathrm{row}(B_\perp) \subseteq \mathrm{row}(B)\)</span> and, hence, the rank of <span class="math notranslate nohighlight">\(B_\perp\)</span> is at most the rank of <span class="math notranslate nohighlight">\(B\)</span>.</p></li>
</ol>
<p>That concludes the proof. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>Recall the approximating subspace problem. That is, think of the rows <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i^T\)</span> of <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> as a collection of <span class="math notranslate nohighlight">\(n\)</span> data points in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>. We are looking for a linear subspace <span class="math notranslate nohighlight">\(\mathcal{Z}\)</span> that minimizes <span class="math notranslate nohighlight">\(\sum_{i=1}^n \|\boldsymbol{\alpha}_i - \mathrm{proj}_\mathcal{Z}(\boldsymbol{\alpha}_i)\|^2\)</span> over all linear subspaces of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> of dimension at most <span class="math notranslate nohighlight">\(k\)</span>. By the <em>Projection and Rank-<span class="math notranslate nohighlight">\(k\)</span> Approximation Lemma</em>, this problem is equivalent to finding a matrix <span class="math notranslate nohighlight">\(B\)</span> that minimizes <span class="math notranslate nohighlight">\(\|A - B\|_F\)</span> among all matrices in <span class="math notranslate nohighlight">\(\mathbb{R}^{n \times m}\)</span> of rank at most <span class="math notranslate nohighlight">\(k\)</span>. Of course we have solved this problem before.</p>
<p>Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix with SVD <span class="math notranslate nohighlight">\(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span>. For <span class="math notranslate nohighlight">\(k &lt; r\)</span>, truncate the sum at the <span class="math notranslate nohighlight">\(k\)</span>-th term <span class="math notranslate nohighlight">\(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span>. The rank of <span class="math notranslate nohighlight">\(A_k\)</span> is exactly <span class="math notranslate nohighlight">\(k\)</span>. Indeed, by construction,</p>
<ol class="arabic simple">
<li><p>the vectors <span class="math notranslate nohighlight">\(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\)</span> are orthonormal, and</p></li>
<li><p>since <span class="math notranslate nohighlight">\(\sigma_j &gt; 0\)</span> for <span class="math notranslate nohighlight">\(j=1,\ldots,k\)</span> and the vectors <span class="math notranslate nohighlight">\(\{\mathbf{v}_j\,:\,j = 1,\ldots,k\}\)</span> are orthonormal, <span class="math notranslate nohighlight">\(\{\mathbf{u}_j\,:\,j = 1,\ldots,k\}\)</span> spans the column space of <span class="math notranslate nohighlight">\(A_k\)</span>.</p></li>
</ol>
<p>We have shown before that <span class="math notranslate nohighlight">\(A_k\)</span> is the best approximation to <span class="math notranslate nohighlight">\(A\)</span> among matrices of rank at most <span class="math notranslate nohighlight">\(k\)</span> in Frobenius norm. Specifically, the <em>Greedy Finds Best Fit Theorem</em> implies that, for any matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times m}\)</span> of rank at most <span class="math notranslate nohighlight">\(k\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|_F \leq \|A - B\|_F.
\]</div>
<p>This result is known as the <em>Eckart-Young Theorem</em><span class="math notranslate nohighlight">\(\idx{Eckart-Young theorem}\xdi\)</span>. It also holds in the induced <span class="math notranslate nohighlight">\(2\)</span>-norm, as we show next.</p>
<p><strong>Low-rank approximation in the induced norm</strong> We show in this section that the same holds in the induced norm. First, some observations.</p>
<p><strong>LEMMA</strong> <strong>(Matrix Norms and Singular Values: Truncation)</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix with SVD</p>
<div class="math notranslate nohighlight">
\[
A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T
\]</div>
<p>where recall that <span class="math notranslate nohighlight">\(\sigma_1 \geq \sigma_2 \geq \cdots \sigma_r &gt; 0\)</span> and let <span class="math notranslate nohighlight">\(A_k\)</span> be the truncation defined above. Then</p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|^2_F = \sum_{j=k+1}^r \sigma_j^2
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|^2_2 = \sigma_{k+1}^2.
\]</div>
<p><span class="math notranslate nohighlight">\(\flat\)</span></p>
<p><em>Proof:</em> For the first claim, by definition, summing over the columns of <span class="math notranslate nohighlight">\(A - A_k\)</span></p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|^2_F
= \left\|\sum_{j=k+1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\right\|_F^2
= \sum_{i=1}^m \left\|\sum_{j=k+1}^r \sigma_j v_{j,i} \mathbf{u}_j  \right\|^2.
\]</div>
<p>Because the <span class="math notranslate nohighlight">\(\mathbf{u}_j\)</span>’s are orthonormal, this is</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^m \sum_{j=k+1}^r  \sigma_j^2 v_{j,i}^2
= \sum_{j=k+1}^r  \sigma_j^2 \left(\sum_{i=1}^m  v_{j,i}^2\right)
= \sum_{j=k+1}^r  \sigma_j^2
\]</div>
<p>where we used that the <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span>’s are also orthonormal.</p>
<p>For the second claim, recall that the induced norm is defined as</p>
<div class="math notranslate nohighlight">
\[
\|B\|_2
= \max_{\mathbf{x} \in \mathbb{S}^{m-1}} \|B \mathbf{x}\|.
\]</div>
<p>For any <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{S}^{m-1}\)</span></p>
<div class="math notranslate nohighlight">
\[
\left\|(A - A_k)\mathbf{x}\right\|^2 
= \left\| \sum_{j=k+1}^r \sigma_j \mathbf{u}_j (\mathbf{v}_j^T \mathbf{x}) \right\|^2
= \sum_{j=k+1}^r \sigma_j^2 \langle \mathbf{v}_j, \mathbf{x}\rangle^2.
\]</div>
<p>Because the <span class="math notranslate nohighlight">\(\sigma_j\)</span>’s are in decreasing order, this is maximized when
<span class="math notranslate nohighlight">\(\langle \mathbf{v}_j, \mathbf{x}\rangle = 1\)</span> if <span class="math notranslate nohighlight">\(j=k+1\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise. That is, we take <span class="math notranslate nohighlight">\(\mathbf{x} = \mathbf{v}_{k+1}\)</span> and the norm is then <span class="math notranslate nohighlight">\(\sigma_{k+1}^2\)</span>, as claimed. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>THEOREM</strong> <strong>(Low-Rank Approximation in the Induced Norm)</strong> <span class="math notranslate nohighlight">\(\idx{low-rank approximation in the induced norm theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix with SVD</p>
<div class="math notranslate nohighlight">
\[
A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T
\]</div>
<p>and let <span class="math notranslate nohighlight">\(A_k\)</span> be the truncation defined above with <span class="math notranslate nohighlight">\(k &lt; r\)</span>. For any matrix <span class="math notranslate nohighlight">\(B \in \mathbb{R}^{n \times m}\)</span> of rank at most <span class="math notranslate nohighlight">\(k\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|_2 \leq \|A - B\|_2.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> We know that <span class="math notranslate nohighlight">\(\|A - A_k\|_2^2 = \sigma_{k+1}^2\)</span>. So we want to lower bound <span class="math notranslate nohighlight">\(\|A - B\|_2^2\)</span> by <span class="math notranslate nohighlight">\(\sigma_{k+1}^2\)</span>. For that, we have to find an appropriate <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> for any given <span class="math notranslate nohighlight">\(B\)</span> of rank at most <span class="math notranslate nohighlight">\(k\)</span>. The idea is to take a vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> in the intersection of the null space of <span class="math notranslate nohighlight">\(B\)</span> and the span of the singular vectors <span class="math notranslate nohighlight">\(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}\)</span>. By the former, the squared norm of <span class="math notranslate nohighlight">\((A - B)\mathbf{z}\)</span> is equal to the squared norm of <span class="math notranslate nohighlight">\(A\mathbf{z}\)</span> which lower bounds <span class="math notranslate nohighlight">\(\|A\|_2^2\)</span>. By the latter, <span class="math notranslate nohighlight">\(\|A \mathbf{z}\|^2\)</span> is at least <span class="math notranslate nohighlight">\(\sigma_{k+1}^2\)</span>.</p>
<p><em>Proof:</em> By the <em>Rank-Nullity Theorem</em>, the dimension of <span class="math notranslate nohighlight">\(\mathrm{null}(B)\)</span> is at least <span class="math notranslate nohighlight">\(m-k\)</span> so there is a unit vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> in the intersection</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z} \in \mathrm{null}(B) \cap \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1}).
\]</div>
<p>(Prove it!) Then <span class="math notranslate nohighlight">\((A-B)\mathbf{z} = A\mathbf{z}\)</span> since <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathrm{null}(B)\)</span>. Also since  <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{k+1})\)</span>, and therefore orthogonal to <span class="math notranslate nohighlight">\(\mathbf{v}_{k+2},\ldots,\mathbf{v}_r\)</span>, we have</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|(A-B)\mathbf{z}\|^2
&amp;= \|A\mathbf{z}\|^2\\
&amp;= \left\|\sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\
&amp;= \left\|\sum_{j=1}^{k+1} \sigma_j \mathbf{u}_j \mathbf{v}_j^T\mathbf{z}\right\|^2\\
&amp;= \sum_{j=1}^{k+1} \sigma_j^2 \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\
&amp;\geq \sigma_{k+1}^2 \sum_{j=1}^{k+1} \langle \mathbf{v}_j, \mathbf{z}\rangle^2\\
&amp;= \sigma_{k+1}^2.
\end{align*}\]</div>
<p>By the previous lemma, <span class="math notranslate nohighlight">\(\sigma_{k+1}^2 = \|A - A_k\|_2\)</span> and we are done. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>An application: Why project?</strong> We return to <span class="math notranslate nohighlight">\(k\)</span>-means clustering and why projecting to a lower-dimensional subspace can produce better results. We prove a simple inequality that provides some insight. Quoting [BHK, Section 7.5.1]:</p>
<blockquote>
<div><p>[…] let’s understand the central advantage of doing the projection to [the top <span class="math notranslate nohighlight">\(k\)</span> right singular vectors]. It is simply that for any reasonable (unknown) clustering of data points, the projection brings data points closer to their cluster centers.</p>
</div></blockquote>
<p>To elaborate, suppose we have <span class="math notranslate nohighlight">\(n\)</span> data points in <span class="math notranslate nohighlight">\(d\)</span> dimensions in the form of the rows <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i^T\)</span>, <span class="math notranslate nohighlight">\(i=1\ldots, n\)</span>, of matrix <span class="math notranslate nohighlight">\(A \in \mathbb{A}^{n \times d}\)</span>, where we assume that <span class="math notranslate nohighlight">\(n &gt; d\)</span> and that <span class="math notranslate nohighlight">\(A\)</span> has full column rank. Imagine these data points come from an unknown ground-truth <span class="math notranslate nohighlight">\(k\)</span>-clustering assignment <span class="math notranslate nohighlight">\(g(i) \in [k]\)</span>, <span class="math notranslate nohighlight">\(i = 1,\ldots, n\)</span>, with corresponding unknown centers <span class="math notranslate nohighlight">\(\mathbf{c}_j\)</span>, <span class="math notranslate nohighlight">\(j = 1,\ldots, k\)</span>. Let <span class="math notranslate nohighlight">\(C \in \mathbb{R}^{n \times d}\)</span> be the corresponding matrix, that is, row <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(C\)</span> is <span class="math notranslate nohighlight">\(\mathbf{c}_j^T\)</span> if <span class="math notranslate nohighlight">\(g(i) = j\)</span>. The <span class="math notranslate nohighlight">\(k\)</span>-means objective of the true clustering is then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\sum_{j\in [k]} \sum_{i:g(i)=j} \|\boldsymbol{\alpha}_i - \mathbf{c}_{j}\|^2
&amp;= \sum_{j\in [k]} \sum_{i:g(i)=j} \sum_{\ell=1}^d (a_{i,\ell} - c_{j,\ell})^2\\
&amp;= \sum_{i=1}^n \sum_{\ell=1}^d (a_{i,\ell} - c_{g(i),\ell})^2\\
&amp;= \|A - C\|_F^2.
\end{align*}\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(A\)</span> has an SVD <span class="math notranslate nohighlight">\(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span> and for <span class="math notranslate nohighlight">\(k &lt; r\)</span> we have the truncation <span class="math notranslate nohighlight">\(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span>. It corresponds to projecting each row of <span class="math notranslate nohighlight">\(A\)</span> onto the linear subspace spanned by the first <span class="math notranslate nohighlight">\(k\)</span> right singular vectors <span class="math notranslate nohighlight">\(\mathbf{v}_1, \ldots, \mathbf{v}_k\)</span>. To see this, note that the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(A\)</span> is <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i^T = \sum_{j=1}^r \sigma_j u_{j,i} \mathbf{v}_j^T\)</span> and that, because the <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span>’s are linearly independent and in particular <span class="math notranslate nohighlight">\(\mathbf{v}_1,\ldots,\mathbf{v}_k\)</span> is an orthonormal basis of its span, the projection of <span class="math notranslate nohighlight">\(\boldsymbol{\alpha}_i\)</span> onto <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_k)\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\sum_{\ell=1}^k  \mathbf{v}_\ell \left\langle\sum_{j=1}^r \sigma_j u_{j,i} \mathbf{v}_j,\mathbf{v}_\ell\right\rangle
= \sum_{\ell=1}^k \sigma_\ell u_{\ell,i} \mathbf{v}_\ell
\]</div>
<p>which is the <span class="math notranslate nohighlight">\(i\)</span>-th row of <span class="math notranslate nohighlight">\(A_k\)</span>. The <span class="math notranslate nohighlight">\(k\)</span>-means objective of <span class="math notranslate nohighlight">\(A_k\)</span> with respect to the ground-truth centers <span class="math notranslate nohighlight">\(\mathbf{c}_j\)</span>, <span class="math notranslate nohighlight">\(j=1,\ldots,k\)</span>, is <span class="math notranslate nohighlight">\(\|A_k - C\|_F^2\)</span>.</p>
<p>One more observation: the rank of <span class="math notranslate nohighlight">\(C\)</span> is at most <span class="math notranslate nohighlight">\(k\)</span>. Indeed, there are <span class="math notranslate nohighlight">\(k\)</span> different rows in <span class="math notranslate nohighlight">\(C\)</span> so its row rank is <span class="math notranslate nohighlight">\(k\)</span> if these different rows are linearly independent and less than <span class="math notranslate nohighlight">\(k\)</span> otherwise.</p>
<p><strong>THEOREM</strong> <strong>(Why Project)</strong> <span class="math notranslate nohighlight">\(\idx{why project theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{A}^{n \times d}\)</span> be a matrix
and let <span class="math notranslate nohighlight">\(A_k\)</span> be the truncation above. For any matrix <span class="math notranslate nohighlight">\(C \in \mathbb{R}^{n \times d}\)</span> of rank <span class="math notranslate nohighlight">\(\leq k\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\|A_k - C\|_F^2 \leq 8 k \|A - C\|_2^2.
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p>Observe that we used different matrix norms on the different sides of the inequality. The content of this inequality is the following. The quantity <span class="math notranslate nohighlight">\(\|A_k - C\|_F^2\)</span> is the <span class="math notranslate nohighlight">\(k\)</span>-means objective of the projection <span class="math notranslate nohighlight">\(A_k\)</span> with respect to the true centers, that is, the sum of the squared distances to the centers. By the <em>Matrix Norms and Singular Values Lemma</em>, the inequality above gives that</p>
<div class="math notranslate nohighlight">
\[
\|A_k - C\|_F^2 \leq 8 k \sigma_1(A - C)^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_j(A - C)\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th singular value of <span class="math notranslate nohighlight">\(A - C\)</span>. On the other hand, by the same lemma, the <span class="math notranslate nohighlight">\(k\)</span>-means objective of the un-projected data is</p>
<div class="math notranslate nohighlight">
\[
\|A - C\|_F^2 = \sum_{j=1}^{\mathrm{rk}(A-C)} \sigma_j(A - C)^2.
\]</div>
<p>If the rank of <span class="math notranslate nohighlight">\(A-C\)</span> is much larger than <span class="math notranslate nohighlight">\(k\)</span> and the singular values of <span class="math notranslate nohighlight">\(A-C\)</span> decay slowly, then the latter quantity may be much larger. In other words, projecting may bring the data points closer to their true centers, potentially making it easier to cluster them.</p>
<p><em>Proof:</em> <em>(Why Project)</em> We have shown previously that, for any matrices <span class="math notranslate nohighlight">\(A, B \in \mathbb{R}^{n \times m}\)</span>, the rank of their sum is less or equal than the sum of their ranks, that is, <span class="math notranslate nohighlight">\(\mathrm{rk}(A+B) \leq \mathrm{rk}(A) + \mathrm{rk}(B)\)</span>. So the rank of the difference <span class="math notranslate nohighlight">\(A_k - C\)</span> is at most the sum of the ranks</p>
<div class="math notranslate nohighlight">
\[
\mathrm{rk}(A_k - C)
\leq \mathrm{rk}(A_k) + \mathrm{rk}(-C) \leq 2 k
\]</div>
<p>where we used that the rank of <span class="math notranslate nohighlight">\(A_k\)</span> is <span class="math notranslate nohighlight">\(k\)</span> and the rank of <span class="math notranslate nohighlight">\(C\)</span> is <span class="math notranslate nohighlight">\(\leq k\)</span> since it has <span class="math notranslate nohighlight">\(k\)</span> distinct rows. By the <em>Matrix Norms and Singular Values Lemma</em>,</p>
<div class="math notranslate nohighlight">
\[
\|A_k - C\|_F^2 \leq 2k \|A_k - C\|_2^2. 
\]</div>
<p>By the triangle inequality for matrix norms,</p>
<div class="math notranslate nohighlight">
\[
\|A_k - C\|_2
\leq \|A_k - A\|_2 + \|A - C\|_2.
\]</div>
<p>By the <em>Low-Rank Approximation in the Induced Norm Theorem</em>,</p>
<div class="math notranslate nohighlight">
\[
\|A - A_k\|_2
\leq \|A - C\|_2
\]</div>
<p>since <span class="math notranslate nohighlight">\(C\)</span> has rank at most <span class="math notranslate nohighlight">\(k\)</span>. Putting these three inequalities together,</p>
<div class="math notranslate nohighlight">
\[
\|A_k - C\|_F^2 
\leq 2k (2 \|A - C\|_2)^2
= 8k \|A - C\|_2^2. 
\]</div>
<p><span class="math notranslate nohighlight">\(\square\)</span></p>
<p><strong>NUMERICAL CORNER:</strong> We return to our example with the two Gaussian clusters. We use function producing two separate clusters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">two_separate_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    
    <span class="n">mu1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    <span class="n">mu2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="o">-</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
    
    <span class="n">X1</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">spherical_gaussian</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">X2</span> <span class="o">=</span> <span class="n">mmids</span><span class="o">.</span><span class="n">spherical_gaussian</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">mu2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X1</span><span class="p">,</span> <span class="n">X2</span>
</pre></div>
</div>
</div>
</div>
<p>We first generate the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">seed</span> <span class="o">=</span> <span class="mi">535</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">3.</span>
<span class="n">X1</span><span class="p">,</span> <span class="n">X2</span> <span class="o">=</span> <span class="n">two_separate_clusters</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">X1</span><span class="p">,</span> <span class="n">X2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>In reality, we cannot compute the matrix norms of <span class="math notranslate nohighlight">\(X-C\)</span> and <span class="math notranslate nohighlight">\(X_k-C\)</span> as the true centers are not known. But, because this is simulated data, we happen to know the truth and we can check the validity of our results in this case. The centers are:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">C1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">C2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(([</span><span class="o">-</span><span class="n">w</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">d</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">C1</span><span class="p">,</span> <span class="n">C2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We use <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.svd.html"><code class="docutils literal notranslate"><span class="pre">numpy.linalg.svd</span></code></a> function to compute the norms from the formulas in the <em>Matrix Norms and Singular Values Lemma</em>. First, we observe that the singular values of <span class="math notranslate nohighlight">\(X-C\)</span> are decaying slowly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">uc</span><span class="p">,</span> <span class="n">sc</span><span class="p">,</span> <span class="n">vhc</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="o">-</span><span class="n">C</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/a4074a56fd16a51eb92a355bbf35c436e52eced8e21bf3bd48d91efacb43deda.png" src="../Images/23b4327417d560c241971d818c1583cb.png" data-original-src="https://mmids-textbook.github.io/_images/a4074a56fd16a51eb92a355bbf35c436e52eced8e21bf3bd48d91efacb43deda.png"/>
</div>
</div>
<p>The <span class="math notranslate nohighlight">\(k\)</span>-means objective with respect to the true centers under the full-dimensional data is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sc</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>200925.669068181
</pre></div>
</div>
</div>
</div>
<p>while the square of the top singular value of <span class="math notranslate nohighlight">\(X-C\)</span> is only:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="nb">print</span><span class="p">(</span><span class="n">sc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>2095.357155856167
</pre></div>
</div>
</div>
</div>
<p>Finally, we compute the <span class="math notranslate nohighlight">\(k\)</span>-means objective with respect to the true centers under the projected one-dimensional data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">vh</span> <span class="o">=</span> <span class="n">LA</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">u</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">vh</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="n">C</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span/>1614.2173799824254
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\unlhd\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot about the applications of SVD in recommendation systems. How is it used to predict user preferences? <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot about nonnegative matrix factorization (NMF) and how it compares to SVD. What are the key differences in their constraints and applications? How does NMF handle interpretability in topics like text analysis or image processing? Explore some algorithms used to compute NMF. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
&#13;

<h2><span class="section-number">4.6.3. </span>Ridge regression<a class="headerlink" href="#ridge-regression" title="Link to this heading">#</a></h2>
<p>Here we consider what is called Tikhonov regularization<span class="math notranslate nohighlight">\(\idx{Tikhonov regularization}\xdi\)</span>, an idea that turns out to be useful in overdetermined linear systems, particularly when the columns of the matrix <span class="math notranslate nohighlight">\(A\)</span> are linearly dependent or close to linearly dependent (which is sometimes referred to as <a class="reference external" href="https://en.wikipedia.org/wiki/Multicollinearity">multicollinearity</a><span class="math notranslate nohighlight">\(\idx{multicollinearity}\xdi\)</span> in statistics). It trades off minimizing the fit to the data versus minimizing the norm of the solution. More precisely, for a parameter <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span> to be chosen, we solve<span class="math notranslate nohighlight">\(\idx{ridge regression}\xdi\)</span></p>
<div class="math notranslate nohighlight">
\[
\min_{\mathbf{x} \in \mathbb{R}^m} \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2.
\]</div>
<p>The second term is referred to as an <span class="math notranslate nohighlight">\(L_2\)</span>-regularizer<span class="math notranslate nohighlight">\(\idx{L2-regularization}\xdi\)</span>. Here <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n\times m}\)</span> with <span class="math notranslate nohighlight">\(n \geq m\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b} \in \mathbb{R}^n\)</span>.</p>
<p>To solve this optimization problem, we show that the objective function is strongly convex. We then find its unique stationary point. Rewriting the objective in quadratic function form</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
f(\mathbf{x})
&amp;= \|A \mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2\\
&amp;=  \mathbf{x}^T A^T A \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T \mathbf{b}
+ \lambda \mathbf{x}^T \mathbf{x}\\
&amp;= \mathbf{x}^T (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 \mathbf{b}^T A \mathbf{x} + \mathbf{b}^T \mathbf{b}\\
&amp;= \frac{1}{2} \mathbf{x}^T  P \mathbf{x} + \mathbf{q}^T \mathbf{x} + r,
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(P = 2 (A^T A + \lambda I_{m \times m})\)</span> is symmetric, <span class="math notranslate nohighlight">\(\mathbf{q} = - 2 A^T \mathbf{b}\)</span>, and <span class="math notranslate nohighlight">\(r= \mathbf{b}^T \mathbf{b} = \|\mathbf{b}\|^2\)</span>.</p>
<p>As we previously computed, the Hessian of <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(H_f(\mathbf{x})= P\)</span>. Now comes a key observation. The matrix <span class="math notranslate nohighlight">\(P\)</span> is positive definite whenever <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>. Indeed, for any <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^m\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\mathbf{z}^T [2 (A^T A + \lambda I_{m \times m})] \mathbf{z}
= 2 \|A \mathbf{z}\|_2^2 + 2 \lambda \|\mathbf{z}\|_2^2 &gt; 0.
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mu = 2 \lambda &gt; 0\)</span>. Then <span class="math notranslate nohighlight">\(f\)</span> is <span class="math notranslate nohighlight">\(\mu\)</span>-strongly convex. This holds whether or not the columns of <span class="math notranslate nohighlight">\(A\)</span> are linearly independent.</p>
<p>The stationary points are easily characterized. Recall that the gradient is <span class="math notranslate nohighlight">\(\nabla f(\mathbf{x}) = P \mathbf{x} + \mathbf{q}\)</span>. Equating to <span class="math notranslate nohighlight">\(\mathbf{0}\)</span> leads to the system</p>
<div class="math notranslate nohighlight">
\[
2 (A^T A + \lambda I_{m \times m}) \mathbf{x} - 2 A^T \mathbf{b} = \mathbf{0},
\]</div>
<p>that is</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{**}
= (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b}.
\]</div>
<p>The matrix in parenthesis is invertible as it is <span class="math notranslate nohighlight">\(1/2\)</span> of the Hessian, which is positive definite.</p>
<p><strong>Connection to SVD</strong> Expressing the solution in terms of a compact SVD <span class="math notranslate nohighlight">\(A = U \Sigma V^T = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span> provides some insights into how ridge regression works. Suppose that <span class="math notranslate nohighlight">\(A\)</span> has full column rank. That implies that <span class="math notranslate nohighlight">\(V V^T = I_{m \times m}\)</span>. Then observe that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
(A^T A + \lambda I_{m \times m})^{-1}
&amp;= (V \Sigma U^T U \Sigma V^T + \lambda I_{m \times m})^{-1}\\
&amp;= (V \Sigma^2 V^T + \lambda I_{m \times m})^{-1}\\
&amp;= (V \Sigma^2 V^T + V \lambda I_{m \times m} V^T)^{-1}\\
&amp;= (V [\Sigma^2 + \lambda I_{m \times m}] V^T)^{-1}\\
&amp;= V (\Sigma^2 + \lambda I_{m \times m})^{-1} V^T.
\end{align*}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
\mathbf{x}^{**}
= (A^T A + \lambda I_{m \times m})^{-1} A^T \mathbf{b}
= V (\Sigma^2 + \lambda I_{m \times m})^{-1} V^T V \Sigma U^T \mathbf{b}
= V (\Sigma^2 + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}.
\]</div>
<p>Our predictions are</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A \mathbf{x}^{**}
&amp;= U \Sigma V^T V (\Sigma^2 + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}\\
&amp;= U \Sigma (\Sigma^2 + \lambda I_{m \times m})^{-1} \Sigma U^T \mathbf{b}\\
&amp;= \sum_{j=1}^r \mathbf{u}_j \left\{\frac{\sigma_j^2}{\sigma_j^2 + \lambda} \right\} \mathbf{u}_j^T \mathbf{b}.
\end{align*}\]</div>
<p>Note that the terms in curly brackets are <span class="math notranslate nohighlight">\(&lt; 1\)</span> when <span class="math notranslate nohighlight">\(\lambda &gt; 0\)</span>.</p>
<p>Compare this to the unregularized least squares solution, which is obtained simply by setting <span class="math notranslate nohighlight">\(\lambda = 0\)</span> above,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
A \mathbf{x}^*
&amp;= \sum_{j=1}^r \mathbf{u}_j  \mathbf{u}_j^T \mathbf{b}.
\end{align*}\]</div>
<p>The difference is that the regularized solution reduces the contributions from the left singular vectors corresponding to small singular values.</p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> Which of the following best describes the Frobenius norm of a matrix <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span>?</p>
<p>a) The maximum singular value of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>b) The square root of the sum of the squares of all entries in <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>c) The maximum absolute row sum of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>d) The maximum absolute column sum of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><strong>2</strong> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times m}\)</span> be a matrix with SVD <span class="math notranslate nohighlight">\(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span> and let <span class="math notranslate nohighlight">\(A_k = \sum_{j=1}^k \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span> be the truncated SVD with <span class="math notranslate nohighlight">\(k &lt; r\)</span>. Which of the following is true about the Frobenius norm of <span class="math notranslate nohighlight">\(A - A_k\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\|A - A_k\|_F^2 = \sum_{j=1}^k \sigma_j^2\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\|A - A_k\|_F^2 = \sigma_k^2\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\|A - A_k\|_F^2 = \sigma_{k+1}^2\)</span></p>
<p><strong>3</strong> The ridge regression problem is formulated as <span class="math notranslate nohighlight">\(\min_{\mathbf{x} \in \mathbb{R}^m} \|A\mathbf{x} - \mathbf{b}\|_2^2 + \lambda \|\mathbf{x}\|_2^2\)</span>. What is the role of the parameter <span class="math notranslate nohighlight">\(\lambda\)</span>?</p>
<p>a) It controls the trade-off between fitting the data and minimizing the norm of the solution.</p>
<p>b) It determines the rank of the matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>c) It is the smallest singular value of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p>d) It is the largest singular value of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><strong>4</strong> Let <span class="math notranslate nohighlight">\(A\)</span> be an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix with compact SVD <span class="math notranslate nohighlight">\(A = \sum_{j=1}^r \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span>. How does the ridge regression solution <span class="math notranslate nohighlight">\(\mathbf{x}^{**}\)</span> compare to the least squares solution <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\mathbf{x}^{**}\)</span> has larger components along the left singular vectors corresponding to small singular values.</p>
<p>b) <span class="math notranslate nohighlight">\(\mathbf{x}^{**}\)</span> has smaller components along the left singular vectors corresponding to small singular values.</p>
<p>c) <span class="math notranslate nohighlight">\(\mathbf{x}^{**}\)</span> is identical to <span class="math notranslate nohighlight">\(\mathbf{x}^*\)</span>.</p>
<p>d) None of the above.</p>
<p><strong>5</strong> (<em>Note:</em> Refers to online supplementary materials.) Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{n \times n}\)</span> be a square nonsingular matrix with compact SVD <span class="math notranslate nohighlight">\(A = \sum_{j=1}^n \sigma_j \mathbf{u}_j \mathbf{v}_j^T\)</span>. Which of the following is true about the induced 2-norm of the inverse <span class="math notranslate nohighlight">\(A^{-1}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\|A^{-1}\|_2 = \sigma_1\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\|A^{-1}\|_2 = \sigma_n\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\|A^{-1}\|_2 = \sigma_1^{-1}\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\|A^{-1}\|_2 = \sigma_n^{-1}\)</span></p>
<p>Answer for 1: b. Justification: The text defines the Frobenius norm of an <span class="math notranslate nohighlight">\(n \times m\)</span> matrix <span class="math notranslate nohighlight">\(A\)</span> as <span class="math notranslate nohighlight">\(\|A\|_F = \sqrt{\sum_{i=1}^{n} \sum_{j=1}^{m} a_{ij}^2}\)</span>.</p>
<p>Answer for 2: b. Justification: The text proves that <span class="math notranslate nohighlight">\(\|A - A_k\|_F^2 = \sum_{j=k+1}^r \sigma_j^2\)</span> in the Matrix Norms and Singular Values: Truncation Lemma.</p>
<p>Answer for 3: a. Justification: The text explains that ridge regression “trades off minimizing the fit to the data versus minimizing the norm of the solution,” and <span class="math notranslate nohighlight">\(\lambda\)</span> is the parameter that controls this trade-off.</p>
<p>Answer for 4: b. Justification: The text notes that the ridge regression solution “reduces the contributions from the left singular vectors corresponding to small singular values.”</p>
<p>Answer for 5: d. Justification: The text shows in an example that for a square nonsingular matrix <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(\|A^{-1}\|_2 = \sigma_n^{-1}\)</span>, where <span class="math notranslate nohighlight">\(\sigma_n\)</span> is the smallest singular value of <span class="math notranslate nohighlight">\(A\)</span>.</p>
    
</body>
</html>