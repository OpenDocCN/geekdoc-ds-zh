- en: 'Chapter 8 Regression II: linear regression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 8 章 回归 II：线性回归
- en: 原文：[https://datasciencebook.ca/regression2.html](https://datasciencebook.ca/regression2.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://datasciencebook.ca/regression2.html](https://datasciencebook.ca/regression2.html)
- en: 8.1 Overview
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 概述
- en: Up to this point, we have solved all of our predictive problems—both classification
    and regression—using K-nearest neighbors (K-NN)-based approaches. In the context
    of regression, there is another commonly used method known as *linear regression*.
    This chapter provides an introduction to the basic concept of linear regression,
    shows how to use `tidymodels` to perform linear regression in R, and characterizes
    its strengths and weaknesses compared to K-NN regression. The focus is, as usual,
    on the case where there is a single predictor and single response variable of
    interest; but the chapter concludes with an example using *multivariable linear
    regression* when there is more than one predictor.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用基于 K 近邻（K-NN）的方法解决了所有预测问题——无论是分类还是回归。在回归的背景下，还有一种常用的方法，称为 *线性回归*。本章介绍了线性回归的基本概念，展示了如何使用
    `tidymodels` 在 R 中执行线性回归，并比较了其与 K-NN 回归的优缺点。通常情况下，我们关注的是只有一个预测变量和一个感兴趣的响应变量的情况；但本章最后通过一个使用
    *多元线性回归* 的例子来结束，当存在多个预测变量时。
- en: 8.2 Chapter learning objectives
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 章节学习目标
- en: 'By the end of the chapter, readers will be able to do the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，读者将能够做到以下内容：
- en: Use R to fit simple and multivariable linear regression models on training data.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 R 对训练数据拟合简单和多元线性回归模型。
- en: Evaluate the linear regression model on test data.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试数据上评估线性回归模型。
- en: Compare and contrast predictions obtained from K-nearest neighbors regression
    to those obtained using linear regression from the same data set.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较和对比从 K 近邻回归和线性回归获得的相同数据集的预测结果。
- en: Describe how linear regression is affected by outliers and multicollinearity.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述线性回归如何受到异常值和多重共线性影响。
- en: 8.3 Simple linear regression
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 简单线性回归
- en: At the end of the previous chapter, we noted some limitations of K-NN regression.
    While the method is simple and easy to understand, K-NN regression does not predict
    well beyond the range of the predictors in the training data, and the method gets
    significantly slower as the training data set grows. Fortunately, there is an
    alternative to K-NN regression—*linear regression*—that addresses both of these
    limitations. Linear regression is also very commonly used in practice because
    it provides an interpretable mathematical equation that describes the relationship
    between the predictor and response variables. In this first part of the chapter,
    we will focus on *simple* linear regression, which involves only one predictor
    variable and one response variable; later on, we will consider *multivariable*
    linear regression, which involves multiple predictor variables. Like K-NN regression,
    simple linear regression involves predicting a numerical response variable (like
    race time, house price, or height); but *how* it makes those predictions for a
    new observation is quite different from K-NN regression. Instead of looking at
    the K nearest neighbors and averaging over their values for a prediction, in simple
    linear regression, we create a straight line of best fit through the training
    data and then “look up” the prediction using the line.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的结尾，我们提到了 K-NN 回归的一些局限性。虽然这种方法简单易懂，但 K-NN 回归在训练数据预测变量范围之外预测效果不佳，并且随着训练数据集的增长，该方法会显著变慢。幸运的是，K-NN
    回归有一个替代方案——*线性回归*，它解决了这两个局限性。线性回归在实践中也非常常用，因为它提供了一个可解释的数学方程，描述了预测变量和响应变量之间的关系。在本章的第一部分，我们将重点关注
    *简单* 线性回归，它只涉及一个预测变量和一个响应变量；稍后，我们将考虑 *多元* 线性回归，它涉及多个预测变量。与 K-NN 回归一样，简单线性回归涉及预测一个数值响应变量（如比赛时间、房价或身高）；但
    *如何* 它对新观测值的预测与 K-NN 回归有很大不同。简单线性回归不是通过查看最近的 K 个邻居并对其值进行平均来预测，而是通过训练数据创建一条最佳拟合直线，然后“查找”使用这条线的预测值。
- en: '**Note:** Although we did not cover it in earlier chapters, there is another
    popular method for classification called *logistic regression* (it is used for
    classification even though the name, somewhat confusingly, has the word “regression”
    in it). In logistic regression—similar to linear regression—you “fit” the model
    to the training data and then “look up” the prediction for each new observation.
    Logistic regression and K-NN classification have an advantage/disadvantage comparison
    similar to that of linear regression and K-NN regression. It is useful to have
    a good understanding of linear regression before learning about logistic regression.
    After reading this chapter, see the “Additional Resources” section at the end
    of the classification chapters to learn more about logistic regression.'
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 虽然我们在前面的章节中没有涉及，但还有一种流行的分类方法叫做 *逻辑回归*（尽管名字中包含“回归”这个词，有些令人困惑，但它用于分类）。在逻辑回归中——类似于线性回归——你将模型“拟合”到训练数据上，然后“查找”每个新观察值的预测。逻辑回归和K-NN分类的优势/劣势比较类似于线性回归和K-NN回归。在学习逻辑回归之前，对线性回归有一个良好的理解是有用的。阅读本章后，请参阅分类章节末尾的“附加资源”部分，以了解更多关于逻辑回归的信息。'
- en: 'Let’s return to the Sacramento housing data from Chapter [7](regression1.html#regression1)
    to learn how to apply linear regression and compare it to K-NN regression. For
    now, we will consider a smaller version of the housing data to help make our visualizations
    clear. Recall our predictive question: can we use the size of a house in the Sacramento,
    CA area to predict its sale price? In particular, recall that we have come across
    a new 2,000 square-foot house we are interested in purchasing with an advertised
    list price of $350,000\. Should we offer the list price, or is that over/undervalued?
    To answer this question using simple linear regression, we use the data we have
    to draw the straight line of best fit through our existing data points. The small
    subset of data as well as the line of best fit are shown in Figure [8.1](regression2.html#fig:08-lin-reg1).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到第[7](regression1.html#regression1)章中的萨克拉门托房屋数据，学习如何应用线性回归并将其与K-NN回归进行比较。目前，我们将考虑房屋数据的一个较小版本，以帮助使我们的可视化更清晰。回想一下我们的预测问题：我们能否使用萨克拉门托，加利福尼亚地区房屋的面积来预测其销售价格？特别是，回想一下我们遇到了一栋新的2,000平方英尺的房屋，我们对其感兴趣，并标价为$350,000。我们应该出价这个标价，还是这个价格过高/低估了？为了使用简单线性回归来回答这个问题，我们使用我们已有的数据来绘制通过现有数据点的最佳拟合直线。数据的小子集以及最佳拟合线如图[8.1](regression2.html#fig:08-lin-reg1)所示。
- en: '![Scatter plot of sale price versus size with line of best fit for subset of
    the Sacramento housing data.](../Images/956f35e77f445673f36ef926b95d4d30.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![萨克拉门托房屋数据中销售价格与面积散点图，以及最佳拟合线](../Images/956f35e77f445673f36ef926b95d4d30.png)'
- en: 'Figure 8.1: Scatter plot of sale price versus size with line of best fit for
    subset of the Sacramento housing data.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1：萨克拉门托房屋数据子集中销售价格与面积的散点图，以及最佳拟合线。
- en: 'The equation for the straight line is:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 直线的方程是：
- en: \[\text{house sale price} = \beta_0 + \beta_1 \cdot (\text{house size}),\] where
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{房屋销售价格} = \beta_0 + \beta_1 \cdot (\text{房屋面积}),\] 其中
- en: \(\beta_0\) is the *vertical intercept* of the line (the price when house size
    is 0)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\beta_0\) 是这条线的 *y轴截距*（当房屋面积为0时的价格）
- en: \(\beta_1\) is the *slope* of the line (how quickly the price increases as you
    increase house size)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\beta_1\) 是这条线的 *斜率*（随着房屋面积的增大，价格增加的速度）
- en: 'Therefore using the data to find the line of best fit is equivalent to finding
    coefficients \(\beta_0\) and \(\beta_1\) that *parametrize* (correspond to) the
    line of best fit. Now of course, in this particular problem, the idea of a 0 square-foot
    house is a bit silly; but you can think of \(\beta_0\) here as the “base price,”
    and \(\beta_1\) as the increase in price for each square foot of space. Let’s
    push this thought even further: what would happen in the equation for the line
    if you tried to evaluate the price of a house with size 6 *million* square feet?
    Or what about *negative* 2,000 square feet? As it turns out, nothing in the formula
    breaks; linear regression will happily make predictions for nonsensical predictor
    values if you ask it to. But even though you *can* make these wild predictions,
    you shouldn’t. You should only make predictions roughly within the range of your
    original data, and perhaps a bit beyond it only if it makes sense. For example,
    the data in Figure [8.1](regression2.html#fig:08-lin-reg1) only reaches around
    800 square feet on the low end, but it would probably be reasonable to use the
    linear regression model to make a prediction at 600 square feet, say.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用数据来找到最佳拟合线等价于找到参数化（对应于）最佳拟合线的系数 \(\beta_0\) 和 \(\beta_1\)。当然，在这个特定的问题中，0平方英尺房屋的概念有点荒谬；但你可以将这里的
    \(\beta_0\) 视为“基础价格”，\(\beta_1\) 视为每平方英尺空间的价格增加。让我们进一步思考这个问题：如果你尝试评估一个6*百万*平方英尺大小的房屋的价格，方程会发生什么变化？或者负2,000平方英尺呢？实际上，公式中没有任何东西会出错；如果你要求它，线性回归会愉快地为非合理的预测值做出预测。但即使你可以做出这些大胆的预测，你也不应该这样做。你应该只在大约原始数据范围内做出预测，如果合理的话，可以稍微超出这个范围。例如，图[8.1](regression2.html#fig:08-lin-reg1)中的数据在低端只达到大约800平方英尺，但使用线性回归模型在600平方英尺处做出预测可能是合理的。
- en: Back to the example! Once we have the coefficients \(\beta_0\) and \(\beta_1\),
    we can use the equation above to evaluate the predicted sale price given the value
    we have for the predictor variable—here 2,000 square feet. Figure [8.2](regression2.html#fig:08-lin-reg2)
    demonstrates this process.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 回到例子！一旦我们得到了系数 \(\beta_0\) 和 \(\beta_1\)，我们就可以使用上面的方程来评估给定预测变量值（这里为2,000平方英尺）的预测销售价格。图[8.2](regression2.html#fig:08-lin-reg2)展示了这个过程。
- en: '![Scatter plot of sale price versus size with line of best fit and a red dot
    at the predicted sale price for a 2,000 square-foot home.](../Images/c3c284f31484272c3f251d30381d66bf.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![销售价格与面积散点图，最佳拟合线和预测的2,000平方英尺房屋销售价格的红色点。](../Images/c3c284f31484272c3f251d30381d66bf.png)'
- en: 'Figure 8.2: Scatter plot of sale price versus size with line of best fit and
    a red dot at the predicted sale price for a 2,000 square-foot home.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2：销售价格与面积散点图，最佳拟合线和预测的2,000平方英尺房屋销售价格的红色点。
- en: By using simple linear regression on this small data set to predict the sale
    price for a 2,000 square-foot house, we get a predicted value of $295,564\. But
    wait a minute… how exactly does simple linear regression choose the line of best
    fit? Many different lines could be drawn through the data points. Some plausible
    examples are shown in Figure [8.3](regression2.html#fig:08-several-lines).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对这个小数据集进行简单线性回归来预测2,000平方英尺房屋的销售价格，我们得到预测值为$295,564。但是等等…简单线性回归是如何选择最佳拟合线的呢？可以通过数据点画出许多不同的线条。一些可能的例子在图[8.3](regression2.html#fig:08-several-lines)中展示。
- en: '![Scatter plot of sale price versus size with many possible lines that could
    be drawn through the data points.](../Images/1b5c9aa4d266f63eb8d10b5159363c15.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![销售价格与面积散点图，数据点之间可以画出许多可能的线条。](../Images/1b5c9aa4d266f63eb8d10b5159363c15.png)'
- en: 'Figure 8.3: Scatter plot of sale price versus size with many possible lines
    that could be drawn through the data points.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3：销售价格与面积散点图，数据点之间可以画出许多可能的线条。
- en: Simple linear regression chooses the straight line of best fit by choosing the
    line that minimizes the **average squared vertical distance** between itself and
    each of the observed data points in the training data (equivalent to minimizing
    the RMSE). Figure [8.4](regression2.html#fig:08-verticalDistToMin) illustrates
    these vertical distances as red lines. Finally, to assess the predictive accuracy
    of a simple linear regression model, we use RMSPE—the same measure of predictive
    performance we used with K-NN regression.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归通过选择使自身与训练数据中每个观测数据点的**平均平方垂直距离**最小化的直线来选择最佳拟合直线（相当于最小化RMSE）。图[8.4](regression2.html#fig:08-verticalDistToMin)展示了这些垂直距离为红色线条。最后，为了评估简单线性回归模型的预测准确性，我们使用RMSPE——与K-NN回归中使用的相同的预测性能度量。
- en: '![Scatter plot of sale price versus size with red lines denoting the vertical
    distances between the predicted values and the observed data points.](../Images/f4e15edcc9d18a8fa0b4b27cedda6b92.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![销售价格与面积散点图，红色线条表示预测值与观测数据点之间的垂直距离。](../Images/f4e15edcc9d18a8fa0b4b27cedda6b92.png)'
- en: 'Figure 8.4: Scatter plot of sale price versus size with red lines denoting
    the vertical distances between the predicted values and the observed data points.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4：销售价格与面积散点图，红色线条表示预测值与观测数据点之间的垂直距离。
- en: 8.4 Linear regression in R
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 R中的线性回归
- en: We can perform simple linear regression in R using `tidymodels` in a very similar
    manner to how we performed K-NN regression. To do this, instead of creating a
    `nearest_neighbor` model specification with the `kknn` engine, we use a `linear_reg`
    model specification with the `lm` engine. Another difference is that we do not
    need to choose \(K\) in the context of linear regression, and so we do not need
    to perform cross-validation. Below we illustrate how we can use the usual `tidymodels`
    workflow to predict house sale price given house size using a simple linear regression
    approach using the full Sacramento real estate data set.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`tidymodels`在R中执行简单的线性回归，其方式与执行K-NN回归非常相似。为此，我们不是使用`kknn`引擎创建一个`nearest_neighbor`模型规范，而是使用一个`linear_reg`模型规范和一个`lm`引擎。另一个不同之处在于，在线性回归的上下文中，我们不需要选择\(K\)，因此我们不需要进行交叉验证。以下我们展示了如何使用常规的`tidymodels`工作流程，使用完整的萨克拉门托房地产数据集，通过简单线性回归方法预测房屋销售价格。
- en: As usual, we start by loading packages, setting the seed, loading data, and
    putting some test data away in a lock box that we can come back to after we choose
    our final model. Let’s take care of that now.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们首先加载包，设置种子，加载数据，并将一些测试数据放在一个锁箱中，以便在选择最终模型后返回。现在让我们处理这些事情。
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have our training data, we will create the model specification
    and recipe, and fit our simple linear regression model:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练数据，我们将创建模型规范和配方，并拟合我们的简单线性回归模型：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Note:** An additional difference that you will notice here is that we do
    not standardize (i.e., scale and center) our predictors. In K-nearest neighbors
    models, recall that the model fit changes depending on whether we standardize
    first or not. In linear regression, standardization does not affect the fit (it
    *does* affect the coefficients in the equation, though!). So you can standardize
    if you want—it won’t hurt anything—but if you leave the predictors in their original
    form, the best fit coefficients are usually easier to interpret afterward.'
  id: totrans-37
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 你在这里会注意到的一个额外差异是，我们没有对预测变量进行标准化（即，缩放和居中）。在K最近邻模型中，回想一下，模型拟合会根据我们是否首先进行标准化而改变。在线性回归中，标准化不会影响拟合（尽管它会影响方程中的系数！）。所以如果你想标准化，那也没关系——不会有什么坏处——但是如果你保留预测变量的原始形式，通常更容易在之后解释最佳拟合系数。'
- en: Our coefficients are (intercept) \(\beta_0=\) 18450 and (slope) \(\beta_1=\)
    135. This means that the equation of the line of best fit is
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系数是（截距）\(\beta_0=\) 18450和（斜率）\(\beta_1=\) 135。这意味着最佳拟合直线的方程是
- en: \[\text{house sale price} = 18450 + 135\cdot (\text{house size}).\]
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{房屋销售价格} = 18450 + 135\cdot (\text{房屋面积}).\]
- en: 'In other words, the model predicts that houses start at $18,450 for 0 square
    feet, and that every extra square foot increases the cost of the house by $135\.
    Finally, we predict on the test data set to assess how well our model does:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，模型预测房屋从0平方英尺的18,450美元开始，每增加一平方英尺，房屋成本增加135美元。最后，我们在测试数据集上进行预测，以评估我们的模型表现如何：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our final model’s test error as assessed by RMSPE is $88,528. Remember that
    this is in units of the response variable, and here that is US Dollars (USD).
    Does this mean our model is “good” at predicting house sale price based off of
    the predictor of home size? Again, answering this is tricky and requires knowledge
    of how you intend to use the prediction.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终模型的测试误差，通过RMSPE评估，为$88,528。请记住，这是以响应变量的单位来计算的，在这里是指美元（USD）。这意味着我们的模型在根据房屋大小预测房价方面“很好”吗？再次，回答这个问题很棘手，需要了解你打算如何使用预测。
- en: To visualize the simple linear regression model, we can plot the predicted house
    sale price across all possible house sizes we might encounter. Since our model
    is linear, we only need to compute the predicted price of the minimum and maximum
    house size, and then connect them with a straight line. We superimpose this prediction
    line on a scatter plot of the original housing price data, so that we can qualitatively
    assess if the model seems to fit the data well. Figure [8.5](regression2.html#fig:08-lm-predict-all)
    displays the result.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化简单线性回归模型，我们可以绘制所有可能遇到的房屋大小的预测房价。由于我们的模型是线性的，我们只需要计算最小和最大房屋大小的预测价格，然后通过直线连接它们。我们将这个预测线叠加在原始房价数据的散点图上，这样我们就可以定性评估模型是否很好地拟合数据。图[8.5](regression2.html#fig:08-lm-predict-all)显示了结果。
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Scatter plot of sale price versus size with line of best fit for the full
    Sacramento housing data.](../Images/64fd258a226e0c8e2c7c4b64f3709017.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![销售价格与大小关系的散点图，以及萨克拉门托全部住房数据的最佳拟合线。](../Images/64fd258a226e0c8e2c7c4b64f3709017.png)'
- en: 'Figure 8.5: Scatter plot of sale price versus size with line of best fit for
    the full Sacramento housing data.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5：销售价格与大小关系的散点图，以及萨克拉门托全部住房数据的最佳拟合线。
- en: 'We can extract the coefficients from our model by accessing the fit object
    that is output by the `fit` function; we first have to extract it from the workflow
    using the `extract_fit_parsnip` function, and then apply the `tidy` function to
    convert the result into a data frame:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过访问由`fit`函数输出的拟合对象来从我们的模型中提取系数；我们首先必须使用`extract_fit_parsnip`函数从工作流程中提取它，然后应用`tidy`函数将结果转换为数据框：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 8.5 Comparing simple linear and K-NN regression
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 比较简单线性回归和K-NN回归
- en: Now that we have a general understanding of both simple linear and K-NN regression,
    we can start to compare and contrast these methods as well as the predictions
    made by them. To start, let’s look at the visualization of the simple linear regression
    model predictions for the Sacramento real estate data (predicting price from house
    size) and the “best” K-NN regression model obtained from the same problem, shown
    in Figure [8.6](regression2.html#fig:08-compareRegression).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对简单线性回归和K-NN回归都有了基本的了解，我们可以开始比较和对比这些方法以及它们所做的预测。首先，让我们看看简单线性回归模型预测的视觉化，这是针对萨克拉门托房地产数据（从房屋大小预测价格）以及从同一问题中获得的“最佳”K-NN回归模型，如图[8.6](regression2.html#fig:08-compareRegression)所示。
- en: '![Comparison of simple linear regression and K-NN regression.](../Images/19283f03e4cbeb3f16540223d9f68a05.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![简单线性回归和K-NN回归的比较。](../Images/19283f03e4cbeb3f16540223d9f68a05.png)'
- en: 'Figure 8.6: Comparison of simple linear regression and K-NN regression.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6：简单线性回归和K-NN回归的比较。
- en: What differences do we observe in Figure [8.6](regression2.html#fig:08-compareRegression)?
    One obvious difference is the shape of the blue lines. In simple linear regression
    we are restricted to a straight line, whereas in K-NN regression our line is much
    more flexible and can be quite wiggly. But there is a major interpretability advantage
    in limiting the model to a straight line. A straight line can be defined by two
    numbers, the vertical intercept and the slope. The intercept tells us what the
    prediction is when all of the predictors are equal to 0; and the slope tells us
    what unit increase in the response variable we predict given a unit increase in
    the predictor variable. K-NN regression, as simple as it is to implement and understand,
    has no such interpretability from its wiggly line.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[8.6](regression2.html#fig:08-compareRegression)中观察到哪些差异？一个明显的差异是蓝色线的形状。在简单线性回归中，我们被限制在一条直线上，而在K-NN回归中，我们的线要灵活得多，可以相当曲折。但是，将模型限制为直线有一个主要的可解释性优势。一条直线可以用两个数字来定义，即垂直截距和斜率。截距告诉我们当所有预测变量都等于0时预测是什么；斜率告诉我们，在预测变量单位增加的情况下，我们预测的响应变量单位增加是多少。K-NN回归，尽管实现和理解都很简单，但其曲折的线没有这样的可解释性。
- en: There can, however, also be a disadvantage to using a simple linear regression
    model in some cases, particularly when the relationship between the response and
    the predictor is not linear, but instead some other shape (e.g., curved or oscillating).
    In these cases the prediction model from a simple linear regression will underfit,
    meaning that model/predicted values do not match the actual observed values very
    well. Such a model would probably have a quite high RMSE when assessing model
    goodness of fit on the training data and a quite high RMSPE when assessing model
    prediction quality on a test data set. On such a data set, K-NN regression may
    fare better. Additionally, there are other types of regression you can learn about
    in future books that may do even better at predicting with such data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，使用简单线性回归模型也可能存在缺点，尤其是在响应变量和预测变量之间的关系不是线性的，而是其他形状（例如，曲线或振荡）时。在这些情况下，简单线性回归模型的预测模型会欠拟合，意味着模型/预测值与实际观察值匹配得不是很好。这样的模型在评估训练数据上的模型拟合优度时可能具有相当高的RMSE，在评估测试数据集上的模型预测质量时可能具有相当高的RMSPE。在这种情况下，K-NN回归可能表现得更好。此外，你可以在未来的书籍中了解到其他类型的回归，这些回归在预测此类数据时可能表现得更好。
- en: How do these two models compare on the Sacramento house prices data set? In
    Figure [8.6](regression2.html#fig:08-compareRegression), we also printed the RMSPE
    as calculated from predicting on the test data set that was not used to train/fit
    the models. The RMSPE for the simple linear regression model is slightly lower
    than the RMSPE for the K-NN regression model. Considering that the simple linear
    regression model is also more interpretable, if we were comparing these in practice
    we would likely choose to use the simple linear regression model.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型在萨克拉门托房价数据集上的比较如何？在图[8.6](regression2.html#fig:08-compareRegression)中，我们还打印了从未用于训练/拟合模型的测试数据集上预测得到的RMSPE。简单线性回归模型的RMSPE略低于K-NN回归模型的RMSPE。考虑到简单线性回归模型也更容易解释，如果我们实际比较这些模型，我们可能会选择使用简单线性回归模型。
- en: Finally, note that the K-NN regression model becomes “flat” at the left and
    right boundaries of the data, while the linear model predicts a constant slope.
    Predicting outside the range of the observed data is known as *extrapolation*;
    K-NN and linear models behave quite differently when extrapolating. Depending
    on the application, the flat or constant slope trend may make more sense. For
    example, if our housing data were slightly different, the linear model may have
    actually predicted a *negative* price for a small house (if the intercept \(\beta_0\)
    was negative), which obviously does not match reality. On the other hand, the
    trend of increasing house size corresponding to increasing house price probably
    continues for large houses, so the “flat” extrapolation of K-NN likely does not
    match reality.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，K-NN回归模型在数据的左右边界处变得“平坦”，而线性模型预测的是恒定的斜率。预测观察数据范围之外的数据被称为*外推*；K-NN和线性模型在外推时的行为相当不同。根据应用的不同，平坦或恒定的斜率趋势可能更有意义。例如，如果我们的房价数据略有不同，线性模型实际上可能预测了一个小房子的*负*价格（如果截距
    \(\beta_0\) 为负），这显然不符合现实。另一方面，随着房价的增加，房屋面积增加的趋势可能对大房子继续适用，因此K-NN的“平坦”外推可能不符合现实。
- en: 8.6 Multivariable linear regression
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 多元线性回归
- en: 'As in K-NN classification and K-NN regression, we can move beyond the simple
    case of only one predictor to the case with multiple predictors, known as *multivariable
    linear regression*. To do this, we follow a very similar approach to what we did
    for K-NN regression: we just add more predictors to the model formula in the recipe.
    But recall that we do not need to use cross-validation to choose any parameters,
    nor do we need to standardize (i.e., center and scale) the data for linear regression.
    Note once again that we have the same concerns regarding multiple predictors as
    in the settings of multivariable K-NN regression and classification: having more
    predictors is **not** always better. But because the same predictor selection
    algorithm from the classification chapter extends to the setting of linear regression,
    it will not be covered again in this chapter.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在K-NN分类和K-NN回归中一样，我们可以从只有一个预测器的简单情况扩展到具有多个预测器的案例，这被称为*多元线性回归*。为此，我们遵循与K-NN回归非常相似的方法：我们只是在配方中的模型公式中添加更多的预测器。但请记住，我们不需要使用交叉验证来选择任何参数，也不需要标准化（即，中心和缩放）数据以进行线性回归。再次提醒，我们关于多个预测器的担忧与多元K-NN回归和分类设置中的担忧相同：拥有更多预测器**并不**总是更好的。但由于分类章节中的相同预测器选择算法也适用于线性回归设置，因此它将不会在本章中再次介绍。
- en: 'We will demonstrate multivariable linear regression using the Sacramento real
    estate data with both house size (measured in square feet) as well as number of
    bedrooms as our predictors, and continue to use house sale price as our response
    variable. We will start by changing the formula in the recipe to include both
    the `sqft` and `beds` variables as predictors:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用萨克拉门托房地产数据来演示多元线性回归，其中房屋面积（以平方英尺计）以及卧室数量作为预测器，并继续使用房屋销售价格作为响应变量。我们将首先修改配方中的公式，包括`sqft`和`beds`变量作为预测器：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we can build our workflow and fit the model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以构建我们的工作流程并拟合模型：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And finally, we make predictions on the test data set to assess the quality
    of our model:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将在测试数据集上进行预测，以评估我们模型的质量：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Our model’s test error as assessed by RMSPE is $88,739. In the case of two predictors,
    we can plot the predictions made by our linear regression creates a *plane* of
    best fit, as shown in Figure [8.7](regression2.html#fig:08-3DlinReg).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型通过RMSPE评估的测试误差为$88,739。在有两个预测器的情况下，我们可以绘制线性回归创建的最佳拟合平面，如图[8.7](regression2.html#fig:08-3DlinReg)所示。
- en: 'Figure 8.7: Linear regression plane of best fit overlaid on top of the data
    (using price, house size, and number of bedrooms as predictors). Note that in
    general we recommend against using 3D visualizations; here we use a 3D visualization
    only to illustrate what the regression plane looks like for learning purposes.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7：最佳拟合线性回归平面叠加在数据之上（使用价格、房屋面积和卧室数量作为预测器）。请注意，通常我们不建议使用3D可视化；这里我们仅使用3D可视化来展示回归平面在学习目的下的样子。
- en: 'We see that the predictions from linear regression with two predictors form
    a flat plane. This is the hallmark of linear regression, and differs from the
    wiggly, flexible surface we get from other methods such as K-NN regression. As
    discussed, this can be advantageous in one aspect, which is that for each predictor,
    we can get slopes/intercept from linear regression, and thus describe the plane
    mathematically. We can extract those slope values from our model object as shown
    below:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，具有两个预测器的线性回归预测形成了一个平坦的平面。这是线性回归的特点，与K-NN回归等其他方法得到的扭曲、灵活的表面不同。正如讨论的那样，这在一方面可能是有利的，即对于每个预测器，我们可以从线性回归中得到斜率和截距，从而从数学上描述平面。我们可以从我们的模型对象中提取这些斜率值，如下所示：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And then use those slopes to write a mathematical equation to describe the
    prediction plane:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用这些斜率来编写一个数学方程来描述预测平面：
- en: '\[\text{house sale price} = \beta_0 + \beta_1\cdot(\text{house size}) + \beta_2\cdot(\text{number
    of bedrooms}),\] where:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{house sale price} = \beta_0 + \beta_1\cdot(\text{house size}) + \beta_2\cdot(\text{number
    of bedrooms}),\] 其中：
- en: \(\beta_0\) is the *vertical intercept* of the hyperplane (the price when both
    house size and number of bedrooms are 0)
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\beta_0\) 是超平面的垂直截距（当房屋面积和卧室数量都是0时的价格）
- en: \(\beta_1\) is the *slope* for the first predictor (how quickly the price changes
    as you increase house size, holding number of bedrooms constant)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\beta_1\) 是第一个预测器的斜率（在保持卧室数量不变的情况下，房价随房屋面积增加的变化速度）
- en: \(\beta_2\) is the *slope* for the second predictor (how quickly the price changes
    as you increase the number of bedrooms, holding house size constant)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\beta_2\)是第二个预测变量的*斜率*（在保持房屋面积不变的情况下，随着卧室数量的增加，价格变化的快慢）
- en: 'Finally, we can fill in the values for \(\beta_0\), \(\beta_1\) and \(\beta_2\)
    from the model output above to create the equation of the plane of best fit to
    the data:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以从上面的模型输出中填充\(\beta_0\)、\(\beta_1\)和\(\beta_2\)的值，以创建最佳拟合数据平面的方程：
- en: \[\text{house sale price} = 72548 + 161\cdot (\text{house size}) -29644 \cdot
    (\text{number of bedrooms})\]
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{房价} = 72548 + 161\cdot (\text{房屋面积}) -29644 \cdot (\text{卧室数量})\]
- en: This model is more interpretable than the multivariable K-NN regression model;
    we can write a mathematical equation that explains how each predictor is affecting
    the predictions. But as always, we should question how well multivariable linear
    regression is doing compared to the other tools we have, such as simple linear
    regression and multivariable K-NN regression. If this comparison is part of the
    model tuning process—for example, if we are trying out many different sets of
    predictors for multivariable linear and K-NN regression—we must perform this comparison
    using cross-validation on only our training data. But if we have already decided
    on a small number (e.g., 2 or 3) of tuned candidate models and we want to make
    a final comparison, we can do so by comparing the prediction error of the methods
    on the test data.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型比多变量K-NN回归模型更可解释；我们可以写出一个数学方程来解释每个预测变量是如何影响预测的。但就像往常一样，我们应该质疑多变量线性回归与其他工具（如简单线性回归和多变量K-NN回归）相比做得如何。如果这种比较是模型调整过程的一部分——例如，如果我们正在尝试多变量线性回归和K-NN回归的许多不同预测变量集——我们必须仅使用我们的训练数据来进行这种比较。但如果我们已经决定了一个小数量（例如，2或3）的调整候选模型，并且我们想要进行最终比较，我们可以通过比较测试数据上方法的预测误差来完成。
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We obtain an RMSPE for the multivariable linear regression model of $88,739.45\.
    This prediction error is less than the prediction error for the multivariable
    K-NN regression model, indicating that we should likely choose linear regression
    for predictions of house sale price on this data set. Revisiting the simple linear
    regression model with only a single predictor from earlier in this chapter, we
    see that the RMSPE for that model was $88,527.75, which is almost the same as
    that of our more complex model. As mentioned earlier, this is not always the case:
    often including more predictors will either positively or negatively impact the
    prediction performance on unseen test data.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到多变量线性回归模型的均方根预测误差（RMSPE）为$88,739.45。这个预测误差小于多变量K-NN回归模型的预测误差，这表明我们可能应该选择线性回归来预测这个数据集上的房价。回顾本章前面提到的只有一个预测变量的简单线性回归模型，我们看到该模型的RMSPE为$88,527.75，这与我们的更复杂模型几乎相同。如前所述，这并不总是如此：通常包括更多预测变量要么会正面要么会负面地影响未见测试数据的预测性能。
- en: 8.7 Multicollinearity and outliers
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.7 多重共线性与异常值
- en: What can go wrong when performing (possibly multivariable) linear regression?
    This section will introduce two common issues—*outliers* and *collinear predictors*—and
    illustrate their impact on predictions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行（可能是多变量）线性回归时可能会出什么问题？本节将介绍两个常见问题——*异常值*和*共线性预测变量*——并说明它们对预测的影响。
- en: 8.7.1 Outliers
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.1 异常值
- en: Outliers are data points that do not follow the usual pattern of the rest of
    the data. In the setting of linear regression, these are points that have a vertical
    distance to the line of best fit that is either much higher or much lower than
    you might expect based on the rest of the data. The problem with outliers is that
    they can have *too much influence* on the line of best fit. In general, it is
    very difficult to judge accurately which data are outliers without advanced techniques
    that are beyond the scope of this book.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值是不遵循其余数据通常模式的数据点。在线性回归的设置中，这些是到最佳拟合线的垂直距离要么远高于，要么远低于基于其余数据的预期。异常值的问题在于它们可能会对最佳拟合线产生*过多的影响*。一般来说，没有超出本书范围的先进技术，很难准确判断哪些数据是异常值。
- en: But to illustrate what can happen when you have outliers, Figure [8.8](regression2.html#fig:08-lm-outlier)
    shows a small subset of the Sacramento housing data again, except we have added
    a *single* data point (highlighted in red). This house is 5,000 square feet in
    size, and sold for only $50,000\. Unbeknownst to the data analyst, this house
    was sold by a parent to their child for an absurdly low price. Of course, this
    is not representative of the real housing market values that the other data points
    follow; the data point is an *outlier*. In blue we plot the original line of best
    fit, and in red we plot the new line of best fit including the outlier. You can
    see how different the red line is from the blue line, which is entirely caused
    by that one extra outlier data point.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 但为了说明当存在异常值时可能发生的情况，图[8.8](regression2.html#fig:08-lm-outlier)再次展示了萨克拉门托住房数据的一个小子集，除了我们增加了一个*单个*数据点（用红色突出显示）。这所房子面积为5,000平方英尺，售价仅为$50,000。数据分析师并不知道，这所房子是以一个荒谬的低价由父母卖给子女的。当然，这并不代表其他数据点所遵循的真实住房市场价值；这个数据点是*异常值*。我们用蓝色绘制了原始的最佳拟合线，用红色绘制了包含异常值的新最佳拟合线。你可以看到红色线与蓝色线有多么不同，这完全是由于那个额外的异常数据点造成的。
- en: '![Scatter plot of a subset of the data, with outlier highlighted in red.](../Images/ee2e51b826b4f22a330de7a97212c1d1.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![数据子集的散点图，异常值用红色突出显示](../Images/ee2e51b826b4f22a330de7a97212c1d1.png)'
- en: 'Figure 8.8: Scatter plot of a subset of the data, with outlier highlighted
    in red.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8：数据子集的散点图，异常值用红色突出显示。
- en: Fortunately, if you have enough data, the inclusion of one or two outliers—as
    long as their values are not *too* wild—will typically not have a large effect
    on the line of best fit. Figure [8.9](regression2.html#fig:08-lm-outlier-2) shows
    how that same outlier data point from earlier influences the line of best fit
    when we are working with the entire original Sacramento training data. You can
    see that with this larger data set, the line changes much less when adding the
    outlier. Nevertheless, it is still important when working with linear regression
    to critically think about how much any individual data point is influencing the
    model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，如果你有足够的数据，包含一个或两个异常值——只要它们的值不是*太*离谱——通常不会对最佳拟合线产生大的影响。图[8.9](regression2.html#fig:08-lm-outlier-2)展示了当我们使用整个原始萨克拉门托训练数据时，之前提到的那个异常数据点如何影响最佳拟合线。你可以看到，在这个更大的数据集中，添加异常值时线的变化很小。尽管如此，当进行线性回归时，仍然非常重要，要批判性地思考任何单个数据点对模型的影响程度。
- en: '![Scatter plot of the full data, with outlier highlighted in red.](../Images/f4e8df982da092d875456886bf240b19.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![完整数据的散点图，异常值用红色突出显示](../Images/f4e8df982da092d875456886bf240b19.png)'
- en: 'Figure 8.9: Scatter plot of the full data, with outlier highlighted in red.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9：完整数据的散点图，异常值用红色突出显示。
- en: 8.7.2 Multicollinearity
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7.2 多重共线性
- en: The second, and much more subtle, issue can occur when performing multivariable
    linear regression. In particular, if you include multiple predictors that are
    strongly linearly related to one another, the coefficients that describe the plane
    of best fit can be very unreliable—small changes to the data can result in large
    changes in the coefficients. Consider an extreme example using the Sacramento
    housing data where the house was measured twice by two people. Since the two people
    are each slightly inaccurate, the two measurements might not agree exactly, but
    they are very strongly linearly related to each other, as shown in Figure [8.10](regression2.html#fig:08-lm-multicol).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行多元线性回归时，第二个问题，并且是一个更加微妙的问题可能会发生。特别是，如果你包括多个彼此之间强线性相关的预测变量，描述最佳拟合平面的系数可能会非常不可靠——数据的小幅变化可能导致系数的大幅变化。考虑一个使用萨克拉门托住房数据的极端例子，其中房子由两个人分别测量了两次。由于两个人各自都有轻微的不准确，两次测量可能不会完全一致，但它们之间非常强地线性相关，如图[8.10](regression2.html#fig:08-lm-multicol)所示。
- en: '![Scatter plot of house size (in square feet) measured by person 1 versus house
    size (in square feet) measured by person 2.](../Images/a8a56b0fd78c34f13a2c5967aef174d8.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![由第1个人测量的房屋面积（平方英尺）与由第2个人测量的房屋面积（平方英尺）的散点图](../Images/a8a56b0fd78c34f13a2c5967aef174d8.png)'
- en: 'Figure 8.10: Scatter plot of house size (in square feet) measured by person
    1 versus house size (in square feet) measured by person 2.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10：由第1个人测量的房屋面积（平方英尺）与由第2个人测量的房屋面积（平方英尺）的散点图。
- en: 'If we again fit the multivariable linear regression model on this data, then
    the plane of best fit has regression coefficients that are very sensitive to the
    exact values in the data. For example, if we change the data ever so slightly—e.g.,
    by running cross-validation, which splits up the data randomly into different
    chunks—the coefficients vary by large amounts:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们再次将多元线性回归模型拟合到这些数据上，那么最佳拟合平面的回归系数对数据的精确值非常敏感。例如，如果我们对数据进行微小的改变——例如，通过运行交叉验证，将数据随机分成不同的块——系数会大幅变化：
- en: 'Best Fit 1: \(\text{house sale price} = 22535 + (220)\cdot (\text{house size
    1 (ft$^2$)}) + (-86) \cdot (\text{house size 2 (ft$^2$)}).\)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳拟合1：\(\text{房价} = 22535 + (220)\cdot (\text{房屋面积1 (平方英尺)}) + (-86) \cdot (\text{房屋面积2
    (平方英尺)})\).
- en: 'Best Fit 2: \(\text{house sale price} = 15966 + (86)\cdot (\text{house size
    1 (ft$^2$)}) + (49) \cdot (\text{house size 2 (ft$^2$)}).\)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳拟合2：\(\text{房价} = 15966 + (86)\cdot (\text{房屋面积1 (平方英尺)}) + (49) \cdot (\text{房屋面积2
    (平方英尺)})\).
- en: 'Best Fit 3: \(\text{house sale price} = 17178 + (107)\cdot (\text{house size
    1 (ft$^2$)}) + (27) \cdot (\text{house size 2 (ft$^2$)}).\)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳拟合3：\(\text{房价} = 17178 + (107)\cdot (\text{房屋面积1 (平方英尺)}) + (27) \cdot (\text{房屋面积2
    (平方英尺)})\).
- en: Therefore, when performing multivariable linear regression, it is important
    to avoid including very linearly related predictors. However, techniques for doing
    so are beyond the scope of this book; see the list of additional resources at
    the end of this chapter to find out where you can learn more.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在进行多元线性回归时，避免包括非常线性相关的预测变量是很重要的。然而，实现这一点的技术超出了本书的范围；请参阅本章末尾的附加资源列表，以了解您可以在哪里了解更多信息。
- en: 8.8 Designing new predictors
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.8 设计新的预测变量
- en: We were quite fortunate in our initial exploration to find a predictor variable
    (house size) that seems to have a meaningful and nearly linear relationship with
    our response variable (sale price). But what should we do if we cannot immediately
    find such a nice variable? Well, sometimes it is just a fact that the variables
    in the data do not have enough of a relationship with the response variable to
    provide useful predictions. For example, if the only available predictor was “the
    current house owner’s favorite ice cream flavor”, we likely would have little
    hope of using that variable to predict the house’s sale price (barring any future
    remarkable scientific discoveries about the relationship between the housing market
    and homeowner ice cream preferences). In cases like these, the only option is
    to obtain measurements of more useful variables.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初步探索中，我们非常幸运地发现了一个预测变量（房屋面积），它似乎与我们的响应变量（售价）有有意义且几乎是线性的关系。但如果我们不能立即找到这样的变量，我们应该怎么办？好吧，有时这仅仅是一个事实，数据中的变量与响应变量之间没有足够的关系，无法提供有用的预测。例如，如果唯一可用的预测变量是“当前房屋所有者最喜欢的冰淇淋口味”，我们可能几乎没有希望使用该变量来预测房屋的售价（除非未来有任何关于房地产市场和房屋所有者冰淇淋偏好的重大科学发现）。在这种情况下，唯一的选择是获取更多有用变量的测量值。
- en: There are, however, a wide variety of cases where the predictor variables do
    have a meaningful relationship with the response variable, but that relationship
    does not fit the assumptions of the regression method you have chosen. For example,
    a data frame `df` with two variables—`x` and `y`—with a nonlinear relationship
    between the two variables will not be fully captured by simple linear regression,
    as shown in Figure [8.11](regression2.html#fig:08-predictor-design).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，存在许多情况，预测变量确实与响应变量有有意义的关系，但这种关系不符合你选择的回归方法的假设。例如，一个包含两个变量——`x`和`y`——的数据框`df`，这两个变量之间存在非线性关系，将无法被简单的线性回归完全捕捉，如图[8.11](regression2.html#fig:08-predictor-design)所示。
- en: '[PRE17]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Example of a data set with a nonlinear relationship between the predictor
    and the response.](../Images/0b1eefb775cd5ea73071f573ccf1bec0.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![预测变量和响应变量之间非线性关系的示例数据集](../Images/0b1eefb775cd5ea73071f573ccf1bec0.png)'
- en: 'Figure 8.11: Example of a data set with a nonlinear relationship between the
    predictor and the response.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11：预测变量和响应变量之间非线性关系的示例数据集。
- en: 'Instead of trying to predict the response `y` using a linear regression on
    `x`, we might have some scientific background about our problem to suggest that
    `y` should be a cubic function of `x`. So before performing regression, we might
    *create a new predictor variable* `z` using the `mutate` function:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是试图通过在`x`上使用线性回归来预测响应`y`，我们可能对我们的问题有一些科学背景，这表明`y`应该是`x`的三次函数。因此，在进行回归之前，我们可能需要使用`mutate`函数*创建一个新的预测变量*
    `z`：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Then we can perform linear regression for `y` using the predictor variable `z`,
    as shown in Figure [8.12](regression2.html#fig:08-predictor-design-2). Here you
    can see that the transformed predictor `z` helps the linear regression model make
    more accurate predictions. Note that none of the `y` response values have changed
    between Figures [8.11](regression2.html#fig:08-predictor-design) and [8.12](regression2.html#fig:08-predictor-design-2);
    the only change is that the `x` values have been replaced by `z` values.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用预测变量 `z` 对 `y` 进行线性回归，如图 [8.12](regression2.html#fig:08-predictor-design-2)
    所示。在这里，你可以看到变换后的预测变量 `z` 有助于线性回归模型做出更准确的预测。请注意，在图 [8.11](regression2.html#fig:08-predictor-design)
    和 [8.12](regression2.html#fig:08-predictor-design-2) 之间，`y` 的响应值没有任何变化；唯一的变化是将
    `x` 值替换为 `z` 值。
- en: '![Relationship between the transformed predictor and the response.](../Images/952aad7c74151234a4dd611703455f21.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![变换后的预测变量与响应变量之间的关系。](../Images/952aad7c74151234a4dd611703455f21.png)'
- en: 'Figure 8.12: Relationship between the transformed predictor and the response.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12：变换后的预测变量与响应变量之间的关系。
- en: The process of transforming predictors (and potentially combining multiple predictors
    in the process) is known as *feature engineering*. In real data analysis problems,
    you will need to rely on a deep understanding of the problem—as well as the wrangling
    tools from previous chapters—to engineer useful new features that improve predictive
    performance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将预测变量（以及在过程中可能组合多个预测变量）进行变换的过程称为*特征工程*。在实际数据分析问题中，你需要依靠对问题的深入理解——以及前几章中的处理工具——来构建有用的新特征，从而提高预测性能。
- en: '**Note:** Feature engineering is *part of tuning your model*, and as such you
    must not use your test data to evaluate the quality of the features you produce.
    You are free to use cross-validation, though!'
  id: totrans-117
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**特征工程是*调整你的模型的一部分*，因此你不应该使用你的测试数据来评估你产生的特征的质量。你可以自由使用交叉验证！'
- en: 8.9 The other sides of regression
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.9 回归的其他方面
- en: So far in this textbook we have used regression only in the context of prediction.
    However, regression can also be seen as a method to understand and quantify the
    effects of individual predictor variables on a response variable of interest.
    In the housing example from this chapter, beyond just using past data to predict
    future sale prices, we might also be interested in describing the individual relationships
    of house size and the number of bedrooms with house price, quantifying how strong
    each of these relationships are, and assessing how accurately we can estimate
    their magnitudes. And even beyond that, we may be interested in understanding
    whether the predictors *cause* changes in the price. These sides of regression
    are well beyond the scope of this book; but the material you have learned here
    should give you a foundation of knowledge that will serve you well when moving
    to more advanced books on the topic.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这本教科书中，我们只将回归用于预测的上下文。然而，回归也可以被视为一种理解和量化单个预测变量对感兴趣响应变量的影响的方法。在本章的房屋示例中，除了使用过去的数据来预测未来的销售价格之外，我们还可能对描述房屋大小和卧室数量与房价之间的个体关系感兴趣，量化这些关系的强度，并评估我们估计其大小准确性的程度。甚至更进一步，我们可能对了解预测变量是否*导致*价格变化感兴趣。回归的这些方面超出了本书的范围；但你在本处学到的材料应该为你提供一个知识基础，这将有助于你在转向更高级的书籍时受益。
- en: 8.10 Exercises
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.10 练习
- en: 'Practice exercises for the material covered in this chapter can be found in
    the accompanying [worksheets repository](https://worksheets.datasciencebook.ca)
    in the “Regression II: linear regression” row. You can launch an interactive version
    of the worksheet in your browser by clicking the “launch binder” button. You can
    also preview a non-interactive version of the worksheet by clicking “view worksheet.”
    If you instead decide to download the worksheet and run it on your own machine,
    make sure to follow the instructions for computer setup found in Chapter [13](setup.html#setup).
    This will ensure that the automated feedback and guidance that the worksheets
    provide will function as intended.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的练习材料可以在配套的 [worksheets 仓库](https://worksheets.datasciencebook.ca) 中找到，位于“回归
    II：线性回归”行。你可以通过点击“启动 binder”按钮在你的浏览器中启动工作表的交互式版本。你也可以通过点击“查看工作表”预览非交互式版本的工作表。如果你决定下载工作表并在自己的机器上运行它，请确保遵循第
    [13](setup.html#setup) 章中找到的计算机设置说明。这将确保工作表提供的自动反馈和指导按预期工作。
- en: 8.11 Additional resources
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.11 其他资源
- en: The [`tidymodels` website](https://tidymodels.org/packages) is an excellent
    reference for more details on, and advanced usage of, the functions and packages
    in the past two chapters. Aside from that, it also has a [nice beginner’s tutorial](https://www.tidymodels.org/start/)
    and [an extensive list of more advanced examples](https://www.tidymodels.org/learn/)
    that you can use to continue learning beyond the scope of this book.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[`tidymodels`网站](https://tidymodels.org/packages)是了解前两章中函数和包的更多细节以及高级使用的优秀参考。除此之外，它还提供了一个[优秀的入门教程](https://www.tidymodels.org/start/)和[一系列更高级的示例](https://www.tidymodels.org/learn/)，你可以使用这些示例在本书范围之外继续学习。'
- en: '*Modern Dive* ([Ismay and Kim 2020](#ref-moderndive)) is another textbook that
    uses the `tidyverse` / `tidymodels` framework. Chapter 6 complements the material
    in the current chapter well; it covers some slightly more advanced concepts than
    we do without getting mathematical. Give this chapter a read before moving on
    to the next reference. It is also worth noting that this book takes a more “explanatory”
    / “inferential” approach to regression in general (in Chapters 5, 6, and 10),
    which provides a nice complement to the predictive tack we take in the present
    book.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*现代探索*》([Ismay and Kim 2020](#ref-moderndive))是另一本使用`tidyverse` / `tidymodels`框架的教科书。第六章很好地补充了本章的内容；它涵盖了一些比我们更不涉及数学的更高级概念。在继续阅读下一参考之前，请先阅读这一章。还值得注意的是，这本书在回归的一般方法上采取了更多的“解释性”/“推断性”方法（在第5、6和10章），这为我们在本书中采取的预测方法提供了一个很好的补充。
- en: '*An Introduction to Statistical Learning* ([James et al. 2013](#ref-james2013introduction))
    provides a great next stop in the process of learning about regression. Chapter
    3 covers linear regression at a slightly more mathematical level than we do here,
    but it is not too large a leap and so should provide a good stepping stone. Chapter
    6 discusses how to pick a subset of “informative” predictors when you have a data
    set with many predictors, and you expect only a few of them to be relevant. Chapter
    7 covers regression models that are more flexible than linear regression models
    but still enjoy the computational efficiency of linear regression. In contrast,
    the K-NN methods we covered earlier are indeed more flexible but become very slow
    when given lots of data.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 《*统计学习导论*》([James et al. 2013](#ref-james2013introduction))是学习回归过程中的一个很好的下一站。第三章在比我们这里所讲稍微数学化的水平上介绍了线性回归，但并不是太大的跳跃，因此应该提供一个良好的过渡。第六章讨论了当你有一个具有许多预测因子的数据集时，如何选择“信息性”预测因子子集，并且你预计其中只有少数是相关的。第七章涵盖了比线性回归模型更灵活的回归模型，但仍然保持了线性回归的计算效率。相比之下，我们之前提到的K-NN方法确实更加灵活，但在给定大量数据时变得非常慢。
- en: References
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ismay, Chester, and Albert Kim. 2020\. *Statistical Inference via Data Science:
    A ModernDive into R and the Tidyverse*. Chapman; Hall/CRC Press. [https://moderndive.com/](https://moderndive.com/).James,
    Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013\. *An Introduction
    to Statistical Learning*. 1st ed. Springer. [https://www.statlearning.com/](https://www.statlearning.com/).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Ismay, Chester, 和 Albert Kim. 2020. *通过数据科学进行统计推断：R和Tidyverse的现代探索*. Chapman;
    Hall/CRC Press. [https://moderndive.com/](https://moderndive.com/).James, Gareth,
    Daniela Witten, Trevor Hastie, 和 Robert Tibshirani. 2013. *统计学习导论*. 第1版. Springer.
    [https://www.statlearning.com/](https://www.statlearning.com/).
