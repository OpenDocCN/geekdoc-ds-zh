<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>GPU programming concepts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>GPU programming concepts</h1>
<blockquote>原文：<a href="https://enccs.github.io/gpu-programming/4-gpu-concepts/">https://enccs.github.io/gpu-programming/4-gpu-concepts/</a></blockquote><nav class="wy-nav-top" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"/>
          <a href="../">GPU programming: why, when and how?</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../" class="icon icon-home" aria-label="Home"/></li>
      <li class="breadcrumb-item active">GPU programming concepts</li>
      <li class="wy-breadcrumbs-aside">
              <a href="https://github.com/ENCCS/gpu-programming/blob/main/content/4-gpu-concepts.rst" class="fa fa-github"> Edit on GitHub</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="gpu-programming-concepts">
<span id="gpu-concepts"/>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>What types of parallel computing is possible?</p></li>
<li><p>How does data parallelism differ from task parallelism, and how are they utilized in parallel computing?</p></li>
<li><p>How is the work parallelized and executed on GPUs?</p></li>
<li><p>What are general considerations for an efficient code running on GPUs?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Understand parallel computing principles and architectures.</p></li>
<li><p>Differentiate data parallelism from task parallelism.</p></li>
<li><p>Learn the GPU execution model.</p></li>
<li><p>Parallelize and execute work on GPUs.</p></li>
<li><p>Develop efficient GPU code for high performance.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>25 min teaching</p></li>
<li><p>0 min exercises</p></li>
</ul>
</div>
<section id="different-types-of-parallelism">
<h2>Different types of parallelism</h2>
<section id="distributed-vs-shared-memory-architecture">
<h3>Distributed- vs. Shared-Memory Architecture</h3>
<p>Most of computing problems are not trivially parallelizable, which means that the subtasks
need to have access from time to time to some of the results computed by other subtasks.
The way subtasks exchange needed information depends on the available hardware.</p>
<figure class="align-center" id="id4">
<img alt="../_images/distributed_vs_shared.png" src="../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png" data-original-src="https://enccs.github.io/gpu-programming/_images/distributed_vs_shared.png"/>
<figcaption>
<p><span class="caption-text">Distributed- vs shared-memory parallel computing.</span></p>
</figcaption>
</figure>
<p>In a distributed memory environment each processing unit operates independently from the
others. It has its own memory and it  <strong>cannot</strong> access the memory in other nodes.
The communication is done via network and each computing unit runs a separate copy of the
operating system. In a shared memory machine all processing units have access to the memory
and can read or modify the variables within.</p>
</section>
<section id="processes-and-threads">
<h3>Processes and Threads</h3>
<p>The type of environment (distributed- or shared-memory) determines the programming model.
There are two types of parallelism possible, process based and thread based.</p>
<figure class="align-center">
<img alt="../_images/processes-threads.png" src="../Images/94b62a3205925cbbdac5058209021754.png" data-original-src="https://enccs.github.io/gpu-programming/_images/processes-threads.png"/>
</figure>
<p>For distributed memory machines, a process-based parallel programming model is employed.
The processes are independent execution units which have their <em>own memory</em> address spaces.
They are created when the parallel program is started and they are only terminated at the
end. The communication between them is done explicitly via message passing like MPI.</p>
<p>On the shared memory architectures it is possible to use a thread based parallelism.
The threads are light execution units and can be created and destroyed at a relatively
small cost. The threads have their own state information, but they <em>share</em> the <em>same memory</em>
address space. When needed the communication is done though the shared memory.</p>
<p>Both approaches have their advantages and disadvantages.  Distributed machines are
relatively cheap to build and they  have an “infinite “ capacity. In principle one could
add more and more computing units. In practice the more computing units are used the more
time consuming is the communication. The shared memory systems can achieve good performance
and the programming model is quite simple. However they are limited by the memory capacity
and by the access speed. In addition in the shared parallel model it is much easier to
create race conditions.</p>
</section>
</section>
<section id="exposing-parallelism">
<h2>Exposing parallelism</h2>
<p>There are two types of parallelism that can be explored.
The data parallelism is when the data can be distributed across computational units that can run in parallel.
The units process the data by applying the same or very similar operation to different data elements.
A common example is applying a blur filter to an image — the same function is applied to all the pixels on an image.
This parallelism is natural for the GPU, where the same instruction set is executed in multiple <abbr title="In OpenCL and SYCL: work-item.">threads</abbr>.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png"><img alt="../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png" src="../Images/df3318bcb375513cb4f8169dcb4efdbc.png" style="width: 715.2px; height: 265.6px;" data-original-src="https://enccs.github.io/gpu-programming/_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png"/>
</a>
<figcaption>
<p><span class="caption-text">Data parallelism and task parallelism.
The data parallelism is when the same operation applies to multiple data (e.g. multiple elements of an array are transformed).
The task parallelism implies that there are more than one independent task that, in principle, can be executed in parallel.</span></p>
</figcaption>
</figure>
<p>Data parallelism can usually be explored by the GPUs quite easily.
The most basic approach would be finding a loop over many data elements and converting it into a GPU kernel.
If the number of elements in the data set is fairly large (tens or hundred of thousands elements), the GPU should perform quite well. Although it would be odd to expect absolute maximum performance from such a naive approach, it is often the one to take. Getting absolute maximum out of the data parallelism requires good understanding of how GPU works.</p>
<p>Another type of parallelism is a task parallelism.
This is when an application consists of more than one task that requiring to perform different operations with (the same or) different data.
An example of task parallelism is cooking: slicing vegetables and grilling are very different tasks and can be done at the same time.
Note that the tasks can consume totally different resources, which also can be explored.</p>
<div class="dropdown admonition">
<p class="admonition-title">In short</p>
<ul class="simple">
<li><p>Computing problems can be parallelized in distributed memory or shared memory architectures.</p></li>
<li><p>In distributed memory, each unit operates independently, with no direct memory access between nodes.</p></li>
<li><p>In shared memory, units have access to the same memory and can communicate through shared variables.</p></li>
<li><p>Parallel programming can be process-based (distributed memory) or thread-based (shared memory).</p></li>
<li><p>Process-based parallelism uses independent processes with separate memory spaces and explicit message passing.</p></li>
<li><p>Thread-based parallelism uses lightweight threads that share the same memory space and communicate through shared memory.</p></li>
<li><p>Data parallelism distributes data across computational units, processing them with the same or similar operations.</p></li>
<li><p>Task parallelism involves multiple independent tasks that perform different operations on the same or different data.</p></li>
<li><p>Task parallelism involves executing different tasks concurrently, leveraging different resources.</p></li>
</ul>
</div>
</section>
<section id="gpu-execution-model">
<h2>GPU Execution Model</h2>
<p>In order to obtain maximum performance it is important to understand how GPUs execute the programs. As mentioned before a CPU is a flexible device oriented towards general purpose usage. It’s fast and versatile, designed to run operating systems and various, very different types of applications. It has lots of features, such as better control logic, caches and cache coherence, that are not related to pure computing. CPUs optimize the execution by trying to achieve low latency via heavy caching and branch prediction.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="../_images/cpu-gpu-highway.png"><img alt="../_images/cpu-gpu-highway.png" src="../Images/2be579a867209f9ee644b6264269be37.png" style="width: 768.0px; height: 432.0px;" data-original-src="https://enccs.github.io/gpu-programming/_images/cpu-gpu-highway.png"/>
</a>
<figcaption>
<p><span class="caption-text">Cars and roads analogy for the CPU and GPU behavior. The compact road is analogous to the CPU
(low latency, low throughput) and the broader road is analogous to the GPU (high latency, high throughput).</span></p>
</figcaption>
</figure>
<p>In contrast the GPUs contain a relatively small amount of transistors dedicated to control and caching, and a much larger fraction of transistors dedicated to the mathematical operations. Since the cores in a GPU are designed just for 3D graphics, they can be made much simpler and there can be a very larger number of cores. The current GPUs contain thousands of CUDA cores. Performance in GPUs is obtain by having a very high degree of parallelism. Lots of threads are launched in parallel. For good performance there should be at least several times more than the number of CUDA cores. GPU <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> are much lighter than the usual CPU threads and they have very little penalty for context switching. This way when some threads are performing some memory operations (reading or writing) others execute instructions.</p>
</section>
<section id="cuda-threads-warps-blocks">
<h2>CUDA Threads, Warps, Blocks</h2>
<p>In order to understand the GPU execution model let’s look at the so called <cite>axpy</cite> operation. On a single CPU core this operation would be executed in a serial manner in a <cite>for/do</cite> loop going over each element on the array, <cite>id</cite>, and computing <cite>y[id]=y[id]+a*x[id]</cite>.</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span/><span class="kt">void</span><span class="w"> </span><span class="nf">axpy_</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">y</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">id</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">id</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">id</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="n">y</span><span class="p">[</span><span class="n">id</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">id</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In order to perform the some operation on a GPU the program launches a function called <em>kernel</em>, which is executed simultaneously by tens of thousands of <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> that can be run on GPU cores parallelly.</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span/><span class="n">GPU_K</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">ker_axpy_</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">id</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="n">y</span><span class="p">[</span><span class="n">id</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">id</span><span class="p">];</span><span class="w"> </span><span class="c1">// id&lt;n</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The programmers control how many instances of <cite>ker_axpy_</cite> are created and they have to make sure that all the elements are processed and also that no out of bounds accessed are happening.</p>
<p>GPU threads are much lighter than the usual CPU threads and they have very little penalty for context switching. By “over-subscribing” the GPU there are threads that are performing some memory operations (reading or writing), while others execute instructions.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/THREAD_CORE.png"><img alt="../_images/THREAD_CORE.png" src="../Images/5d5381fb2a0059b880639b60b35a9e5e.png" style="width: 638.8000000000001px; height: 265.6px;" data-original-src="https://enccs.github.io/gpu-programming/_images/THREAD_CORE.png"/>
</a>
</figure>
<p>Every <abbr title="In OpenCL and SYCL: work-item.">thread</abbr> is associated with a particular intrinsic index which can be used to calculate and access  memory locations in an array. Each thread has its context and set of private variables. All threads have access to the global GPU memory, but there is no general way to synchronize when executing a kernel. If some threads need data from the global memory which was modified by other threads the code would have to be split in several kernels because only at the completion of a kernel it is ensured that the writing to the global memory was completed.</p>
<p>Apart from being much light weighted there are more differences between GPU threads and CPU threads. GPU <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> are grouped together in groups called <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warps</abbr>. This done at hardware level.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/WARP_SMTU.png"><img alt="../_images/WARP_SMTU.png" src="../Images/3bce412cee380c39c9c2d4125dbd029f.png" style="width: 548.8000000000001px; height: 435.6px;" data-original-src="https://enccs.github.io/gpu-programming/_images/WARP_SMTU.png"/>
</a>
</figure>
<p>All memory accesses to the GPU memory are as a group in blocks of specific sizes (32B, 64B, 128B etc.). To obtain good performance the CUDA threads in the same warp need to access elements of the data which are adjacent in the memory. This is called <em>coalesced</em> memory access.</p>
<p>On some architectures, all members of a <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr> have to execute the
same instruction, the so-called “lock-step” execution. This is done to achieve
higher performance, but there are some drawbacks. If an <strong>if</strong> statement
is present inside a <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr> will cause the warp to be executed more than once,
one time for each branch. When different threads within a single <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr>
take different execution paths based on a conditional statement (if), both
branches are executed sequentially, with some threads being active while
others are inactive. On architectures without lock-step execution, such
as NVIDIA Volta / Turing (e.g., GeForce 16xx-series) or newer, <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr>
divergence is less costly.</p>
<p>There is another level in the GPU <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> hierarchy. The <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> are grouped together in so called <abbr title="In OpenCL and SYCL: work-group.">blocks</abbr>. Each block is assigned to one Streaming Multiprocessor (SMP) unit. A SMP contains one or more SIMT (single instruction multiple threads) units, schedulers, and very fast on-chip memory. Some of this on-chip memory can be used in the programs, this is called <abbr title="In OpenCL and SYCL: local memory (not to be confused with CUDA and HIP local memory).">shared memory</abbr>. The shared memory can be used to “cache” data that is used by more than one thread, thus avoiding multiple reads from the global memory. It can also be used to avoid memory accesses which are not efficient. For example in a matrix transpose operation, we have two memory operations per element and only can be coalesced. In the first step a tile of the matrix is saved read a coalesced manner in the shared memory. After all the reads of the block are done the tile can be locally transposed (which is very fast) and then written to the destination matrix in a coalesced manner as well. Shared memory can also be used to perform block-level reductions and similar collective operations. All threads can be synchronized at block level. Furthermore when the shared memory is written in order to ensure that all threads have completed the operation the synchronization is compulsory to ensure correctness of the program.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/BLOCK_SMP.png"><img alt="../_images/BLOCK_SMP.png" src="../Images/ef0f46b8c50be0dbedbce258b377a92e.png" style="width: 550.8000000000001px; height: 430.0px;" data-original-src="https://enccs.github.io/gpu-programming/_images/BLOCK_SMP.png"/>
</a>
</figure>
<p>Finally, a block of threads can not be split among SMPs. For performance blocks should have more than one <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr>. The more warps are active on an SMP the better is hidden the latency associated with the memory operations. If the resources are sufficient, due to fast context switching, an SMP can have more than one block active in the same time. However these blocks can not share data with each other via the on-chip memory.</p>
<p>To summarize this section. In order to take advantage of GPUs the algorithms must allow the division of work in many small subtasks which can be executed in the same time. The computations are offloaded to GPUs, by launching tens of thousands of threads all executing the same function, <em>kernel</em>, each thread working on different part of the problem. The threads are executed in groups called <em>blocks</em>, each block being assigned to a SMP. Furthermore the threads of a block are divided in <em>warps</em>, each executed by SIMT unit. All threads in a warp execute the same instructions and all memory accesses are done collectively at warp level. The threads can synchronize and share data only at block level. Depending on the architecture, some data sharing can be done as well at warp level.</p>
<p>In order to hide latencies it is recommended to “over-subscribe” the GPU. There should be many more blocks than SMPs present on the device. Also in order to ensure a good occupancy of the CUDA cores there should be more warps active on a given SMP than SIMT units. This way while some warps of threads are idle waiting for some memory operations to complete, others use the CUDA cores, thus ensuring a high occupancy of the GPU.</p>
<p>In addition to this there are some architecture-specific features of which the developers can take advantage. <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">Warp</abbr>-level operations are primitives provided by the GPU architecture to allow for efficient communication and synchronization within a warp. They allow <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> within a warp to exchange data efficiently, without the need for explicit synchronization. These warp-level operations, combined with the organization of threads into blocks and clusters, make it possible to implement complex algorithms and achieve high performance on the GPU. The cooperative groups feature introduced in recent versions of CUDA provides even finer-grained control over thread execution, allowing for even more efficient processing by giving more flexibility to the thread hierarchy. Cooperative groups allow threads within a block to organize themselves into smaller groups, called cooperative groups, and to synchronize their execution and share data within the group.</p>
<p>Below there is an example of how the threads in a grid can be associated with specific elements of an array</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/Indexing.png"><img alt="../_images/Indexing.png" src="../Images/fccb3e8f6b92e4b53a0805282127878f.png" style="width: 546.4px; height: 247.20000000000002px;" data-original-src="https://enccs.github.io/gpu-programming/_images/Indexing.png"/>
</a>
</figure>
<p>The thread marked by orange color is part of a grid of threads size 4096. The threads are grouped in blocks of size 256. The “orange” thread has index 3 in the block 2 and the global calculated index 515.</p>
<p>For a vector addition example this would be used as follow <code class="docutils literal notranslate"><span class="pre">c[index]=a[index]+b[index]</span></code>.</p>
<div class="dropdown admonition">
<p class="admonition-title">In short</p>
<ul class="simple">
<li><p>GPUs have a different execution model compared to CPUs, with a focus on parallelism and mathematical operations.</p></li>
<li><p>GPUs consist of thousands of lightweight threads that can be executed simultaneously on GPU cores.</p></li>
<li><p>Threads are organized into warps, and warps are grouped into blocks assigned to streaming multiprocessors (SMPs).</p></li>
<li><p>GPUs achieve performance through high degrees of parallelism and efficient memory access.</p></li>
<li><p>Shared memory can be used to cache data and improve memory access efficiency within a block.</p></li>
<li><p>Synchronization and data sharing are limited to the block level, with some possible sharing at the warp level depending on the architecture.</p></li>
<li><p>Over-subscribing the GPU and maximizing warp and block occupancy help hide latencies and improve performance.</p></li>
<li><p>Warp-level operations and cooperative groups provide efficient communication and synchronization within a warp or block.</p></li>
<li><p>Thread indexing allows associating threads with specific elements in an array for parallel processing.</p></li>
</ul>
</div>
</section>
<section id="terminology">
<h2>Terminology</h2>
<p>At the moment there are three major GPU producers: NVIDIA, Intel, and AMD. While the basic concept behind GPUs is pretty similar they use different names for the various parts. Furthermore there are software environments for GPU programming, some from the producers and some from external groups all having different naming as well. Below there is a short compilation of the some terms used across different platforms and software environments.</p>
<table class="docutils align-center" id="id7">
<caption><span class="caption-text">Software mapping naming</span></caption>
<thead>
<tr class="row-odd"><th class="head"><p>CUDA</p></th>
<th class="head"><p>HIP</p></th>
<th class="head"><p>OpenCL</p></th>
<th class="head"><p>SYCL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td colspan="2"><p>grid of threads</p></td>
<td colspan="2"><p>NDRange</p></td>
</tr>
<tr class="row-odd"><td colspan="2"><p>block</p></td>
<td colspan="2"><p>work-group</p></td>
</tr>
<tr class="row-even"><td><p>warp</p></td>
<td><p>wavefront</p></td>
<td colspan="2"><p>sub-group</p></td>
</tr>
<tr class="row-odd"><td colspan="2"><p>thread</p></td>
<td colspan="2"><p>work-item</p></td>
</tr>
<tr class="row-even"><td colspan="2"><p>registers</p></td>
<td colspan="2"><p>private memory</p></td>
</tr>
<tr class="row-odd"><td><p>shared memory</p></td>
<td><p>local data share</p></td>
<td colspan="2"><p>local memory</p></td>
</tr>
<tr class="row-even"><td colspan="2"><p>threadIdx.{x,y,z}</p></td>
<td><p>get_local_id({0,1,2})</p></td>
<td><p>nd_item::get_local({2,1,0}) <a class="footnote-reference brackets" href="#syclindex" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-odd"><td colspan="2"><p>blockIdx.{x,y,z}</p></td>
<td><p>get_group_id({0,1,2})</p></td>
<td><p>nd_item::get_group({2,1,0}) <a class="footnote-reference brackets" href="#syclindex" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><td colspan="2"><p>blockDim.{x,y,z}</p></td>
<td><p>get_local_size({0,1,2})</p></td>
<td><p>nd_item::get_local_range({2,1,0}) <a class="footnote-reference brackets" href="#syclindex" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
</tr>
</tbody>
</table>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="syclindex" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id3">3</a>)</span>
<p>In SYCL, the thread indexing is inverted. In a 3D grid, physically adjacent threads have consecutive X (0) index in CUDA, HIP, and OpenCL, but consecutive Z (2) index in SYCL.
In a 2D grid, CUDA, HIP, and OpenCL still has contiguous indexing along X (0) dimension, while in SYCL it is Y (1).
Same applies to block dimensions and indexing.</p>
</aside>
</aside>
</section>
<section id="exercises">
<h2>Exercises</h2>
<div class="admonition-what-are-threads-in-the-context-of-shared-memory-architectures exercise important admonition" id="exercise-0">
<p class="admonition-title">What are threads in the context of shared memory architectures?</p>
<ol class="loweralpha simple">
<li><p>Independent execution units with their own memory address spaces</p></li>
<li><p>Light execution units with shared memory address spaces</p></li>
<li><p>Communication devices between separate memory units</p></li>
<li><p>Programming models for distributed memory machines</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>Correct answer:  <em>b) Light execution units with shared memory address spaces</em></p>
</div>
</div>
<div class="admonition-what-is-data-parallelism exercise important admonition" id="exercise-1">
<p class="admonition-title">What is data parallelism?</p>
<ol class="loweralpha simple">
<li><p>Distributing data across computational units that run in parallel, applying the same or similar operations to different data elements.</p></li>
<li><p>Distributing tasks across computational units that run in parallel, applying different operations to the same data elements.</p></li>
<li><p>Distributing data across computational units that run sequentially, applying the same operation to all data elements.</p></li>
<li><p>Distributing tasks across computational units that run sequentially, applying different operations to different data elements.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>a) Distributing data across computational units that run in parallel, applying the same or similar operations to different data elements.</em></p>
</div>
</div>
<div class="admonition-what-type-of-parallelism-is-natural-for-gpu exercise important admonition" id="exercise-2">
<p class="admonition-title">What type of parallelism is natural for GPU?</p>
<ol class="loweralpha simple">
<li><p>Task Parallelism</p></li>
<li><p>Data Parallelism</p></li>
<li><p>Both data and task parallelism</p></li>
<li><p>Neither data nor task parallelism</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>b) Data Parallelism</em></p>
</div>
</div>
<div class="admonition-what-is-a-kernel-in-the-context-of-gpu-execution exercise important admonition" id="exercise-3">
<p class="admonition-title">What is a kernel in the context of GPU execution?</p>
<ol class="loweralpha simple">
<li><p>A specific section of the CPU used for memory operations.</p></li>
<li><p>A specific section of the GPU used for memory operations.</p></li>
<li><p>A type of thread that operates on the GPU.</p></li>
<li><p>A function that is executed simultaneously by tens of thousands of threads on GPU cores.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-3">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>d) A function that is executed simultaneously by tens of thousands of threads on GPU cores.</em></p>
</div>
</div>
<div class="admonition-what-is-coalesced-memory-access exercise important admonition" id="exercise-4">
<p class="admonition-title">What is coalesced memory access?</p>
<ol class="loweralpha simple">
<li><p>It’s when CUDA threads in the same warp access elements of the data which are adjacent in the memory.</p></li>
<li><p>It’s when CUDA threads in different warps access elements of the data which are far in the memory.</p></li>
<li><p>It’s when all threads have access to the global GPU memory.</p></li>
<li><p>It’s when threads in a warp perform different operations.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-4">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>a) It’s when CUDA threads in the same warp access elements of the data which are adjacent in the memory.</em></p>
</div>
</div>
<div class="admonition-what-is-the-function-of-shared-memory-in-the-context-of-gpu-execution exercise important admonition" id="exercise-5">
<p class="admonition-title">What is the function of shared memory in the context of GPU execution?</p>
<ol class="loweralpha simple">
<li><p>It’s used to store global memory.</p></li>
<li><p>It’s used to store all the threads in a block.</p></li>
<li><p>It can be used to “cache” data that is used by more than one thread, avoiding multiple reads from the global memory.</p></li>
<li><p>It’s used to store all the CUDA cores.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-5">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>c) It can be used to “cache” data that is used by more than one thread, avoiding multiple reads from the global memory.</em></p>
</div>
</div>
<div class="admonition-what-is-the-significance-of-over-subscribing-the-gpu exercise important admonition" id="exercise-6">
<p class="admonition-title">What is the significance of over-subscribing the GPU?</p>
<ol class="loweralpha simple">
<li><p>It reduces the overall performance of the GPU.</p></li>
<li><p>It ensures that there are more blocks than SMPs present on the device, helping to hide latencies and ensure high occupancy of the GPU.</p></li>
<li><p>It leads to a memory overflow in the GPU.</p></li>
<li><p>It ensures that there are more SMPs than blocks present on the device.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-6">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>b) It ensures that there are more blocks than SMPs present on the device, helping to hide latencies and ensure high occupancy of the GPU.</em></p>
</div>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Parallel computing can be classified into distributed-memory and shared-memory architectures</p></li>
<li><p>Two types of parallelism that can be explored are data parallelism and task parallelism.</p></li>
<li><p>GPUs are a type of shared memory architecture suitable for data parallelism.</p></li>
<li><p>GPUs have high parallelism, with threads organized into warps and blocks and.</p></li>
<li><p>GPU optimization involves coalesced memory access, shared memory usage, and high thread and warp occupancy. Additionally, architecture-specific features such as warp-level operations and cooperative groups can be leveraged for more efficient processing.</p></li>
</ul>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../3-gpu-problems/" class="btn btn-neutral float-left" title="What problems fit to GPU?" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"/> Previous</a>
        <a href="../5-intro-to-gpu-prog-models/" class="btn btn-neutral float-right" title="Introduction to GPU programming models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"/></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>© Copyright 2023-2024, The contributors.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    &#13;

<span id="gpu-concepts"/>
<div class="admonition-questions questions admonition" id="questions-0">
<p class="admonition-title">Questions</p>
<ul class="simple">
<li><p>What types of parallel computing is possible?</p></li>
<li><p>How does data parallelism differ from task parallelism, and how are they utilized in parallel computing?</p></li>
<li><p>How is the work parallelized and executed on GPUs?</p></li>
<li><p>What are general considerations for an efficient code running on GPUs?</p></li>
</ul>
</div>
<div class="admonition-objectives objectives admonition" id="objectives-0">
<p class="admonition-title">Objectives</p>
<ul class="simple">
<li><p>Understand parallel computing principles and architectures.</p></li>
<li><p>Differentiate data parallelism from task parallelism.</p></li>
<li><p>Learn the GPU execution model.</p></li>
<li><p>Parallelize and execute work on GPUs.</p></li>
<li><p>Develop efficient GPU code for high performance.</p></li>
</ul>
</div>
<div class="admonition-instructor-note instructor-note admonition" id="instructor-note-0">
<p class="admonition-title">Instructor note</p>
<ul class="simple">
<li><p>25 min teaching</p></li>
<li><p>0 min exercises</p></li>
</ul>
</div>
<section id="different-types-of-parallelism">
<h2>Different types of parallelism</h2>
<section id="distributed-vs-shared-memory-architecture">
<h3>Distributed- vs. Shared-Memory Architecture</h3>
<p>Most of computing problems are not trivially parallelizable, which means that the subtasks
need to have access from time to time to some of the results computed by other subtasks.
The way subtasks exchange needed information depends on the available hardware.</p>
<figure class="align-center" id="id4">
<img alt="../_images/distributed_vs_shared.png" src="../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png" data-original-src="https://enccs.github.io/gpu-programming/_images/distributed_vs_shared.png"/>
<figcaption>
<p><span class="caption-text">Distributed- vs shared-memory parallel computing.</span></p>
</figcaption>
</figure>
<p>In a distributed memory environment each processing unit operates independently from the
others. It has its own memory and it  <strong>cannot</strong> access the memory in other nodes.
The communication is done via network and each computing unit runs a separate copy of the
operating system. In a shared memory machine all processing units have access to the memory
and can read or modify the variables within.</p>
</section>
<section id="processes-and-threads">
<h3>Processes and Threads</h3>
<p>The type of environment (distributed- or shared-memory) determines the programming model.
There are two types of parallelism possible, process based and thread based.</p>
<figure class="align-center">
<img alt="../_images/processes-threads.png" src="../Images/94b62a3205925cbbdac5058209021754.png" data-original-src="https://enccs.github.io/gpu-programming/_images/processes-threads.png"/>
</figure>
<p>For distributed memory machines, a process-based parallel programming model is employed.
The processes are independent execution units which have their <em>own memory</em> address spaces.
They are created when the parallel program is started and they are only terminated at the
end. The communication between them is done explicitly via message passing like MPI.</p>
<p>On the shared memory architectures it is possible to use a thread based parallelism.
The threads are light execution units and can be created and destroyed at a relatively
small cost. The threads have their own state information, but they <em>share</em> the <em>same memory</em>
address space. When needed the communication is done though the shared memory.</p>
<p>Both approaches have their advantages and disadvantages.  Distributed machines are
relatively cheap to build and they  have an “infinite “ capacity. In principle one could
add more and more computing units. In practice the more computing units are used the more
time consuming is the communication. The shared memory systems can achieve good performance
and the programming model is quite simple. However they are limited by the memory capacity
and by the access speed. In addition in the shared parallel model it is much easier to
create race conditions.</p>
</section>
</section>
<section id="exposing-parallelism">
<h2>Exposing parallelism</h2>
<p>There are two types of parallelism that can be explored.
The data parallelism is when the data can be distributed across computational units that can run in parallel.
The units process the data by applying the same or very similar operation to different data elements.
A common example is applying a blur filter to an image — the same function is applied to all the pixels on an image.
This parallelism is natural for the GPU, where the same instruction set is executed in multiple <abbr title="In OpenCL and SYCL: work-item.">threads</abbr>.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png"><img alt="../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png" src="../Images/df3318bcb375513cb4f8169dcb4efdbc.png" style="width: 715.2px; height: 265.6px;" data-original-src="https://enccs.github.io/gpu-programming/_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png"/>
</a>
<figcaption>
<p><span class="caption-text">Data parallelism and task parallelism.
The data parallelism is when the same operation applies to multiple data (e.g. multiple elements of an array are transformed).
The task parallelism implies that there are more than one independent task that, in principle, can be executed in parallel.</span></p>
</figcaption>
</figure>
<p>Data parallelism can usually be explored by the GPUs quite easily.
The most basic approach would be finding a loop over many data elements and converting it into a GPU kernel.
If the number of elements in the data set is fairly large (tens or hundred of thousands elements), the GPU should perform quite well. Although it would be odd to expect absolute maximum performance from such a naive approach, it is often the one to take. Getting absolute maximum out of the data parallelism requires good understanding of how GPU works.</p>
<p>Another type of parallelism is a task parallelism.
This is when an application consists of more than one task that requiring to perform different operations with (the same or) different data.
An example of task parallelism is cooking: slicing vegetables and grilling are very different tasks and can be done at the same time.
Note that the tasks can consume totally different resources, which also can be explored.</p>
<div class="dropdown admonition">
<p class="admonition-title">In short</p>
<ul class="simple">
<li><p>Computing problems can be parallelized in distributed memory or shared memory architectures.</p></li>
<li><p>In distributed memory, each unit operates independently, with no direct memory access between nodes.</p></li>
<li><p>In shared memory, units have access to the same memory and can communicate through shared variables.</p></li>
<li><p>Parallel programming can be process-based (distributed memory) or thread-based (shared memory).</p></li>
<li><p>Process-based parallelism uses independent processes with separate memory spaces and explicit message passing.</p></li>
<li><p>Thread-based parallelism uses lightweight threads that share the same memory space and communicate through shared memory.</p></li>
<li><p>Data parallelism distributes data across computational units, processing them with the same or similar operations.</p></li>
<li><p>Task parallelism involves multiple independent tasks that perform different operations on the same or different data.</p></li>
<li><p>Task parallelism involves executing different tasks concurrently, leveraging different resources.</p></li>
</ul>
</div>
</section>
<section id="gpu-execution-model">
<h2>GPU Execution Model</h2>
<p>In order to obtain maximum performance it is important to understand how GPUs execute the programs. As mentioned before a CPU is a flexible device oriented towards general purpose usage. It’s fast and versatile, designed to run operating systems and various, very different types of applications. It has lots of features, such as better control logic, caches and cache coherence, that are not related to pure computing. CPUs optimize the execution by trying to achieve low latency via heavy caching and branch prediction.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="../_images/cpu-gpu-highway.png"><img alt="../_images/cpu-gpu-highway.png" src="../Images/2be579a867209f9ee644b6264269be37.png" style="width: 768.0px; height: 432.0px;" data-original-src="https://enccs.github.io/gpu-programming/_images/cpu-gpu-highway.png"/>
</a>
<figcaption>
<p><span class="caption-text">Cars and roads analogy for the CPU and GPU behavior. The compact road is analogous to the CPU
(low latency, low throughput) and the broader road is analogous to the GPU (high latency, high throughput).</span></p>
</figcaption>
</figure>
<p>In contrast the GPUs contain a relatively small amount of transistors dedicated to control and caching, and a much larger fraction of transistors dedicated to the mathematical operations. Since the cores in a GPU are designed just for 3D graphics, they can be made much simpler and there can be a very larger number of cores. The current GPUs contain thousands of CUDA cores. Performance in GPUs is obtain by having a very high degree of parallelism. Lots of threads are launched in parallel. For good performance there should be at least several times more than the number of CUDA cores. GPU <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> are much lighter than the usual CPU threads and they have very little penalty for context switching. This way when some threads are performing some memory operations (reading or writing) others execute instructions.</p>
</section>
<section id="cuda-threads-warps-blocks">
<h2>CUDA Threads, Warps, Blocks</h2>
<p>In order to understand the GPU execution model let’s look at the so called <cite>axpy</cite> operation. On a single CPU core this operation would be executed in a serial manner in a <cite>for/do</cite> loop going over each element on the array, <cite>id</cite>, and computing <cite>y[id]=y[id]+a*x[id]</cite>.</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span/><span class="kt">void</span><span class="w"> </span><span class="nf">axpy_</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">y</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">id</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">id</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">id</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="n">y</span><span class="p">[</span><span class="n">id</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">id</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In order to perform the some operation on a GPU the program launches a function called <em>kernel</em>, which is executed simultaneously by tens of thousands of <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> that can be run on GPU cores parallelly.</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span/><span class="n">GPU_K</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">ker_axpy_</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">id</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="n">y</span><span class="p">[</span><span class="n">id</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">id</span><span class="p">];</span><span class="w"> </span><span class="c1">// id&lt;n</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The programmers control how many instances of <cite>ker_axpy_</cite> are created and they have to make sure that all the elements are processed and also that no out of bounds accessed are happening.</p>
<p>GPU threads are much lighter than the usual CPU threads and they have very little penalty for context switching. By “over-subscribing” the GPU there are threads that are performing some memory operations (reading or writing), while others execute instructions.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/THREAD_CORE.png"><img alt="../_images/THREAD_CORE.png" src="../Images/5d5381fb2a0059b880639b60b35a9e5e.png" style="width: 638.8000000000001px; height: 265.6px;" data-original-src="https://enccs.github.io/gpu-programming/_images/THREAD_CORE.png"/>
</a>
</figure>
<p>Every <abbr title="In OpenCL and SYCL: work-item.">thread</abbr> is associated with a particular intrinsic index which can be used to calculate and access  memory locations in an array. Each thread has its context and set of private variables. All threads have access to the global GPU memory, but there is no general way to synchronize when executing a kernel. If some threads need data from the global memory which was modified by other threads the code would have to be split in several kernels because only at the completion of a kernel it is ensured that the writing to the global memory was completed.</p>
<p>Apart from being much light weighted there are more differences between GPU threads and CPU threads. GPU <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> are grouped together in groups called <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warps</abbr>. This done at hardware level.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/WARP_SMTU.png"><img alt="../_images/WARP_SMTU.png" src="../Images/3bce412cee380c39c9c2d4125dbd029f.png" style="width: 548.8000000000001px; height: 435.6px;" data-original-src="https://enccs.github.io/gpu-programming/_images/WARP_SMTU.png"/>
</a>
</figure>
<p>All memory accesses to the GPU memory are as a group in blocks of specific sizes (32B, 64B, 128B etc.). To obtain good performance the CUDA threads in the same warp need to access elements of the data which are adjacent in the memory. This is called <em>coalesced</em> memory access.</p>
<p>On some architectures, all members of a <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr> have to execute the
same instruction, the so-called “lock-step” execution. This is done to achieve
higher performance, but there are some drawbacks. If an <strong>if</strong> statement
is present inside a <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr> will cause the warp to be executed more than once,
one time for each branch. When different threads within a single <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr>
take different execution paths based on a conditional statement (if), both
branches are executed sequentially, with some threads being active while
others are inactive. On architectures without lock-step execution, such
as NVIDIA Volta / Turing (e.g., GeForce 16xx-series) or newer, <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr>
divergence is less costly.</p>
<p>There is another level in the GPU <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> hierarchy. The <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> are grouped together in so called <abbr title="In OpenCL and SYCL: work-group.">blocks</abbr>. Each block is assigned to one Streaming Multiprocessor (SMP) unit. A SMP contains one or more SIMT (single instruction multiple threads) units, schedulers, and very fast on-chip memory. Some of this on-chip memory can be used in the programs, this is called <abbr title="In OpenCL and SYCL: local memory (not to be confused with CUDA and HIP local memory).">shared memory</abbr>. The shared memory can be used to “cache” data that is used by more than one thread, thus avoiding multiple reads from the global memory. It can also be used to avoid memory accesses which are not efficient. For example in a matrix transpose operation, we have two memory operations per element and only can be coalesced. In the first step a tile of the matrix is saved read a coalesced manner in the shared memory. After all the reads of the block are done the tile can be locally transposed (which is very fast) and then written to the destination matrix in a coalesced manner as well. Shared memory can also be used to perform block-level reductions and similar collective operations. All threads can be synchronized at block level. Furthermore when the shared memory is written in order to ensure that all threads have completed the operation the synchronization is compulsory to ensure correctness of the program.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/BLOCK_SMP.png"><img alt="../_images/BLOCK_SMP.png" src="../Images/ef0f46b8c50be0dbedbce258b377a92e.png" style="width: 550.8000000000001px; height: 430.0px;" data-original-src="https://enccs.github.io/gpu-programming/_images/BLOCK_SMP.png"/>
</a>
</figure>
<p>Finally, a block of threads can not be split among SMPs. For performance blocks should have more than one <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr>. The more warps are active on an SMP the better is hidden the latency associated with the memory operations. If the resources are sufficient, due to fast context switching, an SMP can have more than one block active in the same time. However these blocks can not share data with each other via the on-chip memory.</p>
<p>To summarize this section. In order to take advantage of GPUs the algorithms must allow the division of work in many small subtasks which can be executed in the same time. The computations are offloaded to GPUs, by launching tens of thousands of threads all executing the same function, <em>kernel</em>, each thread working on different part of the problem. The threads are executed in groups called <em>blocks</em>, each block being assigned to a SMP. Furthermore the threads of a block are divided in <em>warps</em>, each executed by SIMT unit. All threads in a warp execute the same instructions and all memory accesses are done collectively at warp level. The threads can synchronize and share data only at block level. Depending on the architecture, some data sharing can be done as well at warp level.</p>
<p>In order to hide latencies it is recommended to “over-subscribe” the GPU. There should be many more blocks than SMPs present on the device. Also in order to ensure a good occupancy of the CUDA cores there should be more warps active on a given SMP than SIMT units. This way while some warps of threads are idle waiting for some memory operations to complete, others use the CUDA cores, thus ensuring a high occupancy of the GPU.</p>
<p>In addition to this there are some architecture-specific features of which the developers can take advantage. <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">Warp</abbr>-level operations are primitives provided by the GPU architecture to allow for efficient communication and synchronization within a warp. They allow <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> within a warp to exchange data efficiently, without the need for explicit synchronization. These warp-level operations, combined with the organization of threads into blocks and clusters, make it possible to implement complex algorithms and achieve high performance on the GPU. The cooperative groups feature introduced in recent versions of CUDA provides even finer-grained control over thread execution, allowing for even more efficient processing by giving more flexibility to the thread hierarchy. Cooperative groups allow threads within a block to organize themselves into smaller groups, called cooperative groups, and to synchronize their execution and share data within the group.</p>
<p>Below there is an example of how the threads in a grid can be associated with specific elements of an array</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/Indexing.png"><img alt="../_images/Indexing.png" src="../Images/fccb3e8f6b92e4b53a0805282127878f.png" style="width: 546.4px; height: 247.20000000000002px;" data-original-src="https://enccs.github.io/gpu-programming/_images/Indexing.png"/>
</a>
</figure>
<p>The thread marked by orange color is part of a grid of threads size 4096. The threads are grouped in blocks of size 256. The “orange” thread has index 3 in the block 2 and the global calculated index 515.</p>
<p>For a vector addition example this would be used as follow <code class="docutils literal notranslate"><span class="pre">c[index]=a[index]+b[index]</span></code>.</p>
<div class="dropdown admonition">
<p class="admonition-title">In short</p>
<ul class="simple">
<li><p>GPUs have a different execution model compared to CPUs, with a focus on parallelism and mathematical operations.</p></li>
<li><p>GPUs consist of thousands of lightweight threads that can be executed simultaneously on GPU cores.</p></li>
<li><p>Threads are organized into warps, and warps are grouped into blocks assigned to streaming multiprocessors (SMPs).</p></li>
<li><p>GPUs achieve performance through high degrees of parallelism and efficient memory access.</p></li>
<li><p>Shared memory can be used to cache data and improve memory access efficiency within a block.</p></li>
<li><p>Synchronization and data sharing are limited to the block level, with some possible sharing at the warp level depending on the architecture.</p></li>
<li><p>Over-subscribing the GPU and maximizing warp and block occupancy help hide latencies and improve performance.</p></li>
<li><p>Warp-level operations and cooperative groups provide efficient communication and synchronization within a warp or block.</p></li>
<li><p>Thread indexing allows associating threads with specific elements in an array for parallel processing.</p></li>
</ul>
</div>
</section>
<section id="terminology">
<h2>Terminology</h2>
<p>At the moment there are three major GPU producers: NVIDIA, Intel, and AMD. While the basic concept behind GPUs is pretty similar they use different names for the various parts. Furthermore there are software environments for GPU programming, some from the producers and some from external groups all having different naming as well. Below there is a short compilation of the some terms used across different platforms and software environments.</p>
<table class="docutils align-center" id="id7">
<caption><span class="caption-text">Software mapping naming</span></caption>
<thead>
<tr class="row-odd"><th class="head"><p>CUDA</p></th>
<th class="head"><p>HIP</p></th>
<th class="head"><p>OpenCL</p></th>
<th class="head"><p>SYCL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td colspan="2"><p>grid of threads</p></td>
<td colspan="2"><p>NDRange</p></td>
</tr>
<tr class="row-odd"><td colspan="2"><p>block</p></td>
<td colspan="2"><p>work-group</p></td>
</tr>
<tr class="row-even"><td><p>warp</p></td>
<td><p>wavefront</p></td>
<td colspan="2"><p>sub-group</p></td>
</tr>
<tr class="row-odd"><td colspan="2"><p>thread</p></td>
<td colspan="2"><p>work-item</p></td>
</tr>
<tr class="row-even"><td colspan="2"><p>registers</p></td>
<td colspan="2"><p>private memory</p></td>
</tr>
<tr class="row-odd"><td><p>shared memory</p></td>
<td><p>local data share</p></td>
<td colspan="2"><p>local memory</p></td>
</tr>
<tr class="row-even"><td colspan="2"><p>threadIdx.{x,y,z}</p></td>
<td><p>get_local_id({0,1,2})</p></td>
<td><p>nd_item::get_local({2,1,0}) <a class="footnote-reference brackets" href="#syclindex" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-odd"><td colspan="2"><p>blockIdx.{x,y,z}</p></td>
<td><p>get_group_id({0,1,2})</p></td>
<td><p>nd_item::get_group({2,1,0}) <a class="footnote-reference brackets" href="#syclindex" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><td colspan="2"><p>blockDim.{x,y,z}</p></td>
<td><p>get_local_size({0,1,2})</p></td>
<td><p>nd_item::get_local_range({2,1,0}) <a class="footnote-reference brackets" href="#syclindex" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
</tr>
</tbody>
</table>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="syclindex" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id3">3</a>)</span>
<p>In SYCL, the thread indexing is inverted. In a 3D grid, physically adjacent threads have consecutive X (0) index in CUDA, HIP, and OpenCL, but consecutive Z (2) index in SYCL.
In a 2D grid, CUDA, HIP, and OpenCL still has contiguous indexing along X (0) dimension, while in SYCL it is Y (1).
Same applies to block dimensions and indexing.</p>
</aside>
</aside>
</section>
<section id="exercises">
<h2>Exercises</h2>
<div class="admonition-what-are-threads-in-the-context-of-shared-memory-architectures exercise important admonition" id="exercise-0">
<p class="admonition-title">What are threads in the context of shared memory architectures?</p>
<ol class="loweralpha simple">
<li><p>Independent execution units with their own memory address spaces</p></li>
<li><p>Light execution units with shared memory address spaces</p></li>
<li><p>Communication devices between separate memory units</p></li>
<li><p>Programming models for distributed memory machines</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>Correct answer:  <em>b) Light execution units with shared memory address spaces</em></p>
</div>
</div>
<div class="admonition-what-is-data-parallelism exercise important admonition" id="exercise-1">
<p class="admonition-title">What is data parallelism?</p>
<ol class="loweralpha simple">
<li><p>Distributing data across computational units that run in parallel, applying the same or similar operations to different data elements.</p></li>
<li><p>Distributing tasks across computational units that run in parallel, applying different operations to the same data elements.</p></li>
<li><p>Distributing data across computational units that run sequentially, applying the same operation to all data elements.</p></li>
<li><p>Distributing tasks across computational units that run sequentially, applying different operations to different data elements.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>a) Distributing data across computational units that run in parallel, applying the same or similar operations to different data elements.</em></p>
</div>
</div>
<div class="admonition-what-type-of-parallelism-is-natural-for-gpu exercise important admonition" id="exercise-2">
<p class="admonition-title">What type of parallelism is natural for GPU?</p>
<ol class="loweralpha simple">
<li><p>Task Parallelism</p></li>
<li><p>Data Parallelism</p></li>
<li><p>Both data and task parallelism</p></li>
<li><p>Neither data nor task parallelism</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>b) Data Parallelism</em></p>
</div>
</div>
<div class="admonition-what-is-a-kernel-in-the-context-of-gpu-execution exercise important admonition" id="exercise-3">
<p class="admonition-title">What is a kernel in the context of GPU execution?</p>
<ol class="loweralpha simple">
<li><p>A specific section of the CPU used for memory operations.</p></li>
<li><p>A specific section of the GPU used for memory operations.</p></li>
<li><p>A type of thread that operates on the GPU.</p></li>
<li><p>A function that is executed simultaneously by tens of thousands of threads on GPU cores.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-3">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>d) A function that is executed simultaneously by tens of thousands of threads on GPU cores.</em></p>
</div>
</div>
<div class="admonition-what-is-coalesced-memory-access exercise important admonition" id="exercise-4">
<p class="admonition-title">What is coalesced memory access?</p>
<ol class="loweralpha simple">
<li><p>It’s when CUDA threads in the same warp access elements of the data which are adjacent in the memory.</p></li>
<li><p>It’s when CUDA threads in different warps access elements of the data which are far in the memory.</p></li>
<li><p>It’s when all threads have access to the global GPU memory.</p></li>
<li><p>It’s when threads in a warp perform different operations.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-4">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>a) It’s when CUDA threads in the same warp access elements of the data which are adjacent in the memory.</em></p>
</div>
</div>
<div class="admonition-what-is-the-function-of-shared-memory-in-the-context-of-gpu-execution exercise important admonition" id="exercise-5">
<p class="admonition-title">What is the function of shared memory in the context of GPU execution?</p>
<ol class="loweralpha simple">
<li><p>It’s used to store global memory.</p></li>
<li><p>It’s used to store all the threads in a block.</p></li>
<li><p>It can be used to “cache” data that is used by more than one thread, avoiding multiple reads from the global memory.</p></li>
<li><p>It’s used to store all the CUDA cores.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-5">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>c) It can be used to “cache” data that is used by more than one thread, avoiding multiple reads from the global memory.</em></p>
</div>
</div>
<div class="admonition-what-is-the-significance-of-over-subscribing-the-gpu exercise important admonition" id="exercise-6">
<p class="admonition-title">What is the significance of over-subscribing the GPU?</p>
<ol class="loweralpha simple">
<li><p>It reduces the overall performance of the GPU.</p></li>
<li><p>It ensures that there are more blocks than SMPs present on the device, helping to hide latencies and ensure high occupancy of the GPU.</p></li>
<li><p>It leads to a memory overflow in the GPU.</p></li>
<li><p>It ensures that there are more SMPs than blocks present on the device.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-6">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>b) It ensures that there are more blocks than SMPs present on the device, helping to hide latencies and ensure high occupancy of the GPU.</em></p>
</div>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Parallel computing can be classified into distributed-memory and shared-memory architectures</p></li>
<li><p>Two types of parallelism that can be explored are data parallelism and task parallelism.</p></li>
<li><p>GPUs are a type of shared memory architecture suitable for data parallelism.</p></li>
<li><p>GPUs have high parallelism, with threads organized into warps and blocks and.</p></li>
<li><p>GPU optimization involves coalesced memory access, shared memory usage, and high thread and warp occupancy. Additionally, architecture-specific features such as warp-level operations and cooperative groups can be leveraged for more efficient processing.</p></li>
</ul>
</div>
</section>
&#13;

<h2>Different types of parallelism</h2>
<section id="distributed-vs-shared-memory-architecture">
<h3>Distributed- vs. Shared-Memory Architecture</h3>
<p>Most of computing problems are not trivially parallelizable, which means that the subtasks
need to have access from time to time to some of the results computed by other subtasks.
The way subtasks exchange needed information depends on the available hardware.</p>
<figure class="align-center" id="id4">
<img alt="../_images/distributed_vs_shared.png" src="../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png" data-original-src="https://enccs.github.io/gpu-programming/_images/distributed_vs_shared.png"/>
<figcaption>
<p><span class="caption-text">Distributed- vs shared-memory parallel computing.</span></p>
</figcaption>
</figure>
<p>In a distributed memory environment each processing unit operates independently from the
others. It has its own memory and it  <strong>cannot</strong> access the memory in other nodes.
The communication is done via network and each computing unit runs a separate copy of the
operating system. In a shared memory machine all processing units have access to the memory
and can read or modify the variables within.</p>
</section>
<section id="processes-and-threads">
<h3>Processes and Threads</h3>
<p>The type of environment (distributed- or shared-memory) determines the programming model.
There are two types of parallelism possible, process based and thread based.</p>
<figure class="align-center">
<img alt="../_images/processes-threads.png" src="../Images/94b62a3205925cbbdac5058209021754.png" data-original-src="https://enccs.github.io/gpu-programming/_images/processes-threads.png"/>
</figure>
<p>For distributed memory machines, a process-based parallel programming model is employed.
The processes are independent execution units which have their <em>own memory</em> address spaces.
They are created when the parallel program is started and they are only terminated at the
end. The communication between them is done explicitly via message passing like MPI.</p>
<p>On the shared memory architectures it is possible to use a thread based parallelism.
The threads are light execution units and can be created and destroyed at a relatively
small cost. The threads have their own state information, but they <em>share</em> the <em>same memory</em>
address space. When needed the communication is done though the shared memory.</p>
<p>Both approaches have their advantages and disadvantages.  Distributed machines are
relatively cheap to build and they  have an “infinite “ capacity. In principle one could
add more and more computing units. In practice the more computing units are used the more
time consuming is the communication. The shared memory systems can achieve good performance
and the programming model is quite simple. However they are limited by the memory capacity
and by the access speed. In addition in the shared parallel model it is much easier to
create race conditions.</p>
</section>
&#13;

<h3>Distributed- vs. Shared-Memory Architecture</h3>
<p>Most of computing problems are not trivially parallelizable, which means that the subtasks
need to have access from time to time to some of the results computed by other subtasks.
The way subtasks exchange needed information depends on the available hardware.</p>
<figure class="align-center" id="id4">
<img alt="../_images/distributed_vs_shared.png" src="../Images/4ad7c16c09a4b9dd6933823c5b6c1f60.png" data-original-src="https://enccs.github.io/gpu-programming/_images/distributed_vs_shared.png"/>
<figcaption>
<p><span class="caption-text">Distributed- vs shared-memory parallel computing.</span></p>
</figcaption>
</figure>
<p>In a distributed memory environment each processing unit operates independently from the
others. It has its own memory and it  <strong>cannot</strong> access the memory in other nodes.
The communication is done via network and each computing unit runs a separate copy of the
operating system. In a shared memory machine all processing units have access to the memory
and can read or modify the variables within.</p>
&#13;

<h3>Processes and Threads</h3>
<p>The type of environment (distributed- or shared-memory) determines the programming model.
There are two types of parallelism possible, process based and thread based.</p>
<figure class="align-center">
<img alt="../_images/processes-threads.png" src="../Images/94b62a3205925cbbdac5058209021754.png" data-original-src="https://enccs.github.io/gpu-programming/_images/processes-threads.png"/>
</figure>
<p>For distributed memory machines, a process-based parallel programming model is employed.
The processes are independent execution units which have their <em>own memory</em> address spaces.
They are created when the parallel program is started and they are only terminated at the
end. The communication between them is done explicitly via message passing like MPI.</p>
<p>On the shared memory architectures it is possible to use a thread based parallelism.
The threads are light execution units and can be created and destroyed at a relatively
small cost. The threads have their own state information, but they <em>share</em> the <em>same memory</em>
address space. When needed the communication is done though the shared memory.</p>
<p>Both approaches have their advantages and disadvantages.  Distributed machines are
relatively cheap to build and they  have an “infinite “ capacity. In principle one could
add more and more computing units. In practice the more computing units are used the more
time consuming is the communication. The shared memory systems can achieve good performance
and the programming model is quite simple. However they are limited by the memory capacity
and by the access speed. In addition in the shared parallel model it is much easier to
create race conditions.</p>
&#13;

<h2>Exposing parallelism</h2>
<p>There are two types of parallelism that can be explored.
The data parallelism is when the data can be distributed across computational units that can run in parallel.
The units process the data by applying the same or very similar operation to different data elements.
A common example is applying a blur filter to an image — the same function is applied to all the pixels on an image.
This parallelism is natural for the GPU, where the same instruction set is executed in multiple <abbr title="In OpenCL and SYCL: work-item.">threads</abbr>.</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png"><img alt="../_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png" src="../Images/df3318bcb375513cb4f8169dcb4efdbc.png" style="width: 715.2px; height: 265.6px;" data-original-src="https://enccs.github.io/gpu-programming/_images/ENCCS-OpenACC-CUDA_TaskParallelism_Explanation.png"/>
</a>
<figcaption>
<p><span class="caption-text">Data parallelism and task parallelism.
The data parallelism is when the same operation applies to multiple data (e.g. multiple elements of an array are transformed).
The task parallelism implies that there are more than one independent task that, in principle, can be executed in parallel.</span></p>
</figcaption>
</figure>
<p>Data parallelism can usually be explored by the GPUs quite easily.
The most basic approach would be finding a loop over many data elements and converting it into a GPU kernel.
If the number of elements in the data set is fairly large (tens or hundred of thousands elements), the GPU should perform quite well. Although it would be odd to expect absolute maximum performance from such a naive approach, it is often the one to take. Getting absolute maximum out of the data parallelism requires good understanding of how GPU works.</p>
<p>Another type of parallelism is a task parallelism.
This is when an application consists of more than one task that requiring to perform different operations with (the same or) different data.
An example of task parallelism is cooking: slicing vegetables and grilling are very different tasks and can be done at the same time.
Note that the tasks can consume totally different resources, which also can be explored.</p>
<div class="dropdown admonition">
<p class="admonition-title">In short</p>
<ul class="simple">
<li><p>Computing problems can be parallelized in distributed memory or shared memory architectures.</p></li>
<li><p>In distributed memory, each unit operates independently, with no direct memory access between nodes.</p></li>
<li><p>In shared memory, units have access to the same memory and can communicate through shared variables.</p></li>
<li><p>Parallel programming can be process-based (distributed memory) or thread-based (shared memory).</p></li>
<li><p>Process-based parallelism uses independent processes with separate memory spaces and explicit message passing.</p></li>
<li><p>Thread-based parallelism uses lightweight threads that share the same memory space and communicate through shared memory.</p></li>
<li><p>Data parallelism distributes data across computational units, processing them with the same or similar operations.</p></li>
<li><p>Task parallelism involves multiple independent tasks that perform different operations on the same or different data.</p></li>
<li><p>Task parallelism involves executing different tasks concurrently, leveraging different resources.</p></li>
</ul>
</div>
&#13;

<h2>GPU Execution Model</h2>
<p>In order to obtain maximum performance it is important to understand how GPUs execute the programs. As mentioned before a CPU is a flexible device oriented towards general purpose usage. It’s fast and versatile, designed to run operating systems and various, very different types of applications. It has lots of features, such as better control logic, caches and cache coherence, that are not related to pure computing. CPUs optimize the execution by trying to achieve low latency via heavy caching and branch prediction.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="../_images/cpu-gpu-highway.png"><img alt="../_images/cpu-gpu-highway.png" src="../Images/2be579a867209f9ee644b6264269be37.png" style="width: 768.0px; height: 432.0px;" data-original-src="https://enccs.github.io/gpu-programming/_images/cpu-gpu-highway.png"/>
</a>
<figcaption>
<p><span class="caption-text">Cars and roads analogy for the CPU and GPU behavior. The compact road is analogous to the CPU
(low latency, low throughput) and the broader road is analogous to the GPU (high latency, high throughput).</span></p>
</figcaption>
</figure>
<p>In contrast the GPUs contain a relatively small amount of transistors dedicated to control and caching, and a much larger fraction of transistors dedicated to the mathematical operations. Since the cores in a GPU are designed just for 3D graphics, they can be made much simpler and there can be a very larger number of cores. The current GPUs contain thousands of CUDA cores. Performance in GPUs is obtain by having a very high degree of parallelism. Lots of threads are launched in parallel. For good performance there should be at least several times more than the number of CUDA cores. GPU <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> are much lighter than the usual CPU threads and they have very little penalty for context switching. This way when some threads are performing some memory operations (reading or writing) others execute instructions.</p>
&#13;

<h2>CUDA Threads, Warps, Blocks</h2>
<p>In order to understand the GPU execution model let’s look at the so called <cite>axpy</cite> operation. On a single CPU core this operation would be executed in a serial manner in a <cite>for/do</cite> loop going over each element on the array, <cite>id</cite>, and computing <cite>y[id]=y[id]+a*x[id]</cite>.</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span/><span class="kt">void</span><span class="w"> </span><span class="nf">axpy_</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">y</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="k">for</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">id</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span><span class="n">id</span><span class="o">&lt;</span><span class="n">n</span><span class="p">;</span><span class="w"> </span><span class="n">id</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">       </span><span class="n">y</span><span class="p">[</span><span class="n">id</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">id</span><span class="p">];</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>In order to perform the some operation on a GPU the program launches a function called <em>kernel</em>, which is executed simultaneously by tens of thousands of <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> that can be run on GPU cores parallelly.</p>
<div class="highlight-C++ notranslate"><div class="highlight"><pre><span/><span class="n">GPU_K</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">ker_axpy_</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">a</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="o">*</span><span class="n">y</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">id</span><span class="p">)</span>
<span class="p">{</span>
<span class="w">    </span><span class="n">y</span><span class="p">[</span><span class="n">id</span><span class="p">]</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">x</span><span class="p">[</span><span class="n">id</span><span class="p">];</span><span class="w"> </span><span class="c1">// id&lt;n</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The programmers control how many instances of <cite>ker_axpy_</cite> are created and they have to make sure that all the elements are processed and also that no out of bounds accessed are happening.</p>
<p>GPU threads are much lighter than the usual CPU threads and they have very little penalty for context switching. By “over-subscribing” the GPU there are threads that are performing some memory operations (reading or writing), while others execute instructions.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/THREAD_CORE.png"><img alt="../_images/THREAD_CORE.png" src="../Images/5d5381fb2a0059b880639b60b35a9e5e.png" style="width: 638.8000000000001px; height: 265.6px;" data-original-src="https://enccs.github.io/gpu-programming/_images/THREAD_CORE.png"/>
</a>
</figure>
<p>Every <abbr title="In OpenCL and SYCL: work-item.">thread</abbr> is associated with a particular intrinsic index which can be used to calculate and access  memory locations in an array. Each thread has its context and set of private variables. All threads have access to the global GPU memory, but there is no general way to synchronize when executing a kernel. If some threads need data from the global memory which was modified by other threads the code would have to be split in several kernels because only at the completion of a kernel it is ensured that the writing to the global memory was completed.</p>
<p>Apart from being much light weighted there are more differences between GPU threads and CPU threads. GPU <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> are grouped together in groups called <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warps</abbr>. This done at hardware level.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/WARP_SMTU.png"><img alt="../_images/WARP_SMTU.png" src="../Images/3bce412cee380c39c9c2d4125dbd029f.png" style="width: 548.8000000000001px; height: 435.6px;" data-original-src="https://enccs.github.io/gpu-programming/_images/WARP_SMTU.png"/>
</a>
</figure>
<p>All memory accesses to the GPU memory are as a group in blocks of specific sizes (32B, 64B, 128B etc.). To obtain good performance the CUDA threads in the same warp need to access elements of the data which are adjacent in the memory. This is called <em>coalesced</em> memory access.</p>
<p>On some architectures, all members of a <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr> have to execute the
same instruction, the so-called “lock-step” execution. This is done to achieve
higher performance, but there are some drawbacks. If an <strong>if</strong> statement
is present inside a <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr> will cause the warp to be executed more than once,
one time for each branch. When different threads within a single <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr>
take different execution paths based on a conditional statement (if), both
branches are executed sequentially, with some threads being active while
others are inactive. On architectures without lock-step execution, such
as NVIDIA Volta / Turing (e.g., GeForce 16xx-series) or newer, <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr>
divergence is less costly.</p>
<p>There is another level in the GPU <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> hierarchy. The <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> are grouped together in so called <abbr title="In OpenCL and SYCL: work-group.">blocks</abbr>. Each block is assigned to one Streaming Multiprocessor (SMP) unit. A SMP contains one or more SIMT (single instruction multiple threads) units, schedulers, and very fast on-chip memory. Some of this on-chip memory can be used in the programs, this is called <abbr title="In OpenCL and SYCL: local memory (not to be confused with CUDA and HIP local memory).">shared memory</abbr>. The shared memory can be used to “cache” data that is used by more than one thread, thus avoiding multiple reads from the global memory. It can also be used to avoid memory accesses which are not efficient. For example in a matrix transpose operation, we have two memory operations per element and only can be coalesced. In the first step a tile of the matrix is saved read a coalesced manner in the shared memory. After all the reads of the block are done the tile can be locally transposed (which is very fast) and then written to the destination matrix in a coalesced manner as well. Shared memory can also be used to perform block-level reductions and similar collective operations. All threads can be synchronized at block level. Furthermore when the shared memory is written in order to ensure that all threads have completed the operation the synchronization is compulsory to ensure correctness of the program.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/BLOCK_SMP.png"><img alt="../_images/BLOCK_SMP.png" src="../Images/ef0f46b8c50be0dbedbce258b377a92e.png" style="width: 550.8000000000001px; height: 430.0px;" data-original-src="https://enccs.github.io/gpu-programming/_images/BLOCK_SMP.png"/>
</a>
</figure>
<p>Finally, a block of threads can not be split among SMPs. For performance blocks should have more than one <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">warp</abbr>. The more warps are active on an SMP the better is hidden the latency associated with the memory operations. If the resources are sufficient, due to fast context switching, an SMP can have more than one block active in the same time. However these blocks can not share data with each other via the on-chip memory.</p>
<p>To summarize this section. In order to take advantage of GPUs the algorithms must allow the division of work in many small subtasks which can be executed in the same time. The computations are offloaded to GPUs, by launching tens of thousands of threads all executing the same function, <em>kernel</em>, each thread working on different part of the problem. The threads are executed in groups called <em>blocks</em>, each block being assigned to a SMP. Furthermore the threads of a block are divided in <em>warps</em>, each executed by SIMT unit. All threads in a warp execute the same instructions and all memory accesses are done collectively at warp level. The threads can synchronize and share data only at block level. Depending on the architecture, some data sharing can be done as well at warp level.</p>
<p>In order to hide latencies it is recommended to “over-subscribe” the GPU. There should be many more blocks than SMPs present on the device. Also in order to ensure a good occupancy of the CUDA cores there should be more warps active on a given SMP than SIMT units. This way while some warps of threads are idle waiting for some memory operations to complete, others use the CUDA cores, thus ensuring a high occupancy of the GPU.</p>
<p>In addition to this there are some architecture-specific features of which the developers can take advantage. <abbr title="In HIP: wavefront. In OpenCL and SYCL: sub-group.">Warp</abbr>-level operations are primitives provided by the GPU architecture to allow for efficient communication and synchronization within a warp. They allow <abbr title="In OpenCL and SYCL: work-item.">threads</abbr> within a warp to exchange data efficiently, without the need for explicit synchronization. These warp-level operations, combined with the organization of threads into blocks and clusters, make it possible to implement complex algorithms and achieve high performance on the GPU. The cooperative groups feature introduced in recent versions of CUDA provides even finer-grained control over thread execution, allowing for even more efficient processing by giving more flexibility to the thread hierarchy. Cooperative groups allow threads within a block to organize themselves into smaller groups, called cooperative groups, and to synchronize their execution and share data within the group.</p>
<p>Below there is an example of how the threads in a grid can be associated with specific elements of an array</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../_images/Indexing.png"><img alt="../_images/Indexing.png" src="../Images/fccb3e8f6b92e4b53a0805282127878f.png" style="width: 546.4px; height: 247.20000000000002px;" data-original-src="https://enccs.github.io/gpu-programming/_images/Indexing.png"/>
</a>
</figure>
<p>The thread marked by orange color is part of a grid of threads size 4096. The threads are grouped in blocks of size 256. The “orange” thread has index 3 in the block 2 and the global calculated index 515.</p>
<p>For a vector addition example this would be used as follow <code class="docutils literal notranslate"><span class="pre">c[index]=a[index]+b[index]</span></code>.</p>
<div class="dropdown admonition">
<p class="admonition-title">In short</p>
<ul class="simple">
<li><p>GPUs have a different execution model compared to CPUs, with a focus on parallelism and mathematical operations.</p></li>
<li><p>GPUs consist of thousands of lightweight threads that can be executed simultaneously on GPU cores.</p></li>
<li><p>Threads are organized into warps, and warps are grouped into blocks assigned to streaming multiprocessors (SMPs).</p></li>
<li><p>GPUs achieve performance through high degrees of parallelism and efficient memory access.</p></li>
<li><p>Shared memory can be used to cache data and improve memory access efficiency within a block.</p></li>
<li><p>Synchronization and data sharing are limited to the block level, with some possible sharing at the warp level depending on the architecture.</p></li>
<li><p>Over-subscribing the GPU and maximizing warp and block occupancy help hide latencies and improve performance.</p></li>
<li><p>Warp-level operations and cooperative groups provide efficient communication and synchronization within a warp or block.</p></li>
<li><p>Thread indexing allows associating threads with specific elements in an array for parallel processing.</p></li>
</ul>
</div>
&#13;

<h2>Terminology</h2>
<p>At the moment there are three major GPU producers: NVIDIA, Intel, and AMD. While the basic concept behind GPUs is pretty similar they use different names for the various parts. Furthermore there are software environments for GPU programming, some from the producers and some from external groups all having different naming as well. Below there is a short compilation of the some terms used across different platforms and software environments.</p>
<table class="docutils align-center" id="id7">
<caption><span class="caption-text">Software mapping naming</span></caption>
<thead>
<tr class="row-odd"><th class="head"><p>CUDA</p></th>
<th class="head"><p>HIP</p></th>
<th class="head"><p>OpenCL</p></th>
<th class="head"><p>SYCL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td colspan="2"><p>grid of threads</p></td>
<td colspan="2"><p>NDRange</p></td>
</tr>
<tr class="row-odd"><td colspan="2"><p>block</p></td>
<td colspan="2"><p>work-group</p></td>
</tr>
<tr class="row-even"><td><p>warp</p></td>
<td><p>wavefront</p></td>
<td colspan="2"><p>sub-group</p></td>
</tr>
<tr class="row-odd"><td colspan="2"><p>thread</p></td>
<td colspan="2"><p>work-item</p></td>
</tr>
<tr class="row-even"><td colspan="2"><p>registers</p></td>
<td colspan="2"><p>private memory</p></td>
</tr>
<tr class="row-odd"><td><p>shared memory</p></td>
<td><p>local data share</p></td>
<td colspan="2"><p>local memory</p></td>
</tr>
<tr class="row-even"><td colspan="2"><p>threadIdx.{x,y,z}</p></td>
<td><p>get_local_id({0,1,2})</p></td>
<td><p>nd_item::get_local({2,1,0}) <a class="footnote-reference brackets" href="#syclindex" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-odd"><td colspan="2"><p>blockIdx.{x,y,z}</p></td>
<td><p>get_group_id({0,1,2})</p></td>
<td><p>nd_item::get_group({2,1,0}) <a class="footnote-reference brackets" href="#syclindex" id="id2" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
</tr>
<tr class="row-even"><td colspan="2"><p>blockDim.{x,y,z}</p></td>
<td><p>get_local_size({0,1,2})</p></td>
<td><p>nd_item::get_local_range({2,1,0}) <a class="footnote-reference brackets" href="#syclindex" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a></p></td>
</tr>
</tbody>
</table>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="syclindex" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id2">2</a>,<a role="doc-backlink" href="#id3">3</a>)</span>
<p>In SYCL, the thread indexing is inverted. In a 3D grid, physically adjacent threads have consecutive X (0) index in CUDA, HIP, and OpenCL, but consecutive Z (2) index in SYCL.
In a 2D grid, CUDA, HIP, and OpenCL still has contiguous indexing along X (0) dimension, while in SYCL it is Y (1).
Same applies to block dimensions and indexing.</p>
</aside>
</aside>
&#13;

<h2>Exercises</h2>
<div class="admonition-what-are-threads-in-the-context-of-shared-memory-architectures exercise important admonition" id="exercise-0">
<p class="admonition-title">What are threads in the context of shared memory architectures?</p>
<ol class="loweralpha simple">
<li><p>Independent execution units with their own memory address spaces</p></li>
<li><p>Light execution units with shared memory address spaces</p></li>
<li><p>Communication devices between separate memory units</p></li>
<li><p>Programming models for distributed memory machines</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-0">
<p class="admonition-title">Solution</p>
<p>Correct answer:  <em>b) Light execution units with shared memory address spaces</em></p>
</div>
</div>
<div class="admonition-what-is-data-parallelism exercise important admonition" id="exercise-1">
<p class="admonition-title">What is data parallelism?</p>
<ol class="loweralpha simple">
<li><p>Distributing data across computational units that run in parallel, applying the same or similar operations to different data elements.</p></li>
<li><p>Distributing tasks across computational units that run in parallel, applying different operations to the same data elements.</p></li>
<li><p>Distributing data across computational units that run sequentially, applying the same operation to all data elements.</p></li>
<li><p>Distributing tasks across computational units that run sequentially, applying different operations to different data elements.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-1">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>a) Distributing data across computational units that run in parallel, applying the same or similar operations to different data elements.</em></p>
</div>
</div>
<div class="admonition-what-type-of-parallelism-is-natural-for-gpu exercise important admonition" id="exercise-2">
<p class="admonition-title">What type of parallelism is natural for GPU?</p>
<ol class="loweralpha simple">
<li><p>Task Parallelism</p></li>
<li><p>Data Parallelism</p></li>
<li><p>Both data and task parallelism</p></li>
<li><p>Neither data nor task parallelism</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-2">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>b) Data Parallelism</em></p>
</div>
</div>
<div class="admonition-what-is-a-kernel-in-the-context-of-gpu-execution exercise important admonition" id="exercise-3">
<p class="admonition-title">What is a kernel in the context of GPU execution?</p>
<ol class="loweralpha simple">
<li><p>A specific section of the CPU used for memory operations.</p></li>
<li><p>A specific section of the GPU used for memory operations.</p></li>
<li><p>A type of thread that operates on the GPU.</p></li>
<li><p>A function that is executed simultaneously by tens of thousands of threads on GPU cores.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-3">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>d) A function that is executed simultaneously by tens of thousands of threads on GPU cores.</em></p>
</div>
</div>
<div class="admonition-what-is-coalesced-memory-access exercise important admonition" id="exercise-4">
<p class="admonition-title">What is coalesced memory access?</p>
<ol class="loweralpha simple">
<li><p>It’s when CUDA threads in the same warp access elements of the data which are adjacent in the memory.</p></li>
<li><p>It’s when CUDA threads in different warps access elements of the data which are far in the memory.</p></li>
<li><p>It’s when all threads have access to the global GPU memory.</p></li>
<li><p>It’s when threads in a warp perform different operations.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-4">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>a) It’s when CUDA threads in the same warp access elements of the data which are adjacent in the memory.</em></p>
</div>
</div>
<div class="admonition-what-is-the-function-of-shared-memory-in-the-context-of-gpu-execution exercise important admonition" id="exercise-5">
<p class="admonition-title">What is the function of shared memory in the context of GPU execution?</p>
<ol class="loweralpha simple">
<li><p>It’s used to store global memory.</p></li>
<li><p>It’s used to store all the threads in a block.</p></li>
<li><p>It can be used to “cache” data that is used by more than one thread, avoiding multiple reads from the global memory.</p></li>
<li><p>It’s used to store all the CUDA cores.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-5">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>c) It can be used to “cache” data that is used by more than one thread, avoiding multiple reads from the global memory.</em></p>
</div>
</div>
<div class="admonition-what-is-the-significance-of-over-subscribing-the-gpu exercise important admonition" id="exercise-6">
<p class="admonition-title">What is the significance of over-subscribing the GPU?</p>
<ol class="loweralpha simple">
<li><p>It reduces the overall performance of the GPU.</p></li>
<li><p>It ensures that there are more blocks than SMPs present on the device, helping to hide latencies and ensure high occupancy of the GPU.</p></li>
<li><p>It leads to a memory overflow in the GPU.</p></li>
<li><p>It ensures that there are more SMPs than blocks present on the device.</p></li>
</ol>
<div class="admonition-solution solution important dropdown admonition" id="solution-6">
<p class="admonition-title">Solution</p>
<p>Correct answer: <em>b) It ensures that there are more blocks than SMPs present on the device, helping to hide latencies and ensure high occupancy of the GPU.</em></p>
</div>
</div>
<div class="admonition-keypoints keypoints admonition" id="keypoints-0">
<p class="admonition-title">Keypoints</p>
<ul class="simple">
<li><p>Parallel computing can be classified into distributed-memory and shared-memory architectures</p></li>
<li><p>Two types of parallelism that can be explored are data parallelism and task parallelism.</p></li>
<li><p>GPUs are a type of shared memory architecture suitable for data parallelism.</p></li>
<li><p>GPUs have high parallelism, with threads organized into warps and blocks and.</p></li>
<li><p>GPU optimization involves coalesced memory access, shared memory usage, and high thread and warp occupancy. Additionally, architecture-specific features such as warp-level operations and cooperative groups can be leveraged for more efficient processing.</p></li>
</ul>
</div>
    
</body>
</html>