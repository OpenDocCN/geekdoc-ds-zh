- en: 3\. Optimization theory and algorithms#
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3. 优化理论与算法#
- en: 原文：[https://mmids-textbook.github.io/chap03_opt/00_intro/roch-mmids-opt-intro.html](https://mmids-textbook.github.io/chap03_opt/00_intro/roch-mmids-opt-intro.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap03_opt/00_intro/roch-mmids-opt-intro.html](https://mmids-textbook.github.io/chap03_opt/00_intro/roch-mmids-opt-intro.html)
- en: In this chapter, we turn to optimization theory and algorithms, which lie at
    the core of modern data science and AI. We derive basic optimality conditions,
    including in the presence of convexity. We then introduce a fundamental optimization
    algorithm, the gradient descent method, and detail a theoretical analysis of its
    convergence under different assumptions. We illustrate these concepts on an important
    supervised learning problem. Here is a more detailed overview of the main sections
    of the chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们转向优化理论与算法，这是现代数据科学和人工智能的核心。我们推导了基本的最优性条件，包括在凸性存在的情况下。然后，我们介绍了一种基本的优化算法——梯度下降法，并详细分析了其在不同假设下的收敛性理论。我们通过一个重要的监督学习问题来阐述这些概念。以下是本章主要部分的更详细概述。
- en: '*“Background: review of differentiable functions of several variables”* This
    section reviews the fundamental concepts of differentiable functions of several
    variables. The gradient is defined as a vector of partial derivatives, generalizing
    the concept of the derivative for functions of a single variable. The section
    also introduces the Hessian matrix, which contains second-order partial derivatives.
    The Chain Rule for functions of several variables is presented and used to prove
    a multivariable version of the Mean Value Theorem. Finally, the section provides
    examples of calculating gradients and Hessians for various functions, including
    affine and quadratic functions.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*“背景：多变量可微函数的回顾”* 本节回顾了多变量可微函数的基本概念。梯度被定义为偏导数向量，它推广了单变量函数导数的概念。本节还介绍了Hessian矩阵，它包含二阶偏导数。本节展示了多变量函数的链式法则，并用于证明均值定理的多变量版本。最后，本节提供了计算各种函数（包括仿射函数和二次函数）的梯度和Hessian矩阵的示例。'
- en: '*“Optimality conditions”* This section focuses on deriving optimality conditions
    for unconstrained continuous optimization problems. It introduces the concept
    of a global minimizer, which is a point where a function attains its minimum value
    over its entire domain. Recognizing the difficulty of finding global minimizers,
    the section also defines local minimizers, points where a function takes its minimum
    value within a neighborhood. It introduces the concepts of first-order conditions
    based on the gradient and second-order conditions based on the Hessian matrix.
    The section explains that while the first-order necessary condition of the gradient
    being zero at a point is not sufficient for a local minimizer, the second-order
    sufficient condition involving the positive definiteness of the Hessian at a point
    satisfying the first-order condition guarantees a strict local minimizer. The
    section concludes by extending the discussion to optimization problems with equality
    constraints, presenting the method of Lagrange multipliers.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*“最优性条件”* 本节专注于推导无约束连续优化问题的最优性条件。它引入了全局最小值点的概念，即函数在其整个定义域上达到最小值的点。鉴于寻找全局最小值点的困难，本节还定义了局部最小值点，即函数在某个邻域内取最小值的点。本节介绍了基于梯度的第一阶条件和基于Hessian矩阵的第二阶条件。本节解释说，虽然梯度在一点为零的第一阶必要条件对于局部最小值点来说并不充分，但涉及满足第一阶条件的点的Hessian矩阵正定性的第二阶充分条件保证了严格的局部最小值点。本节最后将讨论扩展到具有等式约束的优化问题，介绍了拉格朗日乘数法。'
- en: '*“Convexity”* This section introduces the concept of convexity, which plays
    a crucial role in optimization problems. It defines convex sets and functions,
    and provides examples and properties of each. A set is convex if the line segment
    connecting any two points in the set lies entirely within the set. Convex functions
    are defined similarly, with the additional requirement that the function’s value
    at any point on the line segment is less than or equal to the corresponding weighted
    average of the function’s values at the endpoints. The section then establishes
    the connection between convexity and optimization, showing that for convex functions,
    any local minimizer is also a global minimizer. It also presents conditions for
    convexity based on the gradient and Hessian. The section concludes by introducing
    the notion of strong convexity, which guarantees the existence and uniqueness
    of global minimizers, and provides examples of strongly convex functions, such
    as least-squares objectives.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*“凸性”* 本节介绍了凸性的概念，这在优化问题中起着至关重要的作用。它定义了凸集和函数，并提供了每个的例子和性质。如果一个集合中的任意两点之间的线段完全位于集合内，则该集合是凸的。凸函数的定义类似，但增加了额外的要求，即线段上任意点的函数值小于或等于函数在端点值的加权平均值。本节接着建立了凸性与优化之间的联系，表明对于凸函数，任何局部最小值也是全局最小值。它还基于梯度和海森矩阵提出了凸性的条件。本节最后介绍了强凸性的概念，这保证了全局最小值的存在和唯一性，并提供了如最小二乘目标函数等强凸函数的例子。'
- en: '*“Gradient descent and its convergence analysis”* This section discusses gradient
    descent, a numerical optimization method for finding the minimum of a continuously
    differentiable function. The method iteratively takes steps in the direction of
    the negative gradient, which is proven to be the steepest descent direction. The
    convergence of gradient descent is analyzed under different assumptions on the
    function being minimized. For smooth functions, gradient descent with an appropriate
    step size is shown to produce a sequence of points whose objective values decrease
    and whose gradients vanish in the limit. When the function is both smooth and
    strongly convex, a faster convergence rate is obtained, with the function values
    converging exponentially to the global minimum. The section also includes examples
    and Python implementations to illustrate the concepts.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*“梯度下降及其收敛性分析”* 本节讨论了梯度下降，这是一种用于寻找连续可微函数最小值的数值优化方法。该方法迭代地沿着负梯度的方向移动，已被证明是最陡下降方向。在关于被最小化函数的不同假设下分析了梯度下降的收敛性。对于光滑函数，当使用适当的步长时，梯度下降会产生一个目标值递减且梯度在极限中消失的点的序列。当函数既是光滑的又是强凸的时，可以获得更快的收敛速度，函数值以指数速度收敛到全局最小值。本节还包括了示例和Python实现来阐述这些概念。'
- en: '*“Application: logistic regression”* The section applies gradient descent to
    logistic regression, a method for modeling the probability of a binary outcome
    based on input features. It derives the gradient and Hessian of the logistic regression
    objective function, proves the objective is convex and smooth, and provides Python
    code to implement gradient descent for logistic regression. The method is applied
    to example datasets.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*“应用：逻辑回归”* 该部分将梯度下降法应用于逻辑回归，这是一种基于输入特征建模二元结果概率的方法。它推导出逻辑回归目标函数的梯度和海森矩阵，证明目标函数是凸的和光滑的，并提供了实现逻辑回归梯度下降的Python代码。该方法应用于示例数据集。'
