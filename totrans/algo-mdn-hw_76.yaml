- en: Matrix Multiplication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://en.algorithmica.org/hpc/algorithms/matmul/](https://en.algorithmica.org/hpc/algorithms/matmul/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this case study, we will design and implement several algorithms for matrix
    multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: We start with the naive “for-for-for” algorithm and incrementally improve it,
    eventually arriving at a version that is 50 times faster and matches the performance
    of BLAS libraries while being under 40 lines of C.
  prefs: []
  type: TYPE_NORMAL
- en: All implementations are compiled with GCC 13 and run on a [Zen 2](https://en.wikichip.org/wiki/amd/microarchitectures/zen_2)
    CPU clocked at 2GHz.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/algorithms/matmul/#baseline)Baseline'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of multiplying an $l \times n$ matrix $A$ by an $n \times m$ matrix
    $B$ is defined as an $l \times m$ matrix $C$ such that:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ C_{ij} = \sum_{k=1}^{n} A_{ik} \cdot B_{kj} $$
  prefs: []
  type: TYPE_NORMAL
- en: For simplicity, we will only consider *square* matrices, where $l = m = n$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement matrix multiplication, we can simply transfer this definition
    into code, but instead of two-dimensional arrays (aka matrices), we will be using
    one-dimensional arrays to be explicit about pointer arithmetic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For reasons that will become apparent later, we will only use matrix sizes that
    are multiples of $48$ for benchmarking, but the implementations remain correct
    for all others. We also use [32-bit floats](/hpc/arithmetic/ieee-754) specifically,
    although all implementations can be easily [generalized](#generalizations) to
    other data types and operations.
  prefs: []
  type: TYPE_NORMAL
- en: Compiled with `g++ -O3 -march=native -ffast-math -funroll-loops`, the naive
    approach multiplies two matrices of size $n = 1920 = 48 \times 40$ in ~16.7 seconds.
    To put it in perspective, this is approximately $\frac{1920^3}{16.7 \times 10^9}
    \approx 0.42$ useful operations per nanosecond (GFLOPS), or roughly 5 CPU cycles
    per multiplication, which doesn’t look that good yet.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/algorithms/matmul/#transposition)Transposition'
  prefs: []
  type: TYPE_NORMAL
- en: In general, when optimizing an algorithm that processes large quantities of
    data — and $1920^2 \times 3 \times 4 \approx 42$ MB clearly is a large quantity
    as it can’t fit into any of the [CPU caches](/hpc/cpu-cache) — one should always
    start with memory before optimizing arithmetic, as it is much more likely to be
    the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: The field $C_{ij}$ can be thought of as the dot product of row $i$ of matrix
    $A$ and column $j$ of matrix $B$. As we increment `k` in the inner loop above,
    we are reading the matrix `a` sequentially, but we are jumping over $n$ elements
    as we iterate over a column of `b`, which is [not as fast](/hpc/cpu-cache/aos-soa)
    as sequential iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'One [well-known](/hpc/external-memory/oblivious/#matrix-multiplication) optimization
    that tackles this problem is to store matrix $B$ in *column-major* order — or,
    alternatively, to *transpose* it before the matrix multiplication. This requires
    $O(n^2)$ additional operations but ensures sequential reads in the innermost loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This code runs in ~12.4s, or about 30% faster.
  prefs: []
  type: TYPE_NORMAL
- en: As we will see in a bit, there are more important benefits to transposing it
    than just the sequential memory reads.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/algorithms/matmul/#vectorization)Vectorization'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that all we do is just sequentially read the elements of `a` and `b`, multiply
    them, and add the result to an accumulator variable, we can use [SIMD](/hpc/simd/)
    instructions to speed it all up. It is pretty straightforward to implement using
    [GCC vector types](/hpc/simd/intrinsics/#gcc-vector-extensions) — we can [memory-align](/hpc/cpu-cache/alignment/)
    matrix rows, pad them with zeros, and then compute the multiply-sum as we would
    normally compute any other [reduction](/hpc/simd/reduction/):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The performance for $n = 1920$ is now around 2.3 GFLOPS — or another ~4 times
    higher compared to the transposed but not vectorized version.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/00d5459969621e683c4e77b5a7a95845.png)'
  prefs: []
  type: TYPE_IMG
- en: This optimization looks neither too complex nor specific to matrix multiplication.
    Why can’t the compiler [auto-vectorizee](/hpc/simd/auto-vectorization/) the inner
    loop by itself?
  prefs: []
  type: TYPE_NORMAL
- en: 'It actually can; the only thing preventing that is the possibility that `c`
    overlaps with either `a` or `b`. To rule it out, you can communicate to the compiler
    that you guarantee `c` is not [aliased](/hpc/compilation/contracts/#memory-aliasing)
    with anything by adding the `__restrict__` keyword to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Both manually and auto-vectorized implementations perform roughly the same.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/algorithms/matmul/#memory-efficiency)Memory
    efficiency'
  prefs: []
  type: TYPE_NORMAL
- en: What is interesting is that the implementation efficiency depends on the problem
    size.
  prefs: []
  type: TYPE_NORMAL
- en: At first, the performance (defined as the number of useful operations per second)
    increases as the overhead of the loop management and the horizontal reduction
    decreases. Then, at around $n=256$, it starts smoothly decreasing as the matrices
    stop fitting into the [cache](/hpc/cpu-cache/) ($2 \times 256^2 \times 4 = 512$
    KB is the size of the L2 cache), and the performance becomes bottlenecked by the
    [memory bandwidth](/hpc/cpu-cache/bandwidth/).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/975e5f9450e0945c80efa3a5e008db91.png)'
  prefs: []
  type: TYPE_IMG
- en: It is also interesting that the naive implementation is mostly on par with the
    non-vectorized transposed version — and even slightly better because it doesn’t
    need to perform a transposition.
  prefs: []
  type: TYPE_NORMAL
- en: 'One might think that there would be some general performance gain from doing
    sequential reads since we are fetching fewer cache lines, but this is not the
    case: fetching the first column of `b` indeed takes more time, but the next 15
    column reads will be in the same cache lines as the first one, so they will be
    cached anyway — unless the matrix is so large that it can’t even fit `n * cache_line_size`
    bytes into the cache, which is not the case for any practical matrix sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, the performance deteriorates on only a few specific matrix sizes due
    to the effects of [cache associativity](/hpc/cpu-cache/associativity/): when $n$
    is a multiple of a large power of two, we are fetching the addresses of `b` that
    all likely map to the same cache line, which reduces the effective cache size.
    This explains the 30% performance dip for $n = 1920 = 2^7 \times 3 \times 5$,
    and you can see an even more noticeable one for $1536 = 2^9 \times 3$: it is roughly
    3 times slower than for $n=1535$.'
  prefs: []
  type: TYPE_NORMAL
- en: So, counterintuitively, transposing the matrix doesn’t help with caching — and
    in the naive scalar implementation, we are not really bottlenecked by the memory
    bandwidth anyway. But our vectorized implementation certainly is, so let’s work
    on its I/O efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/algorithms/matmul/#register-reuse)Register
    reuse'
  prefs: []
  type: TYPE_NORMAL
- en: Using a Python-like notation to refer to submatrices, to compute the cell $C[x][y]$,
    we need to calculate the dot product of $A[x][:]$ and $B[:][y]$, which requires
    fetching $2n$ elements, even if we store $B$ in column-major order.
  prefs: []
  type: TYPE_NORMAL
- en: To compute $C[x:x+2][y:y+2]$, a $2 \times 2$ submatrix of $C$, we would need
    two rows from $A$ and two columns from $B$, namely $A[x:x+2][:]$ and $B[:][y:y+2]$,
    containing $4n$ elements in total, to update *four* elements instead of *one*
    — which is $\frac{2n / 1}{4n / 4} = 2$ times better in terms of I/O efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid fetching data more than once, we need to iterate over these rows and
    columns in parallel and calculate all $2 \times 2$ possible combinations of products.
    Here is a proof of concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now simply call this kernel on all 2x2 submatrices of $C$, but we won’t
    bother evaluating it: although this algorithm is better in terms of I/O operations,
    it would still not beat our SIMD-based implementation. Instead, we will extend
    this approach and develop a similar *vectorized* kernel right away.'
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/algorithms/matmul/#designing-the-kernel)Designing
    the kernel'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of designing a kernel that computes an $h \times w$ submatrix of $C$
    from scratch, we will declare a function that *updates* it using columns from
    $l$ to $r$ of $A$ and rows from $l$ to $r$ of $B$. For now, this seems like an
    over-generalization, but this function interface will prove useful later.
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine $h$ and $w$, we have several performance considerations:'
  prefs: []
  type: TYPE_NORMAL
- en: In general, to compute an $h \times w$ submatrix, we need to fetch $2 \cdot
    n \cdot (h + w)$ elements. To optimize the I/O efficiency, we want the $\frac{h
    \cdot w}{h + w}$ ratio to be high, which is achieved with large and square-ish
    submatrices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to use the [FMA](https://en.wikipedia.org/wiki/FMA_instruction_set)
    (“fused multiply-add”) instruction available on all modern x86 architectures.
    As you can guess from the name, it performs the `c += a * b` operation — which
    is the core of a dot product — on 8-element vectors in one go, which saves us
    from executing vector multiplication and addition separately.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To achieve better utilization of this instruction, we want to make use of [instruction-level
    parallelism](/hpc/pipelining/). On Zen 2, the `fma` instruction has a latency
    of 5 and a throughput of 2, meaning that we need to concurrently execute at least
    $5 \times 2 = 10$ of them to saturate its execution ports.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We want to avoid register spill (move data to and from registers more than necessary),
    and we only have $16$ logical vector registers that we can use as accumulators
    (minus those that we need to hold temporary values).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For these reasons, we settle on a $6 \times 16$ kernel. This way, we process
    $96$ elements at once that are stored in $6 \times 2 = 12$ vector registers. To
    update them efficiently, we use the following procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We need `t` so that the compiler stores these elements in vector registers.
    We could just update their final destinations in `c`, but, unfortunately, the
    compiler re-writes them back to memory, causing a slowdown (wrapping everything
    in `__restrict__` keywords doesn’t help).
  prefs: []
  type: TYPE_NORMAL
- en: 'After unrolling these loops and hoisting `b` out of the `i` loop (`b[(k * n
    + y) / 8 + j]` does not depend on `i` and can be loaded once and reused in all
    6 iterations), the compiler generates something more similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We are using $12+3=15$ vector registers and a total of $6 \times 3 + 2 = 20$
    instructions to perform $16 \times 6 = 96$ updates. Assuming that there are no
    other bottleneks, we should be hitting the throughput of `_mm256_fmadd_ps`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this kernel is architecture-specific. If we didn’t have `fma`, or
    if its throughput/latency were different, or if the SIMD width was 128 or 512
    bits, we would have made different design choices. Multi-platform BLAS implementations
    ship [many kernels](https://github.com/xianyi/OpenBLAS/tree/develop/kernel), each
    written in assembly by hand and optimized for a particular architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the implementation is straightforward. Similar to the previous
    vectorized implementation, we just move the matrices to memory-aligned arrays
    and call the kernel instead of the innermost loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This improves the benchmark performance, but only by ~40%:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f2657a7ecd6c529f6e6452ee1e44234e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The speedup is much higher (2-3x) on smaller arrays, indicating that there
    is still a memory bandwidth problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/40a8fec92b43c74fb7f4f8f73db525c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, if you’ve read the section on [cache-oblivious algorithms](/hpc/external-memory/oblivious/),
    you know that one universal solution to these types of things is to split all
    matrices into four parts, perform eight recursive block matrix multiplications,
    and carefully combine the results together. This solution is okay in practice,
    but there is some [overhead to recursion](/hpc/architecture/functions/), and it
    also doesn’t allow us to fine-tune the algorithm, so instead, we will follow a
    different, simpler approach.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/algorithms/matmul/#blocking)Blocking'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *cache-aware* alternative to the divide-and-conquer trick is *cache blocking*:
    splitting the data into blocks that can fit into the cache and processing them
    one by one. If we have more than one layer of cache, we can do hierarchical blocking:
    we first select a block of data that fits into the L3 cache, then we split it
    into blocks that fit into the L2 cache, and so on. This approach requires knowing
    the cache sizes in advance, but it is usually easier to implement and also faster
    in practice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cache blocking is less trivial to do with matrices than with arrays, but the
    general idea is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a submatrix of $B$ that fits into the L3 cache (say, a subset of its
    columns).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select a submatrix of $A$ that fits into the L2 cache (say, a subset of its
    rows).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select a submatrix of the previously selected submatrix of $B$ (a subset of
    its rows) that fits into the L1 cache.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the relevant submatrix of $C$ using the kernel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here is a good [visualization](https://jukkasuomela.fi/cache-blocking-demo/)
    by Jukka Suomela (it features many different approaches; you are interested in
    the last one).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the decision to start this process with matrix $B$ is not arbitrary.
    During the kernel execution, we are reading the elements of $A$ much slower than
    the elements of $B$: we fetch and broadcast just one element of $A$ and then multiply
    it with $16$ elements of $B$. Therefore, we want $B$ to be in the L1 cache while
    $A$ can stay in the L2 cache and not the other way around.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This sounds complicated, but we can implement it with just three more outer
    `for` loops, which are collectively called *macro-kernel* (and the highly optimized
    low-level function that updates a 6x16 submatrix is called *micro-kernel*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Cache blocking completely removes the memory bottleneck:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e03a7a28939cc2aa9d85d80e659b63ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The performance is no longer (significantly) affected by the problem size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3c3d2bcb24bee8e8ec7020c3651d3570.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Notice that the dip at $1536$ is still there: cache associativity still affects
    the performance. To mitigate this, we can adjust the step constants or insert
    holes into the layout, but we will not bother doing that for now.'
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/algorithms/matmul/#optimization)Optimization'
  prefs: []
  type: TYPE_NORMAL
- en: 'To approach closer to the performance limit, we need a few more optimizations:'
  prefs: []
  type: TYPE_NORMAL
- en: Remove memory allocation and operate directly on the arrays that are passed
    to the function. Note that we don’t need to do anything with `a` as we are reading
    just one element at a time, and we can use an [unaligned](/hpc/simd/moving/#aligned-loads-and-stores)
    `store` for `c` as we only use it rarely, so our only concern is reading `b`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get rid of the `std::min` so that the size parameters are (mostly) constant
    and can be embedded into the machine code by the compiler (which also lets it
    [unroll](/hpc/architecture/loops/) the micro-kernel loop more efficiently and
    avoid runtime checks).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rewrite the micro-kernel by hand using 12 vector variables (the compiler seems
    to struggle with keeping them in registers and writes them first to a temporary
    memory location and only then to $C$).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These optimizations are straightforward but quite tedious to implement, so we
    are not going to list [the code](https://github.com/sslotin/amh-code/blob/main/matmul/v5-unrolled.cc)
    here in the article. It also requires some more work to effectively support “weird”
    matrix sizes, which is why we only run benchmarks for sizes that are multiple
    of $48 = \frac{6 \cdot 16}{\gcd(6, 16)}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'These individually small improvements compound and result in another 50% improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f3b30c45c12e557f364590624dbeadfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are actually not that far from the theoretical performance limit — which
    can be calculated as the SIMD width times the `fma` instruction throughput times
    the clock frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \underbrace{8}_{SIMD} \cdot \underbrace{2}_{thr.} \cdot \underbrace{2 \cdot
    10^9}_{cycles/sec} = 32 \; GFLOPS \;\; (3.2 \cdot 10^{10}) $$
  prefs: []
  type: TYPE_NORMAL
- en: 'It is more representative to compare against some practical library, such as
    [OpenBLAS](https://www.openblas.net/). The laziest way to do it is to simply [invoke
    matrix multiplication from NumPy](/hpc/complexity/languages/#blas). There may
    be some minor overhead due to Python, but it ends up reaching 80% of the theoretical
    limit, which seems plausible (a 20% overhead is okay: matrix multiplication is
    not the only thing that CPUs are made for).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3df0f282b3daecc24455ce8a340c9b27.png)'
  prefs: []
  type: TYPE_IMG
- en: We’ve reached ~93% of BLAS performance and ~75% of the theoretical performance
    limit, which is really great for what is essentially just 40 lines of C.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, the whole thing can be rolled into just one deeply nested `for`
    loop with a BLAS level of performance (assuming that we’re in 2050 and using GCC
    version 35, which finally stopped screwing up with register spilling):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There is also an approach that performs asymptotically fewer arithmetic operations
    — [the Strassen algorithm](/hpc/external-memory/oblivious/#strassen-algorithm)
    — but it has a large constant factor, and it is only efficient for [very large
    matrices](https://arxiv.org/pdf/1605.01078.pdf) ($n > 4000$), where we typically
    have to use either multiprocessing or some approximate dimensionality-reducing
    methods anyway.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/algorithms/matmul/#generalizations)Generalizations'
  prefs: []
  type: TYPE_NORMAL
- en: 'FMA also supports 64-bit floating-point numbers, but it does not support integers:
    you need to perform addition and multiplication separately, which results in decreased
    performance. If you can guarantee that all intermediate results can be represented
    exactly as 32- or 64-bit floating-point numbers (which is [often the case](/hpc/arithmetic/errors/)),
    it may be faster to just convert them to and from floats.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach can be also applied to some similar-looking computations. One
    example is the “min-plus matrix multiplication” defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ (A \circ B)_{ij} = \min_{1 \le k \le n} (A_{ik} + B_{kj}) $$
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also known as the “distance product” due to its graph interpretation:
    when applied to itself $(D \circ D)$, the result is the matrix of shortest paths
    of length two between all pairs of vertices in a fully-connected weighted graph
    specified by the edge weight matrix $D$.'
  prefs: []
  type: TYPE_NORMAL
- en: A cool thing about the distance product is that if we iterate the process and
    calculate
  prefs: []
  type: TYPE_NORMAL
- en: $$ D_2 = D \circ D \\ D_4 = D_2 \circ D_2 \\ D_8 = D_4 \circ D_4 \\ \ldots $$
  prefs: []
  type: TYPE_NORMAL
- en: '…we can find all-pairs shortest paths in $O(\log n)$ steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This requires $O(n^3 \log n)$ operations. If we do these two-edge relaxations
    in a particular order, we can do it with just one pass, which is known as the
    [Floyd-Warshall algorithm](https://en.wikipedia.org/wiki/Floyd%E2%80%93Warshall_algorithm):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Interestingly, similarly vectorizing the distance product and executing it $O(\log
    n)$ times ([or possibly fewer](https://arxiv.org/pdf/1904.01210.pdf)) in $O(n^3
    \log n)$ total operations is faster than naively executing the Floyd-Warshall
    algorithm in $O(n^3)$ operations, although not by a lot.
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, try to speed up this “for-for-for” computation. It is harder
    to do than in the matrix multiplication case because now there is a logical dependency
    between the iterations, and you need to perform updates in a particular order,
    but it is still possible to design [a similar kernel and a block iteration order](https://github.com/sslotin/amh-code/blob/main/floyd/blocked.cc)
    that achieves a 30-50x total speedup.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/algorithms/matmul/#acknowledgements)Acknowledgements'
  prefs: []
  type: TYPE_NORMAL
- en: The final algorithm was originally designed by Kazushige Goto, and it is the
    basis of GotoBLAS and OpenBLAS. The author himself describes it in more detail
    in “[Anatomy of High-Performance Matrix Multiplication](https://www.cs.utexas.edu/~flame/pubs/GotoTOMS_revision.pdf)”.
  prefs: []
  type: TYPE_NORMAL
- en: The exposition style is inspired by the “[Programming Parallel Computers](http://ppc.cs.aalto.fi/)”
    course by Jukka Suomela, which features a [similar case study](http://ppc.cs.aalto.fi/ch2/)
    on speeding up the distance product. [← Prefix Sum with SIMD](https://en.algorithmica.org/hpc/algorithms/prefix/)[../Data
    Structures Case Studies →](https://en.algorithmica.org/hpc/data-structures/)
  prefs: []
  type: TYPE_NORMAL
