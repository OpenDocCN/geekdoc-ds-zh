- en: '24'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A QUICK LOOK AT MACHINE LEARNING
  prefs: []
  type: TYPE_NORMAL
- en: The amount of digital data in the world has been growing at a rate that defies
    human comprehension. The world's data storage capacity has doubled about every
    three years since the 1980s. During the time it will take you to read this chapter,
    approximately `10`^(18) bits of data will be added to the world's store. It's
    not easy to relate to a number that large. One way to think about it is that `10`^(18)
    Canadian pennies would have a surface area roughly twice that of the earth.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, more data does not always lead to more useful information. Evolution
    is a slow process, and the ability of the human mind to assimilate data does not,
    alas, double every three years. One approach that the world is using to attempt
    to wring more useful information from “big data” is **statistical machine learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is hard to define. In some sense, every useful program learns
    something. For example, an implementation of Newton's method learns the roots
    of a polynomial. One of the earliest definitions was proposed by the American
    electrical engineer and computer scientist Arthur Samuel,[^(181)](#c24-fn-0001)
    who defined it as a “field of study that gives computers the ability to learn
    without being explicitly programmed.”
  prefs: []
  type: TYPE_NORMAL
- en: Humans learn in two ways—memorization and generalization. We use memorization
    to accumulate individual facts. In England, for example, primary school students
    might learn a list of English monarchs. Humans use **generalization** to deduce
    new facts from old facts. A student of political science, for example, might observe
    the behavior of a large number of politicians, and generalize from those observations
    to conclude that all politicians lie when campaigning.
  prefs: []
  type: TYPE_NORMAL
- en: When computer scientists speak about machine learning, they most often mean
    the discipline of writing programs that automatically learn to make useful inferences
    from implicit patterns in data. For example, linear regression (see Chapter 20)
    learns a curve that is a model of a collection of examples. That model can then
    be used to make predictions about previously unseen examples. The basic paradigm
    is
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Observe a set of examples, frequently called the **training data**, that
    represents incomplete information about some statistical phenomenon.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Use inference techniques to create a model of a process that could have
    generated the observed examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. Use that model to make predictions about previously unseen examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose, for example, you were given the two sets of names in [Figure 24-1](#c24-fig-0001)
    and the **feature vectors** in [Figure 24-2](#c24-fig-0002).
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0001.jpg](../images/c24-fig-0001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 24-1](#c24-fig-0001a) Two sets of names'
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0002.jpg](../images/c24-fig-0002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 24-2](#c24-fig-0002a) Associating a feature vector with each name'
  prefs: []
  type: TYPE_NORMAL
- en: Each element of a vector corresponds to some aspect (i.e., feature) of the person.
    Based on this limited information about these historical figures, you might infer
    that the process assigning either the label `A` or the label `B` to each example
    was intended to separate tall presidents from shorter ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many approaches to machine learning, but all try to learn a model
    that is a generalization of the provided examples. All have three components:'
  prefs: []
  type: TYPE_NORMAL
- en: A representation of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An objective function for assessing the goodness of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An optimization method for learning a model that minimizes or maximizes the
    value of the objective function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Broadly speaking, machine learning algorithms can be thought of as either supervised
    or unsupervised.
  prefs: []
  type: TYPE_NORMAL
- en: In **supervised learning**, we start with a set of feature vector/value pairs.
    The goal is to derive from these pairs a rule that predicts the value associated
    with a previously unseen feature vector. **Regression models** associate a real
    number with each feature vector. **Classification models** associate one of a
    finite number of **labels** with each feature vector.[^(182)](#c24-fn-0002)
  prefs: []
  type: TYPE_NORMAL
- en: In Chapter 20, we looked at one kind of regression model, linear regression.
    Each feature vector was an x-coordinate, and the value associated with it was
    the corresponding y-coordinate. From the set of feature vector/value pairs we
    learned a model that could be used to predict the y-coordinate associated with
    any x-coordinate.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at a simple classification model. Given the sets of presidents
    we labeled `A` and `B` in [Figure 24-1](#c24-fig-0001) and the feature vectors
    in [Figure 24-2](#c24-fig-0002), we can generate the feature vector/label pairs
    in [Figure 24-3](#c24-fig-0003).
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0003.jpg](../images/c24-fig-0003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 24-3](#c24-fig-0003a) Feature vector/label pairs for presidents'
  prefs: []
  type: TYPE_NORMAL
- en: From these labeled examples, a learning algorithm might infer that all tall
    presidents should be labeled `A` and all short presidents labeled `B`. When asked
    to assign a label to
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: it would use the rule it had learned to choose label `A`.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised machine learning is broadly used for such tasks as detecting fraudulent
    use of credit cards and recommending movies to people.
  prefs: []
  type: TYPE_NORMAL
- en: In **unsupervised learning**, we are given a set of feature vectors but no labels.
    The goal of unsupervised learning is to uncover latent structure in the set of
    feature vectors. For example, given the set of presidential feature vectors, an
    unsupervised learning algorithm might separate the presidents into tall and short,
    or perhaps into American and French. Approaches to unsupervised machine learning
    can be categorized as either methods for clustering or methods for learning latent
    variable models.
  prefs: []
  type: TYPE_NORMAL
- en: A **latent variable** is a variable whose value is not directly observed but
    can be inferred from the values of variables that are observed. Admissions officers
    at universities, for example, try to infer the probability of an applicant being
    a successful student (the latent variable), based on a set of observable values
    such as secondary school grades and performance on standardized tests. There is
    a rich set of methods for learning latent variable models, but we do not cover
    them in this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering** partitions a set of examples into groups (called clusters) such
    that examples in the same group are more similar to each other than they are to
    examples in other groups. Geneticists, for example, use clustering to find groups
    of related genes. Many popular clustering methods are surprisingly simple.'
  prefs: []
  type: TYPE_NORMAL
- en: We present a widely used clustering algorithm in Chapter 25, and several approaches
    to supervised learning in Chapter 26\. In the remainder of this chapter, we discuss
    the process of building feature vectors and different ways of calculating the
    similarity between two feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 24.1 Feature Vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of **signal-to-noise ratio** (**SNR**) is used in many branches
    of engineering and science. The precise definition varies across applications,
    but the basic idea is simple. Think of it as the ratio of useful input to irrelevant
    input. In a restaurant, the signal might be the voice of your dinner date, and
    the noise the voices of the other diners.[^(184)](#c24-fn-0004) If we were trying
    to predict which students would do well in a programming course, previous programming
    experience and mathematical aptitude would be part of the signal, but hair color
    merely noise. Separating the signal from the noise is not always easy. When it
    is done poorly, the noise can be a distraction that obscures the truth in the
    signal.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of **feature engineering** is to separate those features in the
    available data that contribute to the signal from those that are merely noise.
    Failure to do an adequate job of this can lead to a bad model. The danger is particularly
    high when the **dimensionality** of the data (i.e., the number of different features)
    is large relative to the number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: Successful feature engineering reduces the vast amount of information that might
    be available to information from which it will be productive to generalize. Imagine,
    for example, that your goal is to learn a model that will predict whether a person
    is likely to suffer a heart attack. Some features, such as their age, are likely
    to be highly relevant. Other features, such as whether they are left-handed, are
    less likely to be relevant.
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature selection** techniques can be used to automatically identify which
    features in a given set of features are most likely to be helpful. For example,
    in the context of supervised learning, we can select those features that are most
    strongly correlated with the labels of the examples.[^(185)](#c24-fn-0005) However,
    these feature selection techniques are of little help if relevant features are
    not there to start with. Suppose that our original feature set for the heart attack
    example includes height and weight. It might be the case that while neither height
    nor weight is highly predictive of a heart attack, body mass index (BMI) is. While
    BMI can be computed from height and weight, the relationship (weight in kilograms
    divided by the square of height in meters) is too complicated to be automatically
    found by typical machine learning techniques. Successful machine learning often
    involves the design of features by those with domain expertise.'
  prefs: []
  type: TYPE_NORMAL
- en: In unsupervised learning, the problem is even harder. Typically, we choose features
    based upon our intuition about which features might be relevant to the kinds of
    structure we would like to find. However, relying on intuition about the potential
    relevance of features is problematic. How good is your intuition about whether
    someone's dental history is a useful predictor of a future heart attack?
  prefs: []
  type: TYPE_NORMAL
- en: Consider [Figure 24-4](#c24-fig-0004), which contains a table of feature vectors
    and the label (reptile or not) with which each vector is associated.
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0004.jpg](../images/c24-fig-0004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 24-4](#c24-fig-0004a) Name, features, and labels for assorted animals'
  prefs: []
  type: TYPE_NORMAL
- en: A supervised machine learning algorithm (or a human) given only the information
    about cobras—i.e., only the first row of the table—cannot do much more than to
    remember the fact that a cobra is a reptile. Now, let's add the information about
    rattlesnakes. We can begin to generalize and might infer the rule that an animal
    is a reptile if it lays eggs, has scales, is poisonous, is cold-blooded, and has
    no legs.
  prefs: []
  type: TYPE_NORMAL
- en: Now, suppose we are asked to decide if a boa constrictor is a reptile. We might
    answer “no,” because a boa constrictor is neither poisonous nor egg-laying. But
    this would be the wrong answer. Of course, it is hardly surprising that attempting
    to generalize from two examples might lead us astray. Once we include the boa
    constrictor in our training data, we might formulate the new rule that an animal
    is a reptile if it has scales, is cold-blooded, and is legless. In doing so, we
    are discarding the features `egg-laying` and `poisonous` as irrelevant to the
    classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: If we use the new rule to classify the alligator, we conclude incorrectly that
    since it has legs it is not a reptile. Once we include the alligator in the training
    data, we reformulate the rule to allow reptiles to have either none or four legs.
    When we look at the dart frog, we correctly conclude that it is not a reptile,
    since it is not cold-blooded. However, when we use our current rule to classify
    the salmon, we incorrectly conclude that a salmon is a reptile. We can add yet
    more complexity to our rule to separate salmon from alligators, but it's a losing
    battle. There is no way to modify our rule so that it will correctly classify
    both salmon and pythons, since the feature vectors of these two species are identical.
  prefs: []
  type: TYPE_NORMAL
- en: This kind of problem is more common than not in machine learning. It is rare
    to have feature vectors that contain enough information to classify things perfectly.
    In this case, the problem is that we don't have enough features.
  prefs: []
  type: TYPE_NORMAL
- en: If we had included the fact that reptile eggs have amnios,[^(186)](#c24-fn-0006)
    we could devise a rule that separates reptiles from fish. Unfortunately, in most
    practical applications of machine learning it is not possible to construct feature
    vectors that allow for perfect discrimination.
  prefs: []
  type: TYPE_NORMAL
- en: Does this mean that we should give up because all of the available features
    are mere noise? No. In this case, the features `scales` and `cold-blooded` are
    necessary conditions for being a reptile, but not sufficient conditions. The rule
    that an animal is a reptile if it has scales and is cold-blooded will not yield
    any false negatives, i.e., any animal classified as a non-reptile will indeed
    not be a reptile. However, the rule will yield some false positives, i.e., some
    of the animals classified as reptiles will not be reptiles.
  prefs: []
  type: TYPE_NORMAL
- en: 24.2 Distance Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [Figure 24-4](#c24-fig-0004) we described animals using four binary features
    and one integer feature. Suppose we want to use these features to evaluate the
    similarity of two animals, for example, to ask whether a rattlesnake is more similar
    to a boa constrictor or to a dart frog.[^(187)](#c24-fn-0007)
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step in doing this kind of comparison is converting the features
    for each animal into a sequence of numbers. If we say `True = 1` and `False =
    0`, we get the following feature vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There are many ways to compare the similarity of vectors of numbers. The most
    commonly used metrics for comparing equal-length vectors are based on the **Minkowski
    distance**:[^(188)](#c24-fn-0008)
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-5001.jpg](../images/c24-fig-5001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *len* is the length of the vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter `p`, which must be at least 1, defines the kinds of paths that
    can be followed in traversing the distance between the vectors *V* and *W*.[^(189)](#c24-fn-0009)
    This can be easily visualized if the vectors are of length two, and can therefore
    be represented using Cartesian coordinates. Consider the picture in [Figure 24-5](#c24-fig-0006).
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0005.jpg](../images/c24-fig-0005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 24-5](#c24-fig-0006a) Visualizing distance metrics'
  prefs: []
  type: TYPE_NORMAL
- en: Is the circle in the bottom-left corner closer to the cross or closer to the
    star? It depends. If we can travel in a straight line, the cross is closer. The
    Pythagorean Theorem tells us that the cross is the square root of `8` units from
    the circle, about `2.8` units, whereas we can easily see that the star is `3`
    units from the circle. These distances are called **Euclidean distances**, and
    correspond to using the Minkowski distance with `p = 2`. But imagine that the
    lines in the picture correspond to streets, and that we have to stay on the streets
    to get from one place to another. The star remains `3` units from the circle,
    but the cross is now `4` units away. These distances are called **Manhattan**
    **distances**,[^(190)](#c24-fn-0010) and they correspond to using the Minkowski
    distance with `p = 1`. [Figure 24-6](#c24-fig-0007) contains a function implementing
    the Minkowski distance.
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0006.jpg](../images/c24-fig-0006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 24-6](#c24-fig-0007a) Minkowski distance'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 24-7](#c24-fig-0008) contains class `Animal`. It defines the distance
    between two animals as the Euclidean distance between the feature vectors associated
    with the animals.'
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0007.jpg](../images/c24-fig-0007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 24-7](#c24-fig-0008a) Class `Animal`'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 24-8](#c24-fig-0009) contains a function that compares a list of animals
    to each other and produces a table showing the pairwise distances. The code uses
    a Matplotlib plotting facility that we have not previously used: `table`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `table` function produces a plot that (surprise!) looks like a table. The
    keyword arguments `rowLabels` and `colLabels` are used to supply the labels (in
    this example the names of the animals) for the rows and columns. The keyword argument
    `cellText` is used to supply the values appearing in the cells of the table. In
    the example, `cellText` is bound to `table_vals`, which is a list of lists of
    strings. Each element in `table_vals` is a list of the values for the cells in
    one row of the table. The keyword argument `cellLoc` is used to specify where
    in each cell the text should appear, and the keyword argument `loc` is used to
    specify where in the figure the table itself should appear. The last keyword parameter
    used in the example is `colWidths`. It is bound to a list of floats giving the
    width (in inches) of each column in the table. The code `table.scale(1, 2.5)`
    instructs Matplotlib to leave the horizontal width of the cells unchanged, but
    to increase the height of the cells by a factor of `2.5` (so the tables look prettier).
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0008.jpg](../images/c24-fig-0008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 24-8](#c24-fig-0009a) Build table of distances between pairs of animals'
  prefs: []
  type: TYPE_NORMAL
- en: If we run the code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: it produces the table in [Figure 24-9](#c24-fig-0010).
  prefs: []
  type: TYPE_NORMAL
- en: As you probably expected, the distance between the rattlesnake and the boa constrictor
    is less than that between either of the snakes and the dart frog. Notice, by the
    way, that the dart frog is a bit closer to the rattlesnake than to the boa constrictor.
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0009.jpg](../images/c24-fig-0009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 24-9](#c24-fig-0010a) Distances between three animals'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's insert before the last line of the above code the lines
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: It produces the table in [Figure 24-10](#c24-fig-0011).
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0010.jpg](../images/c24-fig-0010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 24-10](#c24-fig-0011a) Distances between four animals'
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps you're surprised that the alligator is considerably closer to the dart
    frog than to either the rattlesnake or the boa constrictor. Take a minute to think
    about why.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature vector for the alligator differs from that of the rattlesnake in
    two places: whether it is poisonous and the number of legs. The feature vector
    for the alligator differs from that of the dart frog in three places: whether
    it is poisonous, whether it has scales, and whether it is cold-blooded. Yet, according
    to our Euclidean distance metric, the alligator is more like the dart frog than
    like the rattlesnake. What''s going on?'
  prefs: []
  type: TYPE_NORMAL
- en: The root of the problem is that the different features have different ranges
    of values. All but one of the features range between `0` and `1`, but the number
    of legs ranges from `0` to `4`. This means that when we calculate the Euclidean
    distance, the number of legs gets disproportionate weight. Let's see what happens
    if we turn the feature into a binary feature, with a value of `0` if the animal
    is legless and `1` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: '![c24-fig-0011.jpg](../images/c24-fig-0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 24-11 Distances using a different feature representation
  prefs: []
  type: TYPE_NORMAL
- en: This looks a lot more plausible.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, it is not always convenient to use only binary features. In Section
    25.4, we will present a more general approach to dealing with differences in scale
    among features.
  prefs: []
  type: TYPE_NORMAL
- en: 24.3 Terms Introduced in Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: statistical machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: generalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: training data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: latent variable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: signal-to-noise ratio (SNR)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dimensionality (of data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minkowski distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: triangle inequality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Euclidean distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manhattan distance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
