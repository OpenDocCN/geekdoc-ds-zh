<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Prefetching</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>Prefetching</h1>
<blockquote>原文：<a href="https://en.algorithmica.org/hpc/cpu-cache/prefetching/">https://en.algorithmica.org/hpc/cpu-cache/prefetching/</a></blockquote><div id="search"><input id="search-bar" type="search" placeholder="Search this book…" oninput="search()"/><div id="search-count"/><div id="search-results"/></div><header><div class="info"/></header><article><p>Taking advantage of the <a href="../mlp">free concurrency</a> available in memory hardware, it can be beneficial to <em>prefetch</em> data that is likely to be accessed next if its location can be predicted. This is easy to do when there are no <a href="/hpc/pipelining/hazards">data of control hazards</a> in the pipeline and the CPU can just run ahead of the instruction stream and execute memory operations out of order.</p><p>But sometimes the memory locations aren’t in the instruction stream, and yet they can still be predicted with high probability. In these cases, they can be prefetched by other means:</p><ul><li>Explicitly, by separately reading the next data word or any of the bytes in the same cache line, so that it is lifted in the cache hierarchy.</li><li>Implicitly, by using simple access patterns such as linear iteration, which are detectable by the memory hardware that can start prefetching automatically.</li></ul><p>Hiding memory latency is crucial for achieving performance, so in this section, we will look into prefetching techniques.</p><span class="anchor" id="hardware-prefetching"/><h3><a class="anchor-link" href="https://en.algorithmica.org/hpc/cpu-cache/prefetching/#hardware-prefetching">#</a>Hardware Prefetching</h3><p>Let’s modify the <a href="../latency">pointer chasing</a> benchmark to show the effect of hardware prefetching. Now, we generate our permutation in a way that makes the CPU request consecutive cache lines when iterating over the permutation, but still accessing the elements inside a cache line in random order:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">p</span><span class="p">[</span><span class="mi">15</span><span class="p">],</span> <span class="n">q</span><span class="p">[</span><span class="n">N</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">iota</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span> <span class="o">+</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">16</span> <span class="o">&lt;</span> <span class="n">N</span><span class="p">;</span> <span class="n">i</span> <span class="o">+=</span> <span class="mi">16</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="n">random_shuffle</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">p</span> <span class="o">+</span> <span class="mi">15</span><span class="p">);</span>
</span></span><span class="line"><span class="cl">    <span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="n">i</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="mi">15</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="n">p</span><span class="p">[</span><span class="n">j</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="n">q</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">16</span><span class="p">;</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>There is no point in making a graph because it would be just flat: the latency is 3ns regardless of the array size. Even though the instruction scheduler still can’t tell what we are going to fetch next, the memory prefetcher can detect a pattern just by looking at the memory accesses and start loading the next cache line ahead of time, mitigating the latency.</p><p>Hardware prefetching is smart enough for most use cases, but it only detects simple patterns. You can iterate forward and backward over multiple arrays in parallel, perhaps with small-to-medium strides, but that’s about it. For anything more complex, the prefetcher won’t figure out what’s happening, and we need to help it out ourselves.</p><span class="anchor" id="software-prefetching"/><h3><a class="anchor-link" href="https://en.algorithmica.org/hpc/cpu-cache/prefetching/#software-prefetching">#</a>Software Prefetching</h3><p>The simplest way to do software prefetching is to load any byte in the cache line with the <code>mov</code> or any other memory instruction, but CPUs have a separate <code>prefetch</code> instruction that lifts a cache line without doing anything with it. This instruction isn’t a part of the C or C++ standard, but is available in most compilers as the <code>__builtin_prefetch</code> intrinsic:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-c++" data-lang="c++"><span class="line"><span class="cl"><span class="n">__builtin_prefetch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">a</span><span class="p">[</span><span class="n">k</span><span class="p">]);</span>
</span></span></code></pre></div><p>It’s quite hard to come up with a <em>simple</em> example when it can be useful. To make the pointer chasing benchmark benefit from software prefetching, we need to construct a permutation that at the same time loops around the whole array, can’t be predicted by hardware prefetcher, and has easily computable next addresses.</p><p>Luckily, the <a href="https://en.wikipedia.org/wiki/Linear_congruential_generator">linear congruential generator</a> has the property that if the modulus $n$ is a prime number, then the period of the generator will be exactly $n$. So we get all the properties we need if we use a permutation generated by the LCG with the current index as its state:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">find_prime</span><span class="p">(</span><span class="n">N</span><span class="p">);</span> <span class="c1">// largest prime not exceeding N
</span></span></span><span class="line"><span class="cl"><span class="c1"/>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">q</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n</span><span class="p">;</span>
</span></span></code></pre></div><p>When we run it, the performance matches a normal random permutation. But now we get the ability to peek ahead:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="kt">int</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">t</span> <span class="o">&lt;</span> <span class="n">K</span><span class="p">;</span> <span class="n">t</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">n</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="n">__builtin_prefetch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">q</span><span class="p">[(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n</span><span class="p">]);</span>
</span></span><span class="line"><span class="cl">        <span class="n">k</span> <span class="o">=</span> <span class="n">q</span><span class="p">[</span><span class="n">k</span><span class="p">];</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>There is some overhead to computing the next address, but for arrays large enough, it is almost two times faster:</p><p><figure><img src="../Images/3bfccc02083790ffa4ddd3297d7bf0ea.png" data-original-src="https://en.algorithmica.org/hpc/cpu-cache/img/sw-prefetch.svg"/><figcaption/></figure></p><p>Interestingly, we can prefetch more than just one element ahead, making use of this pattern in the LCG function:</p>$$
\begin{aligned}
f(x) &amp;= 2 \cdot x + 1
\\ f^2(x) &amp;= 4 \cdot x + 2 + 1
\\ f^3(x) &amp;= 8 \cdot x + 4 + 2 + 1
\\ &amp;\ldots
\\ f^k(x) &amp;= 2^k \cdot x + (2^k - 1)
\end{aligned}
$$<p>Hence, to load the <code>D</code>-th element ahead, we can do this:</p><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-cpp" data-lang="cpp"><span class="line"><span class="cl"><span class="n">__builtin_prefetch</span><span class="p">(</span><span class="o">&amp;</span><span class="n">q</span><span class="p">[((</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">D</span><span class="p">)</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">&lt;&lt;</span> <span class="n">D</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n</span><span class="p">]);</span>
</span></span></code></pre></div><p>If we execute this request on every iteration, we will be simultaneously prefetching <code>D</code> elements ahead on average, increasing the throughput by <code>D</code> times. Ignoring some issues such as the integer overflow when <code>D</code> is too large, we can reduce the average latency arbitrarily close to the cost of computing the next index (which, in this case, is dominated by the <a href="/hpc/arithmetic/division">modulo operation</a>).</p><p><figure><img src="../Images/77b30bf5058b2fae823154acfdb0c8e6.png" data-original-src="https://en.algorithmica.org/hpc/cpu-cache/img/sw-prefetch-others.svg"/><figcaption/></figure></p><p>Note that this is an artificial example, and you actually fail more often than not when trying to insert software prefetching into practical programs. This is largely because you need to issue a separate memory instruction that may compete for resources with the others. At the same time, hardware prefetching is 100% harmless as it only activates when the memory and cache buses are not busy.</p><p>You can also specify a specific level of cache the data needs to be brought to when doing software prefetching — when you aren’t sure if you will be using it and don’t want to kick out what is already in the L1 cache. You can use it with the <code>_mm_prefetch</code> intrinsic, which takes an integer value as the second parameter, specifying the cache level. This is useful in combination with <a href="../bandwidth#bypassing-the-cache">non-temporal loads and stores</a>.</p></article><div class="nextprev"><div class="left"><a href="https://en.algorithmica.org/hpc/cpu-cache/mlp/" id="prev-article">← Memory-Level Parallelism</a></div><div class="right"><a href="https://en.algorithmica.org/hpc/cpu-cache/alignment/" id="next-article">Alignment and Packing →</a></div></div>    
</body>
</html>