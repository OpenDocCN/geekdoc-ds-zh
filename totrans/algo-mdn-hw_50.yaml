- en: Cache-Oblivious Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://en.algorithmica.org/hpc/external-memory/oblivious/](https://en.algorithmica.org/hpc/external-memory/oblivious/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In the context of the [external memory model](../model), there are two types
    of efficient algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Cache-aware* algorithms that are efficient for *known* $B$ and $M$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cache-oblivious* algorithms that are efficient for *any* $B$ and $M$.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, [external merge sorting](../sorting) is a cache-aware, but not
    cache-oblivious algorithm: we need to know the memory characteristics of the system,
    namely the ratio of available memory to the block size, to find the right $k$
    to perform $k$-way merge sort.'
  prefs: []
  type: TYPE_NORMAL
- en: Cache-oblivious algorithms are interesting because they automatically become
    optimal for all memory levels in the cache hierarchy, and not just the one for
    which they were specifically tuned. In this article, we consider some of their
    applications in matrix calculations.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/external-memory/oblivious/#matrix-transposition)Matrix
    Transposition'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume we have a square matrix $A$ of size $N \times N$, and we need to transpose
    it. The naive by-definition approach would go something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here we used a single pointer to the beginning of the memory region instead
    of a 2d array to be more explicit about its memory operations.
  prefs: []
  type: TYPE_NORMAL
- en: The I/O complexity of this code is $O(N^2)$ because the writes are not sequential.
    If you try to swap the iteration variables, it will be the other way around, but
    the result is going to be the same.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/oblivious/#algorithm)Algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *cache-oblivious* algorithm relies on the following block matrix identity:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{pmatrix} A & B \\ C & D \end{pmatrix}^T= \begin{pmatrix} A^T & C^T
    \\ B^T & D^T \end{pmatrix} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'It lets us solve the problem recursively using a divide-and-conquer approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide the input matrix into 4 smaller matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transpose each one recursively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine results by swapping the corner result matrices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing D&C on matrices is a bit more complex than on arrays, but the main
    idea is the same. Instead of copying submatrices explicitly, we want to use “views”
    into them, and also switch to the naive method when the data starts fitting in
    the L1 cache (or pick something small like $32 \times 32$ if you don’t know it
    in advance). We also need to carefully handle the case when we have odd $n$ and
    thus can’t split the matrix into 4 equal submatrices.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The I/O complexity of the algorithm is $O(\frac{N^2}{B})$, as we only need to
    touch roughly half the memory blocks during each merge stage, meaning that on
    each stage our problem becomes smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Adapting this code for the general case of non-square matrices is left as an
    exercise to the reader.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/external-memory/oblivious/#matrix-multiplication)Matrix
    Multiplication'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s consider something slightly more complex: matrix multiplication.'
  prefs: []
  type: TYPE_NORMAL
- en: $$ C_{ij} = \sum_k A_{ik} B_{kj} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'The naive algorithm just translates its definition into code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It needs to access $O(N^3)$ blocks in total as each scalar multiplication needs
    a separate block read.
  prefs: []
  type: TYPE_NORMAL
- en: 'One well-known optimization is to transpose $B$ first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Whether the transpose is done naively or with the cache-oblivious method we
    previously developed, the matrix multiplication with one of the matrices transposed
    would work in $O(N^3/B + N^2)$ as all memory accesses are now sequential.
  prefs: []
  type: TYPE_NORMAL
- en: It seems like we can’t do better, but it turns out we can.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/oblivious/#algorithm)Algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cache-oblivious matrix multiplication relies on essentially the same trick
    as the transposition. We need to divide the data until it fits into lowest cache
    (i.e., $N^2 \leq M$). For matrix multiplication, this equates to using this formula:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ \begin{pmatrix} A_{11} & A_{12} \\ A_{21} & A_{22} \\ \end{pmatrix} \begin{pmatrix}
    B_{11} & B_{12} \\ B_{21} & B_{22} \\ \end{pmatrix} = \begin{pmatrix} A_{11} B_{11}
    + A_{12} B_{21} & A_{11} B_{12} + A_{12} B_{22}\\ A_{21} B_{11} + A_{22} B_{21}
    & A_{21} B_{12} + A_{22} B_{22}\\ \end{pmatrix} $$
  prefs: []
  type: TYPE_NORMAL
- en: 'It is slightly harder to implement though because we now have a total of 8
    recursive matrix multiplications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Because there are many other factors in play here, we are not going to benchmark
    this implementation, and instead just do its theoretical performance analysis
    in external memory model.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/oblivious/#analysis)Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: Arithmetic complexity of the algorithm remains is the same, because the recurrence
  prefs: []
  type: TYPE_NORMAL
- en: $$ T(N) = 8 \cdot T(N/2) + \Theta(N^2) $$
  prefs: []
  type: TYPE_NORMAL
- en: is solved by $T(N) = \Theta(N^3)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'It doesn’t seem like we “conquered” anything yet, but let’s think about its
    I/O complexity:'
  prefs: []
  type: TYPE_NORMAL
- en: $$ T(N) = \begin{cases} O(\frac{N^2}{B}) & N \leq \sqrt M & \text{(we only need
    to read it)} \\ 8 \cdot T(N/2) + O(\frac{N^2}{B}) & \text{otherwise} \end{cases}
    $$ The recurrence is dominated by $O((\frac{N}{\sqrt M})^3)$ base cases, meaning
    that the total complexity is $$ T(N) = O\left(\frac{(\sqrt{M})^2}{B} \cdot \left(\frac{N}{\sqrt
    M}\right)^3\right) = O\left(\frac{N^3}{B\sqrt{M}}\right) $$
  prefs: []
  type: TYPE_NORMAL
- en: This is better than just $O(\frac{N^3}{B})$, and by quite a lot.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/oblivious/#strassen-algorithm)Strassen
    Algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: In a spirit similar to the Karatsuba algorithm, matrix multiplication can be
    decomposed in a way that involves 7 matrix multiplications of size $\frac{n}{2}$,
    and the master theorem tells us that such divide-and-conquer algorithm would work
    in $O(n^{\log_2 7}) \approx O(n^{2.81})$ time and a similar asymptotic in the
    external memory model.
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique, known as the Strassen algorithm, similarly splits each matrix
    into 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '$$ \begin{pmatrix} C_{11} & C_{12} \\ C_{21} & C_{22} \\ \end{pmatrix} =\begin{pmatrix}
    A_{11} & A_{12} \\ A_{21} & A_{22} \\ \end{pmatrix} \begin{pmatrix} B_{11} & B_{12}
    \\ B_{21} & B_{22} \\ \end{pmatrix} $$ But then it computes intermediate products
    of the $\frac{N}{2} \times \frac{N}{2}$ matrices and combines them to get matrix
    $C$: $$ \begin{aligned} M_1 &= (A_{11} + A_{22})(B_{11} + B_{22}) & C_{11} &=
    M_1 + M_4 - M_5 + M_7 \\ M_2 &= (A_{21} + A_{22}) B_{11} & C_{12} &= M_3 + M_5
    \\ M_3 &= A_{11} (B_{21} - B_{22}) & C_{21} &= M_2 + M_4 \\ M_4 &= A_{22} (B_{21}
    - B_{11}) & C_{22} &= M_1 - M_2 + M_3 + M_6 \\ M_5 &= (A_{11} + A_{12}) B_{22}
    \\ M_6 &= (A_{21} - A_{11}) (B_{11} + B_{12}) \\ M_7 &= (A_{12} - A_{22}) (B_{21}
    + B_{22}) \end{aligned} $$'
  prefs: []
  type: TYPE_NORMAL
- en: You can verify these formulas with simple substitution if you feel like it.
  prefs: []
  type: TYPE_NORMAL
- en: As far as I know, none of the mainstream optimized linear algebra libraries
    use the Strassen algorithm, although there are [some prototype implementations](https://arxiv.org/pdf/1605.01078.pdf)
    that are efficient for matrices larger than 2000 or so.
  prefs: []
  type: TYPE_NORMAL
- en: This technique can and actually has been extended multiple times to reduce the
    asymptotic even further by considering more submatrix products. As of 2020, current
    world record is $O(n^{2.3728596})$. Whether you can multiply matrices in $O(n^2)$
    or at least $O(n^2 \log^k n)$ time is an open problem.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/external-memory/oblivious/#further-reading)Further
    Reading'
  prefs: []
  type: TYPE_NORMAL
- en: For a solid theoretical viewpoint, consider reading [Cache-Oblivious Algorithms
    and Data Structures](https://erikdemaine.org/papers/BRICS2002/paper.pdf) by Erik
    Demaine. [← Eviction Policies](https://en.algorithmica.org/hpc/external-memory/policies/)[Spatial
    and Temporal Locality →](https://en.algorithmica.org/hpc/external-memory/locality/)
  prefs: []
  type: TYPE_NORMAL
