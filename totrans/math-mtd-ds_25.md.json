["```py\ndef fhat(x,a):\n    return 1 / ( 1 + np.exp(-np.outer(x,a)) )\n\ndef loss(x,a,b): \n    return np.mean(-b*np.log(fhat(x,a)) - (1 - b)*np.log(1 - fhat(x,a)), axis=1)\n\ndef grad(x,a,b):\n    return -np.mean((b - fhat(x,a))*a, axis=1) \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\n\nn = 10000\na, b = 2*rng.uniform(0,1,n) - 1, rng.integers(2, size=n) \n```", "```py\nx0 = -0.3\nx = np.linspace(x0-0.05,x0+0.05,100)\nupper = loss(x0,a,b) + (x - x0)*grad(x0,a,b) + (1/2)*(x - x0)**2\nlower = loss(x0,a,b) + (x - x0)*grad(x0,a,b) - (1/2)*(x - x0)**2\n\nplt.plot(x, loss(x,a,b), label='loss')\nplt.plot(x, upper, label='upper')\nplt.plot(x, lower, label='lower')\nplt.legend()\nplt.show() \n```", "```py\ndef sigmoid(z): \n    return 1/(1+np.exp(-z))\n\ndef desc_update_for_logreg(grad_fn, A, b, curr_x, beta):\n    gradient = grad_fn(curr_x, A, b)\n    return curr_x - beta*gradient\n\ndef gd_for_logreg(loss_fn, grad_fn, A, b, init_x, beta=1e-3, niters=int(1e5)):\n    curr_x = init_x\n\n    for iter in range(niters):\n        curr_x = desc_update_for_logreg(grad_fn, A, b, curr_x, beta)\n\n    return curr_x\n\ndef pred_fn(x, A): \n    return sigmoid(A @ x)\n\ndef loss_fn(x, A, b): \n    return np.mean(-b*np.log(pred_fn(x, A)) - (1 - b)*np.log(1 - pred_fn(x, A)))\n\ndef grad_fn(x, A, b):\n    return -A.T @ (b - pred_fn(x, A))/len(b)\n\ndef stepsize_for_logreg(A, b):\n    L = LA.norm(A)**2 / (4 * len(b))\n    return 1/L \n```", "```py\ndata = pd.read_csv('lebron.csv')\ndata.head() \n```", "```py\ndata.describe() \n```", "```py\nfeature = data['shot_distance']\nlabel = data['shot_made']\n\nplt.scatter(feature, label, s=10, color='b', alpha=0.2)\nplt.show() \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\n\nlabel_jitter = label + 0.05*rng.normal(0,1,len(label))\n\nplt.scatter(feature, label_jitter, s=10, color='b', alpha=0.2)\nplt.show() \n```", "```py\nA = np.stack((np.ones(len(label)),feature),axis=-1)\nb = label \n```", "```py\nstepsize = stepsize_for_logreg(A, b)\nprint(stepsize) \n```", "```py\n0.017671625306319678 \n```", "```py\ninit_x = np.zeros(A.shape[1])\nbest_x = gd_for_logreg(loss_fn, grad_fn, A, b, init_x, beta=stepsize)\nprint(best_x) \n```", "```py\n[ 0.90959003 -0.05890828] \n```", "```py\ngrid = np.linspace(np.min(feature), np.max(feature), 100)\nfeature_grid = np.stack((np.ones(len(grid)),grid),axis=-1)\npredict_grid = sigmoid(feature_grid @ best_x)\n\nplt.scatter(feature, label_jitter, s=10, color='b', alpha=0.2)\nplt.plot(grid,predict_grid,'r')\nplt.show() \n```", "```py\ndef fhat(x,a):\n    return 1 / ( 1 + np.exp(-np.outer(x,a)) )\n\ndef loss(x,a,b): \n    return np.mean(-b*np.log(fhat(x,a)) - (1 - b)*np.log(1 - fhat(x,a)), axis=1)\n\ndef grad(x,a,b):\n    return -np.mean((b - fhat(x,a))*a, axis=1) \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\n\nn = 10000\na, b = 2*rng.uniform(0,1,n) - 1, rng.integers(2, size=n) \n```", "```py\nx0 = -0.3\nx = np.linspace(x0-0.05,x0+0.05,100)\nupper = loss(x0,a,b) + (x - x0)*grad(x0,a,b) + (1/2)*(x - x0)**2\nlower = loss(x0,a,b) + (x - x0)*grad(x0,a,b) - (1/2)*(x - x0)**2\n\nplt.plot(x, loss(x,a,b), label='loss')\nplt.plot(x, upper, label='upper')\nplt.plot(x, lower, label='lower')\nplt.legend()\nplt.show() \n```", "```py\ndef sigmoid(z): \n    return 1/(1+np.exp(-z))\n\ndef desc_update_for_logreg(grad_fn, A, b, curr_x, beta):\n    gradient = grad_fn(curr_x, A, b)\n    return curr_x - beta*gradient\n\ndef gd_for_logreg(loss_fn, grad_fn, A, b, init_x, beta=1e-3, niters=int(1e5)):\n    curr_x = init_x\n\n    for iter in range(niters):\n        curr_x = desc_update_for_logreg(grad_fn, A, b, curr_x, beta)\n\n    return curr_x\n\ndef pred_fn(x, A): \n    return sigmoid(A @ x)\n\ndef loss_fn(x, A, b): \n    return np.mean(-b*np.log(pred_fn(x, A)) - (1 - b)*np.log(1 - pred_fn(x, A)))\n\ndef grad_fn(x, A, b):\n    return -A.T @ (b - pred_fn(x, A))/len(b)\n\ndef stepsize_for_logreg(A, b):\n    L = LA.norm(A)**2 / (4 * len(b))\n    return 1/L \n```", "```py\ndata = pd.read_csv('lebron.csv')\ndata.head() \n```", "```py\ndata.describe() \n```", "```py\nfeature = data['shot_distance']\nlabel = data['shot_made']\n\nplt.scatter(feature, label, s=10, color='b', alpha=0.2)\nplt.show() \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\n\nlabel_jitter = label + 0.05*rng.normal(0,1,len(label))\n\nplt.scatter(feature, label_jitter, s=10, color='b', alpha=0.2)\nplt.show() \n```", "```py\nA = np.stack((np.ones(len(label)),feature),axis=-1)\nb = label \n```", "```py\nstepsize = stepsize_for_logreg(A, b)\nprint(stepsize) \n```", "```py\n0.017671625306319678 \n```", "```py\ninit_x = np.zeros(A.shape[1])\nbest_x = gd_for_logreg(loss_fn, grad_fn, A, b, init_x, beta=stepsize)\nprint(best_x) \n```", "```py\n[ 0.90959003 -0.05890828] \n```", "```py\ngrid = np.linspace(np.min(feature), np.max(feature), 100)\nfeature_grid = np.stack((np.ones(len(grid)),grid),axis=-1)\npredict_grid = sigmoid(feature_grid @ best_x)\n\nplt.scatter(feature, label_jitter, s=10, color='b', alpha=0.2)\nplt.plot(grid,predict_grid,'r')\nplt.show() \n```", "```py\ndef fhat(x,a):\n    return 1 / ( 1 + np.exp(-np.outer(x,a)) )\n\ndef loss(x,a,b): \n    return np.mean(-b*np.log(fhat(x,a)) - (1 - b)*np.log(1 - fhat(x,a)), axis=1)\n\ndef grad(x,a,b):\n    return -np.mean((b - fhat(x,a))*a, axis=1) \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\n\nn = 10000\na, b = 2*rng.uniform(0,1,n) - 1, rng.integers(2, size=n) \n```", "```py\nx0 = -0.3\nx = np.linspace(x0-0.05,x0+0.05,100)\nupper = loss(x0,a,b) + (x - x0)*grad(x0,a,b) + (1/2)*(x - x0)**2\nlower = loss(x0,a,b) + (x - x0)*grad(x0,a,b) - (1/2)*(x - x0)**2\n\nplt.plot(x, loss(x,a,b), label='loss')\nplt.plot(x, upper, label='upper')\nplt.plot(x, lower, label='lower')\nplt.legend()\nplt.show() \n```", "```py\ndef sigmoid(z): \n    return 1/(1+np.exp(-z))\n\ndef desc_update_for_logreg(grad_fn, A, b, curr_x, beta):\n    gradient = grad_fn(curr_x, A, b)\n    return curr_x - beta*gradient\n\ndef gd_for_logreg(loss_fn, grad_fn, A, b, init_x, beta=1e-3, niters=int(1e5)):\n    curr_x = init_x\n\n    for iter in range(niters):\n        curr_x = desc_update_for_logreg(grad_fn, A, b, curr_x, beta)\n\n    return curr_x\n\ndef pred_fn(x, A): \n    return sigmoid(A @ x)\n\ndef loss_fn(x, A, b): \n    return np.mean(-b*np.log(pred_fn(x, A)) - (1 - b)*np.log(1 - pred_fn(x, A)))\n\ndef grad_fn(x, A, b):\n    return -A.T @ (b - pred_fn(x, A))/len(b)\n\ndef stepsize_for_logreg(A, b):\n    L = LA.norm(A)**2 / (4 * len(b))\n    return 1/L \n```", "```py\ndata = pd.read_csv('lebron.csv')\ndata.head() \n```", "```py\ndata.describe() \n```", "```py\nfeature = data['shot_distance']\nlabel = data['shot_made']\n\nplt.scatter(feature, label, s=10, color='b', alpha=0.2)\nplt.show() \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\n\nlabel_jitter = label + 0.05*rng.normal(0,1,len(label))\n\nplt.scatter(feature, label_jitter, s=10, color='b', alpha=0.2)\nplt.show() \n```", "```py\nA = np.stack((np.ones(len(label)),feature),axis=-1)\nb = label \n```", "```py\nstepsize = stepsize_for_logreg(A, b)\nprint(stepsize) \n```", "```py\n0.017671625306319678 \n```", "```py\ninit_x = np.zeros(A.shape[1])\nbest_x = gd_for_logreg(loss_fn, grad_fn, A, b, init_x, beta=stepsize)\nprint(best_x) \n```", "```py\n[ 0.90959003 -0.05890828] \n```", "```py\ngrid = np.linspace(np.min(feature), np.max(feature), 100)\nfeature_grid = np.stack((np.ones(len(grid)),grid),axis=-1)\npredict_grid = sigmoid(feature_grid @ best_x)\n\nplt.scatter(feature, label_jitter, s=10, color='b', alpha=0.2)\nplt.plot(grid,predict_grid,'r')\nplt.show() \n```"]