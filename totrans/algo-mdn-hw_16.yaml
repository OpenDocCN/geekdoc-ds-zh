- en: Throughput Computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://en.algorithmica.org/hpc/pipelining/throughput/](https://en.algorithmica.org/hpc/pipelining/throughput/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Optimizing for *latency* is usually quite different from optimizing for *throughput*:'
  prefs: []
  type: TYPE_NORMAL
- en: When optimizing data structure queries or small one-time or branchy algorithms,
    you need to [look up the latencies](../tables) of its instructions, mentally construct
    the execution graph of the computation, and then try to reorganize it so that
    the critical path is shorter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When optimizing hot loops and large-dataset algorithms, you need to look up
    the throughputs of their instructions, count how many times each one is used per
    iteration, determine which of them is the bottleneck, and then try to restructure
    the loop so that it is used less often.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last advice only works for *data-parallel* loops, where each iteration is
    fully independent of the previous one. When there is some interdependency between
    consecutive iterations, there may potentially be a pipeline stall caused by a
    [data hazard](../hazards) as the next iteration is waiting for the previous one
    to complete.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/pipelining/throughput/#example)Example'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a simple example, consider how the sum of an array is computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s assume for a moment that the compiler doesn’t [vectorize](/hpc/simd)
    this loop, [the memory bandwidth](/hpc/cpu-cache/bandwidth) isn’t a concern, and
    that the loop is [unrolled](/hpc/architecture/loops) so that we don’t pay any
    additional cost associated with maintaining the loop variables. In this case,
    the computation becomes very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How fast can we compute this? At exactly one cycle per element — because we
    need one cycle each iteration to `add` another value to `s`. The latency of the
    memory read doesn’t matter because the CPU can start it ahead of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we can go higher than that. The *throughput* of `add`^([1](#fn:1)) is 2
    on my CPU (Zen 2), meaning we could theoretically execute two of them every cycle.
    But right now this isn’t possible: while `s` is being used to accumulate $i$-th
    element, it can’t be used for $(i+1)$-th for at least one cycle.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution is to use *two* accumulators and just sum up odd and and even
    elements separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now our superscalar CPU can execute these two “threads” simultaneously, and
    our computation no longer has any critical paths that limit the throughput.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/pipelining/throughput/#the-general-case)The
    General Case'
  prefs: []
  type: TYPE_NORMAL
- en: If an instruction has a latency of $x$ and a throughput of $y$, then you would
    need to use $x \cdot y$ accumulators to saturate it. This also implies that you
    need $x \cdot y$ logical registers to hold their values, which is an important
    consideration for CPU designs, limiting the maximum number of usable execution
    units for high-latency instructions.
  prefs: []
  type: TYPE_NORMAL
- en: This technique is mostly used with [SIMD](/hpc/simd) and not in scalar code.
    You can [generalize](/hpc/simd/reduction) the code above and compute sums and
    other reductions faster than the compiler.
  prefs: []
  type: TYPE_NORMAL
- en: In general, when optimizing loops, you usually have just one or a few *execution
    ports* that you want to utilize to their fullest, and you engineer the rest of
    the loop around them. As different instructions may use different sets of ports,
    it is not always clear which one is going to be overused. In situations like this,
    [machine code analyzers](/hpc/profiling/mca) can be very helpful for finding the
    bottlenecks of small assembly loops.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The throughput of register-register `add` is 4, but since we are reading its
    second operand from memory, it is bottlenecked by the throughput of memory `mov`,
    which is 2 on Zen 2. [↩︎](#fnref:1) [← Instruction Tables](https://en.algorithmica.org/hpc/pipelining/tables/)[../Compilation
    →](https://en.algorithmica.org/hpc/compilation/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
