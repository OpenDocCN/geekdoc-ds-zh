- en: '21'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RANDOMIZED TRIALS AND HYPOTHESIS CHECKING
  prefs: []
  type: TYPE_NORMAL
- en: Dr. X invented a drug, PED-X, designed to help professional bicycle racers ride
    faster. When he tried to market it, the racers insisted that Dr. X demonstrate
    that his drug was superior to PED-Y, the banned drug that they had been using
    for years. Dr. X raised money from some investors and launched a **randomized
    trial**.
  prefs: []
  type: TYPE_NORMAL
- en: 'He persuaded `200` professional cyclists to participate in his trial. He then
    divided them randomly into two groups: treatment and control. Each member of the
    **treatment group** received a dose of PED-X. Members of the **control group**
    were told that they were being given a dose of PED-X, but were instead given a
    dose of PED-Y.'
  prefs: []
  type: TYPE_NORMAL
- en: Each cyclist was asked to bike `50` miles as fast as possible. The finishing
    times for each group were normally distributed. The mean finishing time of the
    treatment group was `118.61` minutes, and that of the control group was `120.62`
    minutes. [Figure 21-1](#c21-fig-0001) shows the time for each cyclist.
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0001.jpg](../images/c21-fig-0001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-1](#c21-fig-0001a) Finishing times for cyclists'
  prefs: []
  type: TYPE_NORMAL
- en: Dr. X was elated until he ran into a statistician who pointed out that it was
    almost inevitable that one of the two groups would have a lower mean than the
    other, and perhaps the difference in means was merely a random occurrence. When
    she saw the crestfallen look on the scientist's face, the statistician offered
    to show him how to check the statistical significance of his study.
  prefs: []
  type: TYPE_NORMAL
- en: 21.1 Checking Significance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In any experiment that involves drawing samples at random from a population,
    there is always the possibility that an observed effect occurred purely by chance.
    [Figure 21-2](#c21-fig-0002) is a visualization of how temperatures in January
    of 2020 varied from the average temperatures in January from 1981 to 2010\. Now,
    imagine that you constructed a sample by choosing 20 random spots on the planet,
    and then discovered that the mean change in temperature for the sample was `+1`
    degree Celsius. What is the probability that the observed change in mean temperature
    was an artifact of the sites you happened to sample rather than an indication
    that the planet as a whole is warming? Answering this kind of question is what
    **statistical significance** is all about.
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0002.jpg](../images/c21-fig-0002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-2](#c21-fig-0002a) January 2020 temperature difference from the
    1981-2010 average[^(145)](#c21-fn-0001)'
  prefs: []
  type: TYPE_NORMAL
- en: In the early part of the twentieth century, Ronald Fisher developed an approach
    to statistical **hypothesis testing** that has become the most common approach
    for evaluating the probability an observed effect occurred purely by chance. Fisher
    says he invented the method in response to a claim by Dr. Muriel Bristol-Roach
    that when she drank tea with milk in it, she could detect whether the tea or the
    milk was poured into the teacup first. Fisher challenged her to a “tea test” in
    which she was given eight cups of tea (four for each order of adding tea and milk),
    and asked to identify those cups into which the tea had been poured before the
    milk. She did this perfectly. Fisher then calculated the probability of her having
    done this purely by chance. As we saw in Section 17.4.4, ![c21-fig-5001.jpg](../images/c21-fig-5001.jpg),
    i.e., there are `70` ways to choose `4` cups out of `8`. Since only one of these
    `70` combinations includes all `4` cups in which the tea was poured first, Fisher
    calculated that the probability of Dr. Bristol-Roach having chosen correctly by
    pure luck was ![c21-fig-5002.jpg](../images/c21-fig-5002.jpg). From this, he concluded
    that it was highly unlikely her success could be attributed to luck.
  prefs: []
  type: TYPE_NORMAL
- en: Fisher's approach to significance testing can be summarized as
  prefs: []
  type: TYPE_NORMAL
- en: 1\. State a **null hypothesis** and an **alternative hypothesis**. The null
    hypothesis is that the “treatment” has no interesting effect. For the “tea test,”
    the null hypothesis was that Dr. Bristol-Roach could not taste the difference.
    The alternative hypothesis is a hypothesis that can be true only if the null hypothesis
    is false, e.g., that Dr. Bristol-Roach could taste the difference.[^(146)](#c21-fn-0002)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Understand statistical assumptions about the sample being evaluated. For
    the “tea test” Fisher assumed that Dr. Bristol-Roach was making independent decisions
    for each cup.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. Compute a relevant **test statistic**. In this case, the test statistic
    was the fraction of correct answers given by Dr. Bristol-Roach.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4\. Derive the probability of that test statistic under the null hypothesis.
    In this case, it is the probability of getting all of the cups right by accident,
    i.e., `0.014`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5\. Decide whether that probability is sufficiently small that you are willing
    to assume the null hypothesis is false, i.e., to **reject** the null hypothesis.
    Common values for the rejection level, which should be chosen in advance, are
    `0.05` and `0.01`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Returning to our cyclists, imagine that the times for the treatment and control
    groups were samples drawn from infinite populations of finishing times for PED-X
    users and PED-Y users. The null hypothesis for this experiment is that the means
    of those two larger populations are the same, i.e., the difference between the
    population mean of the treatment group and the population mean of the control
    group is 0\. The alternative hypothesis is that they are not the same, i.e., the
    difference in means is not equal to 0.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we go about trying to reject the null hypothesis. We choose a threshold,
    α, for statistical significance, and try to show that the probability of the data
    having been drawn from distributions consistent with the null hypothesis is less
    than α. We then say that we can reject the null hypothesis with confidence α,
    and accept the negation of the null hypothesis with probability `1 –` α.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of α affects the kind of errors we make. The larger α, the more often
    we will reject a null hypothesis that is actually true. These are known as **type
    I errors**. When α is smaller, we will more often accept a null hypothesis that
    is actually false. These are known as **type II errors**.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, people choose α = `0.05`. However, depending upon the consequences
    of being wrong, it might be preferable to choose a smaller or larger α. Imagine,
    for example, that the null hypothesis is that there is no difference in the rate
    of premature death between those taking PED-X and those taking PED-Y. We might
    well want to choose a small α, say `0.001`, as the basis for rejecting that hypothesis
    before deciding whether one drug was safer than the other. On the other hand,
    if the null hypothesis were that there is no difference in the performance-enhancing
    effect of PED-X and PED-Y, we might comfortably choose a pretty large α.[^(147)](#c21-fn-0003)
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to compute the test statistic. The most common test statistic
    is the **t-statistic**. The t-statistic tells us how different, measured in units
    of standard error, the estimate derived from the data is from the null hypothesis.
    The larger the t-statistic, the more likely the null hypothesis can be rejected.
    For our example, the t-statistic tells us how many standard errors the difference
    in the two means (`118.44 – 119.82 = -1.38`) is from `0`. The t-statistic for
    our PED-X example is `-2.11` (you'll see how to compute this shortly). What does
    this mean? How do we use it?
  prefs: []
  type: TYPE_NORMAL
- en: We use the t-statistic in much the same way we use the number of standard deviations
    from the mean to compute confidence intervals (see Section 17.4.2). Recall that
    for all normal distributions, the probability of an example lying within a fixed
    number of standard deviations of the mean is fixed. Here we do something slightly
    more complex that accounts for the number of samples used to compute the standard
    error. Instead of assuming a normal distribution, we assume a **t-distribution**.
  prefs: []
  type: TYPE_NORMAL
- en: T-distributions were first described, in 1908, by William Gosset, a statistician
    working for the Arthur Guinness and Son brewery.[^(148)](#c21-fn-0004) The t-distribution
    is actually a family of distributions, since the shape of the distribution depends
    upon the degrees of freedom in the sample.
  prefs: []
  type: TYPE_NORMAL
- en: The **degrees of freedom** describes the amount of independent information used
    to derive the t-statistic. In general, we can think of degrees of freedom as the
    number of independent observations in a sample that are available to estimate
    some statistic about the population from which that sample is drawn.
  prefs: []
  type: TYPE_NORMAL
- en: A t-distribution resembles a normal distribution, and the larger the degrees
    of freedom, the closer it is to a normal distribution. For small degrees of freedom,
    the t-distributions have notably fatter tails than normal distributions. For degrees
    of freedom of 30 or more, t-distributions are very close to normal.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's use the sample variance to estimate the population variance. Consider
    a sample containing the three examples 100, 200, and 300\. Recall that
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-5003.jpg](../images/c21-fig-5003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: so the variance of our sample is
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-5004.jpg](../images/c21-fig-5004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: It might appear that we are using three independent pieces of information, but
    we are not. The three terms in the numerator are not independent of each other,
    because all three observations were used to compute the mean of the sample of
    `200` riders. The degrees of freedom is `2`, since if we know the mean and any
    two of the three observations, the value of the third observation is fixed.
  prefs: []
  type: TYPE_NORMAL
- en: The larger the degrees of freedom, the higher the probability that the sample
    statistic is representative of the population. The degrees of freedom in a t-statistic
    computed from a single sample is one less than the sample size, because the mean
    of the sample is used in calculating the t-statistic. If two samples are used,
    the degrees of freedom is two less than the sum of the sample sizes, because the
    mean of each sample is used in calculating the t-statistic. For example, for the
    PED-X/PED-Y experiment, the degrees of freedom is `198`.
  prefs: []
  type: TYPE_NORMAL
- en: Given the degrees of freedom, we can draw a plot showing the appropriate t-distribution,
    and then see where the t-statistic we have computed for our PED-X example lies
    on the distribution. The code in [Figure 21-3](#c21-fig-0005) does that, and produces
    the plot in [Figure 21-4](#c21-fig-0006). The code first uses the function `scipy.random.standard_t`
    to generate many examples drawn from a t-distribution with `198` degrees of freedom.
    It then draws white lines at the t-statistic and the negative of the t-statistic
    for the PED-X sample.
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0003.jpg](../images/c21-fig-0003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-3](#c21-fig-0005a) Plotting a t-distribution'
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0004.jpg](../images/c21-fig-0004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-4](#c21-fig-0006a) Visualizing the t-statistic'
  prefs: []
  type: TYPE_NORMAL
- en: The sum of the fractions of the area of the histogram to the left and right
    of the white lines equals the probability of getting a value at least as extreme
    as the observed value if
  prefs: []
  type: TYPE_NORMAL
- en: the sample is representative of the population, and
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: the null hypothesis is true.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to look at both tails because our null hypothesis is that the population
    means are equal. So, the test should fail if the mean of the treatment group is
    either statistically significantly larger or smaller than the mean of the control
    group.
  prefs: []
  type: TYPE_NORMAL
- en: Under the assumption that the null hypothesis holds, the probability of getting
    a value at least as extreme as the observed value is called a **p-value**. For
    our PED-X example, the p-value is the probability of seeing a difference in the
    means at least as large as the observed difference, under the assumption that
    the actual population means of the treatment and controls are identical.
  prefs: []
  type: TYPE_NORMAL
- en: It may seem odd that p-values tell us something about the probability of an
    event occurring if the null hypothesis holds, when what we are usually hoping
    is that the null hypothesis doesn't hold. However, it is not so different in character
    from the classic **scientific method**, which is based upon designing experiments
    that have the potential to refute a hypothesis. The code in [Figure 21-5](#c21-fig-0007)
    computes and prints the t-statistic and p-value for our two samples, one containing
    the times of the control group and the other the times of the treatment group.
    The library function `scipy.stats.ttest_ind` performs a two-tailed two-sample
    **t-test** and returns both the t-statistic and the p-value. Setting the parameter
    `equal_var` to `False` indicates that we don't know whether the two populations
    have the same variance.
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0005.jpg](../images/c21-fig-0005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-5](#c21-fig-0007a) Compute and print t-statistic and p-value'
  prefs: []
  type: TYPE_NORMAL
- en: When we run the code, it reports
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: “Yes,” Dr. X crowed, “it seems that the probability of PED-X being no better
    than PED-Y is only `4%`, and therefore the probability that PED-X has an effect
    is 96`%`. Let the cash registers start ringing.” Alas, his elation lasted only
    until he read the next section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 21.2 Beware of P-values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is way too easy to read something into a p-value that it doesn't really imply.
    It is tempting to think of a p-value as the probability of the null hypothesis
    being true. But this is not what it actually is.
  prefs: []
  type: TYPE_NORMAL
- en: The null hypothesis is analogous to a defendant in the Anglo-American criminal
    justice system. That system is based on a principle called “presumption of innocence,”
    i.e., innocent until proven guilty. Analogously, we assume that the null hypothesis
    is true unless we see enough evidence to the contrary. In a trial, a jury can
    rule that a defendant is “guilty” or “not guilty.” A “not guilty” verdict implies
    that the evidence was insufficient to convince the jury that the defendant was
    guilty “beyond a reasonable doubt.” [^(149)](#c21-fn-0005) Think of it as equivalent
    to “guilt was not proven.” A verdict of “not guilty” does not imply that the evidence
    was sufficient to convince the jury that the defendant was innocent. And it says
    nothing about what the jury would have concluded had it seen different evidence.
    Think of a p-value as a jury verdict where the standard “beyond a reasonable doubt”
    is defined by α, and the evidence is the data from which the t-statistic was constructed.
  prefs: []
  type: TYPE_NORMAL
- en: A small p-value indicates that a particular sample is unlikely if the null hypothesis
    is true. It is akin to a jury concluding that it was unlikely that it would have
    been presented with this set of evidence if the defendant were innocent, and therefore
    reaching a guilty verdict. Of course, that doesn't mean that the defendant is
    actually guilty. Perhaps the jury was presented with misleading evidence. Analogously,
    a low p-value might be attributable to the null hypothesis actually being false,
    or it could simply be that the sample is unrepresentative of the population from
    which it is drawn, i.e., the evidence is misleading.
  prefs: []
  type: TYPE_NORMAL
- en: As you might expect, Dr. X staunchly claimed that his experiment showed that
    the null hypothesis was probably false. Dr. Y insisted that the low p-value was
    probably attributable to an unrepresentative sample and funded another experiment
    of the same size as Dr. X's. When the statistics were computed using the samples
    from her experiment, the code printed
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This p-value is more than 17 times larger than that obtained from Dr. X's experiment,
    and certainly provides no reason to doubt the null hypothesis. Confusion reigned.
    But we can clear it up!
  prefs: []
  type: TYPE_NORMAL
- en: You may not be surprised to discover that this is not a true story—after all,
    the idea of a cyclist taking a performance-enhancing drug strains credulity. In
    fact, the samples for the experiments were generated by the code in [Figure 21-6](#c21-fig-0008).
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0006.jpg](../images/c21-fig-0006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-6](#c21-fig-0008a) Code for generating racing examples'
  prefs: []
  type: TYPE_NORMAL
- en: Since the experiment is purely computational, we can run it many times to get
    many different samples. When we generated `10,000` pairs of samples (one from
    each distribution) and plotted the probability of the p-values, we got the plot
    in [Figure 21-7](#c21-fig-0009).
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0007.jpg](../images/c21-fig-0007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-7](#c21-fig-0009a) Probability of p-values'
  prefs: []
  type: TYPE_NORMAL
- en: Since about 10`%` of the p-values lie below `0.04`, it is not terribly surprising
    that an experiment happened to show significance at the `4%` level. On the other
    hand, that the second experiment yielded a completely different result is also
    not surprising. What does seem surprising is that given we know that the means
    of the two distributions are actually different, we get a result that is significant
    at the `5%` level only about `12%` of the time. Roughly 88`%` of the time we would
    fail to reject a fallacious null hypothesis at the `5%` level.
  prefs: []
  type: TYPE_NORMAL
- en: That p-values can be unreliable indicators of whether it is truly appropriate
    to reject a null hypothesis is one of the reasons so many of the results appearing
    in the scientific literature cannot be reproduced by other scientists. One problem
    is that there is a strong relationship between the **study power** (the size of
    the samples) and the credibility of the statistical finding.[^(150)](#c21-fn-0006)
    If we increase the sample size in our example to `3000`, we fail to reject the
    fallacious null hypothesis only about `1%` of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Why are so many studies under-powered? If we were truly running an experiment
    with people (rather than a simulation), it would be 20 times more expensive to
    draw samples of size `2000` than samples of size `100`.
  prefs: []
  type: TYPE_NORMAL
- en: The problem of sample size is an intrinsic attribute of what is called the frequentist
    approach to statistics. In Section 21.7, we discuss an alternative approach that
    attempts to mitigate this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 21.3 One-tail and One-sample Tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thus far in this chapter, we have looked only at two-tailed two-sample tests.
    Sometimes, it is more appropriate to use a **one-tailed** and/or a **one-sample**
    t-test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first consider a one-tailed two-sample test. In our two-tailed test
    of the relative effectiveness of PED-X and PED-Y, we considered three cases: 1)
    they were equally effective, 2) PED-X was more effective than PED-Y, and 3) PED-Y
    was more effective than PED-X. The goal was to reject the null hypothesis (case 1)
    by arguing that if it were true, it would be unlikely to see as large a difference
    as observed in the means of the PED-X and PED-Y samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose, however, that PED-X were substantially less expensive than PED-Y. To
    find a market for his compound, Dr. X would only need to show that PED-X is at
    least as effective as PED-Y. One way to think about this is that we want to reject
    the hypothesis that the means are equal or that the PED-X mean is larger. Note
    that this is strictly weaker than the hypothesis that the means are equal. (Hypothesis
    `A` is strictly weaker than hypothesis `B`, if whenever `B` is true `A` is true,
    but not vice versa.)
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we start with a two-sample test with the original null hypothesis
    computed by the code in [Figure 21-5](#c21-fig-0007). It printed
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: allowing us to reject the null hypothesis at about the 4`%` level.
  prefs: []
  type: TYPE_NORMAL
- en: How about our weaker hypothesis? Recall [Figure 21-4](#c21-fig-0006). We observed
    that under the assumption that the null hypothesis holds, the sum of the fractions
    of the areas of the histogram to the left and right of the white lines equals
    the probability of getting a value at least as extreme as the observed value.
    However, to reject our weaker hypothesis we don't need to take into account the
    area under the left tail, because that corresponds to PED-X being more effective
    than PED-Y (a negative time difference), and we're interested only in rejecting
    the hypothesis that PED-X is less effective. That is, we can do a one-tailed test.
  prefs: []
  type: TYPE_NORMAL
- en: Since the t-distribution is symmetric, to get the value for a one-tailed test
    we divide the p-value from the two-tailed test in half. So the p-value for the
    one-tailed test is `0.02`. This allows us to reject our weaker hypothesis at the
    `2%` level, something that we could not do using the two-tailed test.
  prefs: []
  type: TYPE_NORMAL
- en: Because a one-tailed test provides more power to detect an effect, it is tempting
    to use a one-tailed test whenever one has a hypothesis about the direction of
    an effect. This is usually not a good idea. A one-tailed test is appropriate only
    if the consequences of missing an effect in the untested direction are negligible.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at a one-sample test. Suppose that, after years of experience
    of people using PED-Y, it was well established that the mean time for a racer
    on PED-Y to complete a 50-mile course is `120` minutes. To discover whether PED-X
    had a different effect than PED-Y, we would test the null hypothesis that the
    mean time for a single PED-X sample is equal to `120`. We can do this using the
    function `scipy.stats.ttest_1samp`, which takes as arguments a single sample and
    the population mean against which it is compared. It returns a tuple containing
    the t-statistic and p-value. For example, if we append to the end of the code
    in [Figure 21-5](#c21-fig-0007) the code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: it prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: It is not surprising that the p-value is smaller than the one we got using the
    two-sample two-tail test. By assuming that we know one of the two means, we have
    removed a source of uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: So, after all this, what have we learned from our statistical analysis of PED-X
    and PED-Y? Even though there is a difference in the expected performance of PED-X
    and PED-Y users, no finite sample of PED-X and PED-Y users is guaranteed to reveal
    that difference. Moreover, because the difference in the expected means is small
    (less than half a percent), it is unlikely that an experiment of the size Dr.
    X ran (`100` riders in each group) will yield evidence that would allow us to
    conclude at the `95%` confidence level that there is a difference in means. We
    could increase the likelihood of getting a result that is statistically significant
    at the `95%` level by using a one-tailed test, but that would be misleading, because
    we have no reason to assume that PED-X is not less effective than PED-Y.
  prefs: []
  type: TYPE_NORMAL
- en: 21.4  Significant or Not?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lyndsay and John have wasted an inordinate amount of time over the last several
    years playing a game called Words with Friends. They have played each other 1,273
    times, and Lyndsay has won 666 of those games, prompting her to boast, “I'm way
    better at this game than you are.” John asserted that Lyndsay's claim was nonsense,
    and that the difference in wins could be (and probably should be) attributed entirely
    to luck.
  prefs: []
  type: TYPE_NORMAL
- en: 'John, who had recently read a book about statistics, proposed the following
    way to find out whether it was reasonable to attribute Lyndsay''s relative success
    to skill:'
  prefs: []
  type: TYPE_NORMAL
- en: Treat each of the `1,273` games as an experiment returning `1` if Lyndsay was
    the victor and `0` if she was not.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the null hypothesis that the mean value of those experiments is `0.5`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform a two-tailed one-sample test for that null hypothesis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When he ran the code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: it printed
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: prompting John to claim that the difference wasn't even close to being significant
    at the `5%` level.
  prefs: []
  type: TYPE_NORMAL
- en: Lyndsay, who had not studied statistics, but had read Chapter 18 of this book,
    was not satisfied. “Let's run a Monte Carlo simulation,” she suggested, and supplied
    the code in [Figure 21-8](#c21-fig-0010).
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0008.jpg](../images/c21-fig-0008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-8](#c21-fig-0010a) Lyndsay''s simulation of games'
  prefs: []
  type: TYPE_NORMAL
- en: When Lyndsay's code was run it printed,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: prompting her to claim that John's statistical test was completely bogus and
    that the difference in wins was statistically significant at the `5%` level.
  prefs: []
  type: TYPE_NORMAL
- en: “No,” John explained patiently, “it's your simulation that's bogus. It assumed
    that you were the better player and performed the equivalent of a one-tailed test.
    The inner loop of your simulation is wrong. You should have performed the equivalent
    of a two-tailed test by testing whether, in the simulation, either player won
    more than the `666` games that you won in actual competition.” John then ran the
    simulation in [Figure 21-9](#c21-fig-0011).
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0009.jpg](../images/c21-fig-0009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-9](#c21-fig-0011a) Correct simulation of games'
  prefs: []
  type: TYPE_NORMAL
- en: John's simulation printed
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: “That's pretty darned close to what my two-tailed test predicted,” crowed John.
    Lyndsay's unladylike response was not appropriate for inclusion in a family-oriented
    book.
  prefs: []
  type: TYPE_NORMAL
- en: '**Finger exercise**: An investigative reporter discovered that not only was
    Lyndsay employing dubious statistical methods, she was applying them to data she
    had merely made up.[^(151)](#c21-fn-0007) In fact, John had defeated Lyndsay 479
    times and lost 443 times. At what level is this difference statistically significant?'
  prefs: []
  type: TYPE_NORMAL
- en: 21.5 Which N?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A professor wondered whether attending lectures was correlated with grades in
    his department. He recruited `40` freshmen and gave them all ankle bracelets so
    that he could track their whereabouts. Half of the students were not allowed to
    attend any of the lectures in any of their classes,[^(152)](#c21-fn-0008) and
    half were required to attend all of the lectures.[^(153)](#c21-fn-0009) Over the
    next four years, each student took `40` different classes, yielding `800` grades
    for each group of students.
  prefs: []
  type: TYPE_NORMAL
- en: When the professor performed a two-tailed t-test on the means of these two samples
    of size `800`, the p-value was about `0.01`. This disappointed the professor,
    who was hoping that there would be no statistically significant effect—so that
    he would feel less guilty about canceling lectures and going to the beach. In
    desperation, he took a look at the mean GPAs of the two groups and discovered
    very little difference. How, he wondered, could such a small difference in means
    be significant at that level?
  prefs: []
  type: TYPE_NORMAL
- en: When the sample size is large enough, even a small effect can be highly statistically
    significant. `N` matters, a lot. [Figure 21-10](#c21-fig-0012) plots the mean
    p-value of `1000` trials against the size of the samples used in those trials.
    For each sample size and each trial we generated two samples. Each was drawn from
    a Gaussian with a standard deviation of `5`. One had a mean of `100` and the other
    a mean of `100.5`. The mean p-value drops linearly with the sample size. The `0.5%`
    difference in means becomes consistently statistically significant at the `5%`
    level when the sample size reaches about `2000`, and at the `1%` level when the
    sample size nears `3000`.
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0010.jpg](../images/c21-fig-0010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-10](#c21-fig-0012a) Impact of sample size on p-value'
  prefs: []
  type: TYPE_NORMAL
- en: Returning to our example, was the professor justified in using an `N` of `800`
    for each **arm** of his study? To put it another way, were there really `800`
    independent examples for each cohort of `20` students? Probably not. There were
    `800` grades per sample, but only `20` students, and the `40` grades associated
    with each student should probably not be viewed as independent examples. After
    all, some students consistently get good grades, and some students consistently
    get grades that disappoint.
  prefs: []
  type: TYPE_NORMAL
- en: The professor decided to look at the data a different way. He computed the GPA
    for each student. When he performed a two-tailed t-test on these two samples,
    each of size `20`, the p-value was about `0.3`. He headed to the beach.
  prefs: []
  type: TYPE_NORMAL
- en: 21.6 Multiple Hypotheses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Chapter 19, we looked at sampling using data from the Boston Marathon. The
    code in [Figure 21-11](#c21-fig-0013) reads in data from the 2012 race and looks
    for statistically significant differences in the mean finishing times of the women
    from a small set of countries. It uses the `get_BM_data` function defined in Figure
    19-2.
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0011.jpg](../images/c21-fig-0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-11](#c21-fig-0013a) Comparing mean finishing times for selected
    countries'
  prefs: []
  type: TYPE_NORMAL
- en: When the code is run, it prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It looks as if either Italy or Japan can claim to have faster women runners
    than the other.[^(154)](#c21-fn-0010) However, such a conclusion would be pretty
    tenuous. While one set of runners did have a faster mean time than the other,
    the sample sizes (`20` and `32`) were small and perhaps not representative of
    the capabilities of women marathoners in each country.
  prefs: []
  type: TYPE_NORMAL
- en: 'More important, there is a flaw in the way we constructed our experiment. We
    checked `10` null hypotheses (one for each distinct pair of countries) and discovered
    that one of them could be rejected at the `5%` level. One way to think about it
    is that we were actually checking the null hypothesis: “for all pairs of countries,
    the mean finishing times of their female marathon runners are the same.” It might
    be fine to reject that null hypothesis, but that is not the same as rejecting
    the null hypothesis that women marathon runners from Italy and Japan are equally
    fast.'
  prefs: []
  type: TYPE_NORMAL
- en: The point is made starkly by the example in [Figure 21-12](#c21-fig-0014). In
    that example, we draw 50 pairs of samples of size `200` from the same population,
    and for each we test whether the means of the samples are statistically different.
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0012.jpg](../images/c21-fig-0012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-12](#c21-fig-0014a) Checking multiple hypotheses'
  prefs: []
  type: TYPE_NORMAL
- en: Since the samples are all drawn from the same population, we know that the null
    hypothesis is true. Yet, when we run the code it prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: indicating that the null hypothesis can be rejected for two pairs.
  prefs: []
  type: TYPE_NORMAL
- en: This is not particularly surprising. Recall that a p-value of `0.05` indicates
    that if the null hypothesis holds, the probability of seeing a difference in means
    at least as large as the difference for the two samples is `0.05\.` Therefore,
    it is not surprising that if we examine 50 pairs of samples, two of them have
    means that are statistically significantly different from each other. Running
    large sets of related experiments, and then **cherry-picking** the result you
    like, can be kindly described as sloppy. An unkind person might call it something
    else.
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to our Boston Marathon experiment, we checked whether we could reject
    the null hypothesis (no difference in means) for `10` pairs of samples. When running
    an experiment involving multiple hypotheses, the simplest and most conservative
    approach is to use something called the **Bonferroni correction**. The intuition
    behind it is simple: when checking a family of `m` hypotheses, one way of maintaining
    an appropriate **family-wise error rate** is to test each individual hypothesis
    at a level of ![c21-fig-5005.jpg](../images/c21-fig-5005.jpg). Using the Bonferroni
    correction to see if the difference between Italy and Japan is significant at
    the *α* = `0.05` level, we should check if the p-value is less than `0.05/10`
    i.e., `0.005`—which it is not.'
  prefs: []
  type: TYPE_NORMAL
- en: The Bonferroni correction is conservative (i.e., it fails to reject the null
    hypothesis more often than necessary) if there are many tests or the test statistics
    for the tests are positively correlated. An additional issue is the absence of
    a generally accepted definition of “family of hypotheses.” It is obvious that
    the hypotheses generated by the code in [Figure 21-12](#c21-fig-0014) are related,
    and therefore a correction needs to be applied. But the situation is not always
    so clear cut.
  prefs: []
  type: TYPE_NORMAL
- en: 21.7 Conditional Probability and Bayesian Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up to this point, we have taken what is called a **frequentist** approach to
    statistics. We have drawn conclusions from samples based entirely on the frequency
    or proportion of the data. This is the most commonly used inference framework,
    and leads to the well-established methodologies of statistical hypothesis testing
    and confidence intervals covered earlier in this book. In principle, it has the
    advantage of being unbiased. Conclusions are reached solely on the basis of observed
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In some situations, however, an alternative approach to statistics, **Bayesian
    statistics**, is more appropriate. Consider the cartoon in [Figure 21-13](#c21-fig-0015).[^(155)](#c21-fn-0011)
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-0013.jpg](../images/c21-fig-0013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 21-13](#c21-fig-0015a) Has the sun exploded?'
  prefs: []
  type: TYPE_NORMAL
- en: 'What''s going on here? The frequentist knows that there are only two possibilities:
    the machine rolls a pair of sixes and is lying, or it doesn''t roll a pair of
    sixes and is telling the truth. Since the probability of not rolling a pair of
    sixes is `35/36` (`97.22%`), the frequentist concludes that the machine is probably
    telling the truth, and therefore the sun has probably exploded.[^(156)](#c21-fn-0012)'
  prefs: []
  type: TYPE_NORMAL
- en: The Bayesian uses additional information in building her probability model.
    She agrees it is unlikely that the machine rolls a pair of sixes; however, she
    argues that the probability of that happening needs to be compared to the *a priori*
    probability that the sun has not exploded. She concludes that the likelihood of
    the sun having not exploded is even higher than `97.22%,` and decides to bet that
    “the sun will come out tomorrow.”
  prefs: []
  type: TYPE_NORMAL
- en: 21.7.1 Conditional Probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key idea underlying Bayesian reasoning is **conditional probability**.
  prefs: []
  type: TYPE_NORMAL
- en: In our earlier discussion of probability, we relied on the assumption that events
    were independent. For example, we assumed that whether a coin flip came up heads
    or tails was unrelated to whether the previous flip came up heads or tails. This
    is convenient mathematically, but life doesn't always work that way. In many practical
    situations, independence is a bad assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the probability that a randomly chosen adult American is male and weighs
    over `197` pounds. The probability of being male is about `0.5` and the probability
    of weighing more than `197` pounds (the average weight in the U.S.[^(157)](#c21-fn-0013))
    is also about `0.5`.[^(158)](#c21-fn-0014) If these were independent events, the
    probability of the selected person being both male and weighing more than `197`
    pounds would be `0.25`. However, these events are not independent, since the average
    American male weighs about `30` pounds more than the average female. So, a better
    question to ask is 1) what is the probability of the selected person being a male,
    and 2) given that the selected person is a male, what is the probability of that
    person weighing more than `197` pounds? The notation of conditional probability
    makes it easy to say just that.
  prefs: []
  type: TYPE_NORMAL
- en: The notation `P(A|B)` stands for the probability of `A` being true under the
    assumption that `B` is true. It is often read as “the probability of `A`, given
    `B`.” Therefore, the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-5006.jpg](../images/c21-fig-5006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: expresses exactly the probability we are looking for. If `P(A)` and `P(B)` are
    independent, `P(A|B) = P(A)`. For the above example, `B` is male and `*A*` is
    weight `> 197`.
  prefs: []
  type: TYPE_NORMAL
- en: In general, if *P*(*B*) ≠ 0,
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-5007.jpg](../images/c21-fig-5007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Like conventional probabilities, conditional probabilities always lie between
    `0` and `1`. Furthermore, if *Ā* stands for *not A*, *P*(*A*|*B*) + *P*(*Ā*|*B*)
    = 1\. People often incorrectly assume that *P*(*A*|*B*) is equal to *P*(*B*|*A*).
    There is no reason to expect this to be true. For example, the value of *P*(*Male*|*Maltese*)
    is roughly `0.5`, but *P*(*Maltese*|*Male*) is about `0.000064`.[^(159)](#c21-fn-0015)
  prefs: []
  type: TYPE_NORMAL
- en: '**Finger exercise:** Estimate the probability that a randomly chosen American
    is both male and weighs more than `197` pounds. Assume that `50%` of the population
    is male, and that the weights of the male population are normally distributed
    with a mean of `210` pounds and a standard deviation of `30` pounds. (Hint: think
    about using the empirical rule.)'
  prefs: []
  type: TYPE_NORMAL
- en: The formula *P*(*A*|*B*, *C*) stands for the probability of *A*, given that
    both *B* and *C* hold. Assuming that *B* and *C* are independent of each other,
    the definition of a conditional probability and the multiplication rule for independent
    probabilities imply that
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-5008.jpg](../images/c21-fig-5008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where the formula *P*(*A*, *B*, *C*) stands for the probability of all of *A*,
    *B*, and *C* being true.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, *P*(*A*, *B*|*C*) stands for the probability of *A and B*, given
    *C*. Assuming that *A* and *B* are independent of each other
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-5009.jpg](../images/c21-fig-5009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 21.7.2 Bayes’ Theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose that an asymptomatic woman in her forties goes for a mammogram and
    receives bad news: the mammogram is “positive.” [^(160)](#c21-fn-0016)'
  prefs: []
  type: TYPE_NORMAL
- en: The probability that a woman who has breast cancer will get a **true positive**
    result on a mammogram is `0.9`. The probability that a woman who does not have
    breast cancer will get a **false positive** on a mammogram is `0.07`.
  prefs: []
  type: TYPE_NORMAL
- en: We can use conditional probabilities to express these facts. Let
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Using these variables, we write the conditional probabilities
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Given these conditional probabilities, how worried should a woman in her forties
    with a positive mammogram be? What is the probability that she actually has breast
    cancer? Is it `0.93`, since the false positive rate is `7%`? More? less?
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s a trick question: We haven''t supplied enough information to allow you
    to answer the question in a sensible way. To do that, you need to know the **prior
    probabilities** for breast cancer for a woman in her forties. The fraction of
    women in their forties who have breast cancer is `0.008 (8` out of `1000).` The
    fraction who do not have breast cancer is therefore `1 – 0.008 = 0.992`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have all the information we need to address the question of how worried
    that woman in her forties should be. To compute the probability that she has breast
    cancer we use something called **Bayes’ Theorem**[^(161)](#c21-fn-0017) (often
    called Bayes’ Law or Bayes’ Rule) :'
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-5010.jpg](../images/c21-fig-5010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the Bayesian world, probability measures a **degree of belief**. Bayes' theorem
    links the degree of belief in a proposition before and after accounting for evidence.
    The formula to the left of the equal sign, `P(A|B),` is the **posterior** probability,
    the degree of belief in `A,` having accounted for `B`. The posterior is defined
    in terms of the **prior**, `P(A),` and the **support** that the evidence, `B`,
    provides for `A`. The support is the ratio of the probability of `B` holding if
    `A` holds and the probability of `B` holding independently of `A`, i.e., ![c21-fig-5011.jpg](../images/c21-fig-5011.jpg).
  prefs: []
  type: TYPE_NORMAL
- en: If we use Bayes’ Theorem to estimate the probability of the woman actually having
    breast cancer, we get (where `canc` plays the role of `A`, and `pos` the role
    of `B` in our statement of Bayes’ Theorem)
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-5012.jpg](../images/c21-fig-5012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The probability of having a positive test is
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-5013.jpg](../images/c21-fig-5013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: so
  prefs: []
  type: TYPE_NORMAL
- en: '![c21-fig-5014.jpg](../images/c21-fig-5014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That is, approximately `90%` of the positive mammograms are false positives![^(162)](#c21-fn-0018)
    Bayes’ Theorem helped us here because we had an accurate estimate of the prior
    probability of a woman in her forties having breast cancer.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that if we had started with an incorrect prior, incorporating that
    prior into our probability estimate would make the estimate worse rather than
    better. For example, if we had started with the prior
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: we would have concluded that the false positive rate was about `5%`, i.e., that
    the probability of a woman in her forties with a positive mammogram having breast
    cancer is roughly `0.95`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Finger exercise:** You are wandering through a forest and see a field of
    delicious-looking mushrooms. You fill your basket with them, and head home prepared
    to cook them up and serve them to your husband. Before you cook them, however,
    he demands that you consult a book about local mushroom species to check whether
    they are poisonous. The book says that 80% of the mushrooms in the local forest
    are poisonous. However, you compare your mushrooms to the ones pictured in the
    book, and decide that you are 95% certain that your mushrooms are safe. How comfortable
    should you be about serving them to your husband (assuming that you would rather
    not become a widow)?'
  prefs: []
  type: TYPE_NORMAL
- en: 21.8 Terms Introduced in Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: randomized trial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: treatment group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: control group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: statistical significance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hypothesis testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: null hypothesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: alternative hypothesis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: test statistic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hypothesis rejection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: type I error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: type II error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-statistic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: degrees of freedom
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: p-value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scientific method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: power of a study
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: two-tailed p-test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: one-tailed p-test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: arm of a study
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cherry-picking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bonferroni correction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: family-wise error rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: frequentist statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: conditional probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: true positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: false positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prior probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayes theorem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: degree of belief
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: posterior probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: prior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
