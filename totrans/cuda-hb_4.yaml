- en: Part II
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 5\. Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To maximize performance, CUDA uses different types of memory, depending on the
    expected usage. *Host memory* refers to the memory attached to the CPU(s) in the
    system. CUDA provides APIs that enable faster access to host memory by page-locking
    and mapping it for the GPU(s). *Device memory* is attached to the GPU and accessed
    by a dedicated memory controller, and, as every beginning CUDA developer knows,
    data must be copied explicitly between host and device memory in order to be processed
    by the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Device memory can be allocated and accessed in a variety of ways.
  prefs: []
  type: TYPE_NORMAL
- en: • *Global memory* may be allocated statically or dynamically and accessed via
    pointers in CUDA kernels, which translate to global load/store instructions.
  prefs: []
  type: TYPE_NORMAL
- en: • *Constant memory* is read-only memory accessed via different instructions
    that cause the read requests to be serviced by a cache hierarchy optimized for
    broadcast to multiple threads.
  prefs: []
  type: TYPE_NORMAL
- en: '• *Local memory* contains the stack: local variables that cannot be held in
    registers, parameters, and return addresses for subroutines.'
  prefs: []
  type: TYPE_NORMAL
- en: • *Texture memory* (in the form of CUDA arrays) is accessed via texture and
    surface load/store instructions. Like constant memory, read requests from texture
    memory are serviced by a separate cache that is optimized for read-only access.
  prefs: []
  type: TYPE_NORMAL
- en: '*Shared memory* is an important type of memory in CUDA that is *not* backed
    by device memory. Instead, it is an abstraction for an on-chip “scratchpad” memory
    that can be used for fast data interchange between threads within a block. Physically,
    shared memory comes in the form of built-in memory on the SM: On SM 1.x hardware,
    shared memory is implemented with a 16K RAM; on SM 2.x and more recent hardware,
    shared memory is implemented using a 64K cache that may be partitioned as 48K
    L1/16K shared, or 48K shared/16K L1.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Host Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In CUDA, *host memory* refers to memory accessible to the CPU(s) in the system.
    By default, this memory is *pageable*, meaning the operating system may move the
    memory or evict it out to disk. Because the physical location of pageable memory
    may change without notice, it cannot be accessed by peripherals like GPUs. To
    enable “direct memory access” (DMA) by hardware, operating systems allow host
    memory to be “page-locked,” and for performance reasons, CUDA includes APIs that
    make these operating system facilities available to application developers. So-called
    *pinned memory* that has been page-locked and mapped for direct access by CUDA
    GPU(s) enables
  prefs: []
  type: TYPE_NORMAL
- en: • Faster transfer performance
  prefs: []
  type: TYPE_NORMAL
- en: • Asynchronous memory copies (i.e., memory copies that return control to the
    caller before the memory copy necessarily has finished; the GPU does the copy
    in parallel with the CPU)
  prefs: []
  type: TYPE_NORMAL
- en: • Mapped pinned memory that can be accessed directly by CUDA kernels
  prefs: []
  type: TYPE_NORMAL
- en: Because the virtual→physical mapping for pageable memory can change unpredictably,
    GPUs cannot access pageable memory at all. CUDA copies pageable memory using a
    pair of *staging buffers* of pinned memory that are allocated by the driver when
    a CUDA context is allocated. [Chapter 6](ch06.html#ch06) includes hand-crafted
    pageable memcpy routines that use CUDA events to do the synchronization needed
    to manage this double-buffering.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1\. Allocating Pinned Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Pinned memory is allocated and freed using special functions provided by CUDA:
    `cudaHostAlloc()/cudaFreeHost()` for the CUDA runtime, and `cuMemHostAlloc()/cuMemFreeHost()`
    for the driver API. These functions work with the host operating system to allocate
    page-locked memory and map it for DMA by the GPU(s).'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA keeps track of memory it has allocated and transparently accelerates memory
    copies that involve host pointers allocated with `cuMemHostAlloc()/cudaHostAlloc()`.
    Additionally, some functions (notably the asynchronous memcpy functions) require
    pinned memory.
  prefs: []
  type: TYPE_NORMAL
- en: The `bandwidthTest` SDK sample enables developers to easily compare the performance
    of pinned memory versus normal pageable memory. The `--memory=pinned` option causes
    the test to use pinned memory instead of pageable memory. [Table 5.1](ch05.html#ch05tab01)
    lists the `bandwidthTest` numbers for a `cg1.4xlarge` instance in Amazon EC2,
    running Windows 7-x64 (numbers in MB/s). Because it involves a significant amount
    of work for the host, including a kernel transition, allocating pinned memory
    is expensive.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.1* Pinned versus Pageable Bandwidth'
  prefs: []
  type: TYPE_NORMAL
- en: 'CUDA 2.2 added several features to pinned memory. *Portable pinned memory*
    can be accessed by any GPU; *mapped pinned memory* is mapped into the CUDA address
    space for direct access by CUDA kernels; and *write-combined pinned memory* enables
    faster bus transfers on some systems. CUDA 4.0 added two important features that
    pertain to host memory: Existing host memory ranges can be page-locked in place
    using *host memory registration*, and Unified Virtual Addressing (UVA) enables
    all pointers to be unique process-wide, including host and device pointers. When
    UVA is in effect, the system can infer from the address range whether memory is
    device memory or host memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2\. Portable Pinned Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By default, pinned memory allocations are only accessible to the GPU that is
    current when `cudaHostAlloc()` or `cuMemHostAlloc()` is called. By specifying
    the `cudaHostAllocPortable` flag to `cudaHostAlloc()`, or the `CU_MEMHOSTALLOC_PORTABLE`
    flag to `cuHostMemAlloc()`, applications can request that the pinned allocation
    be mapped for *all* GPUs instead. Portable pinned allocations benefit from the
    transparent acceleration of memcpy described earlier and can participate in asynchronous
    memcpys for any GPU in the system. For applications that intend to use multiple
    GPUs, it is good practice to specify all pinned allocations as portable.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When UVA is in effect, all pinned memory allocations are portable.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3\. Mapped Pinned Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: By default, pinned memory allocations are mapped for the GPU outside the CUDA
    address space. They can be directly accessed by the GPU, but only through memcpy
    functions. CUDA kernels cannot read or write the host memory directly. On GPUs
    of SM 1.2 capability and higher, however, CUDA kernels are able to read and write
    host memory directly; they just need allocations to be mapped into the device
    memory address space.
  prefs: []
  type: TYPE_NORMAL
- en: To enable mapped pinned allocations, applications using the CUDA runtime must
    call `cudaSetDeviceFlags()` with the `cudaDeviceMapHost` flag before any initialization
    has been performed. Driver API applications specify the `CU_CTX_MAP_HOST` flag
    to `cuCtxCreate()`.
  prefs: []
  type: TYPE_NORMAL
- en: Once mapped pinned memory has been enabled, it may be allocated by calling `cudaHostAlloc()`
    with the `cudaHostAllocMapped` flag, or `cuMemHostAlloc()` with the `CU_MEMALLOCHOST_DEVICEMAP`
    flag. Unless UVA is in effect, the application then must query the device pointer
    corresponding to the allocation with `cudaHostGetDevicePointer()` or `cuMemHostGetDevicePointer()`.
    The resulting device pointer then can be passed to CUDA kernels. Best practices
    with mapped pinned memory are described in the section “[Mapped Pinned Memory
    Usage](ch05.html#ch05lev2sec7).”
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When UVA is in effect, all pinned memory allocations are mapped.^([1](ch05.html#ch05fn1))
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch05.html#ch05fn1a). Except those marked as write combining.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.4\. Write-Combined Pinned Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Write-combined memory, also known as write-combining or Uncacheable Write Combining
    (USWC) memory, was created to enable the CPU to write to GPU frame buffers quickly
    and without polluting the CPU cache.^([2](ch05.html#ch05fn2)) To that end, Intel
    added a new page table kind that steered writes into special write-combining buffers
    instead of the main processor cache hierarchy. Later, Intel also added “nontemporal”
    store instructions (e.g., `MOVNTPS` and `MOVNTI`) that enabled applications to
    steer writes into the write-combining buffers on a per-instruction basis. In general,
    memory fence instructions (such as `MFENCE`) are needed to maintain coherence
    with WC memory. These operations are not needed for CUDA applications because
    they are done automatically when the CUDA driver submits work to the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch05.html#ch05fn2a). WC memory originally was announced by Intel in 1997,
    at the same time as the Accelerated Graphics Port (AGP). AGP was used for graphics
    boards before PCI Express.'
  prefs: []
  type: TYPE_NORMAL
- en: For CUDA, write-combining memory can be requested by calling `cudaHostAlloc()`
    with the `cudaHostWriteCombined` flag, or `cuMemHostAlloc()` with the `CU_MEMHOSTALLOC_WRITECOMBINED`
    flag. Besides setting the page table entries to bypass the CPU caches, this memory
    also is not snooped during PCI Express bus transfers. On systems with front side
    buses (pre-Opteron and pre-Nehalem), avoiding the snoops improves PCI Express
    transfer performance. There is little, if any, performance advantage to WC memory
    on NUMA systems.
  prefs: []
  type: TYPE_NORMAL
- en: Reading WC memory with the CPU is very slow (about 6x slower), unless the reads
    are done with the `MOVNTDQA` instruction (new with SSE4). On NVIDIA’s integrated
    GPUs, write-combined memory is as fast as the system memory carveout—system memory
    that was set aside at boot time for use by the GPU and is not available to the
    CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the purported benefits, as of this writing, there is little reason for
    CUDA developers to use write-combined memory. It’s just too easy for a host memory
    pointer to WC memory to “leak” into some part of the application that would try
    to read the memory. In the absence of empirical evidence to the contrary, it should
    be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: When UVA is in effect, write-combined pinned allocations are *not* mapped into
    the unified address space.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.5\. Registering Pinned Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA developers don’t always get the opportunity to allocate host memory they
    want the GPU(s) to access directly. For example, a large, extensible application
    may have an interface that passes pointers to CUDA-aware plugins, or the application
    may be using an API for some other peripheral (notably high-speed networking)
    that has its own dedicated allocation function for much the same reason CUDA does.
    To accommodate these usage scenarios, CUDA 4.0 added the ability to *register*
    pinned memory.
  prefs: []
  type: TYPE_NORMAL
- en: Pinned memory registration decouples allocation from the page-locking and mapping
    of host memory. It takes an *already-allocated* virtual address range, page-locks
    it, and maps it for the GPU. Just as with `cudaHostAlloc()`, the memory optionally
    may be mapped into the CUDA address space or made portable (accessible to all
    GPUs).
  prefs: []
  type: TYPE_NORMAL
- en: 'The `cuMemHostRegister()/cudaHostRegister()` and `cuMemHostUnregister()/ cudaHostUnregister()`
    functions register and unregister host memory for access by the GPU(s), respectively.
    The memory range to register must be page-aligned: In other words, both the base
    address and the size must be evenly divisible by the page size of the operating
    system. Applications can allocate page-aligned address ranges in two ways.'
  prefs: []
  type: TYPE_NORMAL
- en: • Allocate the memory with operating system facilities that traffic in whole
    pages, such as `VirtualAlloc()` on Windows or `valloc()` or `mmap()`^([3](ch05.html#ch05fn3))
    on other platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch05.html#ch05fn3a). Or `posix_memalign()` in conjunction with `getpagesize()`.'
  prefs: []
  type: TYPE_NORMAL
- en: • Given an arbitrary address range (say, memory allocated with `malloc()` or
    `operator new[]`), clamp the address range to the next-lower page boundary and
    pad to the next page size.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Even when UVA is in effect, registered pinned memory that has been mapped into
    the CUDA address space has a different device pointer than the host pointer. Applications
    must call `cudaHostGetDevicePointer()/cuMemHostGetDevicePointer()` in order to
    obtain the device pointer.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.6\. Pinned Memory and UVA
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When UVA (Unified Virtual Addressing) is in effect, all pinned memory allocations
    are both mapped and portable. The exceptions to this rule are write-combined memory
    and registered memory. For those, the device pointer may differ from the host
    pointer, and applications still must query it with `cudaHostGetDevicePointer()/cuMemHostGetDevicePointer()`.
  prefs: []
  type: TYPE_NORMAL
- en: UVA is supported on all 64-bit platforms except Windows Vista and Windows 7\.
    On Windows Vista and Windows 7, only the TCC driver (which may be enabled or disabled
    using `nvidia-smi`) supports UVA. Applications can query whether UVA is in effect
    by calling `cudaGetDeviceProperties()` and examining the `cudaDeviceProp::unifiedAddressing`
    structure member, or by calling `cuDeviceGetAttribute()` with `CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING`.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.7\. Mapped Pinned Memory Usage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For applications whose performance relies on PCI Express transfer performance,
    mapped pinned memory can be a boon. Since the GPU can read or write host memory
    directly from kernels, it eliminates the need to perform some memory copies, reducing
    overhead. Here are some common idioms for using mapped pinned memory.
  prefs: []
  type: TYPE_NORMAL
- en: '• Posting writes to host memory: Multi-GPU applications often must stage results
    back to system memory for interchange with other GPUs; writing these results via
    mapped pinned memory avoids an extraneous device→host memory copy. Write-only
    access patterns to host memory are appealing because there is no latency to cover.'
  prefs: []
  type: TYPE_NORMAL
- en: '• Streaming: These workloads otherwise would use CUDA streams to coordinate
    concurrent memcpys to and from device memory, while kernels do their processing
    on device memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '• “Copy with panache”: Some workloads benefit from performing computations
    as data is transferred across PCI Express. For example, the GPU may compute subarray
    reductions while transferring data for Scan.'
  prefs: []
  type: TYPE_NORMAL
- en: Caveats
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Mapped pinned memory is not a panacea. Here are some caveats to consider when
    using it.
  prefs: []
  type: TYPE_NORMAL
- en: • Texturing from mapped pinned memory is possible, but very slow.
  prefs: []
  type: TYPE_NORMAL
- en: • It is important that mapped pinned memory be accessed with coalesced memory
    transactions (see [Section 5.2.9](ch05.html#ch05lev2sec17)). The performance penalty
    for uncoalesced memory transactions ranges from 6x to 2x. But even on SM 2.x and
    later GPUs, whose caches were supposed to make coalescing an obsolete consideration,
    the penalty is significant.
  prefs: []
  type: TYPE_NORMAL
- en: • Polling host memory with a kernel (e.g., for CPU/GPU synchronization) is not
    recommended.
  prefs: []
  type: TYPE_NORMAL
- en: • Do not try to use atomics on mapped pinned host memory, either for the host
    (locked compare-exchange) or the device (`atomicAdd()`). On the CPU side, the
    facilities to enforce mutual exclusion for locked operations are not visible to
    peripherals on the PCI Express bus. Conversely, on the GPU side, atomic operations
    only work on local device memory locations because they are implemented using
    the GPU’s local memory controller.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.8\. NUMA, Thread Affinity, and Pinned Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Beginning with the AMD Opteron and Intel Nehalem, CPU memory controllers were
    integrated directly into CPUs. Previously, the memory had been attached to the
    so-called “front-side bus” (FSB) of the “northbridge” of the chipset. In multi-CPU
    systems, the northbridge could service memory requests from any CPU, and memory
    access performance was reasonably uniform from one CPU to another. With the introduction
    of integrated memory controllers, each CPU has its own dedicated pool of “local”
    physical memory that is directly attached to that CPU. Although any CPU can access
    any other CPU’s memory, “nonlocal” accesses—accesses by one CPU to memory attached
    to another CPU—are performed across the AMD HyperTransport (HT) or Intel QuickPath
    Interconnect (QPI), incurring latency penalties and bandwidth limitations. To
    contrast with the uniform memory access times exhibited by systems with FSBs,
    these system architectures are known as NUMA for *nonuniform memory access*.
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, performance of multithreaded applications can be heavily
    dependent on whether memory references are local to the CPU that is running the
    current thread. For most applications, however, the higher cost of a nonlocal
    access is offset by the CPUs’ on-board caches. Once nonlocal memory is fetched
    into a CPU, it remains in-cache until evicted or needed by a memory access to
    the same page by another CPU. In fact, it is common for NUMA systems to include
    a System BIOS option to “interleave” memory physically between CPUs. When this
    BIOS option is enabled, the memory is evenly divided between CPUs on a per-cache
    line (typically 64 bytes) basis, so, for example, on a 2-CPU system, about 50%
    of memory accesses will be nonlocal on average.
  prefs: []
  type: TYPE_NORMAL
- en: For CUDA applications, PCI Express transfer performance can be dependent on
    whether memory references are local. If there is more than one I/O hub (IOH) in
    the system, the GPU(s) attached to a given IOH have better performance and reduce
    demand for QPI bandwidth when the pinned memory is local. Because some high-end
    NUMA systems are hierarchical but don’t associate the pools of memory bandwidth
    strictly with CPUs, NUMA APIs refer to *nodes* that may or may not strictly correspond
    with CPUs in the system.
  prefs: []
  type: TYPE_NORMAL
- en: If NUMA is enabled on the system, it is good practice to allocate host memory
    on the same node as a given GPU. Unfortunately, there is no official CUDA API
    to affiliate a GPU with a given CPU. Developers with a priori knowledge of the
    system design may know which node to associate with which GPU. Then platform-specific,
    NUMA-aware APIs may be used to perform these memory allocations, and host memory
    registration (see [Section 5.1.5](ch05.html#ch05lev2sec5)) can be used to pin
    those virtual allocations and map them for the GPU(s).
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5.1](ch05.html#ch05lis01) gives a code fragment to perform NUMA-aware
    allocations on Linux,^([4](ch05.html#ch05fn4)) and [Listing 5.2](ch05.html#ch05lis02)
    gives a code fragment to perform NUMA-aware allocations on Windows.^([5](ch05.html#ch05fn5))'
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch05.html#ch05fn4a). [http://bit.ly/USy4e7](http://bit.ly/USy4e7)'
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch05.html#ch05fn5a). [http://bit.ly/XY1g8m](http://bit.ly/XY1g8m)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.1.* NUMA-aware allocation (Linux).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bool
  prefs: []
  type: TYPE_NORMAL
- en: numNodes( int *p )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: if ( numa_available() >= 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: '*p = numa_max_node() + 1;'
  prefs: []
  type: TYPE_NORMAL
- en: return true;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return false;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: void *
  prefs: []
  type: TYPE_NORMAL
- en: pageAlignedNumaAlloc( size_t bytes, int node )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: void *ret;
  prefs: []
  type: TYPE_NORMAL
- en: printf( "Allocating on node %d\n", node ); fflush(stdout);
  prefs: []
  type: TYPE_NORMAL
- en: ret = numa_alloc_onnode( bytes, node );
  prefs: []
  type: TYPE_NORMAL
- en: return ret;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: pageAlignedNumaFree( void *p, size_t bytes )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: numa_free( p, bytes );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.2.* NUMA-aware allocation (Windows).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bool
  prefs: []
  type: TYPE_NORMAL
- en: numNodes( int *p )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: ULONG maxNode;
  prefs: []
  type: TYPE_NORMAL
- en: if ( GetNumaHighestNodeNumber( &maxNode ) ) {
  prefs: []
  type: TYPE_NORMAL
- en: '*p = (int) maxNode+1;'
  prefs: []
  type: TYPE_NORMAL
- en: return true;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return false;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: void *
  prefs: []
  type: TYPE_NORMAL
- en: pageAlignedNumaAlloc( size_t bytes, int node )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: void *ret;
  prefs: []
  type: TYPE_NORMAL
- en: printf( "Allocating on node %d\n", node ); fflush(stdout);
  prefs: []
  type: TYPE_NORMAL
- en: ret = VirtualAllocExNuma( GetCurrentProcess(),
  prefs: []
  type: TYPE_NORMAL
- en: NULL,
  prefs: []
  type: TYPE_NORMAL
- en: bytes,
  prefs: []
  type: TYPE_NORMAL
- en: MEM_COMMIT | MEM_RESERVE,
  prefs: []
  type: TYPE_NORMAL
- en: PAGE_READWRITE,
  prefs: []
  type: TYPE_NORMAL
- en: node );
  prefs: []
  type: TYPE_NORMAL
- en: return ret;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: pageAlignedNumaFree( void *p )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: VirtualFreeEx( GetCurrentProcess(), p, 0, MEM_RELEASE );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Global Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Global memory is the main abstraction by which CUDA kernels read or write device
    memory.^([6](ch05.html#ch05fn6)) Since device memory is directly attached to the
    GPU and read and written using a memory controller integrated into the GPU, the
    peak bandwidth is extremely high: typically more than 100G/s for high-end CUDA
    cards.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch05.html#ch05fn6a). For maximum developer confusion, CUDA uses the term
    *device pointer* to refer to pointers that reside in *global memory* (device memory
    addressable by CUDA kernels).'
  prefs: []
  type: TYPE_NORMAL
- en: Device memory can be accessed by CUDA kernels using *device pointers*. The following
    simple memset kernel gives an example.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p131pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: GPUmemset( int *base, int value, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += gridDim.x*blockDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: base[i] = value;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: The device pointer `base` resides in the *device address space*, separate from
    the CPU address space used by the host code in the CUDA program. As a result,
    host code in the CUDA program can perform pointer arithmetic on device pointers,
    but they may not dereference them.^([7](ch05.html#ch05fn7))
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch05.html#ch05fn7a). Mapped pinned pointers represent an exception to this
    rule. They are located in system memory but can be accessed by the GPU. On non-UVA
    systems, the host and device pointers to this memory are different: The application
    must call `cuMemHostGetDevicePointer()` or `cudaHostGetDevicePointer()` to map
    the host pointer to the corresponding device pointer. But when UVA is in effect,
    the pointers are the same.'
  prefs: []
  type: TYPE_NORMAL
- en: This kernel writes the integer `value` into the address range given by `base`
    and `N`. The references to `blockIdx`, `blockDim`, and `gridDim` enable the kernel
    to operate correctly, using whatever block and grid parameters were specified
    to the kernel launch.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. Pointers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When using the CUDA runtime, device pointers and host pointers both are typed
    as `void *`. The driver API uses an integer-valued typedef called `CUdeviceptr`
    that is the same width as host pointers (i.e., 32 bits on 32-bit operating systems
    and 64 bits on 64-bit operating systems), as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p131pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64)'
  prefs: []
  type: TYPE_NORMAL
- en: typedef unsigned long long CUdeviceptr;
  prefs: []
  type: TYPE_NORMAL
- en: '#else'
  prefs: []
  type: TYPE_NORMAL
- en: typedef unsigned int CUdeviceptr;
  prefs: []
  type: TYPE_NORMAL
- en: '#endif'
  prefs: []
  type: TYPE_NORMAL
- en: The `uintptr_t` type, available in `<stdint.h>` and introduced in C++0x, may
    be used to portably convert between host pointers (`void *`) and device pointers
    (`CUdeviceptr`), as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p131pro03a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUdeviceptr devicePtr;
  prefs: []
  type: TYPE_NORMAL
- en: void *p;
  prefs: []
  type: TYPE_NORMAL
- en: p = (void *) (uintptr_t) devicePtr;
  prefs: []
  type: TYPE_NORMAL
- en: devicePtr = (CUdeviceptr) (uintptr_t) p;
  prefs: []
  type: TYPE_NORMAL
- en: The host can do pointer arithmetic on device pointers to pass to a kernel or
    memcpy call, but the host cannot read or write device memory with these pointers.
  prefs: []
  type: TYPE_NORMAL
- en: 32- and 64-Bit Pointers in the Driver API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Because the original driver API definition for a pointer was 32-bit, the addition
    of 64-bit support to CUDA required the definition of `CUdeviceptr` and, in turn,
    all driver API functions that took `CUdeviceptr` as a parameter, to change.^([8](ch05.html#ch05fn8))
    `cuMemAlloc()`, for example, changed from
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch05.html#ch05fn8a). The old functions had to stay for compatibility reasons.'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, unsigned int bytesize);
  prefs: []
  type: TYPE_NORMAL
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
  prefs: []
  type: TYPE_NORMAL
- en: To accommodate both old applications (which linked against a `cuMemAlloc()`
    with 32-bit `CUdeviceptr` and size) and new ones, `cuda.h` includes two blocks
    of code that use the preprocessor to change the bindings without requiring function
    names to be changed as developers update to the new API.
  prefs: []
  type: TYPE_NORMAL
- en: First, a block of code surreptitiously changes function names to map to newer
    functions that have different semantics.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p132pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '#if defined(__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION >= 3020'
  prefs: []
  type: TYPE_NORMAL
- en: '#define cuDeviceTotalMem cuDeviceTotalMem_v2'
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: '#define cuTexRefGetAddress cuTexRefGetAddress_v2'
  prefs: []
  type: TYPE_NORMAL
- en: '#endif /* __CUDA_API_VERSION_INTERNAL || __CUDA_API_VERSION >= 3020 */'
  prefs: []
  type: TYPE_NORMAL
- en: This way, the client code uses the same old function names, but the compiled
    code generates references to the new function names with `_v2` appended.
  prefs: []
  type: TYPE_NORMAL
- en: Later in the header, the old functions are defined as they were. As a result,
    developers compiling for the latest version of CUDA get the latest function definitions
    and semantics. `cuda.h` uses a similar strategy for functions whose semantics
    changed from one version to the next, such as `cuStreamDestroy()`.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Dynamic Allocations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most global memory in CUDA is obtained through dynamic allocation. Using the
    CUDA runtime, the functions
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p133pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMalloc( void **, size_t );
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaFree( void );
  prefs: []
  type: TYPE_NORMAL
- en: allocate and free global memory, respectively. The corresponding driver API
    functions are
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p133pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemFree(CUdeviceptr dptr);
  prefs: []
  type: TYPE_NORMAL
- en: Allocating global memory is expensive. The CUDA driver implements a sub-allocator
    to satisfy small allocation requests, but if the suballocator must create a new
    memory block, that requires an expensive operating system call to the kernel mode
    driver. If that happens, the CUDA driver also must synchronize with the GPU, which
    may break CPU/GPU concurrency. As a result, it’s good practice to avoid allocating
    or freeing global memory in performance-sensitive code.
  prefs: []
  type: TYPE_NORMAL
- en: Pitched Allocations
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The coalescing constraints, coupled with alignment restrictions for texturing
    and 2D memory copy, motivated the creation of *pitched* memory allocations. The
    idea is that when creating a 2D array, a pointer into the array should have the
    same alignment characteristics when updated to point to a different row. The *pitch*
    of the array is the number of bytes per row of the array.^([9](ch05.html#ch05fn9))
    The pitch allocations take a width (in bytes) and height, pad the width to a suitable
    hardware-specific pitch, and pass back the base pointer and pitch of the allocation.
    By using these allocation functions to delegate selection of the pitch to the
    driver, developers can future-proof their code against architectures that widen
    alignment requirements.^([10](ch05.html#ch05fn10))
  prefs: []
  type: TYPE_NORMAL
- en: '[9](ch05.html#ch05fn9a). The idea of padding 2D allocations is much older than
    CUDA. Graphics APIs such as Apple QuickDraw and Microsoft DirectX exposed “rowBytes”
    and “pitch,” respectively. At one time, the padding simplified addressing computations
    by replacing a multiplication by a shift, or even replacing a multiplication by
    two shifts and an add with “two powers of 2” such as 640 (512 + 128). But these
    days, integer multiplication is so fast that pitch allocations have other motivations,
    such as avoiding negative performance interactions with caches.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10](ch05.html#ch05fn10a). Not an unexpected trend. Fermi widened several alignment
    requirements over Tesla.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA programs often must adhere to alignment constraints enforced by the hardware,
    not only on base addresses but also on the widths (in bytes) of memory copies
    and linear memory bound to textures. Because the alignment constraints are hardware-specific,
    CUDA provides APIs that enable developers to delegate the selection of the appropriate
    alignment to the driver. Using these APIs enables CUDA applications to implement
    hardware-independent code and to be “future-proof” against CUDA architectures
    that have not yet shipped.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 5.1](ch05.html#ch05fig01) shows a pitch allocation being performed
    on an array that is 352 bytes wide. The pitch is padded to the next multiple of
    64 bytes before allocating the memory. Given the pitch of the array in addition
    to the row and column, the address of an array element can be computed as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p134pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: inline T *
  prefs: []
  type: TYPE_NORMAL
- en: getElement( T *base, size_t Pitch, int row, int col )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: return (T *) ((char *) base + row*Pitch) + col;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 5.1* Pitch versus width.'
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime function to perform a pitched allocation is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p134pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: __inline__ __host__ cudaError_t cudaMallocPitch(
  prefs: []
  type: TYPE_NORMAL
- en: T **devPtr,
  prefs: []
  type: TYPE_NORMAL
- en: size_t *pitch,
  prefs: []
  type: TYPE_NORMAL
- en: size_t widthInBytes,
  prefs: []
  type: TYPE_NORMAL
- en: size_t height
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime also includes the function `cudaMalloc3D()`, which allocates
    3D memory regions using the `cudaPitchedPtr` and `cudaExtent` structures.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p134pro03a)'
  prefs: []
  type: TYPE_NORMAL
- en: extern __host__ cudaError_t CUDARTAPI cudaMalloc3D(struct
  prefs: []
  type: TYPE_NORMAL
- en: cudaPitchedPtr* pitchedDevPtr, struct cudaExtent extent);
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaPitchedPtr`, which receives the allocated memory, is defined as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: void *ptr;
  prefs: []
  type: TYPE_NORMAL
- en: size_t pitch;
  prefs: []
  type: TYPE_NORMAL
- en: size_t xsize;
  prefs: []
  type: TYPE_NORMAL
- en: size_t ysize;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaPitchedPtr::ptr` specifies the pointer; `cudaPitchedPtr::pitch` specifies
    the pitch (width in bytes) of the allocation; and `cudaPitchedPtr::xsize` and
    `cudaPitchedPtr::ysize` are the logical width and height of the allocation, respectively.
    `cudaExtent` is defined as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaExtent
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: size_t width;
  prefs: []
  type: TYPE_NORMAL
- en: size_t height;
  prefs: []
  type: TYPE_NORMAL
- en: size_t depth;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaExtent::width` is treated differently for arrays and linear device memory.
    For arrays, it specifies the width in array elements; for linear device memory,
    it specifies the pitch (width in bytes).'
  prefs: []
  type: TYPE_NORMAL
- en: The driver API function to allocate memory with a pitch is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p135pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemAllocPitch(CUdeviceptr *dptr, size_t *pPitch,
  prefs: []
  type: TYPE_NORMAL
- en: size_t WidthInBytes, size_t Height, unsigned int ElementSizeBytes);
  prefs: []
  type: TYPE_NORMAL
- en: The `ElementSizeBytes` parameter may be 4, 8, or 16 bytes, and it causes the
    allocation pitch to be padded to 64-, 128-, or 256-byte boundaries. Those are
    the alignment requirements for coalescing of 4-, 8-, and 16-byte memory transactions
    on SM 1.0 and SM 1.1 hardware. Applications that are not concerned with running
    well on that hardware can specify 4.
  prefs: []
  type: TYPE_NORMAL
- en: The pitch returned by `cudaMallocPitch()/cuMemAllocPitch()` is the width-in-bytes
    passed in by the caller, padded to an alignment that meets the alignment constraints
    for both coalescing of global load/store operations, and texture bind APIs. The
    amount of memory allocated is `height*pitch`.
  prefs: []
  type: TYPE_NORMAL
- en: For 3D arrays, developers can multiply the height by the depth before performing
    the allocation. This consideration only applies to arrays that will be accessed
    via global loads and stores, since 3D textures cannot be bound to global memory.
  prefs: []
  type: TYPE_NORMAL
- en: Allocations within Kernels
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Fermi-class hardware can dynamically allocate global memory using `malloc()`.
    Since this may require the GPU to interrupt the CPU, it is potentially slow. The
    sample program `mallocSpeed.cu` measures the performance of `malloc()` and `free()`
    in kernels.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5.3](ch05.html#ch05lis03) shows the key kernels and timing routine
    in `mallocSpeed.cu`. As an important note, the `cudaSetDeviceLimit()` function
    must be called with `cudaLimitMallocHeapSize` before `malloc()` may be called
    in kernels. The invocation in `mallocSpeed.cu` requests a full gigabyte (2^(30)
    bytes).'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaDeviceSetLimit(cudaLimitMallocHeapSize, 1<<30) );
  prefs: []
  type: TYPE_NORMAL
- en: When `cudaDeviceSetLimit()` is called, the requested amount of memory is allocated
    and may not be used for any other purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.3.* `MallocSpeed` function and kernels.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: AllocateBuffers( void **out, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: out[i] = malloc( N );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: FreeBuffers( void **in )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: free( in[i] );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t
  prefs: []
  type: TYPE_NORMAL
- en: MallocSpeed( double *msPerAlloc, double *msPerFree,
  prefs: []
  type: TYPE_NORMAL
- en: void **devicePointers, size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: cudaEvent_t evStart, cudaEvent_t evStop,
  prefs: []
  type: TYPE_NORMAL
- en: int cBlocks, int cThreads )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float etAlloc, etFree;
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( evStart ) );
  prefs: []
  type: TYPE_NORMAL
- en: AllocateBuffers<<<cBlocks,cThreads>>>( devicePointers, N );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( evStop ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaThreadSynchronize() );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaGetLastError() );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventElapsedTime( &etAlloc, evStart, evStop ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( evStart ) );
  prefs: []
  type: TYPE_NORMAL
- en: FreeBuffers<<<cBlocks,cThreads>>>( devicePointers );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( evStop ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaThreadSynchronize() );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaGetLastError() );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventElapsedTime( &etFree, evStart, evStop ) );
  prefs: []
  type: TYPE_NORMAL
- en: '*msPerAlloc = etAlloc / (double) (cBlocks*cThreads);'
  prefs: []
  type: TYPE_NORMAL
- en: '*msPerFree = etFree / (double) (cBlocks*cThreads);'
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: return status;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5.4](ch05.html#ch05lis04) shows the output from a sample run of `mallocSpeed.cu`
    on Amazon’s `cg1.4xlarge` instance type. It is clear that the allocator is optimized
    for small allocations: The 64-byte allocations take an average of 0.39 microseconds
    to perform, while allocations of 12K take at least 3 to 5 microseconds. The first
    result (155 microseconds per allocation) is having 1 thread per each of 500 blocks
    allocate a 1MB buffer.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.4.* Sample `mallocSpeed.cu` output.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis04a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Microseconds per alloc/free (1 thread per block):'
  prefs: []
  type: TYPE_NORMAL
- en: alloc       free
  prefs: []
  type: TYPE_NORMAL
- en: 154.93      4.57
  prefs: []
  type: TYPE_NORMAL
- en: Microseconds per alloc/free (32-512 threads per block, 12K
  prefs: []
  type: TYPE_NORMAL
- en: 'allocations):'
  prefs: []
  type: TYPE_NORMAL
- en: 32            64            128           256           512
  prefs: []
  type: TYPE_NORMAL
- en: alloc  free   alloc  free   alloc  free   alloc  free   alloc  free
  prefs: []
  type: TYPE_NORMAL
- en: 3.53   1.18   4.27   1.17   4.89   1.14   5.48   1.14   10.38  1.11
  prefs: []
  type: TYPE_NORMAL
- en: Microseconds per alloc/free (32-512 threads per block, 64-byte
  prefs: []
  type: TYPE_NORMAL
- en: 'allocations):'
  prefs: []
  type: TYPE_NORMAL
- en: 32            64            128           256           512
  prefs: []
  type: TYPE_NORMAL
- en: alloc  free   alloc  free   alloc  free   alloc  free   alloc  free
  prefs: []
  type: TYPE_NORMAL
- en: 0.35   0.27   0.37   0.29   0.34   0.27   0.37   0.22   0.53   0.27
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Memory allocated by invoking `malloc()` in a kernel must be freed by a *kernel*
    calling `free()`. Calling `cudaFree()` on the host will not work.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3\. Querying the Amount of Global Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The amount of global memory in a system may be queried even before CUDA has
    been initialized.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Call `cudaGetDeviceProperties()` and examine `cudaDeviceProp.totalGlobalMem:`
  prefs: []
  type: TYPE_NORMAL
- en: size_t totalGlobalMem; /**< Global memory on device in bytes */.
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Call this driver API function.
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuDeviceTotalMem(size_t *bytes, CUdevice dev);
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: WDDM and Available Memory
  prefs: []
  type: TYPE_NORMAL
- en: The Windows Display Driver Model (WDDM) introduced with Windows Vista changed
    the model for memory management by display drivers to enable chunks of video memory
    to be swapped in and out of host memory as needed to perform rendering. As a result,
    the amount of memory reported by `cuDeviceTotalMem() / cudaDeviceProp::totalGlobalMem`
    will not exactly reflect the amount of physical memory on the card.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4\. Static Allocations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Applications can statically allocate global memory by annotating a memory declaration
    with the `__device__` keyword. This memory is allocated by the CUDA driver when
    the module is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Memory copies to and from statically allocated memory can be performed by `cudaMemcpyToSymbol()`
    and `cudaMemcpyFromSymbol().`
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p138pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMemcpyToSymbol(
  prefs: []
  type: TYPE_NORMAL
- en: char *symbol,
  prefs: []
  type: TYPE_NORMAL
- en: const void *src,
  prefs: []
  type: TYPE_NORMAL
- en: size_t count,
  prefs: []
  type: TYPE_NORMAL
- en: size_t offset = 0,
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaMemcpyKind kind = cudaMemcpyHostToDevice
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMemcpyFromSymbol(
  prefs: []
  type: TYPE_NORMAL
- en: void *dst,
  prefs: []
  type: TYPE_NORMAL
- en: char *symbol,
  prefs: []
  type: TYPE_NORMAL
- en: size_t count,
  prefs: []
  type: TYPE_NORMAL
- en: size_t offset = 0,
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaMemcpyKind kind = cudaMemcpyDeviceToHost
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: When calling `cudaMemcpyToSymbol()` or `cudaMemcpyFromSymbol()`, do not enclose
    the symbol name in quotation marks. In other words, use
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyToSymbol(g_xOffset, poffsetx, Width*Height*sizeof(int));
  prefs: []
  type: TYPE_NORMAL
- en: not
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyToSymbol("g_xOffset", poffsetx, ... );
  prefs: []
  type: TYPE_NORMAL
- en: Both formulations work, but the latter formulation will compile for any symbol
    name (even undefined symbols). If you want the compiler to report errors for invalid
    symbols, avoid the quotation marks.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA runtime applications can query the pointer corresponding to a static allocation
    by calling `cudaGetSymbolAddress().`
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaGetSymbolAddress( void **devPtr, char *symbol );
  prefs: []
  type: TYPE_NORMAL
- en: '*Beware:* It is all too easy to pass the symbol for a statically declared device
    memory allocation to a CUDA kernel, but this does not work. You must call `cudaGetSymbolAddress()`
    and use the resulting pointer.'
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Developers using the driver API can obtain pointers to statically allocated
    memory by calling `cuModuleGetGlobal().`
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p139pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuModuleGetGlobal(CUdeviceptr *dptr, size_t *bytes,
  prefs: []
  type: TYPE_NORMAL
- en: CUmodule hmod, const char *name);
  prefs: []
  type: TYPE_NORMAL
- en: Note that `cuModuleGetGlobal()` passes back both the base pointer and the size
    of the object. If the size is not needed, developers can pass `NULL` for the `bytes`
    parameter. Once this pointer has been obtained, the memory can be accessed by
    passing the `CUdeviceptr` to memory copy calls or CUDA kernel invocations.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.5\. Memset APIs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For developer convenience, CUDA provides 1D and 2D memset functions. Since they
    are implemented using kernels, they are asynchronous even when no stream parameter
    is specified. For applications that must serialize the execution of a memset within
    a stream, however, there are `*Async()` variants that take a stream parameter.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The CUDA runtime supports byte-sized memset only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p139pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMemset(void *devPtr, int value, size_t count);
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMemset2D(void *devPtr, size_t pitch, int value,
  prefs: []
  type: TYPE_NORMAL
- en: size_t width, size_t height);
  prefs: []
  type: TYPE_NORMAL
- en: The pitch parameter specifies the bytes per row of the memset operation.
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The driver API supports 1D and 2D memset of a variety of sizes, shown in [Table
    5.2](ch05.html#ch05tab02). These memset functions take the destination pointer,
    value to set, and number of values to write starting at the base address. The
    pitch parameter is the bytes per row (not elements per row!).
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p140pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemsetD8(CUdeviceptr dstDevice, unsigned char uc,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N);
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemsetD16(CUdeviceptr dstDevice, unsigned short
  prefs: []
  type: TYPE_NORMAL
- en: us, size_t N);
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemsetD32(CUdeviceptr dstDevice, unsigned int ui,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N);
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemsetD2D8(CUdeviceptr dstDevice, size_t dstPitch,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char uc, size_t Width, size_t Height);
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemsetD2D16(CUdeviceptr dstDevice, size_t
  prefs: []
  type: TYPE_NORMAL
- en: dstPitch, unsigned short us, size_t Width, size_t Height);
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemsetD2D32(CUdeviceptr dstDevice, size_t
  prefs: []
  type: TYPE_NORMAL
- en: dstPitch, unsigned int ui, size_t Width, size_t Height);
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.2* Memset Variations'
  prefs: []
  type: TYPE_NORMAL
- en: Now that CUDA runtime and driver API functions can peacefully coexist in the
    same application, CUDA runtime developers can use these functions as needed. The
    `unsigned char`, `unsigned short`, and `unsigned int` parameters just specify
    a bit pattern; to fill a global memory range with some other type, such as `float`,
    use a `volatile union` to coerce the `float` to `unsigned int`.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.6\. Pointer Queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA tracks all of its memory allocations, and provides APIs that enable applications
    to query CUDA about pointers that were passed in from some other party. Libraries
    or plugins may wish to pursue different strategies based on this information.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `cudaPointerGetAttributes()` function takes a pointer as input and passes
    back a `cudaPointerAttributes` structure containing information about the pointer.
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPointerAttributes {
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaMemoryType memoryType;
  prefs: []
  type: TYPE_NORMAL
- en: int device;
  prefs: []
  type: TYPE_NORMAL
- en: void *devicePointer;
  prefs: []
  type: TYPE_NORMAL
- en: void *hostPointer;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: When UVA is in effect, pointers are unique process-wide, so there is no ambiguity
    as to the input pointer’s address space. When UVA is not in effect, the input
    pointer is assumed to be in the current device’s address space ([Table 5.3](ch05.html#ch05tab03)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.3* `cudaPointerAttributes` Members'
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Developers can query the address range where a given device pointer resides
    using the `cuMemGetAddressRange()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p141pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuMemGetAddressRange(CUdeviceptr *pbase, size_t
  prefs: []
  type: TYPE_NORMAL
- en: '*psize, CUdeviceptr dptr);'
  prefs: []
  type: TYPE_NORMAL
- en: This function takes a device pointer as input and passes back the base and size
    of the allocation containing that device pointer.
  prefs: []
  type: TYPE_NORMAL
- en: With the addition of UVA in CUDA 4.0, developers can query CUDA to get even
    more information about an address using `cuPointerGetAttribute().`
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p142pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuPointerGetAttribute(void *data, CUpointer_
  prefs: []
  type: TYPE_NORMAL
- en: attribute attribute, CUdeviceptr ptr);
  prefs: []
  type: TYPE_NORMAL
- en: This function takes a device pointer as input and passes back the information
    corresponding to the attribute parameter, as shown in [Table 5.4](ch05.html#ch05tab04).
    Note that for unified addresses, using `CU_POINTER_ATTRIBUTE_DEVICE_POINTER` or
    `CU_POINTER_ATTRIBUTE_HOST_POINTER` will cause the same pointer value to be returned
    as the one passed in.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.4* `cuPointerAttribute` Usage'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Queries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: On SM 2.x (Fermi) hardware and later, developers can query whether a given pointer
    points into global space. The `__isGlobal()` intrinsic
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int __isGlobal( const void *p );
  prefs: []
  type: TYPE_NORMAL
- en: returns 1 if the input pointer refers to global memory and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.7\. Peer-to-Peer Access
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Under certain circumstances, SM 2.0-class and later hardware can map memory
    belonging to other, similarly capable GPUs. The following conditions apply.
  prefs: []
  type: TYPE_NORMAL
- en: • UVA must be in effect.
  prefs: []
  type: TYPE_NORMAL
- en: • Both GPUs must be Fermi-class and be based on the same chip.
  prefs: []
  type: TYPE_NORMAL
- en: • The GPUs must be on the same I/O hub.
  prefs: []
  type: TYPE_NORMAL
- en: Since peer-to-peer mapping is intrinsically a multi-GPU feature, it is described
    in detail in the multi-GPU chapter (see [Section 9.2](ch09.html#ch09lev1sec2)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.8\. Reading and Writing Global Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA kernels can read or write global memory using standard C semantics such
    as pointer indirection (`operator*, operator->`) or array subscripting (`operator[]`).
    Here is a simple templatized kernel to write a constant into a memory range.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p143pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: GlobalWrites( T *out, T value, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[i] = value;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'This kernel works correctly for any inputs: any component size, any block size,
    any grid size. Its code is intended more for illustrative purposes than maximum
    performance. CUDA kernels that use more registers and operate on multiple values
    in the inner loop go faster, but for some block and grid configurations, its performance
    is perfectly acceptable. In particular, provided the base address and block size
    are specified correctly, it performs coalesced memory transactions that maximize
    memory bandwidth.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.9\. Coalescing Constraints
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For best performance when reading and writing data, CUDA kernels must perform
    *coalesced* memory transactions. Any memory transaction that does not meet the
    full set of criteria needed for coalescing is “uncoalesced.” The penalty for uncoalesced
    memory transactions varies from 2x to 8x, depending on the chip implementation.
    Coalesced memory transactions have a much less dramatic impact on performance
    on more recent hardware, as shown in [Table 5.5](ch05.html#ch05tab05).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.5* Bandwidth Penalties for Uncoalesced Memory Access'
  prefs: []
  type: TYPE_NORMAL
- en: Transactions are coalesced on a per-warp basis. A simplified set of criteria
    must be met in order for the memory read or write being performed by the warp
    to be coalesced.
  prefs: []
  type: TYPE_NORMAL
- en: • The words must be at least 32 bits in size. Reading or writing bytes or 16-bit
    words is always uncoalesced.
  prefs: []
  type: TYPE_NORMAL
- en: • The addresses being accessed by the threads of the warp must be contiguous
    and increasing (i.e., offset by the thread ID).
  prefs: []
  type: TYPE_NORMAL
- en: • The base address of the warp (the address being accessed by the first thread
    in the warp) must be aligned as shown in [Table 5.6](ch05.html#ch05tab06).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.6* Alignment Criteria for Coalescing'
  prefs: []
  type: TYPE_NORMAL
- en: The `ElementSizeBytes` parameter to `cuMemAllocPitch()` is intended to accommodate
    the size restriction. It specifies the size in bytes of the memory accesses intended
    by the application, so the pitch guarantees that a set of coalesced memory transactions
    for a given row of the allocation also will be coalesced for other rows.
  prefs: []
  type: TYPE_NORMAL
- en: Most kernels in this book perform coalesced memory transactions, provided the
    input addresses are properly aligned. NVIDIA has provided more detailed, architecture-specific
    information on how global memory transactions are handled, as detailed below.
  prefs: []
  type: TYPE_NORMAL
- en: SM 1.x (Tesla)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: SM 1.0 and SM 1.1 hardware require that each thread in a warp access adjacent
    memory locations in sequence, as described above. SM 1.2 and 1.3 hardware relaxed
    the coalescing constraints somewhat. To issue a coalesced memory request, divide
    each 32-thread warp into two “half warps,” lanes 0–15 and lanes 16–31\. To service
    the memory request from each half-warp, the hardware performs the following algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** Find the active thread with the lowest thread ID and locate the memory
    segment that contains that thread’s requested address. The segment size depends
    on the word size: 1-byte requests result in 32-byte segments; 2-byte requests
    result in 64-byte segments; and all other requests result in 128-byte segments.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** Find all other active threads whose requested address lies in the same
    segment.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3.** If possible, reduce the segment transaction size to 64 or 32 bytes.'
  prefs: []
  type: TYPE_NORMAL
- en: '**4.** Carry out the transaction and mark the services threads as inactive.'
  prefs: []
  type: TYPE_NORMAL
- en: '**5.** Repeat steps 1–4 until all threads in the half-warp have been serviced.'
  prefs: []
  type: TYPE_NORMAL
- en: Although these requirements are somewhat relaxed compared to the SM 1.0–1.1
    constraints, a great deal of locality is still required for effective coalescing.
    In practice, the relaxed coalescing means the threads within a warp can permute
    the inputs within small segments of memory, if desired.
  prefs: []
  type: TYPE_NORMAL
- en: SM 2.x (Fermi)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: SM 2.x and later hardware includes L1 and L2 caches. The L2 cache services the
    entire chip; the L1 caches are per-SM and may be configured to be 16K or 48K in
    size. The cache lines are 128 bytes and map to 128-byte aligned segments in device
    memory. Memory accesses that are cached in both L1 and L2 are serviced with 128-byte
    memory transactions, whereas memory accesses that are cached in L2 only are serviced
    with 32-byte memory transactions. Caching in L2 only can therefore reduce overfetch,
    for example, in the case of scattered memory accesses.
  prefs: []
  type: TYPE_NORMAL
- en: The hardware can specify the cacheability of global memory accesses on a per-instruction
    basis. By default, the compiler emits instructions that cache memory accesses
    in both L1 and L2 (`-Xptxas -dlcm=ca`). This can be changed to cache in L2 only
    by specifying `-Xptxas -dlcm=cg`. Memory accesses that are not present in L1 but
    cached in L2 only are serviced with 32-byte memory transactions, which may improve
    cache utilization for applications that are performing scattered memory accesses.
  prefs: []
  type: TYPE_NORMAL
- en: Reading via pointers that are declared `volatile` causes any cached results
    to be discarded and for the data to be refetched. This idiom is mainly useful
    for polling host memory locations. [Table 5.7](ch05.html#ch05tab07) summarizes
    how memory requests by a warp are broken down into 128-byte cache line requests.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.7* SM 2.x Cache Line Requests'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: On SM 2.x and higher architectures, threads within a warp can access any words
    in any order, including the same words.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: SM 3.x (Kepler)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The L2 cache architecture is the same as SM 2.x. SM 3.x does not cache global
    memory accesses in L1\. In SM 3.5, global memory may be accessed via the texture
    cache (which is 48K per SM in size) by accessing memory via `const restricted`
    pointers or by using the `__ldg()` intrinsics in `sm_35_intrinsics.h`. As when
    texturing directly from device memory, it is important not to access memory that
    might be accessed concurrently by other means, since this cache is not kept coherent
    with respect to the L2.
  prefs: []
  type: TYPE_NORMAL
- en: '5.2.10\. Microbenchmarks: Peak Memory Bandwidth'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The source code accompanying this book includes microbenchmarks that determine
    which combination of operand size, loop unroll factor, and block size maximizes
    bandwidth for a given GPU. Rewriting the earlier `GlobalWrites` code as a template
    that takes an additional parameter `n` (the number of writes to perform in the
    inner loop) yields the kernel in [Listing 5.5](ch05.html#ch05lis05).
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.5.* `GlobalWrites` kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis05a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, const int n>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: GlobalWrites( T *out, T value, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: size_t i;
  prefs: []
  type: TYPE_NORMAL
- en: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N-n*blockDim.x*gridDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: i += n*blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+j*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: out[index] = value;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // to avoid the (index<N) conditional in the inner loop,
  prefs: []
  type: TYPE_NORMAL
- en: // we left off some work at the end
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+j*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: if ( index<N ) out[index] = value;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`ReportRow()`, the function given in [Listing 5.6](ch05.html#ch05lis06) that
    writes one row of output calls by calling a template function `BandwidthWrites`
    (not shown), reports the bandwidth for a given type, grid, and block size.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.6.* `ReportRow` function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis06a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, const int n, bool bOffset>
  prefs: []
  type: TYPE_NORMAL
- en: double
  prefs: []
  type: TYPE_NORMAL
- en: ReportRow( size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: size_t threadStart,
  prefs: []
  type: TYPE_NORMAL
- en: size_t threadStop,
  prefs: []
  type: TYPE_NORMAL
- en: size_t cBlocks )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int maxThreads = 0;
  prefs: []
  type: TYPE_NORMAL
- en: double maxBW = 0.0;
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%d\t", n );
  prefs: []
  type: TYPE_NORMAL
- en: for ( int cThreads = threadStart;
  prefs: []
  type: TYPE_NORMAL
- en: cThreads <= threadStop;
  prefs: []
  type: TYPE_NORMAL
- en: cThreads *= 2 ) {
  prefs: []
  type: TYPE_NORMAL
- en: double bw;
  prefs: []
  type: TYPE_NORMAL
- en: bw = BandwidthWrites<T,n,bOffset>( N, cBlocks, cThreads );
  prefs: []
  type: TYPE_NORMAL
- en: if ( bw > maxBW ) {
  prefs: []
  type: TYPE_NORMAL
- en: maxBW = bw;
  prefs: []
  type: TYPE_NORMAL
- en: maxThreads = cThreads;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%.2f\t", bw );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%.2f\t%d\n", maxBW, maxThreads );
  prefs: []
  type: TYPE_NORMAL
- en: return maxBW;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The `threadStart` and `threadStop` parameters typically are 32 and 512, 32 being
    the warp size and the minimum number of threads per block that can occupy the
    machine. The `bOffset` template parameter specifies whether `BandwidthWrites`
    should offset the base pointer, causing all memory transactions to become uncoalesced.
    If the program is invoked with the `--uncoalesced` command line option, it will
    perform the bandwidth measurements with the offset pointer.
  prefs: []
  type: TYPE_NORMAL
- en: Note that depending on `sizeof(T)`, kernels with `n` above a certain level will
    fall off a performance cliff as the number of temporary variables in the inner
    loop grows too high to hold in registers.
  prefs: []
  type: TYPE_NORMAL
- en: The five applications summarized in [Table 5.8](ch05.html#ch05tab08) implement
    this strategy. They measure the memory bandwidth delivered for different operand
    sizes (8-, 16-, 32-, 64-, and 128-bit), threadblock sizes (32, 64, 128, 256, and
    512), and loop unroll factors (1–16). CUDA hardware isn’t necessarily sensitive
    to all of these parameters. For example, many parameter settings enable a GK104
    to deliver 140GB/s of bandwidth via texturing, but only if the operand size is
    at least 32-bit. For a given workload and hardware, however, the microbenchmarks
    highlight which parameters matter. Also, for small operand sizes, they highlight
    how loop unrolling can help increase performance (not all applications can be
    refactored to read larger operands).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.8* Memory Bandwidth Microbenchmarks'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5.7](ch05.html#ch05lis07) gives example output from `globalRead.cu`,
    run on a GeForce GTX 680 GPU. The output is grouped by operand size, from bytes
    to 16-byte quads; the leftmost column of each group gives the loop unroll factor.
    The bandwidth delivered for blocks of sizes 32 to 512 is given in each column,
    and the `maxBW` and `maxThreads` columns give the highest bandwidth and the block
    size that delivered the highest bandwidth, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: The GeForce GTX 680 can deliver up to 140GB/s, so [Listing 5.7](ch05.html#ch05lis07)
    makes it clear that when reading 8- and 16-bit words on SM 3.0, global loads are
    not the way to go. Bytes deliver at most 60GB/s, and 16-bit words deliver at most
    101GB/s.^([11](ch05.html#ch05fn11)) For 32-bit operands, a 2x loop unroll and
    at least 256 threads per block are needed to get maximum bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: '[11](ch05.html#ch05fn11a). Texturing works better. Readers can run `globalReadTex.cu`
    to confirm.'
  prefs: []
  type: TYPE_NORMAL
- en: These microbenchmarks can help developers optimize their bandwidth-bound applications.
    Choose the one whose memory access pattern most closely resembles your application,
    and either run the microbenchmark on the target GPU or, if possible, modify the
    microbenchmark to resemble the actual workload more closely and run it to determine
    the optimal parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.7.* Sample output, `globalRead.cu`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis07a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Running globalRead.cu microbenchmark on GeForce GTX 680
  prefs: []
  type: TYPE_NORMAL
- en: Using coalesced memory transactions
  prefs: []
  type: TYPE_NORMAL
- en: 'Operand size: 1 byte'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input size: 16M operands'
  prefs: []
  type: TYPE_NORMAL
- en: Block Size
  prefs: []
  type: TYPE_NORMAL
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  prefs: []
  type: TYPE_NORMAL
- en: 1       9.12    17.39   30.78   30.78   28.78   30.78   128
  prefs: []
  type: TYPE_NORMAL
- en: 2       18.37   34.54   56.36   53.53   49.33   56.36   128
  prefs: []
  type: TYPE_NORMAL
- en: 3       23.55   42.32   61.56   60.15   52.91   61.56   128
  prefs: []
  type: TYPE_NORMAL
- en: 4       21.25   38.26   58.99   58.09   51.26   58.99   128
  prefs: []
  type: TYPE_NORMAL
- en: 5       25.29   42.17   60.13   58.49   52.57   60.13   128
  prefs: []
  type: TYPE_NORMAL
- en: 6       25.68   42.15   59.93   55.42   47.46   59.93   128
  prefs: []
  type: TYPE_NORMAL
- en: 7       28.84   47.03   56.20   51.41   41.41   56.20   128
  prefs: []
  type: TYPE_NORMAL
- en: 8       29.88   48.55   55.75   50.68   39.96   55.75   128
  prefs: []
  type: TYPE_NORMAL
- en: 9       28.65   47.75   56.84   51.17   37.56   56.84   128
  prefs: []
  type: TYPE_NORMAL
- en: 10      27.35   45.16   52.99   46.30   32.94   52.99   128
  prefs: []
  type: TYPE_NORMAL
- en: 11      22.27   38.51   48.17   42.74   32.81   48.17   128
  prefs: []
  type: TYPE_NORMAL
- en: 12      23.39   40.51   49.78   42.42   31.89   49.78   128
  prefs: []
  type: TYPE_NORMAL
- en: 13      21.62   37.49   40.89   34.98   21.43   40.89   128
  prefs: []
  type: TYPE_NORMAL
- en: 14      18.55   32.12   36.04   31.41   19.96   36.04   128
  prefs: []
  type: TYPE_NORMAL
- en: 15      21.47   36.87   39.94   33.36   19.98   39.94   128
  prefs: []
  type: TYPE_NORMAL
- en: 16      21.59   36.79   39.49   32.71   19.42   39.49   128
  prefs: []
  type: TYPE_NORMAL
- en: 'Operand size: 2 bytes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input size: 16M operands'
  prefs: []
  type: TYPE_NORMAL
- en: Block Size
  prefs: []
  type: TYPE_NORMAL
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  prefs: []
  type: TYPE_NORMAL
- en: 1       18.29   35.07   60.30   59.16   56.06   60.30   128
  prefs: []
  type: TYPE_NORMAL
- en: 2       34.94   64.39   94.28   92.65   85.99   94.28   128
  prefs: []
  type: TYPE_NORMAL
- en: 3       45.02   72.90   101.38  99.02   90.07   101.38  128
  prefs: []
  type: TYPE_NORMAL
- en: 4       38.54   68.35   100.30  98.29   90.28   100.30  128
  prefs: []
  type: TYPE_NORMAL
- en: 5       45.49   75.73   98.68   98.11   90.05   98.68   128
  prefs: []
  type: TYPE_NORMAL
- en: 6       47.58   77.50   100.35  97.15   86.17   100.35  128
  prefs: []
  type: TYPE_NORMAL
- en: 7       53.64   81.04   92.89   87.39   74.14   92.89   128
  prefs: []
  type: TYPE_NORMAL
- en: 8       44.79   74.02   89.19   83.96   69.65   89.19   128
  prefs: []
  type: TYPE_NORMAL
- en: 9       47.63   76.63   91.60   83.52   68.06   91.60   128
  prefs: []
  type: TYPE_NORMAL
- en: 10      51.02   79.82   93.85   84.69   66.62   93.85   128
  prefs: []
  type: TYPE_NORMAL
- en: 11      42.00   72.11   88.23   79.24   62.27   88.23   128
  prefs: []
  type: TYPE_NORMAL
- en: 12      40.53   69.27   85.75   76.32   59.73   85.75   128
  prefs: []
  type: TYPE_NORMAL
- en: 13      44.90   73.44   78.08   66.96   41.27   78.08   128
  prefs: []
  type: TYPE_NORMAL
- en: 14      39.18   68.43   74.46   63.27   39.27   74.46   128
  prefs: []
  type: TYPE_NORMAL
- en: 15      37.60   64.11   69.93   60.22   37.09   69.93   128
  prefs: []
  type: TYPE_NORMAL
- en: 16      40.36   67.90   73.07   60.79   36.66   73.07   128
  prefs: []
  type: TYPE_NORMAL
- en: 'Operand size: 4 bytes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input size: 16M operands'
  prefs: []
  type: TYPE_NORMAL
- en: Block Size
  prefs: []
  type: TYPE_NORMAL
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  prefs: []
  type: TYPE_NORMAL
- en: 1       36.37   67.89   108.04  105.99  104.09  108.04  128
  prefs: []
  type: TYPE_NORMAL
- en: 2       73.85   120.90  139.91  139.93  136.04  139.93  256
  prefs: []
  type: TYPE_NORMAL
- en: 3       62.62   109.24  140.07  139.66  138.38  140.07  128
  prefs: []
  type: TYPE_NORMAL
- en: 4       56.02   101.73  138.70  137.42  135.10  138.70  128
  prefs: []
  type: TYPE_NORMAL
- en: 5       87.34   133.65  140.64  140.33  139.00  140.64  128
  prefs: []
  type: TYPE_NORMAL
- en: 6       100.64  137.47  140.61  139.53  127.18  140.61  128
  prefs: []
  type: TYPE_NORMAL
- en: 7       89.08   133.99  139.60  138.23  124.28  139.60  128
  prefs: []
  type: TYPE_NORMAL
- en: 8       58.46   103.09  129.24  122.28  110.58  129.24  128
  prefs: []
  type: TYPE_NORMAL
- en: 9       68.99   116.59  134.17  128.64  114.80  134.17  128
  prefs: []
  type: TYPE_NORMAL
- en: 10      54.64   97.90   123.91  118.84  106.96  123.91  128
  prefs: []
  type: TYPE_NORMAL
- en: 11      64.35   110.30  131.43  123.90  109.31  131.43  128
  prefs: []
  type: TYPE_NORMAL
- en: 12      68.03   113.89  130.95  125.40  108.02  130.95  128
  prefs: []
  type: TYPE_NORMAL
- en: 13      71.34   117.88  123.85  113.08  76.98   123.85  128
  prefs: []
  type: TYPE_NORMAL
- en: 14      54.72   97.31   109.41  101.28  71.13   109.41  128
  prefs: []
  type: TYPE_NORMAL
- en: 15      67.28   111.24  118.88  108.35  72.30   118.88  128
  prefs: []
  type: TYPE_NORMAL
- en: 16      63.32   108.56  117.77  103.24  69.76   117.77  128
  prefs: []
  type: TYPE_NORMAL
- en: 'Operand size: 8 bytes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input size: 16M operands'
  prefs: []
  type: TYPE_NORMAL
- en: Block Size
  prefs: []
  type: TYPE_NORMAL
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  prefs: []
  type: TYPE_NORMAL
- en: 1       74.64   127.73  140.91  142.08  142.16  142.16  512
  prefs: []
  type: TYPE_NORMAL
- en: 2       123.70  140.35  141.31  141.99  142.42  142.42  512
  prefs: []
  type: TYPE_NORMAL
- en: 3       137.28  141.15  140.86  141.94  142.63  142.63  512
  prefs: []
  type: TYPE_NORMAL
- en: 4       128.38  141.39  141.85  142.56  142.00  142.56  256
  prefs: []
  type: TYPE_NORMAL
- en: 5       117.57  140.95  141.17  142.08  141.78  142.08  256
  prefs: []
  type: TYPE_NORMAL
- en: 6       112.10  140.62  141.48  141.86  141.95  141.95  512
  prefs: []
  type: TYPE_NORMAL
- en: 7       85.02   134.82  141.59  141.50  141.09  141.59  128
  prefs: []
  type: TYPE_NORMAL
- en: 8       94.44   138.71  140.86  140.25  128.91  140.86  128
  prefs: []
  type: TYPE_NORMAL
- en: 9       100.69  139.83  141.09  141.45  127.82  141.45  256
  prefs: []
  type: TYPE_NORMAL
- en: 10      92.51   137.76  140.74  140.93  126.50  140.93  256
  prefs: []
  type: TYPE_NORMAL
- en: 11      104.87  140.38  140.67  136.70  128.48  140.67  128
  prefs: []
  type: TYPE_NORMAL
- en: 12      97.71   138.62  140.12  135.74  125.37  140.12  128
  prefs: []
  type: TYPE_NORMAL
- en: 13      95.87   138.28  139.90  134.18  123.41  139.90  128
  prefs: []
  type: TYPE_NORMAL
- en: 14      85.69   134.18  133.84  131.16  120.95  134.18  64
  prefs: []
  type: TYPE_NORMAL
- en: 15      94.43   135.43  135.30  133.47  120.52  135.43  64
  prefs: []
  type: TYPE_NORMAL
- en: 16      91.62   136.69  133.59  129.95  117.99  136.69  64
  prefs: []
  type: TYPE_NORMAL
- en: 'Operand size: 16 bytes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input size: 16M operands'
  prefs: []
  type: TYPE_NORMAL
- en: Block Size
  prefs: []
  type: TYPE_NORMAL
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  prefs: []
  type: TYPE_NORMAL
- en: 1       125.37  140.67  141.15  142.06  142.59  142.59  512
  prefs: []
  type: TYPE_NORMAL
- en: 2       131.26  141.95  141.72  142.32  142.49  142.49  512
  prefs: []
  type: TYPE_NORMAL
- en: 3       141.03  141.65  141.63  142.43  138.44  142.43  256
  prefs: []
  type: TYPE_NORMAL
- en: 4       139.90  142.70  142.62  142.20  142.84  142.84  512
  prefs: []
  type: TYPE_NORMAL
- en: 5       138.24  142.08  142.18  142.79  140.94  142.79  256
  prefs: []
  type: TYPE_NORMAL
- en: 6       131.41  142.45  142.32  142.51  142.08  142.51  256
  prefs: []
  type: TYPE_NORMAL
- en: 7       131.98  142.26  142.27  142.11  142.26  142.27  128
  prefs: []
  type: TYPE_NORMAL
- en: 8       132.70  142.47  142.10  142.67  142.19  142.67  256
  prefs: []
  type: TYPE_NORMAL
- en: 9       136.58  142.28  141.89  142.42  142.09  142.42  256
  prefs: []
  type: TYPE_NORMAL
- en: 10      135.61  142.67  141.85  142.86  142.36  142.86  256
  prefs: []
  type: TYPE_NORMAL
- en: 11      136.27  142.48  142.45  142.14  142.41  142.48  64
  prefs: []
  type: TYPE_NORMAL
- en: 12      130.62  141.79  142.06  142.39  142.16  142.39  256
  prefs: []
  type: TYPE_NORMAL
- en: 13      107.98  103.07  105.54  106.51  107.35  107.98  32
  prefs: []
  type: TYPE_NORMAL
- en: 14      103.53  95.38   96.38   98.34   102.92  103.53  32
  prefs: []
  type: TYPE_NORMAL
- en: 15      89.47   84.86   85.31   87.01   90.26   90.26   512
  prefs: []
  type: TYPE_NORMAL
- en: 16      81.53   75.49   75.82   74.36   76.91   81.53   32
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.11\. Atomic Operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Support for atomic operations was added in SM 1.x, but they were prohibitively
    slow; atomics on global memory were improved on SM 2.x (Fermi-class) hardware
    and vastly improved on SM 3.x (Kepler-class) hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Most atomic operations, such as `atomicAdd()`, enable code to be simplified
    by replacing reductions (which often require shared memory and synchronization)
    with “fire and forget” semantics. Until SM 3.x hardware arrived, however, that
    type of programming idiom incurred huge performance degradations because pre-Kepler
    hardware was not efficient at dealing with “contended” memory locations (i.e.,
    when many GPU threads are performing atomics on the same memory location).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Because atomic operations are implemented in the GPU memory controller, they
    only work on local device memory locations. As of this writing, trying to perform
    atomic operations on remote GPUs (via peer-to-peer addresses) or host memory (via
    mapped pinned memory) will not work.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Atomics and Synchronization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Besides “fire and forget” semantics, atomics also may be used for synchronization
    between blocks. CUDA hardware supports the workhorse base abstraction for synchronization,
    “compare and swap” (or CAS). On CUDA, compare-and-swap (also known as compare-and-exchange—e.g.,
    the `CMPXCHG` instruction in x86) is defined as follows.
  prefs: []
  type: TYPE_NORMAL
- en: int atomicCAS( int *address, int expected, int value);^([12](ch05.html#ch05fn12))
  prefs: []
  type: TYPE_NORMAL
- en: '[12](ch05.html#ch05fn12a). Unsigned and 64-bit variants of `atomicCAS()` also
    are available.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This function reads the word `old` at `address`, computes `(old == expected
    ? value : old)`, stores the result back to `address`, and returns `old`. In other
    words, the memory location is left alone *unless it was equal to the expected
    value specified by the caller*, in which case it is updated with the new value.'
  prefs: []
  type: TYPE_NORMAL
- en: A simple critical section called a “spin lock” can be built out of CAS, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: void enter_spinlock( int *address )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: while atomicCAS( address, 0, 1 );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Assuming the spin lock’s value is initialized to 0, the `while` loop iterates
    until the spin lock value is 0 when the `atomicCAS()` is executed. When that happens,
    `*address` atomically becomes 1 (the third parameter to `atomicCAS()`) and any
    other threads trying to acquire the critical section spin waiting for the critical
    section value to become 0 again.
  prefs: []
  type: TYPE_NORMAL
- en: The thread owning the spin lock can give it up by atomically swapping the 0
    back in
  prefs: []
  type: TYPE_NORMAL
- en: void leave_spinlock( int *address )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: atomicExch( m_p, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: On CPUs, compare-and-swap instructions are used to implement all manner of synchronization.
    Operating systems use them (sometimes in conjunction with the kernel-level thread
    context switching code) to implement higher-level synchronization primitives.
    CAS also may be used directly to implement “lock-free” queues and other data structures.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA execution model, however, imposes restrictions on the use of global
    memory atomics for synchronization. Unlike CPU threads, some CUDA threads within
    a kernel launch may not begin execution until other threads in the same kernel
    have exited. On CUDA hardware, each SM can context switch a limited number of
    thread blocks, so any kernel launch with more than *MaxThreadBlocksPerSM***NumSMs*
    requires the first thread blocks to exit before more thread blocks can begin execution.
    As a result, it is important that developers not assume all of the threads in
    a given kernel launch are active.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the `enter_spinlock()` routine above is prone to deadlock if used
    for intrablock synchronization,^([13](ch05.html#ch05fn13)) for which it is unsuitable
    in any case, since the hardware supports so many better ways for threads within
    the same block to communicate and synchronize with one another (shared memory
    and `__syncthreads()`, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: '[13](ch05.html#ch05fn13a). Expected usage is for one thread in each block to
    attempt to acquire the spinlock. Otherwise, the divergent code execution tends
    to deadlock.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5.8](ch05.html#ch05lis08) shows the implementation of the `cudaSpinlock`
    class, which uses the algorithm listed above and is subject to the just-described
    limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.8.* `cudaSpinlock` class.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis08a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: class cudaSpinlock {
  prefs: []
  type: TYPE_NORMAL
- en: 'public:'
  prefs: []
  type: TYPE_NORMAL
- en: cudaSpinlock( int *p );
  prefs: []
  type: TYPE_NORMAL
- en: void acquire();
  prefs: []
  type: TYPE_NORMAL
- en: void release();
  prefs: []
  type: TYPE_NORMAL
- en: 'private:'
  prefs: []
  type: TYPE_NORMAL
- en: int *m_p;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__
  prefs: []
  type: TYPE_NORMAL
- en: cudaSpinlock::cudaSpinlock( int *p )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: m_p = p;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ void
  prefs: []
  type: TYPE_NORMAL
- en: cudaSpinlock::acquire( )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: while ( atomicCAS( m_p, 0, 1 ) );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ void
  prefs: []
  type: TYPE_NORMAL
- en: cudaSpinlock::release( )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: atomicExch( m_p, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Use of `cudaSpinlock` is illustrated in the `spinlockReduction.cu` sample, which
    computes the sum of an array of `double` values by having each block perform a
    reduction in shared memory, then using the spin lock to synchronize for the summation.
    [Listing 5.9](ch05.html#ch05lis09) gives the `SumDoubles` function from this sample.
    Note how adding the partial sum is performed only by thread 0 of each block.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.9.* `SumDoubles` function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis09a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: SumDoubles(
  prefs: []
  type: TYPE_NORMAL
- en: double *pSum,
  prefs: []
  type: TYPE_NORMAL
- en: int *spinlock,
  prefs: []
  type: TYPE_NORMAL
- en: const double *in,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: int *acquireCount )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: SharedMemory<double> shared;
  prefs: []
  type: TYPE_NORMAL
- en: cudaSpinlock globalSpinlock( spinlock );
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: shared[threadIdx.x] = in[i];
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: double blockSum = Reduce_block<double,double>( );
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: if ( threadIdx.x == 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: globalSpinlock.acquire( );
  prefs: []
  type: TYPE_NORMAL
- en: '*pSum += blockSum;'
  prefs: []
  type: TYPE_NORMAL
- en: __threadfence();
  prefs: []
  type: TYPE_NORMAL
- en: globalSpinlock.release( );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.12\. Texturing from Global Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For applications that cannot conveniently adhere to the coalescing constraints,
    the texture mapping hardware presents a satisfactory alternative. The hardware
    supports texturing from global memory (via `cudaBindTexture()/cuTexRefSetAddress()`,
    which has lower peak performance than coalesced global reads but higher performance
    for less-regular access. The texture cache resources are also separate from other
    cache resources on the chip. A software coherency scheme is enforced by the driver
    invalidating the texture cache before kernel invocations that contain `TEX` instructions.^([14](ch05.html#ch05fn14))
    See [Chapter 10](ch10.html#ch10) for details.
  prefs: []
  type: TYPE_NORMAL
- en: '[14](ch05.html#ch05fn14a). `TEX` is the SASS mnemonic for microcode instructions
    that perform texture fetches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SM 3.x hardware added the ability to read global memory through the texture
    cache hierarchy without setting up and binding a texture reference. This functionality
    may be accessed with a standard C++ language constructs: the `const restrict`
    keywords. Alternatively, you can use the `__ldg()` intrinsics defined in `sm_35_intrinsics.h`.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.13\. ECC (Error Correcting Codes)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SM 2.x and later GPUs in the Tesla (i.e., server GPU) product line come with
    the ability to run with error correction. In exchange for a smaller amount of
    memory (since some memory is used to record some redundancy) and lower bandwidth,
    GPUs with ECC enabled can silently correct single-bit errors and report double-bit
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: ECC has the following characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: • It reduces the amount of available memory by 12.5%. On a `cg1.4xlarge` instance
    in Amazon EC2, for example, it reduces the amount of memory from 3071MB to 2687MB.
  prefs: []
  type: TYPE_NORMAL
- en: • It makes context synchronization more expensive.
  prefs: []
  type: TYPE_NORMAL
- en: • Uncoalesced memory transactions are more expensive when ECC is enabled than
    otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: ECC can be enabled and disabled using the `nvidia-smi` command-line tool (described
    in [Section 4.4](ch04.html#ch04lev1sec4)) or by using the NVML (NVIDIA Management
    Library).
  prefs: []
  type: TYPE_NORMAL
- en: When an uncorrectable ECC error is detected, synchronous error-reporting mechanisms
    will return `cudaErrorECCUncorrectable` (for the CUDA runtime) and `CUDA_ERROR_ECC_UNCORRECTABLE`
    (for the driver API).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Constant Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Constant memory is optimized for read-only broadcast to multiple threads. As
    the name implies, the compiler uses constant memory to hold constants that couldn’t
    be easily computed or otherwise compiled directly into the machine code. Constant
    memory resides in device memory but is accessed using different instructions that
    cause the GPU to access it using a special “constant cache.”
  prefs: []
  type: TYPE_NORMAL
- en: The compiler for constants has 64K of memory available to use at its discretion.
    The developer has another 64K of memory available that can be declared with the
    `__constant__` keyword. These limits are per-module (for driver API applications)
    or per-file (for CUDA runtime applications).
  prefs: []
  type: TYPE_NORMAL
- en: Naïvely, one might expect `__constant__` memory to be analogous to the `const`
    keyword in C/C++, where it cannot be changed after initialization. But `__constant__`
    memory can be changed, either by memory copies or by querying the pointer to `__constant__`
    memory and writing to it with a kernel. CUDA kernels must not write to `__constant__`
    memory ranges that they may be accessing because the constant cache is not kept
    coherent with respect to the rest of the memory hierarchy during kernel execution.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1\. Host and Device `__constant__` Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mark Harris describes the following idiom that uses the predefined macro `__CUDA_ARCH__`
    to maintain host and device copies of `__constant__` memory that are conveniently
    accessed by both the CPU and GPU.^([15](ch05.html#ch05fn15))
  prefs: []
  type: TYPE_NORMAL
- en: '[15](ch05.html#ch05fn15a). [http://bit.ly/OpMdN5](http://bit.ly/OpMdN5)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p157pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: __constant__ double dc_vals[2] = { 0.0, 1000.0 };
  prefs: []
  type: TYPE_NORMAL
- en: const double hc_vals[2] = { 0.0, 1000.0 };
  prefs: []
  type: TYPE_NORMAL
- en: __device__ __host__ double f(size_t i)
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '#ifdef __CUDA_ARCH__'
  prefs: []
  type: TYPE_NORMAL
- en: return dc_vals[i];
  prefs: []
  type: TYPE_NORMAL
- en: '#else'
  prefs: []
  type: TYPE_NORMAL
- en: return hc_vals[i];
  prefs: []
  type: TYPE_NORMAL
- en: '#endif'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2\. Accessing `__constant__` Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Besides the accesses to constant memory implicitly caused by C/C++ operators,
    developers can copy to and from constant memory, and even query the pointer to
    a constant memory allocation.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: CUDA runtime applications can copy to and from `__constant__` memory using `cudaMemcpyToSymbol()`
    and `cudaMemcpyFromSymbol()`, respectively. The pointer to `__constant__` memory
    can be queried with `cudaGetSymbolAddress()`.
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaGetSymbolAddress( void **devPtr, char *symbol );
  prefs: []
  type: TYPE_NORMAL
- en: This pointer may be used to write to constant memory with a kernel, though developers
    must take care not to write to the constant memory while another kernel is reading
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Driver API applications can query the device pointer of constant memory using
    `cuModuleGetGlobal()`. The driver API does not include a special memory copy function
    like `cudaMemcpyToSymbol()`, since it does not have the language integration of
    the CUDA runtime. Applications must query the address with `cuModuleGetGlobal()`
    and then call `cuMemcpyHtoD()` or `cuMemcpyDtoH()`. The amount of constant memory
    used by a kernel may be queried with `cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_CONSTANT_SIZE_BYTES)`.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. Local Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Local memory contains the stack for every thread in a CUDA kernel. It is used
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: • To implement the application binary interface (ABI)—that is, the calling convention
  prefs: []
  type: TYPE_NORMAL
- en: • To spill data out of registers
  prefs: []
  type: TYPE_NORMAL
- en: • To hold arrays whose indices cannot be resolved by the compiler
  prefs: []
  type: TYPE_NORMAL
- en: In early implementations of CUDA hardware, *any* use of local memory was the
    “kiss of death.” It slowed things down so much that developers were encouraged
    to take whatever measure was needed to get rid of the local memory usage. With
    the advent of an L1 cache in Fermi, these performance concerns are less urgent,
    provided the local memory traffic is confined to L1.^([16](ch05.html#ch05fn16))
  prefs: []
  type: TYPE_NORMAL
- en: '[16](ch05.html#ch05fn16a). The L1 cache is per-SM and is physically implemented
    in the same hardware as shared memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Developers can make the compiler report the amount of local memory needed by
    a given kernel with the `nvcc` options: `-Xptxas –v,abi=no.` At runtime, the amount
    of local memory used by a kernel may be queried with'
  prefs: []
  type: TYPE_NORMAL
- en: cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES).
  prefs: []
  type: TYPE_NORMAL
- en: 'Paulius Micikevicius of NVIDIA gave a good presentation on how to determine
    whether local memory usage was impacting performance and what to do about it.^([17](ch05.html#ch05fn17))
    Register spilling can incur two costs: an increased number of instructions and
    an increase in the amount of memory traffic.'
  prefs: []
  type: TYPE_NORMAL
- en: '[17](ch05.html#ch05fn17a). [http://bit.ly/ZAeHc5](http://bit.ly/ZAeHc5)'
  prefs: []
  type: TYPE_NORMAL
- en: The L1 and L2 performance counters can be used to determine if the memory traffic
    is impacting performance. Here are some strategies to improve performance in this
    case.
  prefs: []
  type: TYPE_NORMAL
- en: • At compile time, specify a higher limit in `–maxregcount.` By increasing the
    number of registers available to the thread, both the instruction count and the
    memory traffic will decrease. The `__launch_bounds__` directive may be used to
    tune this parameter when the kernel is being compiled online by PTXAS.
  prefs: []
  type: TYPE_NORMAL
- en: • Use noncaching loads for global memory, such as `nvcc –Xptxas -dlcm=cg.`
  prefs: []
  type: TYPE_NORMAL
- en: • Increase the L1 size to 48K. (Call `cudaFuncSetCacheConfig()` or `cudaDeviceSetCacheconfig().`)
  prefs: []
  type: TYPE_NORMAL
- en: When launching a kernel that uses more than the default amount of memory allocated
    for local memory, the CUDA driver must allocate a new local memory buffer before
    the kernel can launch. As a result, the kernel launch may take extra time; may
    cause unexpected CPU/GPU synchronization; and, if the driver is unable to allocate
    the buffer for local memory, may fail.^([18](ch05.html#ch05fn18)) By default,
    the CUDA driver will free these larger local memory allocations after the kernel
    has launched. This behavior can be inhibited by specifying the `CU_CTX_RESIZE_LMEM_TO_MAX`
    flag to `cuCtxCreate()` or calling `cudaSetDeviceFlags()` with the `cudaDeviceLmemResizeToMax`
    flag set.
  prefs: []
  type: TYPE_NORMAL
- en: '[18](ch05.html#ch05fn18a). Since most resources are preallocated, an inability
    to allocate local memory is one of the few circumstances that can cause a kernel
    launch to fail at runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: It is not difficult to build a templated function that illustrates the “performance
    cliff” when register spills occur. The templated `GlobalCopy` kernel in [Listing
    5.10](ch05.html#ch05lis10) implements a simple memcpy routine that uses a local
    array temp to stage global memory references. The template parameter `n` specifies
    the number of elements in `temp` and thus the number of loads and stores to perform
    in the inner loop of the memory copy.
  prefs: []
  type: TYPE_NORMAL
- en: As a quick review of the SASS microcode emitted by the compiler will confirm,
    the compiler can keep `temp` in registers until `n` becomes too large.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.10.* `GlobalCopy` kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis10a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, const int n>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: GlobalCopy( T *out, const T *in, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: T temp[n];
  prefs: []
  type: TYPE_NORMAL
- en: size_t i;
  prefs: []
  type: TYPE_NORMAL
- en: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N-n*blockDim.x*gridDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: i += n*blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+j*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: temp[j] = in[index];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+j*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: out[index] = temp[j];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // to avoid the (index<N) conditional in the inner loop,
  prefs: []
  type: TYPE_NORMAL
- en: // we left off some work at the end
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+j*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: if ( index<N ) temp[j] = in[index];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+j*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: if ( index<N ) out[index] = temp[j];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 5.11](ch05.html#ch05lis11) shows an excerpt of the output from `globalCopy.cu`
    on a GK104 GPU: the copy performance of 64-bit operands only. The degradation
    in performance due to register spilling becomes obvious in the row corresponding
    to a loop unroll of 12, where the delivered bandwidth decreases from 117GB/s to
    less than 90GB/s, and degrades further to under 30GB/s as the loop unroll increases
    to 16.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5.9](ch05.html#ch05tab09) summarizes the register and local memory usage
    for the kernels corresponding to the unrolled loops. The performance degradation
    of the copy corresponds to the local memory usage. In this case, every thread
    always spills in the inner loop; presumably, the performance wouldn’t degrade
    so much if only some of the threads were spilling (for example, when executing
    a divergent code path).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.9* `globalCopy` Register and Local Memory Usage'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 5.11.* `globalCopy.cu` output (64-bit only).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p05lis11a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operand size: 8 bytes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input size: 16M operands'
  prefs: []
  type: TYPE_NORMAL
- en: Block Size
  prefs: []
  type: TYPE_NORMAL
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  prefs: []
  type: TYPE_NORMAL
- en: 1       75.57   102.57  116.03  124.51  126.21  126.21  512
  prefs: []
  type: TYPE_NORMAL
- en: 2       105.73  117.09  121.84  123.07  124.00  124.00  512
  prefs: []
  type: TYPE_NORMAL
- en: 3       112.49  120.88  121.56  123.09  123.44  123.44  512
  prefs: []
  type: TYPE_NORMAL
- en: 4       115.54  122.89  122.38  122.15  121.22  122.89  64
  prefs: []
  type: TYPE_NORMAL
- en: 5       113.81  121.29  120.11  119.69  116.02  121.29  64
  prefs: []
  type: TYPE_NORMAL
- en: 6       114.84  119.49  120.56  118.09  117.88  120.56  128
  prefs: []
  type: TYPE_NORMAL
- en: 7       117.53  122.94  118.74  116.52  110.99  122.94  64
  prefs: []
  type: TYPE_NORMAL
- en: 8       116.89  121.68  119.00  113.49  105.69  121.68  64
  prefs: []
  type: TYPE_NORMAL
- en: 9       116.10  120.73  115.96  109.48  99.60   120.73  64
  prefs: []
  type: TYPE_NORMAL
- en: 10      115.02  116.70  115.30  106.31  93.56   116.70  64
  prefs: []
  type: TYPE_NORMAL
- en: 11      113.67  117.36  111.48  102.84  88.31   117.36  64
  prefs: []
  type: TYPE_NORMAL
- en: 12      88.16   86.91   83.68   73.78   58.55   88.16   32
  prefs: []
  type: TYPE_NORMAL
- en: 13      85.27   85.58   80.09   68.51   52.66   85.58   64
  prefs: []
  type: TYPE_NORMAL
- en: 14      78.60   76.30   69.50   56.59   41.29   78.60   32
  prefs: []
  type: TYPE_NORMAL
- en: 15      69.00   65.78   59.82   48.41   34.65   69.00   32
  prefs: []
  type: TYPE_NORMAL
- en: 16      65.68   62.16   54.71   43.02   29.92   65.68   32
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5\. Texture Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In CUDA, the concept of texture memory is realized in two parts: a *CUDA array*
    contains the physical memory allocation, and a *texture reference* or *surface
    reference*^([19](ch05.html#ch05fn19)) contains a “view” that can be used to read
    or write a CUDA array. The CUDA array is just an untyped “bag of bits” with a
    memory layout optimized for 1D, 2D, or 3D access. A texture reference contains
    information on how the CUDA array should be addressed and how its contents should
    be interpreted.'
  prefs: []
  type: TYPE_NORMAL
- en: '[19](ch05.html#ch05fn19a). Surface references can be used only on SM 2.x and
    later hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: When using a texture reference to read from a CUDA array, the hardware uses
    a separate, read-only cache to resolve the memory references. While the kernel
    is executing, the texture cache is not kept coherent with respect to the rest
    of the memory subsystem, so it is important not to use texture references to alias
    memory that will be operated on by the kernel. (The cache is invalidated between
    kernel launches.)
  prefs: []
  type: TYPE_NORMAL
- en: On SM 3.5 hardware, reads via texture can be explicitly requested by the developer
    using the `const restricted` keywords. The `restricted` keyword does nothing more
    than make the just-described “no aliasing” guarantee that the memory in question
    won’t be referenced by the kernel in any other way. When reading or writing a
    CUDA array with a surface reference, the memory traffic goes through the same
    memory hierarchy as global loads and stores. [Chapter 10](ch10.html#ch10) contains
    a detailed discussion of how to allocate and use textures in CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6\. Shared Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Shared memory is used to exchange data between CUDA threads within a block.
    Physically, it is implemented with a per-SM memory that can be accessed very quickly.
    In terms of speed, shared memory is perhaps 10x slower than register accesses
    but 10x faster than accesses to global memory. As a result, shared memory is often
    a critical resource to reduce the external bandwidth needed by CUDA kernels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since developers explicitly allocate and reference shared memory, it can be
    thought of as a “manually managed” cache or “scratchpad” memory. Developers can
    request different cache configurations at both the kernel and the device level:
    `cudaDeviceSetCacheConfig()/cuCtxSetCacheConfig()` specify the preferred cache
    configuration for a CUDA device, while `cudaFuncSetCacheConfig()/cuFuncSetCacheConfig()`
    specify the preferred cache configuration for a given kernel. If both are specified,
    the per-kernel request takes precedence, but in any case, the requirements of
    the kernel may override the developer’s preference.'
  prefs: []
  type: TYPE_NORMAL
- en: Kernels that use shared memory typically are written in three phases.
  prefs: []
  type: TYPE_NORMAL
- en: • Load shared memory and `__syncthreads()`
  prefs: []
  type: TYPE_NORMAL
- en: • Process shared memory and `__syncthreads()`
  prefs: []
  type: TYPE_NORMAL
- en: • Write results
  prefs: []
  type: TYPE_NORMAL
- en: 'Developers can make the compiler report the amount of shared memory used by
    a given kernel with the `nvcc` options: `-Xptxas –v,abi=no.` At runtime, the amount
    of shared memory used by a kernel may be queried with `cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.6.1\. Unsized Shared Memory Declarations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Any shared memory declared in the kernel itself is automatically allocated for
    each block when the kernel is launched. If the kernel also includes an unsized
    declaration of shared memory, the amount of memory needed by that declaration
    must be specified when the kernel is launched.
  prefs: []
  type: TYPE_NORMAL
- en: If there is more than one `extern __shared__` memory declaration, they are aliased
    with respect to one another, so the declaration
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ char sharedChars[];
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ int sharedInts[];
  prefs: []
  type: TYPE_NORMAL
- en: enables the same shared memory to be addressed as 8- or 32-bit integers, as
    needed. One motivation for using this type of aliasing is to use wider types when
    possible to read and write global memory, while using the narrow ones for kernel
    computations.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you have more than one kernel that uses unsized shared memory, they must
    be compiled in separate files.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 5.6.2\. Warp-Synchronous Coding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Shared memory variables that will be participating in warp-synchronous programming
    must be declared as `volatile` to prevent the compiler from applying optimizations
    that will render the code incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6.3\. Pointers to Shared Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is valid—and often convenient—to use pointers to refer to shared memory.
    Example kernels that use this idiom include the reduction kernels in [Chapter
    12](ch12.html#ch12) ([Listing 12.3](ch12.html#ch12lis03)) and the `scanBlock`
    kernel in [Chapter 13](ch13.html#ch13) ([Listing 13.3](ch13.html#ch13lis03)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.7\. Memory Copy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CUDA has three different memory types—host memory, device memory, and CUDA arrays—and
    a full complement of functions to copy between them. For host↔device memcpy, an
    additional set of functions provide *asynchronous* memcpy between pinned host
    memory and device memory or CUDA arrays. Additionally, a set of peer-to-peer memcpy
    functions enable memory to be copied between GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime and the driver API take very different approaches. For 1D memcpy,
    the driver API defined a family of functions with type-strong parameters. The
    host-to-device, device-to-host, and device-to-device memcpy functions are separate.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p164pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuMemcpyHtoD(CUdeviceptr dstDevice, const void *srcHost,
  prefs: []
  type: TYPE_NORMAL
- en: size_t ByteCount);
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuMemcpyDtoH(void *dstHost, CUdeviceptr srcDevice, size_t
  prefs: []
  type: TYPE_NORMAL
- en: ByteCount);
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuMemcpyDtoD(CUdeviceptr dstDevice, CUdeviceptr srcDevice,
  prefs: []
  type: TYPE_NORMAL
- en: size_t ByteCount);
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the CUDA runtime tends to define functions that take an extra “memcpy
    kind” parameter that depends on the memory types of the host and destination pointers.
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaMemcpyKind
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToHost = 0,
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice = 1,
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost = 2,
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToDevice = 3,
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDefault = 4
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: For more complex memcpy operations, both APIs use descriptor structures to specify
    the memcpy.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7.1\. Synchronous versus Asynchronous Memcpy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because most aspects of memcpy (dimensionality, memory type) are independent
    of whether the memory copy is asynchronous, this section examines the difference
    in detail, and later sections include minimal coverage of asynchronous memcpy.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, any memcpy involving host memory is *synchronous*: The function
    does not return until after the operation has been performed.^([20](ch05.html#ch05fn20))
    Even when operating on pinned memory, such as memory allocated with `cudaMallocHost()`,
    synchronous memcpy routines must wait until the operation is completed because
    the application may rely on that behavior.^([21](ch05.html#ch05fn21))'
  prefs: []
  type: TYPE_NORMAL
- en: '[20](ch05.html#ch05fn20a). This is because the hardware cannot directly access
    host memory unless it has been page-locked and mapped for the GPU. An asynchronous
    memory copy for pageable memory could be implemented by spawning another CPU thread,
    but so far, the CUDA team has chosen to avoid that additional complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: '[21](ch05.html#ch05fn21a). When pinned memory is specified to a synchronous
    memcpy routine, the driver does take advantage by having the hardware use DMA,
    which is generally faster.'
  prefs: []
  type: TYPE_NORMAL
- en: When possible, synchronous memcpy should be avoided for performance reasons.
    Even when streams are not being used, keeping all operations asynchronous improves
    performance by enabling the CPU and GPU to run concurrently. If nothing else,
    the CPU can set up more GPU operations such as kernel launches and other memcpys
    while the GPU is running! If CPU/GPU concurrency is the only goal, there is no
    need to create any CUDA streams; calling an asynchronous memcpy with the NULL
    stream will suffice.
  prefs: []
  type: TYPE_NORMAL
- en: While memcpys involving host memory are synchronous by default, any memory copy
    not involving host memory (device↔device or device↔array) is asynchronous. The
    GPU hardware internally enforces serialization on these operations, so there is
    no need for the functions to wait until the GPU has finished before returning.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous memcpy functions have the suffix `Async()`. For example, the driver
    API function for asynchronous host→device memcpy is `cuMemcpyHtoDAsync()` and
    the CUDA runtime function is `cudaMemcpyAsync()`.
  prefs: []
  type: TYPE_NORMAL
- en: The hardware that implements asynchronous memcpy has evolved over time. The
    very first CUDA-capable GPU (the GeForce 8800 GTX) did not have any copy engines,
    so asynchronous memcpy only enabled CPU/GPU concurrency. Later GPUs added copy
    engines that could perform 1D transfers while the SMs were running, and still
    later, fully capable copy engines were added that could accelerate 2D and 3D transfers,
    even if the copy involved converting between pitch layouts and the block-linear
    layouts used by CUDA arrays. Additionally, early CUDA hardware only had one copy
    engine, whereas today, it sometimes has two. More than two copy engines wouldn’t
    necessarily make sense. Because a single copy engine can saturate the PCI Express
    bus in one direction, only two copy engines are needed to maximize both bus performance
    and concurrency between bus transfers and GPU computation.
  prefs: []
  type: TYPE_NORMAL
- en: The number of copy engines can be queried by calling `cuDeviceGetAttribute()`
    with `CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT`, or by examining the `cudaDeviceProp::asyncEngineCount`.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7.2\. Unified Virtual Addressing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unified Virtual Addressing enables CUDA to make inferences about memory types
    based on address ranges. Because CUDA tracks which address ranges contain device
    addresses versus host addresses, there is no need to specify `cudaMemcpyKind`
    parameter to the `cudaMemcpy()` function. The driver API added a `cuMemcpy()`
    function that similarly infers the memory types from the addresses.
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuMemcpy(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount);
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime equivalent, not surprisingly, is called `cudaMemcpy():`
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMemcpy( void *dst, const void *src, size_t bytes );.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7.3\. CUDA Runtime
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Table 5.10](ch05.html#ch05tab10) summarizes the memcpy functions available
    in the CUDA runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab10.jpg)![Image](graphics/05tab10a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.10* Memcpy Functions (CUDA Runtime)'
  prefs: []
  type: TYPE_NORMAL
- en: 1D and 2D memcpy functions take base pointers, pitches, and sizes as required.
    The 3D memcpy routines take a descriptor structure `cudaMemcpy3Dparms`, defined
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p168pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaMemcpy3DParms
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaArray *srcArray;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPos srcPos;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr srcPtr;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaArray *dstArray;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPos dstPos;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr dstPtr;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaExtent extent;
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaMemcpyKind kind;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 5.11](ch05.html#ch05tab11) summarizes the meaning of each member of
    the `cudaMemcpy3DParms` structure. The `cudaPos` and `cudaExtent` structures are
    defined as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaExtent {
  prefs: []
  type: TYPE_NORMAL
- en: size_t width;
  prefs: []
  type: TYPE_NORMAL
- en: size_t height;
  prefs: []
  type: TYPE_NORMAL
- en: size_t depth;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPos {
  prefs: []
  type: TYPE_NORMAL
- en: size_t x;
  prefs: []
  type: TYPE_NORMAL
- en: size_t y;
  prefs: []
  type: TYPE_NORMAL
- en: size_t z;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.11* cudaMemcpy3DParms Structure Members'
  prefs: []
  type: TYPE_NORMAL
- en: 5.7.4\. Driver API
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Table 5.12](ch05.html#ch05tab12) summarizes the driver API’s memcpy functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/05tab12.jpg)![Image](graphics/05tab12a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 5.12* Memcpy Functions (Driver API)'
  prefs: []
  type: TYPE_NORMAL
- en: '`cuMemcpy3D()` is designed to implement a strict superset of all previous memcpy
    functionality. Any 1D, 2D, or 3D memcpy may be performed between any host, device,
    or CUDA array memory, and any offset into either the source or destination may
    be applied. The `WidthInBytes`, `Height,` and `Depth` members of the input structure,
    `CUDA_MEMCPY_3D`, define the dimensionality of the memcpy: `Height==0` implies
    a 1D memcpy, and `Depth==0` implies a 2D memcpy. The source and destination memory
    types are given by the `srcMemoryType` and `dstMemoryType` structure elements,
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Structure elements that are not needed by `cuMemcpy3D()` are defined to be ignored.
    For example, if a 1D host→device memcpy is requested, the `srcPitch`, `srcHeight`,
    `dstPitch`, and `dstHeight` elements are ignored. If `srcMemoryType` is `CU_MEMORYTYPE_HOST`,
    the `srcDevice` and `srcArray` elements are ignored. This API semantic, coupled
    with the C idiom that assigning {0} to a structure zero-initializes it, enables
    memory copies to be described very concisely. Most other memcpy functions can
    be implemented in a few lines of code, such as the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch05_images.html#p171pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult
  prefs: []
  type: TYPE_NORMAL
- en: my_cuMemcpyHtoD( CUdevice dst, const void *src, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_MEMCPY_3D cp = {0};
  prefs: []
  type: TYPE_NORMAL
- en: cp.srcMemoryType = CU_MEMORYTYPE_HOST;
  prefs: []
  type: TYPE_NORMAL
- en: cp.srcHost = srcHost;
  prefs: []
  type: TYPE_NORMAL
- en: cp.dstMemoryType = CU_MEMORYTYPE_DEVICE;
  prefs: []
  type: TYPE_NORMAL
- en: cp.dstDevice = dst;
  prefs: []
  type: TYPE_NORMAL
- en: cp.WidthInBytes = N;
  prefs: []
  type: TYPE_NORMAL
- en: return cuMemcpy3D( &cp );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 6\. Streams and Events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CUDA is best known for enabling fine-grained concurrency, with hardware facilities
    that enable threads to closely collaborate within blocks using a combination of
    shared memory and thread synchronization. But it also has hardware and software
    facilities that enable more coarse-grained concurrency:'
  prefs: []
  type: TYPE_NORMAL
- en: • **CPU/GPU concurrency:** Since they are separate devices, the CPU and GPU
    can operate independently of each other.
  prefs: []
  type: TYPE_NORMAL
- en: • **Memcpy/kernel processing concurrency:** For GPUs that have one or more copy
    engines, host↔device memcpy can be performed while the SMs are processing kernels.
  prefs: []
  type: TYPE_NORMAL
- en: • **Kernel concurrency:** SM 2.x-class and later hardware can run up to 4 kernels
    in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: • **Multi-GPU concurrency:** For problems with enough computational density,
    multiple GPUs can operate in parallel. ([Chapter 9](ch09.html#ch09) is dedicated
    to multi-GPU programming.)
  prefs: []
  type: TYPE_NORMAL
- en: CUDA streams enable these types of concurrency. Within a given stream, operations
    are performed in sequential order, but operations in different streams may be
    performed in parallel. CUDA events complement CUDA streams by providing the synchronization
    mechanisms needed to coordinate the parallel execution enabled by streams. CUDA
    events may be asynchronously “recorded” into a stream, and the CUDA event becomes
    signaled when the operations preceding the CUDA event have been completed.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA events may be used for CPU/GPU synchronization, for synchronization between
    the engines on the GPU, and for synchronization between GPUs. They also provide
    a GPU-based timing mechanism that cannot be perturbed by system events such as
    page faults or interrupts from disk or network controllers. Wall clock timers
    are best for overall timing, but CUDA events are useful for optimizing kernels
    or figuring out which of a series of pipelined GPU operations is taking the longest.
    All of the performance results reported in this chapter were gathered on a `cg1.4xlarge`
    cloud-based server from Amazon’s EC2 service, as described in [Section 4.5](ch04.html#ch04lev1sec5).
  prefs: []
  type: TYPE_NORMAL
- en: '6.1\. CPU/GPU Concurrency: Covering Driver Overhead'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CPU/GPU concurrency refers to the CPU’s ability to continue processing after
    having sent some request to the GPU. Arguably, the most important use of CPU/GPU
    concurrency is hiding the overhead of requesting work from the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1\. Kernel Launches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kernel launches have always been asynchronous. A series of kernel launches,
    with no intervening CUDA operations in between, cause the CPU to submit the kernel
    launch to the GPU and return control to the caller before the GPU has finished
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: We can measure the driver overhead by bracketing a series of NULL kernel launches
    with timing operations. [Listing 6.1](ch06.html#ch06lis01) shows `nullKernelAsync.cu`,
    a small program that measures the amount of time needed to perform a kernel launch.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 6.1.* `nullKernelAsync.cu.`'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p06lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '#include <stdio.h>'
  prefs: []
  type: TYPE_NORMAL
- en: '#include "chTimer.h"'
  prefs: []
  type: TYPE_NORMAL
- en: __global__
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: NullKernel()
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: main( int argc, char *argv[] )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: const int cIterations = 1000000;
  prefs: []
  type: TYPE_NORMAL
- en: printf( "Launches... " ); fflush( stdout );
  prefs: []
  type: TYPE_NORMAL
- en: chTimerTimestamp start, stop;
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &start );
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < cIterations; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: NullKernel<<<1,1>>>();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: cudaThreadSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &stop );
  prefs: []
  type: TYPE_NORMAL
- en: double microseconds = 1e6*chTimerElapsedTime( &start, &stop );
  prefs: []
  type: TYPE_NORMAL
- en: double usPerLaunch = microseconds / (float) cIterations;
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%.2f us\n", usPerLaunch );
  prefs: []
  type: TYPE_NORMAL
- en: return 0;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The `chTimerGetTime()` calls, described in [Appendix A](app01.html#app01), use
    the host operating system’s high-resolution timing facilities, such as `QueryPerformanceCounter()`
    or `gettimeofday()`. The `cudaThreadSynchronize()` call in line 23 is needed for
    accurate timing. Without it, the GPU would still be processing the last kernel
    invocations when the end top is recorded with the following function call.
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &stop );
  prefs: []
  type: TYPE_NORMAL
- en: If you run this program, you will see that invoking a kernel—even a kernel that
    does nothing—costs anywhere from 2.0 to 8.0 microseconds. Most of that time is
    spent in the driver. The CPU/GPU concurrency enabled by kernel launches only helps
    if the kernel runs for longer than it takes the driver to invoke it! To underscore
    the importance of CPU/GPU concurrency for small kernel launches, let’s move the
    `cudaThreadSynchronize()` call into the inner loop.^([1](ch06.html#ch06fn1))
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch06.html#ch06fn1a). This program is in the source code as `nullKernelSync.cu`
    and is not reproduced here because it is almost identical to [Listing 6.1](ch06.html#ch06lis01).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p175pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &start );
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < cIterations; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: NullKernel<<<1,1>>>();
  prefs: []
  type: TYPE_NORMAL
- en: cudaThreadSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &stop );
  prefs: []
  type: TYPE_NORMAL
- en: The only difference here is that the CPU is waiting until the GPU has finished
    processing each NULL kernel launch before launching the next kernel, as shown
    in [Figure 6.1](ch06.html#ch06fig01). As an example, on an Amazon EC2 instance
    with ECC disabled, `nullKernelNoSync` reports a time of 3.4 ms per launch and
    `nullKernelSync` reports a time of 100 ms per launch. So besides giving up CPU/GPU
    concurrency, the synchronization itself is worth avoiding.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/06fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6.1* CPU/GPU concurrency.'
  prefs: []
  type: TYPE_NORMAL
- en: Even without synchronizations, if the kernel doesn’t run for longer than the
    amount of time it took to launch the kernel (3.4 ms), the GPU may go idle before
    the CPU has submitted more work. To explore just how much work a kernel might
    need to do to make the launch worthwhile, let’s switch to a kernel that busy-waits
    until a certain number of clock cycles (according to the `clock()` intrinsic)
    has completed.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p176pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: __device__ int deviceTime;
  prefs: []
  type: TYPE_NORMAL
- en: __global__
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: WaitKernel( int cycles, bool bWrite )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int start = clock();
  prefs: []
  type: TYPE_NORMAL
- en: int stop;
  prefs: []
  type: TYPE_NORMAL
- en: do {
  prefs: []
  type: TYPE_NORMAL
- en: stop = clock();
  prefs: []
  type: TYPE_NORMAL
- en: '} while ( stop - start < cycles );'
  prefs: []
  type: TYPE_NORMAL
- en: if ( bWrite && threadIdx.x==0 && blockIdx.x==0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: deviceTime = stop - start;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: By conditionally writing the result to `deviceTime`, this kernel prevents the
    compiler from optimizing out the busy wait. The compiler does not know that we
    are just going to pass `false` as the second parameter.^([2](ch06.html#ch06fn2))
    The code in our `main()` function then checks the launch time for various values
    of cycles, from 0 to 2500.
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch06.html#ch06fn2a). The compiler could still invalidate our timing results
    by branching around the loop if `bWrite is false`. If the timing results looked
    suspicious, we could see if this is happening by looking at the microcode with
    `cuobjdump`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p177pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int cycles = 0; cycles < 2500; cycles += 100 ) {
  prefs: []
  type: TYPE_NORMAL
- en: 'printf( "Cycles: %d - ", cycles ); fflush( stdout );'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &start );
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < cIterations; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: WaitKernel<<<1,1>>>( cycles, false );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: cudaThreadSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &stop );
  prefs: []
  type: TYPE_NORMAL
- en: double microseconds = 1e6*chTimerElapsedTime( &start, &stop );
  prefs: []
  type: TYPE_NORMAL
- en: double usPerLaunch = microseconds / (float) cIterations;
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%.2f us\n", usPerLaunch );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: This program may be found in `waitKernelAsync.cu`. On our EC2 instance, the
    output is as in [Figure 6.2](ch06.html#ch06fig02). On this host platform, the
    breakeven mark where the kernel launch time crosses over 2x that of a NULL kernel
    launch (4.90 μs) is at 4500 GPU clock cycles.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/06fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6.2* Microseconds/cycles plot for `waitKernelAsync.cu`.'
  prefs: []
  type: TYPE_NORMAL
- en: These performance characteristics can vary widely and depend on many factors,
    including the following.
  prefs: []
  type: TYPE_NORMAL
- en: • Performance of the host CPU
  prefs: []
  type: TYPE_NORMAL
- en: • Host operating system
  prefs: []
  type: TYPE_NORMAL
- en: • Driver version
  prefs: []
  type: TYPE_NORMAL
- en: • Driver model (TCC versus WDDM on Windows)
  prefs: []
  type: TYPE_NORMAL
- en: • Whether ECC is enabled on the GPU^([3](ch06.html#ch06fn3))
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch06.html#ch06fn3a). When ECC is enabled, the driver must perform a kernel
    thunk to check whether any memory errors have occurred. As a result, `cudaThreadSynchronize()`
    is expensive even on platforms with user-mode client drivers.'
  prefs: []
  type: TYPE_NORMAL
- en: But the common underlying theme is that for most CUDA applications, developers
    should do their best to avoid breaking CPU/GPU concurrency. Only applications
    that are very compute-intensive and only perform large data transfers can afford
    to ignore this overhead. To take advantage of CPU/GPU concurrency when performing
    memory copies as well as kernel launches, developers must use *asynchronous memcpy*.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Asynchronous Memcpy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Like kernel launches, asynchronous memcpy calls return before the GPU has performed
    the memcpy in question. Because the GPU operates autonomously and can read or
    write the host memory without any operating system involvement, only pinned memory
    is eligible for asynchronous memcpy.
  prefs: []
  type: TYPE_NORMAL
- en: The earliest application for asynchronous memcpy in CUDA was hidden inside the
    CUDA 1.0 driver. The GPU cannot access pageable memory directly, so the driver
    implements pageable memcpy using a pair of pinned “staging buffers” that are allocated
    with the CUDA context. [Figure 6.3](ch06.html#ch06fig03) shows how this process
    works.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/06fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6.3* Pageable memcpy.'
  prefs: []
  type: TYPE_NORMAL
- en: To perform a host→device memcpy, the driver first “primes the pump” by copying
    to one staging buffer, then kicks off a DMA operation to read that data with the
    GPU. While the GPU begins processing that request, the driver copies more data
    into the other staging buffer. The CPU and GPU keep ping-ponging between staging
    buffers, with appropriate synchronization, until it is time for the GPU to perform
    the final memcpy. Besides copying data, the CPU also naturally pages in any nonresident
    pages while the data is being copied.
  prefs: []
  type: TYPE_NORMAL
- en: '6.2.1\. Asynchronous Memcpy: Host→Device'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As with kernel launches, asynchronous memcpys incur fixed CPU overhead in the
    driver. In the case of host→device memcpy, *all* memcpys below a certain size
    are asynchronous, because the driver copies the source data directly into the
    command buffer that it uses to control the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: We can write an application that measures asynchronous memcpy overhead, much
    as we measured kernel launch overhead earlier. The following code, in a program
    called `nullHtoDMemcpyAsync.cu`, reports that on a `cg1.4xlarge` instance in Amazon
    EC2, each memcpy takes 3.3 ms. Since PCI Express can transfer almost 2K in that
    time, it makes sense to examine how the time needed to perform a small memcpy
    grows with the size.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p180pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMalloc( &deviceInt, sizeof(int) ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaHostAlloc( &hostInt, sizeof(int), 0 ) );
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &start );
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < cIterations; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync( deviceInt, hostInt, sizeof(int),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice, NULL ) );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaThreadSynchronize() );
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &stop );
  prefs: []
  type: TYPE_NORMAL
- en: The `breakevenHtoDMemcpy.cu` program measures memcpy performance for sizes from
    4K to 64K. On a `cg1.4xlarge` instance in Amazon EC2, it generates [Figure 6.4](ch06.html#ch06fig04).
    The data generated by this program is clean enough to fit to a linear regression
    curve—in this case, with intercept 3.3 μs and slope 0.000170 μs/byte. The slope
    corresponds to 5.9GB/s, about the expected bandwidth from PCI Express 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/06fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6.4* Small host→device memcpy performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '6.2.2\. Asynchronous Memcpy: Device→Host'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `nullDtoHMemcpyNoSync.cu` and `breakevenDtoHMemcpy.cu` programs perform
    the same measurements for small device→host memcpys. On our trusty Amazon EC2
    instance, the minimum time for a memcpy is 4.00 μs ([Figure 6.5](ch06.html#ch06fig05)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/06fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6.5* Small device→host memcpy performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3\. The NULL Stream and Concurrency Breaks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Any streamed operation may be called with NULL as the stream parameter, and
    the operation will not be initiated until all the preceding operations on the
    GPU have been completed.^([4](ch06.html#ch06fn4)) Applications that have no need
    for copy engines to overlap memcpy operations with kernel processing can use the
    NULL stream to facilitate CPU/GPU concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch06.html#ch06fn4a). When CUDA streams were added in CUDA 1.1, the designers
    had a choice between making the NULL stream “its own” stream, separate from other
    streams and serialized only with itself, or making it synchronize with (“join”)
    all engines on the GPU. They opted for the latter, in part because CUDA did not
    yet have facilities for interstream synchronization.'
  prefs: []
  type: TYPE_NORMAL
- en: Once a streamed operation has been initiated with the NULL stream, the application
    must use synchronization functions such as `cuCtxSynchronize()` or `cudaThreadSynchronize()`
    to ensure that the operation has been completed before proceeding. But the application
    may request many such operations before performing the synchronization. For example,
    the application may perform an asynchronous host→device memcpy, one or more kernel
    launches, and an asynchronous device→host memcpy before synchronizing with the
    context. The `cuCtxSynchronize()` or `cudaThreadSynchronize()` call returns once
    the GPU has performed the most recently requested operation. This idiom is especially
    useful when performing smaller memcpys or launching kernels that will not run
    for long. The CUDA driver takes valuable CPU time to write commands to the GPU,
    and overlapping that CPU execution with the GPU’s processing of the commands can
    improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: '*Note:* Even in CUDA 1.0, kernel launches were asynchronous. As a result, the
    NULL stream is implicitly specified to all kernel launches if no stream is given.'
  prefs: []
  type: TYPE_NORMAL
- en: Breaking Concurrency
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Whenever an application performs a full CPU/GPU synchronization (having the
    CPU wait until the GPU is completely idle), performance suffers. We can measure
    this performance impact by switching our NULL-memcpy calls from asynchronous ones
    to synchronous ones just by changing the `cudaMemcpyAsync()` calls to `cudaMemcpy()`
    calls. The `nullDtoHMemcpySync.cu` program does just that for device→host memcpy.
  prefs: []
  type: TYPE_NORMAL
- en: On our trusty Amazon `cg1.4xlarge` instance, `nullDtoHMemcpySync.cu` reports
    about 7.9 μs per memcpy. If a Windows driver has to perform a kernel thunk, or
    the driver on an ECC-enabled GPU must check for ECC errors, full GPU synchronization
    is much costlier.
  prefs: []
  type: TYPE_NORMAL
- en: Explicit ways to perform this synchronization include the following.
  prefs: []
  type: TYPE_NORMAL
- en: • `cuCtxSynchronize()/cudaDeviceSynchronize()`
  prefs: []
  type: TYPE_NORMAL
- en: • `cuStreamSynchronize()/cudaStreamSynchronize()` on the NULL stream
  prefs: []
  type: TYPE_NORMAL
- en: • Unstreamed memcpy between host and device—for example, `cuMemcpyHtoD()`, `cuMemcpyDtoH()`,
    `cudaMemcpy()`
  prefs: []
  type: TYPE_NORMAL
- en: Other, more subtle ways to break CPU/GPU concurrency include the following.
  prefs: []
  type: TYPE_NORMAL
- en: • Running with the `CUDA_LAUNCH_BLOCKING` environment variable set
  prefs: []
  type: TYPE_NORMAL
- en: • Launching kernels that require local memory to be reallocated
  prefs: []
  type: TYPE_NORMAL
- en: • Performing large memory allocations or host memory allocations
  prefs: []
  type: TYPE_NORMAL
- en: • Destroying objects such as CUDA streams and CUDA events
  prefs: []
  type: TYPE_NORMAL
- en: Nonblocking Streams
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To create a stream that is exempt from the requirement to synchronize with the
    NULL stream (and therefore less likely to suffer a “concurrency break” as described
    above), specify the `CUDA_STREAM_NON_BLOCKING` flag to `cuStreamCreate()` or the
    `cudaStreamNonBlocking` flag to `cudaStreamCreateWithFlags()`.
  prefs: []
  type: TYPE_NORMAL
- en: '6.3\. CUDA Events: CPU/GPU Synchronization'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the key features of CUDA events is that they can enable “partial” CPU/GPU
    synchronization. Instead of full CPU/GPU synchronization where the CPU waits until
    the GPU is idle, introducing a bubble into the GPU’s work pipeline, CUDA events
    may be *recorded* into the asynchronous stream of GPU commands. The CPU then can
    wait until all of the work preceding the event has been done. The GPU can continue
    doing whatever work was submitted after the `cuEventRecord()/cudaEventRecord()`.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of CPU/GPU concurrency, [Listing 6.2](ch06.html#ch06lis02) gives
    a memcpy routine for pageable memory. The code for this program implements the
    algorithm described in [Figure 6.3](ch06.html#ch06fig03) and is located in `pageableMemcpyHtoD.cu`.
    It uses two pinned memory buffers, stored in global variables declared as follows.
  prefs: []
  type: TYPE_NORMAL
- en: void *g_hostBuffers[2];
  prefs: []
  type: TYPE_NORMAL
- en: and two CUDA events declared as
  prefs: []
  type: TYPE_NORMAL
- en: cudaEvent_t g_events[2];
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 6.2.* `chMemcpyHtoD()`—pageable memcpy.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p06lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: chMemcpyHtoD( void *device, const void *host, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: char *dst = (char *) device;
  prefs: []
  type: TYPE_NORMAL
- en: const char *src = (const char *) host;
  prefs: []
  type: TYPE_NORMAL
- en: int stagingIndex = 0;
  prefs: []
  type: TYPE_NORMAL
- en: while ( N ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventSynchronize( g_events[stagingIndex] );
  prefs: []
  type: TYPE_NORMAL
- en: memcpy( g_hostBuffers[stagingIndex], src, thisCopySize );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize, cudaMemcpyHostToDevice, NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( g_events[1-stagingIndex], NULL );
  prefs: []
  type: TYPE_NORMAL
- en: dst += thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: src += thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: N -= thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: stagingIndex = 1 - stagingIndex;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: return;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`chMemcpyHtoD()` is designed to maximize CPU/GPU concurrency by “ping-ponging”
    between the two host buffers. The CPU copies into one buffer, while the GPU pulls
    from the other. There is some “overhang” where no CPU/GPU concurrency is possible
    at the beginning and end of the operation when the CPU is copying the first and
    last buffers, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: In this program, the only synchronization needed—the `cudaEventSynchronize()`
    in line 11—ensures that the GPU has finished with a buffer before starting to
    copy into it. `cudaMemcpyAsync()` returns as soon as the GPU commands have been
    enqueued. It does not wait until the operation is complete. The `cudaEventRecord()`
    is also asynchronous. It causes the event to be signaled when the just-requested
    asynchronous memcpy has been completed.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA events are recorded immediately after creation so the first `cudaEventSynchronize()`
    calls in line 11 work correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p184pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventCreate( &g_events[0] ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventCreate( &g_events[1] ) );
  prefs: []
  type: TYPE_NORMAL
- en: // record events so they are signaled on first synchronize
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( g_events[0], 0 ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( g_events[1], 0 ) );
  prefs: []
  type: TYPE_NORMAL
- en: If you run `pageableMemcpyHtoD.cu`, it will report a bandwidth number much smaller
    than the pageable memcpy bandwidth delivered by the CUDA driver. That’s because
    the C runtime’s `memcpy()` implementation is not optimized to move memory as fast
    as the CPU can. For best performance, the memory must be copied using SSE instructions
    that can move data 16 bytes at a time. Writing a general-purpose memcpy using
    these instructions is complicated by their alignment restrictions, but a simple
    version that requires the source, destination, and byte count to be 16-byte aligned
    is not difficult.^([5](ch06.html#ch06fn5))
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch06.html#ch06fn5a). On some platforms, `nvcc` does not compile this code
    seamlessly. In the code accompanying this book, `memcpy16()` is in a separate
    file called `memcpy16.cpp`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p185pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '#include <xmmintrin.h>'
  prefs: []
  type: TYPE_NORMAL
- en: bool
  prefs: []
  type: TYPE_NORMAL
- en: memcpy16( void *_dst, const void *_src, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: if ( N & 0xf ) {
  prefs: []
  type: TYPE_NORMAL
- en: return false;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: float *dst = (float *) _dst;
  prefs: []
  type: TYPE_NORMAL
- en: const float *src = (const float *) _src;
  prefs: []
  type: TYPE_NORMAL
- en: while ( N ) {
  prefs: []
  type: TYPE_NORMAL
- en: _mm_store_ps( dst, _mm_load_ps( src ) );
  prefs: []
  type: TYPE_NORMAL
- en: src += 4;
  prefs: []
  type: TYPE_NORMAL
- en: dst += 4;
  prefs: []
  type: TYPE_NORMAL
- en: N -= 16;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return true;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: When the C runtime `memcpy()` is replaced by this one, performance on an Amazon
    EC2 `cg1.4xlarge` instance increases from 2155MB/s to 3267MB/s. More complicated
    memcpy routines can deal with relaxed alignment constraints, and slightly higher
    performance is possible by unrolling the inner loop. On `cg1.4xlarge`, the CUDA
    driver’s more optimized SSE memcpy achieves about 100MB/s higher performance than
    `pageableMemcpyHtoD16.cu`.
  prefs: []
  type: TYPE_NORMAL
- en: How important is the CPU/GPU concurrency for performance of pageable memcpy?
    If we move the event synchronization, we can make the host→device memcpy synchronous,
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p185pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: while ( N ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  prefs: []
  type: TYPE_NORMAL
- en: < CUDART_CHECK( cudaEventSynchronize( g_events[stagingIndex] ) );
  prefs: []
  type: TYPE_NORMAL
- en: memcpy( g_hostBuffers[stagingIndex], src, thisCopySize );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize, cudaMemcpyHostToDevice, NULL ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( g_events[1-stagingIndex], NULL ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventSynchronize( g_events[1-stagingIndex] ) );
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: dst += thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: src += thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: N -= thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: stagingIndex = 1 - stagingIndex;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: This code is available in `pageableMemcpyHtoD16Synchronous.cu`, and it is about
    70% as fast (2334MB/s instead of 3267MB/s) on the same `cg1.4xlarge` instance.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1\. Blocking Events
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA events also optionally can be made “blocking,” in which they use an interrupt-based
    mechanism for CPU synchronization. The CUDA driver then implements `cu(da)EventSynchronize()`
    calls using thread synchronization primitives that suspend the CPU thread instead
    of polling the event’s 32-bit tracking value.
  prefs: []
  type: TYPE_NORMAL
- en: For latency-sensitive applications, blocking events may impose a performance
    penalty. In the case of our pageable memcpy routine, using blocking events causes
    a slight slowdown (about 100MB/s) on our `cg1.4xlarge` instance. But for more
    GPU-intensive applications, or for applications with “mixed workloads” that need
    significant amounts of processing from both CPU and GPU, the benefits of having
    the CPU thread idle outweigh the costs of handling the interrupt that occurs when
    the wait is over. An example of a mixed workload is video transcoding, which features
    divergent code suitable for the CPU and signal and pixel processing suitable for
    the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2\. Queries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Both CUDA streams and CUDA events may be queried with `cu(da)StreamQuery()`
    and `cu(da)EventQuery()`, respectively. If `cu(da)StreamQuery()` returns success,
    all of the operations pending in a given stream have been completed. If `cu(da)EventQuery()`
    returns success, the event has been recorded.
  prefs: []
  type: TYPE_NORMAL
- en: Although these queries are intended to be lightweight, if ECC is enabled, they
    do perform kernel thunks to check the current error status of the GPU. Additionally,
    on Windows, any pending commands will be submitted to the GPU, which also requires
    a kernel thunk.
  prefs: []
  type: TYPE_NORMAL
- en: '6.4\. CUDA Events: Timing'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CUDA events work by submitting a command to the GPU that, when the preceding
    commands have been completed, causes the GPU to write a 32-bit memory location
    with a known value. The CUDA driver implements `cuEventQuery()` and `cuEventSynchronize()`
    by examining that 32-bit value. But besides the 32-bit “tracking” value, the GPU
    also can write a 64-bit timer value that is sourced from a high-resolution, GPU-based
    clock.
  prefs: []
  type: TYPE_NORMAL
- en: Because they use a GPU-based clock, timing using CUDA events is less subject
    to perturbations from system events such as page faults or interrupts, and the
    function to compute elapsed times from timestamps is portable across all operating
    systems. That said, the so-called “wall clock” times of operations are ultimately
    what users see, so CUDA events are best used in a targeted fashion to tune kernels
    or other GPU-intensive operations, not to report absolute times to the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The stream parameter to `cuEventRecord()` is for interstream synchronization,
    not for timing. When using CUDA events for timing, it is best to record them in
    the NULL stream. The rationale is similar to the reason the machine instructions
    in superscalar CPUs to read time stamp counters (e.g., `RDTSC` on x86) are serializing
    instructions that flush the pipeline: Forcing a “join” on all the GPU engines
    eliminates any possible ambiguity on the operations being timed.^([6](ch06.html#ch06fn6))
    Just make sure the `cu(da)EventRecord()` calls bracket enough work so that the
    timing delivers meaningful results.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch06.html#ch06fn6a). An additional consideration: On CUDA hardware with
    SM 1.1, timing events could only be recorded by the hardware unit that performed
    kernel computation.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, note that CUDA events are intended to time GPU operations. Any synchronous
    CUDA operations will result in the GPU being used to time the resulting CPU/GPU
    synchronization operations.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p187pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( startEvent, NULL ) );
  prefs: []
  type: TYPE_NORMAL
- en: // synchronous memcpy – invalidates CUDA event timing
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpy( deviceIn, hostIn, N*sizeof(int) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( stopEvent, NULL ) );
  prefs: []
  type: TYPE_NORMAL
- en: The example explored in the next section illustrates how to use CUDA events
    for timing.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5\. Concurrent Copying and Kernel Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since CUDA applications must transfer data across the PCI Express bus in order
    for the GPU to operate on it, another performance opportunity presents itself
    in the form of performing those host↔device memory transfers concurrently with
    kernel processing. According to Amdahl’s Law,^([7](ch06.html#ch06fn7)) the maximum
    speedup achievable by using multiple processors is
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch06.html#ch06fn7a). [http://bit.ly/13UqBm0](http://bit.ly/13UqBm0)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/188equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'where *r[s]* + *r[p]* = 1 and *N* is the number of processors. In the case
    of concurrent copying and kernel processing, the “number of processors” is the
    number of autonomous hardware units in the GPU: one or two copy engines, plus
    the SMs that execute the kernels. For *N* = 2, [Figure 6.6](ch06.html#ch06fig06)
    shows the idealized speedup curve as *r[s]* and *r[p]* vary.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/06fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6.6* Idealized Amdahl’s Law curve.'
  prefs: []
  type: TYPE_NORMAL
- en: So in theory, a 2x performance improvement is possible on a GPU with one copy
    engine, but only if the program gets perfect overlap between the SMs and the copy
    engine, and only if the program spends equal time transferring and processing
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Before undertaking this endeavor, you should take a close look at whether it
    will benefit your application. Applications that are extremely transfer-bound
    (i.e., they spend most of their time transferring data to and from the GPU) or
    extremely compute-bound (i.e., they spend most of their time processing data on
    the GPU) will derive little benefit from overlapping transfer and compute.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1\. `concurrencyMemcpyKernel.cu`
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The program `concurrencyMemcpyKernel.cu` is designed to illustrate not only
    how to implement concurrent memcpy and kernel execution but also how to determine
    whether it is worth doing at all. [Listing 6.3](ch06.html#ch06lis03) gives a `AddKernel()`,
    a “makework” kernel that has a parameter `cycles` to control how long it runs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 6.3.* `AddKernel()`, a makework kernel with parameterized computational
    density.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p06lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: AddKernel( int *out, const int *in, size_t N, int addValue, int
  prefs: []
  type: TYPE_NORMAL
- en: cycles )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: volatile int value = in[i];
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < cycles; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: value += addValue;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: out[i] = value;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`AddKernel()` streams an array of integers from `in` to `out`, looping over
    each input value `cycles` times. By varying the value of `cycles`, we can make
    the kernel range from a trivial streaming kernel that pushes the memory bandwidth
    limits of the machine to a totally compute-bound kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: These two routines in the program measure the performance of `AddKernel().`
  prefs: []
  type: TYPE_NORMAL
- en: • `TimeSequentialMemcpyKernel()` copies the input data to the GPU, invokes `AddKernel()`,
    and copies the output back from the GPU in separate, sequential steps.
  prefs: []
  type: TYPE_NORMAL
- en: • `TimeConcurrentOperations()` allocates a number of CUDA streams and performs
    the host→device memcpys, kernel processing, and device→host memcpys in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '`TimeSequentialMemcpyKernel()`, given in [Listing 6.4](ch06.html#ch06lis04),
    uses four CUDA events to separately time the host→device memcpy, kernel processing,
    and device→host memcpy. It also reports back the total time, as measured by the
    CUDA events.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 6.4.* `TimeSequentialMemcpyKernel()` function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p06lis04a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: bool
  prefs: []
  type: TYPE_NORMAL
- en: TimeSequentialMemcpyKernel(
  prefs: []
  type: TYPE_NORMAL
- en: float *timesHtoD,
  prefs: []
  type: TYPE_NORMAL
- en: float *timesKernel,
  prefs: []
  type: TYPE_NORMAL
- en: float *timesDtoH,
  prefs: []
  type: TYPE_NORMAL
- en: float *timesTotal,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: const chShmooRange& cyclesRange,
  prefs: []
  type: TYPE_NORMAL
- en: int numBlocks )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: bool ret = false;
  prefs: []
  type: TYPE_NORMAL
- en: int *hostIn = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int *hostOut = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int *deviceIn = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int *deviceOut = 0;
  prefs: []
  type: TYPE_NORMAL
- en: const int numEvents = 4;
  prefs: []
  type: TYPE_NORMAL
- en: cudaEvent_t events[numEvents];
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < numEvents; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: events[i] = NULL;
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventCreate( &events[i] ) );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: cudaMallocHost( &hostIn, N*sizeof(int) );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMallocHost( &hostOut, N*sizeof(int) );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMalloc( &deviceIn, N*sizeof(int) );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMalloc( &deviceOut, N*sizeof(int) );
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0; i < N; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: hostIn[i] = rand();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: for ( chShmooIterator cycles(cyclesRange); cycles; cycles++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: printf( "." ); fflush( stdout );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( events[0], NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( deviceIn, hostIn, N*sizeof(int),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice, NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( events[1], NULL );
  prefs: []
  type: TYPE_NORMAL
- en: AddKernel<<<numBlocks, 256>>>(
  prefs: []
  type: TYPE_NORMAL
- en: deviceOut, deviceIn, N, 0xcc, *cycles );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( events[2], NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( hostOut, deviceOut, N*sizeof(int),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost, NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( events[3], NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventElapsedTime( timesHtoD, events[0], events[1] );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventElapsedTime( timesKernel, events[1], events[2] );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventElapsedTime( timesDtoH, events[2], events[3] );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventElapsedTime( timesTotal, events[0], events[3] );
  prefs: []
  type: TYPE_NORMAL
- en: timesHtoD += 1;
  prefs: []
  type: TYPE_NORMAL
- en: timesKernel += 1;
  prefs: []
  type: TYPE_NORMAL
- en: timesDtoH += 1;
  prefs: []
  type: TYPE_NORMAL
- en: timesTotal += 1;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: ret = true;
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < numEvents; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventDestroy( events[i] );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( deviceIn );
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( deviceOut );
  prefs: []
  type: TYPE_NORMAL
- en: cudaFreeHost( hostOut );
  prefs: []
  type: TYPE_NORMAL
- en: cudaFreeHost( hostIn );
  prefs: []
  type: TYPE_NORMAL
- en: return ret;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The `cyclesRange` parameter, which uses the “shmoo” functionality described
    in [Section A.4](app01.html#app01lev1sec4), specifies the range of cycles values
    to use when invoking `AddKernel()`. On a `cg1.4xlarge` instance in EC2, the times
    (in ms) for `cycles` values from 4..64 are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/191tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For values of `*cycles` around 48 (highlighted), where the kernel takes about
    the same amount of time as the memcpy operations, we presume there would be a
    benefit in performing the operations concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: The routine `TimeConcurrentMemcpyKernel()` divides the computation performed
    by `AddKernel()` evenly into segments of size `streamIncrement` and uses a separate
    CUDA stream to compute each. The code fragment in [Listing 6.5](ch06.html#ch06lis05),
    from `TimeConcurrentMemcpyKernel()`, highlights the complexity of programming
    with streams.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 6.5.* `TimeConcurrentMemcpyKernel()` fragment.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p06lis05a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft = N;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int stream = 0; stream < numStreams; stream++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t intsToDo = (intsLeft < intsPerStream) ?
  prefs: []
  type: TYPE_NORMAL
- en: 'intsLeft : intsPerStream;'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync(
  prefs: []
  type: TYPE_NORMAL
- en: deviceIn+stream*intsPerStream,
  prefs: []
  type: TYPE_NORMAL
- en: hostIn+stream*intsPerStream,
  prefs: []
  type: TYPE_NORMAL
- en: intsToDo*sizeof(int),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice, streams[stream] ) );
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft -= intsToDo;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft = N;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int stream = 0; stream < numStreams; stream++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t intsToDo = (intsLeft < intsPerStream) ?
  prefs: []
  type: TYPE_NORMAL
- en: 'intsLeft : intsPerStream;'
  prefs: []
  type: TYPE_NORMAL
- en: AddKernel<<<numBlocks, 256, 0, streams[stream]>>>(
  prefs: []
  type: TYPE_NORMAL
- en: deviceOut+stream*intsPerStream,
  prefs: []
  type: TYPE_NORMAL
- en: deviceIn+stream*intsPerStream,
  prefs: []
  type: TYPE_NORMAL
- en: intsToDo, 0xcc, *cycles );
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft -= intsToDo;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft = N;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int stream = 0; stream < numStreams; stream++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t intsToDo = (intsLeft < intsPerStream) ?
  prefs: []
  type: TYPE_NORMAL
- en: 'intsLeft : intsPerStream;'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync(
  prefs: []
  type: TYPE_NORMAL
- en: hostOut+stream*intsPerStream,
  prefs: []
  type: TYPE_NORMAL
- en: deviceOut+stream*intsPerStream,
  prefs: []
  type: TYPE_NORMAL
- en: intsToDo*sizeof(int),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost, streams[stream] ) );
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft -= intsToDo;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Besides requiring the application to create and destroy CUDA streams, the streams
    must be looped over separately for each of the host→device memcpy, kernel processing,
    and device→host memcpy operations. Without this “software-pipelining,” there would
    be no concurrent execution of the different streams’ work, as each streamed operation
    is preceded by an “interlock” operation that prevents the operation from proceeding
    until the previous operation in that stream has completed. The result would be
    not only a failure to get parallel execution between the engines but also an additional
    performance degradation due to the slight overhead of managing stream concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: The computation cannot be made fully concurrent, since no kernel processing
    can be overlapped with the first or last memcpys, and there is some overhead in
    synchronizing between CUDA streams and, as we saw in the previous section, in
    invoking the memcpy and kernel operations themselves. As a result, the optimal
    number of streams depends on the application and should be determined empirically.
    The `concurrencyMemcpyKernel.cu` program enables the number of streams to be specified
    on the command line using the `-- numStreams` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2\. Performance Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `concurrencyMemcpyKernel.cu` program generates a report on performance characteristics
    over a variety of `cycles` values, with a fixed buffer size and number of streams.
    On a `cg1.4xlarge` instance in Amazon EC2, with a buffer size of 128M integers
    and 8 streams, the report is as follows for cycles values from 4..64.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/194tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The full graph for `cycles` values from 4..256 is given in [Figure 6.7](ch06.html#ch06fig07).
    Unfortunately, for these settings, the 50% speedup shown here falls well short
    of the 3x speedup that theoretically could be obtained.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/06fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6.7* Speedup due memcpy/kernel concurrency (Tesla M2050).'
  prefs: []
  type: TYPE_NORMAL
- en: The benefit on a GeForce GTX 280, which contains only one copy engine, is more
    pronounced. Here, the results from varying `cycles` up to 512 are shown. The maximum
    speedup, shown in [Figure 6.8](ch06.html#ch06fig08), is much closer to the theoretical
    maximum of 2x.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/06fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6.8* Speedup due to memcpy/kernel concurrency (GeForce GTX 280).'
  prefs: []
  type: TYPE_NORMAL
- en: As written, `concurrencyMemcpyKernel.cu` serves little more than an illustrative
    purpose, because `AddValues()` is just make-work. But you can plug your own kernel(s)
    into this application to help determine whether the additional complexity of using
    streams is justified by the performance improvement. Note that unless concurrent
    kernel execution is desired (see [Section 6.7](ch06.html#ch06lev1sec7)), the kernel
    invocation in [Listing 6.5](ch06.html#ch06lis05) could be replaced by successive
    kernel invocations in the same stream, and the application will still get the
    desired concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a side note, the number of copy engines can be queried by calling`cudaGet``DeviceProperties()`
    and examining `cudaDeviceProp:: asyncEngineCount`, or calling `cuDeviceQueryAttribute()`
    with `CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT`.'
  prefs: []
  type: TYPE_NORMAL
- en: The copy engines accompanying SM 1.1 and some SM 1.2 hardware could copy linear
    memory only, but more recent copy engines offer full support for 2D memcpy, including
    2D and 3D CUDA arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.3\. Breaking Interengine Concurrency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using CUDA streams for concurrent memcpy and kernel execution introduces many
    more opportunities to “break concurrency.” In the previous section, CPU/GPU concurrency
    could be broken by unintentionally doing something that caused CUDA to perform
    a full CPU/GPU synchronization. Here, CPU/GPU concurrency can be broken by unintentionally
    performing an unstreamed CUDA operation. Recall that the NULL stream performs
    a “join” on all GPU engines, so even an asynchronous memcpy operation will stall
    interengine concurrency if the NULL stream is specified.
  prefs: []
  type: TYPE_NORMAL
- en: Besides specifying the NULL stream explicitly, the main avenue for these unintentional
    “concurrency breaks” is calling functions that run in the NULL stream implicitly
    because they do not take a stream parameter. When streams were first introduced
    in CUDA 1.1, functions such as `cudaMemset()` and `cuMemcpyDtoD()`, and the interfaces
    for libraries such as CUFFT and CUBLAS, did not have any way for applications
    to specify stream parameters. The Thrust library still does not include support.
    The CUDA Visual Profiler will call out concurrency breaks in its reporting.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6\. Mapped Pinned Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mapped pinned memory can be used to overlap PCI Express transfers and kernel
    processing, especially for device→host copies, where there is no need to cover
    the long latency to host memory. Mapped pinned memory has stricter alignment requirements
    than the native GPU memcpy, since they must be coalesced. Uncoalesced memory transactions
    run two to six times slower when using mapped pinned memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'A naïve port of our `concurrencyMemcpyKernelMapped.cu` program yields an interesting
    result: On a `cg1.4xlarge` instance in Amazon EC2, mapped pinned memory runs very
    slowly for values of `cycles` below 64.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/197tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For small values of `cycles`, the kernel takes a long time to run, as if `cycles`
    were greater than 200! Only NVIDIA can discover the reason for this performance
    anomaly for certain, but it is not difficult to work around: By unrolling the
    inner loop of the kernel, we create more work per thread, and performance improves.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 6.6.* `AddKernel()` with loop unrolling.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p06lis06a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<const int unrollFactor>
  prefs: []
  type: TYPE_NORMAL
- en: __device__ void
  prefs: []
  type: TYPE_NORMAL
- en: AddKernel_helper( int *out, const int *in, size_t N, int increment, int cycles
    )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = unrollFactor*blockIdx.x*blockDim.x+threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += unrollFactor*blockDim.x*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int values[unrollFactor];
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+iUnroll*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: values[iUnroll] = in[index];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int k = 0; k < cycles; k++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: values[iUnroll] += increment;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+iUnroll*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: out[index] = values[iUnroll];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __device__ void
  prefs: []
  type: TYPE_NORMAL
- en: AddKernel( int *out, const int *in, size_t N, int increment, int cycles, int
  prefs: []
  type: TYPE_NORMAL
- en: unrollFactor )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: switch ( unrollFactor ) {
  prefs: []
  type: TYPE_NORMAL
- en: 'case 1: return AddKernel_helper<1>( out, in, N, increment, cycles );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case 2: return AddKernel_helper<2>( out, in, N, increment, cycles );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case 4: return AddKernel_helper<4>( out, in, N, increment, cycles );'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this version of `AddKernel()` in [Listing 6.6](ch06.html#ch06lis06)
    is functionally identical to the one in [Listing 6.3](ch06.html#ch06lis03).^([8](ch06.html#ch06fn8))
    It just computes `unrollFactor` outputs per loop iteration. Since the unroll factor
    is a template parameter, the compiler can use registers to hold the `values` array,
    and the innermost for loops can be unrolled completely.
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch06.html#ch06fn8a). Except that, as written, N must be divisible by `unrollFactor`.
    This is easily fixed, of course, with a small change to the for loop and a bit
    of cleanup code afterward.'
  prefs: []
  type: TYPE_NORMAL
- en: For `unrollFactor==1`, this implementation is identical to that of [Listing
    6.3](ch06.html#ch06lis03). For `unrollFactor==2`, mapped pinned formulation shows
    some improvement over the streamed formulation. The tipping point drops from `cycles==64`
    to `cycles==48`. For `unrollFactor==4`, performance is uniformly better than the
    streamed version.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/199tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: These values are given for 32M integers, so the program reads and writes 128MB
    of data. For `cycles==48`, the program runs in 26ms. To achieve that effective
    bandwidth rate (more than 9GB/s over PCI Express 2.0), the GPU is concurrently
    reading and writing over PCI Express while performing the kernel processing!
  prefs: []
  type: TYPE_NORMAL
- en: 6.7\. Concurrent Kernel Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SM 2.x-class and later GPUs are capable of concurrently running multiple kernels,
    provided they are launched in different streams and have block sizes that are
    small enough so a single kernel will not fill the whole GPU. The code in [Listing
    6.5](ch06.html#ch06lis05) (lines 9–14) will cause kernels to run concurrently,
    provided the number of blocks in each kernel launch is small enough. Since the
    kernels can only communicate through global memory, we can add some instrumentation
    to `AddKernel()` to track how many kernels are running concurrently. Using the
    following “kernel concurrency tracking” structure
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p200pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: static const int g_maxStreams = 8;
  prefs: []
  type: TYPE_NORMAL
- en: typedef struct KernelConcurrencyData_st {
  prefs: []
  type: TYPE_NORMAL
- en: int mask; // mask of active kernels
  prefs: []
  type: TYPE_NORMAL
- en: int maskMax; // atomic max of mask popcount
  prefs: []
  type: TYPE_NORMAL
- en: int masks[g_maxStreams];
  prefs: []
  type: TYPE_NORMAL
- en: int count; // number of active kernels
  prefs: []
  type: TYPE_NORMAL
- en: int countMax; // atomic max of kernel count
  prefs: []
  type: TYPE_NORMAL
- en: int counts[g_maxStreams];
  prefs: []
  type: TYPE_NORMAL
- en: '} KernelConcurrencyData;'
  prefs: []
  type: TYPE_NORMAL
- en: we can add code to `AddKernel()` to “check in” and “check out” at the beginning
    and end of the function, respectively. The “check in” takes the “kernel id” parameter
    `kid` (a value in the range 0..*NumStreams*-1 passed to the kernel), computes
    a mask `1<<kid` corresponding to the kernel ID into a global, and atomically OR’s
    that value into the global. Note that `atomicOR()` returns the value that was
    in the memory location before the OR was performed. As a result, the return value
    has one bit set for every kernel that was active when the atomic OR operation
    was performed.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, this code tracks the number of active kernels by incrementing `kernelData->count`
    and calling `atomicMax()` on a shared global.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p200pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: // check in, and record active kernel mask and count
  prefs: []
  type: TYPE_NORMAL
- en: // as seen by this kernel.
  prefs: []
  type: TYPE_NORMAL
- en: if ( kernelData && blockIdx.x==0 && threadIdx.x == 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: int myMask = atomicOr( &kernelData->mask, 1<<kid );
  prefs: []
  type: TYPE_NORMAL
- en: kernelData->masks[kid] = myMask | (1<<kid);
  prefs: []
  type: TYPE_NORMAL
- en: int myCount = atomicAdd( &kernelData->count, 1 );
  prefs: []
  type: TYPE_NORMAL
- en: atomicMax( &kernelData->countMax, myCount+1 );
  prefs: []
  type: TYPE_NORMAL
- en: kernelData->counts[kid] = myCount+1;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: At the bottom of the kernel, similar code clears the mask and decrements the
    active-kernel count.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p200pro03a)'
  prefs: []
  type: TYPE_NORMAL
- en: // check out
  prefs: []
  type: TYPE_NORMAL
- en: if ( kernelData && blockIdx.x==0 && threadIdx.x==0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: atomicAnd( &kernelData->mask, ~(1<<kid) );
  prefs: []
  type: TYPE_NORMAL
- en: atomicAdd( &kernelData->count, -1 );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: The `kernelData` parameter refers to a `__device__` variable declared at file
    scope.
  prefs: []
  type: TYPE_NORMAL
- en: __device__ KernelConcurrencyData g_kernelData;
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the pointer to `g_kernelData` must be obtained by calling `cudaGetSymbolAddress()`.
    It is possible to write code that references `&g_kernelData`, but CUDA’s language
    integration will not correctly resolve the address.
  prefs: []
  type: TYPE_NORMAL
- en: The `concurrencyKernelKernel.cu` program adds support for a command line option
    `blocksPerSM` to specify the number of blocks with which to launch these kernels.
    It will generate a report on the number of kernels that were active. Two sample
    invocations of `concurrencyKernelKernel` are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p201pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: $ ./concurrencyKernelKernel –blocksPerSM 2
  prefs: []
  type: TYPE_NORMAL
- en: Using 2 blocks per SM on GPU with 14 SMs = 28 blocks
  prefs: []
  type: TYPE_NORMAL
- en: 'Timing sequential operations... Kernel data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Masks: ( 0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to 1 kernels were active: (0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  prefs: []
  type: TYPE_NORMAL
- en: Timing concurrent operations...
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Masks: ( 0x1 0x3 0x7 0xe 0x1c 0x38 0x60 0xe0 )'
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to 3 kernels were active: (0x1 0x2 0x3 0x3 0x3 0x3 0x2 0x3 )'
  prefs: []
  type: TYPE_NORMAL
- en: $ ./concurrencyKernelKernel –blocksPerSM 3
  prefs: []
  type: TYPE_NORMAL
- en: Using 3 blocks per SM on GPU with 14 SMs = 42 blocks
  prefs: []
  type: TYPE_NORMAL
- en: 'Timing sequential operations... Kernel data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Masks: ( 0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to 1 kernels were active: (0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  prefs: []
  type: TYPE_NORMAL
- en: 'Timing concurrent operations... Kernel data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Masks: ( 0x1 0x3 0x6 0xc 0x10 0x30 0x60 0x80 )'
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to 2 kernels were active: (0x1 0x2 0x2 0x2 0x1 0x2 0x2 0x1 )'
  prefs: []
  type: TYPE_NORMAL
- en: Note that `blocksPerSM` is the number of blocks specified to each kernel launch,
    so a total of `numStreams*blocksPerSM` blocks are launched in `numStreams` separate
    kernels. You can see that the hardware can run more kernels concurrently when
    the kernel grids are smaller, but there is no performance benefit to concurrent
    kernel processing for the workload discussed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '6.8\. GPU/GPU Synchronization: `cudaStreamWaitEvent()`'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up to this point, all of the synchronization functions described in this chapter
    have pertained to CPU/GPU synchronization. They either wait for or query the status
    of a GPU operation. The `cudaStreamWaitEvent()function` is asynchronous with respect
    to the CPU and causes the specified *stream* to wait until an event has been recorded.
    The stream and event need not be associated with the same CUDA device. [Section
    9.3](ch09.html#ch09lev1sec3) describes how such inter-GPU synchronization may
    be performed and uses the feature to implement a peer-to-peer memcpy (see [Listing
    9.1](ch09.html#ch09lis01)).
  prefs: []
  type: TYPE_NORMAL
- en: '6.8.1\. Streams and Events on Multi-GPU: Notes and Limitations'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: • Streams and events exist in the scope of the context (or device). When `cuCtxDestroy()`
    or `cudaDeviceReset()` is called, the associated streams and events are destroyed.
  prefs: []
  type: TYPE_NORMAL
- en: • Kernel launches and `cu(da)EventRecord()` can only use CUDA streams in the
    same context/device.
  prefs: []
  type: TYPE_NORMAL
- en: • `cudaMemcpy()` can be called with any stream, but it is best to call it from
    the *source* context/device.
  prefs: []
  type: TYPE_NORMAL
- en: • `cudaStreamWaitEvent()` may be called on any event, using any stream.
  prefs: []
  type: TYPE_NORMAL
- en: 6.9\. Source Code Reference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The source code referenced in this chapter resides in the `concurrency` directory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/202tab01.jpg)![Image](graphics/202tab01a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Chapter 7\. Kernel Execution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter gives a detailed description of how kernels are executed on the
    GPU: how they are launched, their execution characteristics, how they are organized
    into grids of blocks of threads, and resource management considerations. The chapter
    concludes with a description of dynamic parallelism—the new CUDA 5.0 feature that
    enables CUDA kernels to launch work for the GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'CUDA kernels execute on the GPU and, since the very first version of CUDA,
    always have executed concurrently with the CPU. In other words, kernel launches
    are *asynchronous*: Control is returned to the CPU before the GPU has completed
    the requested operation. When CUDA was first introduced, there was no need for
    developers to concern themselves with the asynchrony (or lack thereof) of kernel
    launches; data had to be copied to and from the GPU explicitly, and the memcpy
    commands would be enqueued after the commands needed to launch kernels. It was
    not possible to write CUDA code that exposed the asynchrony of kernel launches;
    the main side effect was to hide driver overhead when performing multiple kernel
    launches consecutively.'
  prefs: []
  type: TYPE_NORMAL
- en: With the introduction of mapped pinned memory (host memory that can be directly
    accessed by the GPU), the asynchrony of kernel launches becomes more important,
    especially for kernels that write to host memory (as opposed to read from it).
    If a kernel is launched and writes host memory without explicit synchronization
    (such as with CUDA events), the code suffers from a race condition between the
    CPU and GPU and may not run correctly. Explicit synchronization often is not needed
    for kernels that *read* via mapped pinned memory, since any pending writes by
    the CPU will be posted before the kernel launches. But for kernels that are returning
    results to CPU by writing to mapped pinned memory, synchronizing to avoid write-after-read
    hazards is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Once a kernel is launched, it runs as a *grid* of *blocks* of *threads*. Not
    all blocks run concurrently, necessarily; each block is assigned to a streaming
    multiprocessor (SM), and each SM can maintain the context for multiple blocks.
    To cover both memory and instruction latencies, the SM generally needs more warps
    than a single block can contain. The maximum number of blocks per SM cannot be
    queried, but it is documented by NVIDIA as having been 8 before SM 3.x and 16
    on SM 3.x and later hardware.
  prefs: []
  type: TYPE_NORMAL
- en: The programming model makes no guarantees whatsoever as to the order of execution
    or whether certain blocks or threads can run concurrently. Developers can never
    assume that all the threads in a kernel launch are executing concurrently. It
    is easy to launch more threads than the machine can hold, and some will not start
    executing until others have finished. Given the lack of ordering guarantees, even
    initialization of global memory at the beginning of a kernel launch is a difficult
    proposition.
  prefs: []
  type: TYPE_NORMAL
- en: '*Dynamic parallelism*, a new feature added with the Tesla K20 (GK110), the
    first SM 3.5–capable GPU, enables kernels to launch other kernels and perform
    synchronization between them. These capabilities address some of the limitations
    that were present in CUDA in previous hardware. For example, a dynamically parallel
    kernel can perform initialization by launching and waiting for a child grid.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Syntax
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using the CUDA runtime, a kernel launch is specified using the familiar
    triple-angle-bracket syntax.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel<<<gridSize, blockSize, sharedMem, Stream>>>( Parameters... )
  prefs: []
  type: TYPE_NORMAL
- en: '*Kernel* specifies the kernel to launch.'
  prefs: []
  type: TYPE_NORMAL
- en: '*gridSize* specifies the size of the grid in the form of a `dim3` structure.'
  prefs: []
  type: TYPE_NORMAL
- en: '*blockSize* specifies the dimension of each threadblock as a `dim3.`'
  prefs: []
  type: TYPE_NORMAL
- en: '*sharedMem* specifies additional shared memory^([1](ch07.html#ch07fn1)) to
    reserve for each block.'
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch07.html#ch07fn1a). The amount of shared memory available to the kernel
    is the sum of this parameter and the amount of shared memory that was statically
    declared within the kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Stream* specifies the stream in which the kernel should be launched.'
  prefs: []
  type: TYPE_NORMAL
- en: The `dim3` structure used to specify the grid and block sizes has 3 members
    (`x`, `y`, and `z`) and, when compiling with C++, a constructor with default parameters
    such that the `y` and `z` members default to 1\. See [Listing 7.1](ch07.html#ch07lis01),
    which is excerpted from the NVIDIA SDK file `vector_types.h.`
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7.1.* `dim3` structure.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p07lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: struct __device_builtin__ dim3
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int x, y, z;
  prefs: []
  type: TYPE_NORMAL
- en: '#if defined(__cplusplus)'
  prefs: []
  type: TYPE_NORMAL
- en: __host__ __device__ dim3(
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int vx = 1,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int vy = 1,
  prefs: []
  type: TYPE_NORMAL
- en: 'unsigned int vz = 1) : x(vx), y(vy), z(vz) {}'
  prefs: []
  type: TYPE_NORMAL
- en: '__host__ __device__ dim3(uint3 v) : x(v.x), y(v.y), z(v.z) {}'
  prefs: []
  type: TYPE_NORMAL
- en: __host__ __device__ operator uint3(void) {
  prefs: []
  type: TYPE_NORMAL
- en: uint3 t;
  prefs: []
  type: TYPE_NORMAL
- en: t.x = x;
  prefs: []
  type: TYPE_NORMAL
- en: t.y = y;
  prefs: []
  type: TYPE_NORMAL
- en: t.z = z;
  prefs: []
  type: TYPE_NORMAL
- en: return t;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '#endif /* __cplusplus */'
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Kernels can be launched via the driver API using `cuLaunchKernel()`, though
    that function takes the grid and block dimensions as discrete parameters rather
    than `dim3.`
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p207pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuLaunchKernel (
  prefs: []
  type: TYPE_NORMAL
- en: CUfunction kernel,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int gridDimX,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int gridDimY,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int gridDimZ,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int blockDimX,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int blockDimY,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int blockDimZ,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int sharedMemBytes,
  prefs: []
  type: TYPE_NORMAL
- en: CUstream hStream,
  prefs: []
  type: TYPE_NORMAL
- en: void **kernelParams,
  prefs: []
  type: TYPE_NORMAL
- en: void **extra
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the triple-angle-bracket syntax, the parameters to `cuLaunchKernel()`
    include the kernel to invoke, the grid and block sizes, the amount of shared memory,
    and the stream. The main difference is in how the parameters to the kernel itself
    are given: Since the kernel microcode emitted by `ptxas` contains metadata that
    describes each kernel’s parameters,^([2](ch07.html#ch07fn2)) `kernelParams` is
    an array of `void *`, where each element corresponds to a kernel parameter. Since
    the type is known by the driver, the correct amount of memory (4 bytes for an
    `int`, 8 bytes for a `double`, etc.) will be copied into the command buffer as
    part of the hardware-specific command used to invoke the kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch07.html#ch07fn2a). `cuLaunchKernel()` will fail on binary images that
    were not compiled with CUDA 3.2 or later, since that is the first version to include
    kernel parameter metadata.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1\. Limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All C++ classes participating in a kernel launch must be “plain old data” (POD)
    with the following characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: • No user-declared constructors
  prefs: []
  type: TYPE_NORMAL
- en: • No user-defined copy assignment operator
  prefs: []
  type: TYPE_NORMAL
- en: • No user-defined destructor
  prefs: []
  type: TYPE_NORMAL
- en: • No nonstatic data members that are not themselves PODs
  prefs: []
  type: TYPE_NORMAL
- en: • No private or protected nonstatic data
  prefs: []
  type: TYPE_NORMAL
- en: • No base classes
  prefs: []
  type: TYPE_NORMAL
- en: • No virtual functions
  prefs: []
  type: TYPE_NORMAL
- en: Note that classes that violate these rules may be used in CUDA, or even in CUDA
    kernels; they simply cannot be used for a kernel launch. In that case, the classes
    used by a CUDA kernel can be constructed using the POD input data from the launch.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA kernels also do not have return values. They must report their results
    back via device memory (which must be copied back to the CPU explicitly) or mapped
    host memory.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2\. Caches and Coherency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The GPU contains numerous caches to accelerate computation when reuse occurs.
    The constant cache is optimized for broadcast to the execution units within an
    SM; the texture cache reduces external bandwidth usage. Neither of these caches
    is kept coherent with respect to writes to memory by the GPU. For example, there
    is no protocol to enforce coherency between these caches and the L1 or L2 caches
    that serve to reduce latency and aggregate bandwidth to global memory. That means
    two things.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** When a kernel is running, it must take care not to write memory that
    it (or a concurrently running kernel) also is accessing via constant or texture
    memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** The CUDA driver must invalidate the constant cache and texture cache
    before each kernel launch.'
  prefs: []
  type: TYPE_NORMAL
- en: For kernels that do not contain `TEX` instructions, there is no need for the
    CUDA driver to invalidate the texture cache; as a result, kernels that do not
    use texture incur less driver overhead.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3\. Asynchrony and Error Handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Kernel launches are *asynchronous*: As soon as a kernel is submitted to the
    hardware, it begins executing in parallel with the CPU.^([3](ch07.html#ch07fn3))
    This asynchrony complicates error handling. If a kernel encounters an error (for
    example, if it reads an invalid memory location), the error is reported to the
    driver (and the application) sometime after the kernel launch. The surest way
    to check for such errors is to synchronize with the GPU using `cudaDeviceSynchronize()`
    or `cuCtxSynchronize()`. If an error in kernel execution has occurred, the error
    code “unspecified launch failure” is returned.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch07.html#ch07fn3a). On most platforms, the kernel will start executing
    on the GPU microseconds after the CPU has finished processing the launch command.
    But on the Windows Display Driver Model (WDDM), it may take longer because the
    driver must perform a kernel thunk in order to submit the launch to the hardware,
    and work for the GPU is enqueued in user mode to amortize the overhead of the
    user→kernel transition.'
  prefs: []
  type: TYPE_NORMAL
- en: Besides explicit CPU/GPU synchronization calls such as `cudaDevice-Synchronize()`
    or `cuCtxSynchronize()`, this error code may be returned by functions that implicitly
    synchronize with the CPU, such as synchronous memcpy calls.
  prefs: []
  type: TYPE_NORMAL
- en: Invalid Kernel Launches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It is possible to request a kernel launch that the hardware cannot perform—for
    example, by specifying more threads per block than the hardware supports. When
    possible, the driver detects these cases and reports an error rather than trying
    to submit the launch to the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime and the driver API handle this case differently. When an invalid
    parameter is specified, the driver API’s explicit API calls such as `cuLaunchGrid()`
    and `cuLaunchKernel()` return error codes. But when using the CUDA runtime, since
    kernels are launched in-line with C/C++ code, there is no API call to return an
    error code. Instead, the error is “recorded” into a thread-local slot and applications
    can query the error value with `cudaGetLastError()`. This same error handling
    mechanism is used for kernel launches that are invalid for other reasons, such
    as a memory access violation.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.4\. Timeouts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because the GPU is not able to context-switch in the midst of kernel execution,
    a long-running CUDA kernel may negatively impact the interactivity of a system
    that uses the GPU to interact with the user. As a result, many CUDA systems implement
    a “timeout” that resets the GPU if it runs too long without context switching.
  prefs: []
  type: TYPE_NORMAL
- en: On WDDM (Windows Display Driver Model), the timeout is enforced by the operating
    system. Microsoft has documented how this “[Timeout Detection and Recovery](ch03.html#ch03lev4sec1)”
    (TDR) works. See [http://bit.ly/WPPSdQ](http://bit.ly/WPPSdQ), which includes
    the Registry keys that control TDR behavior.^([4](ch07.html#ch07fn4)) TDR can
    be safely disabled by using the Tesla Compute Cluster (TCC) driver, though the
    TCC driver is not available for all hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch07.html#ch07fn4a). Modifying the Registry should only be done for test
    purposes, of course.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On Linux, the NVIDIA driver enforces a default timeout of 2 seconds. No time
    out is enforced on secondary GPUs that are not being used for display. Developers
    can query whether a runtime limit is being enforced on a given GPU by calling
    `cuDeviceGetAttribute()` with `CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_``TIMEOUT`, or
    by examining `cudaDeviceProp:: kernelExecTimeoutEnabled`.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.5\. Local Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since local memory is per-thread, and a grid in CUDA can contain thousands of
    threads, the amount of local memory needed by a CUDA grid can be considerable.
    The developers of CUDA took pains to preallocate resources to minimize the likelihood
    that operations such as kernel launches would fail due to a lack of resources,
    but in the case of local memory, a conservative allocation simply would have consumed
    too much memory. As a result, kernels that use a large amount of local memory
    take longer and may be synchronous because the CUDA driver must allocate memory
    before performing the kernel launch. Furthermore, if the memory allocation fails,
    the kernel launch will fail due to a lack of resources.
  prefs: []
  type: TYPE_NORMAL
- en: By default, when the CUDA driver must allocate local memory to run a kernel,
    it frees the memory after the kernel has finished. This behavior additionally
    makes the kernel launch synchronous. But this behavior can be inhibited by specifying
    `CU_CTX_LMEM_RESIZE_TO_MAX to cuCtxCreate()` or by calling `cudaSetDeviceFlags()`
    with `cudaDeviceLmemResizeToMax` before the primary context is created. In this
    case, the increased amount of local memory available will persist after launching
    a kernel that required more local memory than the default.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.6\. Shared Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Shared memory is allocated when the kernel is launched, and it stays allocated
    for the duration of the kernel’s execution. Besides static allocations that can
    be declared in the kernel, shared memory can be declared as an unsized `extern`;
    in that case, the amount of shared memory to allocate for the unsized array is
    specified as the third parameter of the kernel launch, or the `sharedMemBytes`
    parameter to `cuLaunchKernel()`.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. Blocks, Threads, Warps, and Lanes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kernels are launched as *grids* of *blocks* of threads. Threads can further
    be divided into 32-thread *warps*, and each thread in a warp is called a *lane*.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1\. Grids of Blocks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Thread blocks are separately scheduled onto SMs, and threads within a given
    block are executed by the same SM. [Figure 7.1](ch07.html#ch07fig01) shows a 2D
    grid (8W × 6H) of 2D blocks (8W × 8H). [Figure 7.2](ch07.html#ch07fig02) shows
    a 3D grid (8W × 6H × 6D) of 3D blocks (8W × 8H × 4D).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.1* 2D grid and thread block.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.2* 3D grid and thread block.'
  prefs: []
  type: TYPE_NORMAL
- en: Grids can be up to 65535 x 65535 blocks (for SM 1.x hardware) or 65535 x 65535
    x 65535 blocks (for SM 2.x hardware).^([5](ch07.html#ch07fn5)) Blocks may be up
    to 512 or 1024 threads in size,^([6](ch07.html#ch07fn6)) and threads within a
    block can communicate via the SM’s shared memory. Blocks within a grid are likely
    to be assigned to different SMs; to maximize throughput of the hardware, a given
    SM can run threads and warps from different blocks at the same time. The warp
    schedulers dispatch instructions as needed resources become available.
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch07.html#ch07fn5a). The maximum grid size is queryable via `CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X,`
    `CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y`, or `CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z`;
    or by calling `cudaGetDeviceGetProperties()` and examining `cudaDeviceProp::maxGridSize`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch07.html#ch07fn6a). The maximum block size is queryable via `CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK`,
    or `deviceProp.maxThreadsPerBlock`.'
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Each threads gets a full complement of registers^([7](ch07.html#ch07fn7)) and
    a thread ID that is unique within the threadblock. To obviate the need to pass
    the size of the grid and threadblock into every kernel, the grid and block size
    also are available for kernels to read at runtime. The built-in variables used
    to reference these registers are given in [Table 7.1](ch07.html#ch07tab01). They
    are all of type `dim3`.
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch07.html#ch07fn7a). The more registers needed per thread, the fewer threads
    can “fit” in a given SM. The percentage of warps executing in an SM as compared
    to the theoretical maximum is called *occupancy* (see [Section 7.4](ch07.html#ch07lev1sec4)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 7.1* Built-In Variables'
  prefs: []
  type: TYPE_NORMAL
- en: Taken together, these variables can be used to compute which part of a problem
    the thread will operate on. A “global” index for a thread can be computed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p213pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: int globalThreadId =
  prefs: []
  type: TYPE_NORMAL
- en: threadIdx.x+blockDim.x*(threadIdx.y+blockDim.y*threadIdx.z);
  prefs: []
  type: TYPE_NORMAL
- en: Warps, Lanes, and ILP
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The threads themselves are executed together, in SIMD fashion, in units of 32
    threads called a *warp*, after the collection of parallel threads in a loom.^([8](ch07.html#ch07fn8))
    (See [Figure 7.3](ch07.html#ch07fig03).) All 32 threads execute the same instruction,
    each using its private set of registers to perform the requested operation. In
    a triumph of mixed metaphor, the ID of a thread within a warp is called its *lane*.
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch07.html#ch07fn8a). The warp size can be queried, but it imposes such
    a huge compatibility burden on the hardware that developers can rely on it staying
    fixed at 32 for the foreseeable future.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.3* Loom.'
  prefs: []
  type: TYPE_NORMAL
- en: The warp ID and lane ID can be computed using a global thread ID as follows.
  prefs: []
  type: TYPE_NORMAL
- en: int warpID = globalThreadId >> 5;
  prefs: []
  type: TYPE_NORMAL
- en: int laneID = globalThreadId & 31;
  prefs: []
  type: TYPE_NORMAL
- en: Warps are an important unit of execution because they are the granularity with
    which GPUs can cover latency. It has been well documented how GPUs use parallelism
    to cover memory latency. It takes hundreds of clock cycles to satisfy a global
    memory request, so when a texture fetch or read is encountered, the GPU issues
    the memory request and then schedules other instructions until the data arrives.
    Once the data has arrived, the warp becomes eligible for execution again.
  prefs: []
  type: TYPE_NORMAL
- en: What has been less well documented is that GPUs also use parallelism to exploit
    ILP (“instruction level parallelism”). ILP refers to fine-grained parallelism
    that occurs during program execution; for example, when computing `(a+b)*(c+d)`,
    the addition operations `a+b` and `c+d` can be performed in parallel before the
    multiplication must be performed. Because the SMs already have a tremendous amount
    of logic to track dependencies and cover latency, they are very good at covering
    instruction latency through parallelism (which is effectively ILP) as well as
    memory latency. GPUs’ support for ILP is part of the reason loop unrolling is
    such an effective optimization strategy. Besides slightly reducing the number
    of instructions per loop iteration, it exposes more parallelism for the warp schedulers
    to exploit.
  prefs: []
  type: TYPE_NORMAL
- en: Object Scopes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The scopes of objects that may be referenced by a kernel grid are summarized
    in [Table 7.2](ch07.html#ch07tab02), from the most local (registers in each thread)
    to the most global (global memory and texture references are per grid). Before
    the advent of dynamic parallelism, thread blocks served primarily as a mechanism
    for interthread synchronization within a thread block (via intrinsics such as
    `__syncthreads()`) and communication (via shared memory). Dynamic parallelism
    adds resource management to the mix, since streams and events created within a
    kernel are only valid for threads within the same thread block.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 7.2* Object Scopes'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2\. Execution Guarantees
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is important that developers never make any assumptions about the order in
    which blocks or threads will execute. In particular, there is no way to know which
    block or thread will execute first, so initialization generally should be performed
    by code outside the kernel invocation.
  prefs: []
  type: TYPE_NORMAL
- en: Execution Guarantees and Interblock Synchronization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Threads within a given thread block are guaranteed to be resident within the
    same SM, so they can communicate via shared memory and synchronize execution using
    intrinsics such as `__syncthreads()`. But thread blocks do not have any similar
    mechanisms for data interchange or synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated CUDA developers may ask, *But what about atomic operations
    in global memory?* Global memory can be updated in a thread-safe manner using
    atomic operations, so it is tempting to build something like a `__syncblocks()`
    function that, like `__syncthreads()`, waits until all blocks in the kernel launch
    have arrived before proceeding. Perhaps it would do an `atomicInc()` on a global
    memory location and, if `atomicInc()` did not return the block count, poll that
    memory location until it did.
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is that the execution pattern of the kernel (for example, the mapping
    of thread blocks onto SMs) varies with the hardware configuration. For example,
    the number of SMs—and unless the GPU context is big enough to hold the entire
    grid—*some thread blocks may execute to completion before other thread blocks
    have started running*. The result is deadlock: Because not all blocks are necessarily
    resident in the GPU, the blocks that are polling the shared memory location prevent
    other blocks in the kernel launch from executing.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a few special cases when interblock synchronization can work. If simple
    mutual exclusion is all that’s desired, `atomicCAS()` certainly can be used to
    provide that. Also, thread blocks can use atomics to signal when they’ve completed,
    so the last thread block in a grid can perform some operation before it exits,
    knowing that all the other thread blocks have completed execution. This strategy
    is employed by the `threadFenceReduction` SDK sample and the `reduction4SinglePass.cu`
    sample that accompanies this book (see [Section 12.2](ch12.html#ch12lev1sec2)).
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.3\. Block and Thread IDs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A set of special read-only registers give each thread context in the form of
    a *thread ID* and *block ID*. The thread and block IDs are assigned as a CUDA
    kernel begins execution; for 2D and 3D grids and blocks, they are assigned in
    row-major order.
  prefs: []
  type: TYPE_NORMAL
- en: Thread block sizes are best specified in multiples of 32, since warps are the
    finest possible granularity of execution on the GPU. [Figure 7.4](ch07.html#ch07fig04)
    shows how thread IDs are assigned in 32-thread blocks that are 32Wx1H, 16Wx2H,
    and 8Wx4H, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.4* Blocks of 32 threads.'
  prefs: []
  type: TYPE_NORMAL
- en: For blocks with a thread count that is not a multiple of 32, some warps are
    not fully populated with active threads. [Figure 7.5](ch07.html#ch07fig05) shows
    thread ID assignments for 28-thread blocks that are 28Wx1H, 14Wx2H, and 7Wx4H;
    in each case, 4 threads in the 32-thread warp are inactive for the duration of
    the kernel launch. For any thread block size not divisible by 32, some execution
    resources are wasted, as some warps will be launched with lanes that are disabled
    for the duration of the kernel execution. There is no performance benefit to 2D
    or 3D blocks or grids, but they sometimes make for a better match to the application.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.5* Blocks of 28 threads.'
  prefs: []
  type: TYPE_NORMAL
- en: The `reportClocks.cu` program illustrates how thread IDs are assigned and how
    warp-based execution works in general ([Listing 7.2](ch07.html#ch07lis02)).
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7.2.* `WriteClockValues` kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p07lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: WriteClockValues(
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int *completionTimes,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int *threadIDs
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: size_t globalBlock = blockIdx.x+blockDim.x*
  prefs: []
  type: TYPE_NORMAL
- en: (blockIdx.y+blockDim.y*blockIdx.z);
  prefs: []
  type: TYPE_NORMAL
- en: size_t globalThread = threadIdx.x+blockDim.x*
  prefs: []
  type: TYPE_NORMAL
- en: (threadIdx.y+blockDim.y*threadIdx.z);
  prefs: []
  type: TYPE_NORMAL
- en: size_t totalBlockSize = blockDim.x*blockDim.y*blockDim.z;
  prefs: []
  type: TYPE_NORMAL
- en: size_t globalIndex = globalBlock*totalBlockSize + globalThread;
  prefs: []
  type: TYPE_NORMAL
- en: completionTimes[globalIndex] = clock();
  prefs: []
  type: TYPE_NORMAL
- en: threadIDs[globalIndex] = threadIdx.y<<4|threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`WriteClockValues()` writes to the two output arrays using a global index computed
    using the block and thread IDs, and the grid and block sizes. One output array
    receives the return value from the `clock()` intrinsic, which returns a high-resolution
    timer value that increments for each warp. In the case of this program, we are
    using `clock()` to identify which warp processed a given value. `clock()` returns
    the value of a per-multiprocessor clock cycle counter, so we normalize the values
    by computing the minimum and subtracting it from all clock cycles values. We call
    the resulting values the thread’s “completion time.”'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at completion times for threads in a pair of 16Wx8H blocks
    ([Listing 7.3](ch07.html#ch07lis03)) and compare them to completion times for
    14Wx8H blocks ([Listing 7.4](ch07.html#ch07lis04)). As expected, they are grouped
    in 32s, corresponding to the warp size.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7.3.* Completion times (16W×8H blocks).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p07lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 0.01 ms for 256 threads = 0.03 us/thread
  prefs: []
  type: TYPE_NORMAL
- en: 'Completion times (clocks):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (0, 0, 0) - slice 0:'
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  prefs: []
  type: TYPE_NORMAL
- en: 8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
  prefs: []
  type: TYPE_NORMAL
- en: 8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
  prefs: []
  type: TYPE_NORMAL
- en: a   a   a   a   a   a   a   a   a   a   a   a   a   a   a   a
  prefs: []
  type: TYPE_NORMAL
- en: a   a   a   a   a   a   a   a   a   a   a   a   a   a   a   a
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (1, 0, 0) - slice 0:'
  prefs: []
  type: TYPE_NORMAL
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  prefs: []
  type: TYPE_NORMAL
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  prefs: []
  type: TYPE_NORMAL
- en: 2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
  prefs: []
  type: TYPE_NORMAL
- en: 2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  prefs: []
  type: TYPE_NORMAL
- en: 'Thread IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (0, 0, 0) - slice 0:'
  prefs: []
  type: TYPE_NORMAL
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d   e   f
  prefs: []
  type: TYPE_NORMAL
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d  1e  1f
  prefs: []
  type: TYPE_NORMAL
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d  2e  2f
  prefs: []
  type: TYPE_NORMAL
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d  3e  3f
  prefs: []
  type: TYPE_NORMAL
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d  4e  4f
  prefs: []
  type: TYPE_NORMAL
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d  5e  5f
  prefs: []
  type: TYPE_NORMAL
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d  6e  6f
  prefs: []
  type: TYPE_NORMAL
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d  7e  7f
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (1, 0, 0) - slice 0:'
  prefs: []
  type: TYPE_NORMAL
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d   e   f
  prefs: []
  type: TYPE_NORMAL
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d  1e  1f
  prefs: []
  type: TYPE_NORMAL
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d  2e  2f
  prefs: []
  type: TYPE_NORMAL
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d  3e  3f
  prefs: []
  type: TYPE_NORMAL
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d  4e  4f
  prefs: []
  type: TYPE_NORMAL
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d  5e  5f
  prefs: []
  type: TYPE_NORMAL
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d  6e  6f
  prefs: []
  type: TYPE_NORMAL
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d  7e  7f
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7.4.* Completion times (14W×8H blocks).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p07lis04a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Completion times (clocks):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (0, 0, 0) - slice 0:'
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   8   8   8   8   8   8   8   8   8   8
  prefs: []
  type: TYPE_NORMAL
- en: 8   8   8   8   8   8   8   8   8   8   8   8   8   8
  prefs: []
  type: TYPE_NORMAL
- en: 8   8   8   8   8   8   8   8   a   a   a   a   a   a
  prefs: []
  type: TYPE_NORMAL
- en: a   a   a   a   a   a   a   a   a   a   a   a   a   a
  prefs: []
  type: TYPE_NORMAL
- en: a   a   a   a   a   a   a   a   a   a   a   a   c   c
  prefs: []
  type: TYPE_NORMAL
- en: c   c   c   c   c   c   c   c   c   c   c   c   c   c
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (1, 0, 0) - slice 0:'
  prefs: []
  type: TYPE_NORMAL
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0
  prefs: []
  type: TYPE_NORMAL
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0
  prefs: []
  type: TYPE_NORMAL
- en: 0   0   0   0   2   2   2   2   2   2   2   2   2   2
  prefs: []
  type: TYPE_NORMAL
- en: 2   2   2   2   2   2   2   2   2   2   2   2   2   2
  prefs: []
  type: TYPE_NORMAL
- en: 2   2   2   2   2   2   2   2   4   4   4   4   4   4
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   6   6
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
  prefs: []
  type: TYPE_NORMAL
- en: 'Thread IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (0, 0, 0) - slice 0:'
  prefs: []
  type: TYPE_NORMAL
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d
  prefs: []
  type: TYPE_NORMAL
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d
  prefs: []
  type: TYPE_NORMAL
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d
  prefs: []
  type: TYPE_NORMAL
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d
  prefs: []
  type: TYPE_NORMAL
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d
  prefs: []
  type: TYPE_NORMAL
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d
  prefs: []
  type: TYPE_NORMAL
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d
  prefs: []
  type: TYPE_NORMAL
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (1, 0, 0) - slice 0:'
  prefs: []
  type: TYPE_NORMAL
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d
  prefs: []
  type: TYPE_NORMAL
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d
  prefs: []
  type: TYPE_NORMAL
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d
  prefs: []
  type: TYPE_NORMAL
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d
  prefs: []
  type: TYPE_NORMAL
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d
  prefs: []
  type: TYPE_NORMAL
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d
  prefs: []
  type: TYPE_NORMAL
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d
  prefs: []
  type: TYPE_NORMAL
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The completion times for the 14Wx8H blocks given in [Listing 7.4](ch07.html#ch07lis04)
    underscore how the thread IDs map to warps. In the case of the 14Wx8H blocks,
    every warp holds only 28 threads; 12.5% of the number of possible thread lanes
    are idle throughout the kernel’s execution. To avoid this waste, developers always
    should try to make sure blocks contain a multiple of 32 threads.
  prefs: []
  type: TYPE_NORMAL
- en: 7.4\. Occupancy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Occupancy is a ratio that measures the number of threads/SM that *will* run
    in a given kernel launch, as opposed to the maximum number of threads that *potentially
    could be running* on that SM.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/220equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The denominator (maximum warps per SM) is a constant that depends only on the
    compute capability of the device. The numerator of this expression, which determines
    the occupancy, is a function of the following.
  prefs: []
  type: TYPE_NORMAL
- en: • Compute capability (1.0, 1.1, 1.2, 1.3, 2.0, 2.1, 3.0, 3.5)
  prefs: []
  type: TYPE_NORMAL
- en: • Threads per block
  prefs: []
  type: TYPE_NORMAL
- en: • Registers per thread
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory configuration^([9](ch07.html#ch07fn9))
  prefs: []
  type: TYPE_NORMAL
- en: '[9](ch07.html#ch07fn9a). For SM 2.x and later only. Developers can split the
    64K L1 cache in the SM as 16K shared/48K L1 or 48K shared/16K L1\. (SM 3.x adds
    the ability to split the cache evenly as 32K shared/32K L1.)'
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory per block
  prefs: []
  type: TYPE_NORMAL
- en: To help developers assess the tradeoffs between these parameters, the CUDA Toolkit
    includes an occupancy calculator in the form of an Excel spreadsheet.^([10](ch07.html#ch07fn10))
    Given the inputs above, the spreadsheet will calculate the following results.
  prefs: []
  type: TYPE_NORMAL
- en: '[10](ch07.html#ch07fn10a). Typically it is in the tools subdirectory—for example,
    `%CUDA_PATH%/tools` (Windows) or `$CUDA_PATH/tools`.'
  prefs: []
  type: TYPE_NORMAL
- en: • Active thread count
  prefs: []
  type: TYPE_NORMAL
- en: • Active warp count
  prefs: []
  type: TYPE_NORMAL
- en: • Active block count
  prefs: []
  type: TYPE_NORMAL
- en: • Occupancy (active warp count divided into the hardware’s maximum number of
    active warps)
  prefs: []
  type: TYPE_NORMAL
- en: The spreadsheet also identifies whichever parameter is limiting the occupancy.
  prefs: []
  type: TYPE_NORMAL
- en: • Registers per multiprocessor
  prefs: []
  type: TYPE_NORMAL
- en: • Maximum number of warps or blocks per multiprocessor
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory per multiprocessor
  prefs: []
  type: TYPE_NORMAL
- en: Note that occupancy is not the be-all and end-all of CUDA performance;^([11](ch07.html#ch07fn11))
    often it is better to use more registers per thread and rely on instruction-level
    parallelism (ILP) to deliver performance. NVIDIA has a good presentation on warps
    and occupancy that discusses the tradeoffs.^([12](ch07.html#ch07fn12))
  prefs: []
  type: TYPE_NORMAL
- en: '[11](ch07.html#ch07fn11a). Vasily Volkov emphatically makes this point in his
    presentation “Better Performance at Lower Occupancy.” It is available at [http://bit.ly/YdScNG](http://bit.ly/YdScNG).'
  prefs: []
  type: TYPE_NORMAL
- en: '[12](ch07.html#ch07fn12a). [http://bit.ly/WHTb5m](http://bit.ly/WHTb5m)'
  prefs: []
  type: TYPE_NORMAL
- en: An example of a low-occupancy kernel that can achieve near-maximum global memory
    bandwidth is given in [Section 5.2.10](ch05.html#ch05lev2sec18) ([Listing 5.5](ch05.html#ch05lis05)).
    The inner loop of the `GlobalReads` kernel can be unrolled according to a template
    parameter; as the number of unrolled iterations increases, the number of needed
    registers increases and the occupancy goes down. For the Tesla M2050’s in the
    `cg1.4xlarge` instance type, for example, the peak read bandwidth reported (with
    ECC disabled) is 124GiB/s, with occupancy of 66%. Volkov reports achieving near-peak
    memory bandwidth when running kernels whose occupancy is in the single digits.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5\. Dynamic Parallelism
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Dynamic parallelism*, a new capability that works only on SM 3.5–class hardware,
    enables CUDA kernels to launch other CUDA kernels, and also to invoke various
    functions in the CUDA runtime. When using dynamic parallelism, a subset of the
    CUDA runtime (known as the *device runtime*) becomes available for use by threads
    running on the device.'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic parallelism introduces the idea of “parent” and “child” grids. Any kernel
    invoked by another CUDA kernel (as opposed to host code, as done in all previous
    CUDA versions) is a “child kernel,” and the invoking grid is its “parent.” By
    default, CUDA supports two (2) nesting levels (one for the parent and one for
    the child), a number that may be increased by calling `cudaSetDeviceLimit()` with
    `cudaLimitDevRuntimeSyncDepth`.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic parallelism was designed to address applications that previously had
    to deliver results to the CPU so the CPU could specify which work to perform on
    the GPU. Such “handshaking” disrupts CPU/GPU concurrency in the execution pipeline
    described in [Section 2.5.1](ch02.html#ch02lev2sec11), in which the CPU produces
    commands for consumption by the GPU. The GPU’s time is too valuable for it to
    wait for the CPU to read and analyze results before issuing more work. Dynamic
    parallelism avoids these pipeline bubbles by enabling the GPU to launch work for
    itself from kernels.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic parallelism can improve performance in several cases.
  prefs: []
  type: TYPE_NORMAL
- en: • It enables initialization of data structures needed by a kernel before the
    kernel can begin execution. Previously, such initialization had to be taken care
    of in host code or by previously invoking a separate kernel.
  prefs: []
  type: TYPE_NORMAL
- en: • It enables simplified recursion for applications such as Barnes-Hut gravitational
    integration or hierarchical grid evaluation for aerodynamic simulations.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic parallelism only works within a given GPU. Kernels can invoke memory
    copies or other kernels, but they cannot submit work to other GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1\. Scoping and Synchronization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With the notable exception of block and grid size, child grids inherit most
    kernel configuration parameters, such as the shared memory configuration (set
    by `cudaDeviceSetCacheConfig()`), from their parents. Thread blocks are a unit
    of scope: Streams and events created by a thread block can only be used by that
    thread block (they are not even inherited for use by child grids), and they are
    automatically destroyed when the thread block exits.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Resources created on the device via dynamic parallelism are strictly separated
    from resources created on the host. Streams and events created on the host may
    not be used on the device via dynamic parallelism, and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA guarantees that a parent grid is not considered complete until all of its
    children have finished. Although the parent may execute concurrently with the
    child, there is no guarantee that a child grid will begin execution until its
    parent calls `cudaDeviceSynchronize().`
  prefs: []
  type: TYPE_NORMAL
- en: If all threads in a thread block exit, execution of the thread block is suspended
    until all child grids have finished. If that synchronization is not sufficient,
    developers can use CUDA streams and events for explicit synchronization. As on
    the host, operations within a given stream are performed in the order of submission.
    Operations can only execute concurrently if they are specified in different streams,
    and there is no guarantee that operations will, in fact, execute concurrently.
    If needed, synchronization primitives such as `__syncthreads()` can be used to
    coordinate the order of submission to a given stream.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Streams and events created on the device may not be used outside the thread
    block that created them.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaDeviceSynchronize()` synchronizes on all pending work launched by any
    thread in the thread block. It does not, however, perform any interthread synchronization,
    so if there is a desire to synchronize on work launched by other threads, developers
    must use `__syncthreads()` or other block-level synchronization primitives (see
    [Section 8.6.2](ch08.html#ch08lev2sec21)).'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.2\. Memory Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Parent and child grids share the same global and constant memory storage, but
    they have distinct local and shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: Global Memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are two points in the execution of a child grid when its view of memory
    is fully consistent with the parent grid: when the child grid is invoked by the
    parent and when the child grid completes as signaled by a synchronization API
    invocation in the parent thread.'
  prefs: []
  type: TYPE_NORMAL
- en: All global memory operations in the parent thread prior to the child thread’s
    invocation are visible to the child grid. All memory operations of the child grid
    are visible to the parent after the parent has synchronized on the child grid’s
    completion. Zero-copy memory has the same coherence and consistency guarantees
    as global memory.
  prefs: []
  type: TYPE_NORMAL
- en: Constant Memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Constants are immutable and may not be modified from the device during kernel
    execution. Taking the address of a constant memory object from within a kernel
    thread has the same semantics as for all CUDA programs,^([13](ch07.html#ch07fn13))
    and passing that pointer between parents and their children is fully supported.
  prefs: []
  type: TYPE_NORMAL
- en: '[13](ch07.html#ch07fn13a). Note that in device code, the address must be taken
    with the “address-of” operator (unary operator&), since `cudaGetSymbolAddress()`
    is not supported by the device runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: Shared and Local Memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Shared and local memory is private to a thread block or thread, respectively,
    and is not visible or coherent between parent and child. When an object in one
    of these locations is referenced outside its scope, the behavior is undefined
    and would likely cause an error.
  prefs: []
  type: TYPE_NORMAL
- en: If `nvcc` detects an attempt to misuse a pointer to shared or local memory,
    it will issue a warning. Developers can use the `__isGlobal()` intrinsic to determine
    whether a given pointer references global memory. Pointers to shared or local
    memory are not valid parameters to `cudaMemcpy*Async()` or `cudaMemset*Async()`.
  prefs: []
  type: TYPE_NORMAL
- en: Local Memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Local memory is private storage for an executing thread and is not visible outside
    of that thread. It is illegal to pass a pointer to local memory as a launch argument
    when launching a child kernel. The result of dereferencing such a local memory
    pointer from a child will be undefined. To guarantee that this rule is not inadvertently
    violated by the compiler, all storage passed to a child kernel should be allocated
    explicitly from the global memory heap.
  prefs: []
  type: TYPE_NORMAL
- en: Texture Memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Concurrent accesses by parent and child may result in inconsistent data and
    should be avoided. That said, a degree of coherency between parent and child is
    enforced by the runtime. A child kernel can use texturing to access memory written
    by its parent, but writes to memory by a child will not be reflected in the texture
    memory accesses by a parent until *after* the parent synchronizes on the child’s
    completion. Texture objects are well supported in the device runtime. They cannot
    be created or destroyed, but they can be passed in and used by any grid in the
    hierarchy (parent or child).
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.3\. Streams and Events
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Streams and events created by the device runtime can be used only within the
    thread block that created the stream. The NULL stream has different semantics
    in the device runtime than in the host runtime. On the host, synchronizing with
    the NULL stream forces a “join” of all the other streamed operations on the GPU
    (as described in [Section 6.2.3](ch06.html#ch06lev2sec4)); on the device, the
    NULL stream is its own stream, and any interstream synchronization must be performed
    using events.
  prefs: []
  type: TYPE_NORMAL
- en: When using the device runtime, streams must be created with the `cudaStream-NonBlocking`
    flag (a parameter to `cudaStreamCreateWithFlags()`). The `cudaStreamSynchronize()`
    call is not supported; synchronization must be implemented in terms of events
    and `cudaStreamWaitEvent()`.
  prefs: []
  type: TYPE_NORMAL
- en: Only the interstream synchronization capabilities of CUDA events are supported.
    As a consequence, `cudaEventSynchronize()`, `cudaEventElapsedTime()`, and `cudaEventQuery()`
    are not supported. Additionally, because timing is not supported, events must
    be created by passing the `cudaEventDisableTiming` flag to `cudaEventCreateWithFlags()`.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.4\. Error Handling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Any function in the device runtime may return an error (`cudaError_t`). The
    error is recorded in a per-thread slot that can be queried by calling `cudaGetLastError()`.
    As with the host-based runtime, CUDA makes a distinction between errors that can
    be returned immediately (e.g., if an invalid parameter is passed to a memcpy function)
    and errors that must be reported asynchronously (e.g., if a launch performed an
    invalid memory access). If a child grid causes an error at runtime, CUDA will
    return an error to the host, not to the parent grid.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.5\. Compiling and Linking
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike the host runtime, developers must explicitly link against the device
    runtime’s static library when using the device runtime. On Windows, the device
    runtime is `cudadevrt.lib`; on Linux and MacOS, it is `cudadevrt.a`. When building
    with `nvcc`, this may be accomplished by appending `-lcudadevrt` to the command
    line.
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.6\. Resource Management
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Whenever a kernel launches a child grid, the child is considered a new *nesting
    level*, and the total number of levels is the *nesting depth* of the program.
    In contrast, the deepest level at which the program will explicitly synchronize
    on a child launch is called the *synchronization depth*. Typically the synchronization
    depth is one less than the nesting depth of the program, but if the program does
    not always need to call `cudaDeviceSynchronize()`, then it may be substantially
    less than the nesting depth.
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical maximum nesting depth is 24, but in practice it is governed
    by the device limit `cudaLimitDevRuntimeSyncDepth`. Any launch that would result
    in a kernel at a deeper level than the maximum will fail. The default maximum
    synchronization depth level is 2\. The limits must be configured before the top-level
    kernel is launched from the host.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Calling a device runtime function such as `cudaMemcpyAsync()` may invoke a kernel,
    increasing the nesting depth by 1.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: For parent kernels that never call `cudaDeviceSynchronize()`, the system does
    not have to reserve space for the parent kernel. In this case, the memory footprint
    required for a program will be much less than the conservative maximum. Such a
    program could specify a shallower maximum synchronization depth to avoid overallocation
    of backing store.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Footprint
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The device runtime system software reserves device memory for the following
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: • To track pending grid launches
  prefs: []
  type: TYPE_NORMAL
- en: • To save saving parent-grid state during synchronization
  prefs: []
  type: TYPE_NORMAL
- en: • To serve as an allocation heap for `malloc()` and `cudaMalloc()` calls from
    kernels
  prefs: []
  type: TYPE_NORMAL
- en: This memory is not available for use by the application, so some applications
    may wish to reduce the default allocations, and some applications may have to
    increase the default values in order to operate correctly. To change the default
    values, developers call `cudaDeviceSetLimit()`, as summarized in [Table 7.3](ch07.html#ch07tab03).
    The limit `cudaLimitDevRuntimeSyncDepth` is especially important, since each nesting
    level costs up to 150MB of device memory.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07tab03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 7.3* `cudaDeviceSetLimit()` Values'
  prefs: []
  type: TYPE_NORMAL
- en: Pending Kernel Launches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When a kernel is launched, all associated configuration and parameter data is
    tracked until the kernel completes. This data is stored within a system-managed
    launch pool. The size of the launch pool is configurable by calling `cudaDeviceSetLimit()`
    from the host and specifying `cudaLimitDevRuntimePendingLaunchCount`.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration Options
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Resource allocation for the device runtime system software is controlled via
    the `cudaDeviceSetLimit()` API from the host program. Limits must be set before
    any kernel is launched and may not be changed while the GPU is actively running
    programs.
  prefs: []
  type: TYPE_NORMAL
- en: Memory allocated by the device runtime must be freed by the device runtime.
    Also, memory is allocated by the device runtime out of a preallocated heap whose
    size is specified by the device limit `cudaLimitMallocHeapSize`. The named limits
    in [Table 7.3](ch07.html#ch07tab03) may be set.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07tab04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 7.4* Device Runtime Limitations'
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.7\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Table 7.4](ch07.html#ch07tab04) summarizes the key differences and limitations
    between the device runtime and the host runtime. [Table 7.5](ch07.html#ch07tab05)
    lists the subset of functions that may be called from the device runtime, along
    with any pertinent limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07tab05.jpg)![Image](graphics/07tab05a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 7.5.* CUDA Device Runtime Functions'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8\. Streaming Multiprocessors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The streaming multiprocessors (SMs) are the part of the GPU that runs our CUDA
    kernels. Each SM contains the following.
  prefs: []
  type: TYPE_NORMAL
- en: • Thousands of registers that can be partitioned among threads of execution
  prefs: []
  type: TYPE_NORMAL
- en: '• Several caches:'
  prefs: []
  type: TYPE_NORMAL
- en: – *Shared memory* for fast data interchange between threads
  prefs: []
  type: TYPE_NORMAL
- en: – *Constant cache* for fast broadcast of reads from constant memory
  prefs: []
  type: TYPE_NORMAL
- en: – *Texture cache* to aggregate bandwidth from texture memory
  prefs: []
  type: TYPE_NORMAL
- en: – *L1 cache* to reduce latency to local or global memory
  prefs: []
  type: TYPE_NORMAL
- en: • *Warp schedulers* that can quickly switch contexts between threads and issue
    instructions to warps that are ready to execute
  prefs: []
  type: TYPE_NORMAL
- en: '• Execution cores for integer and floating-point operations:'
  prefs: []
  type: TYPE_NORMAL
- en: – Integer and single-precision floating point operations
  prefs: []
  type: TYPE_NORMAL
- en: – Double-precision floating point
  prefs: []
  type: TYPE_NORMAL
- en: – Special Function Units (SFUs) for single-precision floating-point transcendental
    functions
  prefs: []
  type: TYPE_NORMAL
- en: The reason there are many registers and the reason the hardware can context
    switch between threads so efficiently are to maximize the throughput of the hardware.
    The GPU is designed to have enough state to cover both execution latency and the
    memory latency of hundreds of clock cycles that it may take for data from device
    memory to arrive after a read instruction is executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SMs are general-purpose processors, but they are designed very differently
    than the execution cores in CPUs: They target much lower clock rates; they support
    instruction-level parallelism, but not branch prediction or speculative execution;
    and they have less cache, if they have any cache at all. For suitable workloads,
    the sheer computing horsepower in a GPU more than makes up for these disadvantages.'
  prefs: []
  type: TYPE_NORMAL
- en: The design of the SM has been evolving rapidly since the introduction of the
    first CUDA-capable hardware in 2006, with three major revisions, codenamed Tesla,
    Fermi, and Kepler. Developers can query the compute capability by calling `cudaGetDeviceProperties()`
    and examining `cudaDeviceProp.major` and `cudaDeviceProp.minor`, or by calling
    the driver API function `cuDeviceComputeCapability()`. Compute capability 1.x,
    2.x, and 3.x correspond to Tesla-class, Fermi-class, and Kepler-class hardware,
    respectively. [Table 8.1](ch08.html#ch08tab01) summarizes the capabilities added
    in each generation of the SM hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.1* SM Capabilities'
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#ch02), [Figures 2.29](ch02.html#ch02fig29) through
    [2.32](ch02.html#ch02fig32) show block diagrams of different SMs. CUDA cores can
    execute integer and single-precision floating-point instructions; one double-precision
    unit implements double-precision support, if available; and Special Function Units
    implement reciprocal, recriprocal square root, sine/cosine, and logarithm/exponential
    functions. Warp schedulers dispatch instructions to these execution units as the
    resources needed to execute the instruction become available.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on the instruction set capabilities of the SM. As such,
    it sometimes refers to the “SASS” instructions, the native instructions into which
    `ptxas` or the CUDA driver translate intermediate PTX code. Developers are not
    able to author SASS code directly; instead, NVIDIA has made these instructions
    visible to developers through the `cuobjdump` utility so they can direct optimizations
    of their source code by examining the compiled microcode.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1\. Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 8.1.1\. Registers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each SM contains thousands of 32-bit registers that are allocated to threads
    as specified when the kernel is launched. Registers are both the fastest and most
    plentiful memory in the SM. As an example, the Kepler-class (SM 3.0) SMX contains
    65,536 registers or 256K, while the texture cache is only 48K.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA registers can contain integer or floating-point data; for hardware capable
    of performing double-precision arithmetic (SM 1.3 and higher), the operands are
    contained in even-valued register pairs. On SM 2.0 and higher hardware, register
    pairs also can hold 64-bit addresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'CUDA hardware also supports wider memory transactions: The built-in `int2/float2`
    and `int4/float4` data types, residing in aligned register pairs or quads, respectively,
    may be read or written using single 64- or 128-bit-wide loads or stores. Once
    in registers, the individual data elements can be referenced as `.x/.y` (for `int2/float2`)
    or `.x/.y/.z/.w` (for `int4/float4`).'
  prefs: []
  type: TYPE_NORMAL
- en: Developers can cause `nvcc` to report the number of registers used by a kernel
    by specifying the command-line option `--ptxas-options -–verbose`. The number
    of registers used by a kernel affects the number of threads that can fit in an
    SM and often must be tuned carefully for optimal performance. The maximum number
    of registers used for a compilation may be specified with `--ptxas-options --maxregcount
    N`.
  prefs: []
  type: TYPE_NORMAL
- en: Register Aliasing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Because registers can hold floating-point or integer data, some intrinsics serve
    only to coerce the compiler into changing its view of a variable. The `__int_as_float()`
    and `__float_as_int()` intrinsics cause a variable to “change personalities” between
    32-bit integer and single-precision floating point.
  prefs: []
  type: TYPE_NORMAL
- en: float __int_as_float( int i );
  prefs: []
  type: TYPE_NORMAL
- en: int __float_as_int( float f );
  prefs: []
  type: TYPE_NORMAL
- en: The `__double2loint()`, `__double2hiint()`, and `__hiloint2double()` intrinsics
    similarly cause registers to change personality (usually in-place). `__double_as_longlong()`
    and `__longlong_as_double()` coerce register pairs in-place; `__double2loint()`
    and `__double2hiint()` return the least and the most significant 32 bits of the
    input operand, respectively; and `__hiloint2double()` constructs a `double` out
    of the high and low halves.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p234pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: int double2loint( double d );
  prefs: []
  type: TYPE_NORMAL
- en: int double2hiint( double d );
  prefs: []
  type: TYPE_NORMAL
- en: int hiloint2double( int hi, int lo );
  prefs: []
  type: TYPE_NORMAL
- en: double long_as_double(long long int i );
  prefs: []
  type: TYPE_NORMAL
- en: long long int __double_as_longlong( double d );
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2\. Local Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Local memory is used to spill registers and also to hold local variables that
    are indexed and whose indices cannot be computed at compile time. Local memory
    is backed by the same pool of device memory as global memory, so it exhibits the
    same latency characteristics and benefits as the L1 and L2 cache hierarchy on
    Fermi and later hardware. Local memory is addressed in such a way that the memory
    transactions are automatically coalesced. The hardware includes special instructions
    to load and store local memory: The SASS variants are `LLD/LST` for Tesla and
    `LDL/STL` for Fermi and Kepler.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.3\. Global Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The SMs can read or write global memory using `GLD/GST` instructions (on Tesla)
    and `LD/ST` instructions (on Fermi and Kepler). Developers can use standard C
    operators to compute and dereference addresses, including pointer arithmetic and
    the dereferencing operators `*`, `[]`, and `->`. Operating on 64- or 128-bit built-in
    data types (`int2/float2/int4/float4`) automatically causes the compiler to issue
    64- or 128-bit load and store instructions. Maximum memory performance is achieved
    through *coalescing* of memory transactions, described in [Section 5.2.9](ch05.html#ch05lev2sec17).
  prefs: []
  type: TYPE_NORMAL
- en: Tesla-class hardware (SM 1.x) uses special address registers to hold pointers;
    later hardware implements a load/store architecture that uses the same register
    file for pointers; integer and floating-point values; and the same address space
    for constant memory, shared memory, and global memory.^([1](ch08.html#ch08fn1))
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch08.html#ch08fn1a). Both constant and shared memory exist in address windows
    that enable them to be referenced by 32-bit addresses even on 64-bit architectures.'
  prefs: []
  type: TYPE_NORMAL
- en: Fermi-class hardware includes several features not available on older hardware.
  prefs: []
  type: TYPE_NORMAL
- en: • 64-bit addressing is supported via “wide” load/store instructions in which
    addresses are held in even-numbered register pairs. 64-bit addressing is not supported
    on 32-bit host platforms; on 64-bit host platforms, 64-bit addressing is enabled
    automatically. As a result, code generated for the same kernels compiled for 32-
    and 64-bit host platforms may have different register counts and performance.
  prefs: []
  type: TYPE_NORMAL
- en: • The L1 cache may be configured to be 16K or 48K in size.^([2](ch08.html#ch08fn2))
    (Kepler added the ability to split the cache as 32K L1/32K shared.) Load instructions
    can include cacheability hints (to tell the hardware to pull the read into L1
    or to bypass the L1 and keep the data only in L2). These may be accessed via inline
    PTX or through the command line option `–X ptxas –dlcm=ca` (cache in L1 and L2,
    the default setting) or `–X ptxas –dlcm=cg` (cache only in L2).
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch08.html#ch08fn2a). The hardware can change this configuration per kernel
    launch, but changing this state is expensive and will break concurrency for concurrent
    kernel launches.'
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations (or just “atomics”) update a memory location in a way that
    works correctly even when multiple GPU threads are operating on the same memory
    location. The hardware enforces mutual exclusion on the memory location for the
    duration of the operation. Since the order of operations is not guaranteed, the
    operators supported generally are associative.^([3](ch08.html#ch08fn3))
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch08.html#ch08fn3a). The only exception is single-precision floating-point
    addition. Then again, floating-point code generally must be robust in the face
    of the lack of associativity of floating-point operations; porting to different
    hardware, or even just recompiling the same code with different compiler options,
    can change the order of floating-point operations and thus the result.'
  prefs: []
  type: TYPE_NORMAL
- en: Atomics first became available for global memory for SM 1.1 and greater and
    for shared memory for SM 1.2 and greater. Until the Kepler generation of hardware,
    however, global memory atomics were too slow to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: The global atomic intrinsics, summarized in [Table 8.2](ch08.html#ch08tab02),
    become automatically available when the appropriate architecture is specified
    to `nvcc` via `--gpu-architecture`. All of these intrinsics can operate on 32-bit
    integers. 64-bit support for `atomicAdd()`, `atomicExch()`, and `atomicCAS()`
    was added in SM 1.2\. `atomicAdd()` of 32-bit floating-point values (`float`)
    was added in SM 2.0\. 64-bit support for `atomicMin()`, `atomicMax()`, `atomicAnd()`,
    `atomicOr()`, and `atomicXor()` was added in SM 3.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.2* Atomic Operations'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Because atomic operations are implemented using hardware in the GPU’s integrated
    memory controller, they do not work across the PCI Express bus and thus do not
    work correctly on device memory pointers that correspond to host memory or peer
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the hardware level, atomics come in two forms: atomic operations that return
    the value that was at the specified memory location before the operator was performed,
    and reduction operations that the developer can “fire and forget” at the memory
    location, ignoring the return value. Since the hardware can perform the operation
    more efficiently if there is no need to return the old value, the compiler detects
    whether the return value is used and, if it is not, emits different instructions.
    In SM 2.0, for example, the instructions are called `ATOM` and `RED`, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.4\. Constant Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Constant memory resides in device memory, but it is backed by a different, read-only
    cache that is optimized to broadcast the results of read requests to threads that
    all reference the same memory location. Each SM contains a small, latency-optimized
    cache for purposes of servicing these read requests. Making the memory (and the
    cache) read-only simplifies cache management, since the hardware has no need to
    implement write-back policies to deal with memory that has been updated.
  prefs: []
  type: TYPE_NORMAL
- en: SM 2.x and subsequent hardware includes a special optimization for memory that
    is not denoted as constant but that the compiler has identified as (1) read-only
    and (2) whose address is not dependent on the block or thread ID. The “load uniform”
    (LDU) instruction reads memory using the constant cache hierarchy and broadcasts
    the data to the threads.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.5\. Shared Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Shared memory is very fast, on-chip memory in the SM that threads can use for
    data interchange within a thread block. Since it is a per-SM resource, shared
    memory usage can affect occupancy, the number of warps that the SM can keep resident.
    SMs load and store shared memory with special instructions: `G2R/R2G` on SM 1.x,
    and `LDS/STS` on SM 2.x and later.'
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory is arranged as interleaved *banks* and generally is optimized
    for 32-bit access. If more than one thread in a warp references the same bank,
    a *bank conflict* occurs, and the hardware must handle memory requests consecutively
    until all requests have been serviced. Typically, to avoid bank conflicts, applications
    access shared memory with an interleaved pattern based on the thread ID, such
    as the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p238pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ float shared[];
  prefs: []
  type: TYPE_NORMAL
- en: float data = shared[BaseIndex + threadIdx.x];
  prefs: []
  type: TYPE_NORMAL
- en: Having all threads in a warp read from the same 32-bit shared memory location
    also is fast. The hardware includes a broadcast mechanism to optimize for this
    case. Writes to the same bank are serialized by the hardware, reducing performance.
    Writes to the same *address* cause race conditions and should be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: For 2D access patterns (such as tiles of pixels in an image processing kernel),
    it’s good practice to pad the shared memory allocation so the kernel can reference
    adjacent rows without causing bank conflicts. SM 2.x and subsequent hardware has
    32 banks,^([4](ch08.html#ch08fn4)) so for 2D tiles where threads in the same warp
    may access the data by row, it is a good strategy to pad the tile size to a multiple
    of 33 32-bit words.
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch08.html#ch08fn4a). SM 1.x hardware had 16 banks (memory traffic from
    the first 16 threads and the second 16 threads of a warp was serviced separately),
    but strategies that work well on subsequent hardware also work well on SM 1.x.'
  prefs: []
  type: TYPE_NORMAL
- en: On SM 1.x hardware, shared memory is about 16K in size;^([5](ch08.html#ch08fn5))
    on later hardware, there is a total of 64K of L1 cache that may be configured
    as 16K or 48K of shared memory, of which the remainder is used as L1 cache.^([6](ch08.html#ch08fn6))
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch08.html#ch08fn5a). 256 bytes of shared memory was reserved for parameter
    passing; in SM 2.x and later, parameters are passed via constant memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch08.html#ch08fn6a). SM 3.x hardware adds the ability to split the cache
    evenly as 32K L1/32K shared.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the last few generations of hardware, NVIDIA has improved the hardware’s
    handling of operand sizes other than 32 bits. On SM 1.x hardware, 8- and 16-bit
    reads from the same bank caused bank conflicts, while SM 2.x and later hardware
    can broadcast reads of any size out of the same bank. Similarly, 64-bit operands
    (such as `double`) in shared memory were so much slower than 32-bit operands on
    SM 1.x that developers sometimes had to resort to storing the data as separate
    high and low halves. SM 3.x hardware adds a new feature for kernels that predominantly
    use 64-bit operands in shared memory: a mode that increases the bank size to 64
    bits.'
  prefs: []
  type: TYPE_NORMAL
- en: Atomics in Shared Memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: SM 1.2 added the ability to perform atomic operations in shared memory. Unlike
    global memory, which implements atomics using single instructions (either `GATOM`
    or `GRED`, depending on whether the return value is used), shared memory atomics
    are implemented with explicit lock/unlock semantics, and the compiler emits code
    that causes each thread to loop over these lock operations until the thread has
    performed its atomic operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 8.1](ch08.html#ch08lis01) gives the source code to `atomic32Shared.cu`,
    a program specifically intended to be compiled to highlight the code generation
    for shared memory atomics. [Listing 8.2](ch08.html#ch08lis02) shows the resulting
    microcode generated for SM 2.0\. Note how the `LDSLK` (load shared with lock)
    instruction returns a predicate that tells whether the lock was acquired, the
    code to perform the update is predicated, and the code loops until the lock is
    acquired and the update performed.'
  prefs: []
  type: TYPE_NORMAL
- en: The lock is performed per 32-bit word, and the index of the lock is determined
    by bits 2–9 of the shared memory address. Take care to avoid contention, or the
    loop in [Listing 8.2](ch08.html#ch08lis02) may iterate up to 32 times.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8.1.* `atomic32Shared.cu`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p08lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: Return32( int *sum, int *out, const int *pIn )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ int s[];
  prefs: []
  type: TYPE_NORMAL
- en: s[threadIdx.x] = pIn[threadIdx.x];
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: (void) atomicAdd( &s[threadIdx.x], *pIn );
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: out[threadIdx.x] = s[threadIdx.x];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8.2.* `atomic32Shared.cubin` (microcode compiled for SM 2.0).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p08lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: code for sm_20
  prefs: []
  type: TYPE_NORMAL
- en: 'Function : _Z8Return32PiS_PKi'
  prefs: []
  type: TYPE_NORMAL
- en: /*0000*/     MOV R1, c [0x1] [0x100];
  prefs: []
  type: TYPE_NORMAL
- en: /*0008*/     S2R R0, SR_Tid_X;
  prefs: []
  type: TYPE_NORMAL
- en: /*0010*/     SHL R3, R0, 0x2;
  prefs: []
  type: TYPE_NORMAL
- en: /*0018*/     MOV R0, c [0x0] [0x28];
  prefs: []
  type: TYPE_NORMAL
- en: /*0020*/     IADD R2, R3, c [0x0] [0x28];
  prefs: []
  type: TYPE_NORMAL
- en: /*0028*/     IMAD.U32.U32 RZ, R0, R1, RZ;
  prefs: []
  type: TYPE_NORMAL
- en: /*0030*/     LD R2, [R2];
  prefs: []
  type: TYPE_NORMAL
- en: /*0038*/     STS [R3], R2;
  prefs: []
  type: TYPE_NORMAL
- en: /*0040*/     SSY 0x80;
  prefs: []
  type: TYPE_NORMAL
- en: /*0048*/     BAR.RED.POPC RZ, RZ;
  prefs: []
  type: TYPE_NORMAL
- en: /*0050*/     LD R0, [R0];
  prefs: []
  type: TYPE_NORMAL
- en: /*0058*/     LDSLK P0, R2, [R3];
  prefs: []
  type: TYPE_NORMAL
- en: /*0060*/     @P0 IADD R2, R2, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0068*/     @P0 STSUL [R3], R2;
  prefs: []
  type: TYPE_NORMAL
- en: /*0070*/     @!P0 BRA 0x58;
  prefs: []
  type: TYPE_NORMAL
- en: /*0078*/     NOP.S CC.T;
  prefs: []
  type: TYPE_NORMAL
- en: /*0080*/     BAR.RED.POPC RZ, RZ;
  prefs: []
  type: TYPE_NORMAL
- en: /*0088*/     LDS R0, [R3];
  prefs: []
  type: TYPE_NORMAL
- en: /*0090*/     IADD R2, R3, c [0x0] [0x24];
  prefs: []
  type: TYPE_NORMAL
- en: /*0098*/     ST [R2], R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*00a0*/     EXIT;
  prefs: []
  type: TYPE_NORMAL
- en: '...................................'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.6\. Barriers and Coherency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The familiar `__syncthreads()` intrinsic waits until all the threads in the
    thread block have arrived before proceeding. It is needed to maintain coherency
    of shared memory within a thread block.^([7](ch08.html#ch08fn7)) Other, similar
    memory barrier instructions can be used to enforce some ordering on broader scopes
    of memory, as described in [Table 8.3](ch08.html#ch08tab03).
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch08.html#ch08fn7a). Note that threads within a warp run in lockstep, sometimes
    enabling developers to write so-called “warp synchronous” code that does not call
    `__syncthreads()`. [Section 7.3](ch07.html#ch07lev1sec3) describes thread and
    warp execution in detail, and [Part III](part03.html#part03) includes several
    examples of warp synchronous code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.3* Memory Barrier Intrinsics'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2\. Integer Support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SMs have the full complement of 32-bit integer operations.
  prefs: []
  type: TYPE_NORMAL
- en: • Addition with optional negation of an operand for subtraction
  prefs: []
  type: TYPE_NORMAL
- en: • Multiplication and multiply-add
  prefs: []
  type: TYPE_NORMAL
- en: • Integer division
  prefs: []
  type: TYPE_NORMAL
- en: • Logical operations
  prefs: []
  type: TYPE_NORMAL
- en: • Condition code manipulation
  prefs: []
  type: TYPE_NORMAL
- en: • Conversion to/from floating point
  prefs: []
  type: TYPE_NORMAL
- en: • Miscellaneous operations (e.g., SIMD instructions for narrow integers, population
    count, find first zero)
  prefs: []
  type: TYPE_NORMAL
- en: CUDA exposes most of this functionality through standard C operators. Nonstandard
    operations, such as 24-bit multiplication, may be accessed using inline PTX assembly
    or intrinsic functions.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1\. Multiplication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multiplication is implemented differently on Tesla- and Fermi-class hardware.
    Tesla implements a 24-bit multiplier, while Fermi implements a 32-bit multiplier.
    As a consequence, full 32-bit multiplication on SM 1.x hardware requires four
    instructions. For performance-sensitive code targeting Tesla-class hardware, it
    is a performance win to use the intrinsics for 24-bit multiply.^([8](ch08.html#ch08fn8))
    [Table 8.4](ch08.html#ch08tab04) shows the intrinsics related to multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch08.html#ch08fn8a). Using `__mul24()` or `__umul24()` on SM 2.x and later
    hardware, however, is a performance *penalty*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.4* Multiplication Intrinsics'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2\. Miscellaneous (Bit Manipulation)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CUDA compiler implements a number of intrinsics for bit manipulation, as
    summarized in [Table 8.5](ch08.html#ch08tab05). On SM 2.x and later architectures,
    these intrinsics map to single instructions. On pre-Fermi architectures, they
    are valid but may compile into many instructions. When in doubt, disassemble and
    look at the microcode! 64-bit variants have “`ll`” (two ells for “long long”)
    appended to the intrinsic name `__clzll(), ffsll(), popcll(), brevll()`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.5* Bit Manipulation Intrinsics'
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3\. Funnel Shift (SM 3.5)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GK110 added a 64-bit “funnel shift” instruction that concatenates two 32-bit
    values together (the least significant and most significant halves are specified
    as separate 32-bit inputs, but the hardware operates on an aligned register pair),
    shifts the resulting 64-bit value left or right, and then returns the most significant
    (for left shift) or least significant (for right shift) 32 bits.
  prefs: []
  type: TYPE_NORMAL
- en: Funnel shift may be accessed with the intrinsics given in [Table 8.6](ch08.html#ch08tab06).
    These intrinsics are implemented as inline device functions (using inline PTX
    assembler) in `sm_35_intrinsics.h`. By default, the least significant 5 bits of
    the shift count are masked off; the `_lc` and `_rc` intrinsics clamp the shift
    value to the range 0..32.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.6* Funnel Shift Intrinsics'
  prefs: []
  type: TYPE_NORMAL
- en: Applications for funnel shift include the following.
  prefs: []
  type: TYPE_NORMAL
- en: • Multiword shift operations
  prefs: []
  type: TYPE_NORMAL
- en: • Memory copies between misaligned buffers using aligned loads and stores
  prefs: []
  type: TYPE_NORMAL
- en: • Rotate
  prefs: []
  type: TYPE_NORMAL
- en: To right-shift data sizes greater than 64 bits, use repeated `__funnelshift_r()`
    calls, operating from the least significant to the most significant word. The
    most significant word of the result is computed using `operator>>`, which shifts
    in zero or sign bits as appropriate for the integer type. To left-shift data sizes
    greater than 64 bits, use repeated `__funnelshift_l()` calls, operating from the
    most significant to the least significant word. The least significant word of
    the result is computed using `operator<<`. If the `hi` and `lo` parameters are
    the same, the funnel shift effects a rotate operation.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3\. Floating-Point Support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fast native floating-point hardware is the *raison d’être* for GPUs, and in
    many ways they are equal to or superior to CPUs in their floating-point implementation.
    Denormals are supported at full speed,^([9](ch08.html#ch08fn9)) directed rounding
    may be specified on a per-instruction basis, and the Special Function Units deliver
    high-performance approximation functions to six popular single-precision transcendentals.
    In contrast, x86 CPUs implement denormals in microcode that runs perhaps 100x
    slower than operating on normalized floating-point operands. Rounding direction
    is specified by a control word that takes dozens of clock cycles to change, and
    the only transcendental approximation functions in the SSE instruction set are
    for reciprocal and reciprocal square root, which give 12-bit approximations that
    must be refined with a Newton-Raphson iteration before being used.
  prefs: []
  type: TYPE_NORMAL
- en: '[9](ch08.html#ch08fn9a). With the exception that single-precision denormals
    are not supported at all on SM 1.x hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: Since GPUs’ greater core counts are offset somewhat by their lower clock frequencies,
    developers can expect at most a 10x (or thereabouts) speedup on a level playing
    field. If a paper reports a 100x or greater speedup from porting an optimized
    CPU implementation to CUDA, chances are one of the above-described “instruction
    set mismatches” played a role.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1\. Formats
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 8.2](ch08.html#ch08fig02) depicts the three (3) IEEE standard floating-point
    formats supported by CUDA: double precision (64-bit), single precision (32-bit),
    and half precision (16-bit). The values are divided into three fields: sign, exponent,
    and mantissa. For `double`, `single`, and `half`, the exponent fields are 11,
    8, and 5 bits in size, respectively; the corresponding mantissa fields are 52,
    23, and 10 bits.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8.2* Floating-point formats.'
  prefs: []
  type: TYPE_NORMAL
- en: The exponent field changes the interpretation of the floating-point value. The
    most common (“normal”) representation encodes an implicit 1 bit into the mantissa
    and multiplies that value by 2^(e-bias), where *bias* is the value added to the
    actual exponent before encoding into the floating-point representation. The bias
    for single precision, for example, is 127.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8.7](ch08.html#ch08tab07) summarizes how floating-point values are encoded.
    For most exponent values (so-called “normal” floating-point values), the mantissa
    is assumed to have an implicit 1, and it is multiplied by the biased value of
    the exponent. The maximum exponent value is reserved for infinity and Not-A-Number
    values. Dividing by zero (or overflowing a division) yields infinity; performing
    an invalid operation (such as taking the square root or logarithm of a negative
    number) yields a NaN. The minimum exponent value is reserved for values too small
    to represent with the implicit leading 1\. As the so-called *denormals*^([10](ch08.html#ch08fn10))
    get closer to zero, they lose bits of effective precision, a phenomenon known
    as *gradual underflow*. [Table 8.8](ch08.html#ch08tab08) gives the encodings and
    values of certain extreme values for the three formats.'
  prefs: []
  type: TYPE_NORMAL
- en: '[10](ch08.html#ch08fn10a). Sometimes called *subnormals*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.7* Floating-Point Representations'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.8* Floating-Point Extreme Values'
  prefs: []
  type: TYPE_NORMAL
- en: Rounding
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The IEEE standard provides for four (4) round modes.
  prefs: []
  type: TYPE_NORMAL
- en: • Round-to-nearest-even (also called “round-to-nearest”)
  prefs: []
  type: TYPE_NORMAL
- en: • Round toward zero (also called “truncate” or “chop”)
  prefs: []
  type: TYPE_NORMAL
- en: • Round down (or “round toward negative infinity”)
  prefs: []
  type: TYPE_NORMAL
- en: • Round up (or “round toward positive infinity”)
  prefs: []
  type: TYPE_NORMAL
- en: Round-to-nearest, where intermediate values are rounded to the nearest representable
    floating-point value after each operation, is by far the most commonly used round
    mode. Round up and round down (the “directed rounding modes”) are used for *interval
    arithmetic*, where a pair of floating-point values are used to bracket the intermediate
    result of a computation. To correctly bracket a result, the lower and upper values
    of the interval must be rounded toward negative infinity (“down”) and toward positive
    infinity (“up”), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The C language does not provide any way to specify round modes on a per-instruction
    basis, and CUDA hardware does not provide a control word to implicitly specify
    rounding modes. Consequently, CUDA provides a set of intrinsics to specify the
    round mode of an operation, as summarized in [Table 8.9](ch08.html#ch08tab09).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.9* Intrinsics for Rounding'
  prefs: []
  type: TYPE_NORMAL
- en: Conversion
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In general, developers can convert between different floating-point representations
    and/or integers using standard C constructs: implicit conversion or explicit typecasts.
    If necessary, however, developers can use the intrinsics listed in [Table 8.10](ch08.html#ch08tab10)
    to perform conversions that are not in the C language specification, such as those
    with directed rounding.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.10* Intrinsics for Conversion'
  prefs: []
  type: TYPE_NORMAL
- en: Because `half` is not standardized in the C programming language, CUDA uses
    `unsigned short` in the interfaces for `__half2float()` and `__float2half()`.
    `__float2half()` only supports the round-to-nearest rounding mode.
  prefs: []
  type: TYPE_NORMAL
- en: float __half2float( unsigned short );
  prefs: []
  type: TYPE_NORMAL
- en: unsigned short __float2half( float );
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2\. Single Precision (32-Bit)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Single-precision floating-point support is the workhorse of GPU computation.
    GPUs have been optimized to natively deliver high performance on this data type,^([11](ch08.html#ch08fn11))
    not only for core standard IEEE operations such as addition and multiplication,
    but also for nonstandard operations such as approximations to transcendentals
    such as `sin()` and `log()`. The 32-bit values are held in the same register file
    as integers, so coercion between single-precision floating-point values and 32-bit
    integers (with `__float_as_int()` and `__int_as_float()`) is free.
  prefs: []
  type: TYPE_NORMAL
- en: '[11](ch08.html#ch08fn11a). In fact, GPUs had full 32-bit floating-point support
    before they had full 32-bit integer support. As a result, some early GPU computing
    literature explained how to implement integer math with floating-point hardware!'
  prefs: []
  type: TYPE_NORMAL
- en: Addition, Multiplication, and Multiply-Add
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The compiler automatically translates +, –, and * operators on floating-point
    values into addition, multiplication, and multiply-add instructions. The `__fadd_rn()`
    and `__fmul_rn()` intrinsics may be used to suppress fusion of addition and multiplication
    operations into multiply-add instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Reciprocal and Division
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For devices of compute capability 2.x and higher, the division operator is IEEE-compliant
    when the code is compiled with `--prec-div=true`. For devices of compute capability
    1.x or for devices of compute capability 2.x when the code is compiled with `--prec-div=false`,
    the division operator and `__fdividef(x,y)` have the same accuracy, but for 2^(126)<y<2^(128),
    `__fdividef(x,y)` delivers a result of zero, whereas the division operator delivers
    the correct result. Also, for 2^(126)<y<2^(128), if x is infinity, `__fdividef(x,y)`
    returns NaN, while the division operator returns infinity.
  prefs: []
  type: TYPE_NORMAL
- en: Transcendentals (SFU)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Special Function Units (SFUs) in the SMs implement very fast versions of
    six common transcendental functions.
  prefs: []
  type: TYPE_NORMAL
- en: • Sine and cosine
  prefs: []
  type: TYPE_NORMAL
- en: • Logarithm and exponential
  prefs: []
  type: TYPE_NORMAL
- en: • Reciprocal and reciprocal square root
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8.11](ch08.html#ch08tab11), excerpted from the paper on the Tesla architecture^([12](ch08.html#ch08fn12))
    summarizes the supported operations and corresponding precision. The SFUs do not
    implement full precision, but they are reasonably good approximations of these
    functions and they are *fast*. For CUDA ports that are significantly faster than
    an optimized CPU equivalent (say, 25x or more), the code most likely relies on
    the SFUs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[12](ch08.html#ch08fn12a). Lindholm, Erik, John Nickolls, Stuart Oberman, and
    John Montrym. NVIDIA Tesla: A unified graphics and computing architecture. *IEEE
    Micro*, March–April 2008, p. 47.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.11* SFU Accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: The SFUs are accessed with the intrinsics given in [Table 8.12](ch08.html#ch08tab12).
    Specifying the `--fast-math` compiler option will cause the compiler to substitute
    conventional C runtime calls with the corresponding SFU intrinsics listed above.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.12* SFU Intrinsics'
  prefs: []
  type: TYPE_NORMAL
- en: Miscellaneous
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`__saturate(x)` returns 0 if `x<0`, 1 if `x>1`, and `x` otherwise.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.3\. Double Precision (64-Bit)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Double-precision floating-point support was added to CUDA with SM 1.3 (first
    implemented in the GeForce GTX 280), and much improved double-precision support
    (both functionality and performance) became available with SM 2.0\. CUDA’s hardware
    support for double precision features full-speed denormals and, starting in SM
    2.x, a native fused multiply-add instruction (FMAD), compliant with IEEE 754 c.
    2008, that performs only one rounding step. Besides being an intrinsically useful
    operation, FMAD enables full accuracy on certain functions that are converged
    with the Newton-Raphson iteration.
  prefs: []
  type: TYPE_NORMAL
- en: As with single-precision operations, the compiler automatically translates standard
    C operators into multiplication, addition, and multiply-add instructions. The
    `__dadd_rn()` and `__dmul_rn()` intrinsics may be used to suppress fusion of addition
    and multiplication operations into multiply-add instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.4\. Half Precision (16-Bit)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With 5 bits of exponent and 10 bits of significand, `half` values have enough
    precision for HDR (high dynamic range) images and can be used to hold other types
    of values that do not require `float` precision, such as angles. Half precision
    values are intended for storage, not computation, so the hardware only provides
    instructions to convert to/from 32-bit.^([13](ch08.html#ch08fn13)) These instructions
    are exposed as the `__halftofloat()` and `__floattohalf()` intrinsics.
  prefs: []
  type: TYPE_NORMAL
- en: '[13](ch08.html#ch08fn13a). `half` floating-point values are supported as a
    texture format, in which case the TEX intrinsics return `float` and the conversion
    is automatically performed by the texture hardware.'
  prefs: []
  type: TYPE_NORMAL
- en: float __halftofloat( unsigned short );
  prefs: []
  type: TYPE_NORMAL
- en: unsigned short __floattohalf( float );
  prefs: []
  type: TYPE_NORMAL
- en: These intrinsics use `unsigned short` because the C language has not standardized
    the `half` floating-point type.
  prefs: []
  type: TYPE_NORMAL
- en: '8.3.5\. Case Study: `float`→`half` Conversion'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Studying the `float`→`half` conversion operation is a useful way to learn the
    details of floating-point encodings and rounding. Because it’s a simple unary
    operation, we can focus on the encoding and rounding without getting distracted
    by the details of floating-point arithmetic and the precision of intermediate
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: When converting from `float` to `half`, the correct output for any `float` too
    large to represent is `half` infinity. Any float too small to represent as a `half`
    (even a denormal `half`) must be clamped to `0.0`. The maximum `float` that rounds
    to `half` 0.0 is `0x32FFFFFF`, or 2.98^(-8), while the smallest `float` that rounds
    to `half` infinity is 65520.0\. `float` values inside this range can be converted
    to `half` by propagating the sign bit, rebiasing the exponent (since float has
    an 8-bit exponent biased by 127 and half has a 5-bit exponent biased by 15), and
    rounding the `float` mantissa to the nearest `half` mantissa value. Rounding is
    straightforward in all cases except when the input value falls exactly between
    the two possible output values. When this is the case, the IEEE standard specifies
    rounding to the “nearest even” value. In decimal arithmetic, this would mean rounding
    1.5 to 2.0, but also rounding 2.5 to 2.0 and (for example) rounding 0.5 to 0.0.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 8.3](ch08.html#ch08lis03) shows a C routine that exactly replicates
    the `float`-to-`half` conversion operation, as implemented by CUDA hardware. The
    variables `exp` and `mag` contain the input exponent and “magnitude,” the mantissa
    and exponent together with the sign bit masked off. Many operations, such as comparisons
    and rounding operations, can be performed on the magnitude without separating
    the exponent and mantissa.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The macro `LG_MAKE_MASK`, used in [Listing 8.3](ch08.html#ch08lis03), creates
    a mask with a given bit count: `#define LG_MAKE_MASK(bits) ((1<<bits)-1)`. A `volatile
    union` is used to treat the same 32-bit value as `float` and `unsigned int`; idioms
    such as `*((float *) (&u))` are not portable. The routine first propagates the
    input sign bit and masks it off the input.'
  prefs: []
  type: TYPE_NORMAL
- en: After extracting the magnitude and exponent, the function deals with the special
    case when the input `float` is INF or NaN, and does an early exit. Note that INF
    is signed, but NaN has a canonical unsigned value. Lines 50–80 clamp the input
    `float` value to the minimum or maximum values that correspond to representable
    `half` values and recompute the magnitude for clamped values. Don’t be fooled
    by the elaborate code constructing `f32MinRInfin` and `f32MaxRf16_zero`; those
    are constants with the values `0x477ff000` and `0x32ffffff`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of the routine deals with the cases of output normal and denormal
    (input denormals are clamped in the preceding code, so `mag` corresponds to a
    normal `float`). As with the clamping code, `f32Minf16Normal` is a constant, and
    its value is `0x38ffffff`.
  prefs: []
  type: TYPE_NORMAL
- en: To construct a normal, the new exponent must be computed (lines 92 and 93) and
    the correctly rounded 10 bits of mantissa shifted into the output. To construct
    a denormal, the implicit 1 must be OR’d into the output mantissa and the resulting
    mantissa shifted by the amount corresponding to the input exponent. For both normals
    and denormals, the rounding of the output mantissa is accomplished in two steps.
    The rounding is accomplished by adding a mask of 1’s that ends just short of the
    output’s LSB, as seen in [Figure 8.3](ch08.html#ch08fig03).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8.3* Rounding mask (`half`).'
  prefs: []
  type: TYPE_NORMAL
- en: This operation increments the output mantissa if bit 12 of the input is set;
    if the input mantissa is all 1’s, the overflow causes the output exponent to correctly
    increment. If we added one more 1 to the MSB of this adjustment, we’d have elementary
    school–style rounding where the tiebreak goes to the larger number. Instead, to
    implement round-to-nearest even, we conditionally increment the output mantissa
    if the LSB of the 10-bit output is set ([Figure 8.4](ch08.html#ch08fig04)). Note
    that these steps can be performed in either order or can be reformulated in many
    different ways.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8.4* Round-to-nearest-even (`half`).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8.3.* `ConvertToHalf()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p08lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: /*
  prefs: []
  type: TYPE_NORMAL
- en: '* exponent shift and mantissa bit count are the same.'
  prefs: []
  type: TYPE_NORMAL
- en: '*    When we are shifting, we use [f16|f32]ExpShift'
  prefs: []
  type: TYPE_NORMAL
- en: '*    When referencing the number of bits in the mantissa,'
  prefs: []
  type: TYPE_NORMAL
- en: '*        we use [f16|f32]MantissaBits'
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  prefs: []
  type: TYPE_NORMAL
- en: const int f16ExpShift = 10;
  prefs: []
  type: TYPE_NORMAL
- en: const int f16MantissaBits = 10;
  prefs: []
  type: TYPE_NORMAL
- en: const int f16ExpBias = 15;
  prefs: []
  type: TYPE_NORMAL
- en: const int f16MinExp = -14;
  prefs: []
  type: TYPE_NORMAL
- en: const int f16MaxExp = 15;
  prefs: []
  type: TYPE_NORMAL
- en: const int f16SignMask = 0x8000;
  prefs: []
  type: TYPE_NORMAL
- en: const int f32ExpShift = 23;
  prefs: []
  type: TYPE_NORMAL
- en: const int f32MantissaBits = 23;
  prefs: []
  type: TYPE_NORMAL
- en: const int f32ExpBias = 127;
  prefs: []
  type: TYPE_NORMAL
- en: const int f32SignMask = 0x80000000;
  prefs: []
  type: TYPE_NORMAL
- en: unsigned short
  prefs: []
  type: TYPE_NORMAL
- en: ConvertFloatToHalf( float f )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: /*
  prefs: []
  type: TYPE_NORMAL
- en: '* Use a volatile union to portably coerce'
  prefs: []
  type: TYPE_NORMAL
- en: '* 32-bit float into 32-bit integer'
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  prefs: []
  type: TYPE_NORMAL
- en: volatile union {
  prefs: []
  type: TYPE_NORMAL
- en: float f;
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int u;
  prefs: []
  type: TYPE_NORMAL
- en: '} uf;'
  prefs: []
  type: TYPE_NORMAL
- en: uf.f = f;
  prefs: []
  type: TYPE_NORMAL
- en: '// return value: start by propagating the sign bit.'
  prefs: []
  type: TYPE_NORMAL
- en: unsigned short w = (uf.u >> 16) & f16SignMask;
  prefs: []
  type: TYPE_NORMAL
- en: // Extract input magnitude and exponent
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int mag = uf.u & ~f32SignMask;
  prefs: []
  type: TYPE_NORMAL
- en: int exp = (int) (mag >> f32ExpShift) - f32ExpBias;
  prefs: []
  type: TYPE_NORMAL
- en: // Handle float32 Inf or NaN
  prefs: []
  type: TYPE_NORMAL
- en: if ( exp == f32ExpBias+1 ) {    // INF or NaN
  prefs: []
  type: TYPE_NORMAL
- en: if ( mag & LG_MAKE_MASK(f32MantissaBits) )
  prefs: []
  type: TYPE_NORMAL
- en: return 0x7fff; // NaN
  prefs: []
  type: TYPE_NORMAL
- en: // INF - propagate sign
  prefs: []
  type: TYPE_NORMAL
- en: return w|0x7c00;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: /*
  prefs: []
  type: TYPE_NORMAL
- en: '* clamp float32 values that are not representable by float16'
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: // min float32 magnitude that rounds to float16 infinity
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int f32MinRInfin = (f16MaxExp+f32ExpBias) <<
  prefs: []
  type: TYPE_NORMAL
- en: f32ExpShift;
  prefs: []
  type: TYPE_NORMAL
- en: f32MinRInfin |= LG_MAKE_MASK( f16MantissaBits+1 ) <<
  prefs: []
  type: TYPE_NORMAL
- en: (f32MantissaBits-f16MantissaBits-1);
  prefs: []
  type: TYPE_NORMAL
- en: if (mag > f32MinRInfin)
  prefs: []
  type: TYPE_NORMAL
- en: mag = f32MinRInfin;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: // max float32 magnitude that rounds to float16 0.0
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int f32MaxRf16_zero = f16MinExp+f32ExpBias
  prefs: []
  type: TYPE_NORMAL
- en: (f32MantissaBits-f16MantissaBits-1);
  prefs: []
  type: TYPE_NORMAL
- en: f32MaxRf16_zero <<= f32ExpShift;
  prefs: []
  type: TYPE_NORMAL
- en: f32MaxRf16_zero |= LG_MAKE_MASK( f32MantissaBits );
  prefs: []
  type: TYPE_NORMAL
- en: if (mag < f32MaxRf16_zero)
  prefs: []
  type: TYPE_NORMAL
- en: mag = f32MaxRf16_zero;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: /*
  prefs: []
  type: TYPE_NORMAL
- en: '* compute exp again, in case mag was clamped above'
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  prefs: []
  type: TYPE_NORMAL
- en: exp = (mag >> f32ExpShift) - f32ExpBias;
  prefs: []
  type: TYPE_NORMAL
- en: // min float32 magnitude that converts to float16 normal
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int f32Minf16Normal = ((f16MinExp+f32ExpBias)<<
  prefs: []
  type: TYPE_NORMAL
- en: f32ExpShift);
  prefs: []
  type: TYPE_NORMAL
- en: f32Minf16Normal |= LG_MAKE_MASK( f32MantissaBits );
  prefs: []
  type: TYPE_NORMAL
- en: if ( mag >= f32Minf16Normal ) {
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: '// Case 1: float16 normal'
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // Modify exponent to be biased for float16, not float32
  prefs: []
  type: TYPE_NORMAL
- en: mag += (unsigned int) ((f16ExpBias-f32ExpBias)<<
  prefs: []
  type: TYPE_NORMAL
- en: f32ExpShift);
  prefs: []
  type: TYPE_NORMAL
- en: int RelativeShift = f32ExpShift-f16ExpShift;
  prefs: []
  type: TYPE_NORMAL
- en: // add rounding bias
  prefs: []
  type: TYPE_NORMAL
- en: mag += LG_MAKE_MASK(RelativeShift-1);
  prefs: []
  type: TYPE_NORMAL
- en: // round-to-nearest even
  prefs: []
  type: TYPE_NORMAL
- en: mag += (mag >> RelativeShift) & 1;
  prefs: []
  type: TYPE_NORMAL
- en: w |= mag >> RelativeShift;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else {
  prefs: []
  type: TYPE_NORMAL
- en: /*
  prefs: []
  type: TYPE_NORMAL
- en: '* Case 2: float16 denormal'
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  prefs: []
  type: TYPE_NORMAL
- en: // mask off exponent bits - now fraction only
  prefs: []
  type: TYPE_NORMAL
- en: mag &= LG_MAKE_MASK(f32MantissaBits);
  prefs: []
  type: TYPE_NORMAL
- en: // make implicit 1 explicit
  prefs: []
  type: TYPE_NORMAL
- en: mag |= (1<<f32ExpShift);
  prefs: []
  type: TYPE_NORMAL
- en: int RelativeShift = f32ExpShift-f16ExpShift+f16MinExp-exp;
  prefs: []
  type: TYPE_NORMAL
- en: // add rounding bias
  prefs: []
  type: TYPE_NORMAL
- en: mag += LG_MAKE_MASK(RelativeShift-1);
  prefs: []
  type: TYPE_NORMAL
- en: // round-to-nearest even
  prefs: []
  type: TYPE_NORMAL
- en: mag += (mag >> RelativeShift) & 1;
  prefs: []
  type: TYPE_NORMAL
- en: w |= mag >> RelativeShift;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return w;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, developers should convert `float` to `half` by using the `__floattohalf()`
    intrinsic, which the compiler translates to a single `F2F` machine instruction.
    This sample routine is provided purely to aid in understanding floating-point
    layout and rounding; also, examining all the special-case code for INF/NAN and
    denormal values helps to illustrate why these features of the IEEE spec have been
    controversial since its inception: They make hardware slower, more costly, or
    both due to increased silicon area and engineering effort for validation.'
  prefs: []
  type: TYPE_NORMAL
- en: In the code accompanying this book, the `ConvertFloatToHalf()` routine in [Listing
    8.3](ch08.html#ch08lis03) is incorporated into a program called `float_to_float16.cu`
    that tests its output for every 32-bit floating-point value.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.6\. Math Library
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'CUDA includes a built-in math library modeled on the C runtime library, with
    a few small differences: CUDA hardware does not include a rounding mode register
    (instead, the round mode is encoded on a per-instruction basis),^([14](ch08.html#ch08fn14))
    so functions such as `rint()` that reference the current rounding mode always
    round-to-nearest. Additionally, the hardware does not raise floating-point exceptions;
    results of aberrant operations, such as taking the square root of a negative number,
    are encoded as NaNs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[14](ch08.html#ch08fn14a). Encoding a round mode per instruction and keeping
    it in a control register are not irreconcilable. The Alpha processor had a 2-bit
    encoding to specify the round mode per instruction, one setting of which was to
    use the rounding mode specified in a control register! CUDA hardware just uses
    a 2-bit encoding for the four round modes specified in the IEEE specification.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8.13](ch08.html#ch08tab13) lists the math library functions and the
    maximum error in ulps for each function. Most functions that operate on `float`
    have an “f” appended to the function name—for example, the functions that compute
    the sine function are as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: double sin( double angle );
  prefs: []
  type: TYPE_NORMAL
- en: float sinf( float angle );
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab13.jpg)![Image](graphics/08tab13a.jpg)![Image](graphics/08tab13b.jpg)![Image](graphics/08tab13c.jpg)![Image](graphics/08tab13d.jpg)![Image](graphics/08tab13e.jpg)![Image](graphics/08tab13f.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.13* Math Library'
  prefs: []
  type: TYPE_NORMAL
- en: These are denoted in [Table 8.13](ch08.html#ch08tab13) as, for example, `sin[f]`.
  prefs: []
  type: TYPE_NORMAL
- en: Conversion to Integer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: According to the C runtime library definition, the `nearbyint()` and `rint()`
    functions round a floating-point value to the nearest integer using the “current
    rounding direction,” which in CUDA is always round-to-nearest-even. In the C runtime,
    `nearbyint()` and `rint()` differ only in their handling of the INEXACT exception.
    But since CUDA does not raise floating-point exceptions, the functions behave
    identically.
  prefs: []
  type: TYPE_NORMAL
- en: '`round()` implements elementary school–style rounding: For floating-point values
    halfway between integers, the input is always rounded away from zero. NVIDIA recommends
    against using this function because it expands to eight (8) instructions as opposed
    to one for `rint()` and its variants. `trunc()` truncates or “chops” the floating-point
    value, rounding toward zero. It compiles to a single instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: Fractions and Exponents
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: float frexpf(float x, int *eptr);
  prefs: []
  type: TYPE_NORMAL
- en: '`frexpf()` breaks the input into a floating-point significand in the range
    [0.5, 1.0) and an integral exponent for 2, such that'
  prefs: []
  type: TYPE_NORMAL
- en: '*x = Significand* · 2^(*Exponent*)'
  prefs: []
  type: TYPE_NORMAL
- en: float logbf( float x );
  prefs: []
  type: TYPE_NORMAL
- en: '`logbf()` extracts the exponent from x and returns it as a floating-point value.
    It is equivalent to `floorf(log2f(x))`, except it is faster. If `x` is a denormal,
    `logbf()` returns the exponent that x would have if it were normalized.'
  prefs: []
  type: TYPE_NORMAL
- en: float ldexpf( float x, int exp );
  prefs: []
  type: TYPE_NORMAL
- en: float scalbnf( float x, int n );
  prefs: []
  type: TYPE_NORMAL
- en: float scanblnf( float x, long n );
  prefs: []
  type: TYPE_NORMAL
- en: '`ldexpf()`, `scalbnf()`, and `scalblnf()` all compute x2^n by direct manipulation
    of floating-point exponents.'
  prefs: []
  type: TYPE_NORMAL
- en: Floating-Point Remainder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`modff()` breaks the input into fractional and integer parts.'
  prefs: []
  type: TYPE_NORMAL
- en: float modff( float x, float *intpart );
  prefs: []
  type: TYPE_NORMAL
- en: The return value is the fractional part of x, with the same sign.
  prefs: []
  type: TYPE_NORMAL
- en: '`remainderf(x,y)` computes the floating-point remainder of dividing x by y.
    The return value is `x-n*y`, where `n` is x/y, rounded to the nearest integer.
    If |x –ny| = 0.5, `n` is chosen to be even.'
  prefs: []
  type: TYPE_NORMAL
- en: float remquof(float x, float y, int *quo);
  prefs: []
  type: TYPE_NORMAL
- en: computes the remainder and passes back the lower bits of the integral quotient
    x/y, with the same sign as x/y.
  prefs: []
  type: TYPE_NORMAL
- en: Bessel Functions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Bessel functions of order *n* relate to the differential equation
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/265equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*n* can be a real number, but for purposes of the C runtime, it is a nonnegative
    integer.'
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this second-order ordinary differential equation combines Bessel
    functions of the first kind and of the second kind.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/265equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The math runtime functions `jn[f]()` and `yn[f]()` compute J[n](x) and Y[n](x),
    respectively. `j0f()`, `j1f()`, `y0f()`, and `y1f()` compute these functions for
    the special cases of n=0 and n=1.
  prefs: []
  type: TYPE_NORMAL
- en: Gamma Function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The gamma function Γ is an extension of the factorial function, with its argument
    shifted down by 1, to real numbers. It has a variety of definitions, one of which
    is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/265equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The function grows so quickly that the return value loses precision for relatively
    small input values, so the library provides the `lgamma()` function, which returns
    the natural logarithm of the gamma function, in addition to the `tgamma()` (“true
    gamma”) function.
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.7\. Additional Reading
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Goldberg’s survey (with the captivating title “What Every Computer Scientist
    Should Know About Floating Point Arithmetic”) is a good introduction to the topic.
  prefs: []
  type: TYPE_NORMAL
- en: '[http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html](http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Nathan Whitehead and Alex Fit-Florea of NVIDIA have coauthored a white paper
    entitled “Precision & Performance: Floating Point and IEEE 754 Compliance for
    NVIDIA GPUs.”'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf](http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing Effective Precision
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Dekker and Kahan developed methods to almost double the effective precision
    of floating-point hardware using pairs of numbers in exchange for a slight reduction
    in exponent range (due to intermediate underflow and overflow at the far ends
    of the range). Some papers on this topic include the following.
  prefs: []
  type: TYPE_NORMAL
- en: Dekker, T.J. Point technique for extending the available precision. *Numer.
    Math.* 18, 1971, pp. 224–242.
  prefs: []
  type: TYPE_NORMAL
- en: Linnainmaa, S. Software for doubled-precision floating point computations. *ACM
    TOMS* 7, pp. 172–283 (1981).
  prefs: []
  type: TYPE_NORMAL
- en: Shewchuk, J.R. Adaptive precision floating-point arithmetic and fast robust
    geometric predicates. *Discrete & Computational Geometry* 18, 1997, pp. 305–363.
  prefs: []
  type: TYPE_NORMAL
- en: Some GPU-specific work on this topic has been done by Andrew Thall, Da Graça,
    and Defour.
  prefs: []
  type: TYPE_NORMAL
- en: Guillaume, Da Graça, and David Defour. Implementation of float-float operators
    on graphics hardware, *7th Conference on Real Numbers and Computers*, RNC7 (2006).
  prefs: []
  type: TYPE_NORMAL
- en: '[http://hal.archives-ouvertes.fr/docs/00/06/33/56/PDF/float-float.pdf](http://hal.archives-ouvertes.fr/docs/00/06/33/56/PDF/float-float.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Thall, Andrew. Extended-precision floating-point numbers for GPU computation.
    2007.
  prefs: []
  type: TYPE_NORMAL
- en: '[http://andrewthall.org/papers/df64_qf128.pdf](http://andrewthall.org/papers/df64_qf128.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 8.4\. Conditional Code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hardware implements “condition code” or CC registers that contain the usual
    4-bit state vector (sign, carry, zero, overflow) used for integer comparison.
    These CC registers can be set using comparison instructions such as `ISET`, and
    they can direct the flow of execution via *predication* or *divergence*. Predication
    allows (or suppresses) the execution of instructions on a per-thread basis within
    a warp, while divergence is the conditional execution of longer instruction sequences.
    Because the processors within an SM execute instructions in SIMD fashion at warp
    granularity (32 threads at a time), divergence can result in fewer instructions
    executed, provided all threads within a warp take the same code path.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1\. Predication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the additional overhead of managing divergence and convergence, the compiler
    uses predication for short instruction sequences. The effect of most instructions
    can be predicated on a condition; if the condition is not TRUE, the instruction
    is suppressed. This suppression occurs early enough that predicated execution
    of instructions such as load/store and `TEX` inhibits the memory traffic that
    the instruction would otherwise generate. Note that predication has no effect
    on the eligibility of memory traffic for global load/store coalescing. The addresses
    specified to all load/store instructions in a warp must reference consecutive
    memory locations, even if they are predicated.
  prefs: []
  type: TYPE_NORMAL
- en: Predication is used when the number of instructions that vary depending on a
    condition is small; the compiler uses heuristics that favor predication up to
    about 7 instructions. Besides avoiding the overhead of managing the branch synchronization
    stack described below, predication also gives the compiler more optimization opportunities
    (such as instruction scheduling) when emitting microcode. The ternary operator
    in C (`? :`) is considered a compiler hint to favor predication.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 8.2](ch08.html#ch08lis02) gives an excellent example of predication,
    as expressed in microcode. When performing an atomic operation on a shared memory
    location, the compiler emits code that loops over the shared memory location until
    it has successfully performed the atomic operation. The `LDSLK` (load shared and
    lock) instruction returns a condition code that tells whether the lock was acquired.
    The instructions to perform the operation then are predicated on that condition
    code.'
  prefs: []
  type: TYPE_NORMAL
- en: /*0058*/ LDSLK P0, R2, [R3];
  prefs: []
  type: TYPE_NORMAL
- en: /*0060*/ @P0 IADD R2, R2, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0068*/ @P0 STSUL [R3], R2;
  prefs: []
  type: TYPE_NORMAL
- en: /*0070*/ @!P0 BRA 0x58;
  prefs: []
  type: TYPE_NORMAL
- en: This code fragment also highlights how predication and branching sometimes work
    together. The last instruction, a conditional branch to attempt to reacquire the
    lock if necessary, also is predicated.
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2\. Divergence and Convergence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Predication works well for small fragments of conditional code, especially `if`
    statements with no corresponding `else`. For larger amounts of conditional code,
    predication becomes inefficient because every instruction is executed, regardless
    of whether it will affect the computation. When the larger number of instructions
    causes the costs of predication to exceed the benefits, the compiler will use
    conditional branches. When the flow of execution within a warp takes different
    paths depending on a condition, the code is called *divergent*.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA is close-mouthed about the details of how their hardware supports divergent
    code paths, and it reserves the right to change the hardware implementation between
    generations. The hardware maintains a bit vector of active threads within each
    warp. For threads that are marked inactive, execution is suppressed in a way similar
    to predication. Before taking a branch, the compiler executes a special instruction
    to push this active-thread bit vector onto a stack. The code is then executed
    *twice*, once for threads for which the condition was TRUE, then for threads for
    which the predicate was FALSE. This two-phased execution is managed with a *branch
    synchronization stack*, as described by Lindholm et al.^([15](ch08.html#ch08fn15))
  prefs: []
  type: TYPE_NORMAL
- en: '[15](ch08.html#ch08fn15a). Lindholm, Erik, John Nickolls, Stuart Oberman, and
    John Montrym. NVIDIA Tesla: A unified graphics and computing architecture. *IEEE
    Micro*, March–April 2008, pp. 39–55.'
  prefs: []
  type: TYPE_NORMAL
- en: If threads of a warp diverge via a data-dependent conditional branch, the warp
    serially executes each branch path taken, disabling threads that are not on that
    path, and when all paths complete, the threads reconverge to the original execution
    path. The SM uses a branch synchronization stack to manage independent threads
    that diverge and converge. Branch divergence only occurs within a warp; different
    warps execute independently regardless of whether they are executing common or
    disjoint code paths.
  prefs: []
  type: TYPE_NORMAL
- en: The PTX specification makes no mention of a branch synchronization stack, so
    the only publicly available evidence of its existence is in the disassembly output
    of `cuobjdump`. The `SSY` instruction pushes a state such as the program counter
    and active thread mask onto the stack; the `.S` instruction prefix pops this state
    and, if any active threads did not take the branch, causes those threads to execute
    the code path whose state was snapshotted by `SSY`.
  prefs: []
  type: TYPE_NORMAL
- en: '`SSY/.S` is only necessary when threads of execution may diverge, so if the
    compiler can guarantee that threads will stay uniform in a code path, you may
    see branches that are not bracketed by `SSY/.S`. The important thing to realize
    about branching in CUDA is that in all cases, it is most efficient for all threads
    within a warp to follow the same execution path.'
  prefs: []
  type: TYPE_NORMAL
- en: The loop in [Listing 8.2](ch08.html#ch08lis02) also includes a good self-contained
    example of divergence and convergence. The `SSY` instruction (offset 0x40) and
    `NOP.S` instruction (offset 0x78) bracket the points of divergence and convergence,
    respectively. The code loops over the `LDSLK` and subsequent predicated instructions,
    retiring active threads until the compiler knows that all threads will have converged
    and the branch synchronization stack can be popped with the `NOP.S` instruction.
  prefs: []
  type: TYPE_NORMAL
- en: /*0040*/ SSY 0x80;
  prefs: []
  type: TYPE_NORMAL
- en: /*0048*/ BAR.RED.POPC RZ, RZ;
  prefs: []
  type: TYPE_NORMAL
- en: /*0050*/ LD R0, [R0];
  prefs: []
  type: TYPE_NORMAL
- en: /*0058*/ LDSLK P0, R2, [R3];
  prefs: []
  type: TYPE_NORMAL
- en: /*0060*/ @P0 IADD R2, R2, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0068*/ @P0 STSUL [R3], R2;
  prefs: []
  type: TYPE_NORMAL
- en: /*0070*/ @!P0 BRA 0x58;
  prefs: []
  type: TYPE_NORMAL
- en: /*0078*/ NOP.S CC.T;
  prefs: []
  type: TYPE_NORMAL
- en: '8.4.3\. Special Cases: Min, Max, and Absolute Value'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some conditional operations are so common that they are supported natively by
    the hardware. Minimum and maximum operations are supported for both integer and
    floating-point operands and are translated to a single instruction. Additionally,
    floating-point instructions include modifiers that can negate or take the absolute
    value of a source operand.
  prefs: []
  type: TYPE_NORMAL
- en: The compiler does a good job of detecting when min/max operations are being
    expressed, but if you want to take no chances, call the `min()/max()` intrinsics
    for integers or `fmin()/fmax()` for floating-point values.
  prefs: []
  type: TYPE_NORMAL
- en: 8.5\. Textures and Surfaces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The instructions that read and write textures and surfaces refer to much more
    implicit state than do other instructions; parameters such as the base address,
    dimensions, format, and interpretation of the texture contents are contained in
    a *header*, an intermediate data structure whose software abstraction is called
    a *texture reference* or *surface reference*. As developers manipulate the texture
    or surface references, the CUDA runtime and driver must translate those changes
    into the headers, which the texture or surface instruction references as an index.^([16](ch08.html#ch08fn16))
  prefs: []
  type: TYPE_NORMAL
- en: '[16](ch08.html#ch08fn16a). SM 3.x added *texture objects*, which enable texture
    and surface headers to be referenced by address rather than an index. Previous
    hardware generations could reference at most 128 textures or surfaces in a kernel,
    but with SM 3.x the number is limited only by memory.'
  prefs: []
  type: TYPE_NORMAL
- en: Before launching a kernel that operates on textures or surfaces, the driver
    must ensure that all this state is set correctly on the hardware. As a result,
    launching such kernels may take longer. Texture reads are serviced through a specialized
    cache subsystem that is separate from the L1/L2 caches in Fermi, and also separate
    from the constant cache. Each SM has an L1 texture cache, and the TPCs (texture
    processor clusters) or GPCs (graphics processor clusters) each additionally have
    L2 texture cache. Surface reads and writes are serviced through the same L1/L2
    caches that service global memory traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kepler added two technologies of note with respect to textures: the ability
    to read from global memory via the texture cache hierarchy without binding a texture
    reference, and the ability to specify a texture header by address rather than
    by index. The latter technology is known as “bindless textures.”'
  prefs: []
  type: TYPE_NORMAL
- en: On SM 3.5 and later hardware, reading global memory via the texture cache can
    be requested by using `const __restrict` pointers or by explicitly invoking the
    `ldg()` intrinsics in `sm_35_intrinsics.h`.
  prefs: []
  type: TYPE_NORMAL
- en: 8.6\. Miscellaneous Instructions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 8.6.1\. Warp-Level Primitives
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It did not take long for the importance of warps as a primitive unit of execution
    (naturally residing between threads and blocks) to become evident to CUDA programmers.
    Starting with SM 1.x, NVIDIA began adding instructions that specifically operate
    on warps.
  prefs: []
  type: TYPE_NORMAL
- en: Vote
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: That CUDA architectures are 32-bit and that warps are comprised of 32 threads
    made an irresistible match to instructions that can evaluate a condition and broadcast
    a 1-bit result to every thread in the warp. The `VOTE` instruction (first available
    in SM 1.2) evaluates a condition and broadcasts the result to all threads in the
    warp. The `__any()` intrinsic returns 1 if the predicate is true for *any* of
    the 32 threads in the warp. The `__all()` intrinsic returns 1 if the predicate
    is true for *all* of the 32 threads in the warp.
  prefs: []
  type: TYPE_NORMAL
- en: The Fermi architecture added a new variant of `VOTE` that passes back the predicate
    result for every thread in the warp. The `__ballot()` intrinsic evaluates a condition
    for all threads in the warp and returns a 32-bit value where each bit gives the
    condition for the corresponding thread in the warp.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Kepler added *shuffle* instructions that enable data interchange between threads
    within a warp without staging the data through shared memory. Although these instructions
    execute with the same latency as shared memory, they have the benefit of doing
    the exchange without performing both a read and a write, and they can reduce shared
    memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: The following instruction is wrapped in a number of device functions that use
    inline PTX assembly defined in `sm_30_intrinsics.h.`
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p271pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: int __shfl(int var, int srcLane, int width=32);
  prefs: []
  type: TYPE_NORMAL
- en: int __shfl_up(int var, unsigned int delta, int width=32);
  prefs: []
  type: TYPE_NORMAL
- en: int __shfl_down(int var, unsigned int delta, int width=32);
  prefs: []
  type: TYPE_NORMAL
- en: int __shfl_xor(int var, int laneMask, int width=32);
  prefs: []
  type: TYPE_NORMAL
- en: The `width` parameter, which defaults to the warp width of 32, must be a power
    of 2 in the range 2..32\. It enables subdivision of the warp into segments; if
    `width<32`, each subsection of the warp behaves as a separate entity with a starting
    logical lane ID of 0\. A thread may only exchange data with other threads in its
    subsection.
  prefs: []
  type: TYPE_NORMAL
- en: '`__shfl`() returns the value of `var` held by the thread whose ID is given
    by `srcLane`. If `srcLane` is outside the range `0..width-1`, the thread’s own
    value of `var` is returned. This variant of the instruction can be used to broadcast
    values within a warp. `__shfl_up()` calculates a source lane ID by subtracting
    `delta` from the caller’s lane ID and clamping to the range `0..width-1`. `__shfl_down()`
    calculates a source lane ID by adding `delta` to the caller’s lane ID.'
  prefs: []
  type: TYPE_NORMAL
- en: '`__shfl_up()` and `__shfl_down()` enable warp-level scan and reverse scan operations,
    respectively. `__shfl_xor()` calculates a source lane ID by performing a bitwise
    XOR of the caller’s lane ID with `laneMask`; the value of `var` held by the resulting
    lane ID is returned. This variant can be used to do a reduction across the warps
    (or subwarps); each thread computes the reduction using a differently ordered
    series of the associative operator.'
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.2\. Block-Level Primitives
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `__syncthreads()` intrinsic serves as a barrier. It causes all threads to
    wait until every thread in the threadblock has arrived at the `__syncthreads()`.
    The Fermi instruction set (SM 2.x) added several new block-level barriers that
    aggregate information about the threads in the threadblock.
  prefs: []
  type: TYPE_NORMAL
- en: '• `__syncthreads_count()`: evaluates a predicate and returns the sum of threads
    for which the predicate was true'
  prefs: []
  type: TYPE_NORMAL
- en: '• `__syncthreads_or()`: returns the OR of all the inputs across the threadblock'
  prefs: []
  type: TYPE_NORMAL
- en: '• `__syncthreads_and()`: returns the AND of all the inputs across the threadblock'
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.3\. Performance Counter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Developers can define their own set of performance counters and increment them
    in live code with the `__prof_trigger()` intrinsic.
  prefs: []
  type: TYPE_NORMAL
- en: void __prof_trigger(int counter);
  prefs: []
  type: TYPE_NORMAL
- en: Calling this function increments the corresponding counter by 1 per warp. `counter`
    must be in the range 0..7; counters 8..15 are reserved. The value of the counters
    may be obtained by listing `prof_trigger_00..prof_trigger_07` in the profiler
    configuration file.
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.4\. Video Instructions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The video instructions described in this section are accessible only via the
    inline PTX assembler. Their basic functionality is described here to help developers
    to decide whether they might be beneficial for their application. Anyone intending
    to use these instructions, however, should consult the PTX ISA specification.
  prefs: []
  type: TYPE_NORMAL
- en: Scalar Video Instructions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The scalar video instructions, added with SM 2.0 hardware, enable efficient
    operations on the short (8- and 16-bit) integer types needed for video processing.
    As described in the PTX 3.1 ISA Specification, the format of these instructions
    is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p273pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: vop.dtype.atype.btype{.sat} d, a{.asel}, b{.bsel};
  prefs: []
  type: TYPE_NORMAL
- en: vop.dtype.atype.btype{.sat}.secop d, a{.asel}, b{.bsel}, c;
  prefs: []
  type: TYPE_NORMAL
- en: 'The source and destination operands are all 32-bit registers. `dtype`, `atype`,
    and `btype` may be `.u32` or `.s32` for unsigned and signed 32-bit integers, respectively.
    The `asel/bsel` specifiers select which 8- or 16-bit value to extract from the
    source operands: `b0`, `b1`, `b2`, and `b3` select bytes (numbering from the least
    significant), and `h0/h1` select the least significant and most significant 16
    bits, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the input values are extracted, they are sign- or zero-extended internally
    to signed 33-bit integers, and the primary operation is performed, producing a
    34-bit intermediate result whose sign depends on `dtype`. Finally, the result
    is clamped to the output range, and one of the following operations is performed.
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** Apply a second operation (add, min or max) to the intermediate result
    and a third operand.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** Truncate the intermediate result to an 8- or 16-bit value and merge
    into a specified position in the third operand to produce the final result.'
  prefs: []
  type: TYPE_NORMAL
- en: The lower 32 bits are then written to the destination operand.
  prefs: []
  type: TYPE_NORMAL
- en: The `vset` instruction performs a comparison between the 8-, 16-, or 32-bit
    input operands and generates the corresponding predicate (1 or 0) as output. The
    PTX scalar video instructions and the corresponding operations are given in [Table
    8.14](ch08.html#ch08tab14).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.14* Scalar Video Instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: Vector Video Instructions (SM 3.0 only)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These instructions, added with SM 3.0, are similar to the scalar video instructions
    in that they promote the inputs to a canonical integer format, perform the core
    operation, and then clamp and optionally merge the output. But they deliver higher
    performance by operating on pairs of 16-bit values or quads of 8-bit values.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8.15](ch08.html#ch08tab15) summarizes the PTX instructions and corresponding
    operations implemented by these instructions. They are most useful for video processing
    and certain image processing operations (such as the median filter).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.15* Vector Video Instructions'
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.5\. Special Registers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many special registers are accessed by referencing the built-in variables `threadIdx`,
    `blockIdx`, `blockDim`, and `gridDim`. These pseudo-variables, described in detail
    in [Section 7.3](ch07.html#ch07lev1sec3), are 3-dimensional structures that specify
    the thread ID, block ID, thread count, and block count, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Besides those, another special register is the SM’s clock register, which increments
    with each clock cycle. This counter can be read with the `__clock()` or `__clock64()`
    intrinsic. The counters are separately tracked for each SM and, like the time
    stamp counters on CPUs, are most useful for measuring relative performance of
    different code sequences and best avoided when trying to calculate wall clock
    times.
  prefs: []
  type: TYPE_NORMAL
- en: 8.7\. Instruction Sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NVIDIA has developed three major architectures: Tesla (SM 1.x), Fermi (SM 2.x),
    and Kepler (SM 3.x). Within those families, new instructions have been added as
    NVIDIA updated their products. For example, global atomic operations were not
    present in the very first Tesla-class processor (the G80, which shipped in 2006
    as the GeForce GTX 8800), but all subsequent Tesla-class GPUs included them. So
    when querying the SM version via `cuDeviceComputeCapability()`, the major and
    minor versions will be 1.0 for G80 and 1.1 (or greater) for all other Tesla-class
    GPUs. Conversely, if the SM version is 1.1 or greater, the application can use
    global atomics.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8.16](ch08.html#ch08tab16) gives the SASS instructions that may be printed
    by `cuobjdump` when disassembling microcode for Tesla-class (SM 1.x) hardware.
    The Fermi and Kepler instruction sets closely resemble each other, with the exception
    of the instructions that support surface load/store, so their instruction sets
    are given together in [Table 8.17](ch08.html#ch08tab17). In both tables, the middle
    column specifies the first SM version to support a given instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab16.jpg)![Image](graphics/08tab16a.jpg)![Image](graphics/08tab16b.jpg)![Image](graphics/08tab16c.jpg)![Image](graphics/08tab16d.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.16.* SM 1.x Instruction Set'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab17.jpg)![Image](graphics/08tab17a.jpg)![Image](graphics/08tab17b.jpg)![Image](graphics/08tab17c.jpg)![Image](graphics/08tab17d.jpg)![Image](graphics/08tab17e.jpg)![Image](graphics/08tab17f.jpg)![Image](graphics/08tab17g.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.17.* SM 2.x and SM 3.x Instruction Sets'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 9\. Multiple GPUs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter describes CUDA’s facilities for multi-GPU programming, including
    threading models, peer-to-peer, and inter-GPU synchronization. As an example,
    we’ll first explore inter-GPU synchronization using CUDA streams and events by
    implementing a peer-to-peer memcpy that stages through portable pinned memory.
    We then discuss how to implement the N-body problem (fully described in [Chapter
    14](ch14.html#ch14)) with single- and multithreaded implementations that use multiple
    GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1\. Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Systems with multiple GPUs generally contain multi-GPU boards with a PCI Express
    bridge chip (such as the GeForce GTX 690) or multiple PCI Express slots, or both,
    as described in [Section 2.3](ch02.html#ch02lev1sec3). Each GPU in such a system
    is separated by PCI Express bandwidth, so there is always a huge disparity in
    bandwidth between memory connected directly to a GPU (its device memory) and its
    connections to other GPUs as well as the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Many CUDA features designed to run on multiple GPUs, such as peer-to-peer addressing,
    require the GPUs to be identical. For applications that can make assumptions about
    the target hardware (such as vertical applications built for specific hardware
    configurations), this requirement is innocuous enough. But applications targeting
    systems with a variety of GPUs (say, a low-power one for everyday use and a powerful
    one for gaming) may have to use heuristics to decide which GPU(s) to use or load-balance
    the workload across GPUs so the faster ones contribute more computation to the
    final output, commensurate with their higher performance.
  prefs: []
  type: TYPE_NORMAL
- en: A key ingredient to all CUDA applications that use multiple GPUs is *portable
    pinned memory*. As described in [Section 5.1.2](ch05.html#ch05lev2sec2), portable
    pinned memory is pinned memory that is mapped for all CUDA contexts such that
    any GPU can read or write the memory directly.
  prefs: []
  type: TYPE_NORMAL
- en: CPU Threading Models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Until CUDA 4.0, the only way to drive multiple GPUs was to create a CPU thread
    for each one. The `cudaSetDevice()` function had to be called once per CPU thread,
    before any CUDA code had executed, in order to tell CUDA which device to initialize
    when the CPU thread started to operate on CUDA. Whichever CPU thread made that
    call would then get exclusive access to the GPU, because the CUDA driver had not
    yet been made thread-safe in a way that would enable multiple threads to access
    the same GPU at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In CUDA 4.0, `cudaSetDevice()` was modified to implement the semantics that
    everyone had previously expected: It tells CUDA which GPU should perform subsequent
    CUDA operations. Having multiple threads operating on the same GPU at the same
    time may incur a slight performance hit, but it should be expected to work. Our
    example N-body application, however, only has one CPU thread operating on any
    given device at a time. The multithreaded formulation has each of *N* threads
    operate on a specific device, and the single-threaded formulation has one thread
    operate on each of the *N* devices in turn.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2\. Peer-to-Peer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When multiple GPUs are used by a CUDA program, they are known as “peers” because
    the application generally treats them equally, as if they were coworkers collaborating
    on a project. CUDA enables two flavors of peer-to-peer: explicit memcpy and peer-to-peer
    addressing.^([1](ch09.html#ch09fn1))'
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch09.html#ch09fn1a). For peer-to-peer addressing, the term *peer* also
    harkens to the requirement that the GPUs be identical.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1\. Peer-to-Peer Memcpy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Memory copies can be performed between the memories of any two different devices.
    When UVA (Unified Virtual Addressing) is in effect, the ordinary family of memcpy
    function can be used for peer-to-peer memcpy, since CUDA can infer which device
    “owns” which memory. If UVA is not in effect, the peer-to-peer memcpy must be
    done explicitly using `cudaMemcpyPeer()`, `cudaMemcpyPeerAsync()`, `cudaMemcpy3DPeer()`,
    or `cudaMemcpy3DPeerAsync()`.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: CUDA can copy memory between any two devices, not just devices that can directly
    address one another’s memory. If necessary, CUDA will stage the memory copy through
    host memory, which can be accessed by any device in the system.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Peer-to-peer memcpy operations do not run concurrently with any other operation.
    Any pending operations on either GPU must complete before the peer-to-peer memcpy
    can begin, and no subsequent operations can start to execute until after the peer-to-peer
    memcpy is done. When possible, CUDA will use direct peer-to-peer mappings between
    the two pointers. The resulting copies are faster and do not have to be staged
    through host memory.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2\. Peer-to-Peer Addressing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Peer-to-peer mappings of device memory, shown in [Figure 2.20](ch02.html#ch02fig20),
    enable a kernel running on one GPU to read or write memory that resides in another
    GPU. Since the GPUs can only use peer-to-peer to read or write data at PCI Express
    rates, developers have to partition the workload in such a way that
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** Each GPU has about an equal amount of work to do.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** The GPUs only need to interchange modest amounts of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of such systems might be a pipelined computer vision system where each
    stage in the pipeline of GPUs computes an intermediate data structure (e.g., locations
    of identified features) that needs to be further analyzed by the next GPU in the
    pipeline or a large so-called “stencil” computation in which separate GPUs can
    perform most of the computation independently but must exchange edge data between
    computation steps.
  prefs: []
  type: TYPE_NORMAL
- en: In order for peer-to-peer addressing to work, the following conditions apply.
  prefs: []
  type: TYPE_NORMAL
- en: • Unified virtual addressing (UVA) must be in effect.
  prefs: []
  type: TYPE_NORMAL
- en: • Both GPUs must be SM 2.x or higher and must be based on the same chip.
  prefs: []
  type: TYPE_NORMAL
- en: • The GPUs must be on the same I/O hub.
  prefs: []
  type: TYPE_NORMAL
- en: '`cu(da)DeviceCanAccessPeer ()` may be called to query whether the current device
    can map another device’s memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p290pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaDeviceCanAccessPeer(int *canAccessPeer, int device,
  prefs: []
  type: TYPE_NORMAL
- en: int peerDevice);
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuDeviceCanAccessPeer(int *canAccessPeer, CUdevice device,
  prefs: []
  type: TYPE_NORMAL
- en: CUdevice peerDevice);
  prefs: []
  type: TYPE_NORMAL
- en: Peer-to-peer mappings are not enabled automatically; they must be specifically
    requested by calling `cudaDeviceEnablePeerAccess()` or `cuCtxEnablePeerAccess()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p290pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaDeviceEnablePeerAccess(int peerDevice, unsigned int
  prefs: []
  type: TYPE_NORMAL
- en: flags);
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuCtxEnablePeerAccess(CUcontext peerContext, unsigned int
  prefs: []
  type: TYPE_NORMAL
- en: Flags);
  prefs: []
  type: TYPE_NORMAL
- en: Once peer-to-peer access has been enabled, all memory in the peer device—including
    new allocations—is accessible to the current device until `cudaDeviceDisablePeerAccess()`
    or `cuCtxDisablePeerAccess()` is called.
  prefs: []
  type: TYPE_NORMAL
- en: Peer-to-peer access uses a small amount of extra memory (to hold more page tables)
    and makes memory allocation more expensive, since the memory must be mapped for
    all participating devices. Peer-to-peer functionality enables contexts to read
    and write memory belonging to other contexts, both via memcpy (which may be implemented
    by staging through system memory) and directly by having kernels read or write
    global memory pointers.
  prefs: []
  type: TYPE_NORMAL
- en: The `cudaDeviceEnablePeerAccess()` function maps the memory belonging to another
    device. Peer-to-peer memory addressing is asymmetric; it is possible for GPU A
    to map GPU B’s allocations without its allocations being available to GPU B. In
    order for two GPUs to see each other’s memory, each GPU must explicitly map the
    other’s memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p290pro03a)'
  prefs: []
  type: TYPE_NORMAL
- en: // tell device 1 to map device 0 memory
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( 1 );
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceEnablePeerAccess( 0, cudaPeerAccessDefault );
  prefs: []
  type: TYPE_NORMAL
- en: // tell device 0 to map device 1 memory
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( 0 );
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceEnablePeerAccess( 1, cudaPeerAccessDefault );
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: On GPU boards with PCI Express 3.0–capable bridge chips (such as the Tesla K10),
    the GPUs can communicate at PCI Express 3.0 speeds even if the board is plugged
    into a PCI Express 2.0 slot.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '9.3\. UVA: Inferring Device from Address'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since UVA is always enabled on peer-to-peer-capable systems, the address ranges
    for different devices do not overlap, and the driver can infer the owning device
    from a pointer value. The `cuPointerGetAttribute()` function may be used to query
    information about UVA pointers, including the owning context.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p291pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuPointerGetAttribute(void *data, CUpointer_
  prefs: []
  type: TYPE_NORMAL
- en: attribute attribute, CUdeviceptr ptr);
  prefs: []
  type: TYPE_NORMAL
- en: '`cuPointerGetAttribute()` or `cudaPointerGetAttributes()` may be used to query
    the attributes of a pointer. [Table 9.1](ch09.html#ch09tab01) gives the values
    that can be passed into `cuPointerGetAttribute()`; the structure passed back by
    `cudaPointerGetAttributes()` is as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p291pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPointerAttributes {
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaMemoryType memoryType;
  prefs: []
  type: TYPE_NORMAL
- en: int device;
  prefs: []
  type: TYPE_NORMAL
- en: void *devicePointer;
  prefs: []
  type: TYPE_NORMAL
- en: void *hostPointer;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/09tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 9.1* `cuPointerGetAttribute()` Attributes'
  prefs: []
  type: TYPE_NORMAL
- en: '`memoryType` may be `cudaMemoryTypeHost` or `cudaMemoryTypeDevice`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`device` is the device for which the pointer was allocated. For device memory,
    `device` identifies the device where the memory corresponding to `ptr` was allocated.
    For host memory, `device` identifies the device that was current when the allocation
    was performed.'
  prefs: []
  type: TYPE_NORMAL
- en: '`devicePointer` gives the device pointer value that may be used to reference
    `ptr` from the current device. If `ptr` cannot be accessed by the current device,
    `devicePointer` is `NULL`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`hostPointer` gives the host pointer value that may be used to reference `ptr`
    from the CPU. If `ptr` cannot be accessed by the current host, `hostPointer` is
    `NULL`.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.4\. Inter-GPU Synchronization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CUDA events may be used for inter-GPU synchronization using `cu(da)StreamWaitEvent()`.
    If there is a producer/consumer relationship between two GPUs, the application
    can have the producer GPU record an event and then have the consumer GPU insert
    a stream-wait on that event into its command stream. When the consumer GPU encounters
    the stream-wait, it will stop processing commands until the producer GPU has passed
    the point of execution where `cu(da)EventRecord()` was called.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In CUDA 5.0, the device runtime, described in [Section 7.5](ch07.html#ch07lev1sec5),
    does not enable any inter-GPU synchronization whatsoever. That limitation may
    be relaxed in a future release.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 9.1](ch09.html#ch09lis01) gives `chMemcpyPeerToPeer()`,^([2](ch09.html#ch09fn2))
    an implementation of peer-to-peer memcpy that uses portable memory and inter-GPU
    synchronization to implement the same type of memcpy that CUDA uses under the
    covers, if no direct mapping between the GPUs exists. The function works similarly
    to the `chMemcpyHtoD()` function in [Listing 6.2](ch06.html#ch06lis02) that performs
    host→device memcpy: A staging buffer is allocated in host memory, and the memcpy
    begins by having the source GPU copy source data into the staging buffer and recording
    an event. But unlike the host→device memcpy, there is never any need for the CPU
    to synchronize because all synchronization is done by the GPUs. Because both the
    memcpy and the event-record are asynchronous, immediately after kicking off the
    initial memcpy and event-record, the CPU can request that the destination GPU
    wait on that event and kick off a memcpy of the same buffer. Two staging buffers
    and two CUDA events are needed, so the two GPUs can copy to and from staging buffers
    concurrently, much as the CPU and GPU concurrently operate on staging buffers
    during the host→device memcpy. The CPU loops over the input buffer and output
    buffers, issuing memcpy and event-record commands and ping-ponging between staging
    buffers, until it has requested copies for all bytes and all that’s left to do
    is wait for both GPUs to finish processing.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch09.html#ch09fn2a). The `CUDART_CHECK` error handling has been removed
    for clarity.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As with the implementations in the CUDA support provided by NVIDIA, our peer-to-peer
    memcpy is synchronous.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.1.* `chMemcpyPeerToPeer()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t
  prefs: []
  type: TYPE_NORMAL
- en: chMemcpyPeerToPeer(
  prefs: []
  type: TYPE_NORMAL
- en: void *_dst, int dstDevice,
  prefs: []
  type: TYPE_NORMAL
- en: const void *_src, int srcDevice,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: char *dst = (char *) _dst;
  prefs: []
  type: TYPE_NORMAL
- en: const char *src = (const char *) _src;
  prefs: []
  type: TYPE_NORMAL
- en: int stagingIndex = 0;
  prefs: []
  type: TYPE_NORMAL
- en: while ( N ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( srcDevice );
  prefs: []
  type: TYPE_NORMAL
- en: cudaStreamWaitEvent( 0, g_events[dstDevice][stagingIndex],0);
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( g_hostBuffers[stagingIndex], src,
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize, cudaMemcpyDeviceToHost, NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( g_events[srcDevice][stagingIndex] );
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( dstDevice );
  prefs: []
  type: TYPE_NORMAL
- en: cudaStreamWaitEvent( 0, g_events[srcDevice][stagingIndex],0);
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize, cudaMemcpyHostToDevice, NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( g_events[dstDevice][stagingIndex] );
  prefs: []
  type: TYPE_NORMAL
- en: dst += thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: src += thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: N -= thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: stagingIndex = 1 - stagingIndex;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // Wait until both devices are done
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( srcDevice );
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( dstDevice );
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: return status;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 9.5\. Single-Threaded Multi-GPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using the CUDA runtime, a single-threaded application can drive multiple
    GPUs by calling `cudaSetDevice()` to specify which GPU will be operated by the
    calling CPU thread. This idiom is used in [Listing 9.1](ch09.html#ch09lis01) to
    switch between the source and destination GPUs during the peer-to-peer memcpy,
    as well as the single-threaded, multi-GPU implementation of N-body described in
    [Section 9.5.2](ch09.html#ch09lev2sec4). In the driver API, CUDA maintains a stack
    of current contexts so that subroutines can easily change and restore the caller’s
    current context.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.1\. Current Context Stack
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Driver API applications can manage the current context with the current-context
    stack: `cuCtxPushCurrent()` makes a new context current, pushing it onto the top
    of the stack, and `cuCtxPopCurrent()` pops the current context and restores the
    previous current context. [Listing 9.2](ch09.html#ch09lis02) gives a driver API
    version of `chMemcpyPeerToPeer()`, which uses `cuCtxPopCurrent()` and `cuCtxPushCurrent()`
    to perform a peer-to-peer memcpy between two contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: The current context stack was introduced to CUDA in v2.2, and at the time, the
    CUDA runtime and driver API could not be used in the same application. That restriction
    has been relaxed in subsequent versions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.2.* `chMemcpyPeerToPeer` (driver API version).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult
  prefs: []
  type: TYPE_NORMAL
- en: chMemcpyPeerToPeer(
  prefs: []
  type: TYPE_NORMAL
- en: void *_dst, CUcontext dstContext, int dstDevice,
  prefs: []
  type: TYPE_NORMAL
- en: const void *_src, CUcontext srcContext, int srcDevice,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult status;
  prefs: []
  type: TYPE_NORMAL
- en: CUdeviceptr dst = (CUdeviceptr) (intptr_t) _dst;
  prefs: []
  type: TYPE_NORMAL
- en: CUdeviceptr src = (CUdeviceptr) (intptr_t) _src;
  prefs: []
  type: TYPE_NORMAL
- en: int stagingIndex = 0;
  prefs: []
  type: TYPE_NORMAL
- en: while ( N ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPushCurrent( srcContext ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuStreamWaitEvent(
  prefs: []
  type: TYPE_NORMAL
- en: NULL, g_events[dstDevice][stagingIndex], 0 ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuMemcpyDtoHAsync(
  prefs: []
  type: TYPE_NORMAL
- en: g_hostBuffers[stagingIndex],
  prefs: []
  type: TYPE_NORMAL
- en: src,
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize,
  prefs: []
  type: TYPE_NORMAL
- en: NULL ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuEventRecord(
  prefs: []
  type: TYPE_NORMAL
- en: g_events[srcDevice][stagingIndex],
  prefs: []
  type: TYPE_NORMAL
- en: 0 ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPopCurrent( &srcContext ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPushCurrent( dstContext ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuStreamWaitEvent(
  prefs: []
  type: TYPE_NORMAL
- en: NULL,
  prefs: []
  type: TYPE_NORMAL
- en: g_events[srcDevice][stagingIndex],
  prefs: []
  type: TYPE_NORMAL
- en: 0 ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuMemcpyHtoDAsync(
  prefs: []
  type: TYPE_NORMAL
- en: dst,
  prefs: []
  type: TYPE_NORMAL
- en: g_hostBuffers[stagingIndex],
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize,
  prefs: []
  type: TYPE_NORMAL
- en: NULL ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuEventRecord(
  prefs: []
  type: TYPE_NORMAL
- en: g_events[dstDevice][stagingIndex],
  prefs: []
  type: TYPE_NORMAL
- en: 0 ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPopCurrent( &dstContext ) );
  prefs: []
  type: TYPE_NORMAL
- en: dst += thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: src += thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: N -= thisCopySize;
  prefs: []
  type: TYPE_NORMAL
- en: stagingIndex = 1 - stagingIndex;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // Wait until both devices are done
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPushCurrent( srcContext ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxSynchronize() );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPopCurrent( &srcContext ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPushCurrent( dstContext ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxSynchronize() );
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPopCurrent( &dstContext ) );
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: return status;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.2\. N-Body
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The N-body computation (described in detail in [Chapter 14](ch14.html#ch14))
    computes *N* forces in O(*N*²) time, and the outputs may be computed independently.
    On a system with *k* GPUs, our multi-GPU implementation splits the computation
    into *k* parts.
  prefs: []
  type: TYPE_NORMAL
- en: Our implementation makes the common assumption that the GPUs are identical,
    so it divides the computation evenly. Applications targeting GPUs of unequal performance,
    or whose workloads have less predictable runtimes, can divide the computation
    more finely and have the host code submit work items to the GPUs from a queue.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 9.3](ch09.html#ch09lis03) gives a modified version of [Listing 14.3](ch14.html#ch14lis03)
    that takes two additional parameters (a base index `base` and size `n` of the
    subarray of forces) to compute a subset of the output array for an N-body computation.
    This `__device__` function is invoked by wrapper kernels that are declared as
    `__global__` . It is structured this way to reuse the code without incurring link
    errors. If the function were declared as `__global__`, the linker would generate
    an error about duplicate symbols.^([3](ch09.html#ch09fn3))'
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch09.html#ch09fn3a). This is a bit of an old-school workaround. CUDA 5.0
    added a linker that enables the `__global__` function to be compiled into a static
    library and linked into the application.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.3.* N-body kernel (multi-GPU).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ void
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_Shared_multiGPU(
  prefs: []
  type: TYPE_NORMAL
- en: float *force,
  prefs: []
  type: TYPE_NORMAL
- en: float *posMass,
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: size_t base,
  prefs: []
  type: TYPE_NORMAL
- en: size_t n,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float4 *posMass4 = (float4 *) posMass;
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ float4 shPosMass[];
  prefs: []
  type: TYPE_NORMAL
- en: for ( int m = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: m < n;
  prefs: []
  type: TYPE_NORMAL
- en: m += blockDim.x*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: size_t i = base+m;
  prefs: []
  type: TYPE_NORMAL
- en: float acc[3] = {0};
  prefs: []
  type: TYPE_NORMAL
- en: float4 myPosMass = posMass4[i];
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma unroll 32'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < N; j += blockDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: shPosMass[threadIdx.x] = posMass4[j+threadIdx.x];
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t k = 0; k < blockDim.x; k++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: float fx, fy, fz;
  prefs: []
  type: TYPE_NORMAL
- en: float4 bodyPosMass = shPosMass[k];
  prefs: []
  type: TYPE_NORMAL
- en: bodyBodyInteraction(
  prefs: []
  type: TYPE_NORMAL
- en: '&fx, &fy, &fz,'
  prefs: []
  type: TYPE_NORMAL
- en: myPosMass.x, myPosMass.y, myPosMass.z,
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.x,
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.y,
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.z,
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.w,
  prefs: []
  type: TYPE_NORMAL
- en: softeningSquared );
  prefs: []
  type: TYPE_NORMAL
- en: acc[0] += fx;
  prefs: []
  type: TYPE_NORMAL
- en: acc[1] += fy;
  prefs: []
  type: TYPE_NORMAL
- en: acc[2] += fz;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: force[3*m+0] = acc[0];
  prefs: []
  type: TYPE_NORMAL
- en: force[3*m+1] = acc[1];
  prefs: []
  type: TYPE_NORMAL
- en: force[3*m+2] = acc[2];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The host code for a single-threaded, multi-GPU version of N-body is shown in
    [Listing 9.4](ch09.html#ch09lis04).^([4](ch09.html#ch09fn4)) The arrays `dptrPosMass`
    and `dptrForce` track the device pointers for the input and output arrays for
    each GPU (the maximum number of GPUs is declared as a constant in `nbody.h`; default
    is 32). Similar to dispatching work into CUDA streams, the function uses separate
    loops for different stages of the computation: The first loop allocates and populates
    the input array for each GPU; the second loop launches the kernel and an asynchronous
    copy of the output data; and the third loop calls `cudaDeviceSynchronize()` on
    each GPU in turn. Structuring the function this way maximizes CPU/GPU overlap.
    During the first loop, asynchronous host→device memcpys to GPUs 0..*i*-1 can proceed
    while the CPU is busy allocating memory for GPU *i*. If the kernel launch and
    asynchronous device→host memcpy were in the first loop, the synchronous `cudaMalloc()`
    calls would decrease performance because they are synchronous with respect to
    the current GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch09.html#ch09fn4a). To avoid awkward formatting, error checking has been
    removed.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.4.* N-body host code (single-threaded multi-GPU).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis04a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: ComputeGravitation_multiGPU_singlethread(
  prefs: []
  type: TYPE_NORMAL
- en: float *force,
  prefs: []
  type: TYPE_NORMAL
- en: float *posMass,
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: float ret = 0.0f;
  prefs: []
  type: TYPE_NORMAL
- en: float *dptrPosMass[g_maxGPUs];
  prefs: []
  type: TYPE_NORMAL
- en: float *dptrForce[g_maxGPUs];
  prefs: []
  type: TYPE_NORMAL
- en: chTimerTimestamp start, end;
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &start );
  prefs: []
  type: TYPE_NORMAL
- en: memset( dptrPosMass, 0, sizeof(dptrPosMass) );
  prefs: []
  type: TYPE_NORMAL
- en: memset( dptrForce, 0, sizeof(dptrForce) );
  prefs: []
  type: TYPE_NORMAL
- en: size_t bodiesPerGPU = N / g_numGPUs;
  prefs: []
  type: TYPE_NORMAL
- en: if ( (0 != N % g_numGPUs) || (g_numGPUs > g_maxGPUs) ) {
  prefs: []
  type: TYPE_NORMAL
- en: return 0.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // kick off the asynchronous memcpy's - overlap GPUs pulling
  prefs: []
  type: TYPE_NORMAL
- en: // host memory with the CPU time needed to do the memory
  prefs: []
  type: TYPE_NORMAL
- en: // allocations.
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( i );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMalloc( &dptrPosMass[i], 4*N*sizeof(float) );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMalloc( &dptrForce[i], 3*bodiesPerGPU*sizeof(float) );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync(
  prefs: []
  type: TYPE_NORMAL
- en: dptrPosMass[i],
  prefs: []
  type: TYPE_NORMAL
- en: g_hostAOS_PosMass,
  prefs: []
  type: TYPE_NORMAL
- en: 4*N*sizeof(float),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( i ) );
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_Shared_device<<<
  prefs: []
  type: TYPE_NORMAL
- en: 300,256,256*sizeof(float4)>>>(
  prefs: []
  type: TYPE_NORMAL
- en: dptrForce[i],
  prefs: []
  type: TYPE_NORMAL
- en: dptrPosMass[i],
  prefs: []
  type: TYPE_NORMAL
- en: softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: i*bodiesPerGPU,
  prefs: []
  type: TYPE_NORMAL
- en: bodiesPerGPU,
  prefs: []
  type: TYPE_NORMAL
- en: N );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync(
  prefs: []
  type: TYPE_NORMAL
- en: g_hostAOS_Force+3*bodiesPerGPU*i,
  prefs: []
  type: TYPE_NORMAL
- en: dptrForce[i],
  prefs: []
  type: TYPE_NORMAL
- en: 3*bodiesPerGPU*sizeof(float),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // Synchronize with each GPU in turn.
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( i );
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &end );
  prefs: []
  type: TYPE_NORMAL
- en: ret = chTimerElapsedTime( &start, &end ) * 1000.0f;
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( dptrPosMass[i] );
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( dptrForce[i] );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return ret;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 9.6\. Multithreaded Multi-GPU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CUDA has supported multiple GPUs since the beginning, but until CUDA 4.0, each
    GPU had to be controlled by a separate CPU thread. For workloads that required
    a lot of CPU power, that requirement was never very onerous because the full power
    of modern multicore processors can be unlocked only through multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: The multithreaded implementation of multi-GPU N-Body creates one CPU thread
    per GPU, and it delegates the dispatch and synchronization of the work for a given
    N-body pass to each thread. The main thread splits the work evenly between GPUs,
    delegates work to each worker thread by signaling an event (or a semaphore, on
    POSIX platforms such as Linux), and then waits for all of the worker threads to
    signal completion before proceeding. As the number of GPUs grows, synchronization
    overhead starts to chip away at the benefits from parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: This implementation of N-body uses the same multithreading library as the multithreaded
    implementation of N-body, described in [Section 14.9](ch14.html#ch14lev1sec9).
    The `workerThread` class, described in [Appendix A](app01.html#app01).2, enables
    the application thread to “delegate” work to CPU threads, then synchronize on
    the worker threads’ completion of the delegated task.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 9.5](ch09.html#ch09lis05) gives the host code that creates and initializes
    the CPU threads. Two globals, `g_numGPUs` and `g_GPUThreadPool`, contain the GPU
    count and a worker thread for each. After each CPU thread is created, it is initialized
    by synchronously calling the `initializeGPU()` function, which affiliates the
    CPU thread with a given GPU—an affiliation that never changes during the course
    of the application’s execution.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.5.* Multithreaded multi-GPU initialization code.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis05a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: workerThread *g_CPUThreadPool;
  prefs: []
  type: TYPE_NORMAL
- en: int g_numCPUCores;
  prefs: []
  type: TYPE_NORMAL
- en: workerThread *g_GPUThreadPool;
  prefs: []
  type: TYPE_NORMAL
- en: int g_numGPUs;
  prefs: []
  type: TYPE_NORMAL
- en: struct gpuInit_struct
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int iGPU;
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: initializeGPU( void *_p )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: gpuInit_struct *p = (gpuInit_struct *) _p;
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaSetDevice( p->iGPU ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaSetDeviceFlags( cudaDeviceMapHost ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaFree(0) );
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: p->status = status;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // ... below is from main()
  prefs: []
  type: TYPE_NORMAL
- en: if ( g_numGPUs ) {
  prefs: []
  type: TYPE_NORMAL
- en: chCommandLineGet( &g_numGPUs, "numgpus", argc, argv );
  prefs: []
  type: TYPE_NORMAL
- en: g_GPUThreadPool = new workerThread[g_numGPUs];
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0; i < g_numGPUs; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: if ( ! g_GPUThreadPool[i].initialize( ) ) {
  prefs: []
  type: TYPE_NORMAL
- en: fprintf( stderr, "Error initializing thread pool\n" );
  prefs: []
  type: TYPE_NORMAL
- en: return 1;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: gpuInit_struct initGPU = {i};
  prefs: []
  type: TYPE_NORMAL
- en: g_GPUThreadPool[i].delegateSynchronous(
  prefs: []
  type: TYPE_NORMAL
- en: initializeGPU,
  prefs: []
  type: TYPE_NORMAL
- en: '&initGPU );'
  prefs: []
  type: TYPE_NORMAL
- en: if ( cudaSuccess != initGPU.status ) {
  prefs: []
  type: TYPE_NORMAL
- en: fprintf( stderr, "Initializing GPU %d failed "
  prefs: []
  type: TYPE_NORMAL
- en: '"with %d (%s)\n",'
  prefs: []
  type: TYPE_NORMAL
- en: i,
  prefs: []
  type: TYPE_NORMAL
- en: initGPU.status,
  prefs: []
  type: TYPE_NORMAL
- en: cudaGetErrorString( initGPU.status ) );
  prefs: []
  type: TYPE_NORMAL
- en: return 1;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the worker threads are initialized, they suspend waiting on a thread synchronization
    primitive until the application thread dispatches work to them. [Listing 9.6](ch09.html#ch09lis06)
    shows the host code that dispatches work to the GPUs: The `gpuDelegation` structure
    encapsulates the work that a given GPU must do, and the `gpuWorkerThread` function
    is invoked for each of the worker threads created by the code in [Listing 9.5](ch09.html#ch09lis05).
    The application thread code, shown in [Listing 9.7](ch09.html#ch09lis07), creates
    a `gpuDelegation` structure for each worker thread and calls the `delegateAsynchronous()`
    method to invoke the code in [Listing 9.6](ch09.html#ch09lis06). The `waitAll()`
    method then waits until all of the worker threads have finished. The performance
    and scaling results of the single-threaded and multithreaded version of multi-GPU
    N-body are summarized in [Section 14.7](ch14.html#ch14lev1sec7).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.6.* Host code (worker thread).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis06a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: struct gpuDelegation {
  prefs: []
  type: TYPE_NORMAL
- en: size_t i;   // base offset for this thread to process
  prefs: []
  type: TYPE_NORMAL
- en: size_t n;   // size of this thread's problem
  prefs: []
  type: TYPE_NORMAL
- en: size_t N;   // total number of bodies
  prefs: []
  type: TYPE_NORMAL
- en: float *hostPosMass;
  prefs: []
  type: TYPE_NORMAL
- en: float *hostForce;
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared;
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: gpuWorkerThread( void *_p )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: gpuDelegation *p = (gpuDelegation *) _p;
  prefs: []
  type: TYPE_NORMAL
- en: float *dptrPosMass = 0;
  prefs: []
  type: TYPE_NORMAL
- en: float *dptrForce = 0;
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // Each GPU has its own device pointer to the host pointer.
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMalloc( &dptrPosMass, 4*p->N*sizeof(float) ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMalloc( &dptrForce, 3*p->n*sizeof(float) ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync(
  prefs: []
  type: TYPE_NORMAL
- en: dptrPosMass,
  prefs: []
  type: TYPE_NORMAL
- en: p->hostPosMass,
  prefs: []
  type: TYPE_NORMAL
- en: 4*p->N*sizeof(float),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice ) );
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_multiGPU<<<300,256,256*sizeof(float4)>>>(
  prefs: []
  type: TYPE_NORMAL
- en: dptrForce,
  prefs: []
  type: TYPE_NORMAL
- en: dptrPosMass,
  prefs: []
  type: TYPE_NORMAL
- en: p->softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: p->i,
  prefs: []
  type: TYPE_NORMAL
- en: p->n,
  prefs: []
  type: TYPE_NORMAL
- en: p->N );
  prefs: []
  type: TYPE_NORMAL
- en: '// NOTE: synchronous memcpy, so no need for further'
  prefs: []
  type: TYPE_NORMAL
- en: // synchronization with device
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpy(
  prefs: []
  type: TYPE_NORMAL
- en: p->hostForce+3*p->i,
  prefs: []
  type: TYPE_NORMAL
- en: dptrForce,
  prefs: []
  type: TYPE_NORMAL
- en: 3*p->n*sizeof(float),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost ) );
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( dptrPosMass );
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( dptrForce );
  prefs: []
  type: TYPE_NORMAL
- en: p->status = status;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.7.* ?Host code (application thread)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis07a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: ComputeGravitation_multiGPU_threaded(
  prefs: []
  type: TYPE_NORMAL
- en: float *force,
  prefs: []
  type: TYPE_NORMAL
- en: float *posMass,
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerTimestamp start, end;
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &start );
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: gpuDelegation *pgpu = new gpuDelegation[g_numGPUs];
  prefs: []
  type: TYPE_NORMAL
- en: size_t bodiesPerGPU = N / g_numGPUs;
  prefs: []
  type: TYPE_NORMAL
- en: if ( N % g_numGPUs ) {
  prefs: []
  type: TYPE_NORMAL
- en: return 0.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: size_t i;
  prefs: []
  type: TYPE_NORMAL
- en: for ( i = 0; i < g_numGPUs; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].hostPosMass = g_hostAOS_PosMass;
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].hostForce = g_hostAOS_Force;
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].softeningSquared = softeningSquared;
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].i = bodiesPerGPU*i;
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].n = bodiesPerGPU;
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].N = N;
  prefs: []
  type: TYPE_NORMAL
- en: g_GPUThreadPool[i].delegateAsynchronous(
  prefs: []
  type: TYPE_NORMAL
- en: gpuWorkerThread,
  prefs: []
  type: TYPE_NORMAL
- en: '&pgpu[i] );'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: workerThread::waitAll( g_GPUThreadPool, g_numGPUs );
  prefs: []
  type: TYPE_NORMAL
- en: delete[] pgpu;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &end );
  prefs: []
  type: TYPE_NORMAL
- en: return chTimerElapsedTime( &start, &end ) * 1000.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 10\. Texturing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 10.1\. Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In CUDA, a software technology for general-purpose parallel computing, texture
    support could not have been justified if the hardware hadn’t already been there,
    due to its graphics-accelerating heritage. Nevertheless, the texturing hardware
    accelerates enough useful operations that NVIDIA saw fit to include support. Although
    many CUDA applications may be built without ever using texture, some rely on it
    to be competitive with CPU-based code.
  prefs: []
  type: TYPE_NORMAL
- en: Texture mapping was invented to enable richer, more realistic-looking objects
    by enabling images to be “painted” onto geometry. Historically, the hardware interpolated
    texture coordinates along with the X, Y, and Z coordinates needed to render a
    triangle, and for each output pixel, the texture value was fetched (optionally
    with bilinear interpolation), processed by blending with interpolated shading
    factors, and blended into the output buffer. With the introduction of programmable
    graphics and texture-like data that might not include color data (for example,
    bump maps), graphics hardware became more sophisticated. The shader programs included
    `TEX` instructions that specified the coordinates to fetch, and the results were
    incorporated into the computations used to generate the output pixel. The hardware
    improves performance using texture caches, memory layouts optimized for dimensional
    locality, and a dedicated hardware pipeline to transform texture coordinates into
    hardware addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Because the functionality grew organically and was informed by a combination
    of application requirements and hardware costs, the texturing features are not
    very orthogonal. For example, the “wrap” and “mirror” texture addressing modes
    do not work unless the texture coordinates are normalized. This chapter explains
    every detail of the texture hardware as supported by CUDA. We will cover everything
    from normalized versus unnormalized coordinates to addressing modes to the limits
    of linear interpolation; 1D, 2D, 3D, and layered textures; and how to use these
    features from both the CUDA runtime and the driver API.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1\. Two Use Cases
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In CUDA, there are two significantly different uses for texture. One is to
    simply use texture as a read path: to work around coalescing constraints or to
    use the texture cache to reduce external bandwidth requirements, or both. The
    other use case takes advantage of the fixed-function hardware that the GPU has
    in place for graphics applications. The texture hardware consists of a configurable
    pipeline of computation stages that can do all of the following.'
  prefs: []
  type: TYPE_NORMAL
- en: • Scale normalized texture coordinates
  prefs: []
  type: TYPE_NORMAL
- en: • Perform boundary condition computations on the texture coordinates
  prefs: []
  type: TYPE_NORMAL
- en: • Convert texture coordinates to addresses with 2D or 3D locality
  prefs: []
  type: TYPE_NORMAL
- en: • Fetch 2, 4, or 8 texture elements for 1D, 2D, or 3D textures and linearly
    interpolate between them
  prefs: []
  type: TYPE_NORMAL
- en: • Convert the texture values from integers to unitized floating-point values
  prefs: []
  type: TYPE_NORMAL
- en: Textures are read through *texture references* that are bound to underlying
    memory (either CUDA arrays or device memory). The memory is just an unshaped bucket
    of bits; it is the texture reference that tells the hardware how to interpret
    the data and deliver it into registers when a TEX instruction is executed.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2\. Texture Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before describing the features of the fixed-function texturing hardware, let’s
    spend some time examining the underlying memory to which texture references may
    be bound. CUDA can texture from either device memory or CUDA arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1\. Device Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In device memory, the textures are addressed in row-major order. A 1024x768
    texture might look like [Figure 10.1](ch10.html#ch10fig01), where *Offset* is
    the offset (in elements) from the base pointer of the image.
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 10.1)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For a byte offset, multiply by the size of the elements.
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 10.2)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10equ02.jpg)![Image](graphics/10fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.1* 1024x768 image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, this addressing calculation only works for the most convenient
    of texture widths: 1024 happens to be convenient because it is a power of 2 and
    conforms to all manner of alignment restrictions. To accommodate less convenient
    texture sizes, CUDA implements *pitch-linear addressing*, where the width of the
    texture memory is different from the width of the texture. For less convenient
    widths, the hardware enforces an alignment restriction and the width in elements
    is treated differently from the width of the texture memory. For a texture width
    of 950, say, and an alignment restriction of 64 bytes, the width-in-bytes is padded
    to 964 (the next multiple of 64), and the texture looks like [Figure 10.2](ch10.html#ch10fig02).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.2* 950x768 image, with pitch.'
  prefs: []
  type: TYPE_NORMAL
- en: In CUDA, the padded width in bytes is called the *pitch*. The total amount of
    device memory used by this image is 964x768 elements. The offset into the image
    now is computed in bytes, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '*ByteOffset = Y * Pitch + XInBytes*'
  prefs: []
  type: TYPE_NORMAL
- en: Applications can call `cudaMallocPitch()/cuMemAllocPitch()` to delegate selection
    of the pitch to the CUDA driver.^([1](ch10.html#ch10fn1)) In 3D, pitch-linear
    images of a given *Depth* are exactly like 2D images, with *Depth* 2D slices laid
    out contiguously in device memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch10.html#ch10fn1a). Code that delegates to the driver is more future-proof
    than code that tries to perform allocations that comply with the documented alignment
    restrictions, since those restrictions are subject to change.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2\. CUDA Arrays and Block Linear Addressing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA arrays are designed specifically to support texturing. They are allocated
    from the same pool of physical memory as device memory, but they have an opaque
    layout and cannot be addressed with pointers. Instead, memory locations in a CUDA
    array must be identified by the array handle and a set of 1D, 2D, or 3D coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 'CUDA arrays perform a more complicated addressing calculation, designed so
    that contiguous addresses exhibit 2D or 3D locality. The addressing calculation
    is hardware-specific and changes from one hardware generation to the next. [Figure
    10.1](ch10.html#ch10fig01) illustrates one of the mechanisms used: The two least
    significant address bits of row and column have been interleaved before undertaking
    the addressing calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in [Figure 10.3](ch10.html#ch10fig03), bit interleaving enables
    contiguous addresses to have “dimensional locality”: A cache line fill pulls in
    a block of pixels in a neighborhood rather than a horizontal span of pixels.^([2](ch10.html#ch10fn2))
    When taken to the limit, bit interleaving imposes some inconvenient requirements
    on the texture dimensions, so it is just one of several strategies used for the
    so-called “block linear” addressing calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch10.html#ch10fn2a). 3D textures similarly interleave the X, Y, and Z coordinate
    bits.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.3* 1024x768 image, interleaved bits.'
  prefs: []
  type: TYPE_NORMAL
- en: In device memory, the location of an image element can be specified by any of
    the following.
  prefs: []
  type: TYPE_NORMAL
- en: • The base pointer, pitch, and a *(XInBytes, Y)* or *(XInBytes, Y, Z)* tuple
  prefs: []
  type: TYPE_NORMAL
- en: • The base pointer and an offset as computed by [Equation 10.1](ch10.html#ch10equ01)
  prefs: []
  type: TYPE_NORMAL
- en: • The device pointer with the offset already applied
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, when CUDA arrays do not have device memory addresses, so memory
    locations must be specified in terms of the CUDA array and a tuple *(XInBytes,
    Y)* or *(XInBytes, Y, Z)*.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Destroying CUDA Arrays
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using the CUDA runtime, CUDA arrays may be created by calling `cudaMallocArray()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p309pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMallocArray(struct cudaArray **array, const struct
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatDesc *desc, size_t width, size_t height __dv(0),
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int flags __dv(0));
  prefs: []
  type: TYPE_NORMAL
- en: '`array` passes back the array handle, and `desc` specifies the number and type
    of components (e.g., 2 floats) in each array element. `width` specifies the width
    of the array *in bytes*. `height` is an optional parameter that specifies the
    height of the array; if the height is not specified, `cudaMallocArray()` creates
    a 1D CUDA array.'
  prefs: []
  type: TYPE_NORMAL
- en: The `flags` parameter is used to hint at the CUDA array’s usage. As of this
    writing, the only flag is `cudaArraySurfaceLoadStore`, which must be specified
    if the CUDA array will be used for surface read/write operations as described
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The `__dv` macro used for the `height` and `flags` parameters causes the declaration
    to behave differently, depending on the language. When compiled for C, it becomes
    a simple parameter, but when compiled for C++, it becomes a parameter with the
    specified default value.
  prefs: []
  type: TYPE_NORMAL
- en: The structure `cudaChannelFormatDesc` describes the contents of a texture.
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaChannelFormatDesc {
  prefs: []
  type: TYPE_NORMAL
- en: int x, y, z, w;
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaChannelFormatKind f;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: The `x`, `y`, `z,` and `w` members of the structure specify the number of bits
    in each member of the texture element. For example, a 1-element float texture
    will contain `x==32` and the other elements will be `0`. The `cudaChannelFormatKind`
    structure specifies whether the data is signed integer, unsigned integer, or floating
    point.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p310pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaChannelFormatKind
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatKindSigned = 0,
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatKindUnsigned = 1,
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatKindFloat = 2,
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatKindNone = 3
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: Developers can create `cudaChannelFormatDesc` structures using the `cudaCreateChannelDesc`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p310pro03a)'
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatDesc cudaCreateChannelDesc(int x, int y, int z, int w, cudaChannelFormatKind
    kind);
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, a templated family of functions can be invoked as follows.
  prefs: []
  type: TYPE_NORMAL
- en: template<class T> cudaCreateChannelDesc<T>();
  prefs: []
  type: TYPE_NORMAL
- en: where `T` may be any of the native formats supported by CUDA. Here are two examples
    of the specializations of this template.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p311pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: template<> __inline__ __host__ cudaChannelFormatDesc cudaCreateChannelDesc<float>(void)
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int e = (int)sizeof(float) * 8;
  prefs: []
  type: TYPE_NORMAL
- en: return cudaCreateChannelDesc(e, 0, 0, 0, cudaChannelFormatKindFloat);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: template<> __inline__ __host__ cudaChannelFormatDesc cudaCreateChannelDesc<uint2>(void)
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int e = (int)sizeof(unsigned int) * 8;
  prefs: []
  type: TYPE_NORMAL
- en: return cudaCreateChannelDesc(e, e, 0, 0, cudaChannelFormatKindUnsigned);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  prefs: []
  type: TYPE_NORMAL
- en: When using the `char` data type, be aware that some compilers assume `char`
    is signed, while others assume it is unsigned. You can always make this distinction
    unambiguous with the `signed` keyword.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 3D CUDA arrays may be allocated with `cudaMalloc3DArray().`
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p311pro03a)'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMalloc3DArray(struct cudaArray** array, const struct
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatDesc* desc, struct cudaExtent extent, unsigned int
  prefs: []
  type: TYPE_NORMAL
- en: flags __dv(0));
  prefs: []
  type: TYPE_NORMAL
- en: Rather than taking width, height, and depth parameters, `cudaMalloc3DArray()`
    takes a `cudaExtent` structure.
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaExtent {
  prefs: []
  type: TYPE_NORMAL
- en: size_t width;
  prefs: []
  type: TYPE_NORMAL
- en: size_t height;
  prefs: []
  type: TYPE_NORMAL
- en: size_t depth;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: The `flags` parameter, like that of `cudaMallocArray()`, must be `cudaArraySurfaceLoadStore`
    if the CUDA array will be used for surface read/write operations.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For array handles, the CUDA runtime and driver API are compatible with one another.
    The pointer passed back by `cudaMallocArray()` can be cast to `CUarray` and passed
    to driver API functions such as `cuArrayGetDescriptor()`.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Driver API*'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The driver API equivalents of `cudaMallocArray()` and `cudaMalloc3DArray()`
    are `cuArrayCreate()` and `cuArray3DCreate()`, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p312pro03a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuArrayCreate(CUarray *pHandle, const CUDA_ARRAY_DESCRIPTOR
  prefs: []
  type: TYPE_NORMAL
- en: '*pAllocateArray);'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuArray3DCreate(CUarray *pHandle, const CUDA_ARRAY3D_
  prefs: []
  type: TYPE_NORMAL
- en: DESCRIPTOR *pAllocateArray);
  prefs: []
  type: TYPE_NORMAL
- en: '`cuArray3DCreate()` can be used to allocate 1D or 2D CUDA arrays by specifying
    0 as the height or depth, respectively. The `CUDA_ARRAY3D_DESCRIPTOR` structure
    is as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p312pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: typedef struct CUDA_ARRAY3D_DESCRIPTOR_st
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: size_t Width;
  prefs: []
  type: TYPE_NORMAL
- en: size_t Height;
  prefs: []
  type: TYPE_NORMAL
- en: size_t Depth;
  prefs: []
  type: TYPE_NORMAL
- en: CUarray_format Format;
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int NumChannels;
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int Flags;
  prefs: []
  type: TYPE_NORMAL
- en: '} CUDA_ARRAY3D_DESCRIPTOR;'
  prefs: []
  type: TYPE_NORMAL
- en: 'Together, the `Format` and `NumChannels` members describe the size of each
    element of the CUDA array: `NumChannels` may be 1, 2, or 4, and `Format` specifies
    the channels’ type, as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p312pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: typedef enum CUarray_format_enum {
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_UNSIGNED_INT8 = 0x01,
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_UNSIGNED_INT16 = 0x02,
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_UNSIGNED_INT32 = 0x03,
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_SIGNED_INT8 = 0x08,
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_SIGNED_INT16 = 0x09,
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_SIGNED_INT32 = 0x0a,
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_HALF = 0x10,
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_FLOAT = 0x20
  prefs: []
  type: TYPE_NORMAL
- en: '} CUarray_format;'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The format specified in `CUDA_ARRAY3D_DESCRIPTOR` is just a convenient way to
    specify the amount of data in the CUDA array. Textures bound to the CUDA array
    can specify a different format, as long as the bytes per element is the same.
    For example, it is perfectly valid to bind a `texture<int>` reference to a CUDA
    array containing 4-component bytes (32 bits per element).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes CUDA array handles are passed to subroutines that need to query the
    dimensions and/or format of the input array. The following `cuArray3DGetDescriptor()`
    function is provided for that purpose.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p313pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuArray3DGetDescriptor(CUDA_ARRAY3D_DESCRIPTOR
  prefs: []
  type: TYPE_NORMAL
- en: '*pArrayDescriptor, CUarray hArray);'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this function may be called on 1D and 2D CUDA arrays, even those that
    were created with `cuArrayCreate()`.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3\. Device Memory versus CUDA Arrays
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For applications that exhibit sparse access patterns, especially patterns with
    dimensional locality (for example, computer vision applications), CUDA arrays
    are a clear win. For applications with regular access patterns, especially those
    with little to no reuse or whose reuse can be explicitly managed by the application
    in shared memory, device pointers are the obvious choice.
  prefs: []
  type: TYPE_NORMAL
- en: Some applications, such as image processing applications, fall into a gray area
    where the choice between device pointers and CUDA arrays is not obvious. All other
    things being equal, device memory is probably preferable to CUDA arrays, but the
    following considerations may be used to help in the decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: • Until CUDA 3.2, CUDA kernels could not write to CUDA arrays. They were only
    able to read from them via texture intrinsics. CUDA 3.2 added the ability for
    Fermi-class hardware to access 2D CUDA arrays via “surface read/write” intrinsics.
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA arrays do not consume any CUDA address space.
  prefs: []
  type: TYPE_NORMAL
- en: • On WDDM drivers (Windows Vista and later), the system can automatically manage
    the residence of CUDA arrays. They can be swapped into and out of device memory
    transparently, depending on whether they are needed by the CUDA kernels that are
    executing. In contrast, WDDM requires all device memory to be resident in order
    for any kernel to execute.
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA arrays can reside only in device memory, and if the GPU contains copy
    engines, it can convert between the two representations while transferring the
    data across the bus. For some applications, keeping a pitch representation in
    host memory and a CUDA array representation in device memory is the best fit.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3\. 1D Texturing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For illustrative purposes, we will deal with 1D textures in detail and then
    expand the discussion to include 2D and 3D textures.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1\. Texture Setup
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The data in textures can consist of 1, 2, or 4 elements of any of the following
    types.
  prefs: []
  type: TYPE_NORMAL
- en: • Signed or unsigned 8-, 16-, or 32-bit integers
  prefs: []
  type: TYPE_NORMAL
- en: • 16-bit floating-point values
  prefs: []
  type: TYPE_NORMAL
- en: • 32-bit floating-point values
  prefs: []
  type: TYPE_NORMAL
- en: In the `.cu` file (whether using the CUDA runtime or the driver API), the texture
    reference is declared as follows.
  prefs: []
  type: TYPE_NORMAL
- en: texture<ReturnType, Dimension, ReadMode> Name;
  prefs: []
  type: TYPE_NORMAL
- en: where `ReturnType` is the value returned by the texture intrinsic; `Dimension`
    is 1, 2, or 3 for 1D, 2D, or 3D, respectively; and `ReadMode` is an optional parameter
    type that defaults to `cudaReadModeElementType`. The read mode only affects integer-valued
    texture data. By default, the texture passes back integers when the texture data
    is integer-valued, promoting them to 32-bit if necessary. But when `cudaReadModeNormalizedFloat`
    is specified as the read mode, 8- or 16-bit integers can be promoted to floating-point
    values in the range [0.0, 1.0] according to the formulas in [Table 10.1](ch10.html#ch10tab01).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 10.1* Floating-Point Promotion (Texture)'
  prefs: []
  type: TYPE_NORMAL
- en: The C versions of this conversion operation are given in [Listing 10.1](ch10.html#ch10lis01).
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 10.1.* Texture unit floating-point conversion.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p10lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: TexPromoteToFloat( signed char c )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: if ( c == (signed char) 0x80 ) {
  prefs: []
  type: TYPE_NORMAL
- en: return -1.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return (float) c / 127.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: TexPromoteToFloat( short s )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: if ( s == (short) 0x8000 ) {
  prefs: []
  type: TYPE_NORMAL
- en: return -1.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return (float) s / 32767.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: TexPromoteToFloat( unsigned char uc )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: return (float) uc / 255.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: TexPromoteToFloat( unsigned short us )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: return (float) us / 65535.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Once the texture reference is declared, it can be used in kernels by invoking
    texture intrinsics. Different intrinsics are used for different types of texture,
    as shown in [Table 10.2](ch10.html#ch10tab02).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 10.2* Texture Intrinsics'
  prefs: []
  type: TYPE_NORMAL
- en: Texture references have file scope and behave similarly to global variables.
    They cannot be created, destroyed, or passed as parameters, so wrapping them in
    higher-level abstractions must be undertaken with care.
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Before invoking a kernel that uses a texture, the texture must be *bound* to
    a CUDA array or device memory by calling `cudaBindTexture()`, `cudaBindTexture2D()`,
    or `cudaBindTextureToArray()`. Due to the language integration of the CUDA runtime,
    the texture can be referenced by name, such as the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p316pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: texture<float, 2, cudaReadModeElementType> tex;
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
  prefs: []
  type: TYPE_NORMAL
- en: Once the texture is bound, kernels that use that texture reference will read
    from the bound memory until the texture binding is changed.
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When a texture is declared in a `.cu` file, driver applications must query it
    using `cuModuleGetTexRef()`. In the driver API, the immutable attributes of the
    texture must be set explicitly, and they must agree with the assumptions used
    by the compiler to generate the code. For most textures, this just means the format
    must agree with the format declared in the `.cu` file; the exception is when textures
    are set up to promote integers or 16-bit floating-point values to normalized 32-bit
    floating-point values.
  prefs: []
  type: TYPE_NORMAL
- en: The `cuTexRefSetFormat()` function is used to specify the format of the data
    in the texture.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p316pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuTexRefSetFormat(CUtexref hTexRef, CUarray_format
  prefs: []
  type: TYPE_NORMAL
- en: fmt, int NumPackedComponents);
  prefs: []
  type: TYPE_NORMAL
- en: The array formats are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/317tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '`NumPackedComponents` specifies the number of components in each texture element.
    It may be 1, 2, or 4\. 16-bit floats (`half`) are a special data type that are
    well suited to representing image data with high integrity.^([3](ch10.html#ch10fn3))
    With 10 bits of floating-point mantissa (effectively 11 bits of precision for
    normalized numbers), there is enough precision to represent data generated by
    most sensors, and 5 bits of exponent gives enough dynamic range to represent starlight
    and sunlight in the same image. Most floating-point architectures do not include
    native instructions to process 16-bit floats, and CUDA is no exception. The texture
    hardware promotes 16-bit floats to 32-bit floats automatically, and CUDA kernels
    can convert between 16- and 32-bit floats with the `__float2half_rn()` and `__half2float_rn()`
    intrinsics.'
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch10.html#ch10fn3a). [Section 8.3.4](ch08.html#ch08lev2sec13) describes
    16-bit floats in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.4\. Texture as a Read Path
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using texture as a read path—that is, using the texturing hardware to get
    around awkward coalescing constraints or to take advantage of the texture cache
    as opposed to accessing hardware features such as linear interpolation—many texturing
    features are unavailable. The highlights of this usage for texture are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: • The texture reference must be bound to device memory with `cudaBind-Texture()`
    or `cuTexRefSetAddress()`.
  prefs: []
  type: TYPE_NORMAL
- en: • The `tex1Dfetch()` intrinsic must be used. It takes a 27-bit integer index.^([4](ch10.html#ch10fn4))
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch10.html#ch10fn4a). All CUDA-capable hardware has the same 27-bit limit,
    so there is not yet any way to query a device for the limit.'
  prefs: []
  type: TYPE_NORMAL
- en: • `tex1Dfetch()` optionally can convert the texture contents to floating-point
    values. Integers are converted to floating-point values in the range [0.0, 1.0],
    and 16-bit floating-point values are promoted to `float`.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of reading device memory via `tex1Dfetch()` are twofold. First,
    memory reads via texture do not have to conform to the coalescing constraints
    that apply when reading global memory. Second, the texture cache can be a useful
    complement to the other hardware resources, even the L2 cache on Fermi-class hardware.
    When an out-of-range index is passed to `tex1Dfetch()`, it returns 0.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.1\. Increasing Effective Address Coverage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since the 27-bit index specifies which texture element to fetch, and the texture
    elements may be up to 16 bytes in size, a texture being read via `tex1Dfetch()`
    can cover up to 31 bits (2^(27)+2⁴) worth of memory. One way to increase the amount
    of data being effectively covered by a texture is to use wider texture elements
    than the actual data size. For example, the application can texture from `float4`
    instead of `float`, then select the appropriate element of the `float4,` depending
    on the least significant bits of the desired index. Similar techniques can be
    applied to integer data, especially 8- or 16-bit data where global memory transactions
    are always uncoalesced. Alternatively, applications can alias multiple textures
    over different segments of the device memory and perform predicated texture fetches
    from each texture in such a way that only one of them is “live.”
  prefs: []
  type: TYPE_NORMAL
- en: 'Microdemo: tex1dfetch_big.cu'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This program illustrates using `tex1Dfetch()` to read from large arrays using
    both multiple components per texture and multiple textures. It is invoked as follows.
  prefs: []
  type: TYPE_NORMAL
- en: tex1dfetch_big <NumMegabytes>
  prefs: []
  type: TYPE_NORMAL
- en: The application allocates the specified number of megabytes of device memory
    (or mapped pinned host memory, if the device memory allocation fails), fills the
    memory with random numbers, and uses 1-, 2-, and 4-component textures to compute
    checksums on the data. Up to four textures of `int4` can be used, enabling the
    application to texture from up to 8192M of memory.
  prefs: []
  type: TYPE_NORMAL
- en: For clarity, `tex1dfetch_big.cu` does not perform any fancy parallel reduction
    techniques. Each thread writes back an intermediate sum, and the final checksums
    are accumulated on the CPU. The application defines the 27-bit hardware limits.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p319pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '#define CUDA_LG_MAX_TEX1DFETCH_INDEX 27'
  prefs: []
  type: TYPE_NORMAL
- en: '#define CUDA_MAX_TEX1DFETCH_INDEX'
  prefs: []
  type: TYPE_NORMAL
- en: (((size_t)1<<CUDA_LG_MAX_TEX1DFETCH_INDEX)-1)
  prefs: []
  type: TYPE_NORMAL
- en: And it defines four textures of `int4.`
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p319pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: texture<int4, 1, cudaReadModeElementType> tex4_0;
  prefs: []
  type: TYPE_NORMAL
- en: texture<int4, 1, cudaReadModeElementType> tex4_1;
  prefs: []
  type: TYPE_NORMAL
- en: texture<int4, 1, cudaReadModeElementType> tex4_2;
  prefs: []
  type: TYPE_NORMAL
- en: texture<int4, 1, cudaReadModeElementType> tex4_3;
  prefs: []
  type: TYPE_NORMAL
- en: A device function `tex4Fetch()` takes an index and teases it apart into a texture
    ordinal and a 27-bit index to pass to `tex1Dfetch()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p319pro03a)'
  prefs: []
  type: TYPE_NORMAL
- en: __device__ int4
  prefs: []
  type: TYPE_NORMAL
- en: tex4Fetch( size_t index )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int texID = (int) (index>>CUDA_LG_MAX_TEX1DFETCH_INDEX);
  prefs: []
  type: TYPE_NORMAL
- en: int i = (int) (index & (CUDA_MAX_TEX1DFETCH_INDEX_SIZE_T-1));
  prefs: []
  type: TYPE_NORMAL
- en: int4 i4;
  prefs: []
  type: TYPE_NORMAL
- en: if ( texID == 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: i4 = tex1Dfetch( tex4_0, i );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else if ( texID == 1 ) {
  prefs: []
  type: TYPE_NORMAL
- en: i4 = tex1Dfetch( tex4_1, i );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else if ( texID == 2 ) {
  prefs: []
  type: TYPE_NORMAL
- en: i4 = tex1Dfetch( tex4_2, i );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else if ( texID == 3 ) {
  prefs: []
  type: TYPE_NORMAL
- en: i4 = tex1Dfetch( tex4_3, i );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return i4;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: This device function compiles to a small amount of code that uses four predicated
    TEX instructions, only one of which is “live.” If random access is desired, the
    application also can use predication to select from the `.x`, `.y`, `.z`, or `.w`
    component of the `int4` return value.
  prefs: []
  type: TYPE_NORMAL
- en: Binding the textures, shown in [Listing 10.2](ch10.html#ch10lis02), is a slightly
    tricky business. This code creates two small arrays `texSizes[]` and `texBases[]`
    and sets them up to cover the device memory range. The `for` loop ensures that
    all four textures have a valid binding, even if fewer than four are needed to
    map the device memory.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 10.2.* `tex1dfetch_big.cu` (excerpt).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p10lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: int iTexture;
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatDesc int4Desc = cudaCreateChannelDesc<int4>();
  prefs: []
  type: TYPE_NORMAL
- en: size_t numInt4s = numBytes / sizeof(int4);
  prefs: []
  type: TYPE_NORMAL
- en: int numTextures = (numInt4s+CUDA_MAX_TEX1DFETCH_INDEX)>>
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_LG_MAX_TEX1DFETCH_INDEX;
  prefs: []
  type: TYPE_NORMAL
- en: size_t Remainder = numBytes & (CUDA_MAX_BYTES_INT4-1);
  prefs: []
  type: TYPE_NORMAL
- en: if ( ! Remainder ) {
  prefs: []
  type: TYPE_NORMAL
- en: Remainder = CUDA_MAX_BYTES_INT4;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: size_t texSizes[4];
  prefs: []
  type: TYPE_NORMAL
- en: char *texBases[4];
  prefs: []
  type: TYPE_NORMAL
- en: for ( iTexture = 0; iTexture < numTextures; iTexture++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: texBases[iTexture] = deviceTex+iTexture*CUDA_MAX_BYTES_INT4;
  prefs: []
  type: TYPE_NORMAL
- en: texSizes[iTexture] = CUDA_MAX_BYTES_INT4;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: texSizes[iTexture-1] = Remainder;
  prefs: []
  type: TYPE_NORMAL
- en: while ( iTexture < 4 ) {
  prefs: []
  type: TYPE_NORMAL
- en: texBases[iTexture] = texBases[iTexture-1];
  prefs: []
  type: TYPE_NORMAL
- en: texSizes[iTexture] = texSizes[iTexture-1];
  prefs: []
  type: TYPE_NORMAL
- en: iTexture++;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: cudaBindTexture( NULL, tex4_0, texBases[0], int4Desc, texSizes[0] );
  prefs: []
  type: TYPE_NORMAL
- en: cudaBindTexture( NULL, tex4_1, texBases[1], int4Desc, texSizes[1] );
  prefs: []
  type: TYPE_NORMAL
- en: cudaBindTexture( NULL, tex4_2, texBases[2], int4Desc, texSizes[2] );
  prefs: []
  type: TYPE_NORMAL
- en: cudaBindTexture( NULL, tex4_3, texBases[3], int4Desc, texSizes[3] );
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Once compiled and run, the application can be invoked with different sizes to
    see the effects. On a CG1 instance running in Amazon’s EC2 cloud compute offering,
    invocations with 512M, 768M, 1280M, and 8192M worked as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p321pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: $ ./tex1dfetch_big 512
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected checksum: 0x7b7c8cd3'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex1 checksum: 0x7b7c8cd3'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex2 checksum: 0x7b7c8cd3'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex4 checksum: 0x7b7c8cd3'
  prefs: []
  type: TYPE_NORMAL
- en: $ ./tex1dfetch_big 768
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected checksum: 0x559a1431'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex1 checksum: (not performed)'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex2 checksum: 0x559a1431'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex4 checksum: 0x559a1431'
  prefs: []
  type: TYPE_NORMAL
- en: $ ./tex1dfetch_big 1280
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected checksum: 0x66a4f9d9'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex1 checksum: (not performed)'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex2 checksum: (not performed)'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex4 checksum: 0x66a4f9d9'
  prefs: []
  type: TYPE_NORMAL
- en: $ ./tex1dfetch_big 8192
  prefs: []
  type: TYPE_NORMAL
- en: Device alloc of 8192 Mb failed, trying mapped host memory
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected checksum: 0xf049c607'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex1 checksum: (not performed)'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex2 checksum: (not performed)'
  prefs: []
  type: TYPE_NORMAL
- en: 'tex4 checksum: 0xf049c607'
  prefs: []
  type: TYPE_NORMAL
- en: Each `int4` texture can “only” read 2G, so invoking the program with numbers
    greater than 8192 causes it to fail. This application highlights the demand for
    indexed textures, where the texture being fetched can be specified as a parameter
    at runtime, but CUDA does not expose support for this feature.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2\. Texturing from Host Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using texture as a read path, applications can read from host memory by allocating
    mapped pinned memory, fetching the device pointer, and then specifying that device
    pointer to `cudaBindAddress()` or `cuTexRefSetAddress()`. The capability is there,
    but reading host memory via texture is *slow*. Tesla-class hardware can texture
    over PCI Express at about 2G/s, and Fermi hardware is much slower. You need some
    other reason to do it, such as code simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Microdemo: tex1dfetch_int2float.cu'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This code fragment uses texture-as-a-read path and texturing from host memory
    to confirm that the `TexPromoteToFloat()` functions work properly. The CUDA kernel
    that we will use for this purpose is a straightforward, blocking-agnostic implementation
    of a memcpy function that reads from the texture and writes to device memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p321pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: texture<signed char, 1, cudaReadModeNormalizedFloat> tex;
  prefs: []
  type: TYPE_NORMAL
- en: extern "C" __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: TexReadout( float *out, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += gridDim.x*blockDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: out[i] = tex1Dfetch( tex, i );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since promoting integers to floating point only works on 8- and 16-bit values,
    we can test every possible conversion by allocating a small buffer, texturing
    from it, and confirming that the output meets our expectations. [Listing 10.3](ch10.html#ch10lis03)
    gives an excerpt from `tex1dfetch_int2float.cu`. Two host buffers are allocated:
    `inHost` holds the input buffer of 256 or 65536 input values, and `fOutHost` holds
    the corresponding float-valued outputs. The device pointers corresponding to these
    mapped host pointers are fetched into `inDevice` and `foutDevice`.'
  prefs: []
  type: TYPE_NORMAL
- en: The input values are initialized to every possible value of the type to be tested,
    and then the input device pointer is bound to the texture reference using `cudaBindTexture()`.
    The `TexReadout()` kernel is then invoked to read each value from the input texture
    and write as output the values returned by `tex1Dfetch()`. In this case, both
    the input and output buffers reside in mapped host memory. Because the kernel
    is writing directly to host memory, we must call `cudaDeviceSynchronize()` to
    make sure there are no race conditions between the CPU and GPU. At the end of
    the function, we call the `TexPromoteToFloat()` specialization corresponding to
    the type being tested and confirm that it is equal to the value returned by the
    kernel. If all tests pass, the function returns `true`; if any API functions or
    comparisons fail, it returns `false`.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 10.3.* `tex1d_int2float.cu` (excerpt).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p10lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: CheckTexPromoteToFloat( size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: T *inHost, *inDevice;
  prefs: []
  type: TYPE_NORMAL
- en: float *foutHost, *foutDevice;
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostAlloc( (void **) &inHost,
  prefs: []
  type: TYPE_NORMAL
- en: N*sizeof(T),
  prefs: []
  type: TYPE_NORMAL
- en: cudaHostAllocMapped));
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostGetDevicePointer( (void **) &inDevice,
  prefs: []
  type: TYPE_NORMAL
- en: inHost,
  prefs: []
  type: TYPE_NORMAL
- en: 0 ));
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostAlloc( (void **) &foutHost,
  prefs: []
  type: TYPE_NORMAL
- en: N*sizeof(float),
  prefs: []
  type: TYPE_NORMAL
- en: cudaHostAllocMapped));
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostGetDevicePointer( (void **) &foutDevice,
  prefs: []
  type: TYPE_NORMAL
- en: foutHost,
  prefs: []
  type: TYPE_NORMAL
- en: 0 ));
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < N; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: inHost[i] = (T) i;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: memset( foutHost, 0, N*sizeof(float) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaBindTexture( NULL,
  prefs: []
  type: TYPE_NORMAL
- en: tex,
  prefs: []
  type: TYPE_NORMAL
- en: inDevice,
  prefs: []
  type: TYPE_NORMAL
- en: cudaCreateChannelDesc<T>(),
  prefs: []
  type: TYPE_NORMAL
- en: N*sizeof(T)));
  prefs: []
  type: TYPE_NORMAL
- en: TexReadout<<<2,384>>>( foutDevice, N );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaDeviceSynchronize());
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < N; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%.2f ", foutHost[i] );
  prefs: []
  type: TYPE_NORMAL
- en: assert( foutHost[i] == TexPromoteToFloat( (T) i ) );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: printf( "\n" );
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: cudaFreeHost( inHost );
  prefs: []
  type: TYPE_NORMAL
- en: cudaFreeHost( foutHost );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 10.5\. Texturing with Unnormalized Coordinates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All texture intrinsics except `tex1Dfetch()` use floating-point values to specify
    coordinates into the texture. When using *unnormalized coordinates*, they fall
    in the range [0, *MaxDim*), where *MaxDim* is the width, height, or depth of the
    texture. Unnormalized coordinates are an intuitive way to index into a texture,
    but some texturing features are not available when using them.
  prefs: []
  type: TYPE_NORMAL
- en: An easy way to study texturing behavior is to populate a texture with elements
    that contain the index into the texture. [Figure 10.4](ch10.html#ch10fig04) shows
    a float-valued 1D texture with 16 elements, populated by the identity elements
    and annotated with some of the values returned by `tex1D()`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.4* Texturing with unnormalized coordinates (without linear filtering).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Not all texturing features are available with unnormalized coordinates, but
    they can be used in conjunction with *linear filtering* and a limited form of
    *texture addressing*. The texture addressing mode specifies how the hardware should
    deal with out-of-range texture coordinates. For unnormalized coordinates, the
    [Figure 10.4](ch10.html#ch10fig04) illustrates the default texture addressing
    mode of clamping to the range [0, *MaxDim*) before fetching data from the texture:
    The value 16.0 is out of range and clamped to fetch the value 15.0\. Another texture
    addressing option available when using unnormalized coordinates is the “border”
    addressing mode where out-of-range coordinates return zero.'
  prefs: []
  type: TYPE_NORMAL
- en: The default filtering mode, so-called “point filtering,” returns one texture
    element depending on the value of the floating-point coordinate. In contrast,
    linear filtering causes the texture hardware to fetch the two neighboring texture
    elements and linearly interpolate between them, weighted by the texture coordinate.
    [Figure 10.5](ch10.html#ch10fig05) shows the 1D texture with 16 elements, with
    some sample values returned by `tex1D()`. Note that you must add `0.5f` to the
    texture coordinate to get the identity element.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.5* Texturing with unnormalized coordinates (with linear filtering).'
  prefs: []
  type: TYPE_NORMAL
- en: Many texturing features can be used in conjunction with one another; for example,
    linear filtering can be combined with the previously discussed promotion from
    integer to floating point. In that case, the floating-point outputs produced by
    `tex1D()` intrinsics are accurate interpolations between the promoted floating-point
    values of the two participating texture elements.
  prefs: []
  type: TYPE_NORMAL
- en: '*Microdemo:* `tex1d_unnormalized.cu`'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The microdemo `tex1d_unnormalized.cu` is like a microscope to closely examine
    texturing behavior by printing the coordinate and the value returned by the `tex1D()`
    intrinsic together. Unlike the `tex1dfetch_int2float.cu` microdemo, this program
    uses a 1D CUDA array to hold the texture data. A certain number of texture fetches
    is performed, along a range of floating-point values specified by a base and increment;
    the interpolated values and the value returned by `tex1D()` are written together
    into an output array of `float2`. The CUDA kernel is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p325pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: texture<float, 1> tex;
  prefs: []
  type: TYPE_NORMAL
- en: extern "C" __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: TexReadout( float2 *out, size_t N, float base, float increment )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += gridDim.x*blockDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float x = base + (float) i * increment;
  prefs: []
  type: TYPE_NORMAL
- en: out[i].x = x;
  prefs: []
  type: TYPE_NORMAL
- en: out[i].y = tex1D( tex, x );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: A host function `CreateAndPrintTex()`, given in [Listing 10.4](ch10.html#ch10lis04),
    takes the size of the texture to create, the number of texture fetches to perform,
    the base and increment of the floating-point range to pass to `tex1D()`, and optionally
    the filter and addressing modes to use on the texture. This function creates the
    CUDA array to hold the texture data, optionally initializes it with the caller-provided
    data (or identity elements if the caller passes NULL), binds the texture to the
    CUDA array, and prints the `float2` output.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 10.4.* `CreateAndPrintTex().`'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p10lis04a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex( T *initTex, size_t texN, size_t outN,
  prefs: []
  type: TYPE_NORMAL
- en: float base, float increment,
  prefs: []
  type: TYPE_NORMAL
- en: cudaTextureFilterMode filterMode = cudaFilterModePoint,
  prefs: []
  type: TYPE_NORMAL
- en: cudaTextureAddressMode addressMode = cudaAddressModeClamp )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: T *texContents = 0;
  prefs: []
  type: TYPE_NORMAL
- en: cudaArray *texArray = 0;
  prefs: []
  type: TYPE_NORMAL
- en: float2 *outHost = 0, *outDevice = 0;
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc<T>();
  prefs: []
  type: TYPE_NORMAL
- en: // use caller-provided array, if any, to initialize texture
  prefs: []
  type: TYPE_NORMAL
- en: if ( initTex ) {
  prefs: []
  type: TYPE_NORMAL
- en: texContents = initTex;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else {
  prefs: []
  type: TYPE_NORMAL
- en: // default is to initialize with identity elements
  prefs: []
  type: TYPE_NORMAL
- en: texContents = (T *) malloc( texN*sizeof(T) );
  prefs: []
  type: TYPE_NORMAL
- en: if ( ! texContents )
  prefs: []
  type: TYPE_NORMAL
- en: goto Error;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < texN; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: texContents[i] = (T) i;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaMallocArray(&texArray, &channelDesc, texN));
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostAlloc( (void **) &outHost,
  prefs: []
  type: TYPE_NORMAL
- en: outN*sizeof(float2),
  prefs: []
  type: TYPE_NORMAL
- en: cudaHostAllocMapped));
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostGetDevicePointer( (void **)
  prefs: []
  type: TYPE_NORMAL
- en: '&outDevice,'
  prefs: []
  type: TYPE_NORMAL
- en: outHost, 0 ));
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaMemcpyToArray( texArray,
  prefs: []
  type: TYPE_NORMAL
- en: 0, 0,
  prefs: []
  type: TYPE_NORMAL
- en: texContents,
  prefs: []
  type: TYPE_NORMAL
- en: texN*sizeof(T),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice));
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
  prefs: []
  type: TYPE_NORMAL
- en: tex.filterMode = filterMode;
  prefs: []
  type: TYPE_NORMAL
- en: tex.addressMode[0] = addressMode;
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostGetDevicePointer(&outDevice, outHost, 0));
  prefs: []
  type: TYPE_NORMAL
- en: TexReadout<<<2,384>>>( outDevice, outN, base, increment );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaThreadSynchronize());
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < outN; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: printf( "(%.2f, %.2f)\n", outHost[i].x, outHost[i].y );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: printf( "\n" );
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: if ( ! initTex ) free( texContents );
  prefs: []
  type: TYPE_NORMAL
- en: if ( texArray ) cudaFreeArray( texArray );
  prefs: []
  type: TYPE_NORMAL
- en: if ( outHost ) cudaFreeHost( outHost );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The `main()` function for this program is intended to be modified to study texturing
    behavior. This version creates an 8-element texture and writes the output of `tex1D()`
    from 0.0 .. 7.0.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p327pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: main( int argc, char *argv[] )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK(cudaSetDeviceFlags(cudaDeviceMapHost));
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( NULL, 8, 8, 0.0f, 1.0f );
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( NULL, 8, 8, 0.0f, 1.0f, cudaFilterModeLinear );
  prefs: []
  type: TYPE_NORMAL
- en: return 0;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: The output from this program is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p327pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: (0.00, 0.00)    <- output from the first CreateAndPrintTex()
  prefs: []
  type: TYPE_NORMAL
- en: (1.00, 1.00)
  prefs: []
  type: TYPE_NORMAL
- en: (2.00, 2.00)
  prefs: []
  type: TYPE_NORMAL
- en: (3.00, 3.00)
  prefs: []
  type: TYPE_NORMAL
- en: (4.00, 4.00)
  prefs: []
  type: TYPE_NORMAL
- en: (5.00, 5.00)
  prefs: []
  type: TYPE_NORMAL
- en: (6.00, 6.00)
  prefs: []
  type: TYPE_NORMAL
- en: (7.00, 7.00)
  prefs: []
  type: TYPE_NORMAL
- en: (0.00, 0.00)    <- output from the second CreateAndPrintTex()
  prefs: []
  type: TYPE_NORMAL
- en: (1.00, 0.50)
  prefs: []
  type: TYPE_NORMAL
- en: (2.00, 1.50)
  prefs: []
  type: TYPE_NORMAL
- en: (3.00, 2.50)
  prefs: []
  type: TYPE_NORMAL
- en: (4.00, 3.50)
  prefs: []
  type: TYPE_NORMAL
- en: (5.00, 4.50)
  prefs: []
  type: TYPE_NORMAL
- en: (6.00, 5.50)
  prefs: []
  type: TYPE_NORMAL
- en: (7.00, 6.50)
  prefs: []
  type: TYPE_NORMAL
- en: If we change `main()` to invoke `CreateAndPrintTex()` as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p327pro03a)'
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( NULL, 8, 20, 0.9f, 0.01f,
  prefs: []
  type: TYPE_NORMAL
- en: cudaFilterModePoint );
  prefs: []
  type: TYPE_NORMAL
- en: The resulting output highlights that when point filtering, 1.0 is the dividing
    line between texture elements 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p327pro04a)'
  prefs: []
  type: TYPE_NORMAL
- en: (0.90, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (0.91, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (0.92, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (0.93, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (0.94, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (0.95, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (0.96, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (0.97, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (0.98, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (0.99, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (1.00, 1.00)     <- transition point
  prefs: []
  type: TYPE_NORMAL
- en: (1.01, 1.00)
  prefs: []
  type: TYPE_NORMAL
- en: (1.02, 1.00)
  prefs: []
  type: TYPE_NORMAL
- en: (1.03, 1.00)
  prefs: []
  type: TYPE_NORMAL
- en: (1.04, 1.00)
  prefs: []
  type: TYPE_NORMAL
- en: (1.05, 1.00)
  prefs: []
  type: TYPE_NORMAL
- en: (1.06, 1.00)
  prefs: []
  type: TYPE_NORMAL
- en: (1.07, 1.00)
  prefs: []
  type: TYPE_NORMAL
- en: (1.08, 1.00)
  prefs: []
  type: TYPE_NORMAL
- en: (1.09, 1.00)
  prefs: []
  type: TYPE_NORMAL
- en: One limitation of linear filtering is that it is performed with 9-bit weighting
    factors. It is important to realize that the precision of the interpolation depends
    not on that of the texture elements but on the weights. As an example, let’s take
    a look at a 10-element texture initialized with normalized identity elements—that
    is, (0.0, 0.1, 0.2, 0.3, . . . 0.9) instead of (0, 1, 2, . . . 9). `CreateAndPrintTex()`
    lets us specify the texture contents, so we can do so as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p328pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float texData[10];
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < 10; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: texData[i] = (float) i / 10.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( texData, 10, 10, 0.0f, 1.0f );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: The output from an unmodified `CreateAndPrintTex()` looks innocuous enough.
  prefs: []
  type: TYPE_NORMAL
- en: (0.00, 0.00)
  prefs: []
  type: TYPE_NORMAL
- en: (1.00, 0.10)
  prefs: []
  type: TYPE_NORMAL
- en: (2.00, 0.20)
  prefs: []
  type: TYPE_NORMAL
- en: (3.00, 0.30)
  prefs: []
  type: TYPE_NORMAL
- en: (4.00, 0.40)
  prefs: []
  type: TYPE_NORMAL
- en: (5.00, 0.50)
  prefs: []
  type: TYPE_NORMAL
- en: (6.00, 0.60)
  prefs: []
  type: TYPE_NORMAL
- en: (7.00, 0.70)
  prefs: []
  type: TYPE_NORMAL
- en: (8.00, 0.80)
  prefs: []
  type: TYPE_NORMAL
- en: (9.00, 0.90)
  prefs: []
  type: TYPE_NORMAL
- en: Or if we invoke `CreateAndPrintTex()` with linear interpolation between the
    first two texture elements (values `0.1` and `0.2`), we get the following.
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>(tex,10,10,1.5f,0.1f,cudaFilterModeLinear);
  prefs: []
  type: TYPE_NORMAL
- en: The resulting output is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: (1.50, 0.10)
  prefs: []
  type: TYPE_NORMAL
- en: (1.60, 0.11)
  prefs: []
  type: TYPE_NORMAL
- en: (1.70, 0.12)
  prefs: []
  type: TYPE_NORMAL
- en: (1.80, 0.13)
  prefs: []
  type: TYPE_NORMAL
- en: (1.90, 0.14)
  prefs: []
  type: TYPE_NORMAL
- en: (2.00, 0.15)
  prefs: []
  type: TYPE_NORMAL
- en: (2.10, 0.16)
  prefs: []
  type: TYPE_NORMAL
- en: (2.20, 0.17)
  prefs: []
  type: TYPE_NORMAL
- en: (2.30, 0.18)
  prefs: []
  type: TYPE_NORMAL
- en: (2.40, 0.19)
  prefs: []
  type: TYPE_NORMAL
- en: Rounded to 2 decimal places, this data looks very well behaved. But if we modify
    `CreateAndPrintTex()` to output hexadecimal instead, the output becomes
  prefs: []
  type: TYPE_NORMAL
- en: (1.50, 0x3dcccccd)
  prefs: []
  type: TYPE_NORMAL
- en: (1.60, 0x3de1999a)
  prefs: []
  type: TYPE_NORMAL
- en: (1.70, 0x3df5999a)
  prefs: []
  type: TYPE_NORMAL
- en: (1.80, 0x3e053333)
  prefs: []
  type: TYPE_NORMAL
- en: (1.90, 0x3e0f3333)
  prefs: []
  type: TYPE_NORMAL
- en: (2.00, 0x3e19999a)
  prefs: []
  type: TYPE_NORMAL
- en: (2.10, 0x3e240000)
  prefs: []
  type: TYPE_NORMAL
- en: (2.20, 0x3e2e0000)
  prefs: []
  type: TYPE_NORMAL
- en: (2.30, 0x3e386667)
  prefs: []
  type: TYPE_NORMAL
- en: (2.40, 0x3e426667)
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that most fractions of 10 are not exactly representable in floating
    point. Nevertheless, when performing interpolation that does not require high
    precision, these values are interpolated at full precision.
  prefs: []
  type: TYPE_NORMAL
- en: '*Microdemo:* `tex1d_9bit.cu`'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To explore this question of precision, we developed another microdemo, `tex1d_9bit.cu`.
    Here, we’ve populated a texture with 32-bit floating-point values that require
    full precision to represent. In addition to passing the base/increment pair for
    the texture coordinates, another base/increment pair specifies the “expected”
    interpolation value, assuming full-precision interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: In `tex1d_9bit`, the `CreateAndPrintTex()` function is modified to write its
    output as shown in [Listing 10.5](ch10.html#ch10lis05).
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 10.5.* `Tex1d_9bit.cu` (excerpt).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p10lis05a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: printf( "X\tY\tActual Value\tExpected Value\tDiff\n" );
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < outN; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: T expected;
  prefs: []
  type: TYPE_NORMAL
- en: if ( bEmulateGPU ) {
  prefs: []
  type: TYPE_NORMAL
- en: float x = base+(float)i*increment - 0.5f;
  prefs: []
  type: TYPE_NORMAL
- en: float frac = x - (float) (int) x;
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int frac256 = (int) (frac*256.0f+0.5f);
  prefs: []
  type: TYPE_NORMAL
- en: frac = frac256/256.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: int index = (int) x;
  prefs: []
  type: TYPE_NORMAL
- en: expected = (1.0f-frac)*initTex[index] +
  prefs: []
  type: TYPE_NORMAL
- en: frac*initTex[index+1];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else {
  prefs: []
  type: TYPE_NORMAL
- en: expected = expectedBase + (float) i*expectedIncrement;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: float diff = fabsf( outHost[i].y - expected );
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%.2f\t%.2f\t", outHost[i].x, outHost[i].y );
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%08x\t", *(int *) (&outHost[i].y) );
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%08x\t", *(int *) (&expected) );
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%E\n", diff );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: printf( "\n" );
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: For the just-described texture with 10 values (incrementing by 0.1), we can
    use this function to generate a comparison of the actual texture results with
    the expected full-precision result. Calling the function
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p330pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( tex, 10, 4, 1.5f, 0.25f, 0.1f, 0.025f );
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( tex, 10, 4, 1.5f, 0.1f, 0.1f, 0.01f );
  prefs: []
  type: TYPE_NORMAL
- en: yields this output.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p330pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: X     Y     Actual Value  Expected Value  Diff
  prefs: []
  type: TYPE_NORMAL
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: 1.75  0.12  3e000000      3e000000        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: 2.00  0.15  3e19999a      3e19999a        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: 2.25  0.17  3e333333      3e333333        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: X     Y     Actual Value  Expected Value  Diff
  prefs: []
  type: TYPE_NORMAL
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: 1.60  0.11  3de1999a      3de147ae        1.562536E-04
  prefs: []
  type: TYPE_NORMAL
- en: 1.70  0.12  3df5999a      3df5c290        7.812679E-05
  prefs: []
  type: TYPE_NORMAL
- en: 1.80  0.13  3e053333      3e051eb8        7.812679E-05
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the “Diff” column on the right, the first set of outputs
    were interpolated at full precision, while the second were not. The explanation
    for this difference lies in Appendix F of the *CUDA Programming Guide*, which
    describes how linear interpolation is performed for 1D textures.
  prefs: []
  type: TYPE_NORMAL
- en: '*tex*(*x*) = (1 - α)*T*(*i*) + α*T*(*i* + 1)'
  prefs: []
  type: TYPE_NORMAL
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '*i* = *floor*(*X*[*B*]), α + *frac*(*X*[*B*]), *X*[*B*] = *x* - 0.5'
  prefs: []
  type: TYPE_NORMAL
- en: and α is stored in a 9-bit fixed-point format with 8 bits of fractional value.
  prefs: []
  type: TYPE_NORMAL
- en: In [Listing 10.5](ch10.html#ch10lis05), this computation in C++ is emulated
    in the `bEmulateGPU` case. The code snippet to emulate 9-bit weights can be enabled
    in `tex1d_9bit.cu` by passing `true` as the `bEmulateGPU` parameter of `CreateAndPrintTex()`.
    The output then becomes
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p331pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: X     Y     Actual Value  Expected Value  Diff
  prefs: []
  type: TYPE_NORMAL
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: 1.75  0.12  3e000000      3e000000        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: 2.00  0.15  3e19999a      3e19999a        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: 2.25  0.17  3e333333      3e333333        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: X     Y     Actual Value  Expected Value  Diff
  prefs: []
  type: TYPE_NORMAL
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: 1.60  0.11  3de1999a      3de1999a        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: 1.70  0.12  3df5999a      3df5999a        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: 1.80  0.13  3e053333      3e053333        0.000000E+00
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the rightmost column of 0’s, when computing the interpolated
    value with 9-bit precision, the differences between “expected” and “actual” output
    disappear.
  prefs: []
  type: TYPE_NORMAL
- en: 10.6\. Texturing with Normalized Coordinates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When texturing with normalized coordinates, the texture is addressed by coordinates
    in the range `[0.0, 1.0)` instead of the range `[0, MaxDim)`. For a 1D texture
    with 16 elements, the normalized coordinates are as in [Figure 10.6](ch10.html#ch10fig06).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.6* Texturing with normalized coordinates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than having texture coordinates that are independent of the texture dimension,
    the texture is dealt with in largely the same way, except that the full range
    of CUDA’s texturing capabilities become available. With normalized coordinates,
    more texture addressing mode besides clamp and border addressing becomes available:
    the *wrap* and *mirror* addressing modes, whose formulas are as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/331tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The four texture addressing modes supported in CUDA in [Figure 10.7](ch10.html#ch10fig07)
    show which in-range texture element is fetched by the first two out-of-range coordinates
    on each end. If you are having trouble visualizing the behavior of these addressing
    modes, check out the `tex2d_opengl.cu` microdemo in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.7* Texture addressing modes.'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: In the driver API, changes to the texture reference are codified by the `cuTexRefSetArray()`
    or `cuTexRefSetAddress()` function. In other words, calls to functions that make
    state changes, such as `cuTexRefSetFilterMode()` or `cuTexRefSetAddressMode()`,
    have no effect until the texture reference is bound to memory.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Floating-Point Coordinates with 1D Device Memory
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For applications that wish to use floating-point coordinates to address the
    texture or use texturing features that are only available for normalized coordinates,
    use `cudaBindTexture2D() / cuTexRefSetAddress2D()` to specify the base address.
    Specify a height of 1 and pitch of `N*sizeof(T)`. The kernel can then call `tex2D(x,0.0f)`
    to read the 1D texture with floating-point coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: 10.7\. 1D Surface Read/Write
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Until SM 2.0 hardware became available, CUDA kernels could access the contents
    of CUDA arrays only via texturing. Other access to CUDA arrays, including all
    write access, could be performed only via memcpy functions such as `cudaMemcpyToArray()`.
    The only way for CUDA kernels to both texture from and write to a given region
    of memory was to bind the texture reference to linear device memory.
  prefs: []
  type: TYPE_NORMAL
- en: But with the surface read/write functions newly available in SM 2.x, developers
    can bind CUDA arrays to *surface references* and use the `surf1Dread()` and `surf1Dwrite()`
    intrinsics to read and write the CUDA arrays from a kernel. Unlike texture reads,
    which have dedicated cache hardware, these reads and writes go through the same
    L2 cache as global loads and stores.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In order for a surface reference to be bound to a CUDA array, the CUDA array
    must have been created with the `cudaArraySurfaceLoadStore` flag.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The 1D surface read/write intrinsics are declared as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p333pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: template<class Type> Type surf1Dread(surface<void, 1> surfRef, int x,
  prefs: []
  type: TYPE_NORMAL
- en: boundaryMode = cudaBoundaryModeTrap);
  prefs: []
  type: TYPE_NORMAL
- en: template<class Type> void surf1Dwrite(Type data, surface<void, 1>
  prefs: []
  type: TYPE_NORMAL
- en: surfRef, int x, boundaryMode = cudaBoundaryModeTrap);
  prefs: []
  type: TYPE_NORMAL
- en: These intrinsics are not type-strong—as you can see, surface references are
    declared as `void`—and the size of the memory transaction depends on `sizeof(Type)`
    for a given invocation of `surf1Dread()` or `surf1Dwrite()`. The x offset is in
    bytes and must be naturally aligned with respect to `sizeof(Type)`. For 4-byte
    operands such as `int` or `float`, `offset` must be evenly divisible by 4, for
    `short` it must be divisible by 2, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Support for surface read/write is far less rich than texturing functionality.^([5](ch10.html#ch10fn5))
    Only unformatted reads and writes are supported, with no conversion or interpolation
    functions, and the border handling is restricted to only two modes.
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch10.html#ch10fn5a). In fact, CUDA could have bypassed implementation of
    surface references entirely, with the intrinsics operating directly on CUDA arrays.
    Surface references were included for orthogonality with texture references to
    provide for behavior defined on a per-surfref basis as opposed to per-instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: Boundary conditions are handled differently for surface read/write than for
    texture reads. For textures, this behavior is controlled by the addressing mode
    in the texture reference. For surface read/write, the method of handling out-of-range
    `offset` values is specified as a parameter of `surf1Dread()` or `surf1Dwrite()`.
    Out-of-range indices can either cause a hardware exception (`cudaBoundaryModeTrap`)
    or read as 0 for `surf1Dread()` and are ignored for `surf1Dwrite()`(`cudaBoundaryModeZero`).
  prefs: []
  type: TYPE_NORMAL
- en: Because of the untyped character of surface references, it is easy to write
    a templated 1D memset routine that works for all types.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p334pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: surface<void, 1> surf1D;
  prefs: []
  type: TYPE_NORMAL
- en: template <typename T>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: surf1Dmemset( int index, T value, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: surf1Dwrite( value, surf1D, (index+i)*sizeof(T) );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: This kernel is in the microdemo `surf1Dmemset.cu`, which creates a 64-byte CUDA
    array for illustrative purposes, initializes it with the above kernel, and prints
    the array in float and integer forms.
  prefs: []
  type: TYPE_NORMAL
- en: A generic template host function wraps this kernel with a call to `cudaBindSurfaceToArray().`
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p334pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: template<typename T>
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t
  prefs: []
  type: TYPE_NORMAL
- en: surf1Dmemset( cudaArray *array, int offset, T value, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaBindSurfaceToArray(surf1D, array));
  prefs: []
  type: TYPE_NORMAL
- en: surf1Dmemset_kernel<<<2,384>>>( 0, value, 4*NUM_VALUES );
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: return status;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: The untyped character of surface references makes this template structure much
    easier to pull off than for textures. Because texture references are both type-strong
    and global, they cannot be templatized in the parameter list of a would-be generic
    function. A one-line change from
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(surf1Dmemset(array, 0, 3.141592654f, NUM_VALUES));
  prefs: []
  type: TYPE_NORMAL
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(surf1Dmemset(array, 0, (short) 0xbeef, 2*NUM_VALUES));
  prefs: []
  type: TYPE_NORMAL
- en: will change the output of this program from
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p335pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: 0x40490fdb 0x40490fdb ... (16 times)
  prefs: []
  type: TYPE_NORMAL
- en: 3.141593E+00 3.141593E+00 ... (16 times)
  prefs: []
  type: TYPE_NORMAL
- en: to
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p335pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: 0xbeefbeef 0xbeefbeef ... (16 times)
  prefs: []
  type: TYPE_NORMAL
- en: -4.68253E-01 -4.68253E-01 ... (16 times)
  prefs: []
  type: TYPE_NORMAL
- en: 10.8\. 2D Texturing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In most ways, 2D texturing is similar to 1D texturing as described above. Applications
    optionally may promote integer texture elements to floating point, and they can
    use unnormalized or normalized coordinates. When linear filtering is supported,
    bilinear filtering is performed between four texture values, weighted by the fractional
    bits of the texture coordinates. The hardware can perform a different addressing
    mode for each dimension. For example, the X coordinate can be clamped while the
    Y coordinate is wrapped.
  prefs: []
  type: TYPE_NORMAL
- en: '10.8.1\. Microdemo: `tex2d_opengl.cu`'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This microdemo graphically illustrates the effects of the different texturing
    modes. It uses OpenGL for portability and the GL Utility Library (GLUT) to minimize
    the amount of setup code. To keep distractions to a minimum, this application
    does not use CUDA’s OpenGL interoperability functions. Instead, we allocate mapped
    host memory and render it to the frame buffer using `glDrawPixels()`. To OpenGL,
    the data might as well be coming from the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: The application supports normalized and unnormalized coordinates and clamp,
    wrap, mirror, and border addressing in both the X and Y directions. For unnormalized
    coordinates, the following kernel is used to write the texture contents into the
    output buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p336pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: RenderTextureUnnormalized( uchar4 *out, int width, int height )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int row = blockIdx.x; row < height; row += gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: out = (uchar4 *) (((char *) out)+row*4*width);
  prefs: []
  type: TYPE_NORMAL
- en: for ( int col = threadIdx.x; col < width; col += blockDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[col] = tex2D( tex2d, (float) col, (float) row );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: This kernel fills the rectangle of `width` × `height` pixels with values read
    from the texture using texture coordinates corresponding to the pixel locations.
    For out-of-range pixels, you can see the effects of the clamp and border addressing
    modes.
  prefs: []
  type: TYPE_NORMAL
- en: For normalized coordinates, the following kernel is used to write the texture
    contents into the output buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p336pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: RenderTextureNormalized(
  prefs: []
  type: TYPE_NORMAL
- en: uchar4 *out,
  prefs: []
  type: TYPE_NORMAL
- en: int width,
  prefs: []
  type: TYPE_NORMAL
- en: int height,
  prefs: []
  type: TYPE_NORMAL
- en: int scale )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = blockIdx.x; j < height; j += gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: int row = height-j-1;
  prefs: []
  type: TYPE_NORMAL
- en: out = (uchar4 *) (((char *) out)+row*4*width);
  prefs: []
  type: TYPE_NORMAL
- en: float texRow = scale * (float) row / (float) height;
  prefs: []
  type: TYPE_NORMAL
- en: float invWidth = scale / (float) width;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int col = threadIdx.x; col < width; col += blockDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: float texCol = col * invWidth;
  prefs: []
  type: TYPE_NORMAL
- en: out[col] = tex2D( tex2d, texCol, texRow );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: The scale parameter specifies the number of times to tile the texture into the
    output buffer. By default, scale=1.0, and the texture is seen only once. When
    running the application, you can hit the 1–9 keys to replicate the texture that
    many times. The C, W, M, and B keys set the addressing mode for the current direction;
    the X and Y keys specify the current direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/337tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Readers are encouraged to run the program, or especially to modify and run the
    program, to see the effects of different texturing settings. [Figure 10.8](ch10.html#ch10fig08)
    shows the output of the program for the four permutations of X Wrap/Mirror and
    Y Wrap/Mirror when replicating the texture five times.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.8* Wrap and mirror addressing modes.'
  prefs: []
  type: TYPE_NORMAL
- en: '10.9\. 2D Texturing: Copy Avoidance'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When CUDA was first introduced, CUDA kernels could read from CUDA arrays only
    via texture. Applications could write to CUDA arrays only with memory copies;
    in order for CUDA kernels to write data that would then be read through texture,
    they had to write to device memory and then perform a device→array memcpy. Since
    then, two mechanisms have been added that remove this step for 2D textures.
  prefs: []
  type: TYPE_NORMAL
- en: • A 2D texture can be bound to a pitch-allocated range of linear device memory.
  prefs: []
  type: TYPE_NORMAL
- en: • Surface load/store intrinsics enable CUDA kernels to write to CUDA arrays
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: 3D texturing from device memory and 3D surface load/store are not supported.
  prefs: []
  type: TYPE_NORMAL
- en: For applications that read most or all the texture contents with a regular access
    pattern (such as a video codec) or applications that must work on Tesla-class
    hardware, it is best to keep the data in device memory. For applications that
    perform random (but localized) access when texturing, it is probably best to keep
    the data in CUDA arrays and use surface read/write intrinsics.
  prefs: []
  type: TYPE_NORMAL
- en: 10.9.1\. 2D Texturing from Device Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Texturing from 2D device memory does not have any of the benefits of “block
    linear” addressing—a cache line fill into the texture cache pulls in a horizontal
    span of texels, not a 2D or 3D block of them—but unless the application performs
    random access into the texture, the benefits of avoiding a copy from device memory
    to a CUDA array likely outweigh the penalties of losing block linear addressing.
  prefs: []
  type: TYPE_NORMAL
- en: To bind a 2D texture reference to a device memory range, call `cudaBindTexture2D().`
  prefs: []
  type: TYPE_NORMAL
- en: cudaBindTexture2D(
  prefs: []
  type: TYPE_NORMAL
- en: NULL,
  prefs: []
  type: TYPE_NORMAL
- en: '&tex,'
  prefs: []
  type: TYPE_NORMAL
- en: texDevice,
  prefs: []
  type: TYPE_NORMAL
- en: '&channelDesc,'
  prefs: []
  type: TYPE_NORMAL
- en: inWidth,
  prefs: []
  type: TYPE_NORMAL
- en: inHeight,
  prefs: []
  type: TYPE_NORMAL
- en: texPitch );
  prefs: []
  type: TYPE_NORMAL
- en: The above call binds the texture reference `tex` to the 2D device memory range
    given by `texDevice / texPitch`. The base address and pitch must conform to hardware-specific
    alignment constraints.^([6](ch10.html#ch10fn6)) The base address must be aligned
    with respect to `cudaDeviceProp.textureAlignment`, and the pitch must be aligned
    with respect to `cudaDeviceProp.texturePitchAlignment`.^([7](ch10.html#ch10fn7))
    The microdemo `tex2d_addressing_device.cu` is identical to `tex2d_addressing.cu`,
    but it uses device memory to hold the texture data. The two programs are designed
    to be so similar that you can look at the differences. A device pointer/pitch
    tuple is declared instead of a CUDA array.
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch10.html#ch10fn6a). CUDA arrays must conform to the same constraints,
    but in that case, the base address and pitch are managed by CUDA and hidden along
    with the memory layout.'
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch10.html#ch10fn7a). In the driver API, the corresponding device attribute
    queries are `CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT` and `CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT`.'
  prefs: []
  type: TYPE_NORMAL
- en: < cudaArray *texArray = 0;
  prefs: []
  type: TYPE_NORMAL
- en: T *texDevice = 0;
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: size_t texPitch;
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`cudaMallocPitch()` is called instead of calling `cudaMallocArray()`. `cudaMallocPitch()`
    delegates selection of the base address and pitch to the driver, so the code will
    continue working on future generations of hardware (which have a tendency to increase
    alignment requirements).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p339pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: < CUDART_CHECK(cudaMallocArray( &texArray,
  prefs: []
  type: TYPE_NORMAL
- en: < &channelDesc,
  prefs: []
  type: TYPE_NORMAL
- en: < inWidth,
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaMallocPitch( &texDevice,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '&texPitch,'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: inWidth*sizeof(T),
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: inHeight));
  prefs: []
  type: TYPE_NORMAL
- en: Next, `cudaTextureBind2D()` is called instead of `cudaBindTextureToArray()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p339pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: < CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaBindTexture2D( NULL,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '&tex,'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: texDevice,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '&channelDesc,'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: inWidth,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: inHeight,
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: texPitch ));
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The final difference is that instead of freeing the CUDA array, `cudaFree()`
    is called on the pointer returned by `cudaMallocPitch()`.
  prefs: []
  type: TYPE_NORMAL
- en: < cudaFreeArray( texArray );
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( texDevice );
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 10.9.2\. 2D Surface Read/Write
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As with 1D surface read/write, Fermi-class hardware enables kernels to write
    directly into CUDA arrays with intrinsic surface read/write functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p340pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: template<class Type> Type surf2Dread(surface<void, 1> surfRef, int x,
  prefs: []
  type: TYPE_NORMAL
- en: int y, boundaryMode = cudaBoundaryModeTrap);
  prefs: []
  type: TYPE_NORMAL
- en: template<class Type> Type surf2Dwrite(surface<void, 1> surfRef, Type
  prefs: []
  type: TYPE_NORMAL
- en: data, int x, int y, boundaryMode = cudaBoundaryModeTrap);
  prefs: []
  type: TYPE_NORMAL
- en: The surface reference declaration and corresponding CUDA kernel for 2D surface
    memset, given in `surf2Dmemset.cu`, is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p340pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: surface<void, 2> surf2D;
  prefs: []
  type: TYPE_NORMAL
- en: template<typename T>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: surf2Dmemset_kernel( T value,
  prefs: []
  type: TYPE_NORMAL
- en: int xOffset, int yOffset,
  prefs: []
  type: TYPE_NORMAL
- en: int Width, int Height )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int row = blockIdx.y*blockDim.y + threadIdx.y;
  prefs: []
  type: TYPE_NORMAL
- en: row < Height;
  prefs: []
  type: TYPE_NORMAL
- en: row += blockDim.y*gridDim.y )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int col = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: col < Width;
  prefs: []
  type: TYPE_NORMAL
- en: col += blockDim.x*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: surf2Dwrite( value,
  prefs: []
  type: TYPE_NORMAL
- en: surf2D,
  prefs: []
  type: TYPE_NORMAL
- en: (xOffset+col)*sizeof(T),
  prefs: []
  type: TYPE_NORMAL
- en: yOffset+row );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the X offset parameter to `surf2Dwrite()` is given in *bytes*.
  prefs: []
  type: TYPE_NORMAL
- en: 10.10\. 3D Texturing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reading from 3D textures is similar to reading from 2D textures, but there are
    more limitations.
  prefs: []
  type: TYPE_NORMAL
- en: • 3D textures have smaller limits (2048x2048x2048 instead of 65536x32768).
  prefs: []
  type: TYPE_NORMAL
- en: '• There are no copy avoidance strategies: CUDA does not support 3D texturing
    from device memory or surface load/store on 3D CUDA arrays.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than that, the differences are straightforward: Kernels can read from
    3D textures using a `tex3D()` intrinsic that takes 3 floating-point parameters,
    and the underlying 3D CUDA arrays must be populated by 3D memcpys. Trilinear filtering
    is supported; 8 texture elements are read and interpolated according to the texture
    coordinates, with the same 9-bit precision limit as 1D and 2D texturing.'
  prefs: []
  type: TYPE_NORMAL
- en: The 3D texture size limits may be queried by calling `cuDeviceGetAttribute()`
    with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH`, `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT`,
    and `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH`, or by calling `cudaGetDeviceProperties()`
    and examining `cudaDeviceProp.maxTexture3D`. Due to the much larger number of
    parameters needed, 3D CUDA arrays must be created and manipulated using a different
    set of APIs than 1D or 2D CUDA arrays.
  prefs: []
  type: TYPE_NORMAL
- en: To create a 3D CUDA array, the `cudaMalloc3DArray()` function takes a `cudaExtent`
    structure instead of width and height parameters.
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMalloc3DArray(struct cudaArray** array, const struct cudaChannelFormatDesc*
    desc, struct cudaExtent extent, unsigned int flags __dv(0));
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaExtent` is defined as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaExtent {
  prefs: []
  type: TYPE_NORMAL
- en: size_t width;
  prefs: []
  type: TYPE_NORMAL
- en: size_t height;
  prefs: []
  type: TYPE_NORMAL
- en: size_t depth;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: Describing 3D memcpy operations is sufficiently complicated that both the CUDA
    runtime and the driver API use structures to specify the parameters. The runtime
    API uses the `cudaMemcpy3DParams` structure, which is declared as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p341pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaMemcpy3DParms {
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaArray *srcArray;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPos srcPos;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr srcPtr;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaArray *dstArray;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPos dstPos;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr dstPtr;
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaExtent extent;
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaMemcpyKind kind;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of these structure members are themselves structures: `extent` gives the
    width, height, and depth of the copy. The `srcPos` and `dstPos` members are `cudaPos`
    structures that specify the start points for the source and destination of the
    copy.'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPos {
  prefs: []
  type: TYPE_NORMAL
- en: size_t x;
  prefs: []
  type: TYPE_NORMAL
- en: size_t y;
  prefs: []
  type: TYPE_NORMAL
- en: size_t z;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: The `cudaPitchedPtr` is a structure that was added with 3D memcpy to contain
    a pointer/pitch tuple.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p342pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: void *ptr; /**< Pointer to allocated memory */
  prefs: []
  type: TYPE_NORMAL
- en: size_t pitch; /**< Pitch of allocated memory in bytes */
  prefs: []
  type: TYPE_NORMAL
- en: size_t xsize; /**< Logical width of allocation in elements */
  prefs: []
  type: TYPE_NORMAL
- en: size_t ysize; /**< Logical height of allocation in elements */
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: A `cudaPitchedPtr` structure may be created with the function `make_cudaPitchedPtr`,
    which takes the base pointer, pitch, and logical width and height of the allocation.
    `make_cudaPitchedPtr` just copies its parameters into the output struct; however,
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p342pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr
  prefs: []
  type: TYPE_NORMAL
- en: make_cudaPitchedPtr(void *d, size_t p, size_t xsz, size_t ysz)
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr s;
  prefs: []
  type: TYPE_NORMAL
- en: s.ptr = d;
  prefs: []
  type: TYPE_NORMAL
- en: s.pitch = p;
  prefs: []
  type: TYPE_NORMAL
- en: s.xsize = xsz;
  prefs: []
  type: TYPE_NORMAL
- en: s.ysize = ysz;
  prefs: []
  type: TYPE_NORMAL
- en: return s;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: The `simpleTexture3D` sample in the SDK illustrates how to do 3D texturing with
    CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: 10.11\. Layered Textures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Layered textures* are known in the graphics world as *texture arrays* because
    they enable 1D or 2D textures to be arranged as arrays accessed by an integer
    index. The main advantage of layered textures over vanilla 2D or 3D textures is
    that they support larger extents within the slices. There is no performance advantage
    to using layered textures.'
  prefs: []
  type: TYPE_NORMAL
- en: Layered textures are laid out in memory differently than 2D or 3D textures,
    in such a way that 2D or 3D textures will not perform as well if they use the
    layout optimized for layered textures. As a result, when creating the CUDA array,
    you must specify `cudaArrayLayered` to `cudaMalloc3DArray()` or specify `CUDA_ARRAY3D_LAYERED`
    to `cuArray3DCreate()`. The `simpleLayeredTexture` sample in the SDK illustrates
    how to use layered textures.
  prefs: []
  type: TYPE_NORMAL
- en: 10.11.1\. 1D Layered Textures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The 1D layered texture size limits may be queried by calling `cuDeviceGet-Attribute()`
    with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_WIDTH` and `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_LAYERS`
    or by calling `cudaGetDeviceProperties()` and examining `cudaDeviceProp.maxTexture1DLayered`.
  prefs: []
  type: TYPE_NORMAL
- en: 10.11.2\. 2D Layered Textures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The 2D layered texture size limits may be queried by calling `cuDeviceGet-Attribute()`
    with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH` and `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT`
    or `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS` or by calling `cudaGetDeviceProperties()`
    and examining `cudaDeviceProp.maxTexture2DLayered`. The layered texture size limits
    may be queried `cudaGetDeviceProperties()` and examining `cudaDeviceProp.maxTexture2DLayered`.
  prefs: []
  type: TYPE_NORMAL
- en: 10.12\. Optimal Block Sizing and Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the texture coordinates are generated in the “obvious” way, such as in
    `tex2d_addressing.cu`
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p343pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: row = blockIdx.y*blockDim.y + threadIdx.y;
  prefs: []
  type: TYPE_NORMAL
- en: col = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: '... tex2D( tex, (float) col, (float) row);'
  prefs: []
  type: TYPE_NORMAL
- en: then texturing performance is dependent on the block size.
  prefs: []
  type: TYPE_NORMAL
- en: To find the optimal size of a thread block, the `tex2D_shmoo.cu` and `surf2Dmemset_shmoo.cu`
    programs time the performance of thread blocks whose width and height vary from
    4..64, inclusive. Some combinations of these thread block sizes are not valid
    because they have too many threads.
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, the texturing kernel is designed to do as little work as
    possible (maximizing exposure to the performance of the texture hardware), while
    still “fooling” the compiler into issuing the code. Each thread computes the floating-point
    sum of the values it reads and writes the sum if the output parameter is non-NULL.
    The trick is that we never pass a non-NULL pointer to this kernel! The reason
    the kernel is structured this way is because if it never wrote any output, the
    compiler would see that the kernel was not doing any work and would emit code
    that did not perform the texturing operations at all.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p344pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: extern "C" __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: TexSums( float *out, size_t Width, size_t Height )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float sum = 0.0f;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int row = blockIdx.y*blockDim.y + threadIdx.y;
  prefs: []
  type: TYPE_NORMAL
- en: row < Height;
  prefs: []
  type: TYPE_NORMAL
- en: row += blockDim.y*gridDim.y )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int col = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: col < Width;
  prefs: []
  type: TYPE_NORMAL
- en: col += blockDim.x*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: sum += tex2D( tex, (float) col, (float) row );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if ( out ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[blockIdx.x*blockDim.x+threadIdx.x] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Even with our “trick,” there is a risk that the compiler will emit code that
    checks the `out` parameter and exits the kernel early if it’s equal to NULL. We’d
    have to synthesize some output that wouldn’t affect performance too much (for
    example, have each thread block compute the reduction of the sums in shared memory
    and write them to `out`). But by compiling the program with the `--keep` option
    and using `cuobjdump ---dump-sass` to examine the microcode, we can see that the
    compiler doesn’t check `out` until after the doubly-nested `for` loop as executed.
  prefs: []
  type: TYPE_NORMAL
- en: 10.12.1\. Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On a GeForce GTX 280 (GT200), the optimal block size was found to be 128 threads,
    which delivered 35.7G/s of bandwidth. Thread blocks of size 32W × 4H were about
    the same speed as 16W × 8H or 8W × 16H, all traversing a 4K × 4K texture of `float`
    in 1.88 ms. On a Tesla M2050, the optimal block size was found to be 192 threads,
    which delivered 35.4G/s of bandwidth. As with the GT200, different-sized thread
    blocks were the same speed, with 6W × 32H, 16W × 12H, and 8W × 24H blocks delivering
    about the same performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The shmoo over 2D surface memset was less conclusive: Block sizes of at least
    128 threads generally had good performance, provided the thread count was evenly
    divisible by the warp size of 32\. The fastest 2D surface memset performance reported
    on a `cg1.4xlarge` without ECC enabled was 48Gb/s.'
  prefs: []
  type: TYPE_NORMAL
- en: For `float`-valued data for both boards we tested, the peak bandwidth numbers
    reported by texturing and surface write are about ¼ and ½ of the achievable peaks
    for global load/store, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 10.13\. Texturing Quick References
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 10.13.1\. Hardware Capabilities
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hardware Limits
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/345tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Queries—Driver API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Most of the hardware limits listed above can be queried with `cuDevice-Attribute()`,
    which may be called with the following values to query.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/346tab01.jpg)![Image](graphics/346tab01a.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Queries—CUDA Runtime
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The following members of `cudaDeviceProp` contain hardware limits as listed
    above.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/347tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 10.13.2\. CUDA Runtime
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 1D Textures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/347tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 2D Textures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/347tab03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 3D Textures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/348tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 1D Layered Textures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/348tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 2D Layered Textures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/348tab03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 10.13.3\. Driver API
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 1D Textures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/349tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The texture size limit for device memory is not queryable; it is 2^(27) elements
    on all CUDA-capable GPUs. The texture size limit for 1D CUDA arrays may be queried
    by calling `cuDeviceGetAttribute()` with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH`.
  prefs: []
  type: TYPE_NORMAL
- en: 2D Textures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/349tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 3D Textures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/350tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 1D Layered Textures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/350tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 2D Layered Textures
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/350tab03.jpg)'
  prefs: []
  type: TYPE_IMG
