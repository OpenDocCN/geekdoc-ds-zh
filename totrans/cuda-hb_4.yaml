- en: Part II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分
- en: Chapter 5\. Memory
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五章 内存
- en: To maximize performance, CUDA uses different types of memory, depending on the
    expected usage. *Host memory* refers to the memory attached to the CPU(s) in the
    system. CUDA provides APIs that enable faster access to host memory by page-locking
    and mapping it for the GPU(s). *Device memory* is attached to the GPU and accessed
    by a dedicated memory controller, and, as every beginning CUDA developer knows,
    data must be copied explicitly between host and device memory in order to be processed
    by the GPU.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化性能，CUDA 根据预期的使用情况使用不同类型的内存。*主机内存*指的是系统中附加在 CPU 上的内存。CUDA 提供了 API，通过页面锁定和将其映射到
    GPU(s) 上，从而实现更快的主机内存访问。*设备内存*附加在 GPU 上，并通过专用的内存控制器进行访问，正如每个初学者 CUDA 开发者所知道的，数据必须显式地在主机内存和设备内存之间复制，才能被
    GPU 处理。
- en: Device memory can be allocated and accessed in a variety of ways.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 设备内存可以通过多种方式进行分配和访问。
- en: • *Global memory* may be allocated statically or dynamically and accessed via
    pointers in CUDA kernels, which translate to global load/store instructions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: • *全局内存*可以静态或动态分配，并通过 CUDA 内核中的指针进行访问，这些指针会转换为全局加载/存储指令。
- en: • *Constant memory* is read-only memory accessed via different instructions
    that cause the read requests to be serviced by a cache hierarchy optimized for
    broadcast to multiple threads.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: • *常量内存*是只读内存，通过不同的指令进行访问，这些指令使得读取请求由优化用于广播到多个线程的缓存层次结构处理。
- en: '• *Local memory* contains the stack: local variables that cannot be held in
    registers, parameters, and return addresses for subroutines.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: • *本地内存*包含堆栈：无法存放在寄存器中的局部变量、参数和子程序的返回地址。
- en: • *Texture memory* (in the form of CUDA arrays) is accessed via texture and
    surface load/store instructions. Like constant memory, read requests from texture
    memory are serviced by a separate cache that is optimized for read-only access.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: • *纹理内存*（以 CUDA 数组的形式）通过纹理和表面加载/存储指令进行访问。与常量内存类似，纹理内存的读取请求由一个单独的缓存处理，该缓存经过优化以供只读访问。
- en: '*Shared memory* is an important type of memory in CUDA that is *not* backed
    by device memory. Instead, it is an abstraction for an on-chip “scratchpad” memory
    that can be used for fast data interchange between threads within a block. Physically,
    shared memory comes in the form of built-in memory on the SM: On SM 1.x hardware,
    shared memory is implemented with a 16K RAM; on SM 2.x and more recent hardware,
    shared memory is implemented using a 64K cache that may be partitioned as 48K
    L1/16K shared, or 48K shared/16K L1.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*共享内存*是 CUDA 中一种重要的内存类型，它*不是*由设备内存支持。相反，它是一个抽象，代表了一个片上“临时存储区”内存，用于线程之间在块内进行快速数据交换。从物理上讲，共享内存以内置内存的形式存在于
    SM 上：在 SM 1.x 硬件上，共享内存通过 16K RAM 实现；在 SM 2.x 及更新的硬件上，共享内存通过 64K 缓存实现，可以划分为 48K
    L1/16K 共享，或 48K 共享/16K L1。'
- en: 5.1\. Host Memory
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 主机内存
- en: In CUDA, *host memory* refers to memory accessible to the CPU(s) in the system.
    By default, this memory is *pageable*, meaning the operating system may move the
    memory or evict it out to disk. Because the physical location of pageable memory
    may change without notice, it cannot be accessed by peripherals like GPUs. To
    enable “direct memory access” (DMA) by hardware, operating systems allow host
    memory to be “page-locked,” and for performance reasons, CUDA includes APIs that
    make these operating system facilities available to application developers. So-called
    *pinned memory* that has been page-locked and mapped for direct access by CUDA
    GPU(s) enables
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，*主机内存*是指系统中可以被CPU访问的内存。默认情况下，这些内存是*可分页*的，这意味着操作系统可能会将其移到其他位置或驱逐到磁盘。由于可分页内存的物理位置可能会在不通知的情况下发生变化，GPU等外设无法访问它。为了使硬件能够进行“直接内存访问”（DMA），操作系统允许将主机内存“锁页”，为了性能优化，CUDA提供了API，允许应用程序开发者利用这些操作系统功能。所谓的*锁页内存*，即经过锁页处理并为CUDA
    GPU(s)提供直接访问的内存，使得
- en: • Faster transfer performance
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: • 更快的传输性能
- en: • Asynchronous memory copies (i.e., memory copies that return control to the
    caller before the memory copy necessarily has finished; the GPU does the copy
    in parallel with the CPU)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: • 异步内存复制（即内存复制在复制操作完成之前返回控制给调用者；GPU与CPU并行执行复制操作）
- en: • Mapped pinned memory that can be accessed directly by CUDA kernels
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: • 可由CUDA内核直接访问的映射锁页内存
- en: Because the virtual→physical mapping for pageable memory can change unpredictably,
    GPUs cannot access pageable memory at all. CUDA copies pageable memory using a
    pair of *staging buffers* of pinned memory that are allocated by the driver when
    a CUDA context is allocated. [Chapter 6](ch06.html#ch06) includes hand-crafted
    pageable memcpy routines that use CUDA events to do the synchronization needed
    to manage this double-buffering.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可分页内存的虚拟→物理映射可能会不可预测地变化，GPU根本无法访问可分页内存。CUDA通过一对*暂存缓冲区*来复制可分页内存，这些缓冲区是由驱动程序在分配CUDA上下文时分配的。[第6章](ch06.html#ch06)包括手工编写的可分页memcpy例程，利用CUDA事件来完成所需的同步操作，以管理这种双缓冲。
- en: 5.1.1\. Allocating Pinned Memory
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 分配锁页内存
- en: 'Pinned memory is allocated and freed using special functions provided by CUDA:
    `cudaHostAlloc()/cudaFreeHost()` for the CUDA runtime, and `cuMemHostAlloc()/cuMemFreeHost()`
    for the driver API. These functions work with the host operating system to allocate
    page-locked memory and map it for DMA by the GPU(s).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 锁页内存的分配和释放使用CUDA提供的特殊函数：对于CUDA运行时使用`cudaHostAlloc()/cudaFreeHost()`，对于驱动程序API使用`cuMemHostAlloc()/cuMemFreeHost()`。这些函数与主机操作系统协作，分配锁页内存并为GPU(s)的DMA映射该内存。
- en: CUDA keeps track of memory it has allocated and transparently accelerates memory
    copies that involve host pointers allocated with `cuMemHostAlloc()/cudaHostAlloc()`.
    Additionally, some functions (notably the asynchronous memcpy functions) require
    pinned memory.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA跟踪它已分配的内存，并透明地加速涉及使用`cuMemHostAlloc()/cudaHostAlloc()`分配的主机指针的内存复制。此外，一些函数（特别是异步memcpy函数）需要锁页内存。
- en: The `bandwidthTest` SDK sample enables developers to easily compare the performance
    of pinned memory versus normal pageable memory. The `--memory=pinned` option causes
    the test to use pinned memory instead of pageable memory. [Table 5.1](ch05.html#ch05tab01)
    lists the `bandwidthTest` numbers for a `cg1.4xlarge` instance in Amazon EC2,
    running Windows 7-x64 (numbers in MB/s). Because it involves a significant amount
    of work for the host, including a kernel transition, allocating pinned memory
    is expensive.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`bandwidthTest` SDK 示例使开发者能够轻松比较固定内存与普通可分页内存的性能。`--memory=pinned` 选项会使测试使用固定内存而非可分页内存。[表
    5.1](ch05.html#ch05tab01)列出了在 Amazon EC2 上运行 Windows 7-x64 的 `cg1.4xlarge` 实例的
    `bandwidthTest` 数值（单位为 MB/s）。由于涉及大量主机工作，包括内核切换，分配固定内存的成本较高。'
- en: '![Image](graphics/05tab01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/05tab01.jpg)'
- en: '*Table 5.1* Pinned versus Pageable Bandwidth'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.1* 固定内存与可分页内存带宽'
- en: 'CUDA 2.2 added several features to pinned memory. *Portable pinned memory*
    can be accessed by any GPU; *mapped pinned memory* is mapped into the CUDA address
    space for direct access by CUDA kernels; and *write-combined pinned memory* enables
    faster bus transfers on some systems. CUDA 4.0 added two important features that
    pertain to host memory: Existing host memory ranges can be page-locked in place
    using *host memory registration*, and Unified Virtual Addressing (UVA) enables
    all pointers to be unique process-wide, including host and device pointers. When
    UVA is in effect, the system can infer from the address range whether memory is
    device memory or host memory.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 2.2 增加了几个固定内存的特性。*可移植固定内存*可以被任何 GPU 访问；*映射固定内存*被映射到 CUDA 地址空间，以便 CUDA 内核直接访问；而
    *写合并固定内存* 在某些系统上可以实现更快的总线传输。CUDA 4.0 添加了与主机内存相关的两个重要特性：现有的主机内存区域可以使用 *主机内存注册*
    进行页面锁定，统一虚拟地址（UVA）使得所有指针在整个进程中都是唯一的，包括主机和设备指针。当 UVA 生效时，系统可以根据地址范围推断出内存是设备内存还是主机内存。
- en: 5.1.2\. Portable Pinned Memory
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 可移植固定内存
- en: By default, pinned memory allocations are only accessible to the GPU that is
    current when `cudaHostAlloc()` or `cuMemHostAlloc()` is called. By specifying
    the `cudaHostAllocPortable` flag to `cudaHostAlloc()`, or the `CU_MEMHOSTALLOC_PORTABLE`
    flag to `cuHostMemAlloc()`, applications can request that the pinned allocation
    be mapped for *all* GPUs instead. Portable pinned allocations benefit from the
    transparent acceleration of memcpy described earlier and can participate in asynchronous
    memcpys for any GPU in the system. For applications that intend to use multiple
    GPUs, it is good practice to specify all pinned allocations as portable.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，固定内存分配仅对在 `cudaHostAlloc()` 或 `cuMemHostAlloc()` 调用时的当前 GPU 可访问。通过在 `cudaHostAlloc()`
    中指定 `cudaHostAllocPortable` 标志，或者在 `cuHostMemAlloc()` 中指定 `CU_MEMHOSTALLOC_PORTABLE`
    标志，应用程序可以请求将固定分配映射到 *所有* GPU。可移植的固定分配受益于前面提到的 memcpy 透明加速，并且可以参与任何 GPU 的异步 memcpys。对于计划使用多个
    GPU 的应用程序，最好将所有固定分配指定为可移植的。
- en: '* * *'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When UVA is in effect, all pinned memory allocations are portable.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当 UVA 生效时，所有固定内存分配都是可移植的。
- en: '* * *'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.1.3\. Mapped Pinned Memory
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 映射固定内存
- en: By default, pinned memory allocations are mapped for the GPU outside the CUDA
    address space. They can be directly accessed by the GPU, but only through memcpy
    functions. CUDA kernels cannot read or write the host memory directly. On GPUs
    of SM 1.2 capability and higher, however, CUDA kernels are able to read and write
    host memory directly; they just need allocations to be mapped into the device
    memory address space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，固定内存分配会映射到 CUDA 地址空间外的 GPU 上。它们可以被 GPU 直接访问，但只能通过 memcpy 函数进行访问。CUDA 内核不能直接读取或写入主机内存。然而，在
    SM 1.2 及更高版本的 GPU 上，CUDA 内核可以直接读取和写入主机内存；它们只需要将分配映射到设备内存地址空间中。
- en: To enable mapped pinned allocations, applications using the CUDA runtime must
    call `cudaSetDeviceFlags()` with the `cudaDeviceMapHost` flag before any initialization
    has been performed. Driver API applications specify the `CU_CTX_MAP_HOST` flag
    to `cuCtxCreate()`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用映射的固定内存分配，使用 CUDA 运行时的应用程序必须在任何初始化操作之前调用`cudaSetDeviceFlags()`并设置`cudaDeviceMapHost`标志。驱动程序
    API 应用程序需要在`cuCtxCreate()`中指定`CU_CTX_MAP_HOST`标志。
- en: Once mapped pinned memory has been enabled, it may be allocated by calling `cudaHostAlloc()`
    with the `cudaHostAllocMapped` flag, or `cuMemHostAlloc()` with the `CU_MEMALLOCHOST_DEVICEMAP`
    flag. Unless UVA is in effect, the application then must query the device pointer
    corresponding to the allocation with `cudaHostGetDevicePointer()` or `cuMemHostGetDevicePointer()`.
    The resulting device pointer then can be passed to CUDA kernels. Best practices
    with mapped pinned memory are described in the section “[Mapped Pinned Memory
    Usage](ch05.html#ch05lev2sec7).”
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启用了映射的固定内存，可以通过调用`cudaHostAlloc()`并设置`cudaHostAllocMapped`标志，或通过`cuMemHostAlloc()`并设置`CU_MEMALLOCHOST_DEVICEMAP`标志来进行分配。除非
    UVA 生效，否则应用程序必须通过`cudaHostGetDevicePointer()`或`cuMemHostGetDevicePointer()`查询与分配对应的设备指针。然后，可以将该设备指针传递给
    CUDA 内核。关于映射固定内存的最佳实践，请参阅章节“[映射固定内存使用](ch05.html#ch05lev2sec7)”。
- en: '* * *'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When UVA is in effect, all pinned memory allocations are mapped.^([1](ch05.html#ch05fn1))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当 UVA 生效时，所有固定内存分配都会被映射。^([1](ch05.html#ch05fn1))
- en: '[1](ch05.html#ch05fn1a). Except those marked as write combining.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch05.html#ch05fn1a). 除非标记为写合并。'
- en: '* * *'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.1.4\. Write-Combined Pinned Memory
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4\. 写合并固定内存
- en: Write-combined memory, also known as write-combining or Uncacheable Write Combining
    (USWC) memory, was created to enable the CPU to write to GPU frame buffers quickly
    and without polluting the CPU cache.^([2](ch05.html#ch05fn2)) To that end, Intel
    added a new page table kind that steered writes into special write-combining buffers
    instead of the main processor cache hierarchy. Later, Intel also added “nontemporal”
    store instructions (e.g., `MOVNTPS` and `MOVNTI`) that enabled applications to
    steer writes into the write-combining buffers on a per-instruction basis. In general,
    memory fence instructions (such as `MFENCE`) are needed to maintain coherence
    with WC memory. These operations are not needed for CUDA applications because
    they are done automatically when the CUDA driver submits work to the hardware.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 写合并内存，也称为写合并或不可缓存写合并（USWC）内存，旨在使CPU能够快速写入GPU帧缓冲区而不会污染CPU缓存。^([2](ch05.html#ch05fn2))
    为此，英特尔新增了一种新的页表类型，将写操作引导到专用的写合并缓冲区，而不是主处理器缓存层次结构。后来，英特尔还增加了“非时间性”存储指令（例如`MOVNTPS`和`MOVNTI`），使得应用程序可以按指令级别将写操作引导到写合并缓冲区。通常，需要使用内存屏障指令（例如`MFENCE`）来维持与WC内存的一致性。对于CUDA应用程序，这些操作不需要手动执行，因为它们在CUDA驱动程序提交工作到硬件时会自动完成。
- en: '[2](ch05.html#ch05fn2a). WC memory originally was announced by Intel in 1997,
    at the same time as the Accelerated Graphics Port (AGP). AGP was used for graphics
    boards before PCI Express.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch05.html#ch05fn2a)。WC内存最初由英特尔在1997年宣布，同时发布的还有加速图形端口（AGP）。AGP曾用于图形卡，后来被PCI
    Express取代。'
- en: For CUDA, write-combining memory can be requested by calling `cudaHostAlloc()`
    with the `cudaHostWriteCombined` flag, or `cuMemHostAlloc()` with the `CU_MEMHOSTALLOC_WRITECOMBINED`
    flag. Besides setting the page table entries to bypass the CPU caches, this memory
    also is not snooped during PCI Express bus transfers. On systems with front side
    buses (pre-Opteron and pre-Nehalem), avoiding the snoops improves PCI Express
    transfer performance. There is little, if any, performance advantage to WC memory
    on NUMA systems.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CUDA，可以通过调用`cudaHostAlloc()`并使用`cudaHostWriteCombined`标志，或者使用`cuMemHostAlloc()`并带上`CU_MEMHOSTALLOC_WRITECOMBINED`标志来请求写合并内存。除了将页表条目设置为绕过CPU缓存之外，这些内存在PCI
    Express总线传输过程中也不会被窥探。在前端总线系统（如Opteron和Nehalem之前的系统）中，避免窥探可以提高PCI Express传输性能。在NUMA系统中，WC内存几乎没有性能优势。
- en: Reading WC memory with the CPU is very slow (about 6x slower), unless the reads
    are done with the `MOVNTDQA` instruction (new with SSE4). On NVIDIA’s integrated
    GPUs, write-combined memory is as fast as the system memory carveout—system memory
    that was set aside at boot time for use by the GPU and is not available to the
    CPU.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 用CPU读取WC内存非常慢（大约慢6倍），除非使用`MOVNTDQA`指令（SSE4新增加的指令）进行读取。在NVIDIA的集成GPU上，写合并内存与系统内存保留区一样快——系统内存是启动时为GPU预留的内存，CPU无法访问。
- en: Despite the purported benefits, as of this writing, there is little reason for
    CUDA developers to use write-combined memory. It’s just too easy for a host memory
    pointer to WC memory to “leak” into some part of the application that would try
    to read the memory. In the absence of empirical evidence to the contrary, it should
    be avoided.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有声称的好处，但截至本文撰写时，CUDA开发人员几乎没有理由使用写合并内存。因为主机内存指针很容易“泄漏”到应用程序的某个部分，从而试图读取该内存。缺乏相反的实验证据时，应该避免使用它。
- en: '* * *'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When UVA is in effect, write-combined pinned allocations are *not* mapped into
    the unified address space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当UVA生效时，写合并固定分配的内存*不会*映射到统一地址空间中。
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.1.5\. Registering Pinned Memory
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.5. 注册固定内存
- en: CUDA developers don’t always get the opportunity to allocate host memory they
    want the GPU(s) to access directly. For example, a large, extensible application
    may have an interface that passes pointers to CUDA-aware plugins, or the application
    may be using an API for some other peripheral (notably high-speed networking)
    that has its own dedicated allocation function for much the same reason CUDA does.
    To accommodate these usage scenarios, CUDA 4.0 added the ability to *register*
    pinned memory.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA开发人员并不总是能直接分配主机内存供GPU(s)访问。例如，一个大型的、可扩展的应用程序可能有一个接口，将指针传递给CUDA感知插件，或者该应用程序可能正在使用某个其他外围设备（特别是高速网络）的API，该设备也有自己的分配函数，原因与CUDA类似。为了适应这些使用场景，CUDA
    4.0引入了*注册*固定内存的功能。
- en: Pinned memory registration decouples allocation from the page-locking and mapping
    of host memory. It takes an *already-allocated* virtual address range, page-locks
    it, and maps it for the GPU. Just as with `cudaHostAlloc()`, the memory optionally
    may be mapped into the CUDA address space or made portable (accessible to all
    GPUs).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 固定内存注册将内存分配与主机内存的页面锁定和映射解耦。它采用一个*已经分配*的虚拟地址范围，进行页面锁定，并为GPU映射。与 `cudaHostAlloc()`
    一样，这块内存可以选择性地映射到CUDA地址空间中，或使其可移植（可以被所有GPU访问）。
- en: 'The `cuMemHostRegister()/cudaHostRegister()` and `cuMemHostUnregister()/ cudaHostUnregister()`
    functions register and unregister host memory for access by the GPU(s), respectively.
    The memory range to register must be page-aligned: In other words, both the base
    address and the size must be evenly divisible by the page size of the operating
    system. Applications can allocate page-aligned address ranges in two ways.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuMemHostRegister()/cudaHostRegister()` 和 `cuMemHostUnregister()/cudaHostUnregister()`
    函数分别用于注册和注销主机内存，以便GPU访问。需要注册的内存范围必须是按页面对齐的：换句话说，基地址和大小必须能被操作系统的页面大小整除。应用程序可以通过两种方式分配按页面对齐的地址范围。'
- en: • Allocate the memory with operating system facilities that traffic in whole
    pages, such as `VirtualAlloc()` on Windows or `valloc()` or `mmap()`^([3](ch05.html#ch05fn3))
    on other platforms.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用操作系统的功能分配内存，这些功能处理整个页面，比如Windows上的 `VirtualAlloc()` 或其他平台上的 `valloc()` 或
    `mmap()`^([3](ch05.html#ch05fn3))。
- en: '[3](ch05.html#ch05fn3a). Or `posix_memalign()` in conjunction with `getpagesize()`.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch05.html#ch05fn3a)。或者使用`posix_memalign()`结合`getpagesize()`。'
- en: • Given an arbitrary address range (say, memory allocated with `malloc()` or
    `operator new[]`), clamp the address range to the next-lower page boundary and
    pad to the next page size.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: • 给定一个任意的地址范围（例如，使用`malloc()`或`operator new[]`分配的内存），将地址范围限制为下一个较低的页面边界，并填充到下一个页面大小。
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Even when UVA is in effect, registered pinned memory that has been mapped into
    the CUDA address space has a different device pointer than the host pointer. Applications
    must call `cudaHostGetDevicePointer()/cuMemHostGetDevicePointer()` in order to
    obtain the device pointer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在启用UVA的情况下，已注册并映射到CUDA地址空间的固定内存，其设备指针与主机指针不同。应用程序必须调用`cudaHostGetDevicePointer()/cuMemHostGetDevicePointer()`来获取设备指针。
- en: '* * *'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.1.6\. Pinned Memory and UVA
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.6. 固定内存和UVA
- en: When UVA (Unified Virtual Addressing) is in effect, all pinned memory allocations
    are both mapped and portable. The exceptions to this rule are write-combined memory
    and registered memory. For those, the device pointer may differ from the host
    pointer, and applications still must query it with `cudaHostGetDevicePointer()/cuMemHostGetDevicePointer()`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当UVA（统一虚拟地址）生效时，所有固定内存分配都是既映射又可移植的。此规则的例外是写合并内存和注册内存。对于这些内存，设备指针可能与主机指针不同，应用程序仍然需要通过`cudaHostGetDevicePointer()/cuMemHostGetDevicePointer()`来查询设备指针。
- en: UVA is supported on all 64-bit platforms except Windows Vista and Windows 7\.
    On Windows Vista and Windows 7, only the TCC driver (which may be enabled or disabled
    using `nvidia-smi`) supports UVA. Applications can query whether UVA is in effect
    by calling `cudaGetDeviceProperties()` and examining the `cudaDeviceProp::unifiedAddressing`
    structure member, or by calling `cuDeviceGetAttribute()` with `CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: UVA在除Windows Vista和Windows 7外的所有64位平台上均受支持。在Windows Vista和Windows 7上，只有TCC驱动程序（可以通过`nvidia-smi`启用或禁用）支持UVA。应用程序可以通过调用`cudaGetDeviceProperties()`并检查`cudaDeviceProp::unifiedAddressing`结构成员，或者通过调用`cuDeviceGetAttribute()`并传入`CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING`来查询UVA是否生效。
- en: 5.1.7\. Mapped Pinned Memory Usage
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.7. 映射固定内存的使用
- en: For applications whose performance relies on PCI Express transfer performance,
    mapped pinned memory can be a boon. Since the GPU can read or write host memory
    directly from kernels, it eliminates the need to perform some memory copies, reducing
    overhead. Here are some common idioms for using mapped pinned memory.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于依赖PCI Express传输性能的应用程序，映射的固定内存可以带来好处。由于GPU可以直接从内核读取或写入主机内存，因此不需要进行一些内存复制，从而减少了开销。以下是使用映射的固定内存的一些常见用法。
- en: '• Posting writes to host memory: Multi-GPU applications often must stage results
    back to system memory for interchange with other GPUs; writing these results via
    mapped pinned memory avoids an extraneous device→host memory copy. Write-only
    access patterns to host memory are appealing because there is no latency to cover.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: • 向主机内存写入数据：多 GPU 应用程序通常必须将结果写回系统内存，以便与其他 GPU 交换；通过映射的固定内存写入这些结果，可以避免不必要的设备→主机内存复制。对于主机内存的只写访问模式很有吸引力，因为不需要处理延迟。
- en: '• Streaming: These workloads otherwise would use CUDA streams to coordinate
    concurrent memcpys to and from device memory, while kernels do their processing
    on device memory.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: • 流式传输：这些工作负载通常会使用 CUDA 流来协调设备内存之间的并行内存复制操作，同时内核在设备内存上进行处理。
- en: '• “Copy with panache”: Some workloads benefit from performing computations
    as data is transferred across PCI Express. For example, the GPU may compute subarray
    reductions while transferring data for Scan.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: • “有风度的复制”：某些工作负载通过在数据传输穿越 PCI Express 时执行计算从而受益。例如，GPU 可以在传输 Scan 数据时进行子数组归约计算。
- en: Caveats
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: Mapped pinned memory is not a panacea. Here are some caveats to consider when
    using it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 映射的固定内存不是万能的。在使用它时，需考虑以下一些警告。
- en: • Texturing from mapped pinned memory is possible, but very slow.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: • 从映射的固定内存进行纹理映射是可能的，但速度非常慢。
- en: • It is important that mapped pinned memory be accessed with coalesced memory
    transactions (see [Section 5.2.9](ch05.html#ch05lev2sec17)). The performance penalty
    for uncoalesced memory transactions ranges from 6x to 2x. But even on SM 2.x and
    later GPUs, whose caches were supposed to make coalescing an obsolete consideration,
    the penalty is significant.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: • 映射的固定内存必须通过合并内存事务来访问（参见[第 5.2.9 节](ch05.html#ch05lev2sec17)）。未合并的内存事务的性能损失范围为
    6 倍到 2 倍。但即使在 SM 2.x 及以后的 GPU 上，缓存本应使合并操作成为过时的考虑因素，性能损失依然显著。
- en: • Polling host memory with a kernel (e.g., for CPU/GPU synchronization) is not
    recommended.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用内核轮询主机内存（例如，进行 CPU/GPU 同步）是不推荐的。
- en: • Do not try to use atomics on mapped pinned host memory, either for the host
    (locked compare-exchange) or the device (`atomicAdd()`). On the CPU side, the
    facilities to enforce mutual exclusion for locked operations are not visible to
    peripherals on the PCI Express bus. Conversely, on the GPU side, atomic operations
    only work on local device memory locations because they are implemented using
    the GPU’s local memory controller.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: • 不要尝试在映射的固定主机内存上使用原子操作，无论是在主机（锁定比较-交换）还是在设备端（`atomicAdd()`）。在 CPU 端，用于强制互斥的设施对于
    PCI Express 总线上的外设是不可见的。相反，在 GPU 端，原子操作仅在本地设备内存位置上有效，因为它们是通过 GPU 的本地内存控制器实现的。
- en: 5.1.8\. NUMA, Thread Affinity, and Pinned Memory
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.8. NUMA、线程亲和性和固定内存
- en: Beginning with the AMD Opteron and Intel Nehalem, CPU memory controllers were
    integrated directly into CPUs. Previously, the memory had been attached to the
    so-called “front-side bus” (FSB) of the “northbridge” of the chipset. In multi-CPU
    systems, the northbridge could service memory requests from any CPU, and memory
    access performance was reasonably uniform from one CPU to another. With the introduction
    of integrated memory controllers, each CPU has its own dedicated pool of “local”
    physical memory that is directly attached to that CPU. Although any CPU can access
    any other CPU’s memory, “nonlocal” accesses—accesses by one CPU to memory attached
    to another CPU—are performed across the AMD HyperTransport (HT) or Intel QuickPath
    Interconnect (QPI), incurring latency penalties and bandwidth limitations. To
    contrast with the uniform memory access times exhibited by systems with FSBs,
    these system architectures are known as NUMA for *nonuniform memory access*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从AMD Opteron和Intel Nehalem开始，CPU内存控制器直接集成到CPU中。此前，内存是连接到芯片组“北桥”的所谓“前端总线”（FSB）。在多CPU系统中，北桥可以为任何CPU提供内存请求服务，并且内存访问性能在不同CPU之间是比较均匀的。随着集成内存控制器的引入，每个CPU都有自己专用的“本地”物理内存池，直接连接到该CPU。虽然任何CPU都可以访问其他CPU的内存，但“非本地”访问——一个CPU访问连接到另一个CPU的内存——是通过AMD
    HyperTransport（HT）或Intel QuickPath Interconnect（QPI）执行的，这会带来延迟惩罚和带宽限制。为了与FSB系统所表现出的统一内存访问时间对比，这些系统架构被称为NUMA，即*非统一内存访问*。
- en: As you can imagine, performance of multithreaded applications can be heavily
    dependent on whether memory references are local to the CPU that is running the
    current thread. For most applications, however, the higher cost of a nonlocal
    access is offset by the CPUs’ on-board caches. Once nonlocal memory is fetched
    into a CPU, it remains in-cache until evicted or needed by a memory access to
    the same page by another CPU. In fact, it is common for NUMA systems to include
    a System BIOS option to “interleave” memory physically between CPUs. When this
    BIOS option is enabled, the memory is evenly divided between CPUs on a per-cache
    line (typically 64 bytes) basis, so, for example, on a 2-CPU system, about 50%
    of memory accesses will be nonlocal on average.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，多线程应用程序的性能可能在很大程度上依赖于内存引用是否位于正在运行当前线程的CPU本地。然而，对于大多数应用程序来说，非本地访问的较高成本会被CPU的板载缓存所抵消。一旦非本地内存被提取到CPU中，它将在缓存中保持，直到被驱逐或被另一个CPU通过对同一页面的内存访问所需。在NUMA系统中，通常会包括一个系统BIOS选项，用于在物理上将内存在CPU之间“交错”分布。当启用此BIOS选项时，内存会按照每个缓存行（通常是64字节）在CPU之间均匀分配，因此例如，在一个2个CPU的系统中，大约50%的内存访问将是非本地的。
- en: For CUDA applications, PCI Express transfer performance can be dependent on
    whether memory references are local. If there is more than one I/O hub (IOH) in
    the system, the GPU(s) attached to a given IOH have better performance and reduce
    demand for QPI bandwidth when the pinned memory is local. Because some high-end
    NUMA systems are hierarchical but don’t associate the pools of memory bandwidth
    strictly with CPUs, NUMA APIs refer to *nodes* that may or may not strictly correspond
    with CPUs in the system.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CUDA应用程序，PCI Express传输性能可能取决于内存引用是否是本地的。如果系统中有多个I/O集线器（IOH），连接到给定IOH的GPU性能更好，并且当固定内存是本地时，减少了对QPI带宽的需求。因为一些高端NUMA系统是分层的，但并不严格将内存带宽池与CPU关联，所以NUMA
    API指代的*节点*可能不严格对应系统中的CPU。
- en: If NUMA is enabled on the system, it is good practice to allocate host memory
    on the same node as a given GPU. Unfortunately, there is no official CUDA API
    to affiliate a GPU with a given CPU. Developers with a priori knowledge of the
    system design may know which node to associate with which GPU. Then platform-specific,
    NUMA-aware APIs may be used to perform these memory allocations, and host memory
    registration (see [Section 5.1.5](ch05.html#ch05lev2sec5)) can be used to pin
    those virtual allocations and map them for the GPU(s).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统启用了NUMA，最佳实践是将主机内存分配到与给定GPU相同的节点。不幸的是，目前没有官方的CUDA API来将GPU与特定的CPU关联。开发人员如果对系统设计有先验知识，可能会知道应将哪个节点与哪个GPU关联。然后，可以使用平台特定的NUMA感知API来执行这些内存分配，并且可以使用主机内存注册（参见[第5.1.5节](ch05.html#ch05lev2sec5)）来固定这些虚拟分配，并为GPU进行映射。
- en: '[Listing 5.1](ch05.html#ch05lis01) gives a code fragment to perform NUMA-aware
    allocations on Linux,^([4](ch05.html#ch05fn4)) and [Listing 5.2](ch05.html#ch05lis02)
    gives a code fragment to perform NUMA-aware allocations on Windows.^([5](ch05.html#ch05fn5))'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5.1](ch05.html#ch05lis01)提供了一个代码片段，用于在Linux上执行NUMA感知分配，^([4](ch05.html#ch05fn4))，而[示例
    5.2](ch05.html#ch05lis02)提供了一个代码片段，用于在Windows上执行NUMA感知分配。^([5](ch05.html#ch05fn5))'
- en: '[4](ch05.html#ch05fn4a). [http://bit.ly/USy4e7](http://bit.ly/USy4e7)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch05.html#ch05fn4a). [http://bit.ly/USy4e7](http://bit.ly/USy4e7)'
- en: '[5](ch05.html#ch05fn5a). [http://bit.ly/XY1g8m](http://bit.ly/XY1g8m)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch05.html#ch05fn5a). [http://bit.ly/XY1g8m](http://bit.ly/XY1g8m)'
- en: '*Listing 5.1.* NUMA-aware allocation (Linux).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 5.1.* NUMA感知分配（Linux）。'
- en: '[Click here to view code image](ch05_images.html#p05lis01a)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis01a)'
- en: '* * *'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: bool
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: bool
- en: numNodes( int *p )
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: numNodes( int *p )
- en: '{'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: if ( numa_available() >= 0 ) {
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: if ( numa_available() >= 0 ) {
- en: '*p = numa_max_node() + 1;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*p = numa_max_node() + 1;'
- en: return true;
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: return true;
- en: '}'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return false;
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: return false;
- en: '}'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void *
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: void *
- en: pageAlignedNumaAlloc( size_t bytes, int node )
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: pageAlignedNumaAlloc( size_t bytes, int node )
- en: '{'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: void *ret;
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: void *ret;
- en: printf( "Allocating on node %d\n", node ); fflush(stdout);
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "在节点 %d 上分配内存\n", node ); fflush(stdout);
- en: ret = numa_alloc_onnode( bytes, node );
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ret = numa_alloc_onnode( bytes, node );
- en: return ret;
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: return ret;
- en: '}'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: pageAlignedNumaFree( void *p, size_t bytes )
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: pageAlignedNumaFree( void *p, size_t bytes )
- en: '{'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: numa_free( p, bytes );
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: numa_free( p, bytes );
- en: '}'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 5.2.* NUMA-aware allocation (Windows).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 5.2.* NUMA感知分配（Windows）。'
- en: '[Click here to view code image](ch05_images.html#p05lis02a)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p05lis02a)'
- en: '* * *'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: bool
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: bool
- en: numNodes( int *p )
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: numNodes( int *p )
- en: '{'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: ULONG maxNode;
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ULONG maxNode;
- en: if ( GetNumaHighestNodeNumber( &maxNode ) ) {
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: if ( GetNumaHighestNodeNumber( &maxNode ) ) {
- en: '*p = (int) maxNode+1;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*p = (int) maxNode+1;'
- en: return true;
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: return true;
- en: '}'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return false;
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: return false;
- en: '}'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void *
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: void *
- en: pageAlignedNumaAlloc( size_t bytes, int node )
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: pageAlignedNumaAlloc( size_t bytes, int node )
- en: '{'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: void *ret;
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: void *ret;
- en: printf( "Allocating on node %d\n", node ); fflush(stdout);
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "Allocating on node %d\n", node ); fflush(stdout);
- en: ret = VirtualAllocExNuma( GetCurrentProcess(),
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ret = VirtualAllocExNuma( GetCurrentProcess(),
- en: NULL,
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: NULL，
- en: bytes,
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: bytes，
- en: MEM_COMMIT | MEM_RESERVE,
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: MEM_COMMIT | MEM_RESERVE，
- en: PAGE_READWRITE,
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: PAGE_READWRITE，
- en: node );
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: node );
- en: return ret;
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: return ret;
- en: '}'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: pageAlignedNumaFree( void *p )
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: pageAlignedNumaFree( void *p )
- en: '{'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: VirtualFreeEx( GetCurrentProcess(), p, 0, MEM_RELEASE );
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: VirtualFreeEx( GetCurrentProcess(), p, 0, MEM_RELEASE );
- en: '}'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.2\. Global Memory
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 全局内存
- en: 'Global memory is the main abstraction by which CUDA kernels read or write device
    memory.^([6](ch05.html#ch05fn6)) Since device memory is directly attached to the
    GPU and read and written using a memory controller integrated into the GPU, the
    peak bandwidth is extremely high: typically more than 100G/s for high-end CUDA
    cards.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 全局内存是CUDA内核读写设备内存的主要抽象方式。^([6](ch05.html#ch05fn6)) 由于设备内存直接连接到GPU，并通过集成到GPU中的内存控制器进行读写，因此其峰值带宽极高：高端CUDA卡的带宽通常超过100G/s。
- en: '[6](ch05.html#ch05fn6a). For maximum developer confusion, CUDA uses the term
    *device pointer* to refer to pointers that reside in *global memory* (device memory
    addressable by CUDA kernels).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](ch05.html#ch05fn6a)。为了最大限度地增加开发者的困惑，CUDA使用*设备指针*一词来指代驻留在*全局内存*中的指针（由CUDA内核可寻址的设备内存）。'
- en: Device memory can be accessed by CUDA kernels using *device pointers*. The following
    simple memset kernel gives an example.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 设备内存可以通过CUDA内核使用*设备指针*进行访问。以下是一个简单的memset内核示例。
- en: '[Click here to view code image](ch05_images.html#p131pro01a)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p131pro01a)'
- en: template<class T>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: __global__ void
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: GPUmemset( int *base, int value, size_t N )
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: GPUmemset( int *base, int value, size_t N )
- en: '{'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
- en: i < N;
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += gridDim.x*blockDim.x )
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: i += gridDim.x*blockDim.x )
- en: '{'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: base[i] = value;
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: base[i] = value;
- en: '}'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: The device pointer `base` resides in the *device address space*, separate from
    the CPU address space used by the host code in the CUDA program. As a result,
    host code in the CUDA program can perform pointer arithmetic on device pointers,
    but they may not dereference them.^([7](ch05.html#ch05fn7))
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 设备指针`base`驻留在*设备地址空间*中，与CUDA程序中主机代码使用的CPU地址空间分开。因此，CUDA程序中的主机代码可以对设备指针进行指针运算，但不能解引用它们。^([7](ch05.html#ch05fn7))
- en: '[7](ch05.html#ch05fn7a). Mapped pinned pointers represent an exception to this
    rule. They are located in system memory but can be accessed by the GPU. On non-UVA
    systems, the host and device pointers to this memory are different: The application
    must call `cuMemHostGetDevicePointer()` or `cudaHostGetDevicePointer()` to map
    the host pointer to the corresponding device pointer. But when UVA is in effect,
    the pointers are the same.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](ch05.html#ch05fn7a). 映射的固定指针是此规则的例外。它们位于系统内存中，但可以被GPU访问。在非UVA系统中，指向此内存的主机指针和设备指针是不同的：应用程序必须调用`cuMemHostGetDevicePointer()`或`cudaHostGetDevicePointer()`将主机指针映射到相应的设备指针。但当启用UVA时，指针是相同的。'
- en: This kernel writes the integer `value` into the address range given by `base`
    and `N`. The references to `blockIdx`, `blockDim`, and `gridDim` enable the kernel
    to operate correctly, using whatever block and grid parameters were specified
    to the kernel launch.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核将整数`value`写入由`base`和`N`给定的地址范围。对`blockIdx`、`blockDim`和`gridDim`的引用使得内核能够正确运行，使用启动内核时指定的块和网格参数。
- en: 5.2.1\. Pointers
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 指针
- en: When using the CUDA runtime, device pointers and host pointers both are typed
    as `void *`. The driver API uses an integer-valued typedef called `CUdeviceptr`
    that is the same width as host pointers (i.e., 32 bits on 32-bit operating systems
    and 64 bits on 64-bit operating systems), as follows.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用CUDA运行时，设备指针和主机指针都被定义为`void *`类型。驱动程序API使用一个称为`CUdeviceptr`的整数类型定义，它与主机指针的宽度相同（即在32位操作系统上为32位，在64位操作系统上为64位），如下所示。
- en: '[Click here to view code image](ch05_images.html#p131pro02a)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p131pro02a)'
- en: '#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64)'
- en: typedef unsigned long long CUdeviceptr;
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: typedef unsigned long long CUdeviceptr;
- en: '#else'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#else'
- en: typedef unsigned int CUdeviceptr;
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: typedef unsigned int CUdeviceptr;
- en: '#endif'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#endif'
- en: The `uintptr_t` type, available in `<stdint.h>` and introduced in C++0x, may
    be used to portably convert between host pointers (`void *`) and device pointers
    (`CUdeviceptr`), as follows.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`uintptr_t`类型，定义在`<stdint.h>`中，并在C++0x中引入，可用于在主机指针（`void *`）和设备指针（`CUdeviceptr`）之间便捷地转换，如下所示。'
- en: '[Click here to view code image](ch05_images.html#p131pro03a)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p131pro03a)'
- en: CUdeviceptr devicePtr;
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: CUdeviceptr devicePtr;
- en: void *p;
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: void *p;
- en: p = (void *) (uintptr_t) devicePtr;
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: p = (void *) (uintptr_t) devicePtr;
- en: devicePtr = (CUdeviceptr) (uintptr_t) p;
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: devicePtr = (CUdeviceptr) (uintptr_t) p;
- en: The host can do pointer arithmetic on device pointers to pass to a kernel or
    memcpy call, but the host cannot read or write device memory with these pointers.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 主机可以对设备指针进行指针运算，以传递给内核或memcpy调用，但主机无法通过这些指针读取或写入设备内存。
- en: 32- and 64-Bit Pointers in the Driver API
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序API中的32位和64位指针
- en: Because the original driver API definition for a pointer was 32-bit, the addition
    of 64-bit support to CUDA required the definition of `CUdeviceptr` and, in turn,
    all driver API functions that took `CUdeviceptr` as a parameter, to change.^([8](ch05.html#ch05fn8))
    `cuMemAlloc()`, for example, changed from
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于原始驱动程序 API 中对指针的定义是 32 位的，因此将 CUDA 支持 64 位时，需要对 `CUdeviceptr` 的定义进行修改，并且所有以
    `CUdeviceptr` 为参数的驱动程序 API 函数都需要进行更改。^([8](ch05.html#ch05fn8)) 例如，`cuMemAlloc()`
    从
- en: '[8](ch05.html#ch05fn8a). The old functions had to stay for compatibility reasons.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[8](ch05.html#ch05fn8a). 出于兼容性原因，旧的函数必须保留。'
- en: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, unsigned int bytesize);
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, unsigned int bytesize);
- en: to
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到
- en: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
- en: To accommodate both old applications (which linked against a `cuMemAlloc()`
    with 32-bit `CUdeviceptr` and size) and new ones, `cuda.h` includes two blocks
    of code that use the preprocessor to change the bindings without requiring function
    names to be changed as developers update to the new API.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了兼容旧版应用程序（这些应用程序使用的是链接到 `cuMemAlloc()` 的 32 位 `CUdeviceptr` 和大小）和新版应用程序，`cuda.h`
    包含了两段代码，使用预处理器在开发人员更新到新 API 时无需更改函数名称即可改变绑定。
- en: First, a block of code surreptitiously changes function names to map to newer
    functions that have different semantics.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，一段代码悄悄地将函数名称修改为映射到语义不同的更新函数。
- en: '[Click here to view code image](ch05_images.html#p132pro01a)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch05_images.html#p132pro01a)'
- en: '#if defined(__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION >= 3020'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#if defined(__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION >= 3020'
- en: '#define cuDeviceTotalMem cuDeviceTotalMem_v2'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#define cuDeviceTotalMem cuDeviceTotalMem_v2'
- en: '...'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: '#define cuTexRefGetAddress cuTexRefGetAddress_v2'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#define cuTexRefGetAddress cuTexRefGetAddress_v2'
- en: '#endif /* __CUDA_API_VERSION_INTERNAL || __CUDA_API_VERSION >= 3020 */'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#endif /* __CUDA_API_VERSION_INTERNAL || __CUDA_API_VERSION >= 3020 */'
- en: This way, the client code uses the same old function names, but the compiled
    code generates references to the new function names with `_v2` appended.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，客户端代码使用相同的旧函数名称，但编译后的代码会生成带有 `_v2` 后缀的新函数名称的引用。
- en: Later in the header, the old functions are defined as they were. As a result,
    developers compiling for the latest version of CUDA get the latest function definitions
    and semantics. `cuda.h` uses a similar strategy for functions whose semantics
    changed from one version to the next, such as `cuStreamDestroy()`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在头文件的后面，旧的函数仍然按原样定义。因此，为最新版本的 CUDA 编译的开发人员将获得最新的函数定义和语义。`cuda.h` 对于那些语义发生变化的函数（例如
    `cuStreamDestroy()`）也使用类似的策略。
- en: 5.2.2\. Dynamic Allocations
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 动态分配
- en: Most global memory in CUDA is obtained through dynamic allocation. Using the
    CUDA runtime, the functions
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CUDA 中，大部分全局内存通过动态分配获得。使用 CUDA 运行时，相关函数
- en: '[Click here to view code image](ch05_images.html#p133pro01a)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch05_images.html#p133pro01a)'
- en: cudaError_t cudaMalloc( void **, size_t );
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMalloc( void **, size_t );
- en: cudaError_t cudaFree( void );
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaFree( void );
- en: allocate and free global memory, respectively. The corresponding driver API
    functions are
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 分别分配和释放全局内存。相应的驱动程序 API 函数是
- en: '[Click here to view code image](ch05_images.html#p133pro02a)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch05_images.html#p133pro02a)'
- en: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
- en: CUresult CUDAAPI cuMemFree(CUdeviceptr dptr);
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemFree(CUdeviceptr dptr);
- en: Allocating global memory is expensive. The CUDA driver implements a sub-allocator
    to satisfy small allocation requests, but if the suballocator must create a new
    memory block, that requires an expensive operating system call to the kernel mode
    driver. If that happens, the CUDA driver also must synchronize with the GPU, which
    may break CPU/GPU concurrency. As a result, it’s good practice to avoid allocating
    or freeing global memory in performance-sensitive code.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 分配全局内存是昂贵的。CUDA 驱动程序实现了一个子分配器来满足小规模的分配请求，但如果子分配器必须创建一个新的内存块，这将需要一次昂贵的操作系统调用到内核模式驱动程序。如果发生这种情况，CUDA
    驱动程序还必须与 GPU 同步，这可能会破坏 CPU/GPU 的并发性。因此，在性能敏感的代码中，避免分配或释放全局内存是一个好的实践。
- en: Pitched Allocations
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 斜向分配
- en: The coalescing constraints, coupled with alignment restrictions for texturing
    and 2D memory copy, motivated the creation of *pitched* memory allocations. The
    idea is that when creating a 2D array, a pointer into the array should have the
    same alignment characteristics when updated to point to a different row. The *pitch*
    of the array is the number of bytes per row of the array.^([9](ch05.html#ch05fn9))
    The pitch allocations take a width (in bytes) and height, pad the width to a suitable
    hardware-specific pitch, and pass back the base pointer and pitch of the allocation.
    By using these allocation functions to delegate selection of the pitch to the
    driver, developers can future-proof their code against architectures that widen
    alignment requirements.^([10](ch05.html#ch05fn10))
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 合并约束和纹理以及 2D 内存复制的对齐限制促使了*斜向*内存分配的创建。其理念是，在创建二维数组时，指向数组的指针在更新指向不同的行时应该具有相同的对齐特性。数组的*步幅*是每行数组的字节数。^([9](ch05.html#ch05fn9))
    步幅分配接受一个宽度（以字节为单位）和高度，将宽度填充到适合硬件的步幅，然后返回分配的基指针和步幅。通过使用这些分配函数将步幅的选择委托给驱动程序，开发人员可以使他们的代码对未来需要更宽对齐要求的架构具有兼容性。^([10](ch05.html#ch05fn10))
- en: '[9](ch05.html#ch05fn9a). The idea of padding 2D allocations is much older than
    CUDA. Graphics APIs such as Apple QuickDraw and Microsoft DirectX exposed “rowBytes”
    and “pitch,” respectively. At one time, the padding simplified addressing computations
    by replacing a multiplication by a shift, or even replacing a multiplication by
    two shifts and an add with “two powers of 2” such as 640 (512 + 128). But these
    days, integer multiplication is so fast that pitch allocations have other motivations,
    such as avoiding negative performance interactions with caches.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[9](ch05.html#ch05fn9a)。二维内存分配的填充概念早于CUDA。图形API，如Apple QuickDraw和Microsoft
    DirectX，分别暴露了“rowBytes”和“pitch”。曾几何时，填充通过将乘法替换为位移操作，甚至将乘法替换为两次位移加一次加法的方式简化了寻址计算，这种方式使用了“两个2的幂”，例如640（512
    + 128）。但如今，整数乘法已经非常快速，以至于pitch分配有了其他动机，例如避免与缓存的负面性能交互。'
- en: '[10](ch05.html#ch05fn10a). Not an unexpected trend. Fermi widened several alignment
    requirements over Tesla.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[10](ch05.html#ch05fn10a)。这一趋势并不令人意外。Fermi相比Tesla扩大了几个对齐要求。'
- en: CUDA programs often must adhere to alignment constraints enforced by the hardware,
    not only on base addresses but also on the widths (in bytes) of memory copies
    and linear memory bound to textures. Because the alignment constraints are hardware-specific,
    CUDA provides APIs that enable developers to delegate the selection of the appropriate
    alignment to the driver. Using these APIs enables CUDA applications to implement
    hardware-independent code and to be “future-proof” against CUDA architectures
    that have not yet shipped.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA程序通常必须遵循硬件强制执行的对齐约束，不仅是基础地址，还包括内存复制和绑定到纹理的线性内存的宽度（以字节为单位）。由于对齐约束是硬件特定的，CUDA提供了API，允许开发者将选择适当对齐的任务委托给驱动程序。使用这些API使得CUDA应用能够实现硬件独立的代码，并且对尚未发布的CUDA架构具有“未来兼容性”。
- en: '[Figure 5.1](ch05.html#ch05fig01) shows a pitch allocation being performed
    on an array that is 352 bytes wide. The pitch is padded to the next multiple of
    64 bytes before allocating the memory. Given the pitch of the array in addition
    to the row and column, the address of an array element can be computed as follows.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.1](ch05.html#ch05fig01)展示了在一个宽度为352字节的数组上执行pitch分配的过程。在分配内存之前，pitch被填充到下一个64字节的倍数。给定数组的pitch以及行和列，可以按如下方式计算数组元素的地址。'
- en: '[Click here to view code image](ch05_images.html#p134pro01a)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch05_images.html#p134pro01a)'
- en: inline T *
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: inline T *
- en: getElement( T *base, size_t Pitch, int row, int col )
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: getElement( T *base, size_t Pitch, int row, int col )
- en: '{'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: return (T *) ((char *) base + row*Pitch) + col;
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: return (T *) ((char *) base + row*Pitch) + col;
- en: '}'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '![Image](graphics/05fig01.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/05fig01.jpg)'
- en: '*Figure 5.1* Pitch versus width.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.1* Pitch与宽度的关系。'
- en: The CUDA runtime function to perform a pitched allocation is as follows.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 执行pitch分配的CUDA运行时函数如下。
- en: '[Click here to view code image](ch05_images.html#p134pro02a)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch05_images.html#p134pro02a)'
- en: template<class T>
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: __inline__ __host__ cudaError_t cudaMallocPitch(
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: __inline__ __host__ cudaError_t cudaMallocPitch(
- en: T **devPtr,
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: T **devPtr,
- en: size_t *pitch,
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: size_t *pitch,
- en: size_t widthInBytes,
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: size_t widthInBytes,
- en: size_t height
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: size_t height
- en: );
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: );
- en: The CUDA runtime also includes the function `cudaMalloc3D()`, which allocates
    3D memory regions using the `cudaPitchedPtr` and `cudaExtent` structures.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 运行时还包括 `cudaMalloc3D()` 函数，该函数使用 `cudaPitchedPtr` 和 `cudaExtent` 结构分配 3D
    内存区域。
- en: '[Click here to view code image](ch05_images.html#p134pro03a)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p134pro03a)'
- en: extern __host__ cudaError_t CUDARTAPI cudaMalloc3D(struct
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: extern __host__ cudaError_t CUDARTAPI cudaMalloc3D(struct
- en: cudaPitchedPtr* pitchedDevPtr, struct cudaExtent extent);
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: cudaPitchedPtr* pitchedDevPtr, struct cudaExtent extent);
- en: '`cudaPitchedPtr`, which receives the allocated memory, is defined as follows.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaPitchedPtr`，它接收分配的内存，定义如下。'
- en: struct cudaPitchedPtr
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr
- en: '{'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: void *ptr;
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: void *ptr;
- en: size_t pitch;
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: size_t pitch;
- en: size_t xsize;
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: size_t xsize;
- en: size_t ysize;
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: size_t ysize;
- en: '};'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '`cudaPitchedPtr::ptr` specifies the pointer; `cudaPitchedPtr::pitch` specifies
    the pitch (width in bytes) of the allocation; and `cudaPitchedPtr::xsize` and
    `cudaPitchedPtr::ysize` are the logical width and height of the allocation, respectively.
    `cudaExtent` is defined as follows.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaPitchedPtr::ptr` 指定指针；`cudaPitchedPtr::pitch` 指定分配的步幅（字节宽度）；`cudaPitchedPtr::xsize`
    和 `cudaPitchedPtr::ysize` 分别指定分配的逻辑宽度和高度。`cudaExtent` 定义如下。'
- en: struct cudaExtent
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaExtent
- en: '{'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t width;
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: size_t width;
- en: size_t height;
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: size_t height;
- en: size_t depth;
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: size_t depth;
- en: '};'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '`cudaExtent::width` is treated differently for arrays and linear device memory.
    For arrays, it specifies the width in array elements; for linear device memory,
    it specifies the pitch (width in bytes).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaExtent::width` 对于数组和线性设备内存的处理有所不同。对于数组，它指定数组元素的宽度；对于线性设备内存，它指定步幅（字节宽度）。'
- en: The driver API function to allocate memory with a pitch is as follows.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分配具有步幅内存的驱动 API 函数如下所示。
- en: '[Click here to view code image](ch05_images.html#p135pro01a)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p135pro01a)'
- en: CUresult CUDAAPI cuMemAllocPitch(CUdeviceptr *dptr, size_t *pPitch,
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemAllocPitch(CUdeviceptr *dptr, size_t *pPitch,
- en: size_t WidthInBytes, size_t Height, unsigned int ElementSizeBytes);
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: size_t WidthInBytes, size_t Height, unsigned int ElementSizeBytes);
- en: The `ElementSizeBytes` parameter may be 4, 8, or 16 bytes, and it causes the
    allocation pitch to be padded to 64-, 128-, or 256-byte boundaries. Those are
    the alignment requirements for coalescing of 4-, 8-, and 16-byte memory transactions
    on SM 1.0 and SM 1.1 hardware. Applications that are not concerned with running
    well on that hardware can specify 4.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`ElementSizeBytes` 参数可能是 4、8 或 16 字节，并且它会使分配的步幅（pitch）对齐到 64、128 或 256 字节的边界。这些是用于在
    SM 1.0 和 SM 1.1 硬件上合并 4 字节、8 字节和 16 字节内存事务的对齐要求。那些不关心在该硬件上运行良好的应用程序可以指定为 4。'
- en: The pitch returned by `cudaMallocPitch()/cuMemAllocPitch()` is the width-in-bytes
    passed in by the caller, padded to an alignment that meets the alignment constraints
    for both coalescing of global load/store operations, and texture bind APIs. The
    amount of memory allocated is `height*pitch`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaMallocPitch()/cuMemAllocPitch()` 返回的步幅是调用方传入的以字节为单位的宽度，填充至满足全局加载/存储操作合并以及纹理绑定
    API 对齐约束的对齐方式。分配的内存量为 `height*pitch`。'
- en: For 3D arrays, developers can multiply the height by the depth before performing
    the allocation. This consideration only applies to arrays that will be accessed
    via global loads and stores, since 3D textures cannot be bound to global memory.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三维数组，开发人员可以在进行分配之前，将高度乘以深度。这个考虑只适用于通过全局加载和存储访问的数组，因为三维纹理不能绑定到全局内存。
- en: Allocations within Kernels
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内核中的内存分配
- en: Fermi-class hardware can dynamically allocate global memory using `malloc()`.
    Since this may require the GPU to interrupt the CPU, it is potentially slow. The
    sample program `mallocSpeed.cu` measures the performance of `malloc()` and `free()`
    in kernels.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi架构的硬件可以使用`malloc()`动态分配全局内存。由于这可能需要GPU中断CPU，因此可能会比较慢。示例程序`mallocSpeed.cu`测量了内核中`malloc()`和`free()`的性能。
- en: '[Listing 5.3](ch05.html#ch05lis03) shows the key kernels and timing routine
    in `mallocSpeed.cu`. As an important note, the `cudaSetDeviceLimit()` function
    must be called with `cudaLimitMallocHeapSize` before `malloc()` may be called
    in kernels. The invocation in `mallocSpeed.cu` requests a full gigabyte (2^(30)
    bytes).'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 5.3](ch05.html#ch05lis03)展示了`mallocSpeed.cu`中的关键内核和计时例程。一个重要的说明是，必须在内核中调用`malloc()`之前，先调用`cudaSetDeviceLimit()`函数并使用`cudaLimitMallocHeapSize`。`mallocSpeed.cu`中的调用请求了一个完整的千兆字节（2^(30)字节）。'
- en: CUDART_CHECK( cudaDeviceSetLimit(cudaLimitMallocHeapSize, 1<<30) );
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaDeviceSetLimit(cudaLimitMallocHeapSize, 1<<30) );
- en: When `cudaDeviceSetLimit()` is called, the requested amount of memory is allocated
    and may not be used for any other purpose.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用`cudaDeviceSetLimit()`时，请求的内存量将被分配，并且可能不能用于其他任何用途。
- en: '*Listing 5.3.* `MallocSpeed` function and kernels.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5.3.* `MallocSpeed` 函数和内核。'
- en: '[Click here to view code image](ch05_images.html#p05lis03a)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p05lis03a)'
- en: '* * *'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: AllocateBuffers( void **out, size_t N )
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: AllocateBuffers( void **out, size_t N )
- en: '{'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i = blockIdx.x*blockDim.x + threadIdx.x;
- en: out[i] = malloc( N );
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] = malloc( N );
- en: '}'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __global__ void
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: FreeBuffers( void **in )
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: FreeBuffers( void **in )
- en: '{'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i = blockIdx.x*blockDim.x + threadIdx.x;
- en: free( in[i] );
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: free( in[i] );
- en: '}'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaError_t
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t
- en: MallocSpeed( double *msPerAlloc, double *msPerFree,
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: MallocSpeed( double *msPerAlloc, double *msPerFree,
- en: void **devicePointers, size_t N,
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: void **devicePointers, size_t N,
- en: cudaEvent_t evStart, cudaEvent_t evStop,
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEvent_t evStart, cudaEvent_t evStop,
- en: int cBlocks, int cThreads )
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: int cBlocks, int cThreads )
- en: '{'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float etAlloc, etFree;
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: float etAlloc, etFree;
- en: cudaError_t status;
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: CUDART_CHECK( cudaEventRecord( evStart ) );
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( evStart ) );
- en: AllocateBuffers<<<cBlocks,cThreads>>>( devicePointers, N );
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: AllocateBuffers<<<cBlocks,cThreads>>>( devicePointers, N );
- en: CUDART_CHECK( cudaEventRecord( evStop ) );
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( evStop ) );
- en: CUDART_CHECK( cudaThreadSynchronize() );
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaThreadSynchronize() );
- en: CUDART_CHECK( cudaGetLastError() );
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaGetLastError() );
- en: CUDART_CHECK( cudaEventElapsedTime( &etAlloc, evStart, evStop ) );
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventElapsedTime( &etAlloc, evStart, evStop ) );
- en: CUDART_CHECK( cudaEventRecord( evStart ) );
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( evStart ) );
- en: FreeBuffers<<<cBlocks,cThreads>>>( devicePointers );
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: FreeBuffers<<<cBlocks,cThreads>>>( devicePointers );
- en: CUDART_CHECK( cudaEventRecord( evStop ) );
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( evStop ) );
- en: CUDART_CHECK( cudaThreadSynchronize() );
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaThreadSynchronize() );
- en: CUDART_CHECK( cudaGetLastError() );
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaGetLastError() );
- en: CUDART_CHECK( cudaEventElapsedTime( &etFree, evStart, evStop ) );
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventElapsedTime( &etFree, evStart, evStop ) );
- en: '*msPerAlloc = etAlloc / (double) (cBlocks*cThreads);'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '*msPerAlloc = etAlloc / (double) (cBlocks*cThreads);'
- en: '*msPerFree = etFree / (double) (cBlocks*cThreads);'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '*msPerFree = etFree / (double) (cBlocks*cThreads);'
- en: 'Error:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: return status;
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 返回状态；
- en: '}'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 5.4](ch05.html#ch05lis04) shows the output from a sample run of `mallocSpeed.cu`
    on Amazon’s `cg1.4xlarge` instance type. It is clear that the allocator is optimized
    for small allocations: The 64-byte allocations take an average of 0.39 microseconds
    to perform, while allocations of 12K take at least 3 to 5 microseconds. The first
    result (155 microseconds per allocation) is having 1 thread per each of 500 blocks
    allocate a 1MB buffer.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单5.4](ch05.html#ch05lis04)显示了在Amazon的`cg1.4xlarge`实例类型上运行`mallocSpeed.cu`示例的输出。很明显，分配器已经针对小型分配进行了优化：64字节的分配平均需要0.39微秒，而12K的分配至少需要3到5微秒。第一个结果（每次分配155微秒）是指每个500个块中的1个线程分配一个1MB的缓冲区。'
- en: '*Listing 5.4.* Sample `mallocSpeed.cu` output.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单5.4.* 示例`mallocSpeed.cu`输出。'
- en: '[Click here to view code image](ch05_images.html#p05lis04a)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p05lis04a)'
- en: '* * *'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Microseconds per alloc/free (1 thread per block):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 每次分配/释放的微秒数（每块1个线程）：
- en: alloc       free
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: alloc       free
- en: 154.93      4.57
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 154.93      4.57
- en: Microseconds per alloc/free (32-512 threads per block, 12K
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 每次分配/释放的微秒数（每块32-512线程，12K
- en: 'allocations):'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 分配次数：
- en: 32            64            128           256           512
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 32            64            128           256           512
- en: alloc  free   alloc  free   alloc  free   alloc  free   alloc  free
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: alloc  free   alloc  free   alloc  free   alloc  free   alloc  free
- en: 3.53   1.18   4.27   1.17   4.89   1.14   5.48   1.14   10.38  1.11
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 3.53   1.18   4.27   1.17   4.89   1.14   5.48   1.14   10.38  1.11
- en: Microseconds per alloc/free (32-512 threads per block, 64-byte
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 每次分配/释放的微秒数（每块32-512线程，64字节
- en: 'allocations):'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 分配次数：
- en: 32            64            128           256           512
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 32            64            128           256           512
- en: alloc  free   alloc  free   alloc  free   alloc  free   alloc  free
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: alloc  free   alloc  free   alloc  free   alloc  free   alloc  free
- en: 0.35   0.27   0.37   0.29   0.34   0.27   0.37   0.22   0.53   0.27
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 0.35   0.27   0.37   0.29   0.34   0.27   0.37   0.22   0.53   0.27
- en: '* * *'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '* * *'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Important Note
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Memory allocated by invoking `malloc()` in a kernel must be freed by a *kernel*
    calling `free()`. Calling `cudaFree()` on the host will not work.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`malloc()`在内核中分配的内存必须通过*内核*调用`free()`来释放。在主机上调用`cudaFree()`是无效的。
- en: '* * *'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.2.3\. Querying the Amount of Global Memory
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 查询全局内存的数量
- en: The amount of global memory in a system may be queried even before CUDA has
    been initialized.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 系统中的全局内存数量甚至在CUDA初始化之前就可以查询。
- en: CUDA Runtime
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA运行时
- en: Call `cudaGetDeviceProperties()` and examine `cudaDeviceProp.totalGlobalMem:`
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '调用`cudaGetDeviceProperties()`并检查`cudaDeviceProp.totalGlobalMem:` '
- en: size_t totalGlobalMem; /**< Global memory on device in bytes */.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: size_t totalGlobalMem; /**< 设备上的全局内存，单位字节 */.
- en: Driver API
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序 API
- en: Call this driver API function.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此驱动程序 API 函数。
- en: CUresult CUDAAPI cuDeviceTotalMem(size_t *bytes, CUdevice dev);
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuDeviceTotalMem(size_t *bytes, CUdevice dev);
- en: '* * *'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: WDDM and Available Memory
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: WDDM 和可用内存
- en: The Windows Display Driver Model (WDDM) introduced with Windows Vista changed
    the model for memory management by display drivers to enable chunks of video memory
    to be swapped in and out of host memory as needed to perform rendering. As a result,
    the amount of memory reported by `cuDeviceTotalMem() / cudaDeviceProp::totalGlobalMem`
    will not exactly reflect the amount of physical memory on the card.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 显示驱动模型（WDDM）随着 Windows Vista 引入，改变了显示驱动程序的内存管理模型，使得视频内存的部分可以根据需要交换进出主机内存以进行渲染。因此，通过
    `cuDeviceTotalMem() / cudaDeviceProp::totalGlobalMem` 获取的内存量不会准确反映显卡上的物理内存大小。
- en: '* * *'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.2.4\. Static Allocations
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. 静态分配
- en: Applications can statically allocate global memory by annotating a memory declaration
    with the `__device__` keyword. This memory is allocated by the CUDA driver when
    the module is loaded.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以通过在内存声明中添加 `__device__` 关键字来静态分配全局内存。当模块加载时，CUDA 驱动程序会分配该内存。
- en: CUDA Runtime
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA 运行时
- en: Memory copies to and from statically allocated memory can be performed by `cudaMemcpyToSymbol()`
    and `cudaMemcpyFromSymbol().`
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对静态分配的内存进行的内存拷贝可以通过 `cudaMemcpyToSymbol()` 和 `cudaMemcpyFromSymbol()` 来完成。
- en: '[Click here to view code image](ch05_images.html#p138pro01a)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p138pro01a)'
- en: cudaError_t cudaMemcpyToSymbol(
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMemcpyToSymbol(
- en: char *symbol,
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: char *symbol，
- en: const void *src,
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: const void *src，
- en: size_t count,
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: size_t count，
- en: size_t offset = 0,
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: size_t offset = 0，
- en: enum cudaMemcpyKind kind = cudaMemcpyHostToDevice
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: enum cudaMemcpyKind kind = cudaMemcpyHostToDevice
- en: );
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: )；
- en: cudaError_t cudaMemcpyFromSymbol(
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMemcpyFromSymbol(
- en: void *dst,
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: void *dst，
- en: char *symbol,
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: char *symbol，
- en: size_t count,
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: size_t count，
- en: size_t offset = 0,
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: size_t offset = 0，
- en: enum cudaMemcpyKind kind = cudaMemcpyDeviceToHost
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: enum cudaMemcpyKind kind = cudaMemcpyDeviceToHost
- en: );
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: )；
- en: When calling `cudaMemcpyToSymbol()` or `cudaMemcpyFromSymbol()`, do not enclose
    the symbol name in quotation marks. In other words, use
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `cudaMemcpyToSymbol()` 或 `cudaMemcpyFromSymbol()` 时，不要将符号名称用引号括起来。换句话说，使用
- en: cudaMemcpyToSymbol(g_xOffset, poffsetx, Width*Height*sizeof(int));
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyToSymbol(g_xOffset, poffsetx, Width*Height*sizeof(int));
- en: not
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 不
- en: cudaMemcpyToSymbol("g_xOffset", poffsetx, ... );
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyToSymbol("g_xOffset", poffsetx, ... );
- en: Both formulations work, but the latter formulation will compile for any symbol
    name (even undefined symbols). If you want the compiler to report errors for invalid
    symbols, avoid the quotation marks.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 两种形式都有效，但后者的形式可以为任何符号名称（即使是未定义的符号）编译。如果希望编译器报告无效符号的错误，避免使用引号。
- en: CUDA runtime applications can query the pointer corresponding to a static allocation
    by calling `cudaGetSymbolAddress().`
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 运行时应用程序可以通过调用 `cudaGetSymbolAddress()` 查询与静态分配对应的指针。
- en: cudaError_t cudaGetSymbolAddress( void **devPtr, char *symbol );
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaGetSymbolAddress( void **devPtr, char *symbol );
- en: '*Beware:* It is all too easy to pass the symbol for a statically declared device
    memory allocation to a CUDA kernel, but this does not work. You must call `cudaGetSymbolAddress()`
    and use the resulting pointer.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：* 将静态声明的设备内存分配符号传递给 CUDA 内核是很容易的，但这样是行不通的。你必须调用`cudaGetSymbolAddress()`并使用返回的指针。'
- en: Driver API
  id: totrans-355
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序 API
- en: Developers using the driver API can obtain pointers to statically allocated
    memory by calling `cuModuleGetGlobal().`
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 使用驱动程序 API 的开发者可以通过调用`cuModuleGetGlobal()`来获取指向静态分配内存的指针。
- en: '[Click here to view code image](ch05_images.html#p139pro01a)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p139pro01a)'
- en: CUresult CUDAAPI cuModuleGetGlobal(CUdeviceptr *dptr, size_t *bytes,
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuModuleGetGlobal(CUdeviceptr *dptr, size_t *bytes,
- en: CUmodule hmod, const char *name);
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: CUmodule hmod, const char *name);
- en: Note that `cuModuleGetGlobal()` passes back both the base pointer and the size
    of the object. If the size is not needed, developers can pass `NULL` for the `bytes`
    parameter. Once this pointer has been obtained, the memory can be accessed by
    passing the `CUdeviceptr` to memory copy calls or CUDA kernel invocations.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`cuModuleGetGlobal()`返回的既有基础指针，又有对象的大小。如果不需要大小，开发者可以为`bytes`参数传递`NULL`。一旦获取了该指针，内存就可以通过传递`CUdeviceptr`给内存复制调用或
    CUDA 内核调用来访问。
- en: 5.2.5\. Memset APIs
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5\. Memset API
- en: For developer convenience, CUDA provides 1D and 2D memset functions. Since they
    are implemented using kernels, they are asynchronous even when no stream parameter
    is specified. For applications that must serialize the execution of a memset within
    a stream, however, there are `*Async()` variants that take a stream parameter.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便开发人员，CUDA 提供了 1D 和 2D memset 函数。由于它们是使用内核实现的，即使没有指定流参数，它们也是异步的。然而，对于那些必须在流内序列化执行
    memset 的应用程序，有`*Async()`版本，它们需要流参数。
- en: CUDA Runtime
  id: totrans-363
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA 运行时
- en: 'The CUDA runtime supports byte-sized memset only:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 运行时仅支持字节大小的 memset：
- en: '[Click here to view code image](ch05_images.html#p139pro02a)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p139pro02a)'
- en: cudaError_t cudaMemset(void *devPtr, int value, size_t count);
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMemset(void *devPtr, int value, size_t count);
- en: cudaError_t cudaMemset2D(void *devPtr, size_t pitch, int value,
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMemset2D(void *devPtr, size_t pitch, int value,
- en: size_t width, size_t height);
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: size_t 宽度，size_t 高度);
- en: The pitch parameter specifies the bytes per row of the memset operation.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: pitch 参数指定 memset 操作的每行字节数。
- en: Driver API
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序 API
- en: The driver API supports 1D and 2D memset of a variety of sizes, shown in [Table
    5.2](ch05.html#ch05tab02). These memset functions take the destination pointer,
    value to set, and number of values to write starting at the base address. The
    pitch parameter is the bytes per row (not elements per row!).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序 API 支持各种大小的 1D 和 2D memset，如[表 5.2](ch05.html#ch05tab02)所示。这些 memset 函数接受目标指针、要设置的值和从基础地址开始写入的值的数量。pitch
    参数是每行字节数（不是每行元素数！）。
- en: '[Click here to view code image](ch05_images.html#p140pro01a)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p140pro01a)'
- en: CUresult CUDAAPI cuMemsetD8(CUdeviceptr dstDevice, unsigned char uc,
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD8(CUdeviceptr dstDevice, unsigned char uc,
- en: size_t N);
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N);
- en: CUresult CUDAAPI cuMemsetD16(CUdeviceptr dstDevice, unsigned short
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD16(CUdeviceptr dstDevice, 无符号短整数
- en: us, size_t N);
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: us, 大小_t N);
- en: CUresult CUDAAPI cuMemsetD32(CUdeviceptr dstDevice, unsigned int ui,
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD32(CUdeviceptr dstDevice, 无符号整数 ui,
- en: size_t N);
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 大小_t N);
- en: CUresult CUDAAPI cuMemsetD2D8(CUdeviceptr dstDevice, size_t dstPitch,
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD2D8(CUdeviceptr dstDevice, 大小_t dstPitch,
- en: unsigned char uc, size_t Width, size_t Height);
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号字符 uc, 大小_t 宽度, 大小_t 高度);
- en: CUresult CUDAAPI cuMemsetD2D16(CUdeviceptr dstDevice, size_t
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD2D16(CUdeviceptr dstDevice, 大小_t
- en: dstPitch, unsigned short us, size_t Width, size_t Height);
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: dstPitch, 无符号短整数 us, 大小_t 宽度, 大小_t 高度);
- en: CUresult CUDAAPI cuMemsetD2D32(CUdeviceptr dstDevice, size_t
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD2D32(CUdeviceptr dstDevice, 大小_t
- en: dstPitch, unsigned int ui, size_t Width, size_t Height);
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: dstPitch, 无符号整数 ui, 大小_t 宽度, 大小_t 高度);
- en: '![Image](graphics/05tab02.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/05tab02.jpg)'
- en: '*Table 5.2* Memset Variations'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.2* Memset 变种'
- en: Now that CUDA runtime and driver API functions can peacefully coexist in the
    same application, CUDA runtime developers can use these functions as needed. The
    `unsigned char`, `unsigned short`, and `unsigned int` parameters just specify
    a bit pattern; to fill a global memory range with some other type, such as `float`,
    use a `volatile union` to coerce the `float` to `unsigned int`.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 CUDA 运行时和驱动程序 API 函数可以在同一应用程序中和平共存，CUDA 运行时开发人员可以根据需要使用这些函数。`无符号字符`、`无符号短整数`
    和 `无符号整数` 参数仅指定位模式；要用其他类型（例如 `float`）填充全局内存范围，可以使用 `volatile union` 强制将 `float`
    转换为 `无符号整数`。
- en: 5.2.6\. Pointer Queries
  id: totrans-388
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.6\. 指针查询
- en: CUDA tracks all of its memory allocations, and provides APIs that enable applications
    to query CUDA about pointers that were passed in from some other party. Libraries
    or plugins may wish to pursue different strategies based on this information.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 跟踪其所有内存分配，并提供 API，使应用程序能够查询从其他方传入的指针。库或插件可能希望根据这些信息采取不同的策略。
- en: CUDA Runtime
  id: totrans-390
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA运行时
- en: The `cudaPointerGetAttributes()` function takes a pointer as input and passes
    back a `cudaPointerAttributes` structure containing information about the pointer.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaPointerGetAttributes()` 函数接受一个指针作为输入，并返回一个包含指针信息的 `cudaPointerAttributes`
    结构体。'
- en: struct cudaPointerAttributes {
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 结构体 cudaPointerAttributes {
- en: enum cudaMemoryType memoryType;
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 枚举 cudaMemoryType 内存类型;
- en: int device;
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: int 设备;
- en: void *devicePointer;
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: void *设备指针;
- en: void *hostPointer;
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: void *主机指针;
- en: '}'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: When UVA is in effect, pointers are unique process-wide, so there is no ambiguity
    as to the input pointer’s address space. When UVA is not in effect, the input
    pointer is assumed to be in the current device’s address space ([Table 5.3](ch05.html#ch05tab03)).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 当 UVA 生效时，指针在进程范围内是唯一的，因此输入指针的地址空间没有歧义。当 UVA 不生效时，假定输入指针位于当前设备的地址空间中（[表 5.3](ch05.html#ch05tab03)）。
- en: '![Image](graphics/05tab03.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/05tab03.jpg)'
- en: '*Table 5.3* `cudaPointerAttributes` Members'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.3* `cudaPointerAttributes` 成员'
- en: Driver API
  id: totrans-401
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序 API
- en: Developers can query the address range where a given device pointer resides
    using the `cuMemGetAddressRange()` function.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以使用 `cuMemGetAddressRange()` 函数查询给定设备指针所在的地址范围。
- en: '[Click here to view code image](ch05_images.html#p141pro01a)'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p141pro01a)'
- en: CUresult CUDAAPI cuMemGetAddressRange(CUdeviceptr *pbase, size_t
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemGetAddressRange(CUdeviceptr *pbase, size_t
- en: '*psize, CUdeviceptr dptr);'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '*psize, CUdeviceptr dptr);'
- en: This function takes a device pointer as input and passes back the base and size
    of the allocation containing that device pointer.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数以设备指针作为输入，并返回包含该设备指针的分配的基地址和大小。
- en: With the addition of UVA in CUDA 4.0, developers can query CUDA to get even
    more information about an address using `cuPointerGetAttribute().`
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 CUDA 4.0 中 UVA 的加入，开发者可以使用 `cuPointerGetAttribute()` 查询 CUDA，以获取有关地址的更多信息。
- en: '[Click here to view code image](ch05_images.html#p142pro01a)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p142pro01a)'
- en: CUresult CUDAAPI cuPointerGetAttribute(void *data, CUpointer_
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuPointerGetAttribute(void *data, CUpointer_
- en: attribute attribute, CUdeviceptr ptr);
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 属性属性，CUdeviceptr ptr);
- en: This function takes a device pointer as input and passes back the information
    corresponding to the attribute parameter, as shown in [Table 5.4](ch05.html#ch05tab04).
    Note that for unified addresses, using `CU_POINTER_ATTRIBUTE_DEVICE_POINTER` or
    `CU_POINTER_ATTRIBUTE_HOST_POINTER` will cause the same pointer value to be returned
    as the one passed in.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数以设备指针作为输入，并返回与属性参数对应的信息，如[表 5.4](ch05.html#ch05tab04)所示。注意，对于统一地址，使用 `CU_POINTER_ATTRIBUTE_DEVICE_POINTER`
    或 `CU_POINTER_ATTRIBUTE_HOST_POINTER` 会返回与传入值相同的指针值。
- en: '![Image](graphics/05tab04.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/05tab04.jpg)'
- en: '*Table 5.4* `cuPointerAttribute` Usage'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.4* `cuPointerAttribute` 使用'
- en: Kernel Queries
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内核查询
- en: On SM 2.x (Fermi) hardware and later, developers can query whether a given pointer
    points into global space. The `__isGlobal()` intrinsic
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SM 2.x（Fermi）及更高版本的硬件上，开发者可以查询给定指针是否指向全局空间。`__isGlobal()` 内建函数
- en: unsigned int __isGlobal( const void *p );
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int __isGlobal( const void *p );
- en: returns 1 if the input pointer refers to global memory and 0 otherwise.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入指针指向全局内存，则返回 1，否则返回 0。
- en: 5.2.7\. Peer-to-Peer Access
  id: totrans-418
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.7\. 点对点访问
- en: Under certain circumstances, SM 2.0-class and later hardware can map memory
    belonging to other, similarly capable GPUs. The following conditions apply.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，SM 2.0 类及更高版本的硬件可以映射属于其他相似能力 GPU 的内存。以下条件适用。
- en: • UVA must be in effect.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: • 必须启用 UVA。
- en: • Both GPUs must be Fermi-class and be based on the same chip.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: • 两个 GPU 必须是 Fermi 类并且基于相同的芯片。
- en: • The GPUs must be on the same I/O hub.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: • 两个 GPU 必须位于同一 I/O 集线器上。
- en: Since peer-to-peer mapping is intrinsically a multi-GPU feature, it is described
    in detail in the multi-GPU chapter (see [Section 9.2](ch09.html#ch09lev1sec2)).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点对点映射本质上是一个多 GPU 特性，因此在多 GPU 章节中有详细描述（见[第 9.2 节](ch09.html#ch09lev1sec2)）。
- en: 5.2.8\. Reading and Writing Global Memory
  id: totrans-424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.8\. 读写全局内存
- en: CUDA kernels can read or write global memory using standard C semantics such
    as pointer indirection (`operator*, operator->`) or array subscripting (`operator[]`).
    Here is a simple templatized kernel to write a constant into a memory range.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 内核可以使用标准 C 语义，如指针间接寻址（`operator*`, `operator->`）或数组下标（`operator[]`），来读写全局内存。以下是一个简单的模板化内核，用于将常量写入内存范围。
- en: '[Click here to view code image](ch05_images.html#p143pro01a)'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p143pro01a)'
- en: template<class T>
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: __global__ void
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: GlobalWrites( T *out, T value, size_t N )
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: GlobalWrites( T *out, T value, size_t N )
- en: '{'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N;
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: out[i] = value;
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] = value;
- en: '}'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'This kernel works correctly for any inputs: any component size, any block size,
    any grid size. Its code is intended more for illustrative purposes than maximum
    performance. CUDA kernels that use more registers and operate on multiple values
    in the inner loop go faster, but for some block and grid configurations, its performance
    is perfectly acceptable. In particular, provided the base address and block size
    are specified correctly, it performs coalesced memory transactions that maximize
    memory bandwidth.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核对于任何输入都能正常工作：任何组件大小、任何块大小、任何网格大小。其代码更多是为了说明用途，而非追求最大性能。使用更多寄存器并在内部循环中处理多个值的
    CUDA 内核运行更快，但对于某些块和网格配置，其性能完全可以接受。特别是，前提是基地址和块大小正确指定，它会执行合并内存事务，最大化内存带宽。
- en: 5.2.9\. Coalescing Constraints
  id: totrans-438
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.9\. 合并约束
- en: For best performance when reading and writing data, CUDA kernels must perform
    *coalesced* memory transactions. Any memory transaction that does not meet the
    full set of criteria needed for coalescing is “uncoalesced.” The penalty for uncoalesced
    memory transactions varies from 2x to 8x, depending on the chip implementation.
    Coalesced memory transactions have a much less dramatic impact on performance
    on more recent hardware, as shown in [Table 5.5](ch05.html#ch05tab05).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在读写数据时获得最佳性能，CUDA 内核必须执行 *合并* 内存事务。任何不满足合并所需全部条件的内存事务被视为“非合并”。非合并内存事务的性能惩罚因芯片实现的不同而有所不同，通常为
    2x 到 8x。合并内存事务在较新的硬件上对性能的影响较小，如 [表 5.5](ch05.html#ch05tab05) 所示。
- en: '![Image](graphics/05tab05.jpg)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/05tab05.jpg)'
- en: '*Table 5.5* Bandwidth Penalties for Uncoalesced Memory Access'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.5* 非合并内存访问的带宽惩罚'
- en: Transactions are coalesced on a per-warp basis. A simplified set of criteria
    must be met in order for the memory read or write being performed by the warp
    to be coalesced.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 事务是在每个 warp 的基础上合并的。为了使 warp 执行的内存读写操作合并，必须满足一组简化的条件。
- en: • The words must be at least 32 bits in size. Reading or writing bytes or 16-bit
    words is always uncoalesced.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: • 字长必须至少为 32 位。读取或写入字节或 16 位字始终是非合并的。
- en: • The addresses being accessed by the threads of the warp must be contiguous
    and increasing (i.e., offset by the thread ID).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: • 由warp线程访问的地址必须是连续且递增的（即，由线程ID偏移）。
- en: • The base address of the warp (the address being accessed by the first thread
    in the warp) must be aligned as shown in [Table 5.6](ch05.html#ch05tab06).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: • warp的基地址（warp中第一个线程访问的地址）必须按照[表5.6](ch05.html#ch05tab06)所示对齐。
- en: '![Image](graphics/05tab06.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab06.jpg)'
- en: '*Table 5.6* Alignment Criteria for Coalescing'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '*表5.6* 合并对齐标准'
- en: The `ElementSizeBytes` parameter to `cuMemAllocPitch()` is intended to accommodate
    the size restriction. It specifies the size in bytes of the memory accesses intended
    by the application, so the pitch guarantees that a set of coalesced memory transactions
    for a given row of the allocation also will be coalesced for other rows.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuMemAllocPitch()`中的`ElementSizeBytes`参数用于适应大小限制。它指定应用程序所需内存访问的字节大小，因此pitch可以保证给定分配行的合并内存事务会与其他行进行合并。'
- en: Most kernels in this book perform coalesced memory transactions, provided the
    input addresses are properly aligned. NVIDIA has provided more detailed, architecture-specific
    information on how global memory transactions are handled, as detailed below.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的大多数内核执行合并内存事务，前提是输入地址已正确对齐。NVIDIA提供了更多关于全局内存事务如何处理的详细架构特定信息，具体如下所示。
- en: SM 1.x (Tesla)
  id: totrans-450
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SM 1.x（Tesla）
- en: SM 1.0 and SM 1.1 hardware require that each thread in a warp access adjacent
    memory locations in sequence, as described above. SM 1.2 and 1.3 hardware relaxed
    the coalescing constraints somewhat. To issue a coalesced memory request, divide
    each 32-thread warp into two “half warps,” lanes 0–15 and lanes 16–31\. To service
    the memory request from each half-warp, the hardware performs the following algorithm.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: SM 1.0和SM 1.1硬件要求warp中的每个线程按顺序访问相邻的内存位置，如上所述。SM 1.2和1.3硬件稍微放宽了合并约束。要发出合并内存请求，将每个32线程的warp分成两个“半warp”，即通道0–15和通道16–31。为了服务来自每个半warp的内存请求，硬件执行以下算法。
- en: '**1.** Find the active thread with the lowest thread ID and locate the memory
    segment that contains that thread’s requested address. The segment size depends
    on the word size: 1-byte requests result in 32-byte segments; 2-byte requests
    result in 64-byte segments; and all other requests result in 128-byte segments.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.** 找到具有最低线程ID的活动线程，并定位包含该线程请求地址的内存段。段大小取决于字大小：1字节请求产生32字节段；2字节请求产生64字节段；所有其他请求产生128字节段。'
- en: '**2.** Find all other active threads whose requested address lies in the same
    segment.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** 查找所有其他活动线程，这些线程的请求地址位于相同的内存段中。'
- en: '**3.** If possible, reduce the segment transaction size to 64 or 32 bytes.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** 如果可能，将段事务大小减少到64或32字节。'
- en: '**4.** Carry out the transaction and mark the services threads as inactive.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.** 执行事务并将服务过的线程标记为非活动。'
- en: '**5.** Repeat steps 1–4 until all threads in the half-warp have been serviced.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.** 重复步骤1–4，直到半warp中的所有线程都得到服务。'
- en: Although these requirements are somewhat relaxed compared to the SM 1.0–1.1
    constraints, a great deal of locality is still required for effective coalescing.
    In practice, the relaxed coalescing means the threads within a warp can permute
    the inputs within small segments of memory, if desired.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些要求相比于SM 1.0–1.1的约束有所放宽，但为了有效地合并，仍然需要大量的局部性。实际上，放宽的合并意味着，如果需要，warp中的线程可以在内存的小段中重新排列输入。
- en: SM 2.x (Fermi)
  id: totrans-458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SM 2.x（Fermi）
- en: SM 2.x and later hardware includes L1 and L2 caches. The L2 cache services the
    entire chip; the L1 caches are per-SM and may be configured to be 16K or 48K in
    size. The cache lines are 128 bytes and map to 128-byte aligned segments in device
    memory. Memory accesses that are cached in both L1 and L2 are serviced with 128-byte
    memory transactions, whereas memory accesses that are cached in L2 only are serviced
    with 32-byte memory transactions. Caching in L2 only can therefore reduce overfetch,
    for example, in the case of scattered memory accesses.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: SM 2.x及以后的硬件包括L1和L2缓存。L2缓存服务整个芯片；L1缓存是每个SM独立的，可以配置为16K或48K大小。缓存行大小为128字节，并映射到设备内存中对齐的128字节段。那些同时在L1和L2中缓存的内存访问将以128字节的内存事务进行处理，而那些仅在L2中缓存的内存访问则以32字节的内存事务进行处理。因此，仅在L2中缓存可以减少过度取值，例如，在散乱内存访问的情况下。
- en: The hardware can specify the cacheability of global memory accesses on a per-instruction
    basis. By default, the compiler emits instructions that cache memory accesses
    in both L1 and L2 (`-Xptxas -dlcm=ca`). This can be changed to cache in L2 only
    by specifying `-Xptxas -dlcm=cg`. Memory accesses that are not present in L1 but
    cached in L2 only are serviced with 32-byte memory transactions, which may improve
    cache utilization for applications that are performing scattered memory accesses.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件可以为每条指令指定全局内存访问的缓存可用性。默认情况下，编译器生成的指令会在L1和L2中缓存内存访问（`-Xptxas -dlcm=ca`）。通过指定`-Xptxas
    -dlcm=cg`，可以将缓存设置为仅在L2中缓存。那些未出现在L1但仅在L2中缓存的内存访问将以32字节的内存事务进行处理，这可能会提高执行散乱内存访问的应用程序的缓存利用率。
- en: Reading via pointers that are declared `volatile` causes any cached results
    to be discarded and for the data to be refetched. This idiom is mainly useful
    for polling host memory locations. [Table 5.7](ch05.html#ch05tab07) summarizes
    how memory requests by a warp are broken down into 128-byte cache line requests.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 通过声明为`volatile`的指针进行读取会导致任何缓存的结果被丢弃，并重新获取数据。这种惯用法主要用于轮询主机内存位置。[表 5.7](ch05.html#ch05tab07)总结了warp如何将内存请求分解为128字节的缓存行请求。
- en: '![Image](graphics/05tab07.jpg)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab07.jpg)'
- en: '*Table 5.7* SM 2.x Cache Line Requests'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.7* SM 2.x 缓存行请求'
- en: '* * *'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: On SM 2.x and higher architectures, threads within a warp can access any words
    in any order, including the same words.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在SM 2.x及更高架构中，warp中的线程可以以任意顺序访问任何字，包括相同的字。
- en: '* * *'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: SM 3.x (Kepler)
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SM 3.x（Kepler）
- en: The L2 cache architecture is the same as SM 2.x. SM 3.x does not cache global
    memory accesses in L1\. In SM 3.5, global memory may be accessed via the texture
    cache (which is 48K per SM in size) by accessing memory via `const restricted`
    pointers or by using the `__ldg()` intrinsics in `sm_35_intrinsics.h`. As when
    texturing directly from device memory, it is important not to access memory that
    might be accessed concurrently by other means, since this cache is not kept coherent
    with respect to the L2.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: L2缓存架构与SM 2.x相同。SM 3.x不在L1中缓存全局内存访问。在SM 3.5中，可以通过纹理缓存（每个SM大小为48K）访问全局内存，方法是通过`const
    restricted`指针或使用`__ldg()`内在函数（在`sm_35_intrinsics.h`中）来访问内存。就像直接从设备内存进行纹理采样一样，必须避免访问可能被其他方式并发访问的内存，因为这个缓存不会与L2保持一致性。
- en: '5.2.10\. Microbenchmarks: Peak Memory Bandwidth'
  id: totrans-470
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.10\. 微基准测试：峰值内存带宽
- en: The source code accompanying this book includes microbenchmarks that determine
    which combination of operand size, loop unroll factor, and block size maximizes
    bandwidth for a given GPU. Rewriting the earlier `GlobalWrites` code as a template
    that takes an additional parameter `n` (the number of writes to perform in the
    inner loop) yields the kernel in [Listing 5.5](ch05.html#ch05lis05).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 本书附带的源代码包括微基准测试，用于确定哪种操作数大小、循环展开因子和块大小的组合能最大化给定GPU的带宽。将早期的`GlobalWrites`代码重写为一个模板，增加一个额外的参数`n`（表示内循环中执行的写入次数），可以得到[列表5.5](ch05.html#ch05lis05)中的内核代码。
- en: '*Listing 5.5.* `GlobalWrites` kernel.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表5.5.* `GlobalWrites`内核。'
- en: '[Click here to view code image](ch05_images.html#p05lis05a)'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis05a)'
- en: '* * *'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, const int n>
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, const int n>
- en: __global__ void
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: GlobalWrites( T *out, T value, size_t N )
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: GlobalWrites( T *out, T value, size_t N )
- en: '{'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t i;
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i;
- en: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N-n*blockDim.x*gridDim.x;
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: i < N-n*blockDim.x*gridDim.x;
- en: i += n*blockDim.x*gridDim.x ) {
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: i += n*blockDim.x*gridDim.x ) {
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: out[index] = value;
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: out[index] = value;
- en: '}'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // to avoid the (index<N) conditional in the inner loop,
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: // 为了避免内循环中的(index<N)条件判断，
- en: // we left off some work at the end
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: // 我们在最后留下了一些工作
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: if ( index<N ) out[index] = value;
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index<N ) out[index] = value;
- en: '}'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '`ReportRow()`, the function given in [Listing 5.6](ch05.html#ch05lis06) that
    writes one row of output calls by calling a template function `BandwidthWrites`
    (not shown), reports the bandwidth for a given type, grid, and block size.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReportRow()`，如[列表5.6](ch05.html#ch05lis06)所示的函数，通过调用模板函数`BandwidthWrites`（未显示）来写入一行输出，并报告给定类型、网格和块大小的带宽。'
- en: '*Listing 5.6.* `ReportRow` function.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表5.6.* `ReportRow`函数。'
- en: '[Click here to view code image](ch05_images.html#p05lis06a)'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis06a)'
- en: '* * *'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, const int n, bool bOffset>
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, const int n, bool bOffset>
- en: double
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: double
- en: ReportRow( size_t N,
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: ReportRow( size_t N,
- en: size_t threadStart,
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: size_t threadStart,
- en: size_t threadStop,
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: size_t threadStop,
- en: size_t cBlocks )
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: size_t cBlocks )
- en: '{'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int maxThreads = 0;
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: int maxThreads = 0;
- en: double maxBW = 0.0;
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: double maxBW = 0.0;
- en: printf( "%d\t", n );
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%d\t", n );
- en: for ( int cThreads = threadStart;
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int cThreads = threadStart;
- en: cThreads <= threadStop;
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: cThreads <= threadStop;
- en: cThreads *= 2 ) {
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: cThreads *= 2 ) {
- en: double bw;
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: double bw;
- en: bw = BandwidthWrites<T,n,bOffset>( N, cBlocks, cThreads );
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: bw = BandwidthWrites<T,n,bOffset>( N, cBlocks, cThreads );
- en: if ( bw > maxBW ) {
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: if ( bw > maxBW ) {
- en: maxBW = bw;
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: maxBW = bw;
- en: maxThreads = cThreads;
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: maxThreads = cThreads;
- en: '}'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: printf( "%.2f\t", bw );
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%.2f\t", bw );
- en: '}'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: printf( "%.2f\t%d\n", maxBW, maxThreads );
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%.2f\t%d\n", maxBW, maxThreads );
- en: return maxBW;
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: return maxBW;
- en: '}'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The `threadStart` and `threadStop` parameters typically are 32 and 512, 32 being
    the warp size and the minimum number of threads per block that can occupy the
    machine. The `bOffset` template parameter specifies whether `BandwidthWrites`
    should offset the base pointer, causing all memory transactions to become uncoalesced.
    If the program is invoked with the `--uncoalesced` command line option, it will
    perform the bandwidth measurements with the offset pointer.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '`threadStart` 和 `threadStop` 参数通常为 32 和 512，其中 32 是波束大小，是每个块能占用的最小线程数。`bOffset`
    模板参数指定是否 `BandwidthWrites` 应该偏移基指针，从而导致所有内存事务变得不合并。如果程序通过 `--uncoalesced` 命令行选项被调用，它将使用偏移指针进行带宽测量。'
- en: Note that depending on `sizeof(T)`, kernels with `n` above a certain level will
    fall off a performance cliff as the number of temporary variables in the inner
    loop grows too high to hold in registers.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，根据 `sizeof(T)` 的不同，若 `n` 超过某一特定值，内核性能将急剧下降，因为内层循环中的临时变量数量增长过快，无法保存在寄存器中。
- en: The five applications summarized in [Table 5.8](ch05.html#ch05tab08) implement
    this strategy. They measure the memory bandwidth delivered for different operand
    sizes (8-, 16-, 32-, 64-, and 128-bit), threadblock sizes (32, 64, 128, 256, and
    512), and loop unroll factors (1–16). CUDA hardware isn’t necessarily sensitive
    to all of these parameters. For example, many parameter settings enable a GK104
    to deliver 140GB/s of bandwidth via texturing, but only if the operand size is
    at least 32-bit. For a given workload and hardware, however, the microbenchmarks
    highlight which parameters matter. Also, for small operand sizes, they highlight
    how loop unrolling can help increase performance (not all applications can be
    refactored to read larger operands).
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表 5.8](ch05.html#ch05tab08) 中总结的五个应用实现了这一策略。它们测量了不同操作数大小（8 位、16 位、32 位、64
    位和 128 位）、线程块大小（32、64、128、256 和 512）以及循环展开因子（1–16）下的内存带宽。CUDA 硬件并不一定对所有这些参数都敏感。例如，许多参数设置可以让
    GK104 通过纹理处理提供 140GB/s 的带宽，但前提是操作数大小至少为 32 位。然而，对于特定的工作负载和硬件，微基准测试可以突出哪些参数是重要的。另外，对于小操作数大小，它们突出了如何通过循环展开来提高性能（并非所有应用都可以重构为读取更大的操作数）。
- en: '![Image](graphics/05tab08.jpg)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab08.jpg)'
- en: '*Table 5.8* Memory Bandwidth Microbenchmarks'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.8* 内存带宽微基准测试'
- en: '[Listing 5.7](ch05.html#ch05lis07) gives example output from `globalRead.cu`,
    run on a GeForce GTX 680 GPU. The output is grouped by operand size, from bytes
    to 16-byte quads; the leftmost column of each group gives the loop unroll factor.
    The bandwidth delivered for blocks of sizes 32 to 512 is given in each column,
    and the `maxBW` and `maxThreads` columns give the highest bandwidth and the block
    size that delivered the highest bandwidth, respectively.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.7](ch05.html#ch05lis07) 给出了从 `globalRead.cu` 输出的示例结果，该程序在 GeForce GTX
    680 GPU 上运行。输出按操作数大小分组，从字节到16字节四元组；每组的最左列给出了循环展开因子。每列给出了32到512大小块的带宽，并且 `maxBW`
    和 `maxThreads` 列分别给出了最高带宽和提供最高带宽的块大小。'
- en: The GeForce GTX 680 can deliver up to 140GB/s, so [Listing 5.7](ch05.html#ch05lis07)
    makes it clear that when reading 8- and 16-bit words on SM 3.0, global loads are
    not the way to go. Bytes deliver at most 60GB/s, and 16-bit words deliver at most
    101GB/s.^([11](ch05.html#ch05fn11)) For 32-bit operands, a 2x loop unroll and
    at least 256 threads per block are needed to get maximum bandwidth.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: GeForce GTX 680 的带宽可达到 140GB/s，因此 [列表 5.7](ch05.html#ch05lis07) 清楚地表明，在 SM 3.0
    上读取 8 位和 16 位字时，全球加载并不是最佳选择。字节的最大带宽为 60GB/s，16 位字的最大带宽为 101GB/s。^([11](ch05.html#ch05fn11))
    对于 32 位操作数，需要 2 倍循环展开并且每个块至少有 256 个线程才能获得最大带宽。
- en: '[11](ch05.html#ch05fn11a). Texturing works better. Readers can run `globalReadTex.cu`
    to confirm.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '[11](ch05.html#ch05fn11a)。纹理化效果更好。读者可以运行 `globalReadTex.cu` 进行验证。'
- en: These microbenchmarks can help developers optimize their bandwidth-bound applications.
    Choose the one whose memory access pattern most closely resembles your application,
    and either run the microbenchmark on the target GPU or, if possible, modify the
    microbenchmark to resemble the actual workload more closely and run it to determine
    the optimal parameters.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 这些微基准测试可以帮助开发者优化带宽受限的应用程序。选择一个内存访问模式最接近你应用程序的基准测试，或者，如果可能的话，修改微基准测试使其更接近实际工作负载，然后运行它以确定最佳参数。
- en: '*Listing 5.7.* Sample output, `globalRead.cu`.'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 5.7.* 示例输出，`globalRead.cu`。'
- en: '[Click here to view code image](ch05_images.html#p05lis07a)'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis07a)'
- en: '* * *'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Running globalRead.cu microbenchmark on GeForce GTX 680
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GeForce GTX 680 上运行 `globalRead.cu` 微基准测试
- en: Using coalesced memory transactions
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 使用合并内存事务
- en: 'Operand size: 1 byte'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数大小：1 字节
- en: 'Input size: 16M operands'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小：16M 操作数
- en: Block Size
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32     64     128    256    512    maxBW   maxThreads
- en: 1       9.12    17.39   30.78   30.78   28.78   30.78   128
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 1       9.12    17.39   30.78   30.78   28.78   30.78   128
- en: 2       18.37   34.54   56.36   53.53   49.33   56.36   128
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 2       18.37   34.54   56.36   53.53   49.33   56.36   128
- en: 3       23.55   42.32   61.56   60.15   52.91   61.56   128
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 3       23.55   42.32   61.56   60.15   52.91   61.56   128
- en: 4       21.25   38.26   58.99   58.09   51.26   58.99   128
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 4       21.25   38.26   58.99   58.09   51.26   58.99   128
- en: 5       25.29   42.17   60.13   58.49   52.57   60.13   128
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 5       25.29   42.17   60.13   58.49   52.57   60.13   128
- en: 6       25.68   42.15   59.93   55.42   47.46   59.93   128
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 6       25.68   42.15   59.93   55.42   47.46   59.93   128
- en: 7       28.84   47.03   56.20   51.41   41.41   56.20   128
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 7       28.84   47.03   56.20   51.41   41.41   56.20   128
- en: 8       29.88   48.55   55.75   50.68   39.96   55.75   128
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 8       29.88   48.55   55.75   50.68   39.96   55.75   128
- en: 9       28.65   47.75   56.84   51.17   37.56   56.84   128
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 9       28.65   47.75   56.84   51.17   37.56   56.84   128
- en: 10      27.35   45.16   52.99   46.30   32.94   52.99   128
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 10      27.35   45.16   52.99   46.30   32.94   52.99   128
- en: 11      22.27   38.51   48.17   42.74   32.81   48.17   128
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 11      22.27   38.51   48.17   42.74   32.81   48.17   128
- en: 12      23.39   40.51   49.78   42.42   31.89   49.78   128
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 12      23.39   40.51   49.78   42.42   31.89   49.78   128
- en: 13      21.62   37.49   40.89   34.98   21.43   40.89   128
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 13      21.62   37.49   40.89   34.98   21.43   40.89   128
- en: 14      18.55   32.12   36.04   31.41   19.96   36.04   128
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 14      18.55   32.12   36.04   31.41   19.96   36.04   128
- en: 15      21.47   36.87   39.94   33.36   19.98   39.94   128
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 15      21.47   36.87   39.94   33.36   19.98   39.94   128
- en: 16      21.59   36.79   39.49   32.71   19.42   39.49   128
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 16      21.59   36.79   39.49   32.71   19.42   39.49   128
- en: 'Operand size: 2 bytes'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数大小：2 字节
- en: 'Input size: 16M operands'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小：16M 操作数
- en: Block Size
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       18.29   35.07   60.30   59.16   56.06   60.30   128
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 1       18.29   35.07   60.30   59.16   56.06   60.30   128
- en: 2       34.94   64.39   94.28   92.65   85.99   94.28   128
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 2       34.94   64.39   94.28   92.65   85.99   94.28   128
- en: 3       45.02   72.90   101.38  99.02   90.07   101.38  128
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 3       45.02   72.90   101.38  99.02   90.07   101.38  128
- en: 4       38.54   68.35   100.30  98.29   90.28   100.30  128
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 4       38.54   68.35   100.30  98.29   90.28   100.30  128
- en: 5       45.49   75.73   98.68   98.11   90.05   98.68   128
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 5       45.49   75.73   98.68   98.11   90.05   98.68   128
- en: 6       47.58   77.50   100.35  97.15   86.17   100.35  128
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 6       47.58   77.50   100.35  97.15   86.17   100.35  128
- en: 7       53.64   81.04   92.89   87.39   74.14   92.89   128
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 7       53.64   81.04   92.89   87.39   74.14   92.89   128
- en: 8       44.79   74.02   89.19   83.96   69.65   89.19   128
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 8       44.79   74.02   89.19   83.96   69.65   89.19   128
- en: 9       47.63   76.63   91.60   83.52   68.06   91.60   128
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 9       47.63   76.63   91.60   83.52   68.06   91.60   128
- en: 10      51.02   79.82   93.85   84.69   66.62   93.85   128
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 10      51.02   79.82   93.85   84.69   66.62   93.85   128
- en: 11      42.00   72.11   88.23   79.24   62.27   88.23   128
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 11      42.00   72.11   88.23   79.24   62.27   88.23   128
- en: 12      40.53   69.27   85.75   76.32   59.73   85.75   128
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 12      40.53   69.27   85.75   76.32   59.73   85.75   128
- en: 13      44.90   73.44   78.08   66.96   41.27   78.08   128
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 13      44.90   73.44   78.08   66.96   41.27   78.08   128
- en: 14      39.18   68.43   74.46   63.27   39.27   74.46   128
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 14      39.18   68.43   74.46   63.27   39.27   74.46   128
- en: 15      37.60   64.11   69.93   60.22   37.09   69.93   128
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 15      37.60   64.11   69.93   60.22   37.09   69.93   128
- en: 16      40.36   67.90   73.07   60.79   36.66   73.07   128
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 16      40.36   67.90   73.07   60.79   36.66   73.07   128
- en: 'Operand size: 4 bytes'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数大小：4 字节
- en: 'Input size: 16M operands'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小：16M 操作数
- en: Block Size
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       36.37   67.89   108.04  105.99  104.09  108.04  128
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 1       36.37   67.89   108.04  105.99  104.09  108.04  128
- en: 2       73.85   120.90  139.91  139.93  136.04  139.93  256
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 2       73.85   120.90  139.91  139.93  136.04  139.93  256
- en: 3       62.62   109.24  140.07  139.66  138.38  140.07  128
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 3       62.62   109.24  140.07  139.66  138.38  140.07  128
- en: 4       56.02   101.73  138.70  137.42  135.10  138.70  128
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 4       56.02   101.73  138.70  137.42  135.10  138.70  128
- en: 5       87.34   133.65  140.64  140.33  139.00  140.64  128
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 5       87.34   133.65  140.64  140.33  139.00  140.64  128
- en: 6       100.64  137.47  140.61  139.53  127.18  140.61  128
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 6       100.64  137.47  140.61  139.53  127.18  140.61  128
- en: 7       89.08   133.99  139.60  138.23  124.28  139.60  128
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 7       89.08   133.99  139.60  138.23  124.28  139.60  128
- en: 8       58.46   103.09  129.24  122.28  110.58  129.24  128
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 8       58.46   103.09  129.24  122.28  110.58  129.24  128
- en: 9       68.99   116.59  134.17  128.64  114.80  134.17  128
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 9       68.99   116.59  134.17  128.64  114.80  134.17  128
- en: 10      54.64   97.90   123.91  118.84  106.96  123.91  128
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 10      54.64   97.90   123.91  118.84  106.96  123.91  128
- en: 11      64.35   110.30  131.43  123.90  109.31  131.43  128
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 11      64.35   110.30  131.43  123.90  109.31  131.43  128
- en: 12      68.03   113.89  130.95  125.40  108.02  130.95  128
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 12      68.03   113.89  130.95  125.40  108.02  130.95  128
- en: 13      71.34   117.88  123.85  113.08  76.98   123.85  128
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 13      71.34   117.88  123.85  113.08  76.98   123.85  128
- en: 14      54.72   97.31   109.41  101.28  71.13   109.41  128
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 14      54.72   97.31   109.41  101.28  71.13   109.41  128
- en: 15      67.28   111.24  118.88  108.35  72.30   118.88  128
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 15      67.28   111.24  118.88  108.35  72.30   118.88  128
- en: 16      63.32   108.56  117.77  103.24  69.76   117.77  128
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 16      63.32   108.56  117.77  103.24  69.76   117.77  128
- en: 'Operand size: 8 bytes'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '操作数大小: 8 字节'
- en: 'Input size: 16M operands'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '输入大小: 16M 操作数'
- en: Block Size
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       74.64   127.73  140.91  142.08  142.16  142.16  512
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 1       74.64   127.73  140.91  142.08  142.16  142.16  512
- en: 2       123.70  140.35  141.31  141.99  142.42  142.42  512
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 2       123.70  140.35  141.31  141.99  142.42  142.42  512
- en: 3       137.28  141.15  140.86  141.94  142.63  142.63  512
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 3       137.28  141.15  140.86  141.94  142.63  142.63  512
- en: 4       128.38  141.39  141.85  142.56  142.00  142.56  256
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 4       128.38  141.39  141.85  142.56  142.00  142.56  256
- en: 5       117.57  140.95  141.17  142.08  141.78  142.08  256
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 5       117.57  140.95  141.17  142.08  141.78  142.08  256
- en: 6       112.10  140.62  141.48  141.86  141.95  141.95  512
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 6       112.10  140.62  141.48  141.86  141.95  141.95  512
- en: 7       85.02   134.82  141.59  141.50  141.09  141.59  128
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 7       85.02   134.82  141.59  141.50  141.09  141.59  128
- en: 8       94.44   138.71  140.86  140.25  128.91  140.86  128
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 8       94.44   138.71  140.86  140.25  128.91  140.86  128
- en: 9       100.69  139.83  141.09  141.45  127.82  141.45  256
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 9       100.69  139.83  141.09  141.45  127.82  141.45  256
- en: 10      92.51   137.76  140.74  140.93  126.50  140.93  256
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 10      92.51   137.76  140.74  140.93  126.50  140.93  256
- en: 11      104.87  140.38  140.67  136.70  128.48  140.67  128
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 11      104.87  140.38  140.67  136.70  128.48  140.67  128
- en: 12      97.71   138.62  140.12  135.74  125.37  140.12  128
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 12      97.71   138.62  140.12  135.74  125.37  140.12  128
- en: 13      95.87   138.28  139.90  134.18  123.41  139.90  128
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 13      95.87   138.28  139.90  134.18  123.41  139.90  128
- en: 14      85.69   134.18  133.84  131.16  120.95  134.18  64
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 14      85.69   134.18  133.84  131.16  120.95  134.18  64
- en: 15      94.43   135.43  135.30  133.47  120.52  135.43  64
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 15      94.43   135.43  135.30  133.47  120.52  135.43  64
- en: 16      91.62   136.69  133.59  129.95  117.99  136.69  64
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 16      91.62   136.69  133.59  129.95  117.99  136.69  64
- en: 'Operand size: 16 bytes'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '操作数大小: 16 字节'
- en: 'Input size: 16M operands'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '输入大小: 16M 操作数'
- en: Block Size
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       125.37  140.67  141.15  142.06  142.59  142.59  512
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 1       125.37  140.67  141.15  142.06  142.59  142.59  512
- en: 2       131.26  141.95  141.72  142.32  142.49  142.49  512
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 2       131.26  141.95  141.72  142.32  142.49  142.49  512
- en: 3       141.03  141.65  141.63  142.43  138.44  142.43  256
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 3       141.03  141.65  141.63  142.43  138.44  142.43  256
- en: 4       139.90  142.70  142.62  142.20  142.84  142.84  512
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 4       139.90  142.70  142.62  142.20  142.84  142.84  512
- en: 5       138.24  142.08  142.18  142.79  140.94  142.79  256
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 5       138.24  142.08  142.18  142.79  140.94  142.79  256
- en: 6       131.41  142.45  142.32  142.51  142.08  142.51  256
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 6       131.41  142.45  142.32  142.51  142.08  142.51  256
- en: 7       131.98  142.26  142.27  142.11  142.26  142.27  128
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 7       131.98  142.26  142.27  142.11  142.26  142.27  128
- en: 8       132.70  142.47  142.10  142.67  142.19  142.67  256
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 8       132.70  142.47  142.10  142.67  142.19  142.67  256
- en: 9       136.58  142.28  141.89  142.42  142.09  142.42  256
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 9       136.58  142.28  141.89  142.42  142.09  142.42  256
- en: 10      135.61  142.67  141.85  142.86  142.36  142.86  256
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 10      135.61  142.67  141.85  142.86  142.36  142.86  256
- en: 11      136.27  142.48  142.45  142.14  142.41  142.48  64
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 11      136.27  142.48  142.45  142.14  142.41  142.48  64
- en: 12      130.62  141.79  142.06  142.39  142.16  142.39  256
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 12      130.62  141.79  142.06  142.39  142.16  142.39  256
- en: 13      107.98  103.07  105.54  106.51  107.35  107.98  32
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 13      107.98  103.07  105.54  106.51  107.35  107.98  32
- en: 14      103.53  95.38   96.38   98.34   102.92  103.53  32
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 14      103.53  95.38   96.38   98.34   102.92  103.53  32
- en: 15      89.47   84.86   85.31   87.01   90.26   90.26   512
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 15      89.47   84.86   85.31   87.01   90.26   90.26   512
- en: 16      81.53   75.49   75.82   74.36   76.91   81.53   32
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 16      81.53   75.49   75.82   74.36   76.91   81.53   32
- en: '* * *'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.2.11\. Atomic Operations
  id: totrans-640
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.11\. 原子操作
- en: Support for atomic operations was added in SM 1.x, but they were prohibitively
    slow; atomics on global memory were improved on SM 2.x (Fermi-class) hardware
    and vastly improved on SM 3.x (Kepler-class) hardware.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 原子操作的支持在SM 1.x中被引入，但它们非常慢；在SM 2.x（Fermi架构）硬件上，全局内存上的原子操作有所改善，而在SM 3.x（Kepler架构）硬件上则得到了极大的提升。
- en: Most atomic operations, such as `atomicAdd()`, enable code to be simplified
    by replacing reductions (which often require shared memory and synchronization)
    with “fire and forget” semantics. Until SM 3.x hardware arrived, however, that
    type of programming idiom incurred huge performance degradations because pre-Kepler
    hardware was not efficient at dealing with “contended” memory locations (i.e.,
    when many GPU threads are performing atomics on the same memory location).
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数原子操作，例如`atomicAdd()`，通过将需要共享内存和同步的归约操作替换为“触发即忘”语义，简化了代码。然而，在SM 3.x硬件出现之前，这种编程模式会导致巨大的性能下降，因为Kepler之前的硬件在处理“争用”内存位置（即多个GPU线程对同一内存位置进行原子操作时）时效率低下。
- en: '* * *'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Because atomic operations are implemented in the GPU memory controller, they
    only work on local device memory locations. As of this writing, trying to perform
    atomic operations on remote GPUs (via peer-to-peer addresses) or host memory (via
    mapped pinned memory) will not work.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 由于原子操作是通过GPU内存控制器实现的，因此它们仅作用于本地设备内存位置。截至目前，尝试在远程GPU（通过对等地址）或主机内存（通过映射的固定内存）上执行原子操作将无法实现。
- en: '* * *'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Atomics and Synchronization
  id: totrans-647
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 原子操作与同步
- en: Besides “fire and forget” semantics, atomics also may be used for synchronization
    between blocks. CUDA hardware supports the workhorse base abstraction for synchronization,
    “compare and swap” (or CAS). On CUDA, compare-and-swap (also known as compare-and-exchange—e.g.,
    the `CMPXCHG` instruction in x86) is defined as follows.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 除了“火并忘记”（fire and forget）语义外，原子操作还可以用于块之间的同步。CUDA硬件支持用于同步的主力基础抽象——“比较并交换”（或CAS）。在CUDA中，比较并交换（也称为比较并交换—例如x86中的`CMPXCHG`指令）定义如下。
- en: int atomicCAS( int *address, int expected, int value);^([12](ch05.html#ch05fn12))
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: int atomicCAS( int *address, int expected, int value);^([12](ch05.html#ch05fn12))
- en: '[12](ch05.html#ch05fn12a). Unsigned and 64-bit variants of `atomicCAS()` also
    are available.'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '[12](ch05.html#ch05fn12a). `atomicCAS()`的无符号和64位变体也可以使用。'
- en: 'This function reads the word `old` at `address`, computes `(old == expected
    ? value : old)`, stores the result back to `address`, and returns `old`. In other
    words, the memory location is left alone *unless it was equal to the expected
    value specified by the caller*, in which case it is updated with the new value.'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '该函数读取`address`处的字`old`，计算`(old == expected ? value : old)`，将结果存回`address`，并返回`old`。换句话说，内存位置保持不变，*除非它等于调用者指定的期望值*，在这种情况下，它将更新为新值。'
- en: A simple critical section called a “spin lock” can be built out of CAS, as follows.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的临界区，称为“自旋锁”，可以通过CAS构建，如下所示。
- en: void enter_spinlock( int *address )
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: void enter_spinlock( int *address )
- en: '{'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: while atomicCAS( address, 0, 1 );
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 当`atomicCAS( address, 0, 1 );`时；
- en: '}'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Assuming the spin lock’s value is initialized to 0, the `while` loop iterates
    until the spin lock value is 0 when the `atomicCAS()` is executed. When that happens,
    `*address` atomically becomes 1 (the third parameter to `atomicCAS()`) and any
    other threads trying to acquire the critical section spin waiting for the critical
    section value to become 0 again.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 假设自旋锁的值初始化为0，则`while`循环会迭代，直到执行`atomicCAS()`时自旋锁的值变为0。发生这种情况时，`*address`会原子地变为1（即`atomicCAS()`的第三个参数），任何其他尝试获取临界区的线程会自旋等待，直到临界区的值再次变为0。
- en: The thread owning the spin lock can give it up by atomically swapping the 0
    back in
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有自旋锁的线程可以通过原子交换0回来来释放锁。
- en: void leave_spinlock( int *address )
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: void leave_spinlock( int *address )
- en: '{'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: atomicExch( m_p, 0 );
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: atomicExch( m_p, 0 );
- en: '}'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: On CPUs, compare-and-swap instructions are used to implement all manner of synchronization.
    Operating systems use them (sometimes in conjunction with the kernel-level thread
    context switching code) to implement higher-level synchronization primitives.
    CAS also may be used directly to implement “lock-free” queues and other data structures.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上，比较并交换指令用于实现各种同步机制。操作系统使用它们（有时与内核级线程上下文切换代码结合使用）来实现更高级别的同步原语。CAS还可以直接用于实现“无锁”队列和其他数据结构。
- en: The CUDA execution model, however, imposes restrictions on the use of global
    memory atomics for synchronization. Unlike CPU threads, some CUDA threads within
    a kernel launch may not begin execution until other threads in the same kernel
    have exited. On CUDA hardware, each SM can context switch a limited number of
    thread blocks, so any kernel launch with more than *MaxThreadBlocksPerSM***NumSMs*
    requires the first thread blocks to exit before more thread blocks can begin execution.
    As a result, it is important that developers not assume all of the threads in
    a given kernel launch are active.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，CUDA 执行模型对使用全局内存原子操作进行同步施加了限制。与 CPU 线程不同，某些 CUDA 线程在内核启动时可能在其他线程退出之前无法开始执行。在
    CUDA 硬件上，每个 SM 可以上下文切换有限数量的线程块，因此任何线程块数量超过 *MaxThreadBlocksPerSM***NumSMs* 的内核启动需要等待第一个线程块退出后，才能开始执行更多的线程块。因此，开发者不应假设给定内核启动中的所有线程都是活动的。
- en: Additionally, the `enter_spinlock()` routine above is prone to deadlock if used
    for intrablock synchronization,^([13](ch05.html#ch05fn13)) for which it is unsuitable
    in any case, since the hardware supports so many better ways for threads within
    the same block to communicate and synchronize with one another (shared memory
    and `__syncthreads()`, respectively).
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，上面的 `enter_spinlock()` 例程如果用于块内同步，容易发生死锁，^([13](ch05.html#ch05fn13))，在任何情况下它都不适用，因为硬件支持许多更好的线程通信和同步方式（分别是共享内存和
    `__syncthreads()`）。
- en: '[13](ch05.html#ch05fn13a). Expected usage is for one thread in each block to
    attempt to acquire the spinlock. Otherwise, the divergent code execution tends
    to deadlock.'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '[13](ch05.html#ch05fn13a)。预期的使用方式是每个块中的一个线程尝试获取自旋锁。否则，分歧的代码执行往往会导致死锁。'
- en: '[Listing 5.8](ch05.html#ch05lis08) shows the implementation of the `cudaSpinlock`
    class, which uses the algorithm listed above and is subject to the just-described
    limitations.'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 5.8](ch05.html#ch05lis08) 展示了 `cudaSpinlock` 类的实现，该类使用上面列出的算法，并且受到刚才描述的限制。'
- en: '*Listing 5.8.* `cudaSpinlock` class.'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5.8.* `cudaSpinlock` 类。'
- en: '[Click here to view code image](ch05_images.html#p05lis08a)'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis08a)'
- en: '* * *'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: class cudaSpinlock {
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: class cudaSpinlock {
- en: 'public:'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 'public:'
- en: cudaSpinlock( int *p );
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSpinlock( int *p );
- en: void acquire();
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: void acquire();
- en: void release();
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: void release();
- en: 'private:'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 'private:'
- en: int *m_p;
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: int *m_p;
- en: '};'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: inline __device__
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__
- en: cudaSpinlock::cudaSpinlock( int *p )
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSpinlock::cudaSpinlock( int *p )
- en: '{'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: m_p = p;
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: m_p = p;
- en: '}'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: inline __device__ void
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ void
- en: cudaSpinlock::acquire( )
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSpinlock::acquire( )
- en: '{'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: while ( atomicCAS( m_p, 0, 1 ) );
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: while ( atomicCAS( m_p, 0, 1 ) );
- en: '}'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: inline __device__ void
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ void
- en: cudaSpinlock::release( )
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSpinlock::release( )
- en: '{'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: atomicExch( m_p, 0 );
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: atomicExch( m_p, 0 );
- en: '}'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Use of `cudaSpinlock` is illustrated in the `spinlockReduction.cu` sample, which
    computes the sum of an array of `double` values by having each block perform a
    reduction in shared memory, then using the spin lock to synchronize for the summation.
    [Listing 5.9](ch05.html#ch05lis09) gives the `SumDoubles` function from this sample.
    Note how adding the partial sum is performed only by thread 0 of each block.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cudaSpinlock`的示例可以在`spinlockReduction.cu`样本中看到，该样本通过让每个块在共享内存中执行归约操作，然后使用自旋锁进行同步来计算一个`double`类型数组的总和。[清单
    5.9](ch05.html#ch05lis09)提供了该示例中的`SumDoubles`函数。注意，部分总和的添加仅由每个块的线程 0 执行。
- en: '*Listing 5.9.* `SumDoubles` function.'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5.9*。`SumDoubles`函数。'
- en: '[Click here to view code image](ch05_images.html#p05lis09a)'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p05lis09a)'
- en: '* * *'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: SumDoubles(
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: SumDoubles(
- en: double *pSum,
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: double *pSum,
- en: int *spinlock,
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: int *spinlock,
- en: const double *in,
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: const double *in,
- en: size_t N,
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: int *acquireCount )
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: int *acquireCount )
- en: '{'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: SharedMemory<double> shared;
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: SharedMemory<double> shared;
- en: cudaSpinlock globalSpinlock( spinlock );
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSpinlock globalSpinlock( spinlock );
- en: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N;
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: shared[threadIdx.x] = in[i];
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: shared[threadIdx.x] = in[i];
- en: __syncthreads();
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: double blockSum = Reduce_block<double,double>( );
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: double blockSum = Reduce_block<double,double>( );
- en: __syncthreads();
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: if ( threadIdx.x == 0 ) {
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: if ( threadIdx.x == 0 ) {
- en: globalSpinlock.acquire( );
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: globalSpinlock.acquire( );
- en: '*pSum += blockSum;'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '*pSum += blockSum;'
- en: __threadfence();
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: __threadfence();
- en: globalSpinlock.release( );
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: globalSpinlock.release( );
- en: '}'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.2.12\. Texturing from Global Memory
  id: totrans-725
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.12\. 来自全局内存的纹理映射
- en: For applications that cannot conveniently adhere to the coalescing constraints,
    the texture mapping hardware presents a satisfactory alternative. The hardware
    supports texturing from global memory (via `cudaBindTexture()/cuTexRefSetAddress()`,
    which has lower peak performance than coalesced global reads but higher performance
    for less-regular access. The texture cache resources are also separate from other
    cache resources on the chip. A software coherency scheme is enforced by the driver
    invalidating the texture cache before kernel invocations that contain `TEX` instructions.^([14](ch05.html#ch05fn14))
    See [Chapter 10](ch10.html#ch10) for details.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些不能方便地遵守合并约束的应用程序，纹理映射硬件提供了一个令人满意的替代方案。该硬件支持从全局内存进行纹理映射（通过`cudaBindTexture()/cuTexRefSetAddress()`，其峰值性能低于合并的全局读取，但对于不规则访问性能更高。纹理缓存资源也与芯片上的其他缓存资源分开。驱动程序通过在包含`TEX`指令的内核调用之前使纹理缓存失效，来强制执行软件一致性方案。^([14](ch05.html#ch05fn14))
    详情请参见[第 10 章](ch10.html#ch10)。
- en: '[14](ch05.html#ch05fn14a). `TEX` is the SASS mnemonic for microcode instructions
    that perform texture fetches.'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '[14](ch05.html#ch05fn14a)。`TEX`是执行纹理获取的微代码指令的SASS助记符。'
- en: 'SM 3.x hardware added the ability to read global memory through the texture
    cache hierarchy without setting up and binding a texture reference. This functionality
    may be accessed with a standard C++ language constructs: the `const restrict`
    keywords. Alternatively, you can use the `__ldg()` intrinsics defined in `sm_35_intrinsics.h`.'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: SM 3.x硬件增加了通过纹理缓存层次结构读取全局内存的能力，无需设置和绑定纹理引用。可以使用标准的C++语言构造来访问此功能：`const restrict`关键字。或者，您可以使用在`sm_35_intrinsics.h`中定义的`__ldg()`内建函数。
- en: 5.2.13\. ECC (Error Correcting Codes)
  id: totrans-729
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.13\. ECC（错误更正码）
- en: SM 2.x and later GPUs in the Tesla (i.e., server GPU) product line come with
    the ability to run with error correction. In exchange for a smaller amount of
    memory (since some memory is used to record some redundancy) and lower bandwidth,
    GPUs with ECC enabled can silently correct single-bit errors and report double-bit
    errors.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉（即服务器GPU）产品线中的SM 2.x及更高版本的GPU具备错误更正能力。通过使用较少的内存（因为一些内存用于记录冗余信息）和较低的带宽，启用ECC的GPU可以默默地修正单比特错误，并报告双比特错误。
- en: ECC has the following characteristics.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: ECC具有以下特点：
- en: • It reduces the amount of available memory by 12.5%. On a `cg1.4xlarge` instance
    in Amazon EC2, for example, it reduces the amount of memory from 3071MB to 2687MB.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: • 它会将可用内存减少12.5%。例如，在Amazon EC2的`cg1.4xlarge`实例上，它将内存从3071MB减少到2687MB。
- en: • It makes context synchronization more expensive.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: • 它增加了上下文同步的开销。
- en: • Uncoalesced memory transactions are more expensive when ECC is enabled than
    otherwise.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: • 启用ECC时，非合并内存事务的开销比未启用时更高。
- en: ECC can be enabled and disabled using the `nvidia-smi` command-line tool (described
    in [Section 4.4](ch04.html#ch04lev1sec4)) or by using the NVML (NVIDIA Management
    Library).
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`nvidia-smi`命令行工具（详见[第4.4节](ch04.html#ch04lev1sec4)）或通过使用NVML（NVIDIA管理库）来启用或禁用ECC。
- en: When an uncorrectable ECC error is detected, synchronous error-reporting mechanisms
    will return `cudaErrorECCUncorrectable` (for the CUDA runtime) and `CUDA_ERROR_ECC_UNCORRECTABLE`
    (for the driver API).
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 当检测到无法更正的ECC错误时，同步错误报告机制将返回`cudaErrorECCUncorrectable`（针对CUDA运行时）和`CUDA_ERROR_ECC_UNCORRECTABLE`（针对驱动程序API）。
- en: 5.3\. Constant Memory
  id: totrans-737
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 常量内存
- en: Constant memory is optimized for read-only broadcast to multiple threads. As
    the name implies, the compiler uses constant memory to hold constants that couldn’t
    be easily computed or otherwise compiled directly into the machine code. Constant
    memory resides in device memory but is accessed using different instructions that
    cause the GPU to access it using a special “constant cache.”
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 常量内存优化为只读广播到多个线程。如其名所示，编译器使用常量内存来存储无法轻易计算或无法直接编译成机器代码的常量。常量内存位于设备内存中，但通过不同的指令进行访问，这使得GPU通过特殊的“常量缓存”来访问它。
- en: The compiler for constants has 64K of memory available to use at its discretion.
    The developer has another 64K of memory available that can be declared with the
    `__constant__` keyword. These limits are per-module (for driver API applications)
    or per-file (for CUDA runtime applications).
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 常量的编译器有 64K 内存可以自由使用。开发者还有 64K 内存可用，可以通过 `__constant__` 关键字声明。这些限制是按模块（对于驱动
    API 应用程序）或按文件（对于 CUDA 运行时应用程序）计算的。
- en: Naïvely, one might expect `__constant__` memory to be analogous to the `const`
    keyword in C/C++, where it cannot be changed after initialization. But `__constant__`
    memory can be changed, either by memory copies or by querying the pointer to `__constant__`
    memory and writing to it with a kernel. CUDA kernels must not write to `__constant__`
    memory ranges that they may be accessing because the constant cache is not kept
    coherent with respect to the rest of the memory hierarchy during kernel execution.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 从表面上看，人们可能会认为 `__constant__` 内存类似于 C/C++ 中的 `const` 关键字，初始化后不能更改。但 `__constant__`
    内存是可以改变的，改变的方式包括内存复制或通过查询指针并使用内核向其写入。CUDA 内核不应写入它们可能访问的 `__constant__` 内存区域，因为常量缓存不会在内核执行期间与其他内存层次结构保持一致。
- en: 5.3.1\. Host and Device `__constant__` Memory
  id: totrans-741
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 主机和设备 `__constant__` 内存
- en: Mark Harris describes the following idiom that uses the predefined macro `__CUDA_ARCH__`
    to maintain host and device copies of `__constant__` memory that are conveniently
    accessed by both the CPU and GPU.^([15](ch05.html#ch05fn15))
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: Mark Harris 描述了以下惯用法，使用预定义宏 `__CUDA_ARCH__` 来维护主机和设备上的 `__constant__` 内存副本，这些副本可以方便地被
    CPU 和 GPU 访问。^([15](ch05.html#ch05fn15))
- en: '[15](ch05.html#ch05fn15a). [http://bit.ly/OpMdN5](http://bit.ly/OpMdN5)'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: '[15](ch05.html#ch05fn15a). [http://bit.ly/OpMdN5](http://bit.ly/OpMdN5)'
- en: '[Click here to view code image](ch05_images.html#p157pro01a)'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p157pro01a)'
- en: __constant__ double dc_vals[2] = { 0.0, 1000.0 };
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: __constant__ double dc_vals[2] = { 0.0, 1000.0 };
- en: const double hc_vals[2] = { 0.0, 1000.0 };
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: const double hc_vals[2] = { 0.0, 1000.0 };
- en: __device__ __host__ double f(size_t i)
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ __host__ double f(size_t i)
- en: '{'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '#ifdef __CUDA_ARCH__'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: '#ifdef __CUDA_ARCH__'
- en: return dc_vals[i];
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: return dc_vals[i];
- en: '#else'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '#else'
- en: return hc_vals[i];
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: return hc_vals[i];
- en: '#endif'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: '#endif'
- en: '}'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 5.3.2\. Accessing `__constant__` Memory
  id: totrans-755
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 访问 `__constant__` 内存
- en: Besides the accesses to constant memory implicitly caused by C/C++ operators,
    developers can copy to and from constant memory, and even query the pointer to
    a constant memory allocation.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 C/C++ 运算符隐式引起的常量内存访问外，开发者还可以将数据复制到常量内存中并从中复制，甚至查询指向常量内存分配的指针。
- en: CUDA Runtime
  id: totrans-757
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA 运行时
- en: CUDA runtime applications can copy to and from `__constant__` memory using `cudaMemcpyToSymbol()`
    and `cudaMemcpyFromSymbol()`, respectively. The pointer to `__constant__` memory
    can be queried with `cudaGetSymbolAddress()`.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 运行时应用程序可以分别使用 `cudaMemcpyToSymbol()` 和 `cudaMemcpyFromSymbol()` 将数据复制到
    `__constant__` 内存中并从中复制。可以通过 `cudaGetSymbolAddress()` 查询 `__constant__` 内存的指针。
- en: cudaError_t cudaGetSymbolAddress( void **devPtr, char *symbol );
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaGetSymbolAddress( void **devPtr, char *symbol );
- en: This pointer may be used to write to constant memory with a kernel, though developers
    must take care not to write to the constant memory while another kernel is reading
    it.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指针可用于在内核中写入常量内存，但开发者必须小心，避免在另一个内核读取常量内存时进行写操作。
- en: Driver API
  id: totrans-761
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序API
- en: Driver API applications can query the device pointer of constant memory using
    `cuModuleGetGlobal()`. The driver API does not include a special memory copy function
    like `cudaMemcpyToSymbol()`, since it does not have the language integration of
    the CUDA runtime. Applications must query the address with `cuModuleGetGlobal()`
    and then call `cuMemcpyHtoD()` or `cuMemcpyDtoH()`. The amount of constant memory
    used by a kernel may be queried with `cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_CONSTANT_SIZE_BYTES)`.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序API应用程序可以使用`cuModuleGetGlobal()`查询常量内存的设备指针。由于驱动程序API没有像`cudaMemcpyToSymbol()`这样的特殊内存复制功能（因为它没有CUDA运行时的语言集成），应用程序必须通过`cuModuleGetGlobal()`查询地址，然后调用`cuMemcpyHtoD()`或`cuMemcpyDtoH()`。可以使用`cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_CONSTANT_SIZE_BYTES)`查询内核使用的常量内存量。
- en: 5.4\. Local Memory
  id: totrans-763
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 局部内存
- en: Local memory contains the stack for every thread in a CUDA kernel. It is used
    as follows.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 局部内存包含每个线程在CUDA内核中的堆栈。其用途如下。
- en: • To implement the application binary interface (ABI)—that is, the calling convention
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: • 用于实现应用程序二进制接口（ABI）——即调用约定
- en: • To spill data out of registers
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: • 用于将数据溢出到寄存器之外
- en: • To hold arrays whose indices cannot be resolved by the compiler
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: • 用于保存编译器无法解析其索引的数组
- en: In early implementations of CUDA hardware, *any* use of local memory was the
    “kiss of death.” It slowed things down so much that developers were encouraged
    to take whatever measure was needed to get rid of the local memory usage. With
    the advent of an L1 cache in Fermi, these performance concerns are less urgent,
    provided the local memory traffic is confined to L1.^([16](ch05.html#ch05fn16))
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA硬件的早期实现中，*任何*使用局部内存的操作几乎都是“致命的”。它会极大地拖慢性能，开发者被鼓励采取任何必要的措施以消除局部内存的使用。随着Fermi中L1缓存的出现，只要局部内存流量限制在L1中，这些性能问题已经不那么紧迫了。^([16](ch05.html#ch05fn16))
- en: '[16](ch05.html#ch05fn16a). The L1 cache is per-SM and is physically implemented
    in the same hardware as shared memory.'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '[16](ch05.html#ch05fn16a)。L1缓存是每个SM的，并且在硬件上与共享内存实现为相同的硬件。'
- en: 'Developers can make the compiler report the amount of local memory needed by
    a given kernel with the `nvcc` options: `-Xptxas –v,abi=no.` At runtime, the amount
    of local memory used by a kernel may be queried with'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可以通过`nvcc`选项：`-Xptxas –v,abi=no`让编译器报告给定内核所需的局部内存大小。
- en: cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES).
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行时，可以使用`cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES)`查询内核使用的局部内存量。
- en: 'Paulius Micikevicius of NVIDIA gave a good presentation on how to determine
    whether local memory usage was impacting performance and what to do about it.^([17](ch05.html#ch05fn17))
    Register spilling can incur two costs: an increased number of instructions and
    an increase in the amount of memory traffic.'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 的 Paulius Micikevicius 做了一个很好的演讲，介绍了如何判断本地内存使用是否影响性能，以及应该采取的措施。^([17](ch05.html#ch05fn17))
    寄存器溢出可能会带来两种成本：增加指令数量和增加内存流量。
- en: '[17](ch05.html#ch05fn17a). [http://bit.ly/ZAeHc5](http://bit.ly/ZAeHc5)'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '[17](ch05.html#ch05fn17a)。 [http://bit.ly/ZAeHc5](http://bit.ly/ZAeHc5)'
- en: The L1 and L2 performance counters can be used to determine if the memory traffic
    is impacting performance. Here are some strategies to improve performance in this
    case.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: L1 和 L2 性能计数器可以用来确定内存流量是否影响性能。以下是一些在这种情况下提高性能的策略。
- en: • At compile time, specify a higher limit in `–maxregcount.` By increasing the
    number of registers available to the thread, both the instruction count and the
    memory traffic will decrease. The `__launch_bounds__` directive may be used to
    tune this parameter when the kernel is being compiled online by PTXAS.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: • 在编译时，指定更高的`–maxregcount`限制。通过增加线程可用的寄存器数量，指令数和内存流量都会减少。`__launch_bounds__`指令可以用于调优此参数，当内核通过
    PTXAS 在线编译时。
- en: • Use noncaching loads for global memory, such as `nvcc –Xptxas -dlcm=cg.`
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: • 对全局内存使用非缓存加载，例如 `nvcc –Xptxas -dlcm=cg.`
- en: • Increase the L1 size to 48K. (Call `cudaFuncSetCacheConfig()` or `cudaDeviceSetCacheconfig().`)
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: • 增加 L1 大小到 48K。（调用 `cudaFuncSetCacheConfig()` 或 `cudaDeviceSetCacheconfig()`。）
- en: When launching a kernel that uses more than the default amount of memory allocated
    for local memory, the CUDA driver must allocate a new local memory buffer before
    the kernel can launch. As a result, the kernel launch may take extra time; may
    cause unexpected CPU/GPU synchronization; and, if the driver is unable to allocate
    the buffer for local memory, may fail.^([18](ch05.html#ch05fn18)) By default,
    the CUDA driver will free these larger local memory allocations after the kernel
    has launched. This behavior can be inhibited by specifying the `CU_CTX_RESIZE_LMEM_TO_MAX`
    flag to `cuCtxCreate()` or calling `cudaSetDeviceFlags()` with the `cudaDeviceLmemResizeToMax`
    flag set.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 当启动一个使用超过默认本地内存分配的内核时，CUDA 驱动程序必须在内核启动之前分配一个新的本地内存缓冲区。因此，内核启动可能会花费额外的时间；可能会导致意外的
    CPU/GPU 同步；如果驱动程序无法为本地内存分配缓冲区，可能会失败。^([18](ch05.html#ch05fn18)) 默认情况下，CUDA 驱动程序会在内核启动后释放这些较大的本地内存分配。通过在调用
    `cuCtxCreate()` 时指定 `CU_CTX_RESIZE_LMEM_TO_MAX` 标志，或者调用 `cudaSetDeviceFlags()`
    时设置 `cudaDeviceLmemResizeToMax` 标志，可以抑制这种行为。
- en: '[18](ch05.html#ch05fn18a). Since most resources are preallocated, an inability
    to allocate local memory is one of the few circumstances that can cause a kernel
    launch to fail at runtime.'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: '[18](ch05.html#ch05fn18a)。由于大多数资源是预分配的，无法分配本地内存是导致内核启动在运行时失败的少数情况之一。'
- en: It is not difficult to build a templated function that illustrates the “performance
    cliff” when register spills occur. The templated `GlobalCopy` kernel in [Listing
    5.10](ch05.html#ch05lis10) implements a simple memcpy routine that uses a local
    array temp to stage global memory references. The template parameter `n` specifies
    the number of elements in `temp` and thus the number of loads and stores to perform
    in the inner loop of the memory copy.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个模板函数来说明寄存器溢出时的“性能悬崖”并不困难。 [列表 5.10](ch05.html#ch05lis10) 中的模板 `GlobalCopy`
    内核实现了一个简单的 memcpy 例程，该例程使用局部数组 temp 来暂存全局内存引用。模板参数 `n` 指定了 `temp` 中元素的数量，从而指定了在内存复制的内循环中执行的加载和存储的数量。
- en: As a quick review of the SASS microcode emitted by the compiler will confirm,
    the compiler can keep `temp` in registers until `n` becomes too large.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编译器发出的 SASS 微代码的快速回顾将确认，编译器可以在 `n` 变得过大之前将 `temp` 保持在寄存器中。
- en: '*Listing 5.10.* `GlobalCopy` kernel.'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 5.10.* `GlobalCopy` 内核。'
- en: '[Click here to view code image](ch05_images.html#p05lis10a)'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis10a)'
- en: '* * *'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, const int n>
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, const int n>
- en: __global__ void
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: GlobalCopy( T *out, const T *in, size_t N )
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: GlobalCopy( T *out, const T *in, size_t N )
- en: '{'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: T temp[n];
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: T temp[n];
- en: size_t i;
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i;
- en: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N-n*blockDim.x*gridDim.x;
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: i < N-n*blockDim.x*gridDim.x;
- en: i += n*blockDim.x*gridDim.x ) {
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: i += n*blockDim.x*gridDim.x ) {
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: temp[j] = in[index];
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: temp[j] = in[index];
- en: '}'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: out[index] = temp[j];
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: out[index] = temp[j];
- en: '}'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // to avoid the (index<N) conditional in the inner loop,
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: // 为了避免内循环中的 (index<N) 条件，
- en: // we left off some work at the end
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: // 我们在最后留下了一些工作
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: if ( index<N ) temp[j] = in[index];
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index<N ) temp[j] = in[index];
- en: '}'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: if ( index<N ) out[index] = temp[j];
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index<N ) out[index] = temp[j];
- en: '}'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 5.11](ch05.html#ch05lis11) shows an excerpt of the output from `globalCopy.cu`
    on a GK104 GPU: the copy performance of 64-bit operands only. The degradation
    in performance due to register spilling becomes obvious in the row corresponding
    to a loop unroll of 12, where the delivered bandwidth decreases from 117GB/s to
    less than 90GB/s, and degrades further to under 30GB/s as the loop unroll increases
    to 16.'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.11](ch05.html#ch05lis11) 显示了在 GK104 GPU 上 `globalCopy.cu` 的输出摘录：仅 64
    位操作数的复制性能。由于寄存器溢出导致的性能下降在对应于循环展开为 12 的行中变得明显，传输带宽从 117GB/s 降低到不到 90GB/s，并随着循环展开增加到
    16 进一步降至 30GB/s 以下。'
- en: '[Table 5.9](ch05.html#ch05tab09) summarizes the register and local memory usage
    for the kernels corresponding to the unrolled loops. The performance degradation
    of the copy corresponds to the local memory usage. In this case, every thread
    always spills in the inner loop; presumably, the performance wouldn’t degrade
    so much if only some of the threads were spilling (for example, when executing
    a divergent code path).'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5.9](ch05.html#ch05tab09) 总结了与展开循环对应的内核的寄存器和局部内存使用情况。拷贝的性能下降与局部内存的使用有关。在这种情况下，每个线程在内循环中都会发生溢出；可以推测，如果只有部分线程发生溢出（例如，在执行分支代码路径时），性能不会如此下降。'
- en: '![Image](graphics/05tab09.jpg)'
  id: totrans-819
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/05tab09.jpg)'
- en: '*Table 5.9* `globalCopy` Register and Local Memory Usage'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.9* `globalCopy` 寄存器和局部内存使用情况'
- en: '*Listing 5.11.* `globalCopy.cu` output (64-bit only).'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5.11.* `globalCopy.cu` 输出（仅限 64 位）。'
- en: '[Click here to view code image](ch05_images.html#p05lis11a)'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis11a)'
- en: '* * *'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Operand size: 8 bytes'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数大小：8 字节
- en: 'Input size: 16M operands'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小：16M 操作数
- en: Block Size
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       75.57   102.57  116.03  124.51  126.21  126.21  512
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 1       75.57   102.57  116.03  124.51  126.21  126.21  512
- en: 2       105.73  117.09  121.84  123.07  124.00  124.00  512
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 2       105.73  117.09  121.84  123.07  124.00  124.00  512
- en: 3       112.49  120.88  121.56  123.09  123.44  123.44  512
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 3       112.49  120.88  121.56  123.09  123.44  123.44  512
- en: 4       115.54  122.89  122.38  122.15  121.22  122.89  64
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 4       115.54  122.89  122.38  122.15  121.22  122.89  64
- en: 5       113.81  121.29  120.11  119.69  116.02  121.29  64
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 5       113.81  121.29  120.11  119.69  116.02  121.29  64
- en: 6       114.84  119.49  120.56  118.09  117.88  120.56  128
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 6       114.84  119.49  120.56  118.09  117.88  120.56  128
- en: 7       117.53  122.94  118.74  116.52  110.99  122.94  64
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 7       117.53  122.94  118.74  116.52  110.99  122.94  64
- en: 8       116.89  121.68  119.00  113.49  105.69  121.68  64
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 8       116.89  121.68  119.00  113.49  105.69  121.68  64
- en: 9       116.10  120.73  115.96  109.48  99.60   120.73  64
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 9       116.10  120.73  115.96  109.48  99.60   120.73  64
- en: 10      115.02  116.70  115.30  106.31  93.56   116.70  64
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 10      115.02  116.70  115.30  106.31  93.56   116.70  64
- en: 11      113.67  117.36  111.48  102.84  88.31   117.36  64
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 11      113.67  117.36  111.48  102.84  88.31   117.36  64
- en: 12      88.16   86.91   83.68   73.78   58.55   88.16   32
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 12      88.16   86.91   83.68   73.78   58.55   88.16   32
- en: 13      85.27   85.58   80.09   68.51   52.66   85.58   64
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 13      85.27   85.58   80.09   68.51   52.66   85.58   64
- en: 14      78.60   76.30   69.50   56.59   41.29   78.60   32
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 14      78.60   76.30   69.50   56.59   41.29   78.60   32
- en: 15      69.00   65.78   59.82   48.41   34.65   69.00   32
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 15      69.00   65.78   59.82   48.41   34.65   69.00   32
- en: 16      65.68   62.16   54.71   43.02   29.92   65.68   32
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 16      65.68   62.16   54.71   43.02   29.92   65.68   32
- en: '* * *'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.5\. Texture Memory
  id: totrans-845
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5\. 纹理内存
- en: 'In CUDA, the concept of texture memory is realized in two parts: a *CUDA array*
    contains the physical memory allocation, and a *texture reference* or *surface
    reference*^([19](ch05.html#ch05fn19)) contains a “view” that can be used to read
    or write a CUDA array. The CUDA array is just an untyped “bag of bits” with a
    memory layout optimized for 1D, 2D, or 3D access. A texture reference contains
    information on how the CUDA array should be addressed and how its contents should
    be interpreted.'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，纹理内存的概念分为两部分：*CUDA数组*包含物理内存分配，*纹理引用*或*表面引用*^([19](ch05.html#ch05fn19))包含一个“视图”，该视图可以用来读取或写入CUDA数组。CUDA数组只是一个未指定类型的“位包”，其内存布局经过优化，适用于1D、2D或3D访问。纹理引用包含关于如何寻址CUDA数组以及如何解释其内容的信息。
- en: '[19](ch05.html#ch05fn19a). Surface references can be used only on SM 2.x and
    later hardware.'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: '[19](ch05.html#ch05fn19a). 表面引用仅能在SM 2.x及更高版本的硬件上使用。'
- en: When using a texture reference to read from a CUDA array, the hardware uses
    a separate, read-only cache to resolve the memory references. While the kernel
    is executing, the texture cache is not kept coherent with respect to the rest
    of the memory subsystem, so it is important not to use texture references to alias
    memory that will be operated on by the kernel. (The cache is invalidated between
    kernel launches.)
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用纹理引用从CUDA数组中读取数据时，硬件使用一个独立的只读缓存来解析内存引用。在内核执行过程中，纹理缓存与其他内存子系统之间的状态不会保持一致，因此重要的是不要使用纹理引用来别名将被内核操作的内存。（缓存会在内核启动之间被失效。）
- en: On SM 3.5 hardware, reads via texture can be explicitly requested by the developer
    using the `const restricted` keywords. The `restricted` keyword does nothing more
    than make the just-described “no aliasing” guarantee that the memory in question
    won’t be referenced by the kernel in any other way. When reading or writing a
    CUDA array with a surface reference, the memory traffic goes through the same
    memory hierarchy as global loads and stores. [Chapter 10](ch10.html#ch10) contains
    a detailed discussion of how to allocate and use textures in CUDA.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 在SM 3.5硬件上，开发者可以通过使用`const restricted`关键字显式地请求通过纹理进行读取。`restricted`关键字的作用仅仅是保证“无别名”的承诺，即保证该内存不会被内核以其他方式引用。当使用表面引用读取或写入CUDA数组时，内存流量会通过与全局加载和存储相同的内存层次结构。[第10章](ch10.html#ch10)详细讨论了如何在CUDA中分配和使用纹理。
- en: 5.6\. Shared Memory
  id: totrans-850
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6\. 共享内存
- en: Shared memory is used to exchange data between CUDA threads within a block.
    Physically, it is implemented with a per-SM memory that can be accessed very quickly.
    In terms of speed, shared memory is perhaps 10x slower than register accesses
    but 10x faster than accesses to global memory. As a result, shared memory is often
    a critical resource to reduce the external bandwidth needed by CUDA kernels.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存用于在CUDA线程块内的线程之间交换数据。在物理上，它是通过每个SM内存实现的，可以非常快速地访问。在速度方面，共享内存的访问速度大约比寄存器访问慢10倍，但比访问全局内存快10倍。因此，共享内存通常是减少CUDA内核所需外部带宽的关键资源。
- en: 'Since developers explicitly allocate and reference shared memory, it can be
    thought of as a “manually managed” cache or “scratchpad” memory. Developers can
    request different cache configurations at both the kernel and the device level:
    `cudaDeviceSetCacheConfig()/cuCtxSetCacheConfig()` specify the preferred cache
    configuration for a CUDA device, while `cudaFuncSetCacheConfig()/cuFuncSetCacheConfig()`
    specify the preferred cache configuration for a given kernel. If both are specified,
    the per-kernel request takes precedence, but in any case, the requirements of
    the kernel may override the developer’s preference.'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 由于开发人员显式地分配和引用共享内存，它可以被视为一种“手动管理”的缓存或“临时存储”内存。开发人员可以在内核和设备级别请求不同的缓存配置：`cudaDeviceSetCacheConfig()/cuCtxSetCacheConfig()`
    用于指定 CUDA 设备的首选缓存配置，而 `cudaFuncSetCacheConfig()/cuFuncSetCacheConfig()` 用于指定给定内核的首选缓存配置。如果两者都被指定，则每个内核的请求具有优先权，但在任何情况下，内核的需求可能会覆盖开发人员的偏好。
- en: Kernels that use shared memory typically are written in three phases.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享内存的内核通常分为三个阶段编写。
- en: • Load shared memory and `__syncthreads()`
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: '• 加载共享内存和`__syncthreads()`  '
- en: • Process shared memory and `__syncthreads()`
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: '• 处理共享内存和`__syncthreads()`  '
- en: • Write results
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: '• 写入结果  '
- en: 'Developers can make the compiler report the amount of shared memory used by
    a given kernel with the `nvcc` options: `-Xptxas –v,abi=no.` At runtime, the amount
    of shared memory used by a kernel may be queried with `cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES)`.'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以使用 `nvcc` 选项让编译器报告给定内核使用的共享内存量：`-Xptxas –v,abi=no.` 在运行时，可以通过 `cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES)`
    查询内核使用的共享内存量。
- en: 5.6.1\. Unsized Shared Memory Declarations
  id: totrans-858
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '5.6.1\. 未指定大小的共享内存声明  '
- en: Any shared memory declared in the kernel itself is automatically allocated for
    each block when the kernel is launched. If the kernel also includes an unsized
    declaration of shared memory, the amount of memory needed by that declaration
    must be specified when the kernel is launched.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中声明的任何共享内存都会在内核启动时自动为每个块分配。如果内核还包括未指定大小的共享内存声明，则必须在启动内核时指定该声明所需的内存量。
- en: If there is more than one `extern __shared__` memory declaration, they are aliased
    with respect to one another, so the declaration
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: '如果有多个 `extern __shared__` 内存声明，它们相互之间是别名，因此该声明  '
- en: extern __shared__ char sharedChars[];
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 'extern __shared__ char sharedChars[];  '
- en: extern __shared__ int sharedInts[];
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: 'extern __shared__ int sharedInts[];  '
- en: enables the same shared memory to be addressed as 8- or 32-bit integers, as
    needed. One motivation for using this type of aliasing is to use wider types when
    possible to read and write global memory, while using the narrow ones for kernel
    computations.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 它使得可以根据需要将相同的共享内存作为 8 位或 32 位整数进行寻址。使用这种别名的一个动机是，当可能时，使用较宽的类型来读写全局内存，而使用较窄的类型进行内核计算。
- en: '* * *'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *  '
- en: Note
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: '注意  '
- en: If you have more than one kernel that uses unsized shared memory, they must
    be compiled in separate files.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有多个使用未指定大小共享内存的内核，它们必须在不同的文件中编译。
- en: '* * *'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.6.2\. Warp-Synchronous Coding
  id: totrans-868
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.6.2\. Warp同步编程
- en: Shared memory variables that will be participating in warp-synchronous programming
    must be declared as `volatile` to prevent the compiler from applying optimizations
    that will render the code incorrect.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 参与warp同步编程的共享内存变量必须声明为`volatile`，以防编译器应用会使代码不正确的优化。
- en: 5.6.3\. Pointers to Shared Memory
  id: totrans-870
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.6.3\. 指向共享内存的指针
- en: It is valid—and often convenient—to use pointers to refer to shared memory.
    Example kernels that use this idiom include the reduction kernels in [Chapter
    12](ch12.html#ch12) ([Listing 12.3](ch12.html#ch12lis03)) and the `scanBlock`
    kernel in [Chapter 13](ch13.html#ch13) ([Listing 13.3](ch13.html#ch13lis03)).
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 使用指针来引用共享内存是有效的——且通常很方便。使用这种习惯的示例内核包括[第12章](ch12.html#ch12)中的归约内核（[列表12.3](ch12.html#ch12lis03)）和[第13章](ch13.html#ch13)中的`scanBlock`内核（[列表13.3](ch13.html#ch13lis03)）。
- en: 5.7\. Memory Copy
  id: totrans-872
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7\. 内存复制
- en: CUDA has three different memory types—host memory, device memory, and CUDA arrays—and
    a full complement of functions to copy between them. For host↔device memcpy, an
    additional set of functions provide *asynchronous* memcpy between pinned host
    memory and device memory or CUDA arrays. Additionally, a set of peer-to-peer memcpy
    functions enable memory to be copied between GPUs.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA有三种不同的内存类型——主机内存、设备内存和CUDA数组——并且提供了一整套函数来在它们之间进行复制。对于主机↔设备的memcpy，额外的一组函数提供了在固定主机内存和设备内存或CUDA数组之间进行*异步*memcpy的功能。此外，一组点对点memcpy函数使得内存可以在多个GPU之间进行复制。
- en: The CUDA runtime and the driver API take very different approaches. For 1D memcpy,
    the driver API defined a family of functions with type-strong parameters. The
    host-to-device, device-to-host, and device-to-device memcpy functions are separate.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA运行时和驱动程序API采用了截然不同的方法。对于1D的memcpy，驱动程序API定义了一组具有强类型参数的函数。主机到设备、设备到主机、以及设备到设备的memcpy函数是分开的。
- en: '[Click here to view code image](ch05_images.html#p164pro01a)'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p164pro01a)'
- en: CUresult cuMemcpyHtoD(CUdeviceptr dstDevice, const void *srcHost,
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuMemcpyHtoD(CUdeviceptr 目标设备，const void *源主机，
- en: size_t ByteCount);
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: size_t 字节数)；
- en: CUresult cuMemcpyDtoH(void *dstHost, CUdeviceptr srcDevice, size_t
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuMemcpyDtoH(void *目标主机，CUdeviceptr 源设备，size_t
- en: ByteCount);
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 字节数)；
- en: CUresult cuMemcpyDtoD(CUdeviceptr dstDevice, CUdeviceptr srcDevice,
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuMemcpyDtoD(CUdeviceptr 目标设备，CUdeviceptr 源设备，
- en: size_t ByteCount);
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: size_t 字节数)；
- en: In contrast, the CUDA runtime tends to define functions that take an extra “memcpy
    kind” parameter that depends on the memory types of the host and destination pointers.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，CUDA运行时倾向于定义带有额外“memcpy种类”参数的函数，这个参数取决于主机和目标指针的内存类型。
- en: enum cudaMemcpyKind
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 枚举 cudaMemcpyKind
- en: '{'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaMemcpyHostToHost = 0,
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToHost = 0，
- en: cudaMemcpyHostToDevice = 1,
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice = 1，
- en: cudaMemcpyDeviceToHost = 2,
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToHost = 2，
- en: cudaMemcpyDeviceToDevice = 3,
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToDevice = 3，
- en: cudaMemcpyDefault = 4
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDefault = 4
- en: '};'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: '}；'
- en: For more complex memcpy operations, both APIs use descriptor structures to specify
    the memcpy.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的memcpy操作，两个API都使用描述符结构来指定memcpy。
- en: 5.7.1\. Synchronous versus Asynchronous Memcpy
  id: totrans-892
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.1\. 同步与异步 Memcpy
- en: Because most aspects of memcpy (dimensionality, memory type) are independent
    of whether the memory copy is asynchronous, this section examines the difference
    in detail, and later sections include minimal coverage of asynchronous memcpy.
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 memcpy 的大多数方面（维度、内存类型）与内存拷贝是否为异步无关，本节详细讨论了二者的差异，后续章节将对异步 memcpy 进行简要介绍。
- en: 'By default, any memcpy involving host memory is *synchronous*: The function
    does not return until after the operation has been performed.^([20](ch05.html#ch05fn20))
    Even when operating on pinned memory, such as memory allocated with `cudaMallocHost()`,
    synchronous memcpy routines must wait until the operation is completed because
    the application may rely on that behavior.^([21](ch05.html#ch05fn21))'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，任何涉及主机内存的 memcpy 操作都是*同步的*：该函数在操作完成之前不会返回。^([20](ch05.html#ch05fn20))
    即使是在固定内存上进行操作，如使用 `cudaMallocHost()` 分配的内存，同步 memcpy 例程也必须等待操作完成，因为应用程序可能依赖于这种行为。^([21](ch05.html#ch05fn21))
- en: '[20](ch05.html#ch05fn20a). This is because the hardware cannot directly access
    host memory unless it has been page-locked and mapped for the GPU. An asynchronous
    memory copy for pageable memory could be implemented by spawning another CPU thread,
    but so far, the CUDA team has chosen to avoid that additional complexity.'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: '[20](ch05.html#ch05fn20a). 这是因为硬件无法直接访问主机内存，除非该内存已被页锁定并为 GPU 映射。对于可分页内存的异步内存拷贝，可以通过生成另一个
    CPU 线程来实现，但迄今为止，CUDA 团队选择避免这种额外的复杂性。'
- en: '[21](ch05.html#ch05fn21a). When pinned memory is specified to a synchronous
    memcpy routine, the driver does take advantage by having the hardware use DMA,
    which is generally faster.'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: '[21](ch05.html#ch05fn21a). 当固定内存被指定给同步 memcpy 例程时，驱动程序通过让硬件使用 DMA 来利用这一点，通常这种方式更快。'
- en: When possible, synchronous memcpy should be avoided for performance reasons.
    Even when streams are not being used, keeping all operations asynchronous improves
    performance by enabling the CPU and GPU to run concurrently. If nothing else,
    the CPU can set up more GPU operations such as kernel launches and other memcpys
    while the GPU is running! If CPU/GPU concurrency is the only goal, there is no
    need to create any CUDA streams; calling an asynchronous memcpy with the NULL
    stream will suffice.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 为了性能考虑，应尽量避免使用同步 memcpy。即使不使用流，保持所有操作异步也能提高性能，因为这能让 CPU 和 GPU 并行运行。如果没有其他需求，CPU
    可以在 GPU 执行时设置更多的 GPU 操作，例如内核启动和其他 memcpy！如果 CPU/GPU 并发是唯一目标，则无需创建任何 CUDA 流；调用带有
    NULL 流的异步 memcpy 即可。
- en: While memcpys involving host memory are synchronous by default, any memory copy
    not involving host memory (device↔device or device↔array) is asynchronous. The
    GPU hardware internally enforces serialization on these operations, so there is
    no need for the functions to wait until the GPU has finished before returning.
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然涉及主机内存的memcpy默认是同步的，但任何不涉及主机内存的内存拷贝（设备↔设备或设备↔数组）都是异步的。GPU硬件内部会对这些操作进行序列化，因此这些函数不需要等待GPU完成后再返回。
- en: Asynchronous memcpy functions have the suffix `Async()`. For example, the driver
    API function for asynchronous host→device memcpy is `cuMemcpyHtoDAsync()` and
    the CUDA runtime function is `cudaMemcpyAsync()`.
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: 异步memcpy函数的后缀为`Async()`。例如，驱动程序API中用于异步主机→设备memcpy的函数是`cuMemcpyHtoDAsync()`，而CUDA运行时的函数是`cudaMemcpyAsync()`。
- en: The hardware that implements asynchronous memcpy has evolved over time. The
    very first CUDA-capable GPU (the GeForce 8800 GTX) did not have any copy engines,
    so asynchronous memcpy only enabled CPU/GPU concurrency. Later GPUs added copy
    engines that could perform 1D transfers while the SMs were running, and still
    later, fully capable copy engines were added that could accelerate 2D and 3D transfers,
    even if the copy involved converting between pitch layouts and the block-linear
    layouts used by CUDA arrays. Additionally, early CUDA hardware only had one copy
    engine, whereas today, it sometimes has two. More than two copy engines wouldn’t
    necessarily make sense. Because a single copy engine can saturate the PCI Express
    bus in one direction, only two copy engines are needed to maximize both bus performance
    and concurrency between bus transfers and GPU computation.
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 实现异步memcpy的硬件经历了多次发展。最初的第一代CUDA支持GPU（GeForce 8800 GTX）没有任何拷贝引擎，因此异步memcpy仅支持CPU/GPU的并发。后来，GPU添加了可以在SMs运行时执行1D传输的拷贝引擎，进一步发展后，增加了完全支持的拷贝引擎，这些引擎能够加速2D和3D传输，即使拷贝涉及将数据布局从跨行布局转换为CUDA数组使用的块线性布局。此外，早期的CUDA硬件只有一个拷贝引擎，而今天，硬件有时会有两个拷贝引擎。超过两个拷贝引擎通常没有必要。因为单个拷贝引擎就能在一个方向上饱和PCI
    Express总线，因此只需要两个拷贝引擎即可最大化总线性能以及总线传输与GPU计算之间的并发性。
- en: The number of copy engines can be queried by calling `cuDeviceGetAttribute()`
    with `CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT`, or by examining the `cudaDeviceProp::asyncEngineCount`.
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 拷贝引擎的数量可以通过调用`cuDeviceGetAttribute()`并传入`CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT`，或者通过检查`cudaDeviceProp::asyncEngineCount`来查询。
- en: 5.7.2\. Unified Virtual Addressing
  id: totrans-902
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.2. 统一虚拟寻址
- en: Unified Virtual Addressing enables CUDA to make inferences about memory types
    based on address ranges. Because CUDA tracks which address ranges contain device
    addresses versus host addresses, there is no need to specify `cudaMemcpyKind`
    parameter to the `cudaMemcpy()` function. The driver API added a `cuMemcpy()`
    function that similarly infers the memory types from the addresses.
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 统一虚拟寻址使得CUDA能够根据地址范围推断内存类型。因为CUDA会跟踪哪些地址范围包含设备地址与主机地址，所以不需要在`cudaMemcpy()`函数中指定`cudaMemcpyKind`参数。驱动程序API增加了一个`cuMemcpy()`函数，类似地，它根据地址推断内存类型。
- en: CUresult cuMemcpy(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount);
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuMemcpy(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount);
- en: The CUDA runtime equivalent, not surprisingly, is called `cudaMemcpy():`
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA运行时的等效函数，毫不意外地，称为`cudaMemcpy()`：
- en: cudaError_t cudaMemcpy( void *dst, const void *src, size_t bytes );.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMemcpy( void *dst, const void *src, size_t bytes );.
- en: 5.7.3\. CUDA Runtime
  id: totrans-907
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.3\. CUDA运行时
- en: '[Table 5.10](ch05.html#ch05tab10) summarizes the memcpy functions available
    in the CUDA runtime.'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5.10](ch05.html#ch05tab10)总结了CUDA运行时中可用的memcpy函数。'
- en: '![Image](graphics/05tab10.jpg)![Image](graphics/05tab10a.jpg)'
  id: totrans-909
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab10.jpg)![Image](graphics/05tab10a.jpg)'
- en: '*Table 5.10* Memcpy Functions (CUDA Runtime)'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.10* Memcpy函数（CUDA运行时）'
- en: 1D and 2D memcpy functions take base pointers, pitches, and sizes as required.
    The 3D memcpy routines take a descriptor structure `cudaMemcpy3Dparms`, defined
    as follows.
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 一维和二维的memcpy函数需要传入基指针、步长和大小等参数。三维的memcpy函数需要一个描述符结构`cudaMemcpy3Dparms`，定义如下。
- en: '[Click here to view code image](ch05_images.html#p168pro01a)'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p168pro01a)'
- en: struct cudaMemcpy3DParms
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaMemcpy3DParms
- en: '{'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: struct cudaArray *srcArray;
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaArray *srcArray;
- en: struct cudaPos srcPos;
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPos srcPos;
- en: struct cudaPitchedPtr srcPtr;
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr srcPtr;
- en: struct cudaArray *dstArray;
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaArray *dstArray;
- en: struct cudaPos dstPos;
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPos dstPos;
- en: struct cudaPitchedPtr dstPtr;
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr dstPtr;
- en: struct cudaExtent extent;
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaExtent extent;
- en: enum cudaMemcpyKind kind;
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: enum cudaMemcpyKind kind;
- en: '};'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '[Table 5.11](ch05.html#ch05tab11) summarizes the meaning of each member of
    the `cudaMemcpy3DParms` structure. The `cudaPos` and `cudaExtent` structures are
    defined as follows.'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5.11](ch05.html#ch05tab11)总结了`cudaMemcpy3DParms`结构的每个成员的含义。`cudaPos`和`cudaExtent`结构定义如下。'
- en: struct cudaExtent {
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaExtent {
- en: size_t width;
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: size_t width;
- en: size_t height;
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: size_t height;
- en: size_t depth;
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: size_t depth;
- en: '};'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: struct cudaPos {
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPos {
- en: size_t x;
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: size_t x;
- en: size_t y;
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: size_t y;
- en: size_t z;
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: size_t z;
- en: '};'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '![Image](graphics/05tab11.jpg)'
  id: totrans-935
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab11.jpg)'
- en: '*Table 5.11* cudaMemcpy3DParms Structure Members'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.11* cudaMemcpy3DParms结构成员'
- en: 5.7.4\. Driver API
  id: totrans-937
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.4\. 驱动程序 API
- en: '[Table 5.12](ch05.html#ch05tab12) summarizes the driver API’s memcpy functions.'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5.12](ch05.html#ch05tab12)总结了驱动程序 API 中的memcpy函数。'
- en: '![Image](graphics/05tab12.jpg)![Image](graphics/05tab12a.jpg)'
  id: totrans-939
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab12.jpg)![Image](graphics/05tab12a.jpg)'
- en: '*Table 5.12* Memcpy Functions (Driver API)'
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.12* Memcpy函数（驱动程序API）'
- en: '`cuMemcpy3D()` is designed to implement a strict superset of all previous memcpy
    functionality. Any 1D, 2D, or 3D memcpy may be performed between any host, device,
    or CUDA array memory, and any offset into either the source or destination may
    be applied. The `WidthInBytes`, `Height,` and `Depth` members of the input structure,
    `CUDA_MEMCPY_3D`, define the dimensionality of the memcpy: `Height==0` implies
    a 1D memcpy, and `Depth==0` implies a 2D memcpy. The source and destination memory
    types are given by the `srcMemoryType` and `dstMemoryType` structure elements,
    respectively.'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuMemcpy3D()` 旨在实现所有先前 memcpy 功能的严格超集。可以在任何主机、设备或 CUDA 数组内存之间执行任何 1D、2D 或
    3D 的 memcpy，并且可以在源或目标的任意偏移量上进行操作。输入结构 `CUDA_MEMCPY_3D` 的 `WidthInBytes`、`Height`
    和 `Depth` 成员定义了 memcpy 的维度：`Height==0` 表示 1D memcpy，`Depth==0` 表示 2D memcpy。源和目标内存类型由
    `srcMemoryType` 和 `dstMemoryType` 结构元素分别给出。'
- en: Structure elements that are not needed by `cuMemcpy3D()` are defined to be ignored.
    For example, if a 1D host→device memcpy is requested, the `srcPitch`, `srcHeight`,
    `dstPitch`, and `dstHeight` elements are ignored. If `srcMemoryType` is `CU_MEMORYTYPE_HOST`,
    the `srcDevice` and `srcArray` elements are ignored. This API semantic, coupled
    with the C idiom that assigning {0} to a structure zero-initializes it, enables
    memory copies to be described very concisely. Most other memcpy functions can
    be implemented in a few lines of code, such as the following.
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `cuMemcpy3D()` 不需要的结构元素，会被定义为忽略。例如，如果请求一个 1D 主机→设备的 memcpy，则会忽略 `srcPitch`、`srcHeight`、`dstPitch`
    和 `dstHeight` 元素。如果 `srcMemoryType` 是 `CU_MEMORYTYPE_HOST`，则会忽略 `srcDevice` 和
    `srcArray` 元素。这个 API 语义，结合 C 语言的惯用法（通过给结构赋值 {0} 来进行零初始化），使得内存拷贝的描述非常简洁。大多数其他 memcpy
    函数可以用几行代码实现，例如以下代码。
- en: '[Click here to view code image](ch05_images.html#p171pro01a)'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p171pro01a)'
- en: CUresult
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult
- en: my_cuMemcpyHtoD( CUdevice dst, const void *src, size_t N )
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: my_cuMemcpyHtoD( CUdevice dst, const void *src, size_t N )
- en: '{'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: CUDA_MEMCPY_3D cp = {0};
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_MEMCPY_3D cp = {0};
- en: cp.srcMemoryType = CU_MEMORYTYPE_HOST;
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: cp.srcMemoryType = CU_MEMORYTYPE_HOST;
- en: cp.srcHost = srcHost;
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: cp.srcHost = srcHost;
- en: cp.dstMemoryType = CU_MEMORYTYPE_DEVICE;
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: cp.dstMemoryType = CU_MEMORYTYPE_DEVICE;
- en: cp.dstDevice = dst;
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: cp.dstDevice = dst;
- en: cp.WidthInBytes = N;
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: cp.WidthInBytes = N;
- en: return cuMemcpy3D( &cp );
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: return cuMemcpy3D( &cp );
- en: '}'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Chapter 6\. Streams and Events
  id: totrans-955
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章：流和事件
- en: 'CUDA is best known for enabling fine-grained concurrency, with hardware facilities
    that enable threads to closely collaborate within blocks using a combination of
    shared memory and thread synchronization. But it also has hardware and software
    facilities that enable more coarse-grained concurrency:'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 最为人知的特点是支持细粒度并发，它通过硬件设施使线程能够在块内利用共享内存和线程同步进行紧密协作。但它也拥有硬件和软件设施，支持更粗粒度的并发：
- en: • **CPU/GPU concurrency:** Since they are separate devices, the CPU and GPU
    can operate independently of each other.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: • **CPU/GPU 并发：** 由于它们是独立的设备，CPU 和 GPU 可以相互独立地操作。
- en: • **Memcpy/kernel processing concurrency:** For GPUs that have one or more copy
    engines, host↔device memcpy can be performed while the SMs are processing kernels.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: • **Memcpy/内核处理并发：** 对于拥有一个或多个拷贝引擎的 GPU，主机↔设备的 memcpy 可以在 SM 处理内核的同时执行。
- en: • **Kernel concurrency:** SM 2.x-class and later hardware can run up to 4 kernels
    in parallel.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: • **内核并发：** SM 2.x 类及更高版本的硬件可以同时运行最多 4 个内核。
- en: • **Multi-GPU concurrency:** For problems with enough computational density,
    multiple GPUs can operate in parallel. ([Chapter 9](ch09.html#ch09) is dedicated
    to multi-GPU programming.)
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: • **多 GPU 并发：** 对于计算密度足够高的问题，可以并行操作多个 GPU。（[第 9 章](ch09.html#ch09)专门讨论多 GPU
    编程。）
- en: CUDA streams enable these types of concurrency. Within a given stream, operations
    are performed in sequential order, but operations in different streams may be
    performed in parallel. CUDA events complement CUDA streams by providing the synchronization
    mechanisms needed to coordinate the parallel execution enabled by streams. CUDA
    events may be asynchronously “recorded” into a stream, and the CUDA event becomes
    signaled when the operations preceding the CUDA event have been completed.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 流使这些类型的并发成为可能。在给定的流内，操作按顺序执行，但不同流中的操作可以并行执行。CUDA 事件通过提供必要的同步机制，补充了 CUDA
    流，使得流所支持的并行执行能够得以协调。CUDA 事件可以异步“记录”到一个流中，并在执行了 CUDA 事件之前的操作完成时，CUDA 事件被触发。
- en: CUDA events may be used for CPU/GPU synchronization, for synchronization between
    the engines on the GPU, and for synchronization between GPUs. They also provide
    a GPU-based timing mechanism that cannot be perturbed by system events such as
    page faults or interrupts from disk or network controllers. Wall clock timers
    are best for overall timing, but CUDA events are useful for optimizing kernels
    or figuring out which of a series of pipelined GPU operations is taking the longest.
    All of the performance results reported in this chapter were gathered on a `cg1.4xlarge`
    cloud-based server from Amazon’s EC2 service, as described in [Section 4.5](ch04.html#ch04lev1sec5).
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件可用于 CPU/GPU 同步、GPU 内部引擎之间的同步以及 GPU 之间的同步。它们还提供了一种基于 GPU 的计时机制，不会受到系统事件（如页面错误或磁盘/网络控制器的中断）的干扰。挂钟计时器最适合整体计时，但
    CUDA 事件对于优化内核或确定一系列流水线 GPU 操作中哪个操作最耗时非常有用。本章报告的所有性能结果均在 Amazon EC2 服务提供的 `cg1.4xlarge`
    云服务器上收集，如[第 4.5 节](ch04.html#ch04lev1sec5)所述。
- en: '6.1\. CPU/GPU Concurrency: Covering Driver Overhead'
  id: totrans-963
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. CPU/GPU 并发：覆盖驱动程序开销
- en: CPU/GPU concurrency refers to the CPU’s ability to continue processing after
    having sent some request to the GPU. Arguably, the most important use of CPU/GPU
    concurrency is hiding the overhead of requesting work from the GPU.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: CPU/GPU 并发指的是 CPU 在向 GPU 发送请求后，仍然能够继续处理的能力。可以说，CPU/GPU 并发最重要的用途是隐藏向 GPU 请求工作时的开销。
- en: 6.1.1\. Kernel Launches
  id: totrans-965
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. 内核启动
- en: Kernel launches have always been asynchronous. A series of kernel launches,
    with no intervening CUDA operations in between, cause the CPU to submit the kernel
    launch to the GPU and return control to the caller before the GPU has finished
    processing.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 内核启动一直是异步的。一系列内核启动，在中间没有插入CUDA操作，会导致CPU将内核启动请求提交给GPU，并在GPU完成处理之前将控制权返回给调用者。
- en: We can measure the driver overhead by bracketing a series of NULL kernel launches
    with timing operations. [Listing 6.1](ch06.html#ch06lis01) shows `nullKernelAsync.cu`,
    a small program that measures the amount of time needed to perform a kernel launch.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过用定时操作包裹一系列NULL内核启动，来测量驱动程序的开销。[列表 6.1](ch06.html#ch06lis01)展示了 `nullKernelAsync.cu`，一个小程序，测量执行内核启动所需的时间。
- en: '*Listing 6.1.* `nullKernelAsync.cu.`'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6.1.* `nullKernelAsync.cu.`'
- en: '[Click here to view code image](ch06_images.html#p06lis01a)'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p06lis01a)'
- en: '* * *'
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '#include <stdio.h>'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: '#include <stdio.h>'
- en: '#include "chTimer.h"'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: '#include "chTimer.h"'
- en: __global__
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: __global__
- en: void
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: NullKernel()
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: NullKernel()
- en: '{'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '}'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: int
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: int
- en: main( int argc, char *argv[] )
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: main( int argc, char *argv[] )
- en: '{'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const int cIterations = 1000000;
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: const int cIterations = 1000000;
- en: printf( "Launches... " ); fflush( stdout );
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "启动次数... " ); fflush( stdout );
- en: chTimerTimestamp start, stop;
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerTimestamp start, stop;
- en: chTimerGetTime( &start );
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: for ( int i = 0; i < cIterations; i++ ) {
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < cIterations; i++ ) {
- en: NullKernel<<<1,1>>>();
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: NullKernel<<<1,1>>>();
- en: '}'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaThreadSynchronize();
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: cudaThreadSynchronize();
- en: chTimerGetTime( &stop );
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &stop );
- en: double microseconds = 1e6*chTimerElapsedTime( &start, &stop );
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: double 微秒 = 1e6*chTimerElapsedTime( &start, &stop );
- en: double usPerLaunch = microseconds / (float) cIterations;
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: double usPerLaunch = 微秒 / (float) cIterations;
- en: printf( "%.2f us\n", usPerLaunch );
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%.2f 微秒\n", usPerLaunch );
- en: return 0;
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: return 0;
- en: '}'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The `chTimerGetTime()` calls, described in [Appendix A](app01.html#app01), use
    the host operating system’s high-resolution timing facilities, such as `QueryPerformanceCounter()`
    or `gettimeofday()`. The `cudaThreadSynchronize()` call in line 23 is needed for
    accurate timing. Without it, the GPU would still be processing the last kernel
    invocations when the end top is recorded with the following function call.
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: '`chTimerGetTime()` 函数调用，如[附录A](app01.html#app01)所述，使用主机操作系统的高分辨率计时功能，如 `QueryPerformanceCounter()`
    或 `gettimeofday()`。第23行的 `cudaThreadSynchronize()` 调用对于准确计时是必要的。如果没有它，GPU可能在记录结束时，仍在处理上一个内核调用。'
- en: chTimerGetTime( &stop );
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &stop );
- en: If you run this program, you will see that invoking a kernel—even a kernel that
    does nothing—costs anywhere from 2.0 to 8.0 microseconds. Most of that time is
    spent in the driver. The CPU/GPU concurrency enabled by kernel launches only helps
    if the kernel runs for longer than it takes the driver to invoke it! To underscore
    the importance of CPU/GPU concurrency for small kernel launches, let’s move the
    `cudaThreadSynchronize()` call into the inner loop.^([1](ch06.html#ch06fn1))
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个程序，你会看到调用一个内核——即使是一个什么都不做的内核——也需要花费从2.0到8.0微秒不等。大部分时间都花费在驱动程序中。内核启动所启用的CPU/GPU并行性只有在内核运行时间超过驱动程序调用它的时间时才有帮助！为了强调CPU/GPU并行性对小内核启动的重要性，让我们把`cudaThreadSynchronize()`调用移到内层循环中。^([1](ch06.html#ch06fn1))
- en: '[1](ch06.html#ch06fn1a). This program is in the source code as `nullKernelSync.cu`
    and is not reproduced here because it is almost identical to [Listing 6.1](ch06.html#ch06lis01).'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch06.html#ch06fn1a)。这个程序的源代码是`nullKernelSync.cu`，在这里没有再复制，因为它与[列表6.1](ch06.html#ch06lis01)几乎完全相同。'
- en: '[Click here to view code image](ch06_images.html#p175pro01a)'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p175pro01a)'
- en: chTimerGetTime( &start );
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: for ( int i = 0; i < cIterations; i++ ) {
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < cIterations; i++ ) {
- en: NullKernel<<<1,1>>>();
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: NullKernel<<<1,1>>>();
- en: cudaThreadSynchronize();
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: cudaThreadSynchronize();
- en: '}'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: chTimerGetTime( &stop );
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &stop );
- en: The only difference here is that the CPU is waiting until the GPU has finished
    processing each NULL kernel launch before launching the next kernel, as shown
    in [Figure 6.1](ch06.html#ch06fig01). As an example, on an Amazon EC2 instance
    with ECC disabled, `nullKernelNoSync` reports a time of 3.4 ms per launch and
    `nullKernelSync` reports a time of 100 ms per launch. So besides giving up CPU/GPU
    concurrency, the synchronization itself is worth avoiding.
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别在于，CPU会等待GPU完成每个NULL内核的处理后，再启动下一个内核，如[图6.1](ch06.html#ch06fig01)所示。例如，在禁用ECC的Amazon
    EC2实例上，`nullKernelNoSync`每次启动的报告时间为3.4毫秒，而`nullKernelSync`每次启动的报告时间为100毫秒。因此，除了放弃CPU/GPU并行性外，同步本身也是值得避免的。
- en: '![Image](graphics/06fig01.jpg)'
  id: totrans-1008
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/06fig01.jpg)'
- en: '*Figure 6.1* CPU/GPU concurrency.'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.1* CPU/GPU并行性。'
- en: Even without synchronizations, if the kernel doesn’t run for longer than the
    amount of time it took to launch the kernel (3.4 ms), the GPU may go idle before
    the CPU has submitted more work. To explore just how much work a kernel might
    need to do to make the launch worthwhile, let’s switch to a kernel that busy-waits
    until a certain number of clock cycles (according to the `clock()` intrinsic)
    has completed.
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有同步，如果内核的运行时间没有超过启动内核所需的时间（3.4毫秒），GPU可能在CPU提交更多工作之前就处于空闲状态。为了探索一个内核可能需要做多少工作才能使启动变得有意义，让我们切换到一个内核，它在完成一定数量的时钟周期（根据`clock()`内建函数）之前进行忙等待。
- en: '[Click here to view code image](ch06_images.html#p176pro01a)'
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p176pro01a)'
- en: __device__ int deviceTime;
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ int deviceTime;
- en: __global__
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: __global__
- en: void
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: WaitKernel( int cycles, bool bWrite )
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: WaitKernel( int cycles, bool bWrite )
- en: '{'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int start = clock();
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: int start = clock();
- en: int stop;
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: int stop;
- en: do {
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: do {
- en: stop = clock();
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: stop = clock();
- en: '} while ( stop - start < cycles );'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: '} 当 stop - start < cycles 时，继续执行；'
- en: if ( bWrite && threadIdx.x==0 && blockIdx.x==0 ) {
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( bWrite && threadIdx.x==0 && blockIdx.x==0 ) {
- en: deviceTime = stop - start;
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: deviceTime = stop - start;
- en: '}'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: By conditionally writing the result to `deviceTime`, this kernel prevents the
    compiler from optimizing out the busy wait. The compiler does not know that we
    are just going to pass `false` as the second parameter.^([2](ch06.html#ch06fn2))
    The code in our `main()` function then checks the launch time for various values
    of cycles, from 0 to 2500.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 通过有条件地将结果写入`deviceTime`，这个内核防止了编译器优化掉忙等待。编译器并不知道我们只是将`false`作为第二个参数传递。^([2](ch06.html#ch06fn2))
    然后我们在`main()`函数中的代码会检查不同周期值（从0到2500）下的启动时间。
- en: '[2](ch06.html#ch06fn2a). The compiler could still invalidate our timing results
    by branching around the loop if `bWrite is false`. If the timing results looked
    suspicious, we could see if this is happening by looking at the microcode with
    `cuobjdump`.'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch06.html#ch06fn2a). 如果`bWrite为false`，编译器仍然可能通过绕过循环使我们的计时结果无效。如果计时结果看起来可疑，我们可以通过使用`cuobjdump`查看微代码来检查这种情况。'
- en: '[Click here to view code image](ch06_images.html#p177pro01a)'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p177pro01a)'
- en: for ( int cycles = 0; cycles < 2500; cycles += 100 ) {
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int cycles = 0; cycles < 2500; cycles += 100 ) {
- en: 'printf( "Cycles: %d - ", cycles ); fflush( stdout );'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 'printf( "周期数: %d - ", cycles ); fflush( stdout );'
- en: chTimerGetTime( &start );
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: for ( int i = 0; i < cIterations; i++ ) {
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < cIterations; i++ ) {
- en: WaitKernel<<<1,1>>>( cycles, false );
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: WaitKernel<<<1,1>>>( cycles, false );
- en: '}'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaThreadSynchronize();
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: cudaThreadSynchronize();
- en: chTimerGetTime( &stop );
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &stop );
- en: double microseconds = 1e6*chTimerElapsedTime( &start, &stop );
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: double microseconds = 1e6*chTimerElapsedTime( &start, &stop );
- en: double usPerLaunch = microseconds / (float) cIterations;
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: double usPerLaunch = microseconds / (float) cIterations;
- en: printf( "%.2f us\n", usPerLaunch );
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%.2f 微秒\n", usPerLaunch );
- en: '}'
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: This program may be found in `waitKernelAsync.cu`. On our EC2 instance, the
    output is as in [Figure 6.2](ch06.html#ch06fig02). On this host platform, the
    breakeven mark where the kernel launch time crosses over 2x that of a NULL kernel
    launch (4.90 μs) is at 4500 GPU clock cycles.
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序可以在`waitKernelAsync.cu`中找到。在我们的EC2实例上，输出如[图 6.2](ch06.html#ch06fig02)所示。在这个主机平台上，当内核启动时间超过NULL内核启动时间的2倍（4.90微秒）时，平衡点出现在4500个GPU时钟周期。
- en: '![Image](graphics/06fig02.jpg)'
  id: totrans-1042
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/06fig02.jpg)'
- en: '*Figure 6.2* Microseconds/cycles plot for `waitKernelAsync.cu`.'
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.2* `waitKernelAsync.cu`的微秒/周期图。'
- en: These performance characteristics can vary widely and depend on many factors,
    including the following.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 这些性能特征可能会有很大差异，并且取决于许多因素，包括以下几点。
- en: • Performance of the host CPU
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: • 主机CPU的性能
- en: • Host operating system
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: • 主机操作系统
- en: • Driver version
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: • 驱动程序版本
- en: • Driver model (TCC versus WDDM on Windows)
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: • 驱动程序模型（Windows上TCC与WDDM的对比）
- en: • Whether ECC is enabled on the GPU^([3](ch06.html#ch06fn3))
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: • GPU是否启用了ECC^([3](ch06.html#ch06fn3))
- en: '[3](ch06.html#ch06fn3a). When ECC is enabled, the driver must perform a kernel
    thunk to check whether any memory errors have occurred. As a result, `cudaThreadSynchronize()`
    is expensive even on platforms with user-mode client drivers.'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch06.html#ch06fn3a)。当启用ECC时，驱动程序必须执行内核调用以检查是否发生了任何内存错误。因此，即使在用户模式客户端驱动程序的平台上，`cudaThreadSynchronize()`也是昂贵的。'
- en: But the common underlying theme is that for most CUDA applications, developers
    should do their best to avoid breaking CPU/GPU concurrency. Only applications
    that are very compute-intensive and only perform large data transfers can afford
    to ignore this overhead. To take advantage of CPU/GPU concurrency when performing
    memory copies as well as kernel launches, developers must use *asynchronous memcpy*.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，常见的基本主题是，对于大多数CUDA应用程序，开发人员应尽最大努力避免破坏CPU/GPU并发性。只有那些非常计算密集型并且仅进行大数据传输的应用程序，才能忽略这种开销。为了在执行内存复制和内核启动时充分利用CPU/GPU并发性，开发人员必须使用*异步memcpy*。
- en: 6.2\. Asynchronous Memcpy
  id: totrans-1052
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. 异步Memcopy
- en: Like kernel launches, asynchronous memcpy calls return before the GPU has performed
    the memcpy in question. Because the GPU operates autonomously and can read or
    write the host memory without any operating system involvement, only pinned memory
    is eligible for asynchronous memcpy.
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: 与内核启动类似，异步memcpy调用在GPU执行相关memcpy之前返回。由于GPU是自主操作的，可以在没有操作系统参与的情况下读取或写入主机内存，因此只有固定内存才有资格进行异步memcpy。
- en: The earliest application for asynchronous memcpy in CUDA was hidden inside the
    CUDA 1.0 driver. The GPU cannot access pageable memory directly, so the driver
    implements pageable memcpy using a pair of pinned “staging buffers” that are allocated
    with the CUDA context. [Figure 6.3](ch06.html#ch06fig03) shows how this process
    works.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 异步memcpy在CUDA中的最早应用隐藏在CUDA 1.0驱动程序中。GPU无法直接访问可分页内存，因此驱动程序使用一对与CUDA上下文分配的固定“临时缓冲区”实现分页memcpy。[图6.3](ch06.html#ch06fig03)展示了这个过程是如何工作的。
- en: '![Image](graphics/06fig03.jpg)'
  id: totrans-1055
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/06fig03.jpg)'
- en: '*Figure 6.3* Pageable memcpy.'
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.3* 可分页memcpy。'
- en: To perform a host→device memcpy, the driver first “primes the pump” by copying
    to one staging buffer, then kicks off a DMA operation to read that data with the
    GPU. While the GPU begins processing that request, the driver copies more data
    into the other staging buffer. The CPU and GPU keep ping-ponging between staging
    buffers, with appropriate synchronization, until it is time for the GPU to perform
    the final memcpy. Besides copying data, the CPU also naturally pages in any nonresident
    pages while the data is being copied.
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行主机→设备的memcpy，驱动程序首先通过复制到一个临时缓冲区来“启动泵”，然后启动DMA操作以让GPU读取该数据。在GPU开始处理该请求时，驱动程序将更多数据复制到另一个临时缓冲区。CPU和GPU在临时缓冲区之间进行交替操作，并进行适当的同步，直到GPU执行最终的memcpy。此外，在复制数据时，CPU还会自然地将任何非驻留页面分页到内存中。
- en: '6.2.1\. Asynchronous Memcpy: Host→Device'
  id: totrans-1058
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1. 异步Memcopy：主机→设备
- en: As with kernel launches, asynchronous memcpys incur fixed CPU overhead in the
    driver. In the case of host→device memcpy, *all* memcpys below a certain size
    are asynchronous, because the driver copies the source data directly into the
    command buffer that it uses to control the hardware.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 与内核启动一样，异步内存拷贝会在驱动程序中产生固定的 CPU 开销。在主机→设备的内存拷贝情况下，*所有*小于某一大小的内存拷贝都是异步的，因为驱动程序将源数据直接复制到它用来控制硬件的命令缓冲区中。
- en: We can write an application that measures asynchronous memcpy overhead, much
    as we measured kernel launch overhead earlier. The following code, in a program
    called `nullHtoDMemcpyAsync.cu`, reports that on a `cg1.4xlarge` instance in Amazon
    EC2, each memcpy takes 3.3 ms. Since PCI Express can transfer almost 2K in that
    time, it makes sense to examine how the time needed to perform a small memcpy
    grows with the size.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编写一个应用程序，测量异步内存拷贝的开销，就像我们之前测量内核启动开销一样。以下代码在名为 `nullHtoDMemcpyAsync.cu` 的程序中报告，在
    Amazon EC2 上的 `cg1.4xlarge` 实例中，每次内存拷贝需要 3.3 毫秒。由于 PCI Express 可以在此时间内传输近 2K 数据，因此查看执行小型内存拷贝所需时间如何随大小变化是有意义的。
- en: '[Click here to view code image](ch06_images.html#p180pro01a)'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch06_images.html#p180pro01a)'
- en: CUDART_CHECK( cudaMalloc( &deviceInt, sizeof(int) ) );
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMalloc( &deviceInt, sizeof(int) ) );
- en: CUDART_CHECK( cudaHostAlloc( &hostInt, sizeof(int), 0 ) );
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaHostAlloc( &hostInt, sizeof(int), 0 ) );
- en: chTimerGetTime( &start );
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: for ( int i = 0; i < cIterations; i++ ) {
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < cIterations; i++ ) {
- en: CUDART_CHECK( cudaMemcpyAsync( deviceInt, hostInt, sizeof(int),
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpyAsync( deviceInt, hostInt, sizeof(int),
- en: cudaMemcpyHostToDevice, NULL ) );
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice, NULL ) );
- en: '}'
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: CUDART_CHECK( cudaThreadSynchronize() );
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaThreadSynchronize() );
- en: chTimerGetTime( &stop );
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &stop );
- en: The `breakevenHtoDMemcpy.cu` program measures memcpy performance for sizes from
    4K to 64K. On a `cg1.4xlarge` instance in Amazon EC2, it generates [Figure 6.4](ch06.html#ch06fig04).
    The data generated by this program is clean enough to fit to a linear regression
    curve—in this case, with intercept 3.3 μs and slope 0.000170 μs/byte. The slope
    corresponds to 5.9GB/s, about the expected bandwidth from PCI Express 2.0.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: '`breakevenHtoDMemcpy.cu` 程序测量了从 4K 到 64K 大小的内存拷贝性能。在 Amazon EC2 上的 `cg1.4xlarge`
    实例上，它生成了 [图 6.4](ch06.html#ch06fig04)。该程序生成的数据足够干净，可以拟合线性回归曲线——在这种情况下，截距为 3.3
    μs，斜率为 0.000170 μs/字节。该斜率对应于 5.9GB/s，接近 PCI Express 2.0 预期的带宽。'
- en: '![Image](graphics/06fig04.jpg)'
  id: totrans-1072
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/06fig04.jpg)'
- en: '*Figure 6.4* Small host→device memcpy performance.'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.4* 小型主机→设备内存拷贝性能。'
- en: '6.2.2\. Asynchronous Memcpy: Device→Host'
  id: totrans-1074
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2\. 异步内存拷贝：设备→主机
- en: The `nullDtoHMemcpyNoSync.cu` and `breakevenDtoHMemcpy.cu` programs perform
    the same measurements for small device→host memcpys. On our trusty Amazon EC2
    instance, the minimum time for a memcpy is 4.00 μs ([Figure 6.5](ch06.html#ch06fig05)).
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: '`nullDtoHMemcpyNoSync.cu` 和 `breakevenDtoHMemcpy.cu` 程序对小型设备→主机内存拷贝进行了相同的测量。在我们信赖的
    Amazon EC2 实例上，最小的内存拷贝时间为 4.00 μs ([图 6.5](ch06.html#ch06fig05))。'
- en: '![Image](graphics/06fig05.jpg)'
  id: totrans-1076
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/06fig05.jpg)'
- en: '*Figure 6.5* Small device→host memcpy performance.'
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.5* 小型设备→主机内存拷贝性能。'
- en: 6.2.3\. The NULL Stream and Concurrency Breaks
  id: totrans-1078
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3\. NULL流和并发中断
- en: Any streamed operation may be called with NULL as the stream parameter, and
    the operation will not be initiated until all the preceding operations on the
    GPU have been completed.^([4](ch06.html#ch06fn4)) Applications that have no need
    for copy engines to overlap memcpy operations with kernel processing can use the
    NULL stream to facilitate CPU/GPU concurrency.
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: 任何流操作都可以将NULL作为流参数进行调用，并且该操作直到GPU上所有前置操作完成后才会启动。^([4](ch06.html#ch06fn4)) 不需要将内存拷贝操作与内核处理重叠的应用程序可以使用NULL流来促进CPU/GPU并发执行。
- en: '[4](ch06.html#ch06fn4a). When CUDA streams were added in CUDA 1.1, the designers
    had a choice between making the NULL stream “its own” stream, separate from other
    streams and serialized only with itself, or making it synchronize with (“join”)
    all engines on the GPU. They opted for the latter, in part because CUDA did not
    yet have facilities for interstream synchronization.'
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch06.html#ch06fn4a). 当CUDA流在CUDA 1.1中添加时，设计人员在选择NULL流的行为时面临两个选择：要么让NULL流成为“独立的”流，只与自己进行串行化，要么让它与GPU上的所有引擎同步（“连接”）。他们选择了后者，部分原因是CUDA当时还没有提供流间同步的功能。'
- en: Once a streamed operation has been initiated with the NULL stream, the application
    must use synchronization functions such as `cuCtxSynchronize()` or `cudaThreadSynchronize()`
    to ensure that the operation has been completed before proceeding. But the application
    may request many such operations before performing the synchronization. For example,
    the application may perform an asynchronous host→device memcpy, one or more kernel
    launches, and an asynchronous device→host memcpy before synchronizing with the
    context. The `cuCtxSynchronize()` or `cudaThreadSynchronize()` call returns once
    the GPU has performed the most recently requested operation. This idiom is especially
    useful when performing smaller memcpys or launching kernels that will not run
    for long. The CUDA driver takes valuable CPU time to write commands to the GPU,
    and overlapping that CPU execution with the GPU’s processing of the commands can
    improve performance.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用NULL流启动了一个流操作，应用程序必须使用同步函数，例如`cuCtxSynchronize()`或`cudaThreadSynchronize()`，以确保操作在继续之前已经完成。但应用程序可以在执行同步操作之前请求多个此类操作。例如，应用程序可以执行异步的主机→设备内存拷贝（memcpy）、一个或多个内核启动以及一个异步的设备→主机内存拷贝，然后再与上下文进行同步。`cuCtxSynchronize()`或`cudaThreadSynchronize()`调用将在GPU完成最近请求的操作后返回。当执行较小的内存拷贝或启动不会运行很长时间的内核时，这种习惯用法特别有用。CUDA驱动程序需要占用宝贵的CPU时间来向GPU写入命令，通过将这些CPU执行与GPU处理命令的过程重叠，可以提高性能。
- en: '*Note:* Even in CUDA 1.0, kernel launches were asynchronous. As a result, the
    NULL stream is implicitly specified to all kernel launches if no stream is given.'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：* 即使在CUDA 1.0中，内核启动也是异步的。因此，如果没有给定流，则NULL流会隐式地指定给所有内核启动。'
- en: Breaking Concurrency
  id: totrans-1083
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 中断并发
- en: Whenever an application performs a full CPU/GPU synchronization (having the
    CPU wait until the GPU is completely idle), performance suffers. We can measure
    this performance impact by switching our NULL-memcpy calls from asynchronous ones
    to synchronous ones just by changing the `cudaMemcpyAsync()` calls to `cudaMemcpy()`
    calls. The `nullDtoHMemcpySync.cu` program does just that for device→host memcpy.
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: 每当应用程序执行完整的 CPU/GPU 同步（让 CPU 等待直到 GPU 完全空闲）时，性能会受到影响。我们可以通过将 NULL-memcpy 调用从异步调用改为同步调用来衡量这种性能影响，方法就是将
    `cudaMemcpyAsync()` 调用改为 `cudaMemcpy()` 调用。`nullDtoHMemcpySync.cu` 程序正是为设备到主机的
    memcpy 执行这一操作。
- en: On our trusty Amazon `cg1.4xlarge` instance, `nullDtoHMemcpySync.cu` reports
    about 7.9 μs per memcpy. If a Windows driver has to perform a kernel thunk, or
    the driver on an ECC-enabled GPU must check for ECC errors, full GPU synchronization
    is much costlier.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可靠的 Amazon `cg1.4xlarge` 实例上，`nullDtoHMemcpySync.cu` 每次 memcpy 报告大约 7.9 微秒。如果
    Windows 驱动程序需要执行内核转换，或者启用了 ECC 的 GPU 上的驱动程序必须检查 ECC 错误，则完全的 GPU 同步会更加昂贵。
- en: Explicit ways to perform this synchronization include the following.
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此同步的显式方式包括以下几种。
- en: • `cuCtxSynchronize()/cudaDeviceSynchronize()`
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: • 调用 `cuCtxSynchronize()/cudaDeviceSynchronize()`
- en: • `cuStreamSynchronize()/cudaStreamSynchronize()` on the NULL stream
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: • 在 NULL 流上调用 `cuStreamSynchronize()/cudaStreamSynchronize()`
- en: • Unstreamed memcpy between host and device—for example, `cuMemcpyHtoD()`, `cuMemcpyDtoH()`,
    `cudaMemcpy()`
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: • 在主机和设备之间进行非流式 memcpy，例如 `cuMemcpyHtoD()`、`cuMemcpyDtoH()`、`cudaMemcpy()`
- en: Other, more subtle ways to break CPU/GPU concurrency include the following.
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: 其他更微妙的打破 CPU/GPU 并发性的方法包括以下几种。
- en: • Running with the `CUDA_LAUNCH_BLOCKING` environment variable set
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: • 设置 `CUDA_LAUNCH_BLOCKING` 环境变量运行
- en: • Launching kernels that require local memory to be reallocated
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: • 启动需要重新分配本地内存的内核
- en: • Performing large memory allocations or host memory allocations
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: • 执行大规模内存分配或主机内存分配
- en: • Destroying objects such as CUDA streams and CUDA events
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: • 销毁诸如 CUDA 流和 CUDA 事件等对象
- en: Nonblocking Streams
  id: totrans-1095
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 非阻塞流
- en: To create a stream that is exempt from the requirement to synchronize with the
    NULL stream (and therefore less likely to suffer a “concurrency break” as described
    above), specify the `CUDA_STREAM_NON_BLOCKING` flag to `cuStreamCreate()` or the
    `cudaStreamNonBlocking` flag to `cudaStreamCreateWithFlags()`.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 若要创建一个不需要与 NULL 流同步的流（因此更不容易遭遇上述所说的“并发中断”），可以为 `cuStreamCreate()` 指定 `CUDA_STREAM_NON_BLOCKING`
    标志，或者为 `cudaStreamCreateWithFlags()` 指定 `cudaStreamNonBlocking` 标志。
- en: '6.3\. CUDA Events: CPU/GPU Synchronization'
  id: totrans-1097
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. CUDA 事件：CPU/GPU 同步
- en: One of the key features of CUDA events is that they can enable “partial” CPU/GPU
    synchronization. Instead of full CPU/GPU synchronization where the CPU waits until
    the GPU is idle, introducing a bubble into the GPU’s work pipeline, CUDA events
    may be *recorded* into the asynchronous stream of GPU commands. The CPU then can
    wait until all of the work preceding the event has been done. The GPU can continue
    doing whatever work was submitted after the `cuEventRecord()/cudaEventRecord()`.
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件的一个关键特性是它们能够实现“部分”CPU/GPU同步。与完整的CPU/GPU同步（即CPU等待直到GPU空闲，从而在GPU的工作管道中引入一个空泡）不同，CUDA
    事件可以被*记录*到GPU命令的异步流中。然后，CPU可以等待直到事件之前的所有工作完成。GPU可以继续执行在`cuEventRecord()/cudaEventRecord()`之后提交的工作。
- en: As an example of CPU/GPU concurrency, [Listing 6.2](ch06.html#ch06lis02) gives
    a memcpy routine for pageable memory. The code for this program implements the
    algorithm described in [Figure 6.3](ch06.html#ch06fig03) and is located in `pageableMemcpyHtoD.cu`.
    It uses two pinned memory buffers, stored in global variables declared as follows.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 作为CPU/GPU并发的一个示例，[列表 6.2](ch06.html#ch06lis02)给出了一个用于可分页内存的memcpy例程。该程序的代码实现了[图
    6.3](ch06.html#ch06fig03)中描述的算法，并位于`pageableMemcpyHtoD.cu`中。它使用了两个固定内存缓冲区，这些缓冲区存储在如下声明的全局变量中。
- en: void *g_hostBuffers[2];
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: void *g_hostBuffers[2];
- en: and two CUDA events declared as
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 以及声明的两个CUDA事件如下
- en: cudaEvent_t g_events[2];
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEvent_t g_events[2];
- en: '*Listing 6.2.* `chMemcpyHtoD()`—pageable memcpy.'
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6.2.* `chMemcpyHtoD()`—可分页的memcpy。'
- en: '[Click here to view code image](ch06_images.html#p06lis02a)'
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p06lis02a)'
- en: '* * *'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: void
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: chMemcpyHtoD( void *device, const void *host, size_t N )
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: chMemcpyHtoD( void *device, const void *host, size_t N )
- en: '{'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: char *dst = (char *) device;
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: char *dst = (char *) device;
- en: const char *src = (const char *) host;
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: const char *src = (const char *) host;
- en: int stagingIndex = 0;
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: int stagingIndex = 0;
- en: while ( N ) {
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: while ( N ) {
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
- en: cudaEventSynchronize( g_events[stagingIndex] );
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventSynchronize( g_events[stagingIndex] );
- en: memcpy( g_hostBuffers[stagingIndex], src, thisCopySize );
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: memcpy( g_hostBuffers[stagingIndex], src, thisCopySize );
- en: cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
- en: thisCopySize, cudaMemcpyHostToDevice, NULL );
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: thisCopySize, cudaMemcpyHostToDevice, NULL );
- en: cudaEventRecord( g_events[1-stagingIndex], NULL );
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( g_events[1-stagingIndex], NULL );
- en: dst += thisCopySize;
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: dst += thisCopySize;
- en: src += thisCopySize;
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: src += thisCopySize;
- en: N -= thisCopySize;
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: N -= thisCopySize;
- en: stagingIndex = 1 - stagingIndex;
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: stagingIndex = 1 - stagingIndex;
- en: '}'
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Error:'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: '错误:'
- en: return;
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: return;
- en: '}'
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '`chMemcpyHtoD()` is designed to maximize CPU/GPU concurrency by “ping-ponging”
    between the two host buffers. The CPU copies into one buffer, while the GPU pulls
    from the other. There is some “overhang” where no CPU/GPU concurrency is possible
    at the beginning and end of the operation when the CPU is copying the first and
    last buffers, respectively.'
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: '`chMemcpyHtoD()`的设计旨在通过在两个主机缓冲区之间进行“乒乓”操作，最大化CPU/GPU并发。CPU将数据复制到一个缓冲区，而GPU从另一个缓冲区中获取数据。在操作的开始和结束时，由于CPU分别复制第一个和最后一个缓冲区，因此存在一定的“悬挂”现象，无法实现CPU/GPU并发。'
- en: In this program, the only synchronization needed—the `cudaEventSynchronize()`
    in line 11—ensures that the GPU has finished with a buffer before starting to
    copy into it. `cudaMemcpyAsync()` returns as soon as the GPU commands have been
    enqueued. It does not wait until the operation is complete. The `cudaEventRecord()`
    is also asynchronous. It causes the event to be signaled when the just-requested
    asynchronous memcpy has been completed.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个程序中，唯一需要同步的操作是第11行的 `cudaEventSynchronize()`，它确保在开始向缓冲区复制之前，GPU 已经完成对该缓冲区的处理。`cudaMemcpyAsync()`
    会在 GPU 命令入队后立即返回，而不会等到操作完成。`cudaEventRecord()` 也是异步的，它会在刚请求的异步 memcpy 完成时触发事件。
- en: The CUDA events are recorded immediately after creation so the first `cudaEventSynchronize()`
    calls in line 11 work correctly.
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件在创建后立即记录，以确保第11行的第一次 `cudaEventSynchronize()` 调用能正确工作。
- en: '[Click here to view code image](ch06_images.html#p184pro01a)'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p184pro01a)'
- en: CUDART_CHECK( cudaEventCreate( &g_events[0] ) );
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventCreate( &g_events[0] ) );
- en: CUDART_CHECK( cudaEventCreate( &g_events[1] ) );
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventCreate( &g_events[1] ) );
- en: // record events so they are signaled on first synchronize
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: // 记录事件，以便在第一次同步时触发
- en: CUDART_CHECK( cudaEventRecord( g_events[0], 0 ) );
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( g_events[0], 0 ) );
- en: CUDART_CHECK( cudaEventRecord( g_events[1], 0 ) );
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( g_events[1], 0 ) );
- en: If you run `pageableMemcpyHtoD.cu`, it will report a bandwidth number much smaller
    than the pageable memcpy bandwidth delivered by the CUDA driver. That’s because
    the C runtime’s `memcpy()` implementation is not optimized to move memory as fast
    as the CPU can. For best performance, the memory must be copied using SSE instructions
    that can move data 16 bytes at a time. Writing a general-purpose memcpy using
    these instructions is complicated by their alignment restrictions, but a simple
    version that requires the source, destination, and byte count to be 16-byte aligned
    is not difficult.^([5](ch06.html#ch06fn5))
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行 `pageableMemcpyHtoD.cu`，它会报告一个比 CUDA 驱动程序提供的 pageable memcpy 带宽要小得多的带宽数字。这是因为
    C 运行时的 `memcpy()` 实现没有经过优化，无法像 CPU 一样快速地移动内存。为了获得最佳性能，内存必须使用 SSE 指令复制，这些指令一次可以移动
    16 字节数据。使用这些指令编写通用的 memcpy 会受到它们对内存对齐的限制，但要求源地址、目的地址和字节数都为 16 字节对齐的简单版本并不难写。^([5](ch06.html#ch06fn5))
- en: '[5](ch06.html#ch06fn5a). On some platforms, `nvcc` does not compile this code
    seamlessly. In the code accompanying this book, `memcpy16()` is in a separate
    file called `memcpy16.cpp`.'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch06.html#ch06fn5a)。在某些平台上，`nvcc` 编译这段代码时可能不太顺利。在本书随附的代码中，`memcpy16()`
    被单独放在一个名为 `memcpy16.cpp` 的文件中。'
- en: '[Click here to view code image](ch06_images.html#p185pro01a)'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p185pro01a)'
- en: '#include <xmmintrin.h>'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: '#include <xmmintrin.h>'
- en: bool
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: bool
- en: memcpy16( void *_dst, const void *_src, size_t N )
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: memcpy16( void *_dst, const void *_src, size_t N )
- en: '{'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: if ( N & 0xf ) {
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 N & 0xf：
- en: return false;
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 false;
- en: '}'
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: float *dst = (float *) _dst;
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: float *dst = (float *) _dst;
- en: const float *src = (const float *) _src;
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: const float *src = (const float *) _src;
- en: while ( N ) {
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: 当 N 不为零时，执行循环：
- en: _mm_store_ps( dst, _mm_load_ps( src ) );
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_store_ps( dst, _mm_load_ps( src ) );
- en: src += 4;
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: src += 4;
- en: dst += 4;
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
- en: N -= 16;
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
- en: return true;
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
- en: When the C runtime `memcpy()` is replaced by this one, performance on an Amazon
    EC2 `cg1.4xlarge` instance increases from 2155MB/s to 3267MB/s. More complicated
    memcpy routines can deal with relaxed alignment constraints, and slightly higher
    performance is possible by unrolling the inner loop. On `cg1.4xlarge`, the CUDA
    driver’s more optimized SSE memcpy achieves about 100MB/s higher performance than
    `pageableMemcpyHtoD16.cu`.
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
- en: How important is the CPU/GPU concurrency for performance of pageable memcpy?
    If we move the event synchronization, we can make the host→device memcpy synchronous,
    as follows.
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p185pro02a)'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
- en: while ( N ) {
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
- en: < CUDART_CHECK( cudaEventSynchronize( g_events[stagingIndex] ) );
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
- en: memcpy( g_hostBuffers[stagingIndex], src, thisCopySize );
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize, cudaMemcpyHostToDevice, NULL ) );
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( g_events[1-stagingIndex], NULL ) );
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventSynchronize( g_events[1-stagingIndex] ) );
  id: totrans-1168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: dst += thisCopySize;
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
- en: src += thisCopySize;
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
- en: N -= thisCopySize;
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
- en: stagingIndex = 1 - stagingIndex;
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
- en: This code is available in `pageableMemcpyHtoD16Synchronous.cu`, and it is about
    70% as fast (2334MB/s instead of 3267MB/s) on the same `cg1.4xlarge` instance.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1\. Blocking Events
  id: totrans-1175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA events also optionally can be made “blocking,” in which they use an interrupt-based
    mechanism for CPU synchronization. The CUDA driver then implements `cu(da)EventSynchronize()`
    calls using thread synchronization primitives that suspend the CPU thread instead
    of polling the event’s 32-bit tracking value.
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
- en: For latency-sensitive applications, blocking events may impose a performance
    penalty. In the case of our pageable memcpy routine, using blocking events causes
    a slight slowdown (about 100MB/s) on our `cg1.4xlarge` instance. But for more
    GPU-intensive applications, or for applications with “mixed workloads” that need
    significant amounts of processing from both CPU and GPU, the benefits of having
    the CPU thread idle outweigh the costs of handling the interrupt that occurs when
    the wait is over. An example of a mixed workload is video transcoding, which features
    divergent code suitable for the CPU and signal and pixel processing suitable for
    the GPU.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于延迟敏感的应用程序，阻塞事件可能会导致性能损失。在我们的可分页 `memcpy` 函数的情况下，使用阻塞事件会导致我们的 `cg1.4xlarge`
    实例稍微变慢（约 100MB/s）。但是，对于更具 GPU 密集型的应用程序，或者对于那些需要大量 CPU 和 GPU 处理的“混合工作负载”应用程序来说，保持
    CPU 线程空闲的好处超过了在等待结束时处理中断的成本。一个混合工作负载的例子是视频转码，它包含适合 CPU 的分支代码，以及适合 GPU 的信号和像素处理。
- en: 6.3.2\. Queries
  id: totrans-1178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2\. 查询
- en: Both CUDA streams and CUDA events may be queried with `cu(da)StreamQuery()`
    and `cu(da)EventQuery()`, respectively. If `cu(da)StreamQuery()` returns success,
    all of the operations pending in a given stream have been completed. If `cu(da)EventQuery()`
    returns success, the event has been recorded.
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 流和 CUDA 事件可以分别通过 `cu(da)StreamQuery()` 和 `cu(da)EventQuery()` 进行查询。如果 `cu(da)StreamQuery()`
    返回成功，则表示给定流中所有待处理的操作已完成。如果 `cu(da)EventQuery()` 返回成功，则表示该事件已被记录。
- en: Although these queries are intended to be lightweight, if ECC is enabled, they
    do perform kernel thunks to check the current error status of the GPU. Additionally,
    on Windows, any pending commands will be submitted to the GPU, which also requires
    a kernel thunk.
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些查询旨在保持轻量级，如果启用了 ECC，它们确实会执行内核调用以检查 GPU 的当前错误状态。此外，在 Windows 上，任何待处理的命令都会提交给
    GPU，这也需要执行内核调用。
- en: '6.4\. CUDA Events: Timing'
  id: totrans-1181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. CUDA 事件：计时
- en: CUDA events work by submitting a command to the GPU that, when the preceding
    commands have been completed, causes the GPU to write a 32-bit memory location
    with a known value. The CUDA driver implements `cuEventQuery()` and `cuEventSynchronize()`
    by examining that 32-bit value. But besides the 32-bit “tracking” value, the GPU
    also can write a 64-bit timer value that is sourced from a high-resolution, GPU-based
    clock.
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件通过向 GPU 提交命令来工作，该命令在前面的命令完成后，会使 GPU 写入一个已知值的 32 位内存位置。CUDA 驱动程序通过检查该
    32 位值来实现 `cuEventQuery()` 和 `cuEventSynchronize()`。但是，除了 32 位的“跟踪”值外，GPU 还可以写入一个来自高分辨率
    GPU 时钟的 64 位计时器值。
- en: Because they use a GPU-based clock, timing using CUDA events is less subject
    to perturbations from system events such as page faults or interrupts, and the
    function to compute elapsed times from timestamps is portable across all operating
    systems. That said, the so-called “wall clock” times of operations are ultimately
    what users see, so CUDA events are best used in a targeted fashion to tune kernels
    or other GPU-intensive operations, not to report absolute times to the user.
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它们使用基于GPU的时钟，使用CUDA事件进行计时时，受到系统事件（如页面错误或中断）的干扰较小，而且从时间戳计算经过时间的函数在所有操作系统上都是可移植的。也就是说，所谓的“墙钟”时间最终是用户看到的，因此CUDA事件最好用于有针对性地调优内核或其他GPU密集型操作，而不是向用户报告绝对时间。
- en: 'The stream parameter to `cuEventRecord()` is for interstream synchronization,
    not for timing. When using CUDA events for timing, it is best to record them in
    the NULL stream. The rationale is similar to the reason the machine instructions
    in superscalar CPUs to read time stamp counters (e.g., `RDTSC` on x86) are serializing
    instructions that flush the pipeline: Forcing a “join” on all the GPU engines
    eliminates any possible ambiguity on the operations being timed.^([6](ch06.html#ch06fn6))
    Just make sure the `cu(da)EventRecord()` calls bracket enough work so that the
    timing delivers meaningful results.'
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuEventRecord()`的stream参数用于流间同步，而不是计时。当使用CUDA事件进行计时时，最好在NULL流中记录它们。其原理类似于超标量CPU中读取时间戳计数器的机器指令（例如x86上的`RDTSC`）是序列化指令，用于刷新流水线：强制所有GPU引擎“合并”可以消除计时操作中的任何歧义。^([6](ch06.html#ch06fn6))
    只需确保`cu(da)EventRecord()`调用括住足够的工作，以便计时结果有意义。'
- en: '[6](ch06.html#ch06fn6a). An additional consideration: On CUDA hardware with
    SM 1.1, timing events could only be recorded by the hardware unit that performed
    kernel computation.'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](ch06.html#ch06fn6a). 另一个考虑因素：在具有SM 1.1的CUDA硬件上，只有执行内核计算的硬件单元才能记录计时事件。'
- en: Finally, note that CUDA events are intended to time GPU operations. Any synchronous
    CUDA operations will result in the GPU being used to time the resulting CPU/GPU
    synchronization operations.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，注意CUDA事件是用于计时GPU操作的。任何同步的CUDA操作都会导致GPU用于计时由此产生的CPU/GPU同步操作。
- en: '[Click here to view code image](ch06_images.html#p187pro01a)'
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p187pro01a)'
- en: CUDART_CHECK( cudaEventRecord( startEvent, NULL ) );
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( startEvent, NULL ) );
- en: // synchronous memcpy – invalidates CUDA event timing
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: // 同步的memcpy——使CUDA事件计时失效
- en: CUDART_CHECK( cudaMemcpy( deviceIn, hostIn, N*sizeof(int) );
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpy( deviceIn, hostIn, N*sizeof(int) );
- en: CUDART_CHECK( cudaEventRecord( stopEvent, NULL ) );
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( stopEvent, NULL ) );
- en: The example explored in the next section illustrates how to use CUDA events
    for timing.
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节的示例展示了如何使用CUDA事件进行计时。
- en: 6.5\. Concurrent Copying and Kernel Processing
  id: totrans-1193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. 并发复制和内核处理
- en: Since CUDA applications must transfer data across the PCI Express bus in order
    for the GPU to operate on it, another performance opportunity presents itself
    in the form of performing those host↔device memory transfers concurrently with
    kernel processing. According to Amdahl’s Law,^([7](ch06.html#ch06fn7)) the maximum
    speedup achievable by using multiple processors is
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CUDA应用程序必须通过PCI Express总线传输数据，以便GPU可以对其进行处理，因此在内核处理的同时执行主机↔设备内存传输，提供了另一个性能提升的机会。根据阿姆达尔定律，使用多个处理器可以实现的最大加速比为
- en: '[7](ch06.html#ch06fn7a). [http://bit.ly/13UqBm0](http://bit.ly/13UqBm0)'
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](ch06.html#ch06fn7a). [http://bit.ly/13UqBm0](http://bit.ly/13UqBm0)'
- en: '![Image](graphics/188equ01.jpg)'
  id: totrans-1196
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/188equ01.jpg)'
- en: 'where *r[s]* + *r[p]* = 1 and *N* is the number of processors. In the case
    of concurrent copying and kernel processing, the “number of processors” is the
    number of autonomous hardware units in the GPU: one or two copy engines, plus
    the SMs that execute the kernels. For *N* = 2, [Figure 6.6](ch06.html#ch06fig06)
    shows the idealized speedup curve as *r[s]* and *r[p]* vary.'
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*r[s]* + *r[p]* = 1，*N*是处理器的数量。在并发拷贝和内核处理的情况下，“处理器的数量”指的是GPU中的自主硬件单元的数量：一个或两个拷贝引擎，加上执行内核的SM（流式多处理器）。对于*N*
    = 2，[图6.6](ch06.html#ch06fig06)显示了随着*r[s]*和*r[p]*变化的理想化加速曲线。
- en: '![Image](graphics/06fig06.jpg)'
  id: totrans-1198
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/06fig06.jpg)'
- en: '*Figure 6.6* Idealized Amdahl’s Law curve.'
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.6* 理想化的阿姆达尔定律曲线。'
- en: So in theory, a 2x performance improvement is possible on a GPU with one copy
    engine, but only if the program gets perfect overlap between the SMs and the copy
    engine, and only if the program spends equal time transferring and processing
    the data.
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，理论上，在具有一个拷贝引擎的GPU上，性能提升2倍是可能的，但前提是程序能够完美地重叠SM和拷贝引擎的工作，而且程序在数据传输和处理上的时间必须相等。
- en: Before undertaking this endeavor, you should take a close look at whether it
    will benefit your application. Applications that are extremely transfer-bound
    (i.e., they spend most of their time transferring data to and from the GPU) or
    extremely compute-bound (i.e., they spend most of their time processing data on
    the GPU) will derive little benefit from overlapping transfer and compute.
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始这个任务之前，你应该仔细考虑它是否对你的应用程序有益。对于那些极度受传输限制的应用（即大部分时间都在将数据传输到GPU和从GPU传输数据）或极度受计算限制的应用（即大部分时间都在GPU上处理数据），重叠传输和计算将不会带来太大的好处。
- en: 6.5.1\. `concurrencyMemcpyKernel.cu`
  id: totrans-1202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.1. `concurrencyMemcpyKernel.cu`
- en: The program `concurrencyMemcpyKernel.cu` is designed to illustrate not only
    how to implement concurrent memcpy and kernel execution but also how to determine
    whether it is worth doing at all. [Listing 6.3](ch06.html#ch06lis03) gives a `AddKernel()`,
    a “makework” kernel that has a parameter `cycles` to control how long it runs.
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: 程序`concurrencyMemcpyKernel.cu`旨在说明如何实现并发的内存拷贝和内核执行，并且如何判断是否值得这样做。[列表6.3](ch06.html#ch06lis03)提供了一个`AddKernel()`，这是一个“假工作”内核，具有一个参数`cycles`，用来控制它运行的时间。
- en: '*Listing 6.3.* `AddKernel()`, a makework kernel with parameterized computational
    density.'
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p06lis03a)'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
- en: AddKernel( int *out, const int *in, size_t N, int addValue, int
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
- en: cycles )
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x )
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
- en: volatile int value = in[i];
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < cycles; j++ ) {
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
- en: value += addValue;
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
- en: out[i] = value;
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
- en: '`AddKernel()` streams an array of integers from `in` to `out`, looping over
    each input value `cycles` times. By varying the value of `cycles`, we can make
    the kernel range from a trivial streaming kernel that pushes the memory bandwidth
    limits of the machine to a totally compute-bound kernel.'
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
- en: These two routines in the program measure the performance of `AddKernel().`
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
- en: • `TimeSequentialMemcpyKernel()` copies the input data to the GPU, invokes `AddKernel()`,
    and copies the output back from the GPU in separate, sequential steps.
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
- en: • `TimeConcurrentOperations()` allocates a number of CUDA streams and performs
    the host→device memcpys, kernel processing, and device→host memcpys in parallel.
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
- en: '`TimeSequentialMemcpyKernel()`, given in [Listing 6.4](ch06.html#ch06lis04),
    uses four CUDA events to separately time the host→device memcpy, kernel processing,
    and device→host memcpy. It also reports back the total time, as measured by the
    CUDA events.'
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 6.4.* `TimeSequentialMemcpyKernel()` function.'
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p06lis04a)'
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
- en: bool
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
- en: TimeSequentialMemcpyKernel(
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
- en: float *timesHtoD,
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
- en: float *timesKernel,
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
- en: float *timesDtoH,
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
- en: float *timesTotal,
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
- en: const chShmooRange& cyclesRange,
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
- en: int numBlocks )
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
- en: bool ret = false;
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
- en: int *hostIn = 0;
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
- en: int *hostOut = 0;
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
- en: int *deviceIn = 0;
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
- en: int *deviceOut = 0;
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
- en: const int numEvents = 4;
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
- en: cudaEvent_t events[numEvents];
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < numEvents; i++ ) {
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
- en: events[i] = NULL;
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventCreate( &events[i] ) );
  id: totrans-1251
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
- en: cudaMallocHost( &hostIn, N*sizeof(int) );
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
- en: cudaMallocHost( &hostOut, N*sizeof(int) );
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
- en: cudaMalloc( &deviceIn, N*sizeof(int) );
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
- en: cudaMalloc( &deviceOut, N*sizeof(int) );
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0; i < N; i++ ) {
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
- en: hostIn[i] = rand();
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
- en: for ( chShmooIterator cycles(cyclesRange); cycles; cycles++ ) {
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
- en: printf( "." ); fflush( stdout );
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( events[0], NULL );
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( deviceIn, hostIn, N*sizeof(int),
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice, NULL );
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( events[1], NULL );
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
- en: AddKernel<<<numBlocks, 256>>>(
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
- en: deviceOut, deviceIn, N, 0xcc, *cycles );
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( events[2], NULL );
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( hostOut, deviceOut, N*sizeof(int),
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost, NULL );
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( events[3], NULL );
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventElapsedTime( timesHtoD, events[0], events[1] );
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventElapsedTime( timesKernel, events[1], events[2] );
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventElapsedTime( timesDtoH, events[2], events[3] );
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventElapsedTime( timesTotal, events[0], events[3] );
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
- en: timesHtoD += 1;
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
- en: timesKernel += 1;
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
- en: timesDtoH += 1;
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
- en: timesTotal += 1;
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
- en: ret = true;
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < numEvents; i++ ) {
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventDestroy( events[i] );
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( deviceIn );
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( deviceOut );
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
- en: cudaFreeHost( hostOut );
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
- en: cudaFreeHost( hostIn );
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
- en: return ret;
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
- en: The `cyclesRange` parameter, which uses the “shmoo” functionality described
    in [Section A.4](app01.html#app01lev1sec4), specifies the range of cycles values
    to use when invoking `AddKernel()`. On a `cg1.4xlarge` instance in EC2, the times
    (in ms) for `cycles` values from 4..64 are as follows.
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/191tab01.jpg)'
  id: totrans-1296
  prefs: []
  type: TYPE_IMG
- en: For values of `*cycles` around 48 (highlighted), where the kernel takes about
    the same amount of time as the memcpy operations, we presume there would be a
    benefit in performing the operations concurrently.
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
- en: The routine `TimeConcurrentMemcpyKernel()` divides the computation performed
    by `AddKernel()` evenly into segments of size `streamIncrement` and uses a separate
    CUDA stream to compute each. The code fragment in [Listing 6.5](ch06.html#ch06lis05),
    from `TimeConcurrentMemcpyKernel()`, highlights the complexity of programming
    with streams.
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 6.5.* `TimeConcurrentMemcpyKernel()` fragment.'
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p06lis05a)'
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft = N;
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
- en: for ( int stream = 0; stream < numStreams; stream++ ) {
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
- en: size_t intsToDo = (intsLeft < intsPerStream) ?
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
- en: 'intsLeft : intsPerStream;'
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync(
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
- en: deviceIn+stream*intsPerStream,
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
- en: hostIn+stream*intsPerStream,
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
- en: intsToDo*sizeof(int),
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice, streams[stream] ) );
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft -= intsToDo;
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft = N;
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
- en: for ( int stream = 0; stream < numStreams; stream++ ) {
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
- en: size_t intsToDo = (intsLeft < intsPerStream) ?
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
- en: 'intsLeft : intsPerStream;'
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
- en: AddKernel<<<numBlocks, 256, 0, streams[stream]>>>(
  id: totrans-1317
  prefs: []
  type: TYPE_NORMAL
- en: deviceOut+stream*intsPerStream,
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
- en: deviceIn+stream*intsPerStream,
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
- en: intsToDo, 0xcc, *cycles );
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft -= intsToDo;
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft = N;
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
- en: for ( int stream = 0; stream < numStreams; stream++ ) {
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
- en: size_t intsToDo = (intsLeft < intsPerStream) ?
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
- en: 'intsLeft : intsPerStream;'
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync(
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
- en: hostOut+stream*intsPerStream,
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
- en: deviceOut+stream*intsPerStream,
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
- en: intsToDo*sizeof(int),
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost, streams[stream] ) );
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
- en: intsLeft -= intsToDo;
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
- en: Besides requiring the application to create and destroy CUDA streams, the streams
    must be looped over separately for each of the host→device memcpy, kernel processing,
    and device→host memcpy operations. Without this “software-pipelining,” there would
    be no concurrent execution of the different streams’ work, as each streamed operation
    is preceded by an “interlock” operation that prevents the operation from proceeding
    until the previous operation in that stream has completed. The result would be
    not only a failure to get parallel execution between the engines but also an additional
    performance degradation due to the slight overhead of managing stream concurrency.
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
- en: The computation cannot be made fully concurrent, since no kernel processing
    can be overlapped with the first or last memcpys, and there is some overhead in
    synchronizing between CUDA streams and, as we saw in the previous section, in
    invoking the memcpy and kernel operations themselves. As a result, the optimal
    number of streams depends on the application and should be determined empirically.
    The `concurrencyMemcpyKernel.cu` program enables the number of streams to be specified
    on the command line using the `-- numStreams` parameter.
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2\. Performance Results
  id: totrans-1337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `concurrencyMemcpyKernel.cu` program generates a report on performance characteristics
    over a variety of `cycles` values, with a fixed buffer size and number of streams.
    On a `cg1.4xlarge` instance in Amazon EC2, with a buffer size of 128M integers
    and 8 streams, the report is as follows for cycles values from 4..64.
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/194tab01.jpg)'
  id: totrans-1339
  prefs: []
  type: TYPE_IMG
- en: The full graph for `cycles` values from 4..256 is given in [Figure 6.7](ch06.html#ch06fig07).
    Unfortunately, for these settings, the 50% speedup shown here falls well short
    of the 3x speedup that theoretically could be obtained.
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/06fig07.jpg)'
  id: totrans-1341
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6.7* Speedup due memcpy/kernel concurrency (Tesla M2050).'
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
- en: The benefit on a GeForce GTX 280, which contains only one copy engine, is more
    pronounced. Here, the results from varying `cycles` up to 512 are shown. The maximum
    speedup, shown in [Figure 6.8](ch06.html#ch06fig08), is much closer to the theoretical
    maximum of 2x.
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/06fig08.jpg)'
  id: totrans-1344
  prefs: []
  type: TYPE_IMG
- en: '*Figure 6.8* Speedup due to memcpy/kernel concurrency (GeForce GTX 280).'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
- en: As written, `concurrencyMemcpyKernel.cu` serves little more than an illustrative
    purpose, because `AddValues()` is just make-work. But you can plug your own kernel(s)
    into this application to help determine whether the additional complexity of using
    streams is justified by the performance improvement. Note that unless concurrent
    kernel execution is desired (see [Section 6.7](ch06.html#ch06lev1sec7)), the kernel
    invocation in [Listing 6.5](ch06.html#ch06lis05) could be replaced by successive
    kernel invocations in the same stream, and the application will still get the
    desired concurrency.
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
- en: 'As a side note, the number of copy engines can be queried by calling`cudaGet``DeviceProperties()`
    and examining `cudaDeviceProp:: asyncEngineCount`, or calling `cuDeviceQueryAttribute()`
    with `CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT`.'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
- en: The copy engines accompanying SM 1.1 and some SM 1.2 hardware could copy linear
    memory only, but more recent copy engines offer full support for 2D memcpy, including
    2D and 3D CUDA arrays.
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.3\. Breaking Interengine Concurrency
  id: totrans-1349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using CUDA streams for concurrent memcpy and kernel execution introduces many
    more opportunities to “break concurrency.” In the previous section, CPU/GPU concurrency
    could be broken by unintentionally doing something that caused CUDA to perform
    a full CPU/GPU synchronization. Here, CPU/GPU concurrency can be broken by unintentionally
    performing an unstreamed CUDA operation. Recall that the NULL stream performs
    a “join” on all GPU engines, so even an asynchronous memcpy operation will stall
    interengine concurrency if the NULL stream is specified.
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
- en: Besides specifying the NULL stream explicitly, the main avenue for these unintentional
    “concurrency breaks” is calling functions that run in the NULL stream implicitly
    because they do not take a stream parameter. When streams were first introduced
    in CUDA 1.1, functions such as `cudaMemset()` and `cuMemcpyDtoD()`, and the interfaces
    for libraries such as CUFFT and CUBLAS, did not have any way for applications
    to specify stream parameters. The Thrust library still does not include support.
    The CUDA Visual Profiler will call out concurrency breaks in its reporting.
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
- en: 6.6\. Mapped Pinned Memory
  id: totrans-1352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Mapped pinned memory can be used to overlap PCI Express transfers and kernel
    processing, especially for device→host copies, where there is no need to cover
    the long latency to host memory. Mapped pinned memory has stricter alignment requirements
    than the native GPU memcpy, since they must be coalesced. Uncoalesced memory transactions
    run two to six times slower when using mapped pinned memory.
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
- en: 'A naïve port of our `concurrencyMemcpyKernelMapped.cu` program yields an interesting
    result: On a `cg1.4xlarge` instance in Amazon EC2, mapped pinned memory runs very
    slowly for values of `cycles` below 64.'
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/197tab01.jpg)'
  id: totrans-1355
  prefs: []
  type: TYPE_IMG
- en: 'For small values of `cycles`, the kernel takes a long time to run, as if `cycles`
    were greater than 200! Only NVIDIA can discover the reason for this performance
    anomaly for certain, but it is not difficult to work around: By unrolling the
    inner loop of the kernel, we create more work per thread, and performance improves.'
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 6.6.* `AddKernel()` with loop unrolling.'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p06lis06a)'
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
- en: template<const int unrollFactor>
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
- en: __device__ void
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
- en: AddKernel_helper( int *out, const int *in, size_t N, int increment, int cycles
    )
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = unrollFactor*blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
- en: i += unrollFactor*blockDim.x*gridDim.x )
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
- en: int values[unrollFactor];
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+iUnroll*blockDim.x;
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
- en: values[iUnroll] = in[index];
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1372
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
- en: for ( int k = 0; k < cycles; k++ ) {
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
- en: values[iUnroll] += increment;
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+iUnroll*blockDim.x;
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
- en: out[index] = values[iUnroll];
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
- en: __device__ void
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
- en: AddKernel( int *out, const int *in, size_t N, int increment, int cycles, int
  id: totrans-1385
  prefs: []
  type: TYPE_NORMAL
- en: unrollFactor )
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
- en: switch ( unrollFactor ) {
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
- en: 'case 1: return AddKernel_helper<1>( out, in, N, increment, cycles );'
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
- en: 'case 2: return AddKernel_helper<2>( out, in, N, increment, cycles );'
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
- en: 'case 4: return AddKernel_helper<4>( out, in, N, increment, cycles );'
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
- en: Note that this version of `AddKernel()` in [Listing 6.6](ch06.html#ch06lis06)
    is functionally identical to the one in [Listing 6.3](ch06.html#ch06lis03).^([8](ch06.html#ch06fn8))
    It just computes `unrollFactor` outputs per loop iteration. Since the unroll factor
    is a template parameter, the compiler can use registers to hold the `values` array,
    and the innermost for loops can be unrolled completely.
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch06.html#ch06fn8a). Except that, as written, N must be divisible by `unrollFactor`.
    This is easily fixed, of course, with a small change to the for loop and a bit
    of cleanup code afterward.'
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
- en: For `unrollFactor==1`, this implementation is identical to that of [Listing
    6.3](ch06.html#ch06lis03). For `unrollFactor==2`, mapped pinned formulation shows
    some improvement over the streamed formulation. The tipping point drops from `cycles==64`
    to `cycles==48`. For `unrollFactor==4`, performance is uniformly better than the
    streamed version.
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/199tab01.jpg)'
  id: totrans-1398
  prefs: []
  type: TYPE_IMG
- en: These values are given for 32M integers, so the program reads and writes 128MB
    of data. For `cycles==48`, the program runs in 26ms. To achieve that effective
    bandwidth rate (more than 9GB/s over PCI Express 2.0), the GPU is concurrently
    reading and writing over PCI Express while performing the kernel processing!
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
- en: 6.7\. Concurrent Kernel Processing
  id: totrans-1400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SM 2.x-class and later GPUs are capable of concurrently running multiple kernels,
    provided they are launched in different streams and have block sizes that are
    small enough so a single kernel will not fill the whole GPU. The code in [Listing
    6.5](ch06.html#ch06lis05) (lines 9–14) will cause kernels to run concurrently,
    provided the number of blocks in each kernel launch is small enough. Since the
    kernels can only communicate through global memory, we can add some instrumentation
    to `AddKernel()` to track how many kernels are running concurrently. Using the
    following “kernel concurrency tracking” structure
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p200pro01a)'
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
- en: static const int g_maxStreams = 8;
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
- en: typedef struct KernelConcurrencyData_st {
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
- en: int mask; // mask of active kernels
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
- en: int maskMax; // atomic max of mask popcount
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
- en: int masks[g_maxStreams];
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
- en: int count; // number of active kernels
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
- en: int countMax; // atomic max of kernel count
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
- en: int counts[g_maxStreams];
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
- en: '} KernelConcurrencyData;'
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
- en: we can add code to `AddKernel()` to “check in” and “check out” at the beginning
    and end of the function, respectively. The “check in” takes the “kernel id” parameter
    `kid` (a value in the range 0..*NumStreams*-1 passed to the kernel), computes
    a mask `1<<kid` corresponding to the kernel ID into a global, and atomically OR’s
    that value into the global. Note that `atomicOR()` returns the value that was
    in the memory location before the OR was performed. As a result, the return value
    has one bit set for every kernel that was active when the atomic OR operation
    was performed.
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, this code tracks the number of active kernels by incrementing `kernelData->count`
    and calling `atomicMax()` on a shared global.
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p200pro02a)'
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
- en: // check in, and record active kernel mask and count
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
- en: // as seen by this kernel.
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
- en: if ( kernelData && blockIdx.x==0 && threadIdx.x == 0 ) {
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
- en: int myMask = atomicOr( &kernelData->mask, 1<<kid );
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
- en: kernelData->masks[kid] = myMask | (1<<kid);
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
- en: int myCount = atomicAdd( &kernelData->count, 1 );
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
- en: atomicMax( &kernelData->countMax, myCount+1 );
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
- en: kernelData->counts[kid] = myCount+1;
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
- en: At the bottom of the kernel, similar code clears the mask and decrements the
    active-kernel count.
  id: totrans-1424
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p200pro03a)'
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
- en: // check out
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
- en: if ( kernelData && blockIdx.x==0 && threadIdx.x==0 ) {
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
- en: atomicAnd( &kernelData->mask, ~(1<<kid) );
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
- en: atomicAdd( &kernelData->count, -1 );
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
- en: The `kernelData` parameter refers to a `__device__` variable declared at file
    scope.
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
- en: __device__ KernelConcurrencyData g_kernelData;
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the pointer to `g_kernelData` must be obtained by calling `cudaGetSymbolAddress()`.
    It is possible to write code that references `&g_kernelData`, but CUDA’s language
    integration will not correctly resolve the address.
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
- en: The `concurrencyKernelKernel.cu` program adds support for a command line option
    `blocksPerSM` to specify the number of blocks with which to launch these kernels.
    It will generate a report on the number of kernels that were active. Two sample
    invocations of `concurrencyKernelKernel` are as follows.
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch06_images.html#p201pro01a)'
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
- en: $ ./concurrencyKernelKernel –blocksPerSM 2
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
- en: Using 2 blocks per SM on GPU with 14 SMs = 28 blocks
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
- en: 'Timing sequential operations... Kernel data:'
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
- en: 'Masks: ( 0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to 1 kernels were active: (0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  id: totrans-1440
  prefs: []
  type: TYPE_NORMAL
- en: Timing concurrent operations...
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel data:'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
- en: 'Masks: ( 0x1 0x3 0x7 0xe 0x1c 0x38 0x60 0xe0 )'
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to 3 kernels were active: (0x1 0x2 0x3 0x3 0x3 0x3 0x2 0x3 )'
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
- en: $ ./concurrencyKernelKernel –blocksPerSM 3
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
- en: Using 3 blocks per SM on GPU with 14 SMs = 42 blocks
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
- en: 'Timing sequential operations... Kernel data:'
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
- en: 'Masks: ( 0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to 1 kernels were active: (0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
- en: 'Timing concurrent operations... Kernel data:'
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
- en: 'Masks: ( 0x1 0x3 0x6 0xc 0x10 0x30 0x60 0x80 )'
  id: totrans-1451
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to 2 kernels were active: (0x1 0x2 0x2 0x2 0x1 0x2 0x2 0x1 )'
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
- en: Note that `blocksPerSM` is the number of blocks specified to each kernel launch,
    so a total of `numStreams*blocksPerSM` blocks are launched in `numStreams` separate
    kernels. You can see that the hardware can run more kernels concurrently when
    the kernel grids are smaller, but there is no performance benefit to concurrent
    kernel processing for the workload discussed in this chapter.
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
- en: '6.8\. GPU/GPU Synchronization: `cudaStreamWaitEvent()`'
  id: totrans-1454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up to this point, all of the synchronization functions described in this chapter
    have pertained to CPU/GPU synchronization. They either wait for or query the status
    of a GPU operation. The `cudaStreamWaitEvent()function` is asynchronous with respect
    to the CPU and causes the specified *stream* to wait until an event has been recorded.
    The stream and event need not be associated with the same CUDA device. [Section
    9.3](ch09.html#ch09lev1sec3) describes how such inter-GPU synchronization may
    be performed and uses the feature to implement a peer-to-peer memcpy (see [Listing
    9.1](ch09.html#ch09lis01)).
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
- en: '6.8.1\. Streams and Events on Multi-GPU: Notes and Limitations'
  id: totrans-1456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: • Streams and events exist in the scope of the context (or device). When `cuCtxDestroy()`
    or `cudaDeviceReset()` is called, the associated streams and events are destroyed.
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
- en: • Kernel launches and `cu(da)EventRecord()` can only use CUDA streams in the
    same context/device.
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
- en: • `cudaMemcpy()` can be called with any stream, but it is best to call it from
    the *source* context/device.
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
- en: • `cudaStreamWaitEvent()` may be called on any event, using any stream.
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
- en: 6.9\. Source Code Reference
  id: totrans-1461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The source code referenced in this chapter resides in the `concurrency` directory.
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/202tab01.jpg)![Image](graphics/202tab01a.jpg)'
  id: totrans-1463
  prefs: []
  type: TYPE_IMG
- en: Chapter 7\. Kernel Execution
  id: totrans-1464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter gives a detailed description of how kernels are executed on the
    GPU: how they are launched, their execution characteristics, how they are organized
    into grids of blocks of threads, and resource management considerations. The chapter
    concludes with a description of dynamic parallelism—the new CUDA 5.0 feature that
    enables CUDA kernels to launch work for the GPU.'
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. Overview
  id: totrans-1466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'CUDA kernels execute on the GPU and, since the very first version of CUDA,
    always have executed concurrently with the CPU. In other words, kernel launches
    are *asynchronous*: Control is returned to the CPU before the GPU has completed
    the requested operation. When CUDA was first introduced, there was no need for
    developers to concern themselves with the asynchrony (or lack thereof) of kernel
    launches; data had to be copied to and from the GPU explicitly, and the memcpy
    commands would be enqueued after the commands needed to launch kernels. It was
    not possible to write CUDA code that exposed the asynchrony of kernel launches;
    the main side effect was to hide driver overhead when performing multiple kernel
    launches consecutively.'
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
- en: With the introduction of mapped pinned memory (host memory that can be directly
    accessed by the GPU), the asynchrony of kernel launches becomes more important,
    especially for kernels that write to host memory (as opposed to read from it).
    If a kernel is launched and writes host memory without explicit synchronization
    (such as with CUDA events), the code suffers from a race condition between the
    CPU and GPU and may not run correctly. Explicit synchronization often is not needed
    for kernels that *read* via mapped pinned memory, since any pending writes by
    the CPU will be posted before the kernel launches. But for kernels that are returning
    results to CPU by writing to mapped pinned memory, synchronizing to avoid write-after-read
    hazards is essential.
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
- en: Once a kernel is launched, it runs as a *grid* of *blocks* of *threads*. Not
    all blocks run concurrently, necessarily; each block is assigned to a streaming
    multiprocessor (SM), and each SM can maintain the context for multiple blocks.
    To cover both memory and instruction latencies, the SM generally needs more warps
    than a single block can contain. The maximum number of blocks per SM cannot be
    queried, but it is documented by NVIDIA as having been 8 before SM 3.x and 16
    on SM 3.x and later hardware.
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
- en: The programming model makes no guarantees whatsoever as to the order of execution
    or whether certain blocks or threads can run concurrently. Developers can never
    assume that all the threads in a kernel launch are executing concurrently. It
    is easy to launch more threads than the machine can hold, and some will not start
    executing until others have finished. Given the lack of ordering guarantees, even
    initialization of global memory at the beginning of a kernel launch is a difficult
    proposition.
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
- en: '*Dynamic parallelism*, a new feature added with the Tesla K20 (GK110), the
    first SM 3.5–capable GPU, enables kernels to launch other kernels and perform
    synchronization between them. These capabilities address some of the limitations
    that were present in CUDA in previous hardware. For example, a dynamically parallel
    kernel can perform initialization by launching and waiting for a child grid.'
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. Syntax
  id: totrans-1472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using the CUDA runtime, a kernel launch is specified using the familiar
    triple-angle-bracket syntax.
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
- en: Kernel<<<gridSize, blockSize, sharedMem, Stream>>>( Parameters... )
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
- en: '*Kernel* specifies the kernel to launch.'
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
- en: '*gridSize* specifies the size of the grid in the form of a `dim3` structure.'
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
- en: '*blockSize* specifies the dimension of each threadblock as a `dim3.`'
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
- en: '*sharedMem* specifies additional shared memory^([1](ch07.html#ch07fn1)) to
    reserve for each block.'
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch07.html#ch07fn1a). The amount of shared memory available to the kernel
    is the sum of this parameter and the amount of shared memory that was statically
    declared within the kernel.'
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
- en: '*Stream* specifies the stream in which the kernel should be launched.'
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
- en: The `dim3` structure used to specify the grid and block sizes has 3 members
    (`x`, `y`, and `z`) and, when compiling with C++, a constructor with default parameters
    such that the `y` and `z` members default to 1\. See [Listing 7.1](ch07.html#ch07lis01),
    which is excerpted from the NVIDIA SDK file `vector_types.h.`
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7.1.* `dim3` structure.'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p07lis01a)'
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1484
  prefs: []
  type: TYPE_NORMAL
- en: struct __device_builtin__ dim3
  id: totrans-1485
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-1486
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int x, y, z;
  id: totrans-1487
  prefs: []
  type: TYPE_NORMAL
- en: '#if defined(__cplusplus)'
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
- en: __host__ __device__ dim3(
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int vx = 1,
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int vy = 1,
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
- en: 'unsigned int vz = 1) : x(vx), y(vy), z(vz) {}'
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
- en: '__host__ __device__ dim3(uint3 v) : x(v.x), y(v.y), z(v.z) {}'
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
- en: __host__ __device__ operator uint3(void) {
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
- en: uint3 t;
  id: totrans-1495
  prefs: []
  type: TYPE_NORMAL
- en: t.x = x;
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
- en: t.y = y;
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
- en: t.z = z;
  id: totrans-1498
  prefs: []
  type: TYPE_NORMAL
- en: return t;
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
- en: '#endif /* __cplusplus */'
  id: totrans-1501
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
- en: Kernels can be launched via the driver API using `cuLaunchKernel()`, though
    that function takes the grid and block dimensions as discrete parameters rather
    than `dim3.`
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p207pro01a)'
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuLaunchKernel (
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
- en: CUfunction kernel,
  id: totrans-1507
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int gridDimX,
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int gridDimY,
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int gridDimZ,
  id: totrans-1510
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int blockDimX,
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int blockDimY,
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int blockDimZ,
  id: totrans-1513
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int sharedMemBytes,
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
- en: CUstream hStream,
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
- en: void **kernelParams,
  id: totrans-1516
  prefs: []
  type: TYPE_NORMAL
- en: void **extra
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
- en: );
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the triple-angle-bracket syntax, the parameters to `cuLaunchKernel()`
    include the kernel to invoke, the grid and block sizes, the amount of shared memory,
    and the stream. The main difference is in how the parameters to the kernel itself
    are given: Since the kernel microcode emitted by `ptxas` contains metadata that
    describes each kernel’s parameters,^([2](ch07.html#ch07fn2)) `kernelParams` is
    an array of `void *`, where each element corresponds to a kernel parameter. Since
    the type is known by the driver, the correct amount of memory (4 bytes for an
    `int`, 8 bytes for a `double`, etc.) will be copied into the command buffer as
    part of the hardware-specific command used to invoke the kernel.'
  id: totrans-1519
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch07.html#ch07fn2a). `cuLaunchKernel()` will fail on binary images that
    were not compiled with CUDA 3.2 or later, since that is the first version to include
    kernel parameter metadata.'
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.1\. Limitations
  id: totrans-1521
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All C++ classes participating in a kernel launch must be “plain old data” (POD)
    with the following characteristics.
  id: totrans-1522
  prefs: []
  type: TYPE_NORMAL
- en: • No user-declared constructors
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
- en: • No user-defined copy assignment operator
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
- en: • No user-defined destructor
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
- en: • No nonstatic data members that are not themselves PODs
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
- en: • No private or protected nonstatic data
  id: totrans-1527
  prefs: []
  type: TYPE_NORMAL
- en: • No base classes
  id: totrans-1528
  prefs: []
  type: TYPE_NORMAL
- en: • No virtual functions
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
- en: Note that classes that violate these rules may be used in CUDA, or even in CUDA
    kernels; they simply cannot be used for a kernel launch. In that case, the classes
    used by a CUDA kernel can be constructed using the POD input data from the launch.
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
- en: CUDA kernels also do not have return values. They must report their results
    back via device memory (which must be copied back to the CPU explicitly) or mapped
    host memory.
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2\. Caches and Coherency
  id: totrans-1532
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The GPU contains numerous caches to accelerate computation when reuse occurs.
    The constant cache is optimized for broadcast to the execution units within an
    SM; the texture cache reduces external bandwidth usage. Neither of these caches
    is kept coherent with respect to writes to memory by the GPU. For example, there
    is no protocol to enforce coherency between these caches and the L1 or L2 caches
    that serve to reduce latency and aggregate bandwidth to global memory. That means
    two things.
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** When a kernel is running, it must take care not to write memory that
    it (or a concurrently running kernel) also is accessing via constant or texture
    memory.'
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** The CUDA driver must invalidate the constant cache and texture cache
    before each kernel launch.'
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
- en: For kernels that do not contain `TEX` instructions, there is no need for the
    CUDA driver to invalidate the texture cache; as a result, kernels that do not
    use texture incur less driver overhead.
  id: totrans-1536
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3\. Asynchrony and Error Handling
  id: totrans-1537
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Kernel launches are *asynchronous*: As soon as a kernel is submitted to the
    hardware, it begins executing in parallel with the CPU.^([3](ch07.html#ch07fn3))
    This asynchrony complicates error handling. If a kernel encounters an error (for
    example, if it reads an invalid memory location), the error is reported to the
    driver (and the application) sometime after the kernel launch. The surest way
    to check for such errors is to synchronize with the GPU using `cudaDeviceSynchronize()`
    or `cuCtxSynchronize()`. If an error in kernel execution has occurred, the error
    code “unspecified launch failure” is returned.'
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch07.html#ch07fn3a). On most platforms, the kernel will start executing
    on the GPU microseconds after the CPU has finished processing the launch command.
    But on the Windows Display Driver Model (WDDM), it may take longer because the
    driver must perform a kernel thunk in order to submit the launch to the hardware,
    and work for the GPU is enqueued in user mode to amortize the overhead of the
    user→kernel transition.'
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
- en: Besides explicit CPU/GPU synchronization calls such as `cudaDevice-Synchronize()`
    or `cuCtxSynchronize()`, this error code may be returned by functions that implicitly
    synchronize with the CPU, such as synchronous memcpy calls.
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
- en: Invalid Kernel Launches
  id: totrans-1541
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: It is possible to request a kernel launch that the hardware cannot perform—for
    example, by specifying more threads per block than the hardware supports. When
    possible, the driver detects these cases and reports an error rather than trying
    to submit the launch to the hardware.
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA runtime and the driver API handle this case differently. When an invalid
    parameter is specified, the driver API’s explicit API calls such as `cuLaunchGrid()`
    and `cuLaunchKernel()` return error codes. But when using the CUDA runtime, since
    kernels are launched in-line with C/C++ code, there is no API call to return an
    error code. Instead, the error is “recorded” into a thread-local slot and applications
    can query the error value with `cudaGetLastError()`. This same error handling
    mechanism is used for kernel launches that are invalid for other reasons, such
    as a memory access violation.
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.4\. Timeouts
  id: totrans-1544
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Because the GPU is not able to context-switch in the midst of kernel execution,
    a long-running CUDA kernel may negatively impact the interactivity of a system
    that uses the GPU to interact with the user. As a result, many CUDA systems implement
    a “timeout” that resets the GPU if it runs too long without context switching.
  id: totrans-1545
  prefs: []
  type: TYPE_NORMAL
- en: On WDDM (Windows Display Driver Model), the timeout is enforced by the operating
    system. Microsoft has documented how this “[Timeout Detection and Recovery](ch03.html#ch03lev4sec1)”
    (TDR) works. See [http://bit.ly/WPPSdQ](http://bit.ly/WPPSdQ), which includes
    the Registry keys that control TDR behavior.^([4](ch07.html#ch07fn4)) TDR can
    be safely disabled by using the Tesla Compute Cluster (TCC) driver, though the
    TCC driver is not available for all hardware.
  id: totrans-1546
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch07.html#ch07fn4a). Modifying the Registry should only be done for test
    purposes, of course.'
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
- en: 'On Linux, the NVIDIA driver enforces a default timeout of 2 seconds. No time
    out is enforced on secondary GPUs that are not being used for display. Developers
    can query whether a runtime limit is being enforced on a given GPU by calling
    `cuDeviceGetAttribute()` with `CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_``TIMEOUT`, or
    by examining `cudaDeviceProp:: kernelExecTimeoutEnabled`.'
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.5\. Local Memory
  id: totrans-1549
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since local memory is per-thread, and a grid in CUDA can contain thousands of
    threads, the amount of local memory needed by a CUDA grid can be considerable.
    The developers of CUDA took pains to preallocate resources to minimize the likelihood
    that operations such as kernel launches would fail due to a lack of resources,
    but in the case of local memory, a conservative allocation simply would have consumed
    too much memory. As a result, kernels that use a large amount of local memory
    take longer and may be synchronous because the CUDA driver must allocate memory
    before performing the kernel launch. Furthermore, if the memory allocation fails,
    the kernel launch will fail due to a lack of resources.
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
- en: By default, when the CUDA driver must allocate local memory to run a kernel,
    it frees the memory after the kernel has finished. This behavior additionally
    makes the kernel launch synchronous. But this behavior can be inhibited by specifying
    `CU_CTX_LMEM_RESIZE_TO_MAX to cuCtxCreate()` or by calling `cudaSetDeviceFlags()`
    with `cudaDeviceLmemResizeToMax` before the primary context is created. In this
    case, the increased amount of local memory available will persist after launching
    a kernel that required more local memory than the default.
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.6\. Shared Memory
  id: totrans-1552
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Shared memory is allocated when the kernel is launched, and it stays allocated
    for the duration of the kernel’s execution. Besides static allocations that can
    be declared in the kernel, shared memory can be declared as an unsized `extern`;
    in that case, the amount of shared memory to allocate for the unsized array is
    specified as the third parameter of the kernel launch, or the `sharedMemBytes`
    parameter to `cuLaunchKernel()`.
  id: totrans-1553
  prefs: []
  type: TYPE_NORMAL
- en: 7.3\. Blocks, Threads, Warps, and Lanes
  id: totrans-1554
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kernels are launched as *grids* of *blocks* of threads. Threads can further
    be divided into 32-thread *warps*, and each thread in a warp is called a *lane*.
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1\. Grids of Blocks
  id: totrans-1556
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Thread blocks are separately scheduled onto SMs, and threads within a given
    block are executed by the same SM. [Figure 7.1](ch07.html#ch07fig01) shows a 2D
    grid (8W × 6H) of 2D blocks (8W × 8H). [Figure 7.2](ch07.html#ch07fig02) shows
    a 3D grid (8W × 6H × 6D) of 3D blocks (8W × 8H × 4D).
  id: totrans-1557
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07fig01.jpg)'
  id: totrans-1558
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.1* 2D grid and thread block.'
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07fig02.jpg)'
  id: totrans-1560
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.2* 3D grid and thread block.'
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
- en: Grids can be up to 65535 x 65535 blocks (for SM 1.x hardware) or 65535 x 65535
    x 65535 blocks (for SM 2.x hardware).^([5](ch07.html#ch07fn5)) Blocks may be up
    to 512 or 1024 threads in size,^([6](ch07.html#ch07fn6)) and threads within a
    block can communicate via the SM’s shared memory. Blocks within a grid are likely
    to be assigned to different SMs; to maximize throughput of the hardware, a given
    SM can run threads and warps from different blocks at the same time. The warp
    schedulers dispatch instructions as needed resources become available.
  id: totrans-1562
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch07.html#ch07fn5a). The maximum grid size is queryable via `CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X,`
    `CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y`, or `CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z`;
    or by calling `cudaGetDeviceGetProperties()` and examining `cudaDeviceProp::maxGridSize`.'
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch07.html#ch07fn6a). The maximum block size is queryable via `CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK`,
    or `deviceProp.maxThreadsPerBlock`.'
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  id: totrans-1565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Each threads gets a full complement of registers^([7](ch07.html#ch07fn7)) and
    a thread ID that is unique within the threadblock. To obviate the need to pass
    the size of the grid and threadblock into every kernel, the grid and block size
    also are available for kernels to read at runtime. The built-in variables used
    to reference these registers are given in [Table 7.1](ch07.html#ch07tab01). They
    are all of type `dim3`.
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch07.html#ch07fn7a). The more registers needed per thread, the fewer threads
    can “fit” in a given SM. The percentage of warps executing in an SM as compared
    to the theoretical maximum is called *occupancy* (see [Section 7.4](ch07.html#ch07lev1sec4)).'
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07tab01.jpg)'
  id: totrans-1568
  prefs: []
  type: TYPE_IMG
- en: '*Table 7.1* Built-In Variables'
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
- en: Taken together, these variables can be used to compute which part of a problem
    the thread will operate on. A “global” index for a thread can be computed as follows.
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p213pro01a)'
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
- en: int globalThreadId =
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
- en: threadIdx.x+blockDim.x*(threadIdx.y+blockDim.y*threadIdx.z);
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
- en: Warps, Lanes, and ILP
  id: totrans-1574
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The threads themselves are executed together, in SIMD fashion, in units of 32
    threads called a *warp*, after the collection of parallel threads in a loom.^([8](ch07.html#ch07fn8))
    (See [Figure 7.3](ch07.html#ch07fig03).) All 32 threads execute the same instruction,
    each using its private set of registers to perform the requested operation. In
    a triumph of mixed metaphor, the ID of a thread within a warp is called its *lane*.
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch07.html#ch07fn8a). The warp size can be queried, but it imposes such
    a huge compatibility burden on the hardware that developers can rely on it staying
    fixed at 32 for the foreseeable future.'
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07fig03.jpg)'
  id: totrans-1577
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.3* Loom.'
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
- en: The warp ID and lane ID can be computed using a global thread ID as follows.
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
- en: int warpID = globalThreadId >> 5;
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
- en: int laneID = globalThreadId & 31;
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
- en: Warps are an important unit of execution because they are the granularity with
    which GPUs can cover latency. It has been well documented how GPUs use parallelism
    to cover memory latency. It takes hundreds of clock cycles to satisfy a global
    memory request, so when a texture fetch or read is encountered, the GPU issues
    the memory request and then schedules other instructions until the data arrives.
    Once the data has arrived, the warp becomes eligible for execution again.
  id: totrans-1582
  prefs: []
  type: TYPE_NORMAL
- en: What has been less well documented is that GPUs also use parallelism to exploit
    ILP (“instruction level parallelism”). ILP refers to fine-grained parallelism
    that occurs during program execution; for example, when computing `(a+b)*(c+d)`,
    the addition operations `a+b` and `c+d` can be performed in parallel before the
    multiplication must be performed. Because the SMs already have a tremendous amount
    of logic to track dependencies and cover latency, they are very good at covering
    instruction latency through parallelism (which is effectively ILP) as well as
    memory latency. GPUs’ support for ILP is part of the reason loop unrolling is
    such an effective optimization strategy. Besides slightly reducing the number
    of instructions per loop iteration, it exposes more parallelism for the warp schedulers
    to exploit.
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
- en: Object Scopes
  id: totrans-1584
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The scopes of objects that may be referenced by a kernel grid are summarized
    in [Table 7.2](ch07.html#ch07tab02), from the most local (registers in each thread)
    to the most global (global memory and texture references are per grid). Before
    the advent of dynamic parallelism, thread blocks served primarily as a mechanism
    for interthread synchronization within a thread block (via intrinsics such as
    `__syncthreads()`) and communication (via shared memory). Dynamic parallelism
    adds resource management to the mix, since streams and events created within a
    kernel are only valid for threads within the same thread block.
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07tab02.jpg)'
  id: totrans-1586
  prefs: []
  type: TYPE_IMG
- en: '*Table 7.2* Object Scopes'
  id: totrans-1587
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2\. Execution Guarantees
  id: totrans-1588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is important that developers never make any assumptions about the order in
    which blocks or threads will execute. In particular, there is no way to know which
    block or thread will execute first, so initialization generally should be performed
    by code outside the kernel invocation.
  id: totrans-1589
  prefs: []
  type: TYPE_NORMAL
- en: Execution Guarantees and Interblock Synchronization
  id: totrans-1590
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Threads within a given thread block are guaranteed to be resident within the
    same SM, so they can communicate via shared memory and synchronize execution using
    intrinsics such as `__syncthreads()`. But thread blocks do not have any similar
    mechanisms for data interchange or synchronization.
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
- en: More sophisticated CUDA developers may ask, *But what about atomic operations
    in global memory?* Global memory can be updated in a thread-safe manner using
    atomic operations, so it is tempting to build something like a `__syncblocks()`
    function that, like `__syncthreads()`, waits until all blocks in the kernel launch
    have arrived before proceeding. Perhaps it would do an `atomicInc()` on a global
    memory location and, if `atomicInc()` did not return the block count, poll that
    memory location until it did.
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem is that the execution pattern of the kernel (for example, the mapping
    of thread blocks onto SMs) varies with the hardware configuration. For example,
    the number of SMs—and unless the GPU context is big enough to hold the entire
    grid—*some thread blocks may execute to completion before other thread blocks
    have started running*. The result is deadlock: Because not all blocks are necessarily
    resident in the GPU, the blocks that are polling the shared memory location prevent
    other blocks in the kernel launch from executing.'
  id: totrans-1593
  prefs: []
  type: TYPE_NORMAL
- en: There are a few special cases when interblock synchronization can work. If simple
    mutual exclusion is all that’s desired, `atomicCAS()` certainly can be used to
    provide that. Also, thread blocks can use atomics to signal when they’ve completed,
    so the last thread block in a grid can perform some operation before it exits,
    knowing that all the other thread blocks have completed execution. This strategy
    is employed by the `threadFenceReduction` SDK sample and the `reduction4SinglePass.cu`
    sample that accompanies this book (see [Section 12.2](ch12.html#ch12lev1sec2)).
  id: totrans-1594
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.3\. Block and Thread IDs
  id: totrans-1595
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A set of special read-only registers give each thread context in the form of
    a *thread ID* and *block ID*. The thread and block IDs are assigned as a CUDA
    kernel begins execution; for 2D and 3D grids and blocks, they are assigned in
    row-major order.
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
- en: Thread block sizes are best specified in multiples of 32, since warps are the
    finest possible granularity of execution on the GPU. [Figure 7.4](ch07.html#ch07fig04)
    shows how thread IDs are assigned in 32-thread blocks that are 32Wx1H, 16Wx2H,
    and 8Wx4H, respectively.
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07fig04.jpg)'
  id: totrans-1598
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.4* Blocks of 32 threads.'
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
- en: For blocks with a thread count that is not a multiple of 32, some warps are
    not fully populated with active threads. [Figure 7.5](ch07.html#ch07fig05) shows
    thread ID assignments for 28-thread blocks that are 28Wx1H, 14Wx2H, and 7Wx4H;
    in each case, 4 threads in the 32-thread warp are inactive for the duration of
    the kernel launch. For any thread block size not divisible by 32, some execution
    resources are wasted, as some warps will be launched with lanes that are disabled
    for the duration of the kernel execution. There is no performance benefit to 2D
    or 3D blocks or grids, but they sometimes make for a better match to the application.
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07fig05.jpg)'
  id: totrans-1601
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.5* Blocks of 28 threads.'
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
- en: The `reportClocks.cu` program illustrates how thread IDs are assigned and how
    warp-based execution works in general ([Listing 7.2](ch07.html#ch07lis02)).
  id: totrans-1603
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7.2.* `WriteClockValues` kernel.'
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p07lis02a)'
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
- en: WriteClockValues(
  id: totrans-1608
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int *completionTimes,
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int *threadIDs
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-1611
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
- en: size_t globalBlock = blockIdx.x+blockDim.x*
  id: totrans-1613
  prefs: []
  type: TYPE_NORMAL
- en: (blockIdx.y+blockDim.y*blockIdx.z);
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
- en: size_t globalThread = threadIdx.x+blockDim.x*
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
- en: (threadIdx.y+blockDim.y*threadIdx.z);
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
- en: size_t totalBlockSize = blockDim.x*blockDim.y*blockDim.z;
  id: totrans-1617
  prefs: []
  type: TYPE_NORMAL
- en: size_t globalIndex = globalBlock*totalBlockSize + globalThread;
  id: totrans-1618
  prefs: []
  type: TYPE_NORMAL
- en: completionTimes[globalIndex] = clock();
  id: totrans-1619
  prefs: []
  type: TYPE_NORMAL
- en: threadIDs[globalIndex] = threadIdx.y<<4|threadIdx.x;
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1622
  prefs: []
  type: TYPE_NORMAL
- en: '`WriteClockValues()` writes to the two output arrays using a global index computed
    using the block and thread IDs, and the grid and block sizes. One output array
    receives the return value from the `clock()` intrinsic, which returns a high-resolution
    timer value that increments for each warp. In the case of this program, we are
    using `clock()` to identify which warp processed a given value. `clock()` returns
    the value of a per-multiprocessor clock cycle counter, so we normalize the values
    by computing the minimum and subtracting it from all clock cycles values. We call
    the resulting values the thread’s “completion time.”'
  id: totrans-1623
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at completion times for threads in a pair of 16Wx8H blocks
    ([Listing 7.3](ch07.html#ch07lis03)) and compare them to completion times for
    14Wx8H blocks ([Listing 7.4](ch07.html#ch07lis04)). As expected, they are grouped
    in 32s, corresponding to the warp size.
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7.3.* Completion times (16W×8H blocks).'
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p07lis03a)'
  id: totrans-1626
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
- en: 0.01 ms for 256 threads = 0.03 us/thread
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
- en: 'Completion times (clocks):'
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (0, 0, 0) - slice 0:'
  id: totrans-1630
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  id: totrans-1632
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
- en: 8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
  id: totrans-1635
  prefs: []
  type: TYPE_NORMAL
- en: 8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
- en: a   a   a   a   a   a   a   a   a   a   a   a   a   a   a   a
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
- en: a   a   a   a   a   a   a   a   a   a   a   a   a   a   a   a
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (1, 0, 0) - slice 0:'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
- en: 2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
  id: totrans-1642
  prefs: []
  type: TYPE_NORMAL
- en: 2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
  id: totrans-1643
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  id: totrans-1644
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1646
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
- en: 'Thread IDs:'
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (0, 0, 0) - slice 0:'
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d   e   f
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d  1e  1f
  id: totrans-1651
  prefs: []
  type: TYPE_NORMAL
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d  2e  2f
  id: totrans-1652
  prefs: []
  type: TYPE_NORMAL
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d  3e  3f
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d  4e  4f
  id: totrans-1654
  prefs: []
  type: TYPE_NORMAL
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d  5e  5f
  id: totrans-1655
  prefs: []
  type: TYPE_NORMAL
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d  6e  6f
  id: totrans-1656
  prefs: []
  type: TYPE_NORMAL
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d  7e  7f
  id: totrans-1657
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (1, 0, 0) - slice 0:'
  id: totrans-1658
  prefs: []
  type: TYPE_NORMAL
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d   e   f
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d  1e  1f
  id: totrans-1660
  prefs: []
  type: TYPE_NORMAL
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d  2e  2f
  id: totrans-1661
  prefs: []
  type: TYPE_NORMAL
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d  3e  3f
  id: totrans-1662
  prefs: []
  type: TYPE_NORMAL
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d  4e  4f
  id: totrans-1663
  prefs: []
  type: TYPE_NORMAL
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d  5e  5f
  id: totrans-1664
  prefs: []
  type: TYPE_NORMAL
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d  6e  6f
  id: totrans-1665
  prefs: []
  type: TYPE_NORMAL
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d  7e  7f
  id: totrans-1666
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1667
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 7.4.* Completion times (14W×8H blocks).'
  id: totrans-1668
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch07_images.html#p07lis04a)'
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1670
  prefs: []
  type: TYPE_NORMAL
- en: 'Completion times (clocks):'
  id: totrans-1671
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (0, 0, 0) - slice 0:'
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1673
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   8   8   8   8   8   8   8   8   8   8
  id: totrans-1675
  prefs: []
  type: TYPE_NORMAL
- en: 8   8   8   8   8   8   8   8   8   8   8   8   8   8
  id: totrans-1676
  prefs: []
  type: TYPE_NORMAL
- en: 8   8   8   8   8   8   8   8   a   a   a   a   a   a
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
- en: a   a   a   a   a   a   a   a   a   a   a   a   a   a
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
- en: a   a   a   a   a   a   a   a   a   a   a   a   c   c
  id: totrans-1679
  prefs: []
  type: TYPE_NORMAL
- en: c   c   c   c   c   c   c   c   c   c   c   c   c   c
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (1, 0, 0) - slice 0:'
  id: totrans-1681
  prefs: []
  type: TYPE_NORMAL
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0
  id: totrans-1683
  prefs: []
  type: TYPE_NORMAL
- en: 0   0   0   0   2   2   2   2   2   2   2   2   2   2
  id: totrans-1684
  prefs: []
  type: TYPE_NORMAL
- en: 2   2   2   2   2   2   2   2   2   2   2   2   2   2
  id: totrans-1685
  prefs: []
  type: TYPE_NORMAL
- en: 2   2   2   2   2   2   2   2   4   4   4   4   4   4
  id: totrans-1686
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
- en: 4   4   4   4   4   4   4   4   4   4   4   4   6   6
  id: totrans-1688
  prefs: []
  type: TYPE_NORMAL
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
- en: 'Thread IDs:'
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (0, 0, 0) - slice 0:'
  id: totrans-1691
  prefs: []
  type: TYPE_NORMAL
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d
  id: totrans-1693
  prefs: []
  type: TYPE_NORMAL
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d
  id: totrans-1694
  prefs: []
  type: TYPE_NORMAL
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d
  id: totrans-1695
  prefs: []
  type: TYPE_NORMAL
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d
  id: totrans-1696
  prefs: []
  type: TYPE_NORMAL
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d
  id: totrans-1697
  prefs: []
  type: TYPE_NORMAL
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d
  id: totrans-1698
  prefs: []
  type: TYPE_NORMAL
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
- en: 'Grid (1, 0, 0) - slice 0:'
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d
  id: totrans-1701
  prefs: []
  type: TYPE_NORMAL
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d
  id: totrans-1702
  prefs: []
  type: TYPE_NORMAL
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d
  id: totrans-1703
  prefs: []
  type: TYPE_NORMAL
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d
  id: totrans-1704
  prefs: []
  type: TYPE_NORMAL
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d
  id: totrans-1705
  prefs: []
  type: TYPE_NORMAL
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d
  id: totrans-1706
  prefs: []
  type: TYPE_NORMAL
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d
  id: totrans-1707
  prefs: []
  type: TYPE_NORMAL
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d
  id: totrans-1708
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
- en: The completion times for the 14Wx8H blocks given in [Listing 7.4](ch07.html#ch07lis04)
    underscore how the thread IDs map to warps. In the case of the 14Wx8H blocks,
    every warp holds only 28 threads; 12.5% of the number of possible thread lanes
    are idle throughout the kernel’s execution. To avoid this waste, developers always
    should try to make sure blocks contain a multiple of 32 threads.
  id: totrans-1710
  prefs: []
  type: TYPE_NORMAL
- en: 7.4\. Occupancy
  id: totrans-1711
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Occupancy is a ratio that measures the number of threads/SM that *will* run
    in a given kernel launch, as opposed to the maximum number of threads that *potentially
    could be running* on that SM.
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/220equ01.jpg)'
  id: totrans-1713
  prefs: []
  type: TYPE_IMG
- en: The denominator (maximum warps per SM) is a constant that depends only on the
    compute capability of the device. The numerator of this expression, which determines
    the occupancy, is a function of the following.
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
- en: • Compute capability (1.0, 1.1, 1.2, 1.3, 2.0, 2.1, 3.0, 3.5)
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
- en: • Threads per block
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
- en: • Registers per thread
  id: totrans-1717
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory configuration^([9](ch07.html#ch07fn9))
  id: totrans-1718
  prefs: []
  type: TYPE_NORMAL
- en: '[9](ch07.html#ch07fn9a). For SM 2.x and later only. Developers can split the
    64K L1 cache in the SM as 16K shared/48K L1 or 48K shared/16K L1\. (SM 3.x adds
    the ability to split the cache evenly as 32K shared/32K L1.)'
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory per block
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
- en: To help developers assess the tradeoffs between these parameters, the CUDA Toolkit
    includes an occupancy calculator in the form of an Excel spreadsheet.^([10](ch07.html#ch07fn10))
    Given the inputs above, the spreadsheet will calculate the following results.
  id: totrans-1721
  prefs: []
  type: TYPE_NORMAL
- en: '[10](ch07.html#ch07fn10a). Typically it is in the tools subdirectory—for example,
    `%CUDA_PATH%/tools` (Windows) or `$CUDA_PATH/tools`.'
  id: totrans-1722
  prefs: []
  type: TYPE_NORMAL
- en: • Active thread count
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
- en: • Active warp count
  id: totrans-1724
  prefs: []
  type: TYPE_NORMAL
- en: • Active block count
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
- en: • Occupancy (active warp count divided into the hardware’s maximum number of
    active warps)
  id: totrans-1726
  prefs: []
  type: TYPE_NORMAL
- en: The spreadsheet also identifies whichever parameter is limiting the occupancy.
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
- en: • Registers per multiprocessor
  id: totrans-1728
  prefs: []
  type: TYPE_NORMAL
- en: • Maximum number of warps or blocks per multiprocessor
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory per multiprocessor
  id: totrans-1730
  prefs: []
  type: TYPE_NORMAL
- en: Note that occupancy is not the be-all and end-all of CUDA performance;^([11](ch07.html#ch07fn11))
    often it is better to use more registers per thread and rely on instruction-level
    parallelism (ILP) to deliver performance. NVIDIA has a good presentation on warps
    and occupancy that discusses the tradeoffs.^([12](ch07.html#ch07fn12))
  id: totrans-1731
  prefs: []
  type: TYPE_NORMAL
- en: '[11](ch07.html#ch07fn11a). Vasily Volkov emphatically makes this point in his
    presentation “Better Performance at Lower Occupancy.” It is available at [http://bit.ly/YdScNG](http://bit.ly/YdScNG).'
  id: totrans-1732
  prefs: []
  type: TYPE_NORMAL
- en: '[12](ch07.html#ch07fn12a). [http://bit.ly/WHTb5m](http://bit.ly/WHTb5m)'
  id: totrans-1733
  prefs: []
  type: TYPE_NORMAL
- en: An example of a low-occupancy kernel that can achieve near-maximum global memory
    bandwidth is given in [Section 5.2.10](ch05.html#ch05lev2sec18) ([Listing 5.5](ch05.html#ch05lis05)).
    The inner loop of the `GlobalReads` kernel can be unrolled according to a template
    parameter; as the number of unrolled iterations increases, the number of needed
    registers increases and the occupancy goes down. For the Tesla M2050’s in the
    `cg1.4xlarge` instance type, for example, the peak read bandwidth reported (with
    ECC disabled) is 124GiB/s, with occupancy of 66%. Volkov reports achieving near-peak
    memory bandwidth when running kernels whose occupancy is in the single digits.
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
- en: 7.5\. Dynamic Parallelism
  id: totrans-1735
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Dynamic parallelism*, a new capability that works only on SM 3.5–class hardware,
    enables CUDA kernels to launch other CUDA kernels, and also to invoke various
    functions in the CUDA runtime. When using dynamic parallelism, a subset of the
    CUDA runtime (known as the *device runtime*) becomes available for use by threads
    running on the device.'
  id: totrans-1736
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic parallelism introduces the idea of “parent” and “child” grids. Any kernel
    invoked by another CUDA kernel (as opposed to host code, as done in all previous
    CUDA versions) is a “child kernel,” and the invoking grid is its “parent.” By
    default, CUDA supports two (2) nesting levels (one for the parent and one for
    the child), a number that may be increased by calling `cudaSetDeviceLimit()` with
    `cudaLimitDevRuntimeSyncDepth`.
  id: totrans-1737
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic parallelism was designed to address applications that previously had
    to deliver results to the CPU so the CPU could specify which work to perform on
    the GPU. Such “handshaking” disrupts CPU/GPU concurrency in the execution pipeline
    described in [Section 2.5.1](ch02.html#ch02lev2sec11), in which the CPU produces
    commands for consumption by the GPU. The GPU’s time is too valuable for it to
    wait for the CPU to read and analyze results before issuing more work. Dynamic
    parallelism avoids these pipeline bubbles by enabling the GPU to launch work for
    itself from kernels.
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic parallelism can improve performance in several cases.
  id: totrans-1739
  prefs: []
  type: TYPE_NORMAL
- en: • It enables initialization of data structures needed by a kernel before the
    kernel can begin execution. Previously, such initialization had to be taken care
    of in host code or by previously invoking a separate kernel.
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
- en: • It enables simplified recursion for applications such as Barnes-Hut gravitational
    integration or hierarchical grid evaluation for aerodynamic simulations.
  id: totrans-1741
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1743
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic parallelism only works within a given GPU. Kernels can invoke memory
    copies or other kernels, but they cannot submit work to other GPUs.
  id: totrans-1744
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1745
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.1\. Scoping and Synchronization
  id: totrans-1746
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With the notable exception of block and grid size, child grids inherit most
    kernel configuration parameters, such as the shared memory configuration (set
    by `cudaDeviceSetCacheConfig()`), from their parents. Thread blocks are a unit
    of scope: Streams and events created by a thread block can only be used by that
    thread block (they are not even inherited for use by child grids), and they are
    automatically destroyed when the thread block exits.'
  id: totrans-1747
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1748
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1749
  prefs: []
  type: TYPE_NORMAL
- en: Resources created on the device via dynamic parallelism are strictly separated
    from resources created on the host. Streams and events created on the host may
    not be used on the device via dynamic parallelism, and vice versa.
  id: totrans-1750
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1751
  prefs: []
  type: TYPE_NORMAL
- en: CUDA guarantees that a parent grid is not considered complete until all of its
    children have finished. Although the parent may execute concurrently with the
    child, there is no guarantee that a child grid will begin execution until its
    parent calls `cudaDeviceSynchronize().`
  id: totrans-1752
  prefs: []
  type: TYPE_NORMAL
- en: If all threads in a thread block exit, execution of the thread block is suspended
    until all child grids have finished. If that synchronization is not sufficient,
    developers can use CUDA streams and events for explicit synchronization. As on
    the host, operations within a given stream are performed in the order of submission.
    Operations can only execute concurrently if they are specified in different streams,
    and there is no guarantee that operations will, in fact, execute concurrently.
    If needed, synchronization primitives such as `__syncthreads()` can be used to
    coordinate the order of submission to a given stream.
  id: totrans-1753
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
- en: Streams and events created on the device may not be used outside the thread
    block that created them.
  id: totrans-1756
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1757
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaDeviceSynchronize()` synchronizes on all pending work launched by any
    thread in the thread block. It does not, however, perform any interthread synchronization,
    so if there is a desire to synchronize on work launched by other threads, developers
    must use `__syncthreads()` or other block-level synchronization primitives (see
    [Section 8.6.2](ch08.html#ch08lev2sec21)).'
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.2\. Memory Model
  id: totrans-1759
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Parent and child grids share the same global and constant memory storage, but
    they have distinct local and shared memory.
  id: totrans-1760
  prefs: []
  type: TYPE_NORMAL
- en: Global Memory
  id: totrans-1761
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are two points in the execution of a child grid when its view of memory
    is fully consistent with the parent grid: when the child grid is invoked by the
    parent and when the child grid completes as signaled by a synchronization API
    invocation in the parent thread.'
  id: totrans-1762
  prefs: []
  type: TYPE_NORMAL
- en: All global memory operations in the parent thread prior to the child thread’s
    invocation are visible to the child grid. All memory operations of the child grid
    are visible to the parent after the parent has synchronized on the child grid’s
    completion. Zero-copy memory has the same coherence and consistency guarantees
    as global memory.
  id: totrans-1763
  prefs: []
  type: TYPE_NORMAL
- en: Constant Memory
  id: totrans-1764
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Constants are immutable and may not be modified from the device during kernel
    execution. Taking the address of a constant memory object from within a kernel
    thread has the same semantics as for all CUDA programs,^([13](ch07.html#ch07fn13))
    and passing that pointer between parents and their children is fully supported.
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
- en: '[13](ch07.html#ch07fn13a). Note that in device code, the address must be taken
    with the “address-of” operator (unary operator&), since `cudaGetSymbolAddress()`
    is not supported by the device runtime.'
  id: totrans-1766
  prefs: []
  type: TYPE_NORMAL
- en: Shared and Local Memory
  id: totrans-1767
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Shared and local memory is private to a thread block or thread, respectively,
    and is not visible or coherent between parent and child. When an object in one
    of these locations is referenced outside its scope, the behavior is undefined
    and would likely cause an error.
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
- en: If `nvcc` detects an attempt to misuse a pointer to shared or local memory,
    it will issue a warning. Developers can use the `__isGlobal()` intrinsic to determine
    whether a given pointer references global memory. Pointers to shared or local
    memory are not valid parameters to `cudaMemcpy*Async()` or `cudaMemset*Async()`.
  id: totrans-1769
  prefs: []
  type: TYPE_NORMAL
- en: Local Memory
  id: totrans-1770
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Local memory is private storage for an executing thread and is not visible outside
    of that thread. It is illegal to pass a pointer to local memory as a launch argument
    when launching a child kernel. The result of dereferencing such a local memory
    pointer from a child will be undefined. To guarantee that this rule is not inadvertently
    violated by the compiler, all storage passed to a child kernel should be allocated
    explicitly from the global memory heap.
  id: totrans-1771
  prefs: []
  type: TYPE_NORMAL
- en: Texture Memory
  id: totrans-1772
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Concurrent accesses by parent and child may result in inconsistent data and
    should be avoided. That said, a degree of coherency between parent and child is
    enforced by the runtime. A child kernel can use texturing to access memory written
    by its parent, but writes to memory by a child will not be reflected in the texture
    memory accesses by a parent until *after* the parent synchronizes on the child’s
    completion. Texture objects are well supported in the device runtime. They cannot
    be created or destroyed, but they can be passed in and used by any grid in the
    hierarchy (parent or child).
  id: totrans-1773
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.3\. Streams and Events
  id: totrans-1774
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Streams and events created by the device runtime can be used only within the
    thread block that created the stream. The NULL stream has different semantics
    in the device runtime than in the host runtime. On the host, synchronizing with
    the NULL stream forces a “join” of all the other streamed operations on the GPU
    (as described in [Section 6.2.3](ch06.html#ch06lev2sec4)); on the device, the
    NULL stream is its own stream, and any interstream synchronization must be performed
    using events.
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
- en: When using the device runtime, streams must be created with the `cudaStream-NonBlocking`
    flag (a parameter to `cudaStreamCreateWithFlags()`). The `cudaStreamSynchronize()`
    call is not supported; synchronization must be implemented in terms of events
    and `cudaStreamWaitEvent()`.
  id: totrans-1776
  prefs: []
  type: TYPE_NORMAL
- en: Only the interstream synchronization capabilities of CUDA events are supported.
    As a consequence, `cudaEventSynchronize()`, `cudaEventElapsedTime()`, and `cudaEventQuery()`
    are not supported. Additionally, because timing is not supported, events must
    be created by passing the `cudaEventDisableTiming` flag to `cudaEventCreateWithFlags()`.
  id: totrans-1777
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.4\. Error Handling
  id: totrans-1778
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Any function in the device runtime may return an error (`cudaError_t`). The
    error is recorded in a per-thread slot that can be queried by calling `cudaGetLastError()`.
    As with the host-based runtime, CUDA makes a distinction between errors that can
    be returned immediately (e.g., if an invalid parameter is passed to a memcpy function)
    and errors that must be reported asynchronously (e.g., if a launch performed an
    invalid memory access). If a child grid causes an error at runtime, CUDA will
    return an error to the host, not to the parent grid.
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.5\. Compiling and Linking
  id: totrans-1780
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike the host runtime, developers must explicitly link against the device
    runtime’s static library when using the device runtime. On Windows, the device
    runtime is `cudadevrt.lib`; on Linux and MacOS, it is `cudadevrt.a`. When building
    with `nvcc`, this may be accomplished by appending `-lcudadevrt` to the command
    line.
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.6\. Resource Management
  id: totrans-1782
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Whenever a kernel launches a child grid, the child is considered a new *nesting
    level*, and the total number of levels is the *nesting depth* of the program.
    In contrast, the deepest level at which the program will explicitly synchronize
    on a child launch is called the *synchronization depth*. Typically the synchronization
    depth is one less than the nesting depth of the program, but if the program does
    not always need to call `cudaDeviceSynchronize()`, then it may be substantially
    less than the nesting depth.
  id: totrans-1783
  prefs: []
  type: TYPE_NORMAL
- en: The theoretical maximum nesting depth is 24, but in practice it is governed
    by the device limit `cudaLimitDevRuntimeSyncDepth`. Any launch that would result
    in a kernel at a deeper level than the maximum will fail. The default maximum
    synchronization depth level is 2\. The limits must be configured before the top-level
    kernel is launched from the host.
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1785
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1786
  prefs: []
  type: TYPE_NORMAL
- en: Calling a device runtime function such as `cudaMemcpyAsync()` may invoke a kernel,
    increasing the nesting depth by 1.
  id: totrans-1787
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1788
  prefs: []
  type: TYPE_NORMAL
- en: For parent kernels that never call `cudaDeviceSynchronize()`, the system does
    not have to reserve space for the parent kernel. In this case, the memory footprint
    required for a program will be much less than the conservative maximum. Such a
    program could specify a shallower maximum synchronization depth to avoid overallocation
    of backing store.
  id: totrans-1789
  prefs: []
  type: TYPE_NORMAL
- en: Memory Footprint
  id: totrans-1790
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The device runtime system software reserves device memory for the following
    purposes.
  id: totrans-1791
  prefs: []
  type: TYPE_NORMAL
- en: • To track pending grid launches
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
- en: • To save saving parent-grid state during synchronization
  id: totrans-1793
  prefs: []
  type: TYPE_NORMAL
- en: • To serve as an allocation heap for `malloc()` and `cudaMalloc()` calls from
    kernels
  id: totrans-1794
  prefs: []
  type: TYPE_NORMAL
- en: This memory is not available for use by the application, so some applications
    may wish to reduce the default allocations, and some applications may have to
    increase the default values in order to operate correctly. To change the default
    values, developers call `cudaDeviceSetLimit()`, as summarized in [Table 7.3](ch07.html#ch07tab03).
    The limit `cudaLimitDevRuntimeSyncDepth` is especially important, since each nesting
    level costs up to 150MB of device memory.
  id: totrans-1795
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07tab03.jpg)'
  id: totrans-1796
  prefs: []
  type: TYPE_IMG
- en: '*Table 7.3* `cudaDeviceSetLimit()` Values'
  id: totrans-1797
  prefs: []
  type: TYPE_NORMAL
- en: Pending Kernel Launches
  id: totrans-1798
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When a kernel is launched, all associated configuration and parameter data is
    tracked until the kernel completes. This data is stored within a system-managed
    launch pool. The size of the launch pool is configurable by calling `cudaDeviceSetLimit()`
    from the host and specifying `cudaLimitDevRuntimePendingLaunchCount`.
  id: totrans-1799
  prefs: []
  type: TYPE_NORMAL
- en: Configuration Options
  id: totrans-1800
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Resource allocation for the device runtime system software is controlled via
    the `cudaDeviceSetLimit()` API from the host program. Limits must be set before
    any kernel is launched and may not be changed while the GPU is actively running
    programs.
  id: totrans-1801
  prefs: []
  type: TYPE_NORMAL
- en: Memory allocated by the device runtime must be freed by the device runtime.
    Also, memory is allocated by the device runtime out of a preallocated heap whose
    size is specified by the device limit `cudaLimitMallocHeapSize`. The named limits
    in [Table 7.3](ch07.html#ch07tab03) may be set.
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07tab04.jpg)'
  id: totrans-1803
  prefs: []
  type: TYPE_IMG
- en: '*Table 7.4* Device Runtime Limitations'
  id: totrans-1804
  prefs: []
  type: TYPE_NORMAL
- en: 7.5.7\. Summary
  id: totrans-1805
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Table 7.4](ch07.html#ch07tab04) summarizes the key differences and limitations
    between the device runtime and the host runtime. [Table 7.5](ch07.html#ch07tab05)
    lists the subset of functions that may be called from the device runtime, along
    with any pertinent limitations.'
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/07tab05.jpg)![Image](graphics/07tab05a.jpg)'
  id: totrans-1807
  prefs: []
  type: TYPE_IMG
- en: '*Table 7.5.* CUDA Device Runtime Functions'
  id: totrans-1808
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 8\. Streaming Multiprocessors
  id: totrans-1809
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The streaming multiprocessors (SMs) are the part of the GPU that runs our CUDA
    kernels. Each SM contains the following.
  id: totrans-1810
  prefs: []
  type: TYPE_NORMAL
- en: • Thousands of registers that can be partitioned among threads of execution
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
- en: '• Several caches:'
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
- en: – *Shared memory* for fast data interchange between threads
  id: totrans-1813
  prefs: []
  type: TYPE_NORMAL
- en: – *Constant cache* for fast broadcast of reads from constant memory
  id: totrans-1814
  prefs: []
  type: TYPE_NORMAL
- en: – *Texture cache* to aggregate bandwidth from texture memory
  id: totrans-1815
  prefs: []
  type: TYPE_NORMAL
- en: – *L1 cache* to reduce latency to local or global memory
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
- en: • *Warp schedulers* that can quickly switch contexts between threads and issue
    instructions to warps that are ready to execute
  id: totrans-1817
  prefs: []
  type: TYPE_NORMAL
- en: '• Execution cores for integer and floating-point operations:'
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
- en: – Integer and single-precision floating point operations
  id: totrans-1819
  prefs: []
  type: TYPE_NORMAL
- en: – Double-precision floating point
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
- en: – Special Function Units (SFUs) for single-precision floating-point transcendental
    functions
  id: totrans-1821
  prefs: []
  type: TYPE_NORMAL
- en: The reason there are many registers and the reason the hardware can context
    switch between threads so efficiently are to maximize the throughput of the hardware.
    The GPU is designed to have enough state to cover both execution latency and the
    memory latency of hundreds of clock cycles that it may take for data from device
    memory to arrive after a read instruction is executed.
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
- en: 'The SMs are general-purpose processors, but they are designed very differently
    than the execution cores in CPUs: They target much lower clock rates; they support
    instruction-level parallelism, but not branch prediction or speculative execution;
    and they have less cache, if they have any cache at all. For suitable workloads,
    the sheer computing horsepower in a GPU more than makes up for these disadvantages.'
  id: totrans-1823
  prefs: []
  type: TYPE_NORMAL
- en: The design of the SM has been evolving rapidly since the introduction of the
    first CUDA-capable hardware in 2006, with three major revisions, codenamed Tesla,
    Fermi, and Kepler. Developers can query the compute capability by calling `cudaGetDeviceProperties()`
    and examining `cudaDeviceProp.major` and `cudaDeviceProp.minor`, or by calling
    the driver API function `cuDeviceComputeCapability()`. Compute capability 1.x,
    2.x, and 3.x correspond to Tesla-class, Fermi-class, and Kepler-class hardware,
    respectively. [Table 8.1](ch08.html#ch08tab01) summarizes the capabilities added
    in each generation of the SM hardware.
  id: totrans-1824
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab01.jpg)'
  id: totrans-1825
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.1* SM Capabilities'
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 2](ch02.html#ch02), [Figures 2.29](ch02.html#ch02fig29) through
    [2.32](ch02.html#ch02fig32) show block diagrams of different SMs. CUDA cores can
    execute integer and single-precision floating-point instructions; one double-precision
    unit implements double-precision support, if available; and Special Function Units
    implement reciprocal, recriprocal square root, sine/cosine, and logarithm/exponential
    functions. Warp schedulers dispatch instructions to these execution units as the
    resources needed to execute the instruction become available.
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on the instruction set capabilities of the SM. As such,
    it sometimes refers to the “SASS” instructions, the native instructions into which
    `ptxas` or the CUDA driver translate intermediate PTX code. Developers are not
    able to author SASS code directly; instead, NVIDIA has made these instructions
    visible to developers through the `cuobjdump` utility so they can direct optimizations
    of their source code by examining the compiled microcode.
  id: totrans-1828
  prefs: []
  type: TYPE_NORMAL
- en: 8.1\. Memory
  id: totrans-1829
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 8.1.1\. Registers
  id: totrans-1830
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each SM contains thousands of 32-bit registers that are allocated to threads
    as specified when the kernel is launched. Registers are both the fastest and most
    plentiful memory in the SM. As an example, the Kepler-class (SM 3.0) SMX contains
    65,536 registers or 256K, while the texture cache is only 48K.
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
- en: CUDA registers can contain integer or floating-point data; for hardware capable
    of performing double-precision arithmetic (SM 1.3 and higher), the operands are
    contained in even-valued register pairs. On SM 2.0 and higher hardware, register
    pairs also can hold 64-bit addresses.
  id: totrans-1832
  prefs: []
  type: TYPE_NORMAL
- en: 'CUDA hardware also supports wider memory transactions: The built-in `int2/float2`
    and `int4/float4` data types, residing in aligned register pairs or quads, respectively,
    may be read or written using single 64- or 128-bit-wide loads or stores. Once
    in registers, the individual data elements can be referenced as `.x/.y` (for `int2/float2`)
    or `.x/.y/.z/.w` (for `int4/float4`).'
  id: totrans-1833
  prefs: []
  type: TYPE_NORMAL
- en: Developers can cause `nvcc` to report the number of registers used by a kernel
    by specifying the command-line option `--ptxas-options -–verbose`. The number
    of registers used by a kernel affects the number of threads that can fit in an
    SM and often must be tuned carefully for optimal performance. The maximum number
    of registers used for a compilation may be specified with `--ptxas-options --maxregcount
    N`.
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
- en: Register Aliasing
  id: totrans-1835
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Because registers can hold floating-point or integer data, some intrinsics serve
    only to coerce the compiler into changing its view of a variable. The `__int_as_float()`
    and `__float_as_int()` intrinsics cause a variable to “change personalities” between
    32-bit integer and single-precision floating point.
  id: totrans-1836
  prefs: []
  type: TYPE_NORMAL
- en: float __int_as_float( int i );
  id: totrans-1837
  prefs: []
  type: TYPE_NORMAL
- en: int __float_as_int( float f );
  id: totrans-1838
  prefs: []
  type: TYPE_NORMAL
- en: The `__double2loint()`, `__double2hiint()`, and `__hiloint2double()` intrinsics
    similarly cause registers to change personality (usually in-place). `__double_as_longlong()`
    and `__longlong_as_double()` coerce register pairs in-place; `__double2loint()`
    and `__double2hiint()` return the least and the most significant 32 bits of the
    input operand, respectively; and `__hiloint2double()` constructs a `double` out
    of the high and low halves.
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p234pro02a)'
  id: totrans-1840
  prefs: []
  type: TYPE_NORMAL
- en: int double2loint( double d );
  id: totrans-1841
  prefs: []
  type: TYPE_NORMAL
- en: int double2hiint( double d );
  id: totrans-1842
  prefs: []
  type: TYPE_NORMAL
- en: int hiloint2double( int hi, int lo );
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
- en: double long_as_double(long long int i );
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
- en: long long int __double_as_longlong( double d );
  id: totrans-1845
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2\. Local Memory
  id: totrans-1846
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Local memory is used to spill registers and also to hold local variables that
    are indexed and whose indices cannot be computed at compile time. Local memory
    is backed by the same pool of device memory as global memory, so it exhibits the
    same latency characteristics and benefits as the L1 and L2 cache hierarchy on
    Fermi and later hardware. Local memory is addressed in such a way that the memory
    transactions are automatically coalesced. The hardware includes special instructions
    to load and store local memory: The SASS variants are `LLD/LST` for Tesla and
    `LDL/STL` for Fermi and Kepler.'
  id: totrans-1847
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.3\. Global Memory
  id: totrans-1848
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The SMs can read or write global memory using `GLD/GST` instructions (on Tesla)
    and `LD/ST` instructions (on Fermi and Kepler). Developers can use standard C
    operators to compute and dereference addresses, including pointer arithmetic and
    the dereferencing operators `*`, `[]`, and `->`. Operating on 64- or 128-bit built-in
    data types (`int2/float2/int4/float4`) automatically causes the compiler to issue
    64- or 128-bit load and store instructions. Maximum memory performance is achieved
    through *coalescing* of memory transactions, described in [Section 5.2.9](ch05.html#ch05lev2sec17).
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
- en: Tesla-class hardware (SM 1.x) uses special address registers to hold pointers;
    later hardware implements a load/store architecture that uses the same register
    file for pointers; integer and floating-point values; and the same address space
    for constant memory, shared memory, and global memory.^([1](ch08.html#ch08fn1))
  id: totrans-1850
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch08.html#ch08fn1a). Both constant and shared memory exist in address windows
    that enable them to be referenced by 32-bit addresses even on 64-bit architectures.'
  id: totrans-1851
  prefs: []
  type: TYPE_NORMAL
- en: Fermi-class hardware includes several features not available on older hardware.
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
- en: • 64-bit addressing is supported via “wide” load/store instructions in which
    addresses are held in even-numbered register pairs. 64-bit addressing is not supported
    on 32-bit host platforms; on 64-bit host platforms, 64-bit addressing is enabled
    automatically. As a result, code generated for the same kernels compiled for 32-
    and 64-bit host platforms may have different register counts and performance.
  id: totrans-1853
  prefs: []
  type: TYPE_NORMAL
- en: • The L1 cache may be configured to be 16K or 48K in size.^([2](ch08.html#ch08fn2))
    (Kepler added the ability to split the cache as 32K L1/32K shared.) Load instructions
    can include cacheability hints (to tell the hardware to pull the read into L1
    or to bypass the L1 and keep the data only in L2). These may be accessed via inline
    PTX or through the command line option `–X ptxas –dlcm=ca` (cache in L1 and L2,
    the default setting) or `–X ptxas –dlcm=cg` (cache only in L2).
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch08.html#ch08fn2a). The hardware can change this configuration per kernel
    launch, but changing this state is expensive and will break concurrency for concurrent
    kernel launches.'
  id: totrans-1855
  prefs: []
  type: TYPE_NORMAL
- en: Atomic operations (or just “atomics”) update a memory location in a way that
    works correctly even when multiple GPU threads are operating on the same memory
    location. The hardware enforces mutual exclusion on the memory location for the
    duration of the operation. Since the order of operations is not guaranteed, the
    operators supported generally are associative.^([3](ch08.html#ch08fn3))
  id: totrans-1856
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch08.html#ch08fn3a). The only exception is single-precision floating-point
    addition. Then again, floating-point code generally must be robust in the face
    of the lack of associativity of floating-point operations; porting to different
    hardware, or even just recompiling the same code with different compiler options,
    can change the order of floating-point operations and thus the result.'
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
- en: Atomics first became available for global memory for SM 1.1 and greater and
    for shared memory for SM 1.2 and greater. Until the Kepler generation of hardware,
    however, global memory atomics were too slow to be useful.
  id: totrans-1858
  prefs: []
  type: TYPE_NORMAL
- en: The global atomic intrinsics, summarized in [Table 8.2](ch08.html#ch08tab02),
    become automatically available when the appropriate architecture is specified
    to `nvcc` via `--gpu-architecture`. All of these intrinsics can operate on 32-bit
    integers. 64-bit support for `atomicAdd()`, `atomicExch()`, and `atomicCAS()`
    was added in SM 1.2\. `atomicAdd()` of 32-bit floating-point values (`float`)
    was added in SM 2.0\. 64-bit support for `atomicMin()`, `atomicMax()`, `atomicAnd()`,
    `atomicOr()`, and `atomicXor()` was added in SM 3.5.
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab02.jpg)'
  id: totrans-1860
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.2* Atomic Operations'
  id: totrans-1861
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1862
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-1863
  prefs: []
  type: TYPE_NORMAL
- en: Because atomic operations are implemented using hardware in the GPU’s integrated
    memory controller, they do not work across the PCI Express bus and thus do not
    work correctly on device memory pointers that correspond to host memory or peer
    memory.
  id: totrans-1864
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1865
  prefs: []
  type: TYPE_NORMAL
- en: 'At the hardware level, atomics come in two forms: atomic operations that return
    the value that was at the specified memory location before the operator was performed,
    and reduction operations that the developer can “fire and forget” at the memory
    location, ignoring the return value. Since the hardware can perform the operation
    more efficiently if there is no need to return the old value, the compiler detects
    whether the return value is used and, if it is not, emits different instructions.
    In SM 2.0, for example, the instructions are called `ATOM` and `RED`, respectively.'
  id: totrans-1866
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.4\. Constant Memory
  id: totrans-1867
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Constant memory resides in device memory, but it is backed by a different, read-only
    cache that is optimized to broadcast the results of read requests to threads that
    all reference the same memory location. Each SM contains a small, latency-optimized
    cache for purposes of servicing these read requests. Making the memory (and the
    cache) read-only simplifies cache management, since the hardware has no need to
    implement write-back policies to deal with memory that has been updated.
  id: totrans-1868
  prefs: []
  type: TYPE_NORMAL
- en: SM 2.x and subsequent hardware includes a special optimization for memory that
    is not denoted as constant but that the compiler has identified as (1) read-only
    and (2) whose address is not dependent on the block or thread ID. The “load uniform”
    (LDU) instruction reads memory using the constant cache hierarchy and broadcasts
    the data to the threads.
  id: totrans-1869
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.5\. Shared Memory
  id: totrans-1870
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Shared memory is very fast, on-chip memory in the SM that threads can use for
    data interchange within a thread block. Since it is a per-SM resource, shared
    memory usage can affect occupancy, the number of warps that the SM can keep resident.
    SMs load and store shared memory with special instructions: `G2R/R2G` on SM 1.x,
    and `LDS/STS` on SM 2.x and later.'
  id: totrans-1871
  prefs: []
  type: TYPE_NORMAL
- en: Shared memory is arranged as interleaved *banks* and generally is optimized
    for 32-bit access. If more than one thread in a warp references the same bank,
    a *bank conflict* occurs, and the hardware must handle memory requests consecutively
    until all requests have been serviced. Typically, to avoid bank conflicts, applications
    access shared memory with an interleaved pattern based on the thread ID, such
    as the following.
  id: totrans-1872
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p238pro01a)'
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ float shared[];
  id: totrans-1874
  prefs: []
  type: TYPE_NORMAL
- en: float data = shared[BaseIndex + threadIdx.x];
  id: totrans-1875
  prefs: []
  type: TYPE_NORMAL
- en: Having all threads in a warp read from the same 32-bit shared memory location
    also is fast. The hardware includes a broadcast mechanism to optimize for this
    case. Writes to the same bank are serialized by the hardware, reducing performance.
    Writes to the same *address* cause race conditions and should be avoided.
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
- en: For 2D access patterns (such as tiles of pixels in an image processing kernel),
    it’s good practice to pad the shared memory allocation so the kernel can reference
    adjacent rows without causing bank conflicts. SM 2.x and subsequent hardware has
    32 banks,^([4](ch08.html#ch08fn4)) so for 2D tiles where threads in the same warp
    may access the data by row, it is a good strategy to pad the tile size to a multiple
    of 33 32-bit words.
  id: totrans-1877
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch08.html#ch08fn4a). SM 1.x hardware had 16 banks (memory traffic from
    the first 16 threads and the second 16 threads of a warp was serviced separately),
    but strategies that work well on subsequent hardware also work well on SM 1.x.'
  id: totrans-1878
  prefs: []
  type: TYPE_NORMAL
- en: On SM 1.x hardware, shared memory is about 16K in size;^([5](ch08.html#ch08fn5))
    on later hardware, there is a total of 64K of L1 cache that may be configured
    as 16K or 48K of shared memory, of which the remainder is used as L1 cache.^([6](ch08.html#ch08fn6))
  id: totrans-1879
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch08.html#ch08fn5a). 256 bytes of shared memory was reserved for parameter
    passing; in SM 2.x and later, parameters are passed via constant memory.'
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch08.html#ch08fn6a). SM 3.x hardware adds the ability to split the cache
    evenly as 32K L1/32K shared.'
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the last few generations of hardware, NVIDIA has improved the hardware’s
    handling of operand sizes other than 32 bits. On SM 1.x hardware, 8- and 16-bit
    reads from the same bank caused bank conflicts, while SM 2.x and later hardware
    can broadcast reads of any size out of the same bank. Similarly, 64-bit operands
    (such as `double`) in shared memory were so much slower than 32-bit operands on
    SM 1.x that developers sometimes had to resort to storing the data as separate
    high and low halves. SM 3.x hardware adds a new feature for kernels that predominantly
    use 64-bit operands in shared memory: a mode that increases the bank size to 64
    bits.'
  id: totrans-1882
  prefs: []
  type: TYPE_NORMAL
- en: Atomics in Shared Memory
  id: totrans-1883
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: SM 1.2 added the ability to perform atomic operations in shared memory. Unlike
    global memory, which implements atomics using single instructions (either `GATOM`
    or `GRED`, depending on whether the return value is used), shared memory atomics
    are implemented with explicit lock/unlock semantics, and the compiler emits code
    that causes each thread to loop over these lock operations until the thread has
    performed its atomic operation.
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 8.1](ch08.html#ch08lis01) gives the source code to `atomic32Shared.cu`,
    a program specifically intended to be compiled to highlight the code generation
    for shared memory atomics. [Listing 8.2](ch08.html#ch08lis02) shows the resulting
    microcode generated for SM 2.0\. Note how the `LDSLK` (load shared with lock)
    instruction returns a predicate that tells whether the lock was acquired, the
    code to perform the update is predicated, and the code loops until the lock is
    acquired and the update performed.'
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
- en: The lock is performed per 32-bit word, and the index of the lock is determined
    by bits 2–9 of the shared memory address. Take care to avoid contention, or the
    loop in [Listing 8.2](ch08.html#ch08lis02) may iterate up to 32 times.
  id: totrans-1886
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8.1.* `atomic32Shared.cu`.'
  id: totrans-1887
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p08lis01a)'
  id: totrans-1888
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1889
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
- en: Return32( int *sum, int *out, const int *pIn )
  id: totrans-1891
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ int s[];
  id: totrans-1893
  prefs: []
  type: TYPE_NORMAL
- en: s[threadIdx.x] = pIn[threadIdx.x];
  id: totrans-1894
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  id: totrans-1895
  prefs: []
  type: TYPE_NORMAL
- en: (void) atomicAdd( &s[threadIdx.x], *pIn );
  id: totrans-1896
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  id: totrans-1897
  prefs: []
  type: TYPE_NORMAL
- en: out[threadIdx.x] = s[threadIdx.x];
  id: totrans-1898
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-1899
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1900
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8.2.* `atomic32Shared.cubin` (microcode compiled for SM 2.0).'
  id: totrans-1901
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p08lis02a)'
  id: totrans-1902
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1903
  prefs: []
  type: TYPE_NORMAL
- en: code for sm_20
  id: totrans-1904
  prefs: []
  type: TYPE_NORMAL
- en: 'Function : _Z8Return32PiS_PKi'
  id: totrans-1905
  prefs: []
  type: TYPE_NORMAL
- en: /*0000*/     MOV R1, c [0x1] [0x100];
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
- en: /*0008*/     S2R R0, SR_Tid_X;
  id: totrans-1907
  prefs: []
  type: TYPE_NORMAL
- en: /*0010*/     SHL R3, R0, 0x2;
  id: totrans-1908
  prefs: []
  type: TYPE_NORMAL
- en: /*0018*/     MOV R0, c [0x0] [0x28];
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
- en: /*0020*/     IADD R2, R3, c [0x0] [0x28];
  id: totrans-1910
  prefs: []
  type: TYPE_NORMAL
- en: /*0028*/     IMAD.U32.U32 RZ, R0, R1, RZ;
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
- en: /*0030*/     LD R2, [R2];
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
- en: /*0038*/     STS [R3], R2;
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
- en: /*0040*/     SSY 0x80;
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
- en: /*0048*/     BAR.RED.POPC RZ, RZ;
  id: totrans-1915
  prefs: []
  type: TYPE_NORMAL
- en: /*0050*/     LD R0, [R0];
  id: totrans-1916
  prefs: []
  type: TYPE_NORMAL
- en: /*0058*/     LDSLK P0, R2, [R3];
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
- en: /*0060*/     @P0 IADD R2, R2, R0;
  id: totrans-1918
  prefs: []
  type: TYPE_NORMAL
- en: /*0068*/     @P0 STSUL [R3], R2;
  id: totrans-1919
  prefs: []
  type: TYPE_NORMAL
- en: /*0070*/     @!P0 BRA 0x58;
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
- en: /*0078*/     NOP.S CC.T;
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
- en: /*0080*/     BAR.RED.POPC RZ, RZ;
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
- en: /*0088*/     LDS R0, [R3];
  id: totrans-1923
  prefs: []
  type: TYPE_NORMAL
- en: /*0090*/     IADD R2, R3, c [0x0] [0x24];
  id: totrans-1924
  prefs: []
  type: TYPE_NORMAL
- en: /*0098*/     ST [R2], R0;
  id: totrans-1925
  prefs: []
  type: TYPE_NORMAL
- en: /*00a0*/     EXIT;
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
- en: '...................................'
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-1928
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.6\. Barriers and Coherency
  id: totrans-1929
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The familiar `__syncthreads()` intrinsic waits until all the threads in the
    thread block have arrived before proceeding. It is needed to maintain coherency
    of shared memory within a thread block.^([7](ch08.html#ch08fn7)) Other, similar
    memory barrier instructions can be used to enforce some ordering on broader scopes
    of memory, as described in [Table 8.3](ch08.html#ch08tab03).
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch08.html#ch08fn7a). Note that threads within a warp run in lockstep, sometimes
    enabling developers to write so-called “warp synchronous” code that does not call
    `__syncthreads()`. [Section 7.3](ch07.html#ch07lev1sec3) describes thread and
    warp execution in detail, and [Part III](part03.html#part03) includes several
    examples of warp synchronous code.'
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab03.jpg)'
  id: totrans-1932
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.3* Memory Barrier Intrinsics'
  id: totrans-1933
  prefs: []
  type: TYPE_NORMAL
- en: 8.2\. Integer Support
  id: totrans-1934
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The SMs have the full complement of 32-bit integer operations.
  id: totrans-1935
  prefs: []
  type: TYPE_NORMAL
- en: • Addition with optional negation of an operand for subtraction
  id: totrans-1936
  prefs: []
  type: TYPE_NORMAL
- en: • Multiplication and multiply-add
  id: totrans-1937
  prefs: []
  type: TYPE_NORMAL
- en: • Integer division
  id: totrans-1938
  prefs: []
  type: TYPE_NORMAL
- en: • Logical operations
  id: totrans-1939
  prefs: []
  type: TYPE_NORMAL
- en: • Condition code manipulation
  id: totrans-1940
  prefs: []
  type: TYPE_NORMAL
- en: • Conversion to/from floating point
  id: totrans-1941
  prefs: []
  type: TYPE_NORMAL
- en: • Miscellaneous operations (e.g., SIMD instructions for narrow integers, population
    count, find first zero)
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
- en: CUDA exposes most of this functionality through standard C operators. Nonstandard
    operations, such as 24-bit multiplication, may be accessed using inline PTX assembly
    or intrinsic functions.
  id: totrans-1943
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1\. Multiplication
  id: totrans-1944
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Multiplication is implemented differently on Tesla- and Fermi-class hardware.
    Tesla implements a 24-bit multiplier, while Fermi implements a 32-bit multiplier.
    As a consequence, full 32-bit multiplication on SM 1.x hardware requires four
    instructions. For performance-sensitive code targeting Tesla-class hardware, it
    is a performance win to use the intrinsics for 24-bit multiply.^([8](ch08.html#ch08fn8))
    [Table 8.4](ch08.html#ch08tab04) shows the intrinsics related to multiplication.
  id: totrans-1945
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch08.html#ch08fn8a). Using `__mul24()` or `__umul24()` on SM 2.x and later
    hardware, however, is a performance *penalty*.'
  id: totrans-1946
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab04.jpg)'
  id: totrans-1947
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.4* Multiplication Intrinsics'
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2\. Miscellaneous (Bit Manipulation)
  id: totrans-1949
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The CUDA compiler implements a number of intrinsics for bit manipulation, as
    summarized in [Table 8.5](ch08.html#ch08tab05). On SM 2.x and later architectures,
    these intrinsics map to single instructions. On pre-Fermi architectures, they
    are valid but may compile into many instructions. When in doubt, disassemble and
    look at the microcode! 64-bit variants have “`ll`” (two ells for “long long”)
    appended to the intrinsic name `__clzll(), ffsll(), popcll(), brevll()`.
  id: totrans-1950
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab05.jpg)'
  id: totrans-1951
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.5* Bit Manipulation Intrinsics'
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.3\. Funnel Shift (SM 3.5)
  id: totrans-1953
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: GK110 added a 64-bit “funnel shift” instruction that concatenates two 32-bit
    values together (the least significant and most significant halves are specified
    as separate 32-bit inputs, but the hardware operates on an aligned register pair),
    shifts the resulting 64-bit value left or right, and then returns the most significant
    (for left shift) or least significant (for right shift) 32 bits.
  id: totrans-1954
  prefs: []
  type: TYPE_NORMAL
- en: Funnel shift may be accessed with the intrinsics given in [Table 8.6](ch08.html#ch08tab06).
    These intrinsics are implemented as inline device functions (using inline PTX
    assembler) in `sm_35_intrinsics.h`. By default, the least significant 5 bits of
    the shift count are masked off; the `_lc` and `_rc` intrinsics clamp the shift
    value to the range 0..32.
  id: totrans-1955
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab06.jpg)'
  id: totrans-1956
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.6* Funnel Shift Intrinsics'
  id: totrans-1957
  prefs: []
  type: TYPE_NORMAL
- en: Applications for funnel shift include the following.
  id: totrans-1958
  prefs: []
  type: TYPE_NORMAL
- en: • Multiword shift operations
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
- en: • Memory copies between misaligned buffers using aligned loads and stores
  id: totrans-1960
  prefs: []
  type: TYPE_NORMAL
- en: • Rotate
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
- en: To right-shift data sizes greater than 64 bits, use repeated `__funnelshift_r()`
    calls, operating from the least significant to the most significant word. The
    most significant word of the result is computed using `operator>>`, which shifts
    in zero or sign bits as appropriate for the integer type. To left-shift data sizes
    greater than 64 bits, use repeated `__funnelshift_l()` calls, operating from the
    most significant to the least significant word. The least significant word of
    the result is computed using `operator<<`. If the `hi` and `lo` parameters are
    the same, the funnel shift effects a rotate operation.
  id: totrans-1962
  prefs: []
  type: TYPE_NORMAL
- en: 8.3\. Floating-Point Support
  id: totrans-1963
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fast native floating-point hardware is the *raison d’être* for GPUs, and in
    many ways they are equal to or superior to CPUs in their floating-point implementation.
    Denormals are supported at full speed,^([9](ch08.html#ch08fn9)) directed rounding
    may be specified on a per-instruction basis, and the Special Function Units deliver
    high-performance approximation functions to six popular single-precision transcendentals.
    In contrast, x86 CPUs implement denormals in microcode that runs perhaps 100x
    slower than operating on normalized floating-point operands. Rounding direction
    is specified by a control word that takes dozens of clock cycles to change, and
    the only transcendental approximation functions in the SSE instruction set are
    for reciprocal and reciprocal square root, which give 12-bit approximations that
    must be refined with a Newton-Raphson iteration before being used.
  id: totrans-1964
  prefs: []
  type: TYPE_NORMAL
- en: '[9](ch08.html#ch08fn9a). With the exception that single-precision denormals
    are not supported at all on SM 1.x hardware.'
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
- en: Since GPUs’ greater core counts are offset somewhat by their lower clock frequencies,
    developers can expect at most a 10x (or thereabouts) speedup on a level playing
    field. If a paper reports a 100x or greater speedup from porting an optimized
    CPU implementation to CUDA, chances are one of the above-described “instruction
    set mismatches” played a role.
  id: totrans-1966
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.1\. Formats
  id: totrans-1967
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 8.2](ch08.html#ch08fig02) depicts the three (3) IEEE standard floating-point
    formats supported by CUDA: double precision (64-bit), single precision (32-bit),
    and half precision (16-bit). The values are divided into three fields: sign, exponent,
    and mantissa. For `double`, `single`, and `half`, the exponent fields are 11,
    8, and 5 bits in size, respectively; the corresponding mantissa fields are 52,
    23, and 10 bits.'
  id: totrans-1968
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08fig02.jpg)'
  id: totrans-1969
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8.2* Floating-point formats.'
  id: totrans-1970
  prefs: []
  type: TYPE_NORMAL
- en: The exponent field changes the interpretation of the floating-point value. The
    most common (“normal”) representation encodes an implicit 1 bit into the mantissa
    and multiplies that value by 2^(e-bias), where *bias* is the value added to the
    actual exponent before encoding into the floating-point representation. The bias
    for single precision, for example, is 127.
  id: totrans-1971
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8.7](ch08.html#ch08tab07) summarizes how floating-point values are encoded.
    For most exponent values (so-called “normal” floating-point values), the mantissa
    is assumed to have an implicit 1, and it is multiplied by the biased value of
    the exponent. The maximum exponent value is reserved for infinity and Not-A-Number
    values. Dividing by zero (or overflowing a division) yields infinity; performing
    an invalid operation (such as taking the square root or logarithm of a negative
    number) yields a NaN. The minimum exponent value is reserved for values too small
    to represent with the implicit leading 1\. As the so-called *denormals*^([10](ch08.html#ch08fn10))
    get closer to zero, they lose bits of effective precision, a phenomenon known
    as *gradual underflow*. [Table 8.8](ch08.html#ch08tab08) gives the encodings and
    values of certain extreme values for the three formats.'
  id: totrans-1972
  prefs: []
  type: TYPE_NORMAL
- en: '[10](ch08.html#ch08fn10a). Sometimes called *subnormals*.'
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab07.jpg)'
  id: totrans-1974
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.7* Floating-Point Representations'
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab08.jpg)'
  id: totrans-1976
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.8* Floating-Point Extreme Values'
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
- en: Rounding
  id: totrans-1978
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The IEEE standard provides for four (4) round modes.
  id: totrans-1979
  prefs: []
  type: TYPE_NORMAL
- en: • Round-to-nearest-even (also called “round-to-nearest”)
  id: totrans-1980
  prefs: []
  type: TYPE_NORMAL
- en: • Round toward zero (also called “truncate” or “chop”)
  id: totrans-1981
  prefs: []
  type: TYPE_NORMAL
- en: • Round down (or “round toward negative infinity”)
  id: totrans-1982
  prefs: []
  type: TYPE_NORMAL
- en: • Round up (or “round toward positive infinity”)
  id: totrans-1983
  prefs: []
  type: TYPE_NORMAL
- en: Round-to-nearest, where intermediate values are rounded to the nearest representable
    floating-point value after each operation, is by far the most commonly used round
    mode. Round up and round down (the “directed rounding modes”) are used for *interval
    arithmetic*, where a pair of floating-point values are used to bracket the intermediate
    result of a computation. To correctly bracket a result, the lower and upper values
    of the interval must be rounded toward negative infinity (“down”) and toward positive
    infinity (“up”), respectively.
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
- en: The C language does not provide any way to specify round modes on a per-instruction
    basis, and CUDA hardware does not provide a control word to implicitly specify
    rounding modes. Consequently, CUDA provides a set of intrinsics to specify the
    round mode of an operation, as summarized in [Table 8.9](ch08.html#ch08tab09).
  id: totrans-1985
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab09.jpg)'
  id: totrans-1986
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.9* Intrinsics for Rounding'
  id: totrans-1987
  prefs: []
  type: TYPE_NORMAL
- en: Conversion
  id: totrans-1988
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In general, developers can convert between different floating-point representations
    and/or integers using standard C constructs: implicit conversion or explicit typecasts.
    If necessary, however, developers can use the intrinsics listed in [Table 8.10](ch08.html#ch08tab10)
    to perform conversions that are not in the C language specification, such as those
    with directed rounding.'
  id: totrans-1989
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab10.jpg)'
  id: totrans-1990
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.10* Intrinsics for Conversion'
  id: totrans-1991
  prefs: []
  type: TYPE_NORMAL
- en: Because `half` is not standardized in the C programming language, CUDA uses
    `unsigned short` in the interfaces for `__half2float()` and `__float2half()`.
    `__float2half()` only supports the round-to-nearest rounding mode.
  id: totrans-1992
  prefs: []
  type: TYPE_NORMAL
- en: float __half2float( unsigned short );
  id: totrans-1993
  prefs: []
  type: TYPE_NORMAL
- en: unsigned short __float2half( float );
  id: totrans-1994
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.2\. Single Precision (32-Bit)
  id: totrans-1995
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Single-precision floating-point support is the workhorse of GPU computation.
    GPUs have been optimized to natively deliver high performance on this data type,^([11](ch08.html#ch08fn11))
    not only for core standard IEEE operations such as addition and multiplication,
    but also for nonstandard operations such as approximations to transcendentals
    such as `sin()` and `log()`. The 32-bit values are held in the same register file
    as integers, so coercion between single-precision floating-point values and 32-bit
    integers (with `__float_as_int()` and `__int_as_float()`) is free.
  id: totrans-1996
  prefs: []
  type: TYPE_NORMAL
- en: '[11](ch08.html#ch08fn11a). In fact, GPUs had full 32-bit floating-point support
    before they had full 32-bit integer support. As a result, some early GPU computing
    literature explained how to implement integer math with floating-point hardware!'
  id: totrans-1997
  prefs: []
  type: TYPE_NORMAL
- en: Addition, Multiplication, and Multiply-Add
  id: totrans-1998
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The compiler automatically translates +, –, and * operators on floating-point
    values into addition, multiplication, and multiply-add instructions. The `__fadd_rn()`
    and `__fmul_rn()` intrinsics may be used to suppress fusion of addition and multiplication
    operations into multiply-add instructions.
  id: totrans-1999
  prefs: []
  type: TYPE_NORMAL
- en: Reciprocal and Division
  id: totrans-2000
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For devices of compute capability 2.x and higher, the division operator is IEEE-compliant
    when the code is compiled with `--prec-div=true`. For devices of compute capability
    1.x or for devices of compute capability 2.x when the code is compiled with `--prec-div=false`,
    the division operator and `__fdividef(x,y)` have the same accuracy, but for 2^(126)<y<2^(128),
    `__fdividef(x,y)` delivers a result of zero, whereas the division operator delivers
    the correct result. Also, for 2^(126)<y<2^(128), if x is infinity, `__fdividef(x,y)`
    returns NaN, while the division operator returns infinity.
  id: totrans-2001
  prefs: []
  type: TYPE_NORMAL
- en: Transcendentals (SFU)
  id: totrans-2002
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Special Function Units (SFUs) in the SMs implement very fast versions of
    six common transcendental functions.
  id: totrans-2003
  prefs: []
  type: TYPE_NORMAL
- en: • Sine and cosine
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
- en: • Logarithm and exponential
  id: totrans-2005
  prefs: []
  type: TYPE_NORMAL
- en: • Reciprocal and reciprocal square root
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8.11](ch08.html#ch08tab11), excerpted from the paper on the Tesla architecture^([12](ch08.html#ch08fn12))
    summarizes the supported operations and corresponding precision. The SFUs do not
    implement full precision, but they are reasonably good approximations of these
    functions and they are *fast*. For CUDA ports that are significantly faster than
    an optimized CPU equivalent (say, 25x or more), the code most likely relies on
    the SFUs.'
  id: totrans-2007
  prefs: []
  type: TYPE_NORMAL
- en: '[12](ch08.html#ch08fn12a). Lindholm, Erik, John Nickolls, Stuart Oberman, and
    John Montrym. NVIDIA Tesla: A unified graphics and computing architecture. *IEEE
    Micro*, March–April 2008, p. 47.'
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab11.jpg)'
  id: totrans-2009
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.11* SFU Accuracy'
  id: totrans-2010
  prefs: []
  type: TYPE_NORMAL
- en: The SFUs are accessed with the intrinsics given in [Table 8.12](ch08.html#ch08tab12).
    Specifying the `--fast-math` compiler option will cause the compiler to substitute
    conventional C runtime calls with the corresponding SFU intrinsics listed above.
  id: totrans-2011
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab12.jpg)'
  id: totrans-2012
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.12* SFU Intrinsics'
  id: totrans-2013
  prefs: []
  type: TYPE_NORMAL
- en: Miscellaneous
  id: totrans-2014
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`__saturate(x)` returns 0 if `x<0`, 1 if `x>1`, and `x` otherwise.'
  id: totrans-2015
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.3\. Double Precision (64-Bit)
  id: totrans-2016
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Double-precision floating-point support was added to CUDA with SM 1.3 (first
    implemented in the GeForce GTX 280), and much improved double-precision support
    (both functionality and performance) became available with SM 2.0\. CUDA’s hardware
    support for double precision features full-speed denormals and, starting in SM
    2.x, a native fused multiply-add instruction (FMAD), compliant with IEEE 754 c.
    2008, that performs only one rounding step. Besides being an intrinsically useful
    operation, FMAD enables full accuracy on certain functions that are converged
    with the Newton-Raphson iteration.
  id: totrans-2017
  prefs: []
  type: TYPE_NORMAL
- en: As with single-precision operations, the compiler automatically translates standard
    C operators into multiplication, addition, and multiply-add instructions. The
    `__dadd_rn()` and `__dmul_rn()` intrinsics may be used to suppress fusion of addition
    and multiplication operations into multiply-add instructions.
  id: totrans-2018
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.4\. Half Precision (16-Bit)
  id: totrans-2019
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With 5 bits of exponent and 10 bits of significand, `half` values have enough
    precision for HDR (high dynamic range) images and can be used to hold other types
    of values that do not require `float` precision, such as angles. Half precision
    values are intended for storage, not computation, so the hardware only provides
    instructions to convert to/from 32-bit.^([13](ch08.html#ch08fn13)) These instructions
    are exposed as the `__halftofloat()` and `__floattohalf()` intrinsics.
  id: totrans-2020
  prefs: []
  type: TYPE_NORMAL
- en: '[13](ch08.html#ch08fn13a). `half` floating-point values are supported as a
    texture format, in which case the TEX intrinsics return `float` and the conversion
    is automatically performed by the texture hardware.'
  id: totrans-2021
  prefs: []
  type: TYPE_NORMAL
- en: float __halftofloat( unsigned short );
  id: totrans-2022
  prefs: []
  type: TYPE_NORMAL
- en: unsigned short __floattohalf( float );
  id: totrans-2023
  prefs: []
  type: TYPE_NORMAL
- en: These intrinsics use `unsigned short` because the C language has not standardized
    the `half` floating-point type.
  id: totrans-2024
  prefs: []
  type: TYPE_NORMAL
- en: '8.3.5\. Case Study: `float`→`half` Conversion'
  id: totrans-2025
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Studying the `float`→`half` conversion operation is a useful way to learn the
    details of floating-point encodings and rounding. Because it’s a simple unary
    operation, we can focus on the encoding and rounding without getting distracted
    by the details of floating-point arithmetic and the precision of intermediate
    representations.
  id: totrans-2026
  prefs: []
  type: TYPE_NORMAL
- en: When converting from `float` to `half`, the correct output for any `float` too
    large to represent is `half` infinity. Any float too small to represent as a `half`
    (even a denormal `half`) must be clamped to `0.0`. The maximum `float` that rounds
    to `half` 0.0 is `0x32FFFFFF`, or 2.98^(-8), while the smallest `float` that rounds
    to `half` infinity is 65520.0\. `float` values inside this range can be converted
    to `half` by propagating the sign bit, rebiasing the exponent (since float has
    an 8-bit exponent biased by 127 and half has a 5-bit exponent biased by 15), and
    rounding the `float` mantissa to the nearest `half` mantissa value. Rounding is
    straightforward in all cases except when the input value falls exactly between
    the two possible output values. When this is the case, the IEEE standard specifies
    rounding to the “nearest even” value. In decimal arithmetic, this would mean rounding
    1.5 to 2.0, but also rounding 2.5 to 2.0 and (for example) rounding 0.5 to 0.0.
  id: totrans-2027
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 8.3](ch08.html#ch08lis03) shows a C routine that exactly replicates
    the `float`-to-`half` conversion operation, as implemented by CUDA hardware. The
    variables `exp` and `mag` contain the input exponent and “magnitude,” the mantissa
    and exponent together with the sign bit masked off. Many operations, such as comparisons
    and rounding operations, can be performed on the magnitude without separating
    the exponent and mantissa.'
  id: totrans-2028
  prefs: []
  type: TYPE_NORMAL
- en: 'The macro `LG_MAKE_MASK`, used in [Listing 8.3](ch08.html#ch08lis03), creates
    a mask with a given bit count: `#define LG_MAKE_MASK(bits) ((1<<bits)-1)`. A `volatile
    union` is used to treat the same 32-bit value as `float` and `unsigned int`; idioms
    such as `*((float *) (&u))` are not portable. The routine first propagates the
    input sign bit and masks it off the input.'
  id: totrans-2029
  prefs: []
  type: TYPE_NORMAL
- en: After extracting the magnitude and exponent, the function deals with the special
    case when the input `float` is INF or NaN, and does an early exit. Note that INF
    is signed, but NaN has a canonical unsigned value. Lines 50–80 clamp the input
    `float` value to the minimum or maximum values that correspond to representable
    `half` values and recompute the magnitude for clamped values. Don’t be fooled
    by the elaborate code constructing `f32MinRInfin` and `f32MaxRf16_zero`; those
    are constants with the values `0x477ff000` and `0x32ffffff`, respectively.
  id: totrans-2030
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of the routine deals with the cases of output normal and denormal
    (input denormals are clamped in the preceding code, so `mag` corresponds to a
    normal `float`). As with the clamping code, `f32Minf16Normal` is a constant, and
    its value is `0x38ffffff`.
  id: totrans-2031
  prefs: []
  type: TYPE_NORMAL
- en: To construct a normal, the new exponent must be computed (lines 92 and 93) and
    the correctly rounded 10 bits of mantissa shifted into the output. To construct
    a denormal, the implicit 1 must be OR’d into the output mantissa and the resulting
    mantissa shifted by the amount corresponding to the input exponent. For both normals
    and denormals, the rounding of the output mantissa is accomplished in two steps.
    The rounding is accomplished by adding a mask of 1’s that ends just short of the
    output’s LSB, as seen in [Figure 8.3](ch08.html#ch08fig03).
  id: totrans-2032
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08fig03.jpg)'
  id: totrans-2033
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8.3* Rounding mask (`half`).'
  id: totrans-2034
  prefs: []
  type: TYPE_NORMAL
- en: This operation increments the output mantissa if bit 12 of the input is set;
    if the input mantissa is all 1’s, the overflow causes the output exponent to correctly
    increment. If we added one more 1 to the MSB of this adjustment, we’d have elementary
    school–style rounding where the tiebreak goes to the larger number. Instead, to
    implement round-to-nearest even, we conditionally increment the output mantissa
    if the LSB of the 10-bit output is set ([Figure 8.4](ch08.html#ch08fig04)). Note
    that these steps can be performed in either order or can be reformulated in many
    different ways.
  id: totrans-2035
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08fig04.jpg)'
  id: totrans-2036
  prefs: []
  type: TYPE_IMG
- en: '*Figure 8.4* Round-to-nearest-even (`half`).'
  id: totrans-2037
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 8.3.* `ConvertToHalf()`.'
  id: totrans-2038
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p08lis03a)'
  id: totrans-2039
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2040
  prefs: []
  type: TYPE_NORMAL
- en: /*
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
- en: '* exponent shift and mantissa bit count are the same.'
  id: totrans-2042
  prefs: []
  type: TYPE_NORMAL
- en: '*    When we are shifting, we use [f16|f32]ExpShift'
  id: totrans-2043
  prefs: []
  type: TYPE_NORMAL
- en: '*    When referencing the number of bits in the mantissa,'
  id: totrans-2044
  prefs: []
  type: TYPE_NORMAL
- en: '*        we use [f16|f32]MantissaBits'
  id: totrans-2045
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  id: totrans-2046
  prefs: []
  type: TYPE_NORMAL
- en: const int f16ExpShift = 10;
  id: totrans-2047
  prefs: []
  type: TYPE_NORMAL
- en: const int f16MantissaBits = 10;
  id: totrans-2048
  prefs: []
  type: TYPE_NORMAL
- en: const int f16ExpBias = 15;
  id: totrans-2049
  prefs: []
  type: TYPE_NORMAL
- en: const int f16MinExp = -14;
  id: totrans-2050
  prefs: []
  type: TYPE_NORMAL
- en: const int f16MaxExp = 15;
  id: totrans-2051
  prefs: []
  type: TYPE_NORMAL
- en: const int f16SignMask = 0x8000;
  id: totrans-2052
  prefs: []
  type: TYPE_NORMAL
- en: const int f32ExpShift = 23;
  id: totrans-2053
  prefs: []
  type: TYPE_NORMAL
- en: const int f32MantissaBits = 23;
  id: totrans-2054
  prefs: []
  type: TYPE_NORMAL
- en: const int f32ExpBias = 127;
  id: totrans-2055
  prefs: []
  type: TYPE_NORMAL
- en: const int f32SignMask = 0x80000000;
  id: totrans-2056
  prefs: []
  type: TYPE_NORMAL
- en: unsigned short
  id: totrans-2057
  prefs: []
  type: TYPE_NORMAL
- en: ConvertFloatToHalf( float f )
  id: totrans-2058
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2059
  prefs: []
  type: TYPE_NORMAL
- en: /*
  id: totrans-2060
  prefs: []
  type: TYPE_NORMAL
- en: '* Use a volatile union to portably coerce'
  id: totrans-2061
  prefs: []
  type: TYPE_NORMAL
- en: '* 32-bit float into 32-bit integer'
  id: totrans-2062
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  id: totrans-2063
  prefs: []
  type: TYPE_NORMAL
- en: volatile union {
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
- en: float f;
  id: totrans-2065
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int u;
  id: totrans-2066
  prefs: []
  type: TYPE_NORMAL
- en: '} uf;'
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
- en: uf.f = f;
  id: totrans-2068
  prefs: []
  type: TYPE_NORMAL
- en: '// return value: start by propagating the sign bit.'
  id: totrans-2069
  prefs: []
  type: TYPE_NORMAL
- en: unsigned short w = (uf.u >> 16) & f16SignMask;
  id: totrans-2070
  prefs: []
  type: TYPE_NORMAL
- en: // Extract input magnitude and exponent
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int mag = uf.u & ~f32SignMask;
  id: totrans-2072
  prefs: []
  type: TYPE_NORMAL
- en: int exp = (int) (mag >> f32ExpShift) - f32ExpBias;
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
- en: // Handle float32 Inf or NaN
  id: totrans-2074
  prefs: []
  type: TYPE_NORMAL
- en: if ( exp == f32ExpBias+1 ) {    // INF or NaN
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
- en: if ( mag & LG_MAKE_MASK(f32MantissaBits) )
  id: totrans-2076
  prefs: []
  type: TYPE_NORMAL
- en: return 0x7fff; // NaN
  id: totrans-2077
  prefs: []
  type: TYPE_NORMAL
- en: // INF - propagate sign
  id: totrans-2078
  prefs: []
  type: TYPE_NORMAL
- en: return w|0x7c00;
  id: totrans-2079
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2080
  prefs: []
  type: TYPE_NORMAL
- en: /*
  id: totrans-2081
  prefs: []
  type: TYPE_NORMAL
- en: '* clamp float32 values that are not representable by float16'
  id: totrans-2082
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  id: totrans-2083
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2084
  prefs: []
  type: TYPE_NORMAL
- en: // min float32 magnitude that rounds to float16 infinity
  id: totrans-2085
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int f32MinRInfin = (f16MaxExp+f32ExpBias) <<
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
- en: f32ExpShift;
  id: totrans-2087
  prefs: []
  type: TYPE_NORMAL
- en: f32MinRInfin |= LG_MAKE_MASK( f16MantissaBits+1 ) <<
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
- en: (f32MantissaBits-f16MantissaBits-1);
  id: totrans-2089
  prefs: []
  type: TYPE_NORMAL
- en: if (mag > f32MinRInfin)
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
- en: mag = f32MinRInfin;
  id: totrans-2091
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2092
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2093
  prefs: []
  type: TYPE_NORMAL
- en: // max float32 magnitude that rounds to float16 0.0
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int f32MaxRf16_zero = f16MinExp+f32ExpBias
  id: totrans-2095
  prefs: []
  type: TYPE_NORMAL
- en: (f32MantissaBits-f16MantissaBits-1);
  id: totrans-2096
  prefs: []
  type: TYPE_NORMAL
- en: f32MaxRf16_zero <<= f32ExpShift;
  id: totrans-2097
  prefs: []
  type: TYPE_NORMAL
- en: f32MaxRf16_zero |= LG_MAKE_MASK( f32MantissaBits );
  id: totrans-2098
  prefs: []
  type: TYPE_NORMAL
- en: if (mag < f32MaxRf16_zero)
  id: totrans-2099
  prefs: []
  type: TYPE_NORMAL
- en: mag = f32MaxRf16_zero;
  id: totrans-2100
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2101
  prefs: []
  type: TYPE_NORMAL
- en: /*
  id: totrans-2102
  prefs: []
  type: TYPE_NORMAL
- en: '* compute exp again, in case mag was clamped above'
  id: totrans-2103
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  id: totrans-2104
  prefs: []
  type: TYPE_NORMAL
- en: exp = (mag >> f32ExpShift) - f32ExpBias;
  id: totrans-2105
  prefs: []
  type: TYPE_NORMAL
- en: // min float32 magnitude that converts to float16 normal
  id: totrans-2106
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int f32Minf16Normal = ((f16MinExp+f32ExpBias)<<
  id: totrans-2107
  prefs: []
  type: TYPE_NORMAL
- en: f32ExpShift);
  id: totrans-2108
  prefs: []
  type: TYPE_NORMAL
- en: f32Minf16Normal |= LG_MAKE_MASK( f32MantissaBits );
  id: totrans-2109
  prefs: []
  type: TYPE_NORMAL
- en: if ( mag >= f32Minf16Normal ) {
  id: totrans-2110
  prefs: []
  type: TYPE_NORMAL
- en: //
  id: totrans-2111
  prefs: []
  type: TYPE_NORMAL
- en: '// Case 1: float16 normal'
  id: totrans-2112
  prefs: []
  type: TYPE_NORMAL
- en: //
  id: totrans-2113
  prefs: []
  type: TYPE_NORMAL
- en: // Modify exponent to be biased for float16, not float32
  id: totrans-2114
  prefs: []
  type: TYPE_NORMAL
- en: mag += (unsigned int) ((f16ExpBias-f32ExpBias)<<
  id: totrans-2115
  prefs: []
  type: TYPE_NORMAL
- en: f32ExpShift);
  id: totrans-2116
  prefs: []
  type: TYPE_NORMAL
- en: int RelativeShift = f32ExpShift-f16ExpShift;
  id: totrans-2117
  prefs: []
  type: TYPE_NORMAL
- en: // add rounding bias
  id: totrans-2118
  prefs: []
  type: TYPE_NORMAL
- en: mag += LG_MAKE_MASK(RelativeShift-1);
  id: totrans-2119
  prefs: []
  type: TYPE_NORMAL
- en: // round-to-nearest even
  id: totrans-2120
  prefs: []
  type: TYPE_NORMAL
- en: mag += (mag >> RelativeShift) & 1;
  id: totrans-2121
  prefs: []
  type: TYPE_NORMAL
- en: w |= mag >> RelativeShift;
  id: totrans-2122
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2123
  prefs: []
  type: TYPE_NORMAL
- en: else {
  id: totrans-2124
  prefs: []
  type: TYPE_NORMAL
- en: /*
  id: totrans-2125
  prefs: []
  type: TYPE_NORMAL
- en: '* Case 2: float16 denormal'
  id: totrans-2126
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  id: totrans-2127
  prefs: []
  type: TYPE_NORMAL
- en: // mask off exponent bits - now fraction only
  id: totrans-2128
  prefs: []
  type: TYPE_NORMAL
- en: mag &= LG_MAKE_MASK(f32MantissaBits);
  id: totrans-2129
  prefs: []
  type: TYPE_NORMAL
- en: // make implicit 1 explicit
  id: totrans-2130
  prefs: []
  type: TYPE_NORMAL
- en: mag |= (1<<f32ExpShift);
  id: totrans-2131
  prefs: []
  type: TYPE_NORMAL
- en: int RelativeShift = f32ExpShift-f16ExpShift+f16MinExp-exp;
  id: totrans-2132
  prefs: []
  type: TYPE_NORMAL
- en: // add rounding bias
  id: totrans-2133
  prefs: []
  type: TYPE_NORMAL
- en: mag += LG_MAKE_MASK(RelativeShift-1);
  id: totrans-2134
  prefs: []
  type: TYPE_NORMAL
- en: // round-to-nearest even
  id: totrans-2135
  prefs: []
  type: TYPE_NORMAL
- en: mag += (mag >> RelativeShift) & 1;
  id: totrans-2136
  prefs: []
  type: TYPE_NORMAL
- en: w |= mag >> RelativeShift;
  id: totrans-2137
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2138
  prefs: []
  type: TYPE_NORMAL
- en: return w;
  id: totrans-2139
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, developers should convert `float` to `half` by using the `__floattohalf()`
    intrinsic, which the compiler translates to a single `F2F` machine instruction.
    This sample routine is provided purely to aid in understanding floating-point
    layout and rounding; also, examining all the special-case code for INF/NAN and
    denormal values helps to illustrate why these features of the IEEE spec have been
    controversial since its inception: They make hardware slower, more costly, or
    both due to increased silicon area and engineering effort for validation.'
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
- en: In the code accompanying this book, the `ConvertFloatToHalf()` routine in [Listing
    8.3](ch08.html#ch08lis03) is incorporated into a program called `float_to_float16.cu`
    that tests its output for every 32-bit floating-point value.
  id: totrans-2143
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.6\. Math Library
  id: totrans-2144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'CUDA includes a built-in math library modeled on the C runtime library, with
    a few small differences: CUDA hardware does not include a rounding mode register
    (instead, the round mode is encoded on a per-instruction basis),^([14](ch08.html#ch08fn14))
    so functions such as `rint()` that reference the current rounding mode always
    round-to-nearest. Additionally, the hardware does not raise floating-point exceptions;
    results of aberrant operations, such as taking the square root of a negative number,
    are encoded as NaNs.'
  id: totrans-2145
  prefs: []
  type: TYPE_NORMAL
- en: '[14](ch08.html#ch08fn14a). Encoding a round mode per instruction and keeping
    it in a control register are not irreconcilable. The Alpha processor had a 2-bit
    encoding to specify the round mode per instruction, one setting of which was to
    use the rounding mode specified in a control register! CUDA hardware just uses
    a 2-bit encoding for the four round modes specified in the IEEE specification.'
  id: totrans-2146
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8.13](ch08.html#ch08tab13) lists the math library functions and the
    maximum error in ulps for each function. Most functions that operate on `float`
    have an “f” appended to the function name—for example, the functions that compute
    the sine function are as follows.'
  id: totrans-2147
  prefs: []
  type: TYPE_NORMAL
- en: double sin( double angle );
  id: totrans-2148
  prefs: []
  type: TYPE_NORMAL
- en: float sinf( float angle );
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab13.jpg)![Image](graphics/08tab13a.jpg)![Image](graphics/08tab13b.jpg)![Image](graphics/08tab13c.jpg)![Image](graphics/08tab13d.jpg)![Image](graphics/08tab13e.jpg)![Image](graphics/08tab13f.jpg)'
  id: totrans-2150
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.13* Math Library'
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
- en: These are denoted in [Table 8.13](ch08.html#ch08tab13) as, for example, `sin[f]`.
  id: totrans-2152
  prefs: []
  type: TYPE_NORMAL
- en: Conversion to Integer
  id: totrans-2153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: According to the C runtime library definition, the `nearbyint()` and `rint()`
    functions round a floating-point value to the nearest integer using the “current
    rounding direction,” which in CUDA is always round-to-nearest-even. In the C runtime,
    `nearbyint()` and `rint()` differ only in their handling of the INEXACT exception.
    But since CUDA does not raise floating-point exceptions, the functions behave
    identically.
  id: totrans-2154
  prefs: []
  type: TYPE_NORMAL
- en: '`round()` implements elementary school–style rounding: For floating-point values
    halfway between integers, the input is always rounded away from zero. NVIDIA recommends
    against using this function because it expands to eight (8) instructions as opposed
    to one for `rint()` and its variants. `trunc()` truncates or “chops” the floating-point
    value, rounding toward zero. It compiles to a single instruction.'
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
- en: Fractions and Exponents
  id: totrans-2156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: float frexpf(float x, int *eptr);
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
- en: '`frexpf()` breaks the input into a floating-point significand in the range
    [0.5, 1.0) and an integral exponent for 2, such that'
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
- en: '*x = Significand* · 2^(*Exponent*)'
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
- en: float logbf( float x );
  id: totrans-2160
  prefs: []
  type: TYPE_NORMAL
- en: '`logbf()` extracts the exponent from x and returns it as a floating-point value.
    It is equivalent to `floorf(log2f(x))`, except it is faster. If `x` is a denormal,
    `logbf()` returns the exponent that x would have if it were normalized.'
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
- en: float ldexpf( float x, int exp );
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
- en: float scalbnf( float x, int n );
  id: totrans-2163
  prefs: []
  type: TYPE_NORMAL
- en: float scanblnf( float x, long n );
  id: totrans-2164
  prefs: []
  type: TYPE_NORMAL
- en: '`ldexpf()`, `scalbnf()`, and `scalblnf()` all compute x2^n by direct manipulation
    of floating-point exponents.'
  id: totrans-2165
  prefs: []
  type: TYPE_NORMAL
- en: Floating-Point Remainder
  id: totrans-2166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '`modff()` breaks the input into fractional and integer parts.'
  id: totrans-2167
  prefs: []
  type: TYPE_NORMAL
- en: float modff( float x, float *intpart );
  id: totrans-2168
  prefs: []
  type: TYPE_NORMAL
- en: The return value is the fractional part of x, with the same sign.
  id: totrans-2169
  prefs: []
  type: TYPE_NORMAL
- en: '`remainderf(x,y)` computes the floating-point remainder of dividing x by y.
    The return value is `x-n*y`, where `n` is x/y, rounded to the nearest integer.
    If |x –ny| = 0.5, `n` is chosen to be even.'
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
- en: float remquof(float x, float y, int *quo);
  id: totrans-2171
  prefs: []
  type: TYPE_NORMAL
- en: computes the remainder and passes back the lower bits of the integral quotient
    x/y, with the same sign as x/y.
  id: totrans-2172
  prefs: []
  type: TYPE_NORMAL
- en: Bessel Functions
  id: totrans-2173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Bessel functions of order *n* relate to the differential equation
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/265equ01.jpg)'
  id: totrans-2175
  prefs: []
  type: TYPE_IMG
- en: '*n* can be a real number, but for purposes of the C runtime, it is a nonnegative
    integer.'
  id: totrans-2176
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this second-order ordinary differential equation combines Bessel
    functions of the first kind and of the second kind.
  id: totrans-2177
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/265equ02.jpg)'
  id: totrans-2178
  prefs: []
  type: TYPE_IMG
- en: The math runtime functions `jn[f]()` and `yn[f]()` compute J[n](x) and Y[n](x),
    respectively. `j0f()`, `j1f()`, `y0f()`, and `y1f()` compute these functions for
    the special cases of n=0 and n=1.
  id: totrans-2179
  prefs: []
  type: TYPE_NORMAL
- en: Gamma Function
  id: totrans-2180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The gamma function Γ is an extension of the factorial function, with its argument
    shifted down by 1, to real numbers. It has a variety of definitions, one of which
    is as follows.
  id: totrans-2181
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/265equ03.jpg)'
  id: totrans-2182
  prefs: []
  type: TYPE_IMG
- en: The function grows so quickly that the return value loses precision for relatively
    small input values, so the library provides the `lgamma()` function, which returns
    the natural logarithm of the gamma function, in addition to the `tgamma()` (“true
    gamma”) function.
  id: totrans-2183
  prefs: []
  type: TYPE_NORMAL
- en: 8.3.7\. Additional Reading
  id: totrans-2184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Goldberg’s survey (with the captivating title “What Every Computer Scientist
    Should Know About Floating Point Arithmetic”) is a good introduction to the topic.
  id: totrans-2185
  prefs: []
  type: TYPE_NORMAL
- en: '[http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html](http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html)'
  id: totrans-2186
  prefs: []
  type: TYPE_NORMAL
- en: 'Nathan Whitehead and Alex Fit-Florea of NVIDIA have coauthored a white paper
    entitled “Precision & Performance: Floating Point and IEEE 754 Compliance for
    NVIDIA GPUs.”'
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
- en: '[http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf](http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf)'
  id: totrans-2188
  prefs: []
  type: TYPE_NORMAL
- en: Increasing Effective Precision
  id: totrans-2189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Dekker and Kahan developed methods to almost double the effective precision
    of floating-point hardware using pairs of numbers in exchange for a slight reduction
    in exponent range (due to intermediate underflow and overflow at the far ends
    of the range). Some papers on this topic include the following.
  id: totrans-2190
  prefs: []
  type: TYPE_NORMAL
- en: Dekker, T.J. Point technique for extending the available precision. *Numer.
    Math.* 18, 1971, pp. 224–242.
  id: totrans-2191
  prefs: []
  type: TYPE_NORMAL
- en: Linnainmaa, S. Software for doubled-precision floating point computations. *ACM
    TOMS* 7, pp. 172–283 (1981).
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
- en: Shewchuk, J.R. Adaptive precision floating-point arithmetic and fast robust
    geometric predicates. *Discrete & Computational Geometry* 18, 1997, pp. 305–363.
  id: totrans-2193
  prefs: []
  type: TYPE_NORMAL
- en: Some GPU-specific work on this topic has been done by Andrew Thall, Da Graça,
    and Defour.
  id: totrans-2194
  prefs: []
  type: TYPE_NORMAL
- en: Guillaume, Da Graça, and David Defour. Implementation of float-float operators
    on graphics hardware, *7th Conference on Real Numbers and Computers*, RNC7 (2006).
  id: totrans-2195
  prefs: []
  type: TYPE_NORMAL
- en: '[http://hal.archives-ouvertes.fr/docs/00/06/33/56/PDF/float-float.pdf](http://hal.archives-ouvertes.fr/docs/00/06/33/56/PDF/float-float.pdf)'
  id: totrans-2196
  prefs: []
  type: TYPE_NORMAL
- en: Thall, Andrew. Extended-precision floating-point numbers for GPU computation.
    2007.
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
- en: '[http://andrewthall.org/papers/df64_qf128.pdf](http://andrewthall.org/papers/df64_qf128.pdf)'
  id: totrans-2198
  prefs: []
  type: TYPE_NORMAL
- en: 8.4\. Conditional Code
  id: totrans-2199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The hardware implements “condition code” or CC registers that contain the usual
    4-bit state vector (sign, carry, zero, overflow) used for integer comparison.
    These CC registers can be set using comparison instructions such as `ISET`, and
    they can direct the flow of execution via *predication* or *divergence*. Predication
    allows (or suppresses) the execution of instructions on a per-thread basis within
    a warp, while divergence is the conditional execution of longer instruction sequences.
    Because the processors within an SM execute instructions in SIMD fashion at warp
    granularity (32 threads at a time), divergence can result in fewer instructions
    executed, provided all threads within a warp take the same code path.
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.1\. Predication
  id: totrans-2201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the additional overhead of managing divergence and convergence, the compiler
    uses predication for short instruction sequences. The effect of most instructions
    can be predicated on a condition; if the condition is not TRUE, the instruction
    is suppressed. This suppression occurs early enough that predicated execution
    of instructions such as load/store and `TEX` inhibits the memory traffic that
    the instruction would otherwise generate. Note that predication has no effect
    on the eligibility of memory traffic for global load/store coalescing. The addresses
    specified to all load/store instructions in a warp must reference consecutive
    memory locations, even if they are predicated.
  id: totrans-2202
  prefs: []
  type: TYPE_NORMAL
- en: Predication is used when the number of instructions that vary depending on a
    condition is small; the compiler uses heuristics that favor predication up to
    about 7 instructions. Besides avoiding the overhead of managing the branch synchronization
    stack described below, predication also gives the compiler more optimization opportunities
    (such as instruction scheduling) when emitting microcode. The ternary operator
    in C (`? :`) is considered a compiler hint to favor predication.
  id: totrans-2203
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 8.2](ch08.html#ch08lis02) gives an excellent example of predication,
    as expressed in microcode. When performing an atomic operation on a shared memory
    location, the compiler emits code that loops over the shared memory location until
    it has successfully performed the atomic operation. The `LDSLK` (load shared and
    lock) instruction returns a condition code that tells whether the lock was acquired.
    The instructions to perform the operation then are predicated on that condition
    code.'
  id: totrans-2204
  prefs: []
  type: TYPE_NORMAL
- en: /*0058*/ LDSLK P0, R2, [R3];
  id: totrans-2205
  prefs: []
  type: TYPE_NORMAL
- en: /*0060*/ @P0 IADD R2, R2, R0;
  id: totrans-2206
  prefs: []
  type: TYPE_NORMAL
- en: /*0068*/ @P0 STSUL [R3], R2;
  id: totrans-2207
  prefs: []
  type: TYPE_NORMAL
- en: /*0070*/ @!P0 BRA 0x58;
  id: totrans-2208
  prefs: []
  type: TYPE_NORMAL
- en: This code fragment also highlights how predication and branching sometimes work
    together. The last instruction, a conditional branch to attempt to reacquire the
    lock if necessary, also is predicated.
  id: totrans-2209
  prefs: []
  type: TYPE_NORMAL
- en: 8.4.2\. Divergence and Convergence
  id: totrans-2210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Predication works well for small fragments of conditional code, especially `if`
    statements with no corresponding `else`. For larger amounts of conditional code,
    predication becomes inefficient because every instruction is executed, regardless
    of whether it will affect the computation. When the larger number of instructions
    causes the costs of predication to exceed the benefits, the compiler will use
    conditional branches. When the flow of execution within a warp takes different
    paths depending on a condition, the code is called *divergent*.
  id: totrans-2211
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA is close-mouthed about the details of how their hardware supports divergent
    code paths, and it reserves the right to change the hardware implementation between
    generations. The hardware maintains a bit vector of active threads within each
    warp. For threads that are marked inactive, execution is suppressed in a way similar
    to predication. Before taking a branch, the compiler executes a special instruction
    to push this active-thread bit vector onto a stack. The code is then executed
    *twice*, once for threads for which the condition was TRUE, then for threads for
    which the predicate was FALSE. This two-phased execution is managed with a *branch
    synchronization stack*, as described by Lindholm et al.^([15](ch08.html#ch08fn15))
  id: totrans-2212
  prefs: []
  type: TYPE_NORMAL
- en: '[15](ch08.html#ch08fn15a). Lindholm, Erik, John Nickolls, Stuart Oberman, and
    John Montrym. NVIDIA Tesla: A unified graphics and computing architecture. *IEEE
    Micro*, March–April 2008, pp. 39–55.'
  id: totrans-2213
  prefs: []
  type: TYPE_NORMAL
- en: If threads of a warp diverge via a data-dependent conditional branch, the warp
    serially executes each branch path taken, disabling threads that are not on that
    path, and when all paths complete, the threads reconverge to the original execution
    path. The SM uses a branch synchronization stack to manage independent threads
    that diverge and converge. Branch divergence only occurs within a warp; different
    warps execute independently regardless of whether they are executing common or
    disjoint code paths.
  id: totrans-2214
  prefs: []
  type: TYPE_NORMAL
- en: The PTX specification makes no mention of a branch synchronization stack, so
    the only publicly available evidence of its existence is in the disassembly output
    of `cuobjdump`. The `SSY` instruction pushes a state such as the program counter
    and active thread mask onto the stack; the `.S` instruction prefix pops this state
    and, if any active threads did not take the branch, causes those threads to execute
    the code path whose state was snapshotted by `SSY`.
  id: totrans-2215
  prefs: []
  type: TYPE_NORMAL
- en: '`SSY/.S` is only necessary when threads of execution may diverge, so if the
    compiler can guarantee that threads will stay uniform in a code path, you may
    see branches that are not bracketed by `SSY/.S`. The important thing to realize
    about branching in CUDA is that in all cases, it is most efficient for all threads
    within a warp to follow the same execution path.'
  id: totrans-2216
  prefs: []
  type: TYPE_NORMAL
- en: The loop in [Listing 8.2](ch08.html#ch08lis02) also includes a good self-contained
    example of divergence and convergence. The `SSY` instruction (offset 0x40) and
    `NOP.S` instruction (offset 0x78) bracket the points of divergence and convergence,
    respectively. The code loops over the `LDSLK` and subsequent predicated instructions,
    retiring active threads until the compiler knows that all threads will have converged
    and the branch synchronization stack can be popped with the `NOP.S` instruction.
  id: totrans-2217
  prefs: []
  type: TYPE_NORMAL
- en: /*0040*/ SSY 0x80;
  id: totrans-2218
  prefs: []
  type: TYPE_NORMAL
- en: /*0048*/ BAR.RED.POPC RZ, RZ;
  id: totrans-2219
  prefs: []
  type: TYPE_NORMAL
- en: /*0050*/ LD R0, [R0];
  id: totrans-2220
  prefs: []
  type: TYPE_NORMAL
- en: /*0058*/ LDSLK P0, R2, [R3];
  id: totrans-2221
  prefs: []
  type: TYPE_NORMAL
- en: /*0060*/ @P0 IADD R2, R2, R0;
  id: totrans-2222
  prefs: []
  type: TYPE_NORMAL
- en: /*0068*/ @P0 STSUL [R3], R2;
  id: totrans-2223
  prefs: []
  type: TYPE_NORMAL
- en: /*0070*/ @!P0 BRA 0x58;
  id: totrans-2224
  prefs: []
  type: TYPE_NORMAL
- en: /*0078*/ NOP.S CC.T;
  id: totrans-2225
  prefs: []
  type: TYPE_NORMAL
- en: '8.4.3\. Special Cases: Min, Max, and Absolute Value'
  id: totrans-2226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some conditional operations are so common that they are supported natively by
    the hardware. Minimum and maximum operations are supported for both integer and
    floating-point operands and are translated to a single instruction. Additionally,
    floating-point instructions include modifiers that can negate or take the absolute
    value of a source operand.
  id: totrans-2227
  prefs: []
  type: TYPE_NORMAL
- en: The compiler does a good job of detecting when min/max operations are being
    expressed, but if you want to take no chances, call the `min()/max()` intrinsics
    for integers or `fmin()/fmax()` for floating-point values.
  id: totrans-2228
  prefs: []
  type: TYPE_NORMAL
- en: 8.5\. Textures and Surfaces
  id: totrans-2229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The instructions that read and write textures and surfaces refer to much more
    implicit state than do other instructions; parameters such as the base address,
    dimensions, format, and interpretation of the texture contents are contained in
    a *header*, an intermediate data structure whose software abstraction is called
    a *texture reference* or *surface reference*. As developers manipulate the texture
    or surface references, the CUDA runtime and driver must translate those changes
    into the headers, which the texture or surface instruction references as an index.^([16](ch08.html#ch08fn16))
  id: totrans-2230
  prefs: []
  type: TYPE_NORMAL
- en: '[16](ch08.html#ch08fn16a). SM 3.x added *texture objects*, which enable texture
    and surface headers to be referenced by address rather than an index. Previous
    hardware generations could reference at most 128 textures or surfaces in a kernel,
    but with SM 3.x the number is limited only by memory.'
  id: totrans-2231
  prefs: []
  type: TYPE_NORMAL
- en: Before launching a kernel that operates on textures or surfaces, the driver
    must ensure that all this state is set correctly on the hardware. As a result,
    launching such kernels may take longer. Texture reads are serviced through a specialized
    cache subsystem that is separate from the L1/L2 caches in Fermi, and also separate
    from the constant cache. Each SM has an L1 texture cache, and the TPCs (texture
    processor clusters) or GPCs (graphics processor clusters) each additionally have
    L2 texture cache. Surface reads and writes are serviced through the same L1/L2
    caches that service global memory traffic.
  id: totrans-2232
  prefs: []
  type: TYPE_NORMAL
- en: 'Kepler added two technologies of note with respect to textures: the ability
    to read from global memory via the texture cache hierarchy without binding a texture
    reference, and the ability to specify a texture header by address rather than
    by index. The latter technology is known as “bindless textures.”'
  id: totrans-2233
  prefs: []
  type: TYPE_NORMAL
- en: On SM 3.5 and later hardware, reading global memory via the texture cache can
    be requested by using `const __restrict` pointers or by explicitly invoking the
    `ldg()` intrinsics in `sm_35_intrinsics.h`.
  id: totrans-2234
  prefs: []
  type: TYPE_NORMAL
- en: 8.6\. Miscellaneous Instructions
  id: totrans-2235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 8.6.1\. Warp-Level Primitives
  id: totrans-2236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It did not take long for the importance of warps as a primitive unit of execution
    (naturally residing between threads and blocks) to become evident to CUDA programmers.
    Starting with SM 1.x, NVIDIA began adding instructions that specifically operate
    on warps.
  id: totrans-2237
  prefs: []
  type: TYPE_NORMAL
- en: Vote
  id: totrans-2238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: That CUDA architectures are 32-bit and that warps are comprised of 32 threads
    made an irresistible match to instructions that can evaluate a condition and broadcast
    a 1-bit result to every thread in the warp. The `VOTE` instruction (first available
    in SM 1.2) evaluates a condition and broadcasts the result to all threads in the
    warp. The `__any()` intrinsic returns 1 if the predicate is true for *any* of
    the 32 threads in the warp. The `__all()` intrinsic returns 1 if the predicate
    is true for *all* of the 32 threads in the warp.
  id: totrans-2239
  prefs: []
  type: TYPE_NORMAL
- en: The Fermi architecture added a new variant of `VOTE` that passes back the predicate
    result for every thread in the warp. The `__ballot()` intrinsic evaluates a condition
    for all threads in the warp and returns a 32-bit value where each bit gives the
    condition for the corresponding thread in the warp.
  id: totrans-2240
  prefs: []
  type: TYPE_NORMAL
- en: Shuffle
  id: totrans-2241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Kepler added *shuffle* instructions that enable data interchange between threads
    within a warp without staging the data through shared memory. Although these instructions
    execute with the same latency as shared memory, they have the benefit of doing
    the exchange without performing both a read and a write, and they can reduce shared
    memory usage.
  id: totrans-2242
  prefs: []
  type: TYPE_NORMAL
- en: The following instruction is wrapped in a number of device functions that use
    inline PTX assembly defined in `sm_30_intrinsics.h.`
  id: totrans-2243
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p271pro01a)'
  id: totrans-2244
  prefs: []
  type: TYPE_NORMAL
- en: int __shfl(int var, int srcLane, int width=32);
  id: totrans-2245
  prefs: []
  type: TYPE_NORMAL
- en: int __shfl_up(int var, unsigned int delta, int width=32);
  id: totrans-2246
  prefs: []
  type: TYPE_NORMAL
- en: int __shfl_down(int var, unsigned int delta, int width=32);
  id: totrans-2247
  prefs: []
  type: TYPE_NORMAL
- en: int __shfl_xor(int var, int laneMask, int width=32);
  id: totrans-2248
  prefs: []
  type: TYPE_NORMAL
- en: The `width` parameter, which defaults to the warp width of 32, must be a power
    of 2 in the range 2..32\. It enables subdivision of the warp into segments; if
    `width<32`, each subsection of the warp behaves as a separate entity with a starting
    logical lane ID of 0\. A thread may only exchange data with other threads in its
    subsection.
  id: totrans-2249
  prefs: []
  type: TYPE_NORMAL
- en: '`__shfl`() returns the value of `var` held by the thread whose ID is given
    by `srcLane`. If `srcLane` is outside the range `0..width-1`, the thread’s own
    value of `var` is returned. This variant of the instruction can be used to broadcast
    values within a warp. `__shfl_up()` calculates a source lane ID by subtracting
    `delta` from the caller’s lane ID and clamping to the range `0..width-1`. `__shfl_down()`
    calculates a source lane ID by adding `delta` to the caller’s lane ID.'
  id: totrans-2250
  prefs: []
  type: TYPE_NORMAL
- en: '`__shfl_up()` and `__shfl_down()` enable warp-level scan and reverse scan operations,
    respectively. `__shfl_xor()` calculates a source lane ID by performing a bitwise
    XOR of the caller’s lane ID with `laneMask`; the value of `var` held by the resulting
    lane ID is returned. This variant can be used to do a reduction across the warps
    (or subwarps); each thread computes the reduction using a differently ordered
    series of the associative operator.'
  id: totrans-2251
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.2\. Block-Level Primitives
  id: totrans-2252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `__syncthreads()` intrinsic serves as a barrier. It causes all threads to
    wait until every thread in the threadblock has arrived at the `__syncthreads()`.
    The Fermi instruction set (SM 2.x) added several new block-level barriers that
    aggregate information about the threads in the threadblock.
  id: totrans-2253
  prefs: []
  type: TYPE_NORMAL
- en: '• `__syncthreads_count()`: evaluates a predicate and returns the sum of threads
    for which the predicate was true'
  id: totrans-2254
  prefs: []
  type: TYPE_NORMAL
- en: '• `__syncthreads_or()`: returns the OR of all the inputs across the threadblock'
  id: totrans-2255
  prefs: []
  type: TYPE_NORMAL
- en: '• `__syncthreads_and()`: returns the AND of all the inputs across the threadblock'
  id: totrans-2256
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.3\. Performance Counter
  id: totrans-2257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Developers can define their own set of performance counters and increment them
    in live code with the `__prof_trigger()` intrinsic.
  id: totrans-2258
  prefs: []
  type: TYPE_NORMAL
- en: void __prof_trigger(int counter);
  id: totrans-2259
  prefs: []
  type: TYPE_NORMAL
- en: Calling this function increments the corresponding counter by 1 per warp. `counter`
    must be in the range 0..7; counters 8..15 are reserved. The value of the counters
    may be obtained by listing `prof_trigger_00..prof_trigger_07` in the profiler
    configuration file.
  id: totrans-2260
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.4\. Video Instructions
  id: totrans-2261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The video instructions described in this section are accessible only via the
    inline PTX assembler. Their basic functionality is described here to help developers
    to decide whether they might be beneficial for their application. Anyone intending
    to use these instructions, however, should consult the PTX ISA specification.
  id: totrans-2262
  prefs: []
  type: TYPE_NORMAL
- en: Scalar Video Instructions
  id: totrans-2263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The scalar video instructions, added with SM 2.0 hardware, enable efficient
    operations on the short (8- and 16-bit) integer types needed for video processing.
    As described in the PTX 3.1 ISA Specification, the format of these instructions
    is as follows.
  id: totrans-2264
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch08_images.html#p273pro01a)'
  id: totrans-2265
  prefs: []
  type: TYPE_NORMAL
- en: vop.dtype.atype.btype{.sat} d, a{.asel}, b{.bsel};
  id: totrans-2266
  prefs: []
  type: TYPE_NORMAL
- en: vop.dtype.atype.btype{.sat}.secop d, a{.asel}, b{.bsel}, c;
  id: totrans-2267
  prefs: []
  type: TYPE_NORMAL
- en: 'The source and destination operands are all 32-bit registers. `dtype`, `atype`,
    and `btype` may be `.u32` or `.s32` for unsigned and signed 32-bit integers, respectively.
    The `asel/bsel` specifiers select which 8- or 16-bit value to extract from the
    source operands: `b0`, `b1`, `b2`, and `b3` select bytes (numbering from the least
    significant), and `h0/h1` select the least significant and most significant 16
    bits, respectively.'
  id: totrans-2268
  prefs: []
  type: TYPE_NORMAL
- en: Once the input values are extracted, they are sign- or zero-extended internally
    to signed 33-bit integers, and the primary operation is performed, producing a
    34-bit intermediate result whose sign depends on `dtype`. Finally, the result
    is clamped to the output range, and one of the following operations is performed.
  id: totrans-2269
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** Apply a second operation (add, min or max) to the intermediate result
    and a third operand.'
  id: totrans-2270
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** Truncate the intermediate result to an 8- or 16-bit value and merge
    into a specified position in the third operand to produce the final result.'
  id: totrans-2271
  prefs: []
  type: TYPE_NORMAL
- en: The lower 32 bits are then written to the destination operand.
  id: totrans-2272
  prefs: []
  type: TYPE_NORMAL
- en: The `vset` instruction performs a comparison between the 8-, 16-, or 32-bit
    input operands and generates the corresponding predicate (1 or 0) as output. The
    PTX scalar video instructions and the corresponding operations are given in [Table
    8.14](ch08.html#ch08tab14).
  id: totrans-2273
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab14.jpg)'
  id: totrans-2274
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.14* Scalar Video Instructions.'
  id: totrans-2275
  prefs: []
  type: TYPE_NORMAL
- en: Vector Video Instructions (SM 3.0 only)
  id: totrans-2276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These instructions, added with SM 3.0, are similar to the scalar video instructions
    in that they promote the inputs to a canonical integer format, perform the core
    operation, and then clamp and optionally merge the output. But they deliver higher
    performance by operating on pairs of 16-bit values or quads of 8-bit values.
  id: totrans-2277
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8.15](ch08.html#ch08tab15) summarizes the PTX instructions and corresponding
    operations implemented by these instructions. They are most useful for video processing
    and certain image processing operations (such as the median filter).'
  id: totrans-2278
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab15.jpg)'
  id: totrans-2279
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.15* Vector Video Instructions'
  id: totrans-2280
  prefs: []
  type: TYPE_NORMAL
- en: 8.6.5\. Special Registers
  id: totrans-2281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many special registers are accessed by referencing the built-in variables `threadIdx`,
    `blockIdx`, `blockDim`, and `gridDim`. These pseudo-variables, described in detail
    in [Section 7.3](ch07.html#ch07lev1sec3), are 3-dimensional structures that specify
    the thread ID, block ID, thread count, and block count, respectively.
  id: totrans-2282
  prefs: []
  type: TYPE_NORMAL
- en: Besides those, another special register is the SM’s clock register, which increments
    with each clock cycle. This counter can be read with the `__clock()` or `__clock64()`
    intrinsic. The counters are separately tracked for each SM and, like the time
    stamp counters on CPUs, are most useful for measuring relative performance of
    different code sequences and best avoided when trying to calculate wall clock
    times.
  id: totrans-2283
  prefs: []
  type: TYPE_NORMAL
- en: 8.7\. Instruction Sets
  id: totrans-2284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'NVIDIA has developed three major architectures: Tesla (SM 1.x), Fermi (SM 2.x),
    and Kepler (SM 3.x). Within those families, new instructions have been added as
    NVIDIA updated their products. For example, global atomic operations were not
    present in the very first Tesla-class processor (the G80, which shipped in 2006
    as the GeForce GTX 8800), but all subsequent Tesla-class GPUs included them. So
    when querying the SM version via `cuDeviceComputeCapability()`, the major and
    minor versions will be 1.0 for G80 and 1.1 (or greater) for all other Tesla-class
    GPUs. Conversely, if the SM version is 1.1 or greater, the application can use
    global atomics.'
  id: totrans-2285
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 8.16](ch08.html#ch08tab16) gives the SASS instructions that may be printed
    by `cuobjdump` when disassembling microcode for Tesla-class (SM 1.x) hardware.
    The Fermi and Kepler instruction sets closely resemble each other, with the exception
    of the instructions that support surface load/store, so their instruction sets
    are given together in [Table 8.17](ch08.html#ch08tab17). In both tables, the middle
    column specifies the first SM version to support a given instruction.'
  id: totrans-2286
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab16.jpg)![Image](graphics/08tab16a.jpg)![Image](graphics/08tab16b.jpg)![Image](graphics/08tab16c.jpg)![Image](graphics/08tab16d.jpg)'
  id: totrans-2287
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.16.* SM 1.x Instruction Set'
  id: totrans-2288
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/08tab17.jpg)![Image](graphics/08tab17a.jpg)![Image](graphics/08tab17b.jpg)![Image](graphics/08tab17c.jpg)![Image](graphics/08tab17d.jpg)![Image](graphics/08tab17e.jpg)![Image](graphics/08tab17f.jpg)![Image](graphics/08tab17g.jpg)'
  id: totrans-2289
  prefs: []
  type: TYPE_IMG
- en: '*Table 8.17.* SM 2.x and SM 3.x Instruction Sets'
  id: totrans-2290
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 9\. Multiple GPUs
  id: totrans-2291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter describes CUDA’s facilities for multi-GPU programming, including
    threading models, peer-to-peer, and inter-GPU synchronization. As an example,
    we’ll first explore inter-GPU synchronization using CUDA streams and events by
    implementing a peer-to-peer memcpy that stages through portable pinned memory.
    We then discuss how to implement the N-body problem (fully described in [Chapter
    14](ch14.html#ch14)) with single- and multithreaded implementations that use multiple
    GPUs.
  id: totrans-2292
  prefs: []
  type: TYPE_NORMAL
- en: 9.1\. Overview
  id: totrans-2293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Systems with multiple GPUs generally contain multi-GPU boards with a PCI Express
    bridge chip (such as the GeForce GTX 690) or multiple PCI Express slots, or both,
    as described in [Section 2.3](ch02.html#ch02lev1sec3). Each GPU in such a system
    is separated by PCI Express bandwidth, so there is always a huge disparity in
    bandwidth between memory connected directly to a GPU (its device memory) and its
    connections to other GPUs as well as the CPU.
  id: totrans-2294
  prefs: []
  type: TYPE_NORMAL
- en: Many CUDA features designed to run on multiple GPUs, such as peer-to-peer addressing,
    require the GPUs to be identical. For applications that can make assumptions about
    the target hardware (such as vertical applications built for specific hardware
    configurations), this requirement is innocuous enough. But applications targeting
    systems with a variety of GPUs (say, a low-power one for everyday use and a powerful
    one for gaming) may have to use heuristics to decide which GPU(s) to use or load-balance
    the workload across GPUs so the faster ones contribute more computation to the
    final output, commensurate with their higher performance.
  id: totrans-2295
  prefs: []
  type: TYPE_NORMAL
- en: A key ingredient to all CUDA applications that use multiple GPUs is *portable
    pinned memory*. As described in [Section 5.1.2](ch05.html#ch05lev2sec2), portable
    pinned memory is pinned memory that is mapped for all CUDA contexts such that
    any GPU can read or write the memory directly.
  id: totrans-2296
  prefs: []
  type: TYPE_NORMAL
- en: CPU Threading Models
  id: totrans-2297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Until CUDA 4.0, the only way to drive multiple GPUs was to create a CPU thread
    for each one. The `cudaSetDevice()` function had to be called once per CPU thread,
    before any CUDA code had executed, in order to tell CUDA which device to initialize
    when the CPU thread started to operate on CUDA. Whichever CPU thread made that
    call would then get exclusive access to the GPU, because the CUDA driver had not
    yet been made thread-safe in a way that would enable multiple threads to access
    the same GPU at the same time.
  id: totrans-2298
  prefs: []
  type: TYPE_NORMAL
- en: 'In CUDA 4.0, `cudaSetDevice()` was modified to implement the semantics that
    everyone had previously expected: It tells CUDA which GPU should perform subsequent
    CUDA operations. Having multiple threads operating on the same GPU at the same
    time may incur a slight performance hit, but it should be expected to work. Our
    example N-body application, however, only has one CPU thread operating on any
    given device at a time. The multithreaded formulation has each of *N* threads
    operate on a specific device, and the single-threaded formulation has one thread
    operate on each of the *N* devices in turn.'
  id: totrans-2299
  prefs: []
  type: TYPE_NORMAL
- en: 9.2\. Peer-to-Peer
  id: totrans-2300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When multiple GPUs are used by a CUDA program, they are known as “peers” because
    the application generally treats them equally, as if they were coworkers collaborating
    on a project. CUDA enables two flavors of peer-to-peer: explicit memcpy and peer-to-peer
    addressing.^([1](ch09.html#ch09fn1))'
  id: totrans-2301
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch09.html#ch09fn1a). For peer-to-peer addressing, the term *peer* also
    harkens to the requirement that the GPUs be identical.'
  id: totrans-2302
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1\. Peer-to-Peer Memcpy
  id: totrans-2303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Memory copies can be performed between the memories of any two different devices.
    When UVA (Unified Virtual Addressing) is in effect, the ordinary family of memcpy
    function can be used for peer-to-peer memcpy, since CUDA can infer which device
    “owns” which memory. If UVA is not in effect, the peer-to-peer memcpy must be
    done explicitly using `cudaMemcpyPeer()`, `cudaMemcpyPeerAsync()`, `cudaMemcpy3DPeer()`,
    or `cudaMemcpy3DPeerAsync()`.
  id: totrans-2304
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2305
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2306
  prefs: []
  type: TYPE_NORMAL
- en: CUDA can copy memory between any two devices, not just devices that can directly
    address one another’s memory. If necessary, CUDA will stage the memory copy through
    host memory, which can be accessed by any device in the system.
  id: totrans-2307
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2308
  prefs: []
  type: TYPE_NORMAL
- en: Peer-to-peer memcpy operations do not run concurrently with any other operation.
    Any pending operations on either GPU must complete before the peer-to-peer memcpy
    can begin, and no subsequent operations can start to execute until after the peer-to-peer
    memcpy is done. When possible, CUDA will use direct peer-to-peer mappings between
    the two pointers. The resulting copies are faster and do not have to be staged
    through host memory.
  id: totrans-2309
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2\. Peer-to-Peer Addressing
  id: totrans-2310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Peer-to-peer mappings of device memory, shown in [Figure 2.20](ch02.html#ch02fig20),
    enable a kernel running on one GPU to read or write memory that resides in another
    GPU. Since the GPUs can only use peer-to-peer to read or write data at PCI Express
    rates, developers have to partition the workload in such a way that
  id: totrans-2311
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** Each GPU has about an equal amount of work to do.'
  id: totrans-2312
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** The GPUs only need to interchange modest amounts of data.'
  id: totrans-2313
  prefs: []
  type: TYPE_NORMAL
- en: Examples of such systems might be a pipelined computer vision system where each
    stage in the pipeline of GPUs computes an intermediate data structure (e.g., locations
    of identified features) that needs to be further analyzed by the next GPU in the
    pipeline or a large so-called “stencil” computation in which separate GPUs can
    perform most of the computation independently but must exchange edge data between
    computation steps.
  id: totrans-2314
  prefs: []
  type: TYPE_NORMAL
- en: In order for peer-to-peer addressing to work, the following conditions apply.
  id: totrans-2315
  prefs: []
  type: TYPE_NORMAL
- en: • Unified virtual addressing (UVA) must be in effect.
  id: totrans-2316
  prefs: []
  type: TYPE_NORMAL
- en: • Both GPUs must be SM 2.x or higher and must be based on the same chip.
  id: totrans-2317
  prefs: []
  type: TYPE_NORMAL
- en: • The GPUs must be on the same I/O hub.
  id: totrans-2318
  prefs: []
  type: TYPE_NORMAL
- en: '`cu(da)DeviceCanAccessPeer ()` may be called to query whether the current device
    can map another device’s memory.'
  id: totrans-2319
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p290pro01a)'
  id: totrans-2320
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaDeviceCanAccessPeer(int *canAccessPeer, int device,
  id: totrans-2321
  prefs: []
  type: TYPE_NORMAL
- en: int peerDevice);
  id: totrans-2322
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuDeviceCanAccessPeer(int *canAccessPeer, CUdevice device,
  id: totrans-2323
  prefs: []
  type: TYPE_NORMAL
- en: CUdevice peerDevice);
  id: totrans-2324
  prefs: []
  type: TYPE_NORMAL
- en: Peer-to-peer mappings are not enabled automatically; they must be specifically
    requested by calling `cudaDeviceEnablePeerAccess()` or `cuCtxEnablePeerAccess()`.
  id: totrans-2325
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p290pro02a)'
  id: totrans-2326
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaDeviceEnablePeerAccess(int peerDevice, unsigned int
  id: totrans-2327
  prefs: []
  type: TYPE_NORMAL
- en: flags);
  id: totrans-2328
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuCtxEnablePeerAccess(CUcontext peerContext, unsigned int
  id: totrans-2329
  prefs: []
  type: TYPE_NORMAL
- en: Flags);
  id: totrans-2330
  prefs: []
  type: TYPE_NORMAL
- en: Once peer-to-peer access has been enabled, all memory in the peer device—including
    new allocations—is accessible to the current device until `cudaDeviceDisablePeerAccess()`
    or `cuCtxDisablePeerAccess()` is called.
  id: totrans-2331
  prefs: []
  type: TYPE_NORMAL
- en: Peer-to-peer access uses a small amount of extra memory (to hold more page tables)
    and makes memory allocation more expensive, since the memory must be mapped for
    all participating devices. Peer-to-peer functionality enables contexts to read
    and write memory belonging to other contexts, both via memcpy (which may be implemented
    by staging through system memory) and directly by having kernels read or write
    global memory pointers.
  id: totrans-2332
  prefs: []
  type: TYPE_NORMAL
- en: The `cudaDeviceEnablePeerAccess()` function maps the memory belonging to another
    device. Peer-to-peer memory addressing is asymmetric; it is possible for GPU A
    to map GPU B’s allocations without its allocations being available to GPU B. In
    order for two GPUs to see each other’s memory, each GPU must explicitly map the
    other’s memory.
  id: totrans-2333
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p290pro03a)'
  id: totrans-2334
  prefs: []
  type: TYPE_NORMAL
- en: // tell device 1 to map device 0 memory
  id: totrans-2335
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( 1 );
  id: totrans-2336
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceEnablePeerAccess( 0, cudaPeerAccessDefault );
  id: totrans-2337
  prefs: []
  type: TYPE_NORMAL
- en: // tell device 0 to map device 1 memory
  id: totrans-2338
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( 0 );
  id: totrans-2339
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceEnablePeerAccess( 1, cudaPeerAccessDefault );
  id: totrans-2340
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2341
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2342
  prefs: []
  type: TYPE_NORMAL
- en: On GPU boards with PCI Express 3.0–capable bridge chips (such as the Tesla K10),
    the GPUs can communicate at PCI Express 3.0 speeds even if the board is plugged
    into a PCI Express 2.0 slot.
  id: totrans-2343
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2344
  prefs: []
  type: TYPE_NORMAL
- en: '9.3\. UVA: Inferring Device from Address'
  id: totrans-2345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since UVA is always enabled on peer-to-peer-capable systems, the address ranges
    for different devices do not overlap, and the driver can infer the owning device
    from a pointer value. The `cuPointerGetAttribute()` function may be used to query
    information about UVA pointers, including the owning context.
  id: totrans-2346
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p291pro01a)'
  id: totrans-2347
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuPointerGetAttribute(void *data, CUpointer_
  id: totrans-2348
  prefs: []
  type: TYPE_NORMAL
- en: attribute attribute, CUdeviceptr ptr);
  id: totrans-2349
  prefs: []
  type: TYPE_NORMAL
- en: '`cuPointerGetAttribute()` or `cudaPointerGetAttributes()` may be used to query
    the attributes of a pointer. [Table 9.1](ch09.html#ch09tab01) gives the values
    that can be passed into `cuPointerGetAttribute()`; the structure passed back by
    `cudaPointerGetAttributes()` is as follows.'
  id: totrans-2350
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p291pro02a)'
  id: totrans-2351
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPointerAttributes {
  id: totrans-2352
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaMemoryType memoryType;
  id: totrans-2353
  prefs: []
  type: TYPE_NORMAL
- en: int device;
  id: totrans-2354
  prefs: []
  type: TYPE_NORMAL
- en: void *devicePointer;
  id: totrans-2355
  prefs: []
  type: TYPE_NORMAL
- en: void *hostPointer;
  id: totrans-2356
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2357
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/09tab01.jpg)'
  id: totrans-2358
  prefs: []
  type: TYPE_IMG
- en: '*Table 9.1* `cuPointerGetAttribute()` Attributes'
  id: totrans-2359
  prefs: []
  type: TYPE_NORMAL
- en: '`memoryType` may be `cudaMemoryTypeHost` or `cudaMemoryTypeDevice`.'
  id: totrans-2360
  prefs: []
  type: TYPE_NORMAL
- en: '`device` is the device for which the pointer was allocated. For device memory,
    `device` identifies the device where the memory corresponding to `ptr` was allocated.
    For host memory, `device` identifies the device that was current when the allocation
    was performed.'
  id: totrans-2361
  prefs: []
  type: TYPE_NORMAL
- en: '`devicePointer` gives the device pointer value that may be used to reference
    `ptr` from the current device. If `ptr` cannot be accessed by the current device,
    `devicePointer` is `NULL`.'
  id: totrans-2362
  prefs: []
  type: TYPE_NORMAL
- en: '`hostPointer` gives the host pointer value that may be used to reference `ptr`
    from the CPU. If `ptr` cannot be accessed by the current host, `hostPointer` is
    `NULL`.'
  id: totrans-2363
  prefs: []
  type: TYPE_NORMAL
- en: 9.4\. Inter-GPU Synchronization
  id: totrans-2364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CUDA events may be used for inter-GPU synchronization using `cu(da)StreamWaitEvent()`.
    If there is a producer/consumer relationship between two GPUs, the application
    can have the producer GPU record an event and then have the consumer GPU insert
    a stream-wait on that event into its command stream. When the consumer GPU encounters
    the stream-wait, it will stop processing commands until the producer GPU has passed
    the point of execution where `cu(da)EventRecord()` was called.
  id: totrans-2365
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2366
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2367
  prefs: []
  type: TYPE_NORMAL
- en: In CUDA 5.0, the device runtime, described in [Section 7.5](ch07.html#ch07lev1sec5),
    does not enable any inter-GPU synchronization whatsoever. That limitation may
    be relaxed in a future release.
  id: totrans-2368
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2369
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 9.1](ch09.html#ch09lis01) gives `chMemcpyPeerToPeer()`,^([2](ch09.html#ch09fn2))
    an implementation of peer-to-peer memcpy that uses portable memory and inter-GPU
    synchronization to implement the same type of memcpy that CUDA uses under the
    covers, if no direct mapping between the GPUs exists. The function works similarly
    to the `chMemcpyHtoD()` function in [Listing 6.2](ch06.html#ch06lis02) that performs
    host→device memcpy: A staging buffer is allocated in host memory, and the memcpy
    begins by having the source GPU copy source data into the staging buffer and recording
    an event. But unlike the host→device memcpy, there is never any need for the CPU
    to synchronize because all synchronization is done by the GPUs. Because both the
    memcpy and the event-record are asynchronous, immediately after kicking off the
    initial memcpy and event-record, the CPU can request that the destination GPU
    wait on that event and kick off a memcpy of the same buffer. Two staging buffers
    and two CUDA events are needed, so the two GPUs can copy to and from staging buffers
    concurrently, much as the CPU and GPU concurrently operate on staging buffers
    during the host→device memcpy. The CPU loops over the input buffer and output
    buffers, issuing memcpy and event-record commands and ping-ponging between staging
    buffers, until it has requested copies for all bytes and all that’s left to do
    is wait for both GPUs to finish processing.'
  id: totrans-2370
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch09.html#ch09fn2a). The `CUDART_CHECK` error handling has been removed
    for clarity.'
  id: totrans-2371
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2372
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2373
  prefs: []
  type: TYPE_NORMAL
- en: As with the implementations in the CUDA support provided by NVIDIA, our peer-to-peer
    memcpy is synchronous.
  id: totrans-2374
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2375
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.1.* `chMemcpyPeerToPeer()`.'
  id: totrans-2376
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis01a)'
  id: totrans-2377
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2378
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t
  id: totrans-2379
  prefs: []
  type: TYPE_NORMAL
- en: chMemcpyPeerToPeer(
  id: totrans-2380
  prefs: []
  type: TYPE_NORMAL
- en: void *_dst, int dstDevice,
  id: totrans-2381
  prefs: []
  type: TYPE_NORMAL
- en: const void *_src, int srcDevice,
  id: totrans-2382
  prefs: []
  type: TYPE_NORMAL
- en: size_t N )
  id: totrans-2383
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2384
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-2385
  prefs: []
  type: TYPE_NORMAL
- en: char *dst = (char *) _dst;
  id: totrans-2386
  prefs: []
  type: TYPE_NORMAL
- en: const char *src = (const char *) _src;
  id: totrans-2387
  prefs: []
  type: TYPE_NORMAL
- en: int stagingIndex = 0;
  id: totrans-2388
  prefs: []
  type: TYPE_NORMAL
- en: while ( N ) {
  id: totrans-2389
  prefs: []
  type: TYPE_NORMAL
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  id: totrans-2390
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( srcDevice );
  id: totrans-2391
  prefs: []
  type: TYPE_NORMAL
- en: cudaStreamWaitEvent( 0, g_events[dstDevice][stagingIndex],0);
  id: totrans-2392
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( g_hostBuffers[stagingIndex], src,
  id: totrans-2393
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize, cudaMemcpyDeviceToHost, NULL );
  id: totrans-2394
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( g_events[srcDevice][stagingIndex] );
  id: totrans-2395
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( dstDevice );
  id: totrans-2396
  prefs: []
  type: TYPE_NORMAL
- en: cudaStreamWaitEvent( 0, g_events[srcDevice][stagingIndex],0);
  id: totrans-2397
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
  id: totrans-2398
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize, cudaMemcpyHostToDevice, NULL );
  id: totrans-2399
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( g_events[dstDevice][stagingIndex] );
  id: totrans-2400
  prefs: []
  type: TYPE_NORMAL
- en: dst += thisCopySize;
  id: totrans-2401
  prefs: []
  type: TYPE_NORMAL
- en: src += thisCopySize;
  id: totrans-2402
  prefs: []
  type: TYPE_NORMAL
- en: N -= thisCopySize;
  id: totrans-2403
  prefs: []
  type: TYPE_NORMAL
- en: stagingIndex = 1 - stagingIndex;
  id: totrans-2404
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2405
  prefs: []
  type: TYPE_NORMAL
- en: // Wait until both devices are done
  id: totrans-2406
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( srcDevice );
  id: totrans-2407
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  id: totrans-2408
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( dstDevice );
  id: totrans-2409
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  id: totrans-2410
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-2411
  prefs: []
  type: TYPE_NORMAL
- en: return status;
  id: totrans-2412
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2413
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2414
  prefs: []
  type: TYPE_NORMAL
- en: 9.5\. Single-Threaded Multi-GPU
  id: totrans-2415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using the CUDA runtime, a single-threaded application can drive multiple
    GPUs by calling `cudaSetDevice()` to specify which GPU will be operated by the
    calling CPU thread. This idiom is used in [Listing 9.1](ch09.html#ch09lis01) to
    switch between the source and destination GPUs during the peer-to-peer memcpy,
    as well as the single-threaded, multi-GPU implementation of N-body described in
    [Section 9.5.2](ch09.html#ch09lev2sec4). In the driver API, CUDA maintains a stack
    of current contexts so that subroutines can easily change and restore the caller’s
    current context.
  id: totrans-2416
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.1\. Current Context Stack
  id: totrans-2417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Driver API applications can manage the current context with the current-context
    stack: `cuCtxPushCurrent()` makes a new context current, pushing it onto the top
    of the stack, and `cuCtxPopCurrent()` pops the current context and restores the
    previous current context. [Listing 9.2](ch09.html#ch09lis02) gives a driver API
    version of `chMemcpyPeerToPeer()`, which uses `cuCtxPopCurrent()` and `cuCtxPushCurrent()`
    to perform a peer-to-peer memcpy between two contexts.'
  id: totrans-2418
  prefs: []
  type: TYPE_NORMAL
- en: The current context stack was introduced to CUDA in v2.2, and at the time, the
    CUDA runtime and driver API could not be used in the same application. That restriction
    has been relaxed in subsequent versions.
  id: totrans-2419
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.2.* `chMemcpyPeerToPeer` (driver API version).'
  id: totrans-2420
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis02a)'
  id: totrans-2421
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2422
  prefs: []
  type: TYPE_NORMAL
- en: CUresult
  id: totrans-2423
  prefs: []
  type: TYPE_NORMAL
- en: chMemcpyPeerToPeer(
  id: totrans-2424
  prefs: []
  type: TYPE_NORMAL
- en: void *_dst, CUcontext dstContext, int dstDevice,
  id: totrans-2425
  prefs: []
  type: TYPE_NORMAL
- en: const void *_src, CUcontext srcContext, int srcDevice,
  id: totrans-2426
  prefs: []
  type: TYPE_NORMAL
- en: size_t N )
  id: totrans-2427
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2428
  prefs: []
  type: TYPE_NORMAL
- en: CUresult status;
  id: totrans-2429
  prefs: []
  type: TYPE_NORMAL
- en: CUdeviceptr dst = (CUdeviceptr) (intptr_t) _dst;
  id: totrans-2430
  prefs: []
  type: TYPE_NORMAL
- en: CUdeviceptr src = (CUdeviceptr) (intptr_t) _src;
  id: totrans-2431
  prefs: []
  type: TYPE_NORMAL
- en: int stagingIndex = 0;
  id: totrans-2432
  prefs: []
  type: TYPE_NORMAL
- en: while ( N ) {
  id: totrans-2433
  prefs: []
  type: TYPE_NORMAL
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  id: totrans-2434
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPushCurrent( srcContext ) );
  id: totrans-2435
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuStreamWaitEvent(
  id: totrans-2436
  prefs: []
  type: TYPE_NORMAL
- en: NULL, g_events[dstDevice][stagingIndex], 0 ) );
  id: totrans-2437
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuMemcpyDtoHAsync(
  id: totrans-2438
  prefs: []
  type: TYPE_NORMAL
- en: g_hostBuffers[stagingIndex],
  id: totrans-2439
  prefs: []
  type: TYPE_NORMAL
- en: src,
  id: totrans-2440
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize,
  id: totrans-2441
  prefs: []
  type: TYPE_NORMAL
- en: NULL ) );
  id: totrans-2442
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuEventRecord(
  id: totrans-2443
  prefs: []
  type: TYPE_NORMAL
- en: g_events[srcDevice][stagingIndex],
  id: totrans-2444
  prefs: []
  type: TYPE_NORMAL
- en: 0 ) );
  id: totrans-2445
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPopCurrent( &srcContext ) );
  id: totrans-2446
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPushCurrent( dstContext ) );
  id: totrans-2447
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuStreamWaitEvent(
  id: totrans-2448
  prefs: []
  type: TYPE_NORMAL
- en: NULL,
  id: totrans-2449
  prefs: []
  type: TYPE_NORMAL
- en: g_events[srcDevice][stagingIndex],
  id: totrans-2450
  prefs: []
  type: TYPE_NORMAL
- en: 0 ) );
  id: totrans-2451
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuMemcpyHtoDAsync(
  id: totrans-2452
  prefs: []
  type: TYPE_NORMAL
- en: dst,
  id: totrans-2453
  prefs: []
  type: TYPE_NORMAL
- en: g_hostBuffers[stagingIndex],
  id: totrans-2454
  prefs: []
  type: TYPE_NORMAL
- en: thisCopySize,
  id: totrans-2455
  prefs: []
  type: TYPE_NORMAL
- en: NULL ) );
  id: totrans-2456
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuEventRecord(
  id: totrans-2457
  prefs: []
  type: TYPE_NORMAL
- en: g_events[dstDevice][stagingIndex],
  id: totrans-2458
  prefs: []
  type: TYPE_NORMAL
- en: 0 ) );
  id: totrans-2459
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPopCurrent( &dstContext ) );
  id: totrans-2460
  prefs: []
  type: TYPE_NORMAL
- en: dst += thisCopySize;
  id: totrans-2461
  prefs: []
  type: TYPE_NORMAL
- en: src += thisCopySize;
  id: totrans-2462
  prefs: []
  type: TYPE_NORMAL
- en: N -= thisCopySize;
  id: totrans-2463
  prefs: []
  type: TYPE_NORMAL
- en: stagingIndex = 1 - stagingIndex;
  id: totrans-2464
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2465
  prefs: []
  type: TYPE_NORMAL
- en: // Wait until both devices are done
  id: totrans-2466
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPushCurrent( srcContext ) );
  id: totrans-2467
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxSynchronize() );
  id: totrans-2468
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPopCurrent( &srcContext ) );
  id: totrans-2469
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPushCurrent( dstContext ) );
  id: totrans-2470
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxSynchronize() );
  id: totrans-2471
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK( cuCtxPopCurrent( &dstContext ) );
  id: totrans-2472
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-2473
  prefs: []
  type: TYPE_NORMAL
- en: return status;
  id: totrans-2474
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2475
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2476
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.2\. N-Body
  id: totrans-2477
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The N-body computation (described in detail in [Chapter 14](ch14.html#ch14))
    computes *N* forces in O(*N*²) time, and the outputs may be computed independently.
    On a system with *k* GPUs, our multi-GPU implementation splits the computation
    into *k* parts.
  id: totrans-2478
  prefs: []
  type: TYPE_NORMAL
- en: Our implementation makes the common assumption that the GPUs are identical,
    so it divides the computation evenly. Applications targeting GPUs of unequal performance,
    or whose workloads have less predictable runtimes, can divide the computation
    more finely and have the host code submit work items to the GPUs from a queue.
  id: totrans-2479
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 9.3](ch09.html#ch09lis03) gives a modified version of [Listing 14.3](ch14.html#ch14lis03)
    that takes two additional parameters (a base index `base` and size `n` of the
    subarray of forces) to compute a subset of the output array for an N-body computation.
    This `__device__` function is invoked by wrapper kernels that are declared as
    `__global__` . It is structured this way to reuse the code without incurring link
    errors. If the function were declared as `__global__`, the linker would generate
    an error about duplicate symbols.^([3](ch09.html#ch09fn3))'
  id: totrans-2480
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch09.html#ch09fn3a). This is a bit of an old-school workaround. CUDA 5.0
    added a linker that enables the `__global__` function to be compiled into a static
    library and linked into the application.'
  id: totrans-2481
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.3.* N-body kernel (multi-GPU).'
  id: totrans-2482
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis03a)'
  id: totrans-2483
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2484
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ void
  id: totrans-2485
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_Shared_multiGPU(
  id: totrans-2486
  prefs: []
  type: TYPE_NORMAL
- en: float *force,
  id: totrans-2487
  prefs: []
  type: TYPE_NORMAL
- en: float *posMass,
  id: totrans-2488
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  id: totrans-2489
  prefs: []
  type: TYPE_NORMAL
- en: size_t base,
  id: totrans-2490
  prefs: []
  type: TYPE_NORMAL
- en: size_t n,
  id: totrans-2491
  prefs: []
  type: TYPE_NORMAL
- en: size_t N )
  id: totrans-2492
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2493
  prefs: []
  type: TYPE_NORMAL
- en: float4 *posMass4 = (float4 *) posMass;
  id: totrans-2494
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ float4 shPosMass[];
  id: totrans-2495
  prefs: []
  type: TYPE_NORMAL
- en: for ( int m = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-2496
  prefs: []
  type: TYPE_NORMAL
- en: m < n;
  id: totrans-2497
  prefs: []
  type: TYPE_NORMAL
- en: m += blockDim.x*gridDim.x )
  id: totrans-2498
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2499
  prefs: []
  type: TYPE_NORMAL
- en: size_t i = base+m;
  id: totrans-2500
  prefs: []
  type: TYPE_NORMAL
- en: float acc[3] = {0};
  id: totrans-2501
  prefs: []
  type: TYPE_NORMAL
- en: float4 myPosMass = posMass4[i];
  id: totrans-2502
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma unroll 32'
  id: totrans-2503
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < N; j += blockDim.x ) {
  id: totrans-2504
  prefs: []
  type: TYPE_NORMAL
- en: shPosMass[threadIdx.x] = posMass4[j+threadIdx.x];
  id: totrans-2505
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  id: totrans-2506
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t k = 0; k < blockDim.x; k++ ) {
  id: totrans-2507
  prefs: []
  type: TYPE_NORMAL
- en: float fx, fy, fz;
  id: totrans-2508
  prefs: []
  type: TYPE_NORMAL
- en: float4 bodyPosMass = shPosMass[k];
  id: totrans-2509
  prefs: []
  type: TYPE_NORMAL
- en: bodyBodyInteraction(
  id: totrans-2510
  prefs: []
  type: TYPE_NORMAL
- en: '&fx, &fy, &fz,'
  id: totrans-2511
  prefs: []
  type: TYPE_NORMAL
- en: myPosMass.x, myPosMass.y, myPosMass.z,
  id: totrans-2512
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.x,
  id: totrans-2513
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.y,
  id: totrans-2514
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.z,
  id: totrans-2515
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.w,
  id: totrans-2516
  prefs: []
  type: TYPE_NORMAL
- en: softeningSquared );
  id: totrans-2517
  prefs: []
  type: TYPE_NORMAL
- en: acc[0] += fx;
  id: totrans-2518
  prefs: []
  type: TYPE_NORMAL
- en: acc[1] += fy;
  id: totrans-2519
  prefs: []
  type: TYPE_NORMAL
- en: acc[2] += fz;
  id: totrans-2520
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2521
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  id: totrans-2522
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2523
  prefs: []
  type: TYPE_NORMAL
- en: force[3*m+0] = acc[0];
  id: totrans-2524
  prefs: []
  type: TYPE_NORMAL
- en: force[3*m+1] = acc[1];
  id: totrans-2525
  prefs: []
  type: TYPE_NORMAL
- en: force[3*m+2] = acc[2];
  id: totrans-2526
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2527
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2528
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2529
  prefs: []
  type: TYPE_NORMAL
- en: 'The host code for a single-threaded, multi-GPU version of N-body is shown in
    [Listing 9.4](ch09.html#ch09lis04).^([4](ch09.html#ch09fn4)) The arrays `dptrPosMass`
    and `dptrForce` track the device pointers for the input and output arrays for
    each GPU (the maximum number of GPUs is declared as a constant in `nbody.h`; default
    is 32). Similar to dispatching work into CUDA streams, the function uses separate
    loops for different stages of the computation: The first loop allocates and populates
    the input array for each GPU; the second loop launches the kernel and an asynchronous
    copy of the output data; and the third loop calls `cudaDeviceSynchronize()` on
    each GPU in turn. Structuring the function this way maximizes CPU/GPU overlap.
    During the first loop, asynchronous host→device memcpys to GPUs 0..*i*-1 can proceed
    while the CPU is busy allocating memory for GPU *i*. If the kernel launch and
    asynchronous device→host memcpy were in the first loop, the synchronous `cudaMalloc()`
    calls would decrease performance because they are synchronous with respect to
    the current GPU.'
  id: totrans-2530
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch09.html#ch09fn4a). To avoid awkward formatting, error checking has been
    removed.'
  id: totrans-2531
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.4.* N-body host code (single-threaded multi-GPU).'
  id: totrans-2532
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis04a)'
  id: totrans-2533
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2534
  prefs: []
  type: TYPE_NORMAL
- en: float
  id: totrans-2535
  prefs: []
  type: TYPE_NORMAL
- en: ComputeGravitation_multiGPU_singlethread(
  id: totrans-2536
  prefs: []
  type: TYPE_NORMAL
- en: float *force,
  id: totrans-2537
  prefs: []
  type: TYPE_NORMAL
- en: float *posMass,
  id: totrans-2538
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  id: totrans-2539
  prefs: []
  type: TYPE_NORMAL
- en: size_t N
  id: totrans-2540
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-2541
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2542
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-2543
  prefs: []
  type: TYPE_NORMAL
- en: float ret = 0.0f;
  id: totrans-2544
  prefs: []
  type: TYPE_NORMAL
- en: float *dptrPosMass[g_maxGPUs];
  id: totrans-2545
  prefs: []
  type: TYPE_NORMAL
- en: float *dptrForce[g_maxGPUs];
  id: totrans-2546
  prefs: []
  type: TYPE_NORMAL
- en: chTimerTimestamp start, end;
  id: totrans-2547
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &start );
  id: totrans-2548
  prefs: []
  type: TYPE_NORMAL
- en: memset( dptrPosMass, 0, sizeof(dptrPosMass) );
  id: totrans-2549
  prefs: []
  type: TYPE_NORMAL
- en: memset( dptrForce, 0, sizeof(dptrForce) );
  id: totrans-2550
  prefs: []
  type: TYPE_NORMAL
- en: size_t bodiesPerGPU = N / g_numGPUs;
  id: totrans-2551
  prefs: []
  type: TYPE_NORMAL
- en: if ( (0 != N % g_numGPUs) || (g_numGPUs > g_maxGPUs) ) {
  id: totrans-2552
  prefs: []
  type: TYPE_NORMAL
- en: return 0.0f;
  id: totrans-2553
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2554
  prefs: []
  type: TYPE_NORMAL
- en: // kick off the asynchronous memcpy's - overlap GPUs pulling
  id: totrans-2555
  prefs: []
  type: TYPE_NORMAL
- en: // host memory with the CPU time needed to do the memory
  id: totrans-2556
  prefs: []
  type: TYPE_NORMAL
- en: // allocations.
  id: totrans-2557
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2558
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( i );
  id: totrans-2559
  prefs: []
  type: TYPE_NORMAL
- en: cudaMalloc( &dptrPosMass[i], 4*N*sizeof(float) );
  id: totrans-2560
  prefs: []
  type: TYPE_NORMAL
- en: cudaMalloc( &dptrForce[i], 3*bodiesPerGPU*sizeof(float) );
  id: totrans-2561
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync(
  id: totrans-2562
  prefs: []
  type: TYPE_NORMAL
- en: dptrPosMass[i],
  id: totrans-2563
  prefs: []
  type: TYPE_NORMAL
- en: g_hostAOS_PosMass,
  id: totrans-2564
  prefs: []
  type: TYPE_NORMAL
- en: 4*N*sizeof(float),
  id: totrans-2565
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice );
  id: totrans-2566
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2567
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2568
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( i ) );
  id: totrans-2569
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_Shared_device<<<
  id: totrans-2570
  prefs: []
  type: TYPE_NORMAL
- en: 300,256,256*sizeof(float4)>>>(
  id: totrans-2571
  prefs: []
  type: TYPE_NORMAL
- en: dptrForce[i],
  id: totrans-2572
  prefs: []
  type: TYPE_NORMAL
- en: dptrPosMass[i],
  id: totrans-2573
  prefs: []
  type: TYPE_NORMAL
- en: softeningSquared,
  id: totrans-2574
  prefs: []
  type: TYPE_NORMAL
- en: i*bodiesPerGPU,
  id: totrans-2575
  prefs: []
  type: TYPE_NORMAL
- en: bodiesPerGPU,
  id: totrans-2576
  prefs: []
  type: TYPE_NORMAL
- en: N );
  id: totrans-2577
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync(
  id: totrans-2578
  prefs: []
  type: TYPE_NORMAL
- en: g_hostAOS_Force+3*bodiesPerGPU*i,
  id: totrans-2579
  prefs: []
  type: TYPE_NORMAL
- en: dptrForce[i],
  id: totrans-2580
  prefs: []
  type: TYPE_NORMAL
- en: 3*bodiesPerGPU*sizeof(float),
  id: totrans-2581
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost );
  id: totrans-2582
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2583
  prefs: []
  type: TYPE_NORMAL
- en: // Synchronize with each GPU in turn.
  id: totrans-2584
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2585
  prefs: []
  type: TYPE_NORMAL
- en: cudaSetDevice( i );
  id: totrans-2586
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  id: totrans-2587
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2588
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &end );
  id: totrans-2589
  prefs: []
  type: TYPE_NORMAL
- en: ret = chTimerElapsedTime( &start, &end ) * 1000.0f;
  id: totrans-2590
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-2591
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2592
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( dptrPosMass[i] );
  id: totrans-2593
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( dptrForce[i] );
  id: totrans-2594
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2595
  prefs: []
  type: TYPE_NORMAL
- en: return ret;
  id: totrans-2596
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2597
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2598
  prefs: []
  type: TYPE_NORMAL
- en: 9.6\. Multithreaded Multi-GPU
  id: totrans-2599
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CUDA has supported multiple GPUs since the beginning, but until CUDA 4.0, each
    GPU had to be controlled by a separate CPU thread. For workloads that required
    a lot of CPU power, that requirement was never very onerous because the full power
    of modern multicore processors can be unlocked only through multithreading.
  id: totrans-2600
  prefs: []
  type: TYPE_NORMAL
- en: The multithreaded implementation of multi-GPU N-Body creates one CPU thread
    per GPU, and it delegates the dispatch and synchronization of the work for a given
    N-body pass to each thread. The main thread splits the work evenly between GPUs,
    delegates work to each worker thread by signaling an event (or a semaphore, on
    POSIX platforms such as Linux), and then waits for all of the worker threads to
    signal completion before proceeding. As the number of GPUs grows, synchronization
    overhead starts to chip away at the benefits from parallelism.
  id: totrans-2601
  prefs: []
  type: TYPE_NORMAL
- en: This implementation of N-body uses the same multithreading library as the multithreaded
    implementation of N-body, described in [Section 14.9](ch14.html#ch14lev1sec9).
    The `workerThread` class, described in [Appendix A](app01.html#app01).2, enables
    the application thread to “delegate” work to CPU threads, then synchronize on
    the worker threads’ completion of the delegated task.
  id: totrans-2602
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 9.5](ch09.html#ch09lis05) gives the host code that creates and initializes
    the CPU threads. Two globals, `g_numGPUs` and `g_GPUThreadPool`, contain the GPU
    count and a worker thread for each. After each CPU thread is created, it is initialized
    by synchronously calling the `initializeGPU()` function, which affiliates the
    CPU thread with a given GPU—an affiliation that never changes during the course
    of the application’s execution.'
  id: totrans-2603
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.5.* Multithreaded multi-GPU initialization code.'
  id: totrans-2604
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis05a)'
  id: totrans-2605
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2606
  prefs: []
  type: TYPE_NORMAL
- en: workerThread *g_CPUThreadPool;
  id: totrans-2607
  prefs: []
  type: TYPE_NORMAL
- en: int g_numCPUCores;
  id: totrans-2608
  prefs: []
  type: TYPE_NORMAL
- en: workerThread *g_GPUThreadPool;
  id: totrans-2609
  prefs: []
  type: TYPE_NORMAL
- en: int g_numGPUs;
  id: totrans-2610
  prefs: []
  type: TYPE_NORMAL
- en: struct gpuInit_struct
  id: totrans-2611
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2612
  prefs: []
  type: TYPE_NORMAL
- en: int iGPU;
  id: totrans-2613
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-2614
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  id: totrans-2615
  prefs: []
  type: TYPE_NORMAL
- en: void
  id: totrans-2616
  prefs: []
  type: TYPE_NORMAL
- en: initializeGPU( void *_p )
  id: totrans-2617
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2618
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-2619
  prefs: []
  type: TYPE_NORMAL
- en: gpuInit_struct *p = (gpuInit_struct *) _p;
  id: totrans-2620
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaSetDevice( p->iGPU ) );
  id: totrans-2621
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaSetDeviceFlags( cudaDeviceMapHost ) );
  id: totrans-2622
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaFree(0) );
  id: totrans-2623
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-2624
  prefs: []
  type: TYPE_NORMAL
- en: p->status = status;
  id: totrans-2625
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2626
  prefs: []
  type: TYPE_NORMAL
- en: // ... below is from main()
  id: totrans-2627
  prefs: []
  type: TYPE_NORMAL
- en: if ( g_numGPUs ) {
  id: totrans-2628
  prefs: []
  type: TYPE_NORMAL
- en: chCommandLineGet( &g_numGPUs, "numgpus", argc, argv );
  id: totrans-2629
  prefs: []
  type: TYPE_NORMAL
- en: g_GPUThreadPool = new workerThread[g_numGPUs];
  id: totrans-2630
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2631
  prefs: []
  type: TYPE_NORMAL
- en: if ( ! g_GPUThreadPool[i].initialize( ) ) {
  id: totrans-2632
  prefs: []
  type: TYPE_NORMAL
- en: fprintf( stderr, "Error initializing thread pool\n" );
  id: totrans-2633
  prefs: []
  type: TYPE_NORMAL
- en: return 1;
  id: totrans-2634
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2635
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2636
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2637
  prefs: []
  type: TYPE_NORMAL
- en: gpuInit_struct initGPU = {i};
  id: totrans-2638
  prefs: []
  type: TYPE_NORMAL
- en: g_GPUThreadPool[i].delegateSynchronous(
  id: totrans-2639
  prefs: []
  type: TYPE_NORMAL
- en: initializeGPU,
  id: totrans-2640
  prefs: []
  type: TYPE_NORMAL
- en: '&initGPU );'
  id: totrans-2641
  prefs: []
  type: TYPE_NORMAL
- en: if ( cudaSuccess != initGPU.status ) {
  id: totrans-2642
  prefs: []
  type: TYPE_NORMAL
- en: fprintf( stderr, "Initializing GPU %d failed "
  id: totrans-2643
  prefs: []
  type: TYPE_NORMAL
- en: '"with %d (%s)\n",'
  id: totrans-2644
  prefs: []
  type: TYPE_NORMAL
- en: i,
  id: totrans-2645
  prefs: []
  type: TYPE_NORMAL
- en: initGPU.status,
  id: totrans-2646
  prefs: []
  type: TYPE_NORMAL
- en: cudaGetErrorString( initGPU.status ) );
  id: totrans-2647
  prefs: []
  type: TYPE_NORMAL
- en: return 1;
  id: totrans-2648
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2649
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2650
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2651
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2652
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the worker threads are initialized, they suspend waiting on a thread synchronization
    primitive until the application thread dispatches work to them. [Listing 9.6](ch09.html#ch09lis06)
    shows the host code that dispatches work to the GPUs: The `gpuDelegation` structure
    encapsulates the work that a given GPU must do, and the `gpuWorkerThread` function
    is invoked for each of the worker threads created by the code in [Listing 9.5](ch09.html#ch09lis05).
    The application thread code, shown in [Listing 9.7](ch09.html#ch09lis07), creates
    a `gpuDelegation` structure for each worker thread and calls the `delegateAsynchronous()`
    method to invoke the code in [Listing 9.6](ch09.html#ch09lis06). The `waitAll()`
    method then waits until all of the worker threads have finished. The performance
    and scaling results of the single-threaded and multithreaded version of multi-GPU
    N-body are summarized in [Section 14.7](ch14.html#ch14lev1sec7).'
  id: totrans-2653
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.6.* Host code (worker thread).'
  id: totrans-2654
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis06a)'
  id: totrans-2655
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2656
  prefs: []
  type: TYPE_NORMAL
- en: struct gpuDelegation {
  id: totrans-2657
  prefs: []
  type: TYPE_NORMAL
- en: size_t i;   // base offset for this thread to process
  id: totrans-2658
  prefs: []
  type: TYPE_NORMAL
- en: size_t n;   // size of this thread's problem
  id: totrans-2659
  prefs: []
  type: TYPE_NORMAL
- en: size_t N;   // total number of bodies
  id: totrans-2660
  prefs: []
  type: TYPE_NORMAL
- en: float *hostPosMass;
  id: totrans-2661
  prefs: []
  type: TYPE_NORMAL
- en: float *hostForce;
  id: totrans-2662
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared;
  id: totrans-2663
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-2664
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  id: totrans-2665
  prefs: []
  type: TYPE_NORMAL
- en: void
  id: totrans-2666
  prefs: []
  type: TYPE_NORMAL
- en: gpuWorkerThread( void *_p )
  id: totrans-2667
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2668
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-2669
  prefs: []
  type: TYPE_NORMAL
- en: gpuDelegation *p = (gpuDelegation *) _p;
  id: totrans-2670
  prefs: []
  type: TYPE_NORMAL
- en: float *dptrPosMass = 0;
  id: totrans-2671
  prefs: []
  type: TYPE_NORMAL
- en: float *dptrForce = 0;
  id: totrans-2672
  prefs: []
  type: TYPE_NORMAL
- en: //
  id: totrans-2673
  prefs: []
  type: TYPE_NORMAL
- en: // Each GPU has its own device pointer to the host pointer.
  id: totrans-2674
  prefs: []
  type: TYPE_NORMAL
- en: //
  id: totrans-2675
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMalloc( &dptrPosMass, 4*p->N*sizeof(float) ) );
  id: totrans-2676
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMalloc( &dptrForce, 3*p->n*sizeof(float) ) );
  id: totrans-2677
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync(
  id: totrans-2678
  prefs: []
  type: TYPE_NORMAL
- en: dptrPosMass,
  id: totrans-2679
  prefs: []
  type: TYPE_NORMAL
- en: p->hostPosMass,
  id: totrans-2680
  prefs: []
  type: TYPE_NORMAL
- en: 4*p->N*sizeof(float),
  id: totrans-2681
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice ) );
  id: totrans-2682
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_multiGPU<<<300,256,256*sizeof(float4)>>>(
  id: totrans-2683
  prefs: []
  type: TYPE_NORMAL
- en: dptrForce,
  id: totrans-2684
  prefs: []
  type: TYPE_NORMAL
- en: dptrPosMass,
  id: totrans-2685
  prefs: []
  type: TYPE_NORMAL
- en: p->softeningSquared,
  id: totrans-2686
  prefs: []
  type: TYPE_NORMAL
- en: p->i,
  id: totrans-2687
  prefs: []
  type: TYPE_NORMAL
- en: p->n,
  id: totrans-2688
  prefs: []
  type: TYPE_NORMAL
- en: p->N );
  id: totrans-2689
  prefs: []
  type: TYPE_NORMAL
- en: '// NOTE: synchronous memcpy, so no need for further'
  id: totrans-2690
  prefs: []
  type: TYPE_NORMAL
- en: // synchronization with device
  id: totrans-2691
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpy(
  id: totrans-2692
  prefs: []
  type: TYPE_NORMAL
- en: p->hostForce+3*p->i,
  id: totrans-2693
  prefs: []
  type: TYPE_NORMAL
- en: dptrForce,
  id: totrans-2694
  prefs: []
  type: TYPE_NORMAL
- en: 3*p->n*sizeof(float),
  id: totrans-2695
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost ) );
  id: totrans-2696
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-2697
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( dptrPosMass );
  id: totrans-2698
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( dptrForce );
  id: totrans-2699
  prefs: []
  type: TYPE_NORMAL
- en: p->status = status;
  id: totrans-2700
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2701
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2702
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 9.7.* ?Host code (application thread)'
  id: totrans-2703
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch09_images.html#p09lis07a)'
  id: totrans-2704
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2705
  prefs: []
  type: TYPE_NORMAL
- en: float
  id: totrans-2706
  prefs: []
  type: TYPE_NORMAL
- en: ComputeGravitation_multiGPU_threaded(
  id: totrans-2707
  prefs: []
  type: TYPE_NORMAL
- en: float *force,
  id: totrans-2708
  prefs: []
  type: TYPE_NORMAL
- en: float *posMass,
  id: totrans-2709
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  id: totrans-2710
  prefs: []
  type: TYPE_NORMAL
- en: size_t N
  id: totrans-2711
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-2712
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2713
  prefs: []
  type: TYPE_NORMAL
- en: chTimerTimestamp start, end;
  id: totrans-2714
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &start );
  id: totrans-2715
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2716
  prefs: []
  type: TYPE_NORMAL
- en: gpuDelegation *pgpu = new gpuDelegation[g_numGPUs];
  id: totrans-2717
  prefs: []
  type: TYPE_NORMAL
- en: size_t bodiesPerGPU = N / g_numGPUs;
  id: totrans-2718
  prefs: []
  type: TYPE_NORMAL
- en: if ( N % g_numGPUs ) {
  id: totrans-2719
  prefs: []
  type: TYPE_NORMAL
- en: return 0.0f;
  id: totrans-2720
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2721
  prefs: []
  type: TYPE_NORMAL
- en: size_t i;
  id: totrans-2722
  prefs: []
  type: TYPE_NORMAL
- en: for ( i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2723
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].hostPosMass = g_hostAOS_PosMass;
  id: totrans-2724
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].hostForce = g_hostAOS_Force;
  id: totrans-2725
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].softeningSquared = softeningSquared;
  id: totrans-2726
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].i = bodiesPerGPU*i;
  id: totrans-2727
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].n = bodiesPerGPU;
  id: totrans-2728
  prefs: []
  type: TYPE_NORMAL
- en: pgpu[i].N = N;
  id: totrans-2729
  prefs: []
  type: TYPE_NORMAL
- en: g_GPUThreadPool[i].delegateAsynchronous(
  id: totrans-2730
  prefs: []
  type: TYPE_NORMAL
- en: gpuWorkerThread,
  id: totrans-2731
  prefs: []
  type: TYPE_NORMAL
- en: '&pgpu[i] );'
  id: totrans-2732
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2733
  prefs: []
  type: TYPE_NORMAL
- en: workerThread::waitAll( g_GPUThreadPool, g_numGPUs );
  id: totrans-2734
  prefs: []
  type: TYPE_NORMAL
- en: delete[] pgpu;
  id: totrans-2735
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2736
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &end );
  id: totrans-2737
  prefs: []
  type: TYPE_NORMAL
- en: return chTimerElapsedTime( &start, &end ) * 1000.0f;
  id: totrans-2738
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2739
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2740
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 10\. Texturing
  id: totrans-2741
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 10.1\. Overview
  id: totrans-2742
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In CUDA, a software technology for general-purpose parallel computing, texture
    support could not have been justified if the hardware hadn’t already been there,
    due to its graphics-accelerating heritage. Nevertheless, the texturing hardware
    accelerates enough useful operations that NVIDIA saw fit to include support. Although
    many CUDA applications may be built without ever using texture, some rely on it
    to be competitive with CPU-based code.
  id: totrans-2743
  prefs: []
  type: TYPE_NORMAL
- en: Texture mapping was invented to enable richer, more realistic-looking objects
    by enabling images to be “painted” onto geometry. Historically, the hardware interpolated
    texture coordinates along with the X, Y, and Z coordinates needed to render a
    triangle, and for each output pixel, the texture value was fetched (optionally
    with bilinear interpolation), processed by blending with interpolated shading
    factors, and blended into the output buffer. With the introduction of programmable
    graphics and texture-like data that might not include color data (for example,
    bump maps), graphics hardware became more sophisticated. The shader programs included
    `TEX` instructions that specified the coordinates to fetch, and the results were
    incorporated into the computations used to generate the output pixel. The hardware
    improves performance using texture caches, memory layouts optimized for dimensional
    locality, and a dedicated hardware pipeline to transform texture coordinates into
    hardware addresses.
  id: totrans-2744
  prefs: []
  type: TYPE_NORMAL
- en: Because the functionality grew organically and was informed by a combination
    of application requirements and hardware costs, the texturing features are not
    very orthogonal. For example, the “wrap” and “mirror” texture addressing modes
    do not work unless the texture coordinates are normalized. This chapter explains
    every detail of the texture hardware as supported by CUDA. We will cover everything
    from normalized versus unnormalized coordinates to addressing modes to the limits
    of linear interpolation; 1D, 2D, 3D, and layered textures; and how to use these
    features from both the CUDA runtime and the driver API.
  id: totrans-2745
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1\. Two Use Cases
  id: totrans-2746
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In CUDA, there are two significantly different uses for texture. One is to
    simply use texture as a read path: to work around coalescing constraints or to
    use the texture cache to reduce external bandwidth requirements, or both. The
    other use case takes advantage of the fixed-function hardware that the GPU has
    in place for graphics applications. The texture hardware consists of a configurable
    pipeline of computation stages that can do all of the following.'
  id: totrans-2747
  prefs: []
  type: TYPE_NORMAL
- en: • Scale normalized texture coordinates
  id: totrans-2748
  prefs: []
  type: TYPE_NORMAL
- en: • Perform boundary condition computations on the texture coordinates
  id: totrans-2749
  prefs: []
  type: TYPE_NORMAL
- en: • Convert texture coordinates to addresses with 2D or 3D locality
  id: totrans-2750
  prefs: []
  type: TYPE_NORMAL
- en: • Fetch 2, 4, or 8 texture elements for 1D, 2D, or 3D textures and linearly
    interpolate between them
  id: totrans-2751
  prefs: []
  type: TYPE_NORMAL
- en: • Convert the texture values from integers to unitized floating-point values
  id: totrans-2752
  prefs: []
  type: TYPE_NORMAL
- en: Textures are read through *texture references* that are bound to underlying
    memory (either CUDA arrays or device memory). The memory is just an unshaped bucket
    of bits; it is the texture reference that tells the hardware how to interpret
    the data and deliver it into registers when a TEX instruction is executed.
  id: totrans-2753
  prefs: []
  type: TYPE_NORMAL
- en: 10.2\. Texture Memory
  id: totrans-2754
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before describing the features of the fixed-function texturing hardware, let’s
    spend some time examining the underlying memory to which texture references may
    be bound. CUDA can texture from either device memory or CUDA arrays.
  id: totrans-2755
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1\. Device Memory
  id: totrans-2756
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In device memory, the textures are addressed in row-major order. A 1024x768
    texture might look like [Figure 10.1](ch10.html#ch10fig01), where *Offset* is
    the offset (in elements) from the base pointer of the image.
  id: totrans-2757
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 10.1)
  id: totrans-2758
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10equ01.jpg)'
  id: totrans-2759
  prefs: []
  type: TYPE_IMG
- en: For a byte offset, multiply by the size of the elements.
  id: totrans-2760
  prefs: []
  type: TYPE_NORMAL
- en: (Equation 10.2)
  id: totrans-2761
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10equ02.jpg)![Image](graphics/10fig01.jpg)'
  id: totrans-2762
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.1* 1024x768 image.'
  id: totrans-2763
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, this addressing calculation only works for the most convenient
    of texture widths: 1024 happens to be convenient because it is a power of 2 and
    conforms to all manner of alignment restrictions. To accommodate less convenient
    texture sizes, CUDA implements *pitch-linear addressing*, where the width of the
    texture memory is different from the width of the texture. For less convenient
    widths, the hardware enforces an alignment restriction and the width in elements
    is treated differently from the width of the texture memory. For a texture width
    of 950, say, and an alignment restriction of 64 bytes, the width-in-bytes is padded
    to 964 (the next multiple of 64), and the texture looks like [Figure 10.2](ch10.html#ch10fig02).'
  id: totrans-2764
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig02.jpg)'
  id: totrans-2765
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.2* 950x768 image, with pitch.'
  id: totrans-2766
  prefs: []
  type: TYPE_NORMAL
- en: In CUDA, the padded width in bytes is called the *pitch*. The total amount of
    device memory used by this image is 964x768 elements. The offset into the image
    now is computed in bytes, as follows.
  id: totrans-2767
  prefs: []
  type: TYPE_NORMAL
- en: '*ByteOffset = Y * Pitch + XInBytes*'
  id: totrans-2768
  prefs: []
  type: TYPE_NORMAL
- en: Applications can call `cudaMallocPitch()/cuMemAllocPitch()` to delegate selection
    of the pitch to the CUDA driver.^([1](ch10.html#ch10fn1)) In 3D, pitch-linear
    images of a given *Depth* are exactly like 2D images, with *Depth* 2D slices laid
    out contiguously in device memory.
  id: totrans-2769
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch10.html#ch10fn1a). Code that delegates to the driver is more future-proof
    than code that tries to perform allocations that comply with the documented alignment
    restrictions, since those restrictions are subject to change.'
  id: totrans-2770
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2\. CUDA Arrays and Block Linear Addressing
  id: totrans-2771
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CUDA arrays are designed specifically to support texturing. They are allocated
    from the same pool of physical memory as device memory, but they have an opaque
    layout and cannot be addressed with pointers. Instead, memory locations in a CUDA
    array must be identified by the array handle and a set of 1D, 2D, or 3D coordinates.
  id: totrans-2772
  prefs: []
  type: TYPE_NORMAL
- en: 'CUDA arrays perform a more complicated addressing calculation, designed so
    that contiguous addresses exhibit 2D or 3D locality. The addressing calculation
    is hardware-specific and changes from one hardware generation to the next. [Figure
    10.1](ch10.html#ch10fig01) illustrates one of the mechanisms used: The two least
    significant address bits of row and column have been interleaved before undertaking
    the addressing calculation.'
  id: totrans-2773
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in [Figure 10.3](ch10.html#ch10fig03), bit interleaving enables
    contiguous addresses to have “dimensional locality”: A cache line fill pulls in
    a block of pixels in a neighborhood rather than a horizontal span of pixels.^([2](ch10.html#ch10fn2))
    When taken to the limit, bit interleaving imposes some inconvenient requirements
    on the texture dimensions, so it is just one of several strategies used for the
    so-called “block linear” addressing calculation.'
  id: totrans-2774
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch10.html#ch10fn2a). 3D textures similarly interleave the X, Y, and Z coordinate
    bits.'
  id: totrans-2775
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig03.jpg)'
  id: totrans-2776
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.3* 1024x768 image, interleaved bits.'
  id: totrans-2777
  prefs: []
  type: TYPE_NORMAL
- en: In device memory, the location of an image element can be specified by any of
    the following.
  id: totrans-2778
  prefs: []
  type: TYPE_NORMAL
- en: • The base pointer, pitch, and a *(XInBytes, Y)* or *(XInBytes, Y, Z)* tuple
  id: totrans-2779
  prefs: []
  type: TYPE_NORMAL
- en: • The base pointer and an offset as computed by [Equation 10.1](ch10.html#ch10equ01)
  id: totrans-2780
  prefs: []
  type: TYPE_NORMAL
- en: • The device pointer with the offset already applied
  id: totrans-2781
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, when CUDA arrays do not have device memory addresses, so memory
    locations must be specified in terms of the CUDA array and a tuple *(XInBytes,
    Y)* or *(XInBytes, Y, Z)*.
  id: totrans-2782
  prefs: []
  type: TYPE_NORMAL
- en: Creating and Destroying CUDA Arrays
  id: totrans-2783
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Using the CUDA runtime, CUDA arrays may be created by calling `cudaMallocArray()`.
  id: totrans-2784
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p309pro01a)'
  id: totrans-2785
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMallocArray(struct cudaArray **array, const struct
  id: totrans-2786
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatDesc *desc, size_t width, size_t height __dv(0),
  id: totrans-2787
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int flags __dv(0));
  id: totrans-2788
  prefs: []
  type: TYPE_NORMAL
- en: '`array` passes back the array handle, and `desc` specifies the number and type
    of components (e.g., 2 floats) in each array element. `width` specifies the width
    of the array *in bytes*. `height` is an optional parameter that specifies the
    height of the array; if the height is not specified, `cudaMallocArray()` creates
    a 1D CUDA array.'
  id: totrans-2789
  prefs: []
  type: TYPE_NORMAL
- en: The `flags` parameter is used to hint at the CUDA array’s usage. As of this
    writing, the only flag is `cudaArraySurfaceLoadStore`, which must be specified
    if the CUDA array will be used for surface read/write operations as described
    later in this chapter.
  id: totrans-2790
  prefs: []
  type: TYPE_NORMAL
- en: The `__dv` macro used for the `height` and `flags` parameters causes the declaration
    to behave differently, depending on the language. When compiled for C, it becomes
    a simple parameter, but when compiled for C++, it becomes a parameter with the
    specified default value.
  id: totrans-2791
  prefs: []
  type: TYPE_NORMAL
- en: The structure `cudaChannelFormatDesc` describes the contents of a texture.
  id: totrans-2792
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaChannelFormatDesc {
  id: totrans-2793
  prefs: []
  type: TYPE_NORMAL
- en: int x, y, z, w;
  id: totrans-2794
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaChannelFormatKind f;
  id: totrans-2795
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  id: totrans-2796
  prefs: []
  type: TYPE_NORMAL
- en: The `x`, `y`, `z,` and `w` members of the structure specify the number of bits
    in each member of the texture element. For example, a 1-element float texture
    will contain `x==32` and the other elements will be `0`. The `cudaChannelFormatKind`
    structure specifies whether the data is signed integer, unsigned integer, or floating
    point.
  id: totrans-2797
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p310pro01a)'
  id: totrans-2798
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaChannelFormatKind
  id: totrans-2799
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2800
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatKindSigned = 0,
  id: totrans-2801
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatKindUnsigned = 1,
  id: totrans-2802
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatKindFloat = 2,
  id: totrans-2803
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatKindNone = 3
  id: totrans-2804
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  id: totrans-2805
  prefs: []
  type: TYPE_NORMAL
- en: Developers can create `cudaChannelFormatDesc` structures using the `cudaCreateChannelDesc`
    function.
  id: totrans-2806
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p310pro03a)'
  id: totrans-2807
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatDesc cudaCreateChannelDesc(int x, int y, int z, int w, cudaChannelFormatKind
    kind);
  id: totrans-2808
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, a templated family of functions can be invoked as follows.
  id: totrans-2809
  prefs: []
  type: TYPE_NORMAL
- en: template<class T> cudaCreateChannelDesc<T>();
  id: totrans-2810
  prefs: []
  type: TYPE_NORMAL
- en: where `T` may be any of the native formats supported by CUDA. Here are two examples
    of the specializations of this template.
  id: totrans-2811
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p311pro01a)'
  id: totrans-2812
  prefs: []
  type: TYPE_NORMAL
- en: template<> __inline__ __host__ cudaChannelFormatDesc cudaCreateChannelDesc<float>(void)
  id: totrans-2813
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2814
  prefs: []
  type: TYPE_NORMAL
- en: int e = (int)sizeof(float) * 8;
  id: totrans-2815
  prefs: []
  type: TYPE_NORMAL
- en: return cudaCreateChannelDesc(e, 0, 0, 0, cudaChannelFormatKindFloat);
  id: totrans-2816
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2817
  prefs: []
  type: TYPE_NORMAL
- en: template<> __inline__ __host__ cudaChannelFormatDesc cudaCreateChannelDesc<uint2>(void)
  id: totrans-2818
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2819
  prefs: []
  type: TYPE_NORMAL
- en: int e = (int)sizeof(unsigned int) * 8;
  id: totrans-2820
  prefs: []
  type: TYPE_NORMAL
- en: return cudaCreateChannelDesc(e, e, 0, 0, cudaChannelFormatKindUnsigned);
  id: totrans-2821
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2822
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2823
  prefs: []
  type: TYPE_NORMAL
- en: Caution
  id: totrans-2824
  prefs: []
  type: TYPE_NORMAL
- en: When using the `char` data type, be aware that some compilers assume `char`
    is signed, while others assume it is unsigned. You can always make this distinction
    unambiguous with the `signed` keyword.
  id: totrans-2825
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2826
  prefs: []
  type: TYPE_NORMAL
- en: 3D CUDA arrays may be allocated with `cudaMalloc3DArray().`
  id: totrans-2827
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p311pro03a)'
  id: totrans-2828
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMalloc3DArray(struct cudaArray** array, const struct
  id: totrans-2829
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatDesc* desc, struct cudaExtent extent, unsigned int
  id: totrans-2830
  prefs: []
  type: TYPE_NORMAL
- en: flags __dv(0));
  id: totrans-2831
  prefs: []
  type: TYPE_NORMAL
- en: Rather than taking width, height, and depth parameters, `cudaMalloc3DArray()`
    takes a `cudaExtent` structure.
  id: totrans-2832
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaExtent {
  id: totrans-2833
  prefs: []
  type: TYPE_NORMAL
- en: size_t width;
  id: totrans-2834
  prefs: []
  type: TYPE_NORMAL
- en: size_t height;
  id: totrans-2835
  prefs: []
  type: TYPE_NORMAL
- en: size_t depth;
  id: totrans-2836
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  id: totrans-2837
  prefs: []
  type: TYPE_NORMAL
- en: The `flags` parameter, like that of `cudaMallocArray()`, must be `cudaArraySurfaceLoadStore`
    if the CUDA array will be used for surface read/write operations.
  id: totrans-2838
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2839
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2840
  prefs: []
  type: TYPE_NORMAL
- en: For array handles, the CUDA runtime and driver API are compatible with one another.
    The pointer passed back by `cudaMallocArray()` can be cast to `CUarray` and passed
    to driver API functions such as `cuArrayGetDescriptor()`.
  id: totrans-2841
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2842
  prefs: []
  type: TYPE_NORMAL
- en: '*Driver API*'
  id: totrans-2843
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The driver API equivalents of `cudaMallocArray()` and `cudaMalloc3DArray()`
    are `cuArrayCreate()` and `cuArray3DCreate()`, respectively.
  id: totrans-2844
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p312pro03a)'
  id: totrans-2845
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuArrayCreate(CUarray *pHandle, const CUDA_ARRAY_DESCRIPTOR
  id: totrans-2846
  prefs: []
  type: TYPE_NORMAL
- en: '*pAllocateArray);'
  id: totrans-2847
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuArray3DCreate(CUarray *pHandle, const CUDA_ARRAY3D_
  id: totrans-2848
  prefs: []
  type: TYPE_NORMAL
- en: DESCRIPTOR *pAllocateArray);
  id: totrans-2849
  prefs: []
  type: TYPE_NORMAL
- en: '`cuArray3DCreate()` can be used to allocate 1D or 2D CUDA arrays by specifying
    0 as the height or depth, respectively. The `CUDA_ARRAY3D_DESCRIPTOR` structure
    is as follows.'
  id: totrans-2850
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p312pro01a)'
  id: totrans-2851
  prefs: []
  type: TYPE_NORMAL
- en: typedef struct CUDA_ARRAY3D_DESCRIPTOR_st
  id: totrans-2852
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2853
  prefs: []
  type: TYPE_NORMAL
- en: size_t Width;
  id: totrans-2854
  prefs: []
  type: TYPE_NORMAL
- en: size_t Height;
  id: totrans-2855
  prefs: []
  type: TYPE_NORMAL
- en: size_t Depth;
  id: totrans-2856
  prefs: []
  type: TYPE_NORMAL
- en: CUarray_format Format;
  id: totrans-2857
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int NumChannels;
  id: totrans-2858
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int Flags;
  id: totrans-2859
  prefs: []
  type: TYPE_NORMAL
- en: '} CUDA_ARRAY3D_DESCRIPTOR;'
  id: totrans-2860
  prefs: []
  type: TYPE_NORMAL
- en: 'Together, the `Format` and `NumChannels` members describe the size of each
    element of the CUDA array: `NumChannels` may be 1, 2, or 4, and `Format` specifies
    the channels’ type, as follows.'
  id: totrans-2861
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p312pro02a)'
  id: totrans-2862
  prefs: []
  type: TYPE_NORMAL
- en: typedef enum CUarray_format_enum {
  id: totrans-2863
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_UNSIGNED_INT8 = 0x01,
  id: totrans-2864
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_UNSIGNED_INT16 = 0x02,
  id: totrans-2865
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_UNSIGNED_INT32 = 0x03,
  id: totrans-2866
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_SIGNED_INT8 = 0x08,
  id: totrans-2867
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_SIGNED_INT16 = 0x09,
  id: totrans-2868
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_SIGNED_INT32 = 0x0a,
  id: totrans-2869
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_HALF = 0x10,
  id: totrans-2870
  prefs: []
  type: TYPE_NORMAL
- en: CU_AD_FORMAT_FLOAT = 0x20
  id: totrans-2871
  prefs: []
  type: TYPE_NORMAL
- en: '} CUarray_format;'
  id: totrans-2872
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2873
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-2874
  prefs: []
  type: TYPE_NORMAL
- en: The format specified in `CUDA_ARRAY3D_DESCRIPTOR` is just a convenient way to
    specify the amount of data in the CUDA array. Textures bound to the CUDA array
    can specify a different format, as long as the bytes per element is the same.
    For example, it is perfectly valid to bind a `texture<int>` reference to a CUDA
    array containing 4-component bytes (32 bits per element).
  id: totrans-2875
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2876
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes CUDA array handles are passed to subroutines that need to query the
    dimensions and/or format of the input array. The following `cuArray3DGetDescriptor()`
    function is provided for that purpose.
  id: totrans-2877
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p313pro01a)'
  id: totrans-2878
  prefs: []
  type: TYPE_NORMAL
- en: CUresult cuArray3DGetDescriptor(CUDA_ARRAY3D_DESCRIPTOR
  id: totrans-2879
  prefs: []
  type: TYPE_NORMAL
- en: '*pArrayDescriptor, CUarray hArray);'
  id: totrans-2880
  prefs: []
  type: TYPE_NORMAL
- en: Note that this function may be called on 1D and 2D CUDA arrays, even those that
    were created with `cuArrayCreate()`.
  id: totrans-2881
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3\. Device Memory versus CUDA Arrays
  id: totrans-2882
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For applications that exhibit sparse access patterns, especially patterns with
    dimensional locality (for example, computer vision applications), CUDA arrays
    are a clear win. For applications with regular access patterns, especially those
    with little to no reuse or whose reuse can be explicitly managed by the application
    in shared memory, device pointers are the obvious choice.
  id: totrans-2883
  prefs: []
  type: TYPE_NORMAL
- en: Some applications, such as image processing applications, fall into a gray area
    where the choice between device pointers and CUDA arrays is not obvious. All other
    things being equal, device memory is probably preferable to CUDA arrays, but the
    following considerations may be used to help in the decision-making process.
  id: totrans-2884
  prefs: []
  type: TYPE_NORMAL
- en: • Until CUDA 3.2, CUDA kernels could not write to CUDA arrays. They were only
    able to read from them via texture intrinsics. CUDA 3.2 added the ability for
    Fermi-class hardware to access 2D CUDA arrays via “surface read/write” intrinsics.
  id: totrans-2885
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA arrays do not consume any CUDA address space.
  id: totrans-2886
  prefs: []
  type: TYPE_NORMAL
- en: • On WDDM drivers (Windows Vista and later), the system can automatically manage
    the residence of CUDA arrays. They can be swapped into and out of device memory
    transparently, depending on whether they are needed by the CUDA kernels that are
    executing. In contrast, WDDM requires all device memory to be resident in order
    for any kernel to execute.
  id: totrans-2887
  prefs: []
  type: TYPE_NORMAL
- en: • CUDA arrays can reside only in device memory, and if the GPU contains copy
    engines, it can convert between the two representations while transferring the
    data across the bus. For some applications, keeping a pitch representation in
    host memory and a CUDA array representation in device memory is the best fit.
  id: totrans-2888
  prefs: []
  type: TYPE_NORMAL
- en: 10.3\. 1D Texturing
  id: totrans-2889
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For illustrative purposes, we will deal with 1D textures in detail and then
    expand the discussion to include 2D and 3D textures.
  id: totrans-2890
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1\. Texture Setup
  id: totrans-2891
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The data in textures can consist of 1, 2, or 4 elements of any of the following
    types.
  id: totrans-2892
  prefs: []
  type: TYPE_NORMAL
- en: • Signed or unsigned 8-, 16-, or 32-bit integers
  id: totrans-2893
  prefs: []
  type: TYPE_NORMAL
- en: • 16-bit floating-point values
  id: totrans-2894
  prefs: []
  type: TYPE_NORMAL
- en: • 32-bit floating-point values
  id: totrans-2895
  prefs: []
  type: TYPE_NORMAL
- en: In the `.cu` file (whether using the CUDA runtime or the driver API), the texture
    reference is declared as follows.
  id: totrans-2896
  prefs: []
  type: TYPE_NORMAL
- en: texture<ReturnType, Dimension, ReadMode> Name;
  id: totrans-2897
  prefs: []
  type: TYPE_NORMAL
- en: where `ReturnType` is the value returned by the texture intrinsic; `Dimension`
    is 1, 2, or 3 for 1D, 2D, or 3D, respectively; and `ReadMode` is an optional parameter
    type that defaults to `cudaReadModeElementType`. The read mode only affects integer-valued
    texture data. By default, the texture passes back integers when the texture data
    is integer-valued, promoting them to 32-bit if necessary. But when `cudaReadModeNormalizedFloat`
    is specified as the read mode, 8- or 16-bit integers can be promoted to floating-point
    values in the range [0.0, 1.0] according to the formulas in [Table 10.1](ch10.html#ch10tab01).
  id: totrans-2898
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10tab01.jpg)'
  id: totrans-2899
  prefs: []
  type: TYPE_IMG
- en: '*Table 10.1* Floating-Point Promotion (Texture)'
  id: totrans-2900
  prefs: []
  type: TYPE_NORMAL
- en: The C versions of this conversion operation are given in [Listing 10.1](ch10.html#ch10lis01).
  id: totrans-2901
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 10.1.* Texture unit floating-point conversion.'
  id: totrans-2902
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p10lis01a)'
  id: totrans-2903
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2904
  prefs: []
  type: TYPE_NORMAL
- en: float
  id: totrans-2905
  prefs: []
  type: TYPE_NORMAL
- en: TexPromoteToFloat( signed char c )
  id: totrans-2906
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2907
  prefs: []
  type: TYPE_NORMAL
- en: if ( c == (signed char) 0x80 ) {
  id: totrans-2908
  prefs: []
  type: TYPE_NORMAL
- en: return -1.0f;
  id: totrans-2909
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2910
  prefs: []
  type: TYPE_NORMAL
- en: return (float) c / 127.0f;
  id: totrans-2911
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2912
  prefs: []
  type: TYPE_NORMAL
- en: float
  id: totrans-2913
  prefs: []
  type: TYPE_NORMAL
- en: TexPromoteToFloat( short s )
  id: totrans-2914
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2915
  prefs: []
  type: TYPE_NORMAL
- en: if ( s == (short) 0x8000 ) {
  id: totrans-2916
  prefs: []
  type: TYPE_NORMAL
- en: return -1.0f;
  id: totrans-2917
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2918
  prefs: []
  type: TYPE_NORMAL
- en: return (float) s / 32767.0f;
  id: totrans-2919
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2920
  prefs: []
  type: TYPE_NORMAL
- en: float
  id: totrans-2921
  prefs: []
  type: TYPE_NORMAL
- en: TexPromoteToFloat( unsigned char uc )
  id: totrans-2922
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2923
  prefs: []
  type: TYPE_NORMAL
- en: return (float) uc / 255.0f;
  id: totrans-2924
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2925
  prefs: []
  type: TYPE_NORMAL
- en: float
  id: totrans-2926
  prefs: []
  type: TYPE_NORMAL
- en: TexPromoteToFloat( unsigned short us )
  id: totrans-2927
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2928
  prefs: []
  type: TYPE_NORMAL
- en: return (float) us / 65535.0f;
  id: totrans-2929
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2930
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-2931
  prefs: []
  type: TYPE_NORMAL
- en: Once the texture reference is declared, it can be used in kernels by invoking
    texture intrinsics. Different intrinsics are used for different types of texture,
    as shown in [Table 10.2](ch10.html#ch10tab02).
  id: totrans-2932
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10tab02.jpg)'
  id: totrans-2933
  prefs: []
  type: TYPE_IMG
- en: '*Table 10.2* Texture Intrinsics'
  id: totrans-2934
  prefs: []
  type: TYPE_NORMAL
- en: Texture references have file scope and behave similarly to global variables.
    They cannot be created, destroyed, or passed as parameters, so wrapping them in
    higher-level abstractions must be undertaken with care.
  id: totrans-2935
  prefs: []
  type: TYPE_NORMAL
- en: CUDA Runtime
  id: totrans-2936
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Before invoking a kernel that uses a texture, the texture must be *bound* to
    a CUDA array or device memory by calling `cudaBindTexture()`, `cudaBindTexture2D()`,
    or `cudaBindTextureToArray()`. Due to the language integration of the CUDA runtime,
    the texture can be referenced by name, such as the following.
  id: totrans-2937
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p316pro01a)'
  id: totrans-2938
  prefs: []
  type: TYPE_NORMAL
- en: texture<float, 2, cudaReadModeElementType> tex;
  id: totrans-2939
  prefs: []
  type: TYPE_NORMAL
- en: '...'
  id: totrans-2940
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
  id: totrans-2941
  prefs: []
  type: TYPE_NORMAL
- en: Once the texture is bound, kernels that use that texture reference will read
    from the bound memory until the texture binding is changed.
  id: totrans-2942
  prefs: []
  type: TYPE_NORMAL
- en: Driver API
  id: totrans-2943
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When a texture is declared in a `.cu` file, driver applications must query it
    using `cuModuleGetTexRef()`. In the driver API, the immutable attributes of the
    texture must be set explicitly, and they must agree with the assumptions used
    by the compiler to generate the code. For most textures, this just means the format
    must agree with the format declared in the `.cu` file; the exception is when textures
    are set up to promote integers or 16-bit floating-point values to normalized 32-bit
    floating-point values.
  id: totrans-2944
  prefs: []
  type: TYPE_NORMAL
- en: The `cuTexRefSetFormat()` function is used to specify the format of the data
    in the texture.
  id: totrans-2945
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p316pro02a)'
  id: totrans-2946
  prefs: []
  type: TYPE_NORMAL
- en: CUresult CUDAAPI cuTexRefSetFormat(CUtexref hTexRef, CUarray_format
  id: totrans-2947
  prefs: []
  type: TYPE_NORMAL
- en: fmt, int NumPackedComponents);
  id: totrans-2948
  prefs: []
  type: TYPE_NORMAL
- en: The array formats are as follows.
  id: totrans-2949
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/317tab01.jpg)'
  id: totrans-2950
  prefs: []
  type: TYPE_IMG
- en: '`NumPackedComponents` specifies the number of components in each texture element.
    It may be 1, 2, or 4\. 16-bit floats (`half`) are a special data type that are
    well suited to representing image data with high integrity.^([3](ch10.html#ch10fn3))
    With 10 bits of floating-point mantissa (effectively 11 bits of precision for
    normalized numbers), there is enough precision to represent data generated by
    most sensors, and 5 bits of exponent gives enough dynamic range to represent starlight
    and sunlight in the same image. Most floating-point architectures do not include
    native instructions to process 16-bit floats, and CUDA is no exception. The texture
    hardware promotes 16-bit floats to 32-bit floats automatically, and CUDA kernels
    can convert between 16- and 32-bit floats with the `__float2half_rn()` and `__half2float_rn()`
    intrinsics.'
  id: totrans-2951
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch10.html#ch10fn3a). [Section 8.3.4](ch08.html#ch08lev2sec13) describes
    16-bit floats in detail.'
  id: totrans-2952
  prefs: []
  type: TYPE_NORMAL
- en: 10.4\. Texture as a Read Path
  id: totrans-2953
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When using texture as a read path—that is, using the texturing hardware to get
    around awkward coalescing constraints or to take advantage of the texture cache
    as opposed to accessing hardware features such as linear interpolation—many texturing
    features are unavailable. The highlights of this usage for texture are as follows.
  id: totrans-2954
  prefs: []
  type: TYPE_NORMAL
- en: • The texture reference must be bound to device memory with `cudaBind-Texture()`
    or `cuTexRefSetAddress()`.
  id: totrans-2955
  prefs: []
  type: TYPE_NORMAL
- en: • The `tex1Dfetch()` intrinsic must be used. It takes a 27-bit integer index.^([4](ch10.html#ch10fn4))
  id: totrans-2956
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch10.html#ch10fn4a). All CUDA-capable hardware has the same 27-bit limit,
    so there is not yet any way to query a device for the limit.'
  id: totrans-2957
  prefs: []
  type: TYPE_NORMAL
- en: • `tex1Dfetch()` optionally can convert the texture contents to floating-point
    values. Integers are converted to floating-point values in the range [0.0, 1.0],
    and 16-bit floating-point values are promoted to `float`.
  id: totrans-2958
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of reading device memory via `tex1Dfetch()` are twofold. First,
    memory reads via texture do not have to conform to the coalescing constraints
    that apply when reading global memory. Second, the texture cache can be a useful
    complement to the other hardware resources, even the L2 cache on Fermi-class hardware.
    When an out-of-range index is passed to `tex1Dfetch()`, it returns 0.
  id: totrans-2959
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.1\. Increasing Effective Address Coverage
  id: totrans-2960
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since the 27-bit index specifies which texture element to fetch, and the texture
    elements may be up to 16 bytes in size, a texture being read via `tex1Dfetch()`
    can cover up to 31 bits (2^(27)+2⁴) worth of memory. One way to increase the amount
    of data being effectively covered by a texture is to use wider texture elements
    than the actual data size. For example, the application can texture from `float4`
    instead of `float`, then select the appropriate element of the `float4,` depending
    on the least significant bits of the desired index. Similar techniques can be
    applied to integer data, especially 8- or 16-bit data where global memory transactions
    are always uncoalesced. Alternatively, applications can alias multiple textures
    over different segments of the device memory and perform predicated texture fetches
    from each texture in such a way that only one of them is “live.”
  id: totrans-2961
  prefs: []
  type: TYPE_NORMAL
- en: 'Microdemo: tex1dfetch_big.cu'
  id: totrans-2962
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This program illustrates using `tex1Dfetch()` to read from large arrays using
    both multiple components per texture and multiple textures. It is invoked as follows.
  id: totrans-2963
  prefs: []
  type: TYPE_NORMAL
- en: tex1dfetch_big <NumMegabytes>
  id: totrans-2964
  prefs: []
  type: TYPE_NORMAL
- en: The application allocates the specified number of megabytes of device memory
    (or mapped pinned host memory, if the device memory allocation fails), fills the
    memory with random numbers, and uses 1-, 2-, and 4-component textures to compute
    checksums on the data. Up to four textures of `int4` can be used, enabling the
    application to texture from up to 8192M of memory.
  id: totrans-2965
  prefs: []
  type: TYPE_NORMAL
- en: For clarity, `tex1dfetch_big.cu` does not perform any fancy parallel reduction
    techniques. Each thread writes back an intermediate sum, and the final checksums
    are accumulated on the CPU. The application defines the 27-bit hardware limits.
  id: totrans-2966
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p319pro01a)'
  id: totrans-2967
  prefs: []
  type: TYPE_NORMAL
- en: '#define CUDA_LG_MAX_TEX1DFETCH_INDEX 27'
  id: totrans-2968
  prefs: []
  type: TYPE_NORMAL
- en: '#define CUDA_MAX_TEX1DFETCH_INDEX'
  id: totrans-2969
  prefs: []
  type: TYPE_NORMAL
- en: (((size_t)1<<CUDA_LG_MAX_TEX1DFETCH_INDEX)-1)
  id: totrans-2970
  prefs: []
  type: TYPE_NORMAL
- en: And it defines four textures of `int4.`
  id: totrans-2971
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p319pro02a)'
  id: totrans-2972
  prefs: []
  type: TYPE_NORMAL
- en: texture<int4, 1, cudaReadModeElementType> tex4_0;
  id: totrans-2973
  prefs: []
  type: TYPE_NORMAL
- en: texture<int4, 1, cudaReadModeElementType> tex4_1;
  id: totrans-2974
  prefs: []
  type: TYPE_NORMAL
- en: texture<int4, 1, cudaReadModeElementType> tex4_2;
  id: totrans-2975
  prefs: []
  type: TYPE_NORMAL
- en: texture<int4, 1, cudaReadModeElementType> tex4_3;
  id: totrans-2976
  prefs: []
  type: TYPE_NORMAL
- en: A device function `tex4Fetch()` takes an index and teases it apart into a texture
    ordinal and a 27-bit index to pass to `tex1Dfetch()`.
  id: totrans-2977
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p319pro03a)'
  id: totrans-2978
  prefs: []
  type: TYPE_NORMAL
- en: __device__ int4
  id: totrans-2979
  prefs: []
  type: TYPE_NORMAL
- en: tex4Fetch( size_t index )
  id: totrans-2980
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-2981
  prefs: []
  type: TYPE_NORMAL
- en: int texID = (int) (index>>CUDA_LG_MAX_TEX1DFETCH_INDEX);
  id: totrans-2982
  prefs: []
  type: TYPE_NORMAL
- en: int i = (int) (index & (CUDA_MAX_TEX1DFETCH_INDEX_SIZE_T-1));
  id: totrans-2983
  prefs: []
  type: TYPE_NORMAL
- en: int4 i4;
  id: totrans-2984
  prefs: []
  type: TYPE_NORMAL
- en: if ( texID == 0 ) {
  id: totrans-2985
  prefs: []
  type: TYPE_NORMAL
- en: i4 = tex1Dfetch( tex4_0, i );
  id: totrans-2986
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2987
  prefs: []
  type: TYPE_NORMAL
- en: else if ( texID == 1 ) {
  id: totrans-2988
  prefs: []
  type: TYPE_NORMAL
- en: i4 = tex1Dfetch( tex4_1, i );
  id: totrans-2989
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2990
  prefs: []
  type: TYPE_NORMAL
- en: else if ( texID == 2 ) {
  id: totrans-2991
  prefs: []
  type: TYPE_NORMAL
- en: i4 = tex1Dfetch( tex4_2, i );
  id: totrans-2992
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2993
  prefs: []
  type: TYPE_NORMAL
- en: else if ( texID == 3 ) {
  id: totrans-2994
  prefs: []
  type: TYPE_NORMAL
- en: i4 = tex1Dfetch( tex4_3, i );
  id: totrans-2995
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2996
  prefs: []
  type: TYPE_NORMAL
- en: return i4;
  id: totrans-2997
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-2998
  prefs: []
  type: TYPE_NORMAL
- en: This device function compiles to a small amount of code that uses four predicated
    TEX instructions, only one of which is “live.” If random access is desired, the
    application also can use predication to select from the `.x`, `.y`, `.z`, or `.w`
    component of the `int4` return value.
  id: totrans-2999
  prefs: []
  type: TYPE_NORMAL
- en: Binding the textures, shown in [Listing 10.2](ch10.html#ch10lis02), is a slightly
    tricky business. This code creates two small arrays `texSizes[]` and `texBases[]`
    and sets them up to cover the device memory range. The `for` loop ensures that
    all four textures have a valid binding, even if fewer than four are needed to
    map the device memory.
  id: totrans-3000
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 10.2.* `tex1dfetch_big.cu` (excerpt).'
  id: totrans-3001
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p10lis02a)'
  id: totrans-3002
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3003
  prefs: []
  type: TYPE_NORMAL
- en: int iTexture;
  id: totrans-3004
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatDesc int4Desc = cudaCreateChannelDesc<int4>();
  id: totrans-3005
  prefs: []
  type: TYPE_NORMAL
- en: size_t numInt4s = numBytes / sizeof(int4);
  id: totrans-3006
  prefs: []
  type: TYPE_NORMAL
- en: int numTextures = (numInt4s+CUDA_MAX_TEX1DFETCH_INDEX)>>
  id: totrans-3007
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_LG_MAX_TEX1DFETCH_INDEX;
  id: totrans-3008
  prefs: []
  type: TYPE_NORMAL
- en: size_t Remainder = numBytes & (CUDA_MAX_BYTES_INT4-1);
  id: totrans-3009
  prefs: []
  type: TYPE_NORMAL
- en: if ( ! Remainder ) {
  id: totrans-3010
  prefs: []
  type: TYPE_NORMAL
- en: Remainder = CUDA_MAX_BYTES_INT4;
  id: totrans-3011
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3012
  prefs: []
  type: TYPE_NORMAL
- en: size_t texSizes[4];
  id: totrans-3013
  prefs: []
  type: TYPE_NORMAL
- en: char *texBases[4];
  id: totrans-3014
  prefs: []
  type: TYPE_NORMAL
- en: for ( iTexture = 0; iTexture < numTextures; iTexture++ ) {
  id: totrans-3015
  prefs: []
  type: TYPE_NORMAL
- en: texBases[iTexture] = deviceTex+iTexture*CUDA_MAX_BYTES_INT4;
  id: totrans-3016
  prefs: []
  type: TYPE_NORMAL
- en: texSizes[iTexture] = CUDA_MAX_BYTES_INT4;
  id: totrans-3017
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3018
  prefs: []
  type: TYPE_NORMAL
- en: texSizes[iTexture-1] = Remainder;
  id: totrans-3019
  prefs: []
  type: TYPE_NORMAL
- en: while ( iTexture < 4 ) {
  id: totrans-3020
  prefs: []
  type: TYPE_NORMAL
- en: texBases[iTexture] = texBases[iTexture-1];
  id: totrans-3021
  prefs: []
  type: TYPE_NORMAL
- en: texSizes[iTexture] = texSizes[iTexture-1];
  id: totrans-3022
  prefs: []
  type: TYPE_NORMAL
- en: iTexture++;
  id: totrans-3023
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3024
  prefs: []
  type: TYPE_NORMAL
- en: cudaBindTexture( NULL, tex4_0, texBases[0], int4Desc, texSizes[0] );
  id: totrans-3025
  prefs: []
  type: TYPE_NORMAL
- en: cudaBindTexture( NULL, tex4_1, texBases[1], int4Desc, texSizes[1] );
  id: totrans-3026
  prefs: []
  type: TYPE_NORMAL
- en: cudaBindTexture( NULL, tex4_2, texBases[2], int4Desc, texSizes[2] );
  id: totrans-3027
  prefs: []
  type: TYPE_NORMAL
- en: cudaBindTexture( NULL, tex4_3, texBases[3], int4Desc, texSizes[3] );
  id: totrans-3028
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3029
  prefs: []
  type: TYPE_NORMAL
- en: Once compiled and run, the application can be invoked with different sizes to
    see the effects. On a CG1 instance running in Amazon’s EC2 cloud compute offering,
    invocations with 512M, 768M, 1280M, and 8192M worked as follows.
  id: totrans-3030
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p321pro01a)'
  id: totrans-3031
  prefs: []
  type: TYPE_NORMAL
- en: $ ./tex1dfetch_big 512
  id: totrans-3032
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected checksum: 0x7b7c8cd3'
  id: totrans-3033
  prefs: []
  type: TYPE_NORMAL
- en: 'tex1 checksum: 0x7b7c8cd3'
  id: totrans-3034
  prefs: []
  type: TYPE_NORMAL
- en: 'tex2 checksum: 0x7b7c8cd3'
  id: totrans-3035
  prefs: []
  type: TYPE_NORMAL
- en: 'tex4 checksum: 0x7b7c8cd3'
  id: totrans-3036
  prefs: []
  type: TYPE_NORMAL
- en: $ ./tex1dfetch_big 768
  id: totrans-3037
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected checksum: 0x559a1431'
  id: totrans-3038
  prefs: []
  type: TYPE_NORMAL
- en: 'tex1 checksum: (not performed)'
  id: totrans-3039
  prefs: []
  type: TYPE_NORMAL
- en: 'tex2 checksum: 0x559a1431'
  id: totrans-3040
  prefs: []
  type: TYPE_NORMAL
- en: 'tex4 checksum: 0x559a1431'
  id: totrans-3041
  prefs: []
  type: TYPE_NORMAL
- en: $ ./tex1dfetch_big 1280
  id: totrans-3042
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected checksum: 0x66a4f9d9'
  id: totrans-3043
  prefs: []
  type: TYPE_NORMAL
- en: 'tex1 checksum: (not performed)'
  id: totrans-3044
  prefs: []
  type: TYPE_NORMAL
- en: 'tex2 checksum: (not performed)'
  id: totrans-3045
  prefs: []
  type: TYPE_NORMAL
- en: 'tex4 checksum: 0x66a4f9d9'
  id: totrans-3046
  prefs: []
  type: TYPE_NORMAL
- en: $ ./tex1dfetch_big 8192
  id: totrans-3047
  prefs: []
  type: TYPE_NORMAL
- en: Device alloc of 8192 Mb failed, trying mapped host memory
  id: totrans-3048
  prefs: []
  type: TYPE_NORMAL
- en: 'Expected checksum: 0xf049c607'
  id: totrans-3049
  prefs: []
  type: TYPE_NORMAL
- en: 'tex1 checksum: (not performed)'
  id: totrans-3050
  prefs: []
  type: TYPE_NORMAL
- en: 'tex2 checksum: (not performed)'
  id: totrans-3051
  prefs: []
  type: TYPE_NORMAL
- en: 'tex4 checksum: 0xf049c607'
  id: totrans-3052
  prefs: []
  type: TYPE_NORMAL
- en: Each `int4` texture can “only” read 2G, so invoking the program with numbers
    greater than 8192 causes it to fail. This application highlights the demand for
    indexed textures, where the texture being fetched can be specified as a parameter
    at runtime, but CUDA does not expose support for this feature.
  id: totrans-3053
  prefs: []
  type: TYPE_NORMAL
- en: 10.4.2\. Texturing from Host Memory
  id: totrans-3054
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using texture as a read path, applications can read from host memory by allocating
    mapped pinned memory, fetching the device pointer, and then specifying that device
    pointer to `cudaBindAddress()` or `cuTexRefSetAddress()`. The capability is there,
    but reading host memory via texture is *slow*. Tesla-class hardware can texture
    over PCI Express at about 2G/s, and Fermi hardware is much slower. You need some
    other reason to do it, such as code simplicity.
  id: totrans-3055
  prefs: []
  type: TYPE_NORMAL
- en: 'Microdemo: tex1dfetch_int2float.cu'
  id: totrans-3056
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: This code fragment uses texture-as-a-read path and texturing from host memory
    to confirm that the `TexPromoteToFloat()` functions work properly. The CUDA kernel
    that we will use for this purpose is a straightforward, blocking-agnostic implementation
    of a memcpy function that reads from the texture and writes to device memory.
  id: totrans-3057
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p321pro02a)'
  id: totrans-3058
  prefs: []
  type: TYPE_NORMAL
- en: texture<signed char, 1, cudaReadModeNormalizedFloat> tex;
  id: totrans-3059
  prefs: []
  type: TYPE_NORMAL
- en: extern "C" __global__ void
  id: totrans-3060
  prefs: []
  type: TYPE_NORMAL
- en: TexReadout( float *out, size_t N )
  id: totrans-3061
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3062
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3063
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  id: totrans-3064
  prefs: []
  type: TYPE_NORMAL
- en: i += gridDim.x*blockDim.x )
  id: totrans-3065
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3066
  prefs: []
  type: TYPE_NORMAL
- en: out[i] = tex1Dfetch( tex, i );
  id: totrans-3067
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3068
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3069
  prefs: []
  type: TYPE_NORMAL
- en: 'Since promoting integers to floating point only works on 8- and 16-bit values,
    we can test every possible conversion by allocating a small buffer, texturing
    from it, and confirming that the output meets our expectations. [Listing 10.3](ch10.html#ch10lis03)
    gives an excerpt from `tex1dfetch_int2float.cu`. Two host buffers are allocated:
    `inHost` holds the input buffer of 256 or 65536 input values, and `fOutHost` holds
    the corresponding float-valued outputs. The device pointers corresponding to these
    mapped host pointers are fetched into `inDevice` and `foutDevice`.'
  id: totrans-3070
  prefs: []
  type: TYPE_NORMAL
- en: The input values are initialized to every possible value of the type to be tested,
    and then the input device pointer is bound to the texture reference using `cudaBindTexture()`.
    The `TexReadout()` kernel is then invoked to read each value from the input texture
    and write as output the values returned by `tex1Dfetch()`. In this case, both
    the input and output buffers reside in mapped host memory. Because the kernel
    is writing directly to host memory, we must call `cudaDeviceSynchronize()` to
    make sure there are no race conditions between the CPU and GPU. At the end of
    the function, we call the `TexPromoteToFloat()` specialization corresponding to
    the type being tested and confirm that it is equal to the value returned by the
    kernel. If all tests pass, the function returns `true`; if any API functions or
    comparisons fail, it returns `false`.
  id: totrans-3071
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 10.3.* `tex1d_int2float.cu` (excerpt).'
  id: totrans-3072
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p10lis03a)'
  id: totrans-3073
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3074
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  id: totrans-3075
  prefs: []
  type: TYPE_NORMAL
- en: void
  id: totrans-3076
  prefs: []
  type: TYPE_NORMAL
- en: CheckTexPromoteToFloat( size_t N )
  id: totrans-3077
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3078
  prefs: []
  type: TYPE_NORMAL
- en: T *inHost, *inDevice;
  id: totrans-3079
  prefs: []
  type: TYPE_NORMAL
- en: float *foutHost, *foutDevice;
  id: totrans-3080
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-3081
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostAlloc( (void **) &inHost,
  id: totrans-3082
  prefs: []
  type: TYPE_NORMAL
- en: N*sizeof(T),
  id: totrans-3083
  prefs: []
  type: TYPE_NORMAL
- en: cudaHostAllocMapped));
  id: totrans-3084
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostGetDevicePointer( (void **) &inDevice,
  id: totrans-3085
  prefs: []
  type: TYPE_NORMAL
- en: inHost,
  id: totrans-3086
  prefs: []
  type: TYPE_NORMAL
- en: 0 ));
  id: totrans-3087
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostAlloc( (void **) &foutHost,
  id: totrans-3088
  prefs: []
  type: TYPE_NORMAL
- en: N*sizeof(float),
  id: totrans-3089
  prefs: []
  type: TYPE_NORMAL
- en: cudaHostAllocMapped));
  id: totrans-3090
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostGetDevicePointer( (void **) &foutDevice,
  id: totrans-3091
  prefs: []
  type: TYPE_NORMAL
- en: foutHost,
  id: totrans-3092
  prefs: []
  type: TYPE_NORMAL
- en: 0 ));
  id: totrans-3093
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < N; i++ ) {
  id: totrans-3094
  prefs: []
  type: TYPE_NORMAL
- en: inHost[i] = (T) i;
  id: totrans-3095
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3096
  prefs: []
  type: TYPE_NORMAL
- en: memset( foutHost, 0, N*sizeof(float) );
  id: totrans-3097
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaBindTexture( NULL,
  id: totrans-3098
  prefs: []
  type: TYPE_NORMAL
- en: tex,
  id: totrans-3099
  prefs: []
  type: TYPE_NORMAL
- en: inDevice,
  id: totrans-3100
  prefs: []
  type: TYPE_NORMAL
- en: cudaCreateChannelDesc<T>(),
  id: totrans-3101
  prefs: []
  type: TYPE_NORMAL
- en: N*sizeof(T)));
  id: totrans-3102
  prefs: []
  type: TYPE_NORMAL
- en: TexReadout<<<2,384>>>( foutDevice, N );
  id: totrans-3103
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaDeviceSynchronize());
  id: totrans-3104
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < N; i++ ) {
  id: totrans-3105
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%.2f ", foutHost[i] );
  id: totrans-3106
  prefs: []
  type: TYPE_NORMAL
- en: assert( foutHost[i] == TexPromoteToFloat( (T) i ) );
  id: totrans-3107
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3108
  prefs: []
  type: TYPE_NORMAL
- en: printf( "\n" );
  id: totrans-3109
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-3110
  prefs: []
  type: TYPE_NORMAL
- en: cudaFreeHost( inHost );
  id: totrans-3111
  prefs: []
  type: TYPE_NORMAL
- en: cudaFreeHost( foutHost );
  id: totrans-3112
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3113
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3114
  prefs: []
  type: TYPE_NORMAL
- en: 10.5\. Texturing with Unnormalized Coordinates
  id: totrans-3115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All texture intrinsics except `tex1Dfetch()` use floating-point values to specify
    coordinates into the texture. When using *unnormalized coordinates*, they fall
    in the range [0, *MaxDim*), where *MaxDim* is the width, height, or depth of the
    texture. Unnormalized coordinates are an intuitive way to index into a texture,
    but some texturing features are not available when using them.
  id: totrans-3116
  prefs: []
  type: TYPE_NORMAL
- en: An easy way to study texturing behavior is to populate a texture with elements
    that contain the index into the texture. [Figure 10.4](ch10.html#ch10fig04) shows
    a float-valued 1D texture with 16 elements, populated by the identity elements
    and annotated with some of the values returned by `tex1D()`.
  id: totrans-3117
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig04.jpg)'
  id: totrans-3118
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.4* Texturing with unnormalized coordinates (without linear filtering).'
  id: totrans-3119
  prefs: []
  type: TYPE_NORMAL
- en: 'Not all texturing features are available with unnormalized coordinates, but
    they can be used in conjunction with *linear filtering* and a limited form of
    *texture addressing*. The texture addressing mode specifies how the hardware should
    deal with out-of-range texture coordinates. For unnormalized coordinates, the
    [Figure 10.4](ch10.html#ch10fig04) illustrates the default texture addressing
    mode of clamping to the range [0, *MaxDim*) before fetching data from the texture:
    The value 16.0 is out of range and clamped to fetch the value 15.0\. Another texture
    addressing option available when using unnormalized coordinates is the “border”
    addressing mode where out-of-range coordinates return zero.'
  id: totrans-3120
  prefs: []
  type: TYPE_NORMAL
- en: The default filtering mode, so-called “point filtering,” returns one texture
    element depending on the value of the floating-point coordinate. In contrast,
    linear filtering causes the texture hardware to fetch the two neighboring texture
    elements and linearly interpolate between them, weighted by the texture coordinate.
    [Figure 10.5](ch10.html#ch10fig05) shows the 1D texture with 16 elements, with
    some sample values returned by `tex1D()`. Note that you must add `0.5f` to the
    texture coordinate to get the identity element.
  id: totrans-3121
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig05.jpg)'
  id: totrans-3122
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.5* Texturing with unnormalized coordinates (with linear filtering).'
  id: totrans-3123
  prefs: []
  type: TYPE_NORMAL
- en: Many texturing features can be used in conjunction with one another; for example,
    linear filtering can be combined with the previously discussed promotion from
    integer to floating point. In that case, the floating-point outputs produced by
    `tex1D()` intrinsics are accurate interpolations between the promoted floating-point
    values of the two participating texture elements.
  id: totrans-3124
  prefs: []
  type: TYPE_NORMAL
- en: '*Microdemo:* `tex1d_unnormalized.cu`'
  id: totrans-3125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The microdemo `tex1d_unnormalized.cu` is like a microscope to closely examine
    texturing behavior by printing the coordinate and the value returned by the `tex1D()`
    intrinsic together. Unlike the `tex1dfetch_int2float.cu` microdemo, this program
    uses a 1D CUDA array to hold the texture data. A certain number of texture fetches
    is performed, along a range of floating-point values specified by a base and increment;
    the interpolated values and the value returned by `tex1D()` are written together
    into an output array of `float2`. The CUDA kernel is as follows.
  id: totrans-3126
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p325pro01a)'
  id: totrans-3127
  prefs: []
  type: TYPE_NORMAL
- en: texture<float, 1> tex;
  id: totrans-3128
  prefs: []
  type: TYPE_NORMAL
- en: extern "C" __global__ void
  id: totrans-3129
  prefs: []
  type: TYPE_NORMAL
- en: TexReadout( float2 *out, size_t N, float base, float increment )
  id: totrans-3130
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3131
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3132
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  id: totrans-3133
  prefs: []
  type: TYPE_NORMAL
- en: i += gridDim.x*blockDim.x )
  id: totrans-3134
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3135
  prefs: []
  type: TYPE_NORMAL
- en: float x = base + (float) i * increment;
  id: totrans-3136
  prefs: []
  type: TYPE_NORMAL
- en: out[i].x = x;
  id: totrans-3137
  prefs: []
  type: TYPE_NORMAL
- en: out[i].y = tex1D( tex, x );
  id: totrans-3138
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3139
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3140
  prefs: []
  type: TYPE_NORMAL
- en: A host function `CreateAndPrintTex()`, given in [Listing 10.4](ch10.html#ch10lis04),
    takes the size of the texture to create, the number of texture fetches to perform,
    the base and increment of the floating-point range to pass to `tex1D()`, and optionally
    the filter and addressing modes to use on the texture. This function creates the
    CUDA array to hold the texture data, optionally initializes it with the caller-provided
    data (or identity elements if the caller passes NULL), binds the texture to the
    CUDA array, and prints the `float2` output.
  id: totrans-3141
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 10.4.* `CreateAndPrintTex().`'
  id: totrans-3142
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p10lis04a)'
  id: totrans-3143
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3144
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  id: totrans-3145
  prefs: []
  type: TYPE_NORMAL
- en: void
  id: totrans-3146
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex( T *initTex, size_t texN, size_t outN,
  id: totrans-3147
  prefs: []
  type: TYPE_NORMAL
- en: float base, float increment,
  id: totrans-3148
  prefs: []
  type: TYPE_NORMAL
- en: cudaTextureFilterMode filterMode = cudaFilterModePoint,
  id: totrans-3149
  prefs: []
  type: TYPE_NORMAL
- en: cudaTextureAddressMode addressMode = cudaAddressModeClamp )
  id: totrans-3150
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3151
  prefs: []
  type: TYPE_NORMAL
- en: T *texContents = 0;
  id: totrans-3152
  prefs: []
  type: TYPE_NORMAL
- en: cudaArray *texArray = 0;
  id: totrans-3153
  prefs: []
  type: TYPE_NORMAL
- en: float2 *outHost = 0, *outDevice = 0;
  id: totrans-3154
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-3155
  prefs: []
  type: TYPE_NORMAL
- en: cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc<T>();
  id: totrans-3156
  prefs: []
  type: TYPE_NORMAL
- en: // use caller-provided array, if any, to initialize texture
  id: totrans-3157
  prefs: []
  type: TYPE_NORMAL
- en: if ( initTex ) {
  id: totrans-3158
  prefs: []
  type: TYPE_NORMAL
- en: texContents = initTex;
  id: totrans-3159
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3160
  prefs: []
  type: TYPE_NORMAL
- en: else {
  id: totrans-3161
  prefs: []
  type: TYPE_NORMAL
- en: // default is to initialize with identity elements
  id: totrans-3162
  prefs: []
  type: TYPE_NORMAL
- en: texContents = (T *) malloc( texN*sizeof(T) );
  id: totrans-3163
  prefs: []
  type: TYPE_NORMAL
- en: if ( ! texContents )
  id: totrans-3164
  prefs: []
  type: TYPE_NORMAL
- en: goto Error;
  id: totrans-3165
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < texN; i++ ) {
  id: totrans-3166
  prefs: []
  type: TYPE_NORMAL
- en: texContents[i] = (T) i;
  id: totrans-3167
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3168
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3169
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaMallocArray(&texArray, &channelDesc, texN));
  id: totrans-3170
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostAlloc( (void **) &outHost,
  id: totrans-3171
  prefs: []
  type: TYPE_NORMAL
- en: outN*sizeof(float2),
  id: totrans-3172
  prefs: []
  type: TYPE_NORMAL
- en: cudaHostAllocMapped));
  id: totrans-3173
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostGetDevicePointer( (void **)
  id: totrans-3174
  prefs: []
  type: TYPE_NORMAL
- en: '&outDevice,'
  id: totrans-3175
  prefs: []
  type: TYPE_NORMAL
- en: outHost, 0 ));
  id: totrans-3176
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaMemcpyToArray( texArray,
  id: totrans-3177
  prefs: []
  type: TYPE_NORMAL
- en: 0, 0,
  id: totrans-3178
  prefs: []
  type: TYPE_NORMAL
- en: texContents,
  id: totrans-3179
  prefs: []
  type: TYPE_NORMAL
- en: texN*sizeof(T),
  id: totrans-3180
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice));
  id: totrans-3181
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
  id: totrans-3182
  prefs: []
  type: TYPE_NORMAL
- en: tex.filterMode = filterMode;
  id: totrans-3183
  prefs: []
  type: TYPE_NORMAL
- en: tex.addressMode[0] = addressMode;
  id: totrans-3184
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaHostGetDevicePointer(&outDevice, outHost, 0));
  id: totrans-3185
  prefs: []
  type: TYPE_NORMAL
- en: TexReadout<<<2,384>>>( outDevice, outN, base, increment );
  id: totrans-3186
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaThreadSynchronize());
  id: totrans-3187
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < outN; i++ ) {
  id: totrans-3188
  prefs: []
  type: TYPE_NORMAL
- en: printf( "(%.2f, %.2f)\n", outHost[i].x, outHost[i].y );
  id: totrans-3189
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3190
  prefs: []
  type: TYPE_NORMAL
- en: printf( "\n" );
  id: totrans-3191
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-3192
  prefs: []
  type: TYPE_NORMAL
- en: if ( ! initTex ) free( texContents );
  id: totrans-3193
  prefs: []
  type: TYPE_NORMAL
- en: if ( texArray ) cudaFreeArray( texArray );
  id: totrans-3194
  prefs: []
  type: TYPE_NORMAL
- en: if ( outHost ) cudaFreeHost( outHost );
  id: totrans-3195
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3196
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3197
  prefs: []
  type: TYPE_NORMAL
- en: The `main()` function for this program is intended to be modified to study texturing
    behavior. This version creates an 8-element texture and writes the output of `tex1D()`
    from 0.0 .. 7.0.
  id: totrans-3198
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p327pro01a)'
  id: totrans-3199
  prefs: []
  type: TYPE_NORMAL
- en: int
  id: totrans-3200
  prefs: []
  type: TYPE_NORMAL
- en: main( int argc, char *argv[] )
  id: totrans-3201
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3202
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-3203
  prefs: []
  type: TYPE_NORMAL
- en: CUDA_CHECK(cudaSetDeviceFlags(cudaDeviceMapHost));
  id: totrans-3204
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( NULL, 8, 8, 0.0f, 1.0f );
  id: totrans-3205
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( NULL, 8, 8, 0.0f, 1.0f, cudaFilterModeLinear );
  id: totrans-3206
  prefs: []
  type: TYPE_NORMAL
- en: return 0;
  id: totrans-3207
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3208
  prefs: []
  type: TYPE_NORMAL
- en: The output from this program is as follows.
  id: totrans-3209
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p327pro02a)'
  id: totrans-3210
  prefs: []
  type: TYPE_NORMAL
- en: (0.00, 0.00)    <- output from the first CreateAndPrintTex()
  id: totrans-3211
  prefs: []
  type: TYPE_NORMAL
- en: (1.00, 1.00)
  id: totrans-3212
  prefs: []
  type: TYPE_NORMAL
- en: (2.00, 2.00)
  id: totrans-3213
  prefs: []
  type: TYPE_NORMAL
- en: (3.00, 3.00)
  id: totrans-3214
  prefs: []
  type: TYPE_NORMAL
- en: (4.00, 4.00)
  id: totrans-3215
  prefs: []
  type: TYPE_NORMAL
- en: (5.00, 5.00)
  id: totrans-3216
  prefs: []
  type: TYPE_NORMAL
- en: (6.00, 6.00)
  id: totrans-3217
  prefs: []
  type: TYPE_NORMAL
- en: (7.00, 7.00)
  id: totrans-3218
  prefs: []
  type: TYPE_NORMAL
- en: (0.00, 0.00)    <- output from the second CreateAndPrintTex()
  id: totrans-3219
  prefs: []
  type: TYPE_NORMAL
- en: (1.00, 0.50)
  id: totrans-3220
  prefs: []
  type: TYPE_NORMAL
- en: (2.00, 1.50)
  id: totrans-3221
  prefs: []
  type: TYPE_NORMAL
- en: (3.00, 2.50)
  id: totrans-3222
  prefs: []
  type: TYPE_NORMAL
- en: (4.00, 3.50)
  id: totrans-3223
  prefs: []
  type: TYPE_NORMAL
- en: (5.00, 4.50)
  id: totrans-3224
  prefs: []
  type: TYPE_NORMAL
- en: (6.00, 5.50)
  id: totrans-3225
  prefs: []
  type: TYPE_NORMAL
- en: (7.00, 6.50)
  id: totrans-3226
  prefs: []
  type: TYPE_NORMAL
- en: If we change `main()` to invoke `CreateAndPrintTex()` as follows.
  id: totrans-3227
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p327pro03a)'
  id: totrans-3228
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( NULL, 8, 20, 0.9f, 0.01f,
  id: totrans-3229
  prefs: []
  type: TYPE_NORMAL
- en: cudaFilterModePoint );
  id: totrans-3230
  prefs: []
  type: TYPE_NORMAL
- en: The resulting output highlights that when point filtering, 1.0 is the dividing
    line between texture elements 0 and 1.
  id: totrans-3231
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p327pro04a)'
  id: totrans-3232
  prefs: []
  type: TYPE_NORMAL
- en: (0.90, 0.00)
  id: totrans-3233
  prefs: []
  type: TYPE_NORMAL
- en: (0.91, 0.00)
  id: totrans-3234
  prefs: []
  type: TYPE_NORMAL
- en: (0.92, 0.00)
  id: totrans-3235
  prefs: []
  type: TYPE_NORMAL
- en: (0.93, 0.00)
  id: totrans-3236
  prefs: []
  type: TYPE_NORMAL
- en: (0.94, 0.00)
  id: totrans-3237
  prefs: []
  type: TYPE_NORMAL
- en: (0.95, 0.00)
  id: totrans-3238
  prefs: []
  type: TYPE_NORMAL
- en: (0.96, 0.00)
  id: totrans-3239
  prefs: []
  type: TYPE_NORMAL
- en: (0.97, 0.00)
  id: totrans-3240
  prefs: []
  type: TYPE_NORMAL
- en: (0.98, 0.00)
  id: totrans-3241
  prefs: []
  type: TYPE_NORMAL
- en: (0.99, 0.00)
  id: totrans-3242
  prefs: []
  type: TYPE_NORMAL
- en: (1.00, 1.00)     <- transition point
  id: totrans-3243
  prefs: []
  type: TYPE_NORMAL
- en: (1.01, 1.00)
  id: totrans-3244
  prefs: []
  type: TYPE_NORMAL
- en: (1.02, 1.00)
  id: totrans-3245
  prefs: []
  type: TYPE_NORMAL
- en: (1.03, 1.00)
  id: totrans-3246
  prefs: []
  type: TYPE_NORMAL
- en: (1.04, 1.00)
  id: totrans-3247
  prefs: []
  type: TYPE_NORMAL
- en: (1.05, 1.00)
  id: totrans-3248
  prefs: []
  type: TYPE_NORMAL
- en: (1.06, 1.00)
  id: totrans-3249
  prefs: []
  type: TYPE_NORMAL
- en: (1.07, 1.00)
  id: totrans-3250
  prefs: []
  type: TYPE_NORMAL
- en: (1.08, 1.00)
  id: totrans-3251
  prefs: []
  type: TYPE_NORMAL
- en: (1.09, 1.00)
  id: totrans-3252
  prefs: []
  type: TYPE_NORMAL
- en: One limitation of linear filtering is that it is performed with 9-bit weighting
    factors. It is important to realize that the precision of the interpolation depends
    not on that of the texture elements but on the weights. As an example, let’s take
    a look at a 10-element texture initialized with normalized identity elements—that
    is, (0.0, 0.1, 0.2, 0.3, . . . 0.9) instead of (0, 1, 2, . . . 9). `CreateAndPrintTex()`
    lets us specify the texture contents, so we can do so as follows.
  id: totrans-3253
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p328pro01a)'
  id: totrans-3254
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3255
  prefs: []
  type: TYPE_NORMAL
- en: float texData[10];
  id: totrans-3256
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < 10; i++ ) {
  id: totrans-3257
  prefs: []
  type: TYPE_NORMAL
- en: texData[i] = (float) i / 10.0f;
  id: totrans-3258
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3259
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( texData, 10, 10, 0.0f, 1.0f );
  id: totrans-3260
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3261
  prefs: []
  type: TYPE_NORMAL
- en: The output from an unmodified `CreateAndPrintTex()` looks innocuous enough.
  id: totrans-3262
  prefs: []
  type: TYPE_NORMAL
- en: (0.00, 0.00)
  id: totrans-3263
  prefs: []
  type: TYPE_NORMAL
- en: (1.00, 0.10)
  id: totrans-3264
  prefs: []
  type: TYPE_NORMAL
- en: (2.00, 0.20)
  id: totrans-3265
  prefs: []
  type: TYPE_NORMAL
- en: (3.00, 0.30)
  id: totrans-3266
  prefs: []
  type: TYPE_NORMAL
- en: (4.00, 0.40)
  id: totrans-3267
  prefs: []
  type: TYPE_NORMAL
- en: (5.00, 0.50)
  id: totrans-3268
  prefs: []
  type: TYPE_NORMAL
- en: (6.00, 0.60)
  id: totrans-3269
  prefs: []
  type: TYPE_NORMAL
- en: (7.00, 0.70)
  id: totrans-3270
  prefs: []
  type: TYPE_NORMAL
- en: (8.00, 0.80)
  id: totrans-3271
  prefs: []
  type: TYPE_NORMAL
- en: (9.00, 0.90)
  id: totrans-3272
  prefs: []
  type: TYPE_NORMAL
- en: Or if we invoke `CreateAndPrintTex()` with linear interpolation between the
    first two texture elements (values `0.1` and `0.2`), we get the following.
  id: totrans-3273
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>(tex,10,10,1.5f,0.1f,cudaFilterModeLinear);
  id: totrans-3274
  prefs: []
  type: TYPE_NORMAL
- en: The resulting output is as follows.
  id: totrans-3275
  prefs: []
  type: TYPE_NORMAL
- en: (1.50, 0.10)
  id: totrans-3276
  prefs: []
  type: TYPE_NORMAL
- en: (1.60, 0.11)
  id: totrans-3277
  prefs: []
  type: TYPE_NORMAL
- en: (1.70, 0.12)
  id: totrans-3278
  prefs: []
  type: TYPE_NORMAL
- en: (1.80, 0.13)
  id: totrans-3279
  prefs: []
  type: TYPE_NORMAL
- en: (1.90, 0.14)
  id: totrans-3280
  prefs: []
  type: TYPE_NORMAL
- en: (2.00, 0.15)
  id: totrans-3281
  prefs: []
  type: TYPE_NORMAL
- en: (2.10, 0.16)
  id: totrans-3282
  prefs: []
  type: TYPE_NORMAL
- en: (2.20, 0.17)
  id: totrans-3283
  prefs: []
  type: TYPE_NORMAL
- en: (2.30, 0.18)
  id: totrans-3284
  prefs: []
  type: TYPE_NORMAL
- en: (2.40, 0.19)
  id: totrans-3285
  prefs: []
  type: TYPE_NORMAL
- en: Rounded to 2 decimal places, this data looks very well behaved. But if we modify
    `CreateAndPrintTex()` to output hexadecimal instead, the output becomes
  id: totrans-3286
  prefs: []
  type: TYPE_NORMAL
- en: (1.50, 0x3dcccccd)
  id: totrans-3287
  prefs: []
  type: TYPE_NORMAL
- en: (1.60, 0x3de1999a)
  id: totrans-3288
  prefs: []
  type: TYPE_NORMAL
- en: (1.70, 0x3df5999a)
  id: totrans-3289
  prefs: []
  type: TYPE_NORMAL
- en: (1.80, 0x3e053333)
  id: totrans-3290
  prefs: []
  type: TYPE_NORMAL
- en: (1.90, 0x3e0f3333)
  id: totrans-3291
  prefs: []
  type: TYPE_NORMAL
- en: (2.00, 0x3e19999a)
  id: totrans-3292
  prefs: []
  type: TYPE_NORMAL
- en: (2.10, 0x3e240000)
  id: totrans-3293
  prefs: []
  type: TYPE_NORMAL
- en: (2.20, 0x3e2e0000)
  id: totrans-3294
  prefs: []
  type: TYPE_NORMAL
- en: (2.30, 0x3e386667)
  id: totrans-3295
  prefs: []
  type: TYPE_NORMAL
- en: (2.40, 0x3e426667)
  id: totrans-3296
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that most fractions of 10 are not exactly representable in floating
    point. Nevertheless, when performing interpolation that does not require high
    precision, these values are interpolated at full precision.
  id: totrans-3297
  prefs: []
  type: TYPE_NORMAL
- en: '*Microdemo:* `tex1d_9bit.cu`'
  id: totrans-3298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To explore this question of precision, we developed another microdemo, `tex1d_9bit.cu`.
    Here, we’ve populated a texture with 32-bit floating-point values that require
    full precision to represent. In addition to passing the base/increment pair for
    the texture coordinates, another base/increment pair specifies the “expected”
    interpolation value, assuming full-precision interpolation.
  id: totrans-3299
  prefs: []
  type: TYPE_NORMAL
- en: In `tex1d_9bit`, the `CreateAndPrintTex()` function is modified to write its
    output as shown in [Listing 10.5](ch10.html#ch10lis05).
  id: totrans-3300
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 10.5.* `Tex1d_9bit.cu` (excerpt).'
  id: totrans-3301
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p10lis05a)'
  id: totrans-3302
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3303
  prefs: []
  type: TYPE_NORMAL
- en: printf( "X\tY\tActual Value\tExpected Value\tDiff\n" );
  id: totrans-3304
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < outN; i++ ) {
  id: totrans-3305
  prefs: []
  type: TYPE_NORMAL
- en: T expected;
  id: totrans-3306
  prefs: []
  type: TYPE_NORMAL
- en: if ( bEmulateGPU ) {
  id: totrans-3307
  prefs: []
  type: TYPE_NORMAL
- en: float x = base+(float)i*increment - 0.5f;
  id: totrans-3308
  prefs: []
  type: TYPE_NORMAL
- en: float frac = x - (float) (int) x;
  id: totrans-3309
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3310
  prefs: []
  type: TYPE_NORMAL
- en: int frac256 = (int) (frac*256.0f+0.5f);
  id: totrans-3311
  prefs: []
  type: TYPE_NORMAL
- en: frac = frac256/256.0f;
  id: totrans-3312
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3313
  prefs: []
  type: TYPE_NORMAL
- en: int index = (int) x;
  id: totrans-3314
  prefs: []
  type: TYPE_NORMAL
- en: expected = (1.0f-frac)*initTex[index] +
  id: totrans-3315
  prefs: []
  type: TYPE_NORMAL
- en: frac*initTex[index+1];
  id: totrans-3316
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3317
  prefs: []
  type: TYPE_NORMAL
- en: else {
  id: totrans-3318
  prefs: []
  type: TYPE_NORMAL
- en: expected = expectedBase + (float) i*expectedIncrement;
  id: totrans-3319
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3320
  prefs: []
  type: TYPE_NORMAL
- en: float diff = fabsf( outHost[i].y - expected );
  id: totrans-3321
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%.2f\t%.2f\t", outHost[i].x, outHost[i].y );
  id: totrans-3322
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%08x\t", *(int *) (&outHost[i].y) );
  id: totrans-3323
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%08x\t", *(int *) (&expected) );
  id: totrans-3324
  prefs: []
  type: TYPE_NORMAL
- en: printf( "%E\n", diff );
  id: totrans-3325
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3326
  prefs: []
  type: TYPE_NORMAL
- en: printf( "\n" );
  id: totrans-3327
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3328
  prefs: []
  type: TYPE_NORMAL
- en: For the just-described texture with 10 values (incrementing by 0.1), we can
    use this function to generate a comparison of the actual texture results with
    the expected full-precision result. Calling the function
  id: totrans-3329
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p330pro02a)'
  id: totrans-3330
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( tex, 10, 4, 1.5f, 0.25f, 0.1f, 0.025f );
  id: totrans-3331
  prefs: []
  type: TYPE_NORMAL
- en: CreateAndPrintTex<float>( tex, 10, 4, 1.5f, 0.1f, 0.1f, 0.01f );
  id: totrans-3332
  prefs: []
  type: TYPE_NORMAL
- en: yields this output.
  id: totrans-3333
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p330pro01a)'
  id: totrans-3334
  prefs: []
  type: TYPE_NORMAL
- en: X     Y     Actual Value  Expected Value  Diff
  id: totrans-3335
  prefs: []
  type: TYPE_NORMAL
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  id: totrans-3336
  prefs: []
  type: TYPE_NORMAL
- en: 1.75  0.12  3e000000      3e000000        0.000000E+00
  id: totrans-3337
  prefs: []
  type: TYPE_NORMAL
- en: 2.00  0.15  3e19999a      3e19999a        0.000000E+00
  id: totrans-3338
  prefs: []
  type: TYPE_NORMAL
- en: 2.25  0.17  3e333333      3e333333        0.000000E+00
  id: totrans-3339
  prefs: []
  type: TYPE_NORMAL
- en: X     Y     Actual Value  Expected Value  Diff
  id: totrans-3340
  prefs: []
  type: TYPE_NORMAL
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  id: totrans-3341
  prefs: []
  type: TYPE_NORMAL
- en: 1.60  0.11  3de1999a      3de147ae        1.562536E-04
  id: totrans-3342
  prefs: []
  type: TYPE_NORMAL
- en: 1.70  0.12  3df5999a      3df5c290        7.812679E-05
  id: totrans-3343
  prefs: []
  type: TYPE_NORMAL
- en: 1.80  0.13  3e053333      3e051eb8        7.812679E-05
  id: totrans-3344
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the “Diff” column on the right, the first set of outputs
    were interpolated at full precision, while the second were not. The explanation
    for this difference lies in Appendix F of the *CUDA Programming Guide*, which
    describes how linear interpolation is performed for 1D textures.
  id: totrans-3345
  prefs: []
  type: TYPE_NORMAL
- en: '*tex*(*x*) = (1 - α)*T*(*i*) + α*T*(*i* + 1)'
  id: totrans-3346
  prefs: []
  type: TYPE_NORMAL
- en: where
  id: totrans-3347
  prefs: []
  type: TYPE_NORMAL
- en: '*i* = *floor*(*X*[*B*]), α + *frac*(*X*[*B*]), *X*[*B*] = *x* - 0.5'
  id: totrans-3348
  prefs: []
  type: TYPE_NORMAL
- en: and α is stored in a 9-bit fixed-point format with 8 bits of fractional value.
  id: totrans-3349
  prefs: []
  type: TYPE_NORMAL
- en: In [Listing 10.5](ch10.html#ch10lis05), this computation in C++ is emulated
    in the `bEmulateGPU` case. The code snippet to emulate 9-bit weights can be enabled
    in `tex1d_9bit.cu` by passing `true` as the `bEmulateGPU` parameter of `CreateAndPrintTex()`.
    The output then becomes
  id: totrans-3350
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p331pro01a)'
  id: totrans-3351
  prefs: []
  type: TYPE_NORMAL
- en: X     Y     Actual Value  Expected Value  Diff
  id: totrans-3352
  prefs: []
  type: TYPE_NORMAL
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  id: totrans-3353
  prefs: []
  type: TYPE_NORMAL
- en: 1.75  0.12  3e000000      3e000000        0.000000E+00
  id: totrans-3354
  prefs: []
  type: TYPE_NORMAL
- en: 2.00  0.15  3e19999a      3e19999a        0.000000E+00
  id: totrans-3355
  prefs: []
  type: TYPE_NORMAL
- en: 2.25  0.17  3e333333      3e333333        0.000000E+00
  id: totrans-3356
  prefs: []
  type: TYPE_NORMAL
- en: X     Y     Actual Value  Expected Value  Diff
  id: totrans-3357
  prefs: []
  type: TYPE_NORMAL
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  id: totrans-3358
  prefs: []
  type: TYPE_NORMAL
- en: 1.60  0.11  3de1999a      3de1999a        0.000000E+00
  id: totrans-3359
  prefs: []
  type: TYPE_NORMAL
- en: 1.70  0.12  3df5999a      3df5999a        0.000000E+00
  id: totrans-3360
  prefs: []
  type: TYPE_NORMAL
- en: 1.80  0.13  3e053333      3e053333        0.000000E+00
  id: totrans-3361
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the rightmost column of 0’s, when computing the interpolated
    value with 9-bit precision, the differences between “expected” and “actual” output
    disappear.
  id: totrans-3362
  prefs: []
  type: TYPE_NORMAL
- en: 10.6\. Texturing with Normalized Coordinates
  id: totrans-3363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When texturing with normalized coordinates, the texture is addressed by coordinates
    in the range `[0.0, 1.0)` instead of the range `[0, MaxDim)`. For a 1D texture
    with 16 elements, the normalized coordinates are as in [Figure 10.6](ch10.html#ch10fig06).
  id: totrans-3364
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig06.jpg)'
  id: totrans-3365
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.6* Texturing with normalized coordinates.'
  id: totrans-3366
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than having texture coordinates that are independent of the texture dimension,
    the texture is dealt with in largely the same way, except that the full range
    of CUDA’s texturing capabilities become available. With normalized coordinates,
    more texture addressing mode besides clamp and border addressing becomes available:
    the *wrap* and *mirror* addressing modes, whose formulas are as follows.'
  id: totrans-3367
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/331tab01.jpg)'
  id: totrans-3368
  prefs: []
  type: TYPE_IMG
- en: The four texture addressing modes supported in CUDA in [Figure 10.7](ch10.html#ch10fig07)
    show which in-range texture element is fetched by the first two out-of-range coordinates
    on each end. If you are having trouble visualizing the behavior of these addressing
    modes, check out the `tex2d_opengl.cu` microdemo in the next section.
  id: totrans-3369
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig07.jpg)'
  id: totrans-3370
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.7* Texture addressing modes.'
  id: totrans-3371
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3372
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  id: totrans-3373
  prefs: []
  type: TYPE_NORMAL
- en: In the driver API, changes to the texture reference are codified by the `cuTexRefSetArray()`
    or `cuTexRefSetAddress()` function. In other words, calls to functions that make
    state changes, such as `cuTexRefSetFilterMode()` or `cuTexRefSetAddressMode()`,
    have no effect until the texture reference is bound to memory.
  id: totrans-3374
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3375
  prefs: []
  type: TYPE_NORMAL
- en: Floating-Point Coordinates with 1D Device Memory
  id: totrans-3376
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For applications that wish to use floating-point coordinates to address the
    texture or use texturing features that are only available for normalized coordinates,
    use `cudaBindTexture2D() / cuTexRefSetAddress2D()` to specify the base address.
    Specify a height of 1 and pitch of `N*sizeof(T)`. The kernel can then call `tex2D(x,0.0f)`
    to read the 1D texture with floating-point coordinates.
  id: totrans-3377
  prefs: []
  type: TYPE_NORMAL
- en: 10.7\. 1D Surface Read/Write
  id: totrans-3378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Until SM 2.0 hardware became available, CUDA kernels could access the contents
    of CUDA arrays only via texturing. Other access to CUDA arrays, including all
    write access, could be performed only via memcpy functions such as `cudaMemcpyToArray()`.
    The only way for CUDA kernels to both texture from and write to a given region
    of memory was to bind the texture reference to linear device memory.
  id: totrans-3379
  prefs: []
  type: TYPE_NORMAL
- en: But with the surface read/write functions newly available in SM 2.x, developers
    can bind CUDA arrays to *surface references* and use the `surf1Dread()` and `surf1Dwrite()`
    intrinsics to read and write the CUDA arrays from a kernel. Unlike texture reads,
    which have dedicated cache hardware, these reads and writes go through the same
    L2 cache as global loads and stores.
  id: totrans-3380
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3381
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-3382
  prefs: []
  type: TYPE_NORMAL
- en: In order for a surface reference to be bound to a CUDA array, the CUDA array
    must have been created with the `cudaArraySurfaceLoadStore` flag.
  id: totrans-3383
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  id: totrans-3384
  prefs: []
  type: TYPE_NORMAL
- en: The 1D surface read/write intrinsics are declared as follows.
  id: totrans-3385
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p333pro01a)'
  id: totrans-3386
  prefs: []
  type: TYPE_NORMAL
- en: template<class Type> Type surf1Dread(surface<void, 1> surfRef, int x,
  id: totrans-3387
  prefs: []
  type: TYPE_NORMAL
- en: boundaryMode = cudaBoundaryModeTrap);
  id: totrans-3388
  prefs: []
  type: TYPE_NORMAL
- en: template<class Type> void surf1Dwrite(Type data, surface<void, 1>
  id: totrans-3389
  prefs: []
  type: TYPE_NORMAL
- en: surfRef, int x, boundaryMode = cudaBoundaryModeTrap);
  id: totrans-3390
  prefs: []
  type: TYPE_NORMAL
- en: These intrinsics are not type-strong—as you can see, surface references are
    declared as `void`—and the size of the memory transaction depends on `sizeof(Type)`
    for a given invocation of `surf1Dread()` or `surf1Dwrite()`. The x offset is in
    bytes and must be naturally aligned with respect to `sizeof(Type)`. For 4-byte
    operands such as `int` or `float`, `offset` must be evenly divisible by 4, for
    `short` it must be divisible by 2, and so on.
  id: totrans-3391
  prefs: []
  type: TYPE_NORMAL
- en: Support for surface read/write is far less rich than texturing functionality.^([5](ch10.html#ch10fn5))
    Only unformatted reads and writes are supported, with no conversion or interpolation
    functions, and the border handling is restricted to only two modes.
  id: totrans-3392
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch10.html#ch10fn5a). In fact, CUDA could have bypassed implementation of
    surface references entirely, with the intrinsics operating directly on CUDA arrays.
    Surface references were included for orthogonality with texture references to
    provide for behavior defined on a per-surfref basis as opposed to per-instruction.'
  id: totrans-3393
  prefs: []
  type: TYPE_NORMAL
- en: Boundary conditions are handled differently for surface read/write than for
    texture reads. For textures, this behavior is controlled by the addressing mode
    in the texture reference. For surface read/write, the method of handling out-of-range
    `offset` values is specified as a parameter of `surf1Dread()` or `surf1Dwrite()`.
    Out-of-range indices can either cause a hardware exception (`cudaBoundaryModeTrap`)
    or read as 0 for `surf1Dread()` and are ignored for `surf1Dwrite()`(`cudaBoundaryModeZero`).
  id: totrans-3394
  prefs: []
  type: TYPE_NORMAL
- en: Because of the untyped character of surface references, it is easy to write
    a templated 1D memset routine that works for all types.
  id: totrans-3395
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p334pro01a)'
  id: totrans-3396
  prefs: []
  type: TYPE_NORMAL
- en: surface<void, 1> surf1D;
  id: totrans-3397
  prefs: []
  type: TYPE_NORMAL
- en: template <typename T>
  id: totrans-3398
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  id: totrans-3399
  prefs: []
  type: TYPE_NORMAL
- en: surf1Dmemset( int index, T value, size_t N )
  id: totrans-3400
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3401
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3402
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  id: totrans-3403
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x )
  id: totrans-3404
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3405
  prefs: []
  type: TYPE_NORMAL
- en: surf1Dwrite( value, surf1D, (index+i)*sizeof(T) );
  id: totrans-3406
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3407
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3408
  prefs: []
  type: TYPE_NORMAL
- en: This kernel is in the microdemo `surf1Dmemset.cu`, which creates a 64-byte CUDA
    array for illustrative purposes, initializes it with the above kernel, and prints
    the array in float and integer forms.
  id: totrans-3409
  prefs: []
  type: TYPE_NORMAL
- en: A generic template host function wraps this kernel with a call to `cudaBindSurfaceToArray().`
  id: totrans-3410
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p334pro02a)'
  id: totrans-3411
  prefs: []
  type: TYPE_NORMAL
- en: template<typename T>
  id: totrans-3412
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t
  id: totrans-3413
  prefs: []
  type: TYPE_NORMAL
- en: surf1Dmemset( cudaArray *array, int offset, T value, size_t N )
  id: totrans-3414
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3415
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  id: totrans-3416
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaBindSurfaceToArray(surf1D, array));
  id: totrans-3417
  prefs: []
  type: TYPE_NORMAL
- en: surf1Dmemset_kernel<<<2,384>>>( 0, value, 4*NUM_VALUES );
  id: totrans-3418
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  id: totrans-3419
  prefs: []
  type: TYPE_NORMAL
- en: return status;
  id: totrans-3420
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3421
  prefs: []
  type: TYPE_NORMAL
- en: The untyped character of surface references makes this template structure much
    easier to pull off than for textures. Because texture references are both type-strong
    and global, they cannot be templatized in the parameter list of a would-be generic
    function. A one-line change from
  id: totrans-3422
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(surf1Dmemset(array, 0, 3.141592654f, NUM_VALUES));
  id: totrans-3423
  prefs: []
  type: TYPE_NORMAL
- en: to
  id: totrans-3424
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(surf1Dmemset(array, 0, (short) 0xbeef, 2*NUM_VALUES));
  id: totrans-3425
  prefs: []
  type: TYPE_NORMAL
- en: will change the output of this program from
  id: totrans-3426
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p335pro01a)'
  id: totrans-3427
  prefs: []
  type: TYPE_NORMAL
- en: 0x40490fdb 0x40490fdb ... (16 times)
  id: totrans-3428
  prefs: []
  type: TYPE_NORMAL
- en: 3.141593E+00 3.141593E+00 ... (16 times)
  id: totrans-3429
  prefs: []
  type: TYPE_NORMAL
- en: to
  id: totrans-3430
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p335pro02a)'
  id: totrans-3431
  prefs: []
  type: TYPE_NORMAL
- en: 0xbeefbeef 0xbeefbeef ... (16 times)
  id: totrans-3432
  prefs: []
  type: TYPE_NORMAL
- en: -4.68253E-01 -4.68253E-01 ... (16 times)
  id: totrans-3433
  prefs: []
  type: TYPE_NORMAL
- en: 10.8\. 2D Texturing
  id: totrans-3434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In most ways, 2D texturing is similar to 1D texturing as described above. Applications
    optionally may promote integer texture elements to floating point, and they can
    use unnormalized or normalized coordinates. When linear filtering is supported,
    bilinear filtering is performed between four texture values, weighted by the fractional
    bits of the texture coordinates. The hardware can perform a different addressing
    mode for each dimension. For example, the X coordinate can be clamped while the
    Y coordinate is wrapped.
  id: totrans-3435
  prefs: []
  type: TYPE_NORMAL
- en: '10.8.1\. Microdemo: `tex2d_opengl.cu`'
  id: totrans-3436
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This microdemo graphically illustrates the effects of the different texturing
    modes. It uses OpenGL for portability and the GL Utility Library (GLUT) to minimize
    the amount of setup code. To keep distractions to a minimum, this application
    does not use CUDA’s OpenGL interoperability functions. Instead, we allocate mapped
    host memory and render it to the frame buffer using `glDrawPixels()`. To OpenGL,
    the data might as well be coming from the CPU.
  id: totrans-3437
  prefs: []
  type: TYPE_NORMAL
- en: The application supports normalized and unnormalized coordinates and clamp,
    wrap, mirror, and border addressing in both the X and Y directions. For unnormalized
    coordinates, the following kernel is used to write the texture contents into the
    output buffer.
  id: totrans-3438
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p336pro01a)'
  id: totrans-3439
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  id: totrans-3440
  prefs: []
  type: TYPE_NORMAL
- en: RenderTextureUnnormalized( uchar4 *out, int width, int height )
  id: totrans-3441
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3442
  prefs: []
  type: TYPE_NORMAL
- en: for ( int row = blockIdx.x; row < height; row += gridDim.x ) {
  id: totrans-3443
  prefs: []
  type: TYPE_NORMAL
- en: out = (uchar4 *) (((char *) out)+row*4*width);
  id: totrans-3444
  prefs: []
  type: TYPE_NORMAL
- en: for ( int col = threadIdx.x; col < width; col += blockDim.x ) {
  id: totrans-3445
  prefs: []
  type: TYPE_NORMAL
- en: out[col] = tex2D( tex2d, (float) col, (float) row );
  id: totrans-3446
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3447
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3448
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3449
  prefs: []
  type: TYPE_NORMAL
- en: This kernel fills the rectangle of `width` × `height` pixels with values read
    from the texture using texture coordinates corresponding to the pixel locations.
    For out-of-range pixels, you can see the effects of the clamp and border addressing
    modes.
  id: totrans-3450
  prefs: []
  type: TYPE_NORMAL
- en: For normalized coordinates, the following kernel is used to write the texture
    contents into the output buffer.
  id: totrans-3451
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p336pro02a)'
  id: totrans-3452
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  id: totrans-3453
  prefs: []
  type: TYPE_NORMAL
- en: RenderTextureNormalized(
  id: totrans-3454
  prefs: []
  type: TYPE_NORMAL
- en: uchar4 *out,
  id: totrans-3455
  prefs: []
  type: TYPE_NORMAL
- en: int width,
  id: totrans-3456
  prefs: []
  type: TYPE_NORMAL
- en: int height,
  id: totrans-3457
  prefs: []
  type: TYPE_NORMAL
- en: int scale )
  id: totrans-3458
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3459
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = blockIdx.x; j < height; j += gridDim.x ) {
  id: totrans-3460
  prefs: []
  type: TYPE_NORMAL
- en: int row = height-j-1;
  id: totrans-3461
  prefs: []
  type: TYPE_NORMAL
- en: out = (uchar4 *) (((char *) out)+row*4*width);
  id: totrans-3462
  prefs: []
  type: TYPE_NORMAL
- en: float texRow = scale * (float) row / (float) height;
  id: totrans-3463
  prefs: []
  type: TYPE_NORMAL
- en: float invWidth = scale / (float) width;
  id: totrans-3464
  prefs: []
  type: TYPE_NORMAL
- en: for ( int col = threadIdx.x; col < width; col += blockDim.x ) {
  id: totrans-3465
  prefs: []
  type: TYPE_NORMAL
- en: float texCol = col * invWidth;
  id: totrans-3466
  prefs: []
  type: TYPE_NORMAL
- en: out[col] = tex2D( tex2d, texCol, texRow );
  id: totrans-3467
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3468
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3469
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3470
  prefs: []
  type: TYPE_NORMAL
- en: The scale parameter specifies the number of times to tile the texture into the
    output buffer. By default, scale=1.0, and the texture is seen only once. When
    running the application, you can hit the 1–9 keys to replicate the texture that
    many times. The C, W, M, and B keys set the addressing mode for the current direction;
    the X and Y keys specify the current direction.
  id: totrans-3471
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/337tab01.jpg)'
  id: totrans-3472
  prefs: []
  type: TYPE_IMG
- en: Readers are encouraged to run the program, or especially to modify and run the
    program, to see the effects of different texturing settings. [Figure 10.8](ch10.html#ch10fig08)
    shows the output of the program for the four permutations of X Wrap/Mirror and
    Y Wrap/Mirror when replicating the texture five times.
  id: totrans-3473
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/10fig08.jpg)'
  id: totrans-3474
  prefs: []
  type: TYPE_IMG
- en: '*Figure 10.8* Wrap and mirror addressing modes.'
  id: totrans-3475
  prefs: []
  type: TYPE_NORMAL
- en: '10.9\. 2D Texturing: Copy Avoidance'
  id: totrans-3476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When CUDA was first introduced, CUDA kernels could read from CUDA arrays only
    via texture. Applications could write to CUDA arrays only with memory copies;
    in order for CUDA kernels to write data that would then be read through texture,
    they had to write to device memory and then perform a device→array memcpy. Since
    then, two mechanisms have been added that remove this step for 2D textures.
  id: totrans-3477
  prefs: []
  type: TYPE_NORMAL
- en: • A 2D texture can be bound to a pitch-allocated range of linear device memory.
  id: totrans-3478
  prefs: []
  type: TYPE_NORMAL
- en: • Surface load/store intrinsics enable CUDA kernels to write to CUDA arrays
    directly.
  id: totrans-3479
  prefs: []
  type: TYPE_NORMAL
- en: 3D texturing from device memory and 3D surface load/store are not supported.
  id: totrans-3480
  prefs: []
  type: TYPE_NORMAL
- en: For applications that read most or all the texture contents with a regular access
    pattern (such as a video codec) or applications that must work on Tesla-class
    hardware, it is best to keep the data in device memory. For applications that
    perform random (but localized) access when texturing, it is probably best to keep
    the data in CUDA arrays and use surface read/write intrinsics.
  id: totrans-3481
  prefs: []
  type: TYPE_NORMAL
- en: 10.9.1\. 2D Texturing from Device Memory
  id: totrans-3482
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Texturing from 2D device memory does not have any of the benefits of “block
    linear” addressing—a cache line fill into the texture cache pulls in a horizontal
    span of texels, not a 2D or 3D block of them—but unless the application performs
    random access into the texture, the benefits of avoiding a copy from device memory
    to a CUDA array likely outweigh the penalties of losing block linear addressing.
  id: totrans-3483
  prefs: []
  type: TYPE_NORMAL
- en: To bind a 2D texture reference to a device memory range, call `cudaBindTexture2D().`
  id: totrans-3484
  prefs: []
  type: TYPE_NORMAL
- en: cudaBindTexture2D(
  id: totrans-3485
  prefs: []
  type: TYPE_NORMAL
- en: NULL,
  id: totrans-3486
  prefs: []
  type: TYPE_NORMAL
- en: '&tex,'
  id: totrans-3487
  prefs: []
  type: TYPE_NORMAL
- en: texDevice,
  id: totrans-3488
  prefs: []
  type: TYPE_NORMAL
- en: '&channelDesc,'
  id: totrans-3489
  prefs: []
  type: TYPE_NORMAL
- en: inWidth,
  id: totrans-3490
  prefs: []
  type: TYPE_NORMAL
- en: inHeight,
  id: totrans-3491
  prefs: []
  type: TYPE_NORMAL
- en: texPitch );
  id: totrans-3492
  prefs: []
  type: TYPE_NORMAL
- en: The above call binds the texture reference `tex` to the 2D device memory range
    given by `texDevice / texPitch`. The base address and pitch must conform to hardware-specific
    alignment constraints.^([6](ch10.html#ch10fn6)) The base address must be aligned
    with respect to `cudaDeviceProp.textureAlignment`, and the pitch must be aligned
    with respect to `cudaDeviceProp.texturePitchAlignment`.^([7](ch10.html#ch10fn7))
    The microdemo `tex2d_addressing_device.cu` is identical to `tex2d_addressing.cu`,
    but it uses device memory to hold the texture data. The two programs are designed
    to be so similar that you can look at the differences. A device pointer/pitch
    tuple is declared instead of a CUDA array.
  id: totrans-3493
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch10.html#ch10fn6a). CUDA arrays must conform to the same constraints,
    but in that case, the base address and pitch are managed by CUDA and hidden along
    with the memory layout.'
  id: totrans-3494
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch10.html#ch10fn7a). In the driver API, the corresponding device attribute
    queries are `CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT` and `CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT`.'
  id: totrans-3495
  prefs: []
  type: TYPE_NORMAL
- en: < cudaArray *texArray = 0;
  id: totrans-3496
  prefs: []
  type: TYPE_NORMAL
- en: T *texDevice = 0;
  id: totrans-3497
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: size_t texPitch;
  id: totrans-3498
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '`cudaMallocPitch()` is called instead of calling `cudaMallocArray()`. `cudaMallocPitch()`
    delegates selection of the base address and pitch to the driver, so the code will
    continue working on future generations of hardware (which have a tendency to increase
    alignment requirements).'
  id: totrans-3499
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p339pro01a)'
  id: totrans-3500
  prefs: []
  type: TYPE_NORMAL
- en: < CUDART_CHECK(cudaMallocArray( &texArray,
  id: totrans-3501
  prefs: []
  type: TYPE_NORMAL
- en: < &channelDesc,
  id: totrans-3502
  prefs: []
  type: TYPE_NORMAL
- en: < inWidth,
  id: totrans-3503
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaMallocPitch( &texDevice,
  id: totrans-3504
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '&texPitch,'
  id: totrans-3505
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: inWidth*sizeof(T),
  id: totrans-3506
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: inHeight));
  id: totrans-3507
  prefs: []
  type: TYPE_NORMAL
- en: Next, `cudaTextureBind2D()` is called instead of `cudaBindTextureToArray()`.
  id: totrans-3508
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p339pro02a)'
  id: totrans-3509
  prefs: []
  type: TYPE_NORMAL
- en: < CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
  id: totrans-3510
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK(cudaBindTexture2D( NULL,
  id: totrans-3511
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '&tex,'
  id: totrans-3512
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: texDevice,
  id: totrans-3513
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '&channelDesc,'
  id: totrans-3514
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: inWidth,
  id: totrans-3515
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: inHeight,
  id: totrans-3516
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: texPitch ));
  id: totrans-3517
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The final difference is that instead of freeing the CUDA array, `cudaFree()`
    is called on the pointer returned by `cudaMallocPitch()`.
  id: totrans-3518
  prefs: []
  type: TYPE_NORMAL
- en: < cudaFreeArray( texArray );
  id: totrans-3519
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( texDevice );
  id: totrans-3520
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 10.9.2\. 2D Surface Read/Write
  id: totrans-3521
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As with 1D surface read/write, Fermi-class hardware enables kernels to write
    directly into CUDA arrays with intrinsic surface read/write functions.
  id: totrans-3522
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p340pro02a)'
  id: totrans-3523
  prefs: []
  type: TYPE_NORMAL
- en: template<class Type> Type surf2Dread(surface<void, 1> surfRef, int x,
  id: totrans-3524
  prefs: []
  type: TYPE_NORMAL
- en: int y, boundaryMode = cudaBoundaryModeTrap);
  id: totrans-3525
  prefs: []
  type: TYPE_NORMAL
- en: template<class Type> Type surf2Dwrite(surface<void, 1> surfRef, Type
  id: totrans-3526
  prefs: []
  type: TYPE_NORMAL
- en: data, int x, int y, boundaryMode = cudaBoundaryModeTrap);
  id: totrans-3527
  prefs: []
  type: TYPE_NORMAL
- en: The surface reference declaration and corresponding CUDA kernel for 2D surface
    memset, given in `surf2Dmemset.cu`, is as follows.
  id: totrans-3528
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p340pro01a)'
  id: totrans-3529
  prefs: []
  type: TYPE_NORMAL
- en: surface<void, 2> surf2D;
  id: totrans-3530
  prefs: []
  type: TYPE_NORMAL
- en: template<typename T>
  id: totrans-3531
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  id: totrans-3532
  prefs: []
  type: TYPE_NORMAL
- en: surf2Dmemset_kernel( T value,
  id: totrans-3533
  prefs: []
  type: TYPE_NORMAL
- en: int xOffset, int yOffset,
  id: totrans-3534
  prefs: []
  type: TYPE_NORMAL
- en: int Width, int Height )
  id: totrans-3535
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3536
  prefs: []
  type: TYPE_NORMAL
- en: for ( int row = blockIdx.y*blockDim.y + threadIdx.y;
  id: totrans-3537
  prefs: []
  type: TYPE_NORMAL
- en: row < Height;
  id: totrans-3538
  prefs: []
  type: TYPE_NORMAL
- en: row += blockDim.y*gridDim.y )
  id: totrans-3539
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3540
  prefs: []
  type: TYPE_NORMAL
- en: for ( int col = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3541
  prefs: []
  type: TYPE_NORMAL
- en: col < Width;
  id: totrans-3542
  prefs: []
  type: TYPE_NORMAL
- en: col += blockDim.x*gridDim.x )
  id: totrans-3543
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3544
  prefs: []
  type: TYPE_NORMAL
- en: surf2Dwrite( value,
  id: totrans-3545
  prefs: []
  type: TYPE_NORMAL
- en: surf2D,
  id: totrans-3546
  prefs: []
  type: TYPE_NORMAL
- en: (xOffset+col)*sizeof(T),
  id: totrans-3547
  prefs: []
  type: TYPE_NORMAL
- en: yOffset+row );
  id: totrans-3548
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3549
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3550
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3551
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the X offset parameter to `surf2Dwrite()` is given in *bytes*.
  id: totrans-3552
  prefs: []
  type: TYPE_NORMAL
- en: 10.10\. 3D Texturing
  id: totrans-3553
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reading from 3D textures is similar to reading from 2D textures, but there are
    more limitations.
  id: totrans-3554
  prefs: []
  type: TYPE_NORMAL
- en: • 3D textures have smaller limits (2048x2048x2048 instead of 65536x32768).
  id: totrans-3555
  prefs: []
  type: TYPE_NORMAL
- en: '• There are no copy avoidance strategies: CUDA does not support 3D texturing
    from device memory or surface load/store on 3D CUDA arrays.'
  id: totrans-3556
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than that, the differences are straightforward: Kernels can read from
    3D textures using a `tex3D()` intrinsic that takes 3 floating-point parameters,
    and the underlying 3D CUDA arrays must be populated by 3D memcpys. Trilinear filtering
    is supported; 8 texture elements are read and interpolated according to the texture
    coordinates, with the same 9-bit precision limit as 1D and 2D texturing.'
  id: totrans-3557
  prefs: []
  type: TYPE_NORMAL
- en: The 3D texture size limits may be queried by calling `cuDeviceGetAttribute()`
    with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH`, `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT`,
    and `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH`, or by calling `cudaGetDeviceProperties()`
    and examining `cudaDeviceProp.maxTexture3D`. Due to the much larger number of
    parameters needed, 3D CUDA arrays must be created and manipulated using a different
    set of APIs than 1D or 2D CUDA arrays.
  id: totrans-3558
  prefs: []
  type: TYPE_NORMAL
- en: To create a 3D CUDA array, the `cudaMalloc3DArray()` function takes a `cudaExtent`
    structure instead of width and height parameters.
  id: totrans-3559
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t cudaMalloc3DArray(struct cudaArray** array, const struct cudaChannelFormatDesc*
    desc, struct cudaExtent extent, unsigned int flags __dv(0));
  id: totrans-3560
  prefs: []
  type: TYPE_NORMAL
- en: '`cudaExtent` is defined as follows.'
  id: totrans-3561
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaExtent {
  id: totrans-3562
  prefs: []
  type: TYPE_NORMAL
- en: size_t width;
  id: totrans-3563
  prefs: []
  type: TYPE_NORMAL
- en: size_t height;
  id: totrans-3564
  prefs: []
  type: TYPE_NORMAL
- en: size_t depth;
  id: totrans-3565
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  id: totrans-3566
  prefs: []
  type: TYPE_NORMAL
- en: Describing 3D memcpy operations is sufficiently complicated that both the CUDA
    runtime and the driver API use structures to specify the parameters. The runtime
    API uses the `cudaMemcpy3DParams` structure, which is declared as follows.
  id: totrans-3567
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p341pro01a)'
  id: totrans-3568
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaMemcpy3DParms {
  id: totrans-3569
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaArray *srcArray;
  id: totrans-3570
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPos srcPos;
  id: totrans-3571
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr srcPtr;
  id: totrans-3572
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaArray *dstArray;
  id: totrans-3573
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPos dstPos;
  id: totrans-3574
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr dstPtr;
  id: totrans-3575
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaExtent extent;
  id: totrans-3576
  prefs: []
  type: TYPE_NORMAL
- en: enum cudaMemcpyKind kind;
  id: totrans-3577
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  id: totrans-3578
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of these structure members are themselves structures: `extent` gives the
    width, height, and depth of the copy. The `srcPos` and `dstPos` members are `cudaPos`
    structures that specify the start points for the source and destination of the
    copy.'
  id: totrans-3579
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPos {
  id: totrans-3580
  prefs: []
  type: TYPE_NORMAL
- en: size_t x;
  id: totrans-3581
  prefs: []
  type: TYPE_NORMAL
- en: size_t y;
  id: totrans-3582
  prefs: []
  type: TYPE_NORMAL
- en: size_t z;
  id: totrans-3583
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  id: totrans-3584
  prefs: []
  type: TYPE_NORMAL
- en: The `cudaPitchedPtr` is a structure that was added with 3D memcpy to contain
    a pointer/pitch tuple.
  id: totrans-3585
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p342pro01a)'
  id: totrans-3586
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr
  id: totrans-3587
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3588
  prefs: []
  type: TYPE_NORMAL
- en: void *ptr; /**< Pointer to allocated memory */
  id: totrans-3589
  prefs: []
  type: TYPE_NORMAL
- en: size_t pitch; /**< Pitch of allocated memory in bytes */
  id: totrans-3590
  prefs: []
  type: TYPE_NORMAL
- en: size_t xsize; /**< Logical width of allocation in elements */
  id: totrans-3591
  prefs: []
  type: TYPE_NORMAL
- en: size_t ysize; /**< Logical height of allocation in elements */
  id: totrans-3592
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  id: totrans-3593
  prefs: []
  type: TYPE_NORMAL
- en: A `cudaPitchedPtr` structure may be created with the function `make_cudaPitchedPtr`,
    which takes the base pointer, pitch, and logical width and height of the allocation.
    `make_cudaPitchedPtr` just copies its parameters into the output struct; however,
  id: totrans-3594
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p342pro02a)'
  id: totrans-3595
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr
  id: totrans-3596
  prefs: []
  type: TYPE_NORMAL
- en: make_cudaPitchedPtr(void *d, size_t p, size_t xsz, size_t ysz)
  id: totrans-3597
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3598
  prefs: []
  type: TYPE_NORMAL
- en: struct cudaPitchedPtr s;
  id: totrans-3599
  prefs: []
  type: TYPE_NORMAL
- en: s.ptr = d;
  id: totrans-3600
  prefs: []
  type: TYPE_NORMAL
- en: s.pitch = p;
  id: totrans-3601
  prefs: []
  type: TYPE_NORMAL
- en: s.xsize = xsz;
  id: totrans-3602
  prefs: []
  type: TYPE_NORMAL
- en: s.ysize = ysz;
  id: totrans-3603
  prefs: []
  type: TYPE_NORMAL
- en: return s;
  id: totrans-3604
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3605
  prefs: []
  type: TYPE_NORMAL
- en: The `simpleTexture3D` sample in the SDK illustrates how to do 3D texturing with
    CUDA.
  id: totrans-3606
  prefs: []
  type: TYPE_NORMAL
- en: 10.11\. Layered Textures
  id: totrans-3607
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Layered textures* are known in the graphics world as *texture arrays* because
    they enable 1D or 2D textures to be arranged as arrays accessed by an integer
    index. The main advantage of layered textures over vanilla 2D or 3D textures is
    that they support larger extents within the slices. There is no performance advantage
    to using layered textures.'
  id: totrans-3608
  prefs: []
  type: TYPE_NORMAL
- en: Layered textures are laid out in memory differently than 2D or 3D textures,
    in such a way that 2D or 3D textures will not perform as well if they use the
    layout optimized for layered textures. As a result, when creating the CUDA array,
    you must specify `cudaArrayLayered` to `cudaMalloc3DArray()` or specify `CUDA_ARRAY3D_LAYERED`
    to `cuArray3DCreate()`. The `simpleLayeredTexture` sample in the SDK illustrates
    how to use layered textures.
  id: totrans-3609
  prefs: []
  type: TYPE_NORMAL
- en: 10.11.1\. 1D Layered Textures
  id: totrans-3610
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The 1D layered texture size limits may be queried by calling `cuDeviceGet-Attribute()`
    with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_WIDTH` and `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_LAYERS`
    or by calling `cudaGetDeviceProperties()` and examining `cudaDeviceProp.maxTexture1DLayered`.
  id: totrans-3611
  prefs: []
  type: TYPE_NORMAL
- en: 10.11.2\. 2D Layered Textures
  id: totrans-3612
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The 2D layered texture size limits may be queried by calling `cuDeviceGet-Attribute()`
    with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH` and `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT`
    or `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS` or by calling `cudaGetDeviceProperties()`
    and examining `cudaDeviceProp.maxTexture2DLayered`. The layered texture size limits
    may be queried `cudaGetDeviceProperties()` and examining `cudaDeviceProp.maxTexture2DLayered`.
  id: totrans-3613
  prefs: []
  type: TYPE_NORMAL
- en: 10.12\. Optimal Block Sizing and Performance
  id: totrans-3614
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the texture coordinates are generated in the “obvious” way, such as in
    `tex2d_addressing.cu`
  id: totrans-3615
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p343pro01a)'
  id: totrans-3616
  prefs: []
  type: TYPE_NORMAL
- en: row = blockIdx.y*blockDim.y + threadIdx.y;
  id: totrans-3617
  prefs: []
  type: TYPE_NORMAL
- en: col = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3618
  prefs: []
  type: TYPE_NORMAL
- en: '... tex2D( tex, (float) col, (float) row);'
  id: totrans-3619
  prefs: []
  type: TYPE_NORMAL
- en: then texturing performance is dependent on the block size.
  id: totrans-3620
  prefs: []
  type: TYPE_NORMAL
- en: To find the optimal size of a thread block, the `tex2D_shmoo.cu` and `surf2Dmemset_shmoo.cu`
    programs time the performance of thread blocks whose width and height vary from
    4..64, inclusive. Some combinations of these thread block sizes are not valid
    because they have too many threads.
  id: totrans-3621
  prefs: []
  type: TYPE_NORMAL
- en: For this exercise, the texturing kernel is designed to do as little work as
    possible (maximizing exposure to the performance of the texture hardware), while
    still “fooling” the compiler into issuing the code. Each thread computes the floating-point
    sum of the values it reads and writes the sum if the output parameter is non-NULL.
    The trick is that we never pass a non-NULL pointer to this kernel! The reason
    the kernel is structured this way is because if it never wrote any output, the
    compiler would see that the kernel was not doing any work and would emit code
    that did not perform the texturing operations at all.
  id: totrans-3622
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch10_images.html#p344pro01a)'
  id: totrans-3623
  prefs: []
  type: TYPE_NORMAL
- en: extern "C" __global__ void
  id: totrans-3624
  prefs: []
  type: TYPE_NORMAL
- en: TexSums( float *out, size_t Width, size_t Height )
  id: totrans-3625
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3626
  prefs: []
  type: TYPE_NORMAL
- en: float sum = 0.0f;
  id: totrans-3627
  prefs: []
  type: TYPE_NORMAL
- en: for ( int row = blockIdx.y*blockDim.y + threadIdx.y;
  id: totrans-3628
  prefs: []
  type: TYPE_NORMAL
- en: row < Height;
  id: totrans-3629
  prefs: []
  type: TYPE_NORMAL
- en: row += blockDim.y*gridDim.y )
  id: totrans-3630
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3631
  prefs: []
  type: TYPE_NORMAL
- en: for ( int col = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3632
  prefs: []
  type: TYPE_NORMAL
- en: col < Width;
  id: totrans-3633
  prefs: []
  type: TYPE_NORMAL
- en: col += blockDim.x*gridDim.x )
  id: totrans-3634
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  id: totrans-3635
  prefs: []
  type: TYPE_NORMAL
- en: sum += tex2D( tex, (float) col, (float) row );
  id: totrans-3636
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3637
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3638
  prefs: []
  type: TYPE_NORMAL
- en: if ( out ) {
  id: totrans-3639
  prefs: []
  type: TYPE_NORMAL
- en: out[blockIdx.x*blockDim.x+threadIdx.x] = sum;
  id: totrans-3640
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3641
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  id: totrans-3642
  prefs: []
  type: TYPE_NORMAL
- en: Even with our “trick,” there is a risk that the compiler will emit code that
    checks the `out` parameter and exits the kernel early if it’s equal to NULL. We’d
    have to synthesize some output that wouldn’t affect performance too much (for
    example, have each thread block compute the reduction of the sums in shared memory
    and write them to `out`). But by compiling the program with the `--keep` option
    and using `cuobjdump ---dump-sass` to examine the microcode, we can see that the
    compiler doesn’t check `out` until after the doubly-nested `for` loop as executed.
  id: totrans-3643
  prefs: []
  type: TYPE_NORMAL
- en: 10.12.1\. Results
  id: totrans-3644
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: On a GeForce GTX 280 (GT200), the optimal block size was found to be 128 threads,
    which delivered 35.7G/s of bandwidth. Thread blocks of size 32W × 4H were about
    the same speed as 16W × 8H or 8W × 16H, all traversing a 4K × 4K texture of `float`
    in 1.88 ms. On a Tesla M2050, the optimal block size was found to be 192 threads,
    which delivered 35.4G/s of bandwidth. As with the GT200, different-sized thread
    blocks were the same speed, with 6W × 32H, 16W × 12H, and 8W × 24H blocks delivering
    about the same performance.
  id: totrans-3645
  prefs: []
  type: TYPE_NORMAL
- en: 'The shmoo over 2D surface memset was less conclusive: Block sizes of at least
    128 threads generally had good performance, provided the thread count was evenly
    divisible by the warp size of 32\. The fastest 2D surface memset performance reported
    on a `cg1.4xlarge` without ECC enabled was 48Gb/s.'
  id: totrans-3646
  prefs: []
  type: TYPE_NORMAL
- en: For `float`-valued data for both boards we tested, the peak bandwidth numbers
    reported by texturing and surface write are about ¼ and ½ of the achievable peaks
    for global load/store, respectively.
  id: totrans-3647
  prefs: []
  type: TYPE_NORMAL
- en: 10.13\. Texturing Quick References
  id: totrans-3648
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 10.13.1\. Hardware Capabilities
  id: totrans-3649
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Hardware Limits
  id: totrans-3650
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/345tab01.jpg)'
  id: totrans-3651
  prefs: []
  type: TYPE_IMG
- en: Queries—Driver API
  id: totrans-3652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Most of the hardware limits listed above can be queried with `cuDevice-Attribute()`,
    which may be called with the following values to query.
  id: totrans-3653
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/346tab01.jpg)![Image](graphics/346tab01a.jpg)'
  id: totrans-3654
  prefs: []
  type: TYPE_IMG
- en: Queries—CUDA Runtime
  id: totrans-3655
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The following members of `cudaDeviceProp` contain hardware limits as listed
    above.
  id: totrans-3656
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/347tab01.jpg)'
  id: totrans-3657
  prefs: []
  type: TYPE_IMG
- en: 10.13.2\. CUDA Runtime
  id: totrans-3658
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 1D Textures
  id: totrans-3659
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/347tab02.jpg)'
  id: totrans-3660
  prefs: []
  type: TYPE_IMG
- en: 2D Textures
  id: totrans-3661
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/347tab03.jpg)'
  id: totrans-3662
  prefs: []
  type: TYPE_IMG
- en: 3D Textures
  id: totrans-3663
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/348tab01.jpg)'
  id: totrans-3664
  prefs: []
  type: TYPE_IMG
- en: 1D Layered Textures
  id: totrans-3665
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/348tab02.jpg)'
  id: totrans-3666
  prefs: []
  type: TYPE_IMG
- en: 2D Layered Textures
  id: totrans-3667
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/348tab03.jpg)'
  id: totrans-3668
  prefs: []
  type: TYPE_IMG
- en: 10.13.3\. Driver API
  id: totrans-3669
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 1D Textures
  id: totrans-3670
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/349tab01.jpg)'
  id: totrans-3671
  prefs: []
  type: TYPE_IMG
- en: The texture size limit for device memory is not queryable; it is 2^(27) elements
    on all CUDA-capable GPUs. The texture size limit for 1D CUDA arrays may be queried
    by calling `cuDeviceGetAttribute()` with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH`.
  id: totrans-3672
  prefs: []
  type: TYPE_NORMAL
- en: 2D Textures
  id: totrans-3673
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/349tab02.jpg)'
  id: totrans-3674
  prefs: []
  type: TYPE_IMG
- en: 3D Textures
  id: totrans-3675
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/350tab01.jpg)'
  id: totrans-3676
  prefs: []
  type: TYPE_IMG
- en: 1D Layered Textures
  id: totrans-3677
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/350tab02.jpg)'
  id: totrans-3678
  prefs: []
  type: TYPE_IMG
- en: 2D Layered Textures
  id: totrans-3679
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Image](graphics/350tab03.jpg)'
  id: totrans-3680
  prefs: []
  type: TYPE_IMG
