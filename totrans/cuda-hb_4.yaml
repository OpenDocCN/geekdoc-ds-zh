- en: Part II
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二部分
- en: Chapter 5\. Memory
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五章 内存
- en: To maximize performance, CUDA uses different types of memory, depending on the
    expected usage. *Host memory* refers to the memory attached to the CPU(s) in the
    system. CUDA provides APIs that enable faster access to host memory by page-locking
    and mapping it for the GPU(s). *Device memory* is attached to the GPU and accessed
    by a dedicated memory controller, and, as every beginning CUDA developer knows,
    data must be copied explicitly between host and device memory in order to be processed
    by the GPU.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化性能，CUDA 根据预期的使用情况使用不同类型的内存。*主机内存*指的是系统中附加在 CPU 上的内存。CUDA 提供了 API，通过页面锁定和将其映射到
    GPU(s) 上，从而实现更快的主机内存访问。*设备内存*附加在 GPU 上，并通过专用的内存控制器进行访问，正如每个初学者 CUDA 开发者所知道的，数据必须显式地在主机内存和设备内存之间复制，才能被
    GPU 处理。
- en: Device memory can be allocated and accessed in a variety of ways.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 设备内存可以通过多种方式进行分配和访问。
- en: • *Global memory* may be allocated statically or dynamically and accessed via
    pointers in CUDA kernels, which translate to global load/store instructions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: • *全局内存*可以静态或动态分配，并通过 CUDA 内核中的指针进行访问，这些指针会转换为全局加载/存储指令。
- en: • *Constant memory* is read-only memory accessed via different instructions
    that cause the read requests to be serviced by a cache hierarchy optimized for
    broadcast to multiple threads.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: • *常量内存*是只读内存，通过不同的指令进行访问，这些指令使得读取请求由优化用于广播到多个线程的缓存层次结构处理。
- en: '• *Local memory* contains the stack: local variables that cannot be held in
    registers, parameters, and return addresses for subroutines.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: • *本地内存*包含堆栈：无法存放在寄存器中的局部变量、参数和子程序的返回地址。
- en: • *Texture memory* (in the form of CUDA arrays) is accessed via texture and
    surface load/store instructions. Like constant memory, read requests from texture
    memory are serviced by a separate cache that is optimized for read-only access.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: • *纹理内存*（以 CUDA 数组的形式）通过纹理和表面加载/存储指令进行访问。与常量内存类似，纹理内存的读取请求由一个单独的缓存处理，该缓存经过优化以供只读访问。
- en: '*Shared memory* is an important type of memory in CUDA that is *not* backed
    by device memory. Instead, it is an abstraction for an on-chip “scratchpad” memory
    that can be used for fast data interchange between threads within a block. Physically,
    shared memory comes in the form of built-in memory on the SM: On SM 1.x hardware,
    shared memory is implemented with a 16K RAM; on SM 2.x and more recent hardware,
    shared memory is implemented using a 64K cache that may be partitioned as 48K
    L1/16K shared, or 48K shared/16K L1.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*共享内存*是 CUDA 中一种重要的内存类型，它*不是*由设备内存支持。相反，它是一个抽象，代表了一个片上“临时存储区”内存，用于线程之间在块内进行快速数据交换。从物理上讲，共享内存以内置内存的形式存在于
    SM 上：在 SM 1.x 硬件上，共享内存通过 16K RAM 实现；在 SM 2.x 及更新的硬件上，共享内存通过 64K 缓存实现，可以划分为 48K
    L1/16K 共享，或 48K 共享/16K L1。'
- en: 5.1\. Host Memory
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 主机内存
- en: In CUDA, *host memory* refers to memory accessible to the CPU(s) in the system.
    By default, this memory is *pageable*, meaning the operating system may move the
    memory or evict it out to disk. Because the physical location of pageable memory
    may change without notice, it cannot be accessed by peripherals like GPUs. To
    enable “direct memory access” (DMA) by hardware, operating systems allow host
    memory to be “page-locked,” and for performance reasons, CUDA includes APIs that
    make these operating system facilities available to application developers. So-called
    *pinned memory* that has been page-locked and mapped for direct access by CUDA
    GPU(s) enables
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，*主机内存*是指系统中可以被CPU访问的内存。默认情况下，这些内存是*可分页*的，这意味着操作系统可能会将其移到其他位置或驱逐到磁盘。由于可分页内存的物理位置可能会在不通知的情况下发生变化，GPU等外设无法访问它。为了使硬件能够进行“直接内存访问”（DMA），操作系统允许将主机内存“锁页”，为了性能优化，CUDA提供了API，允许应用程序开发者利用这些操作系统功能。所谓的*锁页内存*，即经过锁页处理并为CUDA
    GPU(s)提供直接访问的内存，使得
- en: • Faster transfer performance
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: • 更快的传输性能
- en: • Asynchronous memory copies (i.e., memory copies that return control to the
    caller before the memory copy necessarily has finished; the GPU does the copy
    in parallel with the CPU)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: • 异步内存复制（即内存复制在复制操作完成之前返回控制给调用者；GPU与CPU并行执行复制操作）
- en: • Mapped pinned memory that can be accessed directly by CUDA kernels
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: • 可由CUDA内核直接访问的映射锁页内存
- en: Because the virtual→physical mapping for pageable memory can change unpredictably,
    GPUs cannot access pageable memory at all. CUDA copies pageable memory using a
    pair of *staging buffers* of pinned memory that are allocated by the driver when
    a CUDA context is allocated. [Chapter 6](ch06.html#ch06) includes hand-crafted
    pageable memcpy routines that use CUDA events to do the synchronization needed
    to manage this double-buffering.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可分页内存的虚拟→物理映射可能会不可预测地变化，GPU根本无法访问可分页内存。CUDA通过一对*暂存缓冲区*来复制可分页内存，这些缓冲区是由驱动程序在分配CUDA上下文时分配的。[第6章](ch06.html#ch06)包括手工编写的可分页memcpy例程，利用CUDA事件来完成所需的同步操作，以管理这种双缓冲。
- en: 5.1.1\. Allocating Pinned Memory
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 分配锁页内存
- en: 'Pinned memory is allocated and freed using special functions provided by CUDA:
    `cudaHostAlloc()/cudaFreeHost()` for the CUDA runtime, and `cuMemHostAlloc()/cuMemFreeHost()`
    for the driver API. These functions work with the host operating system to allocate
    page-locked memory and map it for DMA by the GPU(s).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 锁页内存的分配和释放使用CUDA提供的特殊函数：对于CUDA运行时使用`cudaHostAlloc()/cudaFreeHost()`，对于驱动程序API使用`cuMemHostAlloc()/cuMemFreeHost()`。这些函数与主机操作系统协作，分配锁页内存并为GPU(s)的DMA映射该内存。
- en: CUDA keeps track of memory it has allocated and transparently accelerates memory
    copies that involve host pointers allocated with `cuMemHostAlloc()/cudaHostAlloc()`.
    Additionally, some functions (notably the asynchronous memcpy functions) require
    pinned memory.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA跟踪它已分配的内存，并透明地加速涉及使用`cuMemHostAlloc()/cudaHostAlloc()`分配的主机指针的内存复制。此外，一些函数（特别是异步memcpy函数）需要锁页内存。
- en: The `bandwidthTest` SDK sample enables developers to easily compare the performance
    of pinned memory versus normal pageable memory. The `--memory=pinned` option causes
    the test to use pinned memory instead of pageable memory. [Table 5.1](ch05.html#ch05tab01)
    lists the `bandwidthTest` numbers for a `cg1.4xlarge` instance in Amazon EC2,
    running Windows 7-x64 (numbers in MB/s). Because it involves a significant amount
    of work for the host, including a kernel transition, allocating pinned memory
    is expensive.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`bandwidthTest` SDK 示例使开发者能够轻松比较固定内存与普通可分页内存的性能。`--memory=pinned` 选项会使测试使用固定内存而非可分页内存。[表
    5.1](ch05.html#ch05tab01)列出了在 Amazon EC2 上运行 Windows 7-x64 的 `cg1.4xlarge` 实例的
    `bandwidthTest` 数值（单位为 MB/s）。由于涉及大量主机工作，包括内核切换，分配固定内存的成本较高。'
- en: '![Image](graphics/05tab01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/05tab01.jpg)'
- en: '*Table 5.1* Pinned versus Pageable Bandwidth'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.1* 固定内存与可分页内存带宽'
- en: 'CUDA 2.2 added several features to pinned memory. *Portable pinned memory*
    can be accessed by any GPU; *mapped pinned memory* is mapped into the CUDA address
    space for direct access by CUDA kernels; and *write-combined pinned memory* enables
    faster bus transfers on some systems. CUDA 4.0 added two important features that
    pertain to host memory: Existing host memory ranges can be page-locked in place
    using *host memory registration*, and Unified Virtual Addressing (UVA) enables
    all pointers to be unique process-wide, including host and device pointers. When
    UVA is in effect, the system can infer from the address range whether memory is
    device memory or host memory.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 2.2 增加了几个固定内存的特性。*可移植固定内存*可以被任何 GPU 访问；*映射固定内存*被映射到 CUDA 地址空间，以便 CUDA 内核直接访问；而
    *写合并固定内存* 在某些系统上可以实现更快的总线传输。CUDA 4.0 添加了与主机内存相关的两个重要特性：现有的主机内存区域可以使用 *主机内存注册*
    进行页面锁定，统一虚拟地址（UVA）使得所有指针在整个进程中都是唯一的，包括主机和设备指针。当 UVA 生效时，系统可以根据地址范围推断出内存是设备内存还是主机内存。
- en: 5.1.2\. Portable Pinned Memory
  id: totrans-22
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 可移植固定内存
- en: By default, pinned memory allocations are only accessible to the GPU that is
    current when `cudaHostAlloc()` or `cuMemHostAlloc()` is called. By specifying
    the `cudaHostAllocPortable` flag to `cudaHostAlloc()`, or the `CU_MEMHOSTALLOC_PORTABLE`
    flag to `cuHostMemAlloc()`, applications can request that the pinned allocation
    be mapped for *all* GPUs instead. Portable pinned allocations benefit from the
    transparent acceleration of memcpy described earlier and can participate in asynchronous
    memcpys for any GPU in the system. For applications that intend to use multiple
    GPUs, it is good practice to specify all pinned allocations as portable.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，固定内存分配仅对在 `cudaHostAlloc()` 或 `cuMemHostAlloc()` 调用时的当前 GPU 可访问。通过在 `cudaHostAlloc()`
    中指定 `cudaHostAllocPortable` 标志，或者在 `cuHostMemAlloc()` 中指定 `CU_MEMHOSTALLOC_PORTABLE`
    标志，应用程序可以请求将固定分配映射到 *所有* GPU。可移植的固定分配受益于前面提到的 memcpy 透明加速，并且可以参与任何 GPU 的异步 memcpys。对于计划使用多个
    GPU 的应用程序，最好将所有固定分配指定为可移植的。
- en: '* * *'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When UVA is in effect, all pinned memory allocations are portable.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当 UVA 生效时，所有固定内存分配都是可移植的。
- en: '* * *'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.1.3\. Mapped Pinned Memory
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 映射固定内存
- en: By default, pinned memory allocations are mapped for the GPU outside the CUDA
    address space. They can be directly accessed by the GPU, but only through memcpy
    functions. CUDA kernels cannot read or write the host memory directly. On GPUs
    of SM 1.2 capability and higher, however, CUDA kernels are able to read and write
    host memory directly; they just need allocations to be mapped into the device
    memory address space.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，固定内存分配会映射到 CUDA 地址空间外的 GPU 上。它们可以被 GPU 直接访问，但只能通过 memcpy 函数进行访问。CUDA 内核不能直接读取或写入主机内存。然而，在
    SM 1.2 及更高版本的 GPU 上，CUDA 内核可以直接读取和写入主机内存；它们只需要将分配映射到设备内存地址空间中。
- en: To enable mapped pinned allocations, applications using the CUDA runtime must
    call `cudaSetDeviceFlags()` with the `cudaDeviceMapHost` flag before any initialization
    has been performed. Driver API applications specify the `CU_CTX_MAP_HOST` flag
    to `cuCtxCreate()`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用映射的固定内存分配，使用 CUDA 运行时的应用程序必须在任何初始化操作之前调用`cudaSetDeviceFlags()`并设置`cudaDeviceMapHost`标志。驱动程序
    API 应用程序需要在`cuCtxCreate()`中指定`CU_CTX_MAP_HOST`标志。
- en: Once mapped pinned memory has been enabled, it may be allocated by calling `cudaHostAlloc()`
    with the `cudaHostAllocMapped` flag, or `cuMemHostAlloc()` with the `CU_MEMALLOCHOST_DEVICEMAP`
    flag. Unless UVA is in effect, the application then must query the device pointer
    corresponding to the allocation with `cudaHostGetDevicePointer()` or `cuMemHostGetDevicePointer()`.
    The resulting device pointer then can be passed to CUDA kernels. Best practices
    with mapped pinned memory are described in the section “[Mapped Pinned Memory
    Usage](ch05.html#ch05lev2sec7).”
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启用了映射的固定内存，可以通过调用`cudaHostAlloc()`并设置`cudaHostAllocMapped`标志，或通过`cuMemHostAlloc()`并设置`CU_MEMALLOCHOST_DEVICEMAP`标志来进行分配。除非
    UVA 生效，否则应用程序必须通过`cudaHostGetDevicePointer()`或`cuMemHostGetDevicePointer()`查询与分配对应的设备指针。然后，可以将该设备指针传递给
    CUDA 内核。关于映射固定内存的最佳实践，请参阅章节“[映射固定内存使用](ch05.html#ch05lev2sec7)”。
- en: '* * *'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When UVA is in effect, all pinned memory allocations are mapped.^([1](ch05.html#ch05fn1))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当 UVA 生效时，所有固定内存分配都会被映射。^([1](ch05.html#ch05fn1))
- en: '[1](ch05.html#ch05fn1a). Except those marked as write combining.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch05.html#ch05fn1a). 除非标记为写合并。'
- en: '* * *'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.1.4\. Write-Combined Pinned Memory
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4\. 写合并固定内存
- en: Write-combined memory, also known as write-combining or Uncacheable Write Combining
    (USWC) memory, was created to enable the CPU to write to GPU frame buffers quickly
    and without polluting the CPU cache.^([2](ch05.html#ch05fn2)) To that end, Intel
    added a new page table kind that steered writes into special write-combining buffers
    instead of the main processor cache hierarchy. Later, Intel also added “nontemporal”
    store instructions (e.g., `MOVNTPS` and `MOVNTI`) that enabled applications to
    steer writes into the write-combining buffers on a per-instruction basis. In general,
    memory fence instructions (such as `MFENCE`) are needed to maintain coherence
    with WC memory. These operations are not needed for CUDA applications because
    they are done automatically when the CUDA driver submits work to the hardware.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 写合并内存，也称为写合并或不可缓存写合并（USWC）内存，旨在使CPU能够快速写入GPU帧缓冲区而不会污染CPU缓存。^([2](ch05.html#ch05fn2))
    为此，英特尔新增了一种新的页表类型，将写操作引导到专用的写合并缓冲区，而不是主处理器缓存层次结构。后来，英特尔还增加了“非时间性”存储指令（例如`MOVNTPS`和`MOVNTI`），使得应用程序可以按指令级别将写操作引导到写合并缓冲区。通常，需要使用内存屏障指令（例如`MFENCE`）来维持与WC内存的一致性。对于CUDA应用程序，这些操作不需要手动执行，因为它们在CUDA驱动程序提交工作到硬件时会自动完成。
- en: '[2](ch05.html#ch05fn2a). WC memory originally was announced by Intel in 1997,
    at the same time as the Accelerated Graphics Port (AGP). AGP was used for graphics
    boards before PCI Express.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch05.html#ch05fn2a)。WC内存最初由英特尔在1997年宣布，同时发布的还有加速图形端口（AGP）。AGP曾用于图形卡，后来被PCI
    Express取代。'
- en: For CUDA, write-combining memory can be requested by calling `cudaHostAlloc()`
    with the `cudaHostWriteCombined` flag, or `cuMemHostAlloc()` with the `CU_MEMHOSTALLOC_WRITECOMBINED`
    flag. Besides setting the page table entries to bypass the CPU caches, this memory
    also is not snooped during PCI Express bus transfers. On systems with front side
    buses (pre-Opteron and pre-Nehalem), avoiding the snoops improves PCI Express
    transfer performance. There is little, if any, performance advantage to WC memory
    on NUMA systems.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CUDA，可以通过调用`cudaHostAlloc()`并使用`cudaHostWriteCombined`标志，或者使用`cuMemHostAlloc()`并带上`CU_MEMHOSTALLOC_WRITECOMBINED`标志来请求写合并内存。除了将页表条目设置为绕过CPU缓存之外，这些内存在PCI
    Express总线传输过程中也不会被窥探。在前端总线系统（如Opteron和Nehalem之前的系统）中，避免窥探可以提高PCI Express传输性能。在NUMA系统中，WC内存几乎没有性能优势。
- en: Reading WC memory with the CPU is very slow (about 6x slower), unless the reads
    are done with the `MOVNTDQA` instruction (new with SSE4). On NVIDIA’s integrated
    GPUs, write-combined memory is as fast as the system memory carveout—system memory
    that was set aside at boot time for use by the GPU and is not available to the
    CPU.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 用CPU读取WC内存非常慢（大约慢6倍），除非使用`MOVNTDQA`指令（SSE4新增加的指令）进行读取。在NVIDIA的集成GPU上，写合并内存与系统内存保留区一样快——系统内存是启动时为GPU预留的内存，CPU无法访问。
- en: Despite the purported benefits, as of this writing, there is little reason for
    CUDA developers to use write-combined memory. It’s just too easy for a host memory
    pointer to WC memory to “leak” into some part of the application that would try
    to read the memory. In the absence of empirical evidence to the contrary, it should
    be avoided.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有声称的好处，但截至本文撰写时，CUDA开发人员几乎没有理由使用写合并内存。因为主机内存指针很容易“泄漏”到应用程序的某个部分，从而试图读取该内存。缺乏相反的实验证据时，应该避免使用它。
- en: '* * *'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When UVA is in effect, write-combined pinned allocations are *not* mapped into
    the unified address space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当UVA生效时，写合并固定分配的内存*不会*映射到统一地址空间中。
- en: '* * *'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.1.5\. Registering Pinned Memory
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.5. 注册固定内存
- en: CUDA developers don’t always get the opportunity to allocate host memory they
    want the GPU(s) to access directly. For example, a large, extensible application
    may have an interface that passes pointers to CUDA-aware plugins, or the application
    may be using an API for some other peripheral (notably high-speed networking)
    that has its own dedicated allocation function for much the same reason CUDA does.
    To accommodate these usage scenarios, CUDA 4.0 added the ability to *register*
    pinned memory.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA开发人员并不总是能直接分配主机内存供GPU(s)访问。例如，一个大型的、可扩展的应用程序可能有一个接口，将指针传递给CUDA感知插件，或者该应用程序可能正在使用某个其他外围设备（特别是高速网络）的API，该设备也有自己的分配函数，原因与CUDA类似。为了适应这些使用场景，CUDA
    4.0引入了*注册*固定内存的功能。
- en: Pinned memory registration decouples allocation from the page-locking and mapping
    of host memory. It takes an *already-allocated* virtual address range, page-locks
    it, and maps it for the GPU. Just as with `cudaHostAlloc()`, the memory optionally
    may be mapped into the CUDA address space or made portable (accessible to all
    GPUs).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 固定内存注册将内存分配与主机内存的页面锁定和映射解耦。它采用一个*已经分配*的虚拟地址范围，进行页面锁定，并为GPU映射。与 `cudaHostAlloc()`
    一样，这块内存可以选择性地映射到CUDA地址空间中，或使其可移植（可以被所有GPU访问）。
- en: 'The `cuMemHostRegister()/cudaHostRegister()` and `cuMemHostUnregister()/ cudaHostUnregister()`
    functions register and unregister host memory for access by the GPU(s), respectively.
    The memory range to register must be page-aligned: In other words, both the base
    address and the size must be evenly divisible by the page size of the operating
    system. Applications can allocate page-aligned address ranges in two ways.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuMemHostRegister()/cudaHostRegister()` 和 `cuMemHostUnregister()/cudaHostUnregister()`
    函数分别用于注册和注销主机内存，以便GPU访问。需要注册的内存范围必须是按页面对齐的：换句话说，基地址和大小必须能被操作系统的页面大小整除。应用程序可以通过两种方式分配按页面对齐的地址范围。'
- en: • Allocate the memory with operating system facilities that traffic in whole
    pages, such as `VirtualAlloc()` on Windows or `valloc()` or `mmap()`^([3](ch05.html#ch05fn3))
    on other platforms.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用操作系统的功能分配内存，这些功能处理整个页面，比如Windows上的 `VirtualAlloc()` 或其他平台上的 `valloc()` 或
    `mmap()`^([3](ch05.html#ch05fn3))。
- en: '[3](ch05.html#ch05fn3a). Or `posix_memalign()` in conjunction with `getpagesize()`.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch05.html#ch05fn3a)。或者使用`posix_memalign()`结合`getpagesize()`。'
- en: • Given an arbitrary address range (say, memory allocated with `malloc()` or
    `operator new[]`), clamp the address range to the next-lower page boundary and
    pad to the next page size.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: • 给定一个任意的地址范围（例如，使用`malloc()`或`operator new[]`分配的内存），将地址范围限制为下一个较低的页面边界，并填充到下一个页面大小。
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Even when UVA is in effect, registered pinned memory that has been mapped into
    the CUDA address space has a different device pointer than the host pointer. Applications
    must call `cudaHostGetDevicePointer()/cuMemHostGetDevicePointer()` in order to
    obtain the device pointer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在启用UVA的情况下，已注册并映射到CUDA地址空间的固定内存，其设备指针与主机指针不同。应用程序必须调用`cudaHostGetDevicePointer()/cuMemHostGetDevicePointer()`来获取设备指针。
- en: '* * *'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.1.6\. Pinned Memory and UVA
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.6. 固定内存和UVA
- en: When UVA (Unified Virtual Addressing) is in effect, all pinned memory allocations
    are both mapped and portable. The exceptions to this rule are write-combined memory
    and registered memory. For those, the device pointer may differ from the host
    pointer, and applications still must query it with `cudaHostGetDevicePointer()/cuMemHostGetDevicePointer()`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当UVA（统一虚拟地址）生效时，所有固定内存分配都是既映射又可移植的。此规则的例外是写合并内存和注册内存。对于这些内存，设备指针可能与主机指针不同，应用程序仍然需要通过`cudaHostGetDevicePointer()/cuMemHostGetDevicePointer()`来查询设备指针。
- en: UVA is supported on all 64-bit platforms except Windows Vista and Windows 7\.
    On Windows Vista and Windows 7, only the TCC driver (which may be enabled or disabled
    using `nvidia-smi`) supports UVA. Applications can query whether UVA is in effect
    by calling `cudaGetDeviceProperties()` and examining the `cudaDeviceProp::unifiedAddressing`
    structure member, or by calling `cuDeviceGetAttribute()` with `CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: UVA在除Windows Vista和Windows 7外的所有64位平台上均受支持。在Windows Vista和Windows 7上，只有TCC驱动程序（可以通过`nvidia-smi`启用或禁用）支持UVA。应用程序可以通过调用`cudaGetDeviceProperties()`并检查`cudaDeviceProp::unifiedAddressing`结构成员，或者通过调用`cuDeviceGetAttribute()`并传入`CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING`来查询UVA是否生效。
- en: 5.1.7\. Mapped Pinned Memory Usage
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.7. 映射固定内存的使用
- en: For applications whose performance relies on PCI Express transfer performance,
    mapped pinned memory can be a boon. Since the GPU can read or write host memory
    directly from kernels, it eliminates the need to perform some memory copies, reducing
    overhead. Here are some common idioms for using mapped pinned memory.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于依赖PCI Express传输性能的应用程序，映射的固定内存可以带来好处。由于GPU可以直接从内核读取或写入主机内存，因此不需要进行一些内存复制，从而减少了开销。以下是使用映射的固定内存的一些常见用法。
- en: '• Posting writes to host memory: Multi-GPU applications often must stage results
    back to system memory for interchange with other GPUs; writing these results via
    mapped pinned memory avoids an extraneous device→host memory copy. Write-only
    access patterns to host memory are appealing because there is no latency to cover.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: • 向主机内存写入数据：多 GPU 应用程序通常必须将结果写回系统内存，以便与其他 GPU 交换；通过映射的固定内存写入这些结果，可以避免不必要的设备→主机内存复制。对于主机内存的只写访问模式很有吸引力，因为不需要处理延迟。
- en: '• Streaming: These workloads otherwise would use CUDA streams to coordinate
    concurrent memcpys to and from device memory, while kernels do their processing
    on device memory.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: • 流式传输：这些工作负载通常会使用 CUDA 流来协调设备内存之间的并行内存复制操作，同时内核在设备内存上进行处理。
- en: '• “Copy with panache”: Some workloads benefit from performing computations
    as data is transferred across PCI Express. For example, the GPU may compute subarray
    reductions while transferring data for Scan.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: • “有风度的复制”：某些工作负载通过在数据传输穿越 PCI Express 时执行计算从而受益。例如，GPU 可以在传输 Scan 数据时进行子数组归约计算。
- en: Caveats
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: Mapped pinned memory is not a panacea. Here are some caveats to consider when
    using it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 映射的固定内存不是万能的。在使用它时，需考虑以下一些警告。
- en: • Texturing from mapped pinned memory is possible, but very slow.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: • 从映射的固定内存进行纹理映射是可能的，但速度非常慢。
- en: • It is important that mapped pinned memory be accessed with coalesced memory
    transactions (see [Section 5.2.9](ch05.html#ch05lev2sec17)). The performance penalty
    for uncoalesced memory transactions ranges from 6x to 2x. But even on SM 2.x and
    later GPUs, whose caches were supposed to make coalescing an obsolete consideration,
    the penalty is significant.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: • 映射的固定内存必须通过合并内存事务来访问（参见[第 5.2.9 节](ch05.html#ch05lev2sec17)）。未合并的内存事务的性能损失范围为
    6 倍到 2 倍。但即使在 SM 2.x 及以后的 GPU 上，缓存本应使合并操作成为过时的考虑因素，性能损失依然显著。
- en: • Polling host memory with a kernel (e.g., for CPU/GPU synchronization) is not
    recommended.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用内核轮询主机内存（例如，进行 CPU/GPU 同步）是不推荐的。
- en: • Do not try to use atomics on mapped pinned host memory, either for the host
    (locked compare-exchange) or the device (`atomicAdd()`). On the CPU side, the
    facilities to enforce mutual exclusion for locked operations are not visible to
    peripherals on the PCI Express bus. Conversely, on the GPU side, atomic operations
    only work on local device memory locations because they are implemented using
    the GPU’s local memory controller.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: • 不要尝试在映射的固定主机内存上使用原子操作，无论是在主机（锁定比较-交换）还是在设备端（`atomicAdd()`）。在 CPU 端，用于强制互斥的设施对于
    PCI Express 总线上的外设是不可见的。相反，在 GPU 端，原子操作仅在本地设备内存位置上有效，因为它们是通过 GPU 的本地内存控制器实现的。
- en: 5.1.8\. NUMA, Thread Affinity, and Pinned Memory
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.8. NUMA、线程亲和性和固定内存
- en: Beginning with the AMD Opteron and Intel Nehalem, CPU memory controllers were
    integrated directly into CPUs. Previously, the memory had been attached to the
    so-called “front-side bus” (FSB) of the “northbridge” of the chipset. In multi-CPU
    systems, the northbridge could service memory requests from any CPU, and memory
    access performance was reasonably uniform from one CPU to another. With the introduction
    of integrated memory controllers, each CPU has its own dedicated pool of “local”
    physical memory that is directly attached to that CPU. Although any CPU can access
    any other CPU’s memory, “nonlocal” accesses—accesses by one CPU to memory attached
    to another CPU—are performed across the AMD HyperTransport (HT) or Intel QuickPath
    Interconnect (QPI), incurring latency penalties and bandwidth limitations. To
    contrast with the uniform memory access times exhibited by systems with FSBs,
    these system architectures are known as NUMA for *nonuniform memory access*.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从AMD Opteron和Intel Nehalem开始，CPU内存控制器直接集成到CPU中。此前，内存是连接到芯片组“北桥”的所谓“前端总线”（FSB）。在多CPU系统中，北桥可以为任何CPU提供内存请求服务，并且内存访问性能在不同CPU之间是比较均匀的。随着集成内存控制器的引入，每个CPU都有自己专用的“本地”物理内存池，直接连接到该CPU。虽然任何CPU都可以访问其他CPU的内存，但“非本地”访问——一个CPU访问连接到另一个CPU的内存——是通过AMD
    HyperTransport（HT）或Intel QuickPath Interconnect（QPI）执行的，这会带来延迟惩罚和带宽限制。为了与FSB系统所表现出的统一内存访问时间对比，这些系统架构被称为NUMA，即*非统一内存访问*。
- en: As you can imagine, performance of multithreaded applications can be heavily
    dependent on whether memory references are local to the CPU that is running the
    current thread. For most applications, however, the higher cost of a nonlocal
    access is offset by the CPUs’ on-board caches. Once nonlocal memory is fetched
    into a CPU, it remains in-cache until evicted or needed by a memory access to
    the same page by another CPU. In fact, it is common for NUMA systems to include
    a System BIOS option to “interleave” memory physically between CPUs. When this
    BIOS option is enabled, the memory is evenly divided between CPUs on a per-cache
    line (typically 64 bytes) basis, so, for example, on a 2-CPU system, about 50%
    of memory accesses will be nonlocal on average.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，多线程应用程序的性能可能在很大程度上依赖于内存引用是否位于正在运行当前线程的CPU本地。然而，对于大多数应用程序来说，非本地访问的较高成本会被CPU的板载缓存所抵消。一旦非本地内存被提取到CPU中，它将在缓存中保持，直到被驱逐或被另一个CPU通过对同一页面的内存访问所需。在NUMA系统中，通常会包括一个系统BIOS选项，用于在物理上将内存在CPU之间“交错”分布。当启用此BIOS选项时，内存会按照每个缓存行（通常是64字节）在CPU之间均匀分配，因此例如，在一个2个CPU的系统中，大约50%的内存访问将是非本地的。
- en: For CUDA applications, PCI Express transfer performance can be dependent on
    whether memory references are local. If there is more than one I/O hub (IOH) in
    the system, the GPU(s) attached to a given IOH have better performance and reduce
    demand for QPI bandwidth when the pinned memory is local. Because some high-end
    NUMA systems are hierarchical but don’t associate the pools of memory bandwidth
    strictly with CPUs, NUMA APIs refer to *nodes* that may or may not strictly correspond
    with CPUs in the system.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CUDA应用程序，PCI Express传输性能可能取决于内存引用是否是本地的。如果系统中有多个I/O集线器（IOH），连接到给定IOH的GPU性能更好，并且当固定内存是本地时，减少了对QPI带宽的需求。因为一些高端NUMA系统是分层的，但并不严格将内存带宽池与CPU关联，所以NUMA
    API指代的*节点*可能不严格对应系统中的CPU。
- en: If NUMA is enabled on the system, it is good practice to allocate host memory
    on the same node as a given GPU. Unfortunately, there is no official CUDA API
    to affiliate a GPU with a given CPU. Developers with a priori knowledge of the
    system design may know which node to associate with which GPU. Then platform-specific,
    NUMA-aware APIs may be used to perform these memory allocations, and host memory
    registration (see [Section 5.1.5](ch05.html#ch05lev2sec5)) can be used to pin
    those virtual allocations and map them for the GPU(s).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统启用了NUMA，最佳实践是将主机内存分配到与给定GPU相同的节点。不幸的是，目前没有官方的CUDA API来将GPU与特定的CPU关联。开发人员如果对系统设计有先验知识，可能会知道应将哪个节点与哪个GPU关联。然后，可以使用平台特定的NUMA感知API来执行这些内存分配，并且可以使用主机内存注册（参见[第5.1.5节](ch05.html#ch05lev2sec5)）来固定这些虚拟分配，并为GPU进行映射。
- en: '[Listing 5.1](ch05.html#ch05lis01) gives a code fragment to perform NUMA-aware
    allocations on Linux,^([4](ch05.html#ch05fn4)) and [Listing 5.2](ch05.html#ch05lis02)
    gives a code fragment to perform NUMA-aware allocations on Windows.^([5](ch05.html#ch05fn5))'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 5.1](ch05.html#ch05lis01)提供了一个代码片段，用于在Linux上执行NUMA感知分配，^([4](ch05.html#ch05fn4))，而[示例
    5.2](ch05.html#ch05lis02)提供了一个代码片段，用于在Windows上执行NUMA感知分配。^([5](ch05.html#ch05fn5))'
- en: '[4](ch05.html#ch05fn4a). [http://bit.ly/USy4e7](http://bit.ly/USy4e7)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch05.html#ch05fn4a). [http://bit.ly/USy4e7](http://bit.ly/USy4e7)'
- en: '[5](ch05.html#ch05fn5a). [http://bit.ly/XY1g8m](http://bit.ly/XY1g8m)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch05.html#ch05fn5a). [http://bit.ly/XY1g8m](http://bit.ly/XY1g8m)'
- en: '*Listing 5.1.* NUMA-aware allocation (Linux).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 5.1.* NUMA感知分配（Linux）。'
- en: '[Click here to view code image](ch05_images.html#p05lis01a)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis01a)'
- en: '* * *'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: bool
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: bool
- en: numNodes( int *p )
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: numNodes( int *p )
- en: '{'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: if ( numa_available() >= 0 ) {
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: if ( numa_available() >= 0 ) {
- en: '*p = numa_max_node() + 1;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*p = numa_max_node() + 1;'
- en: return true;
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: return true;
- en: '}'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return false;
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: return false;
- en: '}'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void *
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: void *
- en: pageAlignedNumaAlloc( size_t bytes, int node )
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: pageAlignedNumaAlloc( size_t bytes, int node )
- en: '{'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: void *ret;
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: void *ret;
- en: printf( "Allocating on node %d\n", node ); fflush(stdout);
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "在节点 %d 上分配内存\n", node ); fflush(stdout);
- en: ret = numa_alloc_onnode( bytes, node );
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ret = numa_alloc_onnode( bytes, node );
- en: return ret;
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: return ret;
- en: '}'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: pageAlignedNumaFree( void *p, size_t bytes )
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: pageAlignedNumaFree( void *p, size_t bytes )
- en: '{'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: numa_free( p, bytes );
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: numa_free( p, bytes );
- en: '}'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 5.2.* NUMA-aware allocation (Windows).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 5.2.* NUMA感知分配（Windows）。'
- en: '[Click here to view code image](ch05_images.html#p05lis02a)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p05lis02a)'
- en: '* * *'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: bool
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: bool
- en: numNodes( int *p )
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: numNodes( int *p )
- en: '{'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: ULONG maxNode;
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ULONG maxNode;
- en: if ( GetNumaHighestNodeNumber( &maxNode ) ) {
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: if ( GetNumaHighestNodeNumber( &maxNode ) ) {
- en: '*p = (int) maxNode+1;'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*p = (int) maxNode+1;'
- en: return true;
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: return true;
- en: '}'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return false;
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: return false;
- en: '}'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void *
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: void *
- en: pageAlignedNumaAlloc( size_t bytes, int node )
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: pageAlignedNumaAlloc( size_t bytes, int node )
- en: '{'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: void *ret;
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: void *ret;
- en: printf( "Allocating on node %d\n", node ); fflush(stdout);
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "Allocating on node %d\n", node ); fflush(stdout);
- en: ret = VirtualAllocExNuma( GetCurrentProcess(),
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ret = VirtualAllocExNuma( GetCurrentProcess(),
- en: NULL,
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: NULL，
- en: bytes,
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: bytes，
- en: MEM_COMMIT | MEM_RESERVE,
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: MEM_COMMIT | MEM_RESERVE，
- en: PAGE_READWRITE,
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: PAGE_READWRITE，
- en: node );
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: node );
- en: return ret;
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: return ret;
- en: '}'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: pageAlignedNumaFree( void *p )
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: pageAlignedNumaFree( void *p )
- en: '{'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: VirtualFreeEx( GetCurrentProcess(), p, 0, MEM_RELEASE );
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: VirtualFreeEx( GetCurrentProcess(), p, 0, MEM_RELEASE );
- en: '}'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.2\. Global Memory
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 全局内存
- en: 'Global memory is the main abstraction by which CUDA kernels read or write device
    memory.^([6](ch05.html#ch05fn6)) Since device memory is directly attached to the
    GPU and read and written using a memory controller integrated into the GPU, the
    peak bandwidth is extremely high: typically more than 100G/s for high-end CUDA
    cards.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 全局内存是CUDA内核读写设备内存的主要抽象方式。^([6](ch05.html#ch05fn6)) 由于设备内存直接连接到GPU，并通过集成到GPU中的内存控制器进行读写，因此其峰值带宽极高：高端CUDA卡的带宽通常超过100G/s。
- en: '[6](ch05.html#ch05fn6a). For maximum developer confusion, CUDA uses the term
    *device pointer* to refer to pointers that reside in *global memory* (device memory
    addressable by CUDA kernels).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](ch05.html#ch05fn6a)。为了最大限度地增加开发者的困惑，CUDA使用*设备指针*一词来指代驻留在*全局内存*中的指针（由CUDA内核可寻址的设备内存）。'
- en: Device memory can be accessed by CUDA kernels using *device pointers*. The following
    simple memset kernel gives an example.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 设备内存可以通过CUDA内核使用*设备指针*进行访问。以下是一个简单的memset内核示例。
- en: '[Click here to view code image](ch05_images.html#p131pro01a)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p131pro01a)'
- en: template<class T>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: __global__ void
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: GPUmemset( int *base, int value, size_t N )
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: GPUmemset( int *base, int value, size_t N )
- en: '{'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
- en: i < N;
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += gridDim.x*blockDim.x )
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: i += gridDim.x*blockDim.x )
- en: '{'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: base[i] = value;
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: base[i] = value;
- en: '}'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: The device pointer `base` resides in the *device address space*, separate from
    the CPU address space used by the host code in the CUDA program. As a result,
    host code in the CUDA program can perform pointer arithmetic on device pointers,
    but they may not dereference them.^([7](ch05.html#ch05fn7))
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 设备指针`base`驻留在*设备地址空间*中，与CUDA程序中主机代码使用的CPU地址空间分开。因此，CUDA程序中的主机代码可以对设备指针进行指针运算，但不能解引用它们。^([7](ch05.html#ch05fn7))
- en: '[7](ch05.html#ch05fn7a). Mapped pinned pointers represent an exception to this
    rule. They are located in system memory but can be accessed by the GPU. On non-UVA
    systems, the host and device pointers to this memory are different: The application
    must call `cuMemHostGetDevicePointer()` or `cudaHostGetDevicePointer()` to map
    the host pointer to the corresponding device pointer. But when UVA is in effect,
    the pointers are the same.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](ch05.html#ch05fn7a). 映射的固定指针是此规则的例外。它们位于系统内存中，但可以被GPU访问。在非UVA系统中，指向此内存的主机指针和设备指针是不同的：应用程序必须调用`cuMemHostGetDevicePointer()`或`cudaHostGetDevicePointer()`将主机指针映射到相应的设备指针。但当启用UVA时，指针是相同的。'
- en: This kernel writes the integer `value` into the address range given by `base`
    and `N`. The references to `blockIdx`, `blockDim`, and `gridDim` enable the kernel
    to operate correctly, using whatever block and grid parameters were specified
    to the kernel launch.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核将整数`value`写入由`base`和`N`给定的地址范围。对`blockIdx`、`blockDim`和`gridDim`的引用使得内核能够正确运行，使用启动内核时指定的块和网格参数。
- en: 5.2.1\. Pointers
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 指针
- en: When using the CUDA runtime, device pointers and host pointers both are typed
    as `void *`. The driver API uses an integer-valued typedef called `CUdeviceptr`
    that is the same width as host pointers (i.e., 32 bits on 32-bit operating systems
    and 64 bits on 64-bit operating systems), as follows.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用CUDA运行时，设备指针和主机指针都被定义为`void *`类型。驱动程序API使用一个称为`CUdeviceptr`的整数类型定义，它与主机指针的宽度相同（即在32位操作系统上为32位，在64位操作系统上为64位），如下所示。
- en: '[Click here to view code image](ch05_images.html#p131pro02a)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p131pro02a)'
- en: '#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#if defined(__x86_64) || defined(AMD64) || defined(_M_AMD64)'
- en: typedef unsigned long long CUdeviceptr;
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: typedef unsigned long long CUdeviceptr;
- en: '#else'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#else'
- en: typedef unsigned int CUdeviceptr;
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: typedef unsigned int CUdeviceptr;
- en: '#endif'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#endif'
- en: The `uintptr_t` type, available in `<stdint.h>` and introduced in C++0x, may
    be used to portably convert between host pointers (`void *`) and device pointers
    (`CUdeviceptr`), as follows.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`uintptr_t`类型，定义在`<stdint.h>`中，并在C++0x中引入，可用于在主机指针（`void *`）和设备指针（`CUdeviceptr`）之间便捷地转换，如下所示。'
- en: '[Click here to view code image](ch05_images.html#p131pro03a)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p131pro03a)'
- en: CUdeviceptr devicePtr;
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: CUdeviceptr devicePtr;
- en: void *p;
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: void *p;
- en: p = (void *) (uintptr_t) devicePtr;
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: p = (void *) (uintptr_t) devicePtr;
- en: devicePtr = (CUdeviceptr) (uintptr_t) p;
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: devicePtr = (CUdeviceptr) (uintptr_t) p;
- en: The host can do pointer arithmetic on device pointers to pass to a kernel or
    memcpy call, but the host cannot read or write device memory with these pointers.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 主机可以对设备指针进行指针运算，以传递给内核或memcpy调用，但主机无法通过这些指针读取或写入设备内存。
- en: 32- and 64-Bit Pointers in the Driver API
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序API中的32位和64位指针
- en: Because the original driver API definition for a pointer was 32-bit, the addition
    of 64-bit support to CUDA required the definition of `CUdeviceptr` and, in turn,
    all driver API functions that took `CUdeviceptr` as a parameter, to change.^([8](ch05.html#ch05fn8))
    `cuMemAlloc()`, for example, changed from
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于原始驱动程序 API 中对指针的定义是 32 位的，因此将 CUDA 支持 64 位时，需要对 `CUdeviceptr` 的定义进行修改，并且所有以
    `CUdeviceptr` 为参数的驱动程序 API 函数都需要进行更改。^([8](ch05.html#ch05fn8)) 例如，`cuMemAlloc()`
    从
- en: '[8](ch05.html#ch05fn8a). The old functions had to stay for compatibility reasons.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[8](ch05.html#ch05fn8a). 出于兼容性原因，旧的函数必须保留。'
- en: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, unsigned int bytesize);
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, unsigned int bytesize);
- en: to
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到
- en: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
- en: To accommodate both old applications (which linked against a `cuMemAlloc()`
    with 32-bit `CUdeviceptr` and size) and new ones, `cuda.h` includes two blocks
    of code that use the preprocessor to change the bindings without requiring function
    names to be changed as developers update to the new API.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了兼容旧版应用程序（这些应用程序使用的是链接到 `cuMemAlloc()` 的 32 位 `CUdeviceptr` 和大小）和新版应用程序，`cuda.h`
    包含了两段代码，使用预处理器在开发人员更新到新 API 时无需更改函数名称即可改变绑定。
- en: First, a block of code surreptitiously changes function names to map to newer
    functions that have different semantics.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，一段代码悄悄地将函数名称修改为映射到语义不同的更新函数。
- en: '[Click here to view code image](ch05_images.html#p132pro01a)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch05_images.html#p132pro01a)'
- en: '#if defined(__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION >= 3020'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#if defined(__CUDA_API_VERSION_INTERNAL) || __CUDA_API_VERSION >= 3020'
- en: '#define cuDeviceTotalMem cuDeviceTotalMem_v2'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '#define cuDeviceTotalMem cuDeviceTotalMem_v2'
- en: '...'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: '#define cuTexRefGetAddress cuTexRefGetAddress_v2'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#define cuTexRefGetAddress cuTexRefGetAddress_v2'
- en: '#endif /* __CUDA_API_VERSION_INTERNAL || __CUDA_API_VERSION >= 3020 */'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#endif /* __CUDA_API_VERSION_INTERNAL || __CUDA_API_VERSION >= 3020 */'
- en: This way, the client code uses the same old function names, but the compiled
    code generates references to the new function names with `_v2` appended.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，客户端代码使用相同的旧函数名称，但编译后的代码会生成带有 `_v2` 后缀的新函数名称的引用。
- en: Later in the header, the old functions are defined as they were. As a result,
    developers compiling for the latest version of CUDA get the latest function definitions
    and semantics. `cuda.h` uses a similar strategy for functions whose semantics
    changed from one version to the next, such as `cuStreamDestroy()`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在头文件的后面，旧的函数仍然按原样定义。因此，为最新版本的 CUDA 编译的开发人员将获得最新的函数定义和语义。`cuda.h` 对于那些语义发生变化的函数（例如
    `cuStreamDestroy()`）也使用类似的策略。
- en: 5.2.2\. Dynamic Allocations
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 动态分配
- en: Most global memory in CUDA is obtained through dynamic allocation. Using the
    CUDA runtime, the functions
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CUDA 中，大部分全局内存通过动态分配获得。使用 CUDA 运行时，相关函数
- en: '[Click here to view code image](ch05_images.html#p133pro01a)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch05_images.html#p133pro01a)'
- en: cudaError_t cudaMalloc( void **, size_t );
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMalloc( void **, size_t );
- en: cudaError_t cudaFree( void );
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaFree( void );
- en: allocate and free global memory, respectively. The corresponding driver API
    functions are
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 分别分配和释放全局内存。相应的驱动程序 API 函数是
- en: '[Click here to view code image](ch05_images.html#p133pro02a)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch05_images.html#p133pro02a)'
- en: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemAlloc(CUdeviceptr *dptr, size_t bytesize);
- en: CUresult CUDAAPI cuMemFree(CUdeviceptr dptr);
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemFree(CUdeviceptr dptr);
- en: Allocating global memory is expensive. The CUDA driver implements a sub-allocator
    to satisfy small allocation requests, but if the suballocator must create a new
    memory block, that requires an expensive operating system call to the kernel mode
    driver. If that happens, the CUDA driver also must synchronize with the GPU, which
    may break CPU/GPU concurrency. As a result, it’s good practice to avoid allocating
    or freeing global memory in performance-sensitive code.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 分配全局内存是昂贵的。CUDA 驱动程序实现了一个子分配器来满足小规模的分配请求，但如果子分配器必须创建一个新的内存块，这将需要一次昂贵的操作系统调用到内核模式驱动程序。如果发生这种情况，CUDA
    驱动程序还必须与 GPU 同步，这可能会破坏 CPU/GPU 的并发性。因此，在性能敏感的代码中，避免分配或释放全局内存是一个好的实践。
- en: Pitched Allocations
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 斜向分配
- en: The coalescing constraints, coupled with alignment restrictions for texturing
    and 2D memory copy, motivated the creation of *pitched* memory allocations. The
    idea is that when creating a 2D array, a pointer into the array should have the
    same alignment characteristics when updated to point to a different row. The *pitch*
    of the array is the number of bytes per row of the array.^([9](ch05.html#ch05fn9))
    The pitch allocations take a width (in bytes) and height, pad the width to a suitable
    hardware-specific pitch, and pass back the base pointer and pitch of the allocation.
    By using these allocation functions to delegate selection of the pitch to the
    driver, developers can future-proof their code against architectures that widen
    alignment requirements.^([10](ch05.html#ch05fn10))
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 合并约束和纹理以及 2D 内存复制的对齐限制促使了*斜向*内存分配的创建。其理念是，在创建二维数组时，指向数组的指针在更新指向不同的行时应该具有相同的对齐特性。数组的*步幅*是每行数组的字节数。^([9](ch05.html#ch05fn9))
    步幅分配接受一个宽度（以字节为单位）和高度，将宽度填充到适合硬件的步幅，然后返回分配的基指针和步幅。通过使用这些分配函数将步幅的选择委托给驱动程序，开发人员可以使他们的代码对未来需要更宽对齐要求的架构具有兼容性。^([10](ch05.html#ch05fn10))
- en: '[9](ch05.html#ch05fn9a). The idea of padding 2D allocations is much older than
    CUDA. Graphics APIs such as Apple QuickDraw and Microsoft DirectX exposed “rowBytes”
    and “pitch,” respectively. At one time, the padding simplified addressing computations
    by replacing a multiplication by a shift, or even replacing a multiplication by
    two shifts and an add with “two powers of 2” such as 640 (512 + 128). But these
    days, integer multiplication is so fast that pitch allocations have other motivations,
    such as avoiding negative performance interactions with caches.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[9](ch05.html#ch05fn9a)。二维内存分配的填充概念早于CUDA。图形API，如Apple QuickDraw和Microsoft
    DirectX，分别暴露了“rowBytes”和“pitch”。曾几何时，填充通过将乘法替换为位移操作，甚至将乘法替换为两次位移加一次加法的方式简化了寻址计算，这种方式使用了“两个2的幂”，例如640（512
    + 128）。但如今，整数乘法已经非常快速，以至于pitch分配有了其他动机，例如避免与缓存的负面性能交互。'
- en: '[10](ch05.html#ch05fn10a). Not an unexpected trend. Fermi widened several alignment
    requirements over Tesla.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '[10](ch05.html#ch05fn10a)。这一趋势并不令人意外。Fermi相比Tesla扩大了几个对齐要求。'
- en: CUDA programs often must adhere to alignment constraints enforced by the hardware,
    not only on base addresses but also on the widths (in bytes) of memory copies
    and linear memory bound to textures. Because the alignment constraints are hardware-specific,
    CUDA provides APIs that enable developers to delegate the selection of the appropriate
    alignment to the driver. Using these APIs enables CUDA applications to implement
    hardware-independent code and to be “future-proof” against CUDA architectures
    that have not yet shipped.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA程序通常必须遵循硬件强制执行的对齐约束，不仅是基础地址，还包括内存复制和绑定到纹理的线性内存的宽度（以字节为单位）。由于对齐约束是硬件特定的，CUDA提供了API，允许开发者将选择适当对齐的任务委托给驱动程序。使用这些API使得CUDA应用能够实现硬件独立的代码，并且对尚未发布的CUDA架构具有“未来兼容性”。
- en: '[Figure 5.1](ch05.html#ch05fig01) shows a pitch allocation being performed
    on an array that is 352 bytes wide. The pitch is padded to the next multiple of
    64 bytes before allocating the memory. Given the pitch of the array in addition
    to the row and column, the address of an array element can be computed as follows.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.1](ch05.html#ch05fig01)展示了在一个宽度为352字节的数组上执行pitch分配的过程。在分配内存之前，pitch被填充到下一个64字节的倍数。给定数组的pitch以及行和列，可以按如下方式计算数组元素的地址。'
- en: '[Click here to view code image](ch05_images.html#p134pro01a)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch05_images.html#p134pro01a)'
- en: inline T *
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: inline T *
- en: getElement( T *base, size_t Pitch, int row, int col )
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: getElement( T *base, size_t Pitch, int row, int col )
- en: '{'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: return (T *) ((char *) base + row*Pitch) + col;
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: return (T *) ((char *) base + row*Pitch) + col;
- en: '}'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '![Image](graphics/05fig01.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/05fig01.jpg)'
- en: '*Figure 5.1* Pitch versus width.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.1* Pitch与宽度的关系。'
- en: The CUDA runtime function to perform a pitched allocation is as follows.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 执行pitch分配的CUDA运行时函数如下。
- en: '[Click here to view code image](ch05_images.html#p134pro02a)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch05_images.html#p134pro02a)'
- en: template<class T>
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: __inline__ __host__ cudaError_t cudaMallocPitch(
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: __inline__ __host__ cudaError_t cudaMallocPitch(
- en: T **devPtr,
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: T **devPtr,
- en: size_t *pitch,
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: size_t *pitch,
- en: size_t widthInBytes,
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: size_t widthInBytes,
- en: size_t height
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: size_t height
- en: );
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: );
- en: The CUDA runtime also includes the function `cudaMalloc3D()`, which allocates
    3D memory regions using the `cudaPitchedPtr` and `cudaExtent` structures.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 运行时还包括 `cudaMalloc3D()` 函数，该函数使用 `cudaPitchedPtr` 和 `cudaExtent` 结构分配 3D
    内存区域。
- en: '[Click here to view code image](ch05_images.html#p134pro03a)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p134pro03a)'
- en: extern __host__ cudaError_t CUDARTAPI cudaMalloc3D(struct
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: extern __host__ cudaError_t CUDARTAPI cudaMalloc3D(struct
- en: cudaPitchedPtr* pitchedDevPtr, struct cudaExtent extent);
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: cudaPitchedPtr* pitchedDevPtr, struct cudaExtent extent);
- en: '`cudaPitchedPtr`, which receives the allocated memory, is defined as follows.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaPitchedPtr`，它接收分配的内存，定义如下。'
- en: struct cudaPitchedPtr
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr
- en: '{'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: void *ptr;
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: void *ptr;
- en: size_t pitch;
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: size_t pitch;
- en: size_t xsize;
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: size_t xsize;
- en: size_t ysize;
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: size_t ysize;
- en: '};'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '`cudaPitchedPtr::ptr` specifies the pointer; `cudaPitchedPtr::pitch` specifies
    the pitch (width in bytes) of the allocation; and `cudaPitchedPtr::xsize` and
    `cudaPitchedPtr::ysize` are the logical width and height of the allocation, respectively.
    `cudaExtent` is defined as follows.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaPitchedPtr::ptr` 指定指针；`cudaPitchedPtr::pitch` 指定分配的步幅（字节宽度）；`cudaPitchedPtr::xsize`
    和 `cudaPitchedPtr::ysize` 分别指定分配的逻辑宽度和高度。`cudaExtent` 定义如下。'
- en: struct cudaExtent
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaExtent
- en: '{'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t width;
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: size_t width;
- en: size_t height;
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: size_t height;
- en: size_t depth;
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: size_t depth;
- en: '};'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '`cudaExtent::width` is treated differently for arrays and linear device memory.
    For arrays, it specifies the width in array elements; for linear device memory,
    it specifies the pitch (width in bytes).'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaExtent::width` 对于数组和线性设备内存的处理有所不同。对于数组，它指定数组元素的宽度；对于线性设备内存，它指定步幅（字节宽度）。'
- en: The driver API function to allocate memory with a pitch is as follows.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分配具有步幅内存的驱动 API 函数如下所示。
- en: '[Click here to view code image](ch05_images.html#p135pro01a)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p135pro01a)'
- en: CUresult CUDAAPI cuMemAllocPitch(CUdeviceptr *dptr, size_t *pPitch,
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemAllocPitch(CUdeviceptr *dptr, size_t *pPitch,
- en: size_t WidthInBytes, size_t Height, unsigned int ElementSizeBytes);
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: size_t WidthInBytes, size_t Height, unsigned int ElementSizeBytes);
- en: The `ElementSizeBytes` parameter may be 4, 8, or 16 bytes, and it causes the
    allocation pitch to be padded to 64-, 128-, or 256-byte boundaries. Those are
    the alignment requirements for coalescing of 4-, 8-, and 16-byte memory transactions
    on SM 1.0 and SM 1.1 hardware. Applications that are not concerned with running
    well on that hardware can specify 4.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`ElementSizeBytes` 参数可能是 4、8 或 16 字节，并且它会使分配的步幅（pitch）对齐到 64、128 或 256 字节的边界。这些是用于在
    SM 1.0 和 SM 1.1 硬件上合并 4 字节、8 字节和 16 字节内存事务的对齐要求。那些不关心在该硬件上运行良好的应用程序可以指定为 4。'
- en: The pitch returned by `cudaMallocPitch()/cuMemAllocPitch()` is the width-in-bytes
    passed in by the caller, padded to an alignment that meets the alignment constraints
    for both coalescing of global load/store operations, and texture bind APIs. The
    amount of memory allocated is `height*pitch`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaMallocPitch()/cuMemAllocPitch()` 返回的步幅是调用方传入的以字节为单位的宽度，填充至满足全局加载/存储操作合并以及纹理绑定
    API 对齐约束的对齐方式。分配的内存量为 `height*pitch`。'
- en: For 3D arrays, developers can multiply the height by the depth before performing
    the allocation. This consideration only applies to arrays that will be accessed
    via global loads and stores, since 3D textures cannot be bound to global memory.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三维数组，开发人员可以在进行分配之前，将高度乘以深度。这个考虑只适用于通过全局加载和存储访问的数组，因为三维纹理不能绑定到全局内存。
- en: Allocations within Kernels
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内核中的内存分配
- en: Fermi-class hardware can dynamically allocate global memory using `malloc()`.
    Since this may require the GPU to interrupt the CPU, it is potentially slow. The
    sample program `mallocSpeed.cu` measures the performance of `malloc()` and `free()`
    in kernels.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi架构的硬件可以使用`malloc()`动态分配全局内存。由于这可能需要GPU中断CPU，因此可能会比较慢。示例程序`mallocSpeed.cu`测量了内核中`malloc()`和`free()`的性能。
- en: '[Listing 5.3](ch05.html#ch05lis03) shows the key kernels and timing routine
    in `mallocSpeed.cu`. As an important note, the `cudaSetDeviceLimit()` function
    must be called with `cudaLimitMallocHeapSize` before `malloc()` may be called
    in kernels. The invocation in `mallocSpeed.cu` requests a full gigabyte (2^(30)
    bytes).'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 5.3](ch05.html#ch05lis03)展示了`mallocSpeed.cu`中的关键内核和计时例程。一个重要的说明是，必须在内核中调用`malloc()`之前，先调用`cudaSetDeviceLimit()`函数并使用`cudaLimitMallocHeapSize`。`mallocSpeed.cu`中的调用请求了一个完整的千兆字节（2^(30)字节）。'
- en: CUDART_CHECK( cudaDeviceSetLimit(cudaLimitMallocHeapSize, 1<<30) );
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaDeviceSetLimit(cudaLimitMallocHeapSize, 1<<30) );
- en: When `cudaDeviceSetLimit()` is called, the requested amount of memory is allocated
    and may not be used for any other purpose.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用`cudaDeviceSetLimit()`时，请求的内存量将被分配，并且可能不能用于其他任何用途。
- en: '*Listing 5.3.* `MallocSpeed` function and kernels.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5.3.* `MallocSpeed` 函数和内核。'
- en: '[Click here to view code image](ch05_images.html#p05lis03a)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p05lis03a)'
- en: '* * *'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: AllocateBuffers( void **out, size_t N )
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: AllocateBuffers( void **out, size_t N )
- en: '{'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i = blockIdx.x*blockDim.x + threadIdx.x;
- en: out[i] = malloc( N );
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] = malloc( N );
- en: '}'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __global__ void
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: FreeBuffers( void **in )
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: FreeBuffers( void **in )
- en: '{'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i = blockIdx.x*blockDim.x + threadIdx.x;
- en: free( in[i] );
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: free( in[i] );
- en: '}'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaError_t
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t
- en: MallocSpeed( double *msPerAlloc, double *msPerFree,
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: MallocSpeed( double *msPerAlloc, double *msPerFree,
- en: void **devicePointers, size_t N,
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: void **devicePointers, size_t N,
- en: cudaEvent_t evStart, cudaEvent_t evStop,
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEvent_t evStart, cudaEvent_t evStop,
- en: int cBlocks, int cThreads )
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: int cBlocks, int cThreads )
- en: '{'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float etAlloc, etFree;
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: float etAlloc, etFree;
- en: cudaError_t status;
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: CUDART_CHECK( cudaEventRecord( evStart ) );
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( evStart ) );
- en: AllocateBuffers<<<cBlocks,cThreads>>>( devicePointers, N );
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: AllocateBuffers<<<cBlocks,cThreads>>>( devicePointers, N );
- en: CUDART_CHECK( cudaEventRecord( evStop ) );
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( evStop ) );
- en: CUDART_CHECK( cudaThreadSynchronize() );
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaThreadSynchronize() );
- en: CUDART_CHECK( cudaGetLastError() );
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaGetLastError() );
- en: CUDART_CHECK( cudaEventElapsedTime( &etAlloc, evStart, evStop ) );
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventElapsedTime( &etAlloc, evStart, evStop ) );
- en: CUDART_CHECK( cudaEventRecord( evStart ) );
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( evStart ) );
- en: FreeBuffers<<<cBlocks,cThreads>>>( devicePointers );
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: FreeBuffers<<<cBlocks,cThreads>>>( devicePointers );
- en: CUDART_CHECK( cudaEventRecord( evStop ) );
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( evStop ) );
- en: CUDART_CHECK( cudaThreadSynchronize() );
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaThreadSynchronize() );
- en: CUDART_CHECK( cudaGetLastError() );
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaGetLastError() );
- en: CUDART_CHECK( cudaEventElapsedTime( &etFree, evStart, evStop ) );
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventElapsedTime( &etFree, evStart, evStop ) );
- en: '*msPerAlloc = etAlloc / (double) (cBlocks*cThreads);'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '*msPerAlloc = etAlloc / (double) (cBlocks*cThreads);'
- en: '*msPerFree = etFree / (double) (cBlocks*cThreads);'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '*msPerFree = etFree / (double) (cBlocks*cThreads);'
- en: 'Error:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: return status;
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 返回状态；
- en: '}'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 5.4](ch05.html#ch05lis04) shows the output from a sample run of `mallocSpeed.cu`
    on Amazon’s `cg1.4xlarge` instance type. It is clear that the allocator is optimized
    for small allocations: The 64-byte allocations take an average of 0.39 microseconds
    to perform, while allocations of 12K take at least 3 to 5 microseconds. The first
    result (155 microseconds per allocation) is having 1 thread per each of 500 blocks
    allocate a 1MB buffer.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单5.4](ch05.html#ch05lis04)显示了在Amazon的`cg1.4xlarge`实例类型上运行`mallocSpeed.cu`示例的输出。很明显，分配器已经针对小型分配进行了优化：64字节的分配平均需要0.39微秒，而12K的分配至少需要3到5微秒。第一个结果（每次分配155微秒）是指每个500个块中的1个线程分配一个1MB的缓冲区。'
- en: '*Listing 5.4.* Sample `mallocSpeed.cu` output.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单5.4.* 示例`mallocSpeed.cu`输出。'
- en: '[Click here to view code image](ch05_images.html#p05lis04a)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p05lis04a)'
- en: '* * *'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Microseconds per alloc/free (1 thread per block):'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 每次分配/释放的微秒数（每块1个线程）：
- en: alloc       free
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: alloc       free
- en: 154.93      4.57
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 154.93      4.57
- en: Microseconds per alloc/free (32-512 threads per block, 12K
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 每次分配/释放的微秒数（每块32-512线程，12K
- en: 'allocations):'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 分配次数：
- en: 32            64            128           256           512
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 32            64            128           256           512
- en: alloc  free   alloc  free   alloc  free   alloc  free   alloc  free
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: alloc  free   alloc  free   alloc  free   alloc  free   alloc  free
- en: 3.53   1.18   4.27   1.17   4.89   1.14   5.48   1.14   10.38  1.11
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 3.53   1.18   4.27   1.17   4.89   1.14   5.48   1.14   10.38  1.11
- en: Microseconds per alloc/free (32-512 threads per block, 64-byte
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 每次分配/释放的微秒数（每块32-512线程，64字节
- en: 'allocations):'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 分配次数：
- en: 32            64            128           256           512
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 32            64            128           256           512
- en: alloc  free   alloc  free   alloc  free   alloc  free   alloc  free
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: alloc  free   alloc  free   alloc  free   alloc  free   alloc  free
- en: 0.35   0.27   0.37   0.29   0.34   0.27   0.37   0.22   0.53   0.27
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 0.35   0.27   0.37   0.29   0.34   0.27   0.37   0.22   0.53   0.27
- en: '* * *'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '* * *'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Important Note
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Memory allocated by invoking `malloc()` in a kernel must be freed by a *kernel*
    calling `free()`. Calling `cudaFree()` on the host will not work.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用`malloc()`在内核中分配的内存必须通过*内核*调用`free()`来释放。在主机上调用`cudaFree()`是无效的。
- en: '* * *'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.2.3\. Querying the Amount of Global Memory
  id: totrans-316
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 查询全局内存的数量
- en: The amount of global memory in a system may be queried even before CUDA has
    been initialized.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 系统中的全局内存数量甚至在CUDA初始化之前就可以查询。
- en: CUDA Runtime
  id: totrans-318
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA运行时
- en: Call `cudaGetDeviceProperties()` and examine `cudaDeviceProp.totalGlobalMem:`
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '调用`cudaGetDeviceProperties()`并检查`cudaDeviceProp.totalGlobalMem:` '
- en: size_t totalGlobalMem; /**< Global memory on device in bytes */.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: size_t totalGlobalMem; /**< 设备上的全局内存，单位字节 */.
- en: Driver API
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序 API
- en: Call this driver API function.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此驱动程序 API 函数。
- en: CUresult CUDAAPI cuDeviceTotalMem(size_t *bytes, CUdevice dev);
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuDeviceTotalMem(size_t *bytes, CUdevice dev);
- en: '* * *'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: WDDM and Available Memory
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: WDDM 和可用内存
- en: The Windows Display Driver Model (WDDM) introduced with Windows Vista changed
    the model for memory management by display drivers to enable chunks of video memory
    to be swapped in and out of host memory as needed to perform rendering. As a result,
    the amount of memory reported by `cuDeviceTotalMem() / cudaDeviceProp::totalGlobalMem`
    will not exactly reflect the amount of physical memory on the card.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Windows 显示驱动模型（WDDM）随着 Windows Vista 引入，改变了显示驱动程序的内存管理模型，使得视频内存的部分可以根据需要交换进出主机内存以进行渲染。因此，通过
    `cuDeviceTotalMem() / cudaDeviceProp::totalGlobalMem` 获取的内存量不会准确反映显卡上的物理内存大小。
- en: '* * *'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.2.4\. Static Allocations
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. 静态分配
- en: Applications can statically allocate global memory by annotating a memory declaration
    with the `__device__` keyword. This memory is allocated by the CUDA driver when
    the module is loaded.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以通过在内存声明中添加 `__device__` 关键字来静态分配全局内存。当模块加载时，CUDA 驱动程序会分配该内存。
- en: CUDA Runtime
  id: totrans-330
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA 运行时
- en: Memory copies to and from statically allocated memory can be performed by `cudaMemcpyToSymbol()`
    and `cudaMemcpyFromSymbol().`
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对静态分配的内存进行的内存拷贝可以通过 `cudaMemcpyToSymbol()` 和 `cudaMemcpyFromSymbol()` 来完成。
- en: '[Click here to view code image](ch05_images.html#p138pro01a)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p138pro01a)'
- en: cudaError_t cudaMemcpyToSymbol(
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMemcpyToSymbol(
- en: char *symbol,
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: char *symbol，
- en: const void *src,
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: const void *src，
- en: size_t count,
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: size_t count，
- en: size_t offset = 0,
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: size_t offset = 0，
- en: enum cudaMemcpyKind kind = cudaMemcpyHostToDevice
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: enum cudaMemcpyKind kind = cudaMemcpyHostToDevice
- en: );
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: )；
- en: cudaError_t cudaMemcpyFromSymbol(
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMemcpyFromSymbol(
- en: void *dst,
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: void *dst，
- en: char *symbol,
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: char *symbol，
- en: size_t count,
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: size_t count，
- en: size_t offset = 0,
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: size_t offset = 0，
- en: enum cudaMemcpyKind kind = cudaMemcpyDeviceToHost
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: enum cudaMemcpyKind kind = cudaMemcpyDeviceToHost
- en: );
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: )；
- en: When calling `cudaMemcpyToSymbol()` or `cudaMemcpyFromSymbol()`, do not enclose
    the symbol name in quotation marks. In other words, use
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `cudaMemcpyToSymbol()` 或 `cudaMemcpyFromSymbol()` 时，不要将符号名称用引号括起来。换句话说，使用
- en: cudaMemcpyToSymbol(g_xOffset, poffsetx, Width*Height*sizeof(int));
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyToSymbol(g_xOffset, poffsetx, Width*Height*sizeof(int));
- en: not
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 不
- en: cudaMemcpyToSymbol("g_xOffset", poffsetx, ... );
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyToSymbol("g_xOffset", poffsetx, ... );
- en: Both formulations work, but the latter formulation will compile for any symbol
    name (even undefined symbols). If you want the compiler to report errors for invalid
    symbols, avoid the quotation marks.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 两种形式都有效，但后者的形式可以为任何符号名称（即使是未定义的符号）编译。如果希望编译器报告无效符号的错误，避免使用引号。
- en: CUDA runtime applications can query the pointer corresponding to a static allocation
    by calling `cudaGetSymbolAddress().`
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 运行时应用程序可以通过调用 `cudaGetSymbolAddress()` 查询与静态分配对应的指针。
- en: cudaError_t cudaGetSymbolAddress( void **devPtr, char *symbol );
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaGetSymbolAddress( void **devPtr, char *symbol );
- en: '*Beware:* It is all too easy to pass the symbol for a statically declared device
    memory allocation to a CUDA kernel, but this does not work. You must call `cudaGetSymbolAddress()`
    and use the resulting pointer.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：* 将静态声明的设备内存分配符号传递给 CUDA 内核是很容易的，但这样是行不通的。你必须调用`cudaGetSymbolAddress()`并使用返回的指针。'
- en: Driver API
  id: totrans-355
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序 API
- en: Developers using the driver API can obtain pointers to statically allocated
    memory by calling `cuModuleGetGlobal().`
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 使用驱动程序 API 的开发者可以通过调用`cuModuleGetGlobal()`来获取指向静态分配内存的指针。
- en: '[Click here to view code image](ch05_images.html#p139pro01a)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p139pro01a)'
- en: CUresult CUDAAPI cuModuleGetGlobal(CUdeviceptr *dptr, size_t *bytes,
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuModuleGetGlobal(CUdeviceptr *dptr, size_t *bytes,
- en: CUmodule hmod, const char *name);
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: CUmodule hmod, const char *name);
- en: Note that `cuModuleGetGlobal()` passes back both the base pointer and the size
    of the object. If the size is not needed, developers can pass `NULL` for the `bytes`
    parameter. Once this pointer has been obtained, the memory can be accessed by
    passing the `CUdeviceptr` to memory copy calls or CUDA kernel invocations.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`cuModuleGetGlobal()`返回的既有基础指针，又有对象的大小。如果不需要大小，开发者可以为`bytes`参数传递`NULL`。一旦获取了该指针，内存就可以通过传递`CUdeviceptr`给内存复制调用或
    CUDA 内核调用来访问。
- en: 5.2.5\. Memset APIs
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5\. Memset API
- en: For developer convenience, CUDA provides 1D and 2D memset functions. Since they
    are implemented using kernels, they are asynchronous even when no stream parameter
    is specified. For applications that must serialize the execution of a memset within
    a stream, however, there are `*Async()` variants that take a stream parameter.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便开发人员，CUDA 提供了 1D 和 2D memset 函数。由于它们是使用内核实现的，即使没有指定流参数，它们也是异步的。然而，对于那些必须在流内序列化执行
    memset 的应用程序，有`*Async()`版本，它们需要流参数。
- en: CUDA Runtime
  id: totrans-363
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA 运行时
- en: 'The CUDA runtime supports byte-sized memset only:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 运行时仅支持字节大小的 memset：
- en: '[Click here to view code image](ch05_images.html#p139pro02a)'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p139pro02a)'
- en: cudaError_t cudaMemset(void *devPtr, int value, size_t count);
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMemset(void *devPtr, int value, size_t count);
- en: cudaError_t cudaMemset2D(void *devPtr, size_t pitch, int value,
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMemset2D(void *devPtr, size_t pitch, int value,
- en: size_t width, size_t height);
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: size_t 宽度，size_t 高度);
- en: The pitch parameter specifies the bytes per row of the memset operation.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: pitch 参数指定 memset 操作的每行字节数。
- en: Driver API
  id: totrans-370
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序 API
- en: The driver API supports 1D and 2D memset of a variety of sizes, shown in [Table
    5.2](ch05.html#ch05tab02). These memset functions take the destination pointer,
    value to set, and number of values to write starting at the base address. The
    pitch parameter is the bytes per row (not elements per row!).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序 API 支持各种大小的 1D 和 2D memset，如[表 5.2](ch05.html#ch05tab02)所示。这些 memset 函数接受目标指针、要设置的值和从基础地址开始写入的值的数量。pitch
    参数是每行字节数（不是每行元素数！）。
- en: '[Click here to view code image](ch05_images.html#p140pro01a)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p140pro01a)'
- en: CUresult CUDAAPI cuMemsetD8(CUdeviceptr dstDevice, unsigned char uc,
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD8(CUdeviceptr dstDevice, unsigned char uc,
- en: size_t N);
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N);
- en: CUresult CUDAAPI cuMemsetD16(CUdeviceptr dstDevice, unsigned short
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD16(CUdeviceptr dstDevice, 无符号短整数
- en: us, size_t N);
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: us, 大小_t N);
- en: CUresult CUDAAPI cuMemsetD32(CUdeviceptr dstDevice, unsigned int ui,
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD32(CUdeviceptr dstDevice, 无符号整数 ui,
- en: size_t N);
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 大小_t N);
- en: CUresult CUDAAPI cuMemsetD2D8(CUdeviceptr dstDevice, size_t dstPitch,
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD2D8(CUdeviceptr dstDevice, 大小_t dstPitch,
- en: unsigned char uc, size_t Width, size_t Height);
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号字符 uc, 大小_t 宽度, 大小_t 高度);
- en: CUresult CUDAAPI cuMemsetD2D16(CUdeviceptr dstDevice, size_t
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD2D16(CUdeviceptr dstDevice, 大小_t
- en: dstPitch, unsigned short us, size_t Width, size_t Height);
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: dstPitch, 无符号短整数 us, 大小_t 宽度, 大小_t 高度);
- en: CUresult CUDAAPI cuMemsetD2D32(CUdeviceptr dstDevice, size_t
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemsetD2D32(CUdeviceptr dstDevice, 大小_t
- en: dstPitch, unsigned int ui, size_t Width, size_t Height);
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: dstPitch, 无符号整数 ui, 大小_t 宽度, 大小_t 高度);
- en: '![Image](graphics/05tab02.jpg)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/05tab02.jpg)'
- en: '*Table 5.2* Memset Variations'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.2* Memset 变种'
- en: Now that CUDA runtime and driver API functions can peacefully coexist in the
    same application, CUDA runtime developers can use these functions as needed. The
    `unsigned char`, `unsigned short`, and `unsigned int` parameters just specify
    a bit pattern; to fill a global memory range with some other type, such as `float`,
    use a `volatile union` to coerce the `float` to `unsigned int`.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 CUDA 运行时和驱动程序 API 函数可以在同一应用程序中和平共存，CUDA 运行时开发人员可以根据需要使用这些函数。`无符号字符`、`无符号短整数`
    和 `无符号整数` 参数仅指定位模式；要用其他类型（例如 `float`）填充全局内存范围，可以使用 `volatile union` 强制将 `float`
    转换为 `无符号整数`。
- en: 5.2.6\. Pointer Queries
  id: totrans-388
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.6\. 指针查询
- en: CUDA tracks all of its memory allocations, and provides APIs that enable applications
    to query CUDA about pointers that were passed in from some other party. Libraries
    or plugins may wish to pursue different strategies based on this information.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 跟踪其所有内存分配，并提供 API，使应用程序能够查询从其他方传入的指针。库或插件可能希望根据这些信息采取不同的策略。
- en: CUDA Runtime
  id: totrans-390
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA运行时
- en: The `cudaPointerGetAttributes()` function takes a pointer as input and passes
    back a `cudaPointerAttributes` structure containing information about the pointer.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaPointerGetAttributes()` 函数接受一个指针作为输入，并返回一个包含指针信息的 `cudaPointerAttributes`
    结构体。'
- en: struct cudaPointerAttributes {
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 结构体 cudaPointerAttributes {
- en: enum cudaMemoryType memoryType;
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 枚举 cudaMemoryType 内存类型;
- en: int device;
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: int 设备;
- en: void *devicePointer;
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: void *设备指针;
- en: void *hostPointer;
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: void *主机指针;
- en: '}'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: When UVA is in effect, pointers are unique process-wide, so there is no ambiguity
    as to the input pointer’s address space. When UVA is not in effect, the input
    pointer is assumed to be in the current device’s address space ([Table 5.3](ch05.html#ch05tab03)).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 当 UVA 生效时，指针在进程范围内是唯一的，因此输入指针的地址空间没有歧义。当 UVA 不生效时，假定输入指针位于当前设备的地址空间中（[表 5.3](ch05.html#ch05tab03)）。
- en: '![Image](graphics/05tab03.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/05tab03.jpg)'
- en: '*Table 5.3* `cudaPointerAttributes` Members'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.3* `cudaPointerAttributes` 成员'
- en: Driver API
  id: totrans-401
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序 API
- en: Developers can query the address range where a given device pointer resides
    using the `cuMemGetAddressRange()` function.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以使用 `cuMemGetAddressRange()` 函数查询给定设备指针所在的地址范围。
- en: '[Click here to view code image](ch05_images.html#p141pro01a)'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p141pro01a)'
- en: CUresult CUDAAPI cuMemGetAddressRange(CUdeviceptr *pbase, size_t
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuMemGetAddressRange(CUdeviceptr *pbase, size_t
- en: '*psize, CUdeviceptr dptr);'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '*psize, CUdeviceptr dptr);'
- en: This function takes a device pointer as input and passes back the base and size
    of the allocation containing that device pointer.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数以设备指针作为输入，并返回包含该设备指针的分配的基地址和大小。
- en: With the addition of UVA in CUDA 4.0, developers can query CUDA to get even
    more information about an address using `cuPointerGetAttribute().`
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 CUDA 4.0 中 UVA 的加入，开发者可以使用 `cuPointerGetAttribute()` 查询 CUDA，以获取有关地址的更多信息。
- en: '[Click here to view code image](ch05_images.html#p142pro01a)'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p142pro01a)'
- en: CUresult CUDAAPI cuPointerGetAttribute(void *data, CUpointer_
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuPointerGetAttribute(void *data, CUpointer_
- en: attribute attribute, CUdeviceptr ptr);
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 属性属性，CUdeviceptr ptr);
- en: This function takes a device pointer as input and passes back the information
    corresponding to the attribute parameter, as shown in [Table 5.4](ch05.html#ch05tab04).
    Note that for unified addresses, using `CU_POINTER_ATTRIBUTE_DEVICE_POINTER` or
    `CU_POINTER_ATTRIBUTE_HOST_POINTER` will cause the same pointer value to be returned
    as the one passed in.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数以设备指针作为输入，并返回与属性参数对应的信息，如[表 5.4](ch05.html#ch05tab04)所示。注意，对于统一地址，使用 `CU_POINTER_ATTRIBUTE_DEVICE_POINTER`
    或 `CU_POINTER_ATTRIBUTE_HOST_POINTER` 会返回与传入值相同的指针值。
- en: '![Image](graphics/05tab04.jpg)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/05tab04.jpg)'
- en: '*Table 5.4* `cuPointerAttribute` Usage'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.4* `cuPointerAttribute` 使用'
- en: Kernel Queries
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内核查询
- en: On SM 2.x (Fermi) hardware and later, developers can query whether a given pointer
    points into global space. The `__isGlobal()` intrinsic
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SM 2.x（Fermi）及更高版本的硬件上，开发者可以查询给定指针是否指向全局空间。`__isGlobal()` 内建函数
- en: unsigned int __isGlobal( const void *p );
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int __isGlobal( const void *p );
- en: returns 1 if the input pointer refers to global memory and 0 otherwise.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入指针指向全局内存，则返回 1，否则返回 0。
- en: 5.2.7\. Peer-to-Peer Access
  id: totrans-418
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.7\. 点对点访问
- en: Under certain circumstances, SM 2.0-class and later hardware can map memory
    belonging to other, similarly capable GPUs. The following conditions apply.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，SM 2.0 类及更高版本的硬件可以映射属于其他相似能力 GPU 的内存。以下条件适用。
- en: • UVA must be in effect.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: • 必须启用 UVA。
- en: • Both GPUs must be Fermi-class and be based on the same chip.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: • 两个 GPU 必须是 Fermi 类并且基于相同的芯片。
- en: • The GPUs must be on the same I/O hub.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: • 两个 GPU 必须位于同一 I/O 集线器上。
- en: Since peer-to-peer mapping is intrinsically a multi-GPU feature, it is described
    in detail in the multi-GPU chapter (see [Section 9.2](ch09.html#ch09lev1sec2)).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 由于点对点映射本质上是一个多 GPU 特性，因此在多 GPU 章节中有详细描述（见[第 9.2 节](ch09.html#ch09lev1sec2)）。
- en: 5.2.8\. Reading and Writing Global Memory
  id: totrans-424
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.8\. 读写全局内存
- en: CUDA kernels can read or write global memory using standard C semantics such
    as pointer indirection (`operator*, operator->`) or array subscripting (`operator[]`).
    Here is a simple templatized kernel to write a constant into a memory range.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 内核可以使用标准 C 语义，如指针间接寻址（`operator*`, `operator->`）或数组下标（`operator[]`），来读写全局内存。以下是一个简单的模板化内核，用于将常量写入内存范围。
- en: '[Click here to view code image](ch05_images.html#p143pro01a)'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p143pro01a)'
- en: template<class T>
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: __global__ void
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: GlobalWrites( T *out, T value, size_t N )
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: GlobalWrites( T *out, T value, size_t N )
- en: '{'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N;
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: out[i] = value;
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] = value;
- en: '}'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'This kernel works correctly for any inputs: any component size, any block size,
    any grid size. Its code is intended more for illustrative purposes than maximum
    performance. CUDA kernels that use more registers and operate on multiple values
    in the inner loop go faster, but for some block and grid configurations, its performance
    is perfectly acceptable. In particular, provided the base address and block size
    are specified correctly, it performs coalesced memory transactions that maximize
    memory bandwidth.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核对于任何输入都能正常工作：任何组件大小、任何块大小、任何网格大小。其代码更多是为了说明用途，而非追求最大性能。使用更多寄存器并在内部循环中处理多个值的
    CUDA 内核运行更快，但对于某些块和网格配置，其性能完全可以接受。特别是，前提是基地址和块大小正确指定，它会执行合并内存事务，最大化内存带宽。
- en: 5.2.9\. Coalescing Constraints
  id: totrans-438
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.9\. 合并约束
- en: For best performance when reading and writing data, CUDA kernels must perform
    *coalesced* memory transactions. Any memory transaction that does not meet the
    full set of criteria needed for coalescing is “uncoalesced.” The penalty for uncoalesced
    memory transactions varies from 2x to 8x, depending on the chip implementation.
    Coalesced memory transactions have a much less dramatic impact on performance
    on more recent hardware, as shown in [Table 5.5](ch05.html#ch05tab05).
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在读写数据时获得最佳性能，CUDA 内核必须执行 *合并* 内存事务。任何不满足合并所需全部条件的内存事务被视为“非合并”。非合并内存事务的性能惩罚因芯片实现的不同而有所不同，通常为
    2x 到 8x。合并内存事务在较新的硬件上对性能的影响较小，如 [表 5.5](ch05.html#ch05tab05) 所示。
- en: '![Image](graphics/05tab05.jpg)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/05tab05.jpg)'
- en: '*Table 5.5* Bandwidth Penalties for Uncoalesced Memory Access'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.5* 非合并内存访问的带宽惩罚'
- en: Transactions are coalesced on a per-warp basis. A simplified set of criteria
    must be met in order for the memory read or write being performed by the warp
    to be coalesced.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 事务是在每个 warp 的基础上合并的。为了使 warp 执行的内存读写操作合并，必须满足一组简化的条件。
- en: • The words must be at least 32 bits in size. Reading or writing bytes or 16-bit
    words is always uncoalesced.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: • 字长必须至少为 32 位。读取或写入字节或 16 位字始终是非合并的。
- en: • The addresses being accessed by the threads of the warp must be contiguous
    and increasing (i.e., offset by the thread ID).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: • 由warp线程访问的地址必须是连续且递增的（即，由线程ID偏移）。
- en: • The base address of the warp (the address being accessed by the first thread
    in the warp) must be aligned as shown in [Table 5.6](ch05.html#ch05tab06).
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: • warp的基地址（warp中第一个线程访问的地址）必须按照[表5.6](ch05.html#ch05tab06)所示对齐。
- en: '![Image](graphics/05tab06.jpg)'
  id: totrans-446
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab06.jpg)'
- en: '*Table 5.6* Alignment Criteria for Coalescing'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '*表5.6* 合并对齐标准'
- en: The `ElementSizeBytes` parameter to `cuMemAllocPitch()` is intended to accommodate
    the size restriction. It specifies the size in bytes of the memory accesses intended
    by the application, so the pitch guarantees that a set of coalesced memory transactions
    for a given row of the allocation also will be coalesced for other rows.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuMemAllocPitch()`中的`ElementSizeBytes`参数用于适应大小限制。它指定应用程序所需内存访问的字节大小，因此pitch可以保证给定分配行的合并内存事务会与其他行进行合并。'
- en: Most kernels in this book perform coalesced memory transactions, provided the
    input addresses are properly aligned. NVIDIA has provided more detailed, architecture-specific
    information on how global memory transactions are handled, as detailed below.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的大多数内核执行合并内存事务，前提是输入地址已正确对齐。NVIDIA提供了更多关于全局内存事务如何处理的详细架构特定信息，具体如下所示。
- en: SM 1.x (Tesla)
  id: totrans-450
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SM 1.x（Tesla）
- en: SM 1.0 and SM 1.1 hardware require that each thread in a warp access adjacent
    memory locations in sequence, as described above. SM 1.2 and 1.3 hardware relaxed
    the coalescing constraints somewhat. To issue a coalesced memory request, divide
    each 32-thread warp into two “half warps,” lanes 0–15 and lanes 16–31\. To service
    the memory request from each half-warp, the hardware performs the following algorithm.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: SM 1.0和SM 1.1硬件要求warp中的每个线程按顺序访问相邻的内存位置，如上所述。SM 1.2和1.3硬件稍微放宽了合并约束。要发出合并内存请求，将每个32线程的warp分成两个“半warp”，即通道0–15和通道16–31。为了服务来自每个半warp的内存请求，硬件执行以下算法。
- en: '**1.** Find the active thread with the lowest thread ID and locate the memory
    segment that contains that thread’s requested address. The segment size depends
    on the word size: 1-byte requests result in 32-byte segments; 2-byte requests
    result in 64-byte segments; and all other requests result in 128-byte segments.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.** 找到具有最低线程ID的活动线程，并定位包含该线程请求地址的内存段。段大小取决于字大小：1字节请求产生32字节段；2字节请求产生64字节段；所有其他请求产生128字节段。'
- en: '**2.** Find all other active threads whose requested address lies in the same
    segment.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** 查找所有其他活动线程，这些线程的请求地址位于相同的内存段中。'
- en: '**3.** If possible, reduce the segment transaction size to 64 or 32 bytes.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** 如果可能，将段事务大小减少到64或32字节。'
- en: '**4.** Carry out the transaction and mark the services threads as inactive.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '**4.** 执行事务并将服务过的线程标记为非活动。'
- en: '**5.** Repeat steps 1–4 until all threads in the half-warp have been serviced.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '**5.** 重复步骤1–4，直到半warp中的所有线程都得到服务。'
- en: Although these requirements are somewhat relaxed compared to the SM 1.0–1.1
    constraints, a great deal of locality is still required for effective coalescing.
    In practice, the relaxed coalescing means the threads within a warp can permute
    the inputs within small segments of memory, if desired.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些要求相比于SM 1.0–1.1的约束有所放宽，但为了有效地合并，仍然需要大量的局部性。实际上，放宽的合并意味着，如果需要，warp中的线程可以在内存的小段中重新排列输入。
- en: SM 2.x (Fermi)
  id: totrans-458
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SM 2.x（Fermi）
- en: SM 2.x and later hardware includes L1 and L2 caches. The L2 cache services the
    entire chip; the L1 caches are per-SM and may be configured to be 16K or 48K in
    size. The cache lines are 128 bytes and map to 128-byte aligned segments in device
    memory. Memory accesses that are cached in both L1 and L2 are serviced with 128-byte
    memory transactions, whereas memory accesses that are cached in L2 only are serviced
    with 32-byte memory transactions. Caching in L2 only can therefore reduce overfetch,
    for example, in the case of scattered memory accesses.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: SM 2.x及以后的硬件包括L1和L2缓存。L2缓存服务整个芯片；L1缓存是每个SM独立的，可以配置为16K或48K大小。缓存行大小为128字节，并映射到设备内存中对齐的128字节段。那些同时在L1和L2中缓存的内存访问将以128字节的内存事务进行处理，而那些仅在L2中缓存的内存访问则以32字节的内存事务进行处理。因此，仅在L2中缓存可以减少过度取值，例如，在散乱内存访问的情况下。
- en: The hardware can specify the cacheability of global memory accesses on a per-instruction
    basis. By default, the compiler emits instructions that cache memory accesses
    in both L1 and L2 (`-Xptxas -dlcm=ca`). This can be changed to cache in L2 only
    by specifying `-Xptxas -dlcm=cg`. Memory accesses that are not present in L1 but
    cached in L2 only are serviced with 32-byte memory transactions, which may improve
    cache utilization for applications that are performing scattered memory accesses.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件可以为每条指令指定全局内存访问的缓存可用性。默认情况下，编译器生成的指令会在L1和L2中缓存内存访问（`-Xptxas -dlcm=ca`）。通过指定`-Xptxas
    -dlcm=cg`，可以将缓存设置为仅在L2中缓存。那些未出现在L1但仅在L2中缓存的内存访问将以32字节的内存事务进行处理，这可能会提高执行散乱内存访问的应用程序的缓存利用率。
- en: Reading via pointers that are declared `volatile` causes any cached results
    to be discarded and for the data to be refetched. This idiom is mainly useful
    for polling host memory locations. [Table 5.7](ch05.html#ch05tab07) summarizes
    how memory requests by a warp are broken down into 128-byte cache line requests.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 通过声明为`volatile`的指针进行读取会导致任何缓存的结果被丢弃，并重新获取数据。这种惯用法主要用于轮询主机内存位置。[表 5.7](ch05.html#ch05tab07)总结了warp如何将内存请求分解为128字节的缓存行请求。
- en: '![Image](graphics/05tab07.jpg)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab07.jpg)'
- en: '*Table 5.7* SM 2.x Cache Line Requests'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.7* SM 2.x 缓存行请求'
- en: '* * *'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: On SM 2.x and higher architectures, threads within a warp can access any words
    in any order, including the same words.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在SM 2.x及更高架构中，warp中的线程可以以任意顺序访问任何字，包括相同的字。
- en: '* * *'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: SM 3.x (Kepler)
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SM 3.x（Kepler）
- en: The L2 cache architecture is the same as SM 2.x. SM 3.x does not cache global
    memory accesses in L1\. In SM 3.5, global memory may be accessed via the texture
    cache (which is 48K per SM in size) by accessing memory via `const restricted`
    pointers or by using the `__ldg()` intrinsics in `sm_35_intrinsics.h`. As when
    texturing directly from device memory, it is important not to access memory that
    might be accessed concurrently by other means, since this cache is not kept coherent
    with respect to the L2.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: L2缓存架构与SM 2.x相同。SM 3.x不在L1中缓存全局内存访问。在SM 3.5中，可以通过纹理缓存（每个SM大小为48K）访问全局内存，方法是通过`const
    restricted`指针或使用`__ldg()`内在函数（在`sm_35_intrinsics.h`中）来访问内存。就像直接从设备内存进行纹理采样一样，必须避免访问可能被其他方式并发访问的内存，因为这个缓存不会与L2保持一致性。
- en: '5.2.10\. Microbenchmarks: Peak Memory Bandwidth'
  id: totrans-470
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.10\. 微基准测试：峰值内存带宽
- en: The source code accompanying this book includes microbenchmarks that determine
    which combination of operand size, loop unroll factor, and block size maximizes
    bandwidth for a given GPU. Rewriting the earlier `GlobalWrites` code as a template
    that takes an additional parameter `n` (the number of writes to perform in the
    inner loop) yields the kernel in [Listing 5.5](ch05.html#ch05lis05).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 本书附带的源代码包括微基准测试，用于确定哪种操作数大小、循环展开因子和块大小的组合能最大化给定GPU的带宽。将早期的`GlobalWrites`代码重写为一个模板，增加一个额外的参数`n`（表示内循环中执行的写入次数），可以得到[列表5.5](ch05.html#ch05lis05)中的内核代码。
- en: '*Listing 5.5.* `GlobalWrites` kernel.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表5.5.* `GlobalWrites`内核。'
- en: '[Click here to view code image](ch05_images.html#p05lis05a)'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis05a)'
- en: '* * *'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, const int n>
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, const int n>
- en: __global__ void
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: GlobalWrites( T *out, T value, size_t N )
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: GlobalWrites( T *out, T value, size_t N )
- en: '{'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t i;
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i;
- en: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N-n*blockDim.x*gridDim.x;
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: i < N-n*blockDim.x*gridDim.x;
- en: i += n*blockDim.x*gridDim.x ) {
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: i += n*blockDim.x*gridDim.x ) {
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: out[index] = value;
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: out[index] = value;
- en: '}'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // to avoid the (index<N) conditional in the inner loop,
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: // 为了避免内循环中的(index<N)条件判断，
- en: // we left off some work at the end
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: // 我们在最后留下了一些工作
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: if ( index<N ) out[index] = value;
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index<N ) out[index] = value;
- en: '}'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '`ReportRow()`, the function given in [Listing 5.6](ch05.html#ch05lis06) that
    writes one row of output calls by calling a template function `BandwidthWrites`
    (not shown), reports the bandwidth for a given type, grid, and block size.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '`ReportRow()`，如[列表5.6](ch05.html#ch05lis06)所示的函数，通过调用模板函数`BandwidthWrites`（未显示）来写入一行输出，并报告给定类型、网格和块大小的带宽。'
- en: '*Listing 5.6.* `ReportRow` function.'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表5.6.* `ReportRow`函数。'
- en: '[Click here to view code image](ch05_images.html#p05lis06a)'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis06a)'
- en: '* * *'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, const int n, bool bOffset>
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, const int n, bool bOffset>
- en: double
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: double
- en: ReportRow( size_t N,
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: ReportRow( size_t N,
- en: size_t threadStart,
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: size_t threadStart,
- en: size_t threadStop,
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: size_t threadStop,
- en: size_t cBlocks )
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: size_t cBlocks )
- en: '{'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int maxThreads = 0;
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: int maxThreads = 0;
- en: double maxBW = 0.0;
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: double maxBW = 0.0;
- en: printf( "%d\t", n );
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%d\t", n );
- en: for ( int cThreads = threadStart;
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int cThreads = threadStart;
- en: cThreads <= threadStop;
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: cThreads <= threadStop;
- en: cThreads *= 2 ) {
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: cThreads *= 2 ) {
- en: double bw;
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: double bw;
- en: bw = BandwidthWrites<T,n,bOffset>( N, cBlocks, cThreads );
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: bw = BandwidthWrites<T,n,bOffset>( N, cBlocks, cThreads );
- en: if ( bw > maxBW ) {
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: if ( bw > maxBW ) {
- en: maxBW = bw;
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: maxBW = bw;
- en: maxThreads = cThreads;
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: maxThreads = cThreads;
- en: '}'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: printf( "%.2f\t", bw );
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%.2f\t", bw );
- en: '}'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: printf( "%.2f\t%d\n", maxBW, maxThreads );
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%.2f\t%d\n", maxBW, maxThreads );
- en: return maxBW;
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: return maxBW;
- en: '}'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The `threadStart` and `threadStop` parameters typically are 32 and 512, 32 being
    the warp size and the minimum number of threads per block that can occupy the
    machine. The `bOffset` template parameter specifies whether `BandwidthWrites`
    should offset the base pointer, causing all memory transactions to become uncoalesced.
    If the program is invoked with the `--uncoalesced` command line option, it will
    perform the bandwidth measurements with the offset pointer.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '`threadStart` 和 `threadStop` 参数通常为 32 和 512，其中 32 是波束大小，是每个块能占用的最小线程数。`bOffset`
    模板参数指定是否 `BandwidthWrites` 应该偏移基指针，从而导致所有内存事务变得不合并。如果程序通过 `--uncoalesced` 命令行选项被调用，它将使用偏移指针进行带宽测量。'
- en: Note that depending on `sizeof(T)`, kernels with `n` above a certain level will
    fall off a performance cliff as the number of temporary variables in the inner
    loop grows too high to hold in registers.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，根据 `sizeof(T)` 的不同，若 `n` 超过某一特定值，内核性能将急剧下降，因为内层循环中的临时变量数量增长过快，无法保存在寄存器中。
- en: The five applications summarized in [Table 5.8](ch05.html#ch05tab08) implement
    this strategy. They measure the memory bandwidth delivered for different operand
    sizes (8-, 16-, 32-, 64-, and 128-bit), threadblock sizes (32, 64, 128, 256, and
    512), and loop unroll factors (1–16). CUDA hardware isn’t necessarily sensitive
    to all of these parameters. For example, many parameter settings enable a GK104
    to deliver 140GB/s of bandwidth via texturing, but only if the operand size is
    at least 32-bit. For a given workload and hardware, however, the microbenchmarks
    highlight which parameters matter. Also, for small operand sizes, they highlight
    how loop unrolling can help increase performance (not all applications can be
    refactored to read larger operands).
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表 5.8](ch05.html#ch05tab08) 中总结的五个应用实现了这一策略。它们测量了不同操作数大小（8 位、16 位、32 位、64
    位和 128 位）、线程块大小（32、64、128、256 和 512）以及循环展开因子（1–16）下的内存带宽。CUDA 硬件并不一定对所有这些参数都敏感。例如，许多参数设置可以让
    GK104 通过纹理处理提供 140GB/s 的带宽，但前提是操作数大小至少为 32 位。然而，对于特定的工作负载和硬件，微基准测试可以突出哪些参数是重要的。另外，对于小操作数大小，它们突出了如何通过循环展开来提高性能（并非所有应用都可以重构为读取更大的操作数）。
- en: '![Image](graphics/05tab08.jpg)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab08.jpg)'
- en: '*Table 5.8* Memory Bandwidth Microbenchmarks'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.8* 内存带宽微基准测试'
- en: '[Listing 5.7](ch05.html#ch05lis07) gives example output from `globalRead.cu`,
    run on a GeForce GTX 680 GPU. The output is grouped by operand size, from bytes
    to 16-byte quads; the leftmost column of each group gives the loop unroll factor.
    The bandwidth delivered for blocks of sizes 32 to 512 is given in each column,
    and the `maxBW` and `maxThreads` columns give the highest bandwidth and the block
    size that delivered the highest bandwidth, respectively.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.7](ch05.html#ch05lis07) 给出了从 `globalRead.cu` 输出的示例结果，该程序在 GeForce GTX
    680 GPU 上运行。输出按操作数大小分组，从字节到16字节四元组；每组的最左列给出了循环展开因子。每列给出了32到512大小块的带宽，并且 `maxBW`
    和 `maxThreads` 列分别给出了最高带宽和提供最高带宽的块大小。'
- en: The GeForce GTX 680 can deliver up to 140GB/s, so [Listing 5.7](ch05.html#ch05lis07)
    makes it clear that when reading 8- and 16-bit words on SM 3.0, global loads are
    not the way to go. Bytes deliver at most 60GB/s, and 16-bit words deliver at most
    101GB/s.^([11](ch05.html#ch05fn11)) For 32-bit operands, a 2x loop unroll and
    at least 256 threads per block are needed to get maximum bandwidth.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: GeForce GTX 680 的带宽可达到 140GB/s，因此 [列表 5.7](ch05.html#ch05lis07) 清楚地表明，在 SM 3.0
    上读取 8 位和 16 位字时，全球加载并不是最佳选择。字节的最大带宽为 60GB/s，16 位字的最大带宽为 101GB/s。^([11](ch05.html#ch05fn11))
    对于 32 位操作数，需要 2 倍循环展开并且每个块至少有 256 个线程才能获得最大带宽。
- en: '[11](ch05.html#ch05fn11a). Texturing works better. Readers can run `globalReadTex.cu`
    to confirm.'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '[11](ch05.html#ch05fn11a)。纹理化效果更好。读者可以运行 `globalReadTex.cu` 进行验证。'
- en: These microbenchmarks can help developers optimize their bandwidth-bound applications.
    Choose the one whose memory access pattern most closely resembles your application,
    and either run the microbenchmark on the target GPU or, if possible, modify the
    microbenchmark to resemble the actual workload more closely and run it to determine
    the optimal parameters.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 这些微基准测试可以帮助开发者优化带宽受限的应用程序。选择一个内存访问模式最接近你应用程序的基准测试，或者，如果可能的话，修改微基准测试使其更接近实际工作负载，然后运行它以确定最佳参数。
- en: '*Listing 5.7.* Sample output, `globalRead.cu`.'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 5.7.* 示例输出，`globalRead.cu`。'
- en: '[Click here to view code image](ch05_images.html#p05lis07a)'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis07a)'
- en: '* * *'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Running globalRead.cu microbenchmark on GeForce GTX 680
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GeForce GTX 680 上运行 `globalRead.cu` 微基准测试
- en: Using coalesced memory transactions
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 使用合并内存事务
- en: 'Operand size: 1 byte'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数大小：1 字节
- en: 'Input size: 16M operands'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小：16M 操作数
- en: Block Size
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32     64     128    256    512    maxBW   maxThreads
- en: 1       9.12    17.39   30.78   30.78   28.78   30.78   128
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 1       9.12    17.39   30.78   30.78   28.78   30.78   128
- en: 2       18.37   34.54   56.36   53.53   49.33   56.36   128
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 2       18.37   34.54   56.36   53.53   49.33   56.36   128
- en: 3       23.55   42.32   61.56   60.15   52.91   61.56   128
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 3       23.55   42.32   61.56   60.15   52.91   61.56   128
- en: 4       21.25   38.26   58.99   58.09   51.26   58.99   128
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 4       21.25   38.26   58.99   58.09   51.26   58.99   128
- en: 5       25.29   42.17   60.13   58.49   52.57   60.13   128
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 5       25.29   42.17   60.13   58.49   52.57   60.13   128
- en: 6       25.68   42.15   59.93   55.42   47.46   59.93   128
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 6       25.68   42.15   59.93   55.42   47.46   59.93   128
- en: 7       28.84   47.03   56.20   51.41   41.41   56.20   128
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 7       28.84   47.03   56.20   51.41   41.41   56.20   128
- en: 8       29.88   48.55   55.75   50.68   39.96   55.75   128
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 8       29.88   48.55   55.75   50.68   39.96   55.75   128
- en: 9       28.65   47.75   56.84   51.17   37.56   56.84   128
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 9       28.65   47.75   56.84   51.17   37.56   56.84   128
- en: 10      27.35   45.16   52.99   46.30   32.94   52.99   128
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 10      27.35   45.16   52.99   46.30   32.94   52.99   128
- en: 11      22.27   38.51   48.17   42.74   32.81   48.17   128
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 11      22.27   38.51   48.17   42.74   32.81   48.17   128
- en: 12      23.39   40.51   49.78   42.42   31.89   49.78   128
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 12      23.39   40.51   49.78   42.42   31.89   49.78   128
- en: 13      21.62   37.49   40.89   34.98   21.43   40.89   128
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 13      21.62   37.49   40.89   34.98   21.43   40.89   128
- en: 14      18.55   32.12   36.04   31.41   19.96   36.04   128
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 14      18.55   32.12   36.04   31.41   19.96   36.04   128
- en: 15      21.47   36.87   39.94   33.36   19.98   39.94   128
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 15      21.47   36.87   39.94   33.36   19.98   39.94   128
- en: 16      21.59   36.79   39.49   32.71   19.42   39.49   128
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 16      21.59   36.79   39.49   32.71   19.42   39.49   128
- en: 'Operand size: 2 bytes'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数大小：2 字节
- en: 'Input size: 16M operands'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小：16M 操作数
- en: Block Size
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       18.29   35.07   60.30   59.16   56.06   60.30   128
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 1       18.29   35.07   60.30   59.16   56.06   60.30   128
- en: 2       34.94   64.39   94.28   92.65   85.99   94.28   128
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 2       34.94   64.39   94.28   92.65   85.99   94.28   128
- en: 3       45.02   72.90   101.38  99.02   90.07   101.38  128
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 3       45.02   72.90   101.38  99.02   90.07   101.38  128
- en: 4       38.54   68.35   100.30  98.29   90.28   100.30  128
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 4       38.54   68.35   100.30  98.29   90.28   100.30  128
- en: 5       45.49   75.73   98.68   98.11   90.05   98.68   128
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 5       45.49   75.73   98.68   98.11   90.05   98.68   128
- en: 6       47.58   77.50   100.35  97.15   86.17   100.35  128
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 6       47.58   77.50   100.35  97.15   86.17   100.35  128
- en: 7       53.64   81.04   92.89   87.39   74.14   92.89   128
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 7       53.64   81.04   92.89   87.39   74.14   92.89   128
- en: 8       44.79   74.02   89.19   83.96   69.65   89.19   128
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 8       44.79   74.02   89.19   83.96   69.65   89.19   128
- en: 9       47.63   76.63   91.60   83.52   68.06   91.60   128
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 9       47.63   76.63   91.60   83.52   68.06   91.60   128
- en: 10      51.02   79.82   93.85   84.69   66.62   93.85   128
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 10      51.02   79.82   93.85   84.69   66.62   93.85   128
- en: 11      42.00   72.11   88.23   79.24   62.27   88.23   128
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 11      42.00   72.11   88.23   79.24   62.27   88.23   128
- en: 12      40.53   69.27   85.75   76.32   59.73   85.75   128
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 12      40.53   69.27   85.75   76.32   59.73   85.75   128
- en: 13      44.90   73.44   78.08   66.96   41.27   78.08   128
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 13      44.90   73.44   78.08   66.96   41.27   78.08   128
- en: 14      39.18   68.43   74.46   63.27   39.27   74.46   128
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 14      39.18   68.43   74.46   63.27   39.27   74.46   128
- en: 15      37.60   64.11   69.93   60.22   37.09   69.93   128
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 15      37.60   64.11   69.93   60.22   37.09   69.93   128
- en: 16      40.36   67.90   73.07   60.79   36.66   73.07   128
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 16      40.36   67.90   73.07   60.79   36.66   73.07   128
- en: 'Operand size: 4 bytes'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数大小：4 字节
- en: 'Input size: 16M operands'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小：16M 操作数
- en: Block Size
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       36.37   67.89   108.04  105.99  104.09  108.04  128
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 1       36.37   67.89   108.04  105.99  104.09  108.04  128
- en: 2       73.85   120.90  139.91  139.93  136.04  139.93  256
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 2       73.85   120.90  139.91  139.93  136.04  139.93  256
- en: 3       62.62   109.24  140.07  139.66  138.38  140.07  128
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 3       62.62   109.24  140.07  139.66  138.38  140.07  128
- en: 4       56.02   101.73  138.70  137.42  135.10  138.70  128
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 4       56.02   101.73  138.70  137.42  135.10  138.70  128
- en: 5       87.34   133.65  140.64  140.33  139.00  140.64  128
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 5       87.34   133.65  140.64  140.33  139.00  140.64  128
- en: 6       100.64  137.47  140.61  139.53  127.18  140.61  128
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 6       100.64  137.47  140.61  139.53  127.18  140.61  128
- en: 7       89.08   133.99  139.60  138.23  124.28  139.60  128
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 7       89.08   133.99  139.60  138.23  124.28  139.60  128
- en: 8       58.46   103.09  129.24  122.28  110.58  129.24  128
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 8       58.46   103.09  129.24  122.28  110.58  129.24  128
- en: 9       68.99   116.59  134.17  128.64  114.80  134.17  128
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 9       68.99   116.59  134.17  128.64  114.80  134.17  128
- en: 10      54.64   97.90   123.91  118.84  106.96  123.91  128
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 10      54.64   97.90   123.91  118.84  106.96  123.91  128
- en: 11      64.35   110.30  131.43  123.90  109.31  131.43  128
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 11      64.35   110.30  131.43  123.90  109.31  131.43  128
- en: 12      68.03   113.89  130.95  125.40  108.02  130.95  128
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 12      68.03   113.89  130.95  125.40  108.02  130.95  128
- en: 13      71.34   117.88  123.85  113.08  76.98   123.85  128
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 13      71.34   117.88  123.85  113.08  76.98   123.85  128
- en: 14      54.72   97.31   109.41  101.28  71.13   109.41  128
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 14      54.72   97.31   109.41  101.28  71.13   109.41  128
- en: 15      67.28   111.24  118.88  108.35  72.30   118.88  128
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 15      67.28   111.24  118.88  108.35  72.30   118.88  128
- en: 16      63.32   108.56  117.77  103.24  69.76   117.77  128
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 16      63.32   108.56  117.77  103.24  69.76   117.77  128
- en: 'Operand size: 8 bytes'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '操作数大小: 8 字节'
- en: 'Input size: 16M operands'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '输入大小: 16M 操作数'
- en: Block Size
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       74.64   127.73  140.91  142.08  142.16  142.16  512
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 1       74.64   127.73  140.91  142.08  142.16  142.16  512
- en: 2       123.70  140.35  141.31  141.99  142.42  142.42  512
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 2       123.70  140.35  141.31  141.99  142.42  142.42  512
- en: 3       137.28  141.15  140.86  141.94  142.63  142.63  512
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 3       137.28  141.15  140.86  141.94  142.63  142.63  512
- en: 4       128.38  141.39  141.85  142.56  142.00  142.56  256
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 4       128.38  141.39  141.85  142.56  142.00  142.56  256
- en: 5       117.57  140.95  141.17  142.08  141.78  142.08  256
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 5       117.57  140.95  141.17  142.08  141.78  142.08  256
- en: 6       112.10  140.62  141.48  141.86  141.95  141.95  512
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 6       112.10  140.62  141.48  141.86  141.95  141.95  512
- en: 7       85.02   134.82  141.59  141.50  141.09  141.59  128
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 7       85.02   134.82  141.59  141.50  141.09  141.59  128
- en: 8       94.44   138.71  140.86  140.25  128.91  140.86  128
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 8       94.44   138.71  140.86  140.25  128.91  140.86  128
- en: 9       100.69  139.83  141.09  141.45  127.82  141.45  256
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 9       100.69  139.83  141.09  141.45  127.82  141.45  256
- en: 10      92.51   137.76  140.74  140.93  126.50  140.93  256
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 10      92.51   137.76  140.74  140.93  126.50  140.93  256
- en: 11      104.87  140.38  140.67  136.70  128.48  140.67  128
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 11      104.87  140.38  140.67  136.70  128.48  140.67  128
- en: 12      97.71   138.62  140.12  135.74  125.37  140.12  128
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 12      97.71   138.62  140.12  135.74  125.37  140.12  128
- en: 13      95.87   138.28  139.90  134.18  123.41  139.90  128
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 13      95.87   138.28  139.90  134.18  123.41  139.90  128
- en: 14      85.69   134.18  133.84  131.16  120.95  134.18  64
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 14      85.69   134.18  133.84  131.16  120.95  134.18  64
- en: 15      94.43   135.43  135.30  133.47  120.52  135.43  64
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 15      94.43   135.43  135.30  133.47  120.52  135.43  64
- en: 16      91.62   136.69  133.59  129.95  117.99  136.69  64
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 16      91.62   136.69  133.59  129.95  117.99  136.69  64
- en: 'Operand size: 16 bytes'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '操作数大小: 16 字节'
- en: 'Input size: 16M operands'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '输入大小: 16M 操作数'
- en: Block Size
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       125.37  140.67  141.15  142.06  142.59  142.59  512
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 1       125.37  140.67  141.15  142.06  142.59  142.59  512
- en: 2       131.26  141.95  141.72  142.32  142.49  142.49  512
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 2       131.26  141.95  141.72  142.32  142.49  142.49  512
- en: 3       141.03  141.65  141.63  142.43  138.44  142.43  256
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 3       141.03  141.65  141.63  142.43  138.44  142.43  256
- en: 4       139.90  142.70  142.62  142.20  142.84  142.84  512
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 4       139.90  142.70  142.62  142.20  142.84  142.84  512
- en: 5       138.24  142.08  142.18  142.79  140.94  142.79  256
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 5       138.24  142.08  142.18  142.79  140.94  142.79  256
- en: 6       131.41  142.45  142.32  142.51  142.08  142.51  256
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 6       131.41  142.45  142.32  142.51  142.08  142.51  256
- en: 7       131.98  142.26  142.27  142.11  142.26  142.27  128
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 7       131.98  142.26  142.27  142.11  142.26  142.27  128
- en: 8       132.70  142.47  142.10  142.67  142.19  142.67  256
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 8       132.70  142.47  142.10  142.67  142.19  142.67  256
- en: 9       136.58  142.28  141.89  142.42  142.09  142.42  256
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 9       136.58  142.28  141.89  142.42  142.09  142.42  256
- en: 10      135.61  142.67  141.85  142.86  142.36  142.86  256
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 10      135.61  142.67  141.85  142.86  142.36  142.86  256
- en: 11      136.27  142.48  142.45  142.14  142.41  142.48  64
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 11      136.27  142.48  142.45  142.14  142.41  142.48  64
- en: 12      130.62  141.79  142.06  142.39  142.16  142.39  256
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 12      130.62  141.79  142.06  142.39  142.16  142.39  256
- en: 13      107.98  103.07  105.54  106.51  107.35  107.98  32
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 13      107.98  103.07  105.54  106.51  107.35  107.98  32
- en: 14      103.53  95.38   96.38   98.34   102.92  103.53  32
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 14      103.53  95.38   96.38   98.34   102.92  103.53  32
- en: 15      89.47   84.86   85.31   87.01   90.26   90.26   512
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 15      89.47   84.86   85.31   87.01   90.26   90.26   512
- en: 16      81.53   75.49   75.82   74.36   76.91   81.53   32
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 16      81.53   75.49   75.82   74.36   76.91   81.53   32
- en: '* * *'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.2.11\. Atomic Operations
  id: totrans-640
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.11\. 原子操作
- en: Support for atomic operations was added in SM 1.x, but they were prohibitively
    slow; atomics on global memory were improved on SM 2.x (Fermi-class) hardware
    and vastly improved on SM 3.x (Kepler-class) hardware.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 原子操作的支持在SM 1.x中被引入，但它们非常慢；在SM 2.x（Fermi架构）硬件上，全局内存上的原子操作有所改善，而在SM 3.x（Kepler架构）硬件上则得到了极大的提升。
- en: Most atomic operations, such as `atomicAdd()`, enable code to be simplified
    by replacing reductions (which often require shared memory and synchronization)
    with “fire and forget” semantics. Until SM 3.x hardware arrived, however, that
    type of programming idiom incurred huge performance degradations because pre-Kepler
    hardware was not efficient at dealing with “contended” memory locations (i.e.,
    when many GPU threads are performing atomics on the same memory location).
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数原子操作，例如`atomicAdd()`，通过将需要共享内存和同步的归约操作替换为“触发即忘”语义，简化了代码。然而，在SM 3.x硬件出现之前，这种编程模式会导致巨大的性能下降，因为Kepler之前的硬件在处理“争用”内存位置（即多个GPU线程对同一内存位置进行原子操作时）时效率低下。
- en: '* * *'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Because atomic operations are implemented in the GPU memory controller, they
    only work on local device memory locations. As of this writing, trying to perform
    atomic operations on remote GPUs (via peer-to-peer addresses) or host memory (via
    mapped pinned memory) will not work.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 由于原子操作是通过GPU内存控制器实现的，因此它们仅作用于本地设备内存位置。截至目前，尝试在远程GPU（通过对等地址）或主机内存（通过映射的固定内存）上执行原子操作将无法实现。
- en: '* * *'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Atomics and Synchronization
  id: totrans-647
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 原子操作与同步
- en: Besides “fire and forget” semantics, atomics also may be used for synchronization
    between blocks. CUDA hardware supports the workhorse base abstraction for synchronization,
    “compare and swap” (or CAS). On CUDA, compare-and-swap (also known as compare-and-exchange—e.g.,
    the `CMPXCHG` instruction in x86) is defined as follows.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 除了“火并忘记”（fire and forget）语义外，原子操作还可以用于块之间的同步。CUDA硬件支持用于同步的主力基础抽象——“比较并交换”（或CAS）。在CUDA中，比较并交换（也称为比较并交换—例如x86中的`CMPXCHG`指令）定义如下。
- en: int atomicCAS( int *address, int expected, int value);^([12](ch05.html#ch05fn12))
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: int atomicCAS( int *address, int expected, int value);^([12](ch05.html#ch05fn12))
- en: '[12](ch05.html#ch05fn12a). Unsigned and 64-bit variants of `atomicCAS()` also
    are available.'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '[12](ch05.html#ch05fn12a). `atomicCAS()`的无符号和64位变体也可以使用。'
- en: 'This function reads the word `old` at `address`, computes `(old == expected
    ? value : old)`, stores the result back to `address`, and returns `old`. In other
    words, the memory location is left alone *unless it was equal to the expected
    value specified by the caller*, in which case it is updated with the new value.'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '该函数读取`address`处的字`old`，计算`(old == expected ? value : old)`，将结果存回`address`，并返回`old`。换句话说，内存位置保持不变，*除非它等于调用者指定的期望值*，在这种情况下，它将更新为新值。'
- en: A simple critical section called a “spin lock” can be built out of CAS, as follows.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的临界区，称为“自旋锁”，可以通过CAS构建，如下所示。
- en: void enter_spinlock( int *address )
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: void enter_spinlock( int *address )
- en: '{'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: while atomicCAS( address, 0, 1 );
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 当`atomicCAS( address, 0, 1 );`时；
- en: '}'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Assuming the spin lock’s value is initialized to 0, the `while` loop iterates
    until the spin lock value is 0 when the `atomicCAS()` is executed. When that happens,
    `*address` atomically becomes 1 (the third parameter to `atomicCAS()`) and any
    other threads trying to acquire the critical section spin waiting for the critical
    section value to become 0 again.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 假设自旋锁的值初始化为0，则`while`循环会迭代，直到执行`atomicCAS()`时自旋锁的值变为0。发生这种情况时，`*address`会原子地变为1（即`atomicCAS()`的第三个参数），任何其他尝试获取临界区的线程会自旋等待，直到临界区的值再次变为0。
- en: The thread owning the spin lock can give it up by atomically swapping the 0
    back in
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有自旋锁的线程可以通过原子交换0回来来释放锁。
- en: void leave_spinlock( int *address )
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: void leave_spinlock( int *address )
- en: '{'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: atomicExch( m_p, 0 );
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: atomicExch( m_p, 0 );
- en: '}'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: On CPUs, compare-and-swap instructions are used to implement all manner of synchronization.
    Operating systems use them (sometimes in conjunction with the kernel-level thread
    context switching code) to implement higher-level synchronization primitives.
    CAS also may be used directly to implement “lock-free” queues and other data structures.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 在CPU上，比较并交换指令用于实现各种同步机制。操作系统使用它们（有时与内核级线程上下文切换代码结合使用）来实现更高级别的同步原语。CAS还可以直接用于实现“无锁”队列和其他数据结构。
- en: The CUDA execution model, however, imposes restrictions on the use of global
    memory atomics for synchronization. Unlike CPU threads, some CUDA threads within
    a kernel launch may not begin execution until other threads in the same kernel
    have exited. On CUDA hardware, each SM can context switch a limited number of
    thread blocks, so any kernel launch with more than *MaxThreadBlocksPerSM***NumSMs*
    requires the first thread blocks to exit before more thread blocks can begin execution.
    As a result, it is important that developers not assume all of the threads in
    a given kernel launch are active.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，CUDA 执行模型对使用全局内存原子操作进行同步施加了限制。与 CPU 线程不同，某些 CUDA 线程在内核启动时可能在其他线程退出之前无法开始执行。在
    CUDA 硬件上，每个 SM 可以上下文切换有限数量的线程块，因此任何线程块数量超过 *MaxThreadBlocksPerSM***NumSMs* 的内核启动需要等待第一个线程块退出后，才能开始执行更多的线程块。因此，开发者不应假设给定内核启动中的所有线程都是活动的。
- en: Additionally, the `enter_spinlock()` routine above is prone to deadlock if used
    for intrablock synchronization,^([13](ch05.html#ch05fn13)) for which it is unsuitable
    in any case, since the hardware supports so many better ways for threads within
    the same block to communicate and synchronize with one another (shared memory
    and `__syncthreads()`, respectively).
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，上面的 `enter_spinlock()` 例程如果用于块内同步，容易发生死锁，^([13](ch05.html#ch05fn13))，在任何情况下它都不适用，因为硬件支持许多更好的线程通信和同步方式（分别是共享内存和
    `__syncthreads()`）。
- en: '[13](ch05.html#ch05fn13a). Expected usage is for one thread in each block to
    attempt to acquire the spinlock. Otherwise, the divergent code execution tends
    to deadlock.'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '[13](ch05.html#ch05fn13a)。预期的使用方式是每个块中的一个线程尝试获取自旋锁。否则，分歧的代码执行往往会导致死锁。'
- en: '[Listing 5.8](ch05.html#ch05lis08) shows the implementation of the `cudaSpinlock`
    class, which uses the algorithm listed above and is subject to the just-described
    limitations.'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 5.8](ch05.html#ch05lis08) 展示了 `cudaSpinlock` 类的实现，该类使用上面列出的算法，并且受到刚才描述的限制。'
- en: '*Listing 5.8.* `cudaSpinlock` class.'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5.8.* `cudaSpinlock` 类。'
- en: '[Click here to view code image](ch05_images.html#p05lis08a)'
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis08a)'
- en: '* * *'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: class cudaSpinlock {
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: class cudaSpinlock {
- en: 'public:'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 'public:'
- en: cudaSpinlock( int *p );
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSpinlock( int *p );
- en: void acquire();
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: void acquire();
- en: void release();
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: void release();
- en: 'private:'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 'private:'
- en: int *m_p;
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: int *m_p;
- en: '};'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: inline __device__
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__
- en: cudaSpinlock::cudaSpinlock( int *p )
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSpinlock::cudaSpinlock( int *p )
- en: '{'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: m_p = p;
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: m_p = p;
- en: '}'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: inline __device__ void
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ void
- en: cudaSpinlock::acquire( )
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSpinlock::acquire( )
- en: '{'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: while ( atomicCAS( m_p, 0, 1 ) );
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: while ( atomicCAS( m_p, 0, 1 ) );
- en: '}'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: inline __device__ void
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ void
- en: cudaSpinlock::release( )
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSpinlock::release( )
- en: '{'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: atomicExch( m_p, 0 );
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: atomicExch( m_p, 0 );
- en: '}'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Use of `cudaSpinlock` is illustrated in the `spinlockReduction.cu` sample, which
    computes the sum of an array of `double` values by having each block perform a
    reduction in shared memory, then using the spin lock to synchronize for the summation.
    [Listing 5.9](ch05.html#ch05lis09) gives the `SumDoubles` function from this sample.
    Note how adding the partial sum is performed only by thread 0 of each block.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cudaSpinlock`的示例可以在`spinlockReduction.cu`样本中看到，该样本通过让每个块在共享内存中执行归约操作，然后使用自旋锁进行同步来计算一个`double`类型数组的总和。[清单
    5.9](ch05.html#ch05lis09)提供了该示例中的`SumDoubles`函数。注意，部分总和的添加仅由每个块的线程 0 执行。
- en: '*Listing 5.9.* `SumDoubles` function.'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5.9*。`SumDoubles`函数。'
- en: '[Click here to view code image](ch05_images.html#p05lis09a)'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p05lis09a)'
- en: '* * *'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: SumDoubles(
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: SumDoubles(
- en: double *pSum,
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: double *pSum,
- en: int *spinlock,
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: int *spinlock,
- en: const double *in,
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: const double *in,
- en: size_t N,
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: int *acquireCount )
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: int *acquireCount )
- en: '{'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: SharedMemory<double> shared;
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: SharedMemory<double> shared;
- en: cudaSpinlock globalSpinlock( spinlock );
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSpinlock globalSpinlock( spinlock );
- en: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N;
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: shared[threadIdx.x] = in[i];
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: shared[threadIdx.x] = in[i];
- en: __syncthreads();
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: double blockSum = Reduce_block<double,double>( );
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: double blockSum = Reduce_block<double,double>( );
- en: __syncthreads();
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: if ( threadIdx.x == 0 ) {
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: if ( threadIdx.x == 0 ) {
- en: globalSpinlock.acquire( );
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: globalSpinlock.acquire( );
- en: '*pSum += blockSum;'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '*pSum += blockSum;'
- en: __threadfence();
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: __threadfence();
- en: globalSpinlock.release( );
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: globalSpinlock.release( );
- en: '}'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.2.12\. Texturing from Global Memory
  id: totrans-725
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.12\. 来自全局内存的纹理映射
- en: For applications that cannot conveniently adhere to the coalescing constraints,
    the texture mapping hardware presents a satisfactory alternative. The hardware
    supports texturing from global memory (via `cudaBindTexture()/cuTexRefSetAddress()`,
    which has lower peak performance than coalesced global reads but higher performance
    for less-regular access. The texture cache resources are also separate from other
    cache resources on the chip. A software coherency scheme is enforced by the driver
    invalidating the texture cache before kernel invocations that contain `TEX` instructions.^([14](ch05.html#ch05fn14))
    See [Chapter 10](ch10.html#ch10) for details.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些不能方便地遵守合并约束的应用程序，纹理映射硬件提供了一个令人满意的替代方案。该硬件支持从全局内存进行纹理映射（通过`cudaBindTexture()/cuTexRefSetAddress()`，其峰值性能低于合并的全局读取，但对于不规则访问性能更高。纹理缓存资源也与芯片上的其他缓存资源分开。驱动程序通过在包含`TEX`指令的内核调用之前使纹理缓存失效，来强制执行软件一致性方案。^([14](ch05.html#ch05fn14))
    详情请参见[第 10 章](ch10.html#ch10)。
- en: '[14](ch05.html#ch05fn14a). `TEX` is the SASS mnemonic for microcode instructions
    that perform texture fetches.'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '[14](ch05.html#ch05fn14a)。`TEX`是执行纹理获取的微代码指令的SASS助记符。'
- en: 'SM 3.x hardware added the ability to read global memory through the texture
    cache hierarchy without setting up and binding a texture reference. This functionality
    may be accessed with a standard C++ language constructs: the `const restrict`
    keywords. Alternatively, you can use the `__ldg()` intrinsics defined in `sm_35_intrinsics.h`.'
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: SM 3.x硬件增加了通过纹理缓存层次结构读取全局内存的能力，无需设置和绑定纹理引用。可以使用标准的C++语言构造来访问此功能：`const restrict`关键字。或者，您可以使用在`sm_35_intrinsics.h`中定义的`__ldg()`内建函数。
- en: 5.2.13\. ECC (Error Correcting Codes)
  id: totrans-729
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.13\. ECC（错误更正码）
- en: SM 2.x and later GPUs in the Tesla (i.e., server GPU) product line come with
    the ability to run with error correction. In exchange for a smaller amount of
    memory (since some memory is used to record some redundancy) and lower bandwidth,
    GPUs with ECC enabled can silently correct single-bit errors and report double-bit
    errors.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉（即服务器GPU）产品线中的SM 2.x及更高版本的GPU具备错误更正能力。通过使用较少的内存（因为一些内存用于记录冗余信息）和较低的带宽，启用ECC的GPU可以默默地修正单比特错误，并报告双比特错误。
- en: ECC has the following characteristics.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: ECC具有以下特点：
- en: • It reduces the amount of available memory by 12.5%. On a `cg1.4xlarge` instance
    in Amazon EC2, for example, it reduces the amount of memory from 3071MB to 2687MB.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: • 它会将可用内存减少12.5%。例如，在Amazon EC2的`cg1.4xlarge`实例上，它将内存从3071MB减少到2687MB。
- en: • It makes context synchronization more expensive.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: • 它增加了上下文同步的开销。
- en: • Uncoalesced memory transactions are more expensive when ECC is enabled than
    otherwise.
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: • 启用ECC时，非合并内存事务的开销比未启用时更高。
- en: ECC can be enabled and disabled using the `nvidia-smi` command-line tool (described
    in [Section 4.4](ch04.html#ch04lev1sec4)) or by using the NVML (NVIDIA Management
    Library).
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`nvidia-smi`命令行工具（详见[第4.4节](ch04.html#ch04lev1sec4)）或通过使用NVML（NVIDIA管理库）来启用或禁用ECC。
- en: When an uncorrectable ECC error is detected, synchronous error-reporting mechanisms
    will return `cudaErrorECCUncorrectable` (for the CUDA runtime) and `CUDA_ERROR_ECC_UNCORRECTABLE`
    (for the driver API).
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 当检测到无法更正的ECC错误时，同步错误报告机制将返回`cudaErrorECCUncorrectable`（针对CUDA运行时）和`CUDA_ERROR_ECC_UNCORRECTABLE`（针对驱动程序API）。
- en: 5.3\. Constant Memory
  id: totrans-737
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 常量内存
- en: Constant memory is optimized for read-only broadcast to multiple threads. As
    the name implies, the compiler uses constant memory to hold constants that couldn’t
    be easily computed or otherwise compiled directly into the machine code. Constant
    memory resides in device memory but is accessed using different instructions that
    cause the GPU to access it using a special “constant cache.”
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 常量内存优化为只读广播到多个线程。如其名所示，编译器使用常量内存来存储无法轻易计算或无法直接编译成机器代码的常量。常量内存位于设备内存中，但通过不同的指令进行访问，这使得GPU通过特殊的“常量缓存”来访问它。
- en: The compiler for constants has 64K of memory available to use at its discretion.
    The developer has another 64K of memory available that can be declared with the
    `__constant__` keyword. These limits are per-module (for driver API applications)
    or per-file (for CUDA runtime applications).
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: 常量的编译器有 64K 内存可以自由使用。开发者还有 64K 内存可用，可以通过 `__constant__` 关键字声明。这些限制是按模块（对于驱动
    API 应用程序）或按文件（对于 CUDA 运行时应用程序）计算的。
- en: Naïvely, one might expect `__constant__` memory to be analogous to the `const`
    keyword in C/C++, where it cannot be changed after initialization. But `__constant__`
    memory can be changed, either by memory copies or by querying the pointer to `__constant__`
    memory and writing to it with a kernel. CUDA kernels must not write to `__constant__`
    memory ranges that they may be accessing because the constant cache is not kept
    coherent with respect to the rest of the memory hierarchy during kernel execution.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 从表面上看，人们可能会认为 `__constant__` 内存类似于 C/C++ 中的 `const` 关键字，初始化后不能更改。但 `__constant__`
    内存是可以改变的，改变的方式包括内存复制或通过查询指针并使用内核向其写入。CUDA 内核不应写入它们可能访问的 `__constant__` 内存区域，因为常量缓存不会在内核执行期间与其他内存层次结构保持一致。
- en: 5.3.1\. Host and Device `__constant__` Memory
  id: totrans-741
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 主机和设备 `__constant__` 内存
- en: Mark Harris describes the following idiom that uses the predefined macro `__CUDA_ARCH__`
    to maintain host and device copies of `__constant__` memory that are conveniently
    accessed by both the CPU and GPU.^([15](ch05.html#ch05fn15))
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: Mark Harris 描述了以下惯用法，使用预定义宏 `__CUDA_ARCH__` 来维护主机和设备上的 `__constant__` 内存副本，这些副本可以方便地被
    CPU 和 GPU 访问。^([15](ch05.html#ch05fn15))
- en: '[15](ch05.html#ch05fn15a). [http://bit.ly/OpMdN5](http://bit.ly/OpMdN5)'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: '[15](ch05.html#ch05fn15a). [http://bit.ly/OpMdN5](http://bit.ly/OpMdN5)'
- en: '[Click here to view code image](ch05_images.html#p157pro01a)'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p157pro01a)'
- en: __constant__ double dc_vals[2] = { 0.0, 1000.0 };
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: __constant__ double dc_vals[2] = { 0.0, 1000.0 };
- en: const double hc_vals[2] = { 0.0, 1000.0 };
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: const double hc_vals[2] = { 0.0, 1000.0 };
- en: __device__ __host__ double f(size_t i)
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ __host__ double f(size_t i)
- en: '{'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '#ifdef __CUDA_ARCH__'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: '#ifdef __CUDA_ARCH__'
- en: return dc_vals[i];
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: return dc_vals[i];
- en: '#else'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '#else'
- en: return hc_vals[i];
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: return hc_vals[i];
- en: '#endif'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: '#endif'
- en: '}'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 5.3.2\. Accessing `__constant__` Memory
  id: totrans-755
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 访问 `__constant__` 内存
- en: Besides the accesses to constant memory implicitly caused by C/C++ operators,
    developers can copy to and from constant memory, and even query the pointer to
    a constant memory allocation.
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 C/C++ 运算符隐式引起的常量内存访问外，开发者还可以将数据复制到常量内存中并从中复制，甚至查询指向常量内存分配的指针。
- en: CUDA Runtime
  id: totrans-757
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA 运行时
- en: CUDA runtime applications can copy to and from `__constant__` memory using `cudaMemcpyToSymbol()`
    and `cudaMemcpyFromSymbol()`, respectively. The pointer to `__constant__` memory
    can be queried with `cudaGetSymbolAddress()`.
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 运行时应用程序可以分别使用 `cudaMemcpyToSymbol()` 和 `cudaMemcpyFromSymbol()` 将数据复制到
    `__constant__` 内存中并从中复制。可以通过 `cudaGetSymbolAddress()` 查询 `__constant__` 内存的指针。
- en: cudaError_t cudaGetSymbolAddress( void **devPtr, char *symbol );
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaGetSymbolAddress( void **devPtr, char *symbol );
- en: This pointer may be used to write to constant memory with a kernel, though developers
    must take care not to write to the constant memory while another kernel is reading
    it.
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指针可用于在内核中写入常量内存，但开发者必须小心，避免在另一个内核读取常量内存时进行写操作。
- en: Driver API
  id: totrans-761
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序API
- en: Driver API applications can query the device pointer of constant memory using
    `cuModuleGetGlobal()`. The driver API does not include a special memory copy function
    like `cudaMemcpyToSymbol()`, since it does not have the language integration of
    the CUDA runtime. Applications must query the address with `cuModuleGetGlobal()`
    and then call `cuMemcpyHtoD()` or `cuMemcpyDtoH()`. The amount of constant memory
    used by a kernel may be queried with `cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_CONSTANT_SIZE_BYTES)`.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序API应用程序可以使用`cuModuleGetGlobal()`查询常量内存的设备指针。由于驱动程序API没有像`cudaMemcpyToSymbol()`这样的特殊内存复制功能（因为它没有CUDA运行时的语言集成），应用程序必须通过`cuModuleGetGlobal()`查询地址，然后调用`cuMemcpyHtoD()`或`cuMemcpyDtoH()`。可以使用`cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_CONSTANT_SIZE_BYTES)`查询内核使用的常量内存量。
- en: 5.4\. Local Memory
  id: totrans-763
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 局部内存
- en: Local memory contains the stack for every thread in a CUDA kernel. It is used
    as follows.
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 局部内存包含每个线程在CUDA内核中的堆栈。其用途如下。
- en: • To implement the application binary interface (ABI)—that is, the calling convention
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: • 用于实现应用程序二进制接口（ABI）——即调用约定
- en: • To spill data out of registers
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: • 用于将数据溢出到寄存器之外
- en: • To hold arrays whose indices cannot be resolved by the compiler
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: • 用于保存编译器无法解析其索引的数组
- en: In early implementations of CUDA hardware, *any* use of local memory was the
    “kiss of death.” It slowed things down so much that developers were encouraged
    to take whatever measure was needed to get rid of the local memory usage. With
    the advent of an L1 cache in Fermi, these performance concerns are less urgent,
    provided the local memory traffic is confined to L1.^([16](ch05.html#ch05fn16))
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA硬件的早期实现中，*任何*使用局部内存的操作几乎都是“致命的”。它会极大地拖慢性能，开发者被鼓励采取任何必要的措施以消除局部内存的使用。随着Fermi中L1缓存的出现，只要局部内存流量限制在L1中，这些性能问题已经不那么紧迫了。^([16](ch05.html#ch05fn16))
- en: '[16](ch05.html#ch05fn16a). The L1 cache is per-SM and is physically implemented
    in the same hardware as shared memory.'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '[16](ch05.html#ch05fn16a)。L1缓存是每个SM的，并且在硬件上与共享内存实现为相同的硬件。'
- en: 'Developers can make the compiler report the amount of local memory needed by
    a given kernel with the `nvcc` options: `-Xptxas –v,abi=no.` At runtime, the amount
    of local memory used by a kernel may be queried with'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可以通过`nvcc`选项：`-Xptxas –v,abi=no`让编译器报告给定内核所需的局部内存大小。
- en: cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES).
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行时，可以使用`cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_LOCAL_SIZE_BYTES)`查询内核使用的局部内存量。
- en: 'Paulius Micikevicius of NVIDIA gave a good presentation on how to determine
    whether local memory usage was impacting performance and what to do about it.^([17](ch05.html#ch05fn17))
    Register spilling can incur two costs: an increased number of instructions and
    an increase in the amount of memory traffic.'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 的 Paulius Micikevicius 做了一个很好的演讲，介绍了如何判断本地内存使用是否影响性能，以及应该采取的措施。^([17](ch05.html#ch05fn17))
    寄存器溢出可能会带来两种成本：增加指令数量和增加内存流量。
- en: '[17](ch05.html#ch05fn17a). [http://bit.ly/ZAeHc5](http://bit.ly/ZAeHc5)'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '[17](ch05.html#ch05fn17a)。 [http://bit.ly/ZAeHc5](http://bit.ly/ZAeHc5)'
- en: The L1 and L2 performance counters can be used to determine if the memory traffic
    is impacting performance. Here are some strategies to improve performance in this
    case.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: L1 和 L2 性能计数器可以用来确定内存流量是否影响性能。以下是一些在这种情况下提高性能的策略。
- en: • At compile time, specify a higher limit in `–maxregcount.` By increasing the
    number of registers available to the thread, both the instruction count and the
    memory traffic will decrease. The `__launch_bounds__` directive may be used to
    tune this parameter when the kernel is being compiled online by PTXAS.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: • 在编译时，指定更高的`–maxregcount`限制。通过增加线程可用的寄存器数量，指令数和内存流量都会减少。`__launch_bounds__`指令可以用于调优此参数，当内核通过
    PTXAS 在线编译时。
- en: • Use noncaching loads for global memory, such as `nvcc –Xptxas -dlcm=cg.`
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: • 对全局内存使用非缓存加载，例如 `nvcc –Xptxas -dlcm=cg.`
- en: • Increase the L1 size to 48K. (Call `cudaFuncSetCacheConfig()` or `cudaDeviceSetCacheconfig().`)
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: • 增加 L1 大小到 48K。（调用 `cudaFuncSetCacheConfig()` 或 `cudaDeviceSetCacheconfig()`。）
- en: When launching a kernel that uses more than the default amount of memory allocated
    for local memory, the CUDA driver must allocate a new local memory buffer before
    the kernel can launch. As a result, the kernel launch may take extra time; may
    cause unexpected CPU/GPU synchronization; and, if the driver is unable to allocate
    the buffer for local memory, may fail.^([18](ch05.html#ch05fn18)) By default,
    the CUDA driver will free these larger local memory allocations after the kernel
    has launched. This behavior can be inhibited by specifying the `CU_CTX_RESIZE_LMEM_TO_MAX`
    flag to `cuCtxCreate()` or calling `cudaSetDeviceFlags()` with the `cudaDeviceLmemResizeToMax`
    flag set.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 当启动一个使用超过默认本地内存分配的内核时，CUDA 驱动程序必须在内核启动之前分配一个新的本地内存缓冲区。因此，内核启动可能会花费额外的时间；可能会导致意外的
    CPU/GPU 同步；如果驱动程序无法为本地内存分配缓冲区，可能会失败。^([18](ch05.html#ch05fn18)) 默认情况下，CUDA 驱动程序会在内核启动后释放这些较大的本地内存分配。通过在调用
    `cuCtxCreate()` 时指定 `CU_CTX_RESIZE_LMEM_TO_MAX` 标志，或者调用 `cudaSetDeviceFlags()`
    时设置 `cudaDeviceLmemResizeToMax` 标志，可以抑制这种行为。
- en: '[18](ch05.html#ch05fn18a). Since most resources are preallocated, an inability
    to allocate local memory is one of the few circumstances that can cause a kernel
    launch to fail at runtime.'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: '[18](ch05.html#ch05fn18a)。由于大多数资源是预分配的，无法分配本地内存是导致内核启动在运行时失败的少数情况之一。'
- en: It is not difficult to build a templated function that illustrates the “performance
    cliff” when register spills occur. The templated `GlobalCopy` kernel in [Listing
    5.10](ch05.html#ch05lis10) implements a simple memcpy routine that uses a local
    array temp to stage global memory references. The template parameter `n` specifies
    the number of elements in `temp` and thus the number of loads and stores to perform
    in the inner loop of the memory copy.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个模板函数来说明寄存器溢出时的“性能悬崖”并不困难。 [列表 5.10](ch05.html#ch05lis10) 中的模板 `GlobalCopy`
    内核实现了一个简单的 memcpy 例程，该例程使用局部数组 temp 来暂存全局内存引用。模板参数 `n` 指定了 `temp` 中元素的数量，从而指定了在内存复制的内循环中执行的加载和存储的数量。
- en: As a quick review of the SASS microcode emitted by the compiler will confirm,
    the compiler can keep `temp` in registers until `n` becomes too large.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 对于编译器发出的 SASS 微代码的快速回顾将确认，编译器可以在 `n` 变得过大之前将 `temp` 保持在寄存器中。
- en: '*Listing 5.10.* `GlobalCopy` kernel.'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 5.10.* `GlobalCopy` 内核。'
- en: '[Click here to view code image](ch05_images.html#p05lis10a)'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis10a)'
- en: '* * *'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, const int n>
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, const int n>
- en: __global__ void
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: GlobalCopy( T *out, const T *in, size_t N )
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: GlobalCopy( T *out, const T *in, size_t N )
- en: '{'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: T temp[n];
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: T temp[n];
- en: size_t i;
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i;
- en: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N-n*blockDim.x*gridDim.x;
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: i < N-n*blockDim.x*gridDim.x;
- en: i += n*blockDim.x*gridDim.x ) {
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: i += n*blockDim.x*gridDim.x ) {
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: temp[j] = in[index];
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: temp[j] = in[index];
- en: '}'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: out[index] = temp[j];
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: out[index] = temp[j];
- en: '}'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // to avoid the (index<N) conditional in the inner loop,
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: // 为了避免内循环中的 (index<N) 条件，
- en: // we left off some work at the end
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: // 我们在最后留下了一些工作
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: if ( index<N ) temp[j] = in[index];
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index<N ) temp[j] = in[index];
- en: '}'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: if ( index<N ) out[index] = temp[j];
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index<N ) out[index] = temp[j];
- en: '}'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 5.11](ch05.html#ch05lis11) shows an excerpt of the output from `globalCopy.cu`
    on a GK104 GPU: the copy performance of 64-bit operands only. The degradation
    in performance due to register spilling becomes obvious in the row corresponding
    to a loop unroll of 12, where the delivered bandwidth decreases from 117GB/s to
    less than 90GB/s, and degrades further to under 30GB/s as the loop unroll increases
    to 16.'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 5.11](ch05.html#ch05lis11) 显示了在 GK104 GPU 上 `globalCopy.cu` 的输出摘录：仅 64
    位操作数的复制性能。由于寄存器溢出导致的性能下降在对应于循环展开为 12 的行中变得明显，传输带宽从 117GB/s 降低到不到 90GB/s，并随着循环展开增加到
    16 进一步降至 30GB/s 以下。'
- en: '[Table 5.9](ch05.html#ch05tab09) summarizes the register and local memory usage
    for the kernels corresponding to the unrolled loops. The performance degradation
    of the copy corresponds to the local memory usage. In this case, every thread
    always spills in the inner loop; presumably, the performance wouldn’t degrade
    so much if only some of the threads were spilling (for example, when executing
    a divergent code path).'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5.9](ch05.html#ch05tab09) 总结了与展开循环对应的内核的寄存器和局部内存使用情况。拷贝的性能下降与局部内存的使用有关。在这种情况下，每个线程在内循环中都会发生溢出；可以推测，如果只有部分线程发生溢出（例如，在执行分支代码路径时），性能不会如此下降。'
- en: '![Image](graphics/05tab09.jpg)'
  id: totrans-819
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/05tab09.jpg)'
- en: '*Table 5.9* `globalCopy` Register and Local Memory Usage'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.9* `globalCopy` 寄存器和局部内存使用情况'
- en: '*Listing 5.11.* `globalCopy.cu` output (64-bit only).'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 5.11.* `globalCopy.cu` 输出（仅限 64 位）。'
- en: '[Click here to view code image](ch05_images.html#p05lis11a)'
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch05_images.html#p05lis11a)'
- en: '* * *'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Operand size: 8 bytes'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数大小：8 字节
- en: 'Input size: 16M operands'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小：16M 操作数
- en: Block Size
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       75.57   102.57  116.03  124.51  126.21  126.21  512
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 1       75.57   102.57  116.03  124.51  126.21  126.21  512
- en: 2       105.73  117.09  121.84  123.07  124.00  124.00  512
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 2       105.73  117.09  121.84  123.07  124.00  124.00  512
- en: 3       112.49  120.88  121.56  123.09  123.44  123.44  512
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 3       112.49  120.88  121.56  123.09  123.44  123.44  512
- en: 4       115.54  122.89  122.38  122.15  121.22  122.89  64
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 4       115.54  122.89  122.38  122.15  121.22  122.89  64
- en: 5       113.81  121.29  120.11  119.69  116.02  121.29  64
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 5       113.81  121.29  120.11  119.69  116.02  121.29  64
- en: 6       114.84  119.49  120.56  118.09  117.88  120.56  128
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 6       114.84  119.49  120.56  118.09  117.88  120.56  128
- en: 7       117.53  122.94  118.74  116.52  110.99  122.94  64
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 7       117.53  122.94  118.74  116.52  110.99  122.94  64
- en: 8       116.89  121.68  119.00  113.49  105.69  121.68  64
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 8       116.89  121.68  119.00  113.49  105.69  121.68  64
- en: 9       116.10  120.73  115.96  109.48  99.60   120.73  64
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 9       116.10  120.73  115.96  109.48  99.60   120.73  64
- en: 10      115.02  116.70  115.30  106.31  93.56   116.70  64
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: 10      115.02  116.70  115.30  106.31  93.56   116.70  64
- en: 11      113.67  117.36  111.48  102.84  88.31   117.36  64
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 11      113.67  117.36  111.48  102.84  88.31   117.36  64
- en: 12      88.16   86.91   83.68   73.78   58.55   88.16   32
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 12      88.16   86.91   83.68   73.78   58.55   88.16   32
- en: 13      85.27   85.58   80.09   68.51   52.66   85.58   64
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 13      85.27   85.58   80.09   68.51   52.66   85.58   64
- en: 14      78.60   76.30   69.50   56.59   41.29   78.60   32
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 14      78.60   76.30   69.50   56.59   41.29   78.60   32
- en: 15      69.00   65.78   59.82   48.41   34.65   69.00   32
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 15      69.00   65.78   59.82   48.41   34.65   69.00   32
- en: 16      65.68   62.16   54.71   43.02   29.92   65.68   32
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 16      65.68   62.16   54.71   43.02   29.92   65.68   32
- en: '* * *'
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.5\. Texture Memory
  id: totrans-845
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5\. 纹理内存
- en: 'In CUDA, the concept of texture memory is realized in two parts: a *CUDA array*
    contains the physical memory allocation, and a *texture reference* or *surface
    reference*^([19](ch05.html#ch05fn19)) contains a “view” that can be used to read
    or write a CUDA array. The CUDA array is just an untyped “bag of bits” with a
    memory layout optimized for 1D, 2D, or 3D access. A texture reference contains
    information on how the CUDA array should be addressed and how its contents should
    be interpreted.'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，纹理内存的概念分为两部分：*CUDA数组*包含物理内存分配，*纹理引用*或*表面引用*^([19](ch05.html#ch05fn19))包含一个“视图”，该视图可以用来读取或写入CUDA数组。CUDA数组只是一个未指定类型的“位包”，其内存布局经过优化，适用于1D、2D或3D访问。纹理引用包含关于如何寻址CUDA数组以及如何解释其内容的信息。
- en: '[19](ch05.html#ch05fn19a). Surface references can be used only on SM 2.x and
    later hardware.'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: '[19](ch05.html#ch05fn19a). 表面引用仅能在SM 2.x及更高版本的硬件上使用。'
- en: When using a texture reference to read from a CUDA array, the hardware uses
    a separate, read-only cache to resolve the memory references. While the kernel
    is executing, the texture cache is not kept coherent with respect to the rest
    of the memory subsystem, so it is important not to use texture references to alias
    memory that will be operated on by the kernel. (The cache is invalidated between
    kernel launches.)
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用纹理引用从CUDA数组中读取数据时，硬件使用一个独立的只读缓存来解析内存引用。在内核执行过程中，纹理缓存与其他内存子系统之间的状态不会保持一致，因此重要的是不要使用纹理引用来别名将被内核操作的内存。（缓存会在内核启动之间被失效。）
- en: On SM 3.5 hardware, reads via texture can be explicitly requested by the developer
    using the `const restricted` keywords. The `restricted` keyword does nothing more
    than make the just-described “no aliasing” guarantee that the memory in question
    won’t be referenced by the kernel in any other way. When reading or writing a
    CUDA array with a surface reference, the memory traffic goes through the same
    memory hierarchy as global loads and stores. [Chapter 10](ch10.html#ch10) contains
    a detailed discussion of how to allocate and use textures in CUDA.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 在SM 3.5硬件上，开发者可以通过使用`const restricted`关键字显式地请求通过纹理进行读取。`restricted`关键字的作用仅仅是保证“无别名”的承诺，即保证该内存不会被内核以其他方式引用。当使用表面引用读取或写入CUDA数组时，内存流量会通过与全局加载和存储相同的内存层次结构。[第10章](ch10.html#ch10)详细讨论了如何在CUDA中分配和使用纹理。
- en: 5.6\. Shared Memory
  id: totrans-850
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6\. 共享内存
- en: Shared memory is used to exchange data between CUDA threads within a block.
    Physically, it is implemented with a per-SM memory that can be accessed very quickly.
    In terms of speed, shared memory is perhaps 10x slower than register accesses
    but 10x faster than accesses to global memory. As a result, shared memory is often
    a critical resource to reduce the external bandwidth needed by CUDA kernels.
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存用于在CUDA线程块内的线程之间交换数据。在物理上，它是通过每个SM内存实现的，可以非常快速地访问。在速度方面，共享内存的访问速度大约比寄存器访问慢10倍，但比访问全局内存快10倍。因此，共享内存通常是减少CUDA内核所需外部带宽的关键资源。
- en: 'Since developers explicitly allocate and reference shared memory, it can be
    thought of as a “manually managed” cache or “scratchpad” memory. Developers can
    request different cache configurations at both the kernel and the device level:
    `cudaDeviceSetCacheConfig()/cuCtxSetCacheConfig()` specify the preferred cache
    configuration for a CUDA device, while `cudaFuncSetCacheConfig()/cuFuncSetCacheConfig()`
    specify the preferred cache configuration for a given kernel. If both are specified,
    the per-kernel request takes precedence, but in any case, the requirements of
    the kernel may override the developer’s preference.'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 由于开发人员显式地分配和引用共享内存，它可以被视为一种“手动管理”的缓存或“临时存储”内存。开发人员可以在内核和设备级别请求不同的缓存配置：`cudaDeviceSetCacheConfig()/cuCtxSetCacheConfig()`
    用于指定 CUDA 设备的首选缓存配置，而 `cudaFuncSetCacheConfig()/cuFuncSetCacheConfig()` 用于指定给定内核的首选缓存配置。如果两者都被指定，则每个内核的请求具有优先权，但在任何情况下，内核的需求可能会覆盖开发人员的偏好。
- en: Kernels that use shared memory typically are written in three phases.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享内存的内核通常分为三个阶段编写。
- en: • Load shared memory and `__syncthreads()`
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: '• 加载共享内存和`__syncthreads()`  '
- en: • Process shared memory and `__syncthreads()`
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: '• 处理共享内存和`__syncthreads()`  '
- en: • Write results
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: '• 写入结果  '
- en: 'Developers can make the compiler report the amount of shared memory used by
    a given kernel with the `nvcc` options: `-Xptxas –v,abi=no.` At runtime, the amount
    of shared memory used by a kernel may be queried with `cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES)`.'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以使用 `nvcc` 选项让编译器报告给定内核使用的共享内存量：`-Xptxas –v,abi=no.` 在运行时，可以通过 `cuFuncGetAttribute(CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES)`
    查询内核使用的共享内存量。
- en: 5.6.1\. Unsized Shared Memory Declarations
  id: totrans-858
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '5.6.1\. 未指定大小的共享内存声明  '
- en: Any shared memory declared in the kernel itself is automatically allocated for
    each block when the kernel is launched. If the kernel also includes an unsized
    declaration of shared memory, the amount of memory needed by that declaration
    must be specified when the kernel is launched.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中声明的任何共享内存都会在内核启动时自动为每个块分配。如果内核还包括未指定大小的共享内存声明，则必须在启动内核时指定该声明所需的内存量。
- en: If there is more than one `extern __shared__` memory declaration, they are aliased
    with respect to one another, so the declaration
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: '如果有多个 `extern __shared__` 内存声明，它们相互之间是别名，因此该声明  '
- en: extern __shared__ char sharedChars[];
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 'extern __shared__ char sharedChars[];  '
- en: extern __shared__ int sharedInts[];
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: 'extern __shared__ int sharedInts[];  '
- en: enables the same shared memory to be addressed as 8- or 32-bit integers, as
    needed. One motivation for using this type of aliasing is to use wider types when
    possible to read and write global memory, while using the narrow ones for kernel
    computations.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 它使得可以根据需要将相同的共享内存作为 8 位或 32 位整数进行寻址。使用这种别名的一个动机是，当可能时，使用较宽的类型来读写全局内存，而使用较窄的类型进行内核计算。
- en: '* * *'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *  '
- en: Note
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: '注意  '
- en: If you have more than one kernel that uses unsized shared memory, they must
    be compiled in separate files.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有多个使用未指定大小共享内存的内核，它们必须在不同的文件中编译。
- en: '* * *'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 5.6.2\. Warp-Synchronous Coding
  id: totrans-868
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.6.2\. Warp同步编程
- en: Shared memory variables that will be participating in warp-synchronous programming
    must be declared as `volatile` to prevent the compiler from applying optimizations
    that will render the code incorrect.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 参与warp同步编程的共享内存变量必须声明为`volatile`，以防编译器应用会使代码不正确的优化。
- en: 5.6.3\. Pointers to Shared Memory
  id: totrans-870
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.6.3\. 指向共享内存的指针
- en: It is valid—and often convenient—to use pointers to refer to shared memory.
    Example kernels that use this idiom include the reduction kernels in [Chapter
    12](ch12.html#ch12) ([Listing 12.3](ch12.html#ch12lis03)) and the `scanBlock`
    kernel in [Chapter 13](ch13.html#ch13) ([Listing 13.3](ch13.html#ch13lis03)).
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 使用指针来引用共享内存是有效的——且通常很方便。使用这种习惯的示例内核包括[第12章](ch12.html#ch12)中的归约内核（[列表12.3](ch12.html#ch12lis03)）和[第13章](ch13.html#ch13)中的`scanBlock`内核（[列表13.3](ch13.html#ch13lis03)）。
- en: 5.7\. Memory Copy
  id: totrans-872
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7\. 内存复制
- en: CUDA has three different memory types—host memory, device memory, and CUDA arrays—and
    a full complement of functions to copy between them. For host↔device memcpy, an
    additional set of functions provide *asynchronous* memcpy between pinned host
    memory and device memory or CUDA arrays. Additionally, a set of peer-to-peer memcpy
    functions enable memory to be copied between GPUs.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA有三种不同的内存类型——主机内存、设备内存和CUDA数组——并且提供了一整套函数来在它们之间进行复制。对于主机↔设备的memcpy，额外的一组函数提供了在固定主机内存和设备内存或CUDA数组之间进行*异步*memcpy的功能。此外，一组点对点memcpy函数使得内存可以在多个GPU之间进行复制。
- en: The CUDA runtime and the driver API take very different approaches. For 1D memcpy,
    the driver API defined a family of functions with type-strong parameters. The
    host-to-device, device-to-host, and device-to-device memcpy functions are separate.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA运行时和驱动程序API采用了截然不同的方法。对于1D的memcpy，驱动程序API定义了一组具有强类型参数的函数。主机到设备、设备到主机、以及设备到设备的memcpy函数是分开的。
- en: '[Click here to view code image](ch05_images.html#p164pro01a)'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p164pro01a)'
- en: CUresult cuMemcpyHtoD(CUdeviceptr dstDevice, const void *srcHost,
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuMemcpyHtoD(CUdeviceptr 目标设备，const void *源主机，
- en: size_t ByteCount);
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: size_t 字节数)；
- en: CUresult cuMemcpyDtoH(void *dstHost, CUdeviceptr srcDevice, size_t
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuMemcpyDtoH(void *目标主机，CUdeviceptr 源设备，size_t
- en: ByteCount);
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 字节数)；
- en: CUresult cuMemcpyDtoD(CUdeviceptr dstDevice, CUdeviceptr srcDevice,
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuMemcpyDtoD(CUdeviceptr 目标设备，CUdeviceptr 源设备，
- en: size_t ByteCount);
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: size_t 字节数)；
- en: In contrast, the CUDA runtime tends to define functions that take an extra “memcpy
    kind” parameter that depends on the memory types of the host and destination pointers.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，CUDA运行时倾向于定义带有额外“memcpy种类”参数的函数，这个参数取决于主机和目标指针的内存类型。
- en: enum cudaMemcpyKind
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 枚举 cudaMemcpyKind
- en: '{'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaMemcpyHostToHost = 0,
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToHost = 0，
- en: cudaMemcpyHostToDevice = 1,
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice = 1，
- en: cudaMemcpyDeviceToHost = 2,
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToHost = 2，
- en: cudaMemcpyDeviceToDevice = 3,
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToDevice = 3，
- en: cudaMemcpyDefault = 4
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDefault = 4
- en: '};'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: '}；'
- en: For more complex memcpy operations, both APIs use descriptor structures to specify
    the memcpy.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的memcpy操作，两个API都使用描述符结构来指定memcpy。
- en: 5.7.1\. Synchronous versus Asynchronous Memcpy
  id: totrans-892
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.1\. 同步与异步 Memcpy
- en: Because most aspects of memcpy (dimensionality, memory type) are independent
    of whether the memory copy is asynchronous, this section examines the difference
    in detail, and later sections include minimal coverage of asynchronous memcpy.
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 memcpy 的大多数方面（维度、内存类型）与内存拷贝是否为异步无关，本节详细讨论了二者的差异，后续章节将对异步 memcpy 进行简要介绍。
- en: 'By default, any memcpy involving host memory is *synchronous*: The function
    does not return until after the operation has been performed.^([20](ch05.html#ch05fn20))
    Even when operating on pinned memory, such as memory allocated with `cudaMallocHost()`,
    synchronous memcpy routines must wait until the operation is completed because
    the application may rely on that behavior.^([21](ch05.html#ch05fn21))'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，任何涉及主机内存的 memcpy 操作都是*同步的*：该函数在操作完成之前不会返回。^([20](ch05.html#ch05fn20))
    即使是在固定内存上进行操作，如使用 `cudaMallocHost()` 分配的内存，同步 memcpy 例程也必须等待操作完成，因为应用程序可能依赖于这种行为。^([21](ch05.html#ch05fn21))
- en: '[20](ch05.html#ch05fn20a). This is because the hardware cannot directly access
    host memory unless it has been page-locked and mapped for the GPU. An asynchronous
    memory copy for pageable memory could be implemented by spawning another CPU thread,
    but so far, the CUDA team has chosen to avoid that additional complexity.'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: '[20](ch05.html#ch05fn20a). 这是因为硬件无法直接访问主机内存，除非该内存已被页锁定并为 GPU 映射。对于可分页内存的异步内存拷贝，可以通过生成另一个
    CPU 线程来实现，但迄今为止，CUDA 团队选择避免这种额外的复杂性。'
- en: '[21](ch05.html#ch05fn21a). When pinned memory is specified to a synchronous
    memcpy routine, the driver does take advantage by having the hardware use DMA,
    which is generally faster.'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: '[21](ch05.html#ch05fn21a). 当固定内存被指定给同步 memcpy 例程时，驱动程序通过让硬件使用 DMA 来利用这一点，通常这种方式更快。'
- en: When possible, synchronous memcpy should be avoided for performance reasons.
    Even when streams are not being used, keeping all operations asynchronous improves
    performance by enabling the CPU and GPU to run concurrently. If nothing else,
    the CPU can set up more GPU operations such as kernel launches and other memcpys
    while the GPU is running! If CPU/GPU concurrency is the only goal, there is no
    need to create any CUDA streams; calling an asynchronous memcpy with the NULL
    stream will suffice.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 为了性能考虑，应尽量避免使用同步 memcpy。即使不使用流，保持所有操作异步也能提高性能，因为这能让 CPU 和 GPU 并行运行。如果没有其他需求，CPU
    可以在 GPU 执行时设置更多的 GPU 操作，例如内核启动和其他 memcpy！如果 CPU/GPU 并发是唯一目标，则无需创建任何 CUDA 流；调用带有
    NULL 流的异步 memcpy 即可。
- en: While memcpys involving host memory are synchronous by default, any memory copy
    not involving host memory (device↔device or device↔array) is asynchronous. The
    GPU hardware internally enforces serialization on these operations, so there is
    no need for the functions to wait until the GPU has finished before returning.
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然涉及主机内存的memcpy默认是同步的，但任何不涉及主机内存的内存拷贝（设备↔设备或设备↔数组）都是异步的。GPU硬件内部会对这些操作进行序列化，因此这些函数不需要等待GPU完成后再返回。
- en: Asynchronous memcpy functions have the suffix `Async()`. For example, the driver
    API function for asynchronous host→device memcpy is `cuMemcpyHtoDAsync()` and
    the CUDA runtime function is `cudaMemcpyAsync()`.
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: 异步memcpy函数的后缀为`Async()`。例如，驱动程序API中用于异步主机→设备memcpy的函数是`cuMemcpyHtoDAsync()`，而CUDA运行时的函数是`cudaMemcpyAsync()`。
- en: The hardware that implements asynchronous memcpy has evolved over time. The
    very first CUDA-capable GPU (the GeForce 8800 GTX) did not have any copy engines,
    so asynchronous memcpy only enabled CPU/GPU concurrency. Later GPUs added copy
    engines that could perform 1D transfers while the SMs were running, and still
    later, fully capable copy engines were added that could accelerate 2D and 3D transfers,
    even if the copy involved converting between pitch layouts and the block-linear
    layouts used by CUDA arrays. Additionally, early CUDA hardware only had one copy
    engine, whereas today, it sometimes has two. More than two copy engines wouldn’t
    necessarily make sense. Because a single copy engine can saturate the PCI Express
    bus in one direction, only two copy engines are needed to maximize both bus performance
    and concurrency between bus transfers and GPU computation.
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 实现异步memcpy的硬件经历了多次发展。最初的第一代CUDA支持GPU（GeForce 8800 GTX）没有任何拷贝引擎，因此异步memcpy仅支持CPU/GPU的并发。后来，GPU添加了可以在SMs运行时执行1D传输的拷贝引擎，进一步发展后，增加了完全支持的拷贝引擎，这些引擎能够加速2D和3D传输，即使拷贝涉及将数据布局从跨行布局转换为CUDA数组使用的块线性布局。此外，早期的CUDA硬件只有一个拷贝引擎，而今天，硬件有时会有两个拷贝引擎。超过两个拷贝引擎通常没有必要。因为单个拷贝引擎就能在一个方向上饱和PCI
    Express总线，因此只需要两个拷贝引擎即可最大化总线性能以及总线传输与GPU计算之间的并发性。
- en: The number of copy engines can be queried by calling `cuDeviceGetAttribute()`
    with `CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT`, or by examining the `cudaDeviceProp::asyncEngineCount`.
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 拷贝引擎的数量可以通过调用`cuDeviceGetAttribute()`并传入`CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT`，或者通过检查`cudaDeviceProp::asyncEngineCount`来查询。
- en: 5.7.2\. Unified Virtual Addressing
  id: totrans-902
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.2. 统一虚拟寻址
- en: Unified Virtual Addressing enables CUDA to make inferences about memory types
    based on address ranges. Because CUDA tracks which address ranges contain device
    addresses versus host addresses, there is no need to specify `cudaMemcpyKind`
    parameter to the `cudaMemcpy()` function. The driver API added a `cuMemcpy()`
    function that similarly infers the memory types from the addresses.
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 统一虚拟寻址使得CUDA能够根据地址范围推断内存类型。因为CUDA会跟踪哪些地址范围包含设备地址与主机地址，所以不需要在`cudaMemcpy()`函数中指定`cudaMemcpyKind`参数。驱动程序API增加了一个`cuMemcpy()`函数，类似地，它根据地址推断内存类型。
- en: CUresult cuMemcpy(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount);
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuMemcpy(CUdeviceptr dst, CUdeviceptr src, size_t ByteCount);
- en: The CUDA runtime equivalent, not surprisingly, is called `cudaMemcpy():`
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA运行时的等效函数，毫不意外地，称为`cudaMemcpy()`：
- en: cudaError_t cudaMemcpy( void *dst, const void *src, size_t bytes );.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMemcpy( void *dst, const void *src, size_t bytes );.
- en: 5.7.3\. CUDA Runtime
  id: totrans-907
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.3\. CUDA运行时
- en: '[Table 5.10](ch05.html#ch05tab10) summarizes the memcpy functions available
    in the CUDA runtime.'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5.10](ch05.html#ch05tab10)总结了CUDA运行时中可用的memcpy函数。'
- en: '![Image](graphics/05tab10.jpg)![Image](graphics/05tab10a.jpg)'
  id: totrans-909
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab10.jpg)![Image](graphics/05tab10a.jpg)'
- en: '*Table 5.10* Memcpy Functions (CUDA Runtime)'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.10* Memcpy函数（CUDA运行时）'
- en: 1D and 2D memcpy functions take base pointers, pitches, and sizes as required.
    The 3D memcpy routines take a descriptor structure `cudaMemcpy3Dparms`, defined
    as follows.
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 一维和二维的memcpy函数需要传入基指针、步长和大小等参数。三维的memcpy函数需要一个描述符结构`cudaMemcpy3Dparms`，定义如下。
- en: '[Click here to view code image](ch05_images.html#p168pro01a)'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch05_images.html#p168pro01a)'
- en: struct cudaMemcpy3DParms
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaMemcpy3DParms
- en: '{'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: struct cudaArray *srcArray;
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaArray *srcArray;
- en: struct cudaPos srcPos;
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPos srcPos;
- en: struct cudaPitchedPtr srcPtr;
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr srcPtr;
- en: struct cudaArray *dstArray;
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaArray *dstArray;
- en: struct cudaPos dstPos;
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPos dstPos;
- en: struct cudaPitchedPtr dstPtr;
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr dstPtr;
- en: struct cudaExtent extent;
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaExtent extent;
- en: enum cudaMemcpyKind kind;
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: enum cudaMemcpyKind kind;
- en: '};'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '[Table 5.11](ch05.html#ch05tab11) summarizes the meaning of each member of
    the `cudaMemcpy3DParms` structure. The `cudaPos` and `cudaExtent` structures are
    defined as follows.'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5.11](ch05.html#ch05tab11)总结了`cudaMemcpy3DParms`结构的每个成员的含义。`cudaPos`和`cudaExtent`结构定义如下。'
- en: struct cudaExtent {
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaExtent {
- en: size_t width;
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: size_t width;
- en: size_t height;
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: size_t height;
- en: size_t depth;
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: size_t depth;
- en: '};'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: struct cudaPos {
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPos {
- en: size_t x;
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: size_t x;
- en: size_t y;
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: size_t y;
- en: size_t z;
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: size_t z;
- en: '};'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '![Image](graphics/05tab11.jpg)'
  id: totrans-935
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab11.jpg)'
- en: '*Table 5.11* cudaMemcpy3DParms Structure Members'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.11* cudaMemcpy3DParms结构成员'
- en: 5.7.4\. Driver API
  id: totrans-937
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.7.4\. 驱动程序 API
- en: '[Table 5.12](ch05.html#ch05tab12) summarizes the driver API’s memcpy functions.'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 5.12](ch05.html#ch05tab12)总结了驱动程序 API 中的memcpy函数。'
- en: '![Image](graphics/05tab12.jpg)![Image](graphics/05tab12a.jpg)'
  id: totrans-939
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/05tab12.jpg)![Image](graphics/05tab12a.jpg)'
- en: '*Table 5.12* Memcpy Functions (Driver API)'
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 5.12* Memcpy函数（驱动程序API）'
- en: '`cuMemcpy3D()` is designed to implement a strict superset of all previous memcpy
    functionality. Any 1D, 2D, or 3D memcpy may be performed between any host, device,
    or CUDA array memory, and any offset into either the source or destination may
    be applied. The `WidthInBytes`, `Height,` and `Depth` members of the input structure,
    `CUDA_MEMCPY_3D`, define the dimensionality of the memcpy: `Height==0` implies
    a 1D memcpy, and `Depth==0` implies a 2D memcpy. The source and destination memory
    types are given by the `srcMemoryType` and `dstMemoryType` structure elements,
    respectively.'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuMemcpy3D()` 旨在实现所有先前 memcpy 功能的严格超集。可以在任何主机、设备或 CUDA 数组内存之间执行任何 1D、2D 或
    3D 的 memcpy，并且可以在源或目标的任意偏移量上进行操作。输入结构 `CUDA_MEMCPY_3D` 的 `WidthInBytes`、`Height`
    和 `Depth` 成员定义了 memcpy 的维度：`Height==0` 表示 1D memcpy，`Depth==0` 表示 2D memcpy。源和目标内存类型由
    `srcMemoryType` 和 `dstMemoryType` 结构元素分别给出。'
- en: Structure elements that are not needed by `cuMemcpy3D()` are defined to be ignored.
    For example, if a 1D host→device memcpy is requested, the `srcPitch`, `srcHeight`,
    `dstPitch`, and `dstHeight` elements are ignored. If `srcMemoryType` is `CU_MEMORYTYPE_HOST`,
    the `srcDevice` and `srcArray` elements are ignored. This API semantic, coupled
    with the C idiom that assigning {0} to a structure zero-initializes it, enables
    memory copies to be described very concisely. Most other memcpy functions can
    be implemented in a few lines of code, such as the following.
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `cuMemcpy3D()` 不需要的结构元素，会被定义为忽略。例如，如果请求一个 1D 主机→设备的 memcpy，则会忽略 `srcPitch`、`srcHeight`、`dstPitch`
    和 `dstHeight` 元素。如果 `srcMemoryType` 是 `CU_MEMORYTYPE_HOST`，则会忽略 `srcDevice` 和
    `srcArray` 元素。这个 API 语义，结合 C 语言的惯用法（通过给结构赋值 {0} 来进行零初始化），使得内存拷贝的描述非常简洁。大多数其他 memcpy
    函数可以用几行代码实现，例如以下代码。
- en: '[Click here to view code image](ch05_images.html#p171pro01a)'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch05_images.html#p171pro01a)'
- en: CUresult
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult
- en: my_cuMemcpyHtoD( CUdevice dst, const void *src, size_t N )
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: my_cuMemcpyHtoD( CUdevice dst, const void *src, size_t N )
- en: '{'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: CUDA_MEMCPY_3D cp = {0};
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_MEMCPY_3D cp = {0};
- en: cp.srcMemoryType = CU_MEMORYTYPE_HOST;
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: cp.srcMemoryType = CU_MEMORYTYPE_HOST;
- en: cp.srcHost = srcHost;
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: cp.srcHost = srcHost;
- en: cp.dstMemoryType = CU_MEMORYTYPE_DEVICE;
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: cp.dstMemoryType = CU_MEMORYTYPE_DEVICE;
- en: cp.dstDevice = dst;
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: cp.dstDevice = dst;
- en: cp.WidthInBytes = N;
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: cp.WidthInBytes = N;
- en: return cuMemcpy3D( &cp );
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: return cuMemcpy3D( &cp );
- en: '}'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Chapter 6\. Streams and Events
  id: totrans-955
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章：流和事件
- en: 'CUDA is best known for enabling fine-grained concurrency, with hardware facilities
    that enable threads to closely collaborate within blocks using a combination of
    shared memory and thread synchronization. But it also has hardware and software
    facilities that enable more coarse-grained concurrency:'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 最为人知的特点是支持细粒度并发，它通过硬件设施使线程能够在块内利用共享内存和线程同步进行紧密协作。但它也拥有硬件和软件设施，支持更粗粒度的并发：
- en: • **CPU/GPU concurrency:** Since they are separate devices, the CPU and GPU
    can operate independently of each other.
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: • **CPU/GPU 并发：** 由于它们是独立的设备，CPU 和 GPU 可以相互独立地操作。
- en: • **Memcpy/kernel processing concurrency:** For GPUs that have one or more copy
    engines, host↔device memcpy can be performed while the SMs are processing kernels.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: • **Memcpy/内核处理并发：** 对于拥有一个或多个拷贝引擎的 GPU，主机↔设备的 memcpy 可以在 SM 处理内核的同时执行。
- en: • **Kernel concurrency:** SM 2.x-class and later hardware can run up to 4 kernels
    in parallel.
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: • **内核并发：** SM 2.x 类及更高版本的硬件可以同时运行最多 4 个内核。
- en: • **Multi-GPU concurrency:** For problems with enough computational density,
    multiple GPUs can operate in parallel. ([Chapter 9](ch09.html#ch09) is dedicated
    to multi-GPU programming.)
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: • **多 GPU 并发：** 对于计算密度足够高的问题，可以并行操作多个 GPU。（[第 9 章](ch09.html#ch09)专门讨论多 GPU
    编程。）
- en: CUDA streams enable these types of concurrency. Within a given stream, operations
    are performed in sequential order, but operations in different streams may be
    performed in parallel. CUDA events complement CUDA streams by providing the synchronization
    mechanisms needed to coordinate the parallel execution enabled by streams. CUDA
    events may be asynchronously “recorded” into a stream, and the CUDA event becomes
    signaled when the operations preceding the CUDA event have been completed.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 流使这些类型的并发成为可能。在给定的流内，操作按顺序执行，但不同流中的操作可以并行执行。CUDA 事件通过提供必要的同步机制，补充了 CUDA
    流，使得流所支持的并行执行能够得以协调。CUDA 事件可以异步“记录”到一个流中，并在执行了 CUDA 事件之前的操作完成时，CUDA 事件被触发。
- en: CUDA events may be used for CPU/GPU synchronization, for synchronization between
    the engines on the GPU, and for synchronization between GPUs. They also provide
    a GPU-based timing mechanism that cannot be perturbed by system events such as
    page faults or interrupts from disk or network controllers. Wall clock timers
    are best for overall timing, but CUDA events are useful for optimizing kernels
    or figuring out which of a series of pipelined GPU operations is taking the longest.
    All of the performance results reported in this chapter were gathered on a `cg1.4xlarge`
    cloud-based server from Amazon’s EC2 service, as described in [Section 4.5](ch04.html#ch04lev1sec5).
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件可用于 CPU/GPU 同步、GPU 内部引擎之间的同步以及 GPU 之间的同步。它们还提供了一种基于 GPU 的计时机制，不会受到系统事件（如页面错误或磁盘/网络控制器的中断）的干扰。挂钟计时器最适合整体计时，但
    CUDA 事件对于优化内核或确定一系列流水线 GPU 操作中哪个操作最耗时非常有用。本章报告的所有性能结果均在 Amazon EC2 服务提供的 `cg1.4xlarge`
    云服务器上收集，如[第 4.5 节](ch04.html#ch04lev1sec5)所述。
- en: '6.1\. CPU/GPU Concurrency: Covering Driver Overhead'
  id: totrans-963
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. CPU/GPU 并发：覆盖驱动程序开销
- en: CPU/GPU concurrency refers to the CPU’s ability to continue processing after
    having sent some request to the GPU. Arguably, the most important use of CPU/GPU
    concurrency is hiding the overhead of requesting work from the GPU.
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: CPU/GPU 并发指的是 CPU 在向 GPU 发送请求后，仍然能够继续处理的能力。可以说，CPU/GPU 并发最重要的用途是隐藏向 GPU 请求工作时的开销。
- en: 6.1.1\. Kernel Launches
  id: totrans-965
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. 内核启动
- en: Kernel launches have always been asynchronous. A series of kernel launches,
    with no intervening CUDA operations in between, cause the CPU to submit the kernel
    launch to the GPU and return control to the caller before the GPU has finished
    processing.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 内核启动一直是异步的。一系列内核启动，在中间没有插入CUDA操作，会导致CPU将内核启动请求提交给GPU，并在GPU完成处理之前将控制权返回给调用者。
- en: We can measure the driver overhead by bracketing a series of NULL kernel launches
    with timing operations. [Listing 6.1](ch06.html#ch06lis01) shows `nullKernelAsync.cu`,
    a small program that measures the amount of time needed to perform a kernel launch.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过用定时操作包裹一系列NULL内核启动，来测量驱动程序的开销。[列表 6.1](ch06.html#ch06lis01)展示了 `nullKernelAsync.cu`，一个小程序，测量执行内核启动所需的时间。
- en: '*Listing 6.1.* `nullKernelAsync.cu.`'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6.1.* `nullKernelAsync.cu.`'
- en: '[Click here to view code image](ch06_images.html#p06lis01a)'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p06lis01a)'
- en: '* * *'
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '#include <stdio.h>'
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: '#include <stdio.h>'
- en: '#include "chTimer.h"'
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: '#include "chTimer.h"'
- en: __global__
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: __global__
- en: void
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: NullKernel()
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: NullKernel()
- en: '{'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: '}'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: int
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: int
- en: main( int argc, char *argv[] )
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: main( int argc, char *argv[] )
- en: '{'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const int cIterations = 1000000;
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: const int cIterations = 1000000;
- en: printf( "Launches... " ); fflush( stdout );
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "启动次数... " ); fflush( stdout );
- en: chTimerTimestamp start, stop;
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerTimestamp start, stop;
- en: chTimerGetTime( &start );
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: for ( int i = 0; i < cIterations; i++ ) {
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < cIterations; i++ ) {
- en: NullKernel<<<1,1>>>();
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: NullKernel<<<1,1>>>();
- en: '}'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaThreadSynchronize();
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: cudaThreadSynchronize();
- en: chTimerGetTime( &stop );
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &stop );
- en: double microseconds = 1e6*chTimerElapsedTime( &start, &stop );
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: double 微秒 = 1e6*chTimerElapsedTime( &start, &stop );
- en: double usPerLaunch = microseconds / (float) cIterations;
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: double usPerLaunch = 微秒 / (float) cIterations;
- en: printf( "%.2f us\n", usPerLaunch );
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%.2f 微秒\n", usPerLaunch );
- en: return 0;
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: return 0;
- en: '}'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The `chTimerGetTime()` calls, described in [Appendix A](app01.html#app01), use
    the host operating system’s high-resolution timing facilities, such as `QueryPerformanceCounter()`
    or `gettimeofday()`. The `cudaThreadSynchronize()` call in line 23 is needed for
    accurate timing. Without it, the GPU would still be processing the last kernel
    invocations when the end top is recorded with the following function call.
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: '`chTimerGetTime()` 函数调用，如[附录A](app01.html#app01)所述，使用主机操作系统的高分辨率计时功能，如 `QueryPerformanceCounter()`
    或 `gettimeofday()`。第23行的 `cudaThreadSynchronize()` 调用对于准确计时是必要的。如果没有它，GPU可能在记录结束时，仍在处理上一个内核调用。'
- en: chTimerGetTime( &stop );
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &stop );
- en: If you run this program, you will see that invoking a kernel—even a kernel that
    does nothing—costs anywhere from 2.0 to 8.0 microseconds. Most of that time is
    spent in the driver. The CPU/GPU concurrency enabled by kernel launches only helps
    if the kernel runs for longer than it takes the driver to invoke it! To underscore
    the importance of CPU/GPU concurrency for small kernel launches, let’s move the
    `cudaThreadSynchronize()` call into the inner loop.^([1](ch06.html#ch06fn1))
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这个程序，你会看到调用一个内核——即使是一个什么都不做的内核——也需要花费从2.0到8.0微秒不等。大部分时间都花费在驱动程序中。内核启动所启用的CPU/GPU并行性只有在内核运行时间超过驱动程序调用它的时间时才有帮助！为了强调CPU/GPU并行性对小内核启动的重要性，让我们把`cudaThreadSynchronize()`调用移到内层循环中。^([1](ch06.html#ch06fn1))
- en: '[1](ch06.html#ch06fn1a). This program is in the source code as `nullKernelSync.cu`
    and is not reproduced here because it is almost identical to [Listing 6.1](ch06.html#ch06lis01).'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch06.html#ch06fn1a)。这个程序的源代码是`nullKernelSync.cu`，在这里没有再复制，因为它与[列表6.1](ch06.html#ch06lis01)几乎完全相同。'
- en: '[Click here to view code image](ch06_images.html#p175pro01a)'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p175pro01a)'
- en: chTimerGetTime( &start );
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: for ( int i = 0; i < cIterations; i++ ) {
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < cIterations; i++ ) {
- en: NullKernel<<<1,1>>>();
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: NullKernel<<<1,1>>>();
- en: cudaThreadSynchronize();
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: cudaThreadSynchronize();
- en: '}'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: chTimerGetTime( &stop );
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &stop );
- en: The only difference here is that the CPU is waiting until the GPU has finished
    processing each NULL kernel launch before launching the next kernel, as shown
    in [Figure 6.1](ch06.html#ch06fig01). As an example, on an Amazon EC2 instance
    with ECC disabled, `nullKernelNoSync` reports a time of 3.4 ms per launch and
    `nullKernelSync` reports a time of 100 ms per launch. So besides giving up CPU/GPU
    concurrency, the synchronization itself is worth avoiding.
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的区别在于，CPU会等待GPU完成每个NULL内核的处理后，再启动下一个内核，如[图6.1](ch06.html#ch06fig01)所示。例如，在禁用ECC的Amazon
    EC2实例上，`nullKernelNoSync`每次启动的报告时间为3.4毫秒，而`nullKernelSync`每次启动的报告时间为100毫秒。因此，除了放弃CPU/GPU并行性外，同步本身也是值得避免的。
- en: '![Image](graphics/06fig01.jpg)'
  id: totrans-1008
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/06fig01.jpg)'
- en: '*Figure 6.1* CPU/GPU concurrency.'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.1* CPU/GPU并行性。'
- en: Even without synchronizations, if the kernel doesn’t run for longer than the
    amount of time it took to launch the kernel (3.4 ms), the GPU may go idle before
    the CPU has submitted more work. To explore just how much work a kernel might
    need to do to make the launch worthwhile, let’s switch to a kernel that busy-waits
    until a certain number of clock cycles (according to the `clock()` intrinsic)
    has completed.
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有同步，如果内核的运行时间没有超过启动内核所需的时间（3.4毫秒），GPU可能在CPU提交更多工作之前就处于空闲状态。为了探索一个内核可能需要做多少工作才能使启动变得有意义，让我们切换到一个内核，它在完成一定数量的时钟周期（根据`clock()`内建函数）之前进行忙等待。
- en: '[Click here to view code image](ch06_images.html#p176pro01a)'
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p176pro01a)'
- en: __device__ int deviceTime;
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ int deviceTime;
- en: __global__
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: __global__
- en: void
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: WaitKernel( int cycles, bool bWrite )
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: WaitKernel( int cycles, bool bWrite )
- en: '{'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int start = clock();
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: int start = clock();
- en: int stop;
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: int stop;
- en: do {
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: do {
- en: stop = clock();
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: stop = clock();
- en: '} while ( stop - start < cycles );'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: '} 当 stop - start < cycles 时，继续执行；'
- en: if ( bWrite && threadIdx.x==0 && blockIdx.x==0 ) {
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( bWrite && threadIdx.x==0 && blockIdx.x==0 ) {
- en: deviceTime = stop - start;
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: deviceTime = stop - start;
- en: '}'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: By conditionally writing the result to `deviceTime`, this kernel prevents the
    compiler from optimizing out the busy wait. The compiler does not know that we
    are just going to pass `false` as the second parameter.^([2](ch06.html#ch06fn2))
    The code in our `main()` function then checks the launch time for various values
    of cycles, from 0 to 2500.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 通过有条件地将结果写入`deviceTime`，这个内核防止了编译器优化掉忙等待。编译器并不知道我们只是将`false`作为第二个参数传递。^([2](ch06.html#ch06fn2))
    然后我们在`main()`函数中的代码会检查不同周期值（从0到2500）下的启动时间。
- en: '[2](ch06.html#ch06fn2a). The compiler could still invalidate our timing results
    by branching around the loop if `bWrite is false`. If the timing results looked
    suspicious, we could see if this is happening by looking at the microcode with
    `cuobjdump`.'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch06.html#ch06fn2a). 如果`bWrite为false`，编译器仍然可能通过绕过循环使我们的计时结果无效。如果计时结果看起来可疑，我们可以通过使用`cuobjdump`查看微代码来检查这种情况。'
- en: '[Click here to view code image](ch06_images.html#p177pro01a)'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p177pro01a)'
- en: for ( int cycles = 0; cycles < 2500; cycles += 100 ) {
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int cycles = 0; cycles < 2500; cycles += 100 ) {
- en: 'printf( "Cycles: %d - ", cycles ); fflush( stdout );'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 'printf( "周期数: %d - ", cycles ); fflush( stdout );'
- en: chTimerGetTime( &start );
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: for ( int i = 0; i < cIterations; i++ ) {
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < cIterations; i++ ) {
- en: WaitKernel<<<1,1>>>( cycles, false );
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: WaitKernel<<<1,1>>>( cycles, false );
- en: '}'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaThreadSynchronize();
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: cudaThreadSynchronize();
- en: chTimerGetTime( &stop );
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &stop );
- en: double microseconds = 1e6*chTimerElapsedTime( &start, &stop );
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: double microseconds = 1e6*chTimerElapsedTime( &start, &stop );
- en: double usPerLaunch = microseconds / (float) cIterations;
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: double usPerLaunch = microseconds / (float) cIterations;
- en: printf( "%.2f us\n", usPerLaunch );
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%.2f 微秒\n", usPerLaunch );
- en: '}'
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: This program may be found in `waitKernelAsync.cu`. On our EC2 instance, the
    output is as in [Figure 6.2](ch06.html#ch06fig02). On this host platform, the
    breakeven mark where the kernel launch time crosses over 2x that of a NULL kernel
    launch (4.90 μs) is at 4500 GPU clock cycles.
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序可以在`waitKernelAsync.cu`中找到。在我们的EC2实例上，输出如[图 6.2](ch06.html#ch06fig02)所示。在这个主机平台上，当内核启动时间超过NULL内核启动时间的2倍（4.90微秒）时，平衡点出现在4500个GPU时钟周期。
- en: '![Image](graphics/06fig02.jpg)'
  id: totrans-1042
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/06fig02.jpg)'
- en: '*Figure 6.2* Microseconds/cycles plot for `waitKernelAsync.cu`.'
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.2* `waitKernelAsync.cu`的微秒/周期图。'
- en: These performance characteristics can vary widely and depend on many factors,
    including the following.
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 这些性能特征可能会有很大差异，并且取决于许多因素，包括以下几点。
- en: • Performance of the host CPU
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: • 主机CPU的性能
- en: • Host operating system
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: • 主机操作系统
- en: • Driver version
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: • 驱动程序版本
- en: • Driver model (TCC versus WDDM on Windows)
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: • 驱动程序模型（Windows上TCC与WDDM的对比）
- en: • Whether ECC is enabled on the GPU^([3](ch06.html#ch06fn3))
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: • GPU是否启用了ECC^([3](ch06.html#ch06fn3))
- en: '[3](ch06.html#ch06fn3a). When ECC is enabled, the driver must perform a kernel
    thunk to check whether any memory errors have occurred. As a result, `cudaThreadSynchronize()`
    is expensive even on platforms with user-mode client drivers.'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch06.html#ch06fn3a)。当启用ECC时，驱动程序必须执行内核调用以检查是否发生了任何内存错误。因此，即使在用户模式客户端驱动程序的平台上，`cudaThreadSynchronize()`也是昂贵的。'
- en: But the common underlying theme is that for most CUDA applications, developers
    should do their best to avoid breaking CPU/GPU concurrency. Only applications
    that are very compute-intensive and only perform large data transfers can afford
    to ignore this overhead. To take advantage of CPU/GPU concurrency when performing
    memory copies as well as kernel launches, developers must use *asynchronous memcpy*.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，常见的基本主题是，对于大多数CUDA应用程序，开发人员应尽最大努力避免破坏CPU/GPU并发性。只有那些非常计算密集型并且仅进行大数据传输的应用程序，才能忽略这种开销。为了在执行内存复制和内核启动时充分利用CPU/GPU并发性，开发人员必须使用*异步memcpy*。
- en: 6.2\. Asynchronous Memcpy
  id: totrans-1052
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. 异步Memcopy
- en: Like kernel launches, asynchronous memcpy calls return before the GPU has performed
    the memcpy in question. Because the GPU operates autonomously and can read or
    write the host memory without any operating system involvement, only pinned memory
    is eligible for asynchronous memcpy.
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: 与内核启动类似，异步memcpy调用在GPU执行相关memcpy之前返回。由于GPU是自主操作的，可以在没有操作系统参与的情况下读取或写入主机内存，因此只有固定内存才有资格进行异步memcpy。
- en: The earliest application for asynchronous memcpy in CUDA was hidden inside the
    CUDA 1.0 driver. The GPU cannot access pageable memory directly, so the driver
    implements pageable memcpy using a pair of pinned “staging buffers” that are allocated
    with the CUDA context. [Figure 6.3](ch06.html#ch06fig03) shows how this process
    works.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 异步memcpy在CUDA中的最早应用隐藏在CUDA 1.0驱动程序中。GPU无法直接访问可分页内存，因此驱动程序使用一对与CUDA上下文分配的固定“临时缓冲区”实现分页memcpy。[图6.3](ch06.html#ch06fig03)展示了这个过程是如何工作的。
- en: '![Image](graphics/06fig03.jpg)'
  id: totrans-1055
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/06fig03.jpg)'
- en: '*Figure 6.3* Pageable memcpy.'
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.3* 可分页memcpy。'
- en: To perform a host→device memcpy, the driver first “primes the pump” by copying
    to one staging buffer, then kicks off a DMA operation to read that data with the
    GPU. While the GPU begins processing that request, the driver copies more data
    into the other staging buffer. The CPU and GPU keep ping-ponging between staging
    buffers, with appropriate synchronization, until it is time for the GPU to perform
    the final memcpy. Besides copying data, the CPU also naturally pages in any nonresident
    pages while the data is being copied.
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行主机→设备的memcpy，驱动程序首先通过复制到一个临时缓冲区来“启动泵”，然后启动DMA操作以让GPU读取该数据。在GPU开始处理该请求时，驱动程序将更多数据复制到另一个临时缓冲区。CPU和GPU在临时缓冲区之间进行交替操作，并进行适当的同步，直到GPU执行最终的memcpy。此外，在复制数据时，CPU还会自然地将任何非驻留页面分页到内存中。
- en: '6.2.1\. Asynchronous Memcpy: Host→Device'
  id: totrans-1058
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1. 异步Memcopy：主机→设备
- en: As with kernel launches, asynchronous memcpys incur fixed CPU overhead in the
    driver. In the case of host→device memcpy, *all* memcpys below a certain size
    are asynchronous, because the driver copies the source data directly into the
    command buffer that it uses to control the hardware.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 与内核启动一样，异步内存拷贝会在驱动程序中产生固定的 CPU 开销。在主机→设备的内存拷贝情况下，*所有*小于某一大小的内存拷贝都是异步的，因为驱动程序将源数据直接复制到它用来控制硬件的命令缓冲区中。
- en: We can write an application that measures asynchronous memcpy overhead, much
    as we measured kernel launch overhead earlier. The following code, in a program
    called `nullHtoDMemcpyAsync.cu`, reports that on a `cg1.4xlarge` instance in Amazon
    EC2, each memcpy takes 3.3 ms. Since PCI Express can transfer almost 2K in that
    time, it makes sense to examine how the time needed to perform a small memcpy
    grows with the size.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以编写一个应用程序，测量异步内存拷贝的开销，就像我们之前测量内核启动开销一样。以下代码在名为 `nullHtoDMemcpyAsync.cu` 的程序中报告，在
    Amazon EC2 上的 `cg1.4xlarge` 实例中，每次内存拷贝需要 3.3 毫秒。由于 PCI Express 可以在此时间内传输近 2K 数据，因此查看执行小型内存拷贝所需时间如何随大小变化是有意义的。
- en: '[Click here to view code image](ch06_images.html#p180pro01a)'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch06_images.html#p180pro01a)'
- en: CUDART_CHECK( cudaMalloc( &deviceInt, sizeof(int) ) );
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMalloc( &deviceInt, sizeof(int) ) );
- en: CUDART_CHECK( cudaHostAlloc( &hostInt, sizeof(int), 0 ) );
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaHostAlloc( &hostInt, sizeof(int), 0 ) );
- en: chTimerGetTime( &start );
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: for ( int i = 0; i < cIterations; i++ ) {
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < cIterations; i++ ) {
- en: CUDART_CHECK( cudaMemcpyAsync( deviceInt, hostInt, sizeof(int),
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpyAsync( deviceInt, hostInt, sizeof(int),
- en: cudaMemcpyHostToDevice, NULL ) );
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice, NULL ) );
- en: '}'
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: CUDART_CHECK( cudaThreadSynchronize() );
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaThreadSynchronize() );
- en: chTimerGetTime( &stop );
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &stop );
- en: The `breakevenHtoDMemcpy.cu` program measures memcpy performance for sizes from
    4K to 64K. On a `cg1.4xlarge` instance in Amazon EC2, it generates [Figure 6.4](ch06.html#ch06fig04).
    The data generated by this program is clean enough to fit to a linear regression
    curve—in this case, with intercept 3.3 μs and slope 0.000170 μs/byte. The slope
    corresponds to 5.9GB/s, about the expected bandwidth from PCI Express 2.0.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: '`breakevenHtoDMemcpy.cu` 程序测量了从 4K 到 64K 大小的内存拷贝性能。在 Amazon EC2 上的 `cg1.4xlarge`
    实例上，它生成了 [图 6.4](ch06.html#ch06fig04)。该程序生成的数据足够干净，可以拟合线性回归曲线——在这种情况下，截距为 3.3
    μs，斜率为 0.000170 μs/字节。该斜率对应于 5.9GB/s，接近 PCI Express 2.0 预期的带宽。'
- en: '![Image](graphics/06fig04.jpg)'
  id: totrans-1072
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/06fig04.jpg)'
- en: '*Figure 6.4* Small host→device memcpy performance.'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.4* 小型主机→设备内存拷贝性能。'
- en: '6.2.2\. Asynchronous Memcpy: Device→Host'
  id: totrans-1074
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2\. 异步内存拷贝：设备→主机
- en: The `nullDtoHMemcpyNoSync.cu` and `breakevenDtoHMemcpy.cu` programs perform
    the same measurements for small device→host memcpys. On our trusty Amazon EC2
    instance, the minimum time for a memcpy is 4.00 μs ([Figure 6.5](ch06.html#ch06fig05)).
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: '`nullDtoHMemcpyNoSync.cu` 和 `breakevenDtoHMemcpy.cu` 程序对小型设备→主机内存拷贝进行了相同的测量。在我们信赖的
    Amazon EC2 实例上，最小的内存拷贝时间为 4.00 μs ([图 6.5](ch06.html#ch06fig05))。'
- en: '![Image](graphics/06fig05.jpg)'
  id: totrans-1076
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/06fig05.jpg)'
- en: '*Figure 6.5* Small device→host memcpy performance.'
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.5* 小型设备→主机内存拷贝性能。'
- en: 6.2.3\. The NULL Stream and Concurrency Breaks
  id: totrans-1078
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3\. NULL流和并发中断
- en: Any streamed operation may be called with NULL as the stream parameter, and
    the operation will not be initiated until all the preceding operations on the
    GPU have been completed.^([4](ch06.html#ch06fn4)) Applications that have no need
    for copy engines to overlap memcpy operations with kernel processing can use the
    NULL stream to facilitate CPU/GPU concurrency.
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: 任何流操作都可以将NULL作为流参数进行调用，并且该操作直到GPU上所有前置操作完成后才会启动。^([4](ch06.html#ch06fn4)) 不需要将内存拷贝操作与内核处理重叠的应用程序可以使用NULL流来促进CPU/GPU并发执行。
- en: '[4](ch06.html#ch06fn4a). When CUDA streams were added in CUDA 1.1, the designers
    had a choice between making the NULL stream “its own” stream, separate from other
    streams and serialized only with itself, or making it synchronize with (“join”)
    all engines on the GPU. They opted for the latter, in part because CUDA did not
    yet have facilities for interstream synchronization.'
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch06.html#ch06fn4a). 当CUDA流在CUDA 1.1中添加时，设计人员在选择NULL流的行为时面临两个选择：要么让NULL流成为“独立的”流，只与自己进行串行化，要么让它与GPU上的所有引擎同步（“连接”）。他们选择了后者，部分原因是CUDA当时还没有提供流间同步的功能。'
- en: Once a streamed operation has been initiated with the NULL stream, the application
    must use synchronization functions such as `cuCtxSynchronize()` or `cudaThreadSynchronize()`
    to ensure that the operation has been completed before proceeding. But the application
    may request many such operations before performing the synchronization. For example,
    the application may perform an asynchronous host→device memcpy, one or more kernel
    launches, and an asynchronous device→host memcpy before synchronizing with the
    context. The `cuCtxSynchronize()` or `cudaThreadSynchronize()` call returns once
    the GPU has performed the most recently requested operation. This idiom is especially
    useful when performing smaller memcpys or launching kernels that will not run
    for long. The CUDA driver takes valuable CPU time to write commands to the GPU,
    and overlapping that CPU execution with the GPU’s processing of the commands can
    improve performance.
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用NULL流启动了一个流操作，应用程序必须使用同步函数，例如`cuCtxSynchronize()`或`cudaThreadSynchronize()`，以确保操作在继续之前已经完成。但应用程序可以在执行同步操作之前请求多个此类操作。例如，应用程序可以执行异步的主机→设备内存拷贝（memcpy）、一个或多个内核启动以及一个异步的设备→主机内存拷贝，然后再与上下文进行同步。`cuCtxSynchronize()`或`cudaThreadSynchronize()`调用将在GPU完成最近请求的操作后返回。当执行较小的内存拷贝或启动不会运行很长时间的内核时，这种习惯用法特别有用。CUDA驱动程序需要占用宝贵的CPU时间来向GPU写入命令，通过将这些CPU执行与GPU处理命令的过程重叠，可以提高性能。
- en: '*Note:* Even in CUDA 1.0, kernel launches were asynchronous. As a result, the
    NULL stream is implicitly specified to all kernel launches if no stream is given.'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：* 即使在CUDA 1.0中，内核启动也是异步的。因此，如果没有给定流，则NULL流会隐式地指定给所有内核启动。'
- en: Breaking Concurrency
  id: totrans-1083
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 中断并发
- en: Whenever an application performs a full CPU/GPU synchronization (having the
    CPU wait until the GPU is completely idle), performance suffers. We can measure
    this performance impact by switching our NULL-memcpy calls from asynchronous ones
    to synchronous ones just by changing the `cudaMemcpyAsync()` calls to `cudaMemcpy()`
    calls. The `nullDtoHMemcpySync.cu` program does just that for device→host memcpy.
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: 每当应用程序执行完整的 CPU/GPU 同步（让 CPU 等待直到 GPU 完全空闲）时，性能会受到影响。我们可以通过将 NULL-memcpy 调用从异步调用改为同步调用来衡量这种性能影响，方法就是将
    `cudaMemcpyAsync()` 调用改为 `cudaMemcpy()` 调用。`nullDtoHMemcpySync.cu` 程序正是为设备到主机的
    memcpy 执行这一操作。
- en: On our trusty Amazon `cg1.4xlarge` instance, `nullDtoHMemcpySync.cu` reports
    about 7.9 μs per memcpy. If a Windows driver has to perform a kernel thunk, or
    the driver on an ECC-enabled GPU must check for ECC errors, full GPU synchronization
    is much costlier.
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可靠的 Amazon `cg1.4xlarge` 实例上，`nullDtoHMemcpySync.cu` 每次 memcpy 报告大约 7.9 微秒。如果
    Windows 驱动程序需要执行内核转换，或者启用了 ECC 的 GPU 上的驱动程序必须检查 ECC 错误，则完全的 GPU 同步会更加昂贵。
- en: Explicit ways to perform this synchronization include the following.
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此同步的显式方式包括以下几种。
- en: • `cuCtxSynchronize()/cudaDeviceSynchronize()`
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: • 调用 `cuCtxSynchronize()/cudaDeviceSynchronize()`
- en: • `cuStreamSynchronize()/cudaStreamSynchronize()` on the NULL stream
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: • 在 NULL 流上调用 `cuStreamSynchronize()/cudaStreamSynchronize()`
- en: • Unstreamed memcpy between host and device—for example, `cuMemcpyHtoD()`, `cuMemcpyDtoH()`,
    `cudaMemcpy()`
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: • 在主机和设备之间进行非流式 memcpy，例如 `cuMemcpyHtoD()`、`cuMemcpyDtoH()`、`cudaMemcpy()`
- en: Other, more subtle ways to break CPU/GPU concurrency include the following.
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: 其他更微妙的打破 CPU/GPU 并发性的方法包括以下几种。
- en: • Running with the `CUDA_LAUNCH_BLOCKING` environment variable set
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: • 设置 `CUDA_LAUNCH_BLOCKING` 环境变量运行
- en: • Launching kernels that require local memory to be reallocated
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: • 启动需要重新分配本地内存的内核
- en: • Performing large memory allocations or host memory allocations
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: • 执行大规模内存分配或主机内存分配
- en: • Destroying objects such as CUDA streams and CUDA events
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: • 销毁诸如 CUDA 流和 CUDA 事件等对象
- en: Nonblocking Streams
  id: totrans-1095
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 非阻塞流
- en: To create a stream that is exempt from the requirement to synchronize with the
    NULL stream (and therefore less likely to suffer a “concurrency break” as described
    above), specify the `CUDA_STREAM_NON_BLOCKING` flag to `cuStreamCreate()` or the
    `cudaStreamNonBlocking` flag to `cudaStreamCreateWithFlags()`.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 若要创建一个不需要与 NULL 流同步的流（因此更不容易遭遇上述所说的“并发中断”），可以为 `cuStreamCreate()` 指定 `CUDA_STREAM_NON_BLOCKING`
    标志，或者为 `cudaStreamCreateWithFlags()` 指定 `cudaStreamNonBlocking` 标志。
- en: '6.3\. CUDA Events: CPU/GPU Synchronization'
  id: totrans-1097
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. CUDA 事件：CPU/GPU 同步
- en: One of the key features of CUDA events is that they can enable “partial” CPU/GPU
    synchronization. Instead of full CPU/GPU synchronization where the CPU waits until
    the GPU is idle, introducing a bubble into the GPU’s work pipeline, CUDA events
    may be *recorded* into the asynchronous stream of GPU commands. The CPU then can
    wait until all of the work preceding the event has been done. The GPU can continue
    doing whatever work was submitted after the `cuEventRecord()/cudaEventRecord()`.
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件的一个关键特性是它们能够实现“部分”CPU/GPU同步。与完整的CPU/GPU同步（即CPU等待直到GPU空闲，从而在GPU的工作管道中引入一个空泡）不同，CUDA
    事件可以被*记录*到GPU命令的异步流中。然后，CPU可以等待直到事件之前的所有工作完成。GPU可以继续执行在`cuEventRecord()/cudaEventRecord()`之后提交的工作。
- en: As an example of CPU/GPU concurrency, [Listing 6.2](ch06.html#ch06lis02) gives
    a memcpy routine for pageable memory. The code for this program implements the
    algorithm described in [Figure 6.3](ch06.html#ch06fig03) and is located in `pageableMemcpyHtoD.cu`.
    It uses two pinned memory buffers, stored in global variables declared as follows.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: 作为CPU/GPU并发的一个示例，[列表 6.2](ch06.html#ch06lis02)给出了一个用于可分页内存的memcpy例程。该程序的代码实现了[图
    6.3](ch06.html#ch06fig03)中描述的算法，并位于`pageableMemcpyHtoD.cu`中。它使用了两个固定内存缓冲区，这些缓冲区存储在如下声明的全局变量中。
- en: void *g_hostBuffers[2];
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: void *g_hostBuffers[2];
- en: and two CUDA events declared as
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 以及声明的两个CUDA事件如下
- en: cudaEvent_t g_events[2];
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEvent_t g_events[2];
- en: '*Listing 6.2.* `chMemcpyHtoD()`—pageable memcpy.'
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 6.2.* `chMemcpyHtoD()`—可分页的memcpy。'
- en: '[Click here to view code image](ch06_images.html#p06lis02a)'
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p06lis02a)'
- en: '* * *'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: void
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: chMemcpyHtoD( void *device, const void *host, size_t N )
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: chMemcpyHtoD( void *device, const void *host, size_t N )
- en: '{'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: char *dst = (char *) device;
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: char *dst = (char *) device;
- en: const char *src = (const char *) host;
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: const char *src = (const char *) host;
- en: int stagingIndex = 0;
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: int stagingIndex = 0;
- en: while ( N ) {
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: while ( N ) {
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
- en: cudaEventSynchronize( g_events[stagingIndex] );
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventSynchronize( g_events[stagingIndex] );
- en: memcpy( g_hostBuffers[stagingIndex], src, thisCopySize );
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: memcpy( g_hostBuffers[stagingIndex], src, thisCopySize );
- en: cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
- en: thisCopySize, cudaMemcpyHostToDevice, NULL );
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: thisCopySize, cudaMemcpyHostToDevice, NULL );
- en: cudaEventRecord( g_events[1-stagingIndex], NULL );
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( g_events[1-stagingIndex], NULL );
- en: dst += thisCopySize;
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: dst += thisCopySize;
- en: src += thisCopySize;
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: src += thisCopySize;
- en: N -= thisCopySize;
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: N -= thisCopySize;
- en: stagingIndex = 1 - stagingIndex;
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: stagingIndex = 1 - stagingIndex;
- en: '}'
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Error:'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: '错误:'
- en: return;
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: return;
- en: '}'
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '`chMemcpyHtoD()` is designed to maximize CPU/GPU concurrency by “ping-ponging”
    between the two host buffers. The CPU copies into one buffer, while the GPU pulls
    from the other. There is some “overhang” where no CPU/GPU concurrency is possible
    at the beginning and end of the operation when the CPU is copying the first and
    last buffers, respectively.'
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: '`chMemcpyHtoD()`的设计旨在通过在两个主机缓冲区之间进行“乒乓”操作，最大化CPU/GPU并发。CPU将数据复制到一个缓冲区，而GPU从另一个缓冲区中获取数据。在操作的开始和结束时，由于CPU分别复制第一个和最后一个缓冲区，因此存在一定的“悬挂”现象，无法实现CPU/GPU并发。'
- en: In this program, the only synchronization needed—the `cudaEventSynchronize()`
    in line 11—ensures that the GPU has finished with a buffer before starting to
    copy into it. `cudaMemcpyAsync()` returns as soon as the GPU commands have been
    enqueued. It does not wait until the operation is complete. The `cudaEventRecord()`
    is also asynchronous. It causes the event to be signaled when the just-requested
    asynchronous memcpy has been completed.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个程序中，唯一需要同步的操作是第11行的 `cudaEventSynchronize()`，它确保在开始向缓冲区复制之前，GPU 已经完成对该缓冲区的处理。`cudaMemcpyAsync()`
    会在 GPU 命令入队后立即返回，而不会等到操作完成。`cudaEventRecord()` 也是异步的，它会在刚请求的异步 memcpy 完成时触发事件。
- en: The CUDA events are recorded immediately after creation so the first `cudaEventSynchronize()`
    calls in line 11 work correctly.
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件在创建后立即记录，以确保第11行的第一次 `cudaEventSynchronize()` 调用能正确工作。
- en: '[Click here to view code image](ch06_images.html#p184pro01a)'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p184pro01a)'
- en: CUDART_CHECK( cudaEventCreate( &g_events[0] ) );
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventCreate( &g_events[0] ) );
- en: CUDART_CHECK( cudaEventCreate( &g_events[1] ) );
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventCreate( &g_events[1] ) );
- en: // record events so they are signaled on first synchronize
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: // 记录事件，以便在第一次同步时触发
- en: CUDART_CHECK( cudaEventRecord( g_events[0], 0 ) );
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( g_events[0], 0 ) );
- en: CUDART_CHECK( cudaEventRecord( g_events[1], 0 ) );
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( g_events[1], 0 ) );
- en: If you run `pageableMemcpyHtoD.cu`, it will report a bandwidth number much smaller
    than the pageable memcpy bandwidth delivered by the CUDA driver. That’s because
    the C runtime’s `memcpy()` implementation is not optimized to move memory as fast
    as the CPU can. For best performance, the memory must be copied using SSE instructions
    that can move data 16 bytes at a time. Writing a general-purpose memcpy using
    these instructions is complicated by their alignment restrictions, but a simple
    version that requires the source, destination, and byte count to be 16-byte aligned
    is not difficult.^([5](ch06.html#ch06fn5))
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行 `pageableMemcpyHtoD.cu`，它会报告一个比 CUDA 驱动程序提供的 pageable memcpy 带宽要小得多的带宽数字。这是因为
    C 运行时的 `memcpy()` 实现没有经过优化，无法像 CPU 一样快速地移动内存。为了获得最佳性能，内存必须使用 SSE 指令复制，这些指令一次可以移动
    16 字节数据。使用这些指令编写通用的 memcpy 会受到它们对内存对齐的限制，但要求源地址、目的地址和字节数都为 16 字节对齐的简单版本并不难写。^([5](ch06.html#ch06fn5))
- en: '[5](ch06.html#ch06fn5a). On some platforms, `nvcc` does not compile this code
    seamlessly. In the code accompanying this book, `memcpy16()` is in a separate
    file called `memcpy16.cpp`.'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch06.html#ch06fn5a)。在某些平台上，`nvcc` 编译这段代码时可能不太顺利。在本书随附的代码中，`memcpy16()`
    被单独放在一个名为 `memcpy16.cpp` 的文件中。'
- en: '[Click here to view code image](ch06_images.html#p185pro01a)'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p185pro01a)'
- en: '#include <xmmintrin.h>'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: '#include <xmmintrin.h>'
- en: bool
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: bool
- en: memcpy16( void *_dst, const void *_src, size_t N )
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: memcpy16( void *_dst, const void *_src, size_t N )
- en: '{'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: if ( N & 0xf ) {
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 N & 0xf：
- en: return false;
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 false;
- en: '}'
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: float *dst = (float *) _dst;
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: float *dst = (float *) _dst;
- en: const float *src = (const float *) _src;
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: const float *src = (const float *) _src;
- en: while ( N ) {
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: 当 N 不为零时，执行循环：
- en: _mm_store_ps( dst, _mm_load_ps( src ) );
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_store_ps( dst, _mm_load_ps( src ) );
- en: src += 4;
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: src += 4;
- en: dst += 4;
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: dst += 4;
- en: N -= 16;
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: N -= 16;
- en: '}'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return true;
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: return true;
- en: '}'
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: When the C runtime `memcpy()` is replaced by this one, performance on an Amazon
    EC2 `cg1.4xlarge` instance increases from 2155MB/s to 3267MB/s. More complicated
    memcpy routines can deal with relaxed alignment constraints, and slightly higher
    performance is possible by unrolling the inner loop. On `cg1.4xlarge`, the CUDA
    driver’s more optimized SSE memcpy achieves about 100MB/s higher performance than
    `pageableMemcpyHtoD16.cu`.
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: 当 C 运行时的 `memcpy()` 被这个替代时，Amazon EC2 `cg1.4xlarge` 实例上的性能从 2155MB/s 提升到 3267MB/s。更复杂的
    memcpy 例程可以处理放宽的对齐约束，通过展开内部循环可以实现稍微更高的性能。在 `cg1.4xlarge` 上，CUDA 驱动程序的更优化的 SSE
    memcpy 的性能比 `pageableMemcpyHtoD16.cu` 高出约 100MB/s。
- en: How important is the CPU/GPU concurrency for performance of pageable memcpy?
    If we move the event synchronization, we can make the host→device memcpy synchronous,
    as follows.
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: CPU/GPU 并发对 pageable memcpy 性能的重要性如何？如果我们移动事件同步，我们可以使主机→设备的 memcpy 同步，如下所示。
- en: '[Click here to view code image](ch06_images.html#p185pro02a)'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch06_images.html#p185pro02a)'
- en: while ( N ) {
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: while ( N ) {
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
- en: < CUDART_CHECK( cudaEventSynchronize( g_events[stagingIndex] ) );
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: < CUDART_CHECK( cudaEventSynchronize( g_events[stagingIndex] ) );
- en: memcpy( g_hostBuffers[stagingIndex], src, thisCopySize );
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: memcpy( g_hostBuffers[stagingIndex], src, thisCopySize );
- en: CUDART_CHECK( cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
- en: thisCopySize, cudaMemcpyHostToDevice, NULL ) );
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: thisCopySize, cudaMemcpyHostToDevice, NULL ) );
- en: CUDART_CHECK( cudaEventRecord( g_events[1-stagingIndex], NULL ) );
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( g_events[1-stagingIndex], NULL ) );
- en: CUDART_CHECK( cudaEventSynchronize( g_events[1-stagingIndex] ) );
  id: totrans-1168
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventSynchronize( g_events[1-stagingIndex] ) );
- en: dst += thisCopySize;
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: dst += thisCopySize;
- en: src += thisCopySize;
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: src += thisCopySize;
- en: N -= thisCopySize;
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: N -= thisCopySize;
- en: stagingIndex = 1 - stagingIndex;
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: stagingIndex = 1 - stagingIndex;
- en: '}'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: This code is available in `pageableMemcpyHtoD16Synchronous.cu`, and it is about
    70% as fast (2334MB/s instead of 3267MB/s) on the same `cg1.4xlarge` instance.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码在 `pageableMemcpyHtoD16Synchronous.cu` 中可用，在同一 `cg1.4xlarge` 实例上，其速度约为 70%（2334MB/s
    而不是 3267MB/s）。
- en: 6.3.1\. Blocking Events
  id: totrans-1175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1\. 阻塞事件
- en: CUDA events also optionally can be made “blocking,” in which they use an interrupt-based
    mechanism for CPU synchronization. The CUDA driver then implements `cu(da)EventSynchronize()`
    calls using thread synchronization primitives that suspend the CPU thread instead
    of polling the event’s 32-bit tracking value.
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件也可以选择性地设置为“阻塞”，在这种情况下，它们使用基于中断的机制进行 CPU 同步。然后，CUDA 驱动程序使用线程同步原语实现 `cu(da)EventSynchronize()`
    调用，这些原语会挂起 CPU 线程，而不是轮询事件的 32 位跟踪值。
- en: For latency-sensitive applications, blocking events may impose a performance
    penalty. In the case of our pageable memcpy routine, using blocking events causes
    a slight slowdown (about 100MB/s) on our `cg1.4xlarge` instance. But for more
    GPU-intensive applications, or for applications with “mixed workloads” that need
    significant amounts of processing from both CPU and GPU, the benefits of having
    the CPU thread idle outweigh the costs of handling the interrupt that occurs when
    the wait is over. An example of a mixed workload is video transcoding, which features
    divergent code suitable for the CPU and signal and pixel processing suitable for
    the GPU.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于延迟敏感的应用程序，阻塞事件可能会导致性能损失。在我们的可分页 `memcpy` 函数的情况下，使用阻塞事件会导致我们的 `cg1.4xlarge`
    实例稍微变慢（约 100MB/s）。但是，对于更具 GPU 密集型的应用程序，或者对于那些需要大量 CPU 和 GPU 处理的“混合工作负载”应用程序来说，保持
    CPU 线程空闲的好处超过了在等待结束时处理中断的成本。一个混合工作负载的例子是视频转码，它包含适合 CPU 的分支代码，以及适合 GPU 的信号和像素处理。
- en: 6.3.2\. Queries
  id: totrans-1178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2\. 查询
- en: Both CUDA streams and CUDA events may be queried with `cu(da)StreamQuery()`
    and `cu(da)EventQuery()`, respectively. If `cu(da)StreamQuery()` returns success,
    all of the operations pending in a given stream have been completed. If `cu(da)EventQuery()`
    returns success, the event has been recorded.
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 流和 CUDA 事件可以分别通过 `cu(da)StreamQuery()` 和 `cu(da)EventQuery()` 进行查询。如果 `cu(da)StreamQuery()`
    返回成功，则表示给定流中所有待处理的操作已完成。如果 `cu(da)EventQuery()` 返回成功，则表示该事件已被记录。
- en: Although these queries are intended to be lightweight, if ECC is enabled, they
    do perform kernel thunks to check the current error status of the GPU. Additionally,
    on Windows, any pending commands will be submitted to the GPU, which also requires
    a kernel thunk.
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些查询旨在保持轻量级，如果启用了 ECC，它们确实会执行内核调用以检查 GPU 的当前错误状态。此外，在 Windows 上，任何待处理的命令都会提交给
    GPU，这也需要执行内核调用。
- en: '6.4\. CUDA Events: Timing'
  id: totrans-1181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. CUDA 事件：计时
- en: CUDA events work by submitting a command to the GPU that, when the preceding
    commands have been completed, causes the GPU to write a 32-bit memory location
    with a known value. The CUDA driver implements `cuEventQuery()` and `cuEventSynchronize()`
    by examining that 32-bit value. But besides the 32-bit “tracking” value, the GPU
    also can write a 64-bit timer value that is sourced from a high-resolution, GPU-based
    clock.
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件通过向 GPU 提交命令来工作，该命令在前面的命令完成后，会使 GPU 写入一个已知值的 32 位内存位置。CUDA 驱动程序通过检查该
    32 位值来实现 `cuEventQuery()` 和 `cuEventSynchronize()`。但是，除了 32 位的“跟踪”值外，GPU 还可以写入一个来自高分辨率
    GPU 时钟的 64 位计时器值。
- en: Because they use a GPU-based clock, timing using CUDA events is less subject
    to perturbations from system events such as page faults or interrupts, and the
    function to compute elapsed times from timestamps is portable across all operating
    systems. That said, the so-called “wall clock” times of operations are ultimately
    what users see, so CUDA events are best used in a targeted fashion to tune kernels
    or other GPU-intensive operations, not to report absolute times to the user.
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它们使用基于GPU的时钟，使用CUDA事件进行计时时，受到系统事件（如页面错误或中断）的干扰较小，而且从时间戳计算经过时间的函数在所有操作系统上都是可移植的。也就是说，所谓的“墙钟”时间最终是用户看到的，因此CUDA事件最好用于有针对性地调优内核或其他GPU密集型操作，而不是向用户报告绝对时间。
- en: 'The stream parameter to `cuEventRecord()` is for interstream synchronization,
    not for timing. When using CUDA events for timing, it is best to record them in
    the NULL stream. The rationale is similar to the reason the machine instructions
    in superscalar CPUs to read time stamp counters (e.g., `RDTSC` on x86) are serializing
    instructions that flush the pipeline: Forcing a “join” on all the GPU engines
    eliminates any possible ambiguity on the operations being timed.^([6](ch06.html#ch06fn6))
    Just make sure the `cu(da)EventRecord()` calls bracket enough work so that the
    timing delivers meaningful results.'
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuEventRecord()`的stream参数用于流间同步，而不是计时。当使用CUDA事件进行计时时，最好在NULL流中记录它们。其原理类似于超标量CPU中读取时间戳计数器的机器指令（例如x86上的`RDTSC`）是序列化指令，用于刷新流水线：强制所有GPU引擎“合并”可以消除计时操作中的任何歧义。^([6](ch06.html#ch06fn6))
    只需确保`cu(da)EventRecord()`调用括住足够的工作，以便计时结果有意义。'
- en: '[6](ch06.html#ch06fn6a). An additional consideration: On CUDA hardware with
    SM 1.1, timing events could only be recorded by the hardware unit that performed
    kernel computation.'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](ch06.html#ch06fn6a). 另一个考虑因素：在具有SM 1.1的CUDA硬件上，只有执行内核计算的硬件单元才能记录计时事件。'
- en: Finally, note that CUDA events are intended to time GPU operations. Any synchronous
    CUDA operations will result in the GPU being used to time the resulting CPU/GPU
    synchronization operations.
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，注意CUDA事件是用于计时GPU操作的。任何同步的CUDA操作都会导致GPU用于计时由此产生的CPU/GPU同步操作。
- en: '[Click here to view code image](ch06_images.html#p187pro01a)'
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p187pro01a)'
- en: CUDART_CHECK( cudaEventRecord( startEvent, NULL ) );
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( startEvent, NULL ) );
- en: // synchronous memcpy – invalidates CUDA event timing
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: // 同步的memcpy——使CUDA事件计时失效
- en: CUDART_CHECK( cudaMemcpy( deviceIn, hostIn, N*sizeof(int) );
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpy( deviceIn, hostIn, N*sizeof(int) );
- en: CUDART_CHECK( cudaEventRecord( stopEvent, NULL ) );
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( stopEvent, NULL ) );
- en: The example explored in the next section illustrates how to use CUDA events
    for timing.
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节的示例展示了如何使用CUDA事件进行计时。
- en: 6.5\. Concurrent Copying and Kernel Processing
  id: totrans-1193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. 并发复制和内核处理
- en: Since CUDA applications must transfer data across the PCI Express bus in order
    for the GPU to operate on it, another performance opportunity presents itself
    in the form of performing those host↔device memory transfers concurrently with
    kernel processing. According to Amdahl’s Law,^([7](ch06.html#ch06fn7)) the maximum
    speedup achievable by using multiple processors is
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于CUDA应用程序必须通过PCI Express总线传输数据，以便GPU可以对其进行处理，因此在内核处理的同时执行主机↔设备内存传输，提供了另一个性能提升的机会。根据阿姆达尔定律，使用多个处理器可以实现的最大加速比为
- en: '[7](ch06.html#ch06fn7a). [http://bit.ly/13UqBm0](http://bit.ly/13UqBm0)'
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](ch06.html#ch06fn7a). [http://bit.ly/13UqBm0](http://bit.ly/13UqBm0)'
- en: '![Image](graphics/188equ01.jpg)'
  id: totrans-1196
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/188equ01.jpg)'
- en: 'where *r[s]* + *r[p]* = 1 and *N* is the number of processors. In the case
    of concurrent copying and kernel processing, the “number of processors” is the
    number of autonomous hardware units in the GPU: one or two copy engines, plus
    the SMs that execute the kernels. For *N* = 2, [Figure 6.6](ch06.html#ch06fig06)
    shows the idealized speedup curve as *r[s]* and *r[p]* vary.'
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，*r[s]* + *r[p]* = 1，*N*是处理器的数量。在并发拷贝和内核处理的情况下，“处理器的数量”指的是GPU中的自主硬件单元的数量：一个或两个拷贝引擎，加上执行内核的SM（流式多处理器）。对于*N*
    = 2，[图6.6](ch06.html#ch06fig06)显示了随着*r[s]*和*r[p]*变化的理想化加速曲线。
- en: '![Image](graphics/06fig06.jpg)'
  id: totrans-1198
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/06fig06.jpg)'
- en: '*Figure 6.6* Idealized Amdahl’s Law curve.'
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.6* 理想化的阿姆达尔定律曲线。'
- en: So in theory, a 2x performance improvement is possible on a GPU with one copy
    engine, but only if the program gets perfect overlap between the SMs and the copy
    engine, and only if the program spends equal time transferring and processing
    the data.
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，理论上，在具有一个拷贝引擎的GPU上，性能提升2倍是可能的，但前提是程序能够完美地重叠SM和拷贝引擎的工作，而且程序在数据传输和处理上的时间必须相等。
- en: Before undertaking this endeavor, you should take a close look at whether it
    will benefit your application. Applications that are extremely transfer-bound
    (i.e., they spend most of their time transferring data to and from the GPU) or
    extremely compute-bound (i.e., they spend most of their time processing data on
    the GPU) will derive little benefit from overlapping transfer and compute.
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始这个任务之前，你应该仔细考虑它是否对你的应用程序有益。对于那些极度受传输限制的应用（即大部分时间都在将数据传输到GPU和从GPU传输数据）或极度受计算限制的应用（即大部分时间都在GPU上处理数据），重叠传输和计算将不会带来太大的好处。
- en: 6.5.1\. `concurrencyMemcpyKernel.cu`
  id: totrans-1202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.1. `concurrencyMemcpyKernel.cu`
- en: The program `concurrencyMemcpyKernel.cu` is designed to illustrate not only
    how to implement concurrent memcpy and kernel execution but also how to determine
    whether it is worth doing at all. [Listing 6.3](ch06.html#ch06lis03) gives a `AddKernel()`,
    a “makework” kernel that has a parameter `cycles` to control how long it runs.
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: 程序`concurrencyMemcpyKernel.cu`旨在说明如何实现并发的内存拷贝和内核执行，并且如何判断是否值得这样做。[列表6.3](ch06.html#ch06lis03)提供了一个`AddKernel()`，这是一个“假工作”内核，具有一个参数`cycles`，用来控制它运行的时间。
- en: '*Listing 6.3.* `AddKernel()`, a makework kernel with parameterized computational
    density.'
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 6.3.* `AddKernel()`，一个具有参数化计算密度的生成任务内核。'
- en: '[Click here to view code image](ch06_images.html#p06lis03a)'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p06lis03a)'
- en: '* * *'
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: AddKernel( int *out, const int *in, size_t N, int addValue, int
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: AddKernel( int *out, const int *in, size_t N, int addValue, int
- en: cycles )
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: cycles )
- en: '{'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N;
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x )
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x )
- en: '{'
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: volatile int value = in[i];
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: volatile int value = in[i];
- en: for ( int j = 0; j < cycles; j++ ) {
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < cycles; j++ ) {
- en: value += addValue;
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: value += addValue;
- en: '}'
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: out[i] = value;
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] = value;
- en: '}'
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '`AddKernel()` streams an array of integers from `in` to `out`, looping over
    each input value `cycles` times. By varying the value of `cycles`, we can make
    the kernel range from a trivial streaming kernel that pushes the memory bandwidth
    limits of the machine to a totally compute-bound kernel.'
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: '`AddKernel()` 将一组整数从`in`流式传输到`out`，并对每个输入值循环`cycles`次。通过改变`cycles`的值，我们可以使得该内核从一个推动机器内存带宽极限的简单流式内核到一个完全计算密集型的内核。'
- en: These two routines in the program measure the performance of `AddKernel().`
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: 程序中的这两个例程衡量了`AddKernel()`的性能。
- en: • `TimeSequentialMemcpyKernel()` copies the input data to the GPU, invokes `AddKernel()`,
    and copies the output back from the GPU in separate, sequential steps.
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: • `TimeSequentialMemcpyKernel()` 将输入数据复制到GPU，调用`AddKernel()`，并在单独的顺序步骤中将输出从GPU复制回来。
- en: • `TimeConcurrentOperations()` allocates a number of CUDA streams and performs
    the host→device memcpys, kernel processing, and device→host memcpys in parallel.
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: • `TimeConcurrentOperations()` 分配多个CUDA流，并并行执行主机→设备的memcpy、内核处理和设备→主机的memcpy。
- en: '`TimeSequentialMemcpyKernel()`, given in [Listing 6.4](ch06.html#ch06lis04),
    uses four CUDA events to separately time the host→device memcpy, kernel processing,
    and device→host memcpy. It also reports back the total time, as measured by the
    CUDA events.'
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: '`TimeSequentialMemcpyKernel()`，在[Listing 6.4](ch06.html#ch06lis04)中给出，使用四个CUDA事件分别计时主机→设备的memcpy、内核处理和设备→主机的memcpy。它还报告了由CUDA事件测量的总时间。'
- en: '*Listing 6.4.* `TimeSequentialMemcpyKernel()` function.'
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 6.4.* `TimeSequentialMemcpyKernel()` 函数。'
- en: '[Click here to view code image](ch06_images.html#p06lis04a)'
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p06lis04a)'
- en: '* * *'
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: bool
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: bool
- en: TimeSequentialMemcpyKernel(
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: TimeSequentialMemcpyKernel(
- en: float *timesHtoD,
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
  zh: float *timesHtoD,
- en: float *timesKernel,
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: float *timesKernel,
- en: float *timesDtoH,
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: float *timesDtoH,
- en: float *timesTotal,
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
  zh: float *timesTotal,
- en: size_t N,
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: const chShmooRange& cyclesRange,
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: const chShmooRange& cyclesRange,
- en: int numBlocks )
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: int numBlocks )
- en: '{'
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: bool ret = false;
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: bool ret = false;
- en: int *hostIn = 0;
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: int *hostIn = 0;
- en: int *hostOut = 0;
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: int *hostOut = 0;
- en: int *deviceIn = 0;
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
  zh: int *deviceIn = 0;
- en: int *deviceOut = 0;
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: int *deviceOut = 0;
- en: const int numEvents = 4;
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: const int numEvents = 4;
- en: cudaEvent_t events[numEvents];
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEvent_t events[numEvents];
- en: for ( int i = 0; i < numEvents; i++ ) {
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < numEvents; i++ ) {
- en: events[i] = NULL;
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
  zh: events[i] = NULL;
- en: CUDART_CHECK( cudaEventCreate( &events[i] ) );
  id: totrans-1251
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventCreate( &events[i] ) );
- en: '}'
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaMallocHost( &hostIn, N*sizeof(int) );
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMallocHost( &hostIn, N*sizeof(int) );
- en: cudaMallocHost( &hostOut, N*sizeof(int) );
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMallocHost( &hostOut, N*sizeof(int) );
- en: cudaMalloc( &deviceIn, N*sizeof(int) );
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMalloc( &deviceIn, N*sizeof(int) );
- en: cudaMalloc( &deviceOut, N*sizeof(int) );
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMalloc( &deviceOut, N*sizeof(int) );
- en: for ( size_t i = 0; i < N; i++ ) {
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = 0; i < N; i++ ) {
- en: hostIn[i] = rand();
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: hostIn[i] = rand();
- en: '}'
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaDeviceSynchronize();
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceSynchronize();
- en: for ( chShmooIterator cycles(cyclesRange); cycles; cycles++ ) {
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: for ( chShmooIterator cycles(cyclesRange); cycles; cycles++ ) {
- en: printf( "." ); fflush( stdout );
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "." ); fflush( stdout );
- en: cudaEventRecord( events[0], NULL );
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( events[0], NULL );
- en: cudaMemcpyAsync( deviceIn, hostIn, N*sizeof(int),
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync( deviceIn, hostIn, N*sizeof(int),
- en: cudaMemcpyHostToDevice, NULL );
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice, NULL );
- en: cudaEventRecord( events[1], NULL );
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( events[1], NULL );
- en: AddKernel<<<numBlocks, 256>>>(
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: AddKernel<<<numBlocks, 256>>>(
- en: deviceOut, deviceIn, N, 0xcc, *cycles );
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: deviceOut, deviceIn, N, 0xcc, *cycles );
- en: cudaEventRecord( events[2], NULL );
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( events[2], NULL );
- en: cudaMemcpyAsync( hostOut, deviceOut, N*sizeof(int),
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync( hostOut, deviceOut, N*sizeof(int),
- en: cudaMemcpyDeviceToHost, NULL );
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToHost, NULL );
- en: cudaEventRecord( events[3], NULL );
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( events[3], NULL );
- en: cudaDeviceSynchronize();
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceSynchronize();
- en: cudaEventElapsedTime( timesHtoD, events[0], events[1] );
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventElapsedTime( timesHtoD, events[0], events[1] );
- en: cudaEventElapsedTime( timesKernel, events[1], events[2] );
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventElapsedTime( timesKernel, events[1], events[2] );
- en: cudaEventElapsedTime( timesDtoH, events[2], events[3] );
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventElapsedTime( timesDtoH, events[2], events[3] );
- en: cudaEventElapsedTime( timesTotal, events[0], events[3] );
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventElapsedTime( timesTotal, events[0], events[3] );
- en: timesHtoD += 1;
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: timesHtoD += 1;
- en: timesKernel += 1;
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: timesKernel += 1;
- en: timesDtoH += 1;
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: timesDtoH += 1;
- en: timesTotal += 1;
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: timesTotal += 1;
- en: '}'
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ret = true;
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: ret = true;
- en: 'Error:'
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: for ( int i = 0; i < numEvents; i++ ) {
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < numEvents; i++ ) {
- en: cudaEventDestroy( events[i] );
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventDestroy( events[i] );
- en: '}'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaFree( deviceIn );
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFree( deviceIn );
- en: cudaFree( deviceOut );
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFree( deviceOut );
- en: cudaFreeHost( hostOut );
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFreeHost( hostOut );
- en: cudaFreeHost( hostIn );
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFreeHost( hostIn );
- en: return ret;
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: return ret;
- en: '}'
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The `cyclesRange` parameter, which uses the “shmoo” functionality described
    in [Section A.4](app01.html#app01lev1sec4), specifies the range of cycles values
    to use when invoking `AddKernel()`. On a `cg1.4xlarge` instance in EC2, the times
    (in ms) for `cycles` values from 4..64 are as follows.
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
  zh: '`cyclesRange`参数，使用[第A.4节](app01.html#app01lev1sec4)中描述的“shmoo”功能，指定调用`AddKernel()`时使用的循环值范围。在EC2上的`cg1.4xlarge`实例中，`cycles`值从4到64的时间（单位：毫秒）如下所示。'
- en: '![Image](graphics/191tab01.jpg)'
  id: totrans-1296
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/191tab01.jpg)'
- en: For values of `*cycles` around 48 (highlighted), where the kernel takes about
    the same amount of time as the memcpy operations, we presume there would be a
    benefit in performing the operations concurrently.
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`*cycles`约为48的值（高亮显示），在此情况下，内核所需时间与memcpy操作大致相同，我们假设同时执行这些操作会带来好处。
- en: The routine `TimeConcurrentMemcpyKernel()` divides the computation performed
    by `AddKernel()` evenly into segments of size `streamIncrement` and uses a separate
    CUDA stream to compute each. The code fragment in [Listing 6.5](ch06.html#ch06lis05),
    from `TimeConcurrentMemcpyKernel()`, highlights the complexity of programming
    with streams.
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: 例程`TimeConcurrentMemcpyKernel()`将`AddKernel()`执行的计算均匀地划分为大小为`streamIncrement`的段，并使用单独的CUDA流来计算每个段。来自`TimeConcurrentMemcpyKernel()`的[清单
    6.5](ch06.html#ch06lis05)中的代码片段，突显了使用流编程的复杂性。
- en: '*Listing 6.5.* `TimeConcurrentMemcpyKernel()` fragment.'
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 6.5.* `TimeConcurrentMemcpyKernel()` 代码片段。'
- en: '[Click here to view code image](ch06_images.html#p06lis05a)'
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch06_images.html#p06lis05a)'
- en: '* * *'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: intsLeft = N;
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
  zh: intsLeft = N;
- en: for ( int stream = 0; stream < numStreams; stream++ ) {
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int stream = 0; stream < numStreams; stream++ ) {
- en: size_t intsToDo = (intsLeft < intsPerStream) ?
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: size_t intsToDo = (intsLeft < intsPerStream) ?
- en: 'intsLeft : intsPerStream;'
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
  zh: 'intsLeft : intsPerStream;'
- en: CUDART_CHECK( cudaMemcpyAsync(
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpyAsync(
- en: deviceIn+stream*intsPerStream,
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: deviceIn+stream*intsPerStream,
- en: hostIn+stream*intsPerStream,
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: hostIn+stream*intsPerStream,
- en: intsToDo*sizeof(int),
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: intsToDo*sizeof(int),
- en: cudaMemcpyHostToDevice, streams[stream] ) );
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice, streams[stream] ) );
- en: intsLeft -= intsToDo;
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: intsLeft -= intsToDo;
- en: '}'
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: intsLeft = N;
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
  zh: intsLeft = N;
- en: for ( int stream = 0; stream < numStreams; stream++ ) {
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int stream = 0; stream < numStreams; stream++ ) {
- en: size_t intsToDo = (intsLeft < intsPerStream) ?
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: size_t intsToDo = (intsLeft < intsPerStream) ?
- en: 'intsLeft : intsPerStream;'
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
  zh: 'intsLeft : intsPerStream;'
- en: AddKernel<<<numBlocks, 256, 0, streams[stream]>>>(
  id: totrans-1317
  prefs: []
  type: TYPE_NORMAL
  zh: AddKernel<<<numBlocks, 256, 0, streams[stream]>>>(
- en: deviceOut+stream*intsPerStream,
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: deviceOut+stream*intsPerStream,
- en: deviceIn+stream*intsPerStream,
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: deviceIn+stream*intsPerStream,
- en: intsToDo, 0xcc, *cycles );
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
  zh: intsToDo, 0xcc, *cycles );
- en: intsLeft -= intsToDo;
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: intsLeft -= intsToDo;
- en: '}'
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: intsLeft = N;
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: intsLeft = N;
- en: for ( int stream = 0; stream < numStreams; stream++ ) {
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int stream = 0; stream < numStreams; stream++ ) {
- en: size_t intsToDo = (intsLeft < intsPerStream) ?
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: size_t intsToDo = (intsLeft < intsPerStream) ?
- en: 'intsLeft : intsPerStream;'
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
  zh: 'intsLeft : intsPerStream;'
- en: CUDART_CHECK( cudaMemcpyAsync(
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpyAsync(
- en: hostOut+stream*intsPerStream,
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: hostOut+stream*intsPerStream,
- en: deviceOut+stream*intsPerStream,
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
  zh: deviceOut+stream*intsPerStream,
- en: intsToDo*sizeof(int),
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
  zh: intsToDo*sizeof(int),
- en: cudaMemcpyDeviceToHost, streams[stream] ) );
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToHost, streams[stream] ) );
- en: intsLeft -= intsToDo;
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
  zh: intsLeft -= intsToDo;
- en: '}'
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Besides requiring the application to create and destroy CUDA streams, the streams
    must be looped over separately for each of the host→device memcpy, kernel processing,
    and device→host memcpy operations. Without this “software-pipelining,” there would
    be no concurrent execution of the different streams’ work, as each streamed operation
    is preceded by an “interlock” operation that prevents the operation from proceeding
    until the previous operation in that stream has completed. The result would be
    not only a failure to get parallel execution between the engines but also an additional
    performance degradation due to the slight overhead of managing stream concurrency.
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: 除了需要应用程序创建和销毁 CUDA 流之外，这些流还必须分别循环用于每个主机→设备的 memcpy、内核处理和设备→主机的 memcpy 操作。如果没有这种“软件流水线”，不同流的工作就无法并发执行，因为每个流的操作之前都有一个“互锁”操作，防止该操作在前一个操作完成之前继续执行。结果不仅无法实现引擎之间的并行执行，还会因管理流并发的轻微开销而导致额外的性能下降。
- en: The computation cannot be made fully concurrent, since no kernel processing
    can be overlapped with the first or last memcpys, and there is some overhead in
    synchronizing between CUDA streams and, as we saw in the previous section, in
    invoking the memcpy and kernel operations themselves. As a result, the optimal
    number of streams depends on the application and should be determined empirically.
    The `concurrencyMemcpyKernel.cu` program enables the number of streams to be specified
    on the command line using the `-- numStreams` parameter.
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: 计算无法完全并发执行，因为没有内核处理可以与第一个或最后一个 memcpy 重叠，并且在 CUDA 流之间进行同步时会有一些开销，正如我们在上一节中看到的那样，在调用
    memcpy 和内核操作时也会有开销。因此，最佳的流数量取决于应用程序，并且应该通过经验来确定。`concurrencyMemcpyKernel.cu` 程序允许通过命令行使用
    `-- numStreams` 参数来指定流的数量。
- en: 6.5.2\. Performance Results
  id: totrans-1337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.2\. 性能结果
- en: The `concurrencyMemcpyKernel.cu` program generates a report on performance characteristics
    over a variety of `cycles` values, with a fixed buffer size and number of streams.
    On a `cg1.4xlarge` instance in Amazon EC2, with a buffer size of 128M integers
    and 8 streams, the report is as follows for cycles values from 4..64.
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
  zh: '`concurrencyMemcpyKernel.cu` 程序生成一个关于在多种 `cycles` 值下性能特征的报告，固定缓冲区大小和流的数量。在
    Amazon EC2 上的 `cg1.4xlarge` 实例中，缓冲区大小为 128M 整数，流的数量为 8，以下是从 4 到 64 的 `cycles`
    值下的报告。'
- en: '![Image](graphics/194tab01.jpg)'
  id: totrans-1339
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/194tab01.jpg)'
- en: The full graph for `cycles` values from 4..256 is given in [Figure 6.7](ch06.html#ch06fig07).
    Unfortunately, for these settings, the 50% speedup shown here falls well short
    of the 3x speedup that theoretically could be obtained.
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: '`cycles` 值从 4 到 256 的完整图表见 [图 6.7](ch06.html#ch06fig07)。不幸的是，对于这些设置，这里显示的 50%
    加速远低于理论上可以获得的 3x 加速。'
- en: '![Image](graphics/06fig07.jpg)'
  id: totrans-1341
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/06fig07.jpg)'
- en: '*Figure 6.7* Speedup due memcpy/kernel concurrency (Tesla M2050).'
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.7* 由于 memcpy/内核并发的加速（Tesla M2050）。'
- en: The benefit on a GeForce GTX 280, which contains only one copy engine, is more
    pronounced. Here, the results from varying `cycles` up to 512 are shown. The maximum
    speedup, shown in [Figure 6.8](ch06.html#ch06fig08), is much closer to the theoretical
    maximum of 2x.
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 对于只包含一个复制引擎的GeForce GTX 280，性能提升更为明显。这里显示了`cycles`从1到512变化时的结果。最大加速比，如[图6.8](ch06.html#ch06fig08)所示，接近理论最大值2倍。
- en: '![Image](graphics/06fig08.jpg)'
  id: totrans-1344
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/06fig08.jpg)'
- en: '*Figure 6.8* Speedup due to memcpy/kernel concurrency (GeForce GTX 280).'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.8* 由于memcpy/内核并发带来的加速（GeForce GTX 280）。'
- en: As written, `concurrencyMemcpyKernel.cu` serves little more than an illustrative
    purpose, because `AddValues()` is just make-work. But you can plug your own kernel(s)
    into this application to help determine whether the additional complexity of using
    streams is justified by the performance improvement. Note that unless concurrent
    kernel execution is desired (see [Section 6.7](ch06.html#ch06lev1sec7)), the kernel
    invocation in [Listing 6.5](ch06.html#ch06lis05) could be replaced by successive
    kernel invocations in the same stream, and the application will still get the
    desired concurrency.
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
  zh: 如文中所述，`concurrencyMemcpyKernel.cu`仅用于说明目的，因为`AddValues()`只是做些无意义的工作。但你可以将自己的内核程序插入到此应用程序中，以帮助判断使用流的额外复杂性是否能够通过性能提升来证明其合理性。请注意，除非需要并发内核执行（参见[第6.7节](ch06.html#ch06lev1sec7)），否则[清单6.5](ch06.html#ch06lis05)中的内核调用可以通过在同一流中的连续内核调用来替代，应用程序仍然可以获得所需的并发性。
- en: 'As a side note, the number of copy engines can be queried by calling`cudaGet``DeviceProperties()`
    and examining `cudaDeviceProp:: asyncEngineCount`, or calling `cuDeviceQueryAttribute()`
    with `CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT`.'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: '顺便提一下，可以通过调用`cudaGet``DeviceProperties()`并检查`cudaDeviceProp:: asyncEngineCount`，或调用`cuDeviceQueryAttribute()`并传入`CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT`来查询复制引擎的数量。'
- en: The copy engines accompanying SM 1.1 and some SM 1.2 hardware could copy linear
    memory only, but more recent copy engines offer full support for 2D memcpy, including
    2D and 3D CUDA arrays.
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: 附带SM 1.1和部分SM 1.2硬件的复制引擎只能复制线性内存，但较新的复制引擎全面支持2D memcpy，包括2D和3D CUDA数组。
- en: 6.5.3\. Breaking Interengine Concurrency
  id: totrans-1349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.3\. 破坏引擎间并发性
- en: Using CUDA streams for concurrent memcpy and kernel execution introduces many
    more opportunities to “break concurrency.” In the previous section, CPU/GPU concurrency
    could be broken by unintentionally doing something that caused CUDA to perform
    a full CPU/GPU synchronization. Here, CPU/GPU concurrency can be broken by unintentionally
    performing an unstreamed CUDA operation. Recall that the NULL stream performs
    a “join” on all GPU engines, so even an asynchronous memcpy operation will stall
    interengine concurrency if the NULL stream is specified.
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CUDA流进行并发的memcpy和内核执行会引入更多“破坏并发性”的机会。在前一节中，通过无意中做某些操作导致CUDA执行完整的CPU/GPU同步，可能会破坏CPU/GPU并发性。而在这里，通过无意中执行未加流的CUDA操作，CPU/GPU并发性也可能被破坏。请记住，NULL流会对所有GPU引擎执行“合并”操作，因此即使是异步的memcpy操作，如果指定了NULL流，也会阻碍引擎间的并发性。
- en: Besides specifying the NULL stream explicitly, the main avenue for these unintentional
    “concurrency breaks” is calling functions that run in the NULL stream implicitly
    because they do not take a stream parameter. When streams were first introduced
    in CUDA 1.1, functions such as `cudaMemset()` and `cuMemcpyDtoD()`, and the interfaces
    for libraries such as CUFFT and CUBLAS, did not have any way for applications
    to specify stream parameters. The Thrust library still does not include support.
    The CUDA Visual Profiler will call out concurrency breaks in its reporting.
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: 除了显式指定NULL流外，导致这些无意“并发中断”的主要原因是调用那些隐式运行在NULL流中的函数，因为它们不需要流参数。当CUDA 1.1首次引入流时，像`cudaMemset()`和`cuMemcpyDtoD()`这样的函数，以及CUFFT和CUBLAS等库的接口，都没有提供让应用程序指定流参数的方式。Thrust库至今仍不支持这一功能。CUDA
    Visual Profiler会在报告中指出并发中断的情况。
- en: 6.6\. Mapped Pinned Memory
  id: totrans-1352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6\. 映射固定内存
- en: Mapped pinned memory can be used to overlap PCI Express transfers and kernel
    processing, especially for device→host copies, where there is no need to cover
    the long latency to host memory. Mapped pinned memory has stricter alignment requirements
    than the native GPU memcpy, since they must be coalesced. Uncoalesced memory transactions
    run two to six times slower when using mapped pinned memory.
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
  zh: 映射固定内存可以用于重叠PCI Express传输和内核处理，特别是在设备→主机拷贝时，在这种情况下无需考虑主机内存的长延迟。映射固定内存比原生GPU的memcpy有更严格的对齐要求，因为它们必须合并。使用映射固定内存时，未合并的内存事务速度会慢2到6倍。
- en: 'A naïve port of our `concurrencyMemcpyKernelMapped.cu` program yields an interesting
    result: On a `cg1.4xlarge` instance in Amazon EC2, mapped pinned memory runs very
    slowly for values of `cycles` below 64.'
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`concurrencyMemcpyKernelMapped.cu`程序的一个简单移植产生了一个有趣的结果：在Amazon EC2的`cg1.4xlarge`实例上，当`cycles`值低于64时，映射固定内存的运行速度非常慢。
- en: '![Image](graphics/197tab01.jpg)'
  id: totrans-1355
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/197tab01.jpg)'
- en: 'For small values of `cycles`, the kernel takes a long time to run, as if `cycles`
    were greater than 200! Only NVIDIA can discover the reason for this performance
    anomaly for certain, but it is not difficult to work around: By unrolling the
    inner loop of the kernel, we create more work per thread, and performance improves.'
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的`cycles`值，内核运行的时间很长，仿佛`cycles`值大于200！只有NVIDIA能确定导致这一性能异常的具体原因，但解决起来并不困难：通过展开内核的内部循环，我们为每个线程创造了更多的工作量，从而提升了性能。
- en: '*Listing 6.6.* `AddKernel()` with loop unrolling.'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 6.6.* `AddKernel()` 与循环展开。'
- en: '[Click here to view code image](ch06_images.html#p06lis06a)'
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch06_images.html#p06lis06a)'
- en: '* * *'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<const int unrollFactor>
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: template<const int unrollFactor>
- en: __device__ void
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ void
- en: AddKernel_helper( int *out, const int *in, size_t N, int increment, int cycles
    )
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
  zh: AddKernel_helper( int *out, const int *in, size_t N, int increment, int cycles
    )
- en: '{'
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = unrollFactor*blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = unrollFactor*blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N;
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += unrollFactor*blockDim.x*gridDim.x )
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
  zh: i += unrollFactor*blockDim.x*gridDim.x )
- en: '{'
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int values[unrollFactor];
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
  zh: int values[unrollFactor];
- en: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
- en: size_t index = i+iUnroll*blockDim.x;
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+iUnroll*blockDim.x;
- en: values[iUnroll] = in[index];
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: values[iUnroll] = in[index];
- en: '}'
  id: totrans-1372
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
- en: for ( int k = 0; k < cycles; k++ ) {
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int k = 0; k < cycles; k++ ) {
- en: values[iUnroll] += increment;
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
  zh: values[iUnroll] += increment;
- en: '}'
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int iUnroll = 0; iUnroll < unrollFactor; iUnroll++ ) {
- en: size_t index = i+iUnroll*blockDim.x;
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+iUnroll*blockDim.x;
- en: out[index] = values[iUnroll];
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
  zh: out[index] = values[iUnroll];
- en: '}'
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __device__ void
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ void
- en: AddKernel( int *out, const int *in, size_t N, int increment, int cycles, int
  id: totrans-1385
  prefs: []
  type: TYPE_NORMAL
  zh: AddKernel( int *out, const int *in, size_t N, int increment, int cycles, int
- en: unrollFactor )
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: unrollFactor )
- en: '{'
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: switch ( unrollFactor ) {
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: switch ( unrollFactor ) {
- en: 'case 1: return AddKernel_helper<1>( out, in, N, increment, cycles );'
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 1: return AddKernel_helper<1>( out, in, N, increment, cycles );'
- en: 'case 2: return AddKernel_helper<2>( out, in, N, increment, cycles );'
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 2: return AddKernel_helper<2>( out, in, N, increment, cycles );'
- en: 'case 4: return AddKernel_helper<4>( out, in, N, increment, cycles );'
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 4: return AddKernel_helper<4>( out, in, N, increment, cycles );'
- en: '}'
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note that this version of `AddKernel()` in [Listing 6.6](ch06.html#ch06lis06)
    is functionally identical to the one in [Listing 6.3](ch06.html#ch06lis03).^([8](ch06.html#ch06fn8))
    It just computes `unrollFactor` outputs per loop iteration. Since the unroll factor
    is a template parameter, the compiler can use registers to hold the `values` array,
    and the innermost for loops can be unrolled completely.
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`AddKernel()` 的这个版本在 [Listing 6.6](ch06.html#ch06lis06) 中与 [Listing 6.3](ch06.html#ch06lis03)
    中的版本在功能上是相同的。^([8](ch06.html#ch06fn8)) 它只是每次循环迭代计算 `unrollFactor` 个输出。由于展开因子是一个模板参数，编译器可以使用寄存器来存储
    `values` 数组，并且最内层的 for 循环可以完全展开。
- en: '[8](ch06.html#ch06fn8a). Except that, as written, N must be divisible by `unrollFactor`.
    This is easily fixed, of course, with a small change to the for loop and a bit
    of cleanup code afterward.'
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: '[8](ch06.html#ch06fn8a)。除了，正如所写，N 必须能被 `unrollFactor` 整除。当然，可以通过对 for 循环做小的改动和之后清理一些代码来轻松解决这个问题。'
- en: For `unrollFactor==1`, this implementation is identical to that of [Listing
    6.3](ch06.html#ch06lis03). For `unrollFactor==2`, mapped pinned formulation shows
    some improvement over the streamed formulation. The tipping point drops from `cycles==64`
    to `cycles==48`. For `unrollFactor==4`, performance is uniformly better than the
    streamed version.
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `unrollFactor==1`，这个实现与 [Listing 6.3](ch06.html#ch06lis03) 的实现相同。对于 `unrollFactor==2`，映射的固定形式相比流式形式有一些改进。临界点从
    `cycles==64` 降到 `cycles==48`。对于 `unrollFactor==4`，性能普遍优于流式版本。
- en: '![Image](graphics/199tab01.jpg)'
  id: totrans-1398
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/199tab01.jpg)'
- en: These values are given for 32M integers, so the program reads and writes 128MB
    of data. For `cycles==48`, the program runs in 26ms. To achieve that effective
    bandwidth rate (more than 9GB/s over PCI Express 2.0), the GPU is concurrently
    reading and writing over PCI Express while performing the kernel processing!
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值是针对32M整数给出的，因此程序读取和写入128MB的数据。在`cycles==48`时，程序运行时长为26毫秒。为了达到这种有效带宽速率（超过9GB/s，基于PCI
    Express 2.0），GPU在执行内核处理的同时，正在通过PCI Express并发读取和写入数据！
- en: 6.7\. Concurrent Kernel Processing
  id: totrans-1400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7\. 并发内核处理
- en: SM 2.x-class and later GPUs are capable of concurrently running multiple kernels,
    provided they are launched in different streams and have block sizes that are
    small enough so a single kernel will not fill the whole GPU. The code in [Listing
    6.5](ch06.html#ch06lis05) (lines 9–14) will cause kernels to run concurrently,
    provided the number of blocks in each kernel launch is small enough. Since the
    kernels can only communicate through global memory, we can add some instrumentation
    to `AddKernel()` to track how many kernels are running concurrently. Using the
    following “kernel concurrency tracking” structure
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
  zh: SM 2.x及更高版本的GPU能够并发运行多个内核，前提是它们在不同的流中启动，并且块大小足够小，使得单个内核不会填满整个GPU。代码[示例6.5](ch06.html#ch06lis05)（第9–14行）将导致内核并发运行，前提是每个内核启动中的块数足够小。由于内核只能通过全局内存进行通信，我们可以在`AddKernel()`中添加一些工具，来跟踪有多少内核正在并发运行。使用以下“内核并发跟踪”结构：
- en: '[Click here to view code image](ch06_images.html#p200pro01a)'
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch06_images.html#p200pro01a)'
- en: static const int g_maxStreams = 8;
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: static const int g_maxStreams = 8;
- en: typedef struct KernelConcurrencyData_st {
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
  zh: typedef struct KernelConcurrencyData_st {
- en: int mask; // mask of active kernels
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: int mask; // 活跃内核的掩码
- en: int maskMax; // atomic max of mask popcount
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
  zh: int maskMax; // 掩码的原子最大值
- en: int masks[g_maxStreams];
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
  zh: int masks[g_maxStreams];
- en: int count; // number of active kernels
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: int count; // 活跃内核的数量
- en: int countMax; // atomic max of kernel count
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
  zh: int countMax; // 内核计数的原子最大值
- en: int counts[g_maxStreams];
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: int counts[g_maxStreams];
- en: '} KernelConcurrencyData;'
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
  zh: '} KernelConcurrencyData;'
- en: we can add code to `AddKernel()` to “check in” and “check out” at the beginning
    and end of the function, respectively. The “check in” takes the “kernel id” parameter
    `kid` (a value in the range 0..*NumStreams*-1 passed to the kernel), computes
    a mask `1<<kid` corresponding to the kernel ID into a global, and atomically OR’s
    that value into the global. Note that `atomicOR()` returns the value that was
    in the memory location before the OR was performed. As a result, the return value
    has one bit set for every kernel that was active when the atomic OR operation
    was performed.
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在`AddKernel()`中添加代码，在函数的开始和结束处分别进行“签到”和“签出”。“签到”接收“内核ID”参数`kid`（该值在0到*NumStreams*-1的范围内，传递给内核），计算出一个掩码`1<<kid`，该掩码对应于内核ID，并将其原子地OR到全局变量中。请注意，`atomicOR()`返回的是执行OR操作之前内存位置中的值。因此，返回值对于每个在执行原子OR操作时活跃的内核，都会设置一个位。
- en: Similarly, this code tracks the number of active kernels by incrementing `kernelData->count`
    and calling `atomicMax()` on a shared global.
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，这段代码通过增加`kernelData->count`并在共享全局变量上调用`atomicMax()`来跟踪活跃内核的数量。
- en: '[Click here to view code image](ch06_images.html#p200pro02a)'
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p200pro02a)'
- en: // check in, and record active kernel mask and count
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: // 检查并记录激活内核掩码和计数
- en: // as seen by this kernel.
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
  zh: // 如此内核所见。
- en: if ( kernelData && blockIdx.x==0 && threadIdx.x == 0 ) {
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
  zh: 如果（kernelData && blockIdx.x==0 && threadIdx.x == 0）{
- en: int myMask = atomicOr( &kernelData->mask, 1<<kid );
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
  zh: int myMask = atomicOr( &kernelData->mask, 1<<kid );
- en: kernelData->masks[kid] = myMask | (1<<kid);
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: kernelData->masks[kid] = myMask | (1<<kid);
- en: int myCount = atomicAdd( &kernelData->count, 1 );
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
  zh: int myCount = atomicAdd( &kernelData->count, 1 );
- en: atomicMax( &kernelData->countMax, myCount+1 );
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
  zh: atomicMax( &kernelData->countMax, myCount+1 );
- en: kernelData->counts[kid] = myCount+1;
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
  zh: kernelData->counts[kid] = myCount+1;
- en: '}'
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: At the bottom of the kernel, similar code clears the mask and decrements the
    active-kernel count.
  id: totrans-1424
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核底部，类似的代码清除掩码并减少激活内核的计数。
- en: '[Click here to view code image](ch06_images.html#p200pro03a)'
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p200pro03a)'
- en: // check out
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
  zh: // 查看
- en: if ( kernelData && blockIdx.x==0 && threadIdx.x==0 ) {
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
  zh: 如果（kernelData && blockIdx.x==0 && threadIdx.x==0）{
- en: atomicAnd( &kernelData->mask, ~(1<<kid) );
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
  zh: atomicAnd( &kernelData->mask, ~(1<<kid) );
- en: atomicAdd( &kernelData->count, -1 );
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: atomicAdd( &kernelData->count, -1 );
- en: '}'
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: The `kernelData` parameter refers to a `__device__` variable declared at file
    scope.
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: '`kernelData`参数指的是在文件范围内声明的`__device__`变量。'
- en: __device__ KernelConcurrencyData g_kernelData;
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ KernelConcurrencyData g_kernelData;
- en: Remember that the pointer to `g_kernelData` must be obtained by calling `cudaGetSymbolAddress()`.
    It is possible to write code that references `&g_kernelData`, but CUDA’s language
    integration will not correctly resolve the address.
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`g_kernelData`的指针必须通过调用`cudaGetSymbolAddress()`来获取。虽然可以编写引用`&g_kernelData`的代码，但CUDA的语言集成无法正确解析地址。
- en: The `concurrencyKernelKernel.cu` program adds support for a command line option
    `blocksPerSM` to specify the number of blocks with which to launch these kernels.
    It will generate a report on the number of kernels that were active. Two sample
    invocations of `concurrencyKernelKernel` are as follows.
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
  zh: '`concurrencyKernelKernel.cu`程序为命令行选项`blocksPerSM`增加了支持，以指定用于启动这些内核的块数。它将生成关于活跃内核数量的报告。以下是`concurrencyKernelKernel`的两个示例调用。'
- en: '[Click here to view code image](ch06_images.html#p201pro01a)'
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch06_images.html#p201pro01a)'
- en: $ ./concurrencyKernelKernel –blocksPerSM 2
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: $ ./concurrencyKernelKernel –blocksPerSM 2
- en: Using 2 blocks per SM on GPU with 14 SMs = 28 blocks
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有14个SM的GPU上，每个SM使用2个块 = 28个块
- en: 'Timing sequential operations... Kernel data:'
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
  zh: 正在计时顺序操作... 内核数据：
- en: 'Masks: ( 0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码：（0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0）
- en: 'Up to 1 kernels were active: (0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  id: totrans-1440
  prefs: []
  type: TYPE_NORMAL
  zh: 最多1个内核处于激活状态：（0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0）
- en: Timing concurrent operations...
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
  zh: 正在计时并行操作...
- en: 'Kernel data:'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: 内核数据：
- en: 'Masks: ( 0x1 0x3 0x7 0xe 0x1c 0x38 0x60 0xe0 )'
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
  zh: 掩码：（0x1 0x3 0x7 0xe 0x1c 0x38 0x60 0xe0）
- en: 'Up to 3 kernels were active: (0x1 0x2 0x3 0x3 0x3 0x3 0x2 0x3 )'
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: '最多有3个内核处于活动状态: (0x1 0x2 0x3 0x3 0x3 0x3 0x2 0x3 )'
- en: $ ./concurrencyKernelKernel –blocksPerSM 3
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
  zh: $ ./concurrencyKernelKernel –blocksPerSM 3
- en: Using 3 blocks per SM on GPU with 14 SMs = 42 blocks
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有14个SM的GPU上使用每个SM 3个块 = 42个块
- en: 'Timing sequential operations... Kernel data:'
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: 计时顺序操作... 内核数据：
- en: 'Masks: ( 0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
  zh: '掩码: ( 0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
- en: 'Up to 1 kernels were active: (0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
  zh: '最多有1个内核处于活动状态: (0x1 0x0 0x0 0x0 0x0 0x0 0x0 0x0 )'
- en: 'Timing concurrent operations... Kernel data:'
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
  zh: 计时并行操作... 内核数据：
- en: 'Masks: ( 0x1 0x3 0x6 0xc 0x10 0x30 0x60 0x80 )'
  id: totrans-1451
  prefs: []
  type: TYPE_NORMAL
  zh: '掩码: ( 0x1 0x3 0x6 0xc 0x10 0x30 0x60 0x80 )'
- en: 'Up to 2 kernels were active: (0x1 0x2 0x2 0x2 0x1 0x2 0x2 0x1 )'
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
  zh: '最多有2个内核处于活动状态: (0x1 0x2 0x2 0x2 0x1 0x2 0x2 0x1 )'
- en: Note that `blocksPerSM` is the number of blocks specified to each kernel launch,
    so a total of `numStreams*blocksPerSM` blocks are launched in `numStreams` separate
    kernels. You can see that the hardware can run more kernels concurrently when
    the kernel grids are smaller, but there is no performance benefit to concurrent
    kernel processing for the workload discussed in this chapter.
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`blocksPerSM`是指定每个内核启动的块数，因此总共会在`numStreams`个独立的内核中启动`numStreams*blocksPerSM`个块。你可以看到，当内核网格较小的时候，硬件可以同时运行更多的内核，但对于本章讨论的工作负载来说，并不会从并行内核处理中获得性能提升。
- en: '6.8\. GPU/GPU Synchronization: `cudaStreamWaitEvent()`'
  id: totrans-1454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8\. GPU/GPU同步：`cudaStreamWaitEvent()`
- en: Up to this point, all of the synchronization functions described in this chapter
    have pertained to CPU/GPU synchronization. They either wait for or query the status
    of a GPU operation. The `cudaStreamWaitEvent()function` is asynchronous with respect
    to the CPU and causes the specified *stream* to wait until an event has been recorded.
    The stream and event need not be associated with the same CUDA device. [Section
    9.3](ch09.html#ch09lev1sec3) describes how such inter-GPU synchronization may
    be performed and uses the feature to implement a peer-to-peer memcpy (see [Listing
    9.1](ch09.html#ch09lis01)).
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，本章中描述的所有同步函数都涉及到CPU/GPU同步。它们要么等待GPU操作的完成，要么查询GPU操作的状态。`cudaStreamWaitEvent()`函数是与CPU异步的，它使指定的*流*在事件记录完成之前处于等待状态。流和事件不必关联到同一CUDA设备。[第9.3节](ch09.html#ch09lev1sec3)描述了如何执行这种跨GPU同步，并利用该特性实现了点对点的内存拷贝（参见[示例9.1](ch09.html#ch09lis01)）。
- en: '6.8.1\. Streams and Events on Multi-GPU: Notes and Limitations'
  id: totrans-1456
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.8.1\. 多GPU上的流和事件：注意事项和限制
- en: • Streams and events exist in the scope of the context (or device). When `cuCtxDestroy()`
    or `cudaDeviceReset()` is called, the associated streams and events are destroyed.
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: • 流和事件存在于上下文（或设备）的作用域内。当调用`cuCtxDestroy()`或`cudaDeviceReset()`时，相关的流和事件会被销毁。
- en: • Kernel launches and `cu(da)EventRecord()` can only use CUDA streams in the
    same context/device.
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: • 内核启动和`cu(da)EventRecord()`只能在同一上下文/设备中的CUDA流上使用。
- en: • `cudaMemcpy()` can be called with any stream, but it is best to call it from
    the *source* context/device.
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
  zh: • `cudaMemcpy()`可以在任何流上调用，但最好从*源*上下文/设备中调用它。
- en: • `cudaStreamWaitEvent()` may be called on any event, using any stream.
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: • `cudaStreamWaitEvent()` 可以在任何事件上调用，使用任何流。
- en: 6.9\. Source Code Reference
  id: totrans-1461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9\. 源代码参考
- en: The source code referenced in this chapter resides in the `concurrency` directory.
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: 本章引用的源代码位于 `concurrency` 目录。
- en: '![Image](graphics/202tab01.jpg)![Image](graphics/202tab01a.jpg)'
  id: totrans-1463
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/202tab01.jpg)![Image](graphics/202tab01a.jpg)'
- en: Chapter 7\. Kernel Execution
  id: totrans-1464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章\. 内核执行
- en: 'This chapter gives a detailed description of how kernels are executed on the
    GPU: how they are launched, their execution characteristics, how they are organized
    into grids of blocks of threads, and resource management considerations. The chapter
    concludes with a description of dynamic parallelism—the new CUDA 5.0 feature that
    enables CUDA kernels to launch work for the GPU.'
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细描述了内核在 GPU 上的执行方式：它们如何被启动、执行特性、如何组织成线程块的网格，以及资源管理的考虑。章节最后介绍了动态并行性——CUDA
    5.0 的新特性，使得 CUDA 内核能够为 GPU 启动工作。
- en: 7.1\. Overview
  id: totrans-1466
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 概述
- en: 'CUDA kernels execute on the GPU and, since the very first version of CUDA,
    always have executed concurrently with the CPU. In other words, kernel launches
    are *asynchronous*: Control is returned to the CPU before the GPU has completed
    the requested operation. When CUDA was first introduced, there was no need for
    developers to concern themselves with the asynchrony (or lack thereof) of kernel
    launches; data had to be copied to and from the GPU explicitly, and the memcpy
    commands would be enqueued after the commands needed to launch kernels. It was
    not possible to write CUDA code that exposed the asynchrony of kernel launches;
    the main side effect was to hide driver overhead when performing multiple kernel
    launches consecutively.'
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 核心在 GPU 上执行，自从 CUDA 的第一个版本以来，一直与 CPU 并行执行。换句话说，内核启动是*异步*的：控制在 GPU 完成请求的操作之前就返回给了
    CPU。当 CUDA 最初被引入时，开发者无需关心内核启动的异步性（或缺乏异步性）；数据必须显式地从 GPU 复制进出，而 `memcpy` 命令会在启动内核所需的命令之后排队。无法编写暴露内核启动异步性的
    CUDA 代码；主要的副作用是隐藏了在连续执行多个内核启动时的驱动开销。
- en: With the introduction of mapped pinned memory (host memory that can be directly
    accessed by the GPU), the asynchrony of kernel launches becomes more important,
    especially for kernels that write to host memory (as opposed to read from it).
    If a kernel is launched and writes host memory without explicit synchronization
    (such as with CUDA events), the code suffers from a race condition between the
    CPU and GPU and may not run correctly. Explicit synchronization often is not needed
    for kernels that *read* via mapped pinned memory, since any pending writes by
    the CPU will be posted before the kernel launches. But for kernels that are returning
    results to CPU by writing to mapped pinned memory, synchronizing to avoid write-after-read
    hazards is essential.
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
  zh: 随着映射固定内存（可以被GPU直接访问的主机内存）的引入，内核启动的异步性变得更加重要，特别是对于那些写入主机内存的内核（与读取主机内存的内核相对）。如果一个内核被启动并且写入主机内存而没有显式同步（例如通过CUDA事件），代码将会遭遇CPU与GPU之间的竞争条件，可能无法正确运行。对于通过映射固定内存进行*读取*的内核，通常不需要显式同步，因为CPU的任何挂起写入将在内核启动前提交。但是，对于那些通过写入映射固定内存将结果返回给CPU的内核，必须进行同步，以避免写后读的危险。
- en: Once a kernel is launched, it runs as a *grid* of *blocks* of *threads*. Not
    all blocks run concurrently, necessarily; each block is assigned to a streaming
    multiprocessor (SM), and each SM can maintain the context for multiple blocks.
    To cover both memory and instruction latencies, the SM generally needs more warps
    than a single block can contain. The maximum number of blocks per SM cannot be
    queried, but it is documented by NVIDIA as having been 8 before SM 3.x and 16
    on SM 3.x and later hardware.
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动内核，它将作为一个*网格*由*块*和*线程*组成进行运行。并不是所有的块都会同时运行；每个块会分配给一个流式多处理器（SM），每个SM可以维持多个块的上下文。为了覆盖内存和指令延迟，SM通常需要比单个块能包含的更多的warp。在SM
    3.x之前，每个SM最多可以容纳8个块，而在SM 3.x及以后版本的硬件中，最大块数为16个，尽管无法查询每个SM的最大块数，但NVIDIA已对此进行了文档说明。
- en: The programming model makes no guarantees whatsoever as to the order of execution
    or whether certain blocks or threads can run concurrently. Developers can never
    assume that all the threads in a kernel launch are executing concurrently. It
    is easy to launch more threads than the machine can hold, and some will not start
    executing until others have finished. Given the lack of ordering guarantees, even
    initialization of global memory at the beginning of a kernel launch is a difficult
    proposition.
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
  zh: 编程模型完全不保证执行顺序，也不保证某些块或线程是否可以并发执行。开发者不能假设内核启动中的所有线程都会并发执行。很容易启动比机器能承载的更多线程，在某些线程完成之前，其他线程可能无法开始执行。由于缺乏顺序保证，即使是内核启动时对全局内存的初始化也是一个困难的任务。
- en: '*Dynamic parallelism*, a new feature added with the Tesla K20 (GK110), the
    first SM 3.5–capable GPU, enables kernels to launch other kernels and perform
    synchronization between them. These capabilities address some of the limitations
    that were present in CUDA in previous hardware. For example, a dynamically parallel
    kernel can perform initialization by launching and waiting for a child grid.'
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: '*动态并行*是 Tesla K20（GK110）中新增的一项功能，该卡是首款支持 SM 3.5 的 GPU，使得内核可以启动其他内核并进行同步。这些功能解决了之前
    CUDA 硬件中的一些限制。例如，动态并行内核可以通过启动并等待子网格来进行初始化。'
- en: 7.2\. Syntax
  id: totrans-1472
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 语法
- en: When using the CUDA runtime, a kernel launch is specified using the familiar
    triple-angle-bracket syntax.
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 CUDA 运行时，内核启动使用常见的三重尖括号语法来指定。
- en: Kernel<<<gridSize, blockSize, sharedMem, Stream>>>( Parameters... )
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: Kernel<<<gridSize, blockSize, sharedMem, Stream>>>( 参数... )
- en: '*Kernel* specifies the kernel to launch.'
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kernel*指定要启动的内核。'
- en: '*gridSize* specifies the size of the grid in the form of a `dim3` structure.'
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: '*gridSize*指定网格的大小，形式为`dim3`结构。'
- en: '*blockSize* specifies the dimension of each threadblock as a `dim3.`'
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: '*blockSize*指定每个线程块的维度，作为`dim3`。'
- en: '*sharedMem* specifies additional shared memory^([1](ch07.html#ch07fn1)) to
    reserve for each block.'
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
  zh: '*sharedMem*指定为每个块保留的额外共享内存^([1](ch07.html#ch07fn1))。'
- en: '[1](ch07.html#ch07fn1a). The amount of shared memory available to the kernel
    is the sum of this parameter and the amount of shared memory that was statically
    declared within the kernel.'
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch07.html#ch07fn1a)。可用于内核的共享内存的大小是此参数与内核中静态声明的共享内存的总和。'
- en: '*Stream* specifies the stream in which the kernel should be launched.'
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: '*Stream*指定内核应该在哪个流中启动。'
- en: The `dim3` structure used to specify the grid and block sizes has 3 members
    (`x`, `y`, and `z`) and, when compiling with C++, a constructor with default parameters
    such that the `y` and `z` members default to 1\. See [Listing 7.1](ch07.html#ch07lis01),
    which is excerpted from the NVIDIA SDK file `vector_types.h.`
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
  zh: 用于指定网格和块大小的`dim3`结构有3个成员（`x`、`y` 和 `z`），并且在使用 C++ 编译时，构造函数具有默认参数，使得 `y` 和 `z`
    成员默认为 1。请参见[示例 7.1](ch07.html#ch07lis01)，这是从 NVIDIA SDK 文件 `vector_types.h` 中摘录的内容。
- en: '*Listing 7.1.* `dim3` structure.'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 7.1.* `dim3` 结构。'
- en: '[Click here to view code image](ch07_images.html#p07lis01a)'
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch07_images.html#p07lis01a)'
- en: '* * *'
  id: totrans-1484
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: struct __device_builtin__ dim3
  id: totrans-1485
  prefs: []
  type: TYPE_NORMAL
  zh: struct __device_builtin__ dim3
- en: '{'
  id: totrans-1486
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: unsigned int x, y, z;
  id: totrans-1487
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int x, y, z;
- en: '#if defined(__cplusplus)'
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
  zh: '#if defined(__cplusplus)'
- en: __host__ __device__ dim3(
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
  zh: __host__ __device__ dim3(
- en: unsigned int vx = 1,
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int vx = 1,
- en: unsigned int vy = 1,
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int vy = 1,
- en: 'unsigned int vz = 1) : x(vx), y(vy), z(vz) {}'
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
  zh: 'unsigned int vz = 1) : x(vx), y(vy), z(vz) {}'
- en: '__host__ __device__ dim3(uint3 v) : x(v.x), y(v.y), z(v.z) {}'
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
  zh: '__host__ __device__ dim3(uint3 v) : x(v.x), y(v.y), z(v.z) {}'
- en: __host__ __device__ operator uint3(void) {
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: __host__ __device__ 操作符 uint3(void) {
- en: uint3 t;
  id: totrans-1495
  prefs: []
  type: TYPE_NORMAL
  zh: uint3 t;
- en: t.x = x;
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
  zh: t.x = x;
- en: t.y = y;
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
  zh: t.y = y;
- en: t.z = z;
  id: totrans-1498
  prefs: []
  type: TYPE_NORMAL
  zh: t.z = z;
- en: return t;
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
  zh: return t;
- en: '}'
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '#endif /* __cplusplus */'
  id: totrans-1501
  prefs: []
  type: TYPE_NORMAL
  zh: '#endif /* __cplusplus */'
- en: '};'
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '* * *'
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Kernels can be launched via the driver API using `cuLaunchKernel()`, though
    that function takes the grid and block dimensions as discrete parameters rather
    than `dim3.`
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数可以通过驱动程序 API 使用 `cuLaunchKernel()` 启动，尽管该函数将网格和块维度作为独立的参数，而不是 `dim3`。
- en: '[Click here to view code image](ch07_images.html#p207pro01a)'
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch07_images.html#p207pro01a)'
- en: CUresult cuLaunchKernel (
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuLaunchKernel (
- en: CUfunction kernel,
  id: totrans-1507
  prefs: []
  type: TYPE_NORMAL
  zh: CUfunction kernel，
- en: unsigned int gridDimX,
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号整数 gridDimX，
- en: unsigned int gridDimY,
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号整数 gridDimY，
- en: unsigned int gridDimZ,
  id: totrans-1510
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号整数 gridDimZ，
- en: unsigned int blockDimX,
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号整数 blockDimX，
- en: unsigned int blockDimY,
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号整数 blockDimY，
- en: unsigned int blockDimZ,
  id: totrans-1513
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号整数 blockDimZ，
- en: unsigned int sharedMemBytes,
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号整数 sharedMemBytes，
- en: CUstream hStream,
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
  zh: CUstream hStream，
- en: void **kernelParams,
  id: totrans-1516
  prefs: []
  type: TYPE_NORMAL
  zh: void **kernelParams，
- en: void **extra
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
  zh: void **extra
- en: );
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
  zh: );
- en: 'As with the triple-angle-bracket syntax, the parameters to `cuLaunchKernel()`
    include the kernel to invoke, the grid and block sizes, the amount of shared memory,
    and the stream. The main difference is in how the parameters to the kernel itself
    are given: Since the kernel microcode emitted by `ptxas` contains metadata that
    describes each kernel’s parameters,^([2](ch07.html#ch07fn2)) `kernelParams` is
    an array of `void *`, where each element corresponds to a kernel parameter. Since
    the type is known by the driver, the correct amount of memory (4 bytes for an
    `int`, 8 bytes for a `double`, etc.) will be copied into the command buffer as
    part of the hardware-specific command used to invoke the kernel.'
  id: totrans-1519
  prefs: []
  type: TYPE_NORMAL
  zh: 与三重尖括号语法一样，`cuLaunchKernel()` 的参数包括要调用的核函数、网格和块大小、共享内存量以及流。主要区别在于传递给核函数本身的参数方式：由于由
    `ptxas` 生成的核微代码包含描述每个核函数参数的元数据，^([2](ch07.html#ch07fn2)) `kernelParams` 是一个 `void
    *` 数组，其中每个元素对应一个核参数。由于驱动程序知道类型，因此正确的内存量（例如 `int` 为 4 字节，`double` 为 8 字节等）将被复制到命令缓冲区，作为调用核函数时使用的硬件特定命令的一部分。
- en: '[2](ch07.html#ch07fn2a). `cuLaunchKernel()` will fail on binary images that
    were not compiled with CUDA 3.2 or later, since that is the first version to include
    kernel parameter metadata.'
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch07.html#ch07fn2a). `cuLaunchKernel()` 将在未使用 CUDA 3.2 或更高版本编译的二进制图像上失败，因为那是首个版本包含了核参数元数据。'
- en: 7.2.1\. Limitations
  id: totrans-1521
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 限制
- en: All C++ classes participating in a kernel launch must be “plain old data” (POD)
    with the following characteristics.
  id: totrans-1522
  prefs: []
  type: TYPE_NORMAL
  zh: 所有参与核函数启动的 C++ 类必须是“传统旧数据”（POD）类型，具备以下特点。
- en: • No user-declared constructors
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
  zh: • 无用户声明的构造函数
- en: • No user-defined copy assignment operator
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
  zh: • 无用户定义的赋值运算符
- en: • No user-defined destructor
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
  zh: • 无用户定义的析构函数
- en: • No nonstatic data members that are not themselves PODs
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
  zh: • 无非静态的数据成员，且这些成员本身不是 POD 类型
- en: • No private or protected nonstatic data
  id: totrans-1527
  prefs: []
  type: TYPE_NORMAL
  zh: • 无私有或受保护的非静态数据
- en: • No base classes
  id: totrans-1528
  prefs: []
  type: TYPE_NORMAL
  zh: • 无基类
- en: • No virtual functions
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
  zh: • 无虚函数
- en: Note that classes that violate these rules may be used in CUDA, or even in CUDA
    kernels; they simply cannot be used for a kernel launch. In that case, the classes
    used by a CUDA kernel can be constructed using the POD input data from the launch.
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，违反这些规则的类可以在CUDA中使用，甚至可以在CUDA内核中使用；它们只是不能用于内核启动。在这种情况下，CUDA内核使用的类可以使用启动时的POD输入数据进行构造。
- en: CUDA kernels also do not have return values. They must report their results
    back via device memory (which must be copied back to the CPU explicitly) or mapped
    host memory.
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA内核没有返回值。它们必须通过设备内存（必须显式复制回CPU）或映射的主机内存将结果报告回来。
- en: 7.2.2\. Caches and Coherency
  id: totrans-1532
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2\. 缓存与一致性
- en: The GPU contains numerous caches to accelerate computation when reuse occurs.
    The constant cache is optimized for broadcast to the execution units within an
    SM; the texture cache reduces external bandwidth usage. Neither of these caches
    is kept coherent with respect to writes to memory by the GPU. For example, there
    is no protocol to enforce coherency between these caches and the L1 or L2 caches
    that serve to reduce latency and aggregate bandwidth to global memory. That means
    two things.
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
  zh: GPU包含多个缓存，用于加速计算，特别是在发生重用时。常量缓存针对广播优化，广播到SM内的执行单元；纹理缓存则减少外部带宽使用。这些缓存都不会与GPU对内存的写操作保持一致。例如，没有协议来强制保持这些缓存与L1或L2缓存之间的一致性，L1或L2缓存用于减少延迟并聚合到全局内存的带宽。这意味着两点。
- en: '**1.** When a kernel is running, it must take care not to write memory that
    it (or a concurrently running kernel) also is accessing via constant or texture
    memory.'
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.** 当内核运行时，必须小心不要写入它自己（或正在并行运行的内核）也通过常量或纹理内存访问的内存。'
- en: '**2.** The CUDA driver must invalidate the constant cache and texture cache
    before each kernel launch.'
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** CUDA驱动程序必须在每次内核启动之前使常量缓存和纹理缓存失效。'
- en: For kernels that do not contain `TEX` instructions, there is no need for the
    CUDA driver to invalidate the texture cache; as a result, kernels that do not
    use texture incur less driver overhead.
  id: totrans-1536
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不包含`TEX`指令的内核，CUDA驱动程序无需使纹理缓存失效；因此，不使用纹理的内核会减少驱动程序的开销。
- en: 7.2.3\. Asynchrony and Error Handling
  id: totrans-1537
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.3\. 异步性与错误处理
- en: 'Kernel launches are *asynchronous*: As soon as a kernel is submitted to the
    hardware, it begins executing in parallel with the CPU.^([3](ch07.html#ch07fn3))
    This asynchrony complicates error handling. If a kernel encounters an error (for
    example, if it reads an invalid memory location), the error is reported to the
    driver (and the application) sometime after the kernel launch. The surest way
    to check for such errors is to synchronize with the GPU using `cudaDeviceSynchronize()`
    or `cuCtxSynchronize()`. If an error in kernel execution has occurred, the error
    code “unspecified launch failure” is returned.'
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
  zh: 内核启动是*异步*的：一旦内核提交给硬件，它便开始与 CPU 并行执行。^([3](ch07.html#ch07fn3)) 这种异步性使得错误处理变得复杂。如果内核遇到错误（例如，如果它读取了无效的内存位置），错误将在内核启动之后的某个时间报告给驱动程序（和应用程序）。检查此类错误的最可靠方法是使用`cudaDeviceSynchronize()`或`cuCtxSynchronize()`与
    GPU 同步。如果内核执行中发生错误，将返回错误代码“未指定的启动失败”。
- en: '[3](ch07.html#ch07fn3a). On most platforms, the kernel will start executing
    on the GPU microseconds after the CPU has finished processing the launch command.
    But on the Windows Display Driver Model (WDDM), it may take longer because the
    driver must perform a kernel thunk in order to submit the launch to the hardware,
    and work for the GPU is enqueued in user mode to amortize the overhead of the
    user→kernel transition.'
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch07.html#ch07fn3a)。在大多数平台上，内核将在 CPU 完成处理启动命令后微秒级别地开始在 GPU 上执行。但在 Windows
    显示驱动程序模型 (WDDM) 中，可能需要更长的时间，因为驱动程序必须执行内核转换才能将启动提交给硬件，且 GPU 的工作会排队到用户模式，以分摊用户→内核转换的开销。'
- en: Besides explicit CPU/GPU synchronization calls such as `cudaDevice-Synchronize()`
    or `cuCtxSynchronize()`, this error code may be returned by functions that implicitly
    synchronize with the CPU, such as synchronous memcpy calls.
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
  zh: 除了显式的 CPU/GPU 同步调用，如`cudaDevice-Synchronize()`或`cuCtxSynchronize()`，此错误代码还可能由隐式与
    CPU 同步的函数返回，例如同步的 memcpy 调用。
- en: Invalid Kernel Launches
  id: totrans-1541
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 无效的内核启动
- en: It is possible to request a kernel launch that the hardware cannot perform—for
    example, by specifying more threads per block than the hardware supports. When
    possible, the driver detects these cases and reports an error rather than trying
    to submit the launch to the hardware.
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会请求硬件无法执行的内核启动——例如，指定比硬件支持的更多线程每个块。当可能时，驱动程序会检测这些情况并报告错误，而不是尝试将启动提交给硬件。
- en: The CUDA runtime and the driver API handle this case differently. When an invalid
    parameter is specified, the driver API’s explicit API calls such as `cuLaunchGrid()`
    and `cuLaunchKernel()` return error codes. But when using the CUDA runtime, since
    kernels are launched in-line with C/C++ code, there is no API call to return an
    error code. Instead, the error is “recorded” into a thread-local slot and applications
    can query the error value with `cudaGetLastError()`. This same error handling
    mechanism is used for kernel launches that are invalid for other reasons, such
    as a memory access violation.
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 运行时和驱动 API 对此情况的处理方式不同。当指定无效参数时，驱动 API 的显式 API 调用，如`cuLaunchGrid()`和`cuLaunchKernel()`会返回错误代码。但在使用
    CUDA 运行时时，由于内核是与 C/C++ 代码内联启动的，因此没有 API 调用来返回错误代码。相反，错误会被“记录”到线程局部槽中，应用程序可以通过`cudaGetLastError()`查询错误值。这个相同的错误处理机制也用于由于其他原因（如内存访问违规）导致的无效内核启动。
- en: 7.2.4\. Timeouts
  id: totrans-1544
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.4\. 超时
- en: Because the GPU is not able to context-switch in the midst of kernel execution,
    a long-running CUDA kernel may negatively impact the interactivity of a system
    that uses the GPU to interact with the user. As a result, many CUDA systems implement
    a “timeout” that resets the GPU if it runs too long without context switching.
  id: totrans-1545
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 GPU 在内核执行过程中无法进行上下文切换，所以长时间运行的 CUDA 内核可能会对使用 GPU 与用户交互的系统的交互性产生负面影响。因此，许多
    CUDA 系统实现了一个“超时”机制，如果 GPU 长时间没有进行上下文切换，就会重置 GPU。
- en: On WDDM (Windows Display Driver Model), the timeout is enforced by the operating
    system. Microsoft has documented how this “[Timeout Detection and Recovery](ch03.html#ch03lev4sec1)”
    (TDR) works. See [http://bit.ly/WPPSdQ](http://bit.ly/WPPSdQ), which includes
    the Registry keys that control TDR behavior.^([4](ch07.html#ch07fn4)) TDR can
    be safely disabled by using the Tesla Compute Cluster (TCC) driver, though the
    TCC driver is not available for all hardware.
  id: totrans-1546
  prefs: []
  type: TYPE_NORMAL
  zh: 在 WDDM（Windows 显示驱动模型）上，超时由操作系统强制执行。微软已记录了 “[超时检测与恢复](ch03.html#ch03lev4sec1)”（TDR）的工作原理。请参见[http://bit.ly/WPPSdQ](http://bit.ly/WPPSdQ)，其中包括控制
    TDR 行为的注册表键。^([4](ch07.html#ch07fn4)) 通过使用 Tesla 计算集群（TCC）驱动程序可以安全地禁用 TDR，尽管 TCC
    驱动程序并非适用于所有硬件。
- en: '[4](ch07.html#ch07fn4a). Modifying the Registry should only be done for test
    purposes, of course.'
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch07.html#ch07fn4a). 修改注册表仅应在测试时进行。'
- en: 'On Linux, the NVIDIA driver enforces a default timeout of 2 seconds. No time
    out is enforced on secondary GPUs that are not being used for display. Developers
    can query whether a runtime limit is being enforced on a given GPU by calling
    `cuDeviceGetAttribute()` with `CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_``TIMEOUT`, or
    by examining `cudaDeviceProp:: kernelExecTimeoutEnabled`.'
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
  zh: '在 Linux 上，NVIDIA 驱动强制执行默认的 2 秒超时。对于未用于显示的次级 GPU，不强制执行超时。开发者可以通过调用`cuDeviceGetAttribute()`并传入`CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT`，或检查`cudaDeviceProp::
    kernelExecTimeoutEnabled`来查询某个 GPU 是否正在执行运行时限制。'
- en: 7.2.5\. Local Memory
  id: totrans-1549
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.5\. 本地内存
- en: Since local memory is per-thread, and a grid in CUDA can contain thousands of
    threads, the amount of local memory needed by a CUDA grid can be considerable.
    The developers of CUDA took pains to preallocate resources to minimize the likelihood
    that operations such as kernel launches would fail due to a lack of resources,
    but in the case of local memory, a conservative allocation simply would have consumed
    too much memory. As a result, kernels that use a large amount of local memory
    take longer and may be synchronous because the CUDA driver must allocate memory
    before performing the kernel launch. Furthermore, if the memory allocation fails,
    the kernel launch will fail due to a lack of resources.
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本地内存是每个线程独立的，而CUDA中的网格可以包含成千上万个线程，因此CUDA网格所需的本地内存可能相当可观。CUDA的开发人员在预分配资源方面下了很大功夫，以尽量减少由于资源不足而导致操作（如内核启动）失败的可能性，但对于本地内存，保守的分配方式往往会消耗过多的内存。因此，使用大量本地内存的内核执行时间较长，且可能是同步的，因为CUDA驱动程序必须在执行内核启动之前分配内存。此外，如果内存分配失败，内核启动将因为资源不足而失败。
- en: By default, when the CUDA driver must allocate local memory to run a kernel,
    it frees the memory after the kernel has finished. This behavior additionally
    makes the kernel launch synchronous. But this behavior can be inhibited by specifying
    `CU_CTX_LMEM_RESIZE_TO_MAX to cuCtxCreate()` or by calling `cudaSetDeviceFlags()`
    with `cudaDeviceLmemResizeToMax` before the primary context is created. In this
    case, the increased amount of local memory available will persist after launching
    a kernel that required more local memory than the default.
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，当CUDA驱动程序必须分配本地内存以运行内核时，它会在内核执行完成后释放内存。这一行为还会使得内核启动变为同步。但通过指定`CU_CTX_LMEM_RESIZE_TO_MAX`给`cuCtxCreate()`，或在创建主上下文之前调用`cudaSetDeviceFlags()`并使用`cudaDeviceLmemResizeToMax`，可以抑制这种行为。在这种情况下，分配的更多本地内存会在启动需要更多本地内存的内核后持续存在，而不会恢复到默认状态。
- en: 7.2.6\. Shared Memory
  id: totrans-1552
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.6\. 共享内存
- en: Shared memory is allocated when the kernel is launched, and it stays allocated
    for the duration of the kernel’s execution. Besides static allocations that can
    be declared in the kernel, shared memory can be declared as an unsized `extern`;
    in that case, the amount of shared memory to allocate for the unsized array is
    specified as the third parameter of the kernel launch, or the `sharedMemBytes`
    parameter to `cuLaunchKernel()`.
  id: totrans-1553
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存在内核启动时分配，并且会在内核执行期间保持分配状态。除了可以在内核中声明的静态分配外，共享内存还可以声明为无大小的`extern`；在这种情况下，要为无大小数组分配的共享内存大小作为内核启动的第三个参数指定，或者通过`cuLaunchKernel()`中的`sharedMemBytes`参数来指定。
- en: 7.3\. Blocks, Threads, Warps, and Lanes
  id: totrans-1554
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 块、线程、波浪和通道
- en: Kernels are launched as *grids* of *blocks* of threads. Threads can further
    be divided into 32-thread *warps*, and each thread in a warp is called a *lane*.
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
  zh: 内核作为*网格*的*块*的线程启动。线程可以进一步划分为32个线程的*波浪*，波浪中的每个线程称为*通道*。
- en: 7.3.1\. Grids of Blocks
  id: totrans-1556
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1\. 块的网格
- en: Thread blocks are separately scheduled onto SMs, and threads within a given
    block are executed by the same SM. [Figure 7.1](ch07.html#ch07fig01) shows a 2D
    grid (8W × 6H) of 2D blocks (8W × 8H). [Figure 7.2](ch07.html#ch07fig02) shows
    a 3D grid (8W × 6H × 6D) of 3D blocks (8W × 8H × 4D).
  id: totrans-1557
  prefs: []
  type: TYPE_NORMAL
  zh: 线程块会被单独调度到SM上，给定块内的线程由同一个SM执行。[图7.1](ch07.html#ch07fig01)展示了一个2D网格（8W × 6H）和2D块（8W
    × 8H）。[图7.2](ch07.html#ch07fig02)展示了一个3D网格（8W × 6H × 6D）和3D块（8W × 8H × 4D）。
- en: '![Image](graphics/07fig01.jpg)'
  id: totrans-1558
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/07fig01.jpg)'
- en: '*Figure 7.1* 2D grid and thread block.'
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.1* 2D网格和线程块。'
- en: '![Image](graphics/07fig02.jpg)'
  id: totrans-1560
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/07fig02.jpg)'
- en: '*Figure 7.2* 3D grid and thread block.'
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.2* 3D网格和线程块。'
- en: Grids can be up to 65535 x 65535 blocks (for SM 1.x hardware) or 65535 x 65535
    x 65535 blocks (for SM 2.x hardware).^([5](ch07.html#ch07fn5)) Blocks may be up
    to 512 or 1024 threads in size,^([6](ch07.html#ch07fn6)) and threads within a
    block can communicate via the SM’s shared memory. Blocks within a grid are likely
    to be assigned to different SMs; to maximize throughput of the hardware, a given
    SM can run threads and warps from different blocks at the same time. The warp
    schedulers dispatch instructions as needed resources become available.
  id: totrans-1562
  prefs: []
  type: TYPE_NORMAL
  zh: 网格最多可以达到65535 x 65535个块（对于SM 1.x硬件）或65535 x 65535 x 65535个块（对于SM 2.x硬件）。^([5](ch07.html#ch07fn5))
    块的最大线程数可以是512或1024。^([6](ch07.html#ch07fn6)) 块内的线程可以通过SM的共享内存进行通信。网格内的块可能会被分配到不同的SM上；为了最大化硬件的吞吐量，给定的SM可以同时运行来自不同块的线程和warp。warp调度器会在所需资源可用时调度指令。
- en: '[5](ch07.html#ch07fn5a). The maximum grid size is queryable via `CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X,`
    `CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y`, or `CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z`;
    or by calling `cudaGetDeviceGetProperties()` and examining `cudaDeviceProp::maxGridSize`.'
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch07.html#ch07fn5a)。最大网格大小可以通过`CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X`、`CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y`或`CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z`查询；也可以通过调用`cudaGetDeviceGetProperties()`并检查`cudaDeviceProp::maxGridSize`来查询。'
- en: '[6](ch07.html#ch07fn6a). The maximum block size is queryable via `CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK`,
    or `deviceProp.maxThreadsPerBlock`.'
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](ch07.html#ch07fn6a)。最大块大小可以通过`CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK`或`deviceProp.maxThreadsPerBlock`查询。'
- en: Threads
  id: totrans-1565
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 线程
- en: Each threads gets a full complement of registers^([7](ch07.html#ch07fn7)) and
    a thread ID that is unique within the threadblock. To obviate the need to pass
    the size of the grid and threadblock into every kernel, the grid and block size
    also are available for kernels to read at runtime. The built-in variables used
    to reference these registers are given in [Table 7.1](ch07.html#ch07tab01). They
    are all of type `dim3`.
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
  zh: 每个线程都获得完整的寄存器组^([7](ch07.html#ch07fn7))和一个在线程块内唯一的线程ID。为了避免在每个内核中都需要传递网格和线程块的大小，网格和块的大小在运行时也可以供内核读取。用于引用这些寄存器的内建变量列在[表7.1](ch07.html#ch07tab01)中。它们都是`dim3`类型。
- en: '[7](ch07.html#ch07fn7a). The more registers needed per thread, the fewer threads
    can “fit” in a given SM. The percentage of warps executing in an SM as compared
    to the theoretical maximum is called *occupancy* (see [Section 7.4](ch07.html#ch07lev1sec4)).'
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](ch07.html#ch07fn7a)。每个线程需要的寄存器越多，给定 SM 中能够“容纳”的线程就越少。相对于理论最大值，SM 中执行的波束百分比被称为*占用率*（见[第
    7.4 节](ch07.html#ch07lev1sec4)）。'
- en: '![Image](graphics/07tab01.jpg)'
  id: totrans-1568
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/07tab01.jpg)'
- en: '*Table 7.1* Built-In Variables'
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 7.1* 内建变量'
- en: Taken together, these variables can be used to compute which part of a problem
    the thread will operate on. A “global” index for a thread can be computed as follows.
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
  zh: 综合来看，这些变量可以用来计算线程将操作问题的哪一部分。线程的“全局”索引可以通过以下方式计算。
- en: '[Click here to view code image](ch07_images.html#p213pro01a)'
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch07_images.html#p213pro01a)'
- en: int globalThreadId =
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
  zh: int globalThreadId =
- en: threadIdx.x+blockDim.x*(threadIdx.y+blockDim.y*threadIdx.z);
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
  zh: threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * threadIdx.z);
- en: Warps, Lanes, and ILP
  id: totrans-1574
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 波束、通道和 ILP
- en: The threads themselves are executed together, in SIMD fashion, in units of 32
    threads called a *warp*, after the collection of parallel threads in a loom.^([8](ch07.html#ch07fn8))
    (See [Figure 7.3](ch07.html#ch07fig03).) All 32 threads execute the same instruction,
    each using its private set of registers to perform the requested operation. In
    a triumph of mixed metaphor, the ID of a thread within a warp is called its *lane*.
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
  zh: 线程本身是一起执行的，以 SIMD 方式，单位是 32 个线程，称为*波束*，它是在一个织机中收集的并行线程之后的单位。^([8](ch07.html#ch07fn8))（参见[图
    7.3](ch07.html#ch07fig03)。）所有 32 个线程执行相同的指令，每个线程使用其私有的寄存器集合执行请求的操作。在混合隐喻的胜利中，波束内线程的
    ID 被称为其*通道*。
- en: '[8](ch07.html#ch07fn8a). The warp size can be queried, but it imposes such
    a huge compatibility burden on the hardware that developers can rely on it staying
    fixed at 32 for the foreseeable future.'
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
  zh: '[8](ch07.html#ch07fn8a)。可以查询波束大小，但它给硬件带来了巨大的兼容性负担，因此开发人员可以依赖它在可预见的未来保持为 32。'
- en: '![Image](graphics/07fig03.jpg)'
  id: totrans-1577
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/07fig03.jpg)'
- en: '*Figure 7.3* Loom.'
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.3* 织机。'
- en: The warp ID and lane ID can be computed using a global thread ID as follows.
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过全局线程 ID 计算波束 ID 和通道 ID，如下所示。
- en: int warpID = globalThreadId >> 5;
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
  zh: int warpID = globalThreadId >> 5;
- en: int laneID = globalThreadId & 31;
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
  zh: int laneID = globalThreadId & 31;
- en: Warps are an important unit of execution because they are the granularity with
    which GPUs can cover latency. It has been well documented how GPUs use parallelism
    to cover memory latency. It takes hundreds of clock cycles to satisfy a global
    memory request, so when a texture fetch or read is encountered, the GPU issues
    the memory request and then schedules other instructions until the data arrives.
    Once the data has arrived, the warp becomes eligible for execution again.
  id: totrans-1582
  prefs: []
  type: TYPE_NORMAL
  zh: 波束是一个重要的执行单元，因为它们是 GPU 可以覆盖延迟的粒度。已经有充分的文献说明了 GPU 如何利用并行性来覆盖内存延迟。满足全局内存请求需要数百个时钟周期，因此当遇到纹理获取或读取时，GPU
    会发出内存请求，并在数据到达之前调度其他指令。数据到达后，波束就可以再次执行。
- en: What has been less well documented is that GPUs also use parallelism to exploit
    ILP (“instruction level parallelism”). ILP refers to fine-grained parallelism
    that occurs during program execution; for example, when computing `(a+b)*(c+d)`,
    the addition operations `a+b` and `c+d` can be performed in parallel before the
    multiplication must be performed. Because the SMs already have a tremendous amount
    of logic to track dependencies and cover latency, they are very good at covering
    instruction latency through parallelism (which is effectively ILP) as well as
    memory latency. GPUs’ support for ILP is part of the reason loop unrolling is
    such an effective optimization strategy. Besides slightly reducing the number
    of instructions per loop iteration, it exposes more parallelism for the warp schedulers
    to exploit.
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
  zh: 较少被文档化的一点是，GPU还使用并行性来利用ILP（“指令级并行”）。ILP指的是在程序执行过程中发生的细粒度并行性；例如，在计算`(a+b)*(c+d)`时，加法运算`a+b`和`c+d`可以在乘法运算之前并行执行。由于SMs已经拥有大量逻辑来跟踪依赖关系并覆盖延迟，它们在通过并行性（实际上就是ILP）以及内存延迟来掩盖指令延迟方面非常高效。GPU对ILP的支持是循环展开作为一种有效优化策略的部分原因。除了稍微减少每次循环迭代中的指令数，它还为warp调度器暴露了更多并行性。
- en: Object Scopes
  id: totrans-1584
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对象作用域
- en: The scopes of objects that may be referenced by a kernel grid are summarized
    in [Table 7.2](ch07.html#ch07tab02), from the most local (registers in each thread)
    to the most global (global memory and texture references are per grid). Before
    the advent of dynamic parallelism, thread blocks served primarily as a mechanism
    for interthread synchronization within a thread block (via intrinsics such as
    `__syncthreads()`) and communication (via shared memory). Dynamic parallelism
    adds resource management to the mix, since streams and events created within a
    kernel are only valid for threads within the same thread block.
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
  zh: 可能被内核网格引用的对象作用域总结在[表 7.2](ch07.html#ch07tab02)中，从最局部（每个线程中的寄存器）到最全局（全局内存和纹理引用按网格计算）。在动态并行性出现之前，线程块主要作为线程块内部线程同步（通过内建函数如`__syncthreads()`）和通信（通过共享内存）的一种机制。动态并行性为这一过程增加了资源管理，因为在内核中创建的流和事件仅对同一线程块中的线程有效。
- en: '![Image](graphics/07tab02.jpg)'
  id: totrans-1586
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/07tab02.jpg)'
- en: '*Table 7.2* Object Scopes'
  id: totrans-1587
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 7.2* 对象作用域'
- en: 7.3.2\. Execution Guarantees
  id: totrans-1588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2 执行保证
- en: It is important that developers never make any assumptions about the order in
    which blocks or threads will execute. In particular, there is no way to know which
    block or thread will execute first, so initialization generally should be performed
    by code outside the kernel invocation.
  id: totrans-1589
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者不应对块或线程的执行顺序做任何假设，这一点非常重要。特别是，没有办法知道哪个块或线程会先执行，因此初始化通常应由内核调用外的代码来执行。
- en: Execution Guarantees and Interblock Synchronization
  id: totrans-1590
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 执行保证与块间同步
- en: Threads within a given thread block are guaranteed to be resident within the
    same SM, so they can communicate via shared memory and synchronize execution using
    intrinsics such as `__syncthreads()`. But thread blocks do not have any similar
    mechanisms for data interchange or synchronization.
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
  zh: 给定线程块中的线程保证会在同一个SM内驻留，因此它们可以通过共享内存进行通信，并使用内在函数如`__syncthreads()`来同步执行。但线程块没有类似的数据交换或同步机制。
- en: More sophisticated CUDA developers may ask, *But what about atomic operations
    in global memory?* Global memory can be updated in a thread-safe manner using
    atomic operations, so it is tempting to build something like a `__syncblocks()`
    function that, like `__syncthreads()`, waits until all blocks in the kernel launch
    have arrived before proceeding. Perhaps it would do an `atomicInc()` on a global
    memory location and, if `atomicInc()` did not return the block count, poll that
    memory location until it did.
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
  zh: 更有经验的CUDA开发者可能会问，*那么全局内存中的原子操作呢？* 可以使用原子操作以线程安全的方式更新全局内存，因此很容易想要构建类似于`__syncblocks()`的函数，像`__syncthreads()`一样，等待内核启动中的所有块到达后再继续执行。也许它会在一个全局内存位置上执行`atomicInc()`，如果`atomicInc()`没有返回块计数，就轮询该内存位置，直到返回结果。
- en: 'The problem is that the execution pattern of the kernel (for example, the mapping
    of thread blocks onto SMs) varies with the hardware configuration. For example,
    the number of SMs—and unless the GPU context is big enough to hold the entire
    grid—*some thread blocks may execute to completion before other thread blocks
    have started running*. The result is deadlock: Because not all blocks are necessarily
    resident in the GPU, the blocks that are polling the shared memory location prevent
    other blocks in the kernel launch from executing.'
  id: totrans-1593
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于内核的执行模式（例如，线程块在SM上的映射）会随着硬件配置的不同而变化。例如，SM的数量——而且除非GPU上下文足够大，能够容纳整个网格——*某些线程块可能在其他线程块还未开始运行时就已经执行完成*。结果是死锁：因为并非所有的块都必然驻留在GPU中，正在轮询共享内存位置的块会阻止内核启动中的其他块执行。
- en: There are a few special cases when interblock synchronization can work. If simple
    mutual exclusion is all that’s desired, `atomicCAS()` certainly can be used to
    provide that. Also, thread blocks can use atomics to signal when they’ve completed,
    so the last thread block in a grid can perform some operation before it exits,
    knowing that all the other thread blocks have completed execution. This strategy
    is employed by the `threadFenceReduction` SDK sample and the `reduction4SinglePass.cu`
    sample that accompanies this book (see [Section 12.2](ch12.html#ch12lev1sec2)).
  id: totrans-1594
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些特殊情况下，块间同步是可行的。如果只需要简单的互斥，`atomicCAS()`当然可以提供这种功能。此外，线程块可以使用原子操作来指示它们何时完成，这样网格中的最后一个线程块在退出之前可以执行某些操作，确保所有其他线程块都已经完成执行。这一策略被`threadFenceReduction`
    SDK示例和本书附带的`reduction4SinglePass.cu`示例所采用（参见[第12.2节](ch12.html#ch12lev1sec2)）。
- en: 7.3.3\. Block and Thread IDs
  id: totrans-1595
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.3\. 块和线程ID
- en: A set of special read-only registers give each thread context in the form of
    a *thread ID* and *block ID*. The thread and block IDs are assigned as a CUDA
    kernel begins execution; for 2D and 3D grids and blocks, they are assigned in
    row-major order.
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
  zh: 一组特殊的只读寄存器为每个线程提供上下文，形式为*线程ID*和*块ID*。线程ID和块ID在CUDA内核开始执行时分配；对于2D和3D网格与线程块，它们按行主序分配。
- en: Thread block sizes are best specified in multiples of 32, since warps are the
    finest possible granularity of execution on the GPU. [Figure 7.4](ch07.html#ch07fig04)
    shows how thread IDs are assigned in 32-thread blocks that are 32Wx1H, 16Wx2H,
    and 8Wx4H, respectively.
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
  zh: 线程块大小最好指定为32的倍数，因为warp是GPU上执行的最细粒度。[图7.4](ch07.html#ch07fig04)展示了如何在32线程线程块中分配线程ID，分别为32Wx1H、16Wx2H和8Wx4H。
- en: '![Image](graphics/07fig04.jpg)'
  id: totrans-1598
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/07fig04.jpg)'
- en: '*Figure 7.4* Blocks of 32 threads.'
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.4* 32线程的线程块。'
- en: For blocks with a thread count that is not a multiple of 32, some warps are
    not fully populated with active threads. [Figure 7.5](ch07.html#ch07fig05) shows
    thread ID assignments for 28-thread blocks that are 28Wx1H, 14Wx2H, and 7Wx4H;
    in each case, 4 threads in the 32-thread warp are inactive for the duration of
    the kernel launch. For any thread block size not divisible by 32, some execution
    resources are wasted, as some warps will be launched with lanes that are disabled
    for the duration of the kernel execution. There is no performance benefit to 2D
    or 3D blocks or grids, but they sometimes make for a better match to the application.
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线程数不是32的倍数的线程块，一些warp并没有完全填充活跃线程。[图7.5](ch07.html#ch07fig05)展示了28线程线程块的线程ID分配，分别为28Wx1H、14Wx2H和7Wx4H；在每种情况下，32线程warp中的4个线程在内核启动期间处于非活跃状态。对于任何线程块大小不是32的倍数的情况，一些执行资源会浪费，因为有些warp会启动时包含禁用的执行通道。在2D或3D线程块或网格的情况下，没有性能提升，但它们有时能更好地匹配应用需求。
- en: '![Image](graphics/07fig05.jpg)'
  id: totrans-1601
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/07fig05.jpg)'
- en: '*Figure 7.5* Blocks of 28 threads.'
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.5* 28线程的线程块。'
- en: The `reportClocks.cu` program illustrates how thread IDs are assigned and how
    warp-based execution works in general ([Listing 7.2](ch07.html#ch07lis02)).
  id: totrans-1603
  prefs: []
  type: TYPE_NORMAL
  zh: '`reportClocks.cu`程序展示了线程ID是如何分配的，以及warp执行如何运作（[列表7.2](ch07.html#ch07lis02)）。'
- en: '*Listing 7.2.* `WriteClockValues` kernel.'
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表7.2.* `WriteClockValues`内核。'
- en: '[Click here to view code image](ch07_images.html#p07lis02a)'
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch07_images.html#p07lis02a)'
- en: '* * *'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: WriteClockValues(
  id: totrans-1608
  prefs: []
  type: TYPE_NORMAL
  zh: WriteClockValues(
- en: unsigned int *completionTimes,
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int *completionTimes,
- en: unsigned int *threadIDs
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int *threadIDs
- en: )
  id: totrans-1611
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '{'
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t globalBlock = blockIdx.x+blockDim.x*
  id: totrans-1613
  prefs: []
  type: TYPE_NORMAL
  zh: size_t globalBlock = blockIdx.x+blockDim.x*
- en: (blockIdx.y+blockDim.y*blockIdx.z);
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: (blockIdx.y+blockDim.y*blockIdx.z);
- en: size_t globalThread = threadIdx.x+blockDim.x*
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
  zh: size_t globalThread = threadIdx.x+blockDim.x*
- en: (threadIdx.y+blockDim.y*threadIdx.z);
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
  zh: (threadIdx.y+blockDim.y*threadIdx.z);
- en: size_t totalBlockSize = blockDim.x*blockDim.y*blockDim.z;
  id: totrans-1617
  prefs: []
  type: TYPE_NORMAL
  zh: size_t totalBlockSize = blockDim.x*blockDim.y*blockDim.z;
- en: size_t globalIndex = globalBlock*totalBlockSize + globalThread;
  id: totrans-1618
  prefs: []
  type: TYPE_NORMAL
  zh: size_t globalIndex = globalBlock*totalBlockSize + globalThread;
- en: completionTimes[globalIndex] = clock();
  id: totrans-1619
  prefs: []
  type: TYPE_NORMAL
  zh: completionTimes[globalIndex] = clock();
- en: threadIDs[globalIndex] = threadIdx.y<<4|threadIdx.x;
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
  zh: threadIDs[globalIndex] = threadIdx.y<<4|threadIdx.x;
- en: '}'
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1622
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '`WriteClockValues()` writes to the two output arrays using a global index computed
    using the block and thread IDs, and the grid and block sizes. One output array
    receives the return value from the `clock()` intrinsic, which returns a high-resolution
    timer value that increments for each warp. In the case of this program, we are
    using `clock()` to identify which warp processed a given value. `clock()` returns
    the value of a per-multiprocessor clock cycle counter, so we normalize the values
    by computing the minimum and subtracting it from all clock cycles values. We call
    the resulting values the thread’s “completion time.”'
  id: totrans-1623
  prefs: []
  type: TYPE_NORMAL
  zh: '`WriteClockValues()` 使用通过块和线程 ID 以及网格和块大小计算的全局索引写入两个输出数组。一个输出数组接收来自 `clock()`
    内建函数的返回值，该函数返回一个高分辨率的定时器值，每次 warp 执行时该值递增。在本程序中，我们使用 `clock()` 来识别哪个 warp 处理了给定的值。`clock()`
    返回每个多处理器时钟周期计数器的值，因此我们通过计算最小值并将其从所有时钟周期值中减去来对值进行归一化处理。我们将得到的值称为线程的“完成时间”。'
- en: Let’s take a look at completion times for threads in a pair of 16Wx8H blocks
    ([Listing 7.3](ch07.html#ch07lis03)) and compare them to completion times for
    14Wx8H blocks ([Listing 7.4](ch07.html#ch07lis04)). As expected, they are grouped
    in 32s, corresponding to the warp size.
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下在一对 16Wx8H 块中的线程完成时间（[清单 7.3](ch07.html#ch07lis03)），并将其与 14Wx8H 块的完成时间（[清单
    7.4](ch07.html#ch07lis04)）进行比较。正如预期的那样，它们按 32 个一组，分别对应于 warp 大小。
- en: '*Listing 7.3.* Completion times (16W×8H blocks).'
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 7.3.* 完成时间（16W×8H 块）。'
- en: '[Click here to view code image](ch07_images.html#p07lis03a)'
  id: totrans-1626
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch07_images.html#p07lis03a)'
- en: '* * *'
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 0.01 ms for 256 threads = 0.03 us/thread
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
  zh: 256 线程的 0.01 毫秒 = 0.03 微秒/线程
- en: 'Completion times (clocks):'
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: 完成时间（时钟）：
- en: 'Grid (0, 0, 0) - slice 0:'
  id: totrans-1630
  prefs: []
  type: TYPE_NORMAL
  zh: 网格 (0, 0, 0) - 切片 0：
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
  zh: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  id: totrans-1632
  prefs: []
  type: TYPE_NORMAL
  zh: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
  zh: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
  zh: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
- en: 8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
  id: totrans-1635
  prefs: []
  type: TYPE_NORMAL
  zh: 8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
- en: 8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
  zh: 8   8   8   8   8   8   8   8   8   8   8   8   8   8   8   8
- en: a   a   a   a   a   a   a   a   a   a   a   a   a   a   a   a
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
  zh: a   a   a   a   a   a   a   a   a   a   a   a   a   a   a   a
- en: a   a   a   a   a   a   a   a   a   a   a   a   a   a   a   a
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
  zh: a   a   a   a   a   a   a   a   a   a   a   a   a   a   a   a
- en: 'Grid (1, 0, 0) - slice 0:'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
  zh: 网格 (1, 0, 0) - 切片 0：
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
  zh: 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
  zh: 0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0
- en: 2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
  id: totrans-1642
  prefs: []
  type: TYPE_NORMAL
  zh: 2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
- en: 2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
  id: totrans-1643
  prefs: []
  type: TYPE_NORMAL
  zh: 2   2   2   2   2   2   2   2   2   2   2   2   2   2   2   2
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  id: totrans-1644
  prefs: []
  type: TYPE_NORMAL
  zh: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
  zh: 4   4   4   4   4   4   4   4   4   4   4   4   4   4   4   4
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1646
  prefs: []
  type: TYPE_NORMAL
  zh: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
  zh: 6   6   6   6   6   6   6   6   6   6   6   6   6   6   6   6
- en: 'Thread IDs:'
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
  zh: 线程 ID：
- en: 'Grid (0, 0, 0) - slice 0:'
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
  zh: 网格 (0, 0, 0) - 切片 0：
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d   e   f
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
  zh: 0   1   2   3   4   5   6   7   8   9   a   b   c   d   e   f
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d  1e  1f
  id: totrans-1651
  prefs: []
  type: TYPE_NORMAL
  zh: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d  1e  1f
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d  2e  2f
  id: totrans-1652
  prefs: []
  type: TYPE_NORMAL
  zh: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d  2e  2f
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d  3e  3f
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
  zh: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d  3e  3f
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d  4e  4f
  id: totrans-1654
  prefs: []
  type: TYPE_NORMAL
  zh: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d  4e  4f
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d  5e  5f
  id: totrans-1655
  prefs: []
  type: TYPE_NORMAL
  zh: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d  5e  5f
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d  6e  6f
  id: totrans-1656
  prefs: []
  type: TYPE_NORMAL
  zh: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d  6e  6f
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d  7e  7f
  id: totrans-1657
  prefs: []
  type: TYPE_NORMAL
  zh: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d  7e  7f
- en: 'Grid (1, 0, 0) - slice 0:'
  id: totrans-1658
  prefs: []
  type: TYPE_NORMAL
  zh: 网格 (1, 0, 0) - 切片 0：
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d   e   f
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
  zh: 0   1   2   3   4   5   6   7   8   9   a   b   c   d   e   f
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d  1e  1f
  id: totrans-1660
  prefs: []
  type: TYPE_NORMAL
  zh: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d  1e  1f
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d  2e  2f
  id: totrans-1661
  prefs: []
  type: TYPE_NORMAL
  zh: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d  2e  2f
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d  3e  3f
  id: totrans-1662
  prefs: []
  type: TYPE_NORMAL
  zh: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d  3e  3f
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d  4e  4f
  id: totrans-1663
  prefs: []
  type: TYPE_NORMAL
  zh: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d  4e  4f
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d  5e  5f
  id: totrans-1664
  prefs: []
  type: TYPE_NORMAL
  zh: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d  5e  5f
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d  6e  6f
  id: totrans-1665
  prefs: []
  type: TYPE_NORMAL
  zh: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d  6e  6f
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d  7e  7f
  id: totrans-1666
  prefs: []
  type: TYPE_NORMAL
  zh: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d  7e  7f
- en: '* * *'
  id: totrans-1667
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 7.4.* Completion times (14W×8H blocks).'
  id: totrans-1668
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 7.4.* 完成时间（14W×8H 块）。'
- en: '[Click here to view code image](ch07_images.html#p07lis04a)'
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch07_images.html#p07lis04a)'
- en: '* * *'
  id: totrans-1670
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Completion times (clocks):'
  id: totrans-1671
  prefs: []
  type: TYPE_NORMAL
  zh: 完成时间（时钟）：
- en: 'Grid (0, 0, 0) - slice 0:'
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
  zh: 网格 (0, 0, 0) - 切片 0：
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1673
  prefs: []
  type: TYPE_NORMAL
  zh: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
  zh: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
- en: 6   6   6   6   8   8   8   8   8   8   8   8   8   8
  id: totrans-1675
  prefs: []
  type: TYPE_NORMAL
  zh: 6   6   6   6   8   8   8   8   8   8   8   8   8   8
- en: 8   8   8   8   8   8   8   8   8   8   8   8   8   8
  id: totrans-1676
  prefs: []
  type: TYPE_NORMAL
  zh: 8   8   8   8   8   8   8   8   8   8   8   8   8   8
- en: 8   8   8   8   8   8   8   8   a   a   a   a   a   a
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
  zh: 8   8   8   8   8   8   8   8   a   a   a   a   a   a
- en: a   a   a   a   a   a   a   a   a   a   a   a   a   a
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
  zh: a   a   a   a   a   a   a   a   a   a   a   a   a   a
- en: a   a   a   a   a   a   a   a   a   a   a   a   c   c
  id: totrans-1679
  prefs: []
  type: TYPE_NORMAL
  zh: a   a   a   a   a   a   a   a   a   a   a   a   c   c
- en: c   c   c   c   c   c   c   c   c   c   c   c   c   c
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
  zh: c   c   c   c   c   c   c   c   c   c   c   c   c   c
- en: 'Grid (1, 0, 0) - slice 0:'
  id: totrans-1681
  prefs: []
  type: TYPE_NORMAL
  zh: 网格 (1, 0, 0) - 切片 0：
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
  zh: 0   0   0   0   0   0   0   0   0   0   0   0   0   0
- en: 0   0   0   0   0   0   0   0   0   0   0   0   0   0
  id: totrans-1683
  prefs: []
  type: TYPE_NORMAL
  zh: 0   0   0   0   0   0   0   0   0   0   0   0   0   0
- en: 0   0   0   0   2   2   2   2   2   2   2   2   2   2
  id: totrans-1684
  prefs: []
  type: TYPE_NORMAL
  zh: 0   0   0   0   2   2   2   2   2   2   2   2   2   2
- en: 2   2   2   2   2   2   2   2   2   2   2   2   2   2
  id: totrans-1685
  prefs: []
  type: TYPE_NORMAL
  zh: 2   2   2   2   2   2   2   2   2   2   2   2   2   2
- en: 2   2   2   2   2   2   2   2   4   4   4   4   4   4
  id: totrans-1686
  prefs: []
  type: TYPE_NORMAL
  zh: 2   2   2   2   2   2   2   2   4   4   4   4   4   4
- en: 4   4   4   4   4   4   4   4   4   4   4   4   4   4
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
  zh: 4   4   4   4   4   4   4   4   4   4   4   4   4   4
- en: 4   4   4   4   4   4   4   4   4   4   4   4   6   6
  id: totrans-1688
  prefs: []
  type: TYPE_NORMAL
  zh: 4   4   4   4   4   4   4   4   4   4   4   4   6   6
- en: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
  zh: 6   6   6   6   6   6   6   6   6   6   6   6   6   6
- en: 'Thread IDs:'
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
  zh: 线程ID：
- en: 'Grid (0, 0, 0) - slice 0:'
  id: totrans-1691
  prefs: []
  type: TYPE_NORMAL
  zh: 网格 (0, 0, 0) - 切片 0：
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
  zh: 0   1   2   3   4   5   6   7   8   9   a   b   c   d
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d
  id: totrans-1693
  prefs: []
  type: TYPE_NORMAL
  zh: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d
  id: totrans-1694
  prefs: []
  type: TYPE_NORMAL
  zh: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d
  id: totrans-1695
  prefs: []
  type: TYPE_NORMAL
  zh: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d
  id: totrans-1696
  prefs: []
  type: TYPE_NORMAL
  zh: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d
  id: totrans-1697
  prefs: []
  type: TYPE_NORMAL
  zh: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d
  id: totrans-1698
  prefs: []
  type: TYPE_NORMAL
  zh: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
  zh: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d
- en: 'Grid (1, 0, 0) - slice 0:'
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
  zh: 网格 (1, 0, 0) - 切片 0：
- en: 0   1   2   3   4   5   6   7   8   9   a   b   c   d
  id: totrans-1701
  prefs: []
  type: TYPE_NORMAL
  zh: 0   1   2   3   4   5   6   7   8   9   a   b   c   d
- en: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d
  id: totrans-1702
  prefs: []
  type: TYPE_NORMAL
  zh: 10  11  12  13  14  15  16  17  18  19  1a  1b  1c  1d
- en: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d
  id: totrans-1703
  prefs: []
  type: TYPE_NORMAL
  zh: 20  21  22  23  24  25  26  27  28  29  2a  2b  2c  2d
- en: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d
  id: totrans-1704
  prefs: []
  type: TYPE_NORMAL
  zh: 30  31  32  33  34  35  36  37  38  39  3a  3b  3c  3d
- en: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d
  id: totrans-1705
  prefs: []
  type: TYPE_NORMAL
  zh: 40  41  42  43  44  45  46  47  48  49  4a  4b  4c  4d
- en: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d
  id: totrans-1706
  prefs: []
  type: TYPE_NORMAL
  zh: 50  51  52  53  54  55  56  57  58  59  5a  5b  5c  5d
- en: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d
  id: totrans-1707
  prefs: []
  type: TYPE_NORMAL
  zh: 60  61  62  63  64  65  66  67  68  69  6a  6b  6c  6d
- en: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d
  id: totrans-1708
  prefs: []
  type: TYPE_NORMAL
  zh: 70  71  72  73  74  75  76  77  78  79  7a  7b  7c  7d
- en: '* * *'
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The completion times for the 14Wx8H blocks given in [Listing 7.4](ch07.html#ch07lis04)
    underscore how the thread IDs map to warps. In the case of the 14Wx8H blocks,
    every warp holds only 28 threads; 12.5% of the number of possible thread lanes
    are idle throughout the kernel’s execution. To avoid this waste, developers always
    should try to make sure blocks contain a multiple of 32 threads.
  id: totrans-1710
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 7.4](ch07.html#ch07lis04)中给出的14Wx8H块的完成时间强调了线程ID如何映射到波段。在14Wx8H块的情况下，每个波段仅包含28个线程；在整个内核执行过程中，12.5%的线程通道处于空闲状态。为了避免这种浪费，开发者始终应该确保块的线程数是32的倍数。'
- en: 7.4\. Occupancy
  id: totrans-1711
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4. 占用率
- en: Occupancy is a ratio that measures the number of threads/SM that *will* run
    in a given kernel launch, as opposed to the maximum number of threads that *potentially
    could be running* on that SM.
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
  zh: 占用率是一个衡量给定内核启动时每个SM上运行线程数的比率，区别于该SM上“潜在可以运行”的最大线程数。
- en: '![Image](graphics/220equ01.jpg)'
  id: totrans-1713
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/220equ01.jpg)'
- en: The denominator (maximum warps per SM) is a constant that depends only on the
    compute capability of the device. The numerator of this expression, which determines
    the occupancy, is a function of the following.
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
  zh: 分母（每个 SM 的最大 warp 数）是一个常量，仅取决于设备的计算能力。该表达式的分子（决定占用率）是以下因素的函数。
- en: • Compute capability (1.0, 1.1, 1.2, 1.3, 2.0, 2.1, 3.0, 3.5)
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
  zh: • 计算能力（1.0、1.1、1.2、1.3、2.0、2.1、3.0、3.5）
- en: • Threads per block
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个块的线程数
- en: • Registers per thread
  id: totrans-1717
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个线程的寄存器数量
- en: • Shared memory configuration^([9](ch07.html#ch07fn9))
  id: totrans-1718
  prefs: []
  type: TYPE_NORMAL
  zh: • 共享内存配置^([9](ch07.html#ch07fn9))
- en: '[9](ch07.html#ch07fn9a). For SM 2.x and later only. Developers can split the
    64K L1 cache in the SM as 16K shared/48K L1 or 48K shared/16K L1\. (SM 3.x adds
    the ability to split the cache evenly as 32K shared/32K L1.)'
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
  zh: '[9](ch07.html#ch07fn9a). 仅适用于 SM 2.x 及更高版本。开发人员可以将 SM 中的 64K L1 缓存分为 16K 共享/48K
    L1 或 48K 共享/16K L1。 （SM 3.x 增加了将缓存均分为 32K 共享/32K L1 的能力。）'
- en: • Shared memory per block
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个块的共享内存
- en: To help developers assess the tradeoffs between these parameters, the CUDA Toolkit
    includes an occupancy calculator in the form of an Excel spreadsheet.^([10](ch07.html#ch07fn10))
    Given the inputs above, the spreadsheet will calculate the following results.
  id: totrans-1721
  prefs: []
  type: TYPE_NORMAL
  zh: 为帮助开发人员评估这些参数之间的权衡，CUDA 工具包包括一个 Excel 电子表格形式的占用率计算器。^([10](ch07.html#ch07fn10))
    给定上述输入，电子表格将计算以下结果。
- en: '[10](ch07.html#ch07fn10a). Typically it is in the tools subdirectory—for example,
    `%CUDA_PATH%/tools` (Windows) or `$CUDA_PATH/tools`.'
  id: totrans-1722
  prefs: []
  type: TYPE_NORMAL
  zh: '[10](ch07.html#ch07fn10a). 通常它位于工具子目录中——例如，`%CUDA_PATH%/tools`（Windows）或 `$CUDA_PATH/tools`。'
- en: • Active thread count
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
  zh: • 活跃线程数
- en: • Active warp count
  id: totrans-1724
  prefs: []
  type: TYPE_NORMAL
  zh: • 活跃 warp 数
- en: • Active block count
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
  zh: • 活跃块数
- en: • Occupancy (active warp count divided into the hardware’s maximum number of
    active warps)
  id: totrans-1726
  prefs: []
  type: TYPE_NORMAL
  zh: • 占用率（活跃 warp 数除以硬件的最大活跃 warp 数）
- en: The spreadsheet also identifies whichever parameter is limiting the occupancy.
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
  zh: 该电子表格还会标明哪个参数限制了占用率。
- en: • Registers per multiprocessor
  id: totrans-1728
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个多处理器的寄存器数量
- en: • Maximum number of warps or blocks per multiprocessor
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个多处理器的最大 warp 或块数
- en: • Shared memory per multiprocessor
  id: totrans-1730
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个多处理器的共享内存
- en: Note that occupancy is not the be-all and end-all of CUDA performance;^([11](ch07.html#ch07fn11))
    often it is better to use more registers per thread and rely on instruction-level
    parallelism (ILP) to deliver performance. NVIDIA has a good presentation on warps
    and occupancy that discusses the tradeoffs.^([12](ch07.html#ch07fn12))
  id: totrans-1731
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，占用率并不是 CUDA 性能的全部；^([11](ch07.html#ch07fn11)) 通常，使用每个线程更多的寄存器并依赖于指令级并行（ILP）来提高性能会更好。NVIDIA
    有一份关于 warp 和占用率的优秀演示，讨论了这些权衡。^([12](ch07.html#ch07fn12))
- en: '[11](ch07.html#ch07fn11a). Vasily Volkov emphatically makes this point in his
    presentation “Better Performance at Lower Occupancy.” It is available at [http://bit.ly/YdScNG](http://bit.ly/YdScNG).'
  id: totrans-1732
  prefs: []
  type: TYPE_NORMAL
  zh: '[11](ch07.html#ch07fn11a). Vasily Volkov 在他的演示“更低占用率下的更好性能”中强调了这一点。该演示可以在 [http://bit.ly/YdScNG](http://bit.ly/YdScNG)
    上找到。'
- en: '[12](ch07.html#ch07fn12a). [http://bit.ly/WHTb5m](http://bit.ly/WHTb5m)'
  id: totrans-1733
  prefs: []
  type: TYPE_NORMAL
  zh: '[12](ch07.html#ch07fn12a). [http://bit.ly/WHTb5m](http://bit.ly/WHTb5m)'
- en: An example of a low-occupancy kernel that can achieve near-maximum global memory
    bandwidth is given in [Section 5.2.10](ch05.html#ch05lev2sec18) ([Listing 5.5](ch05.html#ch05lis05)).
    The inner loop of the `GlobalReads` kernel can be unrolled according to a template
    parameter; as the number of unrolled iterations increases, the number of needed
    registers increases and the occupancy goes down. For the Tesla M2050’s in the
    `cg1.4xlarge` instance type, for example, the peak read bandwidth reported (with
    ECC disabled) is 124GiB/s, with occupancy of 66%. Volkov reports achieving near-peak
    memory bandwidth when running kernels whose occupancy is in the single digits.
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
  zh: 一个低占用内核的示例可以在[第5.2.10节](ch05.html#ch05lev2sec18)（[代码清单5.5](ch05.html#ch05lis05)）中找到，它能够实现接近最大值的全局内存带宽。`GlobalReads`内核的内层循环可以根据模板参数进行展开；随着展开迭代次数的增加，所需的寄存器数量也增加，占用率则下降。例如，对于`cg1.4xlarge`实例类型中的Tesla
    M2050，报告的峰值读取带宽（禁用ECC时）为124GiB/s，占用率为66%。Volkov报告称，当内核的占用率为个位数时，能够接近峰值内存带宽。
- en: 7.5\. Dynamic Parallelism
  id: totrans-1735
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5. 动态并行性
- en: '*Dynamic parallelism*, a new capability that works only on SM 3.5–class hardware,
    enables CUDA kernels to launch other CUDA kernels, and also to invoke various
    functions in the CUDA runtime. When using dynamic parallelism, a subset of the
    CUDA runtime (known as the *device runtime*) becomes available for use by threads
    running on the device.'
  id: totrans-1736
  prefs: []
  type: TYPE_NORMAL
  zh: '*动态并行性*，这是一项仅在SM 3.5及以上硬件上有效的新功能，允许CUDA内核启动其他CUDA内核，并调用CUDA运行时中的各种函数。使用动态并行性时，CUDA运行时的一个子集（称为*设备运行时*）将可供设备上运行的线程使用。'
- en: Dynamic parallelism introduces the idea of “parent” and “child” grids. Any kernel
    invoked by another CUDA kernel (as opposed to host code, as done in all previous
    CUDA versions) is a “child kernel,” and the invoking grid is its “parent.” By
    default, CUDA supports two (2) nesting levels (one for the parent and one for
    the child), a number that may be increased by calling `cudaSetDeviceLimit()` with
    `cudaLimitDevRuntimeSyncDepth`.
  id: totrans-1737
  prefs: []
  type: TYPE_NORMAL
  zh: 动态并行性引入了“父”网格和“子”网格的概念。任何由另一个CUDA内核调用的内核（与以前所有CUDA版本中通过主机代码调用不同）都被称为“子内核”，而调用的网格则是其“父网格”。默认情况下，CUDA支持两个（2）嵌套层级（一个用于父网格，一个用于子网格），这个数字可以通过调用`cudaSetDeviceLimit()`并设置`cudaLimitDevRuntimeSyncDepth`来增加。
- en: Dynamic parallelism was designed to address applications that previously had
    to deliver results to the CPU so the CPU could specify which work to perform on
    the GPU. Such “handshaking” disrupts CPU/GPU concurrency in the execution pipeline
    described in [Section 2.5.1](ch02.html#ch02lev2sec11), in which the CPU produces
    commands for consumption by the GPU. The GPU’s time is too valuable for it to
    wait for the CPU to read and analyze results before issuing more work. Dynamic
    parallelism avoids these pipeline bubbles by enabling the GPU to launch work for
    itself from kernels.
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
  zh: 动态并行性旨在解决以前需要将结果传送到 CPU 以便 CPU 指定要在 GPU 上执行的工作的问题。这种“握手”打乱了 [第 2.5.1 节](ch02.html#ch02lev2sec11)中描述的
    CPU/GPU 并行执行管线，其中 CPU 生成供 GPU 消费的命令。GPU 的时间非常宝贵，不可能等到 CPU 读取并分析结果之后再发布更多的工作。动态并行性通过使
    GPU 从内核启动自己的工作，避免了这些管线空闲时间。
- en: Dynamic parallelism can improve performance in several cases.
  id: totrans-1739
  prefs: []
  type: TYPE_NORMAL
  zh: 动态并行性在多个情况下可以提高性能。
- en: • It enables initialization of data structures needed by a kernel before the
    kernel can begin execution. Previously, such initialization had to be taken care
    of in host code or by previously invoking a separate kernel.
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
  zh: • 它支持在内核开始执行之前初始化内核所需的数据结构。以前，这种初始化必须在主机代码中处理，或者通过先前调用一个独立的内核来完成。
- en: • It enables simplified recursion for applications such as Barnes-Hut gravitational
    integration or hierarchical grid evaluation for aerodynamic simulations.
  id: totrans-1741
  prefs: []
  type: TYPE_NORMAL
  zh: • 它简化了递归过程，适用于诸如 Barnes-Hut 引力积分或用于气动模拟的分层网格评估等应用。
- en: '* * *'
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-1743
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Dynamic parallelism only works within a given GPU. Kernels can invoke memory
    copies or other kernels, but they cannot submit work to other GPUs.
  id: totrans-1744
  prefs: []
  type: TYPE_NORMAL
  zh: 动态并行性仅在给定的 GPU 内部有效。内核可以调用内存复制或其他内核，但不能将工作提交给其他 GPU。
- en: '* * *'
  id: totrans-1745
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 7.5.1\. Scoping and Synchronization
  id: totrans-1746
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.1\. 范围和同步
- en: 'With the notable exception of block and grid size, child grids inherit most
    kernel configuration parameters, such as the shared memory configuration (set
    by `cudaDeviceSetCacheConfig()`), from their parents. Thread blocks are a unit
    of scope: Streams and events created by a thread block can only be used by that
    thread block (they are not even inherited for use by child grids), and they are
    automatically destroyed when the thread block exits.'
  id: totrans-1747
  prefs: []
  type: TYPE_NORMAL
  zh: 除了块和网格大小的显著例外，子网格会继承大部分内核配置参数，例如共享内存配置（由 `cudaDeviceSetCacheConfig()` 设置），这些参数会从父网格继承。线程块是一个作用域单位：由线程块创建的流和事件只能被该线程块使用（它们甚至不会被子网格继承使用），并且当线程块退出时，它们会自动销毁。
- en: '* * *'
  id: totrans-1748
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-1749
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Resources created on the device via dynamic parallelism are strictly separated
    from resources created on the host. Streams and events created on the host may
    not be used on the device via dynamic parallelism, and vice versa.
  id: totrans-1750
  prefs: []
  type: TYPE_NORMAL
  zh: 通过动态并行性在设备上创建的资源与在主机上创建的资源严格分离。主机上创建的流和事件不能通过动态并行性在设备上使用，反之亦然。
- en: '* * *'
  id: totrans-1751
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: CUDA guarantees that a parent grid is not considered complete until all of its
    children have finished. Although the parent may execute concurrently with the
    child, there is no guarantee that a child grid will begin execution until its
    parent calls `cudaDeviceSynchronize().`
  id: totrans-1752
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 保证父网格在所有子网格完成之前不会被视为完成。尽管父网格可能与子网格并行执行，但不能保证子网格在父网格调用`cudaDeviceSynchronize()`之前开始执行。
- en: If all threads in a thread block exit, execution of the thread block is suspended
    until all child grids have finished. If that synchronization is not sufficient,
    developers can use CUDA streams and events for explicit synchronization. As on
    the host, operations within a given stream are performed in the order of submission.
    Operations can only execute concurrently if they are specified in different streams,
    and there is no guarantee that operations will, in fact, execute concurrently.
    If needed, synchronization primitives such as `__syncthreads()` can be used to
    coordinate the order of submission to a given stream.
  id: totrans-1753
  prefs: []
  type: TYPE_NORMAL
  zh: 如果线程块中的所有线程都退出，则线程块的执行会暂停，直到所有子网格完成。如果该同步不足够，开发者可以使用 CUDA 流和事件进行显式同步。与主机上类似，给定流中的操作会按提交顺序执行。只有在指定不同的流时，操作才能并行执行，但不能保证操作实际上会并行执行。如有需要，可以使用同步原语（例如`__syncthreads()`）来协调对给定流的提交顺序。
- en: '* * *'
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Streams and events created on the device may not be used outside the thread
    block that created them.
  id: totrans-1756
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备上创建的流和事件可能无法在创建它们的线程块之外使用。
- en: '* * *'
  id: totrans-1757
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '`cudaDeviceSynchronize()` synchronizes on all pending work launched by any
    thread in the thread block. It does not, however, perform any interthread synchronization,
    so if there is a desire to synchronize on work launched by other threads, developers
    must use `__syncthreads()` or other block-level synchronization primitives (see
    [Section 8.6.2](ch08.html#ch08lev2sec21)).'
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaDeviceSynchronize()`会同步线程块中所有线程启动的所有待处理工作。然而，它不会执行任何线程间同步，因此如果需要同步其他线程启动的工作，开发者必须使用`__syncthreads()`或其他块级同步原语（见[第8.6.2节](ch08.html#ch08lev2sec21)）。'
- en: 7.5.2\. Memory Model
  id: totrans-1759
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.2\. 内存模型
- en: Parent and child grids share the same global and constant memory storage, but
    they have distinct local and shared memory.
  id: totrans-1760
  prefs: []
  type: TYPE_NORMAL
  zh: 父子网格共享相同的全局和常量内存存储，但它们具有不同的本地和共享内存。
- en: Global Memory
  id: totrans-1761
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 全局内存
- en: 'There are two points in the execution of a child grid when its view of memory
    is fully consistent with the parent grid: when the child grid is invoked by the
    parent and when the child grid completes as signaled by a synchronization API
    invocation in the parent thread.'
  id: totrans-1762
  prefs: []
  type: TYPE_NORMAL
  zh: 在子网格的执行过程中，有两个时刻它对内存的视图与父网格完全一致：当子网格被父网格调用时，以及当子网格通过父线程中的同步 API 调用完成时。
- en: All global memory operations in the parent thread prior to the child thread’s
    invocation are visible to the child grid. All memory operations of the child grid
    are visible to the parent after the parent has synchronized on the child grid’s
    completion. Zero-copy memory has the same coherence and consistency guarantees
    as global memory.
  id: totrans-1763
  prefs: []
  type: TYPE_NORMAL
  zh: 在子线程调用之前，父线程中的所有全局内存操作对子网格是可见的。子网格的所有内存操作在父线程同步子网格完成后对父线程是可见的。零拷贝内存具有与全局内存相同的可见性和一致性保证。
- en: Constant Memory
  id: totrans-1764
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 常量内存
- en: Constants are immutable and may not be modified from the device during kernel
    execution. Taking the address of a constant memory object from within a kernel
    thread has the same semantics as for all CUDA programs,^([13](ch07.html#ch07fn13))
    and passing that pointer between parents and their children is fully supported.
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
  zh: 常量是不可变的，在内核执行过程中无法修改。通过内核线程获取常量内存对象的地址与所有CUDA程序中的语义相同，^([13](ch07.html#ch07fn13))，并且父子线程之间传递该指针是完全支持的。
- en: '[13](ch07.html#ch07fn13a). Note that in device code, the address must be taken
    with the “address-of” operator (unary operator&), since `cudaGetSymbolAddress()`
    is not supported by the device runtime.'
  id: totrans-1766
  prefs: []
  type: TYPE_NORMAL
  zh: '[13](ch07.html#ch07fn13a)。请注意，在设备代码中，必须使用“取地址”操作符（单目运算符&）来获取地址，因为`cudaGetSymbolAddress()`不被设备运行时支持。'
- en: Shared and Local Memory
  id: totrans-1767
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 共享内存和本地内存
- en: Shared and local memory is private to a thread block or thread, respectively,
    and is not visible or coherent between parent and child. When an object in one
    of these locations is referenced outside its scope, the behavior is undefined
    and would likely cause an error.
  id: totrans-1768
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存和本地内存分别是线程块或线程的私有内存，它们在父子线程之间不可见且不一致。当引用一个位于这些位置的对象且超出其作用域时，行为是未定义的，并且可能导致错误。
- en: If `nvcc` detects an attempt to misuse a pointer to shared or local memory,
    it will issue a warning. Developers can use the `__isGlobal()` intrinsic to determine
    whether a given pointer references global memory. Pointers to shared or local
    memory are not valid parameters to `cudaMemcpy*Async()` or `cudaMemset*Async()`.
  id: totrans-1769
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`nvcc`检测到滥用指向共享内存或本地内存的指针，它会发出警告。开发者可以使用`__isGlobal()`内建函数来判断给定的指针是否引用了全局内存。指向共享内存或本地内存的指针不能作为`cudaMemcpy*Async()`或`cudaMemset*Async()`的有效参数。
- en: Local Memory
  id: totrans-1770
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 本地内存
- en: Local memory is private storage for an executing thread and is not visible outside
    of that thread. It is illegal to pass a pointer to local memory as a launch argument
    when launching a child kernel. The result of dereferencing such a local memory
    pointer from a child will be undefined. To guarantee that this rule is not inadvertently
    violated by the compiler, all storage passed to a child kernel should be allocated
    explicitly from the global memory heap.
  id: totrans-1771
  prefs: []
  type: TYPE_NORMAL
  zh: 本地内存是执行线程的私有存储，在线程外部不可见。在启动子内核时，将指向本地内存的指针作为启动参数是非法的。从子线程解引用这样的本地内存指针将导致未定义的结果。为了确保编译器不会无意中违反这一规则，所有传递给子内核的存储应显式从全局内存堆中分配。
- en: Texture Memory
  id: totrans-1772
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 纹理内存
- en: Concurrent accesses by parent and child may result in inconsistent data and
    should be avoided. That said, a degree of coherency between parent and child is
    enforced by the runtime. A child kernel can use texturing to access memory written
    by its parent, but writes to memory by a child will not be reflected in the texture
    memory accesses by a parent until *after* the parent synchronizes on the child’s
    completion. Texture objects are well supported in the device runtime. They cannot
    be created or destroyed, but they can be passed in and used by any grid in the
    hierarchy (parent or child).
  id: totrans-1773
  prefs: []
  type: TYPE_NORMAL
  zh: 父进程和子进程的并发访问可能导致数据不一致，应避免这种情况。也就是说，运行时会强制父进程和子进程之间保持一定的连贯性。子内核可以使用纹理访问父进程写入的内存，但子进程对内存的写入在父进程的纹理内存访问中不会反映出来，*直到*父进程在子进程完成后进行同步。设备运行时很好地支持纹理对象。纹理对象不能被创建或销毁，但可以在层级结构中的任何网格（父进程或子进程）中传递并使用。
- en: 7.5.3\. Streams and Events
  id: totrans-1774
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.3\. 流和事件
- en: Streams and events created by the device runtime can be used only within the
    thread block that created the stream. The NULL stream has different semantics
    in the device runtime than in the host runtime. On the host, synchronizing with
    the NULL stream forces a “join” of all the other streamed operations on the GPU
    (as described in [Section 6.2.3](ch06.html#ch06lev2sec4)); on the device, the
    NULL stream is its own stream, and any interstream synchronization must be performed
    using events.
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
  zh: 由设备运行时创建的流和事件只能在创建该流的线程块内使用。NULL流在设备运行时的语义与主机运行时不同。在主机上，同步NULL流会强制所有其他GPU上的流操作“合并”（如[第6.2.3节](ch06.html#ch06lev2sec4)所述）；在设备上，NULL流是它自己的流，任何流间同步必须通过事件来实现。
- en: When using the device runtime, streams must be created with the `cudaStream-NonBlocking`
    flag (a parameter to `cudaStreamCreateWithFlags()`). The `cudaStreamSynchronize()`
    call is not supported; synchronization must be implemented in terms of events
    and `cudaStreamWaitEvent()`.
  id: totrans-1776
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用设备运行时时，流必须使用`cudaStream-NonBlocking`标志创建（这是`cudaStreamCreateWithFlags()`的一个参数）。`cudaStreamSynchronize()`调用不受支持；同步必须通过事件和`cudaStreamWaitEvent()`来实现。
- en: Only the interstream synchronization capabilities of CUDA events are supported.
    As a consequence, `cudaEventSynchronize()`, `cudaEventElapsedTime()`, and `cudaEventQuery()`
    are not supported. Additionally, because timing is not supported, events must
    be created by passing the `cudaEventDisableTiming` flag to `cudaEventCreateWithFlags()`.
  id: totrans-1777
  prefs: []
  type: TYPE_NORMAL
  zh: 仅支持CUDA事件的流间同步能力。因此，`cudaEventSynchronize()`、`cudaEventElapsedTime()`和`cudaEventQuery()`不受支持。此外，由于不支持计时，事件必须通过传递`cudaEventDisableTiming`标志来创建，方法是调用`cudaEventCreateWithFlags()`。
- en: 7.5.4\. Error Handling
  id: totrans-1778
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.4\. 错误处理
- en: Any function in the device runtime may return an error (`cudaError_t`). The
    error is recorded in a per-thread slot that can be queried by calling `cudaGetLastError()`.
    As with the host-based runtime, CUDA makes a distinction between errors that can
    be returned immediately (e.g., if an invalid parameter is passed to a memcpy function)
    and errors that must be reported asynchronously (e.g., if a launch performed an
    invalid memory access). If a child grid causes an error at runtime, CUDA will
    return an error to the host, not to the parent grid.
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
  zh: 设备运行时中的任何函数可能会返回错误（`cudaError_t`）。错误会记录在每个线程的槽位中，可以通过调用`cudaGetLastError()`进行查询。与基于主机的运行时一样，CUDA区分可以立即返回的错误（例如，如果传递给memcpy函数的参数无效）和必须异步报告的错误（例如，如果启动执行了无效的内存访问）。如果子网格在运行时发生错误，CUDA会将错误返回给主机，而不是返回给父网格。
- en: 7.5.5\. Compiling and Linking
  id: totrans-1780
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.5\. 编译与链接
- en: Unlike the host runtime, developers must explicitly link against the device
    runtime’s static library when using the device runtime. On Windows, the device
    runtime is `cudadevrt.lib`; on Linux and MacOS, it is `cudadevrt.a`. When building
    with `nvcc`, this may be accomplished by appending `-lcudadevrt` to the command
    line.
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
  zh: 与主机运行时不同，开发人员在使用设备运行时时必须显式地链接设备运行时的静态库。在Windows上，设备运行时是`cudadevrt.lib`；在Linux和MacOS上，是`cudadevrt.a`。使用`nvcc`进行构建时，可以通过将`-lcudadevrt`附加到命令行来完成此操作。
- en: 7.5.6\. Resource Management
  id: totrans-1782
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.6\. 资源管理
- en: Whenever a kernel launches a child grid, the child is considered a new *nesting
    level*, and the total number of levels is the *nesting depth* of the program.
    In contrast, the deepest level at which the program will explicitly synchronize
    on a child launch is called the *synchronization depth*. Typically the synchronization
    depth is one less than the nesting depth of the program, but if the program does
    not always need to call `cudaDeviceSynchronize()`, then it may be substantially
    less than the nesting depth.
  id: totrans-1783
  prefs: []
  type: TYPE_NORMAL
  zh: 每当内核启动一个子网格时，子网格就被视为一个新的*嵌套级别*，程序的总级别即为程序的*嵌套深度*。相反，程序将在子启动时显式同步的最深级别称为*同步深度*。通常，同步深度比程序的嵌套深度少1，但如果程序不总是需要调用`cudaDeviceSynchronize()`，则同步深度可能远小于嵌套深度。
- en: The theoretical maximum nesting depth is 24, but in practice it is governed
    by the device limit `cudaLimitDevRuntimeSyncDepth`. Any launch that would result
    in a kernel at a deeper level than the maximum will fail. The default maximum
    synchronization depth level is 2\. The limits must be configured before the top-level
    kernel is launched from the host.
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上的最大嵌套深度为24，但实际上它受到设备限制`cudaLimitDevRuntimeSyncDepth`的约束。任何导致内核嵌套深度超过最大值的启动都会失败。默认的最大同步深度为2。限制必须在从主机启动顶级内核之前进行配置。
- en: '* * *'
  id: totrans-1785
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-1786
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Calling a device runtime function such as `cudaMemcpyAsync()` may invoke a kernel,
    increasing the nesting depth by 1.
  id: totrans-1787
  prefs: []
  type: TYPE_NORMAL
  zh: 调用设备运行时函数，例如`cudaMemcpyAsync()`，可能会调用一个内核，从而将嵌套深度增加1。
- en: '* * *'
  id: totrans-1788
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: For parent kernels that never call `cudaDeviceSynchronize()`, the system does
    not have to reserve space for the parent kernel. In this case, the memory footprint
    required for a program will be much less than the conservative maximum. Such a
    program could specify a shallower maximum synchronization depth to avoid overallocation
    of backing store.
  id: totrans-1789
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从不调用`cudaDeviceSynchronize()`的父内核，系统不需要为父内核保留空间。在这种情况下，程序所需的内存占用将远小于保守的最大值。这样的程序可以指定较浅的最大同步深度，以避免过度分配后备存储。
- en: Memory Footprint
  id: totrans-1790
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内存占用
- en: The device runtime system software reserves device memory for the following
    purposes.
  id: totrans-1791
  prefs: []
  type: TYPE_NORMAL
  zh: 设备运行时系统软件为以下目的保留设备内存。
- en: • To track pending grid launches
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
  zh: • 跟踪待处理的网格启动
- en: • To save saving parent-grid state during synchronization
  id: totrans-1793
  prefs: []
  type: TYPE_NORMAL
  zh: • 在同步过程中保存父网格状态
- en: • To serve as an allocation heap for `malloc()` and `cudaMalloc()` calls from
    kernels
  id: totrans-1794
  prefs: []
  type: TYPE_NORMAL
  zh: • 作为`malloc()`和`cudaMalloc()`调用的分配堆
- en: This memory is not available for use by the application, so some applications
    may wish to reduce the default allocations, and some applications may have to
    increase the default values in order to operate correctly. To change the default
    values, developers call `cudaDeviceSetLimit()`, as summarized in [Table 7.3](ch07.html#ch07tab03).
    The limit `cudaLimitDevRuntimeSyncDepth` is especially important, since each nesting
    level costs up to 150MB of device memory.
  id: totrans-1795
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内存无法被应用程序使用，因此一些应用程序可能希望减少默认分配，而另一些应用程序可能需要增加默认值以确保正确运行。要更改默认值，开发者调用`cudaDeviceSetLimit()`，具体信息总结在[表
    7.3](ch07.html#ch07tab03)中。`cudaLimitDevRuntimeSyncDepth`的限制尤为重要，因为每一层嵌套最多需要150MB的设备内存。
- en: '![Image](graphics/07tab03.jpg)'
  id: totrans-1796
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/07tab03.jpg)'
- en: '*Table 7.3* `cudaDeviceSetLimit()` Values'
  id: totrans-1797
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 7.3* `cudaDeviceSetLimit()` 值'
- en: Pending Kernel Launches
  id: totrans-1798
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 待处理内核启动
- en: When a kernel is launched, all associated configuration and parameter data is
    tracked until the kernel completes. This data is stored within a system-managed
    launch pool. The size of the launch pool is configurable by calling `cudaDeviceSetLimit()`
    from the host and specifying `cudaLimitDevRuntimePendingLaunchCount`.
  id: totrans-1799
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个内核被启动时，所有相关的配置和参数数据都会被追踪，直到内核完成。此数据存储在由系统管理的启动池中。启动池的大小可以通过主机调用`cudaDeviceSetLimit()`并指定`cudaLimitDevRuntimePendingLaunchCount`来配置。
- en: Configuration Options
  id: totrans-1800
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 配置选项
- en: Resource allocation for the device runtime system software is controlled via
    the `cudaDeviceSetLimit()` API from the host program. Limits must be set before
    any kernel is launched and may not be changed while the GPU is actively running
    programs.
  id: totrans-1801
  prefs: []
  type: TYPE_NORMAL
  zh: 设备运行时系统软件的资源分配由主机程序通过`cudaDeviceSetLimit()` API进行控制。限制必须在任何内核启动之前设置，并且在GPU正在运行程序时不能更改。
- en: Memory allocated by the device runtime must be freed by the device runtime.
    Also, memory is allocated by the device runtime out of a preallocated heap whose
    size is specified by the device limit `cudaLimitMallocHeapSize`. The named limits
    in [Table 7.3](ch07.html#ch07tab03) may be set.
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
  zh: 由设备运行时分配的内存必须由设备运行时释放。此外，设备运行时从一个预分配的堆中分配内存，该堆的大小由设备限制`cudaLimitMallocHeapSize`指定。[表7.3](ch07.html#ch07tab03)中列出的命名限制可以设置。
- en: '![Image](graphics/07tab04.jpg)'
  id: totrans-1803
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/07tab04.jpg)'
- en: '*Table 7.4* Device Runtime Limitations'
  id: totrans-1804
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.4* 设备运行时限制'
- en: 7.5.7\. Summary
  id: totrans-1805
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.7\. 总结
- en: '[Table 7.4](ch07.html#ch07tab04) summarizes the key differences and limitations
    between the device runtime and the host runtime. [Table 7.5](ch07.html#ch07tab05)
    lists the subset of functions that may be called from the device runtime, along
    with any pertinent limitations.'
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
  zh: '[表7.4](ch07.html#ch07tab04)总结了设备运行时和主机运行时之间的主要区别和限制。[表7.5](ch07.html#ch07tab05)列出了可以从设备运行时调用的函数子集，并附有相关限制。'
- en: '![Image](graphics/07tab05.jpg)![Image](graphics/07tab05a.jpg)'
  id: totrans-1807
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/07tab05.jpg)![Image](graphics/07tab05a.jpg)'
- en: '*Table 7.5.* CUDA Device Runtime Functions'
  id: totrans-1808
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.5.* CUDA设备运行时函数'
- en: Chapter 8\. Streaming Multiprocessors
  id: totrans-1809
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章 流处理多处理器
- en: The streaming multiprocessors (SMs) are the part of the GPU that runs our CUDA
    kernels. Each SM contains the following.
  id: totrans-1810
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理多处理器（SM）是运行我们CUDA内核的GPU部分。每个SM包含以下内容：
- en: • Thousands of registers that can be partitioned among threads of execution
  id: totrans-1811
  prefs: []
  type: TYPE_NORMAL
  zh: • 成千上万的寄存器，可以在执行线程之间进行划分
- en: '• Several caches:'
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
  zh: • 多个缓存：
- en: – *Shared memory* for fast data interchange between threads
  id: totrans-1813
  prefs: []
  type: TYPE_NORMAL
  zh: – *共享内存*，用于线程之间的快速数据交换
- en: – *Constant cache* for fast broadcast of reads from constant memory
  id: totrans-1814
  prefs: []
  type: TYPE_NORMAL
  zh: – *常量缓存*，用于快速广播来自常量内存的读取
- en: – *Texture cache* to aggregate bandwidth from texture memory
  id: totrans-1815
  prefs: []
  type: TYPE_NORMAL
  zh: – *纹理缓存*，用于聚合来自纹理内存的带宽
- en: – *L1 cache* to reduce latency to local or global memory
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
  zh: – *L1缓存*，用于减少访问本地或全局内存的延迟
- en: • *Warp schedulers* that can quickly switch contexts between threads and issue
    instructions to warps that are ready to execute
  id: totrans-1817
  prefs: []
  type: TYPE_NORMAL
  zh: • *Warp调度器*，可以在线程之间快速切换上下文，并向准备好执行的warp发出指令
- en: '• Execution cores for integer and floating-point operations:'
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
  zh: • 执行核心，用于整数和浮点操作：
- en: – Integer and single-precision floating point operations
  id: totrans-1819
  prefs: []
  type: TYPE_NORMAL
  zh: – 整数和单精度浮点数操作
- en: – Double-precision floating point
  id: totrans-1820
  prefs: []
  type: TYPE_NORMAL
  zh: – 双精度浮点数
- en: – Special Function Units (SFUs) for single-precision floating-point transcendental
    functions
  id: totrans-1821
  prefs: []
  type: TYPE_NORMAL
  zh: – 用于单精度浮点数超越函数的特殊功能单元（SFU）
- en: The reason there are many registers and the reason the hardware can context
    switch between threads so efficiently are to maximize the throughput of the hardware.
    The GPU is designed to have enough state to cover both execution latency and the
    memory latency of hundreds of clock cycles that it may take for data from device
    memory to arrive after a read instruction is executed.
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多寄存器，以及硬件能够高效地在线程之间进行上下文切换的原因，是为了最大化硬件的吞吐量。GPU被设计成有足够的状态，以覆盖执行延迟和内存延迟——即设备内存中的数据在执行读取指令后，可能需要数百个时钟周期才能到达。
- en: 'The SMs are general-purpose processors, but they are designed very differently
    than the execution cores in CPUs: They target much lower clock rates; they support
    instruction-level parallelism, but not branch prediction or speculative execution;
    and they have less cache, if they have any cache at all. For suitable workloads,
    the sheer computing horsepower in a GPU more than makes up for these disadvantages.'
  id: totrans-1823
  prefs: []
  type: TYPE_NORMAL
  zh: SM是通用处理器，但它们的设计与CPU中的执行核心有很大的不同：它们的目标时钟频率较低；它们支持指令级并行性，但不支持分支预测或推测执行；并且它们的缓存较少，甚至可能没有缓存。对于合适的工作负载，GPU的强大计算能力完全可以弥补这些缺点。
- en: The design of the SM has been evolving rapidly since the introduction of the
    first CUDA-capable hardware in 2006, with three major revisions, codenamed Tesla,
    Fermi, and Kepler. Developers can query the compute capability by calling `cudaGetDeviceProperties()`
    and examining `cudaDeviceProp.major` and `cudaDeviceProp.minor`, or by calling
    the driver API function `cuDeviceComputeCapability()`. Compute capability 1.x,
    2.x, and 3.x correspond to Tesla-class, Fermi-class, and Kepler-class hardware,
    respectively. [Table 8.1](ch08.html#ch08tab01) summarizes the capabilities added
    in each generation of the SM hardware.
  id: totrans-1824
  prefs: []
  type: TYPE_NORMAL
  zh: SM的设计自2006年首次推出支持CUDA的硬件以来，迅速发展，经历了三次主要的修订，代号分别为Tesla、Fermi和Kepler。开发者可以通过调用`cudaGetDeviceProperties()`并检查`cudaDeviceProp.major`和`cudaDeviceProp.minor`，或者调用驱动程序API函数`cuDeviceComputeCapability()`来查询计算能力。计算能力1.x、2.x和3.x分别对应Tesla类、Fermi类和Kepler类硬件。[表8.1](ch08.html#ch08tab01)总结了每一代SM硬件所新增的功能。
- en: '![Image](graphics/08tab01.jpg)'
  id: totrans-1825
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/08tab01.jpg)'
- en: '*Table 8.1* SM Capabilities'
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
  zh: '*表8.1* SM功能'
- en: In [Chapter 2](ch02.html#ch02), [Figures 2.29](ch02.html#ch02fig29) through
    [2.32](ch02.html#ch02fig32) show block diagrams of different SMs. CUDA cores can
    execute integer and single-precision floating-point instructions; one double-precision
    unit implements double-precision support, if available; and Special Function Units
    implement reciprocal, recriprocal square root, sine/cosine, and logarithm/exponential
    functions. Warp schedulers dispatch instructions to these execution units as the
    resources needed to execute the instruction become available.
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](ch02.html#ch02)，[图2.29](ch02.html#ch02fig29)至[2.32](ch02.html#ch02fig32)展示了不同SM的框图。CUDA核心可以执行整数和单精度浮点指令；一个双精度单元实现双精度支持（如果可用）；特殊功能单元实现倒数、倒数平方根、正弦/余弦以及对数/指数函数。当执行指令所需的资源可用时，Warp调度器会将指令分配到这些执行单元。
- en: This chapter focuses on the instruction set capabilities of the SM. As such,
    it sometimes refers to the “SASS” instructions, the native instructions into which
    `ptxas` or the CUDA driver translate intermediate PTX code. Developers are not
    able to author SASS code directly; instead, NVIDIA has made these instructions
    visible to developers through the `cuobjdump` utility so they can direct optimizations
    of their source code by examining the compiled microcode.
  id: totrans-1828
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍 SM 的指令集能力。因此，它有时会提到“SASS”指令，即 `ptxas` 或 CUDA 驱动程序将中间 PTX 代码转换为的本机指令。开发人员无法直接编写
    SASS 代码；相反，NVIDIA 通过 `cuobjdump` 工具使这些指令对开发人员可见，以便他们通过检查编译后的微代码来指导源代码的优化。
- en: 8.1\. Memory
  id: totrans-1829
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 内存
- en: 8.1.1\. Registers
  id: totrans-1830
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.1\. 寄存器
- en: Each SM contains thousands of 32-bit registers that are allocated to threads
    as specified when the kernel is launched. Registers are both the fastest and most
    plentiful memory in the SM. As an example, the Kepler-class (SM 3.0) SMX contains
    65,536 registers or 256K, while the texture cache is only 48K.
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 SM 包含数千个 32 位寄存器，这些寄存器在内核启动时根据指定分配给线程。寄存器是 SM 中速度最快且数量最多的内存。例如，Kepler 类（SM
    3.0）SMX 包含 65,536 个寄存器或 256K，而纹理缓存仅为 48K。
- en: CUDA registers can contain integer or floating-point data; for hardware capable
    of performing double-precision arithmetic (SM 1.3 and higher), the operands are
    contained in even-valued register pairs. On SM 2.0 and higher hardware, register
    pairs also can hold 64-bit addresses.
  id: totrans-1832
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 寄存器可以包含整数或浮点数据；对于能够执行双精度算术运算的硬件（SM 1.3 及更高版本），操作数包含在偶数值的寄存器对中。在 SM 2.0
    及更高版本的硬件上，寄存器对还可以保存 64 位地址。
- en: 'CUDA hardware also supports wider memory transactions: The built-in `int2/float2`
    and `int4/float4` data types, residing in aligned register pairs or quads, respectively,
    may be read or written using single 64- or 128-bit-wide loads or stores. Once
    in registers, the individual data elements can be referenced as `.x/.y` (for `int2/float2`)
    or `.x/.y/.z/.w` (for `int4/float4`).'
  id: totrans-1833
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 硬件还支持更宽的内存事务：内置的 `int2/float2` 和 `int4/float4` 数据类型，分别驻留在对齐的寄存器对或四元组中，可以使用单个
    64 位或 128 位宽的加载或存储进行读取或写入。一旦在寄存器中，单个数据元素可以引用为 `.x/.y`（对于 `int2/float2`）或 `.x/.y/.z/.w`（对于
    `int4/float4`）。
- en: Developers can cause `nvcc` to report the number of registers used by a kernel
    by specifying the command-line option `--ptxas-options -–verbose`. The number
    of registers used by a kernel affects the number of threads that can fit in an
    SM and often must be tuned carefully for optimal performance. The maximum number
    of registers used for a compilation may be specified with `--ptxas-options --maxregcount
    N`.
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以通过指定命令行选项 `--ptxas-options -–verbose` 来使 `nvcc` 报告内核使用的寄存器数量。内核使用的寄存器数量会影响可以适应于
    SM 的线程数量，通常必须仔细调整以获得最佳性能。可以使用 `--ptxas-options --maxregcount N` 指定编译时使用的最大寄存器数量。
- en: Register Aliasing
  id: totrans-1835
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 寄存器别名
- en: Because registers can hold floating-point or integer data, some intrinsics serve
    only to coerce the compiler into changing its view of a variable. The `__int_as_float()`
    and `__float_as_int()` intrinsics cause a variable to “change personalities” between
    32-bit integer and single-precision floating point.
  id: totrans-1836
  prefs: []
  type: TYPE_NORMAL
  zh: 由于寄存器可以存储浮点数或整数数据，一些内建函数仅用于强制编译器改变变量的视图。`__int_as_float()` 和 `__float_as_int()`
    这两个内建函数使得变量在32位整数和单精度浮点数之间“切换身份”。
- en: float __int_as_float( int i );
  id: totrans-1837
  prefs: []
  type: TYPE_NORMAL
  zh: float __int_as_float( int i );
- en: int __float_as_int( float f );
  id: totrans-1838
  prefs: []
  type: TYPE_NORMAL
  zh: int __float_as_int( float f );
- en: The `__double2loint()`, `__double2hiint()`, and `__hiloint2double()` intrinsics
    similarly cause registers to change personality (usually in-place). `__double_as_longlong()`
    and `__longlong_as_double()` coerce register pairs in-place; `__double2loint()`
    and `__double2hiint()` return the least and the most significant 32 bits of the
    input operand, respectively; and `__hiloint2double()` constructs a `double` out
    of the high and low halves.
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
  zh: '`__double2loint()`、`__double2hiint()` 和 `__hiloint2double()` 这些内建函数类似地使得寄存器切换身份（通常是就地转换）。`__double_as_longlong()`
    和 `__longlong_as_double()` 使寄存器对就地转换；`__double2loint()` 和 `__double2hiint()` 分别返回输入操作数的最低和最高32位；而
    `__hiloint2double()` 则将高低两部分组合成一个`double`。'
- en: '[Click here to view code image](ch08_images.html#p234pro02a)'
  id: totrans-1840
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch08_images.html#p234pro02a)'
- en: int double2loint( double d );
  id: totrans-1841
  prefs: []
  type: TYPE_NORMAL
  zh: int double2loint( double d );
- en: int double2hiint( double d );
  id: totrans-1842
  prefs: []
  type: TYPE_NORMAL
  zh: int double2hiint( double d );
- en: int hiloint2double( int hi, int lo );
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
  zh: int hiloint2double( int hi, int lo );
- en: double long_as_double(long long int i );
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
  zh: double long_as_double(long long int i );
- en: long long int __double_as_longlong( double d );
  id: totrans-1845
  prefs: []
  type: TYPE_NORMAL
  zh: long long int __double_as_longlong( double d );
- en: 8.1.2\. Local Memory
  id: totrans-1846
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.2\. 局部内存
- en: 'Local memory is used to spill registers and also to hold local variables that
    are indexed and whose indices cannot be computed at compile time. Local memory
    is backed by the same pool of device memory as global memory, so it exhibits the
    same latency characteristics and benefits as the L1 and L2 cache hierarchy on
    Fermi and later hardware. Local memory is addressed in such a way that the memory
    transactions are automatically coalesced. The hardware includes special instructions
    to load and store local memory: The SASS variants are `LLD/LST` for Tesla and
    `LDL/STL` for Fermi and Kepler.'
  id: totrans-1847
  prefs: []
  type: TYPE_NORMAL
  zh: 局部内存用于溢出寄存器，还用于存放那些被索引且索引在编译时无法计算的局部变量。局部内存与全局内存共享同一设备内存池，因此它具有与Fermi及更高版本硬件上的L1和L2缓存层次结构相同的延迟特性和优点。局部内存的地址方式使得内存事务能够自动合并。硬件包含了专门的指令来加载和存储局部内存：Tesla的SASS变种为`LLD/LST`，而Fermi和Kepler的SASS变种为`LDL/STL`。
- en: 8.1.3\. Global Memory
  id: totrans-1848
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.3\. 全局内存
- en: The SMs can read or write global memory using `GLD/GST` instructions (on Tesla)
    and `LD/ST` instructions (on Fermi and Kepler). Developers can use standard C
    operators to compute and dereference addresses, including pointer arithmetic and
    the dereferencing operators `*`, `[]`, and `->`. Operating on 64- or 128-bit built-in
    data types (`int2/float2/int4/float4`) automatically causes the compiler to issue
    64- or 128-bit load and store instructions. Maximum memory performance is achieved
    through *coalescing* of memory transactions, described in [Section 5.2.9](ch05.html#ch05lev2sec17).
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
  zh: SM 可以通过 `GLD/GST` 指令（在特斯拉上）和 `LD/ST` 指令（在费米和凯普勒上）读取或写入全局内存。开发者可以使用标准 C 运算符来计算和解引用地址，包括指针运算和解引用运算符
    `*`、`[]` 和 `->`。对 64 位或 128 位内建数据类型（`int2/float2/int4/float4`）的操作会自动导致编译器发出 64
    位或 128 位的加载和存储指令。通过*合并*内存事务可以实现最大内存性能，详见[第 5.2.9 节](ch05.html#ch05lev2sec17)。
- en: Tesla-class hardware (SM 1.x) uses special address registers to hold pointers;
    later hardware implements a load/store architecture that uses the same register
    file for pointers; integer and floating-point values; and the same address space
    for constant memory, shared memory, and global memory.^([1](ch08.html#ch08fn1))
  id: totrans-1850
  prefs: []
  type: TYPE_NORMAL
  zh: 特斯拉级硬件（SM 1.x）使用特殊的地址寄存器来保存指针；后来的硬件实现了一个加载/存储架构，使用相同的寄存器文件来存储指针、整数和浮点值，并且为常量内存、共享内存和全局内存提供相同的地址空间。^([1](ch08.html#ch08fn1))
- en: '[1](ch08.html#ch08fn1a). Both constant and shared memory exist in address windows
    that enable them to be referenced by 32-bit addresses even on 64-bit architectures.'
  id: totrans-1851
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch08.html#ch08fn1a). 常量内存和共享内存都存在于地址窗口中，使得它们即使在 64 位架构上也可以通过 32 位地址进行引用。'
- en: Fermi-class hardware includes several features not available on older hardware.
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
  zh: 费米级硬件包括一些旧硬件所不具备的特性。
- en: • 64-bit addressing is supported via “wide” load/store instructions in which
    addresses are held in even-numbered register pairs. 64-bit addressing is not supported
    on 32-bit host platforms; on 64-bit host platforms, 64-bit addressing is enabled
    automatically. As a result, code generated for the same kernels compiled for 32-
    and 64-bit host platforms may have different register counts and performance.
  id: totrans-1853
  prefs: []
  type: TYPE_NORMAL
  zh: • 64 位寻址通过“宽”加载/存储指令得到支持，其中地址存储在偶数编号的寄存器对中。32 位主机平台不支持 64 位寻址；在 64 位主机平台上，64
    位寻址会自动启用。因此，为 32 位和 64 位主机平台编译的相同内核生成的代码可能具有不同的寄存器计数和性能。
- en: • The L1 cache may be configured to be 16K or 48K in size.^([2](ch08.html#ch08fn2))
    (Kepler added the ability to split the cache as 32K L1/32K shared.) Load instructions
    can include cacheability hints (to tell the hardware to pull the read into L1
    or to bypass the L1 and keep the data only in L2). These may be accessed via inline
    PTX or through the command line option `–X ptxas –dlcm=ca` (cache in L1 and L2,
    the default setting) or `–X ptxas –dlcm=cg` (cache only in L2).
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
  zh: • L1缓存的大小可以配置为16K或48K。^([2](ch08.html#ch08fn2))（凯普勒架构新增了将缓存分割为32K L1/32K共享缓存的功能。）加载指令可以包含缓存性提示（告诉硬件将读取的数据拉入L1缓存，或绕过L1缓存并仅将数据保留在L2缓存中）。这些可以通过内联PTX或通过命令行选项`–X
    ptxas –dlcm=ca`（在L1和L2缓存中缓存，默认设置）或`–X ptxas –dlcm=cg`（仅在L2缓存中缓存）来访问。
- en: '[2](ch08.html#ch08fn2a). The hardware can change this configuration per kernel
    launch, but changing this state is expensive and will break concurrency for concurrent
    kernel launches.'
  id: totrans-1855
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch08.html#ch08fn2a). 硬件可以在每次内核启动时更改此配置，但更改此状态是昂贵的，并且会破坏并发内核启动时的并发性。'
- en: Atomic operations (or just “atomics”) update a memory location in a way that
    works correctly even when multiple GPU threads are operating on the same memory
    location. The hardware enforces mutual exclusion on the memory location for the
    duration of the operation. Since the order of operations is not guaranteed, the
    operators supported generally are associative.^([3](ch08.html#ch08fn3))
  id: totrans-1856
  prefs: []
  type: TYPE_NORMAL
  zh: 原子操作（或简称“原子操作”）以一种即使在多个GPU线程操作同一内存位置时也能正确工作的方式更新内存位置。硬件会在操作过程中强制执行对内存位置的互斥访问。由于操作的顺序无法保证，因此一般支持的操作符是具有结合性的。^([3](ch08.html#ch08fn3))
- en: '[3](ch08.html#ch08fn3a). The only exception is single-precision floating-point
    addition. Then again, floating-point code generally must be robust in the face
    of the lack of associativity of floating-point operations; porting to different
    hardware, or even just recompiling the same code with different compiler options,
    can change the order of floating-point operations and thus the result.'
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch08.html#ch08fn3a). 唯一的例外是单精度浮动点加法。另一方面，浮动点代码通常必须应对浮动点操作不具结合性的问题；移植到不同的硬件，甚至仅仅是重新编译相同的代码并使用不同的编译器选项，都可能会改变浮动点操作的顺序，从而改变结果。'
- en: Atomics first became available for global memory for SM 1.1 and greater and
    for shared memory for SM 1.2 and greater. Until the Kepler generation of hardware,
    however, global memory atomics were too slow to be useful.
  id: totrans-1858
  prefs: []
  type: TYPE_NORMAL
  zh: 原子操作首次可用于全局内存（适用于SM 1.1及更高版本）和共享内存（适用于SM 1.2及更高版本）。然而，直到凯普勒（Kepler）架构的硬件问世之前，全局内存原子操作的速度太慢，无法实际使用。
- en: The global atomic intrinsics, summarized in [Table 8.2](ch08.html#ch08tab02),
    become automatically available when the appropriate architecture is specified
    to `nvcc` via `--gpu-architecture`. All of these intrinsics can operate on 32-bit
    integers. 64-bit support for `atomicAdd()`, `atomicExch()`, and `atomicCAS()`
    was added in SM 1.2\. `atomicAdd()` of 32-bit floating-point values (`float`)
    was added in SM 2.0\. 64-bit support for `atomicMin()`, `atomicMax()`, `atomicAnd()`,
    `atomicOr()`, and `atomicXor()` was added in SM 3.5.
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
  zh: 全局原子内建函数，如[表 8.2](ch08.html#ch08tab02)所总结的，当通过`--gpu-architecture`在`nvcc`中指定适当的架构时会自动可用。所有这些内建函数都可以操作32位整数。`atomicAdd()`、`atomicExch()`和`atomicCAS()`的64位支持是在
    SM 1.2 中添加的。32位浮点值（`float`）的`atomicAdd()`是在 SM 2.0 中添加的。`atomicMin()`、`atomicMax()`、`atomicAnd()`、`atomicOr()`和`atomicXor()`的64位支持是在
    SM 3.5 中添加的。
- en: '![Image](graphics/08tab02.jpg)'
  id: totrans-1860
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/08tab02.jpg)'
- en: '*Table 8.2* Atomic Operations'
  id: totrans-1861
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.2* 原子操作'
- en: '* * *'
  id: totrans-1862
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-1863
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Because atomic operations are implemented using hardware in the GPU’s integrated
    memory controller, they do not work across the PCI Express bus and thus do not
    work correctly on device memory pointers that correspond to host memory or peer
    memory.
  id: totrans-1864
  prefs: []
  type: TYPE_NORMAL
  zh: 由于原子操作是通过 GPU 集成内存控制器中的硬件实现的，它们无法跨 PCI Express 总线工作，因此无法在对应主机内存或对等内存的设备内存指针上正确工作。
- en: '* * *'
  id: totrans-1865
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'At the hardware level, atomics come in two forms: atomic operations that return
    the value that was at the specified memory location before the operator was performed,
    and reduction operations that the developer can “fire and forget” at the memory
    location, ignoring the return value. Since the hardware can perform the operation
    more efficiently if there is no need to return the old value, the compiler detects
    whether the return value is used and, if it is not, emits different instructions.
    In SM 2.0, for example, the instructions are called `ATOM` and `RED`, respectively.'
  id: totrans-1866
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件层面，原子操作有两种形式：一种是原子操作，它返回在执行操作之前指定内存位置的值；另一种是归约操作，开发人员可以在内存位置上执行“触发并忘记”操作，而忽略返回值。由于如果不需要返回旧值，硬件可以更高效地执行操作，因此编译器会检测返回值是否被使用，如果未被使用，则会发出不同的指令。例如，在
    SM 2.0 中，这些指令分别称为`ATOM`和`RED`。
- en: 8.1.4\. Constant Memory
  id: totrans-1867
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.4\. 常量内存
- en: Constant memory resides in device memory, but it is backed by a different, read-only
    cache that is optimized to broadcast the results of read requests to threads that
    all reference the same memory location. Each SM contains a small, latency-optimized
    cache for purposes of servicing these read requests. Making the memory (and the
    cache) read-only simplifies cache management, since the hardware has no need to
    implement write-back policies to deal with memory that has been updated.
  id: totrans-1868
  prefs: []
  type: TYPE_NORMAL
  zh: 常量内存位于设备内存中，但它由一个不同的只读缓存支持，该缓存经过优化，可以将读取请求的结果广播到所有引用相同内存位置的线程。每个 SM 都包含一个小型的、优化延迟的缓存，用于处理这些读取请求。将内存（及其缓存）设置为只读可以简化缓存管理，因为硬件不需要实现写回策略来处理已经更新的内存。
- en: SM 2.x and subsequent hardware includes a special optimization for memory that
    is not denoted as constant but that the compiler has identified as (1) read-only
    and (2) whose address is not dependent on the block or thread ID. The “load uniform”
    (LDU) instruction reads memory using the constant cache hierarchy and broadcasts
    the data to the threads.
  id: totrans-1869
  prefs: []
  type: TYPE_NORMAL
  zh: 'SM 2.x及后续硬件包括一种特殊的内存优化，用于那些没有标记为常量，但编译器已识别为（1）只读并且（2）其地址不依赖于块或线程ID的内存。“加载统一”（LDU）指令使用常量缓存层次结构读取内存，并将数据广播到线程。  '
- en: 8.1.5\. Shared Memory
  id: totrans-1870
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '8.1.5\. 共享内存  '
- en: 'Shared memory is very fast, on-chip memory in the SM that threads can use for
    data interchange within a thread block. Since it is a per-SM resource, shared
    memory usage can affect occupancy, the number of warps that the SM can keep resident.
    SMs load and store shared memory with special instructions: `G2R/R2G` on SM 1.x,
    and `LDS/STS` on SM 2.x and later.'
  id: totrans-1871
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存是SM中非常快速的片上内存，线程可以用于线程块内的数据交换。由于它是每个SM的资源，共享内存的使用会影响占用率，即SM可以保持驻留的warp数量。SM使用特殊的指令来加载和存储共享内存：在SM
    1.x上是`G2R/R2G`，在SM 2.x及以后版本上是`LDS/STS`。
- en: Shared memory is arranged as interleaved *banks* and generally is optimized
    for 32-bit access. If more than one thread in a warp references the same bank,
    a *bank conflict* occurs, and the hardware must handle memory requests consecutively
    until all requests have been serviced. Typically, to avoid bank conflicts, applications
    access shared memory with an interleaved pattern based on the thread ID, such
    as the following.
  id: totrans-1872
  prefs: []
  type: TYPE_NORMAL
  zh: '共享内存被组织为交错的*内存银行*，并通常针对32位访问进行优化。如果warp中的多个线程引用同一内存银行，就会发生*内存银行冲突*，硬件必须按顺序处理内存请求，直到所有请求都得到服务。通常，为了避免银行冲突，应用程序通过基于线程ID的交错模式访问共享内存，如下所示。  '
- en: '[Click here to view code image](ch08_images.html#p238pro01a)'
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch08_images.html#p238pro01a)  '
- en: extern __shared__ float shared[];
  id: totrans-1874
  prefs: []
  type: TYPE_NORMAL
  zh: 'extern __shared__ float shared[];  '
- en: float data = shared[BaseIndex + threadIdx.x];
  id: totrans-1875
  prefs: []
  type: TYPE_NORMAL
  zh: 'float data = shared[BaseIndex + threadIdx.x];  '
- en: Having all threads in a warp read from the same 32-bit shared memory location
    also is fast. The hardware includes a broadcast mechanism to optimize for this
    case. Writes to the same bank are serialized by the hardware, reducing performance.
    Writes to the same *address* cause race conditions and should be avoided.
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
  zh: '让同一warp中的所有线程读取同一个32位共享内存位置也是非常快速的。硬件包括一个广播机制来优化这种情况。对同一内存银行的写入会被硬件序列化，从而降低性能。对相同*地址*的写入会引发竞争条件，应避免发生这种情况。  '
- en: For 2D access patterns (such as tiles of pixels in an image processing kernel),
    it’s good practice to pad the shared memory allocation so the kernel can reference
    adjacent rows without causing bank conflicts. SM 2.x and subsequent hardware has
    32 banks,^([4](ch08.html#ch08fn4)) so for 2D tiles where threads in the same warp
    may access the data by row, it is a good strategy to pad the tile size to a multiple
    of 33 32-bit words.
  id: totrans-1877
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 2D 访问模式（例如图像处理内核中的像素块），一个好的做法是对共享内存分配进行填充，以便内核可以引用相邻的行而不造成银行冲突。SM 2.x 及更高版本的硬件有
    32 个内存银行，^([4](ch08.html#ch08fn4))因此，对于 2D 瓷砖，其中同一 warp 中的线程可能按行访问数据，最好将瓷砖大小填充为
    33 个 32 位字的倍数。
- en: '[4](ch08.html#ch08fn4a). SM 1.x hardware had 16 banks (memory traffic from
    the first 16 threads and the second 16 threads of a warp was serviced separately),
    but strategies that work well on subsequent hardware also work well on SM 1.x.'
  id: totrans-1878
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch08.html#ch08fn4a)。SM 1.x 硬件有 16 个内存银行（一个 warp 的前 16 个线程和后 16 个线程的内存流量是分别服务的），但在后续硬件上有效的策略同样适用于
    SM 1.x。'
- en: On SM 1.x hardware, shared memory is about 16K in size;^([5](ch08.html#ch08fn5))
    on later hardware, there is a total of 64K of L1 cache that may be configured
    as 16K or 48K of shared memory, of which the remainder is used as L1 cache.^([6](ch08.html#ch08fn6))
  id: totrans-1879
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SM 1.x 硬件上，共享内存的大小约为 16K；^([5](ch08.html#ch08fn5))在后续硬件上，共有 64K 的 L1 缓存，可以配置为
    16K 或 48K 的共享内存，其余部分用作 L1 缓存。^([6](ch08.html#ch08fn6))
- en: '[5](ch08.html#ch08fn5a). 256 bytes of shared memory was reserved for parameter
    passing; in SM 2.x and later, parameters are passed via constant memory.'
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch08.html#ch08fn5a)。256 字节的共享内存用于参数传递；在 SM 2.x 及更高版本中，参数通过常量内存传递。'
- en: '[6](ch08.html#ch08fn6a). SM 3.x hardware adds the ability to split the cache
    evenly as 32K L1/32K shared.'
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](ch08.html#ch08fn6a)。SM 3.x 硬件增加了将缓存平均拆分为 32K L1/32K 共享内存的能力。'
- en: 'Over the last few generations of hardware, NVIDIA has improved the hardware’s
    handling of operand sizes other than 32 bits. On SM 1.x hardware, 8- and 16-bit
    reads from the same bank caused bank conflicts, while SM 2.x and later hardware
    can broadcast reads of any size out of the same bank. Similarly, 64-bit operands
    (such as `double`) in shared memory were so much slower than 32-bit operands on
    SM 1.x that developers sometimes had to resort to storing the data as separate
    high and low halves. SM 3.x hardware adds a new feature for kernels that predominantly
    use 64-bit operands in shared memory: a mode that increases the bank size to 64
    bits.'
  id: totrans-1882
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几代硬件中，NVIDIA 改进了硬件对非 32 位操作数大小的处理。在 SM 1.x 硬件上，来自同一内存银行的 8 位和 16 位读取会导致银行冲突，而
    SM 2.x 及更高版本的硬件可以在同一内存银行中广播任何大小的读取。同样，SM 1.x 上的共享内存中的 64 位操作数（例如 `double`）比 32
    位操作数慢得多，以至于开发人员有时不得不将数据存储为分开的高半部分和低半部分。SM 3.x 硬件为主要使用 64 位操作数的内核提供了一种新特性：将银行大小增加到
    64 位的模式。
- en: Atomics in Shared Memory
  id: totrans-1883
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 共享内存中的原子操作
- en: SM 1.2 added the ability to perform atomic operations in shared memory. Unlike
    global memory, which implements atomics using single instructions (either `GATOM`
    or `GRED`, depending on whether the return value is used), shared memory atomics
    are implemented with explicit lock/unlock semantics, and the compiler emits code
    that causes each thread to loop over these lock operations until the thread has
    performed its atomic operation.
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
  zh: SM 1.2 增加了在共享内存中执行原子操作的能力。与全局内存不同，全局内存通过单条指令（根据是否使用返回值，使用`GATOM`或`GRED`）来实现原子操作，而共享内存的原子操作通过显式的锁/解锁语义实现，编译器生成的代码使得每个线程循环执行这些锁操作，直到线程完成其原子操作。
- en: '[Listing 8.1](ch08.html#ch08lis01) gives the source code to `atomic32Shared.cu`,
    a program specifically intended to be compiled to highlight the code generation
    for shared memory atomics. [Listing 8.2](ch08.html#ch08lis02) shows the resulting
    microcode generated for SM 2.0\. Note how the `LDSLK` (load shared with lock)
    instruction returns a predicate that tells whether the lock was acquired, the
    code to perform the update is predicated, and the code loops until the lock is
    acquired and the update performed.'
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8.1](ch08.html#ch08lis01)给出了`atomic32Shared.cu`的源代码，这是一个专门为展示共享内存原子操作代码生成而编写的程序。[清单
    8.2](ch08.html#ch08lis02)展示了为SM 2.0生成的微代码。请注意，`LDSLK`（带锁的加载共享）指令返回一个谓词，告诉是否获得了锁，更新代码会根据这个谓词执行，代码会循环，直到锁被获得并且更新操作执行完成。'
- en: The lock is performed per 32-bit word, and the index of the lock is determined
    by bits 2–9 of the shared memory address. Take care to avoid contention, or the
    loop in [Listing 8.2](ch08.html#ch08lis02) may iterate up to 32 times.
  id: totrans-1886
  prefs: []
  type: TYPE_NORMAL
  zh: 锁是按每个32位字进行的，锁的索引由共享内存地址的第2至第9位确定。注意避免争用，否则[清单 8.2](ch08.html#ch08lis02)中的循环可能会最多迭代32次。
- en: '*Listing 8.1.* `atomic32Shared.cu`.'
  id: totrans-1887
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8.1.* `atomic32Shared.cu`。'
- en: '[Click here to view code image](ch08_images.html#p08lis01a)'
  id: totrans-1888
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch08_images.html#p08lis01a)'
- en: '* * *'
  id: totrans-1889
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: Return32( int *sum, int *out, const int *pIn )
  id: totrans-1891
  prefs: []
  type: TYPE_NORMAL
  zh: Return32( int *sum, int *out, const int *pIn )
- en: '{'
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ int s[];
  id: totrans-1893
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ int s[];
- en: s[threadIdx.x] = pIn[threadIdx.x];
  id: totrans-1894
  prefs: []
  type: TYPE_NORMAL
  zh: s[threadIdx.x] = pIn[threadIdx.x];
- en: __syncthreads();
  id: totrans-1895
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: (void) atomicAdd( &s[threadIdx.x], *pIn );
  id: totrans-1896
  prefs: []
  type: TYPE_NORMAL
  zh: (void) atomicAdd( &s[threadIdx.x], *pIn );
- en: __syncthreads();
  id: totrans-1897
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: out[threadIdx.x] = s[threadIdx.x];
  id: totrans-1898
  prefs: []
  type: TYPE_NORMAL
  zh: out[threadIdx.x] = s[threadIdx.x];
- en: '}'
  id: totrans-1899
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1900
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 8.2.* `atomic32Shared.cubin` (microcode compiled for SM 2.0).'
  id: totrans-1901
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 8.2.* `atomic32Shared.cubin`（为SM 2.0编译的微代码）。'
- en: '[Click here to view code image](ch08_images.html#p08lis02a)'
  id: totrans-1902
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch08_images.html#p08lis02a)'
- en: '* * *'
  id: totrans-1903
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: code for sm_20
  id: totrans-1904
  prefs: []
  type: TYPE_NORMAL
  zh: sm_20的代码
- en: 'Function : _Z8Return32PiS_PKi'
  id: totrans-1905
  prefs: []
  type: TYPE_NORMAL
  zh: 函数：_Z8Return32PiS_PKi
- en: /*0000*/     MOV R1, c [0x1] [0x100];
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
  zh: /*0000*/      MOV R1, c [0x1] [0x100];
- en: /*0008*/     S2R R0, SR_Tid_X;
  id: totrans-1907
  prefs: []
  type: TYPE_NORMAL
  zh: /*0008*/      S2R R0, SR_Tid_X;
- en: /*0010*/     SHL R3, R0, 0x2;
  id: totrans-1908
  prefs: []
  type: TYPE_NORMAL
  zh: /*0010*/      SHL R3, R0, 0x2;
- en: /*0018*/     MOV R0, c [0x0] [0x28];
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
  zh: /*0018*/      MOV R0, c [0x0] [0x28];
- en: /*0020*/     IADD R2, R3, c [0x0] [0x28];
  id: totrans-1910
  prefs: []
  type: TYPE_NORMAL
  zh: /*0020*/      IADD R2, R3, c [0x0] [0x28];
- en: /*0028*/     IMAD.U32.U32 RZ, R0, R1, RZ;
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
  zh: /*0028*/      IMAD.U32.U32 RZ, R0, R1, RZ;
- en: /*0030*/     LD R2, [R2];
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
  zh: /*0030*/      LD R2, [R2];
- en: /*0038*/     STS [R3], R2;
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
  zh: /*0038*/      STS [R3], R2;
- en: /*0040*/     SSY 0x80;
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
  zh: /*0040*/     SSY 0x80;
- en: /*0048*/     BAR.RED.POPC RZ, RZ;
  id: totrans-1915
  prefs: []
  type: TYPE_NORMAL
  zh: /*0048*/     BAR.RED.POPC RZ, RZ;
- en: /*0050*/     LD R0, [R0];
  id: totrans-1916
  prefs: []
  type: TYPE_NORMAL
  zh: /*0050*/     LD R0, [R0];
- en: /*0058*/     LDSLK P0, R2, [R3];
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
  zh: /*0058*/     LDSLK P0, R2, [R3];
- en: /*0060*/     @P0 IADD R2, R2, R0;
  id: totrans-1918
  prefs: []
  type: TYPE_NORMAL
  zh: /*0060*/     @P0 IADD R2, R2, R0;
- en: /*0068*/     @P0 STSUL [R3], R2;
  id: totrans-1919
  prefs: []
  type: TYPE_NORMAL
  zh: /*0068*/     @P0 STSUL [R3], R2;
- en: /*0070*/     @!P0 BRA 0x58;
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
  zh: /*0070*/     @!P0 BRA 0x58;
- en: /*0078*/     NOP.S CC.T;
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
  zh: /*0078*/     NOP.S CC.T;
- en: /*0080*/     BAR.RED.POPC RZ, RZ;
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: /*0080*/     BAR.RED.POPC RZ, RZ;
- en: /*0088*/     LDS R0, [R3];
  id: totrans-1923
  prefs: []
  type: TYPE_NORMAL
  zh: /*0088*/     LDS R0, [R3];
- en: /*0090*/     IADD R2, R3, c [0x0] [0x24];
  id: totrans-1924
  prefs: []
  type: TYPE_NORMAL
  zh: /*0090*/     IADD R2, R3, c [0x0] [0x24];
- en: /*0098*/     ST [R2], R0;
  id: totrans-1925
  prefs: []
  type: TYPE_NORMAL
  zh: /*0098*/     ST [R2], R0;
- en: /*00a0*/     EXIT;
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
  zh: /*00a0*/     EXIT;
- en: '...................................'
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
  zh: '...................................'
- en: '* * *'
  id: totrans-1928
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 8.1.6\. Barriers and Coherency
  id: totrans-1929
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.6\. 屏障与一致性
- en: The familiar `__syncthreads()` intrinsic waits until all the threads in the
    thread block have arrived before proceeding. It is needed to maintain coherency
    of shared memory within a thread block.^([7](ch08.html#ch08fn7)) Other, similar
    memory barrier instructions can be used to enforce some ordering on broader scopes
    of memory, as described in [Table 8.3](ch08.html#ch08tab03).
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉的 `__syncthreads()` 内建函数会等待直到线程块中的所有线程都到达，然后才继续执行。它用于维持线程块内共享内存的一致性。^([7](ch08.html#ch08fn7))
    其他类似的内存屏障指令可以用于强制更广泛内存范围内的某些顺序，如 [表 8.3](ch08.html#ch08tab03) 中所描述。
- en: '[7](ch08.html#ch08fn7a). Note that threads within a warp run in lockstep, sometimes
    enabling developers to write so-called “warp synchronous” code that does not call
    `__syncthreads()`. [Section 7.3](ch07.html#ch07lev1sec3) describes thread and
    warp execution in detail, and [Part III](part03.html#part03) includes several
    examples of warp synchronous code.'
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](ch08.html#ch08fn7a). 请注意，warp 内的线程以锁步方式运行，这有时使得开发人员能够编写所谓的“warp 同步”代码，而不需要调用
    `__syncthreads()`。[第 7.3 节](ch07.html#ch07lev1sec3)详细描述了线程和 warp 执行，[第 III 部分](part03.html#part03)包含了几个
    warp 同步代码的示例。'
- en: '![Image](graphics/08tab03.jpg)'
  id: totrans-1932
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/08tab03.jpg)'
- en: '*Table 8.3* Memory Barrier Intrinsics'
  id: totrans-1933
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.3* 内存屏障内建函数'
- en: 8.2\. Integer Support
  id: totrans-1934
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 整数支持
- en: The SMs have the full complement of 32-bit integer operations.
  id: totrans-1935
  prefs: []
  type: TYPE_NORMAL
  zh: SM（流式多处理器）具备完整的 32 位整数操作功能。
- en: • Addition with optional negation of an operand for subtraction
  id: totrans-1936
  prefs: []
  type: TYPE_NORMAL
  zh: • 带有可选操作数负号的加法，用于减法
- en: • Multiplication and multiply-add
  id: totrans-1937
  prefs: []
  type: TYPE_NORMAL
  zh: • 乘法和乘加
- en: • Integer division
  id: totrans-1938
  prefs: []
  type: TYPE_NORMAL
  zh: • 整数除法
- en: • Logical operations
  id: totrans-1939
  prefs: []
  type: TYPE_NORMAL
  zh: • 逻辑运算
- en: • Condition code manipulation
  id: totrans-1940
  prefs: []
  type: TYPE_NORMAL
  zh: • 条件代码操作
- en: • Conversion to/from floating point
  id: totrans-1941
  prefs: []
  type: TYPE_NORMAL
  zh: • 浮动点数的转换
- en: • Miscellaneous operations (e.g., SIMD instructions for narrow integers, population
    count, find first zero)
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
  zh: • 杂项操作（例如，针对窄整数的 SIMD 指令、人口计数、查找第一个零）
- en: CUDA exposes most of this functionality through standard C operators. Nonstandard
    operations, such as 24-bit multiplication, may be accessed using inline PTX assembly
    or intrinsic functions.
  id: totrans-1943
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 通过标准 C 运算符公开了大多数此功能。非标准操作，如 24 位乘法，可以通过内联 PTX 汇编或内建函数访问。
- en: 8.2.1\. Multiplication
  id: totrans-1944
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1\. 乘法
- en: Multiplication is implemented differently on Tesla- and Fermi-class hardware.
    Tesla implements a 24-bit multiplier, while Fermi implements a 32-bit multiplier.
    As a consequence, full 32-bit multiplication on SM 1.x hardware requires four
    instructions. For performance-sensitive code targeting Tesla-class hardware, it
    is a performance win to use the intrinsics for 24-bit multiply.^([8](ch08.html#ch08fn8))
    [Table 8.4](ch08.html#ch08tab04) shows the intrinsics related to multiplication.
  id: totrans-1945
  prefs: []
  type: TYPE_NORMAL
  zh: 乘法在Tesla和Fermi类硬件上的实现有所不同。Tesla实现了一个24位乘法器，而Fermi实现了一个32位乘法器。因此，在SM 1.x硬件上进行完整的32位乘法需要四条指令。对于面向Tesla类硬件的性能敏感代码，使用24位乘法的内建函数能够带来性能提升。^([8](ch08.html#ch08fn8))
    [表 8.4](ch08.html#ch08tab04)展示了与乘法相关的内建函数。
- en: '[8](ch08.html#ch08fn8a). Using `__mul24()` or `__umul24()` on SM 2.x and later
    hardware, however, is a performance *penalty*.'
  id: totrans-1946
  prefs: []
  type: TYPE_NORMAL
  zh: '[8](ch08.html#ch08fn8a). 然而，在SM 2.x及更高版本的硬件上使用`__mul24()`或`__umul24()`会带来性能*惩罚*。'
- en: '![Image](graphics/08tab04.jpg)'
  id: totrans-1947
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/08tab04.jpg)'
- en: '*Table 8.4* Multiplication Intrinsics'
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.4* 乘法内建函数'
- en: 8.2.2\. Miscellaneous (Bit Manipulation)
  id: totrans-1949
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2\. 杂项（位操作）
- en: The CUDA compiler implements a number of intrinsics for bit manipulation, as
    summarized in [Table 8.5](ch08.html#ch08tab05). On SM 2.x and later architectures,
    these intrinsics map to single instructions. On pre-Fermi architectures, they
    are valid but may compile into many instructions. When in doubt, disassemble and
    look at the microcode! 64-bit variants have “`ll`” (two ells for “long long”)
    appended to the intrinsic name `__clzll(), ffsll(), popcll(), brevll()`.
  id: totrans-1950
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA编译器实现了一些位操作的内建函数，具体内容总结在[表 8.5](ch08.html#ch08tab05)中。在SM 2.x及更高版本的架构上，这些内建函数映射为单条指令。在Fermi之前的架构上，它们是有效的，但可能会编译成多条指令。若有疑问，可以反汇编并查看微代码！64位变体在内建函数名称`__clzll(),
    ffsll(), popcll(), brevll()`后附加了“`ll`”（表示“long long”）。
- en: '![Image](graphics/08tab05.jpg)'
  id: totrans-1951
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/08tab05.jpg)'
- en: '*Table 8.5* Bit Manipulation Intrinsics'
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.5* 位操作内建函数'
- en: 8.2.3\. Funnel Shift (SM 3.5)
  id: totrans-1953
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.3\. 漏斗移位（SM 3.5）
- en: GK110 added a 64-bit “funnel shift” instruction that concatenates two 32-bit
    values together (the least significant and most significant halves are specified
    as separate 32-bit inputs, but the hardware operates on an aligned register pair),
    shifts the resulting 64-bit value left or right, and then returns the most significant
    (for left shift) or least significant (for right shift) 32 bits.
  id: totrans-1954
  prefs: []
  type: TYPE_NORMAL
  zh: GK110新增了一条64位的“漏斗移位”指令，它将两个32位值连接在一起（最低有效位和最高有效位分别作为两个32位输入，但硬件在对齐的寄存器对上操作），然后将得到的64位值左移或右移，最后返回最高有效位（左移）或最低有效位（右移）的32位。
- en: Funnel shift may be accessed with the intrinsics given in [Table 8.6](ch08.html#ch08tab06).
    These intrinsics are implemented as inline device functions (using inline PTX
    assembler) in `sm_35_intrinsics.h`. By default, the least significant 5 bits of
    the shift count are masked off; the `_lc` and `_rc` intrinsics clamp the shift
    value to the range 0..32.
  id: totrans-1955
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过[表 8.6](ch08.html#ch08tab06)中给出的内建函数访问漏斗移位。这些内建函数作为内联设备函数（使用内联 PTX 汇编）在`sm_35_intrinsics.h`中实现。默认情况下，移位计数的最低有效
    5 位会被屏蔽；`_lc`和`_rc`内建函数将移位值限制在 0..32 范围内。
- en: '![Image](graphics/08tab06.jpg)'
  id: totrans-1956
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/08tab06.jpg)'
- en: '*Table 8.6* Funnel Shift Intrinsics'
  id: totrans-1957
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.6* 漏斗移位内建函数'
- en: Applications for funnel shift include the following.
  id: totrans-1958
  prefs: []
  type: TYPE_NORMAL
  zh: 漏斗移位的应用包括以下内容。
- en: • Multiword shift operations
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
  zh: • 多字移位操作
- en: • Memory copies between misaligned buffers using aligned loads and stores
  id: totrans-1960
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用对齐的加载和存储在未对齐的缓冲区之间进行内存拷贝
- en: • Rotate
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
  zh: • 旋转
- en: To right-shift data sizes greater than 64 bits, use repeated `__funnelshift_r()`
    calls, operating from the least significant to the most significant word. The
    most significant word of the result is computed using `operator>>`, which shifts
    in zero or sign bits as appropriate for the integer type. To left-shift data sizes
    greater than 64 bits, use repeated `__funnelshift_l()` calls, operating from the
    most significant to the least significant word. The least significant word of
    the result is computed using `operator<<`. If the `hi` and `lo` parameters are
    the same, the funnel shift effects a rotate operation.
  id: totrans-1962
  prefs: []
  type: TYPE_NORMAL
  zh: 要右移大于 64 位的数据大小，请使用重复的`__funnelshift_r()`调用，从最低有效字开始操作，直到最高有效字。结果的最高有效字使用`operator>>`计算，适当地为整数类型移入零或符号位。要左移大于
    64 位的数据大小，请使用重复的`__funnelshift_l()`调用，从最高有效字开始操作，直到最低有效字。结果的最低有效字使用`operator<<`计算。如果`hi`和`lo`参数相同，则漏斗移位会执行旋转操作。
- en: 8.3\. Floating-Point Support
  id: totrans-1963
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 浮点数支持
- en: Fast native floating-point hardware is the *raison d’être* for GPUs, and in
    many ways they are equal to or superior to CPUs in their floating-point implementation.
    Denormals are supported at full speed,^([9](ch08.html#ch08fn9)) directed rounding
    may be specified on a per-instruction basis, and the Special Function Units deliver
    high-performance approximation functions to six popular single-precision transcendentals.
    In contrast, x86 CPUs implement denormals in microcode that runs perhaps 100x
    slower than operating on normalized floating-point operands. Rounding direction
    is specified by a control word that takes dozens of clock cycles to change, and
    the only transcendental approximation functions in the SSE instruction set are
    for reciprocal and reciprocal square root, which give 12-bit approximations that
    must be refined with a Newton-Raphson iteration before being used.
  id: totrans-1964
  prefs: []
  type: TYPE_NORMAL
  zh: 快速本地浮点硬件是GPU的*raison d’être*（存在理由），在许多方面，它们在浮点实现上与CPU相当甚至优于CPU。非正规数以全速支持，^([9](ch08.html#ch08fn9))可以在每条指令上指定舍入方向，特殊功能单元提供六种流行的单精度超越函数的高性能近似计算。相比之下，x86
    CPU在微代码中实现非正规数，而该微代码的运行速度可能比在正规浮点操作数上运行慢100倍。舍入方向由控制字指定，修改这个控制字可能需要几十个时钟周期，而且SSE指令集中的唯一超越函数是倒数和倒数平方根，这些函数提供12位近似值，必须通过牛顿-拉夫森迭代进行精细调整后才能使用。
- en: '[9](ch08.html#ch08fn9a). With the exception that single-precision denormals
    are not supported at all on SM 1.x hardware.'
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
  zh: '[9](ch08.html#ch08fn9a)。唯一的例外是SM 1.x硬件完全不支持单精度的非正规数。'
- en: Since GPUs’ greater core counts are offset somewhat by their lower clock frequencies,
    developers can expect at most a 10x (or thereabouts) speedup on a level playing
    field. If a paper reports a 100x or greater speedup from porting an optimized
    CPU implementation to CUDA, chances are one of the above-described “instruction
    set mismatches” played a role.
  id: totrans-1966
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GPU的更多核心数量在一定程度上被其较低的时钟频率所抵消，开发者可以在公平的环境下预期最多10倍（或大致如此）的加速。如果某篇论文报告了将优化后的CPU实现移植到CUDA上带来了100倍或更高的加速，那么很可能是上述所描述的“指令集不匹配”起了作用。
- en: 8.3.1\. Formats
  id: totrans-1967
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1\. 格式
- en: '[Figure 8.2](ch08.html#ch08fig02) depicts the three (3) IEEE standard floating-point
    formats supported by CUDA: double precision (64-bit), single precision (32-bit),
    and half precision (16-bit). The values are divided into three fields: sign, exponent,
    and mantissa. For `double`, `single`, and `half`, the exponent fields are 11,
    8, and 5 bits in size, respectively; the corresponding mantissa fields are 52,
    23, and 10 bits.'
  id: totrans-1968
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 8.2](ch08.html#ch08fig02)展示了CUDA支持的三种IEEE标准浮点格式：双精度（64位）、单精度（32位）和半精度（16位）。这些值被分为三个字段：符号位、指数位和尾数位。对于`double`、`single`和`half`，指数字段分别为11位、8位和5位；相应的尾数字段为52位、23位和10位。'
- en: '![Image](graphics/08fig02.jpg)'
  id: totrans-1969
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/08fig02.jpg)'
- en: '*Figure 8.2* Floating-point formats.'
  id: totrans-1970
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.2* 浮点格式。'
- en: The exponent field changes the interpretation of the floating-point value. The
    most common (“normal”) representation encodes an implicit 1 bit into the mantissa
    and multiplies that value by 2^(e-bias), where *bias* is the value added to the
    actual exponent before encoding into the floating-point representation. The bias
    for single precision, for example, is 127.
  id: totrans-1971
  prefs: []
  type: TYPE_NORMAL
  zh: 指数字段改变浮动点值的解释。最常见的（“正常”）表示法将一个隐式的1位编码到尾数中，并将该值乘以2^(e-bias)，其中*bias*是编码到浮动点表示前加到实际指数的值。例如，单精度的偏置值为127。
- en: '[Table 8.7](ch08.html#ch08tab07) summarizes how floating-point values are encoded.
    For most exponent values (so-called “normal” floating-point values), the mantissa
    is assumed to have an implicit 1, and it is multiplied by the biased value of
    the exponent. The maximum exponent value is reserved for infinity and Not-A-Number
    values. Dividing by zero (or overflowing a division) yields infinity; performing
    an invalid operation (such as taking the square root or logarithm of a negative
    number) yields a NaN. The minimum exponent value is reserved for values too small
    to represent with the implicit leading 1\. As the so-called *denormals*^([10](ch08.html#ch08fn10))
    get closer to zero, they lose bits of effective precision, a phenomenon known
    as *gradual underflow*. [Table 8.8](ch08.html#ch08tab08) gives the encodings and
    values of certain extreme values for the three formats.'
  id: totrans-1972
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8.7](ch08.html#ch08tab07)总结了浮动点值的编码方式。对于大多数指数值（即所谓的“正常”浮动点值），假定尾数有一个隐式的1，并将其与指数的偏置值相乘。最大指数值保留给无穷大和非数值（NaN）。除以零（或除法溢出）会得到无穷大；执行无效操作（例如对负数取平方根或对数）会得到NaN。最小指数值保留给那些无法用隐式前导1表示的过小的值。当所谓的*非正规数*^([10](ch08.html#ch08fn10))接近零时，它们会失去有效精度的位数，这一现象称为*逐渐下溢*。[表8.8](ch08.html#ch08tab08)给出了三种格式的某些极值的编码和数值。'
- en: '[10](ch08.html#ch08fn10a). Sometimes called *subnormals*.'
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
  zh: '[10](ch08.html#ch08fn10a). 有时称为*子正规数*。'
- en: '![Image](graphics/08tab07.jpg)'
  id: totrans-1974
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/08tab07.jpg)'
- en: '*Table 8.7* Floating-Point Representations'
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
  zh: '*表8.7* 浮动点表示法'
- en: '![Image](graphics/08tab08.jpg)'
  id: totrans-1976
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/08tab08.jpg)'
- en: '*Table 8.8* Floating-Point Extreme Values'
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
  zh: '*表8.8* 浮动点极值'
- en: Rounding
  id: totrans-1978
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 舍入
- en: The IEEE standard provides for four (4) round modes.
  id: totrans-1979
  prefs: []
  type: TYPE_NORMAL
  zh: IEEE标准定义了四种（4）舍入模式。
- en: • Round-to-nearest-even (also called “round-to-nearest”)
  id: totrans-1980
  prefs: []
  type: TYPE_NORMAL
  zh: • 向最近偶数舍入（也称“向最近舍入”）
- en: • Round toward zero (also called “truncate” or “chop”)
  id: totrans-1981
  prefs: []
  type: TYPE_NORMAL
  zh: • 向零舍入（也称“截断”或“切掉”）
- en: • Round down (or “round toward negative infinity”)
  id: totrans-1982
  prefs: []
  type: TYPE_NORMAL
  zh: • 向下舍入（或称“向负无穷舍入”）
- en: • Round up (or “round toward positive infinity”)
  id: totrans-1983
  prefs: []
  type: TYPE_NORMAL
  zh: • 向上舍入（或称“向正无穷舍入”）
- en: Round-to-nearest, where intermediate values are rounded to the nearest representable
    floating-point value after each operation, is by far the most commonly used round
    mode. Round up and round down (the “directed rounding modes”) are used for *interval
    arithmetic*, where a pair of floating-point values are used to bracket the intermediate
    result of a computation. To correctly bracket a result, the lower and upper values
    of the interval must be rounded toward negative infinity (“down”) and toward positive
    infinity (“up”), respectively.
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
  zh: 最近舍入模式，其中中间值在每次操作后舍入到最接近的可表示浮点值，是最常用的舍入模式。向上舍入和向下舍入（即“定向舍入模式”）用于*区间算术*，在区间算术中，成对的浮点值用于围绕计算的中间结果。为了正确地界定结果，区间的下限值和上限值必须分别朝负无穷（“下舍”）和正无穷（“上舍”）舍入。
- en: The C language does not provide any way to specify round modes on a per-instruction
    basis, and CUDA hardware does not provide a control word to implicitly specify
    rounding modes. Consequently, CUDA provides a set of intrinsics to specify the
    round mode of an operation, as summarized in [Table 8.9](ch08.html#ch08tab09).
  id: totrans-1985
  prefs: []
  type: TYPE_NORMAL
  zh: C语言没有提供按指令指定舍入模式的方法，CUDA硬件也没有提供控制字来隐式指定舍入模式。因此，CUDA提供了一组内建函数来指定操作的舍入模式，如[表 8.9](ch08.html#ch08tab09)所总结。
- en: '![Image](graphics/08tab09.jpg)'
  id: totrans-1986
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/08tab09.jpg)'
- en: '*Table 8.9* Intrinsics for Rounding'
  id: totrans-1987
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.9* 舍入的内建函数'
- en: Conversion
  id: totrans-1988
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 转换
- en: 'In general, developers can convert between different floating-point representations
    and/or integers using standard C constructs: implicit conversion or explicit typecasts.
    If necessary, however, developers can use the intrinsics listed in [Table 8.10](ch08.html#ch08tab10)
    to perform conversions that are not in the C language specification, such as those
    with directed rounding.'
  id: totrans-1989
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，开发人员可以使用标准C构造进行不同浮点表示和/或整数之间的转换：隐式转换或显式类型转换。然而，如果有必要，开发人员可以使用[表 8.10](ch08.html#ch08tab10)中列出的内建函数来执行不在C语言规范中的转换，例如具有定向舍入的转换。
- en: '![Image](graphics/08tab10.jpg)'
  id: totrans-1990
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/08tab10.jpg)'
- en: '*Table 8.10* Intrinsics for Conversion'
  id: totrans-1991
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.10* 转换的内建函数'
- en: Because `half` is not standardized in the C programming language, CUDA uses
    `unsigned short` in the interfaces for `__half2float()` and `__float2half()`.
    `__float2half()` only supports the round-to-nearest rounding mode.
  id: totrans-1992
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`half`在C编程语言中没有标准化，CUDA在`__half2float()`和`__float2half()`接口中使用`unsigned short`。`__float2half()`仅支持最近舍入模式。
- en: float __half2float( unsigned short );
  id: totrans-1993
  prefs: []
  type: TYPE_NORMAL
  zh: float __half2float( unsigned short );
- en: unsigned short __float2half( float );
  id: totrans-1994
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned short __float2half( float );
- en: 8.3.2\. Single Precision (32-Bit)
  id: totrans-1995
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2 单精度（32位）
- en: Single-precision floating-point support is the workhorse of GPU computation.
    GPUs have been optimized to natively deliver high performance on this data type,^([11](ch08.html#ch08fn11))
    not only for core standard IEEE operations such as addition and multiplication,
    but also for nonstandard operations such as approximations to transcendentals
    such as `sin()` and `log()`. The 32-bit values are held in the same register file
    as integers, so coercion between single-precision floating-point values and 32-bit
    integers (with `__float_as_int()` and `__int_as_float()`) is free.
  id: totrans-1996
  prefs: []
  type: TYPE_NORMAL
  zh: 单精度浮点支持是GPU计算的主力。GPU已优化以原生支持这种数据类型的高性能计算，不仅适用于标准的IEEE核心操作，如加法和乘法，还适用于非标准操作，如对超越函数（如`sin()`和`log()`）的近似。32位值与整数保存在同一寄存器文件中，因此单精度浮点值与32位整数之间的强制转换（通过`__float_as_int()`和`__int_as_float()`）是免费的。
- en: '[11](ch08.html#ch08fn11a). In fact, GPUs had full 32-bit floating-point support
    before they had full 32-bit integer support. As a result, some early GPU computing
    literature explained how to implement integer math with floating-point hardware!'
  id: totrans-1997
  prefs: []
  type: TYPE_NORMAL
  zh: '[11](ch08.html#ch08fn11a)。实际上，GPU在完全支持32位整数之前就已完全支持32位浮点。结果，一些早期的GPU计算文献解释了如何使用浮点硬件实现整数运算！'
- en: Addition, Multiplication, and Multiply-Add
  id: totrans-1998
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 加法、乘法和乘加
- en: The compiler automatically translates +, –, and * operators on floating-point
    values into addition, multiplication, and multiply-add instructions. The `__fadd_rn()`
    and `__fmul_rn()` intrinsics may be used to suppress fusion of addition and multiplication
    operations into multiply-add instructions.
  id: totrans-1999
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器会自动将浮点值上的+、–和*运算符转换为加法、乘法和乘加指令。可以使用`__fadd_rn()`和`__fmul_rn()`内建函数来抑制加法和乘法操作融合为乘加指令。
- en: Reciprocal and Division
  id: totrans-2000
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 取倒数和除法
- en: For devices of compute capability 2.x and higher, the division operator is IEEE-compliant
    when the code is compiled with `--prec-div=true`. For devices of compute capability
    1.x or for devices of compute capability 2.x when the code is compiled with `--prec-div=false`,
    the division operator and `__fdividef(x,y)` have the same accuracy, but for 2^(126)<y<2^(128),
    `__fdividef(x,y)` delivers a result of zero, whereas the division operator delivers
    the correct result. Also, for 2^(126)<y<2^(128), if x is infinity, `__fdividef(x,y)`
    returns NaN, while the division operator returns infinity.
  id: totrans-2001
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计算能力为2.x及更高版本的设备，当代码使用`--prec-div=true`编译时，除法运算符符合IEEE标准。对于计算能力为1.x的设备，或者当代码使用`--prec-div=false`编译时，计算能力为2.x的设备，除法运算符和`__fdividef(x,y)`具有相同的精度，但对于2^(126)<y<2^(128)，`__fdividef(x,y)`返回零，而除法运算符返回正确的结果。此外，对于2^(126)<y<2^(128)，如果x为无穷大，`__fdividef(x,y)`返回NaN，而除法运算符返回无穷大。
- en: Transcendentals (SFU)
  id: totrans-2002
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 超越函数（SFU）
- en: The Special Function Units (SFUs) in the SMs implement very fast versions of
    six common transcendental functions.
  id: totrans-2003
  prefs: []
  type: TYPE_NORMAL
  zh: SM中的特殊功能单元（SFUs）实现了六个常见超越函数的非常快速版本。
- en: • Sine and cosine
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
  zh: • 正弦和余弦
- en: • Logarithm and exponential
  id: totrans-2005
  prefs: []
  type: TYPE_NORMAL
  zh: • 对数和指数
- en: • Reciprocal and reciprocal square root
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
  zh: • 取倒数和倒数平方根
- en: '[Table 8.11](ch08.html#ch08tab11), excerpted from the paper on the Tesla architecture^([12](ch08.html#ch08fn12))
    summarizes the supported operations and corresponding precision. The SFUs do not
    implement full precision, but they are reasonably good approximations of these
    functions and they are *fast*. For CUDA ports that are significantly faster than
    an optimized CPU equivalent (say, 25x or more), the code most likely relies on
    the SFUs.'
  id: totrans-2007
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8.11](ch08.html#ch08tab11)，摘自关于Tesla架构的论文^([12](ch08.html#ch08fn12))，总结了支持的操作及其对应的精度。SFU不实现完全精度，但它们是这些函数的合理近似，而且它们*速度很快*。对于比优化过的CPU版本快得多的CUDA端口（比如25倍或更多），代码很可能依赖于SFU。'
- en: '[12](ch08.html#ch08fn12a). Lindholm, Erik, John Nickolls, Stuart Oberman, and
    John Montrym. NVIDIA Tesla: A unified graphics and computing architecture. *IEEE
    Micro*, March–April 2008, p. 47.'
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
  zh: '[12](ch08.html#ch08fn12a). Lindholm, Erik, John Nickolls, Stuart Oberman, 和
    John Montrym. NVIDIA Tesla：统一的图形和计算架构。*IEEE Micro*，2008年3月–4月，第47页。'
- en: '![Image](graphics/08tab11.jpg)'
  id: totrans-2009
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/08tab11.jpg)'
- en: '*Table 8.11* SFU Accuracy'
  id: totrans-2010
  prefs: []
  type: TYPE_NORMAL
  zh: '*表8.11* SFU 精度'
- en: The SFUs are accessed with the intrinsics given in [Table 8.12](ch08.html#ch08tab12).
    Specifying the `--fast-math` compiler option will cause the compiler to substitute
    conventional C runtime calls with the corresponding SFU intrinsics listed above.
  id: totrans-2011
  prefs: []
  type: TYPE_NORMAL
  zh: SFU通过[表8.12](ch08.html#ch08tab12)中列出的内在函数进行访问。指定`--fast-math`编译器选项会导致编译器将常规的C运行时调用替换为上面列出的相应SFU内在函数。
- en: '![Image](graphics/08tab12.jpg)'
  id: totrans-2012
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/08tab12.jpg)'
- en: '*Table 8.12* SFU Intrinsics'
  id: totrans-2013
  prefs: []
  type: TYPE_NORMAL
  zh: '*表8.12* SFU 内在函数'
- en: Miscellaneous
  id: totrans-2014
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他
- en: '`__saturate(x)` returns 0 if `x<0`, 1 if `x>1`, and `x` otherwise.'
  id: totrans-2015
  prefs: []
  type: TYPE_NORMAL
  zh: '`__saturate(x)` 如果`x<0`，则返回0；如果`x>1`，则返回1；否则返回`x`。'
- en: 8.3.3\. Double Precision (64-Bit)
  id: totrans-2016
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.3\. 双精度（64位）
- en: Double-precision floating-point support was added to CUDA with SM 1.3 (first
    implemented in the GeForce GTX 280), and much improved double-precision support
    (both functionality and performance) became available with SM 2.0\. CUDA’s hardware
    support for double precision features full-speed denormals and, starting in SM
    2.x, a native fused multiply-add instruction (FMAD), compliant with IEEE 754 c.
    2008, that performs only one rounding step. Besides being an intrinsically useful
    operation, FMAD enables full accuracy on certain functions that are converged
    with the Newton-Raphson iteration.
  id: totrans-2017
  prefs: []
  type: TYPE_NORMAL
  zh: 双精度浮点数支持在CUDA中从SM 1.3开始引入（首次在GeForce GTX 280中实现），并且在SM 2.0中，双精度支持（包括功能和性能）得到了极大的改进。CUDA对双精度的硬件支持包括全速的非正规数处理，并且从SM
    2.x开始，支持符合IEEE 754 c. 2008标准的本地融合乘加指令（FMAD），该指令仅执行一次舍入操作。除了作为一种固有有用的操作外，FMAD还可以在某些通过牛顿-拉夫森迭代收敛的函数中提供完整的精度。
- en: As with single-precision operations, the compiler automatically translates standard
    C operators into multiplication, addition, and multiply-add instructions. The
    `__dadd_rn()` and `__dmul_rn()` intrinsics may be used to suppress fusion of addition
    and multiplication operations into multiply-add instructions.
  id: totrans-2018
  prefs: []
  type: TYPE_NORMAL
  zh: 与单精度操作类似，编译器会自动将标准C运算符转换为乘法、加法和乘加指令。可以使用`__dadd_rn()`和`__dmul_rn()`内建函数来抑制加法和乘法操作的融合，从而避免转化为乘加指令。
- en: 8.3.4\. Half Precision (16-Bit)
  id: totrans-2019
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.4. 半精度（16位）
- en: With 5 bits of exponent and 10 bits of significand, `half` values have enough
    precision for HDR (high dynamic range) images and can be used to hold other types
    of values that do not require `float` precision, such as angles. Half precision
    values are intended for storage, not computation, so the hardware only provides
    instructions to convert to/from 32-bit.^([13](ch08.html#ch08fn13)) These instructions
    are exposed as the `__halftofloat()` and `__floattohalf()` intrinsics.
  id: totrans-2020
  prefs: []
  type: TYPE_NORMAL
  zh: 使用5位指数和10位有效数字，`half`值具有足够的精度，可以用于HDR（高动态范围）图像，并且可以用于存储不需要`float`精度的其他类型的值，例如角度。半精度值旨在用于存储，而非计算，因此硬件仅提供将其转换为/从32位的指令。^([13](ch08.html#ch08fn13))
    这些指令以`__halftofloat()`和`__floattohalf()`内建函数的形式提供。
- en: '[13](ch08.html#ch08fn13a). `half` floating-point values are supported as a
    texture format, in which case the TEX intrinsics return `float` and the conversion
    is automatically performed by the texture hardware.'
  id: totrans-2021
  prefs: []
  type: TYPE_NORMAL
  zh: '[13](ch08.html#ch08fn13a). `half`浮点值支持作为纹理格式，此时TEX内建函数返回`float`，并且转换会由纹理硬件自动执行。'
- en: float __halftofloat( unsigned short );
  id: totrans-2022
  prefs: []
  type: TYPE_NORMAL
  zh: float __halftofloat( unsigned short );
- en: unsigned short __floattohalf( float );
  id: totrans-2023
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned short __floattohalf( float );
- en: These intrinsics use `unsigned short` because the C language has not standardized
    the `half` floating-point type.
  id: totrans-2024
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内建函数使用`unsigned short`，因为C语言尚未标准化`half`浮点类型。
- en: '8.3.5\. Case Study: `float`→`half` Conversion'
  id: totrans-2025
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.5. 案例研究：`float`→`half`转换
- en: Studying the `float`→`half` conversion operation is a useful way to learn the
    details of floating-point encodings and rounding. Because it’s a simple unary
    operation, we can focus on the encoding and rounding without getting distracted
    by the details of floating-point arithmetic and the precision of intermediate
    representations.
  id: totrans-2026
  prefs: []
  type: TYPE_NORMAL
  zh: 研究`float`→`half`转换操作是学习浮点编码和舍入细节的一个有用方法。因为这是一个简单的单目操作，我们可以专注于编码和舍入，而不受浮点算术和中间表示精度的干扰。
- en: When converting from `float` to `half`, the correct output for any `float` too
    large to represent is `half` infinity. Any float too small to represent as a `half`
    (even a denormal `half`) must be clamped to `0.0`. The maximum `float` that rounds
    to `half` 0.0 is `0x32FFFFFF`, or 2.98^(-8), while the smallest `float` that rounds
    to `half` infinity is 65520.0\. `float` values inside this range can be converted
    to `half` by propagating the sign bit, rebiasing the exponent (since float has
    an 8-bit exponent biased by 127 and half has a 5-bit exponent biased by 15), and
    rounding the `float` mantissa to the nearest `half` mantissa value. Rounding is
    straightforward in all cases except when the input value falls exactly between
    the two possible output values. When this is the case, the IEEE standard specifies
    rounding to the “nearest even” value. In decimal arithmetic, this would mean rounding
    1.5 to 2.0, but also rounding 2.5 to 2.0 and (for example) rounding 0.5 to 0.0.
  id: totrans-2027
  prefs: []
  type: TYPE_NORMAL
  zh: 当从`float`转换到`half`时，任何无法表示的过大的`float`的正确输出是`half`的无穷大。任何过小无法表示为`half`（即使是一个非正规`half`）的`float`必须被限制为`0.0`。能四舍五入为`half`
    0.0的最大`float`是`0x32FFFFFF`，即2.98^(-8)，而能四舍五入为`half`无穷大的最小`float`是65520.0。这个范围内的`float`值可以通过传播符号位、重新偏移指数（因为`float`有一个8位的指数，偏移量为127，而`half`有一个5位的指数，偏移量为15），以及将`float`的尾数四舍五入到最接近的`half`尾数值来转换成`half`。在所有情况下，四舍五入都很直接，除了当输入值恰好位于两个可能的输出值之间时。在这种情况下，IEEE标准规定要四舍五入到“最接近的偶数”值。在十进制算术中，这意味着将1.5四舍五入为2.0，但也将2.5四舍五入为2.0，并且（例如）将0.5四舍五入为0.0。
- en: '[Listing 8.3](ch08.html#ch08lis03) shows a C routine that exactly replicates
    the `float`-to-`half` conversion operation, as implemented by CUDA hardware. The
    variables `exp` and `mag` contain the input exponent and “magnitude,” the mantissa
    and exponent together with the sign bit masked off. Many operations, such as comparisons
    and rounding operations, can be performed on the magnitude without separating
    the exponent and mantissa.'
  id: totrans-2028
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 8.3](ch08.html#ch08lis03)展示了一个C语言例程，它完全复制了`float`到`half`的转换操作，该操作是由CUDA硬件实现的。变量`exp`和`mag`包含输入的指数和“幅度”，即尾数和指数与符号位一起被屏蔽掉。许多操作，如比较和四舍五入操作，可以在幅度上执行，而无需分离指数和尾数。'
- en: 'The macro `LG_MAKE_MASK`, used in [Listing 8.3](ch08.html#ch08lis03), creates
    a mask with a given bit count: `#define LG_MAKE_MASK(bits) ((1<<bits)-1)`. A `volatile
    union` is used to treat the same 32-bit value as `float` and `unsigned int`; idioms
    such as `*((float *) (&u))` are not portable. The routine first propagates the
    input sign bit and masks it off the input.'
  id: totrans-2029
  prefs: []
  type: TYPE_NORMAL
  zh: 宏`LG_MAKE_MASK`，在[示例 8.3](ch08.html#ch08lis03)中使用，用于创建具有给定位数的掩码：`#define LG_MAKE_MASK(bits)
    ((1<<bits)-1)`。一个`volatile union`被用来将同一个32位值视为`float`和`unsigned int`；诸如`*((float
    *) (&u))`这样的写法是不可移植的。该例程首先传播输入的符号位，并将其从输入中屏蔽掉。
- en: After extracting the magnitude and exponent, the function deals with the special
    case when the input `float` is INF or NaN, and does an early exit. Note that INF
    is signed, but NaN has a canonical unsigned value. Lines 50–80 clamp the input
    `float` value to the minimum or maximum values that correspond to representable
    `half` values and recompute the magnitude for clamped values. Don’t be fooled
    by the elaborate code constructing `f32MinRInfin` and `f32MaxRf16_zero`; those
    are constants with the values `0x477ff000` and `0x32ffffff`, respectively.
  id: totrans-2030
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取了大小和指数后，函数处理了输入`float`为INF或NaN的特殊情况，并做了提前退出。注意，INF是有符号的，而NaN有一个规范的无符号值。第50到80行将输入`float`值钳制到对应可表示的`half`值的最小或最大值，并重新计算钳制后的大小。不要被构造`f32MinRInfin`和`f32MaxRf16_zero`的复杂代码所迷惑；它们分别是值为`0x477ff000`和`0x32ffffff`的常量。
- en: The remainder of the routine deals with the cases of output normal and denormal
    (input denormals are clamped in the preceding code, so `mag` corresponds to a
    normal `float`). As with the clamping code, `f32Minf16Normal` is a constant, and
    its value is `0x38ffffff`.
  id: totrans-2031
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的部分处理输出为正常值和非规范值的情况（输入非规范值在前面的代码中已经被钳制，因此`mag`对应的是一个正常的`float`）。与钳制代码一样，`f32Minf16Normal`是一个常量，其值为`0x38ffffff`。
- en: To construct a normal, the new exponent must be computed (lines 92 and 93) and
    the correctly rounded 10 bits of mantissa shifted into the output. To construct
    a denormal, the implicit 1 must be OR’d into the output mantissa and the resulting
    mantissa shifted by the amount corresponding to the input exponent. For both normals
    and denormals, the rounding of the output mantissa is accomplished in two steps.
    The rounding is accomplished by adding a mask of 1’s that ends just short of the
    output’s LSB, as seen in [Figure 8.3](ch08.html#ch08fig03).
  id: totrans-2032
  prefs: []
  type: TYPE_NORMAL
  zh: 要构造一个正常数，必须计算新的指数（第92和93行），并将正确四舍五入的10位尾数移入输出。要构造一个非规范数，必须将隐式的1按位或到输出的尾数中，并将结果尾数按输入指数对应的量进行位移。对于正常数和非规范数，输出尾数的四舍五入是通过两步完成的。四舍五入是通过添加一个由1组成的掩码实现的，该掩码恰好不到达输出的最低有效位（LSB），如[图
    8.3](ch08.html#ch08fig03)所示。
- en: '![Image](graphics/08fig03.jpg)'
  id: totrans-2033
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/08fig03.jpg)'
- en: '*Figure 8.3* Rounding mask (`half`).'
  id: totrans-2034
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 8.3* 四舍五入掩码（`half`）。'
- en: This operation increments the output mantissa if bit 12 of the input is set;
    if the input mantissa is all 1’s, the overflow causes the output exponent to correctly
    increment. If we added one more 1 to the MSB of this adjustment, we’d have elementary
    school–style rounding where the tiebreak goes to the larger number. Instead, to
    implement round-to-nearest even, we conditionally increment the output mantissa
    if the LSB of the 10-bit output is set ([Figure 8.4](ch08.html#ch08fig04)). Note
    that these steps can be performed in either order or can be reformulated in many
    different ways.
  id: totrans-2035
  prefs: []
  type: TYPE_NORMAL
  zh: 该操作会在输入的第 12 位被设置时，递增输出的尾数；如果输入的尾数全为 1，溢出会导致输出指数正确递增。如果我们在这个调整的最高有效位上再加上一个 1，就会得到类似小学式的四舍五入，其中平局情况下选择较大的数字。相反，为了实现四舍五入到最近偶数，我们会在
    10 位输出的最低有效位被设置时，条件性地递增输出的尾数（参见[图 8.4](ch08.html#ch08fig04)）。请注意，这些步骤可以以任意顺序执行，或者可以通过不同的方式重新表达。
- en: '![Image](graphics/08fig04.jpg)'
  id: totrans-2036
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/08fig04.jpg)'
- en: '*Figure 8.4* Round-to-nearest-even (`half`).'
  id: totrans-2037
  prefs: []
  type: TYPE_NORMAL
  zh: '*Figure 8.4* 四舍五入到最近偶数（`half`）。'
- en: '*Listing 8.3.* `ConvertToHalf()`.'
  id: totrans-2038
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 8.3.* `ConvertToHalf()`。'
- en: '[Click here to view code image](ch08_images.html#p08lis03a)'
  id: totrans-2039
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch08_images.html#p08lis03a)'
- en: '* * *'
  id: totrans-2040
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: /*
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
  zh: /*
- en: '* exponent shift and mantissa bit count are the same.'
  id: totrans-2042
  prefs: []
  type: TYPE_NORMAL
  zh: '* 指数位移和尾数位数是相同的。'
- en: '*    When we are shifting, we use [f16|f32]ExpShift'
  id: totrans-2043
  prefs: []
  type: TYPE_NORMAL
  zh: '*    当我们进行位移时，我们使用 [f16|f32]ExpShift'
- en: '*    When referencing the number of bits in the mantissa,'
  id: totrans-2044
  prefs: []
  type: TYPE_NORMAL
  zh: '*    引用尾数中的位数时，'
- en: '*        we use [f16|f32]MantissaBits'
  id: totrans-2045
  prefs: []
  type: TYPE_NORMAL
  zh: '*        我们使用[f16|f32]MantissaBits'
- en: '*/'
  id: totrans-2046
  prefs: []
  type: TYPE_NORMAL
  zh: '*/'
- en: const int f16ExpShift = 10;
  id: totrans-2047
  prefs: []
  type: TYPE_NORMAL
  zh: const int f16ExpShift = 10;
- en: const int f16MantissaBits = 10;
  id: totrans-2048
  prefs: []
  type: TYPE_NORMAL
  zh: const int f16MantissaBits = 10;
- en: const int f16ExpBias = 15;
  id: totrans-2049
  prefs: []
  type: TYPE_NORMAL
  zh: const int f16ExpBias = 15;
- en: const int f16MinExp = -14;
  id: totrans-2050
  prefs: []
  type: TYPE_NORMAL
  zh: const int f16MinExp = -14;
- en: const int f16MaxExp = 15;
  id: totrans-2051
  prefs: []
  type: TYPE_NORMAL
  zh: const int f16MaxExp = 15;
- en: const int f16SignMask = 0x8000;
  id: totrans-2052
  prefs: []
  type: TYPE_NORMAL
  zh: const int f16SignMask = 0x8000;
- en: const int f32ExpShift = 23;
  id: totrans-2053
  prefs: []
  type: TYPE_NORMAL
  zh: const int f32ExpShift = 23;
- en: const int f32MantissaBits = 23;
  id: totrans-2054
  prefs: []
  type: TYPE_NORMAL
  zh: const int f32MantissaBits = 23;
- en: const int f32ExpBias = 127;
  id: totrans-2055
  prefs: []
  type: TYPE_NORMAL
  zh: const int f32ExpBias = 127;
- en: const int f32SignMask = 0x80000000;
  id: totrans-2056
  prefs: []
  type: TYPE_NORMAL
  zh: const int f32SignMask = 0x80000000;
- en: unsigned short
  id: totrans-2057
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned short
- en: ConvertFloatToHalf( float f )
  id: totrans-2058
  prefs: []
  type: TYPE_NORMAL
  zh: ConvertFloatToHalf( float f )
- en: '{'
  id: totrans-2059
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: /*
  id: totrans-2060
  prefs: []
  type: TYPE_NORMAL
  zh: /*
- en: '* Use a volatile union to portably coerce'
  id: totrans-2061
  prefs: []
  type: TYPE_NORMAL
  zh: '* 使用 volatile 联合体以便可移植地强制转换'
- en: '* 32-bit float into 32-bit integer'
  id: totrans-2062
  prefs: []
  type: TYPE_NORMAL
  zh: '* 32 位浮点数转为 32 位整数'
- en: '*/'
  id: totrans-2063
  prefs: []
  type: TYPE_NORMAL
  zh: '*/'
- en: volatile union {
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
  zh: volatile union {
- en: float f;
  id: totrans-2065
  prefs: []
  type: TYPE_NORMAL
  zh: float f;
- en: unsigned int u;
  id: totrans-2066
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int u;
- en: '} uf;'
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
  zh: '} uf;'
- en: uf.f = f;
  id: totrans-2068
  prefs: []
  type: TYPE_NORMAL
  zh: uf.f = f;
- en: '// return value: start by propagating the sign bit.'
  id: totrans-2069
  prefs: []
  type: TYPE_NORMAL
  zh: // 返回值：首先传播符号位
- en: unsigned short w = (uf.u >> 16) & f16SignMask;
  id: totrans-2070
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned short w = (uf.u >> 16) & f16SignMask;
- en: // Extract input magnitude and exponent
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
  zh: // 提取输入的幅度和指数
- en: unsigned int mag = uf.u & ~f32SignMask;
  id: totrans-2072
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int mag = uf.u & ~f32SignMask;
- en: int exp = (int) (mag >> f32ExpShift) - f32ExpBias;
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
  zh: int exp = (int) (mag >> f32ExpShift) - f32ExpBias;
- en: // Handle float32 Inf or NaN
  id: totrans-2074
  prefs: []
  type: TYPE_NORMAL
  zh: // 处理 float32 的 Inf 或 NaN
- en: if ( exp == f32ExpBias+1 ) {    // INF or NaN
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
  zh: if ( exp == f32ExpBias+1 ) {    // INF 或 NaN
- en: if ( mag & LG_MAKE_MASK(f32MantissaBits) )
  id: totrans-2076
  prefs: []
  type: TYPE_NORMAL
  zh: if ( mag & LG_MAKE_MASK(f32MantissaBits) )
- en: return 0x7fff; // NaN
  id: totrans-2077
  prefs: []
  type: TYPE_NORMAL
  zh: return 0x7fff; // NaN
- en: // INF - propagate sign
  id: totrans-2078
  prefs: []
  type: TYPE_NORMAL
  zh: // INF - 传播符号位
- en: return w|0x7c00;
  id: totrans-2079
  prefs: []
  type: TYPE_NORMAL
  zh: return w|0x7c00;
- en: '}'
  id: totrans-2080
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: /*
  id: totrans-2081
  prefs: []
  type: TYPE_NORMAL
  zh: /*
- en: '* clamp float32 values that are not representable by float16'
  id: totrans-2082
  prefs: []
  type: TYPE_NORMAL
  zh: '* 限制 float32 值，若无法用 float16 表示'
- en: '*/'
  id: totrans-2083
  prefs: []
  type: TYPE_NORMAL
  zh: '*/'
- en: '{'
  id: totrans-2084
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: // min float32 magnitude that rounds to float16 infinity
  id: totrans-2085
  prefs: []
  type: TYPE_NORMAL
  zh: // 最小的 float32 幅度，四舍五入为 float16 无限大
- en: unsigned int f32MinRInfin = (f16MaxExp+f32ExpBias) <<
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int f32MinRInfin = (f16MaxExp+f32ExpBias) <<
- en: f32ExpShift;
  id: totrans-2087
  prefs: []
  type: TYPE_NORMAL
  zh: f32ExpShift;
- en: f32MinRInfin |= LG_MAKE_MASK( f16MantissaBits+1 ) <<
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: f32MinRInfin |= LG_MAKE_MASK( f16MantissaBits+1 ) <<
- en: (f32MantissaBits-f16MantissaBits-1);
  id: totrans-2089
  prefs: []
  type: TYPE_NORMAL
  zh: (f32MantissaBits-f16MantissaBits-1);
- en: if (mag > f32MinRInfin)
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: if (mag > f32MinRInfin)
- en: mag = f32MinRInfin;
  id: totrans-2091
  prefs: []
  type: TYPE_NORMAL
  zh: mag = f32MinRInfin;
- en: '}'
  id: totrans-2092
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '{'
  id: totrans-2093
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: // max float32 magnitude that rounds to float16 0.0
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
  zh: // 转换为 float16 0.0 的最大 float32 大小
- en: unsigned int f32MaxRf16_zero = f16MinExp+f32ExpBias
  id: totrans-2095
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int f32MaxRf16_zero = f16MinExp+f32ExpBias
- en: (f32MantissaBits-f16MantissaBits-1);
  id: totrans-2096
  prefs: []
  type: TYPE_NORMAL
  zh: (f32MantissaBits-f16MantissaBits-1);
- en: f32MaxRf16_zero <<= f32ExpShift;
  id: totrans-2097
  prefs: []
  type: TYPE_NORMAL
  zh: f32MaxRf16_zero <<= f32ExpShift;
- en: f32MaxRf16_zero |= LG_MAKE_MASK( f32MantissaBits );
  id: totrans-2098
  prefs: []
  type: TYPE_NORMAL
  zh: f32MaxRf16_zero |= LG_MAKE_MASK( f32MantissaBits );
- en: if (mag < f32MaxRf16_zero)
  id: totrans-2099
  prefs: []
  type: TYPE_NORMAL
  zh: if (mag < f32MaxRf16_zero)
- en: mag = f32MaxRf16_zero;
  id: totrans-2100
  prefs: []
  type: TYPE_NORMAL
  zh: mag = f32MaxRf16_zero;
- en: '}'
  id: totrans-2101
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: /*
  id: totrans-2102
  prefs: []
  type: TYPE_NORMAL
  zh: /*
- en: '* compute exp again, in case mag was clamped above'
  id: totrans-2103
  prefs: []
  type: TYPE_NORMAL
  zh: '* 重新计算指数，以防万一 mag 被压缩到上限'
- en: '*/'
  id: totrans-2104
  prefs: []
  type: TYPE_NORMAL
  zh: '*/'
- en: exp = (mag >> f32ExpShift) - f32ExpBias;
  id: totrans-2105
  prefs: []
  type: TYPE_NORMAL
  zh: exp = (mag >> f32ExpShift) - f32ExpBias;
- en: // min float32 magnitude that converts to float16 normal
  id: totrans-2106
  prefs: []
  type: TYPE_NORMAL
  zh: // 转换为 float16 正常值的最小 float32 大小
- en: unsigned int f32Minf16Normal = ((f16MinExp+f32ExpBias)<<
  id: totrans-2107
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int f32Minf16Normal = ((f16MinExp+f32ExpBias)<<
- en: f32ExpShift);
  id: totrans-2108
  prefs: []
  type: TYPE_NORMAL
  zh: f32ExpShift);
- en: f32Minf16Normal |= LG_MAKE_MASK( f32MantissaBits );
  id: totrans-2109
  prefs: []
  type: TYPE_NORMAL
  zh: f32Minf16Normal |= LG_MAKE_MASK( f32MantissaBits );
- en: if ( mag >= f32Minf16Normal ) {
  id: totrans-2110
  prefs: []
  type: TYPE_NORMAL
  zh: if ( mag >= f32Minf16Normal ) {
- en: //
  id: totrans-2111
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: '// Case 1: float16 normal'
  id: totrans-2112
  prefs: []
  type: TYPE_NORMAL
  zh: // 情况 1：float16 正常数
- en: //
  id: totrans-2113
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // Modify exponent to be biased for float16, not float32
  id: totrans-2114
  prefs: []
  type: TYPE_NORMAL
  zh: // 修改指数使其适应 float16 的偏置，而不是 float32
- en: mag += (unsigned int) ((f16ExpBias-f32ExpBias)<<
  id: totrans-2115
  prefs: []
  type: TYPE_NORMAL
  zh: mag += (unsigned int) ((f16ExpBias-f32ExpBias)<<
- en: f32ExpShift);
  id: totrans-2116
  prefs: []
  type: TYPE_NORMAL
  zh: f32ExpShift);
- en: int RelativeShift = f32ExpShift-f16ExpShift;
  id: totrans-2117
  prefs: []
  type: TYPE_NORMAL
  zh: int RelativeShift = f32ExpShift-f16ExpShift;
- en: // add rounding bias
  id: totrans-2118
  prefs: []
  type: TYPE_NORMAL
  zh: // 添加舍入偏差
- en: mag += LG_MAKE_MASK(RelativeShift-1);
  id: totrans-2119
  prefs: []
  type: TYPE_NORMAL
  zh: mag += LG_MAKE_MASK(RelativeShift-1);
- en: // round-to-nearest even
  id: totrans-2120
  prefs: []
  type: TYPE_NORMAL
  zh: // 四舍五入到最接近的偶数
- en: mag += (mag >> RelativeShift) & 1;
  id: totrans-2121
  prefs: []
  type: TYPE_NORMAL
  zh: mag += (mag >> RelativeShift) & 1;
- en: w |= mag >> RelativeShift;
  id: totrans-2122
  prefs: []
  type: TYPE_NORMAL
  zh: w |= mag >> RelativeShift;
- en: '}'
  id: totrans-2123
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else {
  id: totrans-2124
  prefs: []
  type: TYPE_NORMAL
  zh: else {
- en: /*
  id: totrans-2125
  prefs: []
  type: TYPE_NORMAL
  zh: /*
- en: '* Case 2: float16 denormal'
  id: totrans-2126
  prefs: []
  type: TYPE_NORMAL
  zh: '* 情况 2：float16 非正规数'
- en: '*/'
  id: totrans-2127
  prefs: []
  type: TYPE_NORMAL
  zh: '*/'
- en: // mask off exponent bits - now fraction only
  id: totrans-2128
  prefs: []
  type: TYPE_NORMAL
  zh: // 屏蔽指数位 - 现在只有尾数部分
- en: mag &= LG_MAKE_MASK(f32MantissaBits);
  id: totrans-2129
  prefs: []
  type: TYPE_NORMAL
  zh: mag &= LG_MAKE_MASK(f32MantissaBits);
- en: // make implicit 1 explicit
  id: totrans-2130
  prefs: []
  type: TYPE_NORMAL
  zh: // 将隐式的 1 显式化
- en: mag |= (1<<f32ExpShift);
  id: totrans-2131
  prefs: []
  type: TYPE_NORMAL
  zh: mag |= (1<<f32ExpShift);
- en: int RelativeShift = f32ExpShift-f16ExpShift+f16MinExp-exp;
  id: totrans-2132
  prefs: []
  type: TYPE_NORMAL
  zh: int RelativeShift = f32ExpShift-f16ExpShift+f16MinExp-exp;
- en: // add rounding bias
  id: totrans-2133
  prefs: []
  type: TYPE_NORMAL
  zh: // 添加舍入偏差
- en: mag += LG_MAKE_MASK(RelativeShift-1);
  id: totrans-2134
  prefs: []
  type: TYPE_NORMAL
  zh: mag += LG_MAKE_MASK(RelativeShift-1);
- en: // round-to-nearest even
  id: totrans-2135
  prefs: []
  type: TYPE_NORMAL
  zh: // 四舍五入到最接近的偶数
- en: mag += (mag >> RelativeShift) & 1;
  id: totrans-2136
  prefs: []
  type: TYPE_NORMAL
  zh: mag += (mag >> RelativeShift) & 1;
- en: w |= mag >> RelativeShift;
  id: totrans-2137
  prefs: []
  type: TYPE_NORMAL
  zh: w |= mag >> RelativeShift;
- en: '}'
  id: totrans-2138
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return w;
  id: totrans-2139
  prefs: []
  type: TYPE_NORMAL
  zh: return w;
- en: '}'
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'In practice, developers should convert `float` to `half` by using the `__floattohalf()`
    intrinsic, which the compiler translates to a single `F2F` machine instruction.
    This sample routine is provided purely to aid in understanding floating-point
    layout and rounding; also, examining all the special-case code for INF/NAN and
    denormal values helps to illustrate why these features of the IEEE spec have been
    controversial since its inception: They make hardware slower, more costly, or
    both due to increased silicon area and engineering effort for validation.'
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，开发者应该通过使用 `__floattohalf()` 内建函数将 `float` 转换为 `half`，该内建函数会被编译器转换为一条单独的
    `F2F` 机器指令。此示例程序仅用于帮助理解浮点布局和舍入；此外，检查所有关于 INF/NAN 和非规范值的特殊情况代码有助于说明为什么自 IEEE 规范开始以来，这些特性一直存在争议：它们使硬件变慢、更昂贵，或者由于增加的硅片面积和验证工程工作量，两者兼而有之。
- en: In the code accompanying this book, the `ConvertFloatToHalf()` routine in [Listing
    8.3](ch08.html#ch08lis03) is incorporated into a program called `float_to_float16.cu`
    that tests its output for every 32-bit floating-point value.
  id: totrans-2143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书随附的代码中，位于 [Listing 8.3](ch08.html#ch08lis03) 中的 `ConvertFloatToHalf()` 例程被集成到一个名为
    `float_to_float16.cu` 的程序中，该程序测试每个 32 位浮点值的输出。
- en: 8.3.6\. Math Library
  id: totrans-2144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.6\. 数学库
- en: 'CUDA includes a built-in math library modeled on the C runtime library, with
    a few small differences: CUDA hardware does not include a rounding mode register
    (instead, the round mode is encoded on a per-instruction basis),^([14](ch08.html#ch08fn14))
    so functions such as `rint()` that reference the current rounding mode always
    round-to-nearest. Additionally, the hardware does not raise floating-point exceptions;
    results of aberrant operations, such as taking the square root of a negative number,
    are encoded as NaNs.'
  id: totrans-2145
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 包含一个内建的数学库，基于 C 运行时库构建，具有一些小的差异：CUDA 硬件不包括舍入模式寄存器（而是将舍入模式在每条指令基础上进行编码），^([14](ch08.html#ch08fn14))
    所以像 `rint()` 这样的函数总是使用四舍五入到最近的值，因为它们参考了当前的舍入模式。此外，硬件不会触发浮点异常；例如对负数取平方根等异常操作的结果会被编码为
    NaN。
- en: '[14](ch08.html#ch08fn14a). Encoding a round mode per instruction and keeping
    it in a control register are not irreconcilable. The Alpha processor had a 2-bit
    encoding to specify the round mode per instruction, one setting of which was to
    use the rounding mode specified in a control register! CUDA hardware just uses
    a 2-bit encoding for the four round modes specified in the IEEE specification.'
  id: totrans-2146
  prefs: []
  type: TYPE_NORMAL
  zh: '[14](ch08.html#ch08fn14a). 每条指令编码一个舍入模式并将其保存在控制寄存器中并不是不可调和的。Alpha 处理器有一个 2
    位编码来指定每条指令的舍入模式，其中一种设置是使用控制寄存器中指定的舍入模式！CUDA 硬件仅使用一个 2 位编码来表示 IEEE 规范中指定的四种舍入模式。'
- en: '[Table 8.13](ch08.html#ch08tab13) lists the math library functions and the
    maximum error in ulps for each function. Most functions that operate on `float`
    have an “f” appended to the function name—for example, the functions that compute
    the sine function are as follows.'
  id: totrans-2147
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table 8.13](ch08.html#ch08tab13) 列出了数学库函数以及每个函数的最大误差（以 ulps 为单位）。大多数对 `float`
    操作的函数在函数名后都加上了 “f” 后缀——例如，计算正弦函数的函数如下所示。'
- en: double sin( double angle );
  id: totrans-2148
  prefs: []
  type: TYPE_NORMAL
  zh: double sin( double angle );
- en: float sinf( float angle );
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
  zh: float sinf( float angle );
- en: '![Image](graphics/08tab13.jpg)![Image](graphics/08tab13a.jpg)![Image](graphics/08tab13b.jpg)![Image](graphics/08tab13c.jpg)![Image](graphics/08tab13d.jpg)![Image](graphics/08tab13e.jpg)![Image](graphics/08tab13f.jpg)'
  id: totrans-2150
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/08tab13.jpg)![Image](graphics/08tab13a.jpg)![Image](graphics/08tab13b.jpg)![Image](graphics/08tab13c.jpg)![Image](graphics/08tab13d.jpg)![Image](graphics/08tab13e.jpg)![Image](graphics/08tab13f.jpg)'
- en: '*Table 8.13* Math Library'
  id: totrans-2151
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.13* 数学库'
- en: These are denoted in [Table 8.13](ch08.html#ch08tab13) as, for example, `sin[f]`.
  id: totrans-2152
  prefs: []
  type: TYPE_NORMAL
  zh: 这些在[表 8.13](ch08.html#ch08tab13)中表示为，例如，`sin[f]`。
- en: Conversion to Integer
  id: totrans-2153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 转换为整数
- en: According to the C runtime library definition, the `nearbyint()` and `rint()`
    functions round a floating-point value to the nearest integer using the “current
    rounding direction,” which in CUDA is always round-to-nearest-even. In the C runtime,
    `nearbyint()` and `rint()` differ only in their handling of the INEXACT exception.
    But since CUDA does not raise floating-point exceptions, the functions behave
    identically.
  id: totrans-2154
  prefs: []
  type: TYPE_NORMAL
  zh: 根据C运行时库的定义，`nearbyint()`和`rint()`函数会根据“当前舍入方向”将浮点值四舍五入到最接近的整数，在CUDA中，这个方向始终是四舍五入到最接近偶数。在C运行时中，`nearbyint()`和`rint()`仅在处理INEXACT异常时有所不同。但由于CUDA不会抛出浮点异常，因此这两个函数的行为是相同的。
- en: '`round()` implements elementary school–style rounding: For floating-point values
    halfway between integers, the input is always rounded away from zero. NVIDIA recommends
    against using this function because it expands to eight (8) instructions as opposed
    to one for `rint()` and its variants. `trunc()` truncates or “chops” the floating-point
    value, rounding toward zero. It compiles to a single instruction.'
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
  zh: '`round()`实现了小学式的四舍五入：对于位于两个整数之间的浮点值，输入总是会向远离零的方向舍入。NVIDIA建议不要使用此函数，因为它展开为八（8）条指令，而`rint()`及其变体只需一条指令。`trunc()`截断或“剪切”浮点值，朝零舍入。它会编译成一条指令。'
- en: Fractions and Exponents
  id: totrans-2156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分数和指数
- en: float frexpf(float x, int *eptr);
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
  zh: float frexpf(float x, int *eptr);
- en: '`frexpf()` breaks the input into a floating-point significand in the range
    [0.5, 1.0) and an integral exponent for 2, such that'
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
  zh: '`frexpf()`将输入拆分为一个浮点尾数，范围为[0.5, 1.0)，以及2的整数指数，满足以下条件：'
- en: '*x = Significand* · 2^(*Exponent*)'
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
  zh: '*x = 尾数* · 2^(*指数*)'
- en: float logbf( float x );
  id: totrans-2160
  prefs: []
  type: TYPE_NORMAL
  zh: float logbf( float x );
- en: '`logbf()` extracts the exponent from x and returns it as a floating-point value.
    It is equivalent to `floorf(log2f(x))`, except it is faster. If `x` is a denormal,
    `logbf()` returns the exponent that x would have if it were normalized.'
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
  zh: '`logbf()`提取x的指数并将其作为浮点值返回。它等同于`floorf(log2f(x))`，只是速度更快。如果`x`是非规格化数，`logbf()`将返回如果x被规范化时的指数。'
- en: float ldexpf( float x, int exp );
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
  zh: float ldexpf( float x, int exp );
- en: float scalbnf( float x, int n );
  id: totrans-2163
  prefs: []
  type: TYPE_NORMAL
  zh: float scalbnf( float x, int n );
- en: float scanblnf( float x, long n );
  id: totrans-2164
  prefs: []
  type: TYPE_NORMAL
  zh: float scanblnf( float x, long n );
- en: '`ldexpf()`, `scalbnf()`, and `scalblnf()` all compute x2^n by direct manipulation
    of floating-point exponents.'
  id: totrans-2165
  prefs: []
  type: TYPE_NORMAL
  zh: '`ldexpf()`、`scalbnf()`和`scalblnf()`都通过直接操作浮点指数来计算x2^n。'
- en: Floating-Point Remainder
  id: totrans-2166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 浮点余数
- en: '`modff()` breaks the input into fractional and integer parts.'
  id: totrans-2167
  prefs: []
  type: TYPE_NORMAL
  zh: '`modff()`将输入值分解为小数部分和整数部分。'
- en: float modff( float x, float *intpart );
  id: totrans-2168
  prefs: []
  type: TYPE_NORMAL
  zh: float modff( float x, float *intpart );
- en: The return value is the fractional part of x, with the same sign.
  id: totrans-2169
  prefs: []
  type: TYPE_NORMAL
  zh: 返回值是x的小数部分，符号与x相同。
- en: '`remainderf(x,y)` computes the floating-point remainder of dividing x by y.
    The return value is `x-n*y`, where `n` is x/y, rounded to the nearest integer.
    If |x –ny| = 0.5, `n` is chosen to be even.'
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
  zh: '`remainderf(x,y)`计算x除以y的浮点余数。返回值为`x-n*y`，其中`n`是x/y，四舍五入到最近的整数。如果|x –ny| = 0.5，则`n`选择为偶数。'
- en: float remquof(float x, float y, int *quo);
  id: totrans-2171
  prefs: []
  type: TYPE_NORMAL
  zh: float remquof(float x, float y, int *quo);
- en: computes the remainder and passes back the lower bits of the integral quotient
    x/y, with the same sign as x/y.
  id: totrans-2172
  prefs: []
  type: TYPE_NORMAL
  zh: 计算余数并返回整数商x/y的低位部分，符号与x/y相同。
- en: Bessel Functions
  id: totrans-2173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贝塞尔函数
- en: The Bessel functions of order *n* relate to the differential equation
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*阶贝塞尔函数与下列微分方程相关：'
- en: '![Image](graphics/265equ01.jpg)'
  id: totrans-2175
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/265equ01.jpg)'
- en: '*n* can be a real number, but for purposes of the C runtime, it is a nonnegative
    integer.'
  id: totrans-2176
  prefs: []
  type: TYPE_NORMAL
  zh: '*n*可以是实数，但在C运行时中，它是一个非负整数。'
- en: The solution to this second-order ordinary differential equation combines Bessel
    functions of the first kind and of the second kind.
  id: totrans-2177
  prefs: []
  type: TYPE_NORMAL
  zh: 这个二阶常微分方程的解结合了第一类和第二类的贝塞尔函数。
- en: '![Image](graphics/265equ02.jpg)'
  id: totrans-2178
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/265equ02.jpg)'
- en: The math runtime functions `jn[f]()` and `yn[f]()` compute J[n](x) and Y[n](x),
    respectively. `j0f()`, `j1f()`, `y0f()`, and `y1f()` compute these functions for
    the special cases of n=0 and n=1.
  id: totrans-2179
  prefs: []
  type: TYPE_NORMAL
  zh: 数学运行时函数`jn[f]()`和`yn[f]()`分别计算J[n](x)和Y[n](x)。`j0f()`、`j1f()`、`y0f()`和`y1f()`分别计算n=0和n=1的特例。
- en: Gamma Function
  id: totrans-2180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 伽马函数
- en: The gamma function Γ is an extension of the factorial function, with its argument
    shifted down by 1, to real numbers. It has a variety of definitions, one of which
    is as follows.
  id: totrans-2181
  prefs: []
  type: TYPE_NORMAL
  zh: 伽马函数Γ是阶乘函数的扩展，其参数下移1，扩展到实数范围。它有多种定义，其中一种如下所示。
- en: '![Image](graphics/265equ03.jpg)'
  id: totrans-2182
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/265equ03.jpg)'
- en: The function grows so quickly that the return value loses precision for relatively
    small input values, so the library provides the `lgamma()` function, which returns
    the natural logarithm of the gamma function, in addition to the `tgamma()` (“true
    gamma”) function.
  id: totrans-2183
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数增长得非常快，以至于对于相对较小的输入值，返回值会失去精度，因此该库提供了`lgamma()`函数，它返回伽马函数的自然对数，此外还有`tgamma()`（“真正的伽马”）函数。
- en: 8.3.7\. Additional Reading
  id: totrans-2184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.7\. 进一步阅读
- en: Goldberg’s survey (with the captivating title “What Every Computer Scientist
    Should Know About Floating Point Arithmetic”) is a good introduction to the topic.
  id: totrans-2185
  prefs: []
  type: TYPE_NORMAL
  zh: Goldberg的调查（以引人入胜的标题“每个计算机科学家应知道的浮点算术”命名）是该主题的一个很好的入门介绍。
- en: '[http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html](http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html)'
  id: totrans-2186
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html](http://download.oracle.com/docs/cd/E19957-01/806-3568/ncg_goldberg.html)'
- en: 'Nathan Whitehead and Alex Fit-Florea of NVIDIA have coauthored a white paper
    entitled “Precision & Performance: Floating Point and IEEE 754 Compliance for
    NVIDIA GPUs.”'
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA的Nathan Whitehead和Alex Fit-Florea共同撰写了一篇题为“精度与性能：浮点数和IEEE 754规范在NVIDIA
    GPU中的应用”的白皮书。
- en: '[http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf](http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf)'
  id: totrans-2188
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf](http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf)'
- en: Increasing Effective Precision
  id: totrans-2189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提高有效精度
- en: Dekker and Kahan developed methods to almost double the effective precision
    of floating-point hardware using pairs of numbers in exchange for a slight reduction
    in exponent range (due to intermediate underflow and overflow at the far ends
    of the range). Some papers on this topic include the following.
  id: totrans-2190
  prefs: []
  type: TYPE_NORMAL
  zh: Dekker和Kahan提出了通过使用数字对来几乎将浮点硬件的有效精度翻倍的方法，代价是稍微减少了指数范围（由于在范围两端的中间下溢和上溢）。有关这个主题的一些论文包括以下几篇。
- en: Dekker, T.J. Point technique for extending the available precision. *Numer.
    Math.* 18, 1971, pp. 224–242.
  id: totrans-2191
  prefs: []
  type: TYPE_NORMAL
  zh: Dekker, T.J. 用于扩展可用精度的点技术。*数值数学* 18, 1971, 第224–242页。
- en: Linnainmaa, S. Software for doubled-precision floating point computations. *ACM
    TOMS* 7, pp. 172–283 (1981).
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
  zh: Linnainmaa, S. 双精度浮点计算的软件。*ACM TOMS* 7, 第172–283页 (1981)。
- en: Shewchuk, J.R. Adaptive precision floating-point arithmetic and fast robust
    geometric predicates. *Discrete & Computational Geometry* 18, 1997, pp. 305–363.
  id: totrans-2193
  prefs: []
  type: TYPE_NORMAL
  zh: Shewchuk, J.R. 自适应精度浮点算术与快速稳健几何判定。*离散与计算几何学* 18, 1997, 第305–363页。
- en: Some GPU-specific work on this topic has been done by Andrew Thall, Da Graça,
    and Defour.
  id: totrans-2194
  prefs: []
  type: TYPE_NORMAL
  zh: Andrew Thall、Da Graça 和 Defour 已经在这个主题上做了一些关于GPU的特定研究。
- en: Guillaume, Da Graça, and David Defour. Implementation of float-float operators
    on graphics hardware, *7th Conference on Real Numbers and Computers*, RNC7 (2006).
  id: totrans-2195
  prefs: []
  type: TYPE_NORMAL
  zh: Guillaume, Da Graça 和 David Defour。图形硬件上实现浮点-浮点操作符，*第七届实数与计算机会议*，RNC7 (2006)。
- en: '[http://hal.archives-ouvertes.fr/docs/00/06/33/56/PDF/float-float.pdf](http://hal.archives-ouvertes.fr/docs/00/06/33/56/PDF/float-float.pdf)'
  id: totrans-2196
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://hal.archives-ouvertes.fr/docs/00/06/33/56/PDF/float-float.pdf](http://hal.archives-ouvertes.fr/docs/00/06/33/56/PDF/float-float.pdf)'
- en: Thall, Andrew. Extended-precision floating-point numbers for GPU computation.
    2007.
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
  zh: Thall, Andrew。GPU计算的扩展精度浮点数。2007年。
- en: '[http://andrewthall.org/papers/df64_qf128.pdf](http://andrewthall.org/papers/df64_qf128.pdf)'
  id: totrans-2198
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://andrewthall.org/papers/df64_qf128.pdf](http://andrewthall.org/papers/df64_qf128.pdf)'
- en: 8.4\. Conditional Code
  id: totrans-2199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 条件代码
- en: The hardware implements “condition code” or CC registers that contain the usual
    4-bit state vector (sign, carry, zero, overflow) used for integer comparison.
    These CC registers can be set using comparison instructions such as `ISET`, and
    they can direct the flow of execution via *predication* or *divergence*. Predication
    allows (or suppresses) the execution of instructions on a per-thread basis within
    a warp, while divergence is the conditional execution of longer instruction sequences.
    Because the processors within an SM execute instructions in SIMD fashion at warp
    granularity (32 threads at a time), divergence can result in fewer instructions
    executed, provided all threads within a warp take the same code path.
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件实现了“条件码”或CC寄存器，这些寄存器包含用于整数比较的常见4位状态向量（符号、进位、零、溢出）。这些CC寄存器可以通过比较指令，如`ISET`，进行设置，并且它们可以通过*预测执行*或*分歧*来引导执行流程。预测执行允许（或抑制）在warp内按线程执行指令，而分歧则是指长指令序列的条件执行。由于SM中的处理器按warp粒度（每次32个线程）以SIMD方式执行指令，分歧可以导致执行的指令减少，前提是warp内的所有线程采取相同的代码路径。
- en: 8.4.1\. Predication
  id: totrans-2201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.1. 预测执行
- en: Due to the additional overhead of managing divergence and convergence, the compiler
    uses predication for short instruction sequences. The effect of most instructions
    can be predicated on a condition; if the condition is not TRUE, the instruction
    is suppressed. This suppression occurs early enough that predicated execution
    of instructions such as load/store and `TEX` inhibits the memory traffic that
    the instruction would otherwise generate. Note that predication has no effect
    on the eligibility of memory traffic for global load/store coalescing. The addresses
    specified to all load/store instructions in a warp must reference consecutive
    memory locations, even if they are predicated.
  id: totrans-2202
  prefs: []
  type: TYPE_NORMAL
  zh: 由于管理分歧和合并的额外开销，编译器对短指令序列使用预测执行。大多数指令的效果可以依赖于一个条件来预测；如果条件不为TRUE，则该指令会被抑制。这种抑制发生得足够早，以至于预测执行的指令，如加载/存储和`TEX`，能够抑制指令本应生成的内存流量。请注意，预测执行对全局加载/存储合并的内存流量资格没有影响。即使指令被预测执行，warp内所有加载/存储指令所指定的地址也必须引用连续的内存位置。
- en: Predication is used when the number of instructions that vary depending on a
    condition is small; the compiler uses heuristics that favor predication up to
    about 7 instructions. Besides avoiding the overhead of managing the branch synchronization
    stack described below, predication also gives the compiler more optimization opportunities
    (such as instruction scheduling) when emitting microcode. The ternary operator
    in C (`? :`) is considered a compiler hint to favor predication.
  id: totrans-2203
  prefs: []
  type: TYPE_NORMAL
  zh: 当依赖于条件的指令数量较少时，会使用预测执行；编译器使用启发式方法，在约7条指令以内优先采用预测执行。除了避免下面描述的分支同步堆栈管理开销外，预测执行还为编译器在生成微代码时提供了更多优化机会（例如指令调度）。C语言中的三元运算符（`?
    :`）被视为编译器提示，表示优先采用预测执行。
- en: '[Listing 8.2](ch08.html#ch08lis02) gives an excellent example of predication,
    as expressed in microcode. When performing an atomic operation on a shared memory
    location, the compiler emits code that loops over the shared memory location until
    it has successfully performed the atomic operation. The `LDSLK` (load shared and
    lock) instruction returns a condition code that tells whether the lock was acquired.
    The instructions to perform the operation then are predicated on that condition
    code.'
  id: totrans-2204
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 8.2](ch08.html#ch08lis02)给出了一个非常好的预测示例，以微代码形式表示。当对共享内存位置执行原子操作时，编译器会生成代码，循环访问共享内存位置，直到成功执行原子操作。`LDSLK`（加载共享并加锁）指令返回一个条件码，指示是否成功获取锁。执行操作的指令则根据该条件码进行预测。'
- en: /*0058*/ LDSLK P0, R2, [R3];
  id: totrans-2205
  prefs: []
  type: TYPE_NORMAL
  zh: /*0058*/ LDSLK P0, R2, [R3];
- en: /*0060*/ @P0 IADD R2, R2, R0;
  id: totrans-2206
  prefs: []
  type: TYPE_NORMAL
  zh: /*0060*/ @P0 IADD R2, R2, R0;
- en: /*0068*/ @P0 STSUL [R3], R2;
  id: totrans-2207
  prefs: []
  type: TYPE_NORMAL
  zh: /*0068*/ @P0 STSUL [R3], R2;
- en: /*0070*/ @!P0 BRA 0x58;
  id: totrans-2208
  prefs: []
  type: TYPE_NORMAL
  zh: /*0070*/ @!P0 BRA 0x58;
- en: This code fragment also highlights how predication and branching sometimes work
    together. The last instruction, a conditional branch to attempt to reacquire the
    lock if necessary, also is predicated.
  id: totrans-2209
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码片段还突出了预测与分支有时如何协同工作。最后一条指令是一个条件分支，尝试在必要时重新获取锁，这条指令同样进行了预测。
- en: 8.4.2\. Divergence and Convergence
  id: totrans-2210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.2\. 发散与汇聚
- en: Predication works well for small fragments of conditional code, especially `if`
    statements with no corresponding `else`. For larger amounts of conditional code,
    predication becomes inefficient because every instruction is executed, regardless
    of whether it will affect the computation. When the larger number of instructions
    causes the costs of predication to exceed the benefits, the compiler will use
    conditional branches. When the flow of execution within a warp takes different
    paths depending on a condition, the code is called *divergent*.
  id: totrans-2211
  prefs: []
  type: TYPE_NORMAL
  zh: 预测在处理小段条件代码时效果很好，特别是没有对应 `else` 的 `if` 语句。对于较大的条件代码段，预测变得低效，因为每条指令都会执行，无论它是否会影响计算。当大量指令的开销超过预测的收益时，编译器将使用条件分支。当一个执行流在一个warp内根据条件走不同路径时，代码被称为*发散*。
- en: NVIDIA is close-mouthed about the details of how their hardware supports divergent
    code paths, and it reserves the right to change the hardware implementation between
    generations. The hardware maintains a bit vector of active threads within each
    warp. For threads that are marked inactive, execution is suppressed in a way similar
    to predication. Before taking a branch, the compiler executes a special instruction
    to push this active-thread bit vector onto a stack. The code is then executed
    *twice*, once for threads for which the condition was TRUE, then for threads for
    which the predicate was FALSE. This two-phased execution is managed with a *branch
    synchronization stack*, as described by Lindholm et al.^([15](ch08.html#ch08fn15))
  id: totrans-2212
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 对其硬件如何支持分歧代码路径的细节保持缄默，并且保留在不同代之间更改硬件实现的权利。硬件维护每个 warp 中活动线程的位向量。对于标记为非活动的线程，执行会以类似于条件执行的方式被抑制。在执行分支之前，编译器会执行一条特殊指令，将这个活动线程的位向量推送到栈上。然后，代码会被*执行两次*，一次是针对条件为真（TRUE）的线程，另一次是针对谓词为假（FALSE）的线程。这种两阶段的执行通过*分支同步栈*进行管理，正如
    Lindholm 等人所描述的那样^([15](ch08.html#ch08fn15))。
- en: '[15](ch08.html#ch08fn15a). Lindholm, Erik, John Nickolls, Stuart Oberman, and
    John Montrym. NVIDIA Tesla: A unified graphics and computing architecture. *IEEE
    Micro*, March–April 2008, pp. 39–55.'
  id: totrans-2213
  prefs: []
  type: TYPE_NORMAL
  zh: '[15](ch08.html#ch08fn15a). Lindholm, Erik, John Nickolls, Stuart Oberman, and
    John Montrym. NVIDIA Tesla: A unified graphics and computing architecture. *IEEE
    Micro*, March–April 2008, pp. 39–55.'
- en: If threads of a warp diverge via a data-dependent conditional branch, the warp
    serially executes each branch path taken, disabling threads that are not on that
    path, and when all paths complete, the threads reconverge to the original execution
    path. The SM uses a branch synchronization stack to manage independent threads
    that diverge and converge. Branch divergence only occurs within a warp; different
    warps execute independently regardless of whether they are executing common or
    disjoint code paths.
  id: totrans-2214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个 warp 中的线程通过数据相关的条件分支发生分歧，warp 会依次执行每个分支路径，禁用不在该路径上的线程，并且当所有路径执行完毕后，线程会重新合并到原始执行路径。SM
    使用一个分支同步栈来管理分歧和合并的独立线程。分支分歧只发生在一个 warp 内部；不同的 warp 会独立执行，无论它们是否执行相同或不相交的代码路径。
- en: The PTX specification makes no mention of a branch synchronization stack, so
    the only publicly available evidence of its existence is in the disassembly output
    of `cuobjdump`. The `SSY` instruction pushes a state such as the program counter
    and active thread mask onto the stack; the `.S` instruction prefix pops this state
    and, if any active threads did not take the branch, causes those threads to execute
    the code path whose state was snapshotted by `SSY`.
  id: totrans-2215
  prefs: []
  type: TYPE_NORMAL
  zh: PTX 规范中没有提到分支同步栈，因此唯一可以公开获取的关于其存在的证据是 `cuobjdump` 的反汇编输出。`SSY` 指令将诸如程序计数器和活动线程掩码等状态推送到栈上；`.S`
    指令前缀则会弹出这个状态，如果有活动线程没有执行分支，它会导致这些线程执行由 `SSY` 快照的代码路径。
- en: '`SSY/.S` is only necessary when threads of execution may diverge, so if the
    compiler can guarantee that threads will stay uniform in a code path, you may
    see branches that are not bracketed by `SSY/.S`. The important thing to realize
    about branching in CUDA is that in all cases, it is most efficient for all threads
    within a warp to follow the same execution path.'
  id: totrans-2216
  prefs: []
  type: TYPE_NORMAL
  zh: '`SSY/.S` 只有在执行线程可能会分歧时才有必要，因此如果编译器能够保证线程在代码路径中保持一致，你可能会看到没有被`SSY/.S`括起来的分支。在CUDA中，关于分支的一个重要认识是，在所有情况下，最有效的做法是保证同一个warp中的所有线程都执行相同的路径。'
- en: The loop in [Listing 8.2](ch08.html#ch08lis02) also includes a good self-contained
    example of divergence and convergence. The `SSY` instruction (offset 0x40) and
    `NOP.S` instruction (offset 0x78) bracket the points of divergence and convergence,
    respectively. The code loops over the `LDSLK` and subsequent predicated instructions,
    retiring active threads until the compiler knows that all threads will have converged
    and the branch synchronization stack can be popped with the `NOP.S` instruction.
  id: totrans-2217
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表8.2](ch08.html#ch08lis02)中的循环也提供了一个很好的关于分歧和汇聚的独立示例。`SSY`指令（偏移量0x40）和`NOP.S`指令（偏移量0x78）分别括住了分歧和汇聚的点。该代码在`LDSLK`及其后续的有条件指令之间循环，直到编译器知道所有线程已经汇聚，并且可以用`NOP.S`指令弹出分支同步栈。'
- en: /*0040*/ SSY 0x80;
  id: totrans-2218
  prefs: []
  type: TYPE_NORMAL
  zh: /*0040*/ SSY 0x80;
- en: /*0048*/ BAR.RED.POPC RZ, RZ;
  id: totrans-2219
  prefs: []
  type: TYPE_NORMAL
  zh: /*0048*/ BAR.RED.POPC RZ, RZ;
- en: /*0050*/ LD R0, [R0];
  id: totrans-2220
  prefs: []
  type: TYPE_NORMAL
  zh: /*0050*/ LD R0, [R0];
- en: /*0058*/ LDSLK P0, R2, [R3];
  id: totrans-2221
  prefs: []
  type: TYPE_NORMAL
  zh: /*0058*/ LDSLK P0, R2, [R3];
- en: /*0060*/ @P0 IADD R2, R2, R0;
  id: totrans-2222
  prefs: []
  type: TYPE_NORMAL
  zh: /*0060*/ @P0 IADD R2, R2, R0;
- en: /*0068*/ @P0 STSUL [R3], R2;
  id: totrans-2223
  prefs: []
  type: TYPE_NORMAL
  zh: /*0068*/ @P0 STSUL [R3], R2;
- en: /*0070*/ @!P0 BRA 0x58;
  id: totrans-2224
  prefs: []
  type: TYPE_NORMAL
  zh: /*0070*/ @!P0 BRA 0x58;
- en: /*0078*/ NOP.S CC.T;
  id: totrans-2225
  prefs: []
  type: TYPE_NORMAL
  zh: /*0078*/ NOP.S CC.T;
- en: '8.4.3\. Special Cases: Min, Max, and Absolute Value'
  id: totrans-2226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.3\. 特殊情况：最小值、最大值和绝对值
- en: Some conditional operations are so common that they are supported natively by
    the hardware. Minimum and maximum operations are supported for both integer and
    floating-point operands and are translated to a single instruction. Additionally,
    floating-point instructions include modifiers that can negate or take the absolute
    value of a source operand.
  id: totrans-2227
  prefs: []
  type: TYPE_NORMAL
  zh: 一些条件操作是如此常见，以至于硬件本身就原生支持它们。最小值和最大值操作支持整数和浮点操作数，并且被转化为单一指令。此外，浮点指令包括修饰符，可以对源操作数取反或求绝对值。
- en: The compiler does a good job of detecting when min/max operations are being
    expressed, but if you want to take no chances, call the `min()/max()` intrinsics
    for integers or `fmin()/fmax()` for floating-point values.
  id: totrans-2228
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器能够很好地检测出何时使用最小值/最大值操作，但如果你希望万无一失，可以调用整数的`min()/max()`内建函数，或者浮点数的`fmin()/fmax()`。
- en: 8.5\. Textures and Surfaces
  id: totrans-2229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5\. 纹理和表面
- en: The instructions that read and write textures and surfaces refer to much more
    implicit state than do other instructions; parameters such as the base address,
    dimensions, format, and interpretation of the texture contents are contained in
    a *header*, an intermediate data structure whose software abstraction is called
    a *texture reference* or *surface reference*. As developers manipulate the texture
    or surface references, the CUDA runtime and driver must translate those changes
    into the headers, which the texture or surface instruction references as an index.^([16](ch08.html#ch08fn16))
  id: totrans-2230
  prefs: []
  type: TYPE_NORMAL
  zh: 读取和写入纹理和表面的指令涉及的隐式状态比其他指令要多；诸如基地址、维度、格式以及纹理内容解释等参数都包含在一个*头部*中，这个中间数据结构的软件抽象称为*纹理引用*或*表面引用*。当开发者操作纹理或表面引用时，CUDA
    运行时和驱动程序必须将这些变化转换为头部，而纹理或表面指令则将头部作为索引引用。^([16](ch08.html#ch08fn16))
- en: '[16](ch08.html#ch08fn16a). SM 3.x added *texture objects*, which enable texture
    and surface headers to be referenced by address rather than an index. Previous
    hardware generations could reference at most 128 textures or surfaces in a kernel,
    but with SM 3.x the number is limited only by memory.'
  id: totrans-2231
  prefs: []
  type: TYPE_NORMAL
  zh: '[16](ch08.html#ch08fn16a)。SM 3.x 增加了*纹理对象*，使得纹理和表面头可以通过地址而非索引进行引用。之前的硬件代只能在内核中最多引用
    128 个纹理或表面，但在 SM 3.x 中，这个数量仅受内存限制。'
- en: Before launching a kernel that operates on textures or surfaces, the driver
    must ensure that all this state is set correctly on the hardware. As a result,
    launching such kernels may take longer. Texture reads are serviced through a specialized
    cache subsystem that is separate from the L1/L2 caches in Fermi, and also separate
    from the constant cache. Each SM has an L1 texture cache, and the TPCs (texture
    processor clusters) or GPCs (graphics processor clusters) each additionally have
    L2 texture cache. Surface reads and writes are serviced through the same L1/L2
    caches that service global memory traffic.
  id: totrans-2232
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动操作纹理或表面的内核之前，驱动程序必须确保所有这些状态在硬件上正确设置。因此，启动这样的内核可能需要更长时间。纹理读取通过一个专门的缓存子系统进行服务，这个子系统与
    Fermi 中的 L1/L2 缓存以及常量缓存是分开的。每个 SM 都有一个 L1 纹理缓存，TPC（纹理处理集群）或 GPC（图形处理集群）则分别还有 L2
    纹理缓存。表面的读取和写入则通过与全局内存流量服务相同的 L1/L2 缓存进行。
- en: 'Kepler added two technologies of note with respect to textures: the ability
    to read from global memory via the texture cache hierarchy without binding a texture
    reference, and the ability to specify a texture header by address rather than
    by index. The latter technology is known as “bindless textures.”'
  id: totrans-2233
  prefs: []
  type: TYPE_NORMAL
  zh: Kepler 引入了与纹理相关的两项重要技术：通过纹理缓存层次结构从全局内存读取数据，而无需绑定纹理引用的能力，以及通过地址而不是索引指定纹理头的能力。后一项技术被称为“无绑定纹理”。
- en: On SM 3.5 and later hardware, reading global memory via the texture cache can
    be requested by using `const __restrict` pointers or by explicitly invoking the
    `ldg()` intrinsics in `sm_35_intrinsics.h`.
  id: totrans-2234
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SM 3.5 及更高版本的硬件上，可以通过使用 `const __restrict` 指针或显式调用 `sm_35_intrinsics.h` 中的
    `ldg()` 内建函数来请求通过纹理缓存读取全局内存。
- en: 8.6\. Miscellaneous Instructions
  id: totrans-2235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6. 杂项指令
- en: 8.6.1\. Warp-Level Primitives
  id: totrans-2236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.6.1. Warp 级别的原语
- en: It did not take long for the importance of warps as a primitive unit of execution
    (naturally residing between threads and blocks) to become evident to CUDA programmers.
    Starting with SM 1.x, NVIDIA began adding instructions that specifically operate
    on warps.
  id: totrans-2237
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CUDA 程序员来说，warp 作为执行的基本单元（自然处于线程和块之间）的重要性很快显现出来。从 SM 1.x 开始，NVIDIA 开始添加专门操作
    warp 的指令。
- en: Vote
  id: totrans-2238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 投票
- en: That CUDA architectures are 32-bit and that warps are comprised of 32 threads
    made an irresistible match to instructions that can evaluate a condition and broadcast
    a 1-bit result to every thread in the warp. The `VOTE` instruction (first available
    in SM 1.2) evaluates a condition and broadcasts the result to all threads in the
    warp. The `__any()` intrinsic returns 1 if the predicate is true for *any* of
    the 32 threads in the warp. The `__all()` intrinsic returns 1 if the predicate
    is true for *all* of the 32 threads in the warp.
  id: totrans-2239
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 架构是 32 位的，warp 由 32 个线程组成，这使得能够评估条件并将 1 位结果广播到 warp 中每个线程的指令具有不可抗拒的匹配性。`VOTE`
    指令（SM 1.2 中首次提供）评估一个条件并将结果广播到 warp 中的所有线程。`__any()` 内建函数如果谓词对 warp 中 *任何* 32 个线程为真，则返回
    1。`__all()` 内建函数如果谓词对 warp 中的 *所有* 32 个线程为真，则返回 1。
- en: The Fermi architecture added a new variant of `VOTE` that passes back the predicate
    result for every thread in the warp. The `__ballot()` intrinsic evaluates a condition
    for all threads in the warp and returns a 32-bit value where each bit gives the
    condition for the corresponding thread in the warp.
  id: totrans-2240
  prefs: []
  type: TYPE_NORMAL
  zh: Fermi 架构新增了 `VOTE` 的变体，它返回每个线程在 warp 中的谓词结果。`__ballot()` 内建函数评估 warp 中所有线程的条件，并返回一个
    32 位值，每一位代表对应线程在 warp 中的条件。
- en: Shuffle
  id: totrans-2241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 洗牌
- en: Kepler added *shuffle* instructions that enable data interchange between threads
    within a warp without staging the data through shared memory. Although these instructions
    execute with the same latency as shared memory, they have the benefit of doing
    the exchange without performing both a read and a write, and they can reduce shared
    memory usage.
  id: totrans-2242
  prefs: []
  type: TYPE_NORMAL
  zh: Kepler 添加了 *shuffle* 指令，使得线程之间的数据交换可以在不通过共享内存暂存数据的情况下完成。尽管这些指令与共享内存具有相同的延迟，但它们的优势在于无需执行读写操作即可完成交换，且能够减少共享内存的使用。
- en: The following instruction is wrapped in a number of device functions that use
    inline PTX assembly defined in `sm_30_intrinsics.h.`
  id: totrans-2243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下指令被包装在多个设备函数中，这些函数使用 `sm_30_intrinsics.h` 中定义的内联 PTX 汇编。
- en: '[Click here to view code image](ch08_images.html#p271pro01a)'
  id: totrans-2244
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch08_images.html#p271pro01a)'
- en: int __shfl(int var, int srcLane, int width=32);
  id: totrans-2245
  prefs: []
  type: TYPE_NORMAL
  zh: int __shfl(int var, int srcLane, int width=32);
- en: int __shfl_up(int var, unsigned int delta, int width=32);
  id: totrans-2246
  prefs: []
  type: TYPE_NORMAL
  zh: int __shfl_up(int var, unsigned int delta, int width=32);
- en: int __shfl_down(int var, unsigned int delta, int width=32);
  id: totrans-2247
  prefs: []
  type: TYPE_NORMAL
  zh: int __shfl_down(int var, unsigned int delta, int width=32);
- en: int __shfl_xor(int var, int laneMask, int width=32);
  id: totrans-2248
  prefs: []
  type: TYPE_NORMAL
  zh: int __shfl_xor(int var, int laneMask, int width=32);
- en: The `width` parameter, which defaults to the warp width of 32, must be a power
    of 2 in the range 2..32\. It enables subdivision of the warp into segments; if
    `width<32`, each subsection of the warp behaves as a separate entity with a starting
    logical lane ID of 0\. A thread may only exchange data with other threads in its
    subsection.
  id: totrans-2249
  prefs: []
  type: TYPE_NORMAL
  zh: '`width` 参数，默认值为 32 的 warp 宽度，必须是 2 的幂，范围为 2..32。它允许将 warp 分割成多个段；如果 `width<32`，warp
    的每个子部分将作为独立的实体运行，起始的逻辑 lane ID 为 0。线程只能与同一子部分中的其他线程交换数据。'
- en: '`__shfl`() returns the value of `var` held by the thread whose ID is given
    by `srcLane`. If `srcLane` is outside the range `0..width-1`, the thread’s own
    value of `var` is returned. This variant of the instruction can be used to broadcast
    values within a warp. `__shfl_up()` calculates a source lane ID by subtracting
    `delta` from the caller’s lane ID and clamping to the range `0..width-1`. `__shfl_down()`
    calculates a source lane ID by adding `delta` to the caller’s lane ID.'
  id: totrans-2250
  prefs: []
  type: TYPE_NORMAL
  zh: '`__shfl`() 返回由线程 ID 为 `srcLane` 的线程持有的 `var` 的值。如果 `srcLane` 超出范围 `0..width-1`，则返回线程自身的
    `var` 值。此指令的变体可以用于在一个 warp 内部广播值。`__shfl_up()` 通过从调用者的 lane ID 中减去 `delta` 来计算源
    lane ID，并将其限制在 `0..width-1` 范围内。`__shfl_down()` 通过将 `delta` 加到调用者的 lane ID 上来计算源
    lane ID。'
- en: '`__shfl_up()` and `__shfl_down()` enable warp-level scan and reverse scan operations,
    respectively. `__shfl_xor()` calculates a source lane ID by performing a bitwise
    XOR of the caller’s lane ID with `laneMask`; the value of `var` held by the resulting
    lane ID is returned. This variant can be used to do a reduction across the warps
    (or subwarps); each thread computes the reduction using a differently ordered
    series of the associative operator.'
  id: totrans-2251
  prefs: []
  type: TYPE_NORMAL
  zh: '`__shfl_up()` 和 `__shfl_down()` 分别实现 warp 级别的扫描和逆向扫描操作。`__shfl_xor()` 通过对调用者的
    lane ID 和 `laneMask` 执行按位异或运算来计算源 lane ID；返回结果 lane ID 持有的 `var` 的值。这个变体可以用于执行
    warp（或子warp）间的归约操作；每个线程使用不同顺序的关联运算符系列来计算归约。'
- en: 8.6.2\. Block-Level Primitives
  id: totrans-2252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.6.2. 块级原语
- en: The `__syncthreads()` intrinsic serves as a barrier. It causes all threads to
    wait until every thread in the threadblock has arrived at the `__syncthreads()`.
    The Fermi instruction set (SM 2.x) added several new block-level barriers that
    aggregate information about the threads in the threadblock.
  id: totrans-2253
  prefs: []
  type: TYPE_NORMAL
  zh: '`__syncthreads()` 内建函数充当一个屏障。它会使所有线程等待，直到线程块中的每个线程都到达 `__syncthreads()`。Fermi
    指令集（SM 2.x）添加了几个新的块级屏障，这些屏障聚合了线程块中线程的信息。'
- en: '• `__syncthreads_count()`: evaluates a predicate and returns the sum of threads
    for which the predicate was true'
  id: totrans-2254
  prefs: []
  type: TYPE_NORMAL
  zh: • `__syncthreads_count()`：评估谓词并返回谓词为真的线程数之和
- en: '• `__syncthreads_or()`: returns the OR of all the inputs across the threadblock'
  id: totrans-2255
  prefs: []
  type: TYPE_NORMAL
  zh: • `__syncthreads_or()`：返回线程块中所有输入的按位 OR 运算结果
- en: '• `__syncthreads_and()`: returns the AND of all the inputs across the threadblock'
  id: totrans-2256
  prefs: []
  type: TYPE_NORMAL
  zh: • `__syncthreads_and()`：返回线程块中所有输入的与运算结果
- en: 8.6.3\. Performance Counter
  id: totrans-2257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.6.3\. 性能计数器
- en: Developers can define their own set of performance counters and increment them
    in live code with the `__prof_trigger()` intrinsic.
  id: totrans-2258
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以定义自己的一组性能计数器，并使用`__prof_trigger()`内建函数在实时代码中递增它们。
- en: void __prof_trigger(int counter);
  id: totrans-2259
  prefs: []
  type: TYPE_NORMAL
  zh: void __prof_trigger(int counter);
- en: Calling this function increments the corresponding counter by 1 per warp. `counter`
    must be in the range 0..7; counters 8..15 are reserved. The value of the counters
    may be obtained by listing `prof_trigger_00..prof_trigger_07` in the profiler
    configuration file.
  id: totrans-2260
  prefs: []
  type: TYPE_NORMAL
  zh: 调用此函数将每个warp对应的计数器增加1。`counter`必须在0到7的范围内；8到15的计数器是保留的。计数器的值可以通过在性能分析器配置文件中列出`prof_trigger_00..prof_trigger_07`来获取。
- en: 8.6.4\. Video Instructions
  id: totrans-2261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.6.4\. 视频指令
- en: The video instructions described in this section are accessible only via the
    inline PTX assembler. Their basic functionality is described here to help developers
    to decide whether they might be beneficial for their application. Anyone intending
    to use these instructions, however, should consult the PTX ISA specification.
  id: totrans-2262
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述的视频指令仅通过内联PTX汇编器可访问。这里描述了它们的基本功能，旨在帮助开发人员判断这些指令是否对他们的应用有益。然而，任何打算使用这些指令的人都应查阅PTX
    ISA规范。
- en: Scalar Video Instructions
  id: totrans-2263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标量视频指令
- en: The scalar video instructions, added with SM 2.0 hardware, enable efficient
    operations on the short (8- and 16-bit) integer types needed for video processing.
    As described in the PTX 3.1 ISA Specification, the format of these instructions
    is as follows.
  id: totrans-2264
  prefs: []
  type: TYPE_NORMAL
  zh: 新增的SM 2.0硬件支持标量视频指令，使得在视频处理所需的短整数类型（8位和16位）上可以高效执行操作。正如PTX 3.1 ISA规范中所描述的，这些指令的格式如下。
- en: '[Click here to view code image](ch08_images.html#p273pro01a)'
  id: totrans-2265
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch08_images.html#p273pro01a)'
- en: vop.dtype.atype.btype{.sat} d, a{.asel}, b{.bsel};
  id: totrans-2266
  prefs: []
  type: TYPE_NORMAL
  zh: vop.dtype.atype.btype{.sat} d, a{.asel}, b{.bsel};
- en: vop.dtype.atype.btype{.sat}.secop d, a{.asel}, b{.bsel}, c;
  id: totrans-2267
  prefs: []
  type: TYPE_NORMAL
  zh: vop.dtype.atype.btype{.sat}.secop d, a{.asel}, b{.bsel}, c;
- en: 'The source and destination operands are all 32-bit registers. `dtype`, `atype`,
    and `btype` may be `.u32` or `.s32` for unsigned and signed 32-bit integers, respectively.
    The `asel/bsel` specifiers select which 8- or 16-bit value to extract from the
    source operands: `b0`, `b1`, `b2`, and `b3` select bytes (numbering from the least
    significant), and `h0/h1` select the least significant and most significant 16
    bits, respectively.'
  id: totrans-2268
  prefs: []
  type: TYPE_NORMAL
  zh: 源操作数和目标操作数都是32位寄存器。`dtype`、`atype`和`btype`可以是`.u32`或`.s32`，分别表示无符号和有符号的32位整数。`asel/bsel`修饰符用于选择从源操作数中提取的8位或16位值：`b0`、`b1`、`b2`和`b3`选择字节（从最低有效位开始编号），而`h0/h1`分别选择最低有效和最高有效的16位。
- en: Once the input values are extracted, they are sign- or zero-extended internally
    to signed 33-bit integers, and the primary operation is performed, producing a
    34-bit intermediate result whose sign depends on `dtype`. Finally, the result
    is clamped to the output range, and one of the following operations is performed.
  id: totrans-2269
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦提取输入值，它们会在内部扩展为带符号的33位整数或零扩展，进行主要操作，生成一个34位的中间结果，其符号取决于`dtype`。最后，结果被限制在输出范围内，并执行以下其中一种操作。
- en: '**1.** Apply a second operation (add, min or max) to the intermediate result
    and a third operand.'
  id: totrans-2270
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.** 对中间结果和第三个操作数应用第二个操作（加法、最小值或最大值）。'
- en: '**2.** Truncate the intermediate result to an 8- or 16-bit value and merge
    into a specified position in the third operand to produce the final result.'
  id: totrans-2271
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** 将中间结果截断为8位或16位值，并合并到第三个操作数的指定位置，生成最终结果。'
- en: The lower 32 bits are then written to the destination operand.
  id: totrans-2272
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将低32位写入目标操作数。
- en: The `vset` instruction performs a comparison between the 8-, 16-, or 32-bit
    input operands and generates the corresponding predicate (1 or 0) as output. The
    PTX scalar video instructions and the corresponding operations are given in [Table
    8.14](ch08.html#ch08tab14).
  id: totrans-2273
  prefs: []
  type: TYPE_NORMAL
  zh: '`vset`指令对8位、16位或32位输入操作数进行比较，并生成相应的谓词（1或0）作为输出。PTX标量视频指令及其对应的操作在[表8.14](ch08.html#ch08tab14)中给出。'
- en: '![Image](graphics/08tab14.jpg)'
  id: totrans-2274
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/08tab14.jpg)'
- en: '*Table 8.14* Scalar Video Instructions.'
  id: totrans-2275
  prefs: []
  type: TYPE_NORMAL
  zh: '*表8.14* 标量视频指令。'
- en: Vector Video Instructions (SM 3.0 only)
  id: totrans-2276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 向量视频指令（仅SM 3.0）
- en: These instructions, added with SM 3.0, are similar to the scalar video instructions
    in that they promote the inputs to a canonical integer format, perform the core
    operation, and then clamp and optionally merge the output. But they deliver higher
    performance by operating on pairs of 16-bit values or quads of 8-bit values.
  id: totrans-2277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指令是与SM 3.0一起新增的，类似于标量视频指令，它们将输入提升为规范的整数格式，执行核心操作，然后限制并可选择地合并输出。但它们通过操作一对16位值或一组8位值的四元组来提供更高的性能。
- en: '[Table 8.15](ch08.html#ch08tab15) summarizes the PTX instructions and corresponding
    operations implemented by these instructions. They are most useful for video processing
    and certain image processing operations (such as the median filter).'
  id: totrans-2278
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8.15](ch08.html#ch08tab15)总结了这些指令和对应的操作。这些指令对视频处理和某些图像处理操作（例如中值滤波）最为有用。'
- en: '![Image](graphics/08tab15.jpg)'
  id: totrans-2279
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/08tab15.jpg)'
- en: '*Table 8.15* Vector Video Instructions'
  id: totrans-2280
  prefs: []
  type: TYPE_NORMAL
  zh: '*表8.15* 向量视频指令'
- en: 8.6.5\. Special Registers
  id: totrans-2281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.6.5\. 特殊寄存器
- en: Many special registers are accessed by referencing the built-in variables `threadIdx`,
    `blockIdx`, `blockDim`, and `gridDim`. These pseudo-variables, described in detail
    in [Section 7.3](ch07.html#ch07lev1sec3), are 3-dimensional structures that specify
    the thread ID, block ID, thread count, and block count, respectively.
  id: totrans-2282
  prefs: []
  type: TYPE_NORMAL
  zh: 许多特殊寄存器是通过引用内置变量`threadIdx`、`blockIdx`、`blockDim`和`gridDim`来访问的。这些伪变量在[第7.3节](ch07.html#ch07lev1sec3)中有详细描述，它们是三维结构，分别指定线程ID、块ID、线程数和块数。
- en: Besides those, another special register is the SM’s clock register, which increments
    with each clock cycle. This counter can be read with the `__clock()` or `__clock64()`
    intrinsic. The counters are separately tracked for each SM and, like the time
    stamp counters on CPUs, are most useful for measuring relative performance of
    different code sequences and best avoided when trying to calculate wall clock
    times.
  id: totrans-2283
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，还有一个特殊的寄存器是 SM 的时钟寄存器，它随着每个时钟周期递增。可以通过 `__clock()` 或 `__clock64()` 内建函数读取这个计数器。每个
    SM 都会分别跟踪这些计数器，类似于 CPU 上的时间戳计数器，它们最适用于测量不同代码序列的相对性能，尽量避免用于计算墙钟时间。
- en: 8.7\. Instruction Sets
  id: totrans-2284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7\. 指令集
- en: 'NVIDIA has developed three major architectures: Tesla (SM 1.x), Fermi (SM 2.x),
    and Kepler (SM 3.x). Within those families, new instructions have been added as
    NVIDIA updated their products. For example, global atomic operations were not
    present in the very first Tesla-class processor (the G80, which shipped in 2006
    as the GeForce GTX 8800), but all subsequent Tesla-class GPUs included them. So
    when querying the SM version via `cuDeviceComputeCapability()`, the major and
    minor versions will be 1.0 for G80 and 1.1 (or greater) for all other Tesla-class
    GPUs. Conversely, if the SM version is 1.1 or greater, the application can use
    global atomics.'
  id: totrans-2285
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 已经开发了三种主要架构：Tesla（SM 1.x）、Fermi（SM 2.x）和 Kepler（SM 3.x）。在这些架构中，随着 NVIDIA
    更新其产品，新增了新的指令。例如，在最早的 Tesla 类处理器（G80，2006 年发布的 GeForce GTX 8800）中没有全球原子操作，但所有后续的
    Tesla 类 GPU 都包括了该功能。因此，当通过 `cuDeviceComputeCapability()` 查询 SM 版本时，G80 的主次版本为
    1.0，所有其他 Tesla 类 GPU 的主次版本为 1.1（或更高）。相反，如果 SM 版本为 1.1 或更高，则应用程序可以使用全球原子操作。
- en: '[Table 8.16](ch08.html#ch08tab16) gives the SASS instructions that may be printed
    by `cuobjdump` when disassembling microcode for Tesla-class (SM 1.x) hardware.
    The Fermi and Kepler instruction sets closely resemble each other, with the exception
    of the instructions that support surface load/store, so their instruction sets
    are given together in [Table 8.17](ch08.html#ch08tab17). In both tables, the middle
    column specifies the first SM version to support a given instruction.'
  id: totrans-2286
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 8.16](ch08.html#ch08tab16) 给出了 `cuobjdump` 在反汇编 Tesla 类硬件（SM 1.x）的微代码时可能打印的
    SASS 指令。Fermi 和 Kepler 指令集非常相似，唯一的例外是支持表面加载/存储的指令，因此它们的指令集合并在 [表 8.17](ch08.html#ch08tab17)
    中给出。在这两张表格中，居中的列指定了支持给定指令的第一个 SM 版本。'
- en: '![Image](graphics/08tab16.jpg)![Image](graphics/08tab16a.jpg)![Image](graphics/08tab16b.jpg)![Image](graphics/08tab16c.jpg)![Image](graphics/08tab16d.jpg)'
  id: totrans-2287
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/08tab16.jpg)![图片](graphics/08tab16a.jpg)![图片](graphics/08tab16b.jpg)![图片](graphics/08tab16c.jpg)![图片](graphics/08tab16d.jpg)'
- en: '*Table 8.16.* SM 1.x Instruction Set'
  id: totrans-2288
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 8.16.* SM 1.x 指令集'
- en: '![Image](graphics/08tab17.jpg)![Image](graphics/08tab17a.jpg)![Image](graphics/08tab17b.jpg)![Image](graphics/08tab17c.jpg)![Image](graphics/08tab17d.jpg)![Image](graphics/08tab17e.jpg)![Image](graphics/08tab17f.jpg)![Image](graphics/08tab17g.jpg)'
  id: totrans-2289
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/08tab17.jpg)![Image](graphics/08tab17a.jpg)![Image](graphics/08tab17b.jpg)![Image](graphics/08tab17c.jpg)![Image](graphics/08tab17d.jpg)![Image](graphics/08tab17e.jpg)![Image](graphics/08tab17f.jpg)![Image](graphics/08tab17g.jpg)'
- en: '*Table 8.17.* SM 2.x and SM 3.x Instruction Sets'
  id: totrans-2290
  prefs: []
  type: TYPE_NORMAL
  zh: '*表8.17.* SM 2.x和SM 3.x指令集'
- en: Chapter 9\. Multiple GPUs
  id: totrans-2291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章\. 多GPU
- en: This chapter describes CUDA’s facilities for multi-GPU programming, including
    threading models, peer-to-peer, and inter-GPU synchronization. As an example,
    we’ll first explore inter-GPU synchronization using CUDA streams and events by
    implementing a peer-to-peer memcpy that stages through portable pinned memory.
    We then discuss how to implement the N-body problem (fully described in [Chapter
    14](ch14.html#ch14)) with single- and multithreaded implementations that use multiple
    GPUs.
  id: totrans-2292
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了CUDA的多GPU编程功能，包括线程模型、点对点通信和GPU间同步。作为示例，我们首先通过实现一个通过便捷的固定内存进行分阶段传输的点对点内存复制（memcpy），来探讨如何使用CUDA流和事件实现GPU间同步。然后，我们讨论如何使用多个GPU实现单线程和多线程的N体问题（在[第14章](ch14.html#ch14)中有详细描述）。
- en: 9.1\. Overview
  id: totrans-2293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. 概述
- en: Systems with multiple GPUs generally contain multi-GPU boards with a PCI Express
    bridge chip (such as the GeForce GTX 690) or multiple PCI Express slots, or both,
    as described in [Section 2.3](ch02.html#ch02lev1sec3). Each GPU in such a system
    is separated by PCI Express bandwidth, so there is always a huge disparity in
    bandwidth between memory connected directly to a GPU (its device memory) and its
    connections to other GPUs as well as the CPU.
  id: totrans-2294
  prefs: []
  type: TYPE_NORMAL
  zh: 配备多个GPU的系统通常包含具有PCI Express桥接芯片（如GeForce GTX 690）的多GPU板卡，或者多个PCI Express插槽，或者两者兼有，如[第2.3节](ch02.html#ch02lev1sec3)所述。在这样的系统中，每个GPU之间由PCI
    Express带宽分隔，因此GPU直接连接的内存（其设备内存）与其与其他GPU以及CPU的连接之间总是存在巨大的带宽差距。
- en: Many CUDA features designed to run on multiple GPUs, such as peer-to-peer addressing,
    require the GPUs to be identical. For applications that can make assumptions about
    the target hardware (such as vertical applications built for specific hardware
    configurations), this requirement is innocuous enough. But applications targeting
    systems with a variety of GPUs (say, a low-power one for everyday use and a powerful
    one for gaming) may have to use heuristics to decide which GPU(s) to use or load-balance
    the workload across GPUs so the faster ones contribute more computation to the
    final output, commensurate with their higher performance.
  id: totrans-2295
  prefs: []
  type: TYPE_NORMAL
  zh: 许多设计用于在多个GPU上运行的CUDA功能，如点对点寻址，需要GPU是相同的。对于那些可以假设目标硬件配置的应用程序（例如为特定硬件配置构建的垂直应用程序），这一要求无关紧要。但针对具有多种GPU的系统（比如一个低功耗的日常使用GPU和一个用于游戏的高性能GPU）的应用程序，可能需要使用启发式方法来决定使用哪个GPU（或多个GPU），或者在多个GPU之间负载平衡，以便更快的GPU在最终输出中贡献更多的计算，以此来匹配其更高的性能。
- en: A key ingredient to all CUDA applications that use multiple GPUs is *portable
    pinned memory*. As described in [Section 5.1.2](ch05.html#ch05lev2sec2), portable
    pinned memory is pinned memory that is mapped for all CUDA contexts such that
    any GPU can read or write the memory directly.
  id: totrans-2296
  prefs: []
  type: TYPE_NORMAL
  zh: 所有使用多个GPU的CUDA应用程序的一个关键要素是*可移植的固定内存*。如[第5.1.2节](ch05.html#ch05lev2sec2)所述，可移植的固定内存是针对所有CUDA上下文映射的固定内存，任何GPU都可以直接读写该内存。
- en: CPU Threading Models
  id: totrans-2297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CPU线程模型
- en: Until CUDA 4.0, the only way to drive multiple GPUs was to create a CPU thread
    for each one. The `cudaSetDevice()` function had to be called once per CPU thread,
    before any CUDA code had executed, in order to tell CUDA which device to initialize
    when the CPU thread started to operate on CUDA. Whichever CPU thread made that
    call would then get exclusive access to the GPU, because the CUDA driver had not
    yet been made thread-safe in a way that would enable multiple threads to access
    the same GPU at the same time.
  id: totrans-2298
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA 4.0之前，驱动多个GPU的唯一方法是为每个GPU创建一个CPU线程。`cudaSetDevice()`函数必须在每个CPU线程上调用一次，在任何CUDA代码执行之前，以告诉CUDA当CPU线程开始在CUDA上操作时，应该初始化哪个设备。哪个CPU线程进行了此调用，哪个线程就会独占访问GPU，因为CUDA驱动程序当时尚未线程安全，无法让多个线程同时访问同一GPU。
- en: 'In CUDA 4.0, `cudaSetDevice()` was modified to implement the semantics that
    everyone had previously expected: It tells CUDA which GPU should perform subsequent
    CUDA operations. Having multiple threads operating on the same GPU at the same
    time may incur a slight performance hit, but it should be expected to work. Our
    example N-body application, however, only has one CPU thread operating on any
    given device at a time. The multithreaded formulation has each of *N* threads
    operate on a specific device, and the single-threaded formulation has one thread
    operate on each of the *N* devices in turn.'
  id: totrans-2299
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA 4.0中，`cudaSetDevice()`被修改为实现之前大家所期望的语义：它告诉CUDA哪个GPU应该执行随后的CUDA操作。多个线程同时在同一GPU上操作可能会导致略微的性能下降，但应该能够正常工作。然而，我们的示例N-body应用程序每次只在一个CPU线程上操作某个设备。多线程方案让每个*N*线程操作一个特定的设备，而单线程方案则让一个线程依次在每个*N*设备上操作。
- en: 9.2\. Peer-to-Peer
  id: totrans-2300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2\. 点对点
- en: 'When multiple GPUs are used by a CUDA program, they are known as “peers” because
    the application generally treats them equally, as if they were coworkers collaborating
    on a project. CUDA enables two flavors of peer-to-peer: explicit memcpy and peer-to-peer
    addressing.^([1](ch09.html#ch09fn1))'
  id: totrans-2301
  prefs: []
  type: TYPE_NORMAL
  zh: 当CUDA程序使用多个GPU时，它们被称为“peer”，因为应用程序通常将它们平等对待，就像是同事在协作完成一个项目。CUDA支持两种类型的点对点：显式内存拷贝和点对点地址分配。^([1](ch09.html#ch09fn1))
- en: '[1](ch09.html#ch09fn1a). For peer-to-peer addressing, the term *peer* also
    harkens to the requirement that the GPUs be identical.'
  id: totrans-2302
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch09.html#ch09fn1a). 对于点对点地址分配，术语*peer*也意味着要求GPU是相同的。'
- en: 9.2.1\. Peer-to-Peer Memcpy
  id: totrans-2303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1\. 点对点内存拷贝
- en: Memory copies can be performed between the memories of any two different devices.
    When UVA (Unified Virtual Addressing) is in effect, the ordinary family of memcpy
    function can be used for peer-to-peer memcpy, since CUDA can infer which device
    “owns” which memory. If UVA is not in effect, the peer-to-peer memcpy must be
    done explicitly using `cudaMemcpyPeer()`, `cudaMemcpyPeerAsync()`, `cudaMemcpy3DPeer()`,
    or `cudaMemcpy3DPeerAsync()`.
  id: totrans-2304
  prefs: []
  type: TYPE_NORMAL
  zh: 内存复制可以在任何两个不同设备的内存之间执行。当启用UVA（统一虚拟地址）时，可以使用普通的memcpy函数进行设备间的内存复制，因为CUDA可以推断出哪个设备“拥有”哪个内存。如果UVA未启用，则必须显式地使用`cudaMemcpyPeer()`、`cudaMemcpyPeerAsync()`、`cudaMemcpy3DPeer()`或`cudaMemcpy3DPeerAsync()`进行设备间的内存复制。
- en: '* * *'
  id: totrans-2305
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-2306
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: CUDA can copy memory between any two devices, not just devices that can directly
    address one another’s memory. If necessary, CUDA will stage the memory copy through
    host memory, which can be accessed by any device in the system.
  id: totrans-2307
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA可以在任何两个设备之间复制内存，而不仅仅是能够直接访问彼此内存的设备。如果有必要，CUDA会通过主机内存进行内存复制，主机内存可以被系统中的任何设备访问。
- en: '* * *'
  id: totrans-2308
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Peer-to-peer memcpy operations do not run concurrently with any other operation.
    Any pending operations on either GPU must complete before the peer-to-peer memcpy
    can begin, and no subsequent operations can start to execute until after the peer-to-peer
    memcpy is done. When possible, CUDA will use direct peer-to-peer mappings between
    the two pointers. The resulting copies are faster and do not have to be staged
    through host memory.
  id: totrans-2309
  prefs: []
  type: TYPE_NORMAL
  zh: 设备间内存复制操作不能与其他任何操作并行执行。任何待处理的操作必须在设备间内存复制开始之前完成，且在设备间内存复制完成之前，不能开始执行后续操作。CUDA在可能的情况下会直接使用设备间的内存映射。这样复制的速度更快，并且不需要通过主机内存进行中转。
- en: 9.2.2\. Peer-to-Peer Addressing
  id: totrans-2310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2\. 设备间寻址
- en: Peer-to-peer mappings of device memory, shown in [Figure 2.20](ch02.html#ch02fig20),
    enable a kernel running on one GPU to read or write memory that resides in another
    GPU. Since the GPUs can only use peer-to-peer to read or write data at PCI Express
    rates, developers have to partition the workload in such a way that
  id: totrans-2311
  prefs: []
  type: TYPE_NORMAL
  zh: 设备内存的设备间映射，如[图2.20](ch02.html#ch02fig20)所示，允许在一个GPU上运行的内核读取或写入驻留在另一个GPU上的内存。由于GPU只能以PCI
    Express速率进行设备间数据读写，开发人员必须将工作负载划分为这样一种方式：
- en: '**1.** Each GPU has about an equal amount of work to do.'
  id: totrans-2312
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.** 每个GPU大致有相等的工作量要完成。'
- en: '**2.** The GPUs only need to interchange modest amounts of data.'
  id: totrans-2313
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** GPU只需要交换适量的数据。'
- en: Examples of such systems might be a pipelined computer vision system where each
    stage in the pipeline of GPUs computes an intermediate data structure (e.g., locations
    of identified features) that needs to be further analyzed by the next GPU in the
    pipeline or a large so-called “stencil” computation in which separate GPUs can
    perform most of the computation independently but must exchange edge data between
    computation steps.
  id: totrans-2314
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的系统示例可能是一个管道化的计算机视觉系统，其中管道中的每个阶段的GPU计算一个中间数据结构（例如，识别出的特征位置），该数据结构需要由管道中的下一个GPU进一步分析，或者是一个大型的所谓“模板”计算，在这种计算中，独立的GPU可以独立执行大部分计算，但必须在计算步骤之间交换边缘数据。
- en: In order for peer-to-peer addressing to work, the following conditions apply.
  id: totrans-2315
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使点对点寻址能够正常工作，必须满足以下条件。
- en: • Unified virtual addressing (UVA) must be in effect.
  id: totrans-2316
  prefs: []
  type: TYPE_NORMAL
  zh: • 必须启用统一虚拟地址（UVA）。
- en: • Both GPUs must be SM 2.x or higher and must be based on the same chip.
  id: totrans-2317
  prefs: []
  type: TYPE_NORMAL
  zh: • 两个GPU必须是SM 2.x或更高版本，并且必须基于相同的芯片。
- en: • The GPUs must be on the same I/O hub.
  id: totrans-2318
  prefs: []
  type: TYPE_NORMAL
  zh: • GPU必须位于相同的I/O集线器上。
- en: '`cu(da)DeviceCanAccessPeer ()` may be called to query whether the current device
    can map another device’s memory.'
  id: totrans-2319
  prefs: []
  type: TYPE_NORMAL
  zh: '`cu(da)DeviceCanAccessPeer()`可以调用来查询当前设备是否可以映射另一个设备的内存。'
- en: '[Click here to view code image](ch09_images.html#p290pro01a)'
  id: totrans-2320
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch09_images.html#p290pro01a)'
- en: cudaError_t cudaDeviceCanAccessPeer(int *canAccessPeer, int device,
  id: totrans-2321
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaDeviceCanAccessPeer(int *canAccessPeer, int device,
- en: int peerDevice);
  id: totrans-2322
  prefs: []
  type: TYPE_NORMAL
  zh: int peerDevice);
- en: CUresult cuDeviceCanAccessPeer(int *canAccessPeer, CUdevice device,
  id: totrans-2323
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuDeviceCanAccessPeer(int *canAccessPeer, CUdevice device,
- en: CUdevice peerDevice);
  id: totrans-2324
  prefs: []
  type: TYPE_NORMAL
  zh: CUdevice peerDevice);
- en: Peer-to-peer mappings are not enabled automatically; they must be specifically
    requested by calling `cudaDeviceEnablePeerAccess()` or `cuCtxEnablePeerAccess()`.
  id: totrans-2325
  prefs: []
  type: TYPE_NORMAL
  zh: 点对点映射不会自动启用；必须通过调用`cudaDeviceEnablePeerAccess()`或`cuCtxEnablePeerAccess()`显式请求。
- en: '[Click here to view code image](ch09_images.html#p290pro02a)'
  id: totrans-2326
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch09_images.html#p290pro02a)'
- en: cudaError_t cudaDeviceEnablePeerAccess(int peerDevice, unsigned int
  id: totrans-2327
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaDeviceEnablePeerAccess(int peerDevice, unsigned int
- en: flags);
  id: totrans-2328
  prefs: []
  type: TYPE_NORMAL
  zh: flags);
- en: CUresult cuCtxEnablePeerAccess(CUcontext peerContext, unsigned int
  id: totrans-2329
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuCtxEnablePeerAccess(CUcontext peerContext, unsigned int
- en: Flags);
  id: totrans-2330
  prefs: []
  type: TYPE_NORMAL
  zh: Flags);
- en: Once peer-to-peer access has been enabled, all memory in the peer device—including
    new allocations—is accessible to the current device until `cudaDeviceDisablePeerAccess()`
    or `cuCtxDisablePeerAccess()` is called.
  id: totrans-2331
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启用了点对点访问，所有同伴设备中的内存——包括新的分配——都可以被当前设备访问，直到调用`cudaDeviceDisablePeerAccess()`或`cuCtxDisablePeerAccess()`为止。
- en: Peer-to-peer access uses a small amount of extra memory (to hold more page tables)
    and makes memory allocation more expensive, since the memory must be mapped for
    all participating devices. Peer-to-peer functionality enables contexts to read
    and write memory belonging to other contexts, both via memcpy (which may be implemented
    by staging through system memory) and directly by having kernels read or write
    global memory pointers.
  id: totrans-2332
  prefs: []
  type: TYPE_NORMAL
  zh: 点对点访问使用少量额外的内存（用于保存更多的页表），并使得内存分配更加昂贵，因为内存必须为所有参与的设备进行映射。点对点功能允许上下文读取和写入属于其他上下文的内存，既可以通过
    memcpy（可能通过系统内存进行阶段性实现）实现，也可以通过内核直接读取或写入全局内存指针。
- en: The `cudaDeviceEnablePeerAccess()` function maps the memory belonging to another
    device. Peer-to-peer memory addressing is asymmetric; it is possible for GPU A
    to map GPU B’s allocations without its allocations being available to GPU B. In
    order for two GPUs to see each other’s memory, each GPU must explicitly map the
    other’s memory.
  id: totrans-2333
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaDeviceEnablePeerAccess()` 函数将另一个设备的内存映射到当前设备。点对点内存寻址是非对称的；GPU A 可以映射 GPU
    B 的分配，而 GPU B 的分配则不会被 GPU A 访问。为了让两块 GPU 能够互相看到对方的内存，每个 GPU 必须明确映射对方的内存。'
- en: '[Click here to view code image](ch09_images.html#p290pro03a)'
  id: totrans-2334
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch09_images.html#p290pro03a)'
- en: // tell device 1 to map device 0 memory
  id: totrans-2335
  prefs: []
  type: TYPE_NORMAL
  zh: // 告诉设备 1 映射设备 0 的内存
- en: cudaSetDevice( 1 );
  id: totrans-2336
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSetDevice( 1 );
- en: cudaDeviceEnablePeerAccess( 0, cudaPeerAccessDefault );
  id: totrans-2337
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceEnablePeerAccess( 0, cudaPeerAccessDefault );
- en: // tell device 0 to map device 1 memory
  id: totrans-2338
  prefs: []
  type: TYPE_NORMAL
  zh: // 告诉设备 0 映射设备 1 的内存
- en: cudaSetDevice( 0 );
  id: totrans-2339
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSetDevice( 0 );
- en: cudaDeviceEnablePeerAccess( 1, cudaPeerAccessDefault );
  id: totrans-2340
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceEnablePeerAccess( 1, cudaPeerAccessDefault );
- en: '* * *'
  id: totrans-2341
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-2342
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: On GPU boards with PCI Express 3.0–capable bridge chips (such as the Tesla K10),
    the GPUs can communicate at PCI Express 3.0 speeds even if the board is plugged
    into a PCI Express 2.0 slot.
  id: totrans-2343
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有 PCI Express 3.0 能力的 GPU 板（如 Tesla K10）上，即使板卡插入 PCI Express 2.0 插槽，GPU 也能以
    PCI Express 3.0 的速度进行通信。
- en: '* * *'
  id: totrans-2344
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '9.3\. UVA: Inferring Device from Address'
  id: totrans-2345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '9.3\. UVA: 从地址推断设备'
- en: Since UVA is always enabled on peer-to-peer-capable systems, the address ranges
    for different devices do not overlap, and the driver can infer the owning device
    from a pointer value. The `cuPointerGetAttribute()` function may be used to query
    information about UVA pointers, including the owning context.
  id: totrans-2346
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 UVA 在支持点对点的系统上始终启用，不同设备的地址范围不会重叠，驱动程序可以通过指针值推断拥有设备。`cuPointerGetAttribute()`
    函数可用于查询有关 UVA 指针的信息，包括所属上下文。
- en: '[Click here to view code image](ch09_images.html#p291pro01a)'
  id: totrans-2347
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch09_images.html#p291pro01a)'
- en: CUresult CUDAAPI cuPointerGetAttribute(void *data, CUpointer_
  id: totrans-2348
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuPointerGetAttribute(void *data, CUpointer_
- en: attribute attribute, CUdeviceptr ptr);
  id: totrans-2349
  prefs: []
  type: TYPE_NORMAL
  zh: attribute attribute, CUdeviceptr ptr);
- en: '`cuPointerGetAttribute()` or `cudaPointerGetAttributes()` may be used to query
    the attributes of a pointer. [Table 9.1](ch09.html#ch09tab01) gives the values
    that can be passed into `cuPointerGetAttribute()`; the structure passed back by
    `cudaPointerGetAttributes()` is as follows.'
  id: totrans-2350
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuPointerGetAttribute()` 或 `cudaPointerGetAttributes()` 可用于查询指针的属性。[表 9.1](ch09.html#ch09tab01)
    给出了可以传递给 `cuPointerGetAttribute()` 的值；`cudaPointerGetAttributes()` 返回的结构如下所示。'
- en: '[Click here to view code image](ch09_images.html#p291pro02a)'
  id: totrans-2351
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch09_images.html#p291pro02a)'
- en: struct cudaPointerAttributes {
  id: totrans-2352
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPointerAttributes {
- en: enum cudaMemoryType memoryType;
  id: totrans-2353
  prefs: []
  type: TYPE_NORMAL
  zh: enum cudaMemoryType memoryType;
- en: int device;
  id: totrans-2354
  prefs: []
  type: TYPE_NORMAL
  zh: int device;
- en: void *devicePointer;
  id: totrans-2355
  prefs: []
  type: TYPE_NORMAL
  zh: void *devicePointer;
- en: void *hostPointer;
  id: totrans-2356
  prefs: []
  type: TYPE_NORMAL
  zh: void *hostPointer;
- en: '}'
  id: totrans-2357
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '![Image](graphics/09tab01.jpg)'
  id: totrans-2358
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/09tab01.jpg)'
- en: '*Table 9.1* `cuPointerGetAttribute()` Attributes'
  id: totrans-2359
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 9.1* `cuPointerGetAttribute()` 属性'
- en: '`memoryType` may be `cudaMemoryTypeHost` or `cudaMemoryTypeDevice`.'
  id: totrans-2360
  prefs: []
  type: TYPE_NORMAL
  zh: '`memoryType` 可能是 `cudaMemoryTypeHost` 或 `cudaMemoryTypeDevice`。'
- en: '`device` is the device for which the pointer was allocated. For device memory,
    `device` identifies the device where the memory corresponding to `ptr` was allocated.
    For host memory, `device` identifies the device that was current when the allocation
    was performed.'
  id: totrans-2361
  prefs: []
  type: TYPE_NORMAL
  zh: '`device` 是分配指针的设备。对于设备内存，`device` 标识与 `ptr` 对应的内存被分配到的设备。对于主机内存，`device` 标识分配时所处的设备。'
- en: '`devicePointer` gives the device pointer value that may be used to reference
    `ptr` from the current device. If `ptr` cannot be accessed by the current device,
    `devicePointer` is `NULL`.'
  id: totrans-2362
  prefs: []
  type: TYPE_NORMAL
  zh: '`devicePointer` 给出了设备指针值，可以用来从当前设备引用 `ptr`。如果当前设备无法访问 `ptr`，则 `devicePointer`
    为 `NULL`。'
- en: '`hostPointer` gives the host pointer value that may be used to reference `ptr`
    from the CPU. If `ptr` cannot be accessed by the current host, `hostPointer` is
    `NULL`.'
  id: totrans-2363
  prefs: []
  type: TYPE_NORMAL
  zh: '`hostPointer` 给出了主机指针值，可以用来从 CPU 引用 `ptr`。如果当前主机无法访问 `ptr`，则 `hostPointer`
    为 `NULL`。'
- en: 9.4\. Inter-GPU Synchronization
  id: totrans-2364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4\. GPU 间同步
- en: CUDA events may be used for inter-GPU synchronization using `cu(da)StreamWaitEvent()`.
    If there is a producer/consumer relationship between two GPUs, the application
    can have the producer GPU record an event and then have the consumer GPU insert
    a stream-wait on that event into its command stream. When the consumer GPU encounters
    the stream-wait, it will stop processing commands until the producer GPU has passed
    the point of execution where `cu(da)EventRecord()` was called.
  id: totrans-2365
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 事件可用于通过 `cu(da)StreamWaitEvent()` 实现 GPU 间同步。如果两个 GPU 之间存在生产者/消费者关系，应用程序可以让生产者
    GPU 记录一个事件，然后让消费者 GPU 在其命令流中插入对该事件的流等待。当消费者 GPU 遇到流等待时，它将停止处理命令，直到生产者 GPU 执行到调用
    `cu(da)EventRecord()` 的位置。
- en: '* * *'
  id: totrans-2366
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-2367
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In CUDA 5.0, the device runtime, described in [Section 7.5](ch07.html#ch07lev1sec5),
    does not enable any inter-GPU synchronization whatsoever. That limitation may
    be relaxed in a future release.
  id: totrans-2368
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CUDA 5.0 中，设备运行时（在 [第 7.5 节](ch07.html#ch07lev1sec5) 中描述）不支持任何 GPU 间同步。这个限制可能会在未来的版本中放宽。
- en: '* * *'
  id: totrans-2369
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 9.1](ch09.html#ch09lis01) gives `chMemcpyPeerToPeer()`,^([2](ch09.html#ch09fn2))
    an implementation of peer-to-peer memcpy that uses portable memory and inter-GPU
    synchronization to implement the same type of memcpy that CUDA uses under the
    covers, if no direct mapping between the GPUs exists. The function works similarly
    to the `chMemcpyHtoD()` function in [Listing 6.2](ch06.html#ch06lis02) that performs
    host→device memcpy: A staging buffer is allocated in host memory, and the memcpy
    begins by having the source GPU copy source data into the staging buffer and recording
    an event. But unlike the host→device memcpy, there is never any need for the CPU
    to synchronize because all synchronization is done by the GPUs. Because both the
    memcpy and the event-record are asynchronous, immediately after kicking off the
    initial memcpy and event-record, the CPU can request that the destination GPU
    wait on that event and kick off a memcpy of the same buffer. Two staging buffers
    and two CUDA events are needed, so the two GPUs can copy to and from staging buffers
    concurrently, much as the CPU and GPU concurrently operate on staging buffers
    during the host→device memcpy. The CPU loops over the input buffer and output
    buffers, issuing memcpy and event-record commands and ping-ponging between staging
    buffers, until it has requested copies for all bytes and all that’s left to do
    is wait for both GPUs to finish processing.'
  id: totrans-2370
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 9.1](ch09.html#ch09lis01)给出了`chMemcpyPeerToPeer()`，^([2](ch09.html#ch09fn2))
    这是一个实现点对点memcpy的函数，使用便携式内存和GPU间同步来实现与CUDA底层相同类型的memcpy（如果两个GPU之间没有直接映射）。该函数的工作方式类似于[清单
    6.2](ch06.html#ch06lis02)中执行主机到设备memcpy的`chMemcpyHtoD()`函数：在主机内存中分配一个暂存缓冲区，memcpy开始时，源GPU将源数据复制到暂存缓冲区并记录一个事件。但与主机到设备memcpy不同的是，不需要CPU进行同步，因为所有同步都由GPU完成。由于memcpy和事件记录是异步的，因此在启动初始memcpy和事件记录后，CPU可以请求目标GPU等待该事件，并启动对同一缓冲区的memcpy。需要两个暂存缓冲区和两个CUDA事件，以便两个GPU可以并行地将数据复制到暂存缓冲区并从中复制，类似于CPU和GPU在主机到设备memcpy中并行操作暂存缓冲区的方式。CPU遍历输入和输出缓冲区，发出memcpy和事件记录命令，并在暂存缓冲区之间进行ping-pong，直到它请求完成所有字节的复制，剩下的就是等待两个GPU完成处理。'
- en: '[2](ch09.html#ch09fn2a). The `CUDART_CHECK` error handling has been removed
    for clarity.'
  id: totrans-2371
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch09.html#ch09fn2a)。为清晰起见，已移除`CUDART_CHECK`错误处理。'
- en: '* * *'
  id: totrans-2372
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-2373
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As with the implementations in the CUDA support provided by NVIDIA, our peer-to-peer
    memcpy is synchronous.
  id: totrans-2374
  prefs: []
  type: TYPE_NORMAL
  zh: 与NVIDIA提供的CUDA支持中的实现一样，我们的点对点memcpy是同步的。
- en: '* * *'
  id: totrans-2375
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 9.1.* `chMemcpyPeerToPeer()`.'
  id: totrans-2376
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 9.1.* `chMemcpyPeerToPeer()`。'
- en: '[Click here to view code image](ch09_images.html#p09lis01a)'
  id: totrans-2377
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch09_images.html#p09lis01a)'
- en: '* * *'
  id: totrans-2378
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: cudaError_t
  id: totrans-2379
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t
- en: chMemcpyPeerToPeer(
  id: totrans-2380
  prefs: []
  type: TYPE_NORMAL
  zh: chMemcpyPeerToPeer(
- en: void *_dst, int dstDevice,
  id: totrans-2381
  prefs: []
  type: TYPE_NORMAL
  zh: void *_dst, int dstDevice,
- en: const void *_src, int srcDevice,
  id: totrans-2382
  prefs: []
  type: TYPE_NORMAL
  zh: const void *_src, int srcDevice,
- en: size_t N )
  id: totrans-2383
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N )
- en: '{'
  id: totrans-2384
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-2385
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t 状态;
- en: char *dst = (char *) _dst;
  id: totrans-2386
  prefs: []
  type: TYPE_NORMAL
  zh: char *dst = (char *) _dst;
- en: const char *src = (const char *) _src;
  id: totrans-2387
  prefs: []
  type: TYPE_NORMAL
  zh: const char *src = (const char *) _src;
- en: int stagingIndex = 0;
  id: totrans-2388
  prefs: []
  type: TYPE_NORMAL
  zh: int stagingIndex = 0;
- en: while ( N ) {
  id: totrans-2389
  prefs: []
  type: TYPE_NORMAL
  zh: while ( N ) {
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  id: totrans-2390
  prefs: []
  type: TYPE_NORMAL
  zh: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
- en: cudaSetDevice( srcDevice );
  id: totrans-2391
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSetDevice( srcDevice );
- en: cudaStreamWaitEvent( 0, g_events[dstDevice][stagingIndex],0);
  id: totrans-2392
  prefs: []
  type: TYPE_NORMAL
  zh: cudaStreamWaitEvent( 0, g_events[dstDevice][stagingIndex], 0);
- en: cudaMemcpyAsync( g_hostBuffers[stagingIndex], src,
  id: totrans-2393
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync( g_hostBuffers[stagingIndex], src,
- en: thisCopySize, cudaMemcpyDeviceToHost, NULL );
  id: totrans-2394
  prefs: []
  type: TYPE_NORMAL
  zh: thisCopySize, cudaMemcpyDeviceToHost, NULL );
- en: cudaEventRecord( g_events[srcDevice][stagingIndex] );
  id: totrans-2395
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( g_events[srcDevice][stagingIndex] );
- en: cudaSetDevice( dstDevice );
  id: totrans-2396
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSetDevice( dstDevice );
- en: cudaStreamWaitEvent( 0, g_events[srcDevice][stagingIndex],0);
  id: totrans-2397
  prefs: []
  type: TYPE_NORMAL
  zh: cudaStreamWaitEvent( 0, g_events[srcDevice][stagingIndex], 0);
- en: cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
  id: totrans-2398
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync( dst, g_hostBuffers[stagingIndex],
- en: thisCopySize, cudaMemcpyHostToDevice, NULL );
  id: totrans-2399
  prefs: []
  type: TYPE_NORMAL
  zh: thisCopySize, cudaMemcpyHostToDevice, NULL );
- en: cudaEventRecord( g_events[dstDevice][stagingIndex] );
  id: totrans-2400
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( g_events[dstDevice][stagingIndex] );
- en: dst += thisCopySize;
  id: totrans-2401
  prefs: []
  type: TYPE_NORMAL
  zh: dst += thisCopySize;
- en: src += thisCopySize;
  id: totrans-2402
  prefs: []
  type: TYPE_NORMAL
  zh: src += thisCopySize;
- en: N -= thisCopySize;
  id: totrans-2403
  prefs: []
  type: TYPE_NORMAL
  zh: N -= thisCopySize;
- en: stagingIndex = 1 - stagingIndex;
  id: totrans-2404
  prefs: []
  type: TYPE_NORMAL
  zh: stagingIndex = 1 - stagingIndex;
- en: '}'
  id: totrans-2405
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // Wait until both devices are done
  id: totrans-2406
  prefs: []
  type: TYPE_NORMAL
  zh: // 等待两个设备都完成
- en: cudaSetDevice( srcDevice );
  id: totrans-2407
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSetDevice( srcDevice );
- en: cudaDeviceSynchronize();
  id: totrans-2408
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceSynchronize();
- en: cudaSetDevice( dstDevice );
  id: totrans-2409
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSetDevice( dstDevice );
- en: cudaDeviceSynchronize();
  id: totrans-2410
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceSynchronize();
- en: 'Error:'
  id: totrans-2411
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: return status;
  id: totrans-2412
  prefs: []
  type: TYPE_NORMAL
  zh: return status;
- en: '}'
  id: totrans-2413
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2414
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 9.5\. Single-Threaded Multi-GPU
  id: totrans-2415
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5\. 单线程多GPU
- en: When using the CUDA runtime, a single-threaded application can drive multiple
    GPUs by calling `cudaSetDevice()` to specify which GPU will be operated by the
    calling CPU thread. This idiom is used in [Listing 9.1](ch09.html#ch09lis01) to
    switch between the source and destination GPUs during the peer-to-peer memcpy,
    as well as the single-threaded, multi-GPU implementation of N-body described in
    [Section 9.5.2](ch09.html#ch09lev2sec4). In the driver API, CUDA maintains a stack
    of current contexts so that subroutines can easily change and restore the caller’s
    current context.
  id: totrans-2416
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CUDA运行时时，单线程应用程序可以通过调用`cudaSetDevice()`来指定调用CPU线程将操作哪个GPU，从而驱动多个GPU。在[Listing
    9.1](ch09.html#ch09lis01)中使用此习惯用法，在点对点内存拷贝期间在源GPU和目标GPU之间切换，以及在[Section 9.5.2](ch09.html#ch09lev2sec4)中描述的单线程多GPU的N体问题实现。在驱动程序API中，CUDA维护当前上下文的栈，以便子程序能够轻松更改和恢复调用者的当前上下文。
- en: 9.5.1\. Current Context Stack
  id: totrans-2417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.1\. 当前上下文栈
- en: 'Driver API applications can manage the current context with the current-context
    stack: `cuCtxPushCurrent()` makes a new context current, pushing it onto the top
    of the stack, and `cuCtxPopCurrent()` pops the current context and restores the
    previous current context. [Listing 9.2](ch09.html#ch09lis02) gives a driver API
    version of `chMemcpyPeerToPeer()`, which uses `cuCtxPopCurrent()` and `cuCtxPushCurrent()`
    to perform a peer-to-peer memcpy between two contexts.'
  id: totrans-2418
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动程序API应用程序可以通过当前上下文栈管理当前上下文：`cuCtxPushCurrent()`使一个新的上下文成为当前上下文，并将其压入栈顶，而`cuCtxPopCurrent()`则弹出当前上下文并恢复之前的当前上下文。[Listing
    9.2](ch09.html#ch09lis02)提供了`chMemcpyPeerToPeer()`的驱动程序API版本，该版本使用`cuCtxPopCurrent()`和`cuCtxPushCurrent()`在两个上下文之间执行点对点内存拷贝。
- en: The current context stack was introduced to CUDA in v2.2, and at the time, the
    CUDA runtime and driver API could not be used in the same application. That restriction
    has been relaxed in subsequent versions.
  id: totrans-2419
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的上下文堆栈是在CUDA v2.2版本中引入的，当时CUDA运行时和驱动程序API不能在同一应用程序中使用。此限制在后续版本中已被放宽。
- en: '*Listing 9.2.* `chMemcpyPeerToPeer` (driver API version).'
  id: totrans-2420
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 9.2.* `chMemcpyPeerToPeer` (驱动程序API版本)。'
- en: '[Click here to view code image](ch09_images.html#p09lis02a)'
  id: totrans-2421
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch09_images.html#p09lis02a)'
- en: '* * *'
  id: totrans-2422
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: CUresult
  id: totrans-2423
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult
- en: chMemcpyPeerToPeer(
  id: totrans-2424
  prefs: []
  type: TYPE_NORMAL
  zh: chMemcpyPeerToPeer(
- en: void *_dst, CUcontext dstContext, int dstDevice,
  id: totrans-2425
  prefs: []
  type: TYPE_NORMAL
  zh: void *_dst, CUcontext dstContext, int dstDevice,
- en: const void *_src, CUcontext srcContext, int srcDevice,
  id: totrans-2426
  prefs: []
  type: TYPE_NORMAL
  zh: const void *_src, CUcontext srcContext, int srcDevice,
- en: size_t N )
  id: totrans-2427
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N )
- en: '{'
  id: totrans-2428
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: CUresult status;
  id: totrans-2429
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult status;
- en: CUdeviceptr dst = (CUdeviceptr) (intptr_t) _dst;
  id: totrans-2430
  prefs: []
  type: TYPE_NORMAL
  zh: CUdeviceptr dst = (CUdeviceptr) (intptr_t) _dst;
- en: CUdeviceptr src = (CUdeviceptr) (intptr_t) _src;
  id: totrans-2431
  prefs: []
  type: TYPE_NORMAL
  zh: CUdeviceptr src = (CUdeviceptr) (intptr_t) _src;
- en: int stagingIndex = 0;
  id: totrans-2432
  prefs: []
  type: TYPE_NORMAL
  zh: int stagingIndex = 0;
- en: while ( N ) {
  id: totrans-2433
  prefs: []
  type: TYPE_NORMAL
  zh: while ( N ) {
- en: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
  id: totrans-2434
  prefs: []
  type: TYPE_NORMAL
  zh: size_t thisCopySize = min( N, STAGING_BUFFER_SIZE );
- en: CUDA_CHECK( cuCtxPushCurrent( srcContext ) );
  id: totrans-2435
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuCtxPushCurrent( srcContext ) );
- en: CUDA_CHECK( cuStreamWaitEvent(
  id: totrans-2436
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuStreamWaitEvent(
- en: NULL, g_events[dstDevice][stagingIndex], 0 ) );
  id: totrans-2437
  prefs: []
  type: TYPE_NORMAL
  zh: NULL, g_events[dstDevice][stagingIndex], 0 ) );
- en: CUDA_CHECK( cuMemcpyDtoHAsync(
  id: totrans-2438
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuMemcpyDtoHAsync(
- en: g_hostBuffers[stagingIndex],
  id: totrans-2439
  prefs: []
  type: TYPE_NORMAL
  zh: g_hostBuffers[stagingIndex],
- en: src,
  id: totrans-2440
  prefs: []
  type: TYPE_NORMAL
  zh: src,
- en: thisCopySize,
  id: totrans-2441
  prefs: []
  type: TYPE_NORMAL
  zh: thisCopySize,
- en: NULL ) );
  id: totrans-2442
  prefs: []
  type: TYPE_NORMAL
  zh: NULL ) );
- en: CUDA_CHECK( cuEventRecord(
  id: totrans-2443
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuEventRecord(
- en: g_events[srcDevice][stagingIndex],
  id: totrans-2444
  prefs: []
  type: TYPE_NORMAL
  zh: g_events[srcDevice][stagingIndex],
- en: 0 ) );
  id: totrans-2445
  prefs: []
  type: TYPE_NORMAL
  zh: 0 ) );
- en: CUDA_CHECK( cuCtxPopCurrent( &srcContext ) );
  id: totrans-2446
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuCtxPopCurrent( &srcContext ) );
- en: CUDA_CHECK( cuCtxPushCurrent( dstContext ) );
  id: totrans-2447
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuCtxPushCurrent( dstContext ) );
- en: CUDA_CHECK( cuStreamWaitEvent(
  id: totrans-2448
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuStreamWaitEvent(
- en: NULL,
  id: totrans-2449
  prefs: []
  type: TYPE_NORMAL
  zh: NULL,
- en: g_events[srcDevice][stagingIndex],
  id: totrans-2450
  prefs: []
  type: TYPE_NORMAL
  zh: g_events[srcDevice][stagingIndex],
- en: 0 ) );
  id: totrans-2451
  prefs: []
  type: TYPE_NORMAL
  zh: 0 ) );
- en: CUDA_CHECK( cuMemcpyHtoDAsync(
  id: totrans-2452
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuMemcpyHtoDAsync(
- en: dst,
  id: totrans-2453
  prefs: []
  type: TYPE_NORMAL
  zh: dst,
- en: g_hostBuffers[stagingIndex],
  id: totrans-2454
  prefs: []
  type: TYPE_NORMAL
  zh: g_hostBuffers[stagingIndex],
- en: thisCopySize,
  id: totrans-2455
  prefs: []
  type: TYPE_NORMAL
  zh: thisCopySize,
- en: NULL ) );
  id: totrans-2456
  prefs: []
  type: TYPE_NORMAL
  zh: NULL ) );
- en: CUDA_CHECK( cuEventRecord(
  id: totrans-2457
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuEventRecord(
- en: g_events[dstDevice][stagingIndex],
  id: totrans-2458
  prefs: []
  type: TYPE_NORMAL
  zh: g_events[dstDevice][stagingIndex],
- en: 0 ) );
  id: totrans-2459
  prefs: []
  type: TYPE_NORMAL
  zh: 0 ) );
- en: CUDA_CHECK( cuCtxPopCurrent( &dstContext ) );
  id: totrans-2460
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuCtxPopCurrent( &dstContext ) );
- en: dst += thisCopySize;
  id: totrans-2461
  prefs: []
  type: TYPE_NORMAL
  zh: dst += thisCopySize;
- en: src += thisCopySize;
  id: totrans-2462
  prefs: []
  type: TYPE_NORMAL
  zh: src += thisCopySize;
- en: N -= thisCopySize;
  id: totrans-2463
  prefs: []
  type: TYPE_NORMAL
  zh: N -= thisCopySize;
- en: stagingIndex = 1 - stagingIndex;
  id: totrans-2464
  prefs: []
  type: TYPE_NORMAL
  zh: stagingIndex = 1 - stagingIndex;
- en: '}'
  id: totrans-2465
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // Wait until both devices are done
  id: totrans-2466
  prefs: []
  type: TYPE_NORMAL
  zh: // 等待直到两个设备都完成
- en: CUDA_CHECK( cuCtxPushCurrent( srcContext ) );
  id: totrans-2467
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuCtxPushCurrent( srcContext ) );
- en: CUDA_CHECK( cuCtxSynchronize() );
  id: totrans-2468
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuCtxSynchronize() );
- en: CUDA_CHECK( cuCtxPopCurrent( &srcContext ) );
  id: totrans-2469
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuCtxPopCurrent( &srcContext ) );
- en: CUDA_CHECK( cuCtxPushCurrent( dstContext ) );
  id: totrans-2470
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuCtxPushCurrent( dstContext ) );
- en: CUDA_CHECK( cuCtxSynchronize() );
  id: totrans-2471
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuCtxSynchronize() );
- en: CUDA_CHECK( cuCtxPopCurrent( &dstContext ) );
  id: totrans-2472
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK( cuCtxPopCurrent( &dstContext ) );
- en: 'Error:'
  id: totrans-2473
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: return status;
  id: totrans-2474
  prefs: []
  type: TYPE_NORMAL
  zh: return status;
- en: '}'
  id: totrans-2475
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2476
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 9.5.2\. N-Body
  id: totrans-2477
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.5.2\. N-Body
- en: The N-body computation (described in detail in [Chapter 14](ch14.html#ch14))
    computes *N* forces in O(*N*²) time, and the outputs may be computed independently.
    On a system with *k* GPUs, our multi-GPU implementation splits the computation
    into *k* parts.
  id: totrans-2478
  prefs: []
  type: TYPE_NORMAL
  zh: N体计算（在[第14章](ch14.html#ch14)中有详细描述）以 O(*N*²) 的时间复杂度计算 *N* 个力，并且输出可以独立计算。在一个包含
    *k* 个GPU的系统中，我们的多GPU实现将计算分为 *k* 部分。
- en: Our implementation makes the common assumption that the GPUs are identical,
    so it divides the computation evenly. Applications targeting GPUs of unequal performance,
    or whose workloads have less predictable runtimes, can divide the computation
    more finely and have the host code submit work items to the GPUs from a queue.
  id: totrans-2479
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现假设GPU是相同的，因此它平均分配计算任务。针对性能不等的GPU或工作负载运行时间较难预测的应用程序，可以更细粒度地划分计算任务，并让主机代码从队列中将工作项提交给GPU。
- en: '[Listing 9.3](ch09.html#ch09lis03) gives a modified version of [Listing 14.3](ch14.html#ch14lis03)
    that takes two additional parameters (a base index `base` and size `n` of the
    subarray of forces) to compute a subset of the output array for an N-body computation.
    This `__device__` function is invoked by wrapper kernels that are declared as
    `__global__` . It is structured this way to reuse the code without incurring link
    errors. If the function were declared as `__global__`, the linker would generate
    an error about duplicate symbols.^([3](ch09.html#ch09fn3))'
  id: totrans-2480
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 9.3](ch09.html#ch09lis03) 给出了 [清单 14.3](ch14.html#ch14lis03) 的修改版，增加了两个额外的参数（基准索引
    `base` 和力的子数组大小 `n`），用于计算 N体计算的输出数组子集。这个 `__device__` 函数是由声明为 `__global__` 的包装核函数调用的。这样设计是为了重用代码而不引发链接错误。如果这个函数声明为
    `__global__`，链接器会报错，提示有重复符号。^([3](ch09.html#ch09fn3))'
- en: '[3](ch09.html#ch09fn3a). This is a bit of an old-school workaround. CUDA 5.0
    added a linker that enables the `__global__` function to be compiled into a static
    library and linked into the application.'
  id: totrans-2481
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch09.html#ch09fn3a)。这是一种有点老派的解决方法。CUDA 5.0 增加了一个链接器，使得 `__global__` 函数可以被编译成静态库并链接到应用程序中。'
- en: '*Listing 9.3.* N-body kernel (multi-GPU).'
  id: totrans-2482
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 9.3.* N体核函数（多GPU）。'
- en: '[Click here to view code image](ch09_images.html#p09lis03a)'
  id: totrans-2483
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch09_images.html#p09lis03a)'
- en: '* * *'
  id: totrans-2484
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: inline __device__ void
  id: totrans-2485
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ void
- en: ComputeNBodyGravitation_Shared_multiGPU(
  id: totrans-2486
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeNBodyGravitation_Shared_multiGPU(
- en: float *force,
  id: totrans-2487
  prefs: []
  type: TYPE_NORMAL
  zh: float *force,
- en: float *posMass,
  id: totrans-2488
  prefs: []
  type: TYPE_NORMAL
  zh: float *posMass,
- en: float softeningSquared,
  id: totrans-2489
  prefs: []
  type: TYPE_NORMAL
  zh: float softeningSquared,
- en: size_t base,
  id: totrans-2490
  prefs: []
  type: TYPE_NORMAL
  zh: size_t base,
- en: size_t n,
  id: totrans-2491
  prefs: []
  type: TYPE_NORMAL
  zh: size_t n,
- en: size_t N )
  id: totrans-2492
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N )
- en: '{'
  id: totrans-2493
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float4 *posMass4 = (float4 *) posMass;
  id: totrans-2494
  prefs: []
  type: TYPE_NORMAL
  zh: float4 *posMass4 = (float4 *) posMass;
- en: extern __shared__ float4 shPosMass[];
  id: totrans-2495
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ float4 shPosMass[];
- en: for ( int m = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-2496
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int m = blockIdx.x*blockDim.x + threadIdx.x;
- en: m < n;
  id: totrans-2497
  prefs: []
  type: TYPE_NORMAL
  zh: m < n;
- en: m += blockDim.x*gridDim.x )
  id: totrans-2498
  prefs: []
  type: TYPE_NORMAL
  zh: m += blockDim.x*gridDim.x )
- en: '{'
  id: totrans-2499
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t i = base+m;
  id: totrans-2500
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i = base+m;
- en: float acc[3] = {0};
  id: totrans-2501
  prefs: []
  type: TYPE_NORMAL
  zh: float acc[3] = {0};
- en: float4 myPosMass = posMass4[i];
  id: totrans-2502
  prefs: []
  type: TYPE_NORMAL
  zh: float4 myPosMass = posMass4[i];
- en: '#pragma unroll 32'
  id: totrans-2503
  prefs: []
  type: TYPE_NORMAL
  zh: '#pragma unroll 32'
- en: for ( int j = 0; j < N; j += blockDim.x ) {
  id: totrans-2504
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < N; j += blockDim.x ) {
- en: shPosMass[threadIdx.x] = posMass4[j+threadIdx.x];
  id: totrans-2505
  prefs: []
  type: TYPE_NORMAL
  zh: shPosMass[threadIdx.x] = posMass4[j+threadIdx.x];
- en: __syncthreads();
  id: totrans-2506
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: for ( size_t k = 0; k < blockDim.x; k++ ) {
  id: totrans-2507
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t k = 0; k < blockDim.x; k++ ) {
- en: float fx, fy, fz;
  id: totrans-2508
  prefs: []
  type: TYPE_NORMAL
  zh: float fx, fy, fz;
- en: float4 bodyPosMass = shPosMass[k];
  id: totrans-2509
  prefs: []
  type: TYPE_NORMAL
  zh: float4 bodyPosMass = shPosMass[k];
- en: bodyBodyInteraction(
  id: totrans-2510
  prefs: []
  type: TYPE_NORMAL
  zh: bodyBodyInteraction(
- en: '&fx, &fy, &fz,'
  id: totrans-2511
  prefs: []
  type: TYPE_NORMAL
  zh: '&fx, &fy, &fz,'
- en: myPosMass.x, myPosMass.y, myPosMass.z,
  id: totrans-2512
  prefs: []
  type: TYPE_NORMAL
  zh: myPosMass.x, myPosMass.y, myPosMass.z,
- en: bodyPosMass.x,
  id: totrans-2513
  prefs: []
  type: TYPE_NORMAL
  zh: bodyPosMass.x，
- en: bodyPosMass.y,
  id: totrans-2514
  prefs: []
  type: TYPE_NORMAL
  zh: bodyPosMass.y,
- en: bodyPosMass.z,
  id: totrans-2515
  prefs: []
  type: TYPE_NORMAL
  zh: bodyPosMass.z,
- en: bodyPosMass.w,
  id: totrans-2516
  prefs: []
  type: TYPE_NORMAL
  zh: bodyPosMass.w，
- en: softeningSquared );
  id: totrans-2517
  prefs: []
  type: TYPE_NORMAL
  zh: 软化平方 );
- en: acc[0] += fx;
  id: totrans-2518
  prefs: []
  type: TYPE_NORMAL
  zh: acc[0] += fx;
- en: acc[1] += fy;
  id: totrans-2519
  prefs: []
  type: TYPE_NORMAL
  zh: acc[1] += fy;
- en: acc[2] += fz;
  id: totrans-2520
  prefs: []
  type: TYPE_NORMAL
  zh: acc[2] += fz;
- en: '}'
  id: totrans-2521
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-2522
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-2523
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: force[3*m+0] = acc[0];
  id: totrans-2524
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*m+0] = acc[0];
- en: force[3*m+1] = acc[1];
  id: totrans-2525
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*m+1] = acc[1];
- en: force[3*m+2] = acc[2];
  id: totrans-2526
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*m+2] = acc[2];
- en: '}'
  id: totrans-2527
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2528
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2529
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'The host code for a single-threaded, multi-GPU version of N-body is shown in
    [Listing 9.4](ch09.html#ch09lis04).^([4](ch09.html#ch09fn4)) The arrays `dptrPosMass`
    and `dptrForce` track the device pointers for the input and output arrays for
    each GPU (the maximum number of GPUs is declared as a constant in `nbody.h`; default
    is 32). Similar to dispatching work into CUDA streams, the function uses separate
    loops for different stages of the computation: The first loop allocates and populates
    the input array for each GPU; the second loop launches the kernel and an asynchronous
    copy of the output data; and the third loop calls `cudaDeviceSynchronize()` on
    each GPU in turn. Structuring the function this way maximizes CPU/GPU overlap.
    During the first loop, asynchronous host→device memcpys to GPUs 0..*i*-1 can proceed
    while the CPU is busy allocating memory for GPU *i*. If the kernel launch and
    asynchronous device→host memcpy were in the first loop, the synchronous `cudaMalloc()`
    calls would decrease performance because they are synchronous with respect to
    the current GPU.'
  id: totrans-2530
  prefs: []
  type: TYPE_NORMAL
  zh: 单线程、多GPU版本的 N-body 主机代码见 [Listing 9.4](ch09.html#ch09lis04)。^([4](ch09.html#ch09fn4))
    数组 `dptrPosMass` 和 `dptrForce` 跟踪每个GPU的输入和输出数组的设备指针（GPU的最大数量在 `nbody.h` 中作为常量声明，默认值为32）。与将任务分配到CUDA流类似，函数使用不同的循环来执行计算的不同阶段：第一个循环为每个GPU分配并填充输入数组；第二个循环启动内核并异步复制输出数据；第三个循环依次调用每个GPU的
    `cudaDeviceSynchronize()`。这样结构化函数有助于最大化CPU/GPU重叠。在第一个循环中，异步的主机→设备内存复制操作可以在CPU忙于为GPU
    *i* 分配内存时继续进行。如果内核启动和异步设备→主机内存复制操作位于第一个循环中，那么同步的 `cudaMalloc()` 调用会降低性能，因为它们与当前GPU的操作是同步的。
- en: '[4](ch09.html#ch09fn4a). To avoid awkward formatting, error checking has been
    removed.'
  id: totrans-2531
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch09.html#ch09fn4a)。为了避免格式问题，已移除错误检查。'
- en: '*Listing 9.4.* N-body host code (single-threaded multi-GPU).'
  id: totrans-2532
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 9.4.* N-body 主机代码（单线程多GPU）。'
- en: '[Click here to view code image](ch09_images.html#p09lis04a)'
  id: totrans-2533
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch09_images.html#p09lis04a)'
- en: '* * *'
  id: totrans-2534
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: float
  id: totrans-2535
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: ComputeGravitation_multiGPU_singlethread(
  id: totrans-2536
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeGravitation_multiGPU_singlethread(
- en: float *force,
  id: totrans-2537
  prefs: []
  type: TYPE_NORMAL
  zh: float *force，
- en: float *posMass,
  id: totrans-2538
  prefs: []
  type: TYPE_NORMAL
  zh: float *posMass，
- en: float softeningSquared,
  id: totrans-2539
  prefs: []
  type: TYPE_NORMAL
  zh: float 软化平方，
- en: size_t N
  id: totrans-2540
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N
- en: )
  id: totrans-2541
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '{'
  id: totrans-2542
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-2543
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t 状态；
- en: float ret = 0.0f;
  id: totrans-2544
  prefs: []
  type: TYPE_NORMAL
  zh: float 返回值 = 0.0f;
- en: float *dptrPosMass[g_maxGPUs];
  id: totrans-2545
  prefs: []
  type: TYPE_NORMAL
  zh: float *dptrPosMass[g_maxGPUs]；
- en: float *dptrForce[g_maxGPUs];
  id: totrans-2546
  prefs: []
  type: TYPE_NORMAL
  zh: float *dptrForce[g_maxGPUs]；
- en: chTimerTimestamp start, end;
  id: totrans-2547
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerTimestamp 开始，结束；
- en: chTimerGetTime( &start );
  id: totrans-2548
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: memset( dptrPosMass, 0, sizeof(dptrPosMass) );
  id: totrans-2549
  prefs: []
  type: TYPE_NORMAL
  zh: memset( dptrPosMass, 0, sizeof(dptrPosMass) );
- en: memset( dptrForce, 0, sizeof(dptrForce) );
  id: totrans-2550
  prefs: []
  type: TYPE_NORMAL
  zh: memset( dptrForce, 0, sizeof(dptrForce) );
- en: size_t bodiesPerGPU = N / g_numGPUs;
  id: totrans-2551
  prefs: []
  type: TYPE_NORMAL
  zh: size_t bodiesPerGPU = N / g_numGPUs;
- en: if ( (0 != N % g_numGPUs) || (g_numGPUs > g_maxGPUs) ) {
  id: totrans-2552
  prefs: []
  type: TYPE_NORMAL
  zh: if ( (0 != N % g_numGPUs) || (g_numGPUs > g_maxGPUs) ) {
- en: return 0.0f;
  id: totrans-2553
  prefs: []
  type: TYPE_NORMAL
  zh: return 0.0f;
- en: '}'
  id: totrans-2554
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // kick off the asynchronous memcpy's - overlap GPUs pulling
  id: totrans-2555
  prefs: []
  type: TYPE_NORMAL
  zh: // 启动异步的 memcpy - 重叠 GPU 拉取
- en: // host memory with the CPU time needed to do the memory
  id: totrans-2556
  prefs: []
  type: TYPE_NORMAL
  zh: // 主机内存与 CPU 处理内存所需的时间
- en: // allocations.
  id: totrans-2557
  prefs: []
  type: TYPE_NORMAL
  zh: // 分配。
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2558
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < g_numGPUs; i++ ) {
- en: cudaSetDevice( i );
  id: totrans-2559
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSetDevice( i );
- en: cudaMalloc( &dptrPosMass[i], 4*N*sizeof(float) );
  id: totrans-2560
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMalloc( &dptrPosMass[i], 4*N*sizeof(float) );
- en: cudaMalloc( &dptrForce[i], 3*bodiesPerGPU*sizeof(float) );
  id: totrans-2561
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMalloc( &dptrForce[i], 3*bodiesPerGPU*sizeof(float) );
- en: cudaMemcpyAsync(
  id: totrans-2562
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync(
- en: dptrPosMass[i],
  id: totrans-2563
  prefs: []
  type: TYPE_NORMAL
  zh: dptrPosMass[i],
- en: g_hostAOS_PosMass,
  id: totrans-2564
  prefs: []
  type: TYPE_NORMAL
  zh: g_hostAOS_PosMass,
- en: 4*N*sizeof(float),
  id: totrans-2565
  prefs: []
  type: TYPE_NORMAL
  zh: 4*N*sizeof(float),
- en: cudaMemcpyHostToDevice );
  id: totrans-2566
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice );
- en: '}'
  id: totrans-2567
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2568
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < g_numGPUs; i++ ) {
- en: cudaSetDevice( i ) );
  id: totrans-2569
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSetDevice( i ) );
- en: ComputeNBodyGravitation_Shared_device<<<
  id: totrans-2570
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeNBodyGravitation_Shared_device<<<
- en: 300,256,256*sizeof(float4)>>>(
  id: totrans-2571
  prefs: []
  type: TYPE_NORMAL
  zh: 300,256,256*sizeof(float4)>>>(
- en: dptrForce[i],
  id: totrans-2572
  prefs: []
  type: TYPE_NORMAL
  zh: dptrForce[i],
- en: dptrPosMass[i],
  id: totrans-2573
  prefs: []
  type: TYPE_NORMAL
  zh: dptrPosMass[i],
- en: softeningSquared,
  id: totrans-2574
  prefs: []
  type: TYPE_NORMAL
  zh: softeningSquared,
- en: i*bodiesPerGPU,
  id: totrans-2575
  prefs: []
  type: TYPE_NORMAL
  zh: i*bodiesPerGPU,
- en: bodiesPerGPU,
  id: totrans-2576
  prefs: []
  type: TYPE_NORMAL
  zh: bodiesPerGPU,
- en: N );
  id: totrans-2577
  prefs: []
  type: TYPE_NORMAL
  zh: N );
- en: cudaMemcpyAsync(
  id: totrans-2578
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync(
- en: g_hostAOS_Force+3*bodiesPerGPU*i,
  id: totrans-2579
  prefs: []
  type: TYPE_NORMAL
  zh: g_hostAOS_Force+3*bodiesPerGPU*i,
- en: dptrForce[i],
  id: totrans-2580
  prefs: []
  type: TYPE_NORMAL
  zh: dptrForce[i],
- en: 3*bodiesPerGPU*sizeof(float),
  id: totrans-2581
  prefs: []
  type: TYPE_NORMAL
  zh: 3*bodiesPerGPU*sizeof(float),
- en: cudaMemcpyDeviceToHost );
  id: totrans-2582
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToHost );
- en: '}'
  id: totrans-2583
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // Synchronize with each GPU in turn.
  id: totrans-2584
  prefs: []
  type: TYPE_NORMAL
  zh: // 与每个 GPU 依次同步。
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2585
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < g_numGPUs; i++ ) {
- en: cudaSetDevice( i );
  id: totrans-2586
  prefs: []
  type: TYPE_NORMAL
  zh: cudaSetDevice( i );
- en: cudaDeviceSynchronize();
  id: totrans-2587
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceSynchronize();
- en: '}'
  id: totrans-2588
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: chTimerGetTime( &end );
  id: totrans-2589
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &end );
- en: ret = chTimerElapsedTime( &start, &end ) * 1000.0f;
  id: totrans-2590
  prefs: []
  type: TYPE_NORMAL
  zh: ret = chTimerElapsedTime( &start, &end ) * 1000.0f;
- en: 'Error:'
  id: totrans-2591
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2592
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < g_numGPUs; i++ ) {
- en: cudaFree( dptrPosMass[i] );
  id: totrans-2593
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFree( dptrPosMass[i] );
- en: cudaFree( dptrForce[i] );
  id: totrans-2594
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFree( dptrForce[i] );
- en: '}'
  id: totrans-2595
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return ret;
  id: totrans-2596
  prefs: []
  type: TYPE_NORMAL
  zh: return ret;
- en: '}'
  id: totrans-2597
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2598
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 9.6\. Multithreaded Multi-GPU
  id: totrans-2599
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6\. 多线程多 GPU
- en: CUDA has supported multiple GPUs since the beginning, but until CUDA 4.0, each
    GPU had to be controlled by a separate CPU thread. For workloads that required
    a lot of CPU power, that requirement was never very onerous because the full power
    of modern multicore processors can be unlocked only through multithreading.
  id: totrans-2600
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 从一开始就支持多个 GPU，但直到 CUDA 4.0，每个 GPU 都必须由一个单独的 CPU 线程来控制。对于那些需要大量 CPU 计算的工作负载来说，这一要求并不算太繁重，因为现代多核处理器的全部性能只能通过多线程来解锁。
- en: The multithreaded implementation of multi-GPU N-Body creates one CPU thread
    per GPU, and it delegates the dispatch and synchronization of the work for a given
    N-body pass to each thread. The main thread splits the work evenly between GPUs,
    delegates work to each worker thread by signaling an event (or a semaphore, on
    POSIX platforms such as Linux), and then waits for all of the worker threads to
    signal completion before proceeding. As the number of GPUs grows, synchronization
    overhead starts to chip away at the benefits from parallelism.
  id: totrans-2601
  prefs: []
  type: TYPE_NORMAL
  zh: 多线程实现的多GPU N-Body为每个GPU创建一个CPU线程，并将给定N-body传递的工作分派和同步任务委托给每个线程。主线程将工作均匀分配到GPU之间，通过触发事件（或在POSIX平台如Linux上使用信号量）将工作分配给每个工作线程，然后等待所有工作线程信号完成后再继续执行。随着GPU数量的增加，同步开销开始侵蚀并行性的优势。
- en: This implementation of N-body uses the same multithreading library as the multithreaded
    implementation of N-body, described in [Section 14.9](ch14.html#ch14lev1sec9).
    The `workerThread` class, described in [Appendix A](app01.html#app01).2, enables
    the application thread to “delegate” work to CPU threads, then synchronize on
    the worker threads’ completion of the delegated task.
  id: totrans-2602
  prefs: []
  type: TYPE_NORMAL
  zh: 这个N-body实现使用与[第14.9节](ch14.html#ch14lev1sec9)中描述的多线程N-body实现相同的多线程库。`workerThread`类，描述于[附录A](app01.html#app01).2，允许应用线程将工作“委派”给CPU线程，然后在工作线程完成委派的任务后进行同步。
- en: '[Listing 9.5](ch09.html#ch09lis05) gives the host code that creates and initializes
    the CPU threads. Two globals, `g_numGPUs` and `g_GPUThreadPool`, contain the GPU
    count and a worker thread for each. After each CPU thread is created, it is initialized
    by synchronously calling the `initializeGPU()` function, which affiliates the
    CPU thread with a given GPU—an affiliation that never changes during the course
    of the application’s execution.'
  id: totrans-2603
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.5](ch09.html#ch09lis05)提供了创建并初始化CPU线程的主机代码。两个全局变量，`g_numGPUs`和`g_GPUThreadPool`，分别包含GPU数量和每个GPU的工作线程。每个CPU线程创建后，都通过同步调用`initializeGPU()`函数进行初始化，这个函数将CPU线程与给定的GPU关联——这种关联在应用执行过程中不会发生变化。'
- en: '*Listing 9.5.* Multithreaded multi-GPU initialization code.'
  id: totrans-2604
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 9.5.* 多线程多GPU初始化代码。'
- en: '[Click here to view code image](ch09_images.html#p09lis05a)'
  id: totrans-2605
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch09_images.html#p09lis05a)'
- en: '* * *'
  id: totrans-2606
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: workerThread *g_CPUThreadPool;
  id: totrans-2607
  prefs: []
  type: TYPE_NORMAL
  zh: workerThread *g_CPUThreadPool;
- en: int g_numCPUCores;
  id: totrans-2608
  prefs: []
  type: TYPE_NORMAL
  zh: int g_numCPUCores;
- en: workerThread *g_GPUThreadPool;
  id: totrans-2609
  prefs: []
  type: TYPE_NORMAL
  zh: workerThread *g_GPUThreadPool;
- en: int g_numGPUs;
  id: totrans-2610
  prefs: []
  type: TYPE_NORMAL
  zh: int g_numGPUs;
- en: struct gpuInit_struct
  id: totrans-2611
  prefs: []
  type: TYPE_NORMAL
  zh: struct gpuInit_struct
- en: '{'
  id: totrans-2612
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int iGPU;
  id: totrans-2613
  prefs: []
  type: TYPE_NORMAL
  zh: int iGPU;
- en: cudaError_t status;
  id: totrans-2614
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: '};'
  id: totrans-2615
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: void
  id: totrans-2616
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: initializeGPU( void *_p )
  id: totrans-2617
  prefs: []
  type: TYPE_NORMAL
  zh: initializeGPU( void *_p )
- en: '{'
  id: totrans-2618
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-2619
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: gpuInit_struct *p = (gpuInit_struct *) _p;
  id: totrans-2620
  prefs: []
  type: TYPE_NORMAL
  zh: gpuInit_struct *p = (gpuInit_struct *) _p;
- en: CUDART_CHECK( cudaSetDevice( p->iGPU ) );
  id: totrans-2621
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaSetDevice( p->iGPU ) );
- en: CUDART_CHECK( cudaSetDeviceFlags( cudaDeviceMapHost ) );
  id: totrans-2622
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaSetDeviceFlags( cudaDeviceMapHost ) );
- en: CUDART_CHECK( cudaFree(0) );
  id: totrans-2623
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaFree(0) );
- en: 'Error:'
  id: totrans-2624
  prefs: []
  type: TYPE_NORMAL
  zh: '错误:'
- en: p->status = status;
  id: totrans-2625
  prefs: []
  type: TYPE_NORMAL
  zh: p->status = status;
- en: '}'
  id: totrans-2626
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // ... below is from main()
  id: totrans-2627
  prefs: []
  type: TYPE_NORMAL
  zh: // ... 以下是来自 main() 的代码
- en: if ( g_numGPUs ) {
  id: totrans-2628
  prefs: []
  type: TYPE_NORMAL
  zh: if ( g_numGPUs ) {
- en: chCommandLineGet( &g_numGPUs, "numgpus", argc, argv );
  id: totrans-2629
  prefs: []
  type: TYPE_NORMAL
  zh: chCommandLineGet( &g_numGPUs, "numgpus", argc, argv );
- en: g_GPUThreadPool = new workerThread[g_numGPUs];
  id: totrans-2630
  prefs: []
  type: TYPE_NORMAL
  zh: g_GPUThreadPool = new workerThread[g_numGPUs];
- en: for ( size_t i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2631
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = 0; i < g_numGPUs; i++ ) {
- en: if ( ! g_GPUThreadPool[i].initialize( ) ) {
  id: totrans-2632
  prefs: []
  type: TYPE_NORMAL
  zh: if ( ! g_GPUThreadPool[i].initialize( ) ) {
- en: fprintf( stderr, "Error initializing thread pool\n" );
  id: totrans-2633
  prefs: []
  type: TYPE_NORMAL
  zh: fprintf( stderr, "初始化线程池时出错\n" );
- en: return 1;
  id: totrans-2634
  prefs: []
  type: TYPE_NORMAL
  zh: return 1;
- en: '}'
  id: totrans-2635
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2636
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2637
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < g_numGPUs; i++ ) {
- en: gpuInit_struct initGPU = {i};
  id: totrans-2638
  prefs: []
  type: TYPE_NORMAL
  zh: gpuInit_struct initGPU = {i};
- en: g_GPUThreadPool[i].delegateSynchronous(
  id: totrans-2639
  prefs: []
  type: TYPE_NORMAL
  zh: g_GPUThreadPool[i].delegateSynchronous(
- en: initializeGPU,
  id: totrans-2640
  prefs: []
  type: TYPE_NORMAL
  zh: initializeGPU,
- en: '&initGPU );'
  id: totrans-2641
  prefs: []
  type: TYPE_NORMAL
  zh: '&initGPU );'
- en: if ( cudaSuccess != initGPU.status ) {
  id: totrans-2642
  prefs: []
  type: TYPE_NORMAL
  zh: if ( cudaSuccess != initGPU.status ) {
- en: fprintf( stderr, "Initializing GPU %d failed "
  id: totrans-2643
  prefs: []
  type: TYPE_NORMAL
  zh: fprintf( stderr, "初始化 GPU %d 失败 "
- en: '"with %d (%s)\n",'
  id: totrans-2644
  prefs: []
  type: TYPE_NORMAL
  zh: '"错误信息: %d (%s)\n",'
- en: i,
  id: totrans-2645
  prefs: []
  type: TYPE_NORMAL
  zh: i,
- en: initGPU.status,
  id: totrans-2646
  prefs: []
  type: TYPE_NORMAL
  zh: initGPU.status,
- en: cudaGetErrorString( initGPU.status ) );
  id: totrans-2647
  prefs: []
  type: TYPE_NORMAL
  zh: cudaGetErrorString( initGPU.status ) );
- en: return 1;
  id: totrans-2648
  prefs: []
  type: TYPE_NORMAL
  zh: return 1;
- en: '}'
  id: totrans-2649
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2650
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2651
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2652
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Once the worker threads are initialized, they suspend waiting on a thread synchronization
    primitive until the application thread dispatches work to them. [Listing 9.6](ch09.html#ch09lis06)
    shows the host code that dispatches work to the GPUs: The `gpuDelegation` structure
    encapsulates the work that a given GPU must do, and the `gpuWorkerThread` function
    is invoked for each of the worker threads created by the code in [Listing 9.5](ch09.html#ch09lis05).
    The application thread code, shown in [Listing 9.7](ch09.html#ch09lis07), creates
    a `gpuDelegation` structure for each worker thread and calls the `delegateAsynchronous()`
    method to invoke the code in [Listing 9.6](ch09.html#ch09lis06). The `waitAll()`
    method then waits until all of the worker threads have finished. The performance
    and scaling results of the single-threaded and multithreaded version of multi-GPU
    N-body are summarized in [Section 14.7](ch14.html#ch14lev1sec7).'
  id: totrans-2653
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦工作线程初始化完成，它们将挂起并等待线程同步原语，直到应用线程向它们分配工作。[清单 9.6](ch09.html#ch09lis06)显示了分配工作给GPU的主机代码：`gpuDelegation`结构封装了给定GPU必须执行的工作，而`gpuWorkerThread`函数会为代码在[清单
    9.5](ch09.html#ch09lis05)中创建的每个工作线程调用。应用线程代码，见[清单 9.7](ch09.html#ch09lis07)，为每个工作线程创建一个`gpuDelegation`结构，并调用`delegateAsynchronous()`方法来执行[清单
    9.6](ch09.html#ch09lis06)中的代码。然后，`waitAll()`方法将等待直到所有工作线程完成。单线程和多线程版本的多GPU N-body的性能和扩展结果总结在[第14.7节](ch14.html#ch14lev1sec7)。
- en: '*Listing 9.6.* Host code (worker thread).'
  id: totrans-2654
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 9.6.* 主机代码（工作线程）。'
- en: '[Click here to view code image](ch09_images.html#p09lis06a)'
  id: totrans-2655
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch09_images.html#p09lis06a)'
- en: '* * *'
  id: totrans-2656
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: struct gpuDelegation {
  id: totrans-2657
  prefs: []
  type: TYPE_NORMAL
  zh: struct gpuDelegation {
- en: size_t i;   // base offset for this thread to process
  id: totrans-2658
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i;   // 该线程处理的基本偏移量
- en: size_t n;   // size of this thread's problem
  id: totrans-2659
  prefs: []
  type: TYPE_NORMAL
  zh: size_t n;   // 该线程问题的大小
- en: size_t N;   // total number of bodies
  id: totrans-2660
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N;   // 总体数量
- en: float *hostPosMass;
  id: totrans-2661
  prefs: []
  type: TYPE_NORMAL
  zh: float *hostPosMass;
- en: float *hostForce;
  id: totrans-2662
  prefs: []
  type: TYPE_NORMAL
  zh: float *hostForce;
- en: float softeningSquared;
  id: totrans-2663
  prefs: []
  type: TYPE_NORMAL
  zh: float softeningSquared;
- en: cudaError_t status;
  id: totrans-2664
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: '};'
  id: totrans-2665
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: void
  id: totrans-2666
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: gpuWorkerThread( void *_p )
  id: totrans-2667
  prefs: []
  type: TYPE_NORMAL
  zh: gpuWorkerThread( void *_p )
- en: '{'
  id: totrans-2668
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-2669
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: gpuDelegation *p = (gpuDelegation *) _p;
  id: totrans-2670
  prefs: []
  type: TYPE_NORMAL
  zh: gpuDelegation *p = (gpuDelegation *) _p;
- en: float *dptrPosMass = 0;
  id: totrans-2671
  prefs: []
  type: TYPE_NORMAL
  zh: float *dptrPosMass = 0;
- en: float *dptrForce = 0;
  id: totrans-2672
  prefs: []
  type: TYPE_NORMAL
  zh: float *dptrForce = 0;
- en: //
  id: totrans-2673
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // Each GPU has its own device pointer to the host pointer.
  id: totrans-2674
  prefs: []
  type: TYPE_NORMAL
  zh: // 每个GPU有自己指向主机指针的设备指针。
- en: //
  id: totrans-2675
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: CUDART_CHECK( cudaMalloc( &dptrPosMass, 4*p->N*sizeof(float) ) );
  id: totrans-2676
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMalloc( &dptrPosMass, 4*p->N*sizeof(float) ) );
- en: CUDART_CHECK( cudaMalloc( &dptrForce, 3*p->n*sizeof(float) ) );
  id: totrans-2677
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMalloc( &dptrForce, 3*p->n*sizeof(float) ) );
- en: CUDART_CHECK( cudaMemcpyAsync(
  id: totrans-2678
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpyAsync(
- en: dptrPosMass,
  id: totrans-2679
  prefs: []
  type: TYPE_NORMAL
  zh: dptrPosMass,
- en: p->hostPosMass,
  id: totrans-2680
  prefs: []
  type: TYPE_NORMAL
  zh: p->hostPosMass,
- en: 4*p->N*sizeof(float),
  id: totrans-2681
  prefs: []
  type: TYPE_NORMAL
  zh: 4*p->N*sizeof(float),
- en: cudaMemcpyHostToDevice ) );
  id: totrans-2682
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice ) );
- en: ComputeNBodyGravitation_multiGPU<<<300,256,256*sizeof(float4)>>>(
  id: totrans-2683
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeNBodyGravitation_multiGPU<<<300,256,256*sizeof(float4)>>>(
- en: dptrForce,
  id: totrans-2684
  prefs: []
  type: TYPE_NORMAL
  zh: dptrForce,
- en: dptrPosMass,
  id: totrans-2685
  prefs: []
  type: TYPE_NORMAL
  zh: dptrPosMass,
- en: p->softeningSquared,
  id: totrans-2686
  prefs: []
  type: TYPE_NORMAL
  zh: p->softeningSquared,
- en: p->i,
  id: totrans-2687
  prefs: []
  type: TYPE_NORMAL
  zh: p->i,
- en: p->n,
  id: totrans-2688
  prefs: []
  type: TYPE_NORMAL
  zh: p->n,
- en: p->N );
  id: totrans-2689
  prefs: []
  type: TYPE_NORMAL
  zh: p->N );
- en: '// NOTE: synchronous memcpy, so no need for further'
  id: totrans-2690
  prefs: []
  type: TYPE_NORMAL
  zh: // 注意：同步的内存拷贝，因此无需进一步操作
- en: // synchronization with device
  id: totrans-2691
  prefs: []
  type: TYPE_NORMAL
  zh: // 与设备同步
- en: CUDART_CHECK( cudaMemcpy(
  id: totrans-2692
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpy(
- en: p->hostForce+3*p->i,
  id: totrans-2693
  prefs: []
  type: TYPE_NORMAL
  zh: p->hostForce+3*p->i,
- en: dptrForce,
  id: totrans-2694
  prefs: []
  type: TYPE_NORMAL
  zh: dptrForce,
- en: 3*p->n*sizeof(float),
  id: totrans-2695
  prefs: []
  type: TYPE_NORMAL
  zh: 3*p->n*sizeof(float),
- en: cudaMemcpyDeviceToHost ) );
  id: totrans-2696
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToHost ) );
- en: 'Error:'
  id: totrans-2697
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: cudaFree( dptrPosMass );
  id: totrans-2698
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFree( dptrPosMass );
- en: cudaFree( dptrForce );
  id: totrans-2699
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFree( dptrForce );
- en: p->status = status;
  id: totrans-2700
  prefs: []
  type: TYPE_NORMAL
  zh: p->status = status;
- en: '}'
  id: totrans-2701
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2702
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 9.7.* ?Host code (application thread)'
  id: totrans-2703
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 9.7.* ?主机代码（应用线程）'
- en: '[Click here to view code image](ch09_images.html#p09lis07a)'
  id: totrans-2704
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch09_images.html#p09lis07a)'
- en: '* * *'
  id: totrans-2705
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: float
  id: totrans-2706
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: ComputeGravitation_multiGPU_threaded(
  id: totrans-2707
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeGravitation_multiGPU_threaded(
- en: float *force,
  id: totrans-2708
  prefs: []
  type: TYPE_NORMAL
  zh: float *force,
- en: float *posMass,
  id: totrans-2709
  prefs: []
  type: TYPE_NORMAL
  zh: float *posMass,
- en: float softeningSquared,
  id: totrans-2710
  prefs: []
  type: TYPE_NORMAL
  zh: float softeningSquared,
- en: size_t N
  id: totrans-2711
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N
- en: )
  id: totrans-2712
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '{'
  id: totrans-2713
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: chTimerTimestamp start, end;
  id: totrans-2714
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerTimestamp start, end;
- en: chTimerGetTime( &start );
  id: totrans-2715
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: '{'
  id: totrans-2716
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: gpuDelegation *pgpu = new gpuDelegation[g_numGPUs];
  id: totrans-2717
  prefs: []
  type: TYPE_NORMAL
  zh: gpuDelegation *pgpu = new gpuDelegation[g_numGPUs];
- en: size_t bodiesPerGPU = N / g_numGPUs;
  id: totrans-2718
  prefs: []
  type: TYPE_NORMAL
  zh: size_t bodiesPerGPU = N / g_numGPUs;
- en: if ( N % g_numGPUs ) {
  id: totrans-2719
  prefs: []
  type: TYPE_NORMAL
  zh: if ( N % g_numGPUs ) {
- en: return 0.0f;
  id: totrans-2720
  prefs: []
  type: TYPE_NORMAL
  zh: return 0.0f;
- en: '}'
  id: totrans-2721
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: size_t i;
  id: totrans-2722
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i;
- en: for ( i = 0; i < g_numGPUs; i++ ) {
  id: totrans-2723
  prefs: []
  type: TYPE_NORMAL
  zh: for ( i = 0; i < g_numGPUs; i++ ) {
- en: pgpu[i].hostPosMass = g_hostAOS_PosMass;
  id: totrans-2724
  prefs: []
  type: TYPE_NORMAL
  zh: pgpu[i].hostPosMass = g_hostAOS_PosMass;
- en: pgpu[i].hostForce = g_hostAOS_Force;
  id: totrans-2725
  prefs: []
  type: TYPE_NORMAL
  zh: pgpu[i].hostForce = g_hostAOS_Force;
- en: pgpu[i].softeningSquared = softeningSquared;
  id: totrans-2726
  prefs: []
  type: TYPE_NORMAL
  zh: pgpu[i].softeningSquared = softeningSquared;
- en: pgpu[i].i = bodiesPerGPU*i;
  id: totrans-2727
  prefs: []
  type: TYPE_NORMAL
  zh: pgpu[i].i = bodiesPerGPU*i;
- en: pgpu[i].n = bodiesPerGPU;
  id: totrans-2728
  prefs: []
  type: TYPE_NORMAL
  zh: pgpu[i].n = bodiesPerGPU;
- en: pgpu[i].N = N;
  id: totrans-2729
  prefs: []
  type: TYPE_NORMAL
  zh: pgpu[i].N = N;
- en: g_GPUThreadPool[i].delegateAsynchronous(
  id: totrans-2730
  prefs: []
  type: TYPE_NORMAL
  zh: g_GPUThreadPool[i].delegateAsynchronous(
- en: gpuWorkerThread,
  id: totrans-2731
  prefs: []
  type: TYPE_NORMAL
  zh: gpuWorkerThread,
- en: '&pgpu[i] );'
  id: totrans-2732
  prefs: []
  type: TYPE_NORMAL
  zh: '&pgpu[i] );'
- en: '}'
  id: totrans-2733
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: workerThread::waitAll( g_GPUThreadPool, g_numGPUs );
  id: totrans-2734
  prefs: []
  type: TYPE_NORMAL
  zh: workerThread::waitAll( g_GPUThreadPool, g_numGPUs );
- en: delete[] pgpu;
  id: totrans-2735
  prefs: []
  type: TYPE_NORMAL
  zh: delete[] pgpu;
- en: '}'
  id: totrans-2736
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: chTimerGetTime( &end );
  id: totrans-2737
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &end );
- en: return chTimerElapsedTime( &start, &end ) * 1000.0f;
  id: totrans-2738
  prefs: []
  type: TYPE_NORMAL
  zh: return chTimerElapsedTime( &start, &end ) * 1000.0f;
- en: '}'
  id: totrans-2739
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2740
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Chapter 10\. Texturing
  id: totrans-2741
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章\. 纹理处理
- en: 10.1\. Overview
  id: totrans-2742
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1\. 概述
- en: In CUDA, a software technology for general-purpose parallel computing, texture
    support could not have been justified if the hardware hadn’t already been there,
    due to its graphics-accelerating heritage. Nevertheless, the texturing hardware
    accelerates enough useful operations that NVIDIA saw fit to include support. Although
    many CUDA applications may be built without ever using texture, some rely on it
    to be competitive with CPU-based code.
  id: totrans-2743
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CUDA（通用并行计算的软件技术）中，如果没有硬件的支持，纹理支持本来是无法得到证明的，因为它继承了图形加速的传统。然而，纹理硬件加速了足够多的有用操作，NVIDIA
    认为有必要包括该支持。尽管许多 CUDA 应用程序可能在构建时从未使用过纹理，但有些应用程序依赖它来与基于 CPU 的代码竞争。
- en: Texture mapping was invented to enable richer, more realistic-looking objects
    by enabling images to be “painted” onto geometry. Historically, the hardware interpolated
    texture coordinates along with the X, Y, and Z coordinates needed to render a
    triangle, and for each output pixel, the texture value was fetched (optionally
    with bilinear interpolation), processed by blending with interpolated shading
    factors, and blended into the output buffer. With the introduction of programmable
    graphics and texture-like data that might not include color data (for example,
    bump maps), graphics hardware became more sophisticated. The shader programs included
    `TEX` instructions that specified the coordinates to fetch, and the results were
    incorporated into the computations used to generate the output pixel. The hardware
    improves performance using texture caches, memory layouts optimized for dimensional
    locality, and a dedicated hardware pipeline to transform texture coordinates into
    hardware addresses.
  id: totrans-2744
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理映射的发明是为了通过将图像“涂绘”到几何体上，使得对象看起来更加丰富和真实。从历史上看，硬件需要在渲染三角形时，除了 X、Y 和 Z 坐标外，还要插值纹理坐标，并且对于每个输出像素，纹理值会被获取（可选地使用双线性插值），然后与插值后的阴影因子混合，最后融入输出缓冲区。随着可编程图形和可能不包含颜色数据的纹理类数据（例如凹凸贴图）的引入，图形硬件变得更加复杂。着色器程序中包含了
    `TEX` 指令，用于指定要获取的坐标，结果被并入计算中，用于生成输出像素。硬件通过使用纹理缓存、优化维度局部性的内存布局以及专用硬件流水线将纹理坐标转换为硬件地址来提高性能。
- en: Because the functionality grew organically and was informed by a combination
    of application requirements and hardware costs, the texturing features are not
    very orthogonal. For example, the “wrap” and “mirror” texture addressing modes
    do not work unless the texture coordinates are normalized. This chapter explains
    every detail of the texture hardware as supported by CUDA. We will cover everything
    from normalized versus unnormalized coordinates to addressing modes to the limits
    of linear interpolation; 1D, 2D, 3D, and layered textures; and how to use these
    features from both the CUDA runtime and the driver API.
  id: totrans-2745
  prefs: []
  type: TYPE_NORMAL
  zh: 由于功能的增长是自然而然的，且受到应用需求与硬件成本的共同影响，纹理特性并不是很正交。例如，除非纹理坐标是归一化的，否则“包裹”（wrap）和“镜像”（mirror）纹理寻址模式无法使用。本章将详细解释
    CUDA 所支持的所有纹理硬件细节。我们将涵盖从归一化与非归一化坐标、寻址模式、线性插值的限制、1D、2D、3D 和层状纹理，到如何通过 CUDA 运行时和驱动程序
    API 使用这些特性的所有内容。
- en: 10.1.1\. Two Use Cases
  id: totrans-2746
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.1.1\. 两种使用场景
- en: 'In CUDA, there are two significantly different uses for texture. One is to
    simply use texture as a read path: to work around coalescing constraints or to
    use the texture cache to reduce external bandwidth requirements, or both. The
    other use case takes advantage of the fixed-function hardware that the GPU has
    in place for graphics applications. The texture hardware consists of a configurable
    pipeline of computation stages that can do all of the following.'
  id: totrans-2747
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CUDA 中，纹理有两种显著不同的使用方式。一种是简单地将纹理作为读取路径：绕过合并约束，或使用纹理缓存来减少外部带宽需求，或两者兼而有之。另一种使用方式则是利用
    GPU 为图形应用所提供的固定功能硬件。纹理硬件由一个可配置的计算阶段管道组成，可以执行以下所有操作。
- en: • Scale normalized texture coordinates
  id: totrans-2748
  prefs: []
  type: TYPE_NORMAL
  zh: • 缩放归一化纹理坐标
- en: • Perform boundary condition computations on the texture coordinates
  id: totrans-2749
  prefs: []
  type: TYPE_NORMAL
  zh: • 对纹理坐标进行边界条件计算
- en: • Convert texture coordinates to addresses with 2D or 3D locality
  id: totrans-2750
  prefs: []
  type: TYPE_NORMAL
  zh: • 将纹理坐标转换为具有 2D 或 3D 局部性的地址
- en: • Fetch 2, 4, or 8 texture elements for 1D, 2D, or 3D textures and linearly
    interpolate between them
  id: totrans-2751
  prefs: []
  type: TYPE_NORMAL
  zh: • 获取 2、4 或 8 个纹理元素用于 1D、2D 或 3D 纹理，并在它们之间进行线性插值
- en: • Convert the texture values from integers to unitized floating-point values
  id: totrans-2752
  prefs: []
  type: TYPE_NORMAL
  zh: • 将纹理值从整数转换为单位化浮点值
- en: Textures are read through *texture references* that are bound to underlying
    memory (either CUDA arrays or device memory). The memory is just an unshaped bucket
    of bits; it is the texture reference that tells the hardware how to interpret
    the data and deliver it into registers when a TEX instruction is executed.
  id: totrans-2753
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理通过绑定到底层内存（可以是 CUDA 数组或设备内存）的*纹理引用*进行读取。内存只是一个没有结构的比特桶；是纹理引用告诉硬件如何解释数据，并在执行
    TEX 指令时将其传递到寄存器。
- en: 10.2\. Texture Memory
  id: totrans-2754
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2\. 纹理内存
- en: Before describing the features of the fixed-function texturing hardware, let’s
    spend some time examining the underlying memory to which texture references may
    be bound. CUDA can texture from either device memory or CUDA arrays.
  id: totrans-2755
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述固定功能纹理硬件的特性之前，让我们花一些时间来研究可能绑定纹理引用的底层内存。CUDA可以从设备内存或CUDA数组中进行纹理操作。
- en: 10.2.1\. Device Memory
  id: totrans-2756
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.1\. 设备内存
- en: In device memory, the textures are addressed in row-major order. A 1024x768
    texture might look like [Figure 10.1](ch10.html#ch10fig01), where *Offset* is
    the offset (in elements) from the base pointer of the image.
  id: totrans-2757
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备内存中，纹理按行主序（row-major）方式进行寻址。一个1024x768的纹理可能类似于[图10.1](ch10.html#ch10fig01)，其中*Offset*是从图像基指针开始的偏移量（以元素为单位）。
- en: (Equation 10.1)
  id: totrans-2758
  prefs: []
  type: TYPE_NORMAL
  zh: （公式 10.1）
- en: '![Image](graphics/10equ01.jpg)'
  id: totrans-2759
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/10equ01.jpg)'
- en: For a byte offset, multiply by the size of the elements.
  id: totrans-2760
  prefs: []
  type: TYPE_NORMAL
  zh: 对于字节偏移量，乘以元素的大小。
- en: (Equation 10.2)
  id: totrans-2761
  prefs: []
  type: TYPE_NORMAL
  zh: （公式 10.2）
- en: '![Image](graphics/10equ02.jpg)![Image](graphics/10fig01.jpg)'
  id: totrans-2762
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/10equ02.jpg)![Image](graphics/10fig01.jpg)'
- en: '*Figure 10.1* 1024x768 image.'
  id: totrans-2763
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.1* 1024x768的图像。'
- en: 'In practice, this addressing calculation only works for the most convenient
    of texture widths: 1024 happens to be convenient because it is a power of 2 and
    conforms to all manner of alignment restrictions. To accommodate less convenient
    texture sizes, CUDA implements *pitch-linear addressing*, where the width of the
    texture memory is different from the width of the texture. For less convenient
    widths, the hardware enforces an alignment restriction and the width in elements
    is treated differently from the width of the texture memory. For a texture width
    of 950, say, and an alignment restriction of 64 bytes, the width-in-bytes is padded
    to 964 (the next multiple of 64), and the texture looks like [Figure 10.2](ch10.html#ch10fig02).'
  id: totrans-2764
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这种寻址计算只适用于最方便的纹理宽度：1024恰好是一个方便的宽度，因为它是2的幂次方，并且符合所有对齐限制。为了适应不那么方便的纹理尺寸，CUDA实现了*步长线性寻址*，即纹理内存的宽度不同于纹理的宽度。对于不那么方便的宽度，硬件强制执行对齐限制，元素宽度与纹理内存的宽度处理方式不同。例如，对于宽度为950且对齐限制为64字节的纹理，宽度（以字节为单位）会被填充为964（64的下一个倍数），该纹理的表现类似于[图10.2](ch10.html#ch10fig02)。
- en: '![Image](graphics/10fig02.jpg)'
  id: totrans-2765
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/10fig02.jpg)'
- en: '*Figure 10.2* 950x768 image, with pitch.'
  id: totrans-2766
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.2* 950x768的图像，带有步长。'
- en: In CUDA, the padded width in bytes is called the *pitch*. The total amount of
    device memory used by this image is 964x768 elements. The offset into the image
    now is computed in bytes, as follows.
  id: totrans-2767
  prefs: []
  type: TYPE_NORMAL
  zh: 在CUDA中，字节对齐宽度被称为*步长（pitch）*。该图像使用的设备内存总量为964x768个元素。现在，图像的偏移量是以字节为单位计算的，具体如下。
- en: '*ByteOffset = Y * Pitch + XInBytes*'
  id: totrans-2768
  prefs: []
  type: TYPE_NORMAL
  zh: '*ByteOffset = Y * Pitch + XInBytes*'
- en: Applications can call `cudaMallocPitch()/cuMemAllocPitch()` to delegate selection
    of the pitch to the CUDA driver.^([1](ch10.html#ch10fn1)) In 3D, pitch-linear
    images of a given *Depth* are exactly like 2D images, with *Depth* 2D slices laid
    out contiguously in device memory.
  id: totrans-2769
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可以调用`cudaMallocPitch()/cuMemAllocPitch()`将步长的选择委托给CUDA驱动程序。^([1](ch10.html#ch10fn1))
    在3D图形中，具有给定*深度（Depth）*的步长线性图像就像2D图像一样，*深度* 2D切片在设备内存中是连续排列的。
- en: '[1](ch10.html#ch10fn1a). Code that delegates to the driver is more future-proof
    than code that tries to perform allocations that comply with the documented alignment
    restrictions, since those restrictions are subject to change.'
  id: totrans-2770
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch10.html#ch10fn1a)。委托给驱动程序的代码比试图执行符合文档化对齐限制的分配的代码更具未来兼容性，因为这些限制可能会发生变化。'
- en: 10.2.2\. CUDA Arrays and Block Linear Addressing
  id: totrans-2771
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.2\. CUDA 数组和块线性寻址
- en: CUDA arrays are designed specifically to support texturing. They are allocated
    from the same pool of physical memory as device memory, but they have an opaque
    layout and cannot be addressed with pointers. Instead, memory locations in a CUDA
    array must be identified by the array handle and a set of 1D, 2D, or 3D coordinates.
  id: totrans-2772
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 数组专门设计用于支持纹理映射。它们从与设备内存相同的物理内存池中分配内存，但具有不透明的布局，并且无法通过指针访问。相反，CUDA 数组中的内存位置必须通过数组句柄和一组
    1D、2D 或 3D 坐标来识别。
- en: 'CUDA arrays perform a more complicated addressing calculation, designed so
    that contiguous addresses exhibit 2D or 3D locality. The addressing calculation
    is hardware-specific and changes from one hardware generation to the next. [Figure
    10.1](ch10.html#ch10fig01) illustrates one of the mechanisms used: The two least
    significant address bits of row and column have been interleaved before undertaking
    the addressing calculation.'
  id: totrans-2773
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 数组执行更复杂的寻址计算，旨在使相邻的地址具有 2D 或 3D 局部性。该寻址计算是硬件特定的，并且会随着硬件代际的变化而变化。[图 10.1](ch10.html#ch10fig01)展示了其中一种机制：行和列的两个最低有效地址位已经在执行寻址计算之前交错。
- en: 'As you can see in [Figure 10.3](ch10.html#ch10fig03), bit interleaving enables
    contiguous addresses to have “dimensional locality”: A cache line fill pulls in
    a block of pixels in a neighborhood rather than a horizontal span of pixels.^([2](ch10.html#ch10fn2))
    When taken to the limit, bit interleaving imposes some inconvenient requirements
    on the texture dimensions, so it is just one of several strategies used for the
    so-called “block linear” addressing calculation.'
  id: totrans-2774
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 10.3](ch10.html#ch10fig03)所示，位交错使得相邻的地址具有“维度局部性”：缓存行填充会拉入一个邻域内的像素块，而不是一段水平像素跨度。^([2](ch10.html#ch10fn2))
    当位交错达到极限时，会对纹理尺寸提出一些不便的要求，因此它只是用于所谓的“块线性”寻址计算的几种策略之一。
- en: '[2](ch10.html#ch10fn2a). 3D textures similarly interleave the X, Y, and Z coordinate
    bits.'
  id: totrans-2775
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch10.html#ch10fn2a)。3D 纹理同样会交错 X、Y 和 Z 坐标位。'
- en: '![Image](graphics/10fig03.jpg)'
  id: totrans-2776
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/10fig03.jpg)'
- en: '*Figure 10.3* 1024x768 image, interleaved bits.'
  id: totrans-2777
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.3* 1024x768 图像，交错位。'
- en: In device memory, the location of an image element can be specified by any of
    the following.
  id: totrans-2778
  prefs: []
  type: TYPE_NORMAL
  zh: 在设备内存中，可以通过以下任何方式指定图像元素的位置。
- en: • The base pointer, pitch, and a *(XInBytes, Y)* or *(XInBytes, Y, Z)* tuple
  id: totrans-2779
  prefs: []
  type: TYPE_NORMAL
  zh: • 基址指针、步幅以及一个*(XInBytes, Y)*或*(XInBytes, Y, Z)*元组
- en: • The base pointer and an offset as computed by [Equation 10.1](ch10.html#ch10equ01)
  id: totrans-2780
  prefs: []
  type: TYPE_NORMAL
  zh: • 基址指针和通过[方程 10.1](ch10.html#ch10equ01)计算出的偏移量
- en: • The device pointer with the offset already applied
  id: totrans-2781
  prefs: []
  type: TYPE_NORMAL
  zh: • 已经应用偏移量的设备指针
- en: In contrast, when CUDA arrays do not have device memory addresses, so memory
    locations must be specified in terms of the CUDA array and a tuple *(XInBytes,
    Y)* or *(XInBytes, Y, Z)*.
  id: totrans-2782
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当 CUDA 数组没有设备内存地址时，必须通过 CUDA 数组和元组 *(XInBytes, Y)* 或 *(XInBytes, Y, Z)* 来指定内存位置。
- en: Creating and Destroying CUDA Arrays
  id: totrans-2783
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建和销毁 CUDA 数组
- en: Using the CUDA runtime, CUDA arrays may be created by calling `cudaMallocArray()`.
  id: totrans-2784
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CUDA 运行时，可以通过调用 `cudaMallocArray()` 来创建 CUDA 数组。
- en: '[Click here to view code image](ch10_images.html#p309pro01a)'
  id: totrans-2785
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p309pro01a)'
- en: cudaError_t cudaMallocArray(struct cudaArray **array, const struct
  id: totrans-2786
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMallocArray(struct cudaArray **array, const struct
- en: cudaChannelFormatDesc *desc, size_t width, size_t height __dv(0),
  id: totrans-2787
  prefs: []
  type: TYPE_NORMAL
  zh: cudaChannelFormatDesc *desc, size_t width, size_t height __dv(0),
- en: unsigned int flags __dv(0));
  id: totrans-2788
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int flags __dv(0));
- en: '`array` passes back the array handle, and `desc` specifies the number and type
    of components (e.g., 2 floats) in each array element. `width` specifies the width
    of the array *in bytes*. `height` is an optional parameter that specifies the
    height of the array; if the height is not specified, `cudaMallocArray()` creates
    a 1D CUDA array.'
  id: totrans-2789
  prefs: []
  type: TYPE_NORMAL
  zh: '`array` 返回数组句柄，`desc` 指定每个数组元素中的组件数量和类型（例如，2 个浮点数）。`width` 指定数组的宽度 *以字节为单位*。`height`
    是一个可选参数，指定数组的高度；如果没有指定高度，`cudaMallocArray()` 会创建一个 1D CUDA 数组。'
- en: The `flags` parameter is used to hint at the CUDA array’s usage. As of this
    writing, the only flag is `cudaArraySurfaceLoadStore`, which must be specified
    if the CUDA array will be used for surface read/write operations as described
    later in this chapter.
  id: totrans-2790
  prefs: []
  type: TYPE_NORMAL
  zh: '`flags` 参数用于提示 CUDA 数组的用途。截至目前，唯一的标志是 `cudaArraySurfaceLoadStore`，如果 CUDA 数组用于后文所述的表面读写操作，则必须指定该标志。'
- en: The `__dv` macro used for the `height` and `flags` parameters causes the declaration
    to behave differently, depending on the language. When compiled for C, it becomes
    a simple parameter, but when compiled for C++, it becomes a parameter with the
    specified default value.
  id: totrans-2791
  prefs: []
  type: TYPE_NORMAL
  zh: 用于 `height` 和 `flags` 参数的 `__dv` 宏会根据语言不同导致声明行为不同。在 C 语言中，它成为一个简单的参数，而在 C++
    中，它成为一个带有默认值的参数。
- en: The structure `cudaChannelFormatDesc` describes the contents of a texture.
  id: totrans-2792
  prefs: []
  type: TYPE_NORMAL
  zh: 结构体 `cudaChannelFormatDesc` 描述了纹理的内容。
- en: struct cudaChannelFormatDesc {
  id: totrans-2793
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaChannelFormatDesc {
- en: int x, y, z, w;
  id: totrans-2794
  prefs: []
  type: TYPE_NORMAL
  zh: int x, y, z, w;
- en: enum cudaChannelFormatKind f;
  id: totrans-2795
  prefs: []
  type: TYPE_NORMAL
  zh: enum cudaChannelFormatKind f;
- en: '};'
  id: totrans-2796
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: The `x`, `y`, `z,` and `w` members of the structure specify the number of bits
    in each member of the texture element. For example, a 1-element float texture
    will contain `x==32` and the other elements will be `0`. The `cudaChannelFormatKind`
    structure specifies whether the data is signed integer, unsigned integer, or floating
    point.
  id: totrans-2797
  prefs: []
  type: TYPE_NORMAL
  zh: 结构体的 `x`、`y`、`z` 和 `w` 成员指定了纹理元素中每个成员的位数。例如，一个包含 1 个元素的浮点纹理将包含 `x==32`，其他元素为
    `0`。`cudaChannelFormatKind` 结构体指定数据是有符号整数、无符号整数还是浮点数。
- en: '[Click here to view code image](ch10_images.html#p310pro01a)'
  id: totrans-2798
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p310pro01a)'
- en: enum cudaChannelFormatKind
  id: totrans-2799
  prefs: []
  type: TYPE_NORMAL
  zh: enum cudaChannelFormatKind
- en: '{'
  id: totrans-2800
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaChannelFormatKindSigned = 0,
  id: totrans-2801
  prefs: []
  type: TYPE_NORMAL
  zh: cudaChannelFormatKindSigned = 0,
- en: cudaChannelFormatKindUnsigned = 1,
  id: totrans-2802
  prefs: []
  type: TYPE_NORMAL
  zh: cudaChannelFormatKindUnsigned = 1，
- en: cudaChannelFormatKindFloat = 2,
  id: totrans-2803
  prefs: []
  type: TYPE_NORMAL
  zh: cudaChannelFormatKindFloat = 2，
- en: cudaChannelFormatKindNone = 3
  id: totrans-2804
  prefs: []
  type: TYPE_NORMAL
  zh: cudaChannelFormatKindNone = 3
- en: '};'
  id: totrans-2805
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: Developers can create `cudaChannelFormatDesc` structures using the `cudaCreateChannelDesc`
    function.
  id: totrans-2806
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员可以使用`cudaCreateChannelDesc`函数创建`cudaChannelFormatDesc`结构体。
- en: '[Click here to view code image](ch10_images.html#p310pro03a)'
  id: totrans-2807
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p310pro03a)'
- en: cudaChannelFormatDesc cudaCreateChannelDesc(int x, int y, int z, int w, cudaChannelFormatKind
    kind);
  id: totrans-2808
  prefs: []
  type: TYPE_NORMAL
  zh: cudaChannelFormatDesc cudaCreateChannelDesc(int x, int y, int z, int w, cudaChannelFormatKind
    kind);
- en: Alternatively, a templated family of functions can be invoked as follows.
  id: totrans-2809
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，可以像下面这样调用一个模板化的函数系列。
- en: template<class T> cudaCreateChannelDesc<T>();
  id: totrans-2810
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T> cudaCreateChannelDesc<T>();
- en: where `T` may be any of the native formats supported by CUDA. Here are two examples
    of the specializations of this template.
  id: totrans-2811
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`T`可以是CUDA支持的任何原生格式。这里有两个模板特化的示例。
- en: '[Click here to view code image](ch10_images.html#p311pro01a)'
  id: totrans-2812
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p311pro01a)'
- en: template<> __inline__ __host__ cudaChannelFormatDesc cudaCreateChannelDesc<float>(void)
  id: totrans-2813
  prefs: []
  type: TYPE_NORMAL
  zh: template<> __inline__ __host__ cudaChannelFormatDesc cudaCreateChannelDesc<float>(void)
- en: '{'
  id: totrans-2814
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int e = (int)sizeof(float) * 8;
  id: totrans-2815
  prefs: []
  type: TYPE_NORMAL
  zh: int e = (int)sizeof(float) * 8;
- en: return cudaCreateChannelDesc(e, 0, 0, 0, cudaChannelFormatKindFloat);
  id: totrans-2816
  prefs: []
  type: TYPE_NORMAL
  zh: return cudaCreateChannelDesc(e, 0, 0, 0, cudaChannelFormatKindFloat);
- en: '}'
  id: totrans-2817
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: template<> __inline__ __host__ cudaChannelFormatDesc cudaCreateChannelDesc<uint2>(void)
  id: totrans-2818
  prefs: []
  type: TYPE_NORMAL
  zh: template<> __inline__ __host__ cudaChannelFormatDesc cudaCreateChannelDesc<uint2>(void)
- en: '{'
  id: totrans-2819
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int e = (int)sizeof(unsigned int) * 8;
  id: totrans-2820
  prefs: []
  type: TYPE_NORMAL
  zh: int e = (int)sizeof(unsigned int) * 8;
- en: return cudaCreateChannelDesc(e, e, 0, 0, cudaChannelFormatKindUnsigned);
  id: totrans-2821
  prefs: []
  type: TYPE_NORMAL
  zh: return cudaCreateChannelDesc(e, e, 0, 0, cudaChannelFormatKindUnsigned);
- en: '}'
  id: totrans-2822
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2823
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Caution
  id: totrans-2824
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When using the `char` data type, be aware that some compilers assume `char`
    is signed, while others assume it is unsigned. You can always make this distinction
    unambiguous with the `signed` keyword.
  id: totrans-2825
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`char`数据类型时，要注意一些编译器假定`char`是有符号的，而另一些则假定它是无符号的。你可以通过`signed`关键字使这一点不再模糊。
- en: '* * *'
  id: totrans-2826
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 3D CUDA arrays may be allocated with `cudaMalloc3DArray().`
  id: totrans-2827
  prefs: []
  type: TYPE_NORMAL
  zh: 3D CUDA 数组可以通过`cudaMalloc3DArray()`来分配。
- en: '[Click here to view code image](ch10_images.html#p311pro03a)'
  id: totrans-2828
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p311pro03a)'
- en: cudaError_t cudaMalloc3DArray(struct cudaArray** array, const struct
  id: totrans-2829
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMalloc3DArray(struct cudaArray** array，const struct
- en: cudaChannelFormatDesc* desc, struct cudaExtent extent, unsigned int
  id: totrans-2830
  prefs: []
  type: TYPE_NORMAL
  zh: cudaChannelFormatDesc* desc，struct cudaExtent extent，unsigned int
- en: flags __dv(0));
  id: totrans-2831
  prefs: []
  type: TYPE_NORMAL
  zh: flags __dv(0));
- en: Rather than taking width, height, and depth parameters, `cudaMalloc3DArray()`
    takes a `cudaExtent` structure.
  id: totrans-2832
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaMalloc3DArray()`不是接受宽度、高度和深度参数，而是接受一个`cudaExtent`结构体。'
- en: struct cudaExtent {
  id: totrans-2833
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaExtent {
- en: size_t width;
  id: totrans-2834
  prefs: []
  type: TYPE_NORMAL
  zh: size_t width;
- en: size_t height;
  id: totrans-2835
  prefs: []
  type: TYPE_NORMAL
  zh: size_t height;
- en: size_t depth;
  id: totrans-2836
  prefs: []
  type: TYPE_NORMAL
  zh: size_t depth;
- en: '};'
  id: totrans-2837
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: The `flags` parameter, like that of `cudaMallocArray()`, must be `cudaArraySurfaceLoadStore`
    if the CUDA array will be used for surface read/write operations.
  id: totrans-2838
  prefs: []
  type: TYPE_NORMAL
  zh: '`flags`参数，像`cudaMallocArray()`的参数一样，如果CUDA数组将用于表面读写操作，则必须为`cudaArraySurfaceLoadStore`。'
- en: '* * *'
  id: totrans-2839
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-2840
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For array handles, the CUDA runtime and driver API are compatible with one another.
    The pointer passed back by `cudaMallocArray()` can be cast to `CUarray` and passed
    to driver API functions such as `cuArrayGetDescriptor()`.
  id: totrans-2841
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数组句柄，CUDA运行时和驱动程序API彼此兼容。通过`cudaMallocArray()`返回的指针可以强制转换为`CUarray`并传递给驱动程序API函数，如`cuArrayGetDescriptor()`。
- en: '* * *'
  id: totrans-2842
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Driver API*'
  id: totrans-2843
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '*驱动程序API*'
- en: The driver API equivalents of `cudaMallocArray()` and `cudaMalloc3DArray()`
    are `cuArrayCreate()` and `cuArray3DCreate()`, respectively.
  id: totrans-2844
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaMallocArray()`和`cudaMalloc3DArray()`的驱动程序API等价函数分别是`cuArrayCreate()`和`cuArray3DCreate()`。'
- en: '[Click here to view code image](ch10_images.html#p312pro03a)'
  id: totrans-2845
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p312pro03a)'
- en: CUresult cuArrayCreate(CUarray *pHandle, const CUDA_ARRAY_DESCRIPTOR
  id: totrans-2846
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuArrayCreate(CUarray *pHandle, const CUDA_ARRAY_DESCRIPTOR
- en: '*pAllocateArray);'
  id: totrans-2847
  prefs: []
  type: TYPE_NORMAL
  zh: '*pAllocateArray);'
- en: CUresult cuArray3DCreate(CUarray *pHandle, const CUDA_ARRAY3D_
  id: totrans-2848
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuArray3DCreate(CUarray *pHandle, const CUDA_ARRAY3D_
- en: DESCRIPTOR *pAllocateArray);
  id: totrans-2849
  prefs: []
  type: TYPE_NORMAL
  zh: DESCRIPTOR *pAllocateArray);
- en: '`cuArray3DCreate()` can be used to allocate 1D or 2D CUDA arrays by specifying
    0 as the height or depth, respectively. The `CUDA_ARRAY3D_DESCRIPTOR` structure
    is as follows.'
  id: totrans-2850
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuArray3DCreate()`可以通过分别指定高度或深度为0来分配1D或2D CUDA数组。`CUDA_ARRAY3D_DESCRIPTOR`结构如下所示。'
- en: '[Click here to view code image](ch10_images.html#p312pro01a)'
  id: totrans-2851
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p312pro01a)'
- en: typedef struct CUDA_ARRAY3D_DESCRIPTOR_st
  id: totrans-2852
  prefs: []
  type: TYPE_NORMAL
  zh: typedef struct CUDA_ARRAY3D_DESCRIPTOR_st
- en: '{'
  id: totrans-2853
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t Width;
  id: totrans-2854
  prefs: []
  type: TYPE_NORMAL
  zh: size_t 宽度；
- en: size_t Height;
  id: totrans-2855
  prefs: []
  type: TYPE_NORMAL
  zh: size_t 高度；
- en: size_t Depth;
  id: totrans-2856
  prefs: []
  type: TYPE_NORMAL
  zh: size_t 深度；
- en: CUarray_format Format;
  id: totrans-2857
  prefs: []
  type: TYPE_NORMAL
  zh: CUarray_format 格式；
- en: unsigned int NumChannels;
  id: totrans-2858
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号整数 NumChannels；
- en: unsigned int Flags;
  id: totrans-2859
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号整数 标志；
- en: '} CUDA_ARRAY3D_DESCRIPTOR;'
  id: totrans-2860
  prefs: []
  type: TYPE_NORMAL
  zh: '} CUDA_ARRAY3D_DESCRIPTOR；'
- en: 'Together, the `Format` and `NumChannels` members describe the size of each
    element of the CUDA array: `NumChannels` may be 1, 2, or 4, and `Format` specifies
    the channels’ type, as follows.'
  id: totrans-2861
  prefs: []
  type: TYPE_NORMAL
  zh: '`Format`和`NumChannels`成员共同描述CUDA数组中每个元素的大小：`NumChannels`可以是1、2或4，`Format`指定通道类型，如下所示。'
- en: '[Click here to view code image](ch10_images.html#p312pro02a)'
  id: totrans-2862
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p312pro02a)'
- en: typedef enum CUarray_format_enum {
  id: totrans-2863
  prefs: []
  type: TYPE_NORMAL
  zh: typedef enum CUarray_format_enum {
- en: CU_AD_FORMAT_UNSIGNED_INT8 = 0x01,
  id: totrans-2864
  prefs: []
  type: TYPE_NORMAL
  zh: CU_AD_FORMAT_UNSIGNED_INT8 = 0x01，
- en: CU_AD_FORMAT_UNSIGNED_INT16 = 0x02,
  id: totrans-2865
  prefs: []
  type: TYPE_NORMAL
  zh: CU_AD_FORMAT_UNSIGNED_INT16 = 0x02，
- en: CU_AD_FORMAT_UNSIGNED_INT32 = 0x03,
  id: totrans-2866
  prefs: []
  type: TYPE_NORMAL
  zh: CU_AD_FORMAT_UNSIGNED_INT32 = 0x03，
- en: CU_AD_FORMAT_SIGNED_INT8 = 0x08,
  id: totrans-2867
  prefs: []
  type: TYPE_NORMAL
  zh: CU_AD_FORMAT_SIGNED_INT8 = 0x08，
- en: CU_AD_FORMAT_SIGNED_INT16 = 0x09,
  id: totrans-2868
  prefs: []
  type: TYPE_NORMAL
  zh: CU_AD_FORMAT_SIGNED_INT16 = 0x09，
- en: CU_AD_FORMAT_SIGNED_INT32 = 0x0a,
  id: totrans-2869
  prefs: []
  type: TYPE_NORMAL
  zh: CU_AD_FORMAT_SIGNED_INT32 = 0x0a，
- en: CU_AD_FORMAT_HALF = 0x10,
  id: totrans-2870
  prefs: []
  type: TYPE_NORMAL
  zh: CU_AD_FORMAT_HALF = 0x10，
- en: CU_AD_FORMAT_FLOAT = 0x20
  id: totrans-2871
  prefs: []
  type: TYPE_NORMAL
  zh: CU_AD_FORMAT_FLOAT = 0x20
- en: '} CUarray_format;'
  id: totrans-2872
  prefs: []
  type: TYPE_NORMAL
  zh: '} CUarray_format；'
- en: '* * *'
  id: totrans-2873
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-2874
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The format specified in `CUDA_ARRAY3D_DESCRIPTOR` is just a convenient way to
    specify the amount of data in the CUDA array. Textures bound to the CUDA array
    can specify a different format, as long as the bytes per element is the same.
    For example, it is perfectly valid to bind a `texture<int>` reference to a CUDA
    array containing 4-component bytes (32 bits per element).
  id: totrans-2875
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA_ARRAY3D_DESCRIPTOR`中指定的格式只是指定CUDA数组中数据量的一种方便方式。绑定到CUDA数组的纹理可以指定不同的格式，只要每个元素的字节数相同。例如，将`texture<int>`引用绑定到包含4组件字节（每个元素32位）的CUDA数组是完全有效的。'
- en: '* * *'
  id: totrans-2876
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Sometimes CUDA array handles are passed to subroutines that need to query the
    dimensions and/or format of the input array. The following `cuArray3DGetDescriptor()`
    function is provided for that purpose.
  id: totrans-2877
  prefs: []
  type: TYPE_NORMAL
  zh: 有时 CUDA 数组句柄会传递给需要查询输入数组的维度和/或格式的子程序。为此提供了以下的 `cuArray3DGetDescriptor()` 函数。
- en: '[Click here to view code image](ch10_images.html#p313pro01a)'
  id: totrans-2878
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch10_images.html#p313pro01a)'
- en: CUresult cuArray3DGetDescriptor(CUDA_ARRAY3D_DESCRIPTOR
  id: totrans-2879
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult cuArray3DGetDescriptor(CUDA_ARRAY3D_DESCRIPTOR
- en: '*pArrayDescriptor, CUarray hArray);'
  id: totrans-2880
  prefs: []
  type: TYPE_NORMAL
  zh: '*pArrayDescriptor, CUarray hArray);'
- en: Note that this function may be called on 1D and 2D CUDA arrays, even those that
    were created with `cuArrayCreate()`.
  id: totrans-2881
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此函数可以在 1D 和 2D CUDA 数组上调用，甚至是那些通过 `cuArrayCreate()` 创建的数组。
- en: 10.2.3\. Device Memory versus CUDA Arrays
  id: totrans-2882
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.3\. 设备内存与 CUDA 数组
- en: For applications that exhibit sparse access patterns, especially patterns with
    dimensional locality (for example, computer vision applications), CUDA arrays
    are a clear win. For applications with regular access patterns, especially those
    with little to no reuse or whose reuse can be explicitly managed by the application
    in shared memory, device pointers are the obvious choice.
  id: totrans-2883
  prefs: []
  type: TYPE_NORMAL
  zh: 对于展示稀疏访问模式的应用程序，特别是具有维度局部性的模式（例如计算机视觉应用程序），CUDA 数组显然更具优势。对于具有规则访问模式的应用程序，尤其是那些几乎没有重用或者其重用可以由应用程序在共享内存中显式管理的应用程序，设备指针是明显的选择。
- en: Some applications, such as image processing applications, fall into a gray area
    where the choice between device pointers and CUDA arrays is not obvious. All other
    things being equal, device memory is probably preferable to CUDA arrays, but the
    following considerations may be used to help in the decision-making process.
  id: totrans-2884
  prefs: []
  type: TYPE_NORMAL
  zh: 一些应用程序，如图像处理应用程序，处于一个灰色地带，在设备指针和 CUDA 数组之间的选择并不明显。在其他条件相等的情况下，设备内存可能比 CUDA 数组更为优选，但以下因素可帮助决策过程。
- en: • Until CUDA 3.2, CUDA kernels could not write to CUDA arrays. They were only
    able to read from them via texture intrinsics. CUDA 3.2 added the ability for
    Fermi-class hardware to access 2D CUDA arrays via “surface read/write” intrinsics.
  id: totrans-2885
  prefs: []
  type: TYPE_NORMAL
  zh: • 在 CUDA 3.2 之前，CUDA 内核无法写入 CUDA 数组。它们只能通过纹理内建函数从中读取数据。CUDA 3.2 增加了 Fermi 类硬件通过“表面读/写”内建函数访问
    2D CUDA 数组的能力。
- en: • CUDA arrays do not consume any CUDA address space.
  id: totrans-2886
  prefs: []
  type: TYPE_NORMAL
  zh: • CUDA 数组不会占用任何 CUDA 地址空间。
- en: • On WDDM drivers (Windows Vista and later), the system can automatically manage
    the residence of CUDA arrays. They can be swapped into and out of device memory
    transparently, depending on whether they are needed by the CUDA kernels that are
    executing. In contrast, WDDM requires all device memory to be resident in order
    for any kernel to execute.
  id: totrans-2887
  prefs: []
  type: TYPE_NORMAL
  zh: • 在 WDDM 驱动程序（Windows Vista 及更高版本）中，系统可以自动管理 CUDA 数组的驻留。它们可以根据是否被正在执行的 CUDA
    内核需要而透明地在设备内存中交换进出。相比之下，WDDM 要求所有设备内存都驻留，以便执行任何内核。
- en: • CUDA arrays can reside only in device memory, and if the GPU contains copy
    engines, it can convert between the two representations while transferring the
    data across the bus. For some applications, keeping a pitch representation in
    host memory and a CUDA array representation in device memory is the best fit.
  id: totrans-2888
  prefs: []
  type: TYPE_NORMAL
  zh: • CUDA数组只能驻留在设备内存中，如果GPU包含复制引擎，它可以在通过总线传输数据时在两种表示形式之间进行转换。对于某些应用程序，保持主机内存中的步幅表示和设备内存中的CUDA数组表示是最佳选择。
- en: 10.3\. 1D Texturing
  id: totrans-2889
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3\. 1D纹理
- en: For illustrative purposes, we will deal with 1D textures in detail and then
    expand the discussion to include 2D and 3D textures.
  id: totrans-2890
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明问题，我们将详细讨论1D纹理，然后扩展讨论2D和3D纹理。
- en: 10.3.1\. Texture Setup
  id: totrans-2891
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.1\. 纹理设置
- en: The data in textures can consist of 1, 2, or 4 elements of any of the following
    types.
  id: totrans-2892
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理中的数据可以由1、2或4个元素组成，且元素类型可以是以下任意类型。
- en: • Signed or unsigned 8-, 16-, or 32-bit integers
  id: totrans-2893
  prefs: []
  type: TYPE_NORMAL
  zh: • 有符号或无符号的8位、16位或32位整数
- en: • 16-bit floating-point values
  id: totrans-2894
  prefs: []
  type: TYPE_NORMAL
  zh: • 16位浮点数值
- en: • 32-bit floating-point values
  id: totrans-2895
  prefs: []
  type: TYPE_NORMAL
  zh: • 32位浮点数值
- en: In the `.cu` file (whether using the CUDA runtime or the driver API), the texture
    reference is declared as follows.
  id: totrans-2896
  prefs: []
  type: TYPE_NORMAL
  zh: 在`.cu`文件中（无论是使用CUDA运行时还是驱动程序API），纹理引用声明如下。
- en: texture<ReturnType, Dimension, ReadMode> Name;
  id: totrans-2897
  prefs: []
  type: TYPE_NORMAL
  zh: texture<ReturnType, Dimension, ReadMode> Name;
- en: where `ReturnType` is the value returned by the texture intrinsic; `Dimension`
    is 1, 2, or 3 for 1D, 2D, or 3D, respectively; and `ReadMode` is an optional parameter
    type that defaults to `cudaReadModeElementType`. The read mode only affects integer-valued
    texture data. By default, the texture passes back integers when the texture data
    is integer-valued, promoting them to 32-bit if necessary. But when `cudaReadModeNormalizedFloat`
    is specified as the read mode, 8- or 16-bit integers can be promoted to floating-point
    values in the range [0.0, 1.0] according to the formulas in [Table 10.1](ch10.html#ch10tab01).
  id: totrans-2898
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，`ReturnType`是纹理内建函数返回的值；`Dimension`为1、2或3，分别表示1D、2D或3D；`ReadMode`是一个可选的参数类型，默认为`cudaReadModeElementType`。读取模式仅影响整数值的纹理数据。默认情况下，当纹理数据是整数值时，纹理会返回整数，并在必要时将其提升为32位整数。但当指定`cudaReadModeNormalizedFloat`作为读取模式时，8位或16位整数可以根据[表10.1](ch10.html#ch10tab01)中的公式被转换为范围在[0.0,
    1.0]之间的浮点数值。
- en: '![Image](graphics/10tab01.jpg)'
  id: totrans-2899
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/10tab01.jpg)'
- en: '*Table 10.1* Floating-Point Promotion (Texture)'
  id: totrans-2900
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 10.1* 浮点数提升（纹理）'
- en: The C versions of this conversion operation are given in [Listing 10.1](ch10.html#ch10lis01).
  id: totrans-2901
  prefs: []
  type: TYPE_NORMAL
  zh: 此转换操作的C语言版本见[列表 10.1](ch10.html#ch10lis01)。
- en: '*Listing 10.1.* Texture unit floating-point conversion.'
  id: totrans-2902
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 10.1.* 纹理单元浮点数转换。'
- en: '[Click here to view code image](ch10_images.html#p10lis01a)'
  id: totrans-2903
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch10_images.html#p10lis01a)'
- en: '* * *'
  id: totrans-2904
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: float
  id: totrans-2905
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: TexPromoteToFloat( signed char c )
  id: totrans-2906
  prefs: []
  type: TYPE_NORMAL
  zh: TexPromoteToFloat( signed char c )
- en: '{'
  id: totrans-2907
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: if ( c == (signed char) 0x80 ) {
  id: totrans-2908
  prefs: []
  type: TYPE_NORMAL
  zh: if ( c == (signed char) 0x80 ) {
- en: return -1.0f;
  id: totrans-2909
  prefs: []
  type: TYPE_NORMAL
  zh: return -1.0f;
- en: '}'
  id: totrans-2910
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return (float) c / 127.0f;
  id: totrans-2911
  prefs: []
  type: TYPE_NORMAL
  zh: return (float) c / 127.0f;
- en: '}'
  id: totrans-2912
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: float
  id: totrans-2913
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: TexPromoteToFloat( short s )
  id: totrans-2914
  prefs: []
  type: TYPE_NORMAL
  zh: TexPromoteToFloat( short s )
- en: '{'
  id: totrans-2915
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: if ( s == (short) 0x8000 ) {
  id: totrans-2916
  prefs: []
  type: TYPE_NORMAL
  zh: if ( s == (short) 0x8000 ) {
- en: return -1.0f;
  id: totrans-2917
  prefs: []
  type: TYPE_NORMAL
  zh: return -1.0f;
- en: '}'
  id: totrans-2918
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return (float) s / 32767.0f;
  id: totrans-2919
  prefs: []
  type: TYPE_NORMAL
  zh: return (float) s / 32767.0f;
- en: '}'
  id: totrans-2920
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: float
  id: totrans-2921
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: TexPromoteToFloat( unsigned char uc )
  id: totrans-2922
  prefs: []
  type: TYPE_NORMAL
  zh: TexPromoteToFloat( unsigned char uc )
- en: '{'
  id: totrans-2923
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: return (float) uc / 255.0f;
  id: totrans-2924
  prefs: []
  type: TYPE_NORMAL
  zh: return (float) uc / 255.0f;
- en: '}'
  id: totrans-2925
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: float
  id: totrans-2926
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: TexPromoteToFloat( unsigned short us )
  id: totrans-2927
  prefs: []
  type: TYPE_NORMAL
  zh: TexPromoteToFloat( unsigned short us )
- en: '{'
  id: totrans-2928
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: return (float) us / 65535.0f;
  id: totrans-2929
  prefs: []
  type: TYPE_NORMAL
  zh: return (float) us / 65535.0f;
- en: '}'
  id: totrans-2930
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2931
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Once the texture reference is declared, it can be used in kernels by invoking
    texture intrinsics. Different intrinsics are used for different types of texture,
    as shown in [Table 10.2](ch10.html#ch10tab02).
  id: totrans-2932
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦纹理引用被声明，它就可以通过调用纹理内建函数在内核中使用。不同类型的纹理使用不同的内建函数，如[表 10.2](ch10.html#ch10tab02)所示。
- en: '![Image](graphics/10tab02.jpg)'
  id: totrans-2933
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/10tab02.jpg)'
- en: '*Table 10.2* Texture Intrinsics'
  id: totrans-2934
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 10.2* 纹理内建函数'
- en: Texture references have file scope and behave similarly to global variables.
    They cannot be created, destroyed, or passed as parameters, so wrapping them in
    higher-level abstractions must be undertaken with care.
  id: totrans-2935
  prefs: []
  type: TYPE_NORMAL
  zh: 纹理引用具有文件作用域，行为类似于全局变量。它们不能被创建、销毁或作为参数传递，因此将它们封装在更高级的抽象中时必须小心。
- en: CUDA Runtime
  id: totrans-2936
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CUDA运行时
- en: Before invoking a kernel that uses a texture, the texture must be *bound* to
    a CUDA array or device memory by calling `cudaBindTexture()`, `cudaBindTexture2D()`,
    or `cudaBindTextureToArray()`. Due to the language integration of the CUDA runtime,
    the texture can be referenced by name, such as the following.
  id: totrans-2937
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用使用纹理的内核之前，必须通过调用`cudaBindTexture()`、`cudaBindTexture2D()`或`cudaBindTextureToArray()`将纹理*绑定*到CUDA数组或设备内存中。由于CUDA运行时的语言集成，纹理可以通过名称引用，如下所示。
- en: '[Click here to view code image](ch10_images.html#p316pro01a)'
  id: totrans-2938
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p316pro01a)'
- en: texture<float, 2, cudaReadModeElementType> tex;
  id: totrans-2939
  prefs: []
  type: TYPE_NORMAL
  zh: texture<float, 2, cudaReadModeElementType> tex;
- en: '...'
  id: totrans-2940
  prefs: []
  type: TYPE_NORMAL
  zh: '...'
- en: CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
  id: totrans-2941
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
- en: Once the texture is bound, kernels that use that texture reference will read
    from the bound memory until the texture binding is changed.
  id: totrans-2942
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦纹理被绑定，使用该纹理引用的内核将从绑定的内存中读取数据，直到纹理绑定被更改。
- en: Driver API
  id: totrans-2943
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 驱动程序API
- en: When a texture is declared in a `.cu` file, driver applications must query it
    using `cuModuleGetTexRef()`. In the driver API, the immutable attributes of the
    texture must be set explicitly, and they must agree with the assumptions used
    by the compiler to generate the code. For most textures, this just means the format
    must agree with the format declared in the `.cu` file; the exception is when textures
    are set up to promote integers or 16-bit floating-point values to normalized 32-bit
    floating-point values.
  id: totrans-2944
  prefs: []
  type: TYPE_NORMAL
  zh: 当纹理在`.cu`文件中声明时，驱动程序应用程序必须使用`cuModuleGetTexRef()`查询它。在驱动程序API中，纹理的不可变属性必须显式设置，并且它们必须与编译器生成代码时使用的假设一致。对于大多数纹理，这仅意味着格式必须与`.cu`文件中声明的格式一致；唯一的例外是当纹理被设置为将整数或16位浮动点值提升为标准化的32位浮动点值时。
- en: The `cuTexRefSetFormat()` function is used to specify the format of the data
    in the texture.
  id: totrans-2945
  prefs: []
  type: TYPE_NORMAL
  zh: '`cuTexRefSetFormat()`函数用于指定纹理中数据的格式。'
- en: '[Click here to view code image](ch10_images.html#p316pro02a)'
  id: totrans-2946
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p316pro02a)'
- en: CUresult CUDAAPI cuTexRefSetFormat(CUtexref hTexRef, CUarray_format
  id: totrans-2947
  prefs: []
  type: TYPE_NORMAL
  zh: CUresult CUDAAPI cuTexRefSetFormat(CUtexref hTexRef, CUarray_format
- en: fmt, int NumPackedComponents);
  id: totrans-2948
  prefs: []
  type: TYPE_NORMAL
  zh: fmt, int NumPackedComponents);
- en: The array formats are as follows.
  id: totrans-2949
  prefs: []
  type: TYPE_NORMAL
  zh: 数组格式如下。
- en: '![Image](graphics/317tab01.jpg)'
  id: totrans-2950
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/317tab01.jpg)'
- en: '`NumPackedComponents` specifies the number of components in each texture element.
    It may be 1, 2, or 4\. 16-bit floats (`half`) are a special data type that are
    well suited to representing image data with high integrity.^([3](ch10.html#ch10fn3))
    With 10 bits of floating-point mantissa (effectively 11 bits of precision for
    normalized numbers), there is enough precision to represent data generated by
    most sensors, and 5 bits of exponent gives enough dynamic range to represent starlight
    and sunlight in the same image. Most floating-point architectures do not include
    native instructions to process 16-bit floats, and CUDA is no exception. The texture
    hardware promotes 16-bit floats to 32-bit floats automatically, and CUDA kernels
    can convert between 16- and 32-bit floats with the `__float2half_rn()` and `__half2float_rn()`
    intrinsics.'
  id: totrans-2951
  prefs: []
  type: TYPE_NORMAL
  zh: '`NumPackedComponents`指定每个纹理元素中的组件数量。它可以是1、2或4。16位浮动点数（`half`）是一种特殊的数据类型，非常适合高保真度表示图像数据。^([3](ch10.html#ch10fn3))
    具有10位浮动点尾数（实际上是11位精度用于规范化数值），足以表示大多数传感器生成的数据，5位的指数提供足够的动态范围，可以在同一图像中表示星光和阳光。大多数浮动点架构并不包含处理16位浮动点数的原生指令，CUDA也不例外。纹理硬件会自动将16位浮动点数提升为32位浮动点数，CUDA内核可以使用`__float2half_rn()`和`__half2float_rn()`内建函数在16位和32位浮动点数之间转换。'
- en: '[3](ch10.html#ch10fn3a). [Section 8.3.4](ch08.html#ch08lev2sec13) describes
    16-bit floats in detail.'
  id: totrans-2952
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch10.html#ch10fn3a)。[第8.3.4节](ch08.html#ch08lev2sec13)详细描述了16位浮动点数。'
- en: 10.4\. Texture as a Read Path
  id: totrans-2953
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4\. 纹理作为读取路径
- en: When using texture as a read path—that is, using the texturing hardware to get
    around awkward coalescing constraints or to take advantage of the texture cache
    as opposed to accessing hardware features such as linear interpolation—many texturing
    features are unavailable. The highlights of this usage for texture are as follows.
  id: totrans-2954
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用纹理作为读取路径时——即使用纹理硬件绕过困难的合并约束，或是利用纹理缓存，而不是访问线性插值等硬件特性——许多纹理功能将不可用。纹理作为读取路径的主要特点如下。
- en: • The texture reference must be bound to device memory with `cudaBind-Texture()`
    or `cuTexRefSetAddress()`.
  id: totrans-2955
  prefs: []
  type: TYPE_NORMAL
  zh: • 纹理引用必须通过`cudaBind-Texture()`或`cuTexRefSetAddress()`绑定到设备内存。
- en: • The `tex1Dfetch()` intrinsic must be used. It takes a 27-bit integer index.^([4](ch10.html#ch10fn4))
  id: totrans-2956
  prefs: []
  type: TYPE_NORMAL
  zh: • 必须使用`tex1Dfetch()`内建函数。它接受一个27位的整数索引。^([4](ch10.html#ch10fn4))
- en: '[4](ch10.html#ch10fn4a). All CUDA-capable hardware has the same 27-bit limit,
    so there is not yet any way to query a device for the limit.'
  id: totrans-2957
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch10.html#ch10fn4a)。所有支持CUDA的硬件都有相同的27位限制，因此目前还没有方法查询设备的该限制。'
- en: • `tex1Dfetch()` optionally can convert the texture contents to floating-point
    values. Integers are converted to floating-point values in the range [0.0, 1.0],
    and 16-bit floating-point values are promoted to `float`.
  id: totrans-2958
  prefs: []
  type: TYPE_NORMAL
  zh: • `tex1Dfetch()`可以选择性地将纹理内容转换为浮点值。整数会被转换为[0.0, 1.0]范围内的浮点值，16位浮点值会被提升为`float`。
- en: The benefits of reading device memory via `tex1Dfetch()` are twofold. First,
    memory reads via texture do not have to conform to the coalescing constraints
    that apply when reading global memory. Second, the texture cache can be a useful
    complement to the other hardware resources, even the L2 cache on Fermi-class hardware.
    When an out-of-range index is passed to `tex1Dfetch()`, it returns 0.
  id: totrans-2959
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`tex1Dfetch()`读取设备内存的好处是双重的。首先，通过纹理读取内存不必遵循读取全局内存时的合并约束。其次，纹理缓存可以成为其他硬件资源的有力补充，甚至是Fermi类硬件上的L2缓存。当传递一个越界的索引给`tex1Dfetch()`时，它会返回0。
- en: 10.4.1\. Increasing Effective Address Coverage
  id: totrans-2960
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.4.1\. 增加有效地址覆盖范围
- en: Since the 27-bit index specifies which texture element to fetch, and the texture
    elements may be up to 16 bytes in size, a texture being read via `tex1Dfetch()`
    can cover up to 31 bits (2^(27)+2⁴) worth of memory. One way to increase the amount
    of data being effectively covered by a texture is to use wider texture elements
    than the actual data size. For example, the application can texture from `float4`
    instead of `float`, then select the appropriate element of the `float4,` depending
    on the least significant bits of the desired index. Similar techniques can be
    applied to integer data, especially 8- or 16-bit data where global memory transactions
    are always uncoalesced. Alternatively, applications can alias multiple textures
    over different segments of the device memory and perform predicated texture fetches
    from each texture in such a way that only one of them is “live.”
  id: totrans-2961
  prefs: []
  type: TYPE_NORMAL
  zh: 由于27位的索引指定了要获取的纹理元素，并且纹理元素的大小可能达到16字节，因此通过`tex1Dfetch()`读取的纹理可以覆盖多达31位（2^(27)+2⁴）的内存。增加纹理有效覆盖数据量的一种方法是使用比实际数据大小更宽的纹理元素。例如，应用程序可以使用`float4`代替`float`进行纹理处理，然后根据所需索引的最低有效位选择合适的`float4`元素。类似的技术也可以应用于整数数据，特别是8位或16位数据，在这些数据中，全局内存事务总是未合并的。或者，应用程序可以将多个纹理别名化到设备内存的不同段上，并从每个纹理执行条件纹理读取，以确保只有一个纹理是“活动”的。
- en: 'Microdemo: tex1dfetch_big.cu'
  id: totrans-2962
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微型演示：tex1dfetch_big.cu
- en: This program illustrates using `tex1Dfetch()` to read from large arrays using
    both multiple components per texture and multiple textures. It is invoked as follows.
  id: totrans-2963
  prefs: []
  type: TYPE_NORMAL
  zh: 本程序演示了如何使用`tex1Dfetch()`从大型数组中读取数据，同时使用每个纹理多个组件和多个纹理。其调用方式如下。
- en: tex1dfetch_big <NumMegabytes>
  id: totrans-2964
  prefs: []
  type: TYPE_NORMAL
  zh: tex1dfetch_big <NumMegabytes>
- en: The application allocates the specified number of megabytes of device memory
    (or mapped pinned host memory, if the device memory allocation fails), fills the
    memory with random numbers, and uses 1-, 2-, and 4-component textures to compute
    checksums on the data. Up to four textures of `int4` can be used, enabling the
    application to texture from up to 8192M of memory.
  id: totrans-2965
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序分配指定数量的设备内存（如果设备内存分配失败，则分配映射的固定主机内存），将内存填充随机数，并使用1、2和4分量纹理来计算数据的校验和。最多可以使用四个`int4`类型的纹理，从而使应用程序能够从多达8192M的内存中提取纹理。
- en: For clarity, `tex1dfetch_big.cu` does not perform any fancy parallel reduction
    techniques. Each thread writes back an intermediate sum, and the final checksums
    are accumulated on the CPU. The application defines the 27-bit hardware limits.
  id: totrans-2966
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，`tex1dfetch_big.cu`没有执行任何复杂的并行归约技术。每个线程都会写回一个中间和，最终的校验和在CPU上累加。该应用程序定义了27位的硬件限制。
- en: '[Click here to view code image](ch10_images.html#p319pro01a)'
  id: totrans-2967
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p319pro01a)'
- en: '#define CUDA_LG_MAX_TEX1DFETCH_INDEX 27'
  id: totrans-2968
  prefs: []
  type: TYPE_NORMAL
  zh: '#define CUDA_LG_MAX_TEX1DFETCH_INDEX 27'
- en: '#define CUDA_MAX_TEX1DFETCH_INDEX'
  id: totrans-2969
  prefs: []
  type: TYPE_NORMAL
  zh: '#define CUDA_MAX_TEX1DFETCH_INDEX'
- en: (((size_t)1<<CUDA_LG_MAX_TEX1DFETCH_INDEX)-1)
  id: totrans-2970
  prefs: []
  type: TYPE_NORMAL
  zh: (((size_t)1<<CUDA_LG_MAX_TEX1DFETCH_INDEX)-1)
- en: And it defines four textures of `int4.`
  id: totrans-2971
  prefs: []
  type: TYPE_NORMAL
  zh: 它定义了四个`int4`类型的纹理。
- en: '[Click here to view code image](ch10_images.html#p319pro02a)'
  id: totrans-2972
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p319pro02a)'
- en: texture<int4, 1, cudaReadModeElementType> tex4_0;
  id: totrans-2973
  prefs: []
  type: TYPE_NORMAL
  zh: texture<int4, 1, cudaReadModeElementType> tex4_0;
- en: texture<int4, 1, cudaReadModeElementType> tex4_1;
  id: totrans-2974
  prefs: []
  type: TYPE_NORMAL
  zh: texture<int4, 1, cudaReadModeElementType> tex4_1;
- en: texture<int4, 1, cudaReadModeElementType> tex4_2;
  id: totrans-2975
  prefs: []
  type: TYPE_NORMAL
  zh: texture<int4, 1, cudaReadModeElementType> tex4_2;
- en: texture<int4, 1, cudaReadModeElementType> tex4_3;
  id: totrans-2976
  prefs: []
  type: TYPE_NORMAL
  zh: texture<int4, 1, cudaReadModeElementType> tex4_3;
- en: A device function `tex4Fetch()` takes an index and teases it apart into a texture
    ordinal and a 27-bit index to pass to `tex1Dfetch()`.
  id: totrans-2977
  prefs: []
  type: TYPE_NORMAL
  zh: 一个设备函数`tex4Fetch()`接受一个索引，并将其分解为一个纹理序号和一个27位的索引，传递给`tex1Dfetch()`。
- en: '[Click here to view code image](ch10_images.html#p319pro03a)'
  id: totrans-2978
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p319pro03a)'
- en: __device__ int4
  id: totrans-2979
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ int4
- en: tex4Fetch( size_t index )
  id: totrans-2980
  prefs: []
  type: TYPE_NORMAL
  zh: tex4Fetch( size_t index )
- en: '{'
  id: totrans-2981
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int texID = (int) (index>>CUDA_LG_MAX_TEX1DFETCH_INDEX);
  id: totrans-2982
  prefs: []
  type: TYPE_NORMAL
  zh: int texID = (int) (index>>CUDA_LG_MAX_TEX1DFETCH_INDEX);
- en: int i = (int) (index & (CUDA_MAX_TEX1DFETCH_INDEX_SIZE_T-1));
  id: totrans-2983
  prefs: []
  type: TYPE_NORMAL
  zh: int i = (int) (index & (CUDA_MAX_TEX1DFETCH_INDEX_SIZE_T-1));
- en: int4 i4;
  id: totrans-2984
  prefs: []
  type: TYPE_NORMAL
  zh: int4 i4;
- en: if ( texID == 0 ) {
  id: totrans-2985
  prefs: []
  type: TYPE_NORMAL
  zh: if ( texID == 0 ) {
- en: i4 = tex1Dfetch( tex4_0, i );
  id: totrans-2986
  prefs: []
  type: TYPE_NORMAL
  zh: i4 = tex1Dfetch( tex4_0, i );
- en: '}'
  id: totrans-2987
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else if ( texID == 1 ) {
  id: totrans-2988
  prefs: []
  type: TYPE_NORMAL
  zh: else if ( texID == 1 ) {
- en: i4 = tex1Dfetch( tex4_1, i );
  id: totrans-2989
  prefs: []
  type: TYPE_NORMAL
  zh: i4 = tex1Dfetch( tex4_1, i );
- en: '}'
  id: totrans-2990
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else if ( texID == 2 ) {
  id: totrans-2991
  prefs: []
  type: TYPE_NORMAL
  zh: else if ( texID == 2 ) {
- en: i4 = tex1Dfetch( tex4_2, i );
  id: totrans-2992
  prefs: []
  type: TYPE_NORMAL
  zh: i4 = tex1Dfetch( tex4_2, i );
- en: '}'
  id: totrans-2993
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else if ( texID == 3 ) {
  id: totrans-2994
  prefs: []
  type: TYPE_NORMAL
  zh: else if ( texID == 3 ) {
- en: i4 = tex1Dfetch( tex4_3, i );
  id: totrans-2995
  prefs: []
  type: TYPE_NORMAL
  zh: i4 = tex1Dfetch( tex4_3, i );
- en: '}'
  id: totrans-2996
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return i4;
  id: totrans-2997
  prefs: []
  type: TYPE_NORMAL
  zh: return i4;
- en: '}'
  id: totrans-2998
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: This device function compiles to a small amount of code that uses four predicated
    TEX instructions, only one of which is “live.” If random access is desired, the
    application also can use predication to select from the `.x`, `.y`, `.z`, or `.w`
    component of the `int4` return value.
  id: totrans-2999
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设备函数编译成一小段代码，使用四条带有条件语句的TEX指令，其中只有一条是“有效”的。如果需要随机访问，应用程序还可以使用条件选择来从`int4`返回值的`.x`、`.y`、`.z`或`.w`分量中选择。
- en: Binding the textures, shown in [Listing 10.2](ch10.html#ch10lis02), is a slightly
    tricky business. This code creates two small arrays `texSizes[]` and `texBases[]`
    and sets them up to cover the device memory range. The `for` loop ensures that
    all four textures have a valid binding, even if fewer than four are needed to
    map the device memory.
  id: totrans-3000
  prefs: []
  type: TYPE_NORMAL
  zh: 绑定纹理，如[第10.2节](ch10.html#ch10lis02)所示，是一项稍微复杂的操作。此代码创建了两个小数组`texSizes[]`和`texBases[]`，并将它们设置为覆盖设备内存范围。`for`循环确保所有四个纹理都有有效的绑定，即使只需要少于四个纹理来映射设备内存。
- en: '*Listing 10.2.* `tex1dfetch_big.cu` (excerpt).'
  id: totrans-3001
  prefs: []
  type: TYPE_NORMAL
  zh: '*第10.2节* `tex1dfetch_big.cu`（摘录）。'
- en: '[Click here to view code image](ch10_images.html#p10lis02a)'
  id: totrans-3002
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p10lis02a)'
- en: '* * *'
  id: totrans-3003
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: int iTexture;
  id: totrans-3004
  prefs: []
  type: TYPE_NORMAL
  zh: int iTexture;
- en: cudaChannelFormatDesc int4Desc = cudaCreateChannelDesc<int4>();
  id: totrans-3005
  prefs: []
  type: TYPE_NORMAL
  zh: cudaChannelFormatDesc int4Desc = cudaCreateChannelDesc<int4>();
- en: size_t numInt4s = numBytes / sizeof(int4);
  id: totrans-3006
  prefs: []
  type: TYPE_NORMAL
  zh: size_t numInt4s = numBytes / sizeof(int4);
- en: int numTextures = (numInt4s+CUDA_MAX_TEX1DFETCH_INDEX)>>
  id: totrans-3007
  prefs: []
  type: TYPE_NORMAL
  zh: 'int numTextures = (numInt4s+CUDA_MAX_TEX1DFETCH_INDEX)>> '
- en: CUDA_LG_MAX_TEX1DFETCH_INDEX;
  id: totrans-3008
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_LG_MAX_TEX1DFETCH_INDEX;
- en: size_t Remainder = numBytes & (CUDA_MAX_BYTES_INT4-1);
  id: totrans-3009
  prefs: []
  type: TYPE_NORMAL
  zh: size_t Remainder = numBytes & (CUDA_MAX_BYTES_INT4-1);
- en: if ( ! Remainder ) {
  id: totrans-3010
  prefs: []
  type: TYPE_NORMAL
  zh: if ( ! Remainder ) {
- en: Remainder = CUDA_MAX_BYTES_INT4;
  id: totrans-3011
  prefs: []
  type: TYPE_NORMAL
  zh: Remainder = CUDA_MAX_BYTES_INT4;
- en: '}'
  id: totrans-3012
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: size_t texSizes[4];
  id: totrans-3013
  prefs: []
  type: TYPE_NORMAL
  zh: size_t texSizes[4];
- en: char *texBases[4];
  id: totrans-3014
  prefs: []
  type: TYPE_NORMAL
  zh: char *texBases[4];
- en: for ( iTexture = 0; iTexture < numTextures; iTexture++ ) {
  id: totrans-3015
  prefs: []
  type: TYPE_NORMAL
  zh: for ( iTexture = 0; iTexture < numTextures; iTexture++ ) {
- en: texBases[iTexture] = deviceTex+iTexture*CUDA_MAX_BYTES_INT4;
  id: totrans-3016
  prefs: []
  type: TYPE_NORMAL
  zh: texBases[iTexture] = deviceTex+iTexture*CUDA_MAX_BYTES_INT4;
- en: texSizes[iTexture] = CUDA_MAX_BYTES_INT4;
  id: totrans-3017
  prefs: []
  type: TYPE_NORMAL
  zh: texSizes[iTexture] = CUDA_MAX_BYTES_INT4;
- en: '}'
  id: totrans-3018
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: texSizes[iTexture-1] = Remainder;
  id: totrans-3019
  prefs: []
  type: TYPE_NORMAL
  zh: texSizes[iTexture-1] = Remainder;
- en: while ( iTexture < 4 ) {
  id: totrans-3020
  prefs: []
  type: TYPE_NORMAL
  zh: while ( iTexture < 4 ) {
- en: texBases[iTexture] = texBases[iTexture-1];
  id: totrans-3021
  prefs: []
  type: TYPE_NORMAL
  zh: texBases[iTexture] = texBases[iTexture-1];
- en: texSizes[iTexture] = texSizes[iTexture-1];
  id: totrans-3022
  prefs: []
  type: TYPE_NORMAL
  zh: texSizes[iTexture] = texSizes[iTexture-1];
- en: iTexture++;
  id: totrans-3023
  prefs: []
  type: TYPE_NORMAL
  zh: iTexture++;
- en: '}'
  id: totrans-3024
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaBindTexture( NULL, tex4_0, texBases[0], int4Desc, texSizes[0] );
  id: totrans-3025
  prefs: []
  type: TYPE_NORMAL
  zh: cudaBindTexture( NULL, tex4_0, texBases[0], int4Desc, texSizes[0] );
- en: cudaBindTexture( NULL, tex4_1, texBases[1], int4Desc, texSizes[1] );
  id: totrans-3026
  prefs: []
  type: TYPE_NORMAL
  zh: cudaBindTexture( NULL, tex4_1, texBases[1], int4Desc, texSizes[1] );
- en: cudaBindTexture( NULL, tex4_2, texBases[2], int4Desc, texSizes[2] );
  id: totrans-3027
  prefs: []
  type: TYPE_NORMAL
  zh: cudaBindTexture( NULL, tex4_2, texBases[2], int4Desc, texSizes[2] );
- en: cudaBindTexture( NULL, tex4_3, texBases[3], int4Desc, texSizes[3] );
  id: totrans-3028
  prefs: []
  type: TYPE_NORMAL
  zh: cudaBindTexture( NULL, tex4_3, texBases[3], int4Desc, texSizes[3] );
- en: '* * *'
  id: totrans-3029
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Once compiled and run, the application can be invoked with different sizes to
    see the effects. On a CG1 instance running in Amazon’s EC2 cloud compute offering,
    invocations with 512M, 768M, 1280M, and 8192M worked as follows.
  id: totrans-3030
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行后，可以使用不同的大小来调用该应用程序以查看效果。在Amazon EC2云计算实例CG1上，使用512M、768M、1280M和8192M的调用效果如下。
- en: '[Click here to view code image](ch10_images.html#p321pro01a)'
  id: totrans-3031
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p321pro01a)'
- en: $ ./tex1dfetch_big 512
  id: totrans-3032
  prefs: []
  type: TYPE_NORMAL
  zh: $ ./tex1dfetch_big 512
- en: 'Expected checksum: 0x7b7c8cd3'
  id: totrans-3033
  prefs: []
  type: TYPE_NORMAL
  zh: '预期校验和: 0x7b7c8cd3'
- en: 'tex1 checksum: 0x7b7c8cd3'
  id: totrans-3034
  prefs: []
  type: TYPE_NORMAL
  zh: 'tex1 校验和: 0x7b7c8cd3'
- en: 'tex2 checksum: 0x7b7c8cd3'
  id: totrans-3035
  prefs: []
  type: TYPE_NORMAL
  zh: 'tex2 校验和: 0x7b7c8cd3'
- en: 'tex4 checksum: 0x7b7c8cd3'
  id: totrans-3036
  prefs: []
  type: TYPE_NORMAL
  zh: 'tex4 校验和: 0x7b7c8cd3'
- en: $ ./tex1dfetch_big 768
  id: totrans-3037
  prefs: []
  type: TYPE_NORMAL
  zh: $ ./tex1dfetch_big 768
- en: 'Expected checksum: 0x559a1431'
  id: totrans-3038
  prefs: []
  type: TYPE_NORMAL
  zh: '预期校验和: 0x559a1431'
- en: 'tex1 checksum: (not performed)'
  id: totrans-3039
  prefs: []
  type: TYPE_NORMAL
  zh: 'tex1 校验和: (未执行)'
- en: 'tex2 checksum: 0x559a1431'
  id: totrans-3040
  prefs: []
  type: TYPE_NORMAL
  zh: 'tex2 校验和: 0x559a1431'
- en: 'tex4 checksum: 0x559a1431'
  id: totrans-3041
  prefs: []
  type: TYPE_NORMAL
  zh: 'tex4 校验和: 0x559a1431'
- en: $ ./tex1dfetch_big 1280
  id: totrans-3042
  prefs: []
  type: TYPE_NORMAL
  zh: $ ./tex1dfetch_big 1280
- en: 'Expected checksum: 0x66a4f9d9'
  id: totrans-3043
  prefs: []
  type: TYPE_NORMAL
  zh: 预期校验和：0x66a4f9d9
- en: 'tex1 checksum: (not performed)'
  id: totrans-3044
  prefs: []
  type: TYPE_NORMAL
  zh: tex1校验和：（未执行）
- en: 'tex2 checksum: (not performed)'
  id: totrans-3045
  prefs: []
  type: TYPE_NORMAL
  zh: tex2校验和：（未执行）
- en: 'tex4 checksum: 0x66a4f9d9'
  id: totrans-3046
  prefs: []
  type: TYPE_NORMAL
  zh: tex4校验和：0x66a4f9d9
- en: $ ./tex1dfetch_big 8192
  id: totrans-3047
  prefs: []
  type: TYPE_NORMAL
  zh: $ ./tex1dfetch_big 8192
- en: Device alloc of 8192 Mb failed, trying mapped host memory
  id: totrans-3048
  prefs: []
  type: TYPE_NORMAL
  zh: 8192 Mb的设备内存分配失败，尝试使用映射的主机内存
- en: 'Expected checksum: 0xf049c607'
  id: totrans-3049
  prefs: []
  type: TYPE_NORMAL
  zh: 预期校验和：0xf049c607
- en: 'tex1 checksum: (not performed)'
  id: totrans-3050
  prefs: []
  type: TYPE_NORMAL
  zh: tex1校验和：（未执行）
- en: 'tex2 checksum: (not performed)'
  id: totrans-3051
  prefs: []
  type: TYPE_NORMAL
  zh: tex2校验和：（未执行）
- en: 'tex4 checksum: 0xf049c607'
  id: totrans-3052
  prefs: []
  type: TYPE_NORMAL
  zh: tex4校验和：0xf049c607
- en: Each `int4` texture can “only” read 2G, so invoking the program with numbers
    greater than 8192 causes it to fail. This application highlights the demand for
    indexed textures, where the texture being fetched can be specified as a parameter
    at runtime, but CUDA does not expose support for this feature.
  id: totrans-3053
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`int4`纹理只能读取2G的数据，因此调用程序时，如果数字大于8192，程序会失败。这个应用程序突出了索引纹理的需求，其中要获取的纹理可以在运行时作为参数指定，但CUDA并没有公开对该功能的支持。
- en: 10.4.2\. Texturing from Host Memory
  id: totrans-3054
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.4.2\. 从主机内存进行纹理映射
- en: Using texture as a read path, applications can read from host memory by allocating
    mapped pinned memory, fetching the device pointer, and then specifying that device
    pointer to `cudaBindAddress()` or `cuTexRefSetAddress()`. The capability is there,
    but reading host memory via texture is *slow*. Tesla-class hardware can texture
    over PCI Express at about 2G/s, and Fermi hardware is much slower. You need some
    other reason to do it, such as code simplicity.
  id: totrans-3055
  prefs: []
  type: TYPE_NORMAL
  zh: 使用纹理作为读取路径，应用程序可以通过分配映射的固定内存、获取设备指针，然后将该设备指针传递给`cudaBindAddress()`或`cuTexRefSetAddress()`来从主机内存读取数据。这个功能是存在的，但通过纹理读取主机内存是*缓慢*的。Tesla级硬件通过PCI
    Express纹理传输的速度约为2G/s，而Fermi硬件则慢得多。你需要其他原因来使用它，比如代码的简化。
- en: 'Microdemo: tex1dfetch_int2float.cu'
  id: totrans-3056
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微型示例：tex1dfetch_int2float.cu
- en: This code fragment uses texture-as-a-read path and texturing from host memory
    to confirm that the `TexPromoteToFloat()` functions work properly. The CUDA kernel
    that we will use for this purpose is a straightforward, blocking-agnostic implementation
    of a memcpy function that reads from the texture and writes to device memory.
  id: totrans-3057
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码片段使用了纹理作为读取路径，并从主机内存进行纹理映射，以确认`TexPromoteToFloat()`函数是否正常工作。我们将用来执行此操作的CUDA内核是一个直接的、与阻塞无关的memcpy函数实现，它从纹理读取数据并写入设备内存。
- en: '[Click here to view code image](ch10_images.html#p321pro02a)'
  id: totrans-3058
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p321pro02a)'
- en: texture<signed char, 1, cudaReadModeNormalizedFloat> tex;
  id: totrans-3059
  prefs: []
  type: TYPE_NORMAL
  zh: texture<signed char, 1, cudaReadModeNormalizedFloat> tex;
- en: extern "C" __global__ void
  id: totrans-3060
  prefs: []
  type: TYPE_NORMAL
  zh: extern "C" __global__ void
- en: TexReadout( float *out, size_t N )
  id: totrans-3061
  prefs: []
  type: TYPE_NORMAL
  zh: TexReadout( float *out, size_t N )
- en: '{'
  id: totrans-3062
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3063
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
- en: i < N;
  id: totrans-3064
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += gridDim.x*blockDim.x )
  id: totrans-3065
  prefs: []
  type: TYPE_NORMAL
  zh: i += gridDim.x*blockDim.x )
- en: '{'
  id: totrans-3066
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: out[i] = tex1Dfetch( tex, i );
  id: totrans-3067
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] = tex1Dfetch( tex, i );
- en: '}'
  id: totrans-3068
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3069
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Since promoting integers to floating point only works on 8- and 16-bit values,
    we can test every possible conversion by allocating a small buffer, texturing
    from it, and confirming that the output meets our expectations. [Listing 10.3](ch10.html#ch10lis03)
    gives an excerpt from `tex1dfetch_int2float.cu`. Two host buffers are allocated:
    `inHost` holds the input buffer of 256 or 65536 input values, and `fOutHost` holds
    the corresponding float-valued outputs. The device pointers corresponding to these
    mapped host pointers are fetched into `inDevice` and `foutDevice`.'
  id: totrans-3070
  prefs: []
  type: TYPE_NORMAL
  zh: 由于将整数提升为浮点数仅适用于8位和16位值，因此我们可以通过分配一个小缓冲区，从中纹理化并确认输出符合预期，来测试所有可能的转换。[示例 10.3](ch10.html#ch10lis03)展示了`tex1dfetch_int2float.cu`的一个片段。分配了两个主机缓冲区：`inHost`保存256个或65536个输入值的输入缓冲区，`fOutHost`保存相应的浮点数输出。与这些映射主机指针对应的设备指针被获取到`inDevice`和`foutDevice`中。
- en: The input values are initialized to every possible value of the type to be tested,
    and then the input device pointer is bound to the texture reference using `cudaBindTexture()`.
    The `TexReadout()` kernel is then invoked to read each value from the input texture
    and write as output the values returned by `tex1Dfetch()`. In this case, both
    the input and output buffers reside in mapped host memory. Because the kernel
    is writing directly to host memory, we must call `cudaDeviceSynchronize()` to
    make sure there are no race conditions between the CPU and GPU. At the end of
    the function, we call the `TexPromoteToFloat()` specialization corresponding to
    the type being tested and confirm that it is equal to the value returned by the
    kernel. If all tests pass, the function returns `true`; if any API functions or
    comparisons fail, it returns `false`.
  id: totrans-3071
  prefs: []
  type: TYPE_NORMAL
  zh: 输入值被初始化为要测试的类型的所有可能值，然后输入设备指针通过`cudaBindTexture()`与纹理引用绑定。接着，调用`TexReadout()`内核从输入纹理中读取每个值，并将`tex1Dfetch()`返回的值作为输出写入。在此情况下，输入和输出缓冲区都位于映射的主机内存中。因为内核直接写入主机内存，我们必须调用`cudaDeviceSynchronize()`以确保CPU与GPU之间没有竞争条件。在函数结束时，我们调用与被测试类型相对应的`TexPromoteToFloat()`专用版本，并确认其值与内核返回的值相等。如果所有测试通过，函数返回`true`；如果任何API函数或比较失败，返回`false`。
- en: '*Listing 10.3.* `tex1d_int2float.cu` (excerpt).'
  id: totrans-3072
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 10.3.* `tex1d_int2float.cu`（片段）。'
- en: '[Click here to view code image](ch10_images.html#p10lis03a)'
  id: totrans-3073
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p10lis03a)'
- en: '* * *'
  id: totrans-3074
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-3075
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: void
  id: totrans-3076
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: CheckTexPromoteToFloat( size_t N )
  id: totrans-3077
  prefs: []
  type: TYPE_NORMAL
  zh: CheckTexPromoteToFloat( size_t N )
- en: '{'
  id: totrans-3078
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: T *inHost, *inDevice;
  id: totrans-3079
  prefs: []
  type: TYPE_NORMAL
  zh: T *inHost, *inDevice;
- en: float *foutHost, *foutDevice;
  id: totrans-3080
  prefs: []
  type: TYPE_NORMAL
  zh: float *foutHost, *foutDevice;
- en: cudaError_t status;
  id: totrans-3081
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: CUDART_CHECK(cudaHostAlloc( (void **) &inHost,
  id: totrans-3082
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaHostAlloc( (void **) &inHost,
- en: N*sizeof(T),
  id: totrans-3083
  prefs: []
  type: TYPE_NORMAL
  zh: N*sizeof(T),
- en: cudaHostAllocMapped));
  id: totrans-3084
  prefs: []
  type: TYPE_NORMAL
  zh: cudaHostAllocMapped));
- en: CUDART_CHECK(cudaHostGetDevicePointer( (void **) &inDevice,
  id: totrans-3085
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaHostGetDevicePointer( (void **) &inDevice,
- en: inHost,
  id: totrans-3086
  prefs: []
  type: TYPE_NORMAL
  zh: inHost,
- en: 0 ));
  id: totrans-3087
  prefs: []
  type: TYPE_NORMAL
  zh: 0 ));
- en: CUDART_CHECK(cudaHostAlloc( (void **) &foutHost,
  id: totrans-3088
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaHostAlloc( (void **) &foutHost,
- en: N*sizeof(float),
  id: totrans-3089
  prefs: []
  type: TYPE_NORMAL
  zh: N*sizeof(float),
- en: cudaHostAllocMapped));
  id: totrans-3090
  prefs: []
  type: TYPE_NORMAL
  zh: cudaHostAllocMapped));
- en: CUDART_CHECK(cudaHostGetDevicePointer( (void **) &foutDevice,
  id: totrans-3091
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaHostGetDevicePointer( (void **) &foutDevice,
- en: foutHost,
  id: totrans-3092
  prefs: []
  type: TYPE_NORMAL
  zh: foutHost,
- en: 0 ));
  id: totrans-3093
  prefs: []
  type: TYPE_NORMAL
  zh: 0 ));
- en: for ( int i = 0; i < N; i++ ) {
  id: totrans-3094
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < N; i++ ) {
- en: inHost[i] = (T) i;
  id: totrans-3095
  prefs: []
  type: TYPE_NORMAL
  zh: inHost[i] = (T) i;
- en: '}'
  id: totrans-3096
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: memset( foutHost, 0, N*sizeof(float) );
  id: totrans-3097
  prefs: []
  type: TYPE_NORMAL
  zh: memset( foutHost, 0, N*sizeof(float) );
- en: CUDART_CHECK( cudaBindTexture( NULL,
  id: totrans-3098
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaBindTexture( NULL,
- en: tex,
  id: totrans-3099
  prefs: []
  type: TYPE_NORMAL
  zh: tex,
- en: inDevice,
  id: totrans-3100
  prefs: []
  type: TYPE_NORMAL
  zh: inDevice,
- en: cudaCreateChannelDesc<T>(),
  id: totrans-3101
  prefs: []
  type: TYPE_NORMAL
  zh: cudaCreateChannelDesc<T>(),
- en: N*sizeof(T)));
  id: totrans-3102
  prefs: []
  type: TYPE_NORMAL
  zh: N*sizeof(T)));
- en: TexReadout<<<2,384>>>( foutDevice, N );
  id: totrans-3103
  prefs: []
  type: TYPE_NORMAL
  zh: TexReadout<<<2,384>>>( foutDevice, N );
- en: CUDART_CHECK(cudaDeviceSynchronize());
  id: totrans-3104
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaDeviceSynchronize());
- en: for ( int i = 0; i < N; i++ ) {
  id: totrans-3105
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < N; i++ ) {
- en: printf( "%.2f ", foutHost[i] );
  id: totrans-3106
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%.2f ", foutHost[i] );
- en: assert( foutHost[i] == TexPromoteToFloat( (T) i ) );
  id: totrans-3107
  prefs: []
  type: TYPE_NORMAL
  zh: assert( foutHost[i] == TexPromoteToFloat( (T) i ) );
- en: '}'
  id: totrans-3108
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: printf( "\n" );
  id: totrans-3109
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "\n" );
- en: 'Error:'
  id: totrans-3110
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: cudaFreeHost( inHost );
  id: totrans-3111
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFreeHost( inHost );
- en: cudaFreeHost( foutHost );
  id: totrans-3112
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFreeHost( foutHost );
- en: '}'
  id: totrans-3113
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-3114
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 10.5\. Texturing with Unnormalized Coordinates
  id: totrans-3115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.5\. 使用未归一化坐标的纹理映射
- en: All texture intrinsics except `tex1Dfetch()` use floating-point values to specify
    coordinates into the texture. When using *unnormalized coordinates*, they fall
    in the range [0, *MaxDim*), where *MaxDim* is the width, height, or depth of the
    texture. Unnormalized coordinates are an intuitive way to index into a texture,
    but some texturing features are not available when using them.
  id: totrans-3116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `tex1Dfetch()` 之外，所有纹理内建函数都使用浮点值来指定纹理中的坐标。当使用*未归一化坐标*时，它们的范围在 [0, *MaxDim*)
    之间，其中 *MaxDim* 是纹理的宽度、高度或深度。未归一化坐标是一种直观的纹理索引方式，但使用时某些纹理特性不可用。
- en: An easy way to study texturing behavior is to populate a texture with elements
    that contain the index into the texture. [Figure 10.4](ch10.html#ch10fig04) shows
    a float-valued 1D texture with 16 elements, populated by the identity elements
    and annotated with some of the values returned by `tex1D()`.
  id: totrans-3117
  prefs: []
  type: TYPE_NORMAL
  zh: 学习纹理行为的一个简单方法是用包含纹理索引的元素填充纹理。[图 10.4](ch10.html#ch10fig04)展示了一个包含 16 个元素的浮点值
    1D 纹理，这些元素由标识元素填充，并附有 `tex1D()` 返回的部分值的注释。
- en: '![Image](graphics/10fig04.jpg)'
  id: totrans-3118
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/10fig04.jpg)'
- en: '*Figure 10.4* Texturing with unnormalized coordinates (without linear filtering).'
  id: totrans-3119
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.4* 使用未归一化坐标的纹理映射（无线性过滤）。'
- en: 'Not all texturing features are available with unnormalized coordinates, but
    they can be used in conjunction with *linear filtering* and a limited form of
    *texture addressing*. The texture addressing mode specifies how the hardware should
    deal with out-of-range texture coordinates. For unnormalized coordinates, the
    [Figure 10.4](ch10.html#ch10fig04) illustrates the default texture addressing
    mode of clamping to the range [0, *MaxDim*) before fetching data from the texture:
    The value 16.0 is out of range and clamped to fetch the value 15.0\. Another texture
    addressing option available when using unnormalized coordinates is the “border”
    addressing mode where out-of-range coordinates return zero.'
  id: totrans-3120
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有纹理特性都可以在未归一化坐标下使用，但它们可以与*线性过滤*以及一种有限的*纹理寻址*方式一起使用。纹理寻址模式指定硬件应如何处理超出范围的纹理坐标。对于未归一化坐标，[图
    10.4](ch10.html#ch10fig04)说明了默认的纹理寻址模式，即在从纹理中获取数据之前将坐标限制在范围[0, *MaxDim*)内：值16.0超出范围并被限制为获取值15.0。另一种在使用未归一化坐标时可用的纹理寻址选项是“边界”寻址模式，其中超出范围的坐标返回零。
- en: The default filtering mode, so-called “point filtering,” returns one texture
    element depending on the value of the floating-point coordinate. In contrast,
    linear filtering causes the texture hardware to fetch the two neighboring texture
    elements and linearly interpolate between them, weighted by the texture coordinate.
    [Figure 10.5](ch10.html#ch10fig05) shows the 1D texture with 16 elements, with
    some sample values returned by `tex1D()`. Note that you must add `0.5f` to the
    texture coordinate to get the identity element.
  id: totrans-3121
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的过滤模式，所谓的“点过滤”，根据浮点坐标的值返回一个纹理元素。相比之下，线性过滤会导致纹理硬件获取两个相邻的纹理元素，并在它们之间进行线性插值，插值权重由纹理坐标决定。[图
    10.5](ch10.html#ch10fig05)显示了包含16个元素的1D纹理，展示了由`tex1D()`返回的一些示例值。请注意，必须向纹理坐标加上`0.5f`才能获取身份元素。
- en: '![Image](graphics/10fig05.jpg)'
  id: totrans-3122
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/10fig05.jpg)'
- en: '*Figure 10.5* Texturing with unnormalized coordinates (with linear filtering).'
  id: totrans-3123
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.5* 使用未归一化坐标的纹理（带线性过滤）。'
- en: Many texturing features can be used in conjunction with one another; for example,
    linear filtering can be combined with the previously discussed promotion from
    integer to floating point. In that case, the floating-point outputs produced by
    `tex1D()` intrinsics are accurate interpolations between the promoted floating-point
    values of the two participating texture elements.
  id: totrans-3124
  prefs: []
  type: TYPE_NORMAL
  zh: 许多纹理特性可以互相结合使用；例如，线性过滤可以与之前讨论的从整数到浮点数的提升结合使用。在这种情况下，`tex1D()`内建函数产生的浮点输出是参与的两个纹理元素的浮点值之间的准确插值。
- en: '*Microdemo:* `tex1d_unnormalized.cu`'
  id: totrans-3125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*微型示例：* `tex1d_unnormalized.cu`'
- en: The microdemo `tex1d_unnormalized.cu` is like a microscope to closely examine
    texturing behavior by printing the coordinate and the value returned by the `tex1D()`
    intrinsic together. Unlike the `tex1dfetch_int2float.cu` microdemo, this program
    uses a 1D CUDA array to hold the texture data. A certain number of texture fetches
    is performed, along a range of floating-point values specified by a base and increment;
    the interpolated values and the value returned by `tex1D()` are written together
    into an output array of `float2`. The CUDA kernel is as follows.
  id: totrans-3126
  prefs: []
  type: TYPE_NORMAL
  zh: 微型演示程序`tex1d_unnormalized.cu`就像一台显微镜，通过同时打印`tex1D()`内建函数返回的坐标和值，仔细检查纹理行为。与`tex1dfetch_int2float.cu`微型演示程序不同，该程序使用一个
    1D CUDA 数组来保存纹理数据。在指定的浮点值范围内，执行一定次数的纹理获取操作，由一个基值和增量来指定该范围；插值值和`tex1D()`返回的值一起写入`float2`类型的输出数组。CUDA内核如下所示。
- en: '[Click here to view code image](ch10_images.html#p325pro01a)'
  id: totrans-3127
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p325pro01a)'
- en: texture<float, 1> tex;
  id: totrans-3128
  prefs: []
  type: TYPE_NORMAL
  zh: texture<float, 1> tex;
- en: extern "C" __global__ void
  id: totrans-3129
  prefs: []
  type: TYPE_NORMAL
  zh: extern "C" __global__ void
- en: TexReadout( float2 *out, size_t N, float base, float increment )
  id: totrans-3130
  prefs: []
  type: TYPE_NORMAL
  zh: TexReadout( float2 *out, size_t N, float base, float increment )
- en: '{'
  id: totrans-3131
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3132
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
- en: i < N;
  id: totrans-3133
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += gridDim.x*blockDim.x )
  id: totrans-3134
  prefs: []
  type: TYPE_NORMAL
  zh: i += gridDim.x*blockDim.x )
- en: '{'
  id: totrans-3135
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float x = base + (float) i * increment;
  id: totrans-3136
  prefs: []
  type: TYPE_NORMAL
  zh: float x = base + (float) i * increment;
- en: out[i].x = x;
  id: totrans-3137
  prefs: []
  type: TYPE_NORMAL
  zh: out[i].x = x;
- en: out[i].y = tex1D( tex, x );
  id: totrans-3138
  prefs: []
  type: TYPE_NORMAL
  zh: out[i].y = tex1D( tex, x );
- en: '}'
  id: totrans-3139
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3140
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: A host function `CreateAndPrintTex()`, given in [Listing 10.4](ch10.html#ch10lis04),
    takes the size of the texture to create, the number of texture fetches to perform,
    the base and increment of the floating-point range to pass to `tex1D()`, and optionally
    the filter and addressing modes to use on the texture. This function creates the
    CUDA array to hold the texture data, optionally initializes it with the caller-provided
    data (or identity elements if the caller passes NULL), binds the texture to the
    CUDA array, and prints the `float2` output.
  id: totrans-3141
  prefs: []
  type: TYPE_NORMAL
  zh: 主机函数`CreateAndPrintTex()`（在[列表 10.4](ch10.html#ch10lis04)中给出）接受要创建的纹理大小、执行纹理获取的次数、传递给`tex1D()`的浮点范围的基值和增量，以及可选的过滤模式和地址模式。该函数创建用于存储纹理数据的
    CUDA 数组，选择性地用调用者提供的数据初始化它（如果调用者传递 NULL，则使用单位元素），将纹理绑定到 CUDA 数组，并打印`float2`输出。
- en: '*Listing 10.4.* `CreateAndPrintTex().`'
  id: totrans-3142
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 10.4.* `CreateAndPrintTex().`'
- en: '[Click here to view code image](ch10_images.html#p10lis04a)'
  id: totrans-3143
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p10lis04a)'
- en: '* * *'
  id: totrans-3144
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-3145
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: void
  id: totrans-3146
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: CreateAndPrintTex( T *initTex, size_t texN, size_t outN,
  id: totrans-3147
  prefs: []
  type: TYPE_NORMAL
  zh: CreateAndPrintTex( T *initTex, size_t texN, size_t outN,
- en: float base, float increment,
  id: totrans-3148
  prefs: []
  type: TYPE_NORMAL
  zh: float base, float increment,
- en: cudaTextureFilterMode filterMode = cudaFilterModePoint,
  id: totrans-3149
  prefs: []
  type: TYPE_NORMAL
  zh: cudaTextureFilterMode filterMode = cudaFilterModePoint,
- en: cudaTextureAddressMode addressMode = cudaAddressModeClamp )
  id: totrans-3150
  prefs: []
  type: TYPE_NORMAL
  zh: cudaTextureAddressMode addressMode = cudaAddressModeClamp )
- en: '{'
  id: totrans-3151
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: T *texContents = 0;
  id: totrans-3152
  prefs: []
  type: TYPE_NORMAL
  zh: T *texContents = 0;
- en: cudaArray *texArray = 0;
  id: totrans-3153
  prefs: []
  type: TYPE_NORMAL
  zh: cudaArray *texArray = 0;
- en: float2 *outHost = 0, *outDevice = 0;
  id: totrans-3154
  prefs: []
  type: TYPE_NORMAL
  zh: float2 *outHost = 0, *outDevice = 0;
- en: cudaError_t status;
  id: totrans-3155
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc<T>();
  id: totrans-3156
  prefs: []
  type: TYPE_NORMAL
  zh: cudaChannelFormatDesc channelDesc = cudaCreateChannelDesc<T>();
- en: // use caller-provided array, if any, to initialize texture
  id: totrans-3157
  prefs: []
  type: TYPE_NORMAL
  zh: // 使用调用者提供的数组（如果有的话）初始化纹理
- en: if ( initTex ) {
  id: totrans-3158
  prefs: []
  type: TYPE_NORMAL
  zh: if ( initTex ) {
- en: texContents = initTex;
  id: totrans-3159
  prefs: []
  type: TYPE_NORMAL
  zh: texContents = initTex;
- en: '}'
  id: totrans-3160
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else {
  id: totrans-3161
  prefs: []
  type: TYPE_NORMAL
  zh: else {
- en: // default is to initialize with identity elements
  id: totrans-3162
  prefs: []
  type: TYPE_NORMAL
  zh: // 默认情况下初始化为单位元素
- en: texContents = (T *) malloc( texN*sizeof(T) );
  id: totrans-3163
  prefs: []
  type: TYPE_NORMAL
  zh: texContents = (T *) malloc( texN*sizeof(T) );
- en: if ( ! texContents )
  id: totrans-3164
  prefs: []
  type: TYPE_NORMAL
  zh: if ( ! texContents )
- en: goto Error;
  id: totrans-3165
  prefs: []
  type: TYPE_NORMAL
  zh: goto Error;
- en: for ( int i = 0; i < texN; i++ ) {
  id: totrans-3166
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < texN; i++ ) {
- en: texContents[i] = (T) i;
  id: totrans-3167
  prefs: []
  type: TYPE_NORMAL
  zh: texContents[i] = (T) i;
- en: '}'
  id: totrans-3168
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3169
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: CUDART_CHECK(cudaMallocArray(&texArray, &channelDesc, texN));
  id: totrans-3170
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaMallocArray(&texArray, &channelDesc, texN));
- en: CUDART_CHECK(cudaHostAlloc( (void **) &outHost,
  id: totrans-3171
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaHostAlloc( (void **) &outHost,
- en: outN*sizeof(float2),
  id: totrans-3172
  prefs: []
  type: TYPE_NORMAL
  zh: outN*sizeof(float2),
- en: cudaHostAllocMapped));
  id: totrans-3173
  prefs: []
  type: TYPE_NORMAL
  zh: cudaHostAllocMapped));
- en: CUDART_CHECK(cudaHostGetDevicePointer( (void **)
  id: totrans-3174
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaHostGetDevicePointer( (void **)
- en: '&outDevice,'
  id: totrans-3175
  prefs: []
  type: TYPE_NORMAL
  zh: '&outDevice,'
- en: outHost, 0 ));
  id: totrans-3176
  prefs: []
  type: TYPE_NORMAL
  zh: outHost, 0 ));
- en: CUDART_CHECK(cudaMemcpyToArray( texArray,
  id: totrans-3177
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaMemcpyToArray( texArray,
- en: 0, 0,
  id: totrans-3178
  prefs: []
  type: TYPE_NORMAL
  zh: 0, 0,
- en: texContents,
  id: totrans-3179
  prefs: []
  type: TYPE_NORMAL
  zh: texContents,
- en: texN*sizeof(T),
  id: totrans-3180
  prefs: []
  type: TYPE_NORMAL
  zh: texN*sizeof(T),
- en: cudaMemcpyHostToDevice));
  id: totrans-3181
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice));
- en: CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
  id: totrans-3182
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
- en: tex.filterMode = filterMode;
  id: totrans-3183
  prefs: []
  type: TYPE_NORMAL
  zh: tex.filterMode = filterMode;
- en: tex.addressMode[0] = addressMode;
  id: totrans-3184
  prefs: []
  type: TYPE_NORMAL
  zh: tex.addressMode[0] = addressMode;
- en: CUDART_CHECK(cudaHostGetDevicePointer(&outDevice, outHost, 0));
  id: totrans-3185
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaHostGetDevicePointer(&outDevice, outHost, 0));
- en: TexReadout<<<2,384>>>( outDevice, outN, base, increment );
  id: totrans-3186
  prefs: []
  type: TYPE_NORMAL
  zh: TexReadout<<<2,384>>>( outDevice, outN, base, increment );
- en: CUDART_CHECK(cudaThreadSynchronize());
  id: totrans-3187
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaThreadSynchronize());
- en: for ( int i = 0; i < outN; i++ ) {
  id: totrans-3188
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < outN; i++ ) {
- en: printf( "(%.2f, %.2f)\n", outHost[i].x, outHost[i].y );
  id: totrans-3189
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "(%.2f, %.2f)\n", outHost[i].x, outHost[i].y );
- en: '}'
  id: totrans-3190
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: printf( "\n" );
  id: totrans-3191
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "\n" );
- en: 'Error:'
  id: totrans-3192
  prefs: []
  type: TYPE_NORMAL
  zh: '错误:'
- en: if ( ! initTex ) free( texContents );
  id: totrans-3193
  prefs: []
  type: TYPE_NORMAL
  zh: if ( ! initTex ) free( texContents );
- en: if ( texArray ) cudaFreeArray( texArray );
  id: totrans-3194
  prefs: []
  type: TYPE_NORMAL
  zh: if ( texArray ) cudaFreeArray( texArray );
- en: if ( outHost ) cudaFreeHost( outHost );
  id: totrans-3195
  prefs: []
  type: TYPE_NORMAL
  zh: if ( outHost ) cudaFreeHost( outHost );
- en: '}'
  id: totrans-3196
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-3197
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The `main()` function for this program is intended to be modified to study texturing
    behavior. This version creates an 8-element texture and writes the output of `tex1D()`
    from 0.0 .. 7.0.
  id: totrans-3198
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序的`main()`函数旨在修改以研究纹理行为。此版本创建了一个8元素的纹理，并写入了从 0.0 到 7.0 的`tex1D()`输出。
- en: '[Click here to view code image](ch10_images.html#p327pro01a)'
  id: totrans-3199
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch10_images.html#p327pro01a)'
- en: int
  id: totrans-3200
  prefs: []
  type: TYPE_NORMAL
  zh: int
- en: main( int argc, char *argv[] )
  id: totrans-3201
  prefs: []
  type: TYPE_NORMAL
  zh: main( int argc, char *argv[] )
- en: '{'
  id: totrans-3202
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-3203
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: CUDA_CHECK(cudaSetDeviceFlags(cudaDeviceMapHost));
  id: totrans-3204
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA_CHECK(cudaSetDeviceFlags(cudaDeviceMapHost));
- en: CreateAndPrintTex<float>( NULL, 8, 8, 0.0f, 1.0f );
  id: totrans-3205
  prefs: []
  type: TYPE_NORMAL
  zh: CreateAndPrintTex<float>( NULL, 8, 8, 0.0f, 1.0f );
- en: CreateAndPrintTex<float>( NULL, 8, 8, 0.0f, 1.0f, cudaFilterModeLinear );
  id: totrans-3206
  prefs: []
  type: TYPE_NORMAL
  zh: CreateAndPrintTex<float>( NULL, 8, 8, 0.0f, 1.0f, cudaFilterModeLinear );
- en: return 0;
  id: totrans-3207
  prefs: []
  type: TYPE_NORMAL
  zh: return 0;
- en: '}'
  id: totrans-3208
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: The output from this program is as follows.
  id: totrans-3209
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序的输出如下：
- en: '[Click here to view code image](ch10_images.html#p327pro02a)'
  id: totrans-3210
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch10_images.html#p327pro02a)'
- en: (0.00, 0.00)    <- output from the first CreateAndPrintTex()
  id: totrans-3211
  prefs: []
  type: TYPE_NORMAL
  zh: (0.00, 0.00)    <- 来自第一次 CreateAndPrintTex() 的输出
- en: (1.00, 1.00)
  id: totrans-3212
  prefs: []
  type: TYPE_NORMAL
  zh: (1.00, 1.00)
- en: (2.00, 2.00)
  id: totrans-3213
  prefs: []
  type: TYPE_NORMAL
  zh: (2.00, 2.00)
- en: (3.00, 3.00)
  id: totrans-3214
  prefs: []
  type: TYPE_NORMAL
  zh: (3.00, 3.00)
- en: (4.00, 4.00)
  id: totrans-3215
  prefs: []
  type: TYPE_NORMAL
  zh: (4.00, 4.00)
- en: (5.00, 5.00)
  id: totrans-3216
  prefs: []
  type: TYPE_NORMAL
  zh: (5.00, 5.00)
- en: (6.00, 6.00)
  id: totrans-3217
  prefs: []
  type: TYPE_NORMAL
  zh: (6.00, 6.00)
- en: (7.00, 7.00)
  id: totrans-3218
  prefs: []
  type: TYPE_NORMAL
  zh: (7.00, 7.00)
- en: (0.00, 0.00)    <- output from the second CreateAndPrintTex()
  id: totrans-3219
  prefs: []
  type: TYPE_NORMAL
  zh: (0.00, 0.00)    <- 来自第二次 CreateAndPrintTex() 的输出
- en: (1.00, 0.50)
  id: totrans-3220
  prefs: []
  type: TYPE_NORMAL
  zh: (1.00, 0.50)
- en: (2.00, 1.50)
  id: totrans-3221
  prefs: []
  type: TYPE_NORMAL
  zh: (2.00, 1.50)
- en: (3.00, 2.50)
  id: totrans-3222
  prefs: []
  type: TYPE_NORMAL
  zh: (3.00, 2.50)
- en: (4.00, 3.50)
  id: totrans-3223
  prefs: []
  type: TYPE_NORMAL
  zh: (4.00, 3.50)
- en: (5.00, 4.50)
  id: totrans-3224
  prefs: []
  type: TYPE_NORMAL
  zh: (5.00, 4.50)
- en: (6.00, 5.50)
  id: totrans-3225
  prefs: []
  type: TYPE_NORMAL
  zh: (6.00, 5.50)
- en: (7.00, 6.50)
  id: totrans-3226
  prefs: []
  type: TYPE_NORMAL
  zh: (7.00, 6.50)
- en: If we change `main()` to invoke `CreateAndPrintTex()` as follows.
  id: totrans-3227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`main()`修改为调用`CreateAndPrintTex()`，如下所示。
- en: '[Click here to view code image](ch10_images.html#p327pro03a)'
  id: totrans-3228
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p327pro03a)'
- en: CreateAndPrintTex<float>( NULL, 8, 20, 0.9f, 0.01f,
  id: totrans-3229
  prefs: []
  type: TYPE_NORMAL
  zh: CreateAndPrintTex<float>( NULL, 8, 20, 0.9f, 0.01f,
- en: cudaFilterModePoint );
  id: totrans-3230
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFilterModePoint );
- en: The resulting output highlights that when point filtering, 1.0 is the dividing
    line between texture elements 0 and 1.
  id: totrans-3231
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出结果突出显示，当进行点滤波时，1.0是纹理元素0和1之间的分界线。
- en: '[Click here to view code image](ch10_images.html#p327pro04a)'
  id: totrans-3232
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p327pro04a)'
- en: (0.90, 0.00)
  id: totrans-3233
  prefs: []
  type: TYPE_NORMAL
  zh: (0.90, 0.00)
- en: (0.91, 0.00)
  id: totrans-3234
  prefs: []
  type: TYPE_NORMAL
  zh: (0.91, 0.00)
- en: (0.92, 0.00)
  id: totrans-3235
  prefs: []
  type: TYPE_NORMAL
  zh: (0.92, 0.00)
- en: (0.93, 0.00)
  id: totrans-3236
  prefs: []
  type: TYPE_NORMAL
  zh: (0.93, 0.00)
- en: (0.94, 0.00)
  id: totrans-3237
  prefs: []
  type: TYPE_NORMAL
  zh: (0.94, 0.00)
- en: (0.95, 0.00)
  id: totrans-3238
  prefs: []
  type: TYPE_NORMAL
  zh: (0.95, 0.00)
- en: (0.96, 0.00)
  id: totrans-3239
  prefs: []
  type: TYPE_NORMAL
  zh: (0.96, 0.00)
- en: (0.97, 0.00)
  id: totrans-3240
  prefs: []
  type: TYPE_NORMAL
  zh: (0.97, 0.00)
- en: (0.98, 0.00)
  id: totrans-3241
  prefs: []
  type: TYPE_NORMAL
  zh: (0.98, 0.00)
- en: (0.99, 0.00)
  id: totrans-3242
  prefs: []
  type: TYPE_NORMAL
  zh: (0.99, 0.00)
- en: (1.00, 1.00)     <- transition point
  id: totrans-3243
  prefs: []
  type: TYPE_NORMAL
  zh: (1.00, 1.00)     <- 过渡点
- en: (1.01, 1.00)
  id: totrans-3244
  prefs: []
  type: TYPE_NORMAL
  zh: (1.01, 1.00)
- en: (1.02, 1.00)
  id: totrans-3245
  prefs: []
  type: TYPE_NORMAL
  zh: (1.02, 1.00)
- en: (1.03, 1.00)
  id: totrans-3246
  prefs: []
  type: TYPE_NORMAL
  zh: (1.03, 1.00)
- en: (1.04, 1.00)
  id: totrans-3247
  prefs: []
  type: TYPE_NORMAL
  zh: (1.04, 1.00)
- en: (1.05, 1.00)
  id: totrans-3248
  prefs: []
  type: TYPE_NORMAL
  zh: (1.05, 1.00)
- en: (1.06, 1.00)
  id: totrans-3249
  prefs: []
  type: TYPE_NORMAL
  zh: (1.06, 1.00)
- en: (1.07, 1.00)
  id: totrans-3250
  prefs: []
  type: TYPE_NORMAL
  zh: (1.07, 1.00)
- en: (1.08, 1.00)
  id: totrans-3251
  prefs: []
  type: TYPE_NORMAL
  zh: (1.08, 1.00)
- en: (1.09, 1.00)
  id: totrans-3252
  prefs: []
  type: TYPE_NORMAL
  zh: (1.09, 1.00)
- en: One limitation of linear filtering is that it is performed with 9-bit weighting
    factors. It is important to realize that the precision of the interpolation depends
    not on that of the texture elements but on the weights. As an example, let’s take
    a look at a 10-element texture initialized with normalized identity elements—that
    is, (0.0, 0.1, 0.2, 0.3, . . . 0.9) instead of (0, 1, 2, . . . 9). `CreateAndPrintTex()`
    lets us specify the texture contents, so we can do so as follows.
  id: totrans-3253
  prefs: []
  type: TYPE_NORMAL
  zh: 线性滤波的一个限制是它是通过9位加权因子进行的。重要的是要认识到，插值的精度不仅仅依赖于纹理元素的精度，而是依赖于权重。例如，让我们看一下一个由标准化的单位元素初始化的10元素纹理——即(0.0,
    0.1, 0.2, 0.3, . . . 0.9)，而不是(0, 1, 2, . . . 9)。`CreateAndPrintTex()`允许我们指定纹理内容，因此我们可以按如下方式进行操作。
- en: '[Click here to view code image](ch10_images.html#p328pro01a)'
  id: totrans-3254
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p328pro01a)'
- en: '{'
  id: totrans-3255
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float texData[10];
  id: totrans-3256
  prefs: []
  type: TYPE_NORMAL
  zh: float texData[10];
- en: for ( int i = 0; i < 10; i++ ) {
  id: totrans-3257
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < 10; i++ ) {
- en: texData[i] = (float) i / 10.0f;
  id: totrans-3258
  prefs: []
  type: TYPE_NORMAL
  zh: texData[i] = (float) i / 10.0f;
- en: '}'
  id: totrans-3259
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: CreateAndPrintTex<float>( texData, 10, 10, 0.0f, 1.0f );
  id: totrans-3260
  prefs: []
  type: TYPE_NORMAL
  zh: CreateAndPrintTex<float>( texData, 10, 10, 0.0f, 1.0f );
- en: '}'
  id: totrans-3261
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: The output from an unmodified `CreateAndPrintTex()` looks innocuous enough.
  id: totrans-3262
  prefs: []
  type: TYPE_NORMAL
  zh: 从未经修改的`CreateAndPrintTex()`输出的结果看起来无害。
- en: (0.00, 0.00)
  id: totrans-3263
  prefs: []
  type: TYPE_NORMAL
  zh: (0.00, 0.00)
- en: (1.00, 0.10)
  id: totrans-3264
  prefs: []
  type: TYPE_NORMAL
  zh: (1.00, 0.10)
- en: (2.00, 0.20)
  id: totrans-3265
  prefs: []
  type: TYPE_NORMAL
  zh: (2.00, 0.20)
- en: (3.00, 0.30)
  id: totrans-3266
  prefs: []
  type: TYPE_NORMAL
  zh: (3.00, 0.30)
- en: (4.00, 0.40)
  id: totrans-3267
  prefs: []
  type: TYPE_NORMAL
  zh: (4.00, 0.40)
- en: (5.00, 0.50)
  id: totrans-3268
  prefs: []
  type: TYPE_NORMAL
  zh: (5.00, 0.50)
- en: (6.00, 0.60)
  id: totrans-3269
  prefs: []
  type: TYPE_NORMAL
  zh: (6.00, 0.60)
- en: (7.00, 0.70)
  id: totrans-3270
  prefs: []
  type: TYPE_NORMAL
  zh: (7.00, 0.70)
- en: (8.00, 0.80)
  id: totrans-3271
  prefs: []
  type: TYPE_NORMAL
  zh: (8.00, 0.80)
- en: (9.00, 0.90)
  id: totrans-3272
  prefs: []
  type: TYPE_NORMAL
  zh: (9.00, 0.90)
- en: Or if we invoke `CreateAndPrintTex()` with linear interpolation between the
    first two texture elements (values `0.1` and `0.2`), we get the following.
  id: totrans-3273
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果我们通过线性插值来调用`CreateAndPrintTex()`，对前两个纹理元素（值`0.1`和`0.2`）进行插值，我们将得到如下结果。
- en: CreateAndPrintTex<float>(tex,10,10,1.5f,0.1f,cudaFilterModeLinear);
  id: totrans-3274
  prefs: []
  type: TYPE_NORMAL
  zh: CreateAndPrintTex<float>(tex, 10, 10, 1.5f, 0.1f, cudaFilterModeLinear);
- en: The resulting output is as follows.
  id: totrans-3275
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出结果如下所示。
- en: (1.50, 0.10)
  id: totrans-3276
  prefs: []
  type: TYPE_NORMAL
  zh: (1.50, 0.10)
- en: (1.60, 0.11)
  id: totrans-3277
  prefs: []
  type: TYPE_NORMAL
  zh: (1.60, 0.11)
- en: (1.70, 0.12)
  id: totrans-3278
  prefs: []
  type: TYPE_NORMAL
  zh: (1.70, 0.12)
- en: (1.80, 0.13)
  id: totrans-3279
  prefs: []
  type: TYPE_NORMAL
  zh: (1.80, 0.13)
- en: (1.90, 0.14)
  id: totrans-3280
  prefs: []
  type: TYPE_NORMAL
  zh: (1.90, 0.14)
- en: (2.00, 0.15)
  id: totrans-3281
  prefs: []
  type: TYPE_NORMAL
  zh: (2.00, 0.15)
- en: (2.10, 0.16)
  id: totrans-3282
  prefs: []
  type: TYPE_NORMAL
  zh: (2.10, 0.16)
- en: (2.20, 0.17)
  id: totrans-3283
  prefs: []
  type: TYPE_NORMAL
  zh: (2.20, 0.17)
- en: (2.30, 0.18)
  id: totrans-3284
  prefs: []
  type: TYPE_NORMAL
  zh: (2.30, 0.18)
- en: (2.40, 0.19)
  id: totrans-3285
  prefs: []
  type: TYPE_NORMAL
  zh: (2.40, 0.19)
- en: Rounded to 2 decimal places, this data looks very well behaved. But if we modify
    `CreateAndPrintTex()` to output hexadecimal instead, the output becomes
  id: totrans-3286
  prefs: []
  type: TYPE_NORMAL
  zh: 四舍五入到小数点后2位，这些数据看起来非常规整。但如果我们将`CreateAndPrintTex()`修改为输出十六进制，结果将变为
- en: (1.50, 0x3dcccccd)
  id: totrans-3287
  prefs: []
  type: TYPE_NORMAL
  zh: (1.50, 0x3dcccccd)
- en: (1.60, 0x3de1999a)
  id: totrans-3288
  prefs: []
  type: TYPE_NORMAL
  zh: (1.60, 0x3de1999a)
- en: (1.70, 0x3df5999a)
  id: totrans-3289
  prefs: []
  type: TYPE_NORMAL
  zh: (1.70, 0x3df5999a)
- en: (1.80, 0x3e053333)
  id: totrans-3290
  prefs: []
  type: TYPE_NORMAL
  zh: (1.80, 0x3e053333)
- en: (1.90, 0x3e0f3333)
  id: totrans-3291
  prefs: []
  type: TYPE_NORMAL
  zh: (1.90, 0x3e0f3333)
- en: (2.00, 0x3e19999a)
  id: totrans-3292
  prefs: []
  type: TYPE_NORMAL
  zh: (2.00, 0x3e19999a)
- en: (2.10, 0x3e240000)
  id: totrans-3293
  prefs: []
  type: TYPE_NORMAL
  zh: (2.10, 0x3e240000)
- en: (2.20, 0x3e2e0000)
  id: totrans-3294
  prefs: []
  type: TYPE_NORMAL
  zh: (2.20, 0x3e2e0000)
- en: (2.30, 0x3e386667)
  id: totrans-3295
  prefs: []
  type: TYPE_NORMAL
  zh: (2.30, 0x3e386667)
- en: (2.40, 0x3e426667)
  id: totrans-3296
  prefs: []
  type: TYPE_NORMAL
  zh: (2.40, 0x3e426667)
- en: It is clear that most fractions of 10 are not exactly representable in floating
    point. Nevertheless, when performing interpolation that does not require high
    precision, these values are interpolated at full precision.
  id: totrans-3297
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，大多数10的分数无法精确地用浮点数表示。然而，当进行不要求高精度的插值时，这些值会以全精度进行插值。
- en: '*Microdemo:* `tex1d_9bit.cu`'
  id: totrans-3298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*小示例程序：* `tex1d_9bit.cu`'
- en: To explore this question of precision, we developed another microdemo, `tex1d_9bit.cu`.
    Here, we’ve populated a texture with 32-bit floating-point values that require
    full precision to represent. In addition to passing the base/increment pair for
    the texture coordinates, another base/increment pair specifies the “expected”
    interpolation value, assuming full-precision interpolation.
  id: totrans-3299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索这个精度问题，我们开发了另一个小示例程序`tex1d_9bit.cu`。在这个示例中，我们用32位浮点值填充了一个纹理，这些值需要完全精确地表示。除了传递纹理坐标的基值/增量对外，我们还传递了另一个基值/增量对，用于指定“期望”的插值值，假设使用全精度插值。
- en: In `tex1d_9bit`, the `CreateAndPrintTex()` function is modified to write its
    output as shown in [Listing 10.5](ch10.html#ch10lis05).
  id: totrans-3300
  prefs: []
  type: TYPE_NORMAL
  zh: 在`tex1d_9bit`中，`CreateAndPrintTex()`函数被修改为按照[清单10.5](ch10.html#ch10lis05)所示的方式输出结果。
- en: '*Listing 10.5.* `Tex1d_9bit.cu` (excerpt).'
  id: totrans-3301
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 10.5.* `Tex1d_9bit.cu`（摘录）。'
- en: '[Click here to view code image](ch10_images.html#p10lis05a)'
  id: totrans-3302
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p10lis05a)'
- en: '* * *'
  id: totrans-3303
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: printf( "X\tY\tActual Value\tExpected Value\tDiff\n" );
  id: totrans-3304
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "X\tY\t实际值\t期望值\t差异\n" );
- en: for ( int i = 0; i < outN; i++ ) {
  id: totrans-3305
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < outN; i++ ) {
- en: T expected;
  id: totrans-3306
  prefs: []
  type: TYPE_NORMAL
  zh: T expected;
- en: if ( bEmulateGPU ) {
  id: totrans-3307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( bEmulateGPU ) {
- en: float x = base+(float)i*increment - 0.5f;
  id: totrans-3308
  prefs: []
  type: TYPE_NORMAL
  zh: float x = base+(float)i*increment - 0.5f;
- en: float frac = x - (float) (int) x;
  id: totrans-3309
  prefs: []
  type: TYPE_NORMAL
  zh: float frac = x - (float) (int) x;
- en: '{'
  id: totrans-3310
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int frac256 = (int) (frac*256.0f+0.5f);
  id: totrans-3311
  prefs: []
  type: TYPE_NORMAL
  zh: int frac256 = (int) (frac*256.0f+0.5f);
- en: frac = frac256/256.0f;
  id: totrans-3312
  prefs: []
  type: TYPE_NORMAL
  zh: frac = frac256/256.0f;
- en: '}'
  id: totrans-3313
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: int index = (int) x;
  id: totrans-3314
  prefs: []
  type: TYPE_NORMAL
  zh: int index = (int) x;
- en: expected = (1.0f-frac)*initTex[index] +
  id: totrans-3315
  prefs: []
  type: TYPE_NORMAL
  zh: expected = (1.0f-frac)*initTex[index] +
- en: frac*initTex[index+1];
  id: totrans-3316
  prefs: []
  type: TYPE_NORMAL
  zh: frac*initTex[index+1];
- en: '}'
  id: totrans-3317
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else {
  id: totrans-3318
  prefs: []
  type: TYPE_NORMAL
  zh: else {
- en: expected = expectedBase + (float) i*expectedIncrement;
  id: totrans-3319
  prefs: []
  type: TYPE_NORMAL
  zh: expected = expectedBase + (float) i*expectedIncrement;
- en: '}'
  id: totrans-3320
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: float diff = fabsf( outHost[i].y - expected );
  id: totrans-3321
  prefs: []
  type: TYPE_NORMAL
  zh: float diff = fabsf( outHost[i].y - expected );
- en: printf( "%.2f\t%.2f\t", outHost[i].x, outHost[i].y );
  id: totrans-3322
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%.2f\t%.2f\t", outHost[i].x, outHost[i].y );
- en: printf( "%08x\t", *(int *) (&outHost[i].y) );
  id: totrans-3323
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%08x\t", *(int *) (&outHost[i].y) );
- en: printf( "%08x\t", *(int *) (&expected) );
  id: totrans-3324
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%08x\t", *(int *) (&expected) );
- en: printf( "%E\n", diff );
  id: totrans-3325
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "%E\n", diff );
- en: '}'
  id: totrans-3326
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: printf( "\n" );
  id: totrans-3327
  prefs: []
  type: TYPE_NORMAL
  zh: printf( "\n" );
- en: '* * *'
  id: totrans-3328
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: For the just-described texture with 10 values (incrementing by 0.1), we can
    use this function to generate a comparison of the actual texture results with
    the expected full-precision result. Calling the function
  id: totrans-3329
  prefs: []
  type: TYPE_NORMAL
  zh: 对于刚刚描述的具有10个值（按0.1递增）的纹理，我们可以使用此函数生成实际纹理结果与期望的全精度结果的比较。调用该函数
- en: '[Click here to view code image](ch10_images.html#p330pro02a)'
  id: totrans-3330
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p330pro02a)'
- en: CreateAndPrintTex<float>( tex, 10, 4, 1.5f, 0.25f, 0.1f, 0.025f );
  id: totrans-3331
  prefs: []
  type: TYPE_NORMAL
  zh: CreateAndPrintTex<float>( tex, 10, 4, 1.5f, 0.25f, 0.1f, 0.025f );
- en: CreateAndPrintTex<float>( tex, 10, 4, 1.5f, 0.1f, 0.1f, 0.01f );
  id: totrans-3332
  prefs: []
  type: TYPE_NORMAL
  zh: CreateAndPrintTex<float>( tex, 10, 4, 1.5f, 0.1f, 0.1f, 0.01f );
- en: yields this output.
  id: totrans-3333
  prefs: []
  type: TYPE_NORMAL
  zh: 产生此输出。
- en: '[Click here to view code image](ch10_images.html#p330pro01a)'
  id: totrans-3334
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p330pro01a)'
- en: X     Y     Actual Value  Expected Value  Diff
  id: totrans-3335
  prefs: []
  type: TYPE_NORMAL
  zh: X     Y     实际值     期望值     差异
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  id: totrans-3336
  prefs: []
  type: TYPE_NORMAL
  zh: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
- en: 1.75  0.12  3e000000      3e000000        0.000000E+00
  id: totrans-3337
  prefs: []
  type: TYPE_NORMAL
  zh: 1.75  0.12  3e000000      3e000000        0.000000E+00
- en: 2.00  0.15  3e19999a      3e19999a        0.000000E+00
  id: totrans-3338
  prefs: []
  type: TYPE_NORMAL
  zh: 2.00  0.15  3e19999a      3e19999a        0.000000E+00
- en: 2.25  0.17  3e333333      3e333333        0.000000E+00
  id: totrans-3339
  prefs: []
  type: TYPE_NORMAL
  zh: 2.25  0.17  3e333333      3e333333        0.000000E+00
- en: X     Y     Actual Value  Expected Value  Diff
  id: totrans-3340
  prefs: []
  type: TYPE_NORMAL
  zh: X     Y     实际值     期望值     差异
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  id: totrans-3341
  prefs: []
  type: TYPE_NORMAL
  zh: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
- en: 1.60  0.11  3de1999a      3de147ae        1.562536E-04
  id: totrans-3342
  prefs: []
  type: TYPE_NORMAL
  zh: 1.60  0.11  3de1999a      3de147ae        1.562536E-04
- en: 1.70  0.12  3df5999a      3df5c290        7.812679E-05
  id: totrans-3343
  prefs: []
  type: TYPE_NORMAL
  zh: 1.70  0.12  3df5999a      3df5c290        7.812679E-05
- en: 1.80  0.13  3e053333      3e051eb8        7.812679E-05
  id: totrans-3344
  prefs: []
  type: TYPE_NORMAL
  zh: 1.80  0.13  3e053333      3e051eb8        7.812679E-05
- en: As you can see from the “Diff” column on the right, the first set of outputs
    were interpolated at full precision, while the second were not. The explanation
    for this difference lies in Appendix F of the *CUDA Programming Guide*, which
    describes how linear interpolation is performed for 1D textures.
  id: totrans-3345
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从右侧“差异”列中看到的，第一组输出是以全精度进行插值的，而第二组则没有。对此差异的解释可以在《*CUDA编程指南*》的附录F中找到，该附录描述了如何对1D纹理执行线性插值。
- en: '*tex*(*x*) = (1 - α)*T*(*i*) + α*T*(*i* + 1)'
  id: totrans-3346
  prefs: []
  type: TYPE_NORMAL
  zh: '*tex*(*x*) = (1 - α)*T*(*i*) + α*T*(*i* + 1)'
- en: where
  id: totrans-3347
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*i* = *floor*(*X*[*B*]), α + *frac*(*X*[*B*]), *X*[*B*] = *x* - 0.5'
  id: totrans-3348
  prefs: []
  type: TYPE_NORMAL
  zh: '*i* = *floor*(*X*[*B*]), α + *frac*(*X*[*B*]), *X*[*B*] = *x* - 0.5'
- en: and α is stored in a 9-bit fixed-point format with 8 bits of fractional value.
  id: totrans-3349
  prefs: []
  type: TYPE_NORMAL
  zh: 并且α以9位定点格式存储，其中8位是小数部分。
- en: In [Listing 10.5](ch10.html#ch10lis05), this computation in C++ is emulated
    in the `bEmulateGPU` case. The code snippet to emulate 9-bit weights can be enabled
    in `tex1d_9bit.cu` by passing `true` as the `bEmulateGPU` parameter of `CreateAndPrintTex()`.
    The output then becomes
  id: totrans-3350
  prefs: []
  type: TYPE_NORMAL
  zh: 在[清单 10.5](ch10.html#ch10lis05)中，此计算在C++中通过`bEmulateGPU`案例进行了仿真。可以通过将`true`作为`CreateAndPrintTex()`的`bEmulateGPU`参数传递，来启用在`tex1d_9bit.cu`中的9位权重仿真代码片段。输出变为
- en: '[Click here to view code image](ch10_images.html#p331pro01a)'
  id: totrans-3351
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p331pro01a)'
- en: X     Y     Actual Value  Expected Value  Diff
  id: totrans-3352
  prefs: []
  type: TYPE_NORMAL
  zh: X     Y     实际值     期望值     差异
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  id: totrans-3353
  prefs: []
  type: TYPE_NORMAL
  zh: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
- en: 1.75  0.12  3e000000      3e000000        0.000000E+00
  id: totrans-3354
  prefs: []
  type: TYPE_NORMAL
  zh: 1.75  0.12  3e000000      3e000000        0.000000E+00
- en: 2.00  0.15  3e19999a      3e19999a        0.000000E+00
  id: totrans-3355
  prefs: []
  type: TYPE_NORMAL
  zh: 2.00  0.15  3e19999a      3e19999a        0.000000E+00
- en: 2.25  0.17  3e333333      3e333333        0.000000E+00
  id: totrans-3356
  prefs: []
  type: TYPE_NORMAL
  zh: 2.25  0.17  3e333333      3e333333        0.000000E+00
- en: X     Y     Actual Value  Expected Value  Diff
  id: totrans-3357
  prefs: []
  type: TYPE_NORMAL
  zh: X     Y     实际值     预期值     差异
- en: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
  id: totrans-3358
  prefs: []
  type: TYPE_NORMAL
  zh: 1.50  0.10  3dcccccd      3dcccccd        0.000000E+00
- en: 1.60  0.11  3de1999a      3de1999a        0.000000E+00
  id: totrans-3359
  prefs: []
  type: TYPE_NORMAL
  zh: 1.60  0.11  3de1999a      3de1999a        0.000000E+00
- en: 1.70  0.12  3df5999a      3df5999a        0.000000E+00
  id: totrans-3360
  prefs: []
  type: TYPE_NORMAL
  zh: 1.70  0.12  3df5999a      3df5999a        0.000000E+00
- en: 1.80  0.13  3e053333      3e053333        0.000000E+00
  id: totrans-3361
  prefs: []
  type: TYPE_NORMAL
  zh: 1.80  0.13  3e053333      3e053333        0.000000E+00
- en: As you can see from the rightmost column of 0’s, when computing the interpolated
    value with 9-bit precision, the differences between “expected” and “actual” output
    disappear.
  id: totrans-3362
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从最右边的0列所看到的，当使用9位精度计算插值值时，“预期”输出和“实际”输出之间的差异消失了。
- en: 10.6\. Texturing with Normalized Coordinates
  id: totrans-3363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.6. 使用归一化坐标进行纹理映射
- en: When texturing with normalized coordinates, the texture is addressed by coordinates
    in the range `[0.0, 1.0)` instead of the range `[0, MaxDim)`. For a 1D texture
    with 16 elements, the normalized coordinates are as in [Figure 10.6](ch10.html#ch10fig06).
  id: totrans-3364
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用归一化坐标进行纹理映射时，纹理是通过`[0.0, 1.0)`范围内的坐标进行寻址，而不是`[0, MaxDim)`范围。对于一个有16个元素的1D纹理，归一化坐标如[图10.6](ch10.html#ch10fig06)所示。
- en: '![Image](graphics/10fig06.jpg)'
  id: totrans-3365
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/10fig06.jpg)'
- en: '*Figure 10.6* Texturing with normalized coordinates.'
  id: totrans-3366
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.6* 使用归一化坐标进行纹理映射。'
- en: 'Other than having texture coordinates that are independent of the texture dimension,
    the texture is dealt with in largely the same way, except that the full range
    of CUDA’s texturing capabilities become available. With normalized coordinates,
    more texture addressing mode besides clamp and border addressing becomes available:
    the *wrap* and *mirror* addressing modes, whose formulas are as follows.'
  id: totrans-3367
  prefs: []
  type: TYPE_NORMAL
  zh: 除了纹理坐标与纹理维度无关外，纹理的处理方式大致相同，只是CUDA的完整纹理功能变得可用。使用归一化坐标时，除了夹紧（clamp）和边界（border）寻址模式，更多的纹理寻址模式变得可用：*环绕（wrap）*和*镜像（mirror）*寻址模式，其公式如下。
- en: '![Image](graphics/331tab01.jpg)'
  id: totrans-3368
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/331tab01.jpg)'
- en: The four texture addressing modes supported in CUDA in [Figure 10.7](ch10.html#ch10fig07)
    show which in-range texture element is fetched by the first two out-of-range coordinates
    on each end. If you are having trouble visualizing the behavior of these addressing
    modes, check out the `tex2d_opengl.cu` microdemo in the next section.
  id: totrans-3369
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA中支持的四种纹理寻址模式如[图10.7](ch10.html#ch10fig07)所示，展示了通过每一端的前两个超出范围的坐标获取哪个范围内的纹理元素。如果你在可视化这些寻址模式的行为时遇到困难，可以查看下一节中的`tex2d_opengl.cu`微示例。
- en: '![Image](graphics/10fig07.jpg)'
  id: totrans-3370
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/10fig07.jpg)'
- en: '*Figure 10.7* Texture addressing modes.'
  id: totrans-3371
  prefs: []
  type: TYPE_NORMAL
  zh: '*图10.7* 纹理寻址模式。'
- en: '* * *'
  id: totrans-3372
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Important Note
  id: totrans-3373
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In the driver API, changes to the texture reference are codified by the `cuTexRefSetArray()`
    or `cuTexRefSetAddress()` function. In other words, calls to functions that make
    state changes, such as `cuTexRefSetFilterMode()` or `cuTexRefSetAddressMode()`,
    have no effect until the texture reference is bound to memory.
  id: totrans-3374
  prefs: []
  type: TYPE_NORMAL
  zh: 在驱动程序 API 中，纹理引用的更改通过 `cuTexRefSetArray()` 或 `cuTexRefSetAddress()` 函数进行编码。换句话说，调用那些改变状态的函数，如
    `cuTexRefSetFilterMode()` 或 `cuTexRefSetAddressMode()`，在纹理引用未绑定到内存之前不会产生任何效果。
- en: '* * *'
  id: totrans-3375
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Floating-Point Coordinates with 1D Device Memory
  id: totrans-3376
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用浮点坐标与 1D 设备内存
- en: For applications that wish to use floating-point coordinates to address the
    texture or use texturing features that are only available for normalized coordinates,
    use `cudaBindTexture2D() / cuTexRefSetAddress2D()` to specify the base address.
    Specify a height of 1 and pitch of `N*sizeof(T)`. The kernel can then call `tex2D(x,0.0f)`
    to read the 1D texture with floating-point coordinates.
  id: totrans-3377
  prefs: []
  type: TYPE_NORMAL
  zh: 对于希望使用浮点坐标来寻址纹理或使用仅对规范化坐标可用的纹理功能的应用，使用 `cudaBindTexture2D() / cuTexRefSetAddress2D()`
    来指定基地址。指定高度为 1，步幅为 `N*sizeof(T)`。然后，内核可以调用 `tex2D(x,0.0f)` 来使用浮点坐标读取 1D 纹理。
- en: 10.7\. 1D Surface Read/Write
  id: totrans-3378
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.7\. 1D 表面读写
- en: Until SM 2.0 hardware became available, CUDA kernels could access the contents
    of CUDA arrays only via texturing. Other access to CUDA arrays, including all
    write access, could be performed only via memcpy functions such as `cudaMemcpyToArray()`.
    The only way for CUDA kernels to both texture from and write to a given region
    of memory was to bind the texture reference to linear device memory.
  id: totrans-3379
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SM 2.0 硬件推出之前，CUDA 内核只能通过纹理访问 CUDA 数组的内容。对于 CUDA 数组的其他访问，包括所有写操作，只能通过如 `cudaMemcpyToArray()`
    之类的 memcpy 函数来执行。CUDA 内核访问并对给定内存区域进行纹理和写入操作的唯一方式是将纹理引用绑定到线性设备内存。
- en: But with the surface read/write functions newly available in SM 2.x, developers
    can bind CUDA arrays to *surface references* and use the `surf1Dread()` and `surf1Dwrite()`
    intrinsics to read and write the CUDA arrays from a kernel. Unlike texture reads,
    which have dedicated cache hardware, these reads and writes go through the same
    L2 cache as global loads and stores.
  id: totrans-3380
  prefs: []
  type: TYPE_NORMAL
  zh: 但随着 SM 2.x 中新推出的表面读写函数，开发者可以将 CUDA 数组绑定到*表面引用*，并使用 `surf1Dread()` 和 `surf1Dwrite()`
    内联函数从内核中读写 CUDA 数组。与具有专用缓存硬件的纹理读取不同，这些读写操作通过与全局加载和存储相同的 L2 缓存进行。
- en: '* * *'
  id: totrans-3381
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Note
  id: totrans-3382
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In order for a surface reference to be bound to a CUDA array, the CUDA array
    must have been created with the `cudaArraySurfaceLoadStore` flag.
  id: totrans-3383
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使表面引用能够绑定到 CUDA 数组，必须使用 `cudaArraySurfaceLoadStore` 标志创建该 CUDA 数组。
- en: '* * *'
  id: totrans-3384
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The 1D surface read/write intrinsics are declared as follows.
  id: totrans-3385
  prefs: []
  type: TYPE_NORMAL
  zh: 1D 表面读写内联函数的声明如下。
- en: '[Click here to view code image](ch10_images.html#p333pro01a)'
  id: totrans-3386
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch10_images.html#p333pro01a)'
- en: template<class Type> Type surf1Dread(surface<void, 1> surfRef, int x,
  id: totrans-3387
  prefs: []
  type: TYPE_NORMAL
  zh: template<class Type> Type surf1Dread(surface<void, 1> surfRef, int x,
- en: boundaryMode = cudaBoundaryModeTrap);
  id: totrans-3388
  prefs: []
  type: TYPE_NORMAL
  zh: boundaryMode = cudaBoundaryModeTrap);
- en: template<class Type> void surf1Dwrite(Type data, surface<void, 1>
  id: totrans-3389
  prefs: []
  type: TYPE_NORMAL
  zh: template<class Type> void surf1Dwrite(Type data, surface<void, 1>
- en: surfRef, int x, boundaryMode = cudaBoundaryModeTrap);
  id: totrans-3390
  prefs: []
  type: TYPE_NORMAL
  zh: surfRef, int x, boundaryMode = cudaBoundaryModeTrap);
- en: These intrinsics are not type-strong—as you can see, surface references are
    declared as `void`—and the size of the memory transaction depends on `sizeof(Type)`
    for a given invocation of `surf1Dread()` or `surf1Dwrite()`. The x offset is in
    bytes and must be naturally aligned with respect to `sizeof(Type)`. For 4-byte
    operands such as `int` or `float`, `offset` must be evenly divisible by 4, for
    `short` it must be divisible by 2, and so on.
  id: totrans-3391
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内建函数并不是类型强制的——如你所见，表面引用被声明为`void`——内存事务的大小取决于给定的`surf1Dread()`或`surf1Dwrite()`调用的`sizeof(Type)`。x偏移量以字节为单位，必须按照`sizeof(Type)`进行自然对齐。对于4字节操作数（如`int`或`float`），`offset`必须能被4整除；对于`short`，必须能被2整除，以此类推。
- en: Support for surface read/write is far less rich than texturing functionality.^([5](ch10.html#ch10fn5))
    Only unformatted reads and writes are supported, with no conversion or interpolation
    functions, and the border handling is restricted to only two modes.
  id: totrans-3392
  prefs: []
  type: TYPE_NORMAL
  zh: 表面读写的支持远不如纹理功能丰富。^([5](ch10.html#ch10fn5)) 仅支持未格式化的读写，没有转换或插值功能，并且边界处理仅限于两种模式。
- en: '[5](ch10.html#ch10fn5a). In fact, CUDA could have bypassed implementation of
    surface references entirely, with the intrinsics operating directly on CUDA arrays.
    Surface references were included for orthogonality with texture references to
    provide for behavior defined on a per-surfref basis as opposed to per-instruction.'
  id: totrans-3393
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch10.html#ch10fn5a)。事实上，CUDA本可以完全绕过表面引用的实现，直接对CUDA数组进行操作。表面引用是为了与纹理引用的正交性而引入的，目的是在每个表面引用基础上定义行为，而不是在每条指令基础上。'
- en: Boundary conditions are handled differently for surface read/write than for
    texture reads. For textures, this behavior is controlled by the addressing mode
    in the texture reference. For surface read/write, the method of handling out-of-range
    `offset` values is specified as a parameter of `surf1Dread()` or `surf1Dwrite()`.
    Out-of-range indices can either cause a hardware exception (`cudaBoundaryModeTrap`)
    or read as 0 for `surf1Dread()` and are ignored for `surf1Dwrite()`(`cudaBoundaryModeZero`).
  id: totrans-3394
  prefs: []
  type: TYPE_NORMAL
  zh: 表面读写的边界条件处理方式与纹理读取不同。对于纹理，行为由纹理引用中的寻址模式控制。对于表面读写，处理越界`offset`值的方法作为`surf1Dread()`或`surf1Dwrite()`的参数指定。越界索引可以导致硬件异常（`cudaBoundaryModeTrap`），对于`surf1Dread()`，会读取为0，而`surf1Dwrite()`则会忽略这些越界值（`cudaBoundaryModeZero`）。
- en: Because of the untyped character of surface references, it is easy to write
    a templated 1D memset routine that works for all types.
  id: totrans-3395
  prefs: []
  type: TYPE_NORMAL
  zh: 由于表面引用的无类型特性，很容易编写一个模板化的1D memset例程，适用于所有类型。
- en: '[Click here to view code image](ch10_images.html#p334pro01a)'
  id: totrans-3396
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p334pro01a)'
- en: surface<void, 1> surf1D;
  id: totrans-3397
  prefs: []
  type: TYPE_NORMAL
  zh: surface<void, 1> surf1D;
- en: template <typename T>
  id: totrans-3398
  prefs: []
  type: TYPE_NORMAL
  zh: template <typename T>
- en: __global__ void
  id: totrans-3399
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: surf1Dmemset( int index, T value, size_t N )
  id: totrans-3400
  prefs: []
  type: TYPE_NORMAL
  zh: surf1Dmemset( int index, T value, size_t N )
- en: '{'
  id: totrans-3401
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3402
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
- en: i < N;
  id: totrans-3403
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x )
  id: totrans-3404
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x )
- en: '{'
  id: totrans-3405
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: surf1Dwrite( value, surf1D, (index+i)*sizeof(T) );
  id: totrans-3406
  prefs: []
  type: TYPE_NORMAL
  zh: surf1Dwrite( value, surf1D, (index+i)*sizeof(T) );
- en: '}'
  id: totrans-3407
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3408
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: This kernel is in the microdemo `surf1Dmemset.cu`, which creates a 64-byte CUDA
    array for illustrative purposes, initializes it with the above kernel, and prints
    the array in float and integer forms.
  id: totrans-3409
  prefs: []
  type: TYPE_NORMAL
  zh: 该内核位于微型演示 `surf1Dmemset.cu` 中，该演示为说明目的创建了一个 64 字节的 CUDA 数组，并用上述内核初始化它，并以浮动和整数形式打印数组。
- en: A generic template host function wraps this kernel with a call to `cudaBindSurfaceToArray().`
  id: totrans-3410
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用模板主机函数将通过调用 `cudaBindSurfaceToArray()` 封装此内核。
- en: '[Click here to view code image](ch10_images.html#p334pro02a)'
  id: totrans-3411
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch10_images.html#p334pro02a)'
- en: template<typename T>
  id: totrans-3412
  prefs: []
  type: TYPE_NORMAL
  zh: template<typename T>
- en: cudaError_t
  id: totrans-3413
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t
- en: surf1Dmemset( cudaArray *array, int offset, T value, size_t N )
  id: totrans-3414
  prefs: []
  type: TYPE_NORMAL
  zh: surf1Dmemset( cudaArray *array, int offset, T value, size_t N )
- en: '{'
  id: totrans-3415
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-3416
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: CUDART_CHECK(cudaBindSurfaceToArray(surf1D, array));
  id: totrans-3417
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaBindSurfaceToArray(surf1D, array));
- en: surf1Dmemset_kernel<<<2,384>>>( 0, value, 4*NUM_VALUES );
  id: totrans-3418
  prefs: []
  type: TYPE_NORMAL
  zh: surf1Dmemset_kernel<<<2,384>>>( 0, value, 4*NUM_VALUES );
- en: 'Error:'
  id: totrans-3419
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: return status;
  id: totrans-3420
  prefs: []
  type: TYPE_NORMAL
  zh: return status;
- en: '}'
  id: totrans-3421
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: The untyped character of surface references makes this template structure much
    easier to pull off than for textures. Because texture references are both type-strong
    and global, they cannot be templatized in the parameter list of a would-be generic
    function. A one-line change from
  id: totrans-3422
  prefs: []
  type: TYPE_NORMAL
  zh: 表面引用的无类型特性使得这个模板结构比纹理更容易实现。由于纹理引用既是类型强制的又是全局的，它们不能在通用函数的参数列表中被模板化。从以下代码的一行更改
- en: CUDART_CHECK(surf1Dmemset(array, 0, 3.141592654f, NUM_VALUES));
  id: totrans-3423
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(surf1Dmemset(array, 0, 3.141592654f, NUM_VALUES));
- en: to
  id: totrans-3424
  prefs: []
  type: TYPE_NORMAL
  zh: 到
- en: CUDART_CHECK(surf1Dmemset(array, 0, (short) 0xbeef, 2*NUM_VALUES));
  id: totrans-3425
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK(surf1Dmemset(array, 0, (short) 0xbeef, 2*NUM_VALUES));
- en: will change the output of this program from
  id: totrans-3426
  prefs: []
  type: TYPE_NORMAL
  zh: 将此程序的输出更改为
- en: '[Click here to view code image](ch10_images.html#p335pro01a)'
  id: totrans-3427
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch10_images.html#p335pro01a)'
- en: 0x40490fdb 0x40490fdb ... (16 times)
  id: totrans-3428
  prefs: []
  type: TYPE_NORMAL
  zh: 0x40490fdb 0x40490fdb ...（16次）
- en: 3.141593E+00 3.141593E+00 ... (16 times)
  id: totrans-3429
  prefs: []
  type: TYPE_NORMAL
  zh: 3.141593E+00 3.141593E+00 ...（16次）
- en: to
  id: totrans-3430
  prefs: []
  type: TYPE_NORMAL
  zh: 到
- en: '[Click here to view code image](ch10_images.html#p335pro02a)'
  id: totrans-3431
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch10_images.html#p335pro02a)'
- en: 0xbeefbeef 0xbeefbeef ... (16 times)
  id: totrans-3432
  prefs: []
  type: TYPE_NORMAL
  zh: 0xbeefbeef 0xbeefbeef ...（16次）
- en: -4.68253E-01 -4.68253E-01 ... (16 times)
  id: totrans-3433
  prefs: []
  type: TYPE_NORMAL
  zh: -4.68253E-01 -4.68253E-01 ...（16次）
- en: 10.8\. 2D Texturing
  id: totrans-3434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.8\. 2D 纹理
- en: In most ways, 2D texturing is similar to 1D texturing as described above. Applications
    optionally may promote integer texture elements to floating point, and they can
    use unnormalized or normalized coordinates. When linear filtering is supported,
    bilinear filtering is performed between four texture values, weighted by the fractional
    bits of the texture coordinates. The hardware can perform a different addressing
    mode for each dimension. For example, the X coordinate can be clamped while the
    Y coordinate is wrapped.
  id: totrans-3435
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数方面，2D 纹理与上述描述的 1D 纹理类似。应用程序可以选择将整数纹理元素提升为浮点数，并且可以使用非归一化或归一化坐标。当支持线性滤波时，会在四个纹理值之间执行双线性滤波，权重由纹理坐标的小数位数决定。硬件可以为每个维度执行不同的寻址模式。例如，X
    坐标可以被限制，而 Y 坐标可以被包裹。
- en: '10.8.1\. Microdemo: `tex2d_opengl.cu`'
  id: totrans-3436
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.8.1\. 微示例：`tex2d_opengl.cu`
- en: This microdemo graphically illustrates the effects of the different texturing
    modes. It uses OpenGL for portability and the GL Utility Library (GLUT) to minimize
    the amount of setup code. To keep distractions to a minimum, this application
    does not use CUDA’s OpenGL interoperability functions. Instead, we allocate mapped
    host memory and render it to the frame buffer using `glDrawPixels()`. To OpenGL,
    the data might as well be coming from the CPU.
  id: totrans-3437
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小示例图形化地展示了不同纹理模式的效果。它使用OpenGL以提高可移植性，并使用GL实用程序库（GLUT）以最小化设置代码的数量。为了减少干扰，本应用程序不使用CUDA的OpenGL互操作功能。相反，我们分配映射的主机内存，并通过`glDrawPixels()`将其渲染到帧缓冲区中。对于OpenGL而言，这些数据看起来就像是来自CPU。
- en: The application supports normalized and unnormalized coordinates and clamp,
    wrap, mirror, and border addressing in both the X and Y directions. For unnormalized
    coordinates, the following kernel is used to write the texture contents into the
    output buffer.
  id: totrans-3438
  prefs: []
  type: TYPE_NORMAL
  zh: 该应用程序支持标准化和非标准化坐标，并在X轴和Y轴方向上支持夹取、环绕、镜像和边界寻址模式。对于非标准化坐标，以下内核用于将纹理内容写入输出缓冲区。
- en: '[Click here to view code image](ch10_images.html#p336pro01a)'
  id: totrans-3439
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p336pro01a)'
- en: __global__ void
  id: totrans-3440
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: RenderTextureUnnormalized( uchar4 *out, int width, int height )
  id: totrans-3441
  prefs: []
  type: TYPE_NORMAL
  zh: RenderTextureUnnormalized( uchar4 *out, int width, int height )
- en: '{'
  id: totrans-3442
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( int row = blockIdx.x; row < height; row += gridDim.x ) {
  id: totrans-3443
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int row = blockIdx.x; row < height; row += gridDim.x ) {
- en: out = (uchar4 *) (((char *) out)+row*4*width);
  id: totrans-3444
  prefs: []
  type: TYPE_NORMAL
  zh: out = (uchar4 *) (((char *) out)+row*4*width);
- en: for ( int col = threadIdx.x; col < width; col += blockDim.x ) {
  id: totrans-3445
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int col = threadIdx.x; col < width; col += blockDim.x ) {
- en: out[col] = tex2D( tex2d, (float) col, (float) row );
  id: totrans-3446
  prefs: []
  type: TYPE_NORMAL
  zh: out[col] = tex2D( tex2d, (float) col, (float) row );
- en: '}'
  id: totrans-3447
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3448
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3449
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: This kernel fills the rectangle of `width` × `height` pixels with values read
    from the texture using texture coordinates corresponding to the pixel locations.
    For out-of-range pixels, you can see the effects of the clamp and border addressing
    modes.
  id: totrans-3450
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内核使用对应像素位置的纹理坐标，从纹理中读取的值填充`宽度`×`高度`的矩形区域。对于越界的像素，可以看到夹取和边界寻址模式的效果。
- en: For normalized coordinates, the following kernel is used to write the texture
    contents into the output buffer.
  id: totrans-3451
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标准化坐标，以下内核用于将纹理内容写入输出缓冲区。
- en: '[Click here to view code image](ch10_images.html#p336pro02a)'
  id: totrans-3452
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch10_images.html#p336pro02a)'
- en: __global__ void
  id: totrans-3453
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: RenderTextureNormalized(
  id: totrans-3454
  prefs: []
  type: TYPE_NORMAL
  zh: RenderTextureNormalized(
- en: uchar4 *out,
  id: totrans-3455
  prefs: []
  type: TYPE_NORMAL
  zh: uchar4 *out,
- en: int width,
  id: totrans-3456
  prefs: []
  type: TYPE_NORMAL
  zh: int width,
- en: int height,
  id: totrans-3457
  prefs: []
  type: TYPE_NORMAL
  zh: int height,
- en: int scale )
  id: totrans-3458
  prefs: []
  type: TYPE_NORMAL
  zh: int scale )
- en: '{'
  id: totrans-3459
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( int j = blockIdx.x; j < height; j += gridDim.x ) {
  id: totrans-3460
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = blockIdx.x; j < height; j += gridDim.x ) {
- en: int row = height-j-1;
  id: totrans-3461
  prefs: []
  type: TYPE_NORMAL
  zh: int row = height-j-1;
- en: out = (uchar4 *) (((char *) out)+row*4*width);
  id: totrans-3462
  prefs: []
  type: TYPE_NORMAL
  zh: out = (uchar4 *) (((char *) out)+row*4*width);
- en: float texRow = scale * (float) row / (float) height;
  id: totrans-3463
  prefs: []
  type: TYPE_NORMAL
  zh: float texRow = scale * (float) row / (float) height;
- en: float invWidth = scale / (float) width;
  id: totrans-3464
  prefs: []
  type: TYPE_NORMAL
  zh: float invWidth = scale / (float) width;
- en: for ( int col = threadIdx.x; col < width; col += blockDim.x ) {
  id: totrans-3465
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int col = threadIdx.x; col < width; col += blockDim.x ) {
- en: float texCol = col * invWidth;
  id: totrans-3466
  prefs: []
  type: TYPE_NORMAL
  zh: float texCol = col * invWidth;
- en: out[col] = tex2D( tex2d, texCol, texRow );
  id: totrans-3467
  prefs: []
  type: TYPE_NORMAL
  zh: out[col] = tex2D( tex2d, texCol, texRow );
- en: '}'
  id: totrans-3468
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3469
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3470
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: The scale parameter specifies the number of times to tile the texture into the
    output buffer. By default, scale=1.0, and the texture is seen only once. When
    running the application, you can hit the 1–9 keys to replicate the texture that
    many times. The C, W, M, and B keys set the addressing mode for the current direction;
    the X and Y keys specify the current direction.
  id: totrans-3471
  prefs: []
  type: TYPE_NORMAL
  zh: scale参数指定将纹理平铺到输出缓冲区的次数。默认情况下，scale=1.0，纹理只显示一次。在运行应用程序时，你可以按1–9键将纹理复制指定次数。C、W、M和B键设置当前方向的寻址模式；X和Y键指定当前方向。
- en: '![Image](graphics/337tab01.jpg)'
  id: totrans-3472
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/337tab01.jpg)'
- en: Readers are encouraged to run the program, or especially to modify and run the
    program, to see the effects of different texturing settings. [Figure 10.8](ch10.html#ch10fig08)
    shows the output of the program for the four permutations of X Wrap/Mirror and
    Y Wrap/Mirror when replicating the texture five times.
  id: totrans-3473
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者运行程序，或者特别是修改并运行程序，以查看不同纹理设置的效果。[图 10.8](ch10.html#ch10fig08)展示了当纹理复制五次时，X包裹/镜像和Y包裹/镜像四种排列组合的程序输出。
- en: '![Image](graphics/10fig08.jpg)'
  id: totrans-3474
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/10fig08.jpg)'
- en: '*Figure 10.8* Wrap and mirror addressing modes.'
  id: totrans-3475
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 10.8* 包裹和镜像寻址模式。'
- en: '10.9\. 2D Texturing: Copy Avoidance'
  id: totrans-3476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.9. 二维纹理处理：避免拷贝
- en: When CUDA was first introduced, CUDA kernels could read from CUDA arrays only
    via texture. Applications could write to CUDA arrays only with memory copies;
    in order for CUDA kernels to write data that would then be read through texture,
    they had to write to device memory and then perform a device→array memcpy. Since
    then, two mechanisms have been added that remove this step for 2D textures.
  id: totrans-3477
  prefs: []
  type: TYPE_NORMAL
  zh: 当CUDA首次推出时，CUDA内核只能通过纹理读取CUDA数组。应用程序只能通过内存拷贝写入CUDA数组；为了使CUDA内核能够写入数据并通过纹理读取，它们必须先写入设备内存，然后执行设备→数组的内存拷贝。从那时起，已添加了两种机制，能够为二维纹理省略这一步骤。
- en: • A 2D texture can be bound to a pitch-allocated range of linear device memory.
  id: totrans-3478
  prefs: []
  type: TYPE_NORMAL
  zh: • 二维纹理可以绑定到线性设备内存的分配范围内。
- en: • Surface load/store intrinsics enable CUDA kernels to write to CUDA arrays
    directly.
  id: totrans-3479
  prefs: []
  type: TYPE_NORMAL
  zh: • 表面加载/存储内建函数使CUDA内核能够直接写入CUDA数组。
- en: 3D texturing from device memory and 3D surface load/store are not supported.
  id: totrans-3480
  prefs: []
  type: TYPE_NORMAL
  zh: 不支持从设备内存进行三维纹理处理及三维表面加载/存储。
- en: For applications that read most or all the texture contents with a regular access
    pattern (such as a video codec) or applications that must work on Tesla-class
    hardware, it is best to keep the data in device memory. For applications that
    perform random (but localized) access when texturing, it is probably best to keep
    the data in CUDA arrays and use surface read/write intrinsics.
  id: totrans-3481
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数或全部以常规访问模式读取纹理内容的应用程序（如视频编解码器）或必须在Tesla级硬件上运行的应用程序，最好将数据保存在设备内存中。对于在纹理处理时执行随机（但局部化）访问的应用程序，最好将数据保存在CUDA数组中，并使用表面读写内建函数。
- en: 10.9.1\. 2D Texturing from Device Memory
  id: totrans-3482
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.9.1. 从设备内存进行二维纹理处理
- en: Texturing from 2D device memory does not have any of the benefits of “block
    linear” addressing—a cache line fill into the texture cache pulls in a horizontal
    span of texels, not a 2D or 3D block of them—but unless the application performs
    random access into the texture, the benefits of avoiding a copy from device memory
    to a CUDA array likely outweigh the penalties of losing block linear addressing.
  id: totrans-3483
  prefs: []
  type: TYPE_NORMAL
  zh: 从 2D 设备内存进行纹理映射并没有“块线性”寻址的任何好处——纹理缓存中的缓存行填充会拉取一行水平的纹素，而不是 2D 或 3D 块的纹素——但除非应用程序对纹理进行随机访问，否则避免从设备内存复制到
    CUDA 数组的好处可能超过失去块线性寻址的惩罚。
- en: To bind a 2D texture reference to a device memory range, call `cudaBindTexture2D().`
  id: totrans-3484
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 2D 纹理引用绑定到设备内存范围，调用 `cudaBindTexture2D()`。
- en: cudaBindTexture2D(
  id: totrans-3485
  prefs: []
  type: TYPE_NORMAL
  zh: cudaBindTexture2D(
- en: NULL,
  id: totrans-3486
  prefs: []
  type: TYPE_NORMAL
  zh: NULL,
- en: '&tex,'
  id: totrans-3487
  prefs: []
  type: TYPE_NORMAL
  zh: '&tex,'
- en: texDevice,
  id: totrans-3488
  prefs: []
  type: TYPE_NORMAL
  zh: texDevice,
- en: '&channelDesc,'
  id: totrans-3489
  prefs: []
  type: TYPE_NORMAL
  zh: '&channelDesc,'
- en: inWidth,
  id: totrans-3490
  prefs: []
  type: TYPE_NORMAL
  zh: inWidth,
- en: inHeight,
  id: totrans-3491
  prefs: []
  type: TYPE_NORMAL
  zh: inHeight,
- en: texPitch );
  id: totrans-3492
  prefs: []
  type: TYPE_NORMAL
  zh: texPitch );
- en: The above call binds the texture reference `tex` to the 2D device memory range
    given by `texDevice / texPitch`. The base address and pitch must conform to hardware-specific
    alignment constraints.^([6](ch10.html#ch10fn6)) The base address must be aligned
    with respect to `cudaDeviceProp.textureAlignment`, and the pitch must be aligned
    with respect to `cudaDeviceProp.texturePitchAlignment`.^([7](ch10.html#ch10fn7))
    The microdemo `tex2d_addressing_device.cu` is identical to `tex2d_addressing.cu`,
    but it uses device memory to hold the texture data. The two programs are designed
    to be so similar that you can look at the differences. A device pointer/pitch
    tuple is declared instead of a CUDA array.
  id: totrans-3493
  prefs: []
  type: TYPE_NORMAL
  zh: 上述调用将纹理引用 `tex` 绑定到由 `texDevice / texPitch` 给出的 2D 设备内存范围。基地址和步长必须符合硬件特定的对齐约束。^([6](ch10.html#ch10fn6))
    基地址必须与 `cudaDeviceProp.textureAlignment` 对齐，步长必须与 `cudaDeviceProp.texturePitchAlignment`
    对齐。^([7](ch10.html#ch10fn7)) 微示例 `tex2d_addressing_device.cu` 与 `tex2d_addressing.cu`
    相同，但它使用设备内存存储纹理数据。这两个程序设计得非常相似，你可以通过查看差异来理解。声明了一个设备指针/步长元组，而不是 CUDA 数组。
- en: '[6](ch10.html#ch10fn6a). CUDA arrays must conform to the same constraints,
    but in that case, the base address and pitch are managed by CUDA and hidden along
    with the memory layout.'
  id: totrans-3494
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](ch10.html#ch10fn6a)。CUDA 数组必须符合相同的约束，但在这种情况下，基地址和步长由 CUDA 管理，并与内存布局一起隐藏。'
- en: '[7](ch10.html#ch10fn7a). In the driver API, the corresponding device attribute
    queries are `CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT` and `CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT`.'
  id: totrans-3495
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](ch10.html#ch10fn7a)。在驱动程序 API 中，对应的设备属性查询是 `CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT`
    和 `CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT`。'
- en: < cudaArray *texArray = 0;
  id: totrans-3496
  prefs: []
  type: TYPE_NORMAL
  zh: < cudaArray *texArray = 0;
- en: T *texDevice = 0;
  id: totrans-3497
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: T *texDevice = 0;
- en: size_t texPitch;
  id: totrans-3498
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: size_t texPitch;
- en: '`cudaMallocPitch()` is called instead of calling `cudaMallocArray()`. `cudaMallocPitch()`
    delegates selection of the base address and pitch to the driver, so the code will
    continue working on future generations of hardware (which have a tendency to increase
    alignment requirements).'
  id: totrans-3499
  prefs: []
  type: TYPE_NORMAL
  zh: 调用了 `cudaMallocPitch()`，而不是 `cudaMallocArray()`。`cudaMallocPitch()` 将基地址和步长的选择委托给驱动程序，因此该代码将继续在未来几代硬件上工作（这些硬件往往增加对齐要求）。
- en: '[Click here to view code image](ch10_images.html#p339pro01a)'
  id: totrans-3500
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p339pro01a)'
- en: < CUDART_CHECK(cudaMallocArray( &texArray,
  id: totrans-3501
  prefs: []
  type: TYPE_NORMAL
  zh: < CUDART_CHECK(cudaMallocArray( &texArray,
- en: < &channelDesc,
  id: totrans-3502
  prefs: []
  type: TYPE_NORMAL
  zh: < &channelDesc,
- en: < inWidth,
  id: totrans-3503
  prefs: []
  type: TYPE_NORMAL
  zh: < inWidth,
- en: CUDART_CHECK(cudaMallocPitch( &texDevice,
  id: totrans-3504
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaMallocPitch( &texDevice,
- en: '&texPitch,'
  id: totrans-3505
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '&texPitch,'
- en: inWidth*sizeof(T),
  id: totrans-3506
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: inWidth*sizeof(T),
- en: inHeight));
  id: totrans-3507
  prefs: []
  type: TYPE_NORMAL
  zh: inHeight));
- en: Next, `cudaTextureBind2D()` is called instead of `cudaBindTextureToArray()`.
  id: totrans-3508
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用`cudaTextureBind2D()`替代`cudaBindTextureToArray()`。
- en: '[Click here to view code image](ch10_images.html#p339pro02a)'
  id: totrans-3509
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p339pro02a)'
- en: < CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
  id: totrans-3510
  prefs: []
  type: TYPE_NORMAL
  zh: < CUDART_CHECK(cudaBindTextureToArray(tex, texArray));
- en: CUDART_CHECK(cudaBindTexture2D( NULL,
  id: totrans-3511
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: CUDART_CHECK(cudaBindTexture2D( NULL,
- en: '&tex,'
  id: totrans-3512
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '&tex,'
- en: texDevice,
  id: totrans-3513
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: texDevice,
- en: '&channelDesc,'
  id: totrans-3514
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '&channelDesc,'
- en: inWidth,
  id: totrans-3515
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: inWidth,
- en: inHeight,
  id: totrans-3516
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: inHeight,
- en: texPitch ));
  id: totrans-3517
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: texPitch ));
- en: The final difference is that instead of freeing the CUDA array, `cudaFree()`
    is called on the pointer returned by `cudaMallocPitch()`.
  id: totrans-3518
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的区别是，不是释放 CUDA 数组，而是对 `cudaMallocPitch()` 返回的指针调用 `cudaFree()`。
- en: < cudaFreeArray( texArray );
  id: totrans-3519
  prefs: []
  type: TYPE_NORMAL
  zh: < cudaFreeArray( texArray );
- en: cudaFree( texDevice );
  id: totrans-3520
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: cudaFree( texDevice );
- en: 10.9.2\. 2D Surface Read/Write
  id: totrans-3521
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.9.2\. 2D 表面读/写
- en: As with 1D surface read/write, Fermi-class hardware enables kernels to write
    directly into CUDA arrays with intrinsic surface read/write functions.
  id: totrans-3522
  prefs: []
  type: TYPE_NORMAL
  zh: 与 1D 表面读/写相似，Fermi 类硬件使内核能够通过固有的表面读/写函数直接写入 CUDA 数组。
- en: '[Click here to view code image](ch10_images.html#p340pro02a)'
  id: totrans-3523
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p340pro02a)'
- en: template<class Type> Type surf2Dread(surface<void, 1> surfRef, int x,
  id: totrans-3524
  prefs: []
  type: TYPE_NORMAL
  zh: template<class Type> Type surf2Dread(surface<void, 1> surfRef, int x,
- en: int y, boundaryMode = cudaBoundaryModeTrap);
  id: totrans-3525
  prefs: []
  type: TYPE_NORMAL
  zh: int y, boundaryMode = cudaBoundaryModeTrap);
- en: template<class Type> Type surf2Dwrite(surface<void, 1> surfRef, Type
  id: totrans-3526
  prefs: []
  type: TYPE_NORMAL
  zh: template<class Type> Type surf2Dwrite(surface<void, 1> surfRef, Type
- en: data, int x, int y, boundaryMode = cudaBoundaryModeTrap);
  id: totrans-3527
  prefs: []
  type: TYPE_NORMAL
  zh: data, int x, int y, boundaryMode = cudaBoundaryModeTrap);
- en: The surface reference declaration and corresponding CUDA kernel for 2D surface
    memset, given in `surf2Dmemset.cu`, is as follows.
  id: totrans-3528
  prefs: []
  type: TYPE_NORMAL
  zh: '`surf2Dmemset.cu` 中给出的 2D 表面 memset 的表面引用声明及相应的 CUDA 内核如下所示。'
- en: '[Click here to view code image](ch10_images.html#p340pro01a)'
  id: totrans-3529
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p340pro01a)'
- en: surface<void, 2> surf2D;
  id: totrans-3530
  prefs: []
  type: TYPE_NORMAL
  zh: surface<void, 2> surf2D;
- en: template<typename T>
  id: totrans-3531
  prefs: []
  type: TYPE_NORMAL
  zh: template<typename T>
- en: __global__ void
  id: totrans-3532
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: surf2Dmemset_kernel( T value,
  id: totrans-3533
  prefs: []
  type: TYPE_NORMAL
  zh: surf2Dmemset_kernel( T value,
- en: int xOffset, int yOffset,
  id: totrans-3534
  prefs: []
  type: TYPE_NORMAL
  zh: int xOffset, int yOffset,
- en: int Width, int Height )
  id: totrans-3535
  prefs: []
  type: TYPE_NORMAL
  zh: int Width, int Height )
- en: '{'
  id: totrans-3536
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( int row = blockIdx.y*blockDim.y + threadIdx.y;
  id: totrans-3537
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int row = blockIdx.y*blockDim.y + threadIdx.y;
- en: row < Height;
  id: totrans-3538
  prefs: []
  type: TYPE_NORMAL
  zh: row < Height;
- en: row += blockDim.y*gridDim.y )
  id: totrans-3539
  prefs: []
  type: TYPE_NORMAL
  zh: row += blockDim.y*gridDim.y )
- en: '{'
  id: totrans-3540
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( int col = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3541
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int col = blockIdx.x*blockDim.x + threadIdx.x;
- en: col < Width;
  id: totrans-3542
  prefs: []
  type: TYPE_NORMAL
  zh: col < Width;
- en: col += blockDim.x*gridDim.x )
  id: totrans-3543
  prefs: []
  type: TYPE_NORMAL
  zh: col += blockDim.x*gridDim.x )
- en: '{'
  id: totrans-3544
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: surf2Dwrite( value,
  id: totrans-3545
  prefs: []
  type: TYPE_NORMAL
  zh: surf2Dwrite( value,
- en: surf2D,
  id: totrans-3546
  prefs: []
  type: TYPE_NORMAL
  zh: surf2D,
- en: (xOffset+col)*sizeof(T),
  id: totrans-3547
  prefs: []
  type: TYPE_NORMAL
  zh: (xOffset+col)*sizeof(T),
- en: yOffset+row );
  id: totrans-3548
  prefs: []
  type: TYPE_NORMAL
  zh: yOffset+row );
- en: '}'
  id: totrans-3549
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3550
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3551
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Remember that the X offset parameter to `surf2Dwrite()` is given in *bytes*.
  id: totrans-3552
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`surf2Dwrite()`的 X 偏移量参数是以*字节*为单位给出的。
- en: 10.10\. 3D Texturing
  id: totrans-3553
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.10\. 3D 纹理
- en: Reading from 3D textures is similar to reading from 2D textures, but there are
    more limitations.
  id: totrans-3554
  prefs: []
  type: TYPE_NORMAL
  zh: 从 3D 纹理中读取数据类似于从 2D 纹理中读取，但有更多的限制。
- en: • 3D textures have smaller limits (2048x2048x2048 instead of 65536x32768).
  id: totrans-3555
  prefs: []
  type: TYPE_NORMAL
  zh: • 3D 纹理的大小限制较小（2048x2048x2048，而不是 65536x32768）。
- en: '• There are no copy avoidance strategies: CUDA does not support 3D texturing
    from device memory or surface load/store on 3D CUDA arrays.'
  id: totrans-3556
  prefs: []
  type: TYPE_NORMAL
  zh: • 没有拷贝避免策略：CUDA 不支持从设备内存或 3D CUDA 数组上的表面加载/存储进行 3D 纹理操作。
- en: 'Other than that, the differences are straightforward: Kernels can read from
    3D textures using a `tex3D()` intrinsic that takes 3 floating-point parameters,
    and the underlying 3D CUDA arrays must be populated by 3D memcpys. Trilinear filtering
    is supported; 8 texture elements are read and interpolated according to the texture
    coordinates, with the same 9-bit precision limit as 1D and 2D texturing.'
  id: totrans-3557
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，差异很简单：内核可以使用 `tex3D()` 内建函数从 3D 纹理中读取数据，该函数接受三个浮点参数，底层的 3D CUDA 数组必须通过
    3D memcpy 操作进行填充。支持三线性过滤；根据纹理坐标读取并插值 8 个纹理元素，精度限制与 1D 和 2D 纹理相同，都是 9 位精度。
- en: The 3D texture size limits may be queried by calling `cuDeviceGetAttribute()`
    with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH`, `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT`,
    and `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH`, or by calling `cudaGetDeviceProperties()`
    and examining `cudaDeviceProp.maxTexture3D`. Due to the much larger number of
    parameters needed, 3D CUDA arrays must be created and manipulated using a different
    set of APIs than 1D or 2D CUDA arrays.
  id: totrans-3558
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 纹理的大小限制可以通过调用 `cuDeviceGetAttribute()` 并传入 `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH`、`CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT`
    和 `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH`，或者调用 `cudaGetDeviceProperties()`
    并检查 `cudaDeviceProp.maxTexture3D` 来查询。由于所需的参数数量较多，3D CUDA 数组必须使用与 1D 或 2D CUDA
    数组不同的 API 集来创建和操作。
- en: To create a 3D CUDA array, the `cudaMalloc3DArray()` function takes a `cudaExtent`
    structure instead of width and height parameters.
  id: totrans-3559
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个 3D CUDA 数组，`cudaMalloc3DArray()` 函数需要传入一个 `cudaExtent` 结构体，而不是宽度和高度参数。
- en: cudaError_t cudaMalloc3DArray(struct cudaArray** array, const struct cudaChannelFormatDesc*
    desc, struct cudaExtent extent, unsigned int flags __dv(0));
  id: totrans-3560
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t cudaMalloc3DArray(struct cudaArray** array, const struct cudaChannelFormatDesc*
    desc, struct cudaExtent extent, unsigned int flags __dv(0));
- en: '`cudaExtent` is defined as follows.'
  id: totrans-3561
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaExtent` 的定义如下。'
- en: struct cudaExtent {
  id: totrans-3562
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaExtent {
- en: size_t width;
  id: totrans-3563
  prefs: []
  type: TYPE_NORMAL
  zh: size_t width;
- en: size_t height;
  id: totrans-3564
  prefs: []
  type: TYPE_NORMAL
  zh: size_t height;
- en: size_t depth;
  id: totrans-3565
  prefs: []
  type: TYPE_NORMAL
  zh: size_t depth;
- en: '};'
  id: totrans-3566
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: Describing 3D memcpy operations is sufficiently complicated that both the CUDA
    runtime and the driver API use structures to specify the parameters. The runtime
    API uses the `cudaMemcpy3DParams` structure, which is declared as follows.
  id: totrans-3567
  prefs: []
  type: TYPE_NORMAL
  zh: 描述 3D memcpy 操作相当复杂，因此 CUDA 运行时和驱动 API 都使用结构体来指定参数。运行时 API 使用 `cudaMemcpy3DParams`
    结构体，其声明如下。
- en: '[Click here to view code image](ch10_images.html#p341pro01a)'
  id: totrans-3568
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p341pro01a)'
- en: struct cudaMemcpy3DParms {
  id: totrans-3569
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaMemcpy3DParms {
- en: struct cudaArray *srcArray;
  id: totrans-3570
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaArray *srcArray;
- en: struct cudaPos srcPos;
  id: totrans-3571
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPos srcPos;
- en: struct cudaPitchedPtr srcPtr;
  id: totrans-3572
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr srcPtr;
- en: struct cudaArray *dstArray;
  id: totrans-3573
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaArray *dstArray;
- en: struct cudaPos dstPos;
  id: totrans-3574
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPos dstPos;
- en: struct cudaPitchedPtr dstPtr;
  id: totrans-3575
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr dstPtr;
- en: struct cudaExtent extent;
  id: totrans-3576
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaExtent extent;
- en: enum cudaMemcpyKind kind;
  id: totrans-3577
  prefs: []
  type: TYPE_NORMAL
  zh: enum cudaMemcpyKind kind;
- en: '};'
  id: totrans-3578
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: 'Most of these structure members are themselves structures: `extent` gives the
    width, height, and depth of the copy. The `srcPos` and `dstPos` members are `cudaPos`
    structures that specify the start points for the source and destination of the
    copy.'
  id: totrans-3579
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结构成员大多数本身也是结构：`extent`给出拷贝的宽度、高度和深度。`srcPos`和`dstPos`成员是`cudaPos`结构，指定拷贝的源和目标的起始点。
- en: struct cudaPos {
  id: totrans-3580
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPos {
- en: size_t x;
  id: totrans-3581
  prefs: []
  type: TYPE_NORMAL
  zh: size_t x;
- en: size_t y;
  id: totrans-3582
  prefs: []
  type: TYPE_NORMAL
  zh: size_t y;
- en: size_t z;
  id: totrans-3583
  prefs: []
  type: TYPE_NORMAL
  zh: size_t z;
- en: '};'
  id: totrans-3584
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: The `cudaPitchedPtr` is a structure that was added with 3D memcpy to contain
    a pointer/pitch tuple.
  id: totrans-3585
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaPitchedPtr`是一个结构，随着3D内存拷贝的引入而添加，用于包含指针/步幅元组。'
- en: '[Click here to view code image](ch10_images.html#p342pro01a)'
  id: totrans-3586
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch10_images.html#p342pro01a)'
- en: struct cudaPitchedPtr
  id: totrans-3587
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr
- en: '{'
  id: totrans-3588
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: void *ptr; /**< Pointer to allocated memory */
  id: totrans-3589
  prefs: []
  type: TYPE_NORMAL
  zh: void *ptr; /**< 指向分配内存的指针 */
- en: size_t pitch; /**< Pitch of allocated memory in bytes */
  id: totrans-3590
  prefs: []
  type: TYPE_NORMAL
  zh: size_t pitch; /**< 分配内存的步幅（以字节为单位） */
- en: size_t xsize; /**< Logical width of allocation in elements */
  id: totrans-3591
  prefs: []
  type: TYPE_NORMAL
  zh: size_t xsize; /**< 分配的逻辑宽度（以元素为单位） */
- en: size_t ysize; /**< Logical height of allocation in elements */
  id: totrans-3592
  prefs: []
  type: TYPE_NORMAL
  zh: size_t ysize; /**< 分配的逻辑高度（以元素为单位） */
- en: '};'
  id: totrans-3593
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: A `cudaPitchedPtr` structure may be created with the function `make_cudaPitchedPtr`,
    which takes the base pointer, pitch, and logical width and height of the allocation.
    `make_cudaPitchedPtr` just copies its parameters into the output struct; however,
  id: totrans-3594
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用函数`make_cudaPitchedPtr`创建`cudaPitchedPtr`结构，该函数接受基指针、步幅以及分配的逻辑宽度和高度。`make_cudaPitchedPtr`只是将其参数复制到输出结构中；然而，
- en: '[Click here to view code image](ch10_images.html#p342pro02a)'
  id: totrans-3595
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch10_images.html#p342pro02a)'
- en: struct cudaPitchedPtr
  id: totrans-3596
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr
- en: make_cudaPitchedPtr(void *d, size_t p, size_t xsz, size_t ysz)
  id: totrans-3597
  prefs: []
  type: TYPE_NORMAL
  zh: make_cudaPitchedPtr(void *d, size_t p, size_t xsz, size_t ysz)
- en: '{'
  id: totrans-3598
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: struct cudaPitchedPtr s;
  id: totrans-3599
  prefs: []
  type: TYPE_NORMAL
  zh: struct cudaPitchedPtr s;
- en: s.ptr = d;
  id: totrans-3600
  prefs: []
  type: TYPE_NORMAL
  zh: s.ptr = d;
- en: s.pitch = p;
  id: totrans-3601
  prefs: []
  type: TYPE_NORMAL
  zh: s.pitch = p;
- en: s.xsize = xsz;
  id: totrans-3602
  prefs: []
  type: TYPE_NORMAL
  zh: s.xsize = xsz;
- en: s.ysize = ysz;
  id: totrans-3603
  prefs: []
  type: TYPE_NORMAL
  zh: s.ysize = ysz;
- en: return s;
  id: totrans-3604
  prefs: []
  type: TYPE_NORMAL
  zh: return s;
- en: '}'
  id: totrans-3605
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: The `simpleTexture3D` sample in the SDK illustrates how to do 3D texturing with
    CUDA.
  id: totrans-3606
  prefs: []
  type: TYPE_NORMAL
  zh: SDK中的`simpleTexture3D`示例演示了如何使用CUDA进行3D纹理处理。
- en: 10.11\. Layered Textures
  id: totrans-3607
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.11\. 分层纹理
- en: '*Layered textures* are known in the graphics world as *texture arrays* because
    they enable 1D or 2D textures to be arranged as arrays accessed by an integer
    index. The main advantage of layered textures over vanilla 2D or 3D textures is
    that they support larger extents within the slices. There is no performance advantage
    to using layered textures.'
  id: totrans-3608
  prefs: []
  type: TYPE_NORMAL
  zh: '*分层纹理*在图形界中被称为*纹理数组*，因为它们使得1D或2D纹理可以作为数组排列，并通过整数索引访问。分层纹理相对于普通的2D或3D纹理的主要优势在于它们支持在切片内更大的范围。使用分层纹理并没有性能上的优势。'
- en: Layered textures are laid out in memory differently than 2D or 3D textures,
    in such a way that 2D or 3D textures will not perform as well if they use the
    layout optimized for layered textures. As a result, when creating the CUDA array,
    you must specify `cudaArrayLayered` to `cudaMalloc3DArray()` or specify `CUDA_ARRAY3D_LAYERED`
    to `cuArray3DCreate()`. The `simpleLayeredTexture` sample in the SDK illustrates
    how to use layered textures.
  id: totrans-3609
  prefs: []
  type: TYPE_NORMAL
  zh: 分层纹理在内存中的布局与 2D 或 3D 纹理不同，以致于如果 2D 或 3D 纹理使用优化为分层纹理的布局，它们的性能就会较差。因此，在创建 CUDA
    数组时，必须指定`cudaArrayLayered`给`cudaMalloc3DArray()`，或指定`CUDA_ARRAY3D_LAYERED`给`cuArray3DCreate()`。SDK
    中的 `simpleLayeredTexture` 示例展示了如何使用分层纹理。
- en: 10.11.1\. 1D Layered Textures
  id: totrans-3610
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.11.1\. 1D 分层纹理
- en: The 1D layered texture size limits may be queried by calling `cuDeviceGet-Attribute()`
    with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_WIDTH` and `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_LAYERS`
    or by calling `cudaGetDeviceProperties()` and examining `cudaDeviceProp.maxTexture1DLayered`.
  id: totrans-3611
  prefs: []
  type: TYPE_NORMAL
  zh: 1D 分层纹理的大小限制可以通过调用`cuDeviceGet-Attribute()`，并使用`CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_WIDTH`和`CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_LAYERS`，或者通过调用`cudaGetDeviceProperties()`并检查`cudaDeviceProp.maxTexture1DLayered`来查询。
- en: 10.11.2\. 2D Layered Textures
  id: totrans-3612
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.11.2\. 2D 分层纹理
- en: The 2D layered texture size limits may be queried by calling `cuDeviceGet-Attribute()`
    with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH` and `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT`
    or `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS` or by calling `cudaGetDeviceProperties()`
    and examining `cudaDeviceProp.maxTexture2DLayered`. The layered texture size limits
    may be queried `cudaGetDeviceProperties()` and examining `cudaDeviceProp.maxTexture2DLayered`.
  id: totrans-3613
  prefs: []
  type: TYPE_NORMAL
  zh: 2D 分层纹理的大小限制可以通过调用`cuDeviceGet-Attribute()`，并使用`CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH`和`CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT`或`CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS`，或者通过调用`cudaGetDeviceProperties()`并检查`cudaDeviceProp.maxTexture2DLayered`来查询。分层纹理的大小限制可以通过`cudaGetDeviceProperties()`并检查`cudaDeviceProp.maxTexture2DLayered`来查询。
- en: 10.12\. Optimal Block Sizing and Performance
  id: totrans-3614
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.12\. 最优块大小和性能
- en: When the texture coordinates are generated in the “obvious” way, such as in
    `tex2d_addressing.cu`
  id: totrans-3615
  prefs: []
  type: TYPE_NORMAL
  zh: 当纹理坐标按“显而易见”的方式生成时，例如在`tex2d_addressing.cu`中，
- en: '[Click here to view code image](ch10_images.html#p343pro01a)'
  id: totrans-3616
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch10_images.html#p343pro01a)'
- en: row = blockIdx.y*blockDim.y + threadIdx.y;
  id: totrans-3617
  prefs: []
  type: TYPE_NORMAL
  zh: row = blockIdx.y*blockDim.y + threadIdx.y;
- en: col = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3618
  prefs: []
  type: TYPE_NORMAL
  zh: col = blockIdx.x*blockDim.x + threadIdx.x;
- en: '... tex2D( tex, (float) col, (float) row);'
  id: totrans-3619
  prefs: []
  type: TYPE_NORMAL
  zh: '... tex2D( tex, (float) col, (float) row);'
- en: then texturing performance is dependent on the block size.
  id: totrans-3620
  prefs: []
  type: TYPE_NORMAL
  zh: 那么纹理性能就取决于块的大小。
- en: To find the optimal size of a thread block, the `tex2D_shmoo.cu` and `surf2Dmemset_shmoo.cu`
    programs time the performance of thread blocks whose width and height vary from
    4..64, inclusive. Some combinations of these thread block sizes are not valid
    because they have too many threads.
  id: totrans-3621
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到线程块的最优大小，`tex2D_shmoo.cu`和`surf2Dmemset_shmoo.cu`程序对线程块的宽度和高度从 4 到 64（含）进行性能计时。这些线程块大小的某些组合无效，因为它们包含的线程数太多。
- en: For this exercise, the texturing kernel is designed to do as little work as
    possible (maximizing exposure to the performance of the texture hardware), while
    still “fooling” the compiler into issuing the code. Each thread computes the floating-point
    sum of the values it reads and writes the sum if the output parameter is non-NULL.
    The trick is that we never pass a non-NULL pointer to this kernel! The reason
    the kernel is structured this way is because if it never wrote any output, the
    compiler would see that the kernel was not doing any work and would emit code
    that did not perform the texturing operations at all.
  id: totrans-3622
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，纹理化内核的设计目标是尽量减少工作量（最大化纹理硬件的性能曝光），同时“欺骗”编译器执行代码。每个线程计算它读取的值的浮点和，并且如果输出参数非NULL，则写入该和。诀窍在于我们从未传递一个非NULL的指针给这个内核！内核这样构造的原因是因为如果它从不写任何输出，编译器会认为该内核没有做任何工作，从而生成不执行纹理化操作的代码。
- en: '[Click here to view code image](ch10_images.html#p344pro01a)'
  id: totrans-3623
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch10_images.html#p344pro01a)'
- en: extern "C" __global__ void
  id: totrans-3624
  prefs: []
  type: TYPE_NORMAL
  zh: extern "C" __global__ void
- en: TexSums( float *out, size_t Width, size_t Height )
  id: totrans-3625
  prefs: []
  type: TYPE_NORMAL
  zh: TexSums( float *out, size_t Width, size_t Height )
- en: '{'
  id: totrans-3626
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float sum = 0.0f;
  id: totrans-3627
  prefs: []
  type: TYPE_NORMAL
  zh: float sum = 0.0f;
- en: for ( int row = blockIdx.y*blockDim.y + threadIdx.y;
  id: totrans-3628
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int row = blockIdx.y*blockDim.y + threadIdx.y;
- en: row < Height;
  id: totrans-3629
  prefs: []
  type: TYPE_NORMAL
  zh: row < Height;
- en: row += blockDim.y*gridDim.y )
  id: totrans-3630
  prefs: []
  type: TYPE_NORMAL
  zh: row += blockDim.y*gridDim.y )
- en: '{'
  id: totrans-3631
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( int col = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-3632
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int col = blockIdx.x*blockDim.x + threadIdx.x;
- en: col < Width;
  id: totrans-3633
  prefs: []
  type: TYPE_NORMAL
  zh: col < Width;
- en: col += blockDim.x*gridDim.x )
  id: totrans-3634
  prefs: []
  type: TYPE_NORMAL
  zh: col += blockDim.x*gridDim.x )
- en: '{'
  id: totrans-3635
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: sum += tex2D( tex, (float) col, (float) row );
  id: totrans-3636
  prefs: []
  type: TYPE_NORMAL
  zh: sum += tex2D( tex, (float) col, (float) row );
- en: '}'
  id: totrans-3637
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3638
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if ( out ) {
  id: totrans-3639
  prefs: []
  type: TYPE_NORMAL
  zh: if ( out ) {
- en: out[blockIdx.x*blockDim.x+threadIdx.x] = sum;
  id: totrans-3640
  prefs: []
  type: TYPE_NORMAL
  zh: out[blockIdx.x*blockDim.x+threadIdx.x] = sum;
- en: '}'
  id: totrans-3641
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-3642
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Even with our “trick,” there is a risk that the compiler will emit code that
    checks the `out` parameter and exits the kernel early if it’s equal to NULL. We’d
    have to synthesize some output that wouldn’t affect performance too much (for
    example, have each thread block compute the reduction of the sums in shared memory
    and write them to `out`). But by compiling the program with the `--keep` option
    and using `cuobjdump ---dump-sass` to examine the microcode, we can see that the
    compiler doesn’t check `out` until after the doubly-nested `for` loop as executed.
  id: totrans-3643
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用我们的“技巧”，仍然存在一个风险，即编译器可能会生成代码来检查`out`参数，如果它等于NULL，则提前退出内核。我们需要合成一些输出，确保不会对性能产生太大影响（例如，让每个线程块计算共享内存中的求和结果并写入`out`）。但是，通过使用`--keep`选项编译程序并使用`cuobjdump
    ---dump-sass`来检查微代码，我们可以看到编译器在执行完双重嵌套的`for`循环之后才检查`out`。
- en: 10.12.1\. Results
  id: totrans-3644
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.12.1\. 结果
- en: On a GeForce GTX 280 (GT200), the optimal block size was found to be 128 threads,
    which delivered 35.7G/s of bandwidth. Thread blocks of size 32W × 4H were about
    the same speed as 16W × 8H or 8W × 16H, all traversing a 4K × 4K texture of `float`
    in 1.88 ms. On a Tesla M2050, the optimal block size was found to be 192 threads,
    which delivered 35.4G/s of bandwidth. As with the GT200, different-sized thread
    blocks were the same speed, with 6W × 32H, 16W × 12H, and 8W × 24H blocks delivering
    about the same performance.
  id: totrans-3645
  prefs: []
  type: TYPE_NORMAL
  zh: 在GeForce GTX 280 (GT200)上，最佳的块大小为128线程，带宽达到35.7G/s。32W × 4H大小的线程块与16W × 8H或8W
    × 16H的性能相当，均在1.88毫秒内遍历了一个4K × 4K的`float`纹理。在Tesla M2050上，最佳块大小为192线程，带宽为35.4G/s。与GT200一样，不同大小的线程块速度相同，6W
    × 32H、16W × 12H和8W × 24H的线程块都提供了相似的性能。
- en: 'The shmoo over 2D surface memset was less conclusive: Block sizes of at least
    128 threads generally had good performance, provided the thread count was evenly
    divisible by the warp size of 32\. The fastest 2D surface memset performance reported
    on a `cg1.4xlarge` without ECC enabled was 48Gb/s.'
  id: totrans-3646
  prefs: []
  type: TYPE_NORMAL
  zh: 对2D表面memset的Shmoo测试结果不太明确：至少128线程的块一般具有较好的性能，前提是线程数能够被32的warp大小整除。没有启用ECC的`cg1.4xlarge`上报告的最快2D表面memset性能为48Gb/s。
- en: For `float`-valued data for both boards we tested, the peak bandwidth numbers
    reported by texturing and surface write are about ¼ and ½ of the achievable peaks
    for global load/store, respectively.
  id: totrans-3647
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们测试的两块板上的`float`值数据，纹理和表面写入报告的峰值带宽分别为全局加载/存储可达峰值的约¼和½。
- en: 10.13\. Texturing Quick References
  id: totrans-3648
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.13\. 纹理快速参考
- en: 10.13.1\. Hardware Capabilities
  id: totrans-3649
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.13.1\. 硬件能力
- en: Hardware Limits
  id: totrans-3650
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 硬件限制
- en: '![Image](graphics/345tab01.jpg)'
  id: totrans-3651
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/345tab01.jpg)'
- en: Queries—Driver API
  id: totrans-3652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 查询—驱动程序API
- en: Most of the hardware limits listed above can be queried with `cuDevice-Attribute()`,
    which may be called with the following values to query.
  id: totrans-3653
  prefs: []
  type: TYPE_NORMAL
  zh: 上述列出的多数硬件限制可以通过`cuDevice-Attribute()`查询，查询时可以使用以下值。
- en: '![Image](graphics/346tab01.jpg)![Image](graphics/346tab01a.jpg)'
  id: totrans-3654
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/346tab01.jpg)![Image](graphics/346tab01a.jpg)'
- en: Queries—CUDA Runtime
  id: totrans-3655
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 查询—CUDA运行时
- en: The following members of `cudaDeviceProp` contain hardware limits as listed
    above.
  id: totrans-3656
  prefs: []
  type: TYPE_NORMAL
  zh: '`cudaDeviceProp`的以下成员包含上述列出的硬件限制。'
- en: '![Image](graphics/347tab01.jpg)'
  id: totrans-3657
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/347tab01.jpg)'
- en: 10.13.2\. CUDA Runtime
  id: totrans-3658
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.13.2\. CUDA运行时
- en: 1D Textures
  id: totrans-3659
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 1D纹理
- en: '![Image](graphics/347tab02.jpg)'
  id: totrans-3660
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/347tab02.jpg)'
- en: 2D Textures
  id: totrans-3661
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2D纹理
- en: '![Image](graphics/347tab03.jpg)'
  id: totrans-3662
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/347tab03.jpg)'
- en: 3D Textures
  id: totrans-3663
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3D纹理
- en: '![Image](graphics/348tab01.jpg)'
  id: totrans-3664
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/348tab01.jpg)'
- en: 1D Layered Textures
  id: totrans-3665
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 1D层纹理
- en: '![Image](graphics/348tab02.jpg)'
  id: totrans-3666
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/348tab02.jpg)'
- en: 2D Layered Textures
  id: totrans-3667
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2D层纹理
- en: '![Image](graphics/348tab03.jpg)'
  id: totrans-3668
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/348tab03.jpg)'
- en: 10.13.3\. Driver API
  id: totrans-3669
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.13.3\. 驱动程序API
- en: 1D Textures
  id: totrans-3670
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 1D纹理
- en: '![Image](graphics/349tab01.jpg)'
  id: totrans-3671
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/349tab01.jpg)'
- en: The texture size limit for device memory is not queryable; it is 2^(27) elements
    on all CUDA-capable GPUs. The texture size limit for 1D CUDA arrays may be queried
    by calling `cuDeviceGetAttribute()` with `CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH`.
  id: totrans-3672
  prefs: []
  type: TYPE_NORMAL
  zh: 设备内存的纹理大小限制无法查询；在所有支持CUDA的GPU上，它是2^(27)个元素。1D CUDA数组的纹理大小限制可以通过调用`cuDeviceGetAttribute()`并传入`CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH`来查询。
- en: 2D Textures
  id: totrans-3673
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2D 纹理
- en: '![Image](graphics/349tab02.jpg)'
  id: totrans-3674
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/349tab02.jpg)'
- en: 3D Textures
  id: totrans-3675
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 3D 纹理
- en: '![Image](graphics/350tab01.jpg)'
  id: totrans-3676
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/350tab01.jpg)'
- en: 1D Layered Textures
  id: totrans-3677
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 1D 层纹理
- en: '![Image](graphics/350tab02.jpg)'
  id: totrans-3678
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/350tab02.jpg)'
- en: 2D Layered Textures
  id: totrans-3679
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 2D 层纹理
- en: '![Image](graphics/350tab03.jpg)'
  id: totrans-3680
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/350tab03.jpg)'
