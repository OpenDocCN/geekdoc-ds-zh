- en: 17  Text as data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17  文本作为数据
- en: 原文：[https://tellingstorieswithdata.com/16-text.html](https://tellingstorieswithdata.com/16-text.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://tellingstorieswithdata.com/16-text.html](https://tellingstorieswithdata.com/16-text.html)
- en: '[Applications](./14-causality_from_obs.html)'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[应用](./14-causality_from_obs.html)'
- en: '[17  Text as data](./16-text.html)'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[17  文本作为数据](./16-text.html)'
- en: '**Prerequisites**'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '**先决条件**'
- en: 'Read *Text as data: An overview*, ([Benoit 2020](99-references.html#ref-benoit2020text))'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读 *文本作为数据：概述*，([Benoit 2020](99-references.html#ref-benoit2020text))
- en: This chapter provides an overview of using text as data.
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章概述了使用文本作为数据的方法。
- en: Read *Supervised Machine Learning for Text Analysis in R*, ([Hvitfeldt and Silge
    2021](99-references.html#ref-hvitfeldt2021supervised))
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读 *R中的文本分析监督学习*，([Hvitfeldt and Silge 2021](99-references.html#ref-hvitfeldt2021supervised))
- en: Focus on Chapters 6 “Regression”, and 7 “Classification”, which implements linear
    and generalized linear models using text as data.
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重点关注第6章“回归”和第7章“分类”，这两章使用文本作为数据实现了线性模型和广义线性模型。
- en: 'Read *The Naked Truth: How the names of 6,816 complexion products can reveal
    bias in beauty*, ([Amaka and Thomas 2021](99-references.html#ref-thenakedtruth))'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读 *《裸真相：6,816种肤色产品的名称如何揭示美容偏见》*，([Amaka and Thomas 2021](99-references.html#ref-thenakedtruth))
- en: Analysis of text on make-up products.
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对化妆品产品的文本分析。
- en: '**Key concepts and skills**'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键概念和技能**'
- en: Understanding text as a source of data that we can analyze enables many interesting
    questions to be considered.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解文本作为我们可以分析的数据源，这使我们能够考虑许多有趣的问题。
- en: Text cleaning and preparation are especially critical because of the large number
    of possible outcomes. There are many decisions that need to be made at this stage,
    which have important effects later in the analysis.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本清洗和准备特别关键，因为可能存在大量不同的结果。在这个阶段需要做出许多决策，这些决策对后续分析有重要影响。
- en: One way to consider a text dataset is to look at which words distinguish particular
    documents.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑文本数据集的一种方式是查看哪些单词可以区分特定的文档。
- en: Another is to consider which topics are contained in a document.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种考虑方式是确定文档中包含哪些主题。
- en: '**Software and packages**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**软件和包**'
- en: Base R ([R Core Team 2024](99-references.html#ref-citeR))
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Base R ([R Core Team 2024](99-references.html#ref-citeR))
- en: '`astrologer` ([Gelfand 2022](99-references.html#ref-astrologer)) (this package
    is not on CRAN, so install it with: `devtools::install_github("sharlagelfand/astrologer")`)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`astrologer` ([Gelfand 2022](99-references.html#ref-astrologer)) (这个包不在CRAN上，所以请使用以下命令安装：`devtools::install_github("sharlagelfand/astrologer")`)'
- en: '`beepr` ([Bååth 2018](99-references.html#ref-beepr))'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beepr` ([Bååth 2018](99-references.html#ref-beepr))'
- en: '`fs` ([Hester, Wickham, and Csárdi 2021](99-references.html#ref-fs))'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fs` ([Hester, Wickham, and Csárdi 2021](99-references.html#ref-fs))'
- en: '`gutenbergr` ([Johnston and Robinson 2022](99-references.html#ref-gutenbergr))'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gutenbergr` ([Johnston and Robinson 2022](99-references.html#ref-gutenbergr))'
- en: '`quanteda` ([Benoit et al. 2018](99-references.html#ref-quanteda))'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quanteda` ([Benoit et al. 2018](99-references.html#ref-quanteda))'
- en: '`stm` ([Roberts, Stewart, and Tingley 2019](99-references.html#ref-stm))'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stm` ([Roberts, Stewart, and Tingley 2019](99-references.html#ref-stm))'
- en: '`tidytext` ([Silge and Robinson 2016](99-references.html#ref-SilgeRobinson2016))'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tidytext` ([Silge and Robinson 2016](99-references.html#ref-SilgeRobinson2016))'
- en: '`tidyverse` ([Wickham et al. 2019](99-references.html#ref-tidyverse))'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tidyverse` ([Wickham et al. 2019](99-references.html#ref-tidyverse))'
- en: '`tinytable` ([Arel-Bundock 2024](99-references.html#ref-tinytable))'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tinytable` ([Arel-Bundock 2024](99-references.html#ref-tinytable))'
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*## 17.1 Introduction'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*## 17.1 引言'
- en: 'Text is all around us. In many cases, text is the earliest type of data that
    we are exposed to. Increases in computational power, the development of new methods,
    and the enormous availability of text, mean that there has been a great deal of
    interest in using text as data. Using text as data provides opportunities for
    unique analyses. For instance:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文本无处不在。在许多情况下，文本是我们最早接触到的数据类型。计算能力的提升、新方法的开发以及文本的巨大可用性，意味着人们对使用文本作为数据产生了极大的兴趣。使用文本作为数据提供了独特的分析机会。例如：
- en: text analysis of state-run newspapers in African countries can identify manipulation
    by governments ([Hassan 2022](99-references.html#ref-Hassan2022));
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非洲国家国有报纸的文本分析可以识别政府的操纵([Hassan 2022](99-references.html#ref-Hassan2022))。
- en: the text from UK daily newspapers can be used to generate better forecasts of
    GDP and inflation ([Kalamara et al. 2022](99-references.html#ref-Kalamara2022)),
    and similarly, *The New York Times* can be used to create an uncertainty index
    which correlates with US economic activity ([Alexopoulos and Cohen 2015](99-references.html#ref-Alexopoulos2015));
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英国日报的文本可以用来生成更好的GDP和通货膨胀预测（[Kalamara等人 2022](99-references.html#ref-Kalamara2022)），同样，*《纽约时报》*可以用来创建一个与美经活动相关的不确定性指数（[Alexopoulos和Cohen
    2015](99-references.html#ref-Alexopoulos2015)）；
- en: the analysis of notes in Electronic Health Records (EHR) can improve the efficiency
    of disease prediction ([Gronsbell et al. 2019](99-references.html#ref-jessgronsbell));
    and
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子健康记录（EHR）中笔记的分析可以提高疾病预测的效率（[Gronsbell等人 2019](99-references.html#ref-jessgronsbell)）；以及
- en: analysis of US congressional records indicates just how often women legislators
    are interrupted by men ([Miller and Sutherland 2022](99-references.html#ref-millersutherland2022)).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 美国国会记录的分析表明，女性立法者被男性打断的频率有多高（[Miller和Sutherland 2022](99-references.html#ref-millersutherland2022)）。
- en: Earlier approaches to the analysis of text tend to convert words into numbers,
    divorced of context. They could then be analyzed using traditional approaches,
    such as variants of logistic regression. More recent methods try to take advantage
    of the structure inherent in text, which can bring additional meaning. The difference
    is perhaps like a child who can group similar colors, compared with a child who
    knows what objects are; although both crocodiles and trees are green, and you
    can do something with that knowledge, it is useful to know that a crocodile could
    eat you while a tree probably would not.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 早期对文本分析的方法往往将单词转换成数字，脱离了上下文。然后可以使用传统的分析方法进行分析，例如逻辑回归的变体。最近的方法试图利用文本中固有的结构，这可以带来额外的意义。这种差异可能就像一个能够将相似颜色分组的孩子，与一个知道物体是什么的孩子相比；尽管鳄鱼和树木都是绿色的，你可以利用这个知识做些事情，但知道鳄鱼可能会吃你，而树可能不会，是有用的。
- en: Text can be considered an unwieldy, yet similar, version of the datasets that
    we have used throughout this book. The main difference is that we will typically
    begin with wide data, where each variable is a word, or token more generally.
    Often each entry is then a count. We would then typically transform this into
    rather long data, with one variable of words and another of the counts. Considering
    text as data naturally requires some abstraction from its context. But this should
    not be entirely separated as this can perpetuate historical inequities. For instance,
    Koenecke et al. ([2020](99-references.html#ref-koenecke2020)) find that automated
    speech recognition systems perform much worse for Black compared with White speakers,
    and Davidson, Bhattacharya, and Weber ([2019](99-references.html#ref-davidson2019racial))
    find that tweets that use Black American English, which is a specifically defined
    technical term, are classified at hate speech at higher rates than similar tweets
    in Standard American English, which again is a technical term.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 文本可以被看作是我们在这本书中使用的所有数据集的一个庞大且相似的版本。主要区别在于，我们通常从宽数据开始，其中每个变量都是一个单词，或者更普遍地说，是一个标记。通常每个条目都是一个计数。然后我们通常会将其转换成相当长的数据，一个变量是单词，另一个是计数。将文本视为数据自然需要从其上下文中抽象出来。但不应完全分离，因为这可能会延续历史不平等。例如，Koenecke等人（[2020](99-references.html#ref-koenecke2020)）发现，与白人说话者相比，自动语音识别系统对黑人说话者的表现要差得多，Davidson、Bhattacharya和Weber（[2019](99-references.html#ref-davidson2019racial)）发现，使用黑人美式英语的推文，这是一个具体定义的技术术语，其被归类为仇恨言论的比率高于使用标准美式英语的类似推文，而标准美式英语也是一个技术术语。
- en: One exciting aspect of text data is that it is typically not generated for the
    purposes of our analysis. The trade-off is that we typically must do a bunch more
    work to get it into a form that we can work with. There are a lot of decisions
    to be made in the data cleaning and preparation stages.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据的一个令人兴奋的方面是，它通常不是为了我们的分析目的而生成的。权衡是，我们通常必须做更多的工作，才能将其转换成我们可以处理的形式。在数据清洗和准备阶段有很多决定需要做出。
- en: The larger size of text datasets means that it is especially important to simulate,
    and start small, when it comes to their analysis. Using text as data is exciting
    because of the quantity and variety of text that is available to us. But in general,
    dealing with text datasets is messy. There is a lot of cleaning and preparation
    that is typically required. Often, text datasets are large. As such having a reproducible
    workflow in place and then clearly communicating your findings, becomes critical.
    Nonetheless, it is an exciting area.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据集的较大规模意味着在分析它们时模拟和从小处开始尤为重要。使用文本作为数据之所以令人兴奋，是因为我们有大量和多样的文本可用。但总的来说，处理文本数据集是混乱的。通常需要进行大量的清洗和准备。通常，文本数据集很大。因此，建立可重复的工作流程并清楚地传达你的发现变得至关重要。尽管如此，这仍然是一个令人兴奋的领域。
- en: '*Shoulders of giants* *Professor Kenneth Benoit is Professor of Computational
    Social Science and Director of the Data Science Institute at the London School
    of Economics and Political Science (LSE). After obtaining a PhD in Government
    from Harvard University in 1998, supervised by Gary King and Kenneth Shepsle,
    he took a position at Trinity College, Dublin, where he was promoted to professor
    in 2007\. He moved to the LSE in 2020\. He is an expert in using quantitative
    methods to analyse text data, especially political text, and social media. Some
    of his important papers include Laver, Benoit, and Garry ([2003](99-references.html#ref-laver2003))
    which extracted policy positions from political texts and helped start the “text
    as data” subfield in political science. He has also worked extensively in other
    methods for estimating policy positions, such as Benoit and Laver ([2006](99-references.html#ref-benoitbook)),
    which provided original expert survey positions in dozens of countries, and Benoit
    and Laver ([2007](99-references.html#ref-benoit2007)) in which he compared expert
    surveys with hand coded analysis of party policy positions. A core contribution
    is the family of R packages known as `quanteda`, for the “quantitative analysis
    of textual data” ([Benoit et al. 2018](99-references.html#ref-quanteda)) which
    makes it easy to analyse text data.*  *In this chapter we first consider preparing
    text datasets. We then consider Term Frequency-Inverse Document Frequency (TF-IDF)
    and topic models.*  *## 17.2 Text cleaning and preparation'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*巨人的肩膀* *肯尼斯·贝诺特教授是伦敦政治经济学院（LSE）计算社会科学教授和数据科学学院院长。1998年，他在哈佛大学获得政府学博士学位，导师是加里·金和肯尼斯·谢普斯勒，之后他在都柏林的三一学院任职。2007年，他被晋升为教授。2020年，他搬到了LSE。他是使用定量方法分析文本数据，特别是政治文本和社交媒体的专家。他的一些重要论文包括Laver,
    Benoit, and Garry ([2003](99-references.html#ref-laver2003))，该论文从政治文本中提取政策立场，并帮助政治科学中的“文本作为数据”子领域开始。他还广泛参与了其他估计政策立场的方法，如Benoit
    and Laver ([2006](99-references.html#ref-benoitbook))，它提供了几十个国家的原始专家调查立场，以及Benoit
    and Laver ([2007](99-references.html#ref-benoit2007))，其中他比较了专家调查与手动编码的政党政策立场分析。一个核心贡献是名为`quanteda`的R包系列，用于“文本数据的定量分析”([Benoit
    et al. 2018](99-references.html#ref-quanteda))，这使得分析文本数据变得容易。*  *在这一章中，我们首先考虑准备文本数据集。然后我们考虑词频-逆文档频率（TF-IDF）和主题模型。*  *##
    17.2 文本清洗和准备'
- en: Text modeling is an exciting area of research. But, and this is true more generally,
    the cleaning and preparation aspect is often at least as difficult as the modeling.
    We will cover some essentials and provide a foundation that can be built on.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 文本建模是一个令人兴奋的研究领域。但，更普遍地说，清洗和准备方面通常至少与建模一样困难。我们将介绍一些基本知识，并提供一个可以在此基础上构建的基础。
- en: 'The first step is to get some data. We discussed data gathering in [Chapter
    7](07-gather.html) and mentioned in passing many sources including:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是获取一些数据。我们在[第7章](07-gather.html)中讨论了数据收集，并顺便提到了许多来源，包括：
- en: Using *Inside Airbnb*, which provides text from reviews.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提供评论文本的*Inside Airbnb*。
- en: Project Gutenberg which provides the text from out-of-copyright books.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Project Gutenberg，它提供了版权已过期的书籍的文本。
- en: Scraping Wikipedia or other websites.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从维基百科或其他网站上抓取数据。
- en: The workhorse packages that we need for text cleaning and preparation are `stringr`,
    which is part of the `tidyverse`, and `quanteda`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的用于文本清洗和准备的“工作马”包是`stringr`，它是`tidyverse`的一部分，以及`quanteda`。
- en: 'For illustrative purposes we construct a corpus of the first sentence or two,
    from three books: *Beloved* by Toni Morrison, *The Last Samurai* by Helen DeWitt,
    and *Jane Eyre* by Charlotte Brontë.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明目的，我们构建了一个来自三本书的第一句或两句话的语料库：托妮·莫里森的《宠儿》，海伦·德维特的《最后的武士》，以及夏洛蒂·勃朗特的《简·爱》。
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '*[PRE2]*  *We typically want to construct a document-feature matrix, which
    has documents in each observation, words in each column, and a count for each
    combination, along with associated metadata. For instance, if our corpus was the
    text from Airbnb reviews, then each document may be a review, and typical features
    could include: “The”, “Airbnb”, “was”, “great”. Notice here that the sentence
    has been split into different words. We typically talk of “tokens” to generalize
    away from words, because of the variety of aspects we may be interested in, but
    words are commonly used.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE2]*  *我们通常希望构建一个文档特征矩阵，其中每个观测值包含文档，每个列包含单词，每个组合包含计数，以及相关的元数据。例如，如果我们的语料库是Airbnb评论的文本，那么每个文档可能是一篇评论，典型的特征可能包括：“The”，“Airbnb”，“was”，“great”。注意，这里的句子已经被分割成不同的单词。我们通常谈论“tokens”来泛指单词，因为我们对可能感兴趣的各种方面，但单词通常被广泛使用。'
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*[PRE4]*  *We use the tokens in the corpus to construct a document-feature
    matrix (DFM) using `dfm()` from `quanteda` ([Benoit et al. 2018](99-references.html#ref-quanteda)).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE4]*  *我们使用语料库中的标记来构建一个文档特征矩阵（DFM），这通过`quanteda`中的`dfm()`函数实现([Benoit et
    al. 2018](99-references.html#ref-quanteda))。'
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*[PRE6]*  *We now consider some of the many decisions that need to be made
    as part of this process. There is no definitive right or wrong answer. Instead,
    we make those decisions based on what we will be using the dataset for.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE6]*  *我们现在考虑在这个过程中需要做出的许多决定。没有绝对的对或错。相反，我们根据我们将要使用的数据集来做出这些决定。'
- en: 17.2.1 Stop words
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.2.1 停用词
- en: Stop words are words such as “the”, “and”, and “a”. For a long time stop words
    were not thought to convey much meaning, and there were concerns around memory-constrained
    computation. A common step of preparing a text dataset was to remove stop words.
    We now know that stop words can have a great deal of meaning ([Schofield, Magnusson,
    and Mimno 2017](99-references.html#ref-schofield2017)). The decision to remove
    them is a nuanced one that depends on circumstances.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词是一些像“the”、“and”和“a”这样的词。长期以来，人们认为停用词传达的意义不大，并且对内存受限的计算存在担忧。准备文本数据集的一个常见步骤就是移除停用词。我们现在知道，停用词可以包含大量的意义([Schofield,
    Magnusson, and Mimno 2017](99-references.html#ref-schofield2017))。是否移除它们是一个需要根据具体情况来决定的微妙决定。
- en: We can get a list of stop words using `stopwords()` from `quanteda`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`quanteda`中的`stopwords()`函数获取停用词列表。
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '*[PRE8]*  *We could then look for all instances of words in that list and crudely
    remove them with `str_replace_all()`.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE8]*  *然后我们可以查找该列表中所有单词的实例，并使用`str_replace_all()`函数粗略地移除它们。'
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*[PRE10]*  *There are many different lists of stop words that have been put
    together by others. For instance, `stopwords()` can use lists including: “snowball”,
    “stopwords-iso”, “smart”, “marimo”, “ancient”, and “nltk”. More generally, if
    we decide to use stop words then we often need to augment such lists with project-specific
    words. We can do this by creating a count of individual words in the corpus, and
    then sorting by the most common and adding those to the stop words list as appropriate.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE10]*  *其他人已经编制了许多不同的停用词列表。例如，`stopwords()`可以使用包括“snowball”、“stopwords-iso”、“smart”、“marimo”、“ancient”和“nltk”在内的列表。更普遍地说，如果我们决定使用停用词，那么我们通常需要根据项目特定的单词来扩充这些列表。我们可以通过在语料库中创建单个单词的计数，然后按最常见排序，并将这些单词添加到停用词列表中来实现这一点。'
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*[PRE12]*  *We can integrate the removal of stop words into our construction
    of the DFM with `dfm_remove()` from `quanteda`.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE12]*  *我们可以通过`quanteda`中的`dfm_remove()`函数将停用词的移除整合到我们的DFM构建过程中。'
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '*[PRE14]*  *When we remove stop words we artificially adjust our dataset. Sometimes
    there may be a good reason to do that. But it must not be done unthinkingly. For
    instance, in [Chapter 6](06-farm.html) and [Chapter 10](10-store_and_share.html)
    we discussed how sometimes datasets may need to be censored, truncated, or manipulated
    in other similar ways, to preserve the privacy of respondents. It is possible
    that the integration of the removal of stop words as a default step in natural
    language processing was due to computational power, which may have been more limited
    when these methods were developed. In any case, Jurafsky and Martin ([[2000] 2023,
    62](99-references.html#ref-jurafskymartin)) conclude that removing stop words
    does not improve performance for text classification. Relatedly, Schofield, Magnusson,
    and Mimno ([2017](99-references.html#ref-schofield2017)) find that inference from
    topic models is not improved by the removal of anything other than the most frequent
    words. If stop words are to be removed, then they recommend doing this after topics
    are constructed.****  ***### 17.2.2 Case, numbers, and punctuation'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE14]*  当我们移除停用词时，我们人为地调整了我们的数据集。有时这样做可能有很好的理由。但绝不能不加思考地这样做。例如，在第6章[06-farm.html](06-farm.html)和第10章[10-store_and_share.html](10-store_and_share.html)中，我们讨论了有时数据集可能需要被审查、截断或以其他类似方式处理，以保护受访者的隐私。可能是因为在开发这些方法时计算能力有限，所以将停用词的移除作为自然语言处理中的默认步骤整合进去。无论如何，Jurafsky和Martin([[2000]
    2023, 62](99-references.html#ref-jurafskymartin))得出结论，移除停用词并不会提高文本分类的性能。相关地，Schofield、Magnusson和Mimno([2017](99-references.html#ref-schofield2017))发现，移除除了最频繁的词之外的其他任何词都不会提高主题模型的推理能力。如果需要移除停用词，他们建议在构建主题之后进行这一操作。****  ***###
    17.2.2 大小写、数字和标点符号'
- en: There are times when all we care about is the word, not the case or punctuation.
    For instance, if the text corpus was particularly messy or the existence of particular
    words was informative. We trade-off the loss of information for the benefit of
    making things simpler. We can convert to lower case with `str_to_lower()`, and
    use `str_replace_all()` to remove punctuation with “[:punct:]”, and numbers with
    “[:digit:]”.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们只关心单词，而不是大小写或标点符号。例如，如果文本语料库特别混乱，或者某些单词的存在具有信息性。我们权衡信息损失的好处，以简化事物。我们可以使用`str_to_lower()`将文本转换为小写，并使用`str_replace_all()`通过“[:punct:]”移除标点符号，以及通过“[:digit:]”移除数字。
- en: '[PRE15]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '*[PRE16]*  *[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE16]*  *[PRE17]'
- en: '*[PRE18]*  *As an aside, we can remove letters, numbers, and punctuation with
    “[:graph:]” in `str_replace_all()`. While this is rarely needed in textbook examples,
    it is especially useful with real datasets, because they will typically have a
    small number of unexpected symbols that we need to identify and then remove. We
    use it to remove everything that we are used to, leaving only that which we are
    not.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE18]*  作为一个插曲，我们可以使用`str_replace_all()`中的“[:graph:]”来移除字母、数字和标点符号。虽然这在教科书示例中很少需要，但在真实数据集中特别有用，因为它们通常只有少数意外的符号需要我们识别并移除。我们用它来移除我们习惯上的所有内容，只留下我们不习惯的内容。'
- en: More generally, we can use arguments in `tokens()` from `quanteda()` to do this.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更普遍地，我们可以使用`quanteda()`中的`tokens()`函数的参数来进行这项操作。
- en: '[PRE19]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '*[PRE20]***  ***### 17.2.3 Typos and uncommon words'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE20]***  ***### 17.2.3 错别字和罕见词'
- en: Then we need to decide what to do about typos and other minor issues. Every
    real-world text has typos. Sometimes these should clearly be fixed. But if they
    are made in a systematic way, for instance, a certain writer always makes the
    same mistakes, then they could have value if we were interested in grouping by
    the writer. The use of OCR will introduce common issues as well, as was seen in
    [Chapter 7](07-gather.html). For instance, “the” is commonly incorrectly recognized
    as “thc”.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要决定如何处理错别字和其他小问题。每个现实世界的文本都存在错别字。有时这些错别字应该被明确地纠正。但如果它们是以系统性的方式出现的，例如，某个作者总是犯同样的错误，那么如果我们对按作者分组感兴趣，它们可能具有价值。OCR的使用也会引入一些常见问题，正如在第7章[07-gather.html](07-gather.html)中看到的。例如，“the”通常被错误地识别为“thc”。
- en: We could fix typos in the same way that we fixed stop words, i.e. with lists
    of corrections. When it comes to uncommon words, we can build this into our document-feature
    matrix creation with `dfm_trim()`. For instance, we could use “min_termfreq =
    2” to remove any word that does not occur at least twice, or “min_docfreq = 0.05”
    to remove any word that is not in at least five per cent of documents or “max_docfreq
    = 0.90” to remove any word that is in at least 90 per cent of documents.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用与修正停用词相同的方式修正拼写错误，即使用修正列表。对于不常见的单词，我们可以在创建文档特征矩阵时将其纳入，使用 `dfm_trim()`。例如，我们可以使用“min_termfreq
    = 2”来删除任何至少不出现两次的单词，或者使用“min_docfreq = 0.05”来删除任何至少不在五分之五的文档中的单词，“max_docfreq
    = 0.90”来删除在任何至少百分之九十的文档中出现的单词。
- en: '[PRE21]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '*[PRE22]*  *### 17.2.4 Tuples'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE22]*  *### 17.2.4 元组'
- en: A tuple is an ordered list of elements. In the context of text it is a series
    of words. If the tuple comprises two words, then we term this a “bi-gram”, three
    words is a “tri-gram”, etc. These are an issue when it comes to text cleaning
    and preparation because we often separate terms based on a space. This would result
    in an inappropriate separation.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 元组是有序元素列表。在文本的上下文中，它是一系列单词。如果元组包含两个单词，那么我们称之为“二元组”，三个单词是“三元组”，等等。在文本清理和准备过程中，这些问题会出现，因为我们通常根据空格来分隔术语。这会导致不适当的分隔。
- en: This is a clear issue when it comes to place names. For instance, consider “British
    Columbia”, “New Hampshire”, “United Kingdom”, and “Port Hedland”. One way forward
    is to create a list of such places and then use `str_replace_all()` to add an
    underscore, for instance, “British_Columbia”, “New_Hampshire”, “United_Kingdom”,
    and “Port_Hedland”. Another option is to use `tokens_compound()` from `quanteda`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理地名时，这是一个明显的问题。例如，考虑“British Columbia”、“New Hampshire”、“United Kingdom”和“Port
    Hedland”。一种前进的方式是创建一个此类地点的列表，然后使用 `str_replace_all()` 添加下划线，例如，“British_Columbia”、“New_Hampshire”、“United_Kingdom”和“Port_Hedland”。另一种选择是使用
    `quanteda` 的 `tokens_compound()`。
- en: '[PRE23]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '*[PRE24]*  *In that case, we knew what the tuples were. But it might be that
    we were not sure what the common tuples were in the corpus. We could use `tokens_ngrams()`
    to identify them. We could ask for, say, all bi-grams in an excerpt from *Jane
    Eyre*. We showed how to download the text of this book from Project Gutenberg
    in [Chapter 13](13-ijaglm.html) and so here we load the local version that we
    saved earlier.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE24]*  *在这种情况下，我们知道元组是什么。但可能我们不确定语料库中常见的元组是什么。我们可以使用 `tokens_ngrams()`
    来识别它们。我们可以要求，例如，从《简·爱》的摘录中获取所有二元组。我们在第13章中展示了如何从Project Gutenberg下载这本书的文本[第13章](13-ijaglm.html)，因此这里我们加载我们之前保存的本地版本。'
- en: '[PRE25]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '*[PRE26]'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE26]'
- en: As there are many blank lines we will remove them.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有很多空白行，我们将删除它们。
- en: '[PRE27]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '*[PRE28]'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE28]'
- en: '*[PRE29]*  *Having identified some common bi-grams, we could add them to the
    list to be changed. This example includes names like “Mr Rochester” and “St John”
    which would need to remain together for analysis.****  ***### 17.2.5 Stemming
    and lemmatizing'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE29]*  *在识别了一些常见的二元组后，我们可以将它们添加到待更改的列表中。这个例子包括像“Mr Rochester”和“St John”这样的名字，它们在分析时需要保持在一起。****  ***###
    17.2.5 词干提取和词形还原'
- en: Stemming and lemmatizing words is another common approach for reducing the dimensionality
    of a text dataset. Stemming means to remove the last part of the word, in the
    expectation that this will result in more general words. For instance, “Canadians”,
    “Canadian”, and “Canada” all stem to “Canad”. Lemmatizing is similar, but is more
    involved. It means changing words, not just on their spelling, but on their canonical
    form ([Grimmer, Roberts, and Stewart 2022, 54](99-references.html#ref-textasdata)).
    For instance, “Canadians”, “Canadian”, “Canucks”, and “Canuck” may all be changed
    to “Canada”.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取和词形还原是减少文本数据集维度的另一种常见方法。词干提取意味着移除单词的最后一部分，期望这会产生更通用的单词。例如，“Canadians”、“Canadian”和“Canada”都词干提取为“Canad”。词形还原与此类似，但更为复杂。它意味着改变单词，不仅是在拼写上，而且在它们的规范形式上([Grimmer,
    Roberts, and Stewart 2022, 54](99-references.html#ref-textasdata))。例如，“Canadians”、“Canadian”、“Canucks”和“Canuck”都可能被更改为“Canada”。
- en: We can do this with `dfm_wordstem()`. We notice, that, say, “minister”, has
    been changed to “minist”.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `dfm_wordstem()` 来实现这一点。我们注意到，例如，“minister” 已经被更改为 “minist”。
- en: '[PRE30]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '*[PRE31]'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE31]'
- en: '[PRE32]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '*[PRE33]**  **While this is a common step in using text as data, Schofield
    et al. ([2017](99-references.html#ref-schofield2017understanding)) find that in
    the context of topic modeling, which we cover later, stemming has little effect
    and there is little need to do it.**  **### 17.2.6 Duplication'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE33]**  **虽然这是使用文本作为数据的一个常见步骤，但Schofield等人([2017](99-references.html#ref-schofield2017understanding))发现，在后续讨论的主题建模的背景下，词干提取的影响很小，几乎没有必要进行这一步骤。**  **###
    17.2.6 重复'
- en: Duplication is a major concern with text datasets because of their size. For
    instance, Bandy and Vincent ([2021](99-references.html#ref-bandy2021addressing))
    showed that around 30 per cent of the data were inappropriately duplicated in
    the BookCorpus dataset, and Schofield, Thompson, and Mimno ([2017](99-references.html#ref-schofield2017quantifying))
    show that this is a major concern and could substantially affect results. However,
    it can be a subtle and difficult to diagnose problem. For instance, in [Chapter
    13](13-ijaglm.html) when we considered counts of page numbers for various authors
    in the context of Poisson regression, we could easily have accidentally included
    each Shakespeare entry twice because not only are there entries for each play,
    but also many anthologies that contained all of them. Careful consideration of
    our dataset identified the issue, but that would be difficult at scale.***************  ***##
    17.3 Term Frequency-Inverse Document Frequency (TF-IDF)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文本数据集的规模，重复是文本数据集的一个主要问题。例如，Bandy和Vincent([2021](99-references.html#ref-bandy2021addressing))表明，在BookCorpus数据集中，大约30%的数据是不适当重复的，Schofield，Thompson和Mimno([2017](99-references.html#ref-schofield2017quantifying))表明这是一个主要问题，可能会严重影响结果。然而，这可能是一个微妙且难以诊断的问题。例如，在[第13章](13-ijaglm.html)中，当我们考虑在泊松回归背景下各种作者的页数计数时，我们很容易不小心将每个莎士比亚条目重复计算两次，因为不仅每个剧本都有条目，而且还有许多包含所有剧本的选集。仔细考虑我们的数据集确定了这个问题，但在大规模上这将很困难。***************  ***##
    17.3 词频-逆文档频率 (TF-IDF)
- en: 17.3.1 Distinguishing horoscopes
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.1 区分运势
- en: Install and load `astrologer`, which is a dataset of horoscopes to explore a
    real dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 安装并加载`astrologer`，这是一个用于探索真实数据集的占星术数据集。
- en: We can then access the “horoscopes” dataset.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以访问“horoscopes”数据集。
- en: '[PRE34]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '*[PRE35]*  *There are four variables: “startdate”, “zodiacsign”, “horoscope”,
    and “url” (note that URL is out-of-date because the website has been updated,
    for instance, the first one refers to [here](https://chaninicholas.com/horoscopes-week-january-5th/)).
    We are interested in the words that are used to distinguish the horoscope of each
    zodiac sign.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE35]*  *有四个变量：“startdate”，“zodiacsign”，“horoscope”和“url”（注意，URL已过时，因为网站已经更新，例如，第一个链接指向[这里](https://chaninicholas.com/horoscopes-week-january-5th/))。我们感兴趣的是用于区分每个星座运势的词汇。'
- en: '[PRE36]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '*[PRE37]*  *There are 106 horoscopes for each zodiac sign. In this example
    we first tokenize by word, and then create counts based on zodiac sign only, not
    date. We use `tidytext` because it is used extensively in Hvitfeldt and Silge
    ([2021](99-references.html#ref-hvitfeldt2021supervised)).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE37]*  *每个星座有106个运势。在这个例子中，我们首先按单词进行分词，然后仅基于星座创建计数，而不是日期。我们使用`tidytext`，因为它在Hvitfeldt和Silge([2021](99-references.html#ref-hvitfeldt2021supervised))的书中被广泛使用。'
- en: '[PRE38]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '*[PRE39]*  *We can see that the most popular words appear to be similar for
    the different zodiacs. At this point, we could use the data in a variety of ways.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE39]*  *我们可以看到，对于不同的星座，最受欢迎的词汇似乎很相似。在这个阶段，我们可以用各种方式使用这些数据。'
- en: We might be interested to know which words characterize each group—that is to
    say, which words are commonly used only in each group. We can do that by first
    looking at a word’s term frequency (TF), which is how many times a word is used
    in the horoscopes for each zodiac sign. The issue is that there are a lot of words
    that are commonly used regardless of context. As such, we may also like to look
    at the inverse document frequency (IDF) in which we “penalize” words that occur
    in the horoscopes for many zodiac signs. A word that occurs in the horoscopes
    of many zodiac signs would have a lower IDF than a word that only occurs in the
    horoscopes of one. The term frequency–inverse document frequency (tf-idf) is then
    the product of these.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想知道哪些词汇能表征每个群体——也就是说，哪些词汇在每个群体中是常用且仅在该群体中使用的。我们可以通过首先查看一个词的词频（TF），即每个星座运势中一个词被使用的次数来实现。问题是，有很多词在无论什么语境下都常用。因此，我们可能还想查看逆文档频率（IDF），其中我们对在许多星座运势中出现的词进行“惩罚”。在许多星座运势中出现的词会有比仅在某个星座运势中出现的词更低的IDF。因此，词频-逆文档频率（tf-idf）是这两个值的乘积。
- en: We can create this value using `bind_tf_idf()` from `tidytext`. It will create
    new variables for each of these measures.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`tidytext`中的`bind_tf_idf()`来创建这个值。它将为这些度量中的每一个创建新的变量。
- en: '[PRE40]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '*[PRE41]*  *In [Table 17.1](#tbl-zodiac) we look at the words that distinguish
    the horoscopes of each zodiac sign. The first thing to notice is that some of
    them have their own zodiac sign. On the one hand, there is an argument for removing
    this, but on the other hand, the fact that it does not happen for all of them
    is perhaps informative of the nature of the horoscopes for each sign.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE41]*  *在[表17.1](#tbl-zodiac)中，我们查看区分每个星座占星术的单词。首先要注意的是，其中一些有它们自己的星座。一方面，有理由去除它们，但另一方面，它们并非对所有星座都发生的事实可能有助于了解每个星座占星术的性质。'
- en: '[PRE42]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '*Table 17.1: Most common words in horoscopes that are unique to a particular
    zodiac sign'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*表17.1：占星术中独特的星座特有的最常见单词'
- en: '| Zodiac sign | Most common words unique to that sign |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 星座 | 该星座特有的最常见单词'
- en: '| --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Capricorn | goat; capricorn; capricorns; signify; neighborhood |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 摩羯座 | goat; capricorn; capricorns; signify; neighborhood |'
- en: '| Pisces | pisces; wasted; missteps; node; shoes |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 双鱼座 | pisces; wasted; missteps; node; shoes |'
- en: '| Sagittarius | sagittarius; rolodex; distorted; coat; reinvest |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 射手座 | sagittarius; rolodex; distorted; coat; reinvest |'
- en: '| Cancer | cancer; organize; overwork; procrastinate; scuttle |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 癌症 | cancer; organize; overwork; procrastinate; scuttle |'
- en: '| Gemini | gemini; mood; output; admit; faces |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 双子座 | gemini; mood; output; admit; faces |'
- en: '| Taurus | bulls; let''s; painfully; virgin; taurus |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 金牛座 | bulls; let''s; painfully; virgin; taurus |'
- en: '| Aries | warns; vesta; aries; fearful; chase |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 白羊座 | warns; vesta; aries; fearful; chase |'
- en: '| Virgo | digesting; trace; liberate; someone''s; final |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 处女座 | digesting; trace; liberate; someone''s; final |'
- en: '| Libra | proof; inevitably; recognizable; reference; disguise |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 天秤座 | proof; inevitably; recognizable; reference; disguise |'
- en: '| Scorpio | skate; advocate; knots; bottle; meditating |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 天蝎座 | skate; advocate; knots; bottle; meditating |'
- en: '| Aquarius | saves; consult; yearnings; sexy; athene |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 水瓶座 | saves; consult; yearnings; sexy; athene |'
- en: '| Leo | trines; blessed; regrets; leo; agree |*****  ***## 17.4 Topic models'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '| 利奥 | trines; blessed; regrets; leo; agree |*****  ***## 17.4 主题模型'
- en: Topic models are useful when we have many statements and we want to create groups
    based on which sentences that use similar words. We consider those groups of similar
    words to define topics. One way to get consistent estimates of the topics of each
    statement is to use topic models. While there are many variants, one way is to
    use the latent Dirichlet allocation (LDA) method of Blei, Ng, and Jordan ([2003](99-references.html#ref-Blei2003latent)),
    as implemented by `stm`. For clarity, in the context of this chapter, LDA refers
    to latent Dirichlet allocation and not Linear Discriminant Analysis, although
    this is another common subject associated with the acronym LDA.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有许多陈述并且想要根据使用相似单词的句子创建组时，主题模型很有用。我们将这些相似单词的组视为定义主题。获取每个陈述主题的一致估计的一种方法是用主题模型。虽然有许多变体，但一种方法是用Blei、Ng和Jordan的潜在狄利克雷分配（LDA）方法（[2003](99-references.html#ref-Blei2003latent)），如`stm`实现。为了清晰起见，在本章的上下文中，LDA指的是潜在狄利克雷分配，而不是线性判别分析，尽管这也是与LDA缩写相关的一个常见主题。
- en: The key assumption behind the LDA method is that for each statement, a document,
    is made by a person who decides the topics they would like to talk about in that
    document, and who then chooses words, terms, that are appropriate to those topics.
    A topic could be thought of as a collection of terms, and a document as a collection
    of topics. The topics are not specified *ex ante*; they are an outcome of the
    method. Terms are not necessarily unique to a particular topic, and a document
    could be about more than one topic. This provides more flexibility than other
    approaches such as a strict word count method. The goal is to have the words found
    in documents group themselves to define topics.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: LDA方法背后的关键假设是，对于每个陈述，即文档，是由一个决定在文档中想要讨论哪些主题的人编写的，然后选择与这些主题适当的单词、术语。可以将主题视为术语的集合，文档视为主题的集合。主题不是事前指定的；它们是方法的结果。术语不一定属于特定的主题，文档可能涉及多个主题。这比其他方法（如严格的词数统计方法）提供了更多的灵活性。目标是让文档中的单词自行分组以定义主题。
- en: LDA considers each statement to be a result of a process where a person first
    chooses the topics they want to speak about. After choosing the topics, the person
    then chooses appropriate words to use for each of those topics. More generally,
    the LDA topic model works by considering each document as having been generated
    by some probability distribution over topics. For instance, if there were five
    topics and two documents, then the first document may be comprised mostly of the
    first few topics; the other document may be mostly about the final few topics
    ([Figure 17.1](#fig-topicsoverdocuments)).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: LDA模型认为每个陈述都是一个过程的结果，在这个过程中，一个人首先选择他们想要讨论的主题。在选择主题之后，这个人然后选择适合每个主题的适当词语。更普遍地，LDA主题模型通过考虑每个文档是由某些主题的概率分布生成的。例如，如果有五个主题和两个文档，那么第一个文档可能主要由前几个主题组成；另一个文档可能主要关于最后几个主题（[图17.1](#fig-topicsoverdocuments)）。
- en: '![](../Images/fbc15fd6ef3ff8378003cea402ea8a39.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fbc15fd6ef3ff8378003cea402ea8a39.png)'
- en: (a) Distribution for Document 1
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 文档1的分布
- en: '![](../Images/1be89f333d393157f69c754533cd56f0.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1be89f333d393157f69c754533cd56f0.png)'
- en: (b) Distribution for Document 2
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 文档2的分布
- en: 'Figure 17.1: Probability distributions over topics'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1：主题的概率分布
- en: Similarly, each topic could be considered a probability distribution over terms.
    To choose the terms used in each document the speaker picks terms from each topic
    in the appropriate proportion. For instance, if there were ten terms, then one
    topic could be defined by giving more weight to terms related to immigration;
    and some other topic may give more weight to terms related to the economy ([Figure 17.2](#fig-topicsoverterms)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，每个主题也可以被认为是一个词语的概率分布。为了选择每个文档中使用的词语，说话者从每个主题中按适当比例选择词语。例如，如果有十个词语，那么一个主题可以通过给予与移民相关的词语更多权重来定义；另一个主题可能更重视与经济相关的词语（[图17.2](#fig-topicsoverterms)）。
- en: '![](../Images/f5195e9764a781ae7ff7f3acfdeb9eac.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f5195e9764a781ae7ff7f3acfdeb9eac.png)'
- en: (a) Distribution for Topic 1
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 主题1的分布
- en: '![](../Images/f77fcbeec6f092f969cc794bb1077e47.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/f77fcbeec6f092f969cc794bb1077e47.png)'
- en: (b) Distribution for Topic 2
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 主题2的分布
- en: 'Figure 17.2: Probability distributions over terms'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2：词语的概率分布
- en: By way of background, the Dirichlet distribution is a variation of the beta
    distribution that is commonly used as a prior for categorical and multinomial
    variables. If there are just two categories, then the Dirichlet and the beta distributions
    are the same. In the special case of a symmetric Dirichlet distribution, \(\eta=1\),
    it is equivalent to a uniform distribution. If \(\eta<1\), then the distribution
    is sparse and concentrated on a smaller number of the values, and this number
    decreases as \(\eta\) decreases. A hyperparameter, in this usage, is a parameter
    of a prior distribution.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 作为背景，Dirichlet分布是beta分布的一种变体，通常用作分类和多项式变量的先验分布。如果只有两个类别，那么Dirichlet分布和beta分布是相同的。在特殊的对称Dirichlet分布情况下，\(\eta=1\)，它等同于均匀分布。如果\(\eta<1\)，那么分布是稀疏的，集中在较少的值上，并且随着\(\eta\)的减小，这个数值会减少。在这个用法中，超参数是先验分布的参数。
- en: After the documents are created, they are all that we can analyze. The term
    usage in each document is observed, but the topics are hidden, or “latent”. We
    do not know the topics of each document, nor how terms defined the topics. That
    is, we do not know the probability distributions of [Figure 17.1](#fig-topicsoverdocuments)
    or [Figure 17.2](#fig-topicsoverterms). In a sense we are trying to reverse the
    document generation process—we have the terms, and we would like to discover the
    topics.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 文档创建之后，它们就是我们可以分析的所有内容。每个文档中的词语使用情况被观察到，但主题是隐藏的，或称为“潜在的”。我们不知道每个文档的主题，也不知道词语是如何定义主题的。也就是说，我们不知道[图17.1](#fig-topicsoverdocuments)或[图17.2](#fig-topicsoverterms)的概率分布。从某种意义上说，我们正在尝试逆转文档生成过程——我们有词语，我们希望发现主题。
- en: If we observe the terms in each document, then we can obtain estimates of the
    topics ([Steyvers and Griffiths 2006](99-references.html#ref-SteyversGriffiths2006)).
    The outcomes of the LDA process are probability distributions. It is these distributions
    that define the topics. Each term will be given a probability of being a member
    of a particular topic, and each document will be given a probability of being
    about a particular topic.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们观察每个文档中的词语，那么我们可以获得主题的估计值（[Steyvers和Griffiths 2006](99-references.html#ref-SteyversGriffiths2006)）。LDA过程的输出是概率分布。正是这些分布定义了主题。每个词语将给出属于特定主题的概率，每个文档也将给出关于特定主题的概率。
- en: The initial practical step when implementing LDA given a corpus of documents
    is usually to remove stop words. Although, as mentioned earlier, this is not necessary,
    and may be better done after the groups are created. We often also remove punctuation
    and capitalization. We then construct our document-feature matrix using `dfm()`
    from `quanteda`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定文档语料库的情况下实施LDA的初始实际步骤通常是删除停用词。尽管如前所述，这并非必要，而且可能最好在创建组之后完成。我们通常还会删除标点符号和大写字母。然后我们使用`quanteda`中的`dfm()`构建我们的文档-特征矩阵。
- en: After the dataset is ready, `stm` can be used to implement LDA and approximate
    the posterior. The process attempts to find a topic for a particular term in a
    particular document, given the topics of all other terms for all other documents.
    Broadly, it does this by first assigning every term in every document to a random
    topic, specified by Dirichlet priors. It then selects a particular term in a particular
    document and assigns it to a new topic based on the conditional distribution where
    the topics for all other terms in all documents are taken as given ([Grün and
    Hornik 2011, 6](99-references.html#ref-Grun2011)). Once this has been estimated,
    then estimates for the distribution of words into topics and topics into documents
    can be backed out.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集准备就绪后，可以使用`stm`来实现LDA并近似后验。这个过程试图在给定所有其他文档中所有其他术语的主题的情况下，为特定文档中的特定术语找到一个主题。广义上，它首先将每个文档中的每个术语分配给一个由Dirichlet先验指定的随机主题。然后，它选择特定文档中的特定术语，并根据所有其他文档中所有其他术语的主题条件分布将其分配给一个新的主题（[Grün和Hornik
    2011, 6](99-references.html#ref-Grun2011)）。一旦这个估计完成，就可以从单词到主题和主题到文档的分布中得出估计。
- en: The conditional distribution assigns topics depending on how often a term has
    been assigned to that topic previously, and how common the topic is in that document
    ([Steyvers and Griffiths 2006](99-references.html#ref-SteyversGriffiths2006)).
    The initial random allocation of topics means that the results of early passes
    through the corpus of document are poor, but given enough time the algorithm converges
    to an appropriate estimate.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 条件分布根据一个术语之前被分配给该主题的频率以及该主题在该文档中的普遍性来分配主题（[Steyvers和Griffiths 2006](99-references.html#ref-SteyversGriffiths2006)）。主题的初始随机分配意味着早期通过文档语料库的遍历结果较差，但给定足够的时间，算法会收敛到一个适当的估计。
- en: The choice of the number of topics, \(k\), affects the results, and must be
    specified *a priori*. If there is a strong reason for a particular number, then
    this can be used. Otherwise, one way to choose an appropriate number is to use
    a test and training set process. Essentially, this means running the process on
    a variety of possible values for *k* and then picking an appropriate value that
    performs well.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 主题数量\(k\)的选择会影响结果，并且必须事先指定。如果有充分的理由选择特定的数字，则可以使用它。否则，选择适当数量的方法之一是使用测试集和训练集过程。本质上，这意味着在多种可能的\(k\)值上运行该过程，然后选择一个表现良好的适当值。
- en: One weakness of the LDA method is that it considers a “bag of words” where the
    order of those words does not matter ([Blei 2012](99-references.html#ref-blei2012)).
    It is possible to extend the model to reduce the impact of the bag-of-words assumption
    and add conditionality to word order. Additionally, alternatives to the Dirichlet
    distribution can be used to extend the model to allow for correlation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: LDA方法的一个弱点是它考虑了一个“词袋”模型，其中那些词的顺序并不重要（[Blei 2012](99-references.html#ref-blei2012)）。可以将模型扩展以减少词袋假设的影响，并添加条件性到词序。此外，可以使用Dirichlet分布的替代品来扩展模型，以允许相关性。
- en: 17.4.1 What is talked about in the Canadian parliament?
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.4.1 加拿大议会上讨论了什么？
- en: Following the example of the British, the written record of what is said in
    the Canadian parliament is called “Hansard”. It is not completely verbatim, but
    is very close. It is available in CSV format from [LiPaD](https://www.lipad.ca),
    which was constructed by Beelen et al. ([2017](99-references.html#ref-BeelenEtc2017)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 沿用英国的做法，加拿大议会的发言记录被称为“Hansard”。它不是完全逐字记录，但非常接近。它可以从[LiPaD](https://www.lipad.ca)以CSV格式获得，该格式由Beelen等人构建（[2017](99-references.html#ref-BeelenEtc2017)）。
- en: We are interested in what was talked about in the Canadian parliament in 2018\.
    To get started we can download the entire corpus from [here](https://www.lipad.ca/data/),
    and then discard all of the years apart from 2018\. If the datasets are in a folder
    called “2018”, we can use `read_csv()` to read and combine all the CSVs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对2018年加拿大议会的讨论内容感兴趣。为了开始，我们可以从[这里](https://www.lipad.ca/data/)下载整个语料库，然后丢弃除2018年之外的所有年份。如果数据集在一个名为“2018”的文件夹中，我们可以使用
    `read_csv()` 来读取和合并所有的CSV文件。
- en: '[PRE43]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '*[PRE44]'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE44]'
- en: The use of `filter()` at the end is needed because sometimes aspects such as
    “directions” and similar non-speech aspects are included in the Hansard. For instance,
    if we do not include that `filter()` then the first line is “The House resumed
    from November 9, 2017, consideration of the motion.” We can then construct a corpus.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后使用 `filter()` 是必要的，因为有时“directions”和类似的非语音方面会被包含在汉萨德（Hansard）中。例如，如果我们不包括那个
    `filter()`，那么第一行将是“The House resumed from November 9, 2017, consideration of the
    motion.” 然后，我们可以构建一个语料库。
- en: '[PRE45]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '*[PRE46]'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE46]'
- en: '[PRE47]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '*[PRE48]**  **We use the tokens in the corpus to construct a document-feature
    matrix. To make our life a little easier, computationally, we remove any word
    that does not occur at least twice, and any word that does not occur in at least
    two documents.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE48]**  **我们使用语料库中的标记来构建一个文档-特征矩阵。为了使我们的计算工作稍微容易一些，我们移除了至少没有出现两次的任何单词，以及至少没有出现在至少两个文档中的任何单词。'
- en: '[PRE49]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '*[PRE50]*  *At this point we can use `stm()` from `stm` to implement a LDA
    model. We need to specify a document-feature matrix and the number of topics.
    Topic models are essentially just summaries. Instead of a document becoming a
    collection of words, they become a collection of topics with some probability
    associated with each topic. But because it is just providing a collection of words
    that tend to be used at similar times, rather than actual underlying meaning,
    we need to specify the number of topics that we are interested in. This decision
    will have a big impact, and we should consider a few different numbers.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE50]*  *在这个时候，我们可以使用 `stm` 中的 `stm()` 来实现LDA模型。我们需要指定一个文档-特征矩阵和主题的数量。主题模型本质上只是摘要。与文档成为单词的集合不同，它们成为具有与每个主题相关概率的主题的集合。但是，因为它只是提供了一组在相似时间使用的单词，而不是实际的下层含义，所以我们需要指定我们感兴趣的主题数量。这个决定将产生重大影响，我们应该考虑几个不同的数字。'
- en: '[PRE51]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '*This will take some time, likely 15-30 minutes, so it is useful to save the
    model when it is done using `write_rds()`, and use `beep` to get a notification
    when it is done. We could then read the results back in with `read_rds()`.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*这将需要一些时间，可能15-30分钟，所以当模型完成时使用 `write_rds()` 保存模型是有用的，并且使用 `beep` 来获取完成的提醒。然后我们可以使用
    `read_rds()` 读取结果。'
- en: '[PRE52]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '*We can look at the words in each topic with `labelTopics()`.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们可以使用 `labelTopics()` 来查看每个主题中的单词。'
- en: '[PRE53]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '*[PRE54]*******  ***## 17.5 Exercises'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE54]*******  ***## 17.5 练习'
- en: Practice
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习
- en: '*(Plan)* Consider the following scenario: *You run a news website and are trying
    to understand whether to allow anonymous comments. You decide to do a A/B test,
    where we keep everything the same, but only allow anonymous comments on one version
    of the site. All you will have to decide is the text data that you obtain from
    the test.* Please sketch out what that dataset could look like and then sketch
    a graph that you could build to show all observations.'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*(Plan)* 考虑以下场景：*你运营一个新闻网站，并试图了解是否允许匿名评论。你决定进行A/B测试，其中我们保持一切不变，但只允许网站的一个版本允许匿名评论。你唯一需要决定的是从测试中获得文本数据。*
    请绘制出这个数据集可能的样子，然后绘制一个图表来展示所有的观察结果。'
- en: '*(Simulate)* Please further consider the scenario described and simulate the
    situation. Please include at least ten tests based on the simulated data.'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*(Simulate)* 请进一步考虑所描述的场景，并模拟这种情况。请至少包含基于模拟数据的十个测试。'
- en: '*(Acquire)* Please describe one possible source of such a dataset.'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*(Acquire)* 请描述这种数据集的一个可能来源。'
- en: '*(Explore)* Please use `ggplot2` to build the graph that you sketched. Use
    `rstanarm` to build a model.'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*(Explore)* 请使用 `ggplot2` 构建你草图中的图形。使用 `rstanarm` 构建模型。'
- en: '*(Communicate)* Please write two paragraphs about what you did.'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*(Communicate)* 请写两段关于你所做的工作的描述。'
- en: Quiz
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测验
- en: Which argument to `str_replace_all()` would remove punctuation?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`str_replace_all()` 的哪个参数可以移除标点符号？'
- en: “[:punct:]”
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “[:punct:]”
- en: “[:digit:]”
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “[:digit:]”
- en: “[:alpha:]”
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “[:alpha:]”
- en: “[:lower:]”
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “[:lower:]”
- en: Change `stopwords(source = "snowball")[1:10]` to find the ninth stopword in
    the “nltk” list.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `stopwords(source = "snowball")[1:10]` 修改为找到“nltk”列表中的第九个停用词。
- en: “her”
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “her”
- en: “my”
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “my”
- en: “you”
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “you”
- en: “i”
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “i”
- en: Which function from quanteda() will tokenize a corpus?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: quanteda() 中哪个函数可以标记语料库？
- en: '`tokenizer()`'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokenizer()`'
- en: '`token()`'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`token()`'
- en: '`tokenize()`'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokenize()`'
- en: '`tokens()`'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokens()`'
- en: Which argument to `dfm_trim()` should be used if we want to only include terms
    that occur at least twice? = 2)
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们只想包括至少出现两次的术语，应该使用 `dfm_trim()` 的哪个参数？ = 2)
- en: “min_wordfreq”
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “min_wordfreq”
- en: “min_termfreq”
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “min_termfreq”
- en: “min_term_occur”
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “min_term_occur”
- en: “min_ occurrence”
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: “min_ occurrence”
- en: What is your favorite example of a tri-gram?
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你最喜欢的三元组示例是什么？
- en: What is the second-most common word used in the zodiac signs for Cancer?
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在巨蟹座占星术中，第二个最常见的单词是什么？
- en: to
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到
- en: your
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的
- en: the
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: the
- en: you
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你
- en: What is the sixth-most common word used in the zodiac signs for Pisces, that
    is unique to that sign?
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在双鱼座占星术中，第六个最常见的单词是什么，这个单词只属于双鱼座？
- en: shoes
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鞋子
- en: prayer
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 祈祷
- en: fishes
  id: totrans-196
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 鱼类
- en: pisces
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: pisces
- en: Re-run the Canadian topic model, but only including five topics. Looking at
    the words in each topic, how would you describe what each of them is about?
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新运行加拿大主题模型，但只包括五个主题。观察每个主题中的单词，你会如何描述每个主题的内容？
- en: Class activities
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 课堂活动
- en: Do children learn “dog”, “cat”, or “bird” first? Use the Wordbank database.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 孩子们是先学会“狗”、“猫”还是“鸟”？使用 Wordbank 数据库。
- en: Task
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务
- en: Please follow the code of Hvitfeldt and Silge ([2021](99-references.html#ref-hvitfeldt2021supervised))
    in *Supervised Machine Learning for Text Analysis in R*, Chapter 5.2 “Understand
    word embeddings by finding them yourself”, freely available [here](https://smltar.com/embeddings.html),
    to implement your own word embeddings for one year’s worth of data from [LiPaD](https://www.lipad.ca).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 请遵循 Hvitfeldt 和 Silge 在《R 中的文本分析监督机器学习》第五章2.2节“通过自己找到它们来理解词嵌入”中的代码（[2021](99-references.html#ref-hvitfeldt2021supervised)），该章节内容可在[这里](https://smltar.com/embeddings.html)免费获取，以实现从
    [LiPaD](https://www.lipad.ca) 一年的数据中创建自己的词嵌入。
- en: 'Alexopoulos, Michelle, and Jon Cohen. 2015\. “The power of print: Uncertainty
    shocks, markets, and the economy.” *International Review of Economics & Finance*
    40 (November): 8–28\. [https://doi.org/10.1016/j.iref.2015.02.002](https://doi.org/10.1016/j.iref.2015.02.002).Amaka,
    Ofunne, and Amber Thomas. 2021\. “The Naked Truth: How the Names of 6,816 Complexion
    Products Can Reveal Bias in Beauty.” *The Pudding*, March. [https://pudding.cool/2021/03/foundation-names/](https://pudding.cool/2021/03/foundation-names/).Arel-Bundock,
    Vincent. 2024\. *tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,”
    “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats*. [https://vincentarelbundock.github.io/tinytable/](https://vincentarelbundock.github.io/tinytable/).Bååth,
    Rasmus. 2018\. *beepr: Easily Play Notification Sounds on any Platform*. [https://CRAN.R-project.org/package=beepr](https://CRAN.R-project.org/package=beepr).Bandy,
    John, and Nicholas Vincent. 2021\. “Addressing ‘Documentation Debt’ in Machine
    Learning: A Retrospective Datasheet for BookCorpus.” In *Proceedings of the Neural
    Information Processing Systems Track on Datasets and Benchmarks*, edited by J.
    Vanschoren and S. Yeung. Vol. 1\. [https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/54229abfcfa5649e7003b83dd4755294-Paper-round1.pdf](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/54229abfcfa5649e7003b83dd4755294-Paper-round1.pdf).Beelen,
    Kaspar, Timothy Alberdingk Thim, Christopher Cochrane, Kees Halvemaan, Graeme
    Hirst, Michael Kimmins, Sander Lijbrink, et al. 2017\. “Digitization of the Canadian
    Parliamentary Debates.” *Canadian Journal of Political Science* 50 (3): 849–64.Benoit,
    Kenneth. 2020\. “Text as Data: An Overview.” In *The SAGE Handbook of Research
    Methods in Political Science and International Relations*, edited by Luigi Curini
    and Robert Franzese, 461–97\. London: SAGE Publishing. [https://doi.org/10.4135/9781526486387.n29](https://doi.org/10.4135/9781526486387.n29).Benoit,
    Kenneth, and Michael Laver. 2006\. *Party Policy in Modern Democracies*. Routledge.———.
    2007\. “Estimating Party Policy Positions: Comparing Expert Surveys and Hand-Coded
    Content Analysis.” *Electoral Studies* 26 (1): 90–107\. [https://doi.org/10.1016/j.electstud.2006.04.008](https://doi.org/10.1016/j.electstud.2006.04.008).Benoit,
    Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and
    Akitaka Matsuo. 2018\. “quanteda: An R package for the quantitative analysis of
    textual data.” *Journal of Open Source Software* 3 (30): 774\. [https://doi.org/10.21105/joss.00774](https://doi.org/10.21105/joss.00774).Blei,
    David. 2012\. “Probabilistic Topic Models.” *Communications of the ACM* 55 (4):
    77–84\. [https://doi.org/10.1145/2133806.2133826](https://doi.org/10.1145/2133806.2133826).Blei,
    David, Andrew Ng, and Michael Jordan. 2003\. “Latent Dirichlet Allocation.” *Journal
    of Machine Learning Research* 3 (Jan): 993–1022\. [https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf).Davidson,
    Thomas, Debasmita Bhattacharya, and Ingmar Weber. 2019\. “Racial Bias in Hate
    Speech and Abusive Language Detection Datasets.” In *Proceedings of the Third
    Workshop on Abusive Language Online*, 25–35.Gelfand, Sharla. 2022\. *Astrologer:
    Chani Nicholas Weekly Horoscopes (2013-2017)*. [http://github.com/sharlagelfand/astrologer](http://github.com/sharlagelfand/astrologer).Grimmer,
    Justin, Margaret Roberts, and Brandon Stewart. 2022\. *Text As Data: A New Framework
    for Machine Learning and the Social Sciences*. New Jersey: Princeton University
    Press.Gronsbell, Jessica, Jessica Minnier, Sheng Yu, Katherine Liao, and Tianxi
    Cai. 2019\. “Automated Feature Selection of Predictors in Electronic Medical Records
    Data.” *Biometrics* 75 (1): 268–77\. [https://doi.org/10.1111/biom.12987](https://doi.org/10.1111/biom.12987).Grün,
    Bettina, and Kurt Hornik. 2011\. “topicmodels: An R Package for Fitting Topic
    Models.” *Journal of Statistical Software* 40 (13): 1–30\. [https://doi.org/10.18637/jss.v040.i13](https://doi.org/10.18637/jss.v040.i13).Hassan,
    Mai. 2022\. “New Insights on Africa’s Autocratic Past.” *African Affairs* 121
    (483): 321–33\. [https://doi.org/10.1093/afraf/adac002](https://doi.org/10.1093/afraf/adac002).Hester,
    Jim, Hadley Wickham, and Gábor Csárdi. 2021\. *fs: Cross-Platform File System
    Operations Based on “libuv”*. [https://CRAN.R-project.org/package=fs](https://CRAN.R-project.org/package=fs).Hvitfeldt,
    Emil, and Julia Silge. 2021\. *Supervised Machine Learning for Text Analysis in
    R*. 1st ed. Chapman; Hall/CRC. [https://doi.org/10.1201/9781003093459](https://doi.org/10.1201/9781003093459).Johnston,
    Myfanwy, and David Robinson. 2022\. *gutenbergr: Download and Process Public Domain
    Works from Project Gutenberg*. [https://CRAN.R-project.org/package=gutenbergr](https://CRAN.R-project.org/package=gutenbergr).Jurafsky,
    Dan, and James Martin. (2000) 2023\. *Speech and Language Processing*. 3rd ed.
    [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/).Kalamara,
    Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. 2022\.
    “Making text count: Economic forecasting using newspaper text.” *Journal of Applied
    Econometrics* 37 (5): 896–919\. [https://doi.org/10.1002/jae.2907](https://doi.org/10.1002/jae.2907).Koenecke,
    Allison, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor
    Toups, John Rickford, Dan Jurafsky, and Sharad Goel. 2020\. “Racial Disparities
    in Automated Speech Recognition.” *Proceedings of the National Academy of Sciences*
    117 (14): 7684–89\. [https://doi.org/10.1073/pnas.1915768117](https://doi.org/10.1073/pnas.1915768117).Laver,
    Michael, Kenneth Benoit, and John Garry. 2003\. “Extracting Policy Positions from
    Political Texts Using Words as Data.” *American Political Science Review* 97 (2):
    311–31\. [https://doi.org/10.1017/S0003055403000698](https://doi.org/10.1017/S0003055403000698).Miller,
    Michael, and Joseph Sutherland. 2022\. “The Effect of Gender on Interruptions
    at Congressional Hearings.” *American Political Science Review*, 1–19\. [https://doi.org/10.1017/S0003055422000260](https://doi.org/10.1017/S0003055422000260).R
    Core Team. 2024\. *R: A Language and Environment for Statistical Computing*. Vienna,
    Austria: R Foundation for Statistical Computing. [https://www.R-project.org/](https://www.R-project.org/).Roberts,
    Margaret, Brandon Stewart, and Dustin Tingley. 2019\. “stm: An R Package for Structural
    Topic Models.” *Journal of Statistical Software* 91 (2): 1–40\. [https://doi.org/10.18637/jss.v091.i02](https://doi.org/10.18637/jss.v091.i02).Schofield,
    Alexandra, Måns Magnusson, and David Mimno. 2017\. “Pulling Out the Stops: Rethinking
    Stopword Removal for Topic Models.” In *Proceedings of the 15th Conference of
    the European Chapter of the Association for Computational Linguistics: Volume
    2, Short Papers*, 432–36\. Valencia, Spain: Association for Computational Linguistics.
    [https://aclanthology.org/E17-2069](https://aclanthology.org/E17-2069).Schofield,
    Alexandra, Måns Magnusson, Laure Thompson, and David Mimno. 2017\. “Understanding
    Text Pre-Processing for Latent Dirichlet Allocation.” In *ACL Workshop for Women
    in NLP (WiNLP)*. [https://www.cs.cornell.edu/~xanda/winlp2017.pdf](https://www.cs.cornell.edu/~xanda/winlp2017.pdf).Schofield,
    Alexandra, Laure Thompson, and David Mimno. 2017\. “Quantifying the Effects of
    Text Duplication on Semantic Models.” In *Proceedings of the 2017 Conference on
    Empirical Methods in Natural Language Processing*, 2737–47\. Copenhagen, Denmark:
    Association for Computational Linguistics. [https://doi.org/10.18653/v1/D17-1290](https://doi.org/10.18653/v1/D17-1290).Silge,
    Julia, and David Robinson. 2016\. “tidytext: Text Mining and Analysis Using Tidy
    Data Principles in R.” *The Journal of Open Source Software* 1 (3). [https://doi.org/10.21105/joss.00037](https://doi.org/10.21105/joss.00037).Steyvers,
    Mark, and Tom Griffiths. 2006\. “Probabilistic Topic Models.” In *Latent Semantic
    Analysis: A Road to Meaning*, edited by T. Landauer, D McNamara, S. Dennis, and
    W. Kintsch. [https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf](https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf).Wickham,
    Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain
    François, Garrett Grolemund, et al. 2019\. “Welcome to the Tidyverse.” *Journal
    of Open Source Software* 4 (43): 1686\. [https://doi.org/10.21105/joss.01686](https://doi.org/10.21105/joss.01686).***********'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 'Alexopoulos, Michelle, and Jon Cohen. 2015\. “The power of print: Uncertainty
    shocks, markets, and the economy.” *International Review of Economics & Finance*
    40 (November): 8–28\. [https://doi.org/10.1016/j.iref.2015.02.002](https://doi.org/10.1016/j.iref.2015.02.002).Amaka,
    Ofunne, and Amber Thomas. 2021\. “The Naked Truth: How the Names of 6,816 Complexion
    Products Can Reveal Bias in Beauty.” *The Pudding*, March. [https://pudding.cool/2021/03/foundation-names/](https://pudding.cool/2021/03/foundation-names/).Arel-Bundock,
    Vincent. 2024\. *tinytable: Simple and Configurable Tables in “HTML,” “LaTeX,”
    “Markdown,” “Word,” “PNG,” “PDF,” and “Typst” Formats*. [https://vincentarelbundock.github.io/tinytable/](https://vincentarelbundock.github.io/tinytable/).Bååth,
    Rasmus. 2018\. *beepr: Easily Play Notification Sounds on any Platform*. [https://CRAN.R-project.org/package=beepr](https://CRAN.R-project.org/package=beepr).Bandy,
    John, and Nicholas Vincent. 2021\. “Addressing ‘Documentation Debt’ in Machine
    Learning: A Retrospective Datasheet for BookCorpus.” In *Proceedings of the Neural
    Information Processing Systems Track on Datasets and Benchmarks*, edited by J.
    Vanschoren and S. Yeung. Vol. 1\. [https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/54229abfcfa5649e7003b83dd4755294-Paper-round1.pdf](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/54229abfcfa5649e7003b83dd4755294-Paper-round1.pdf).Beelen,
    Kaspar, Timothy Alberdingk Thim, Christopher Cochrane, Kees Halvemaan, Graeme
    Hirst, Michael Kimmins, Sander Lijbrink, et al. 2017\. “Digitization of the Canadian
    Parliamentary Debates.” *Canadian Journal of Political Science* 50 (3): 849–64.Benoit,
    Kenneth. 2020\. “Text as Data: An Overview.” In *The SAGE Handbook of Research
    Methods in Political Science and International Relations*, edited by Luigi Curini
    and Robert Franzese, 461–97\. London: SAGE Publishing. [https://doi.org/10.4135/9781526486387.n29](https://doi.org/10.4135/9781526486387.n29).Benoit,
    Kenneth, and Michael Laver. 2006\. *Party Policy in Modern Democracies*. Routledge.———.
    2007\. “Estimating Party Policy Positions: Comparing Expert Surveys and Hand-Coded
    Content Analysis.” *Electoral Studies* 26 (1): 90–107\. [https://doi.org/10.1016/j.electstud.2006.04.008](https://doi.org/10.1016/j.electstud.2006.04.008).Benoit,
    Kenneth, Kohei Watanabe, Haiyan Wang, Paul Nulty, Adam Obeng, Stefan Müller, and
    Akitaka Matsuo. 2018\. “quanteda: An R package for the quantitative analysis of
    textual data.” *Journal of Open Source Software* 3 (30): 774\. [https://doi.org/10.21105/joss.00774](https://doi.org/10.21105/joss.00774).Blei,
    David. 2012\. “Probabilistic Topic Models.” *Communications of the ACM* 55 (4):
    77–84\. [https://doi.org/10.1145/2133806.2133826](https://doi.org/10.1145/2133806.2133826).Blei,
    David, Andrew Ng, and Michael Jordan. 2003\. “Latent Dirichlet Allocation.” *Journal
    of Machine Learning Research* 3 (Jan): 993–1022\. [https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf).Davidson,
    Thomas, Debasmita Bhattacharya, and Ingmar Weber. 2019\. “Racial Bias in Hate
    Speech and Abusive Language Detection Datasets.” In *Proceedings of the Third
    Workshop on Abusive Language Online*, 25–35.Gelfand, Sharla. 2022\. *Astrologer:
    Chani Nicholas Weekly Horoscopes (2013-2017)*. [http://github.com/sharlagelfand/astrologer](http://github.com/sharlagelfand/astrologer).Grimmer,
    Justin, Margaret Roberts, and Brandon Stewart. 2022\. *Text As Data: A New Framework
    for Machine Learning and the Social Sciences*. New Jersey: Princeton University
    Press.Gronsbell, Jessica, Jessica Minnier, Sheng Yu, Katherine Liao, and Tianxi
    Cai. 2019\. “Automated Feature Selection of Predictors in Electronic Medical Records
    Data.” *Biometrics* 75 (1): 268–77\. [https://doi.org/10.1111/biom.12987](https://doi.org/10.1111/biom.12987).Grün,
    Bettina, and Kurt Hornik. 2011\. “topicmodels: An R Package for Fitting Topic
    Models.” *Journal of Statistical Software* 40 (13): 1–30\. [https://doi.org/10.18637/jss.v040.i13](https://doi.org/10.18637/jss.v040.i13).Hassan,
    Mai. 2022\. “New Insights on Africa’s Autocratic Past.” *African Affairs* 121
    (483): 321–33\. [https://doi.org/10.1093/afraf/adac002](https://doi.org/10.1093/afraf/adac002).Hester,
    Jim, Hadley Wickham, and Gábor Csárdi. 2021\. *fs: Cross-Platform File System
    Operations Based on “libuv”*. [https://CRAN.R-project.org/package=fs](https://CRAN.R-project.org/package=fs).Hvitfeldt,
    Emil, and Julia Silge. 2021\. *Supervised Machine Learning for Text Analysis in
    R*. 1st ed. Chapman; Hall/CRC. [https://doi.org/10.1201/9781003093459](https://doi.org/10.1201/9781003093459).Johnston,
    Myfanwy, and David Robinson. 2022\. *gutenbergr: Download and Process Public Domain
    Works from Project Gutenberg*. [https://CRAN.R-project.org/package=gutenbergr](https://CRAN.R-project.org/package=gutenbergr).Jurafsky,
    Dan, and James Martin. (2000) 2023\. *Speech and Language Processing*. 3rd ed.
    [https://web.stanford.edu/~jurafsky/slp3/](https://web.stanford.edu/~jurafsky/slp3/).Kalamara,
    Eleni, Arthur Turrell, Chris Redl, George Kapetanios, and Sujit Kapadia. 2022\.
    “Making text count: Economic forecasting using newspaper text.” *Journal of Applied
    Econometrics* 37 (5): 896–919\. [https://doi.org/10.1002/jae.2907](https://doi.org/10.1002/jae.2907).Koenecke,
    Allison, Andrew Nam, Emily Lake, Joe Nudell, Minnie Quartey, Zion Mengesha, Connor
    Toups, John Rickford, Dan Jurafsky, and Sharad Goel. 2020\. “Racial Disparities
    in Automated Speech Recognition.” *Proceedings of the National Academy of Sciences*
    117 (14): 7684–89\. [https://doi.org/10.1073/pnas.1915768117](https://doi.org/10.1073/pnas.1915768117).Laver,
    Michael, Kenneth Benoit, and John Garry. 2003\. “Extracting Policy Positions from
    Political Texts Using Words as Data.” *American Political Science Review* 97 (2):
    311–31\. [https://doi.org/10.1017/S0003055403000698](https://doi.org/10.1017/S0003055403000698).Miller,
    Michael, and Joseph Sutherland. 2022\. “The Effect of Gender on Interruptions
    at Congressional Hearings.” *American Political Science Review*, 1–19\. [https://doi.org/10.1017/S0003055422000260](https://doi.org/10.1017/S0003055422000260).R
    Core Team. 2024\. *R: A Language and Environment for Statistical Computing*. Vienna,
    Austria: R Foundation for Statistical Computing. [https://www.R-project.org/](https://www.R-project.org/).Roberts,
    Margaret, Brandon Stewart, and Dustin Tingley. 2019\. “stm: An R Package for Structural
    Topic Models.” *Journal of Statistical Software* 91 (2): 1–40\. [https://doi.org/10.18637/jss.v091.i02](https://doi.org/10.18637/jss.v091.i02).Schofield,
    Alexandra, Måns Magnusson, and David Mimno. 2017\. “Pulling Out the Stops: Rethinking
    Stopword Removal for Topic Models.” In *Proceedings of the 15th Conference of
    the European Chapter of the Association for Computational Linguistics: Volume
    2, Short Papers*, 432–36\. Valencia, Spain: Association for Computational Linguistics.
    [https://aclanthology.org/E17-2069](https://aclanthology.org/E17-2069).Schofield,
    Alexandra, Måns Magnusson, Laure Thompson, and David Mimno. 2017\. “Understanding
    Text Pre-Processing for Latent Dirichlet Allocation.” In *ACL Workshop for Women
    in NLP (WiNLP)*. [https://www.cs.cornell.edu/~xanda/winlp2017.pdf](https://www.cs.cornell.edu/~xanda/winlp2017.pdf).Schofield,
    Alexandra, Laure Thompson, and David Mimno. 2017\. “Quantifying the Effects of
    Text Duplication on Semantic Models.” In *Proceedings of the 2017 Conference on
    Empirical Methods in Natural Language Processing*, 2737–47\. Copenhagen, Denmark:
    Association for Computational Linguistics. [https://doi.org/10.18653/v1/D17-1290](https://doi.org/10.18653/v1/D17-1290).Silge,
    Julia, and David Robinson. 2016\. “tidytext: Text Mining and Analysis Using Tidy
    Data Principles in R.” *The Journal of Open Source Software* 1 (3). [https://doi.org/10.21105/joss.00037](https://doi.org/10.21105/joss.00037).Steyvers,
    Mark, and Tom Griffiths. 2006\. “Probabilistic Topic Models.” In *Latent Semantic
    Analysis: A Road to Meaning*, edited by T. Landauer, D McNamara, S. Dennis, and
    W. Kintsch. [https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf](https://cocosci.princeton.edu/tom/papers/SteyversGriffiths.pdf).Wickham,
    Hadley, Mara Averick, Jenny Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain
    François, Garrett Grolemund, et al. 2019\. “Welcome to the Tidyverse.” *Journal
    of Open Source Software* 4 (43): 1686\. [https://doi.org/10.21105/joss.01686](https://doi.org/10.21105/joss.01686).'
