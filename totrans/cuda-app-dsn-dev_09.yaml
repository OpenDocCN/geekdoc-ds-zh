- en: Chapter 8\. CUDA for All GPU and CPU ApplicationsSoftware development takes
    time and costs money. CUDA has evolved from a solid platform to accelerate numerical
    computation into a platform that is appropriate for *all* application development.
    What this means is that a single source tree of CUDA code can support applications
    that run exclusively on conventional x86 processors, exclusively on GPU hardware,
    or as hybrid applications that simultaneously use all the CPU and GPU devices
    in a system to achieve maximal performance. The Portland Group, Inc. (PGI) native
    CUDA-x86 compiler is one realization of this maturation process that has made
    CUDA C/C++ a viable source platform for generic application development, just
    like C++ and Java. Unlike Java and other popular application languages, CUDA can
    efficiently support tens of thousands of concurrent threads of execution. CUDA
    also supports the conventional approach to cross-language development that uses
    language bindings to interface with existing languages. As with other languages,
    libraries simplify application development and handle commonly used methods such
    as linear algebra, matrix operations, and the Fast Fourier Transform (FFT). Dynamic
    compilation is also blurring the distinction between CUDA and other languages,
    as exemplified by the Copperhead project (discussed shortly), which lets Python
    programmers write their code entirely in Python. The Copperhead runtime then dynamically
    compiles the Python methods to run on CUDA-enabled hardware.**Keywords**Libraries,
    x86, ROI (Return on Investment), Python, R, Java, PGI (the Portland Group)Software
    development takes time and costs money. CUDA has evolved from a solid platform
    to accelerate numerical computation into a platform that is appropriate for *all*
    application development. What this means is that a single source tree of CUDA
    code can support applications that run exclusively on conventional x86 processors,
    exclusively on GPU hardware, or as hybrid applications that simultaneously use
    all the CPU and GPU devices in a system to achieve maximal performance. The Portland
    Group, Inc. (PGI) [¹](#fn0015) native CUDA-x86 compiler is one realization of
    this maturation process that has made CUDA C/C++ a viable source platform for
    generic application development, just like C++ and Java. Unlike Java and other
    popular application languages, CUDA can efficiently support tens of thousands
    of concurrent threads of execution. CUDA also supports the conventional approach
    to cross-language development that uses language bindings to interface with existing
    languages. As with other languages, libraries simplify application development
    and handle commonly used methods such as linear algebra, matrix operations, and
    the Fast Fourier Transform (FFT). Dynamic compilation is also blurring the distinction
    between CUDA and other languages, as exemplified by the Copperhead project (discussed
    shortly), which lets Python programmers write their code entirely in Python. The
    Copperhead runtime then dynamically compiles the Python methods to run on CUDA-enabled
    hardware.¹[http://www.pgroup.com/](http://www.pgroup.com/).At the end of this
    chapter, the reader will have a basic understanding of:■ Tools to transparently
    build and run CUDA applications on non-GPU systems and GPUs manufactured by any
    vendor.■ The Copperhead project, which dynamically compiles Python for CUDA execution.■
    How to incorporate CUDA with most languages, including Python, FORTRAN, R, Java,
    and others.■ Important numerical libraries such as CUBLAS, CUFFT, and MAGMA.■
    How to use CUFFT concurrently on multiple GPUs.■ CURAND and problems with naïve
    approaches to random number generation.■ The effect of PCIe hardware on multi-GPU
    applications.
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章 CUDA用于所有GPU和CPU应用程序软件开发需要时间并且耗费金钱。CUDA已经从一个加速数值计算的稳固平台发展成一个适合*所有*应用程序开发的平台。这意味着，单一的CUDA代码源树可以支持仅在传统x86处理器上运行的应用程序，仅在GPU硬件上运行的应用程序，或者作为混合应用程序，能够同时使用系统中所有的CPU和GPU设备以实现最大性能。波特兰集团公司（PGI）本地的CUDA-x86编译器是这一成熟过程的体现之一，使得CUDA
    C/C++成为一种可行的通用应用程序开发源平台，就像C++和Java一样。与Java和其他流行的应用语言不同，CUDA可以高效地支持成千上万的并发执行线程。CUDA还支持传统的跨语言开发方法，使用语言绑定与现有语言进行接口。与其他语言一样，库简化了应用程序开发，并处理常用的数学方法，如线性代数、矩阵运算和快速傅里叶变换（FFT）。动态编译也模糊了CUDA与其他语言之间的区别，正如Copperhead项目所示（稍后讨论），该项目允许Python程序员完全用Python编写代码，然后Copperhead运行时将Python方法动态编译为在CUDA启用的硬件上运行。**关键词**
    库，x86，ROI（投资回报率），Python，R，Java，PGI（波特兰集团）软件开发需要时间并且耗费金钱。CUDA已经从一个加速数值计算的稳固平台发展成一个适合*所有*应用程序开发的平台。这意味着，单一的CUDA代码源树可以支持仅在传统x86处理器上运行的应用程序，仅在GPU硬件上运行的应用程序，或者作为混合应用程序，能够同时使用系统中所有的CPU和GPU设备以实现最大性能。波特兰集团公司（PGI）本地的CUDA-x86编译器是这一成熟过程的体现之一，使得CUDA
    C/C++成为一种可行的通用应用程序开发源平台，就像C++和Java一样。与Java和其他流行的应用语言不同，CUDA可以高效地支持成千上万的并发执行线程。CUDA还支持传统的跨语言开发方法，使用语言绑定与现有语言进行接口。与其他语言一样，库简化了应用程序开发，并处理常用的数学方法，如线性代数、矩阵运算和快速傅里叶变换（FFT）。动态编译也模糊了CUDA与其他语言之间的区别，正如Copperhead项目所示（稍后讨论），该项目允许Python程序员完全用Python编写代码，然后Copperhead运行时将Python方法动态编译为在CUDA启用的硬件上运行。¹[http://www.pgroup.com/](http://www.pgroup.com/)。在本章结束时，读者将对以下内容有基本的理解：■
    透明构建和运行CUDA应用程序的工具，无论是在非GPU系统上还是在任何厂商制造的GPU上。■ Copperhead项目，动态编译Python以执行CUDA。■
    如何将CUDA与大多数语言结合使用，包括Python、FORTRAN、R、Java等。■ 重要的数值库，如CUBLAS、CUFFT和MAGMA。■ 如何在多个GPU上并发使用CUFFT。■
    CURAND以及天真的随机数生成方法的问题。■ PCIe硬件对多GPU应用程序的影响。
- en: Pathways from CUDA to Multiple Hardware Backends
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从CUDA到多个硬件后端的路径
- en: '[Example 8.1](#tb0010) illustrates the paths that are currently available to
    run CUDA C/C++ on x86 and GPU hardware. These are capabilities that give CUDA
    programmers the ability—with a single source tree—to create applications that
    can reach both those customers who own the third of a billion CUDA-enabled GPUs
    sold to date as well as the massive base of customers who already own x86-based
    systems. [Example 8.1](#tb0010), “Pathway to use CUDA source code on CPUs, GPUs,
    and other vendor GPUs.”'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 8.1](#tb0010)展示了当前可用于在x86和GPU硬件上运行CUDA C/C++的路径。这些功能使得CUDA程序员能够通过一个源代码树创建应用程序，既可以面向已经购买了三亿多个CUDA支持GPU的客户，也可以面向已经拥有x86系统的大量客户。[示例
    8.1](#tb0010)，“在CPU、GPU和其他供应商GPU上使用CUDA源代码的路径”。'
- en: '| ![B9780123884268000082/u08-01-9780123884268.jpg is missing](B9780123884268000082/u08-01-9780123884268.jpg)
    |'
  id: totrans-3
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000082/u08-01-9780123884268.jpg is missing](B9780123884268000082/u08-01-9780123884268.jpg)
    |'
- en: The PGI CUDA x86 Compiler
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PGI CUDA x86 编译器
- en: The concept behind native x86 compilation is to give CUDA programmers the ability—with
    a single source tree—to create applications that can reach most customers with
    a computer. When released, the PGI unified binary will greatly simplify product
    support and delivery as a single application binary can be shipped to customers.
    At runtime, the PGI unified binary will query the hardware and selectively run
    on any CUDA-enabled GPUs or the host multicore processor.What distinguishes the
    PGI effort from source translators such as the Swan[²](#fn0020) CUDA to OpenCL
    translator; the MCUDA[³](#fn0025) CUDA to C translator; the Ocelot[⁴](#fn0030)
    open source emulator and PTX translation project; and the ability of the NVIDIA
    **nvcc** compiler to generate both x86 and GPU based code is:²[http://www.multiscalelab.org/swan](http://www.multiscalelab.org/swan).³[http://impact.crhc.illinois.edu/mcuda.php](http://impact.crhc.illinois.edu/mcuda.php).⁴[http://code.google.com/p/gpuocelot/](http://code.google.com/p/gpuocelot/).■
    **Speed:** The PGI CUDA C/C++ compiler is a native compiler that transparently
    compiles CUDA to run on x86 systems even when a GPU is not present in the system.
    This gives the compiler the opportunity to perform x86 specific optimization to
    best use the multiple cores of the x86 processor as well as the SIMD parallelism
    in the AVX or SSE instructions within each core. (AVX is an extension of SSE to
    256-bit operation.)■ **Transparency:** Both NVIDIA and PGI state that even CUDA
    applications utilizing proprietary features of the GPU texture units will exhibit
    identical behavior on both x86 and GPU hardware.■ **Convenience:** In 2012, the
    PGI compiler will be able to create a unified binary, which will simplify the
    software distribution process tremendously as mentioned previously. The simplicity
    of shipping a single binary to customers reflects the completeness of the thought
    behind the PGI CUDA C/C++ project.From a planning perspective, CUDA for x86 dramatically
    impacts the software development decision-making process. Rather than CUDA filling
    the role of a niche development platform for GPU-based products, CUDA is now a
    platform for all product development—even for applications that are not intended
    to be accelerated by GPUs!The motivation is clearly exemplified by the variety
    and number of projects on the NVIDIA showcase that have achieved 100 times or
    greater performance over commodity processors, as summarized in [Figure 8.1](#f0010).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本地x86编译的概念是为CUDA程序员提供一种能力——通过一个单一的源代码树——来创建可以覆盖大多数计算机用户的应用程序。一旦发布，PGI统一二进制将大大简化产品支持和交付，因为单一的应用程序二进制可以直接交付给客户。在运行时，PGI统一二进制将查询硬件，并根据需要选择性地在任何支持CUDA的GPU或主机多核处理器上运行。PGI的努力与源代码翻译器，如Swan[²](#fn0020)
    CUDA到OpenCL翻译器；MCUDA[³](#fn0025) CUDA到C翻译器；Ocelot[⁴](#fn0030)开源模拟器和PTX翻译项目；以及NVIDIA
    **nvcc**编译器能够生成x86和GPU代码之间的区别在于：²[http://www.multiscalelab.org/swan](http://www.multiscalelab.org/swan)。³[http://impact.crhc.illinois.edu/mcuda.php](http://impact.crhc.illinois.edu/mcuda.php)。⁴[http://code.google.com/p/gpuocelot/](http://code.google.com/p/gpuocelot/)。■
    **速度：** PGI CUDA C/C++编译器是一个本地编译器，可以透明地将CUDA编译为在x86系统上运行，即使系统中没有GPU。这使得编译器有机会执行x86特定的优化，最好地利用x86处理器的多个核心以及每个核心中的AVX或SSE指令中的SIMD并行性。（AVX是SSE的256位操作扩展。）■
    **透明性：** NVIDIA和PGI都声明，即使是利用GPU纹理单元专有特性的CUDA应用程序，在x86和GPU硬件上也会表现出相同的行为。■ **便利性：**
    到2012年，PGI编译器将能够创建统一的二进制文件，这将大大简化软件分发过程。如前所述，将单一的二进制文件交付给客户的简便性反映了PGI CUDA C/C++项目背后完整的思路。从规划角度来看，CUDA
    for x86显著影响了软件开发决策过程。CUDA不再仅仅作为一个为GPU加速产品提供小众开发平台的角色，而是成为了一个用于所有产品开发的平台——即使是那些并不打算通过GPU加速的应用程序！这一动机通过NVIDIA展示的各种项目以及这些项目相较于传统处理器取得的100倍或更高性能的实例得到了清晰的体现，如[图8.1](#f0010)所总结。
- en: '| ![B9780123884268000082/f08-01-9780123884268.jpg is missing](B9780123884268000082/f08-01-9780123884268.jpg)
    |'
  id: totrans-6
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000082/f08-01-9780123884268.jpg 文件丢失](B9780123884268000082/f08-01-9780123884268.jpg)
    |'
- en: '| **Figure 8.1**NVIDIA''s top 100 fastest reported speedups over conventional
    processors. |'
  id: totrans-7
  prefs: []
  type: TYPE_TB
  zh: '| **图 8.1**NVIDIA 相较于传统处理器的前 100 名最快加速报告。 |'
- en: In short, the performance of applications that fail to capitalize on the parallel
    performance capabilities of multicore and GPU devices will plateau at or near
    current levels and not increase with future hardware generations. Such applications
    risk stagnation and a loss of competitiveness ([Farber, 2010](B978012388426800015X.xhtml#ref43)).From
    a software development point of view, prudence dictates the selection of a platform
    that works well right now. Foresight requires picking a software framework that
    keeps the application running competitively on future hardware platforms without
    requiring a substantial rewrite or additional software investment.Following are
    the top reasons to use CUDA for all application development:■ CUDA is based on
    standard C and C++. Both of these languages have a solid history of application
    development spanning decades.■ Applications written in CUDA and compiled with
    CUDA-x86 can potentially run faster on x86 platforms than code written in traditional
    languages through better use of parallelism and the multicore SIMD units.■ Multicore
    CUDA codes will contain fewer bugs because the CUDA execution model precludes
    many common parallel programming errors including race conditions and deadlock.■
    CUDA will future-proof the application because CUDA was designed to scale effectively
    to tens of thousands of concurrent threads of execution. This benefit can save
    future software development dollars and allow fast penetration into new markets.■
    GPU acceleration comes for free, which opens the door for order of magnitude application
    acceleration on the third of a billion CUDA-enabled GPUs that have already been
    sold worldwide.■ CUDA has a large base of educated developers; plus, this developer
    base is rapidly expanding. CUDA is currently taught at more than 450 universities
    and colleges worldwide. The number of institutions teaching CUDA is also rapidly
    expanding.In other words, the future looks bright for literate CUDA programmers!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: The PGI CUDA x86 Compiler
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PGI CUDA x86 编译器
- en: 'Using the PGI CUDA-x86 compiler is straightforward. Currently, PGI offers the
    compiler a free evaluation period. Just download and install it per the instructions
    on the PGI website. [⁵](#fn0035)⁵[http://pgroup.com](http://pgroup.com).Setup
    is straightforward and well described in the installation guide. [Example 8.2](#tb0015),
    “Setting the Environment for the PGI Compilor,” contains the commands to set the
    environment using bash under Linux:`PGI=/opt/pgi; export PGI``MANPATH=$MANPATH:$PGI/linux86-64/11.5/man;
    export MANPATH``LM_LICENSE_FILE=$PGI/license.dat; export LM_LICENSE_FILE``PATH=$PGI/linux86-64/11.5/bin:$PATH;
    export PATH`It is quite easy to use the software. For example, copy the PGI NVIDIA
    SDK samples to a convenient location and build them, as in [Example 8.3](#tb0020),
    “Building the PGI SDK”:`cp –r /opt/pgi/linux86-64/2011/cuda/cudaX86SDK .``cd cudaX86SDK
    ;``make`[Example 8.4](#tb0025), “Output of deviceQuery When Running on a Quad-CoreCPU,”
    shows the output of **deviceQuery** on an Intel Xeon e5560:`CUDA Device Query
    (Runtime API) version (CUDART static linking)``There is 1 device supporting CUDA``Device
    0: "DEVICE EMULATION MODE"``CUDA Driver Version:99.99``CUDA Runtime Version:99.99``CUDA
    Capability Major revision number:9998``CUDA Capability Minor revision number:9998``Total
    amount of global memory:128000000 bytes``Number of multiprocessors:1``Number of
    cores:0``Total amount of constant memory:1021585952 bytes``Total amount of shared
    memory per block:1021586048 bytes``Total number of registers available per block:1021585904``Warp
    size:1``Maximum number of threads per block:1021585920``Maximum sizes of each
    dimension of a block:32767 × 2 × 0``Maximum sizes of each dimension of a grid:1021586032
    × 32767 × 1021586048``Maximum memory pitch:4206313 bytes``Texture alignment:1021585952
    bytes``Clock rate:0.00 GHz``Concurrent copy and execution:Yes``Run time limit
    on kernels:Yes``Integrated:No``Support host page-locked memory mapping:Yes``Compute
    mode:Unknown``Concurrent kernel execution:Yes``Device has ECC support enabled:Yes``deviceQuery,
    CUDA Driver = CUDART, CUDA Driver Version = 99.99, CUDA Runtime Version = 99.99,
    NumDevs = 1, Device = DEVICE EMULATION MODE``PASSED``Press <Enter> to Quit…``----------------------------------------------------------`Similarly,
    the output of **bandwidthTest** shows that device transfers work as expected ([Example
    8.5](#tb0030), “Output of bandwidthTest When Running on a Quad-Core CPU):`Running
    on…``Device 0: DEVICE EMULATION MODE``Quick Mode``Host to Device Bandwidth, 1
    Device(s), Paged memory``Transfer Size (Bytes)Bandwidth(MB/s)``335544324152.5``Device
    to Host Bandwidth, 1 Device(s), Paged memory``Transfer Size (Bytes) Bandwidth(MB/s)``335544324257.0``Device
    to Device Bandwidth, 1 Device(s)``Transfer Size (Bytes)Bandwidth(MB/s)``335544328459.2``[bandwidthTest]
    − Test results:``PASSED``Press <Enter> to Quit…``----------------------------------------------------------`Just
    as with NVIDIA''s **nvcc** compiler, it is easy to use the PGI **pgCC** compiler
    to build an executable from a CUDA source file. For example, the *arrayReversal_multiblock_fast.cu*
    code from part 3 of my Doctor Dobb''s article series just compiles and runs. [⁶](#fn0040)⁶[http://drdobbs.com/high-performance-computing/207603131](http://drdobbs.com/high-performance-computing/207603131).To
    compile and run it under Linux, type the code in [Example 8.6](#tb0035), “Output
    of *arrayReversal_multiblock_fast.cu* When Running on a Quad-Core CPU”:`pgCC arrayReversal_multiblock_fast.cu``./a.out``Correct!`'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PGI CUDA-x86 编译器非常简单。目前，PGI 提供了该编译器的免费评估期。只需根据 PGI 网站上的说明下载并安装即可。[⁵](#fn0035)⁵[http://pgroup.com](http://pgroup.com)。安装过程非常简单，并且在安装指南中有详细描述。[示例
    8.2](#tb0015)，《为 PGI 编译器设置环境》包含了使用 bash 在 Linux 上设置环境的命令：`PGI=/opt/pgi; export
    PGI``MANPATH=$MANPATH:$PGI/linux86-64/11.5/man; export MANPATH``LM_LICENSE_FILE=$PGI/license.dat;
    export LM_LICENSE_FILE``PATH=$PGI/linux86-64/11.5/bin:$PATH; export PATH`使用软件非常简单。例如，将
    PGI NVIDIA SDK 示例复制到方便的位置并进行构建，如[示例 8.3](#tb0020)，《构建 PGI SDK》所示：`cp –r /opt/pgi/linux86-64/2011/cuda/cudaX86SDK
    .``cd cudaX86SDK ;``make`[示例 8.4](#tb0025)，《在四核 CPU 上运行时 deviceQuery 的输出》显示了在
    Intel Xeon e5560 上运行 **deviceQuery** 的输出：`CUDA 设备查询（运行时 API）版本（CUDART 静态链接）``有
    1 个设备支持 CUDA``设备 0：“设备仿真模式”``CUDA 驱动版本：99.99``CUDA 运行时版本：99.99``CUDA 能力主修订号：9998``CUDA
    能力次修订号：9998``全球内存总量：128000000 字节``多处理器数量：1``核心数量：0``常量内存总量：1021585952 字节``每个块的共享内存总量：1021586048
    字节``每个块可用的寄存器总数：1021585904``Warp 大小：1``每个块的最大线程数：1021585920``每个维度的最大块大小：32767
    × 2 × 0``每个维度的最大网格大小：1021586032 × 32767 × 1021586048``最大内存间距：4206313 字节``纹理对齐：1021585952
    字节``时钟频率：0.00 GHz``并发复制与执行：是``内核运行时间限制：是``集成：否``支持主机页面锁定内存映射：是``计算模式：未知``并发内核执行：是``设备支持
    ECC 启用：是``deviceQuery，CUDA 驱动 = CUDART，CUDA 驱动版本 = 99.99，CUDA 运行时版本 = 99.99，NumDevs
    = 1，设备 = 设备仿真模式``通过``按 <Enter> 键退出…``----------------------------------------------------------`类似地，**bandwidthTest**
    的输出显示设备传输按预期工作（[示例 8.5](#tb0030)，《在四核 CPU 上运行时 bandwidthTest 的输出》）：`正在运行…``设备
    0：设备仿真模式``快速模式``主机到设备带宽，1 个设备，分页内存``传输大小（字节）带宽（MB/s）``335544324152.5``设备到主机带宽，1
    个设备，分页内存``传输大小（字节） 带宽（MB/s）``335544324257.0``设备到设备带宽，1 个设备``传输大小（字节）带宽（MB/s）``335544328459.2``[bandwidthTest]
    − 测试结果：``通过``按 <Enter> 键退出…``----------------------------------------------------------`与
    NVIDIA 的 **nvcc** 编译器一样，使用 PGI **pgCC** 编译器从 CUDA 源文件构建可执行文件也非常容易。例如，我在《Doctor
    Dobb's》杂志系列文章第 3 部分中的 *arrayReversal_multiblock_fast.cu* 代码就可以直接编译并运行。[⁶](#fn0040)⁶[http://drdobbs.com/high-performance-computing/207603131](http://drdobbs.com/high-performance-computing/207603131)。在
    Linux 上编译并运行它，请输入[示例 8.6](#tb0035)，《*arrayReversal_multiblock_fast.cu* 在四核 CPU
    上运行时的输出》中的代码：`pgCC arrayReversal_multiblock_fast.cu``./a.out``正确！`
- en: An x86 core as an SM
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: x86 核心作为 SM
- en: In CUDA-x86, thread blocks are efficiently mapped to the processor cores. Thread-level
    parallelism is mapped to the SSE (Streaming SIMD Extensions) or AVX SIMD units,
    as shown in [Figure 8.2](#f0015).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CUDA-x86 中，线程块被高效地映射到处理器核心。线程级并行性被映射到 SSE（流式 SIMD 扩展）或 AVX SIMD 单元，如[图 8.2](#f0015)所示。
- en: '| ![B9780123884268000082/f08-02-9780123884268.jpg is missing](B9780123884268000082/f08-02-9780123884268.jpg)
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000082/f08-02-9780123884268.jpg 文件丢失](B9780123884268000082/f08-02-9780123884268.jpg)
    |'
- en: '| **Figure 8.2**Mapping GPU computing onto a CPU. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| **图 8.2** 将 GPU 计算映射到 CPU。 |'
- en: CUDA programmers should note:■ The size of a warp will be different from the
    expected 32 threads per warp for a GPU. For x86 computing, a warp might be the
    size of the SIMD units on the x86 core (either four or eight) or one thread per
    warp when SIMD execution is not utilized.■ In many cases, the PGI CUDA C compiler
    will remove explicit synchronization of the thread processors when the compiler
    can determine that it is safe to split loops where the synchronization calls occur.■
    CUDA programmers must consider data transfer times, as explicit movement of data
    between host and device memory and global to shared memory will still happen in
    CUDA-x86.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 程序员应注意：■ 一个 warp 的大小将与预期的每个 warp 32 个线程在 GPU 上不同。对于 x86 计算，warp 的大小可能与
    x86 核心上的 SIMD 单元大小相同（四个或八个），或者在未使用 SIMD 执行时，每个 warp 只有一个线程。■ 在许多情况下，PGI CUDA C
    编译器会在编译器确定安全拆分包含同步调用的循环时，移除线程处理器的显式同步。■ CUDA 程序员必须考虑数据传输时间，因为在 CUDA-x86 中，主机和设备内存之间的显式数据移动以及从全局内存到共享内存的传输仍然会发生。
- en: The NVIDIA NVCC Compiler
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NVIDIA NVCC 编译器
- en: The NVIDIA **nvcc** compiler can generate both host- and device-based kernels.
    [Chapter 2](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1) utilized this
    capability for testing and performance analysis purposes. Though very useful,
    this approach requires manual effort on the part of the CUDA programmer to set
    up memory correctly and to call the functor appropriately.Thrust can also transparently
    generate code for different backends such as x86 processors just by passing some
    additional command-line options to nvcc. No source code modification is required.The
    following **nvcc** command-line demonstrated building the NVIDIA SDK Monte Carlo
    example to run on the host processor:`nvcc -O2 -o monte_carlo monte_carlo.cu -Xcompiler
    -fopenmp \``-DTHRUST_DEVICE_BACKEND=THRUST_DEVICE_BACKEND_OMP -lcudart -lgomp`Timing
    reported on the Thrust website shows that the performance is acceptable (see [Table
    8.1](#t0010)). Be aware that Thrust is not optimized to produce the best x86 runtimes.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA **nvcc** 编译器可以生成主机和设备端的内核。[第二章](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1)利用了这一功能进行测试和性能分析。尽管这一方法非常有用，但它要求
    CUDA 程序员手动设置内存，并正确调用功能对象。Thrust 还可以通过传递一些额外的命令行选项给 nvcc，透明地为不同的后端（如 x86 处理器）生成代码。无需修改源代码。以下
    **nvcc** 命令行演示了构建 NVIDIA SDK 蒙特卡罗示例以便在主机处理器上运行：`nvcc -O2 -o monte_carlo monte_carlo.cu
    -Xcompiler -fopenmp \``-DTHRUST_DEVICE_BACKEND=THRUST_DEVICE_BACKEND_OMP -lcudart
    -lgomp`。Thrust 网站上报告的时间表明，性能是可接受的（参见 [表 8.1](#t0010)）。请注意，Thrust 并没有针对最佳的 x86
    运行时进行优化。
- en: '**Table 8.1** Reported Time When Running a Thrust Example Using OpenMP'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 8.1** 使用 OpenMP 运行 Thrust 示例时报告的时间'
- en: '| Device | Seconds |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 设备 | 秒 |'
- en: '| GPU | 0.222 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| GPU | 0.222 |'
- en: '| 4 OpenMP threads | 2.090 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 4 OpenMP 线程 | 2.090 |'
- en: '| 2 OpenMP threads | 4.168 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 2 OpenMP 线程 | 4.168 |'
- en: '| 1 OpenMP thread | 8.333 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 1 OpenMP 线程 | 8.333 |'
- en: Ocelot
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ocelot
- en: Ocelot is a popular, actively maintained package with a large user base. The
    website notes that Ocelot can run CUDA binaries on NVIDIA GPUS, AMD GPUs, and
    x86 processors at full speed without recompilation. It can be freely downloaded
    and is licensed under a new BSD license. A paper by Gregory Diamos, “The Design
    and Implementation Ocelot's[⁷](#fn9000) Dynamic Binary Translator from PTX to
    Multi-Core x86,” is recommended reading ([Diamos, 2009](B978012388426800015X.xhtml#ref32)),
    as is his chapter in *GPU Computing Gems* ([Hwu, 2011](B978012388426800015X.xhtml#ref72)).⁷[http://code.google.com/p/gpuocelot/](http://code.google.com/p/gpuocelot/).Ocelot's
    core capabilities consist of:■ An implementation of the CUDA Runtime API.■ A complete
    internal representation of PTX kernels coupled to control- and data-flow analysis
    procedures for analysis.■ A functional emulator for PTX.■ A PTX translator to
    multicore x86-based CPUs for efficient execution.■ A backend to NVIDIA GPUs via
    the CUDA driver API.■ Support for an extensible trace generation framework in
    which application behavior can be observed at instruction-level granularity.Ocelot
    has three backend execution targets:■ A PTX emulator.■ A translator from PTX to
    multicore instructions.■ A CUDA-enabled GPU.Ocelot has been validated against
    more than 130 applications taken from the CUDA SDK, the UIUC Parboil benchmark,
    the Virginia Rodinia benchmarks ([Che et al., 2009](B978012388426800015X.xhtml#ref15)
    and [Che et al., 2010](B978012388426800015X.xhtml#ref16)), the GPU-VSIPL signal
    and image processing library, [⁸](#fn0045) the thrust library, and several domain-specific
    applications. It is an exemplary tool to use in profiling and analyzing the behavior
    of CUDA applications that can also be used as a platform for CUDA application
    portability.⁸[http://gpu-vsipl.gtri.gatech.edu/](http://gpu-vsipl.gtri.gatech.edu/).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Ocelot 是一个流行的、积极维护的包，拥有大量的用户群体。网站指出，Ocelot 可以在 NVIDIA GPU、AMD GPU 和 x86 处理器上以全速运行
    CUDA 二进制文件，无需重新编译。它可以自由下载，并且采用新的 BSD 许可证。Gregory Diamos 的一篇论文《Ocelot 从 PTX 到多核
    x86 的动态二进制翻译器的设计与实现》[⁷](#fn9000) 推荐阅读 ([Diamos, 2009](B978012388426800015X.xhtml#ref32))，以及他在
    *GPU 计算宝石* 一书中的章节 ([Hwu, 2011](B978012388426800015X.xhtml#ref72))。⁷[http://code.google.com/p/gpuocelot/](http://code.google.com/p/gpuocelot/)。Ocelot
    的核心功能包括：■ CUDA 运行时 API 的实现。■ 完整的 PTX 内部表示，结合控制流和数据流分析过程进行分析。■ 一个功能性 PTX 模拟器。■
    一个将 PTX 转换为多核 x86 架构的翻译器，用于高效执行。■ 通过 CUDA 驱动程序 API 支持 NVIDIA GPU。■ 支持可扩展的追踪生成框架，可以在指令级粒度下观察应用行为。Ocelot
    有三个后端执行目标：■ 一个 PTX 模拟器。■ 一个将 PTX 转换为多核指令的翻译器。■ 一个支持 CUDA 的 GPU。Ocelot 已通过来自 CUDA
    SDK、UIUC Parboil 基准、Virginia Rodinia 基准（[Che et al., 2009](B978012388426800015X.xhtml#ref15)
    和 [Che et al., 2010](B978012388426800015X.xhtml#ref16)）、GPU-VSIPL 信号与图像处理库、[⁸](#fn0045)
    thrust 库和多个领域特定应用的 130 多个应用程序进行了验证。它是一个示范性的工具，可以用于分析和剖析 CUDA 应用程序的行为，同时也可以作为 CUDA
    应用程序可移植性的一个平台。⁸[http://gpu-vsipl.gtri.gatech.edu/](http://gpu-vsipl.gtri.gatech.edu/)。
- en: Swan
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 天鹅
- en: Swan is a freely available source-to-source translation tool that converts an
    existing CUDA code to use the OpenCL model. Note that the conversion process requires
    human intervention and is not automatic.The authors report that the performance
    of a CUDA application ported to OpenCL run about 50 percent slower ([Harvey &
    De Fabritiis, 2010](B978012388426800015X.xhtml#ref60)). The authors attribute
    the performance reduction to the immaturity of the OpenCL compilers. They conclude
    that OpenCL is a viable platform for developing portable GPU applications but
    also that the more mature CUDA tools continue to provide best performance.It is
    not clear how active the development effort is on the Swan project, as the most
    recent update was in December 2010\. The current version of Swan does not support:■
    CUDA C++ templates in kernel code.■ OpenCL Images/Samplers (analogous to Textures)—texture
    interpolation done in software.■ Multiple device management in a single process.■
    Compiling kernels for the CPU.■ CUDA device-emulation mode.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Swan是一个自由提供的源到源翻译工具，用于将现有的CUDA代码转换为OpenCL模型。请注意，转换过程需要人工干预，不能自动完成。作者报告称，将CUDA应用程序移植到OpenCL后，性能大约慢50%（[Harvey
    & De Fabritiis, 2010](B978012388426800015X.xhtml#ref60)）。作者将性能下降归因于OpenCL编译器的不成熟。他们总结认为，OpenCL是开发可移植GPU应用程序的可行平台，但也指出，成熟的CUDA工具仍然提供最佳性能。目前不清楚Swan项目的开发工作有多活跃，因为最近的更新是在2010年12月。Swan的当前版本不支持：■
    内核代码中的CUDA C++模板。■ OpenCL图像/采样器（类似于纹理）—纹理插值通过软件完成。■ 在单个进程中管理多个设备。■ 为CPU编译内核。■
    CUDA设备仿真模式。
- en: MCUDA
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MCUDA
- en: 'MCUDA is an academic effort by the IMPACT Research Group at the University
    of Illinois. It is available for free download. This project does not appear to
    be actively maintained.The paper “MCUDA: An Efficient Implementation of CUDA Kernels
    for Multi-Core CPUs” is interesting reading ([Stratton, Stone, & Hwu, 2008](B978012388426800015X.xhtml#ref126)).
    A related paper, “FCUDA: Enabling Efficient Compilation of CUDA Kernels onto FPGAs,”
    discusses translating CUDA to FPGAs (Field Programmable Gate Arrays) ([Papakonstantinou
    et al., 2009](B978012388426800015X.xhtml#ref103)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: MCUDA是伊利诺伊大学IMPACT研究小组的学术项目，免费提供下载。这个项目似乎没有得到积极维护。论文《MCUDA：一种高效的CUDA内核在多核CPU上的实现》非常值得阅读（[Stratton,
    Stone, & Hwu, 2008](B978012388426800015X.xhtml#ref126)）。相关论文《FCUDA：启用CUDA内核高效编译到FPGA》讨论了将CUDA转换为FPGA（现场可编程门阵列）（[Papakonstantinou
    et al., 2009](B978012388426800015X.xhtml#ref103)）。
- en: Accessing CUDA from Other Languages
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 访问其他语言中的CUDA
- en: CUDA can be incorporated into any language that provides a mechanism for calling
    C or C++. To simplify this process, general-purpose interface generators have
    been created that will create most of the boilerplate code automatically. One
    of the most popular interface generators is SWIG. An alternative approach is to
    seamlessly integrate CUDA into the language, which is being investigated by the
    Copperhead Python project.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 可以融入任何提供调用 C 或 C++ 机制的语言。为简化这一过程，已经创建了通用接口生成器，这些生成器可以自动创建大部分样板代码。最受欢迎的接口生成器之一是
    SWIG。另一种方法是将 CUDA 无缝集成到语言中，这一方法正在由 Copperhead Python 项目进行研究。
- en: SWIG
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SWIG
- en: SWIG (Simplified Wrapper and Interface Generator) is a software development
    tool that connects programs written in C and C++—including CUDA C/C++ applications—with
    a variety of high-level programming languages. SWIG is actively supported and
    widely used. It can be freely downloaded from the SWIG website. [⁹](#fn0050)⁹[http://swig.org](http://swig.org).As
    of the current 2.0.4 release, SWIG generates interfaces for the following languages:■
    AllegroCL■ C# – Mono■ C# – MS.NET■ CFFI■ CHICKEN■ CLISP■ D■ Go language■ Guile■
    Java■ Lua■ MzScheme/Racket■ Ocaml■ Octave■ Perl■ PHP■ Python■ R■ Ruby■ Tcl/TkPart
    9 of my *Doctor Dobb's Journal* “Supercomputing for the Masses” tutorial series[^(10)](#fn0055)
    provides a complete working example that uses SWIG to interface a CUDA matrix
    operation with Python. This example can interface with other languages as well
    ([Farber, 2008](B978012388426800015X.xhtml#ref41)).^(10)[http://drdobbs.com/high-performance-computing/211800683](http://drdobbs.com/high-performance-computing/211800683).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: SWIG（简化包装器和接口生成器）是一种软件开发工具，它连接用 C 和 C++ 编写的程序——包括 CUDA C/C++ 应用程序——与各种高级编程语言。SWIG
    得到积极支持并被广泛使用。它可以从 SWIG 网站免费下载。[⁹](#fn0050)⁹[http://swig.org](http://swig.org)。截至当前
    2.0.4 版本，SWIG 为以下语言生成接口：■ AllegroCL■ C# – Mono■ C# – MS.NET■ CFFI■ CHICKEN■ CLISP■
    D■ Go 语言■ Guile■ Java■ Lua■ MzScheme/Racket■ Ocaml■ Octave■ Perl■ PHP■ Python■
    R■ Ruby■ Tcl/Tk我的 *《Doctor Dobb's Journal》*《大众超级计算》教程系列第9部分[^(10)](#fn0055) 提供了一个完整的工作示例，演示了如何使用
    SWIG 将 CUDA 矩阵操作与 Python 接口。这个示例也可以与其他语言接口([Farber, 2008](B978012388426800015X.xhtml#ref41))。^(10)[http://drdobbs.com/high-performance-computing/211800683](http://drdobbs.com/high-performance-computing/211800683)。
- en: Copperhead
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Copperhead
- en: Copperhead is an early-stage research project to bring data parallelism to the
    Python language. It defines a small, data-parallel subset of Python that is dynamically
    compiled to run on parallel platforms. Right now, NVIDIA GPGPUs are the only parallel
    backend.[Example 8.8](#tb0045), “Example Copperhead Python Code,” is a simple
    example from the Copperhead website:`from copperhead import *``import numpy as
    np``@cu``def axpy(a, x, y):``return [a * xi + yi for xi, yi in zip(x, y)]``x =
    np.arange(100, dtype=np.float64)``y = np.arange(100, dtype=np.float64)``with places.gpu0:``gpu
    = axpy(2.0, x, y)``with places.here:``cpu = axpy(2.0, x, y)`Copperhead organizes
    computations around data parallel arrays. In this example, the Copperhead runtime
    intercepts the call to **axpy()** and compiles the function to CUDA. The runtime
    converts the input arguments to a type of parallel array, **CuArrays**, that are
    managed by the runtime.The programmer specifies where the execution is to take
    place using the **with** construct shown in the previous example. Data is lazily
    copied to and from the execution location. Copperhead currently supports GPU execution
    and Python interpreter execution. Use of the Python interpreter is intended for
    algorithm prototyping.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Copperhead 是一个早期的研究项目，旨在将数据并行性引入 Python 语言。它定义了一个小型的数据并行子集，能够动态编译并在并行平台上运行。目前，NVIDIA
    GPGPU 是唯一支持的并行后端。[示例 8.8](#tb0045)，“示例 Copperhead Python 代码”，是来自 Copperhead 网站的一个简单示例：`from
    copperhead import *``import numpy as np``@cu``def axpy(a, x, y):``return [a *
    xi + yi for xi, yi in zip(x, y)]``x = np.arange(100, dtype=np.float64)``y = np.arange(100,
    dtype=np.float64)``with places.gpu0:``gpu = axpy(2.0, x, y)``with places.here:``cpu
    = axpy(2.0, x, y)`Copperhead 将计算组织在数据并行数组周围。在此示例中，Copperhead 运行时拦截了对 **axpy()**
    的调用，并将该函数编译为 CUDA 代码。运行时将输入参数转换为并行数组类型，**CuArrays**，并由运行时管理。程序员使用前面示例中的 **with**
    构造指定执行位置。数据会懒加载地在执行位置之间进行复制。Copperhead 当前支持 GPU 执行和 Python 解释器执行。Python 解释器的使用旨在用于算法原型开发。
- en: EXCEL
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EXCEL
- en: Microsoft Excel is a widely adopted commercial spreadsheet application written
    and distributed by Microsoft for Windows and Mac OS. It features calculation,
    graphing tools, and a variety of other tools. Functionality can be programmed
    in Visual Basic. NVIDIA distributes the “Excel 2010 CUDA Integration Example”[^(11)](#fn9005)
    on their website, which shows how to use CUDA in Excel. This SDK example is not
    included in the standard SDK samples.^(11)[http://developer.nvidia.com/cuda-cc-sdk-code-samples](http://developer.nvidia.com/cuda-cc-sdk-code-samples).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft Excel 是一款由微软开发并分发的广泛使用的商业电子表格应用程序，支持 Windows 和 Mac OS 平台。它具有计算、绘图工具和各种其他工具。功能可以通过
    Visual Basic 编程。NVIDIA 在其网站上发布了“Excel 2010 CUDA 集成示例”[^(11)](#fn9005)，展示了如何在 Excel
    中使用 CUDA。该 SDK 示例不包含在标准 SDK 示例中。^(11)[http://developer.nvidia.com/cuda-cc-sdk-code-samples](http://developer.nvidia.com/cuda-cc-sdk-code-samples)。
- en: MATLAB
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MATLAB
- en: MATLAB is a commercial application developed by MathWorks. The MATLAB software
    allows matrix manipulation, plotting of functions and data, and a wealth of other
    functionality. Developers can implement algorithms and user interfaces as well
    as integrate MATLAB into other applications. MATLAB GPU support is available in
    the Parallel Computing Toolbox. It supports NVIDIA CUDA-enabled GPUs of compute
    1.3 and higher. Third-party products such as those by Accelereyes provide both
    MATLAB access to GPU computing as well as matrix libraries for CUDA. [^(12)](#fn0060)^(12)[http://www.accelereyes.com/](http://www.accelereyes.com/).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: MATLAB 是由 MathWorks 开发的商业应用程序。MATLAB 软件允许进行矩阵操作、函数和数据的绘图，以及其他丰富的功能。开发人员可以实现算法和用户界面，并将
    MATLAB 集成到其他应用程序中。MATLAB GPU 支持可以在并行计算工具箱中找到，支持计算能力为 1.3 及以上的 NVIDIA CUDA 启用 GPU。像
    Accelereyes 这样的第三方产品提供了 MATLAB 对 GPU 计算的访问权限，并且提供了 CUDA 的矩阵库。[^(12)](#fn0060)^(12)[http://www.accelereyes.com/](http://www.accelereyes.com/)。
- en: Libraries
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 库
- en: The use of optimized libraries are often an easy way to improve the performance
    of an application. When porting large legacy projects, libraries may be the only
    real way to optimize for a new platform because code changes would require extensive
    validation efforts. Essentially, libraries are convenient and can greatly accelerate
    code development as well as application performance, but they cannot be blindly
    utilized. GPU-based libraries in particular require the developer to think carefully
    about data placement and how the library is used, or poor performance and excessive
    memory consumption will result.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用优化的库通常是提高应用程序性能的简便方法。在移植大型遗留项目时，库可能是优化新平台的唯一实际方法，因为代码更改将需要大量的验证工作。本质上，库是方便的，可以大大加速代码开发和应用程序性能，但不能盲目使用。尤其是
    GPU 基于的库，要求开发人员仔细考虑数据放置和库的使用方式，否则可能会导致性能不佳和过度的内存消耗。
- en: CUBLAS
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUBLAS
- en: The Basic Linear Algebra Subprograms (BLAS) package is the de facto programming
    interface for basic linear algebra operations such as vector and matrix multiplication.
    NVIDIA supports this interface with their own library for the GPU called CUBLAS.
    The basic model by which applications use the CUBLAS library is to create matrix
    and vector objects in GPU memory space, fill these objects with data, call a sequence
    of CUBLAS functions, and return the result(s) to the host.CUBLAS provides helper
    functions to create and destroy objects in GPU space and to utilize the data in
    these objects. CUDA programmers should note that CUBLAS uses column-major storage
    and 1-based indexing for maximum FORTRAN compatibility. C and C++ applications
    need to use macros or inline functions to facilitate access to CUBLAS created
    objects.[Chapter 1](B978012388426800001X.xhtml#B978-0-12-388426-8.00001-X) contains
    a discussion of the BLAS runtime levels and the importance of minimizing or eliminating
    data movement. The simpleBLAS example, which can be downloaded from the NVIDIA
    website, provides a basic demonstration how to use CUBLAS. [^(13)](#fn9010)^(13)[http://www.nvidia.com/object/cuda_sample_linear_algebra.html](http://www.nvidia.com/object/cuda_sample_linear_algebra.html).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基本线性代数子程序（BLAS）包是基本线性代数操作的事实标准编程接口，例如向量和矩阵乘法。NVIDIA通过其专有的GPU库CUBLAS来支持该接口。应用程序使用CUBLAS库的基本模型是：在GPU内存空间中创建矩阵和向量对象，将数据填充到这些对象中，调用一系列CUBLAS函数，并将结果返回给主机。CUBLAS提供了帮助函数，用于在GPU空间中创建和销毁对象，并利用这些对象中的数据。CUDA程序员应该注意，CUBLAS使用列主序存储和基于1的索引，以最大程度地兼容FORTRAN。C和C++应用程序需要使用宏或内联函数来便捷地访问CUBLAS创建的对象。[第1章](B978012388426800001X.xhtml#B978-0-12-388426-8.00001-X)讨论了BLAS运行时级别以及最小化或消除数据移动的重要性。可以从NVIDIA网站下载的simpleBLAS示例提供了如何使用CUBLAS的基本演示。[^(13)](#fn9010)^(13)[http://www.nvidia.com/object/cuda_sample_linear_algebra.html](http://www.nvidia.com/object/cuda_sample_linear_algebra.html)。
- en: CUFFT
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUFFT
- en: 'CUFFT is another NVIDIA supported library that provides a GPU based implementation
    of the FFT, a commonly used method in scientific and signal processing applications.
    CUFFT is modeled after FFTW, a very highly optimized and popular FFT package for
    general-purpose processors.CUFFT utilizes a plan, which is a simple configuration
    mechanism that specifies the best “plan” of execution for a particular algorithm
    given a specified problem size, data type, and destination hardware platform.
    The advantage of this approach is that once a plan is created, it can be used
    for the remainder of the application lifetime. This is a commonly used programming
    pattern to perform runtime configuration in libraries and other high-performance
    portable codes. The NVIDIA CUFFT library uses this configuration model because
    different sizes and types of FFTs require different thread configurations and
    GPU resources. The simpleCUFFT example can be downloaded from the NVIDIA website.
    [^(14)](#fn9015) It covers the basics of how to use the CUFFT library.^(14)[http://www.nvidia.com/object/cuda_sample_basic_topics.html](http://www.nvidia.com/object/cuda_sample_basic_topics.html).CUFFT
    also provides the method **cufftPlanMany()**, which creates a plan that will run
    multiple FFT operations at the same time. This can be both a performance boon
    and a memory hog.The following test program demonstrates the use of CUFFT on one
    or more GPUs. It performs a 3D complex-to-complex forward and inverse in-place
    transform and calculates the error introduced by these transforms. Correct usage
    will result in a small error.The user can specify the following runtime characteristics
    via the command line:■ The number of GPUs to use. This number must be less than
    or equal to the number of GPUs in the system.■ The size of each dimension.■ The
    total number of FFTs to perform.■ An optional value that specifies how many FFTs
    to perform on each call to CUFFT.In addition, this test code makes use of C++
    type defines. To create a double-precision executable, simply specify -**D REAL**=**double**
    on the **nvcc** command line.General include files and constants are defined at
    the beginning of the example file. The preprocessor variable **REAL** is set to
    default to **float** if not otherwise defined during compilation. [Example 8.9](#tb0050),
    “Part 1 of *fft3Dtest.cu*,” contains a walkthrough of the test code:`#include
    <iostream>``#include <cassert>``using namespace std;``#include <cuda.h>``#define
    CUFFT_FORWARD -1``#define CUFFT_INVERSE1``#include "thrust/host_vector.h"``#include
    "thrust/device_vector.h"``#include <cufft.h>``#ifndef REAL``#define REAL float``#endif`The
    template class **DistFFT3D** is defined and a number of variables are created
    in [Example 8.10](#tb0055), “Part 2 of *fft3Dtest.cu*”:`template <typename Real>``class
    DistFFT3D {``protected:``int nGPU;``cudaStream_t *streams;``int nPerCall;``vector<cufftHandle>
    fftPlanMany;``int dim[3];``Real *h_data;``long h_data_elements;``long nfft, n2ft3d,
    h_memsize, nelements;``long totalFFT;``vector<Real *> d_data;``long bytesPerGPU;`The
    public constructor takes a vector of host data and partitions it equally across
    the user defined number of GPUs. These vectors are kept in the variable **d_data**.For
    generality, an array of streams is passed to the constructor, which allows the
    user to queue work before and after the test. This capability is not used but
    is provided, in case the reader wishes to use this example in another code. It
    is assumed that one stream is associated with each GPU.A vector of CUFFT handles
    is created in **fftPlanMany()** where one plan is associated with each stream.
    This setup implies that there will be one plan per GPU, as each stream is associated
    with a different GPU.The number of multiplan FFTs is defined in the call to **NperCall()**.
    The method **initFFTs()** creates the handles. Similarly, a vector of device vectors
    is created per GPU. The pointers to device memory are held in the vector **d_data**.
    See [Example 8.11](#tb0060), “Part 3 of *fft3Dtest.cu*”:`public:``DistFFT3D(int
    _nGPU, Real* _h_data, long _h_data_elements, int *_dim, int _nPerCall, cudaStream_t
    *_streams) {``nGPU= _nGPU;``h_data = _h_data;``h_data_elements = _h_data_elements;``dim[0]
    = _dim[0]; dim[1] = _dim[1]; dim[2] = _dim[2];``nfft = dim[0]*dim[1]*dim[2];``n2ft3d
    = 2*dim[0]*dim[1]*dim[2];``totalFFT = h_data_elements/n2ft3d;``set_NperCall(_nPerCall);``bytesPerGPU
    = nPerCall*n2ft3d*sizeof(Real);``h_memsize = h_data_elements*sizeof(Real);``assert(
    (totalFFT/nPerCall*bytesPerGPU) == h_memsize);``streams = _streams;``fftPlanMany
    = vector<cufftHandle>(nGPU);``initFFTs();``for(int i=0; i<nGPU; i++) {``Real*
    tmp;``cudaSetDevice(i);``if(cudaMalloc(&tmp,bytesPerGPU)) {``cerr << "Cannot allocate
    space on device!" << endl;``exit(1);``}``d_data.push_back(tmp);``}``}``void set_NperCall(int
    n) {``cerr << "Setting nPerCall " << n << endl;``nPerCall = n;``if( (nGPU * nPerCall)
    > totalFFT) {``cerr << "Too many nPerCall specified! max " << (totalFFT/nGPU)
    << endl;``exit(1);``}``}`The destructor frees the data allocated on the devices
    in [Example 8.12](#tb0065), “Part 4 of *fft3Dtest.cu*”:`~DistFFT3D() {``for(int
    i=0; i < nGPU; i++){``cudaSetDevice(i);``cudaFree(d_data[i]);``}``}`[Example 8.13](#tb0070),
    “Part 5 of *fft3Dtest.cu*,” initializes the multiplan FFTs and provides the template
    wrappers to correctly call the CUFFT method for **float** and **double** variable
    types. Note that the stream is set with **cufftSetStream()**:`void inline initFFTs()``{``if((nPerCall*nGPU)
    > totalFFT) {``cerr << "nPerCall must be a multiple of totalFFT" << endl;``exit(1);``}``//
    Create a batched 3D plan``for(int sid=0; sid < nGPU; sid++) {``cudaSetDevice(sid);``if(sizeof(Real)
    == sizeof(float) ) {``cufftPlanMany(&fftPlanMany[sid], 3, dim, NULL, 1, 0, NULL,
    1, 0, CUFFT_C2C,nPerCall);``} else {``cufftPlanMany(&fftPlanMany[sid], 3, dim,
    NULL, 1, 0, NULL, 1, 0, CUFFT_Z2Z,nPerCall);``}``if(cufftSetStream(fftPlanMany[sid],streams[sid]))
    {``cerr << "cufftSetStream failed!" << endl;``}``}``cudaSetDevice(0);``}``inline
    void _FFTerror(int ret) {``switch(ret) {``case CUFFT_SETUP_FAILED: cerr << "SETUP_FAILED"
    << endl; break;``case CUFFT_INVALID_PLAN: cerr << "INVALID_PLAN" << endl; break;``case
    CUFFT_INVALID_VALUE: cerr << "INVALID_VALUE" << endl; break;``case CUFFT_EXEC_FAILED:
    cerr << "EXEC_FAILED" << endl; break;``default: cerr << "UNKNOWN ret code " <<
    ret << endl;``}``}``//template specialization to handle different data types (float,double)``inline
    void cinverseFFT_(cufftHandle myFFTplan, float* A, float* B ) {``int ret=cufftExecC2C(myFFTplan,
    (cufftComplex*)A, (cufftComplex*) B, CUFFT_INVERSE);``if(ret != CUFFT_SUCCESS)
    {``cerr << "C2C FFT failed! ret code " << ret << endl;``_FFTerror(ret); exit(1);``}``}``inline
    void cinverseFFT_(cufftHandle myFFTplan, double *A, double *B) {``int ret = cufftExecZ2Z(myFFTplan,
    (cufftDoubleComplex*)A, (cufftDoubleComplex*) B, CUFFT_INVERSE);``if(ret != CUFFT_SUCCESS)
    {``cerr << "Z2Z FFT failed! ret code " << ret << endl;``_FFTerror(ret); exit(1);``}``}``inline
    void cforwardFFT_(cufftHandle myFFTplan, float* A, float* B ) {``int ret = cufftExecC2C(myFFTplan,
    (cufftComplex*)A, (cufftComplex*) B, CUFFT_FORWARD);``if(ret != CUFFT_SUCCESS)
    {``cerr << "C C2C FFT failed!" << endl; _FFTerror(ret); exit(1);``}``}``inline
    void cforwardFFT_(cufftHandle myFFTplan, double *A, double *B) {``int ret = cufftExecZ2Z(myFFTplan,
    (cufftDoubleComplex*)A, (cufftDoubleComplex*) B, CUFFT_FORWARD);``if(ret != CUFFT_SUCCESS)
    {``cerr << "Z2Z FFT failed!" << endl; _FFTerror(ret); exit(1);``}``}`Calculate
    the error on the host using scaled values of the FFT results and the original
    host vector, as in [Example 8.14](#tb0075), “Part 6 of *fft3Dtest.cu*”:`double
    showError(Real* h_A1)``{``double error=0.;``#pragma omp parallel for reduction
    (+ : error)``for(int i=0; i < h_data_elements; i++) {``h_data[i] /= (Real)nfft;``error
    += abs(h_data[i] − h_A1[i]);``}``return error;``}`[Example 8.15](#tb0080), “Part
    7 of *fft3Dtest.cu*,” performs the actual test:`void doit()``{``double startTime
    = omp_get_wtime();``long h_offset=0;``for(int i=0; i < totalFFT; i += nGPU*nPerCall)
    {``for(int j=0; j < nGPU; j++) {``cudaSetDevice(j);``cudaMemcpyAsync(d_data[j],
    ((char*)h_data)+h_offset, bytesPerGPU, cudaMemcpyDefault,streams[j]);``cforwardFFT_(fftPlanMany[j],d_data[j],
    d_data[j]);``cinverseFFT_(fftPlanMany[j],d_data[j], d_data[j]);``cudaMemcpyAsync(((char*)h_data)+h_offset,
    d_data[j], bytesPerGPU, cudaMemcpyDefault,streams[j]);``h_offset += bytesPerGPU;``}``}``cudaDeviceSynchronize();``cudaSetDevice(0);``double
    endTime = omp_get_wtime();``cout << dim[0] << " " << dim[1] << " " << dim[2]``<<
    " nFFT/s " << 1./(0.5*(endTime-startTime)/totalFFT)``<< " average 3D fft time
    " << (0.5*(endTime-startTime)/totalFFT)``<< " total " << (endTime-startTime) <<
    endl;``}``};`The outer loop ensures that all the FFTs are performed by iterating
    from zero to **totalFFT** in steps of the number of multiplan FFT performed by
    all the GPUs.The inner loop sets the device and queues:■ The transfer of data
    to the device from the host. Note that the transfer with **cudamemcpyAsync()**
    is asynchronous to the host but not to the other tasks in the queue.■ The forward
    and inverse FFT transforms.■ The asynchronous transfer of data from the device
    back to the host.The increment of **h_offset** at the end of the inner loop ensures
    that each GPU (and queued FFT operation) operates on different, nonoverlapping
    regions of host memory, so no synchronization between GPUs is required. Finally,
    the overall performance of this method is measured and the average number of FFTs
    performed per second is reported.The **main()** routine parses the command line
    and performs some basic checks. It also creates the streams array and associates
    one stream per GPU.Of particular interest is the use of pinned memory on the host
    for fast asynchronous data transfers between the host and devices. This memory
    is created with **cudaHostAlloc()**, using the flag **cudaHostAllocPortable**.
    The host vector is filled with recurring sequential values. See [Example 8.16](#tb0085),
    “Part 8 of *fft3Dtest.cu*”:`main(int argc, char *argv[])``{``if(argc < 6) {``cerr
    << "Use nGPU dim[0] dim[1] dim[2] numberFFT [nFFT per call]" << endl;``exit(1);``}``int
    nPerCall = 1;``int nGPU = atoi(argv[1]);``int dim[] = { atoi(argv[2]), atoi(argv[3]),
    atoi(argv[4])};``int totalFFT=atoi(argv[5]);``nPerCall = totalFFT;``if( argc >
    6) {``nPerCall = atoi(argv[6]);``if(totalFFT % nPerCall != 0) {``cerr << "nPerCall
    must be a multiple of totalFFT!" << endl;``return(1);``}``}``int systemGPUcount;``cudaGetDeviceCount(&systemGPUcount);``if(nGPU
    > systemGPUcount) {``cerr << "Attempting to use too many GPUs!" << endl;``return(1);``}``cerr
    << "nGPU = " << nGPU << endl;``cerr << "dim[0] = " << dim[0] << endl;``cerr <<
    "dim[1] = " << dim[1] << endl;``cerr << "dim[2] = " << dim[2] << endl;``cerr <<
    "totalFFT = " << totalFFT << endl;``cerr << "sizeof(REAL) is " << sizeof(REAL)
    << " bytes" << endl;``cudaStream_t streams[nGPU];``for(int sid=0; sid < nGPU;
    sid++) {``cudaSetDevice(sid);``if(cudaStreamCreate(&streams[sid]) != 0) {``cerr
    << "Stream create failed!" << endl;``}``}``cudaSetDevice(0);``long nfft = dim[0]*dim[1]*dim[2];``long
    n2ft3d = 2*dim[0]*dim[1]*dim[2];``long nelements = n2ft3d*totalFFT;``REAL *h_A,
    *h_A1;``if(cudaHostAlloc(&h_A, nelements*sizeof(REAL), cudaHostAllocPortable)``!=
    cudaSuccess) {``cerr << "cudaHostAlloc failed!" << endl; exit(1);``}``h_A1 = (REAL*)
    malloc(nelements*sizeof(REAL));``if(!h_A1) {``cerr << "malloc failed!" << endl;
    exit(1);``}``// fill the test data``#pragma omp parallel for``for(long i=0; i
    < nelements; i++) h_A1[i] = h_A[i] = i%n2ft3d;``DistFFT3D<REAL> dfft3d(nGPU, h_A,
    nelements, dim, nPerCall, streams);``dfft3d.doit();``double error = dfft3d.showError(h_A1);``cout
    << "average error per fft " << (error/nfft/totalFFT) << endl;``cudaFreeHost(h_A1);``}`This
    source code can be compiled with the command-line [Example 8.17](#tb0090), “command-line
    to compile *fft3dtest.cu*”, which assumes that the file is saved in *fft3dtest.cu*:`nvcc
    -Xcompiler -fopenmp -O3 -arch sm_20 fft3Dtest.cu -lcufft \``-o fft3Dtest_float``nvcc
    -Xcompiler -fopenmp -O3 -arch sm_20 -D REAL=double fft3Dtest.cu \``-lcufft -o
    fft3Dtest_double`Running the test shows that the test program correctly utilizes
    CUFFT, as the error is small even when using single precision. For example, a
    32³ run that performs 128 FFT tests generates an error of 0.00225.Further, there
    is a speedup benefit when running on two GPUs, as seen in [Figure 8.3](#f0020),
    which shows plotted results for both single- and double-precision FFTs. The second
    GPU appears to provide limited additional performance in the double-precision
    runs, probably due to doubling the amount of data that must be transferred across
    the PCIe bus. Performing more than a few FFTs per CUFFT call also appears to have
    a minimal impact on performance.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 'CUFFT 是另一个由 NVIDIA 支持的库，提供了基于 GPU 的 FFT 实现，FFT 是在科学和信号处理应用中常用的方法。CUFFT 模仿了
    FFTW，这是一个为通用处理器高度优化且广泛使用的 FFT 包。CUFFT 使用一个计划（plan），这是一个简单的配置机制，它指定了在给定特定问题大小、数据类型和目标硬件平台的情况下，执行特定算法的最佳“计划”。这种方法的优势在于，一旦创建了计划，它就可以在应用程序的整个生命周期中使用。这是执行运行时配置的常见编程模式，广泛应用于库和其他高性能便携代码中。NVIDIA
    CUFFT 库使用此配置模型，因为不同大小和类型的 FFT 需要不同的线程配置和 GPU 资源。可以从 NVIDIA 网站下载简单的 CUFFT 示例 [^(14)](#fn9015)，它涵盖了如何使用
    CUFFT 库的基础知识。^(14)[http://www.nvidia.com/object/cuda_sample_basic_topics.html](http://www.nvidia.com/object/cuda_sample_basic_topics.html)。CUFFT
    还提供了 **cufftPlanMany()** 方法，它创建一个计划，可以同时运行多个 FFT 操作。这既可以提高性能，也可能占用大量内存。以下测试程序演示了如何在一个或多个
    GPU 上使用 CUFFT。它执行 3D 复数到复数的前向和反向就地变换，并计算这些变换引入的误差。正确使用将导致较小的误差。用户可以通过命令行指定以下运行时特征：■
    要使用的 GPU 数量。该数字必须小于或等于系统中的 GPU 数量。■ 每个维度的大小。■ 要执行的 FFT 总数。■ 一个可选值，指定每次调用 CUFFT
    时执行多少个 FFT。此外，此测试代码使用了 C++ 类型定义。要创建一个双精度可执行文件，只需在 **nvcc** 命令行中指定 -**D REAL**=**double**。常规的包含文件和常量在示例文件的开头定义。预处理器变量
    **REAL** 在编译期间如果未定义，将默认为 **float**。[示例 8.9](#tb0050)，“*fft3Dtest.cu* 的第 1 部分”，包含了测试代码的逐步说明：`#include
    <iostream>``#include <cassert>``using namespace std;``#include <cuda.h>``#define
    CUFFT_FORWARD -1``#define CUFFT_INVERSE1``#include "thrust/host_vector.h"``#include
    "thrust/device_vector.h"``#include <cufft.h>``#ifndef REAL``#define REAL float``#endif`模板类
    **DistFFT3D** 在 [示例 8.10](#tb0055)，“*fft3Dtest.cu* 的第 2 部分”中定义，并创建了一些变量：`template
    <typename Real>``class DistFFT3D {``protected:``int nGPU;``cudaStream_t *streams;``int
    nPerCall;``vector<cufftHandle> fftPlanMany;``int dim[3];``Real *h_data;``long
    h_data_elements;``long nfft, n2ft3d, h_memsize, nelements;``long totalFFT;``vector<Real
    *> d_data;``long bytesPerGPU;`公共构造函数接受一个主机数据的向量，并将其平均分配到用户定义的 GPU 数量上。这些向量保存在变量
    **d_data** 中。为了通用性，构造函数中传递了一个流数组，允许用户在测试之前和之后排队工作。这个功能虽然没有使用，但提供了，以防读者希望在其他代码中使用这个示例。假设每个流与每个
    GPU 相关联。在 **fftPlanMany()** 中创建了一个 CUFFT 句柄的向量，每个流都与一个计划相关联。这个设置意味着每个 GPU 会有一个计划，因为每个流与不同的
    GPU 相关联。多个计划的 FFT 数量在调用 **NperCall()** 时定义。**initFFTs()** 方法创建句柄。同样，每个 GPU 都会创建一个设备向量的向量。设备内存的指针保存在
    **d_data** 向量中。请参见 [示例 8.11](#tb0060)，“*fft3Dtest.cu* 的第 3 部分”：`public:``DistFFT3D(int
    _nGPU, Real* _h_data, long _h_data_elements, int *_dim, int _nPerCall, cudaStream_t
    *_streams) {``nGPU= _nGPU;``h_data = _h_data;``h_data_elements = _h_data_elements;``dim[0]
    = _dim[0]; dim[1] = _dim[1]; dim[2] = _dim[2];``nfft = dim[0]*dim[1]*dim[2];``n2ft3d
    = 2*dim[0]*dim[1]*dim[2];``totalFFT = h_data_elements/n2ft3d;``set_NperCall(_nPerCall);``bytesPerGPU
    = nPerCall*n2ft3d*sizeof(Real);``h_memsize = h_data_elements*sizeof(Real);``assert(
    (totalFFT/nPerCall*bytesPerGPU) == h_memsize);``streams = _streams;``fftPlanMany
    = vector<cufftHandle>(nGPU);``initFFTs();``for(int i=0; i<nGPU; i++) {``Real*
    tmp;``cudaSetDevice(i);``if(cudaMalloc(&tmp,bytesPerGPU)) {``cerr << "无法在设备上分配空间！"
    << endl;``exit(1);``}``d_data.push_back(tmp);``}``}``void set_NperCall(int n)
    {``cerr << "设置 nPerCall " << n << endl;``nPerCall = n;``if( (nGPU * nPerCall)
    > totalFFT) {``cerr << "指定的 nPerCall 太多！最大值为 " << (totalFFT/nGPU) << endl;``exit(1);``}``}`析构函数在
    [示例 8.12](#tb0065)，“*fft3Dtest.cu* 的第 4 部分”中释放了在设备上分配的数据：`~DistFFT3D() {``for(int
    i=0; i < nGPU; i++){``cudaSetDevice(i);``cudaFree(d_data[i]);``}``}``[示例 8.13](#tb0070)，“*fft3Dtest.cu*
    的第 5 部分”，初始化了多计划 FFT，并提供了模板包装器，以正确调用 CUFFT 方法处理 **float** 和 **double** 数据类型。请注意，流是通过
    **cufftSetStream()** 设置的：`void inline initFFTs()``{``if((nPerCall*nGPU) > totalFFT)
    {``cerr << "nPerCall 必须是 totalFFT 的倍数" << endl;``exit(1);``}``// 创建批量 3D 计划``for(int
    sid=0; sid < nGPU; sid++) {``cudaSetDevice(sid);``if(sizeof(Real) == sizeof(float)
    ) {``cufftPlanMany(&fftPlanMany[sid], 3, dim, NULL, 1, 0, NULL, 1, 0, CUFFT_C2C,nPerCall);``}
    else {``cufftPlanMany(&fftPlanMany[sid], 3, dim, NULL, 1, 0, NULL, 1, 0, CUFFT_Z2Z,nPerCall);``}``if(cufftSetStream(fftPlanMany[sid],streams[sid]))
    {``cerr << "cufftSetStream 失败！" << endl;``}``}``cudaSetDevice(0);``}``inline void
    _FFTerror(int ret) {``switch(ret) {``case CUFFT_SETUP_FAILED: cerr << "SETUP_FAILED"
    << endl; break;``case CUFFT_INVALID_PLAN: cerr << "INVALID_PLAN" << endl; break;``case
    CUFFT_INVALID_VALUE: cerr << "INVALID_VALUE" << endl; break;``case CUFFT_EXEC_FAILED:
    cerr << "EXEC_FAILED" << endl; break;``default: cerr << "未知的返回代码 " << ret << endl;``}``}``//模板特化以处理不同的数据类型（float,
    double）``inline void cinverseFFT_(cufftHandle myFFTplan, float* A, float* B )
    {``int ret=cufftExecC2C(myFFTplan, (cufftComplex*)A, (cufftComplex*) B, CUFFT_INVERSE);``if(ret
    != CUFFT_SUCCESS) {``cerr << "C2C FFT 失败！返回代码 " << ret << endl;``_FFTerror(ret);
    exit(1);``}``}``inline void cinverseFFT_(cufftHandle myFFTplan, double *A, double
    *B) {``int ret = cufftExecZ2Z(myFFTplan, (cufftDouble'
- en: '| ![B9780123884268000082/f08-03-9780123884268.jpg is missing](B9780123884268000082/f08-03-9780123884268.jpg)
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000082/f08-03-9780123884268.jpg 缺失](B9780123884268000082/f08-03-9780123884268.jpg)
    |'
- en: '| **Figure 8.3**Single- and double-precision 32x32x32 FFT performance. |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **图 8.3** 单精度和双精度32x32x32 FFT性能。 |'
- en: A **computeprof** width plot shows that the application is concurrently running
    on both GPUs. The following width plot was captured on a Dell Precision 7500 with
    two NVIDIA C2050 GPUs. [^(15)](#fn0065) The original color screenshot was converted
    to grayscale for this book. The H2D (Host to Device) and D2H (Device to Host)
    transfers are noted as well as the computational kernels that implement the 3D
    FFT.^(15)Access to this machine was provided by the ICHEC (Irish Center for High-End
    Computing) NVIDIA CUDA Research Center.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个**computeprof**宽度图显示，应用程序正在两块GPU上并行运行。以下的宽度图是在带有两块NVIDIA C2050 GPU的Dell Precision
    7500上捕获的。[^(15)](#fn0065)原始彩色截图已被转换为灰度图用于本书。图中标注了H2D（主机到设备）和D2H（设备到主机）的传输，以及实现3D
    FFT的计算内核。^(15)此机器的访问由ICHEC（爱尔兰高性能计算中心）NVIDIA CUDA研究中心提供。
- en: '| ![B9780123884268000082/f08-04-9780123884268.jpg is missing](B9780123884268000082/f08-04-9780123884268.jpg)
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000082/f08-04-9780123884268.jpg 缺失](B9780123884268000082/f08-04-9780123884268.jpg)
    |'
- en: '| Figure 8.4\. |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 图 8.4\. |'
- en: '| Width plot for a Dell Precision 7500 with two NVIDIA C2050 GPUs.  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 带有两块NVIDIA C2050 GPU的Dell Precision 7500宽度图。 |'
- en: Results from the same run on an HP Z800 illustrate the importance of the PCIe
    hardware. Slow performance in the asynchronous transfer of data to one of the
    GPUs can dramatically decrease performance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 来自同一次实验的HP Z800结果说明了PCIe硬件的重要性。在异步传输数据到其中一个GPU时，性能的下降可能会显著降低整体表现。
- en: '| ![B9780123884268000082/f08-05-9780123884268.jpg is missing](B9780123884268000082/f08-05-9780123884268.jpg)
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000082/f08-05-9780123884268000082.jpg 缺失](B9780123884268000082/f08-05-9780123884268.jpg)
    |'
- en: '| Figure 8.5\. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 图 8.5\. |'
- en: '| Width plot for an HP Z800 with two GPUs.  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 带有两块GPU的HP Z800宽度图。 |'
- en: This observation is confirmed via the **computeprof** height plot for each GPU
    on the HP Z800, as there are clear differences between the two GPU devices as
    well as variations in the transfers performed on the device.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这一观察通过HP Z800上每个GPU的**computeprof**高度图得到了确认，因为在两块GPU设备之间明显存在差异，并且设备上的传输也有所变化。
- en: '| ![B9780123884268000082/f08-06-9780123884268.jpg is missing](B9780123884268000082/f08-06-9780123884268.jpg)
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000082/f08-06-9780123884268000082.jpg 缺失](B9780123884268000082/f08-06-9780123884268.jpg)
    |'
- en: '| Figure 8.6\. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 图 8.6\. |'
- en: '| Height plot for device 0 on an HP Z800.  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| HP Z800上设备0的高度图。 |'
- en: '| ![B9780123884268000082/f08-07-9780123884268.jpg is missing](B9780123884268000082/f08-07-9780123884268.jpg)
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000082/f08-07-9780123884268000082.jpg 缺失](B9780123884268000082/f08-07-9780123884268.jpg)
    |'
- en: '| Figure 8.7\. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 图 8.7\. |'
- en: '| Height plot for device 1 on an HP Z800.  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| HP Z800上设备1的高度图。 |'
- en: MAGMA
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MAGMA
- en: The MAGMA project aims to develop a dense linear algebra library similar to
    LAPACK but for heterogeneous/hybrid architectures. The initial focus is to provide
    a package that will concurrently run on multicore processors and a single GPU
    to deliver the combined performance of this hybrid environment. Later efforts
    will combine multiple GPUs with multiple cores.The MAGMA team has made the conclusion
    that dense linear algebra (DLA), “has become a better fit for the new GPU architectures,
    to the point where DLA can run more efficiently on GPUs than on current, high-end
    homogeneous multicore-based systems” ([Nath, Stanimire, & Dongerra, 2010, p. 1](B978012388426800015X.xhtml#ref100)).
    According to Volkov, the current MAGMA BLAS libraries achieve up to 838 GF/s ([Volkov,
    2010](B978012388426800015X.xhtml#ref139)).The MAGMA software is freely downloadable
    from the University of Tennessee Innovative Computing Laboratory website. [^(16)](#fn9020)^(16)[http://icl.cs.utk.edu/](http://icl.cs.utk.edu/).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: MAGMA项目旨在开发一个类似于LAPACK的稠密线性代数库，但针对异构/混合架构。最初的重点是提供一个可以在多核处理器和单个GPU上同时运行的包，以发挥这种混合环境的综合性能。后续的工作将结合多个GPU与多个核心。MAGMA团队得出结论，稠密线性代数（DLA）“已经更适合新的GPU架构，以至于DLA在GPU上的运行效率比当前的高端同构多核系统更高”([Nath,
    Stanimire, & Dongerra, 2010, p. 1](B978012388426800015X.xhtml#ref100))。根据Volkov的说法，目前的MAGMA
    BLAS库能够达到838 GF/s的性能([Volkov, 2010](B978012388426800015X.xhtml#ref139))。MAGMA软件可以从田纳西大学创新计算实验室网站免费下载。[^(16)](#fn9020)^(16)[http://icl.cs.utk.edu/](http://icl.cs.utk.edu/)。
- en: phiGEMM Library
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: phiGEMM库
- en: The phiGEMM library is a freely available open source library that was written
    by Ivan Girotto and Filippo Spiga at ICHEC. [^(17)](#fn0070) It performs matrix–matrix
    multiplication (e.g., GEMM operations) on heterogeneous systems containing multiple
    GPUs and multicore processors.^(17)[http://qe-forge.org/projects/phigemm/](http://qe-forge.org/projects/phigemm/).The
    phiGEMM library extends the mapping of Fatica ([Fatica, 2009](B978012388426800015X.xhtml#ref50))
    to support single-precision, double-precision, and complex matrices. The LINPACK
    TOP 500 benchmark HPL benchmark suite uses this mapping for a heterogeneous matrix
    multiply as one of the core methods to evaluate the fastest supercomputers in
    the world. [^(18)](#fn0075) The phiGEMM library is able to deliver comparable
    performance to the LINPACK benchmark using a single GPU and multicore processor.
    As can be seen in [Figure 8.8](#f0045), even greater performance can be achieved
    using two or more GPUs.^(18)[http://www.nvidia.com/content/GTC-2010/pdfs/2057_GTC2010.pdf](http://www.nvidia.com/content/GTC-2010/pdfs/2057_GTC2010.pdf).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: phiGEMM 库是一个免费提供的开源库，由 ICHEC 的 Ivan Girotto 和 Filippo Spiga 编写。[^(17)](#fn0070)
    它在包含多个 GPU 和多核处理器的异构系统上执行矩阵乘法（例如，GEMM 操作）。^(17)[http://qe-forge.org/projects/phigemm/](http://qe-forge.org/projects/phigemm/)。phiGEMM
    库扩展了 Fatica 的映射（[Fatica, 2009](B978012388426800015X.xhtml#ref50)），支持单精度、双精度和复数矩阵。LINPACK
    TOP 500 基准 HPL 基准套件使用此映射进行异构矩阵乘法，作为评估全球最快超级计算机的核心方法之一。[^(18)](#fn0075) phiGEMM
    库能够在单个 GPU 和多核处理器上提供与 LINPACK 基准相当的性能。如[图 8.8](#f0045)所示，使用两块或更多 GPU 可以实现更高的性能。^(18)[http://www.nvidia.com/content/GTC-2010/pdfs/2057_GTC2010.pdf](http://www.nvidia.com/content/GTC-2010/pdfs/2057_GTC2010.pdf)。
- en: '| ![B9780123884268000082/f08-08-9780123884268.jpg is missing](B9780123884268000082/f08-08-9780123884268.jpg)
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000082/f08-08-9780123884268.jpg 图片缺失](B9780123884268000082/f08-08-9780123884268.jpg)
    |'
- en: '| **Figure 8.8**phiGEMM performance using one and two GPUs (data obtained on
    two Intel Xeon X5680 3.33 GHz and two Tesla NVIDIA C2050 GPUs). |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| **图 8.8** 使用一块和两块 GPU 的 phiGEMM 性能（数据在两台 Intel Xeon X5680 3.33 GHz 和两块 Tesla
    NVIDIA C2050 GPU 上获得）。 |'
- en: The phiGEMM library transparently manages memory and the asynchronous data transfers
    amongst all the devices in the system (i.e., multiple GPUs and multicore processors).
    It can process matrices that are much larger than the memory capacity of a single
    GPU by recursively multiplying smaller submatrices. Again, the library makes this
    process transparent to the user. [Figure 8.9](#f0050) shows performance using
    up to four GPUs for a 15 GB matrix. The phiGEMM library is being considered for
    inclusion in the MAGMA library discussed earlier.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: phiGEMM 库透明地管理系统中所有设备（即多个 GPU 和多核处理器）之间的内存和异步数据传输。它通过递归地乘以较小的子矩阵，可以处理远超单个 GPU
    内存容量的矩阵。同样，库将这一过程对用户透明化。[图 8.9](#f0050) 显示了使用最多四块 GPU 处理 15 GB 矩阵的性能。phiGEMM 库正在考虑加入之前讨论过的
    MAGMA 库中。
- en: '| ![B9780123884268000082/f08-09-9780123884268.jpg is missing](B9780123884268000082/f08-09-9780123884268.jpg)
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ![B9780123884268000082/f08-09-9780123884268.jpg is missing](B9780123884268000082/f08-09-9780123884268.jpg)
    |'
- en: '| **Figure 8.9**Performance of phiGEMM for a 15 GB matrix, M = K = N = 25000(DP)
    (data obtained on two Intel Xeon X5670 2.93 GHz and four Tesla NVIDIA C2050 GPUs).
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| **图 8.9** phiGEMM 在 15 GB 矩阵上表现的性能，M = K = N = 25000（双精度），数据采集自两台 Intel Xeon
    X5670 2.93 GHz 处理器和四个 Tesla NVIDIA C2050 GPU。 |'
- en: CURAND
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CURAND
- en: Generating random numbers on parallel computers is challenging. The naïve approach
    is to have each thread supply a different seed to a common random number generation
    method. This approach is not guaranteed to be independent and can inadvertently
    introduce correlations into the “random” data ([Coddington, 1997](B978012388426800015X.xhtml#ref19)).The
    CURAND library provides a simple and efficient API to generate high-quality *pseudorandom*
    and *quasirandom* numbers. A pseudorandom sequence of numbers satisfies most of
    the statistical properties of a truly random sequence but is generated by a deterministic
    algorithm. A quasirandom sequence of *n*-dimensional points is generated by a
    deterministic algorithm designed to fill an *n*-dimensional space evenly.Application
    areas include:■ **Simulation:** Random numbers are required to make things realistic.
    In the movies, computer-generated characters start and move in slightly different
    ways. People also arrive a airports at random intervals.■ **Sampling:** For many
    interesting problems, it is impractical to examine all possible cases. However,
    random sampling can provide insight into what constitutes “typical” behavior.■
    **Computer programming:** Testing an algorithm with random numbers is an effective
    way to find problems and programmer biases.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行计算机上生成随机数是一个挑战。传统方法是让每个线程为共同的随机数生成方法提供不同的种子。该方法并不能保证生成的随机数是独立的，可能无意间引入相关性到“随机”数据中（[Coddington,
    1997](B978012388426800015X.xhtml#ref19)）。CURAND 库提供了一个简单高效的 API 来生成高质量的 *伪随机*
    和 *准随机* 数字。伪随机数列满足大多数真正随机数列的统计特性，但其生成是通过一个确定性的算法。准随机数列是通过设计的确定性算法生成的 *n* 维点集，旨在均匀填充
    *n* 维空间。应用领域包括：■ **仿真：** 随机数是使事物逼真的必要条件。在电影中，计算机生成的角色以略微不同的方式开始和移动。人们也会在随机的间隔时间到达机场。■
    **采样：** 对于许多有趣的问题，逐一检查所有可能的情况是不现实的。然而，随机采样可以帮助深入了解“典型”行为是什么。■ **计算机编程：** 用随机数测试算法是一种有效的发现问题和程序员偏差的方式。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Commercial and research projects must now have parallel applications to compete
    for customer and research dollars. This need translates into pressure on software
    development efforts to control costs while supporting a range of rapidly evolving
    parallel hardware platforms. CUDA has been rapidly evolving to meet this need.
    Unlike current programming languages, CUDA was designed to create applications
    that run on hundreds of parallel processing elements and manage many thousands
    of threads. This design makes CUDA an attractive choice compared with current
    development languages like C++ and Java. It also creates a demand for CUDA developers.Projects
    like Copperhead are expanding the boundaries of what can be done with dynamic
    compilation and CUDA. Solid library support makes CUDA attractive for numerical
    and signal-processing applications. General interface packages like SWIG create
    the opportunity to add massively parallel CUDA support to many existing languages.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 商业和研究项目现在必须同时具备并行应用，以便争夺客户和研究资金。这一需求转化为对软件开发工作的一种压力，即在支持快速发展的并行硬件平台的同时控制成本。CUDA正在快速发展以满足这一需求。与当前的编程语言不同，CUDA的设计目的是创建能够在数百个并行处理单元上运行并管理成千上万个线程的应用程序。这一设计使得CUDA相比当前的开发语言如C++和Java更具吸引力。它还创造了对CUDA开发者的需求。像Copperhead这样的项目正在扩展动态编译和CUDA的应用边界。完善的库支持使得CUDA在数值计算和信号处理应用中非常有吸引力。像SWIG这样的通用接口包为将大规模并行的CUDA支持添加到许多现有语言中提供了机会。
