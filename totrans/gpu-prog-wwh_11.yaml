- en: Portable kernel-based models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://enccs.github.io/gpu-programming/8-portable-kernel-models/](https://enccs.github.io/gpu-programming/8-portable-kernel-models/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*[GPU programming: why, when and how?](../)* **   Portable kernel-based models'
  prefs: []
  type: TYPE_NORMAL
- en: '[Edit on GitHub](https://github.com/ENCCS/gpu-programming/blob/main/content/8-portable-kernel-models.rst)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs: []
  type: TYPE_NORMAL
- en: How to program GPUs with alpaka, C++ StdPar, Kokkos, OpenCL, and SYCL?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the differences between these programming models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: Be able to use portable kernel-based models to write simple codes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how different approaches to memory and synchronization in Kokkos
    and SYCL work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 60 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 30 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of the cross-platform portability ecosystems is to allow the same code
    to run on multiple architectures, therefore reducing code duplication. They are
    usually based on C++, and use function objects/lambda functions to define the
    loop body (i.e., the kernel), which can run on multiple architectures like CPU,
    GPU, and FPGA from different vendors. An exception to this is OpenCL, which originally
    offered only a C API (although currently also C++ API is available), and uses
    a separate-source model for the kernel code. However, unlike in many conventional
    CUDA or HIP implementations, the portability ecosystems require kernels to be
    written only once if one prefers to run it on CPU and GPU for example. Some notable
    cross-platform portability ecosystems are alpaka, Kokkos, OpenCL, RAJA, and SYCL.
    Kokkos, alpaka, and RAJA are individual projects whereas OpenCL and SYCL are standards
    followed by several projects implementing (and extending) them. For example, some
    notable SYCL implementations include [Intel oneAPI DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html),
    [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/) (previously known as
    hipSYCL or Open SYCL), [triSYCL](https://github.com/triSYCL/triSYCL), and [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/).
  prefs: []
  type: TYPE_NORMAL
- en: C++ StdPar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In C++17, the initial support for parallel execution of standard algorithms
    has been introduced. Most algorithms available via the standard `<algorithms>`
    header were given an overload accepting with an [*execution policy*](https://en.cppreference.com/w/cpp/algorithm)
    argument which allows the programmer to request parallel execution of the standard
    library function. While the main goal was to allow low-effort, high-level interface
    to run existing algorithms like `std::sort` on many CPU cores, implementations
    are allowed to use other hardware, and functions like `std::for_each` or `std::transform`
    offer great flexibility in writing the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: C++ StdPar, also called Parallel STL or PSTL, could be considered similar to
    directive-based models, as it is very high-level and does not give the programmer
    fine-grained control over data movement or any access to hardware-specific features
    like shared (local) memory. Even the GPU to run on is selected automatically,
    since standard C++ does not have the concept of a *device* (but there are vendor
    extensions allowing the programmer more control) However, for applications that
    already relies on algorithms from C++ standard library, StdPar can be a good way
    to reap the performance benefits of both CPUs and GPUs with minimal code modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'For GPU programming, all three vendors offer their implementations of StdPar
    with the ability to offload code to the GPU: NVIDIA has `nvc++`, AMD has experimental
    [roc-stdpar](https://github.com/ROCm/roc-stdpar), and Intel offers StdPar offload
    with their oneAPI compiler. [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)
    offers an independent StdPar implementation, able to target devices from all three
    vendors. While being a part of the C++ standard, the level of support and the
    maturity of StdPar implementations varies a lot between different compilers: not
    all compilers support all algorithms, and different heuristics for mapping the
    algorithm to hardware and for managing data movement can have effect on performance.'
  prefs: []
  type: TYPE_NORMAL
- en: StdPar compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The build process depends a lot on the used compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'AdaptiveCpp: Add `--acpp-stdpar` flag when calling `acpp`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intel oneAPI: Add `-fsycl -fsycl-pstl-offload=gpu` flags when calling `icpx`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA NVC++: Add `-stdpar` flag when calling `nvc++` (not supported with plain
    `nvcc`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StdPar programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In its simplest form, using C++ standard parallelism requires including an additional
    `<execution>` header and adding one argument to a supported standard library function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s look at the following sequential code sorting a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To make it run sorting on the GPU, only a minor modification is needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, when compiled with one of the supported compilers, the code will run the
    sorting on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: While the can initially seem very limiting, many standard algorithms, such as
    `std::transform`, `std::accumulate`, `std::transform_reduce`, and `std::for_each`
    can run custom functions over an array, thus allowing one to offload an arbitrary
    algorithm, as long as it does not violate typical limitations of GPU kernels,
    such as not throwing any exceptions and not doing system calls.
  prefs: []
  type: TYPE_NORMAL
- en: StdPar execution policies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In C++, there are four different execution policies to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::execution::seq`: run algorithm serially, don’t parallelize it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par`: allow parallelizing the algorithm (as if using multiple
    threads),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::unseq`: allow vectorizing the algorithm (as if using SIMD),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par_unseq`: allow both vectorizing and parallelizing the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main difference between `par` and `unseq` is related to thread progress
    and locks: using `unseq` or `par_unseq` requires that the algorithms does not
    contain mutexes and other locks between the processes, while `par` does not have
    this limitation.'
  prefs: []
  type: TYPE_NORMAL
- en: For GPU, the optimal choice is `par_unseq`, since this places the least requirement
    on the compiler in terms of operation ordering. While `par` is also supported
    in some cases, it is best avoided, both due to limited compiler support and as
    an indication that the algorithm is likely a poor fit for the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kokkos is an open-source performance portability ecosystem for parallelization
    on large heterogeneous hardware architectures of which development has mostly
    taken place on Sandia National Laboratories. The project started in 2011 as a
    parallel C++ programming model, but have since expanded into a more broad ecosystem
    including Kokkos Core (the programming model), Kokkos Kernels (math library),
    and Kokkos Tools (debugging, profiling and tuning tools). By preparing proposals
    for the C++ standard committee, the project also aims to influence the ISO/C++
    language standard such that, eventually, Kokkos capabilities will become native
    to the language standard. A more detailed introduction is found [HERE](https://www.sandia.gov/news/publications/hpc-annual-reports/article/kokkos/).
  prefs: []
  type: TYPE_NORMAL
- en: The Kokkos library provides an abstraction layer for a variety of different
    parallel programming models, currently CUDA, HIP, SYCL, HPX, OpenMP, and C++ threads.
    Therefore, it allows better portability across different hardware manufactured
    by different vendors, but introduces an additional dependency to the software
    stack. For example, when using CUDA, only CUDA installation is required, but when
    using Kokkos with NVIDIA GPUs, Kokkos and CUDA installation are both required.
    Kokkos is not a very popular choice for parallel programming, and therefore, learning
    and using Kokkos can be more difficult compared to more established programming
    models such as CUDA, for which a much larger amount of search results and Stack
    Overflow discussions can be found.
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Furthermore, one challenge with some cross-platform portability libraries is
    that even on the same system, different projects may require different combinations
    of compilation settings for the portability library. For example, in Kokkos, one
    project may wish the default execution space to be a CUDA device, whereas another
    requires a CPU. Even if the projects prefer the same execution space, one project
    may desire the Unified Memory to be the default memory space and the other may
    wish to use pinned GPU memory. It may be burdensome to maintain a large number
    of library instances on a single system.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Kokkos offers a simple way to compile Kokkos library simultaneously
    with the user project. This is achieved by specifying Kokkos compilation settings
    (see [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Compiling.html))
    and including the Kokkos Makefile in the user Makefile. CMake is also supported.
    This way, the user application and Kokkos library are compiled together. The following
    is an example Makefile for a single-file Kokkos project (hello.cpp) that uses
    CUDA (Volta architecture) as the backend (default execution space) and Unified
    Memory as the default memory space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To build a **hello.cpp** project with the above Makefile, no steps other than
    cloning the Kokkos project into the current directory is required.
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When starting to write a project using Kokkos, the first step is understand
    Kokkos initialization and finalization. Kokkos must be initialized by calling
    `Kokkos::initialize(int& argc, char* argv[])` and finalized by calling `Kokkos::finalize()`.
    More details are given in [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Initialization.html).
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos uses an execution space model to abstract the details of parallel hardware.
    The execution space instances map to the available backend options such as CUDA,
    OpenMP, HIP, or SYCL. If the execution space is not explicitly chosen by the programmer
    in the source code, the default execution space `Kokkos::DefaultExecutionSpace`
    is used. This is chosen when the Kokkos library is compiled. The Kokkos execution
    space model is described in more detail in [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Machine-Model.html#kokkos-spaces).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Kokkos uses a memory space model for different types of memory, such
    as host memory or device memory. If not defined explicitly, Kokkos uses the default
    memory space specified during Kokkos compilation as described [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Machine-Model.html#kokkos-memory-spaces).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a Kokkos program that initializes Kokkos and
    prints the execution space and memory space instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With Kokkos, the data can be accessed either through raw pointers or through
    Kokkos Views. With raw pointers, the memory allocation into the default memory
    space can be done using `Kokkos::kokkos_malloc(n * sizeof(int))`. Kokkos Views
    are a data type that provides a way to access data more efficiently in memory
    corresponding to a certain Kokkos memory space, such as host memory or device
    memory. A 1-dimensional view of type int* can be created by `Kokkos::View<int*>
    a("a", n)`, where `"a"` is a label, and `n` is the size of the allocation in the
    number of integers. Kokkos determines the optimal layout for the data at compile
    time for best overall performance as a function of the computer architecture.
    Furthermore, Kokkos handles the deallocation of such memory automatically. More
    details about Kokkos Views are found [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/View.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, Kokkos provides three different parallel operations: `parallel_for`,
    `parallel_reduce`, and `parallel_scan`. The `parallel_for` operation is used to
    execute a loop in parallel. The `parallel_reduce` operation is used to execute
    a loop in parallel and reduce the results to a single value. The `parallel_scan`
    operation implements a prefix scan. The usage of `parallel_for` and `parallel_reduce`
    are demonstrated in the examples later in this chapter. More detail about the
    parallel operations are found [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/ParallelDispatch.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Run Kokkos hello.cpp example in simple steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following should work on AMD VEGA90A devices straight out of the box (needs
    ROCm installation). On NVIDIA Volta V100 devices (needs CUDA installation), use
    the variables commented out on the Makefile.
  prefs: []
  type: TYPE_NORMAL
- en: '`git clone https://github.com/kokkos/kokkos.git`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the above Makefile into the current folder (make sure the indentation of
    the last line is tab, and not space)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the above hello.cpp file into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`make`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`./hello`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenCL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenCL is a cross-platform, open-standard API for writing parallel programs
    that execute across heterogeneous platforms consisting of CPUs, GPUs, FPGAs and
    other devices. The first version of OpenCL (1.0) was released in December 2008,
    and the latest version of OpenCL (3.0) was released in September 2020\. OpenCL
    is supported by a number of vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm.
    It is a royalty-free standard, and the OpenCL specification is maintained by the
    Khronos Group. OpenCL provides a low-level programming interface initially based
    on C, but more recently also a C++ interface has become available.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenCL supports two modes for compiling the programs: online and offline. Online
    compilation occurs at runtime, when the host program calls a function to compile
    the source code. Online mode allows dynamic generation and loading of kernels,
    but may incur some overhead due to compilation time and possible errors. Offline
    compilation occurs before runtime, when the source code of a kernel is compiled
    into a binary format that can be loaded by the host program. This mode allows
    faster execution and better optimization of kernels, but may limit the portability
    of the program, because the binary can only run on the architectures it was compiled
    for.'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCL comes bundled with several parallel programming ecosystems, such as
    NVIDIA CUDA and Intel oneAPI. For example, after successfully installing such
    packages and setting up the environment, one may simply compile an OpenCL program
    by the commands such as `icx cl_devices.c -lOpenCL` (Intel oneAPI) or `nvcc cl_devices.c
    -lOpenCL` (NVIDIA CUDA), where `cl_devices.c` is the compiled file. Unlike most
    other programming models, OpenCL stores kernels as text and compiles them for
    the device in runtime (JIT-compilation), and thus does not require any special
    compiler support: one can compile the code using simply `gcc cl_devices.c -lOpenCL`
    (or `g++` when using C++ API), as long as the required libraries and headers are
    installed in a standard locations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The AMD compiler installed on LUMI supports both OpenCL C and C++ API, the
    latter with some limitations. To compile a program, you can use the AMD compilers
    on a GPU partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: OpenCL programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenCL programs consist of two parts: a host program that runs on the host
    device (usually a CPU) and one or more kernels that run on compute devices (such
    as GPUs). The host program is responsible for the tasks such as managing the devices
    for the selected platform, allocating memory objects, building and enqueueing
    kernels, and managing memory objects.'
  prefs: []
  type: TYPE_NORMAL
- en: The first steps when writing an OpenCL program are to initialize the OpenCL
    environment by selecting the platform and devices, creating a context or contexts
    associated with the selected device(s), and creating a command queue for each
    device. A simple example of selecting the default device, creating a context and
    a queue associated with the device is show below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenCL provides two main programming models to manage the memory hierarchy
    of host and accelerator devices: buffers and shared virtual memory (SVM). Buffers
    are the traditional memory model of OpenCL, where the host and the devices have
    separate address spaces and the programmer has to explicitly specify the memory
    allocations and how and where the memory is accessed. This can be done with class
    `cl::Buffer` and functions such as `cl::CommandQueue::enqueueReadBuffer()`. Buffers
    are supported since early versions of OpenCL, and work well across different architectures.
    Buffers can also take advantage of device-specific memory features, such as constant
    or local memory.'
  prefs: []
  type: TYPE_NORMAL
- en: SVM is a newer memory model of OpenCL, introduced in version 2.0, where the
    host and the devices share a single virtual address space. Thus, the programmer
    can use the same pointers to access the data from host and devices simplifying
    the programming effort. In OpenCL, SVM comes in different levels such as coarse-grained
    buffer SVM, fine-grained buffer SVM, and fine-grained system SVM. All levels allow
    using the same pointers across a host and devices, but they differ in their granularity
    and synchronization requirements for the memory regions. Furthermore, the support
    for SVM is not universal across all OpenCL platforms and devices, and for example,
    GPUs such as NVIDIA V100 and A100 only support the coarse-grained SVM buffer.
    This level requires explicit synchronization for memory accesses from a host and
    devices (using functions such as `cl::CommandQueue::enqueueMapSVM()` and `cl::CommandQueue::enqueueUnmapSVM()`),
    making the usage of SVM less convenient. It is further noted that this is unlike
    the regular Unified Memory offered by CUDA, which is closer to the fine-grained
    system SVM level in OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL uses a separate-source kernel model where the kernel code is often kept
    in separate files that may be compiled during runtime. The model allows the kernel
    source code to be passed as a string to the OpenCL driver after which the program
    object can be executed on a specific device. Although referred to as the separate-source
    kernel model, the kernels can still be defined as a string in the host program
    compilation units as well, which may be a more convenient approach in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: The online compilation with the separate-source kernel model has several advantages
    over the binary model, which requires offline compilation of kernels into device-specific
    binaries that can are loaded by the application at runtime. Online compilation
    preserves the portability and flexibility of OpenCL, as the same kernel source
    code can run on any supported device. Furthermore, dynamic optimization of kernels
    based on runtime information, such as input size, work-group size, or device capabilities,
    is possible. An example of an OpenCL kernel, defined by a string in the host compilation
    unit, and assigning the global thread index into a global device memory is shown
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The above kernel named `dot` and stored in the string `kernel_source` can be
    set to build in the host code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: SYCL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[SYCL](https://www.khronos.org/sycl/) is a royalty-free, open-standard C++
    programming model for multi-device programming. It provides a high-level, single-source
    programming model for heterogeneous systems, including GPUs. There are several
    implementations of the standard. For GPU programming, [Intel oneAPI DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)
    and [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/) (also known as
    hipSYCL) are the most popular for desktop and HPC GPUs; [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/)
    is a good choice for embedded devices. The same standard-compliant SYCL code should
    work with any implementation, but they are not binary-compatible.'
  prefs: []
  type: TYPE_NORMAL
- en: The most recent version of the SYCL standard is SYCL 2020, and it is the version
    we will be using in this course.
  prefs: []
  type: TYPE_NORMAL
- en: SYCL compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intel oneAPI DPC++
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For targeting Intel GPUs, it is enough to install [Intel oneAPI Base Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html).
    Then, the compilation is as simple as `icpx -fsycl file.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to use oneAPI for NVIDIA and AMD GPUs. In addition to oneAPI
    Base Toolkit, the vendor-provided runtime (CUDA or HIP) and the corresponding
    [Codeplay oneAPI plugin](https://codeplay.com/solutions/oneapi/) must be installed.
    Then, the code can be compiled using Intel LLVM compiler bundled with oneAPI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clang++ -fsycl -fsycl-targets=nvidia_gpu_sm_86 file.cpp` for targeting CUDA
    8.6 NVIDIA GPU,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clang++ -fsycl -fsycl-targets=amd_gpu_gfx90a` for targeting GFX90a AMD GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaptiveCpp
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using AdaptiveCpp for NVIDIA or AMD GPUs also requires having CUDA or HIP installed
    first. Then `acpp` can be used for compiling the code, specifying the target devices.
    For example, here is how to compile the program supporting an AMD and an NVIDIA
    device:'
  prefs: []
  type: TYPE_NORMAL
- en: '`acpp --acpp-targets=''hip:gfx90a;cuda:sm_70'' file.cpp`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SYCL on LUMI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LUMI does not have a system-wide installation of any SYCL framework, but a
    recent AdaptiveCpp installation is available in CSC modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The default compilation target is preset to MI250 GPUs, so to compile a single
    C++ file it is enough to call `acpp -O2 file.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When running applications built with AdaptiveCpp, one can often see the warning
    “dag_direct_scheduler: Detected a requirement that is neither of discard access
    mode”, reflecting the lack of an optimization hint when using buffer-accessor
    model. The warning is harmless and can be ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: SYCL programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SYCL is, in many aspects, similar to OpenCL, but uses, like Kokkos, a single-source
    model with kernel lambdas.
  prefs: []
  type: TYPE_NORMAL
- en: 'To submit a task to device, first a sycl::queue must be created, which is used
    as a way to manage the task scheduling and execution. In the simplest case, that’s
    all the initialization one needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If one wants more control, the device can be explicitly specified, or additional
    properties can be passed to a queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Memory management can be done in two different ways: *buffer-accessor* model
    and *unified shared memory* (USM). The choice of the memory management models
    also influences how the GPU tasks are synchronized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *buffer-accessor* model, a `sycl::buffer` objects are used to represent
    arrays of data. A buffer is not mapped to any single one memory space, and can
    be migrated between the GPU and the CPU memory transparently. The data in `sycl::buffer`
    cannot be read or written directly, an accessor must be created. `sycl::accessor`
    objects specify the location of data access (host or a certain GPU kernel) and
    the access mode (read-only, write-only, read-write). Such approach allows optimizing
    task scheduling by building a directed acyclic graph (DAG) of data dependencies:
    if kernel *A* creates a write-only accessor to a buffer, and then kernel *B* is
    submitted with a read-only accessor to the same buffer, and then a host-side read-only
    accessor is requested, then it can be deduced that *A* must complete before *B*
    is launched and also that the results must be copied to the host before the host
    task can proceed, but the host task can run in parallel with kernel *B*. Since
    the dependencies between tasks can be built automatically, by default SYCL uses
    *out-of-order queues*: when two tasks are submitted to the same `sycl::queue`,
    it is not guaranteed that the second one will launch only after the first one
    completes. When launching a kernel, accessors must be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Buffer-accessor model simplifies many aspects of heterogeneous programming and
    prevents many synchronization-related bugs, but it only allows very coarse control
    of data movement and kernel execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *USM* model is similar to how NVIDIA CUDA or AMD HIP manage memory. The
    programmer has to explicitly allocate the memory on the device (`sycl::malloc_device`),
    on the host (`sycl::malloc_host`), or in the shared memory space (`sycl::malloc_shared`).
    Despite its name, unified shared memory, and the similarity to OpenCL’s SVM, not
    all USM allocations are shared: for example, a memory allocated by `sycl::malloc_device`
    cannot be accessed from the host. The allocation functions return memory pointers
    that can be used directly, without accessors. This means that the programmer have
    to ensure the correct synchronization between host and device tasks to avoid data
    races. With USM, it is often convenient to use *in-order queues* with USM, instead
    of the default *out-of-order* queues. More information on USM can be found in
    the [Section 4.8 of SYCL 2020 specification](https://registry.khronos.org/SYCL/specs/sycl-2020/html/sycl-2020.html#sec:usm).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Exercise: Implement SAXPY in SYCL'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to write (fill-in-the-blanks) a simple code doing
    SAXPY (vector addition).
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile and run the code interactively, first make an allocation and load
    the AdaptiveCpp module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can run a simple device-detection utility to check that a GPU is available
    (note `srun`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: If you have not done it already, clone the repository using `git clone https://github.com/ENCCS/gpu-programming.git`
    or **update it** using `git pull origin main`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the example code in `content/examples/portable-kernel-models/exercise-sycl-saxpy.cpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile and run the code, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The code will not compile as-is! Your task is to fill in missing bits indicated
    by `TODO` comments. You can also test your understanding using the “Bonus questions”
    in the code.
  prefs: []
  type: TYPE_NORMAL
- en: If you feel stuck, take a look at the `exercise-sycl-saxpy-solution.cpp` file.
  prefs: []
  type: TYPE_NORMAL
- en: alpaka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [alpaka](https://github.com/alpaka-group/alpaka3) library is an open-source
    header-only C++20 abstraction library for accelerator development.
  prefs: []
  type: TYPE_NORMAL
- en: Its aim is to provide performance portability across accelerators by abstracting
    the underlying levels of parallelism. The project provides a single-source C++
    API that enables developers to write parallel code once and run it on different
    hardware architectures without modification. The name “alpaka” comes from **A**bstractions
    for **L**evels of **P**arallelism, **A**lgorithms, and **K**ernels for **A**ccelerators.
    The library is platform-independent and supports the concurrent and cooperative
    use of multiple devices, including host CPUs (x86, ARM, and RISC-V) and GPUs from
    different vendors (NVIDIA, AMD, and Intel). A variety of accelerator backends,
    CUDA, HIP, SYCL, OpenMP, and serial execution, are available and can be selected
    based on the target device. Only a single implementation of a user kernel is required,
    expressed as a function object with a standardized interface. This eliminates
    the need to write specialized CUDA, HIP, SYCL, OpenMP, Intel TBB or threading
    code. Moreover, multiple accelerator backends can be combined to target different
    vendor hardware within a single system and even within a single application.
  prefs: []
  type: TYPE_NORMAL
- en: The abstraction is based on a virtual index domain decomposed into equally sized
    chunks called frames. **alpaka** provides a uniform abstraction to traverse these
    frames, independent of the underlying hardware. Algorithms to be parallelized
    map the chunked index domain and native worker threads onto the data, expressing
    the computation as kernels that are executed in parallel threads (SIMT), thereby
    also leveraging SIMD units. Unlike native parallelism models such as CUDA, HIP,
    and SYCL, **alpaka** kernels are not restricted to three dimensions. Explicit
    caching of data within a frame via shared memory allows developers to fully unleash
    the performance of the compute device. Additionally, **alpaka** offers primitive
    functions such as iota, transform, transform-reduce, reduce, and concurrent, simplifying
    the development of portable high-performance applications. Host, device, mapped,
    and managed multi-dimensional views provide a natural way to operate on data.
  prefs: []
  type: TYPE_NORMAL
- en: Here we demonstrate the usage of **alpaka3**, which is a complete rewrite of
    [alpaka](https://github.com/alpaka-group/alpaka). It is planned to merge this
    separate codebase back into the mainline alpaka repository before the first release
    in Q2/Q3 of 2026. Nevertheless, the code is well-tested and can be used for development
    today.
  prefs: []
  type: TYPE_NORMAL
- en: Installing alpaka on your system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For ease of use, we recommend installing alpaka using CMake as described below.
    For other ways to use alpaka in your projects, see the [alpaka3 documentation](https://alpaka3.readthedocs.io/en/latest/basic/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Clone the repository**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clone the alpaka source code from GitHub to a directory of your choice:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Set installation directory**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `ALPAKA_DIR` environment variable to the directory where you want to
    install alpaka. This can be any directory you choose where you have write access.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Build and install**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a build directory and use CMake to build and install alpaka. We use `CMAKE_INSTALL_PREFIX`
    to tell CMake where to install the library.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Update environment**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To make sure that other projects can find your alpaka installation, you should
    add the installation directory to your `CMAKE_PREFIX_PATH`. You can do this by
    adding the following line to your shell configuration file (e.g. `~/.bashrc`):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will need to source your shell configuration file or open a new terminal
    for the changes to take effect.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: alpaka Compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We recommend building your projects which use alpaka using CMake. A variety
    of strategies can be used to deal with building your application for a specific
    device or set of devices. Here we show a minimal way to get started, but this
    is by no means the only way to set up your projects. Please refer to the [alpaka3
    documentation](https://alpaka3.readthedocs.io/en/latest/basic/install.html) for
    alternative ways to use alpaka in your project, including a way to make your source
    code agnostic to the accelerator being targeted by defining a device specification
    in CMake.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates a `CMakeLists.txt` for a single-file project
    using alpaka3 (`main.cpp` which is presented in the section below):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Using alpaka on LUMI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To load the environment for using the AMD GPUs on LUMI with HIP, one can use
    the following modules -
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: alpaka Programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When starting with alpaka3, the first step is understanding the **device selection
    model**. Unlike frameworks that require explicit initialization calls, alpaka3
    uses a device specification to determine which backend and hardware to use. The
    device specification consists of two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**API**: The parallel programming interface (host, cuda, hip, oneApi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Device Kind**: The type of hardware (cpu, nvidiaGpu, amdGpu, intelGpu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we specify and use these at runtime to select and initialize devices. The
    device selection process is described in detail in the alpaka3 documentation.
  prefs: []
  type: TYPE_NORMAL
- en: alpaka3 uses an **execution space model** to abstract parallel hardware details.
    A device selector is created using `alpaka::onHost::makeDeviceSelector(devSpec)`,
    which returns an object that can query available devices and create device instances
    for the selected backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates a basic alpaka program that initializes
    a device and prints information about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: alpaka3 provides memory management abstractions through buffers and views. Memory
    can be allocated on host or device using `alpaka::allocBuf<T, Idx>(device, extent)`.
    Data transfers between host and device are handled through `alpaka::memcpy(queue,
    dst, src)`. The library automatically manages memory layouts for optimal performance
    on different architectures.
  prefs: []
  type: TYPE_NORMAL
- en: For parallel execution, alpaka3 provides kernel abstractions. Kernels are defined
    as functors or lambda functions and executed using work division specifications
    that define the parallelization strategy. The framework supports various parallel
    patterns including element-wise operations, reductions, and scans.
  prefs: []
  type: TYPE_NORMAL
- en: Tour of **alpaka** Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now we will quickly explore the most commonly used features of alpaka and go
    over some basic usage. A quick reference of commonly used alpaka features is available
    [here.](https://alpaka3.readthedocs.io/en/latest/basic/cheatsheet.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**General setup**: Include the consolidated header once and you are ready to
    start using alpaka.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**Accelerator, platform, and device management**: Select devices by combining
    the desired API with the appropriate hardware kind using the device selector.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '**Queues and events**: Create blocking or non-blocking queues per device, record
    events, and synchronize work as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '**Memory management**: Allocate host, device, mapped, unified, or deferred
    buffers, create non-owning views, and move data portably with memcpy, memset,
    and fill.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '**Kernel execution**: Build a FrameSpec manually or request one tuned for your
    data type, then enqueue kernels with automatic or explicit executors.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '**Kernel implementation**: Write kernels as functors annotated with ALPAKA_FN_ACC,
    use shared memory, synchronization, atomics, and math helpers directly inside
    the kernel body.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Run alpaka3 Example in Simple Steps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following example works on systems with CMake 3.25+ and an appropriate C++
    compiler. For GPU execution, ensure the corresponding runtime (CUDA, ROCm, or
    oneAPI) is installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory for your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Copy the CMakeLists.txt from above into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the main.cpp file into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configure and build:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the executable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The device specification system allows you to select the target device at CMake
    configuration time. The format is `"api:deviceKind"`, where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**api**: The parallel programming interface (`host`, `cuda`, `hip`, `oneApi`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**deviceKind**: The type of device (`cpu`, `nvidiaGpu`, `amdGpu`, `intelGpu`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Available combinations are: `host:cpu`, `cuda:nvidiaGpu`, `hip:amdGpu`, `oneApi:cpu`,
    `oneApi:intelGpu`, `oneApi:nvidiaGpu`, `oneApi:amdGpu`'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA, HIP, or Intel backends only work if the CUDA SDK, HIP SDK, or OneAPI
    SDK are available respectively
  prefs: []
  type: TYPE_NORMAL
- en: Expected output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The device name will vary depending on your hardware (e.g., “NVIDIA A100”, “AMD
    MI250X”, or your CPU model).
  prefs: []
  type: TYPE_NORMAL
- en: Compile and Execute Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can test the **alpaka** provided examples from the [example section](#examples).
    The examples have hard coded the usage of the AMD ROCm platform required on LUMI.
    To switch to CPU usage only you can simply replace `ap::onHost::makeDeviceSelector(ap::api::hip,
    ap::deviceKind::amdGpu);` with `ap::onHost::makeDeviceSelector(ap::api::host,
    ap::deviceKind::cpu);`
  prefs: []
  type: TYPE_NORMAL
- en: The following steps assume you have downloaded alpaka already and the path to
    the **alapka** source code is stored in the environment variable `ALPAKA_DIR`.
    To test the example copy the code into a file `main.cpp`
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, [click here](https://godbolt.org/z/69exnG4xb) to try the first
    example using in the godbolt compiler explorer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To use oneAPI Sycl with AMD or NVIDIA Gpus you must install the corresponding
    Codeplay oneAPI plugin as described [here](https://codeplay.com/solutions/oneapi/plugins/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To use oneAPI Sycl with AMD or NVIDIA Gpus you must install the corresponding
    Codeplay oneAPI plugin as described [here](https://codeplay.com/solutions/oneapi/plugins/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Exercise: Write a vector add kernel in alpaka'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to write (fill-in-the-blanks) a simple kernel
    to add two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile and run the code interactively, first we first need to get an allocation
    on a GPU node and load the modules for alpaka:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can run a simple device-detection utility to check that a GPU is available
    (note `srun`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s look at the code to set up the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Below we use fetch content with our CMake to get started with alpaka quickly.
  prefs: []
  type: TYPE_NORMAL
- en: CMakeLists.txt
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Below we have the main alpaka code doing a vector addition on device using a
    high level transform function
  prefs: []
  type: TYPE_NORMAL
- en: main.cpp
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: To set up our project, we create a folder and place our CMakeLists.txt and main.cpp
    in there.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile and run the code, use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Now your task will be to write and launch your first alpaka kernel. This kernel
    will do the vector addition and we will use this instead of the transform helper.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the vector add kernel
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parallel for with Unified Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Parallel for with GPU buffers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Asynchronous parallel for kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Pros and cons of cross-platform portability ecosystems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: General observations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The amount of code duplication is minimized.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The same code can be compiled to multiple architectures from different vendors.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Limited learning resources compared to CUDA (Stack Overflow, course material,
    documentation).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambda-based kernel models (Kokkos, SYCL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Higher level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Less knowledge of the underlying architecture is needed for initial porting.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Very nice and readable source code (C++ API).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The models are relatively new and not very popular yet.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Functor-based kernel model (alpaka)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very good portability.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Higher level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Low-level API always awailable which gives more control and allows fine tuning.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: User friendly C++ API for both the host and kernel code.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Small community and ecosystem.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate-source kernel models (OpenCL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very good portability.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Mature ecosystem.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Limited number of vendor-provided libraries.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Low-level API gives more control and allows fine tuning.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Both C and C++ APIs available (C++ API is less well supported).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The low-level API and separate-source kernel model are less user friendly.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: C++ Standard Parallelism (StdPar, PSTL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very high level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Easy to speed up code which already relying on STL algorithms.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Very little control over hardware.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Support by compilers is improving, but is far from mature.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: General code organization is similar to non-portable kernel-based models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As long as no vendor-specific functionality is used, the same code can run on
    any GPU. [Previous](../7-non-portable-kernel-models/ "Non-portable kernel-based
    models") [Next](../9-language-support/ "High-level language support")
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: © Copyright 2023-2024, The contributors.
  prefs: []
  type: TYPE_NORMAL
- en: Built with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme)
    provided by [Read the Docs](https://readthedocs.org). Questions
  prefs: []
  type: TYPE_NORMAL
- en: How to program GPUs with alpaka, C++ StdPar, Kokkos, OpenCL, and SYCL?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the differences between these programming models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: Be able to use portable kernel-based models to write simple codes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand how different approaches to memory and synchronization in Kokkos
    and SYCL work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 60 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 30 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of the cross-platform portability ecosystems is to allow the same code
    to run on multiple architectures, therefore reducing code duplication. They are
    usually based on C++, and use function objects/lambda functions to define the
    loop body (i.e., the kernel), which can run on multiple architectures like CPU,
    GPU, and FPGA from different vendors. An exception to this is OpenCL, which originally
    offered only a C API (although currently also C++ API is available), and uses
    a separate-source model for the kernel code. However, unlike in many conventional
    CUDA or HIP implementations, the portability ecosystems require kernels to be
    written only once if one prefers to run it on CPU and GPU for example. Some notable
    cross-platform portability ecosystems are alpaka, Kokkos, OpenCL, RAJA, and SYCL.
    Kokkos, alpaka, and RAJA are individual projects whereas OpenCL and SYCL are standards
    followed by several projects implementing (and extending) them. For example, some
    notable SYCL implementations include [Intel oneAPI DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html),
    [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/) (previously known as
    hipSYCL or Open SYCL), [triSYCL](https://github.com/triSYCL/triSYCL), and [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/).
  prefs: []
  type: TYPE_NORMAL
- en: C++ StdPar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In C++17, the initial support for parallel execution of standard algorithms
    has been introduced. Most algorithms available via the standard `<algorithms>`
    header were given an overload accepting with an [*execution policy*](https://en.cppreference.com/w/cpp/algorithm)
    argument which allows the programmer to request parallel execution of the standard
    library function. While the main goal was to allow low-effort, high-level interface
    to run existing algorithms like `std::sort` on many CPU cores, implementations
    are allowed to use other hardware, and functions like `std::for_each` or `std::transform`
    offer great flexibility in writing the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: C++ StdPar, also called Parallel STL or PSTL, could be considered similar to
    directive-based models, as it is very high-level and does not give the programmer
    fine-grained control over data movement or any access to hardware-specific features
    like shared (local) memory. Even the GPU to run on is selected automatically,
    since standard C++ does not have the concept of a *device* (but there are vendor
    extensions allowing the programmer more control) However, for applications that
    already relies on algorithms from C++ standard library, StdPar can be a good way
    to reap the performance benefits of both CPUs and GPUs with minimal code modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'For GPU programming, all three vendors offer their implementations of StdPar
    with the ability to offload code to the GPU: NVIDIA has `nvc++`, AMD has experimental
    [roc-stdpar](https://github.com/ROCm/roc-stdpar), and Intel offers StdPar offload
    with their oneAPI compiler. [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)
    offers an independent StdPar implementation, able to target devices from all three
    vendors. While being a part of the C++ standard, the level of support and the
    maturity of StdPar implementations varies a lot between different compilers: not
    all compilers support all algorithms, and different heuristics for mapping the
    algorithm to hardware and for managing data movement can have effect on performance.'
  prefs: []
  type: TYPE_NORMAL
- en: StdPar compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The build process depends a lot on the used compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'AdaptiveCpp: Add `--acpp-stdpar` flag when calling `acpp`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intel oneAPI: Add `-fsycl -fsycl-pstl-offload=gpu` flags when calling `icpx`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA NVC++: Add `-stdpar` flag when calling `nvc++` (not supported with plain
    `nvcc`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StdPar programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In its simplest form, using C++ standard parallelism requires including an additional
    `<execution>` header and adding one argument to a supported standard library function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s look at the following sequential code sorting a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'To make it run sorting on the GPU, only a minor modification is needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Now, when compiled with one of the supported compilers, the code will run the
    sorting on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: While the can initially seem very limiting, many standard algorithms, such as
    `std::transform`, `std::accumulate`, `std::transform_reduce`, and `std::for_each`
    can run custom functions over an array, thus allowing one to offload an arbitrary
    algorithm, as long as it does not violate typical limitations of GPU kernels,
    such as not throwing any exceptions and not doing system calls.
  prefs: []
  type: TYPE_NORMAL
- en: StdPar execution policies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In C++, there are four different execution policies to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::execution::seq`: run algorithm serially, don’t parallelize it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par`: allow parallelizing the algorithm (as if using multiple
    threads),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::unseq`: allow vectorizing the algorithm (as if using SIMD),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par_unseq`: allow both vectorizing and parallelizing the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main difference between `par` and `unseq` is related to thread progress
    and locks: using `unseq` or `par_unseq` requires that the algorithms does not
    contain mutexes and other locks between the processes, while `par` does not have
    this limitation.'
  prefs: []
  type: TYPE_NORMAL
- en: For GPU, the optimal choice is `par_unseq`, since this places the least requirement
    on the compiler in terms of operation ordering. While `par` is also supported
    in some cases, it is best avoided, both due to limited compiler support and as
    an indication that the algorithm is likely a poor fit for the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kokkos is an open-source performance portability ecosystem for parallelization
    on large heterogeneous hardware architectures of which development has mostly
    taken place on Sandia National Laboratories. The project started in 2011 as a
    parallel C++ programming model, but have since expanded into a more broad ecosystem
    including Kokkos Core (the programming model), Kokkos Kernels (math library),
    and Kokkos Tools (debugging, profiling and tuning tools). By preparing proposals
    for the C++ standard committee, the project also aims to influence the ISO/C++
    language standard such that, eventually, Kokkos capabilities will become native
    to the language standard. A more detailed introduction is found [HERE](https://www.sandia.gov/news/publications/hpc-annual-reports/article/kokkos/).
  prefs: []
  type: TYPE_NORMAL
- en: The Kokkos library provides an abstraction layer for a variety of different
    parallel programming models, currently CUDA, HIP, SYCL, HPX, OpenMP, and C++ threads.
    Therefore, it allows better portability across different hardware manufactured
    by different vendors, but introduces an additional dependency to the software
    stack. For example, when using CUDA, only CUDA installation is required, but when
    using Kokkos with NVIDIA GPUs, Kokkos and CUDA installation are both required.
    Kokkos is not a very popular choice for parallel programming, and therefore, learning
    and using Kokkos can be more difficult compared to more established programming
    models such as CUDA, for which a much larger amount of search results and Stack
    Overflow discussions can be found.
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Furthermore, one challenge with some cross-platform portability libraries is
    that even on the same system, different projects may require different combinations
    of compilation settings for the portability library. For example, in Kokkos, one
    project may wish the default execution space to be a CUDA device, whereas another
    requires a CPU. Even if the projects prefer the same execution space, one project
    may desire the Unified Memory to be the default memory space and the other may
    wish to use pinned GPU memory. It may be burdensome to maintain a large number
    of library instances on a single system.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Kokkos offers a simple way to compile Kokkos library simultaneously
    with the user project. This is achieved by specifying Kokkos compilation settings
    (see [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Compiling.html))
    and including the Kokkos Makefile in the user Makefile. CMake is also supported.
    This way, the user application and Kokkos library are compiled together. The following
    is an example Makefile for a single-file Kokkos project (hello.cpp) that uses
    CUDA (Volta architecture) as the backend (default execution space) and Unified
    Memory as the default memory space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: To build a **hello.cpp** project with the above Makefile, no steps other than
    cloning the Kokkos project into the current directory is required.
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When starting to write a project using Kokkos, the first step is understand
    Kokkos initialization and finalization. Kokkos must be initialized by calling
    `Kokkos::initialize(int& argc, char* argv[])` and finalized by calling `Kokkos::finalize()`.
    More details are given in [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Initialization.html).
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos uses an execution space model to abstract the details of parallel hardware.
    The execution space instances map to the available backend options such as CUDA,
    OpenMP, HIP, or SYCL. If the execution space is not explicitly chosen by the programmer
    in the source code, the default execution space `Kokkos::DefaultExecutionSpace`
    is used. This is chosen when the Kokkos library is compiled. The Kokkos execution
    space model is described in more detail in [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Machine-Model.html#kokkos-spaces).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Kokkos uses a memory space model for different types of memory, such
    as host memory or device memory. If not defined explicitly, Kokkos uses the default
    memory space specified during Kokkos compilation as described [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Machine-Model.html#kokkos-memory-spaces).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a Kokkos program that initializes Kokkos and
    prints the execution space and memory space instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: With Kokkos, the data can be accessed either through raw pointers or through
    Kokkos Views. With raw pointers, the memory allocation into the default memory
    space can be done using `Kokkos::kokkos_malloc(n * sizeof(int))`. Kokkos Views
    are a data type that provides a way to access data more efficiently in memory
    corresponding to a certain Kokkos memory space, such as host memory or device
    memory. A 1-dimensional view of type int* can be created by `Kokkos::View<int*>
    a("a", n)`, where `"a"` is a label, and `n` is the size of the allocation in the
    number of integers. Kokkos determines the optimal layout for the data at compile
    time for best overall performance as a function of the computer architecture.
    Furthermore, Kokkos handles the deallocation of such memory automatically. More
    details about Kokkos Views are found [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/View.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, Kokkos provides three different parallel operations: `parallel_for`,
    `parallel_reduce`, and `parallel_scan`. The `parallel_for` operation is used to
    execute a loop in parallel. The `parallel_reduce` operation is used to execute
    a loop in parallel and reduce the results to a single value. The `parallel_scan`
    operation implements a prefix scan. The usage of `parallel_for` and `parallel_reduce`
    are demonstrated in the examples later in this chapter. More detail about the
    parallel operations are found [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/ParallelDispatch.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Run Kokkos hello.cpp example in simple steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following should work on AMD VEGA90A devices straight out of the box (needs
    ROCm installation). On NVIDIA Volta V100 devices (needs CUDA installation), use
    the variables commented out on the Makefile.
  prefs: []
  type: TYPE_NORMAL
- en: '`git clone https://github.com/kokkos/kokkos.git`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the above Makefile into the current folder (make sure the indentation of
    the last line is tab, and not space)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the above hello.cpp file into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`make`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`./hello`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenCL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenCL is a cross-platform, open-standard API for writing parallel programs
    that execute across heterogeneous platforms consisting of CPUs, GPUs, FPGAs and
    other devices. The first version of OpenCL (1.0) was released in December 2008,
    and the latest version of OpenCL (3.0) was released in September 2020\. OpenCL
    is supported by a number of vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm.
    It is a royalty-free standard, and the OpenCL specification is maintained by the
    Khronos Group. OpenCL provides a low-level programming interface initially based
    on C, but more recently also a C++ interface has become available.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenCL supports two modes for compiling the programs: online and offline. Online
    compilation occurs at runtime, when the host program calls a function to compile
    the source code. Online mode allows dynamic generation and loading of kernels,
    but may incur some overhead due to compilation time and possible errors. Offline
    compilation occurs before runtime, when the source code of a kernel is compiled
    into a binary format that can be loaded by the host program. This mode allows
    faster execution and better optimization of kernels, but may limit the portability
    of the program, because the binary can only run on the architectures it was compiled
    for.'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCL comes bundled with several parallel programming ecosystems, such as
    NVIDIA CUDA and Intel oneAPI. For example, after successfully installing such
    packages and setting up the environment, one may simply compile an OpenCL program
    by the commands such as `icx cl_devices.c -lOpenCL` (Intel oneAPI) or `nvcc cl_devices.c
    -lOpenCL` (NVIDIA CUDA), where `cl_devices.c` is the compiled file. Unlike most
    other programming models, OpenCL stores kernels as text and compiles them for
    the device in runtime (JIT-compilation), and thus does not require any special
    compiler support: one can compile the code using simply `gcc cl_devices.c -lOpenCL`
    (or `g++` when using C++ API), as long as the required libraries and headers are
    installed in a standard locations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The AMD compiler installed on LUMI supports both OpenCL C and C++ API, the
    latter with some limitations. To compile a program, you can use the AMD compilers
    on a GPU partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: OpenCL programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenCL programs consist of two parts: a host program that runs on the host
    device (usually a CPU) and one or more kernels that run on compute devices (such
    as GPUs). The host program is responsible for the tasks such as managing the devices
    for the selected platform, allocating memory objects, building and enqueueing
    kernels, and managing memory objects.'
  prefs: []
  type: TYPE_NORMAL
- en: The first steps when writing an OpenCL program are to initialize the OpenCL
    environment by selecting the platform and devices, creating a context or contexts
    associated with the selected device(s), and creating a command queue for each
    device. A simple example of selecting the default device, creating a context and
    a queue associated with the device is show below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenCL provides two main programming models to manage the memory hierarchy
    of host and accelerator devices: buffers and shared virtual memory (SVM). Buffers
    are the traditional memory model of OpenCL, where the host and the devices have
    separate address spaces and the programmer has to explicitly specify the memory
    allocations and how and where the memory is accessed. This can be done with class
    `cl::Buffer` and functions such as `cl::CommandQueue::enqueueReadBuffer()`. Buffers
    are supported since early versions of OpenCL, and work well across different architectures.
    Buffers can also take advantage of device-specific memory features, such as constant
    or local memory.'
  prefs: []
  type: TYPE_NORMAL
- en: SVM is a newer memory model of OpenCL, introduced in version 2.0, where the
    host and the devices share a single virtual address space. Thus, the programmer
    can use the same pointers to access the data from host and devices simplifying
    the programming effort. In OpenCL, SVM comes in different levels such as coarse-grained
    buffer SVM, fine-grained buffer SVM, and fine-grained system SVM. All levels allow
    using the same pointers across a host and devices, but they differ in their granularity
    and synchronization requirements for the memory regions. Furthermore, the support
    for SVM is not universal across all OpenCL platforms and devices, and for example,
    GPUs such as NVIDIA V100 and A100 only support the coarse-grained SVM buffer.
    This level requires explicit synchronization for memory accesses from a host and
    devices (using functions such as `cl::CommandQueue::enqueueMapSVM()` and `cl::CommandQueue::enqueueUnmapSVM()`),
    making the usage of SVM less convenient. It is further noted that this is unlike
    the regular Unified Memory offered by CUDA, which is closer to the fine-grained
    system SVM level in OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL uses a separate-source kernel model where the kernel code is often kept
    in separate files that may be compiled during runtime. The model allows the kernel
    source code to be passed as a string to the OpenCL driver after which the program
    object can be executed on a specific device. Although referred to as the separate-source
    kernel model, the kernels can still be defined as a string in the host program
    compilation units as well, which may be a more convenient approach in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: The online compilation with the separate-source kernel model has several advantages
    over the binary model, which requires offline compilation of kernels into device-specific
    binaries that can are loaded by the application at runtime. Online compilation
    preserves the portability and flexibility of OpenCL, as the same kernel source
    code can run on any supported device. Furthermore, dynamic optimization of kernels
    based on runtime information, such as input size, work-group size, or device capabilities,
    is possible. An example of an OpenCL kernel, defined by a string in the host compilation
    unit, and assigning the global thread index into a global device memory is shown
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The above kernel named `dot` and stored in the string `kernel_source` can be
    set to build in the host code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: SYCL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[SYCL](https://www.khronos.org/sycl/) is a royalty-free, open-standard C++
    programming model for multi-device programming. It provides a high-level, single-source
    programming model for heterogeneous systems, including GPUs. There are several
    implementations of the standard. For GPU programming, [Intel oneAPI DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)
    and [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/) (also known as
    hipSYCL) are the most popular for desktop and HPC GPUs; [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/)
    is a good choice for embedded devices. The same standard-compliant SYCL code should
    work with any implementation, but they are not binary-compatible.'
  prefs: []
  type: TYPE_NORMAL
- en: The most recent version of the SYCL standard is SYCL 2020, and it is the version
    we will be using in this course.
  prefs: []
  type: TYPE_NORMAL
- en: SYCL compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intel oneAPI DPC++
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For targeting Intel GPUs, it is enough to install [Intel oneAPI Base Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html).
    Then, the compilation is as simple as `icpx -fsycl file.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to use oneAPI for NVIDIA and AMD GPUs. In addition to oneAPI
    Base Toolkit, the vendor-provided runtime (CUDA or HIP) and the corresponding
    [Codeplay oneAPI plugin](https://codeplay.com/solutions/oneapi/) must be installed.
    Then, the code can be compiled using Intel LLVM compiler bundled with oneAPI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clang++ -fsycl -fsycl-targets=nvidia_gpu_sm_86 file.cpp` for targeting CUDA
    8.6 NVIDIA GPU,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clang++ -fsycl -fsycl-targets=amd_gpu_gfx90a` for targeting GFX90a AMD GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaptiveCpp
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using AdaptiveCpp for NVIDIA or AMD GPUs also requires having CUDA or HIP installed
    first. Then `acpp` can be used for compiling the code, specifying the target devices.
    For example, here is how to compile the program supporting an AMD and an NVIDIA
    device:'
  prefs: []
  type: TYPE_NORMAL
- en: '`acpp --acpp-targets=''hip:gfx90a;cuda:sm_70'' file.cpp`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SYCL on LUMI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LUMI does not have a system-wide installation of any SYCL framework, but a
    recent AdaptiveCpp installation is available in CSC modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: The default compilation target is preset to MI250 GPUs, so to compile a single
    C++ file it is enough to call `acpp -O2 file.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When running applications built with AdaptiveCpp, one can often see the warning
    “dag_direct_scheduler: Detected a requirement that is neither of discard access
    mode”, reflecting the lack of an optimization hint when using buffer-accessor
    model. The warning is harmless and can be ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: SYCL programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SYCL is, in many aspects, similar to OpenCL, but uses, like Kokkos, a single-source
    model with kernel lambdas.
  prefs: []
  type: TYPE_NORMAL
- en: 'To submit a task to device, first a sycl::queue must be created, which is used
    as a way to manage the task scheduling and execution. In the simplest case, that’s
    all the initialization one needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'If one wants more control, the device can be explicitly specified, or additional
    properties can be passed to a queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Memory management can be done in two different ways: *buffer-accessor* model
    and *unified shared memory* (USM). The choice of the memory management models
    also influences how the GPU tasks are synchronized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *buffer-accessor* model, a `sycl::buffer` objects are used to represent
    arrays of data. A buffer is not mapped to any single one memory space, and can
    be migrated between the GPU and the CPU memory transparently. The data in `sycl::buffer`
    cannot be read or written directly, an accessor must be created. `sycl::accessor`
    objects specify the location of data access (host or a certain GPU kernel) and
    the access mode (read-only, write-only, read-write). Such approach allows optimizing
    task scheduling by building a directed acyclic graph (DAG) of data dependencies:
    if kernel *A* creates a write-only accessor to a buffer, and then kernel *B* is
    submitted with a read-only accessor to the same buffer, and then a host-side read-only
    accessor is requested, then it can be deduced that *A* must complete before *B*
    is launched and also that the results must be copied to the host before the host
    task can proceed, but the host task can run in parallel with kernel *B*. Since
    the dependencies between tasks can be built automatically, by default SYCL uses
    *out-of-order queues*: when two tasks are submitted to the same `sycl::queue`,
    it is not guaranteed that the second one will launch only after the first one
    completes. When launching a kernel, accessors must be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Buffer-accessor model simplifies many aspects of heterogeneous programming and
    prevents many synchronization-related bugs, but it only allows very coarse control
    of data movement and kernel execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *USM* model is similar to how NVIDIA CUDA or AMD HIP manage memory. The
    programmer has to explicitly allocate the memory on the device (`sycl::malloc_device`),
    on the host (`sycl::malloc_host`), or in the shared memory space (`sycl::malloc_shared`).
    Despite its name, unified shared memory, and the similarity to OpenCL’s SVM, not
    all USM allocations are shared: for example, a memory allocated by `sycl::malloc_device`
    cannot be accessed from the host. The allocation functions return memory pointers
    that can be used directly, without accessors. This means that the programmer have
    to ensure the correct synchronization between host and device tasks to avoid data
    races. With USM, it is often convenient to use *in-order queues* with USM, instead
    of the default *out-of-order* queues. More information on USM can be found in
    the [Section 4.8 of SYCL 2020 specification](https://registry.khronos.org/SYCL/specs/sycl-2020/html/sycl-2020.html#sec:usm).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Exercise: Implement SAXPY in SYCL'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to write (fill-in-the-blanks) a simple code doing
    SAXPY (vector addition).
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile and run the code interactively, first make an allocation and load
    the AdaptiveCpp module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can run a simple device-detection utility to check that a GPU is available
    (note `srun`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: If you have not done it already, clone the repository using `git clone https://github.com/ENCCS/gpu-programming.git`
    or **update it** using `git pull origin main`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the example code in `content/examples/portable-kernel-models/exercise-sycl-saxpy.cpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile and run the code, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: The code will not compile as-is! Your task is to fill in missing bits indicated
    by `TODO` comments. You can also test your understanding using the “Bonus questions”
    in the code.
  prefs: []
  type: TYPE_NORMAL
- en: If you feel stuck, take a look at the `exercise-sycl-saxpy-solution.cpp` file.
  prefs: []
  type: TYPE_NORMAL
- en: alpaka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [alpaka](https://github.com/alpaka-group/alpaka3) library is an open-source
    header-only C++20 abstraction library for accelerator development.
  prefs: []
  type: TYPE_NORMAL
- en: Its aim is to provide performance portability across accelerators by abstracting
    the underlying levels of parallelism. The project provides a single-source C++
    API that enables developers to write parallel code once and run it on different
    hardware architectures without modification. The name “alpaka” comes from **A**bstractions
    for **L**evels of **P**arallelism, **A**lgorithms, and **K**ernels for **A**ccelerators.
    The library is platform-independent and supports the concurrent and cooperative
    use of multiple devices, including host CPUs (x86, ARM, and RISC-V) and GPUs from
    different vendors (NVIDIA, AMD, and Intel). A variety of accelerator backends,
    CUDA, HIP, SYCL, OpenMP, and serial execution, are available and can be selected
    based on the target device. Only a single implementation of a user kernel is required,
    expressed as a function object with a standardized interface. This eliminates
    the need to write specialized CUDA, HIP, SYCL, OpenMP, Intel TBB or threading
    code. Moreover, multiple accelerator backends can be combined to target different
    vendor hardware within a single system and even within a single application.
  prefs: []
  type: TYPE_NORMAL
- en: The abstraction is based on a virtual index domain decomposed into equally sized
    chunks called frames. **alpaka** provides a uniform abstraction to traverse these
    frames, independent of the underlying hardware. Algorithms to be parallelized
    map the chunked index domain and native worker threads onto the data, expressing
    the computation as kernels that are executed in parallel threads (SIMT), thereby
    also leveraging SIMD units. Unlike native parallelism models such as CUDA, HIP,
    and SYCL, **alpaka** kernels are not restricted to three dimensions. Explicit
    caching of data within a frame via shared memory allows developers to fully unleash
    the performance of the compute device. Additionally, **alpaka** offers primitive
    functions such as iota, transform, transform-reduce, reduce, and concurrent, simplifying
    the development of portable high-performance applications. Host, device, mapped,
    and managed multi-dimensional views provide a natural way to operate on data.
  prefs: []
  type: TYPE_NORMAL
- en: Here we demonstrate the usage of **alpaka3**, which is a complete rewrite of
    [alpaka](https://github.com/alpaka-group/alpaka). It is planned to merge this
    separate codebase back into the mainline alpaka repository before the first release
    in Q2/Q3 of 2026. Nevertheless, the code is well-tested and can be used for development
    today.
  prefs: []
  type: TYPE_NORMAL
- en: Installing alpaka on your system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For ease of use, we recommend installing alpaka using CMake as described below.
    For other ways to use alpaka in your projects, see the [alpaka3 documentation](https://alpaka3.readthedocs.io/en/latest/basic/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Clone the repository**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clone the alpaka source code from GitHub to a directory of your choice:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Set installation directory**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `ALPAKA_DIR` environment variable to the directory where you want to
    install alpaka. This can be any directory you choose where you have write access.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Build and install**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a build directory and use CMake to build and install alpaka. We use `CMAKE_INSTALL_PREFIX`
    to tell CMake where to install the library.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Update environment**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To make sure that other projects can find your alpaka installation, you should
    add the installation directory to your `CMAKE_PREFIX_PATH`. You can do this by
    adding the following line to your shell configuration file (e.g. `~/.bashrc`):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will need to source your shell configuration file or open a new terminal
    for the changes to take effect.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: alpaka Compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We recommend building your projects which use alpaka using CMake. A variety
    of strategies can be used to deal with building your application for a specific
    device or set of devices. Here we show a minimal way to get started, but this
    is by no means the only way to set up your projects. Please refer to the [alpaka3
    documentation](https://alpaka3.readthedocs.io/en/latest/basic/install.html) for
    alternative ways to use alpaka in your project, including a way to make your source
    code agnostic to the accelerator being targeted by defining a device specification
    in CMake.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates a `CMakeLists.txt` for a single-file project
    using alpaka3 (`main.cpp` which is presented in the section below):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Using alpaka on LUMI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To load the environment for using the AMD GPUs on LUMI with HIP, one can use
    the following modules -
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: alpaka Programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When starting with alpaka3, the first step is understanding the **device selection
    model**. Unlike frameworks that require explicit initialization calls, alpaka3
    uses a device specification to determine which backend and hardware to use. The
    device specification consists of two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**API**: The parallel programming interface (host, cuda, hip, oneApi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Device Kind**: The type of hardware (cpu, nvidiaGpu, amdGpu, intelGpu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we specify and use these at runtime to select and initialize devices. The
    device selection process is described in detail in the alpaka3 documentation.
  prefs: []
  type: TYPE_NORMAL
- en: alpaka3 uses an **execution space model** to abstract parallel hardware details.
    A device selector is created using `alpaka::onHost::makeDeviceSelector(devSpec)`,
    which returns an object that can query available devices and create device instances
    for the selected backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates a basic alpaka program that initializes
    a device and prints information about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: alpaka3 provides memory management abstractions through buffers and views. Memory
    can be allocated on host or device using `alpaka::allocBuf<T, Idx>(device, extent)`.
    Data transfers between host and device are handled through `alpaka::memcpy(queue,
    dst, src)`. The library automatically manages memory layouts for optimal performance
    on different architectures.
  prefs: []
  type: TYPE_NORMAL
- en: For parallel execution, alpaka3 provides kernel abstractions. Kernels are defined
    as functors or lambda functions and executed using work division specifications
    that define the parallelization strategy. The framework supports various parallel
    patterns including element-wise operations, reductions, and scans.
  prefs: []
  type: TYPE_NORMAL
- en: Tour of **alpaka** Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now we will quickly explore the most commonly used features of alpaka and go
    over some basic usage. A quick reference of commonly used alpaka features is available
    [here.](https://alpaka3.readthedocs.io/en/latest/basic/cheatsheet.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**General setup**: Include the consolidated header once and you are ready to
    start using alpaka.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '**Accelerator, platform, and device management**: Select devices by combining
    the desired API with the appropriate hardware kind using the device selector.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '**Queues and events**: Create blocking or non-blocking queues per device, record
    events, and synchronize work as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '**Memory management**: Allocate host, device, mapped, unified, or deferred
    buffers, create non-owning views, and move data portably with memcpy, memset,
    and fill.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '**Kernel execution**: Build a FrameSpec manually or request one tuned for your
    data type, then enqueue kernels with automatic or explicit executors.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '**Kernel implementation**: Write kernels as functors annotated with ALPAKA_FN_ACC,
    use shared memory, synchronization, atomics, and math helpers directly inside
    the kernel body.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: Run alpaka3 Example in Simple Steps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following example works on systems with CMake 3.25+ and an appropriate C++
    compiler. For GPU execution, ensure the corresponding runtime (CUDA, ROCm, or
    oneAPI) is installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory for your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Copy the CMakeLists.txt from above into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the main.cpp file into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configure and build:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the executable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The device specification system allows you to select the target device at CMake
    configuration time. The format is `"api:deviceKind"`, where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**api**: The parallel programming interface (`host`, `cuda`, `hip`, `oneApi`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**deviceKind**: The type of device (`cpu`, `nvidiaGpu`, `amdGpu`, `intelGpu`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Available combinations are: `host:cpu`, `cuda:nvidiaGpu`, `hip:amdGpu`, `oneApi:cpu`,
    `oneApi:intelGpu`, `oneApi:nvidiaGpu`, `oneApi:amdGpu`'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA, HIP, or Intel backends only work if the CUDA SDK, HIP SDK, or OneAPI
    SDK are available respectively
  prefs: []
  type: TYPE_NORMAL
- en: Expected output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: The device name will vary depending on your hardware (e.g., “NVIDIA A100”, “AMD
    MI250X”, or your CPU model).
  prefs: []
  type: TYPE_NORMAL
- en: Compile and Execute Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can test the **alpaka** provided examples from the [example section](#examples).
    The examples have hard coded the usage of the AMD ROCm platform required on LUMI.
    To switch to CPU usage only you can simply replace `ap::onHost::makeDeviceSelector(ap::api::hip,
    ap::deviceKind::amdGpu);` with `ap::onHost::makeDeviceSelector(ap::api::host,
    ap::deviceKind::cpu);`
  prefs: []
  type: TYPE_NORMAL
- en: The following steps assume you have downloaded alpaka already and the path to
    the **alapka** source code is stored in the environment variable `ALPAKA_DIR`.
    To test the example copy the code into a file `main.cpp`
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, [click here](https://godbolt.org/z/69exnG4xb) to try the first
    example using in the godbolt compiler explorer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To use oneAPI Sycl with AMD or NVIDIA Gpus you must install the corresponding
    Codeplay oneAPI plugin as described [here](https://codeplay.com/solutions/oneapi/plugins/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To use oneAPI Sycl with AMD or NVIDIA Gpus you must install the corresponding
    Codeplay oneAPI plugin as described [here](https://codeplay.com/solutions/oneapi/plugins/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Exercise: Write a vector add kernel in alpaka'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to write (fill-in-the-blanks) a simple kernel
    to add two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile and run the code interactively, first we first need to get an allocation
    on a GPU node and load the modules for alpaka:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can run a simple device-detection utility to check that a GPU is available
    (note `srun`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s look at the code to set up the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Below we use fetch content with our CMake to get started with alpaka quickly.
  prefs: []
  type: TYPE_NORMAL
- en: CMakeLists.txt
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: Below we have the main alpaka code doing a vector addition on device using a
    high level transform function
  prefs: []
  type: TYPE_NORMAL
- en: main.cpp
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: To set up our project, we create a folder and place our CMakeLists.txt and main.cpp
    in there.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile and run the code, use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Now your task will be to write and launch your first alpaka kernel. This kernel
    will do the vector addition and we will use this instead of the transform helper.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the vector add kernel
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parallel for with Unified Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: Parallel for with GPU buffers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: Asynchronous parallel for kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: Pros and cons of cross-platform portability ecosystems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: General observations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The amount of code duplication is minimized.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The same code can be compiled to multiple architectures from different vendors.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Limited learning resources compared to CUDA (Stack Overflow, course material,
    documentation).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambda-based kernel models (Kokkos, SYCL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Higher level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Less knowledge of the underlying architecture is needed for initial porting.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Very nice and readable source code (C++ API).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The models are relatively new and not very popular yet.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Functor-based kernel model (alpaka)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very good portability.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Higher level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Low-level API always awailable which gives more control and allows fine tuning.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: User friendly C++ API for both the host and kernel code.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Small community and ecosystem.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate-source kernel models (OpenCL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very good portability.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Mature ecosystem.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Limited number of vendor-provided libraries.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Low-level API gives more control and allows fine tuning.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Both C and C++ APIs available (C++ API is less well supported).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The low-level API and separate-source kernel model are less user friendly.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: C++ Standard Parallelism (StdPar, PSTL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very high level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Easy to speed up code which already relying on STL algorithms.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Very little control over hardware.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Support by compilers is improving, but is far from mature.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: General code organization is similar to non-portable kernel-based models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As long as no vendor-specific functionality is used, the same code can run on
    any GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C++ StdPar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In C++17, the initial support for parallel execution of standard algorithms
    has been introduced. Most algorithms available via the standard `<algorithms>`
    header were given an overload accepting with an [*execution policy*](https://en.cppreference.com/w/cpp/algorithm)
    argument which allows the programmer to request parallel execution of the standard
    library function. While the main goal was to allow low-effort, high-level interface
    to run existing algorithms like `std::sort` on many CPU cores, implementations
    are allowed to use other hardware, and functions like `std::for_each` or `std::transform`
    offer great flexibility in writing the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: C++ StdPar, also called Parallel STL or PSTL, could be considered similar to
    directive-based models, as it is very high-level and does not give the programmer
    fine-grained control over data movement or any access to hardware-specific features
    like shared (local) memory. Even the GPU to run on is selected automatically,
    since standard C++ does not have the concept of a *device* (but there are vendor
    extensions allowing the programmer more control) However, for applications that
    already relies on algorithms from C++ standard library, StdPar can be a good way
    to reap the performance benefits of both CPUs and GPUs with minimal code modifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'For GPU programming, all three vendors offer their implementations of StdPar
    with the ability to offload code to the GPU: NVIDIA has `nvc++`, AMD has experimental
    [roc-stdpar](https://github.com/ROCm/roc-stdpar), and Intel offers StdPar offload
    with their oneAPI compiler. [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/)
    offers an independent StdPar implementation, able to target devices from all three
    vendors. While being a part of the C++ standard, the level of support and the
    maturity of StdPar implementations varies a lot between different compilers: not
    all compilers support all algorithms, and different heuristics for mapping the
    algorithm to hardware and for managing data movement can have effect on performance.'
  prefs: []
  type: TYPE_NORMAL
- en: StdPar compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The build process depends a lot on the used compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'AdaptiveCpp: Add `--acpp-stdpar` flag when calling `acpp`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intel oneAPI: Add `-fsycl -fsycl-pstl-offload=gpu` flags when calling `icpx`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA NVC++: Add `-stdpar` flag when calling `nvc++` (not supported with plain
    `nvcc`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StdPar programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In its simplest form, using C++ standard parallelism requires including an additional
    `<execution>` header and adding one argument to a supported standard library function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s look at the following sequential code sorting a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: 'To make it run sorting on the GPU, only a minor modification is needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: Now, when compiled with one of the supported compilers, the code will run the
    sorting on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: While the can initially seem very limiting, many standard algorithms, such as
    `std::transform`, `std::accumulate`, `std::transform_reduce`, and `std::for_each`
    can run custom functions over an array, thus allowing one to offload an arbitrary
    algorithm, as long as it does not violate typical limitations of GPU kernels,
    such as not throwing any exceptions and not doing system calls.
  prefs: []
  type: TYPE_NORMAL
- en: StdPar execution policies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In C++, there are four different execution policies to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::execution::seq`: run algorithm serially, don’t parallelize it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par`: allow parallelizing the algorithm (as if using multiple
    threads),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::unseq`: allow vectorizing the algorithm (as if using SIMD),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par_unseq`: allow both vectorizing and parallelizing the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main difference between `par` and `unseq` is related to thread progress
    and locks: using `unseq` or `par_unseq` requires that the algorithms does not
    contain mutexes and other locks between the processes, while `par` does not have
    this limitation.'
  prefs: []
  type: TYPE_NORMAL
- en: For GPU, the optimal choice is `par_unseq`, since this places the least requirement
    on the compiler in terms of operation ordering. While `par` is also supported
    in some cases, it is best avoided, both due to limited compiler support and as
    an indication that the algorithm is likely a poor fit for the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: StdPar compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The build process depends a lot on the used compiler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'AdaptiveCpp: Add `--acpp-stdpar` flag when calling `acpp`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intel oneAPI: Add `-fsycl -fsycl-pstl-offload=gpu` flags when calling `icpx`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'NVIDIA NVC++: Add `-stdpar` flag when calling `nvc++` (not supported with plain
    `nvcc`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StdPar programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In its simplest form, using C++ standard parallelism requires including an additional
    `<execution>` header and adding one argument to a supported standard library function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s look at the following sequential code sorting a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'To make it run sorting on the GPU, only a minor modification is needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: Now, when compiled with one of the supported compilers, the code will run the
    sorting on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: While the can initially seem very limiting, many standard algorithms, such as
    `std::transform`, `std::accumulate`, `std::transform_reduce`, and `std::for_each`
    can run custom functions over an array, thus allowing one to offload an arbitrary
    algorithm, as long as it does not violate typical limitations of GPU kernels,
    such as not throwing any exceptions and not doing system calls.
  prefs: []
  type: TYPE_NORMAL
- en: StdPar execution policies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In C++, there are four different execution policies to choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '`std::execution::seq`: run algorithm serially, don’t parallelize it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par`: allow parallelizing the algorithm (as if using multiple
    threads),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::unseq`: allow vectorizing the algorithm (as if using SIMD),'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`std::execution::par_unseq`: allow both vectorizing and parallelizing the algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The main difference between `par` and `unseq` is related to thread progress
    and locks: using `unseq` or `par_unseq` requires that the algorithms does not
    contain mutexes and other locks between the processes, while `par` does not have
    this limitation.'
  prefs: []
  type: TYPE_NORMAL
- en: For GPU, the optimal choice is `par_unseq`, since this places the least requirement
    on the compiler in terms of operation ordering. While `par` is also supported
    in some cases, it is best avoided, both due to limited compiler support and as
    an indication that the algorithm is likely a poor fit for the hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kokkos is an open-source performance portability ecosystem for parallelization
    on large heterogeneous hardware architectures of which development has mostly
    taken place on Sandia National Laboratories. The project started in 2011 as a
    parallel C++ programming model, but have since expanded into a more broad ecosystem
    including Kokkos Core (the programming model), Kokkos Kernels (math library),
    and Kokkos Tools (debugging, profiling and tuning tools). By preparing proposals
    for the C++ standard committee, the project also aims to influence the ISO/C++
    language standard such that, eventually, Kokkos capabilities will become native
    to the language standard. A more detailed introduction is found [HERE](https://www.sandia.gov/news/publications/hpc-annual-reports/article/kokkos/).
  prefs: []
  type: TYPE_NORMAL
- en: The Kokkos library provides an abstraction layer for a variety of different
    parallel programming models, currently CUDA, HIP, SYCL, HPX, OpenMP, and C++ threads.
    Therefore, it allows better portability across different hardware manufactured
    by different vendors, but introduces an additional dependency to the software
    stack. For example, when using CUDA, only CUDA installation is required, but when
    using Kokkos with NVIDIA GPUs, Kokkos and CUDA installation are both required.
    Kokkos is not a very popular choice for parallel programming, and therefore, learning
    and using Kokkos can be more difficult compared to more established programming
    models such as CUDA, for which a much larger amount of search results and Stack
    Overflow discussions can be found.
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Furthermore, one challenge with some cross-platform portability libraries is
    that even on the same system, different projects may require different combinations
    of compilation settings for the portability library. For example, in Kokkos, one
    project may wish the default execution space to be a CUDA device, whereas another
    requires a CPU. Even if the projects prefer the same execution space, one project
    may desire the Unified Memory to be the default memory space and the other may
    wish to use pinned GPU memory. It may be burdensome to maintain a large number
    of library instances on a single system.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Kokkos offers a simple way to compile Kokkos library simultaneously
    with the user project. This is achieved by specifying Kokkos compilation settings
    (see [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Compiling.html))
    and including the Kokkos Makefile in the user Makefile. CMake is also supported.
    This way, the user application and Kokkos library are compiled together. The following
    is an example Makefile for a single-file Kokkos project (hello.cpp) that uses
    CUDA (Volta architecture) as the backend (default execution space) and Unified
    Memory as the default memory space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: To build a **hello.cpp** project with the above Makefile, no steps other than
    cloning the Kokkos project into the current directory is required.
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When starting to write a project using Kokkos, the first step is understand
    Kokkos initialization and finalization. Kokkos must be initialized by calling
    `Kokkos::initialize(int& argc, char* argv[])` and finalized by calling `Kokkos::finalize()`.
    More details are given in [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Initialization.html).
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos uses an execution space model to abstract the details of parallel hardware.
    The execution space instances map to the available backend options such as CUDA,
    OpenMP, HIP, or SYCL. If the execution space is not explicitly chosen by the programmer
    in the source code, the default execution space `Kokkos::DefaultExecutionSpace`
    is used. This is chosen when the Kokkos library is compiled. The Kokkos execution
    space model is described in more detail in [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Machine-Model.html#kokkos-spaces).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Kokkos uses a memory space model for different types of memory, such
    as host memory or device memory. If not defined explicitly, Kokkos uses the default
    memory space specified during Kokkos compilation as described [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Machine-Model.html#kokkos-memory-spaces).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a Kokkos program that initializes Kokkos and
    prints the execution space and memory space instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: With Kokkos, the data can be accessed either through raw pointers or through
    Kokkos Views. With raw pointers, the memory allocation into the default memory
    space can be done using `Kokkos::kokkos_malloc(n * sizeof(int))`. Kokkos Views
    are a data type that provides a way to access data more efficiently in memory
    corresponding to a certain Kokkos memory space, such as host memory or device
    memory. A 1-dimensional view of type int* can be created by `Kokkos::View<int*>
    a("a", n)`, where `"a"` is a label, and `n` is the size of the allocation in the
    number of integers. Kokkos determines the optimal layout for the data at compile
    time for best overall performance as a function of the computer architecture.
    Furthermore, Kokkos handles the deallocation of such memory automatically. More
    details about Kokkos Views are found [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/View.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, Kokkos provides three different parallel operations: `parallel_for`,
    `parallel_reduce`, and `parallel_scan`. The `parallel_for` operation is used to
    execute a loop in parallel. The `parallel_reduce` operation is used to execute
    a loop in parallel and reduce the results to a single value. The `parallel_scan`
    operation implements a prefix scan. The usage of `parallel_for` and `parallel_reduce`
    are demonstrated in the examples later in this chapter. More detail about the
    parallel operations are found [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/ParallelDispatch.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Run Kokkos hello.cpp example in simple steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following should work on AMD VEGA90A devices straight out of the box (needs
    ROCm installation). On NVIDIA Volta V100 devices (needs CUDA installation), use
    the variables commented out on the Makefile.
  prefs: []
  type: TYPE_NORMAL
- en: '`git clone https://github.com/kokkos/kokkos.git`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the above Makefile into the current folder (make sure the indentation of
    the last line is tab, and not space)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the above hello.cpp file into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`make`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`./hello`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kokkos compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Furthermore, one challenge with some cross-platform portability libraries is
    that even on the same system, different projects may require different combinations
    of compilation settings for the portability library. For example, in Kokkos, one
    project may wish the default execution space to be a CUDA device, whereas another
    requires a CPU. Even if the projects prefer the same execution space, one project
    may desire the Unified Memory to be the default memory space and the other may
    wish to use pinned GPU memory. It may be burdensome to maintain a large number
    of library instances on a single system.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Kokkos offers a simple way to compile Kokkos library simultaneously
    with the user project. This is achieved by specifying Kokkos compilation settings
    (see [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Compiling.html))
    and including the Kokkos Makefile in the user Makefile. CMake is also supported.
    This way, the user application and Kokkos library are compiled together. The following
    is an example Makefile for a single-file Kokkos project (hello.cpp) that uses
    CUDA (Volta architecture) as the backend (default execution space) and Unified
    Memory as the default memory space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: To build a **hello.cpp** project with the above Makefile, no steps other than
    cloning the Kokkos project into the current directory is required.
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When starting to write a project using Kokkos, the first step is understand
    Kokkos initialization and finalization. Kokkos must be initialized by calling
    `Kokkos::initialize(int& argc, char* argv[])` and finalized by calling `Kokkos::finalize()`.
    More details are given in [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Initialization.html).
  prefs: []
  type: TYPE_NORMAL
- en: Kokkos uses an execution space model to abstract the details of parallel hardware.
    The execution space instances map to the available backend options such as CUDA,
    OpenMP, HIP, or SYCL. If the execution space is not explicitly chosen by the programmer
    in the source code, the default execution space `Kokkos::DefaultExecutionSpace`
    is used. This is chosen when the Kokkos library is compiled. The Kokkos execution
    space model is described in more detail in [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Machine-Model.html#kokkos-spaces).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Kokkos uses a memory space model for different types of memory, such
    as host memory or device memory. If not defined explicitly, Kokkos uses the default
    memory space specified during Kokkos compilation as described [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/Machine-Model.html#kokkos-memory-spaces).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of a Kokkos program that initializes Kokkos and
    prints the execution space and memory space instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: With Kokkos, the data can be accessed either through raw pointers or through
    Kokkos Views. With raw pointers, the memory allocation into the default memory
    space can be done using `Kokkos::kokkos_malloc(n * sizeof(int))`. Kokkos Views
    are a data type that provides a way to access data more efficiently in memory
    corresponding to a certain Kokkos memory space, such as host memory or device
    memory. A 1-dimensional view of type int* can be created by `Kokkos::View<int*>
    a("a", n)`, where `"a"` is a label, and `n` is the size of the allocation in the
    number of integers. Kokkos determines the optimal layout for the data at compile
    time for best overall performance as a function of the computer architecture.
    Furthermore, Kokkos handles the deallocation of such memory automatically. More
    details about Kokkos Views are found [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/View.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, Kokkos provides three different parallel operations: `parallel_for`,
    `parallel_reduce`, and `parallel_scan`. The `parallel_for` operation is used to
    execute a loop in parallel. The `parallel_reduce` operation is used to execute
    a loop in parallel and reduce the results to a single value. The `parallel_scan`
    operation implements a prefix scan. The usage of `parallel_for` and `parallel_reduce`
    are demonstrated in the examples later in this chapter. More detail about the
    parallel operations are found [HERE](https://kokkos.org/kokkos-core-wiki/ProgrammingGuide/ParallelDispatch.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Run Kokkos hello.cpp example in simple steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The following should work on AMD VEGA90A devices straight out of the box (needs
    ROCm installation). On NVIDIA Volta V100 devices (needs CUDA installation), use
    the variables commented out on the Makefile.
  prefs: []
  type: TYPE_NORMAL
- en: '`git clone https://github.com/kokkos/kokkos.git`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the above Makefile into the current folder (make sure the indentation of
    the last line is tab, and not space)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the above hello.cpp file into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`make`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`./hello`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenCL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenCL is a cross-platform, open-standard API for writing parallel programs
    that execute across heterogeneous platforms consisting of CPUs, GPUs, FPGAs and
    other devices. The first version of OpenCL (1.0) was released in December 2008,
    and the latest version of OpenCL (3.0) was released in September 2020\. OpenCL
    is supported by a number of vendors, including AMD, ARM, Intel, NVIDIA, and Qualcomm.
    It is a royalty-free standard, and the OpenCL specification is maintained by the
    Khronos Group. OpenCL provides a low-level programming interface initially based
    on C, but more recently also a C++ interface has become available.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenCL supports two modes for compiling the programs: online and offline. Online
    compilation occurs at runtime, when the host program calls a function to compile
    the source code. Online mode allows dynamic generation and loading of kernels,
    but may incur some overhead due to compilation time and possible errors. Offline
    compilation occurs before runtime, when the source code of a kernel is compiled
    into a binary format that can be loaded by the host program. This mode allows
    faster execution and better optimization of kernels, but may limit the portability
    of the program, because the binary can only run on the architectures it was compiled
    for.'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCL comes bundled with several parallel programming ecosystems, such as
    NVIDIA CUDA and Intel oneAPI. For example, after successfully installing such
    packages and setting up the environment, one may simply compile an OpenCL program
    by the commands such as `icx cl_devices.c -lOpenCL` (Intel oneAPI) or `nvcc cl_devices.c
    -lOpenCL` (NVIDIA CUDA), where `cl_devices.c` is the compiled file. Unlike most
    other programming models, OpenCL stores kernels as text and compiles them for
    the device in runtime (JIT-compilation), and thus does not require any special
    compiler support: one can compile the code using simply `gcc cl_devices.c -lOpenCL`
    (or `g++` when using C++ API), as long as the required libraries and headers are
    installed in a standard locations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The AMD compiler installed on LUMI supports both OpenCL C and C++ API, the
    latter with some limitations. To compile a program, you can use the AMD compilers
    on a GPU partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: OpenCL programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenCL programs consist of two parts: a host program that runs on the host
    device (usually a CPU) and one or more kernels that run on compute devices (such
    as GPUs). The host program is responsible for the tasks such as managing the devices
    for the selected platform, allocating memory objects, building and enqueueing
    kernels, and managing memory objects.'
  prefs: []
  type: TYPE_NORMAL
- en: The first steps when writing an OpenCL program are to initialize the OpenCL
    environment by selecting the platform and devices, creating a context or contexts
    associated with the selected device(s), and creating a command queue for each
    device. A simple example of selecting the default device, creating a context and
    a queue associated with the device is show below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenCL provides two main programming models to manage the memory hierarchy
    of host and accelerator devices: buffers and shared virtual memory (SVM). Buffers
    are the traditional memory model of OpenCL, where the host and the devices have
    separate address spaces and the programmer has to explicitly specify the memory
    allocations and how and where the memory is accessed. This can be done with class
    `cl::Buffer` and functions such as `cl::CommandQueue::enqueueReadBuffer()`. Buffers
    are supported since early versions of OpenCL, and work well across different architectures.
    Buffers can also take advantage of device-specific memory features, such as constant
    or local memory.'
  prefs: []
  type: TYPE_NORMAL
- en: SVM is a newer memory model of OpenCL, introduced in version 2.0, where the
    host and the devices share a single virtual address space. Thus, the programmer
    can use the same pointers to access the data from host and devices simplifying
    the programming effort. In OpenCL, SVM comes in different levels such as coarse-grained
    buffer SVM, fine-grained buffer SVM, and fine-grained system SVM. All levels allow
    using the same pointers across a host and devices, but they differ in their granularity
    and synchronization requirements for the memory regions. Furthermore, the support
    for SVM is not universal across all OpenCL platforms and devices, and for example,
    GPUs such as NVIDIA V100 and A100 only support the coarse-grained SVM buffer.
    This level requires explicit synchronization for memory accesses from a host and
    devices (using functions such as `cl::CommandQueue::enqueueMapSVM()` and `cl::CommandQueue::enqueueUnmapSVM()`),
    making the usage of SVM less convenient. It is further noted that this is unlike
    the regular Unified Memory offered by CUDA, which is closer to the fine-grained
    system SVM level in OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL uses a separate-source kernel model where the kernel code is often kept
    in separate files that may be compiled during runtime. The model allows the kernel
    source code to be passed as a string to the OpenCL driver after which the program
    object can be executed on a specific device. Although referred to as the separate-source
    kernel model, the kernels can still be defined as a string in the host program
    compilation units as well, which may be a more convenient approach in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: The online compilation with the separate-source kernel model has several advantages
    over the binary model, which requires offline compilation of kernels into device-specific
    binaries that can are loaded by the application at runtime. Online compilation
    preserves the portability and flexibility of OpenCL, as the same kernel source
    code can run on any supported device. Furthermore, dynamic optimization of kernels
    based on runtime information, such as input size, work-group size, or device capabilities,
    is possible. An example of an OpenCL kernel, defined by a string in the host compilation
    unit, and assigning the global thread index into a global device memory is shown
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: 'The above kernel named `dot` and stored in the string `kernel_source` can be
    set to build in the host code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: OpenCL compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenCL supports two modes for compiling the programs: online and offline. Online
    compilation occurs at runtime, when the host program calls a function to compile
    the source code. Online mode allows dynamic generation and loading of kernels,
    but may incur some overhead due to compilation time and possible errors. Offline
    compilation occurs before runtime, when the source code of a kernel is compiled
    into a binary format that can be loaded by the host program. This mode allows
    faster execution and better optimization of kernels, but may limit the portability
    of the program, because the binary can only run on the architectures it was compiled
    for.'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenCL comes bundled with several parallel programming ecosystems, such as
    NVIDIA CUDA and Intel oneAPI. For example, after successfully installing such
    packages and setting up the environment, one may simply compile an OpenCL program
    by the commands such as `icx cl_devices.c -lOpenCL` (Intel oneAPI) or `nvcc cl_devices.c
    -lOpenCL` (NVIDIA CUDA), where `cl_devices.c` is the compiled file. Unlike most
    other programming models, OpenCL stores kernels as text and compiles them for
    the device in runtime (JIT-compilation), and thus does not require any special
    compiler support: one can compile the code using simply `gcc cl_devices.c -lOpenCL`
    (or `g++` when using C++ API), as long as the required libraries and headers are
    installed in a standard locations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The AMD compiler installed on LUMI supports both OpenCL C and C++ API, the
    latter with some limitations. To compile a program, you can use the AMD compilers
    on a GPU partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: OpenCL programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenCL programs consist of two parts: a host program that runs on the host
    device (usually a CPU) and one or more kernels that run on compute devices (such
    as GPUs). The host program is responsible for the tasks such as managing the devices
    for the selected platform, allocating memory objects, building and enqueueing
    kernels, and managing memory objects.'
  prefs: []
  type: TYPE_NORMAL
- en: The first steps when writing an OpenCL program are to initialize the OpenCL
    environment by selecting the platform and devices, creating a context or contexts
    associated with the selected device(s), and creating a command queue for each
    device. A simple example of selecting the default device, creating a context and
    a queue associated with the device is show below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenCL provides two main programming models to manage the memory hierarchy
    of host and accelerator devices: buffers and shared virtual memory (SVM). Buffers
    are the traditional memory model of OpenCL, where the host and the devices have
    separate address spaces and the programmer has to explicitly specify the memory
    allocations and how and where the memory is accessed. This can be done with class
    `cl::Buffer` and functions such as `cl::CommandQueue::enqueueReadBuffer()`. Buffers
    are supported since early versions of OpenCL, and work well across different architectures.
    Buffers can also take advantage of device-specific memory features, such as constant
    or local memory.'
  prefs: []
  type: TYPE_NORMAL
- en: SVM is a newer memory model of OpenCL, introduced in version 2.0, where the
    host and the devices share a single virtual address space. Thus, the programmer
    can use the same pointers to access the data from host and devices simplifying
    the programming effort. In OpenCL, SVM comes in different levels such as coarse-grained
    buffer SVM, fine-grained buffer SVM, and fine-grained system SVM. All levels allow
    using the same pointers across a host and devices, but they differ in their granularity
    and synchronization requirements for the memory regions. Furthermore, the support
    for SVM is not universal across all OpenCL platforms and devices, and for example,
    GPUs such as NVIDIA V100 and A100 only support the coarse-grained SVM buffer.
    This level requires explicit synchronization for memory accesses from a host and
    devices (using functions such as `cl::CommandQueue::enqueueMapSVM()` and `cl::CommandQueue::enqueueUnmapSVM()`),
    making the usage of SVM less convenient. It is further noted that this is unlike
    the regular Unified Memory offered by CUDA, which is closer to the fine-grained
    system SVM level in OpenCL.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL uses a separate-source kernel model where the kernel code is often kept
    in separate files that may be compiled during runtime. The model allows the kernel
    source code to be passed as a string to the OpenCL driver after which the program
    object can be executed on a specific device. Although referred to as the separate-source
    kernel model, the kernels can still be defined as a string in the host program
    compilation units as well, which may be a more convenient approach in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: The online compilation with the separate-source kernel model has several advantages
    over the binary model, which requires offline compilation of kernels into device-specific
    binaries that can are loaded by the application at runtime. Online compilation
    preserves the portability and flexibility of OpenCL, as the same kernel source
    code can run on any supported device. Furthermore, dynamic optimization of kernels
    based on runtime information, such as input size, work-group size, or device capabilities,
    is possible. An example of an OpenCL kernel, defined by a string in the host compilation
    unit, and assigning the global thread index into a global device memory is shown
    below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: 'The above kernel named `dot` and stored in the string `kernel_source` can be
    set to build in the host code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: SYCL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[SYCL](https://www.khronos.org/sycl/) is a royalty-free, open-standard C++
    programming model for multi-device programming. It provides a high-level, single-source
    programming model for heterogeneous systems, including GPUs. There are several
    implementations of the standard. For GPU programming, [Intel oneAPI DPC++](https://www.intel.com/content/www/us/en/developer/tools/oneapi/dpc-compiler.html)
    and [AdaptiveCpp](https://github.com/AdaptiveCpp/AdaptiveCpp/) (also known as
    hipSYCL) are the most popular for desktop and HPC GPUs; [ComputeCPP](https://developer.codeplay.com/products/computecpp/ce/home/)
    is a good choice for embedded devices. The same standard-compliant SYCL code should
    work with any implementation, but they are not binary-compatible.'
  prefs: []
  type: TYPE_NORMAL
- en: The most recent version of the SYCL standard is SYCL 2020, and it is the version
    we will be using in this course.
  prefs: []
  type: TYPE_NORMAL
- en: SYCL compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intel oneAPI DPC++
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For targeting Intel GPUs, it is enough to install [Intel oneAPI Base Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html).
    Then, the compilation is as simple as `icpx -fsycl file.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to use oneAPI for NVIDIA and AMD GPUs. In addition to oneAPI
    Base Toolkit, the vendor-provided runtime (CUDA or HIP) and the corresponding
    [Codeplay oneAPI plugin](https://codeplay.com/solutions/oneapi/) must be installed.
    Then, the code can be compiled using Intel LLVM compiler bundled with oneAPI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clang++ -fsycl -fsycl-targets=nvidia_gpu_sm_86 file.cpp` for targeting CUDA
    8.6 NVIDIA GPU,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clang++ -fsycl -fsycl-targets=amd_gpu_gfx90a` for targeting GFX90a AMD GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaptiveCpp
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using AdaptiveCpp for NVIDIA or AMD GPUs also requires having CUDA or HIP installed
    first. Then `acpp` can be used for compiling the code, specifying the target devices.
    For example, here is how to compile the program supporting an AMD and an NVIDIA
    device:'
  prefs: []
  type: TYPE_NORMAL
- en: '`acpp --acpp-targets=''hip:gfx90a;cuda:sm_70'' file.cpp`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SYCL on LUMI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LUMI does not have a system-wide installation of any SYCL framework, but a
    recent AdaptiveCpp installation is available in CSC modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: The default compilation target is preset to MI250 GPUs, so to compile a single
    C++ file it is enough to call `acpp -O2 file.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When running applications built with AdaptiveCpp, one can often see the warning
    “dag_direct_scheduler: Detected a requirement that is neither of discard access
    mode”, reflecting the lack of an optimization hint when using buffer-accessor
    model. The warning is harmless and can be ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: SYCL programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SYCL is, in many aspects, similar to OpenCL, but uses, like Kokkos, a single-source
    model with kernel lambdas.
  prefs: []
  type: TYPE_NORMAL
- en: 'To submit a task to device, first a sycl::queue must be created, which is used
    as a way to manage the task scheduling and execution. In the simplest case, that’s
    all the initialization one needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: 'If one wants more control, the device can be explicitly specified, or additional
    properties can be passed to a queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: 'Memory management can be done in two different ways: *buffer-accessor* model
    and *unified shared memory* (USM). The choice of the memory management models
    also influences how the GPU tasks are synchronized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *buffer-accessor* model, a `sycl::buffer` objects are used to represent
    arrays of data. A buffer is not mapped to any single one memory space, and can
    be migrated between the GPU and the CPU memory transparently. The data in `sycl::buffer`
    cannot be read or written directly, an accessor must be created. `sycl::accessor`
    objects specify the location of data access (host or a certain GPU kernel) and
    the access mode (read-only, write-only, read-write). Such approach allows optimizing
    task scheduling by building a directed acyclic graph (DAG) of data dependencies:
    if kernel *A* creates a write-only accessor to a buffer, and then kernel *B* is
    submitted with a read-only accessor to the same buffer, and then a host-side read-only
    accessor is requested, then it can be deduced that *A* must complete before *B*
    is launched and also that the results must be copied to the host before the host
    task can proceed, but the host task can run in parallel with kernel *B*. Since
    the dependencies between tasks can be built automatically, by default SYCL uses
    *out-of-order queues*: when two tasks are submitted to the same `sycl::queue`,
    it is not guaranteed that the second one will launch only after the first one
    completes. When launching a kernel, accessors must be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: Buffer-accessor model simplifies many aspects of heterogeneous programming and
    prevents many synchronization-related bugs, but it only allows very coarse control
    of data movement and kernel execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *USM* model is similar to how NVIDIA CUDA or AMD HIP manage memory. The
    programmer has to explicitly allocate the memory on the device (`sycl::malloc_device`),
    on the host (`sycl::malloc_host`), or in the shared memory space (`sycl::malloc_shared`).
    Despite its name, unified shared memory, and the similarity to OpenCL’s SVM, not
    all USM allocations are shared: for example, a memory allocated by `sycl::malloc_device`
    cannot be accessed from the host. The allocation functions return memory pointers
    that can be used directly, without accessors. This means that the programmer have
    to ensure the correct synchronization between host and device tasks to avoid data
    races. With USM, it is often convenient to use *in-order queues* with USM, instead
    of the default *out-of-order* queues. More information on USM can be found in
    the [Section 4.8 of SYCL 2020 specification](https://registry.khronos.org/SYCL/specs/sycl-2020/html/sycl-2020.html#sec:usm).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Exercise: Implement SAXPY in SYCL'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to write (fill-in-the-blanks) a simple code doing
    SAXPY (vector addition).
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile and run the code interactively, first make an allocation and load
    the AdaptiveCpp module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can run a simple device-detection utility to check that a GPU is available
    (note `srun`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: If you have not done it already, clone the repository using `git clone https://github.com/ENCCS/gpu-programming.git`
    or **update it** using `git pull origin main`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the example code in `content/examples/portable-kernel-models/exercise-sycl-saxpy.cpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile and run the code, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: The code will not compile as-is! Your task is to fill in missing bits indicated
    by `TODO` comments. You can also test your understanding using the “Bonus questions”
    in the code.
  prefs: []
  type: TYPE_NORMAL
- en: If you feel stuck, take a look at the `exercise-sycl-saxpy-solution.cpp` file.
  prefs: []
  type: TYPE_NORMAL
- en: SYCL compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intel oneAPI DPC++
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For targeting Intel GPUs, it is enough to install [Intel oneAPI Base Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html).
    Then, the compilation is as simple as `icpx -fsycl file.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to use oneAPI for NVIDIA and AMD GPUs. In addition to oneAPI
    Base Toolkit, the vendor-provided runtime (CUDA or HIP) and the corresponding
    [Codeplay oneAPI plugin](https://codeplay.com/solutions/oneapi/) must be installed.
    Then, the code can be compiled using Intel LLVM compiler bundled with oneAPI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clang++ -fsycl -fsycl-targets=nvidia_gpu_sm_86 file.cpp` for targeting CUDA
    8.6 NVIDIA GPU,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clang++ -fsycl -fsycl-targets=amd_gpu_gfx90a` for targeting GFX90a AMD GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaptiveCpp
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using AdaptiveCpp for NVIDIA or AMD GPUs also requires having CUDA or HIP installed
    first. Then `acpp` can be used for compiling the code, specifying the target devices.
    For example, here is how to compile the program supporting an AMD and an NVIDIA
    device:'
  prefs: []
  type: TYPE_NORMAL
- en: '`acpp --acpp-targets=''hip:gfx90a;cuda:sm_70'' file.cpp`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SYCL on LUMI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LUMI does not have a system-wide installation of any SYCL framework, but a
    recent AdaptiveCpp installation is available in CSC modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: The default compilation target is preset to MI250 GPUs, so to compile a single
    C++ file it is enough to call `acpp -O2 file.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When running applications built with AdaptiveCpp, one can often see the warning
    “dag_direct_scheduler: Detected a requirement that is neither of discard access
    mode”, reflecting the lack of an optimization hint when using buffer-accessor
    model. The warning is harmless and can be ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: Intel oneAPI DPC++
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For targeting Intel GPUs, it is enough to install [Intel oneAPI Base Toolkit](https://www.intel.com/content/www/us/en/developer/tools/oneapi/base-toolkit.html).
    Then, the compilation is as simple as `icpx -fsycl file.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to use oneAPI for NVIDIA and AMD GPUs. In addition to oneAPI
    Base Toolkit, the vendor-provided runtime (CUDA or HIP) and the corresponding
    [Codeplay oneAPI plugin](https://codeplay.com/solutions/oneapi/) must be installed.
    Then, the code can be compiled using Intel LLVM compiler bundled with oneAPI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`clang++ -fsycl -fsycl-targets=nvidia_gpu_sm_86 file.cpp` for targeting CUDA
    8.6 NVIDIA GPU,'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clang++ -fsycl -fsycl-targets=amd_gpu_gfx90a` for targeting GFX90a AMD GPU.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AdaptiveCpp
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Using AdaptiveCpp for NVIDIA or AMD GPUs also requires having CUDA or HIP installed
    first. Then `acpp` can be used for compiling the code, specifying the target devices.
    For example, here is how to compile the program supporting an AMD and an NVIDIA
    device:'
  prefs: []
  type: TYPE_NORMAL
- en: '`acpp --acpp-targets=''hip:gfx90a;cuda:sm_70'' file.cpp`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SYCL on LUMI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LUMI does not have a system-wide installation of any SYCL framework, but a
    recent AdaptiveCpp installation is available in CSC modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: The default compilation target is preset to MI250 GPUs, so to compile a single
    C++ file it is enough to call `acpp -O2 file.cpp`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When running applications built with AdaptiveCpp, one can often see the warning
    “dag_direct_scheduler: Detected a requirement that is neither of discard access
    mode”, reflecting the lack of an optimization hint when using buffer-accessor
    model. The warning is harmless and can be ignored.'
  prefs: []
  type: TYPE_NORMAL
- en: SYCL programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SYCL is, in many aspects, similar to OpenCL, but uses, like Kokkos, a single-source
    model with kernel lambdas.
  prefs: []
  type: TYPE_NORMAL
- en: 'To submit a task to device, first a sycl::queue must be created, which is used
    as a way to manage the task scheduling and execution. In the simplest case, that’s
    all the initialization one needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: 'If one wants more control, the device can be explicitly specified, or additional
    properties can be passed to a queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: 'Memory management can be done in two different ways: *buffer-accessor* model
    and *unified shared memory* (USM). The choice of the memory management models
    also influences how the GPU tasks are synchronized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *buffer-accessor* model, a `sycl::buffer` objects are used to represent
    arrays of data. A buffer is not mapped to any single one memory space, and can
    be migrated between the GPU and the CPU memory transparently. The data in `sycl::buffer`
    cannot be read or written directly, an accessor must be created. `sycl::accessor`
    objects specify the location of data access (host or a certain GPU kernel) and
    the access mode (read-only, write-only, read-write). Such approach allows optimizing
    task scheduling by building a directed acyclic graph (DAG) of data dependencies:
    if kernel *A* creates a write-only accessor to a buffer, and then kernel *B* is
    submitted with a read-only accessor to the same buffer, and then a host-side read-only
    accessor is requested, then it can be deduced that *A* must complete before *B*
    is launched and also that the results must be copied to the host before the host
    task can proceed, but the host task can run in parallel with kernel *B*. Since
    the dependencies between tasks can be built automatically, by default SYCL uses
    *out-of-order queues*: when two tasks are submitted to the same `sycl::queue`,
    it is not guaranteed that the second one will launch only after the first one
    completes. When launching a kernel, accessors must be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: Buffer-accessor model simplifies many aspects of heterogeneous programming and
    prevents many synchronization-related bugs, but it only allows very coarse control
    of data movement and kernel execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *USM* model is similar to how NVIDIA CUDA or AMD HIP manage memory. The
    programmer has to explicitly allocate the memory on the device (`sycl::malloc_device`),
    on the host (`sycl::malloc_host`), or in the shared memory space (`sycl::malloc_shared`).
    Despite its name, unified shared memory, and the similarity to OpenCL’s SVM, not
    all USM allocations are shared: for example, a memory allocated by `sycl::malloc_device`
    cannot be accessed from the host. The allocation functions return memory pointers
    that can be used directly, without accessors. This means that the programmer have
    to ensure the correct synchronization between host and device tasks to avoid data
    races. With USM, it is often convenient to use *in-order queues* with USM, instead
    of the default *out-of-order* queues. More information on USM can be found in
    the [Section 4.8 of SYCL 2020 specification](https://registry.khronos.org/SYCL/specs/sycl-2020/html/sycl-2020.html#sec:usm).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Exercise: Implement SAXPY in SYCL'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to write (fill-in-the-blanks) a simple code doing
    SAXPY (vector addition).
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile and run the code interactively, first make an allocation and load
    the AdaptiveCpp module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can run a simple device-detection utility to check that a GPU is available
    (note `srun`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: If you have not done it already, clone the repository using `git clone https://github.com/ENCCS/gpu-programming.git`
    or **update it** using `git pull origin main`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the example code in `content/examples/portable-kernel-models/exercise-sycl-saxpy.cpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile and run the code, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: The code will not compile as-is! Your task is to fill in missing bits indicated
    by `TODO` comments. You can also test your understanding using the “Bonus questions”
    in the code.
  prefs: []
  type: TYPE_NORMAL
- en: If you feel stuck, take a look at the `exercise-sycl-saxpy-solution.cpp` file.
  prefs: []
  type: TYPE_NORMAL
- en: alpaka
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The [alpaka](https://github.com/alpaka-group/alpaka3) library is an open-source
    header-only C++20 abstraction library for accelerator development.
  prefs: []
  type: TYPE_NORMAL
- en: Its aim is to provide performance portability across accelerators by abstracting
    the underlying levels of parallelism. The project provides a single-source C++
    API that enables developers to write parallel code once and run it on different
    hardware architectures without modification. The name “alpaka” comes from **A**bstractions
    for **L**evels of **P**arallelism, **A**lgorithms, and **K**ernels for **A**ccelerators.
    The library is platform-independent and supports the concurrent and cooperative
    use of multiple devices, including host CPUs (x86, ARM, and RISC-V) and GPUs from
    different vendors (NVIDIA, AMD, and Intel). A variety of accelerator backends,
    CUDA, HIP, SYCL, OpenMP, and serial execution, are available and can be selected
    based on the target device. Only a single implementation of a user kernel is required,
    expressed as a function object with a standardized interface. This eliminates
    the need to write specialized CUDA, HIP, SYCL, OpenMP, Intel TBB or threading
    code. Moreover, multiple accelerator backends can be combined to target different
    vendor hardware within a single system and even within a single application.
  prefs: []
  type: TYPE_NORMAL
- en: The abstraction is based on a virtual index domain decomposed into equally sized
    chunks called frames. **alpaka** provides a uniform abstraction to traverse these
    frames, independent of the underlying hardware. Algorithms to be parallelized
    map the chunked index domain and native worker threads onto the data, expressing
    the computation as kernels that are executed in parallel threads (SIMT), thereby
    also leveraging SIMD units. Unlike native parallelism models such as CUDA, HIP,
    and SYCL, **alpaka** kernels are not restricted to three dimensions. Explicit
    caching of data within a frame via shared memory allows developers to fully unleash
    the performance of the compute device. Additionally, **alpaka** offers primitive
    functions such as iota, transform, transform-reduce, reduce, and concurrent, simplifying
    the development of portable high-performance applications. Host, device, mapped,
    and managed multi-dimensional views provide a natural way to operate on data.
  prefs: []
  type: TYPE_NORMAL
- en: Here we demonstrate the usage of **alpaka3**, which is a complete rewrite of
    [alpaka](https://github.com/alpaka-group/alpaka). It is planned to merge this
    separate codebase back into the mainline alpaka repository before the first release
    in Q2/Q3 of 2026. Nevertheless, the code is well-tested and can be used for development
    today.
  prefs: []
  type: TYPE_NORMAL
- en: Installing alpaka on your system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For ease of use, we recommend installing alpaka using CMake as described below.
    For other ways to use alpaka in your projects, see the [alpaka3 documentation](https://alpaka3.readthedocs.io/en/latest/basic/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Clone the repository**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clone the alpaka source code from GitHub to a directory of your choice:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Set installation directory**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `ALPAKA_DIR` environment variable to the directory where you want to
    install alpaka. This can be any directory you choose where you have write access.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Build and install**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a build directory and use CMake to build and install alpaka. We use `CMAKE_INSTALL_PREFIX`
    to tell CMake where to install the library.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Update environment**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To make sure that other projects can find your alpaka installation, you should
    add the installation directory to your `CMAKE_PREFIX_PATH`. You can do this by
    adding the following line to your shell configuration file (e.g. `~/.bashrc`):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will need to source your shell configuration file or open a new terminal
    for the changes to take effect.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: alpaka Compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We recommend building your projects which use alpaka using CMake. A variety
    of strategies can be used to deal with building your application for a specific
    device or set of devices. Here we show a minimal way to get started, but this
    is by no means the only way to set up your projects. Please refer to the [alpaka3
    documentation](https://alpaka3.readthedocs.io/en/latest/basic/install.html) for
    alternative ways to use alpaka in your project, including a way to make your source
    code agnostic to the accelerator being targeted by defining a device specification
    in CMake.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates a `CMakeLists.txt` for a single-file project
    using alpaka3 (`main.cpp` which is presented in the section below):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Using alpaka on LUMI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To load the environment for using the AMD GPUs on LUMI with HIP, one can use
    the following modules -
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: alpaka Programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When starting with alpaka3, the first step is understanding the **device selection
    model**. Unlike frameworks that require explicit initialization calls, alpaka3
    uses a device specification to determine which backend and hardware to use. The
    device specification consists of two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**API**: The parallel programming interface (host, cuda, hip, oneApi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Device Kind**: The type of hardware (cpu, nvidiaGpu, amdGpu, intelGpu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we specify and use these at runtime to select and initialize devices. The
    device selection process is described in detail in the alpaka3 documentation.
  prefs: []
  type: TYPE_NORMAL
- en: alpaka3 uses an **execution space model** to abstract parallel hardware details.
    A device selector is created using `alpaka::onHost::makeDeviceSelector(devSpec)`,
    which returns an object that can query available devices and create device instances
    for the selected backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates a basic alpaka program that initializes
    a device and prints information about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: alpaka3 provides memory management abstractions through buffers and views. Memory
    can be allocated on host or device using `alpaka::allocBuf<T, Idx>(device, extent)`.
    Data transfers between host and device are handled through `alpaka::memcpy(queue,
    dst, src)`. The library automatically manages memory layouts for optimal performance
    on different architectures.
  prefs: []
  type: TYPE_NORMAL
- en: For parallel execution, alpaka3 provides kernel abstractions. Kernels are defined
    as functors or lambda functions and executed using work division specifications
    that define the parallelization strategy. The framework supports various parallel
    patterns including element-wise operations, reductions, and scans.
  prefs: []
  type: TYPE_NORMAL
- en: Tour of **alpaka** Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now we will quickly explore the most commonly used features of alpaka and go
    over some basic usage. A quick reference of commonly used alpaka features is available
    [here.](https://alpaka3.readthedocs.io/en/latest/basic/cheatsheet.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**General setup**: Include the consolidated header once and you are ready to
    start using alpaka.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '**Accelerator, platform, and device management**: Select devices by combining
    the desired API with the appropriate hardware kind using the device selector.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: '**Queues and events**: Create blocking or non-blocking queues per device, record
    events, and synchronize work as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: '**Memory management**: Allocate host, device, mapped, unified, or deferred
    buffers, create non-owning views, and move data portably with memcpy, memset,
    and fill.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: '**Kernel execution**: Build a FrameSpec manually or request one tuned for your
    data type, then enqueue kernels with automatic or explicit executors.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '**Kernel implementation**: Write kernels as functors annotated with ALPAKA_FN_ACC,
    use shared memory, synchronization, atomics, and math helpers directly inside
    the kernel body.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: Run alpaka3 Example in Simple Steps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following example works on systems with CMake 3.25+ and an appropriate C++
    compiler. For GPU execution, ensure the corresponding runtime (CUDA, ROCm, or
    oneAPI) is installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory for your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Copy the CMakeLists.txt from above into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the main.cpp file into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configure and build:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the executable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The device specification system allows you to select the target device at CMake
    configuration time. The format is `"api:deviceKind"`, where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**api**: The parallel programming interface (`host`, `cuda`, `hip`, `oneApi`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**deviceKind**: The type of device (`cpu`, `nvidiaGpu`, `amdGpu`, `intelGpu`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Available combinations are: `host:cpu`, `cuda:nvidiaGpu`, `hip:amdGpu`, `oneApi:cpu`,
    `oneApi:intelGpu`, `oneApi:nvidiaGpu`, `oneApi:amdGpu`'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA, HIP, or Intel backends only work if the CUDA SDK, HIP SDK, or OneAPI
    SDK are available respectively
  prefs: []
  type: TYPE_NORMAL
- en: Expected output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: The device name will vary depending on your hardware (e.g., “NVIDIA A100”, “AMD
    MI250X”, or your CPU model).
  prefs: []
  type: TYPE_NORMAL
- en: Compile and Execute Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can test the **alpaka** provided examples from the [example section](#examples).
    The examples have hard coded the usage of the AMD ROCm platform required on LUMI.
    To switch to CPU usage only you can simply replace `ap::onHost::makeDeviceSelector(ap::api::hip,
    ap::deviceKind::amdGpu);` with `ap::onHost::makeDeviceSelector(ap::api::host,
    ap::deviceKind::cpu);`
  prefs: []
  type: TYPE_NORMAL
- en: The following steps assume you have downloaded alpaka already and the path to
    the **alapka** source code is stored in the environment variable `ALPAKA_DIR`.
    To test the example copy the code into a file `main.cpp`
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, [click here](https://godbolt.org/z/69exnG4xb) to try the first
    example using in the godbolt compiler explorer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To use oneAPI Sycl with AMD or NVIDIA Gpus you must install the corresponding
    Codeplay oneAPI plugin as described [here](https://codeplay.com/solutions/oneapi/plugins/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To use oneAPI Sycl with AMD or NVIDIA Gpus you must install the corresponding
    Codeplay oneAPI plugin as described [here](https://codeplay.com/solutions/oneapi/plugins/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Exercise: Write a vector add kernel in alpaka'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to write (fill-in-the-blanks) a simple kernel
    to add two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile and run the code interactively, first we first need to get an allocation
    on a GPU node and load the modules for alpaka:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can run a simple device-detection utility to check that a GPU is available
    (note `srun`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s look at the code to set up the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Below we use fetch content with our CMake to get started with alpaka quickly.
  prefs: []
  type: TYPE_NORMAL
- en: CMakeLists.txt
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: Below we have the main alpaka code doing a vector addition on device using a
    high level transform function
  prefs: []
  type: TYPE_NORMAL
- en: main.cpp
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: To set up our project, we create a folder and place our CMakeLists.txt and main.cpp
    in there.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile and run the code, use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: Now your task will be to write and launch your first alpaka kernel. This kernel
    will do the vector addition and we will use this instead of the transform helper.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the vector add kernel
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: Installing alpaka on your system
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For ease of use, we recommend installing alpaka using CMake as described below.
    For other ways to use alpaka in your projects, see the [alpaka3 documentation](https://alpaka3.readthedocs.io/en/latest/basic/install.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Clone the repository**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Clone the alpaka source code from GitHub to a directory of your choice:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Set installation directory**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the `ALPAKA_DIR` environment variable to the directory where you want to
    install alpaka. This can be any directory you choose where you have write access.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Build and install**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a build directory and use CMake to build and install alpaka. We use `CMAKE_INSTALL_PREFIX`
    to tell CMake where to install the library.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Update environment**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To make sure that other projects can find your alpaka installation, you should
    add the installation directory to your `CMAKE_PREFIX_PATH`. You can do this by
    adding the following line to your shell configuration file (e.g. `~/.bashrc`):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You will need to source your shell configuration file or open a new terminal
    for the changes to take effect.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: alpaka Compilation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We recommend building your projects which use alpaka using CMake. A variety
    of strategies can be used to deal with building your application for a specific
    device or set of devices. Here we show a minimal way to get started, but this
    is by no means the only way to set up your projects. Please refer to the [alpaka3
    documentation](https://alpaka3.readthedocs.io/en/latest/basic/install.html) for
    alternative ways to use alpaka in your project, including a way to make your source
    code agnostic to the accelerator being targeted by defining a device specification
    in CMake.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates a `CMakeLists.txt` for a single-file project
    using alpaka3 (`main.cpp` which is presented in the section below):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE216]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: Using alpaka on LUMI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To load the environment for using the AMD GPUs on LUMI with HIP, one can use
    the following modules -
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: Using alpaka on LUMI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To load the environment for using the AMD GPUs on LUMI with HIP, one can use
    the following modules -
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: alpaka Programming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When starting with alpaka3, the first step is understanding the **device selection
    model**. Unlike frameworks that require explicit initialization calls, alpaka3
    uses a device specification to determine which backend and hardware to use. The
    device specification consists of two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**API**: The parallel programming interface (host, cuda, hip, oneApi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Device Kind**: The type of hardware (cpu, nvidiaGpu, amdGpu, intelGpu)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here we specify and use these at runtime to select and initialize devices. The
    device selection process is described in detail in the alpaka3 documentation.
  prefs: []
  type: TYPE_NORMAL
- en: alpaka3 uses an **execution space model** to abstract parallel hardware details.
    A device selector is created using `alpaka::onHost::makeDeviceSelector(devSpec)`,
    which returns an object that can query available devices and create device instances
    for the selected backend.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates a basic alpaka program that initializes
    a device and prints information about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs:
  - PREF_BQ
  type: TYPE_PRE
- en: alpaka3 provides memory management abstractions through buffers and views. Memory
    can be allocated on host or device using `alpaka::allocBuf<T, Idx>(device, extent)`.
    Data transfers between host and device are handled through `alpaka::memcpy(queue,
    dst, src)`. The library automatically manages memory layouts for optimal performance
    on different architectures.
  prefs: []
  type: TYPE_NORMAL
- en: For parallel execution, alpaka3 provides kernel abstractions. Kernels are defined
    as functors or lambda functions and executed using work division specifications
    that define the parallelization strategy. The framework supports various parallel
    patterns including element-wise operations, reductions, and scans.
  prefs: []
  type: TYPE_NORMAL
- en: Tour of **alpaka** Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now we will quickly explore the most commonly used features of alpaka and go
    over some basic usage. A quick reference of commonly used alpaka features is available
    [here.](https://alpaka3.readthedocs.io/en/latest/basic/cheatsheet.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**General setup**: Include the consolidated header once and you are ready to
    start using alpaka.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE220]'
  prefs: []
  type: TYPE_PRE
- en: '**Accelerator, platform, and device management**: Select devices by combining
    the desired API with the appropriate hardware kind using the device selector.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE221]'
  prefs: []
  type: TYPE_PRE
- en: '**Queues and events**: Create blocking or non-blocking queues per device, record
    events, and synchronize work as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: '**Memory management**: Allocate host, device, mapped, unified, or deferred
    buffers, create non-owning views, and move data portably with memcpy, memset,
    and fill.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE223]'
  prefs: []
  type: TYPE_PRE
- en: '**Kernel execution**: Build a FrameSpec manually or request one tuned for your
    data type, then enqueue kernels with automatic or explicit executors.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs: []
  type: TYPE_PRE
- en: '**Kernel implementation**: Write kernels as functors annotated with ALPAKA_FN_ACC,
    use shared memory, synchronization, atomics, and math helpers directly inside
    the kernel body.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: Run alpaka3 Example in Simple Steps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following example works on systems with CMake 3.25+ and an appropriate C++
    compiler. For GPU execution, ensure the corresponding runtime (CUDA, ROCm, or
    oneAPI) is installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory for your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Copy the CMakeLists.txt from above into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the main.cpp file into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configure and build:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the executable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The device specification system allows you to select the target device at CMake
    configuration time. The format is `"api:deviceKind"`, where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**api**: The parallel programming interface (`host`, `cuda`, `hip`, `oneApi`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**deviceKind**: The type of device (`cpu`, `nvidiaGpu`, `amdGpu`, `intelGpu`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Available combinations are: `host:cpu`, `cuda:nvidiaGpu`, `hip:amdGpu`, `oneApi:cpu`,
    `oneApi:intelGpu`, `oneApi:nvidiaGpu`, `oneApi:amdGpu`'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA, HIP, or Intel backends only work if the CUDA SDK, HIP SDK, or OneAPI
    SDK are available respectively
  prefs: []
  type: TYPE_NORMAL
- en: Expected output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: The device name will vary depending on your hardware (e.g., “NVIDIA A100”, “AMD
    MI250X”, or your CPU model).
  prefs: []
  type: TYPE_NORMAL
- en: Tour of **alpaka** Features
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now we will quickly explore the most commonly used features of alpaka and go
    over some basic usage. A quick reference of commonly used alpaka features is available
    [here.](https://alpaka3.readthedocs.io/en/latest/basic/cheatsheet.html)
  prefs: []
  type: TYPE_NORMAL
- en: '**General setup**: Include the consolidated header once and you are ready to
    start using alpaka.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: '**Accelerator, platform, and device management**: Select devices by combining
    the desired API with the appropriate hardware kind using the device selector.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: '**Queues and events**: Create blocking or non-blocking queues per device, record
    events, and synchronize work as needed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: '**Memory management**: Allocate host, device, mapped, unified, or deferred
    buffers, create non-owning views, and move data portably with memcpy, memset,
    and fill.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE233]'
  prefs: []
  type: TYPE_PRE
- en: '**Kernel execution**: Build a FrameSpec manually or request one tuned for your
    data type, then enqueue kernels with automatic or explicit executors.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE234]'
  prefs: []
  type: TYPE_PRE
- en: '**Kernel implementation**: Write kernels as functors annotated with ALPAKA_FN_ACC,
    use shared memory, synchronization, atomics, and math helpers directly inside
    the kernel body.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs: []
  type: TYPE_PRE
- en: Run alpaka3 Example in Simple Steps
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following example works on systems with CMake 3.25+ and an appropriate C++
    compiler. For GPU execution, ensure the corresponding runtime (CUDA, ROCm, or
    oneAPI) is installed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a directory for your project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Copy the CMakeLists.txt from above into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Copy the main.cpp file into the current folder
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configure and build:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the executable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The device specification system allows you to select the target device at CMake
    configuration time. The format is `"api:deviceKind"`, where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**api**: The parallel programming interface (`host`, `cuda`, `hip`, `oneApi`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**deviceKind**: The type of device (`cpu`, `nvidiaGpu`, `amdGpu`, `intelGpu`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Available combinations are: `host:cpu`, `cuda:nvidiaGpu`, `hip:amdGpu`, `oneApi:cpu`,
    `oneApi:intelGpu`, `oneApi:nvidiaGpu`, `oneApi:amdGpu`'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA, HIP, or Intel backends only work if the CUDA SDK, HIP SDK, or OneAPI
    SDK are available respectively
  prefs: []
  type: TYPE_NORMAL
- en: Expected output
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: The device name will vary depending on your hardware (e.g., “NVIDIA A100”, “AMD
    MI250X”, or your CPU model).
  prefs: []
  type: TYPE_NORMAL
- en: Compile and Execute Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can test the **alpaka** provided examples from the [example section](#examples).
    The examples have hard coded the usage of the AMD ROCm platform required on LUMI.
    To switch to CPU usage only you can simply replace `ap::onHost::makeDeviceSelector(ap::api::hip,
    ap::deviceKind::amdGpu);` with `ap::onHost::makeDeviceSelector(ap::api::host,
    ap::deviceKind::cpu);`
  prefs: []
  type: TYPE_NORMAL
- en: The following steps assume you have downloaded alpaka already and the path to
    the **alapka** source code is stored in the environment variable `ALPAKA_DIR`.
    To test the example copy the code into a file `main.cpp`
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, [click here](https://godbolt.org/z/69exnG4xb) to try the first
    example using in the godbolt compiler explorer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE243]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE244]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To use oneAPI Sycl with AMD or NVIDIA Gpus you must install the corresponding
    Codeplay oneAPI plugin as described [here](https://codeplay.com/solutions/oneapi/plugins/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE245]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To use oneAPI Sycl with AMD or NVIDIA Gpus you must install the corresponding
    Codeplay oneAPI plugin as described [here](https://codeplay.com/solutions/oneapi/plugins/).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: Exercise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Exercise: Write a vector add kernel in alpaka'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise we would like to write (fill-in-the-blanks) a simple kernel
    to add two vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compile and run the code interactively, first we first need to get an allocation
    on a GPU node and load the modules for alpaka:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can run a simple device-detection utility to check that a GPU is available
    (note `srun`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE248]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s look at the code to set up the exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Below we use fetch content with our CMake to get started with alpaka quickly.
  prefs: []
  type: TYPE_NORMAL
- en: CMakeLists.txt
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE249]'
  prefs: []
  type: TYPE_PRE
- en: Below we have the main alpaka code doing a vector addition on device using a
    high level transform function
  prefs: []
  type: TYPE_NORMAL
- en: main.cpp
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE250]'
  prefs: []
  type: TYPE_PRE
- en: To set up our project, we create a folder and place our CMakeLists.txt and main.cpp
    in there.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE251]'
  prefs: []
  type: TYPE_PRE
- en: 'To compile and run the code, use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE252]'
  prefs: []
  type: TYPE_PRE
- en: Now your task will be to write and launch your first alpaka kernel. This kernel
    will do the vector addition and we will use this instead of the transform helper.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the vector add kernel
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE253]'
  prefs: []
  type: TYPE_PRE
- en: Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parallel for with Unified Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE254]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE255]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE256]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE257]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE258]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE259]'
  prefs: []
  type: TYPE_PRE
- en: Parallel for with GPU buffers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE260]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE261]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE262]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE263]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE264]'
  prefs: []
  type: TYPE_PRE
- en: Asynchronous parallel for kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE265]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE266]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE267]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE268]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE269]'
  prefs: []
  type: TYPE_PRE
- en: Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE270]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE271]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE272]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE273]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE274]'
  prefs: []
  type: TYPE_PRE
- en: Parallel for with Unified Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE275]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE276]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE277]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE278]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE279]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE280]'
  prefs: []
  type: TYPE_PRE
- en: Parallel for with GPU buffers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE281]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE282]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE283]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE284]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE285]'
  prefs: []
  type: TYPE_PRE
- en: Asynchronous parallel for kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE286]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE287]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE288]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE289]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE290]'
  prefs: []
  type: TYPE_PRE
- en: Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE291]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE292]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE293]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE294]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE295]'
  prefs: []
  type: TYPE_PRE
- en: Pros and cons of cross-platform portability ecosystems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: General observations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The amount of code duplication is minimized.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The same code can be compiled to multiple architectures from different vendors.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Limited learning resources compared to CUDA (Stack Overflow, course material,
    documentation).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambda-based kernel models (Kokkos, SYCL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Higher level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Less knowledge of the underlying architecture is needed for initial porting.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Very nice and readable source code (C++ API).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The models are relatively new and not very popular yet.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Functor-based kernel model (alpaka)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very good portability.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Higher level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Low-level API always awailable which gives more control and allows fine tuning.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: User friendly C++ API for both the host and kernel code.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Small community and ecosystem.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate-source kernel models (OpenCL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very good portability.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Mature ecosystem.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Limited number of vendor-provided libraries.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Low-level API gives more control and allows fine tuning.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Both C and C++ APIs available (C++ API is less well supported).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The low-level API and separate-source kernel model are less user friendly.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: C++ Standard Parallelism (StdPar, PSTL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very high level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Easy to speed up code which already relying on STL algorithms.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Very little control over hardware.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Support by compilers is improving, but is far from mature.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: General code organization is similar to non-portable kernel-based models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As long as no vendor-specific functionality is used, the same code can run on
    any GPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General observations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The amount of code duplication is minimized.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The same code can be compiled to multiple architectures from different vendors.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Limited learning resources compared to CUDA (Stack Overflow, course material,
    documentation).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambda-based kernel models (Kokkos, SYCL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Higher level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Less knowledge of the underlying architecture is needed for initial porting.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Very nice and readable source code (C++ API).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The models are relatively new and not very popular yet.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Functor-based kernel model (alpaka)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very good portability.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Higher level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Low-level API always awailable which gives more control and allows fine tuning.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: User friendly C++ API for both the host and kernel code.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Small community and ecosystem.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Separate-source kernel models (OpenCL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very good portability.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Mature ecosystem.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Limited number of vendor-provided libraries.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Low-level API gives more control and allows fine tuning.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Both C and C++ APIs available (C++ API is less well supported).
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: The low-level API and separate-source kernel model are less user friendly.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: C++ Standard Parallelism (StdPar, PSTL)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Very high level of abstraction.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Easy to speed up code which already relying on STL algorithms.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Very little control over hardware.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  - PREF_IND
  type: TYPE_NORMAL
- en: Support by compilers is improving, but is far from mature.
  prefs:
  - PREF_BQ
  - PREF_UL
  type: TYPE_NORMAL
- en: Keypoints
  prefs: []
  type: TYPE_NORMAL
- en: General code organization is similar to non-portable kernel-based models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As long as no vendor-specific functionality is used, the same code can run on
    any GPU.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
