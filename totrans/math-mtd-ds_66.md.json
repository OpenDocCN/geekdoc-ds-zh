["```py\nimport torch.nn.functional as F\n\nx = torch.tensor([1.,0.,-1.])\ny = torch.tensor([0.,1.])\nW0 = torch.tensor([[0.,1.,-1.],[2.,0.,1.]], requires_grad=True)\nW1 = torch.tensor([[-1.,0.],[2.,-1.]], requires_grad=True)\n\nz0 = x\nz1 = F.sigmoid(W0 @ z0)\nz2 = F.softmax(W1 @ z1, dim=0)\nf = -torch.dot(torch.log(z2), y)\n\nprint(z0) \n```", "```py\ntensor([ 1.,  0., -1.]) \n```", "```py\nprint(z1) \n```", "```py\ntensor([0.7311, 0.7311], grad_fn=<SigmoidBackward0>) \n```", "```py\nprint(z2) \n```", "```py\ntensor([0.1881, 0.8119], grad_fn=<SoftmaxBackward0>) \n```", "```py\nprint(f) \n```", "```py\ntensor(0.2084, grad_fn=<NegBackward0>) \n```", "```py\nf.backward()\nprint(W0.grad) \n```", "```py\ntensor([[-0.1110, -0.0000,  0.1110],\n        [ 0.0370,  0.0000, -0.0370]]) \n```", "```py\nprint(W1.grad) \n```", "```py\ntensor([[ 0.1375,  0.1375],\n        [-0.1375, -0.1375]]) \n```", "```py\nwith torch.no_grad():\n    grad_W0 = torch.kron((z2 - y).unsqueeze(0) @ W1 @ torch.diag(z1 * (1-z1)), z0.unsqueeze(0))\n    grad_W1 = torch.kron((z2 - y).unsqueeze(0), z1.unsqueeze(0))\n\nprint(grad_W0) \n```", "```py\ntensor([[-0.1110, -0.0000,  0.1110,  0.0370,  0.0000, -0.0370]]) \n```", "```py\nprint(grad_W1) \n```", "```py\ntensor([[ 0.1375,  0.1375, -0.1375, -0.1375]]) \n```", "```py\ndevice = torch.device('cuda' if torch.cuda.is_available() \n                      else ('mps' if torch.backends.mps.is_available() \n                            else 'cpu'))\nprint('Using device:', device) \n```", "```py\nUsing device: mps \n```", "```py\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\n\nseed = 42\ntorch.manual_seed(seed)\n\nif device.type == 'cuda': # device-specific seeding and settings\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nelif device.type == 'mps':\n    torch.mps.manual_seed(seed)  # MPS-specific seeding\n\ng = torch.Generator()\ng.manual_seed(seed)\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                               download=True, transform=transforms.ToTensor())\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, \n                              download=True, transform=transforms.ToTensor())\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=g)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n```", "```py\nmodel = nn.Sequential(\n    nn.Flatten(),                      # Flatten the input\n    nn.Linear(28 * 28, 32),            # First Linear layer with 32 nodes\n    nn.Sigmoid(),                      # Sigmoid activation function\n    nn.Linear(32, 10)                  # Second Linear layer with 10 nodes (output layer)\n).to(device) \n```", "```py\nloss_fn = nn.CrossEntropyLoss()  \noptimizer = optim.SGD(model.parameters(), lr=1e-3) \n```", "```py\nmmids.training_loop(train_loader, model, loss_fn, optimizer, device, epochs=10) \n```", "```py\nmmids.test(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 64.0% accuracy \n```", "```py\nloss_fn = nn.CrossEntropyLoss()  \noptimizer = optim.Adam(model.parameters())\nmmids.training_loop(train_loader, model, loss_fn, optimizer, device, epochs=10) \n```", "```py\nmmids.test(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 87.1% accuracy \n```", "```py\nimport torch.nn.functional as F\n\nx = torch.tensor([1.,0.,-1.])\ny = torch.tensor([0.,1.])\nW0 = torch.tensor([[0.,1.,-1.],[2.,0.,1.]], requires_grad=True)\nW1 = torch.tensor([[-1.,0.],[2.,-1.]], requires_grad=True)\n\nz0 = x\nz1 = F.sigmoid(W0 @ z0)\nz2 = F.softmax(W1 @ z1, dim=0)\nf = -torch.dot(torch.log(z2), y)\n\nprint(z0) \n```", "```py\ntensor([ 1.,  0., -1.]) \n```", "```py\nprint(z1) \n```", "```py\ntensor([0.7311, 0.7311], grad_fn=<SigmoidBackward0>) \n```", "```py\nprint(z2) \n```", "```py\ntensor([0.1881, 0.8119], grad_fn=<SoftmaxBackward0>) \n```", "```py\nprint(f) \n```", "```py\ntensor(0.2084, grad_fn=<NegBackward0>) \n```", "```py\nf.backward()\nprint(W0.grad) \n```", "```py\ntensor([[-0.1110, -0.0000,  0.1110],\n        [ 0.0370,  0.0000, -0.0370]]) \n```", "```py\nprint(W1.grad) \n```", "```py\ntensor([[ 0.1375,  0.1375],\n        [-0.1375, -0.1375]]) \n```", "```py\nwith torch.no_grad():\n    grad_W0 = torch.kron((z2 - y).unsqueeze(0) @ W1 @ torch.diag(z1 * (1-z1)), z0.unsqueeze(0))\n    grad_W1 = torch.kron((z2 - y).unsqueeze(0), z1.unsqueeze(0))\n\nprint(grad_W0) \n```", "```py\ntensor([[-0.1110, -0.0000,  0.1110,  0.0370,  0.0000, -0.0370]]) \n```", "```py\nprint(grad_W1) \n```", "```py\ntensor([[ 0.1375,  0.1375, -0.1375, -0.1375]]) \n```", "```py\ndevice = torch.device('cuda' if torch.cuda.is_available() \n                      else ('mps' if torch.backends.mps.is_available() \n                            else 'cpu'))\nprint('Using device:', device) \n```", "```py\nUsing device: mps \n```", "```py\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.optim as optim\n\nseed = 42\ntorch.manual_seed(seed)\n\nif device.type == 'cuda': # device-specific seeding and settings\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nelif device.type == 'mps':\n    torch.mps.manual_seed(seed)  # MPS-specific seeding\n\ng = torch.Generator()\ng.manual_seed(seed)\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, \n                               download=True, transform=transforms.ToTensor())\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, \n                              download=True, transform=transforms.ToTensor())\n\nBATCH_SIZE = 32\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, generator=g)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n```", "```py\nmodel = nn.Sequential(\n    nn.Flatten(),                      # Flatten the input\n    nn.Linear(28 * 28, 32),            # First Linear layer with 32 nodes\n    nn.Sigmoid(),                      # Sigmoid activation function\n    nn.Linear(32, 10)                  # Second Linear layer with 10 nodes (output layer)\n).to(device) \n```", "```py\nloss_fn = nn.CrossEntropyLoss()  \noptimizer = optim.SGD(model.parameters(), lr=1e-3) \n```", "```py\nmmids.training_loop(train_loader, model, loss_fn, optimizer, device, epochs=10) \n```", "```py\nmmids.test(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 64.0% accuracy \n```", "```py\nloss_fn = nn.CrossEntropyLoss()  \noptimizer = optim.Adam(model.parameters())\nmmids.training_loop(train_loader, model, loss_fn, optimizer, device, epochs=10) \n```", "```py\nmmids.test(test_loader, model, loss_fn, device) \n```", "```py\nTest error: 87.1% accuracy \n```"]