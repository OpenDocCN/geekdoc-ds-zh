- en: '20'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: UNDERSTANDING EXPERIMENTAL DATA
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is about understanding experimental data. We will make extensive
    use of plotting to visualize the data and show how to use linear regression to
    build a model of experimental data. We will also talk about the interplay between
    physical and computational experiments. We defer our discussion of how to draw
    valid statistical conclusions about data to Chapter 21.
  prefs: []
  type: TYPE_NORMAL
- en: 20.1 The Behavior of Springs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Springs are wonderful things. When they are compressed or stretched by some
    force, they store energy. When that force is no longer applied, they release the
    stored energy. This property allows them to smooth the ride in cars, help mattresses
    conform to our bodies, retract seat belts, and launch projectiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 1676 the British physicist Robert Hooke formulated **Hooke''s law** of elasticity:
    *Ut tensio, sic vis*, in English, `F = -kx`. In other words, the force `F` stored
    in a spring is linearly related to the distance the spring has been compressed
    (or stretched). (The minus sign indicates that the force exerted by the spring
    is in the opposite direction of the displacement.) Hooke''s law holds for a wide
    variety of materials and systems, including many biological systems. Of course,
    it does not hold for an arbitrarily large force. All springs have an **elastic
    limit**, beyond which the law fails. Those of you who have stretched a Slinky
    too far know this all too well.'
  prefs: []
  type: TYPE_NORMAL
- en: The constant of proportionality, `k`, is called the **spring constant**. If
    the spring is stiff (like the ones in the suspension of a car or the limbs of
    an archer's bow), `k` is large. If the spring is weak, like the spring in a ballpoint
    pen, `k` is small.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the spring constant of a particular spring can be a matter of some import.
    The calibrations of both simple scales and atomic force microscopes depend upon
    knowing the spring constants of components. The mechanical behavior of a strand
    of DNA is related to the force required to compress it. The force with which a
    bow launches an arrow is related to the spring constant of its limbs. And so on.
  prefs: []
  type: TYPE_NORMAL
- en: Generations of physics students have learned to estimate spring constants using
    an experimental apparatus similar to one pictured in [Figure 20-1](#c20-fig-0001).
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0001.jpg](../images/c20-fig-0001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-1](#c20-fig-0001a) A classic experiment'
  prefs: []
  type: TYPE_NORMAL
- en: We start with a spring with no attached weight, and measure the distance to
    the bottom of the spring from the top of the stand. We then hang a known mass
    on the spring and wait for it to stop moving. At this point, the force stored
    in the spring is the force exerted on the spring by the weight hanging from it.
    This is the value of `F` in Hooke's law. We again measure the distance from the
    bottom of the spring to the top of the stand. The difference between this distance
    and the distance before we hung the weight becomes the value of `x` in Hooke's
    law.
  prefs: []
  type: TYPE_NORMAL
- en: We know that the force, `F`, being exerted on the spring is equal to the mass,
    `m`, multiplied by the acceleration due to gravity, `g` (`9.81 m/s`² is a pretty
    good approximation of `g` on the surface of this planet), so we substitute `m`*`g`
    for `F`. By simple algebra, we know that `k = -(m`*`g)/x.`
  prefs: []
  type: TYPE_NORMAL
- en: Suppose, for example, that `m = 1kg` and `x = 0.1m`, then
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-5001.jpg](../images/c20-fig-5001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: According to this calculation, it will take `*98.1*` Newtons[^(137)](#c20-fn-0001)
    of force to stretch the spring one meter.
  prefs: []
  type: TYPE_NORMAL
- en: This would all be well and good if
  prefs: []
  type: TYPE_NORMAL
- en: We had complete confidence that we would conduct this experiment perfectly.
    In that case, we could take one measurement, perform the calculation, and know
    that we had found `k`. Unfortunately, experimental science hardly ever works this
    way.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We could be sure that we were operating below the elastic limit of the spring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A more robust experiment would be to hang a series of increasingly heavier
    weights on the spring, measure the stretch of the spring each time, and plot the
    results. We ran such an experiment, and typed the results into a file named `springData.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The function in [Figure 20-2](#c20-fig-0003) reads data from a file such as
    the one we saved, and returns lists containing the distances and masses.
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0002.jpg](../images/c20-fig-0002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-2](#c20-fig-0003a) Extracting the data from a file'
  prefs: []
  type: TYPE_NORMAL
- en: The function in [Figure 20-3](#c20-fig-0004) uses `get_data` to extract the
    experimental data from the file and then produces the plot in [Figure 20-4](#c20-fig-0005).
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0003.jpg](../images/c20-fig-0003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-3](#c20-fig-0004a) Plotting the data'
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0004.jpg](../images/c20-fig-0004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-4](#c20-fig-0005a) Displacement of spring'
  prefs: []
  type: TYPE_NORMAL
- en: This is not what Hooke's law predicts. Hooke's law tells us that the distance
    should increase linearly with the mass, i.e., the points should lie on a straight
    line the slope of which is determined by the spring constant. Of course, we know
    that when we take real measurements, the experimental data are rarely a perfect
    match for the theory. Measurement error is to be expected, so we should expect
    the points to lie around a line rather than on it.
  prefs: []
  type: TYPE_NORMAL
- en: Still, it would be nice to see a line that represents our best guess of where
    the points would have been if we had no measurement error. The usual way to do
    this is to fit a line to the data.
  prefs: []
  type: TYPE_NORMAL
- en: 20.1.1 Using Linear Regression to Find a Fit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Whenever we fit any curve (including a line) to data, we need some way to decide
    which curve is the best **fit** for the data. This means that we need to define
    an objective function that provides a quantitative assessment of how well the
    curve fits the data. Once we have such a function, finding the best fit can be
    formulated as finding a curve that minimizes (or maximizes) the value of that
    function, i.e., as an optimization problem (see Chapters 14 and 15).
  prefs: []
  type: TYPE_NORMAL
- en: The most commonly used objective function is called **least squares**. Let `*observed*`
    and `*predicted*` be vectors of equal length, where `*observed*` contains the
    measured points and *predicted* the corresponding data points on the proposed
    fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The objective function is then defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-5002.jpg](../images/c20-fig-5002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Squaring the difference between observed and predicted points makes large differences
    between observed and predicted points relatively more important than small differences.
    Squaring the difference also discards information about whether the difference
    is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: How might we go about finding the best least-squares fit? One way is to use
    a successive approximation algorithm similar to the Newton–Raphson algorithm in
    Chapter 3\. Alternatively, there are analytic solutions that are often applicable.
    But we don't have to implement either Newton–Raphson or an analytic solution,
    because `numpy` provides a built-in function, `polyfit`, that finds an approximation
    to the best least-squares fit. The call
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: finds the coefficients of a polynomial of degree `n` that provides a best least-squares
    fit for the set of points defined by the two arrays `observed_x_vals` and `observed_y_vals`.
    For example, the call
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: will find a line described by the polynomial `y = ax + b`, where `a` is the
    slope of the line and `b` the y-intercept. In this case, the call returns an array
    with two floating-point values. Similarly, a parabola is described by the quadratic
    equation `y = ax`² `+ bx + c`. Therefore, the call
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: returns an array with three floating-point values.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm used by `polyfit` is called **linear regression**. This may seem
    a bit confusing, since we can use it to fit curves other than lines. Some authors
    do make a distinction between linear regression (when the model is a line) and
    **polynomial regression** (when the model is a polynomial with degree greater
    than `1`), but most do not.[^(138)](#c20-fn-0002)
  prefs: []
  type: TYPE_NORMAL
- en: The function `fit_data` in [Figure 20-5](#c20-fig-0007) extends the `plot_data`
    function in [Figure 20-3](#c20-fig-0004) by adding a line that represents the
    best fit for the data. It uses `polyfit` to find the coefficients `a` and `b`,
    and then uses those coefficients to generate the predicted spring displacement
    for each force. Notice that there is an asymmetry in the way `forces` and `distance`
    are treated. The values in `forces` (which are derived from the mass suspended
    from the spring) are treated as independent, and used to produce the values in
    the dependent variable `predicted_distances` (a prediction of the displacements
    produced by suspending the mass).
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0005.jpg](../images/c20-fig-0005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-5](#c20-fig-0007a) Fitting a curve to data'
  prefs: []
  type: TYPE_NORMAL
- en: The function also computes the spring constant, `k`. The slope of the line,
    `a`, is `Δdistance/Δforce`. The spring constant, on the other hand, is `Δforce/Δdistance`.
    Consequently, `k` is the inverse of `a.`
  prefs: []
  type: TYPE_NORMAL
- en: The call `fit_data('springData.csv')` produces the plot in [Figure 20-6](#c20-fig-0008).
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0006.jpg](../images/c20-fig-0006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-6](#c20-fig-0008a) Measured points and linear model'
  prefs: []
  type: TYPE_NORMAL
- en: It is interesting to observe that very few points actually lie on the least-squares
    fit. This is plausible because we are trying to minimize the sum of the squared
    errors, rather than maximize the number of points that lie on the line. Still,
    it doesn't look like a great fit. Let's try a cubic fit by adding to `fit_data`
    the code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we have used the function `polyval` to generate the points associated
    with the cubic fit. This function takes two arguments: a sequence of polynomial
    coefficients and a sequence of values at which the polynomial is to be evaluated.
    The code fragments'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: are equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: This produces the plot in [Figure 20-7](#c20-fig-0009). The cubic fit looks
    like a much better model of the data than the linear fit, but is it? Probably
    not.
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0007.jpg](../images/c20-fig-0007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-7](#c20-fig-0009a) Linear and cubic fits'
  prefs: []
  type: TYPE_NORMAL
- en: Both technical and popular articles frequently include plots like this that
    show both raw data and a curve fit to the data. All too often, however, the authors
    then go on to assume that the fitted curve is the description of the real situation,
    and the raw data merely an indication of experimental error. This can be dangerous.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that we started with a theory that there should be a linear relationship
    between the `x` and `y` values, not a cubic one. Let's see what happens if we
    use our linear and cubic fits to predict where the point corresponding to hanging
    a `1.5kg` weight would lie, [Figure 20-8](#c20-fig-0010).
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0008.jpg](../images/c20-fig-0008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-8](#c20-fig-0010a) Using the model to make a prediction'
  prefs: []
  type: TYPE_NORMAL
- en: Now the cubic fit doesn't look so good. In particular, it seems highly unlikely
    that by hanging a large weight on the spring we can cause the spring to rise above
    (the y value is negative) the bar from which it is suspended. What we have is
    an example of **overfitting**. Overfitting typically occurs when a model is excessively
    complex, e.g., it has too many parameters relative to the amount of data. When
    this happens, the fit can capture noise in the data rather than meaningful relationships.
    A model that has been overfit usually has poor predictive power, as seen in this
    example.
  prefs: []
  type: TYPE_NORMAL
- en: '**Finger exercise:** Modify the code in [Figure 20-5](#c20-fig-0007) so that
    it produces the plot in [Figure 20-8](#c20-fig-0010).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's go back to the linear fit. For the moment, forget the line and study the
    raw data. Does anything about it seem odd? If we were to fit a line to the rightmost
    six points it would be nearly parallel to the x-axis. This seems to contradict
    Hooke's law—until we recall that Hooke's law holds only up to some elastic limit.
    Perhaps that limit is reached for this spring somewhere around `7N` (approximately
    `0.7kg`).
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what happens if we eliminate the last six points by replacing the
    second and third lines of the `fit_data` by
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As [Figure 20-9](#c20-fig-0011) shows, eliminating those points certainly makes
    a difference: `k` has dropped dramatically and the linear and cubic fits are almost
    indistinguishable. But how do we know which of the two linear fits is a better
    representation of how our spring performs up to its elastic limit? We could use
    some statistical test to determine which line is a better fit for the data, but
    that would be beside the point. This is not a question that can be answered by
    statistics. After all we could throw out all the data except any two points and
    know that `polyfit` would find a line that would be a perfect fit for those two
    points. It is never appropriate to throw out experimental results merely to get
    a better fit.[^(139)](#c20-fn-0003) Here we justified throwing out the rightmost
    points by appealing to the theory underlying Hooke''s law, i.e., that springs
    have an elastic limit. That justification could not have been appropriately used
    to eliminate points elsewhere in the data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0009.jpg](../images/c20-fig-0009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-9](#c20-fig-0011a) A model up to the elastic limit'
  prefs: []
  type: TYPE_NORMAL
- en: 20.2 The Behavior of Projectiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Growing bored with merely stretching springs, we decided to use one of our springs
    to build a device capable of launching a projectile.[^(140)](#c20-fn-0004) We
    used the device four times to fire a projectile at a target `30` yards (`1080`
    inches) from the launching point. Each time, we measured the height of the projectile
    at various distances from the launching point. The launching point and the target
    were at the same height, which we treated as `0.0` in our measurements.
  prefs: []
  type: TYPE_NORMAL
- en: The data was stored in a file, part of which shown in [Figure 20-10](#c20-fig-0012).
    The first column contains distances of the projectile from the target. The other
    columns contain the height of the projectile at that distance for each of the
    four trials. All of the measurements are in inches.
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0010.jpg](../images/c20-fig-0010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-10](#c20-fig-0012a) Data from projectile experiment'
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Figure 20-11](#c20-fig-0013) was used to plot the mean altitude
    of the projectile in the four trials against the distance from the point of launch.
    It also plots the best linear and quadratic fits to those points. (In case you
    have forgotten the meaning of multiplying a list by an integer, the expression
    `[0]*len(distances)` produces a list of `len(distances)` `0`'s.)
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0011.jpg](../images/c20-fig-0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-11](#c20-fig-0013a) Plotting the trajectory of a projectile'
  prefs: []
  type: TYPE_NORMAL
- en: A quick look at the plot in [Figure 20-12](#c20-fig-0014) makes it quite clear
    that a quadratic fit is far better than a linear one.[^(141)](#c20-fn-0005) But,
    in an absolute sense, just how bad a fit is the line and how good is the quadratic
    fit?
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0012.jpg](../images/c20-fig-0012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-12](#c20-fig-0014a) Plot of trajectory'
  prefs: []
  type: TYPE_NORMAL
- en: 20.2.1  Coefficient of Determination
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When we fit a curve to a set of data, we are finding a function that relates
    an independent variable (inches horizontally from the launch point in this example)
    to a predicted value of a dependent variable (inches above the launch point in
    this example). Asking about the **goodness of a fit** is equivalent to asking
    about the accuracy of these predictions. Recall that the fits were found by minimizing
    the mean square error. This suggests that we could evaluate the goodness of a
    fit by looking at the mean square error. The problem with that approach is that
    while there is a lower bound for the mean square error (0), there is no upper
    bound. This means that while the mean square error is useful for comparing the
    relative goodness of two fits to the same data, it is not particularly useful
    for getting a sense of the absolute goodness of a fit.
  prefs: []
  type: TYPE_NORMAL
- en: We can calculate the absolute goodness of a fit using the **coefficient of determination**,
    often written as `R`**²**.[^(142)](#c20-fn-0006) Let *y[i]* be the *i^(th)*observed
    value, *p[i]* be the corresponding value predicted by the model, and *μ* be the
    mean of the observed values.
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-5003.jpg](../images/c20-fig-5003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By comparing the estimation errors (the numerator) with the variability of the
    original values (the denominator), `R`² is intended to capture the proportion
    of variability (relative to the mean) in a data set that is accounted for by the
    statistical model provided by the fit. When the model being evaluated is produced
    by a linear regression, the value of `R`² always lies between `0` and `1`. If
    `R`² `= 1`, the model is a perfect fit to the data. If `R`² `= 0`, there is no
    relationship between the values predicted by the model and the way the data is
    distributed around the mean.
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Figure 20-13](#c20-fig-0016) provides a straightforward implementation
    of this statistical measure. Its compactness stems from the expressiveness of
    the operations on numpy arrays. The expression `(predicted - measured)**2` subtracts
    the elements of one array from the elements of another, and then squares each
    element in the result. The expression `(measured - mean_of_measured)**2)` subtracts
    the scalar value `mean_of_measured` from each element of the array `measured`,
    and then squares each element of the results.
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0013.jpg](../images/c20-fig-0013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-13](#c20-fig-0016a) Computing R²'
  prefs: []
  type: TYPE_NORMAL
- en: When the lines of code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: are inserted after the appropriate calls to `plt.plot` in `process_trajectories`
    (see [Figure 20-11](#c20-fig-0013)), they print
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Roughly speaking, this tells us that less than `2%` of the variation in the
    measured data can be explained by the linear model, but more than `98%` of the
    variation can be explained by the quadratic model.
  prefs: []
  type: TYPE_NORMAL
- en: 20.2.2 Using a Computational Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have what seems to be a good model of our data, we can use this
    model to help answer questions about our original data. One interesting question
    is the horizontal speed at which the projectile is traveling when it hits the
    target. We might use the following train of thought to design a computation that
    answers this question:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. We know that the trajectory of the projectile is given by a formula of the
    form `y = ax`² `+ bx + c`, i.e., it is a parabola. Since every parabola is symmetrical
    around its vertex, we know that its peak occurs halfway between the launch point
    and the target; call this distance `xMid`. The peak height, yPeak, is therefore
    given by yPeak =a*xMid² + b*xMid + c.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. If we ignore air resistance (remember that no model is perfect), we can
    compute the amount of time it takes for the projectile to fall from `yPeak` to
    the height of the target, because that is purely a function of gravity. It is
    given by the equation ![c20-fig-5004.jpg](../images/c20-fig-5004.jpg).[^(143)](#c20-fn-0007)
    This is also the amount of time it takes for the projectile to travel the horizontal
    distance from `xMid` to the target, because once it reaches the target it stops
    moving.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. Given the time to go from `xMid` to the target, we can easily compute the
    average horizontal speed of the projectile over that interval. If we assume that
    the projectile was neither accelerating nor decelerating in the horizontal direction
    during that interval, we can use the average horizontal speed as an estimate of
    the horizontal speed when the projectile hits the target.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Figure 20-14](#c20-fig-0017) implements this technique for estimating the
    horizontal velocity of the projectile.[^(144)](#c20-fn-0008)'
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0014.jpg](../images/c20-fig-0014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-14](#c20-fig-0017a) Computing the horizontal speed of a projectile'
  prefs: []
  type: TYPE_NORMAL
- en: "When the line `\uFEFFget_horizontal_speed(fit, distances[-1], distances[0])`\
    \ is inserted at the end of `process_trajectories (`[Figure 20-11](#c20-fig-0013)`)`,\
    \ it prints"
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The sequence of steps we have just worked through follows a common pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 1\. We started by performing an experiment to get some data about the behavior
    of a physical system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. We then used computation to find and evaluate the quality of a model of
    the behavior of the system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. Finally, we used some theory and analysis to design a simple computation
    to derive an interesting consequence of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Finger exercise:** In a vacuum, the speed of a falling object is defined
    by the equation `v = v0 + gt`, where `v0` is the initial velocity of the object,
    `t` is the number of seconds the object has been falling, and *g* is the gravitational
    constant, roughly `9.8 m/sec`² on the surface of the Earth and `3.711 m/ sec`²
    on Mars. A scientist measures the velocity of a falling object on an unknown planet.
    She does this by measuring the downward velocity of an object at different points
    in time. At time `0`, the object has an unknown velocity of `v0`. Implement a
    function that fits a model to the time and velocity data and estimates `g` for
    that planet and `v0` for the experiment. It should return its estimates for `g`
    and `v0`, and also r-squared for the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 20.3 Fitting Exponentially Distributed Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Polyfit` uses linear regression to find a polynomial of a given degree that
    is the best least-squares fit for some data. It works well if the data can be
    directly approximated by a polynomial. But this is not always possible. Consider,
    for example, the simple exponential growth function `y = 3`^x. The code in [Figure
    20-15](#c20-fig-0018) fits a fifth-degree polynomial to the first tenpoints and
    plots the results as shown in [Figure 20-16](#c20-fig-0019). It uses the function
    call `np.arange(10)`, which returns an `array` containing the integers `0-9.`
    The parameter setting `markeredgewidth = 2` sets the width of the lines used in
    the marker.'
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0015.jpg](../images/c20-fig-0015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-15](#c20-fig-0018a) Fitting a polynomial curve to an exponential
    distribution'
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0016.jpg](../images/c20-fig-0016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-16](#c20-fig-0019a) Fitting an exponential distribution'
  prefs: []
  type: TYPE_NORMAL
- en: The fit is clearly a good one for these data points. However, let's look at
    what the model predicts for `3`^(20). When we add the code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: to the end of [Figure 20-15](#c20-fig-0018), it prints,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Oh dear! Despite fitting the data, the model produced by `polyfit` is apparently
    not a good one. Is it because `5` was not the right degree? No. It is because
    no polynomial is a good fit for an exponential distribution. Does this mean that
    we cannot use `polyfit` to build a model of an exponential distribution? Fortunately,
    it does not, because we can use `polyfit` to find a curve that fits the original
    independent values and the log of the dependent values.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the exponential sequence `[1, 2, 4, 8, 16, 32, 64, 128, 256, 512]`.
    If we take the log base `2` of each value, we get the sequence `[0, 1, 2, 3, 4,
    5, 6, 7, 8, 9]`, i.e., a sequence that grows linearly. In fact, if a function
    `y = f(x)` exhibits exponential growth, the log (to any base) of `f(x)` grows
    linearly. This can be visualized by plotting an exponential function with a logarithmic
    y-axis. The code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: produces the plot in [Figure 20-17](#c20-fig-0020).
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0017.jpg](../images/c20-fig-0017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-17](#c20-fig-0020a) An exponential on a semilog plot'
  prefs: []
  type: TYPE_NORMAL
- en: The fact that taking the log of an exponential function produces a linear function
    can be used to construct a model for an exponentially distributed set of data
    points, as illustrated by the code in [Figure 20-18](#c20-fig-0021). We use `polyfit`
    to find a curve that fits the `x` values and the log of the `y` values. Notice
    that we use yet another Python standard library module, `math`, which supplies
    a `log` function. (We could have used `np.log2`, but wanted to point out that
    `math` has a more general log function.)
  prefs: []
  type: TYPE_NORMAL
- en: '![c20-fig-0018.jpg](../images/c20-fig-0018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-18](#c20-fig-0021a) Using `polyfit` to fit an exponential'
  prefs: []
  type: TYPE_NORMAL
- en: When run, the code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: produces the plot in [Figure 20-19](#c20-fig-0022), in which the actual values
    and the predicted values coincide. Moreover, when the model is tested on a value
    (`20`) that was not used to produce the fit, it prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![c20-fig-0019.jpg](../images/c20-fig-0019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 20-19](#c20-fig-0022a) A fit for an exponential function'
  prefs: []
  type: TYPE_NORMAL
- en: This method of using `polyfit` to find a model for data works when the relationship
    can be described by an equation of the form `y = base`^(ax+b). If used on data
    for which such a model is not a reasonably good fit, it will yield poor results.
  prefs: []
  type: TYPE_NORMAL
- en: To see this, let's create `yVals` using
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The model now makes a poor prediction, printing
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 20.4 When Theory Is Missing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have emphasized the interplay between theoretical, experimental,
    and computational science. Sometimes, however, we find ourselves with lots of
    interesting data, but little or no theory. In such cases, we often resort to using
    computational techniques to develop a theory by building a model that seems to
    fit the data.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, we would run a controlled experiment (e.g., hang weights
    from a spring), study the results, and retrospectively formulate a model consistent
    with those results. We would then run a new experiment (e.g., hang different weights
    from the same spring), and compare the results of that experiment to what the
    model predicted.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, in many cases it is impossible to run even one controlled experiment.
    Imagine, for example, building a model designed to shed light on how interest
    rates affect stock prices. Very few of us are in a position to set interest rates
    and see what happens. On the other hand, there is no shortage of relevant historical
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In such situations, we can simulate a set of experiments by dividing the existing
    data into a **training set** and a **holdout set** to use as a **test set**. Without
    looking at the holdout set, we build a model that seems to explain the training
    set. For example, we find a curve that has a reasonable `R`² for the training
    set. We then test that model on the holdout set. Most of the time the model will
    fit the training set more closely than it fits the holdout set. But if the model
    is a good one, it should fit the holdout set reasonably well. If it doesn't, the
    model should probably be discarded.
  prefs: []
  type: TYPE_NORMAL
- en: How do we choose the training set? We want it to be representative of the data
    set as a whole. One way to do this is to randomly choose the samples for the training
    set. If the data set is sufficiently large, this often works pretty well.
  prefs: []
  type: TYPE_NORMAL
- en: A related but slightly different way to check a model is to train on many randomly
    selected subsets of the original data and see how similar the models are to one
    another. If they are quite similar, then we can feel pretty good. This approach
    is known as **cross validation**.
  prefs: []
  type: TYPE_NORMAL
- en: Cross validation is discussed in more detail in Chapters 21 and 24.
  prefs: []
  type: TYPE_NORMAL
- en: 20.5 Terms Introduced in Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hooke's law
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: elastic limit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: spring constant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: curve fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: least squares
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: polynomial regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: overfitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: goodness of fit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: coefficient of determination (R²)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: training set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: test set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: holdout set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cross validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
