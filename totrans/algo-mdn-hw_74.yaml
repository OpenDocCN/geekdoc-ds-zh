- en: Argmin with SIMD
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SIMD的Argmin
- en: 原文：[https://en.algorithmica.org/hpc/algorithms/argmin/](https://en.algorithmica.org/hpc/algorithms/argmin/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://en.algorithmica.org/hpc/algorithms/argmin/](https://en.algorithmica.org/hpc/algorithms/argmin/)
- en: 'Computing the *minimum* of an array is [easily vectorizable](/hpc/simd/reduction),
    as it is not different from any other reduction: in AVX2, you just need to use
    a convenient `_mm256_min_epi32` intrinsic as the inner operation. It computes
    the minimum of two 8-element vectors in one cycle — even faster than in the scalar
    case, which requires at least a comparison and a conditional move.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 计算数组的*最小值*是[易于向量化](/hpc/simd/reduction)，因为它与其他任何归约操作没有区别：在AVX2中，你只需要使用一个方便的`_mm256_min_epi32`内建函数作为内部操作。它在一个周期内计算两个8元素向量的最小值——甚至比标量情况更快，标量情况至少需要一个比较和一个条件跳转。
- en: Finding the *index* of that minimum element (*argmin*) is much harder, but it
    is still possible to vectorize very efficiently. In this section, we design an
    algorithm that computes the argmin (almost) at the speed of computing the minimum
    and ~15x faster than the naive scalar approach.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 找到最小元素(*argmin*)的*索引*要困难得多，但仍然可以非常高效地将其向量化。在本节中，我们设计了一个算法，该算法以计算最小值的速度（几乎）计算argmin，比原始标量方法快15倍左右。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#scalar-baseline)Scalar
    Baseline'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#scalar-baseline)标量基线'
- en: 'For our benchmark, we create an array of random 32-bit integers, and then repeatedly
    try to find the index of the minimum among them (the first one if it isn’t unique):'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的基准测试，我们创建一个随机的32位整数数组，然后反复尝试找到其中最小值的索引（如果不是唯一的，则是第一个）：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For the sake of exposition, we assume that $N$ is a power of two, and run all
    our experiments for $N=2^{13}$ so that the [memory bandwidth](/hpc/cpu-cache/bandwidth)
    is not a concern.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们假设$N$是2的幂，并且对所有实验进行$N=2^{13}$，这样[内存带宽](/hpc/cpu-cache/bandwidth)就不是问题。
- en: 'To implement argmin in the scalar case, we just need to maintain the index
    instead of the minimum value:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在标量情况下实现argmin，我们只需要维护索引而不是最小值：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It works at around 1.5 GFLOPS — meaning $1.5 \cdot 10^9$ values per second processed
    on average, or about 0.75 values per cycle (the CPU is clocked at 2GHz).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作效率大约在1.5 GFLOPS——这意味着平均每秒处理$1.5 \times 10^9$个值，或者每个周期大约处理0.75个值（CPU的时钟频率为2GHz）。
- en: 'Let’s compare it to `std::min_element`:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其与`std::min_element`进行比较：
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The version from GCC gives ~0.28 GFLOPS — apparently, the compiler couldn’t
    pierce through all the abstractions. Another reminder to never use STL.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GCC提供的版本大约是0.28 GFLOPS——显然，编译器无法穿透所有抽象。这是永远不要使用STL的另一个提醒。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#vector-of-indices)Vector
    of Indices'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#vector-of-indices)索引向量'
- en: The problem with vectorizing the scalar implementation is that there is a dependency
    between consequent iterations. When we optimized [array sum](/hpc/simd/reduction),
    we faced the same problem, and we solved it by splitting the array into 8 slices,
    each representing a subset of its indices with the same remainder modulo 8\. We
    can apply the same trick here, except that we also have to take array indices
    into account.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化标量实现的问题在于后续迭代之间存在依赖关系。当我们优化[数组求和](/hpc/simd/reduction)时，我们遇到了相同的问题，我们通过将数组分成8个切片来解决它，每个切片代表其索引的子集，这些索引具有相同的余数模8。我们也可以在这里应用同样的技巧，只不过我们还要考虑数组索引。
- en: 'When we have the consecutive elements and their indices in vectors, we can
    process them in parallel using [predication](/hpc/pipelining/branchless):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拥有连续元素及其索引的向量时，我们可以使用[predication](/hpc/pipelining/branchless)并行处理它们：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: It works at around 8-8.5 GFLOPS. There is still some inter-dependency between
    the iterations, so we can optimize it by considering more than 8 elements per
    iteration and taking advantage of the [instruction-level parallelism](/hpc/simd/reduction#instruction-level-parallelism).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作效率大约在8-8.5 GFLOPS。迭代之间仍然存在一些依赖关系，因此我们可以通过考虑每个迭代超过8个元素并利用[指令级并行](/hpc/simd/reduction#instruction-level-parallelism)来优化它。
- en: This would help performance a lot, but not enough to match the speed of computing
    the minimum (~24 GFLOPS) because there is another bottleneck. On each iteration,
    we need a load-fused comparison, a load-fused minimum, a blend, and an addition
    — that is 4 instructions in total to process 8 elements. Since the decode width
    of this CPU (Zen 2) is just 4, the performance will still be limited by 8 × 2
    = 16 GFLOPS even if we somehow got rid of all the other bottlenecks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这将大大提高性能，但不足以匹配计算最小值的速度（约24 GFLOPS），因为还有一个瓶颈。在每次迭代中，我们需要一个加载融合的比较、一个加载融合的最小值、一个混合和一个加法——总共4条指令来处理8个元素。由于这个CPU（Zen
    2）的解码宽度仅为4，即使我们设法消除了所有其他瓶颈，性能仍然会受限于8 × 2 = 16 GFLOPS。
- en: Instead, we will switch to another approach that requires fewer instructions
    per element.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将切换到另一种需要每个元素更少指令的方法。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#branches-arent-scary)Branches
    Aren’t Scary'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#branches-arent-scary)分支并不可怕'
- en: When we run the scalar version, how often do we update the minimum?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行标量版本时，我们多久更新一次最小值？
- en: 'Intuition tells us that, if all the values are drawn independently at random,
    then the event when the next element is less than all the previous ones shouldn’t
    be frequent. More precisely, it equals the reciprocal of the number of processed
    elements. Therefore, the expected number of times the `a[i] < a[k]` condition
    is satisfied equals the sum of the harmonic series:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉告诉我们，如果所有值都是独立随机抽取的，那么下一个元素小于所有前一个元素的事件不应该很频繁。更精确地说，它等于处理元素数量的倒数。因此，满足`a[i]
    < a[k]`条件的预期次数等于调和级数的和：
- en: $$ \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \ldots + \frac{1}{n} = O(\ln(n))
    $$
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: $$ \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \ldots + \frac{1}{n} = O(\ln(n))
    $$
- en: So the minimum is updated around 5 times for a hundred-element array, 7 for
    a thousand-element, and just 14 for a million-element array — which isn’t large
    at all when looked at as a fraction of all is-new-minimum checks.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于一百个元素的数组，最小值大约更新5次，对于一千个元素的数组更新7次，而对于一百万个元素的数组则只需更新14次——当作为所有新最小值检查的分数来看时，这并不算多。
- en: 'The compiler probably couldn’t figure it out on its own, so let’s [explicitly
    provide](/hpc/compilation/situational) this information:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器可能无法自己解决这个问题，所以让我们[明确提供](/hpc/compilation/situational)这个信息：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The compiler [optimized the machine code layout](/hpc/architecture/layout),
    and the CPU is now able to execute the loop at around 2 GFLOPS — a slight but
    sizeable improvement from 1.5 GFLOPS of the non-hinted loop.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 编译器[优化了机器代码布局](/hpc/architecture/layout)，现在CPU能够以大约2 GFLOPS的速度执行循环——这比非提示循环的1.5
    GFLOPS略有提高，但幅度不大。
- en: 'Here is the idea: if we are only updating the minimum a dozen or so times during
    the entire computation, we can ditch all the vector-blending and index updating
    and just maintain the minimum and regularly check if it has changed. Inside this
    check, we can use however slow method of updating the argmin we want because it
    will only be called a few times.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是想法：如果我们整个计算过程中只更新最小值大约十几次，我们可以丢弃所有的向量混合和索引更新，只需维护最小值并定期检查它是否已更改。在这个检查中，我们可以使用我们想要的任何慢速方法来更新argmin，因为它只会被调用几次。
- en: 'To implement it with SIMD, all we need to do on each iteration is a vector
    load, a comparison, and a test-if-zero:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用SIMD实现它，我们每个迭代只需要进行一次向量加载、一次比较和一次测试是否为零：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It already performs at ~8.5 GFLOPS, but now the loop is bottlenecked by the
    `testz` instruction which only has a throughput of one. The solution is to load
    two consecutive SIMD blocks and use the minimum of them so that the `testz` effectively
    processes 16 elements in one go:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 它已经以约8.5 GFLOPS的速度运行，但现在循环被`testz`指令所限制，该指令的吞吐量仅为一次。解决方案是加载两个连续的SIMD块并使用它们的最小值，这样`testz`就可以一次有效地处理16个元素：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This version works in ~10 GFLOPS. To remove the other obstacles, we can do
    two things:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个版本以约10 GFLOPS的速度运行。为了消除其他障碍，我们可以做两件事：
- en: Increase the block size to 32 elements to allow for more instruction-level parallelism.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将块大小增加到32个元素，以允许更多的指令级并行性。
- en: 'Optimize the local argmin: instead of calculating its exact location, we can
    just save the index of the block and then come back at the end and find it just
    once. This lets us only compute the minimum on each positive check and broadcast
    it to a vector, which is simpler and much faster.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化局部argmin：我们不需要计算其确切位置，只需保存块的索引，然后在结束时只查找一次。这使得我们只需在每次正检查时计算最小值，并将其广播到向量中，这更简单且更快。
- en: 'With these two optimizations implemented, the performance increases to a whopping
    ~22 GFLOPS:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 实现了这两个优化后，性能增加到惊人的 ~22 GFLOPS：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is almost as high as it can get as just computing the minimum itself works
    at around 24-25 GFLOPS.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎达到了极限，因为仅仅计算最小值本身在大约 24-25 GFLOPS 的速度下工作。
- en: The only problem of all these branch-happy SIMD implementations is that they
    rely on the minimum being updated very infrequently. This is true for random input
    distributions, but not in the worst case. If we fill the array with a sequence
    of decreasing numbers, the performance of the last implementation drops to about
    2.7 GFLOPS — almost 10 times as slow (although still faster than the scalar code
    because we only calculate the minimum on each block).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些喜欢分支的 SIMD 实现的唯一问题是它们依赖于最小值很少更新。这在随机输入分布中是正确的，但在最坏的情况下不是。如果我们用一个递减的数字序列填充数组，最后一个实现的性能将下降到大约
    2.7 GFLOPS — 慢了大约 10 倍（尽管仍然比标量代码快，因为我们只计算每个块的最小值）。
- en: 'One way to fix this is to do the same thing that the quicksort-like randomized
    algorithms do: just shuffle the input yourself and iterate over the array in random
    order. This lets you avoid this worst-case penalty, but it is tricky to implement
    due to RNG- and [memory](/hpc/cpu-cache/prefetching)-related issues. There is
    a simpler solution.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是与快速排序类似的随机算法做同样的事情：自己 shuffle 输入并随机顺序遍历数组。这让你避免了这种最坏情况惩罚，但由于 RNG-
    和 [内存](/hpc/cpu-cache/prefetching)-相关的问题，实现起来很棘手。有一个更简单的解决方案。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#find-the-minimum-then-find-the-index)Find
    the Minimum, Then Find the Index'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#find-the-minimum-then-find-the-index)
    找到最小值，然后找到索引'
- en: We know how to [calculate the minimum of an array](/hpc/simd/reduction) fast
    and how to [find an element in an array](/hpc/simd/masking#searching) fast — so
    why don’t we just separately compute the minimum and then find it?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 how to [calculate the minimum of an array](/hpc/simd/reduction) 快速以及 how
    to [find an element in an array](/hpc/simd/masking#searching) 快速 — 那么，我们为什么不在分别计算最小值后再去寻找它呢？
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: If we implement the two subroutines optimally (check the linked articles), the
    performance will be ~18 GFLOPS for random arrays and ~12 GFLOPS for decreasing
    arrays — which makes sense as we are expected to read the array 1.5 and 2 times
    respectively. This isn’t that bad by itself — at least we avoid the 10x worst-case
    performance penalty — but the problem is that this penalized performance also
    translates to larger arrays, when we are bottlenecked by the [memory bandwidth](/hpc/cpu-cache/bandwidth)
    rather than compute.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们最优地实现这两个子程序（检查相关文章），对于随机数组性能将是 ~18 GFLOPS，对于递减数组是 ~12 GFLOPS — 这是有道理的，因为我们预计要分别读取数组
    1.5 和 2 次。这本身并不糟糕 — 至少我们避免了 10 倍的最坏情况性能惩罚 — 但问题是，这种受惩罚的性能也转化为更大的数组，当我们受限于 [内存带宽](/hpc/cpu-cache/bandwidth)
    而不是计算时。
- en: Luckily, we already know how to fix it. We can split the array into blocks of
    fixed size $B$ and compute the minima on these blocks while also maintaining the
    global minimum. When the minimum on a new block is lower than the global minimum,
    we update it and also remember the block number of where the global minimum currently
    is. After we’ve processed the entire array, we just return to that block and scan
    through its $B$ elements to find the argmin.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们 already know how to fix it. 我们可以 split the array into blocks of fixed
    size $B$ 并在这些块上计算最小值，同时维护全局最小值。当新块上的最小值低于全局最小值时，我们更新它，并记住全局最小值当前所在的块号。处理完整个数组后，我们只需回到那个块，扫描其
    $B$ 个元素以找到 argmin。
- en: 'This way we only process $(N + B)$ elements and don’t have to sacrifice neither
    ½ nor ⅓ of the performance:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们只处理 $(N + B)$ 个元素，而且不必牺牲 ½ 或 ⅓ 的性能：
- en: '[PRE9]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This results for the final implementation are ~22 and ~19 GFLOPS for random
    and decreasing arrays respectively.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了最终实现的结果，对于随机数组和递减数组分别是 ~22 和 ~19 GFLOPS。
- en: The full implementation, including both `min()` and `find()`, is about 100 lines
    long. [Take a look](https://github.com/sslotin/amh-code/blob/main/argmin/combined.cc)
    if you want, although it is still far from being production-grade.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的实现，包括 `min()` 和 `find()`，大约有 100 行代码。 [查看这里](https://github.com/sslotin/amh-code/blob/main/argmin/combined.cc)，尽管它还远未达到生产级别。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#summary)Summary'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#summary) 总结'
- en: 'Here are the results combined for all implementations:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'Here are the results combined for all implementations:'
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Take these results with a grain of salt: the measurements are [quite noisy](/hpc/profiling/noise),
    they were done for just for two input distributions, for a specific array size
    ($N=2^{13}$, the size of the L1 cache), for a specific architecture (Zen 2), and
    for a specific and slightly outdated compiler (GCC 9.3) — the compiler optimizations
    were also very fragile to little changes in the benchmarking code.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对这些结果持保留态度：测量结果[相当嘈杂](/hpc/profiling/noise)，它们仅针对两种输入分布、特定数组大小（$N=2^{13}$，L1缓存的大小）、特定架构（Zen
    2）和特定且略过时的编译器（GCC 9.3）进行了测试——编译器的优化也非常容易受到基准测试代码中微小变化的影响。
- en: There are also still some minor things to optimize, but the potential improvement
    is less than 10% so I didn’t bother. One day I may pluck up the courage, optimize
    the algorithm to the theoretical limit, handle the non-divisible-by-block-size
    array sizes and non-aligned memory cases, and then re-run the benchmarks properly
    on many architectures, with p-values and such. In case someone does it before
    me, please [ping me back](http://sereja.me/).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有一些小事情可以优化，但潜在的提升不到10%，所以我没有费心去做。有一天我可能会鼓起勇气，将算法优化到理论极限，处理不能被块大小整除的数组大小和非对齐的内存情况，然后在许多架构上正确地重新运行基准测试，包括p值等。如果有人在我之前做了这件事，请[提醒我](http://sereja.me/)。
- en: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#acknowledgements)Acknowledgements'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/algorithms/argmin/#acknowledgements)致谢'
- en: The first, index-based SIMD algorithm was [originally designed](http://0x80.pl/notesen/2018-10-03-simd-index-of-min.html)
    by Wojciech Muła in 2018.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个基于索引的SIMD算法最初是由Wojciech Muła在2018年设计的。[原文链接](http://0x80.pl/notesen/2018-10-03-simd-index-of-min.html)。
- en: Thanks to Zach Wegner for [pointing out](https://twitter.com/zwegner/status/1491520929138151425)
    that the performance of the Muła’s algorithm is improved when implemented manually
    using intrinsics (I originally used the [GCC vector types](/hpc/simd/intrinsics/#gcc-vector-extensions)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢Zach Wegner[指出](https://twitter.com/zwegner/status/1491520929138151425)使用内联函数手动实现Muła算法时性能有所提升（我最初使用了[GCC向量类型](/hpc/simd/intrinsics/#gcc-vector-extensions)）。
- en: After publication, I’ve discovered that [Marshall Lochbaum](https://www.aplwiki.com/wiki/Marshall_Lochbaum),
    the creator of [BQN](https://mlochbaum.github.io/BQN/), designed a [very similar
    algorithm](https://forums.dyalog.com/viewtopic.php?f=13&t=1579&sid=e2cbd69817a17a6e7b1f76c677b1f69e#p6239)
    while he was working on Dyalog APL in 2019\. Pay more attention to the world of
    array programming languages! [← Integer Factorization](https://en.algorithmica.org/hpc/algorithms/factorization/)[Prefix
    Sum with SIMD →](https://en.algorithmica.org/hpc/algorithms/prefix/)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在发表后，我发现[BQN](https://mlochbaum.github.io/BQN/)的创造者[Marshall Lochbaum](https://www.aplwiki.com/wiki/Marshall_Lochbaum)在2019年开发Dyalog
    APL时设计了一个[非常相似的算法](https://forums.dyalog.com/viewtopic.php?f=13&t=1579&sid=e2cbd69817a17a6e7b1f76c677b1f69e#p6239)。请更加关注数组编程语言的世界！[←
    整数分解](https://en.algorithmica.org/hpc/algorithms/factorization/)[使用SIMD进行前缀和 →](https://en.algorithmica.org/hpc/algorithms/prefix/)
