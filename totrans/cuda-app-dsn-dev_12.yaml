- en: Chapter 11\. CUDA for Real ProblemsCUDA gives developers the ability to accelerate
    applications on the GPU by one to three orders of magnitude over conventional
    processors. Knowing how to think and program in CUDA is the necessary first step
    in the process of growing and maturing into an adept CUDA programmer. Look to
    the Internet and technical literature to find projects that provide general solutions
    and design patterns that can be adapted to your projects. CUDA is fast; reinventing
    the wheel is slow. The chapters in this book were designed to teach and provide
    some initial frameworks for future applications. For example, the simple C++ functor
    framework in [Chapter 2](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1)
    implemented a general parallel mapping for pattern recognition and optimization
    that runs 85 times faster on a GPU than on a high-end quad-core processor and
    delivers a 341 times greater increase over single-core performance. [Chapter 10](B9780123884268000100.xhtml#B978-0-12-388426-8.00010-0)
    extended this example to use MPI to run on computational clusters containing hundreds
    of GPUs. The 500 GPU TACC Longhorn cluster is one system that can provide nearly
    half a petaflop (500,000 gigaflops) of single-precision floating-point performance
    using this parallel mapping. These examples turn your GPU into an engine to find
    patterns in complex data that is more powerful than the largest supercomputer
    that existed prior to 1996\. Data visualization is a natural extension of the
    computational capabilities of GPUs. As shown in [Chapter 9](B9780123884268000094.xhtml#B978-0-12-388426-8.00009-4),
    mixing CUDA with OpenGL interoperability means that the data does not even need
    to move off the GPU. That said, online information and technical literature extends
    far beyond these examples and the pages of this book. This chapter points the
    way to other projects for advanced usage and programming.**Keywords**MDS (Multi
    Dimensional Scaling), Mutual Information, Force directed graph layout, phylogenetic
    trees, data reduction. Curse of dimensionality, Support Vector Machine (SVM),
    Monte Carlo.CUDA gives developers the ability to accelerate applications on the
    GPU by one to three orders of magnitude over conventional processors. Knowing
    how to think and program in CUDA is the necessary first step in the process of
    growing and maturing into an adept CUDA programmer. Look to the Internet and technical
    literature to find projects that provide general solutions and design patterns
    that can be adapted to your projects. CUDA is fast; reinventing the wheel is slow.
    The chapters in this book were designed to teach and provide some initial frameworks
    for future applications. For example, the simple C++ functor framework in [Chapter
    2](B9780123884268000021.xhtml#B978-0-12-388426-8.00002-1) implemented a general
    parallel mapping for pattern recognition and optimization that runs 85 times faster
    on a GPU than on a high-end quad-core processor and delivers a 341 times greater
    increase over single-core performance. [Chapter 10](B9780123884268000100.xhtml#B978-0-12-388426-8.00010-0)
    extended this example to use MPI to run on computational clusters containing hundreds
    of GPUs. The 500 GPU TACC Longhorn cluster is one system that can provide nearly
    half a petaflop (500,000 gigaflops) of single-precision floating-point performance
    using this parallel mapping. These examples turn your GPU into an engine to find
    patterns in complex data that is more powerful than the largest supercomputer
    that existed prior to 1996\. [¹](#fn0010) Data visualization is a natural extension
    of the computational capabilities of GPUs. As shown in [Chapter 9](B9780123884268000094.xhtml#B978-0-12-388426-8.00009-4),
    mixing CUDA with OpenGL interoperability means that the data does not even need
    to move off the GPU. That said, online information and technical literature extends
    far beyond these examples and the pages of this book. This chapter points the
    way to other projects for advanced usage and programming.¹The United States Sandia
    National Laboratory built the first supercomputer that was able to perform more
    than a trillion floating-point operations in 1996.Do not consider this chapter
    to be an exhaustive list. Instead, it provides a broad introduction to some of
    the popular techniques and packages that use CUDA. Most of the methods discussed
    will provide a link to a downloadable CUDA source package that can be reviewed,
    built, and used. There are many other excellent resources available on the Internet
    and in technical literature. A good starting point is the CUDA showcase in the
    developer zone that contains hundreds of links to CUDA techniques and papers.
    [²](#fn9000)²[http://developer.nvidia.com/cuda-action-research-apps](http://developer.nvidia.com/cuda-action-research-apps).At
    the end of this chapter, the reader will have been introduced to:■ A few techniques
    to reduce high-dimensional data to low dimensions.■ Force-directed graphs for
    visualization.■ Multidimensional scaling (MDS).■ Mutual information.■ Monte Carlo
    methods.■ Molecular modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Working with High-Dimensional Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We live in the age of data. Automatic systems collect most minute details of
    transactions and interactions in both physical and social systems. The patterns
    are there, but where?With a simple database query or Internet search, it is possible
    to extract an interesting data set that concatenates a number of different measures
    (or observations) for each event. The events might be customer purchases or the
    spectral data of hundreds of stars. Multiple events can be represented as the
    rows of a matrix in which each row is an event and the columns of each row contain
    measurements concerning the event. A common question to ask is, “Which rows are
    similar to each other?” Phrased another way, the question becomes, “Which events,
    or vectors, are close to each other?”For many problems, it is possible to calculate
    the distance between each row in the matrix using a metric such as *Euclidean
    distance*. Euclidean distance is a measure of distance that one would obtain with
    a ruler and the Pythagorean formula. Those rows that are close in distance can
    be considered “similar” to each other and those that are far away are “not similar.”
    There are many other distance measures aside from Euclidean distance, but Euclidean
    distance is intuitively easy to understand.The curse of dimensionality discussed
    in [Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3) is why working
    in a high-dimensional space is not practical for many problems. For example, asking
    for all rows that are closer than some distance from a given point in high-dimensional
    space can return either no values (because the sampling is very sparse) or nearly
    all the data (because the volume of the search region is so large that it encompasses
    most or all of the known data). In either case, the answer is meaningless.Decreasing
    the dimensionality of a search exponentially decreases the volume of the search
    region. This is one of the reasons why data miners like working in lower-dimensional
    spaces—it increases the chance of getting a meaningful answer to the question,
    “Which points in the low-dimensional space are close to each other?” Of course,
    the projection used to convert the high-dimensional data to the low-dimensional
    space must preserve the distance relationships between the points.
  prefs: []
  type: TYPE_NORMAL
- en: PCA/NLPCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in [Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3),
    autoencoders, Principle Components Analysis (PCA), and Nonlinear Principle Components
    Analysis (NLPCA) are some of the techniques used to represent high-dimensional
    data in a lower-dimensional space. The example code in [Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)
    essentially passed high-dimensional data through an information bottleneck (the
    bottleneck neurons) in such a way that the high-dimensional data can be reconstructed.
    Measuring the error between the original and reconstructed high-dimensional data
    provides a measure of success. A low error implies that the projection to low
    dimensions preserved the distance relationships between the points in the high-dimensional
    data.The example code in [Chapter 3](B9780123884268000033.xhtml#B978-0-12-388426-8.00003-3)
    can be adapted to work with your own data sets simply by modifying the **genData()**
    method. Similarly, the data generator in [Chapter 10](B9780123884268000100.xhtml#B978-0-12-388426-8.00010-0)
    can be changed to one that writes your own data to a file. In either case, modifying
    the *CalcError.h* file will let you define your own PCA or NLPCA architecture.
    Using one, two, or three bottleneck neurons means that the low-dimensional data
    can be visualized with most 2D and 3D graphics packages such as gnuPlot, Excel,
    MATLAB, and many others.For more information on autoencoders and other neural
    network based approaches, see the work of Geoffrey Hinton ([Hinton & Salakhutdinov,
    2006](B978012388426800015X.xhtml#ref63)) [³](#fn0015) and many others. The website
    [http://nlpca.org](http://nlpca.org) is a good starting point to learn about NLPCA,
    especially for MATLAB users.³[http://www.cs.toronto.edu/~hinton/](http://www.cs.toronto.edu/~hinton).
  prefs: []
  type: TYPE_NORMAL
- en: Multidimensional Scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative perspective on dimensionality reduction is offered by multidimensional
    scaling (MDS). MDS is another classical approach that maps a high-dimensional
    data set to a lower dimensional space, but does so in an attempt to preserve pairwise
    distances. Numerous variants of the classical method have been developed. An excellent
    reference is *Multidimensional Scaling* ([Cox & Cox, 2008](B978012388426800015X.xhtml#ref25)).The
    Glimmer package implements MDS on the GPU ([Ingram, Munzner, & Olano, 2009](B978012388426800015X.xhtml#ref76)).
    Variable speedups depending on data size range from 10–15 times over a conventional
    processor. The source code can be freely downloaded from the University of British
    Columbia website. [⁴](#fn0020)⁴[http://www.cs.ubc.ca/~sfingram/glimmer/](http://www.cs.ubc.ca/~sfingram/glimmer).
  prefs: []
  type: TYPE_NORMAL
- en: K-Means Clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: K-means clustering partitions a number of events into k-clusters in which each
    event belongs to the cluster with the nearest mean. Data sets with a billion points
    are common in today's real-world applications. With CUDA GPU acceleration, Hewlett-Packard
    reports that a data set with a billion data points can be clustered within minutes
    ([Wu, Zhang, & Hsu, 2009](B978012388426800015X.xhtml#ref144)). The HP software
    claims an order of magnitude increased performance over a highly optimized CPU-only
    version running on eight cores, and about 300 times performance boost over the
    popular MineBench benchmark ([Narayanan et al., 2006](B978012388426800015X.xhtml#ref99))
    running on a single core. Many other researchers ([Hong-tao, Li-li, Dan-tong,
    Zhan-shan, & He, 2009](B978012388426800015X.xhtml#ref65); [Ma & Agrawal, 2009](B978012388426800015X.xhtml#ref88);
    [Shalom, Dash, & Tue, 2008](B978012388426800015X.xhtml#ref118)) report large speedups
    on K-means.A freely downloadable version of K-means written by Serban Giuroiu
    is available from GitHub. [⁵](#fn0025) It is reportedly 50 times faster than a
    sequential version.⁵[https://github.com/serban/kmeans](https://github.com/serban/kmeans).
  prefs: []
  type: TYPE_NORMAL
- en: Expectation-Maximization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: K-means and MDS are similar to the expectation-maximization (EM) algorithm for
    mixtures of Gaussians in that they both attempt to find the centers of natural
    clusters in the data as well as in the iterative refinement approach employed
    by both algorithms. The EM algorithm is widely used in the fields of signal processing
    and data mining. A freely downloadable version of the EM method with a reported
    170-times increase over a reference CPU implementation is available. [⁶](#fn0030)
    NVIDIA published a paper with a similar reported speedup over a naïve implementation
    ([Kumar, Satoor, & Buck, 2009](B978012388426800015X.xhtml#ref83)). The EM method
    can also be implemented as a functor in the optimization framework provided in
    this book. The CUDA-MEME package discussed later in this chapter is another freely
    downloadable EM package.⁶[http://andrewharp.com/gmmcuda](http://andrewharp.com/gmmcuda).The
    expectation-maximization algorithm is also used to discover motifs in social network
    and biological data. GPU-MEME is reported to provide an order of magnitude speedup
    on a single GPU and two orders of magnitude speedup on a compute cluster ([Chen,
    Schmidt, Weiguo, & Müller-Wittig, 2008](B978012388426800015X.xhtml#ref17)). CUDA-MEME
    is a freely downloadable motif discovery software package[⁷](#fn0035) based on
    the GPU-MEME paper.⁷[http://www.nvidia.com/object/meme_on_tesla.html](http://www.nvidia.com/object/meme_on_tesla.html).
  prefs: []
  type: TYPE_NORMAL
- en: Support Vector Machines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Support vector machines (SVM) are a popular machine learning method to analyze
    data and recognize patterns. An SVM performs classification by constructing an
    *N*-dimensional *hyperplane* (a plane generalized into *N* dimensions) that optimally
    separates the data into two categories. They are used for classification and regression
    analysis, among other tasks. SVM models are closely related to neural networks.
    In fact, an SVM model using a sigmoid kernel function is equivalent to a two-layer
    perceptron neural network. Speedups of 150 times on a GPU have been reported in
    the literature ([Catanzaro, Sundaram, & Keutzer, 2008](B978012388426800015X.xhtml#ref14)).SVM
    can also be implemented as a functor in the optimization framework provided in
    this book. The cuSVM package can be freely downloaded from the Internet. [⁸](#fn0040)
    This same package is accessible from the R statistical language via the **gpuSvnTrain()**
    method in the gputools package. Speedups between 22 and 172 times the rate of
    state-of-the-art software are reported by the author ([Carpenter, 2011](B978012388426800015X.xhtml#ref13)).
    The CUDA-SVM package from Nanyang Technological University is available on [http://sourceforge.net](http://sourceforge.net).
    Another SVM package, multisvm, can be freely downloaded from [http://code.google.com](http://code.google.com).⁸[http://patternsonascreen.net/cuSVM.html](http://patternsonascreen.net/cuSVM.html).
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Bayesian belief net is a directed graph, together with an associated set of
    probability tables. The nodes represent variables, which can be discrete or continuous.
    The arcs in the graph represent causal/influential relationships between variables.
    The key feature of Bayesian networks is that they can be used to reason about
    uncertainty. When used in conjunction with statistical techniques, these graphical
    models have several advantages for data analysis:■ Because the model encodes dependencies
    among all variables, it can handle missing data.■ A Bayesian network can be used
    to learn causal relationships, and hence can be used to gain an understanding
    about a problem domain and to predict the consequences of intervention.■ A Bayesian
    model has both a causal and probabilistic semantics, which makes it a natural
    representation to combine prior knowledge (which often comes in causal form) and
    data.■ Bayesian statistical methods in conjunction with Bayesian networks offer
    an efficient and principled approach for avoiding the overfitting of data.Duke
    University provides a freely available Bayesian software that can also be accessed
    via MATLAB and the R statistical language. [⁹](#fn0045) Speedups of 160 times
    over a conventional multicore processor are reported ([Suchard et al., 2010](B978012388426800015X.xhtml#ref128)).
    CUDA allows large data sets to be analyzed with Bayesian techniques ([Chen, Schmidt,
    Weiguo, & Müller-Wittig, 2008](B978012388426800015X.xhtml#ref17)).⁹[http://www.stat.duke.edu/research/software/west/gpu/software.html](http://www.stat.duke.edu/research/software/west/gpu/software.html).BEAGLE
    (Broad-platform Evolutionary Analysis General Likelihood Evaluator) is a high-performance
    library that can perform the core calculations at the heart of most Bayesian and
    Maximum Likelihood phylogenetics packages. Phylogenetics is the study of evolutionary
    relatedness among various groups, species, or populations of organisms that is
    discovered through molecular sequencing data and morphological data matrices.
    The authors achieved a 90-times speedup over an optimized CPU version and a 140-times
    speedup over the general CPU version ([Suchard & Rambaut, 2009](B978012388426800015X.xhtml#ref129)).
    [^(10)](#fn9005) This library is used in the Bayesian phylogenetics framework
    (BEAST). [^(11)](#fn9010)^(10)[http://beagle-lib.googlecode.com](http://beagle-lib.googlecode.com).^(11)[http://beast.bio.ed.ac.uk](http://beast.bio.ed.ac.uk).
  prefs: []
  type: TYPE_NORMAL
- en: Mutual information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of mutual information has its origins in information theory and
    is widely used in many disciplines. For example, mutual information can be used
    to determine sites in the AIDS virus that are related for structural or functional
    reasons ([Korber, Farber, Wolpert, & Lapedes, 1993](B978012388426800015X.xhtml#ref81)).
    In particular, the paper by Korber et al. demonstrates how *bootstrapping* can
    be used to determine a confidence that high pair-wise mutual information did not
    arise by chance. (Bootstrapping in statistics can be implemented by various techniques,
    including by random sampling with replacement from the original data set.) Mutual
    information can also be used to improve prediction in machine-learning ([Farber,
    Lapedes, & Sirotkin, 1992](B978012388426800015X.xhtml#ref48)). There are numerous
    texts on this topic ([Cover & Thomas, 2006](B978012388426800015X.xhtml#ref24);
    [Stanisław, Carbonell, & Mitchell, 1985](B978012388426800015X.xhtml#ref120)) and
    many sources of information on the Internet.Pairwise mutual information provides
    a measure of the amount of information between two random variables. Generally,
    this measure is expressed in number of bits of information. Mutual information
    is defined in terms of entropies involving the joint probability distribution,
    ![B9780123884268000112/si1.gif is missing](B9780123884268000112/si1.gif), of occurrence
    symbol *s* at position and *s′* at position *j*. It can be expressed as the relation
    to the log-likelihood ratio of the expected occurrence of pairs (under the assumption
    of independence) to the observed occurrence. See [Equation 11.1](#fm0010), “A
    definition of mutual information.”(11.1)![B9780123884268000112/si2.gif is missing](B9780123884268000112/si2.gif)In
    medical image analysis, mutual information is used as a similarity measure for
    multimodal registration. CUDA provides a 50-times speedup, allowing real-time
    registration of medical images and in robotics ([Ines & Hirschmüller, 2008](B978012388426800015X.xhtml#ref75)).There
    are many freely downloadable implementations on the Internet. CUDA-MI[^(12)](#fn0050)
    is one; another is the version by Shams at the Australian National University.
    [^(13)](#fn0055)^(12)[https://sites.google.com/site/liuweiguohome/cuda-mi](https://sites.google.com/site/liuweiguohome/cuda-mi).^(13)[http://users.cecs.anu.edu.au/~ramtin/cuda.htm](http://users.cecs.anu.edu.au/~ramtin/cuda.htm).
  prefs: []
  type: TYPE_NORMAL
- en: Force-Directed Graphs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many graph based problems only provide information about the relationships between
    nodes in a graph. Such problems arise when people send messages to each other,
    talk on the phone, and travel to various locations. Visualizing mass amounts of
    such information can be challenging. Excellent surveys on drawing graphs can be
    found in [Di Battista, Eades, Tamassia, and Tollis (1999)](B978012388426800015X.xhtml#ref30)
    and [Brandes (2001)](B978012388426800015X.xhtml#ref11).In the absence of other
    information, force-directed placement algorithms for graph layout based on Hooke's
    law for springs have been used to create pleasing and informative graph layouts
    as described by Eades' spring-mass equations ([Eades, 1984](B978012388426800015X.xhtml#ref35))
    and later adapted by Fruchterman and Reingold to emulate particle physics in a
    simulated annealing algorithm ([Fruchterman & Reingold, 1991](B978012388426800015X.xhtml#ref52)).Graph
    placement and layout remains an active area of research, with Frishman and Ayellet
    ([Frishman & Ayellet, 2008](B978012388426800015X.xhtml#ref51)) using GPUs to speed-up
    incremental graph layout to provide results 17 times faster than a CPU. Godiyal
    et al. ([Godiyal, Hoberock, Garland, & Hart, 2009](B978012388426800015X.xhtml#ref54))
    use a variation of the Fast Multipole Method (FMM) to estimate the long-distance
    repulsive forces in force-directed layout. (The FMM method was developed to speed
    the calculation of long-range forces in the *N*-body problem.) Multipole computations
    are efficiently supported with a GPU-based k-d tree (a space-partitioning data
    structure for organizing points in a k-dimensional space). The authors report
    their technique achieves impressive speedup over previous CPU and GPU methods,
    drawing graphs with hundreds of thousands of vertices within a few seconds via
    CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Monte Carlo methods rely on repeated random sampling to compute their results.
    They are often used in simulating physical and mathematical systems and are especially
    useful for simulating systems with many coupled degrees of freedom (such as fluids)
    to model phenomena as well as systems with significant uncertainty in inputs such
    as the calculation of risk in business. Hubbard notes that Monte Carlo predictions
    of failures, cost overruns, and schedule overruns are routinely better than human
    intuition or alternative “soft” methods ([Hubbard, 2009](B978012388426800015X.xhtml#ref70)).Monte
    Carlo methods are widely used in mathematics to evaluate multidimensional definite
    integrals with complicated boundary conditions. For example, the Metropolis algorithm
    is a Monte Carlo method for obtaining a sequence of random samples from a probability
    distribution for which direct sampling is difficult. This sequence can be used
    to approximate the distribution or to compute an integral to get an expected value.I
    had the personal pleasure of knowing Nick Metropolis during my career in the theoretical
    division at Los Alamos National Laboratory, which demonstrates how knowledge of
    parallel computing can indirectly enrich your life. He was a wonderful person.
    The Metropolis algorithm is considered to be among the ten algorithms that have
    had the greatest influence on the development and practice of science and engineering
    in the twentieth century ([Andrieu, de Freitas, Doucet, & Jordan, 2003](B978012388426800015X.xhtml#ref4);
    [Beichel & Sullivan, 2000](B978012388426800015X.xhtml#ref5)). For some applications,
    Markov chain Monte Carlo (MCMC) simulation is the only known general approach
    for providing a solution within a reasonable time ([Andrieu, de Freitas, Doucet,
    & Jordan, 2003](B978012388426800015X.xhtml#ref4); [Dyer, Frieze, & Kannan, 1991](B978012388426800015X.xhtml#ref34);
    [Jerrum & Sinclair, 1996](B978012388426800015X.xhtml#ref77)).The large number
    of CUDA projects implementing Monte Carlo methods demonstrates the power of CUDA
    and interest in this technique. Speedups range from 100 times to over 1,000 times
    greater on multi-GPU implementations. A good starting reference is “Understanding
    GPU Programming for Statistical Computation: Studies in Massively Parallel Massive
    Mixtures” ([Suchard, Wang, Chan, Frelinger, Cron, & West, 2010](B978012388426800015X.xhtml#ref128)).A
    number of Monte Carlo packages are available for free download:■ The MCX (Monte
    Carlo eXtreme) package quotes 300- to 400-times speedups ([Fang & Boas, 2009](B978012388426800015X.xhtml#ref39)).
    The code can be freely downloaded from [http://mcx.sourceforge.net](http://mcx.sourceforge.net).■
    GPUSS (GPU Stochastic Simulation for data analysis) is freely available from Oxford
    University. The authors report a speedup of 500 times on the NVIDIA Community
    Showcase. [^(14)](#fn0060) This package is accessible from PyCUDA for statistical
    GPU computing. Please see the Oxford website for an extensive list of publications
    that use this package.^(14)[http://developer.nvidia.com/cuda-action-research-apps](http://developer.nvidia.com/cuda-action-research-apps).'
  prefs: []
  type: TYPE_NORMAL
- en: Molecular Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Molecular modeling has also achieved significant performance benefits using
    CUDA ([Eastman & Pande, 2010](B978012388426800015X.xhtml#ref36); [Hampton, Agarwal,
    Alam, & Crozier, 2010](B978012388426800015X.xhtml#ref57); [Roberts, Stone, Sepulveda,
    Hwu, & Luthey-Schulten, 2009](B978012388426800015X.xhtml#ref107); [Rodrigues,
    Hardy, Stone, Schulten, & Hwu, 2008](B978012388426800015X.xhtml#ref108)), including
    packages such as:■ The NAMD/VMD molecular modeling system ([Humphrey, Dalke, &
    Schulten, 1996](B978012388426800015X.xhtml#ref71); [Laxmikant et al., 1999](B978012388426800015X.xhtml#ref86);
    [Stone, Hardy, Ufimtsev, & Schulten, 2010](B978012388426800015X.xhtml#ref123)).■
    HOOMD-Blue ([Anderson, Lorenz, & Travesset, 2008](B978012388426800015X.xhtml#ref3))
    was written from the ground up for GPUs.■ The OpenMM library for molecular dynamics.
    The authors report speedups of 100 times over commodity processors.All of these
    packages are freely downloadable from the Internet. The paper “GPU-Accelerated
    Molecular Modeling Coming of Age” ([Stone, Hardy, Ufimtsev, & Schulten, 2010](B978012388426800015X.xhtml#ref123))
    is required reading for those who are interested in molecular modeling. CUDA and
    GPU computing have also stimulated the development of new approximations for electrostatics
    that can provide up to three orders of magnitude speedup ([Anandakrishnan et al.,
    2010](B978012388426800015X.xhtml#ref1)). Over the past few years, molecular modeling
    has been a vibrant and growing area of CUDA research.
  prefs: []
  type: TYPE_NORMAL
- en: Quantum Chemistry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quantum chemistry applies quantum mechanics to explain and predict the behavior
    of chemical processes. Understanding the electronic structure of matter is an
    important question in materials design and the creation of more efficient batteries,
    membranes, solar panels, and a multitude of common materials. Many approaches
    use approximate solutions to the Schrödinger equation for determining the energy
    levels of molecules and the properties such as conductance, charge distribution,
    and reactivity. Other quantum chemical results include molecular geometry, the
    strengths and other characteristics of chemical bonds, optical and other spectra,
    intermolecular forces, and many other chemical properties and features of chemical
    behavior.Quantum ESPRESSO is a freely downloadable integrated suite of computer
    codes for electronic-structure calculations and materials modeling at the nano
    scale. It is based on density-functional theory, plane waves, and pseudopotentials
    (both norm-conserving and ultrasoft). A good starting article is “Speeding Up
    Plane-Wave Electronic-Structure Calculations Using Graphics-Processing Units”
    ([Maintz, Eck, & Dronskowski, 2011](B978012388426800015X.xhtml#ref90)). ICHEC
    has collaborated with the Quantum ESPRESSO project to create a GPU-based version
    of this code. Current speedups are reported at eight times that of a single-core
    processor. It is expected that this performance will increase as the GPU project
    matures.GPU-enabled quantum chemistry packages such as TeraChem ([Ufimtsev & Martinez,
    2008](B978012388426800015X.xhtml#ref135), [Ufimtsev & Martinez, 2009a](B978012388426800015X.xhtml#ref136)
    and [Ufimtsev & Martinez, 2009b](B978012388426800015X.xhtml#ref137)), now called
    PetaChem, are being deployed at major supercomputing centers such as National
    Center for Supercomputing Applications (NCSA).A number of researchers and organizations
    report excellent speedups on quantum chemistry simulations ([Hwu, 2011](B978012388426800015X.xhtml#ref72))
    including Gaussian and GAMESS ([Ufimtsev & Martinez, 2008](B978012388426800015X.xhtml#ref135)
    and [Ufimtsev & Martinez, 2009](B978012388426800015X.xhtml#ref136)). Pseudospectral
    methods run well on both GPUs and multicore processors. One implementation is
    BigDFT (BigDFT) ([Genovese et al., 2009](B978012388426800015X.xhtml#ref53)).
  prefs: []
  type: TYPE_NORMAL
- en: Interactive Workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPGPUs provide additional performance benefits for dedicated usage, visualization,
    and interactive workflows, as they have been designed to be plugged into computers
    situated next to an instrument or individual's desk. The recent scientific literature
    demonstrates applications of this technology to a number of instruments and projects.
    For example, visualization packages such as Visual Molecular Dynamics (VMD) help
    make molecular modeling more interactive ([Stone, Hardy, Ufimtsev, & Schulten,
    2010](B978012388426800015X.xhtml#ref123)) through the use of haptic feedback ([Stone,
    Kohlmeyer, Vandivort, & Schulten, 2010](B978012388426800015X.xhtml#ref123)). Researchers
    Balanchi and Di Leonardo report a 350-times speedup that allowed them to incorporate
    real-time hologram generation into an optical micro manipulation workflow ([Bianchi
    & Di Leonardo, 2010](B978012388426800015X.xhtml#ref7)).
  prefs: []
  type: TYPE_NORMAL
- en: A Plethora of Projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous examples represent but a few of many research efforts reporting
    significant benefits from the application of GPGPU technology. Bioinformatics
    ([Schmidt, 2010](B978012388426800015X.xhtml#ref112)), systems biology ([Dematte
    & Prandi, 2010](B978012388426800015X.xhtml#ref29)), multicellular biological modeling
    ([Christley, Lee, Dai, & Nie, 2010](B978012388426800015X.xhtml#ref18)), chemical
    and protein search ([Haque, Pande, & Walters, 2010](B978012388426800015X.xhtml#ref58);
    [Stivala, Stuckey, & Wirth, 2010](B978012388426800015X.xhtml#ref122)) are but
    a few additional broad areas. Use of a good Internet search engine can help identify
    research projects specific to your interests. Many projects make their software
    freely available.As discussed in [Chapter 8](B9780123884268000082.xhtml#B978-0-12-388426-8.00008-2),
    several general-purpose libraries to facilitate scientific computation such as
    NVIDIA's CUBLAS and CUFFT libraries along with the MAGMA (Matrix Algebra on GPU
    and Multicore Architectures) hybrid CPU/GPU library are also included in the SDK
    or available for download for free.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The wealth of CUDA-based applications that have and are being developed makes
    this chapter incomplete. The intention is to provide links to some (and certainly
    not all!) important concepts and projects. Most of the referenced projects have
    software that can be freely downloaded, built, and used. Learning the basics of
    CUDA can be done quickly. The process of maturation takes much longer and involves
    reading papers, examining source code, and talking with your peers.The focus in
    this chapter has been on projects that can be freely downloaded from the Internet.
    Numerous for-sale projects are also out there. For example, AMBER is a commercial
    molecular dynamics application that can run on GPUs. [^(15)](#fn0065) Commercial
    drug discovery software also runs on GPUs, such as the OpenEye scientific software.
    [^(16)](#fn0070) Along with learning CUDA, such products show that work can be
    had for commercial CUDA/GPU development as well as opportunities to start your
    own technology company.^(15)[http://ambermd.org/gpus/](http://ambermd.org/gpus).^(16)[http://eyesopen.com](http://eyesopen.com).
  prefs: []
  type: TYPE_NORMAL
