<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>5.3. Variational characterization of eigenvalues#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>5.3. Variational characterization of eigenvalues#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap05_specgraph/03_extremal/roch-mmids-specgraph-extremal.html">https://mmids-textbook.github.io/chap05_specgraph/03_extremal/roch-mmids-specgraph-extremal.html</a></blockquote>

<p>In this section, we characterize eigenvalues in terms of certain optimization problems, an idea we have already encountered in the closely related context of the SVD. Such variational characterizations are very useful in applications, as we will see later in the chapter. We first give a proof of the spectral theorem where this idea is used explicitly.</p>
<section id="proof-of-spectral-theorem">
<h2><span class="section-number">5.3.1. </span>Proof of Spectral Theorem<a class="headerlink" href="#proof-of-spectral-theorem" title="Link to this heading">#</a></h2>
<p>We will need the following block matrix formula. Suppose</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A
=
\begin{pmatrix}
A_{11} \\ A_{21}
\end{pmatrix}
\quad
\text{and}
\quad
B
=
\begin{pmatrix}
B_{11} &amp; B_{12}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(A_{11} \in \mathbb{R}^{n_1\times p}\)</span>, <span class="math notranslate nohighlight">\(A_{21} \in \mathbb{R}^{n_2\times p}\)</span>, <span class="math notranslate nohighlight">\(B_{11} \in \mathbb{R}^{p\times n_1}\)</span>, <span class="math notranslate nohighlight">\(B_{12} \in \mathbb{R}^{p\times n_2}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A B
=
\begin{pmatrix}
A_{11} B_{11} &amp; A_{11} B_{12} \\ 
A_{21} B_{11} &amp; A_{21} B_{12}
\end{pmatrix}.
\end{split}\]</div>
<p>Indeed this is just a special case of the <span class="math notranslate nohighlight">\(2 \times 2\)</span> block matrix product formula we have encountered previously, with empty blocks <span class="math notranslate nohighlight">\(A_{12}, A_{22}, B_{21}, B_{22}\)</span>.</p>
<p><em>Proof idea (Spectral Theorem):</em> Similarly to how we used Householder transformations to “add zeros under the diagonal”, here we will use a sequence of orthogonal transformations to add zeros both below and above the diagonal. Recall that two matrices <span class="math notranslate nohighlight">\(C, D \in \mathbb{R}^{d \times d}\)</span> are <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_similarity">similar</a> if there is an invertible matrix <span class="math notranslate nohighlight">\(P\)</span> such that <span class="math notranslate nohighlight">\(C = P^{-1} D P\)</span>, and that, when <span class="math notranslate nohighlight">\(P = W\)</span> is an orthogonal matrix, the transformation simplifies to <span class="math notranslate nohighlight">\(C = W^T D W\)</span>.</p>
<p>We construct a sequence of orthogonal matrices <span class="math notranslate nohighlight">\(\hat{W}_1,\ldots, \hat{W}_d\)</span> and progressively compute <span class="math notranslate nohighlight">\(W_1^T A W_1\)</span>, <span class="math notranslate nohighlight">\(W_2^T W_1^T A W_1 W_2\)</span>, and so on in such a way that</p>
<div class="math notranslate nohighlight">
\[
\Lambda = W_d^T \cdots W_2^T W_1^T A W_1 W_2 \cdots W_d
\]</div>
<p>is diagonal. Then the matrix <span class="math notranslate nohighlight">\(Q\)</span> is simply <span class="math notranslate nohighlight">\(W_1 W_2 \cdots W_d\)</span> (check it!).</p>
<p>To define these matrices, we use a greedy sequence maximizing the quadratic form <span class="math notranslate nohighlight">\(\langle \mathbf{v}, A \mathbf{v}\rangle\)</span>. How is this quadratic form related to eigenvalues? Recall that, for a unit eigenvector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>, we have <span class="math notranslate nohighlight">\(\langle \mathbf{v}, A \mathbf{v}\rangle = \langle \mathbf{v}, \lambda \mathbf{v}\rangle = \lambda\)</span>.</p>
<p><em>Proof:</em> <em>(Spectral Theorem)</em> <span class="math notranslate nohighlight">\(\idx{spectral theorem}\xdi\)</span> We proceed by induction. We have already encountered the idea that eigenvectors are stationary points of an optimization problem. The proof in fact uses that idea.</p>
<p><em><strong>A first eigenvector:</strong></em> Let <span class="math notranslate nohighlight">\(A_1 = A\)</span>. Define</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_1 \in \arg\max\{\langle \mathbf{v}, A_1 \mathbf{v}\rangle:\|\mathbf{v}\| = 1\}
\]</div>
<p>which exists by the <em>Extreme Value Theorem</em>, and further define</p>
<div class="math notranslate nohighlight">
\[
\lambda_1 = \max\{\langle \mathbf{v}, A_1 \mathbf{v}\rangle:\|\mathbf{v}\| = 1\}.
\]</div>
<p>Complete <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> into an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>, <span class="math notranslate nohighlight">\(\hat{\mathbf{v}}_2, \ldots, \hat{\mathbf{v}}_d\)</span>, and form the block matrix</p>
<div class="math notranslate nohighlight">
\[
\hat{W}_1 
=
\begin{pmatrix}
\mathbf{v}_1 &amp; \hat{V}_1
\end{pmatrix}
\]</div>
<p>where the columns of <span class="math notranslate nohighlight">\(\hat{V}_1\)</span> are <span class="math notranslate nohighlight">\(\hat{\mathbf{v}}_2, \ldots, \hat{\mathbf{v}}_d\)</span>. Note that <span class="math notranslate nohighlight">\(\hat{W}_1\)</span> is orthogonal by construction.</p>
<p><em><strong>Getting one step closer to diagonalization:</strong></em> We show next that <span class="math notranslate nohighlight">\(\hat{W}_1\)</span> gets us one step closer to a diagonal matrix by similarity transformation. Note first that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{W}_1^T A_1 \hat{W}_1
&amp;=
\begin{pmatrix}
\mathbf{v}_1^T \\ \hat{V}_1^T
\end{pmatrix}
A_1
\begin{pmatrix}
\mathbf{v}_1 &amp; \hat{V}_1
\end{pmatrix}\\
&amp;=
\begin{pmatrix}
\mathbf{v}_1^T \\ \hat{V}_1^T
\end{pmatrix}
\begin{pmatrix}
A_1 \mathbf{v}_1 &amp; A_1 \hat{V}_1
\end{pmatrix}\\
&amp;=
\begin{pmatrix}
\mathbf{v}_1^T A_1 \mathbf{v}_1 &amp; \mathbf{v}_1^T A_1 \hat{V}_1\\
\hat{V}_1^T A_1 \mathbf{v}_1 &amp; \hat{V}_1^T A_1 \hat{V}_1
\end{pmatrix}\\
&amp;= 
\begin{pmatrix}
\lambda_1 &amp; \mathbf{w}_1^T \\
\mathbf{w}_1 &amp; A_2
\end{pmatrix}
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w}_1 = \hat{V}_1^T A_1 \mathbf{v}_1\)</span> and <span class="math notranslate nohighlight">\(A_2 = \hat{V}_1^T A_1 \hat{V}_1\)</span>.</p>
<p>The key claim is that <span class="math notranslate nohighlight">\(\mathbf{w}_1 = \mathbf{0}\)</span>. This follows from a contradiction argument. Indeed, suppose <span class="math notranslate nohighlight">\(\mathbf{w}_1 \neq \mathbf{0}\)</span> and consider the unit vector (check that <span class="math notranslate nohighlight">\(\|\mathbf{z}\| = 1\)</span>)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{z}
=
\hat{W}_1 
\frac{1}{\sqrt{1 + \delta^2 \|\mathbf{w}_1\|^2}} \begin{pmatrix}
1 \\
\delta \mathbf{w}_1
\end{pmatrix}
\end{split}\]</div>
<p>which achieves the objective value (check it!)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}^T A_1 \mathbf{z}
&amp;=
\frac{1}{1 + \delta^2 \|\mathbf{w}_1\|^2}
\begin{pmatrix}
1 \\
\delta \mathbf{w}_1
\end{pmatrix}^T
\begin{pmatrix}
\lambda_1 &amp; \mathbf{w}_1^T \\
\mathbf{w}_1 &amp; A_2
\end{pmatrix}
\begin{pmatrix}
1 \\
\delta \mathbf{w}_1
\end{pmatrix}\\
&amp;= 
\frac{1}{1 + \delta^2 \|\mathbf{w}_1\|^2}
\left(
\lambda_1
+ 2 \delta \|\mathbf{w}_1\|^2
+ \delta^2 \mathbf{w}_1^T A_2 \mathbf{w}_1
\right).
\end{align*}\]</div>
<p>By the <a class="reference external" href="https://en.wikipedia.org/wiki/Geometric_series">sum of a geometric series</a>, for <span class="math notranslate nohighlight">\(\varepsilon \in (0,1)\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{1 + \varepsilon^2}
= 1 - \varepsilon^2 + \varepsilon^4 + \cdots.
\]</div>
<p>So, for <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> small enough,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}^T A_1 \mathbf{z}
&amp;\approx
(\lambda_1
+ 2 \delta \|\mathbf{w}_1\|^2
+ \delta^2 \mathbf{w}_1^T A_2 \mathbf{w}_1)
(1 - \delta^2 \|\mathbf{w}_1\|^2)\\
&amp;\approx \lambda_1 
+ 2 \delta \|\mathbf{w}_1\|^2
+ C \delta^2\\ 
&amp;&gt; \lambda_1
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(C \in \mathbb{R}\)</span> depends on <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span>, and where we ignored the “higher-order” terms involving <span class="math notranslate nohighlight">\(\delta^3, \delta^4, \delta^5, \ldots\)</span> whose overall contribution is negligible when <span class="math notranslate nohighlight">\(\delta\)</span> is small. That gives the desired contradiction – that is, it would imply the existence of a vector achieving a strictly better objective value than the optimal <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>. (<em>Exercise:</em> Make this argument rigorous.)</p>
<p>So, letting <span class="math notranslate nohighlight">\(W_1 = \hat{W}_1\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
W_1^T A_1 W_1
= 
\begin{pmatrix}
\lambda_1 &amp; \mathbf{0} \\
\mathbf{0} &amp; A_2
\end{pmatrix}.
\end{split}\]</div>
<p>Finally note that <span class="math notranslate nohighlight">\(A_2 = \hat{V}_1^T A_1 \hat{V}_1\)</span> is symmetric</p>
<div class="math notranslate nohighlight">
\[
A_2^T = (\hat{V}_1^T A_1 \hat{V}_1)^T = \hat{V}_1^T A_1^T \hat{V}_1 = \hat{V}_1^T A_1 \hat{V}_1 = A_2
\]</div>
<p>by the symmetry of <span class="math notranslate nohighlight">\(A_1\)</span> itself.</p>
<p><em><strong>Next step of the induction:</strong></em> Apply the same argument to the symmetric matrix <span class="math notranslate nohighlight">\(A_2 \in \mathbb{R}^{(d-1)\times (d-1)}\)</span>, let <span class="math notranslate nohighlight">\(\hat{W}_2 = (\mathbf{v}_2\ \hat{V}_2) \in \mathbb{R}^{(d-1)\times (d-1)}\)</span> be the corresponding orthogonal matrix, and define <span class="math notranslate nohighlight">\(\lambda_2\)</span> and <span class="math notranslate nohighlight">\(A_3\)</span> through the equation</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{W}_2^T A_2 \hat{W}_2
= 
\begin{pmatrix}
\lambda_2 &amp; \mathbf{0} \\
\mathbf{0} &amp; A_3
\end{pmatrix}.
\end{split}\]</div>
<p>Now define the block matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
W_2
=
\begin{pmatrix}
1 &amp; \mathbf{0}\\
\mathbf{0} &amp; \hat{W}_2
\end{pmatrix}.
\end{split}\]</div>
<p>Observe that (check it!)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
W_2^T W_1^T A_1 W_1 W_2
= W_2^T \begin{pmatrix}
\lambda_1 &amp; \mathbf{0} \\
\mathbf{0} &amp; A_2
\end{pmatrix}
W_2
=
\begin{pmatrix}
\lambda_1 &amp; \mathbf{0}\\
\mathbf{0} &amp; \hat{W}_2^T A_2 \hat{W}_2
\end{pmatrix}
=\begin{pmatrix}
\lambda_1 &amp; 0 &amp; \mathbf{0} \\
0 &amp; \lambda_2 &amp; \mathbf{0} \\
\mathbf{0} &amp; \mathbf{0} &amp; A_3
\end{pmatrix}.
\end{split}\]</div>
<p>Proceeding similarly by induction gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>A couple remarks about the proof:</p>
<p>1- The fact that <span class="math notranslate nohighlight">\(\mathbf{w}_1 = \mathbf{0}\)</span> is perhaps more intuitively understood through vector calculus by using the method of Lagrangian multipliers on <span class="math notranslate nohighlight">\(\max\{\langle \mathbf{v}, A_1 \mathbf{v}\rangle:\|\mathbf{v}\| = 1\}\)</span> to see that <span class="math notranslate nohighlight">\(A_1 \mathbf{v}_1\)</span> must be proportional to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>. Hence <span class="math notranslate nohighlight">\(\mathbf{w}_1 = \hat{V}_1^T A_1 \mathbf{v}_1 = \mathbf{0}\)</span> by construction of <span class="math notranslate nohighlight">\(\hat{V}_1^T\)</span>. Indeed, define the Lagrangian function</p>
<div class="math notranslate nohighlight">
\[
L(\mathbf{v}, \lambda) = \langle \mathbf{v}, A_1 \mathbf{v}\rangle - \lambda(\|\mathbf{v}\|^2 - 1).
\]</div>
<p>The first-order necessary conditions for a local maximizer <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> are (check it!)</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mathbf{v}} L(\mathbf{v}_1, \lambda_1) = 2A_1 \mathbf{v}_1 - 2\lambda_1 \mathbf{v}_1 = \mathbf{0}
\]</div>
<div class="math notranslate nohighlight">
\[
\nabla_{\lambda} L(\mathbf{v}_1, \lambda_1) = \|\mathbf{v}_1\|^2 - 1 = 0.
\]</div>
<p>From the first condition, we have</p>
<div class="math notranslate nohighlight">
\[
A_1 \mathbf{v}_1 = \lambda_1 \mathbf{v}_1.
\]</div>
<p>This shows that <span class="math notranslate nohighlight">\(A_1 \mathbf{v}_1\)</span> is proportional to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> as claimed.</p>
<p>2- By construction, the vector <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span> (i.e., the first column of <span class="math notranslate nohighlight">\(\hat{W}_2\)</span>) is the solution to</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_2 \in \arg\max\{\langle \mathbf{v}, A_2 \mathbf{v}\rangle:\|\mathbf{v}\| = 1\}.
\]</div>
<p>Note that, by definition of <span class="math notranslate nohighlight">\(A_2\)</span> (and the fact that <span class="math notranslate nohighlight">\(A_1 = A\)</span>),</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}^T A_2 \mathbf{v}
= \mathbf{v}^T \hat{V}_1^T A_1 \hat{V}_1 \mathbf{v}
= (\hat{V}_1 \mathbf{v})^T \,A \,(\hat{V}_1 \mathbf{v}).
\]</div>
<p>So we can think of the solution <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span> as specifying an optimal linear combination of the columns of <span class="math notranslate nohighlight">\(\hat{V}_1\)</span> – which form a basis of the space <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{v}_1)^\perp\)</span> of vectors orthogonal to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>. In essence <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span> solves the same problem as <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>, <em>but restricted to <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{v}_1)^\perp\)</span></em>. We will come back to this below.</p>
</section>
<section id="varational-characterization-special-cases">
<h2><span class="section-number">5.3.2. </span>Varational characterization: special cases<a class="headerlink" href="#varational-characterization-special-cases" title="Link to this heading">#</a></h2>
<p>We begin with a definition.</p>
<p><strong>DEFINITION</strong> <strong>(Rayleigh Quotient)</strong> <span class="math notranslate nohighlight">\(\idx{Rayleigh quotient}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span> be a symmetric matrix. The Rayleigh quotient is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle}
\]</div>
<p>which is defined for any <span class="math notranslate nohighlight">\(\mathbf{u} \neq \mathbf{0}\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^{d}\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>To start seeing the connection to the spectral decomposition, let <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> be a (not necessarily unit) eigenvector of <span class="math notranslate nohighlight">\(A\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>. One can show that <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}) = \lambda\)</span>. (Try it!) In fact, recall that we have previously shown that eigenvectors of <span class="math notranslate nohighlight">\(A\)</span> are stationary points of <span class="math notranslate nohighlight">\(\mathcal{R}_A\)</span>.</p>
<p>Before stating a general variational characterization, we prove a few special cases. Throughout, let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span> be a symmetric matrix with spectral decomposition <span class="math notranslate nohighlight">\(A = \sum_{i=1}^d \lambda_i \mathbf{v}_i \mathbf{v}_i^T\)</span> where <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span>.</p>
<p><em>The largest eigenvalue:</em>  Since <span class="math notranslate nohighlight">\(\mathbf{v}_1, \ldots,  \mathbf{v}_d\)</span> forms an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>, any nonzero vector <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> can be written as <span class="math notranslate nohighlight">\(\mathbf{u} = \sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle \mathbf{v}_i\)</span> and it follows that, by the <em>Properties of Orthonormal Lists</em> and the bilinearity of the inner product,</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, \mathbf{u} \rangle
= \|\mathbf{u}\|^2
= \left\|\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle \mathbf{v}_i\right\|^2
= \sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2
\]</div>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, A \mathbf{u} \rangle
= \left\langle \mathbf{u}, \sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle A \mathbf{v}_i \right\rangle
= \left\langle \mathbf{u}, \sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle \lambda_i \mathbf{v}_i \right\rangle
= \sum_{i=1}^d \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2.
\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle}
= \frac{\sum_{i=1}^d \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
\leq \lambda_1 \frac{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
= \lambda_1,
\]</div>
<p>where we used <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span> and the fact that <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{v}_i \rangle^2 \geq 0\)</span>. Moreover <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}_1) = \lambda_1\)</span>. So we have established</p>
<div class="math notranslate nohighlight">
\[
\lambda_1 
= \max_{\mathbf{u} \neq \mathbf{0}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p><em>The smallest eigenvalue:</em>  Arguing in the opposite direction, we get a characterization of the smallest eigenvalue. Using the same notation as before, we have</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle}
= \frac{\sum_{i=1}^d \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
\geq \lambda_d \frac{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
= \lambda_d,
\]</div>
<p>where, again, we used <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span> and the fact that <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{v}_i \rangle^2 \geq 0\)</span>. Moreover <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}_d) = \lambda_d\)</span>. So we have established</p>
<div class="math notranslate nohighlight">
\[
\lambda_d 
= \min_{\mathbf{u} \neq \mathbf{0}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p><em>The second smallest eigenvalue:</em> To pick out the second smallest eigenvalue, we argue as above but restrict the optimization to the space <span class="math notranslate nohighlight">\(\mathcal{V}_{d-1} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{d-1})\)</span>. Indeed, if <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is in the linear subspace <span class="math notranslate nohighlight">\(\mathcal{V}_{d-1}\)</span>, it can be written as <span class="math notranslate nohighlight">\(\mathbf{u} = \sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle \mathbf{v}_i\)</span> (since <span class="math notranslate nohighlight">\(\mathbf{v}_1,\ldots,\mathbf{v}_{d-1}\)</span> forms an orthonormal basis of it; why?) and it follows that</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, \mathbf{u} \rangle
= \sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle^2
\]</div>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, A \mathbf{u} \rangle
= \left\langle \mathbf{u}, \sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle \lambda_i \mathbf{v}_i \right\rangle
= \sum_{i=1}^{d-1} \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2.
\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle}
= \frac{\sum_{i=1}^{d-1} \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
\geq \lambda_{d-1} \frac{\sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
= \lambda_{d-1},
\]</div>
<p>where we used <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_{d-1}\)</span> and the fact that <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{v}_i \rangle^2 \geq 0\)</span>. Moreover <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}_{d-1}) = \lambda_{d-1}\)</span> and of course <span class="math notranslate nohighlight">\(\mathbf{v}_{d-1} \in \mathcal{V}_{d-1}\)</span>. So we have established</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-1} 
= \min_{\mathbf{0} \neq \mathbf{u} \in \mathcal{V}_{d-1}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p>Now what is <span class="math notranslate nohighlight">\(\mathcal{V}_{d-1}\)</span> ? It is spanned by the orthonormal list <span class="math notranslate nohighlight">\(\mathbf{v}_1,\ldots,\mathbf{v}_{d-1}\)</span>, each of  which is orthogonal to <span class="math notranslate nohighlight">\(\mathbf{v}_d\)</span>. So (Why?)</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V}_{d-1}
= \mathrm{span}(\mathbf{v}_d)^\perp.
\]</div>
<p>So, equivalently,</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-1} 
= \min\left\{\mathcal{R}_A(\mathbf{u})\,:\ \mathbf{u} \neq \mathbf{0}, \langle \mathbf{u}, \mathbf{v}_d\rangle = 0 \right\}.
\]</div>
<p>In fact, for any <span class="math notranslate nohighlight">\(\mathbf{u} \neq \mathbf{0}\)</span>, we can normalize it by defining <span class="math notranslate nohighlight">\(\mathbf{z} = \mathbf{u}/\|\mathbf{u}\|\)</span> and we note that</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u}\rangle}{\langle \mathbf{u},\mathbf{u}\rangle}
= \frac{\langle \mathbf{u}, A \mathbf{u}\rangle}{\|\mathbf{u}\|^2}
= \left\langle \frac{\mathbf{u}}{\|\mathbf{u}\|}, A \frac{\mathbf{u}}{\|\mathbf{u}\|}\right\rangle
= \langle \mathbf{z}, A \mathbf{z}\rangle.
\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-1} 
= \min\left\{\langle \mathbf{z}, A \mathbf{z}\rangle\,:\ \|\mathbf{z}\|=1, \langle \mathbf{z}, \mathbf{v}_d\rangle = 0 \right\}.
\]</div>
</section>
<section id="general-statement-courant-fischer">
<h2><span class="section-number">5.3.3. </span>General statement: Courant-Fischer<a class="headerlink" href="#general-statement-courant-fischer" title="Link to this heading">#</a></h2>
<p>Before stating a general result, we give one more example.</p>
<p><em>The second smallest eigenvalue (take two):</em> Interestingly, there is a second characterization of the second smallest eigenvalue. Indeed, suppose we restrict the optimization to the space <span class="math notranslate nohighlight">\(\mathcal{W}_{2} = \mathrm{span}(\mathbf{v}_{d-1},\mathbf{v}_d)\)</span> instead. If <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is in the linear subspace <span class="math notranslate nohighlight">\(\mathcal{W}_{2}\)</span>, it can be written as <span class="math notranslate nohighlight">\(\mathbf{u} = \sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle \mathbf{v}_i\)</span> (since <span class="math notranslate nohighlight">\(\mathbf{v}_{d-1},\mathbf{v}_{d}\)</span> forms an orthonormal basis of it) and it follows that</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, \mathbf{u} \rangle
= \sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle^2
\]</div>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, A \mathbf{u} \rangle
= \left\langle \mathbf{u}, \sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle \lambda_i \mathbf{v}_i \right\rangle
= \sum_{i=d-1}^{d} \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2.
\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle}
= \frac{\sum_{i=d-1}^{d} \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
\leq \lambda_{d-1} \frac{\sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
= \lambda_{d-1},
\]</div>
<p>where we used <span class="math notranslate nohighlight">\(\lambda_{d-1} \geq \lambda_{d}\)</span> and the fact that <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{v}_i \rangle^2 \geq 0\)</span>. Moreover <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}_{d-1}) = \lambda_{d-1}\)</span>. So we have established</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-1} 
= \max_{\mathbf{0} \neq \mathbf{u} \in \mathcal{W}_{2}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p><strong>THEOREM</strong> <strong>(Courant-Fischer)</strong> <span class="math notranslate nohighlight">\(\idx{Courant-Fischer theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span> be a symmetric matrix with spectral decomposition <span class="math notranslate nohighlight">\(A = \sum_{i=1}^d \lambda_i \mathbf{v}_i \mathbf{v}_i^T\)</span> where <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span>. For each <span class="math notranslate nohighlight">\(k = 1,\ldots,d\)</span>, define the subspace</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V}_k = \mathrm{span}(\mathbf{v}_1, \ldots,  \mathbf{v}_k)
\quad\text{and}\quad
\mathcal{W}_{d-k+1} = \mathrm{span}(\mathbf{v}_k, \ldots,  \mathbf{v}_d).
\]</div>
<p>Then, for all <span class="math notranslate nohighlight">\(k = 1,\ldots,d\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\lambda_k 
= \min_{\mathbf{u} \in \mathcal{V}_k} \mathcal{R}_A(\mathbf{u})
= \max_{\mathbf{u} \in \mathcal{W}_{d-k+1}} \mathcal{R}_A(\mathbf{u}),
\]</div>
<p>which are referred to as the local formulas. Furthermore we have the following min-max (or global) formulas, which do not depend on the choice of a spectral decomposition: for all <span class="math notranslate nohighlight">\(k = 1,\ldots,d\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\lambda_k 
= \max_{\mathrm{dim}(\mathcal{V}) = k} \min_{\mathbf{u} \in \mathcal{V}} \mathcal{R}_A(\mathbf{u})
= \min_{\mathrm{dim}(\mathcal{W}) = d-k+1} \max_{\mathbf{u} \in \mathcal{W}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> For the local formula, we expand a vector in <span class="math notranslate nohighlight">\(\mathcal{V}_k\)</span> into the basis <span class="math notranslate nohighlight">\(\mathbf{v}_1,\ldots,\mathbf{v}_k\)</span> and use the fact that <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}_i) = \lambda_i\)</span> and that eigenvalues are in non-increasing order. The global formulas follow from a dimension argument.</p>
<p><strong>EXAMPLE:</strong> <strong>(Third Smallest Eigenvalue)</strong> The <em>Courant-Fischer theorem</em> can be used to recover the special cases of the previous subsection. One also gets new cases of interest. We give a characterization of the third smallest eigenvalue next. Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span> be a symmetric matrix with spectral decomposition <span class="math notranslate nohighlight">\(A = \sum_{i=1}^d \lambda_i \mathbf{v}_i \mathbf{v}_i^T\)</span> where <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span>. Using <span class="math notranslate nohighlight">\(k=d-2\)</span> in the <em>Courant-Fischer theorem</em> gives</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-2} 
= \min_{\mathbf{u} \in \mathcal{V}_{d-2}} \mathcal{R}_A(\mathbf{u}),
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V}_{d-2} = \mathrm{span}(\mathbf{v}_1, \ldots,  \mathbf{v}_{d-2}).
\]</div>
<p>It can be seen that</p>
<div class="math notranslate nohighlight">
\[
\mathrm{span}(\mathbf{v}_1, \ldots,  \mathbf{v}_{d-2})
= \mathrm{span}(\mathbf{v}_{d-1}, \mathbf{v}_{d})^\perp.
\]</div>
<p>So we finally arrive at the following characterization of the third smallest eigenvalue</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-2} 
= \min\{\mathcal{R}_A(\mathbf{u})\,:\, \mathbf{0} \neq \mathbf{u}, \langle \mathbf{u},\mathbf{v}_d\rangle = 0, 
\langle \mathbf{u},\mathbf{v}_{d-1}\rangle = 0\}.
\]</div>
<p>Using the same argument as we used for <span class="math notranslate nohighlight">\(\lambda_{d-1}\)</span>, we get also</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-2} 
= \min\left\{\langle \mathbf{z}, A \mathbf{z}\rangle\,:\ \|\mathbf{z}\|=1, \langle \mathbf{z}, \mathbf{v}_d\rangle = 0, \langle \mathbf{z}, \mathbf{v}_{d-1}\rangle = 0 \right\}.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>We now give a proof of the <em>Courant-Fischer theorem</em>. The local formulas follows from the same argument used to derive the special cases above so omit the general proof (but try to prove it!). The global formulas require a new idea.</p>
<p><em>Proof:</em> <em>(Courant-Fischer)</em> <span class="math notranslate nohighlight">\(\idx{Courant-Fischer theorem}\xdi\)</span> Since <span class="math notranslate nohighlight">\(\mathcal{V}_k\)</span> has dimension <span class="math notranslate nohighlight">\(k\)</span>, it follows from the local formula that</p>
<div class="math notranslate nohighlight">
\[
\lambda_k 
= \min_{\mathbf{u} \in \mathcal{V}_k} \mathcal{R}_A(\mathbf{u})
\leq \max_{\mathrm{dim}(\mathcal{V}) = k} \min_{\mathbf{u} \in \mathcal{V}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> be any subspace with dimension <span class="math notranslate nohighlight">\(k\)</span>. Because <span class="math notranslate nohighlight">\(\mathcal{W}_{d-k+1}\)</span> has dimension <span class="math notranslate nohighlight">\(d - k + 1\)</span>, we have that <span class="math notranslate nohighlight">\(\dim(\mathcal{V}) + \mathrm{dim}(\mathcal{W}_{d-k+1}) &gt; d\)</span> and there must be non-zero vector <span class="math notranslate nohighlight">\(\mathbf{u}_0\)</span> in the intersection <span class="math notranslate nohighlight">\(\mathcal{V} \cap \mathcal{W}_{d-k+1}\)</span> (Prove it!).</p>
<p>We then have by the other local formula that</p>
<div class="math notranslate nohighlight">
\[
\lambda_k 
= \max_{\mathbf{u} \in \mathcal{W}_{d-k+1}} \mathcal{R}_A(\mathbf{u})
\geq \mathcal{R}_A(\mathbf{u}_0)
\geq \min_{\mathbf{u} \in \mathcal{V}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p>Since this inequality holds for any subspace of dimension <span class="math notranslate nohighlight">\(k\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\lambda_k 
\geq \max_{\mathrm{dim}(\mathcal{V}) = k} \min_{\mathbf{u} \in \mathcal{V}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p>Combining with the inequality in the other direction above gives the claim. The other global formula is proved similarly. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> According to the variational characterization of the largest eigenvalue <span class="math notranslate nohighlight">\(\lambda_1\)</span> of a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span>, which of the following is true?</p>
<p>a) <span class="math notranslate nohighlight">\(\lambda_1 = \min_{\mathbf{u} \neq 0} R_A(\mathbf{u})\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\lambda_1 = \max_{\mathbf{u} \neq 0} R_A(\mathbf{u})\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\lambda_1 = \min_{\|\mathbf{u}\| = 0} R_A(\mathbf{u})\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\lambda_1 = \max_{\|\mathbf{u}\| = 0} R_A(\mathbf{u})\)</span></p>
<p><strong>2</strong> Let <span class="math notranslate nohighlight">\(\mathcal{V}_{d-1} = \mathrm{span}(\mathbf{v}_1, \ldots, \mathbf{v}_{d-1})\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{v}_1, \ldots, \mathbf{v}_d\)</span> are the eigenvectors of a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span> with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span>. Which of the following characterizes the second smallest eigenvalue <span class="math notranslate nohighlight">\(\lambda_{d-1}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \min_{0 \neq \mathbf{u} \in \mathcal{V}_{d-1}} R_A(\mathbf{u})\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \max_{0 \neq \mathbf{u} \in \mathcal{V}_{d-1}} R_A(\mathbf{u})\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \min_{\|\mathbf{u}\| = 0, \mathbf{u} \in \mathcal{V}_{d-1}} R_A(\mathbf{u})\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \max_{\|\mathbf{u}\| = 0, \mathbf{u} \in \mathcal{V}_{d-1}} R_A(\mathbf{u})\)</span></p>
<p><strong>3</strong> Let <span class="math notranslate nohighlight">\(\mathcal{W}_2 = \mathrm{span}(\mathbf{v}_{d-1}, \mathbf{v}_d)\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{v}_1, \ldots, \mathbf{v}_d\)</span> are the eigenvectors of a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span> with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span>. Which of the following characterizes the second smallest eigenvalue <span class="math notranslate nohighlight">\(\lambda_{d-1}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \min_{0 \neq \mathbf{u} \in \mathcal{W}_2} R_A(\mathbf{u})\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \max_{0 \neq \mathbf{u} \in \mathcal{W}_2} R_A(\mathbf{u})\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \min_{\|\mathbf{u}\| = 0, \mathbf{u} \in \mathcal{W}_2} R_A(\mathbf{u})\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \max_{\|\mathbf{u}\| = 0, \mathbf{u} \in \mathcal{W}_2} R_A(\mathbf{u})\)</span></p>
<p><strong>4</strong> According to the <em>Courant-Fischer Theorem</em>, which of the following is the global formula for the <span class="math notranslate nohighlight">\(k\)</span>-th eigenvalue <span class="math notranslate nohighlight">\(\lambda_k\)</span> of a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\lambda_k = \min_{\mathbf{u} \in \mathcal{V}_k} R_A(\mathbf{u}) = \max_{\mathbf{u} \in \mathcal{W}_{d-k+1}} R_A(\mathbf{u})\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\lambda_k = \max_{\mathbf{u} \in \mathcal{V}_k} R_A(\mathbf{u}) = \min_{\mathbf{u} \in \mathcal{W}_{d-k+1}} R_A(\mathbf{u})\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\lambda_k = \max_{\mathrm{dim}(\mathcal{V}) = k} \min_{\mathbf{u} \in \mathcal{V}} R_A(\mathbf{u}) = \min_{\mathrm{dim}(\mathcal{W}) = d-k+1} \max_{\mathbf{u} \in \mathcal{W}} R_A(\mathbf{u})\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\lambda_k = \max_{\mathrm{dim}(\mathcal{V}) = k} \max_{\mathbf{u} \in \mathcal{V}} R_A(\mathbf{u}) = \min_{\mathrm{dim}(\mathcal{W}) = d-k+1} \min_{\mathbf{u} \in \mathcal{W}} R_A(\mathbf{u})\)</span></p>
<p><strong>5</strong> What is the main difference between the local and global formulas in the <em>Courant-Fischer Theorem</em>?</p>
<p>a) The local formulas are easier to compute than the global formulas.</p>
<p>b) The local formulas depend on a specific choice of eigenvectors, while the global formulas do not.</p>
<p>c) The local formulas apply only to symmetric matrices, while the global formulas apply to any matrix.</p>
<p>d) The local formulas provide upper bounds on the eigenvalues, while the global formulas provide lower bounds.</p>
<p>Answer for 1: b. Justification: The text establishes that <span class="math notranslate nohighlight">\(\lambda_1 = \max_{\mathbf{u} \neq 0} R_A(\mathbf{u})\)</span>.</p>
<p>Answer for 2: a. Justification: The text establishes that <span class="math notranslate nohighlight">\(\lambda_{d-1} = \min_{0 \neq \mathbf{u} \in \mathcal{V}_{d-1}} R_A(\mathbf{u})\)</span>.</p>
<p>Answer for 3: b. Justification: The text establishes that <span class="math notranslate nohighlight">\(\lambda_{d-1} = \max_{0 \neq \mathbf{u} \in \mathcal{W}_2} R_A(\mathbf{u})\)</span>.</p>
<p>Answer for 4: c. Justification: The <em>Courant-Fischer Theorem</em> states that the global formula for the <span class="math notranslate nohighlight">\(k\)</span>-th eigenvalue is <span class="math notranslate nohighlight">\(\lambda_k = \max_{\mathrm{dim}(\mathcal{V}) = k} \min_{\mathbf{u} \in \mathcal{V}} R_A(\mathbf{u}) = \min_{\mathrm{dim}(\mathcal{W}) = d-k+1} \max_{\mathbf{u} \in \mathcal{W}} R_A(\mathbf{u})\)</span>.</p>
<p>Answer for 5: b. Justification: The text highlights that the global formulas “do not depend on the choice of spectral decomposition,” unlike the local formulas, which rely on a specific set of eigenvectors.</p>
</section>
&#13;

<h2><span class="section-number">5.3.1. </span>Proof of Spectral Theorem<a class="headerlink" href="#proof-of-spectral-theorem" title="Link to this heading">#</a></h2>
<p>We will need the following block matrix formula. Suppose</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A
=
\begin{pmatrix}
A_{11} \\ A_{21}
\end{pmatrix}
\quad
\text{and}
\quad
B
=
\begin{pmatrix}
B_{11} &amp; B_{12}
\end{pmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(A_{11} \in \mathbb{R}^{n_1\times p}\)</span>, <span class="math notranslate nohighlight">\(A_{21} \in \mathbb{R}^{n_2\times p}\)</span>, <span class="math notranslate nohighlight">\(B_{11} \in \mathbb{R}^{p\times n_1}\)</span>, <span class="math notranslate nohighlight">\(B_{12} \in \mathbb{R}^{p\times n_2}\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A B
=
\begin{pmatrix}
A_{11} B_{11} &amp; A_{11} B_{12} \\ 
A_{21} B_{11} &amp; A_{21} B_{12}
\end{pmatrix}.
\end{split}\]</div>
<p>Indeed this is just a special case of the <span class="math notranslate nohighlight">\(2 \times 2\)</span> block matrix product formula we have encountered previously, with empty blocks <span class="math notranslate nohighlight">\(A_{12}, A_{22}, B_{21}, B_{22}\)</span>.</p>
<p><em>Proof idea (Spectral Theorem):</em> Similarly to how we used Householder transformations to “add zeros under the diagonal”, here we will use a sequence of orthogonal transformations to add zeros both below and above the diagonal. Recall that two matrices <span class="math notranslate nohighlight">\(C, D \in \mathbb{R}^{d \times d}\)</span> are <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_similarity">similar</a> if there is an invertible matrix <span class="math notranslate nohighlight">\(P\)</span> such that <span class="math notranslate nohighlight">\(C = P^{-1} D P\)</span>, and that, when <span class="math notranslate nohighlight">\(P = W\)</span> is an orthogonal matrix, the transformation simplifies to <span class="math notranslate nohighlight">\(C = W^T D W\)</span>.</p>
<p>We construct a sequence of orthogonal matrices <span class="math notranslate nohighlight">\(\hat{W}_1,\ldots, \hat{W}_d\)</span> and progressively compute <span class="math notranslate nohighlight">\(W_1^T A W_1\)</span>, <span class="math notranslate nohighlight">\(W_2^T W_1^T A W_1 W_2\)</span>, and so on in such a way that</p>
<div class="math notranslate nohighlight">
\[
\Lambda = W_d^T \cdots W_2^T W_1^T A W_1 W_2 \cdots W_d
\]</div>
<p>is diagonal. Then the matrix <span class="math notranslate nohighlight">\(Q\)</span> is simply <span class="math notranslate nohighlight">\(W_1 W_2 \cdots W_d\)</span> (check it!).</p>
<p>To define these matrices, we use a greedy sequence maximizing the quadratic form <span class="math notranslate nohighlight">\(\langle \mathbf{v}, A \mathbf{v}\rangle\)</span>. How is this quadratic form related to eigenvalues? Recall that, for a unit eigenvector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>, we have <span class="math notranslate nohighlight">\(\langle \mathbf{v}, A \mathbf{v}\rangle = \langle \mathbf{v}, \lambda \mathbf{v}\rangle = \lambda\)</span>.</p>
<p><em>Proof:</em> <em>(Spectral Theorem)</em> <span class="math notranslate nohighlight">\(\idx{spectral theorem}\xdi\)</span> We proceed by induction. We have already encountered the idea that eigenvectors are stationary points of an optimization problem. The proof in fact uses that idea.</p>
<p><em><strong>A first eigenvector:</strong></em> Let <span class="math notranslate nohighlight">\(A_1 = A\)</span>. Define</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_1 \in \arg\max\{\langle \mathbf{v}, A_1 \mathbf{v}\rangle:\|\mathbf{v}\| = 1\}
\]</div>
<p>which exists by the <em>Extreme Value Theorem</em>, and further define</p>
<div class="math notranslate nohighlight">
\[
\lambda_1 = \max\{\langle \mathbf{v}, A_1 \mathbf{v}\rangle:\|\mathbf{v}\| = 1\}.
\]</div>
<p>Complete <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> into an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>, <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>, <span class="math notranslate nohighlight">\(\hat{\mathbf{v}}_2, \ldots, \hat{\mathbf{v}}_d\)</span>, and form the block matrix</p>
<div class="math notranslate nohighlight">
\[
\hat{W}_1 
=
\begin{pmatrix}
\mathbf{v}_1 &amp; \hat{V}_1
\end{pmatrix}
\]</div>
<p>where the columns of <span class="math notranslate nohighlight">\(\hat{V}_1\)</span> are <span class="math notranslate nohighlight">\(\hat{\mathbf{v}}_2, \ldots, \hat{\mathbf{v}}_d\)</span>. Note that <span class="math notranslate nohighlight">\(\hat{W}_1\)</span> is orthogonal by construction.</p>
<p><em><strong>Getting one step closer to diagonalization:</strong></em> We show next that <span class="math notranslate nohighlight">\(\hat{W}_1\)</span> gets us one step closer to a diagonal matrix by similarity transformation. Note first that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\hat{W}_1^T A_1 \hat{W}_1
&amp;=
\begin{pmatrix}
\mathbf{v}_1^T \\ \hat{V}_1^T
\end{pmatrix}
A_1
\begin{pmatrix}
\mathbf{v}_1 &amp; \hat{V}_1
\end{pmatrix}\\
&amp;=
\begin{pmatrix}
\mathbf{v}_1^T \\ \hat{V}_1^T
\end{pmatrix}
\begin{pmatrix}
A_1 \mathbf{v}_1 &amp; A_1 \hat{V}_1
\end{pmatrix}\\
&amp;=
\begin{pmatrix}
\mathbf{v}_1^T A_1 \mathbf{v}_1 &amp; \mathbf{v}_1^T A_1 \hat{V}_1\\
\hat{V}_1^T A_1 \mathbf{v}_1 &amp; \hat{V}_1^T A_1 \hat{V}_1
\end{pmatrix}\\
&amp;= 
\begin{pmatrix}
\lambda_1 &amp; \mathbf{w}_1^T \\
\mathbf{w}_1 &amp; A_2
\end{pmatrix}
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{w}_1 = \hat{V}_1^T A_1 \mathbf{v}_1\)</span> and <span class="math notranslate nohighlight">\(A_2 = \hat{V}_1^T A_1 \hat{V}_1\)</span>.</p>
<p>The key claim is that <span class="math notranslate nohighlight">\(\mathbf{w}_1 = \mathbf{0}\)</span>. This follows from a contradiction argument. Indeed, suppose <span class="math notranslate nohighlight">\(\mathbf{w}_1 \neq \mathbf{0}\)</span> and consider the unit vector (check that <span class="math notranslate nohighlight">\(\|\mathbf{z}\| = 1\)</span>)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{z}
=
\hat{W}_1 
\frac{1}{\sqrt{1 + \delta^2 \|\mathbf{w}_1\|^2}} \begin{pmatrix}
1 \\
\delta \mathbf{w}_1
\end{pmatrix}
\end{split}\]</div>
<p>which achieves the objective value (check it!)</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}^T A_1 \mathbf{z}
&amp;=
\frac{1}{1 + \delta^2 \|\mathbf{w}_1\|^2}
\begin{pmatrix}
1 \\
\delta \mathbf{w}_1
\end{pmatrix}^T
\begin{pmatrix}
\lambda_1 &amp; \mathbf{w}_1^T \\
\mathbf{w}_1 &amp; A_2
\end{pmatrix}
\begin{pmatrix}
1 \\
\delta \mathbf{w}_1
\end{pmatrix}\\
&amp;= 
\frac{1}{1 + \delta^2 \|\mathbf{w}_1\|^2}
\left(
\lambda_1
+ 2 \delta \|\mathbf{w}_1\|^2
+ \delta^2 \mathbf{w}_1^T A_2 \mathbf{w}_1
\right).
\end{align*}\]</div>
<p>By the <a class="reference external" href="https://en.wikipedia.org/wiki/Geometric_series">sum of a geometric series</a>, for <span class="math notranslate nohighlight">\(\varepsilon \in (0,1)\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\frac{1}{1 + \varepsilon^2}
= 1 - \varepsilon^2 + \varepsilon^4 + \cdots.
\]</div>
<p>So, for <span class="math notranslate nohighlight">\(\delta &gt; 0\)</span> small enough,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathbf{z}^T A_1 \mathbf{z}
&amp;\approx
(\lambda_1
+ 2 \delta \|\mathbf{w}_1\|^2
+ \delta^2 \mathbf{w}_1^T A_2 \mathbf{w}_1)
(1 - \delta^2 \|\mathbf{w}_1\|^2)\\
&amp;\approx \lambda_1 
+ 2 \delta \|\mathbf{w}_1\|^2
+ C \delta^2\\ 
&amp;&gt; \lambda_1
\end{align*}\]</div>
<p>where <span class="math notranslate nohighlight">\(C \in \mathbb{R}\)</span> depends on <span class="math notranslate nohighlight">\(\mathbf{w}_1\)</span> and <span class="math notranslate nohighlight">\(A_2\)</span>, and where we ignored the “higher-order” terms involving <span class="math notranslate nohighlight">\(\delta^3, \delta^4, \delta^5, \ldots\)</span> whose overall contribution is negligible when <span class="math notranslate nohighlight">\(\delta\)</span> is small. That gives the desired contradiction – that is, it would imply the existence of a vector achieving a strictly better objective value than the optimal <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>. (<em>Exercise:</em> Make this argument rigorous.)</p>
<p>So, letting <span class="math notranslate nohighlight">\(W_1 = \hat{W}_1\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
W_1^T A_1 W_1
= 
\begin{pmatrix}
\lambda_1 &amp; \mathbf{0} \\
\mathbf{0} &amp; A_2
\end{pmatrix}.
\end{split}\]</div>
<p>Finally note that <span class="math notranslate nohighlight">\(A_2 = \hat{V}_1^T A_1 \hat{V}_1\)</span> is symmetric</p>
<div class="math notranslate nohighlight">
\[
A_2^T = (\hat{V}_1^T A_1 \hat{V}_1)^T = \hat{V}_1^T A_1^T \hat{V}_1 = \hat{V}_1^T A_1 \hat{V}_1 = A_2
\]</div>
<p>by the symmetry of <span class="math notranslate nohighlight">\(A_1\)</span> itself.</p>
<p><em><strong>Next step of the induction:</strong></em> Apply the same argument to the symmetric matrix <span class="math notranslate nohighlight">\(A_2 \in \mathbb{R}^{(d-1)\times (d-1)}\)</span>, let <span class="math notranslate nohighlight">\(\hat{W}_2 = (\mathbf{v}_2\ \hat{V}_2) \in \mathbb{R}^{(d-1)\times (d-1)}\)</span> be the corresponding orthogonal matrix, and define <span class="math notranslate nohighlight">\(\lambda_2\)</span> and <span class="math notranslate nohighlight">\(A_3\)</span> through the equation</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\hat{W}_2^T A_2 \hat{W}_2
= 
\begin{pmatrix}
\lambda_2 &amp; \mathbf{0} \\
\mathbf{0} &amp; A_3
\end{pmatrix}.
\end{split}\]</div>
<p>Now define the block matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
W_2
=
\begin{pmatrix}
1 &amp; \mathbf{0}\\
\mathbf{0} &amp; \hat{W}_2
\end{pmatrix}.
\end{split}\]</div>
<p>Observe that (check it!)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
W_2^T W_1^T A_1 W_1 W_2
= W_2^T \begin{pmatrix}
\lambda_1 &amp; \mathbf{0} \\
\mathbf{0} &amp; A_2
\end{pmatrix}
W_2
=
\begin{pmatrix}
\lambda_1 &amp; \mathbf{0}\\
\mathbf{0} &amp; \hat{W}_2^T A_2 \hat{W}_2
\end{pmatrix}
=\begin{pmatrix}
\lambda_1 &amp; 0 &amp; \mathbf{0} \\
0 &amp; \lambda_2 &amp; \mathbf{0} \\
\mathbf{0} &amp; \mathbf{0} &amp; A_3
\end{pmatrix}.
\end{split}\]</div>
<p>Proceeding similarly by induction gives the claim. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p>A couple remarks about the proof:</p>
<p>1- The fact that <span class="math notranslate nohighlight">\(\mathbf{w}_1 = \mathbf{0}\)</span> is perhaps more intuitively understood through vector calculus by using the method of Lagrangian multipliers on <span class="math notranslate nohighlight">\(\max\{\langle \mathbf{v}, A_1 \mathbf{v}\rangle:\|\mathbf{v}\| = 1\}\)</span> to see that <span class="math notranslate nohighlight">\(A_1 \mathbf{v}_1\)</span> must be proportional to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>. Hence <span class="math notranslate nohighlight">\(\mathbf{w}_1 = \hat{V}_1^T A_1 \mathbf{v}_1 = \mathbf{0}\)</span> by construction of <span class="math notranslate nohighlight">\(\hat{V}_1^T\)</span>. Indeed, define the Lagrangian function</p>
<div class="math notranslate nohighlight">
\[
L(\mathbf{v}, \lambda) = \langle \mathbf{v}, A_1 \mathbf{v}\rangle - \lambda(\|\mathbf{v}\|^2 - 1).
\]</div>
<p>The first-order necessary conditions for a local maximizer <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> are (check it!)</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\mathbf{v}} L(\mathbf{v}_1, \lambda_1) = 2A_1 \mathbf{v}_1 - 2\lambda_1 \mathbf{v}_1 = \mathbf{0}
\]</div>
<div class="math notranslate nohighlight">
\[
\nabla_{\lambda} L(\mathbf{v}_1, \lambda_1) = \|\mathbf{v}_1\|^2 - 1 = 0.
\]</div>
<p>From the first condition, we have</p>
<div class="math notranslate nohighlight">
\[
A_1 \mathbf{v}_1 = \lambda_1 \mathbf{v}_1.
\]</div>
<p>This shows that <span class="math notranslate nohighlight">\(A_1 \mathbf{v}_1\)</span> is proportional to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span> as claimed.</p>
<p>2- By construction, the vector <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span> (i.e., the first column of <span class="math notranslate nohighlight">\(\hat{W}_2\)</span>) is the solution to</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}_2 \in \arg\max\{\langle \mathbf{v}, A_2 \mathbf{v}\rangle:\|\mathbf{v}\| = 1\}.
\]</div>
<p>Note that, by definition of <span class="math notranslate nohighlight">\(A_2\)</span> (and the fact that <span class="math notranslate nohighlight">\(A_1 = A\)</span>),</p>
<div class="math notranslate nohighlight">
\[
\mathbf{v}^T A_2 \mathbf{v}
= \mathbf{v}^T \hat{V}_1^T A_1 \hat{V}_1 \mathbf{v}
= (\hat{V}_1 \mathbf{v})^T \,A \,(\hat{V}_1 \mathbf{v}).
\]</div>
<p>So we can think of the solution <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span> as specifying an optimal linear combination of the columns of <span class="math notranslate nohighlight">\(\hat{V}_1\)</span> – which form a basis of the space <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{v}_1)^\perp\)</span> of vectors orthogonal to <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>. In essence <span class="math notranslate nohighlight">\(\mathbf{v}_2\)</span> solves the same problem as <span class="math notranslate nohighlight">\(\mathbf{v}_1\)</span>, <em>but restricted to <span class="math notranslate nohighlight">\(\mathrm{span}(\mathbf{v}_1)^\perp\)</span></em>. We will come back to this below.</p>
&#13;

<h2><span class="section-number">5.3.2. </span>Varational characterization: special cases<a class="headerlink" href="#varational-characterization-special-cases" title="Link to this heading">#</a></h2>
<p>We begin with a definition.</p>
<p><strong>DEFINITION</strong> <strong>(Rayleigh Quotient)</strong> <span class="math notranslate nohighlight">\(\idx{Rayleigh quotient}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span> be a symmetric matrix. The Rayleigh quotient is defined as</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle}
\]</div>
<p>which is defined for any <span class="math notranslate nohighlight">\(\mathbf{u} \neq \mathbf{0}\)</span> in <span class="math notranslate nohighlight">\(\mathbb{R}^{d}\)</span>. <span class="math notranslate nohighlight">\(\natural\)</span></p>
<p>To start seeing the connection to the spectral decomposition, let <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> be a (not necessarily unit) eigenvector of <span class="math notranslate nohighlight">\(A\)</span> with eigenvalue <span class="math notranslate nohighlight">\(\lambda\)</span>. One can show that <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}) = \lambda\)</span>. (Try it!) In fact, recall that we have previously shown that eigenvectors of <span class="math notranslate nohighlight">\(A\)</span> are stationary points of <span class="math notranslate nohighlight">\(\mathcal{R}_A\)</span>.</p>
<p>Before stating a general variational characterization, we prove a few special cases. Throughout, let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span> be a symmetric matrix with spectral decomposition <span class="math notranslate nohighlight">\(A = \sum_{i=1}^d \lambda_i \mathbf{v}_i \mathbf{v}_i^T\)</span> where <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span>.</p>
<p><em>The largest eigenvalue:</em>  Since <span class="math notranslate nohighlight">\(\mathbf{v}_1, \ldots,  \mathbf{v}_d\)</span> forms an orthonormal basis of <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>, any nonzero vector <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> can be written as <span class="math notranslate nohighlight">\(\mathbf{u} = \sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle \mathbf{v}_i\)</span> and it follows that, by the <em>Properties of Orthonormal Lists</em> and the bilinearity of the inner product,</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, \mathbf{u} \rangle
= \|\mathbf{u}\|^2
= \left\|\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle \mathbf{v}_i\right\|^2
= \sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2
\]</div>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, A \mathbf{u} \rangle
= \left\langle \mathbf{u}, \sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle A \mathbf{v}_i \right\rangle
= \left\langle \mathbf{u}, \sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle \lambda_i \mathbf{v}_i \right\rangle
= \sum_{i=1}^d \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2.
\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle}
= \frac{\sum_{i=1}^d \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
\leq \lambda_1 \frac{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
= \lambda_1,
\]</div>
<p>where we used <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span> and the fact that <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{v}_i \rangle^2 \geq 0\)</span>. Moreover <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}_1) = \lambda_1\)</span>. So we have established</p>
<div class="math notranslate nohighlight">
\[
\lambda_1 
= \max_{\mathbf{u} \neq \mathbf{0}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p><em>The smallest eigenvalue:</em>  Arguing in the opposite direction, we get a characterization of the smallest eigenvalue. Using the same notation as before, we have</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle}
= \frac{\sum_{i=1}^d \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
\geq \lambda_d \frac{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^d \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
= \lambda_d,
\]</div>
<p>where, again, we used <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span> and the fact that <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{v}_i \rangle^2 \geq 0\)</span>. Moreover <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}_d) = \lambda_d\)</span>. So we have established</p>
<div class="math notranslate nohighlight">
\[
\lambda_d 
= \min_{\mathbf{u} \neq \mathbf{0}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p><em>The second smallest eigenvalue:</em> To pick out the second smallest eigenvalue, we argue as above but restrict the optimization to the space <span class="math notranslate nohighlight">\(\mathcal{V}_{d-1} = \mathrm{span}(\mathbf{v}_1,\ldots,\mathbf{v}_{d-1})\)</span>. Indeed, if <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is in the linear subspace <span class="math notranslate nohighlight">\(\mathcal{V}_{d-1}\)</span>, it can be written as <span class="math notranslate nohighlight">\(\mathbf{u} = \sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle \mathbf{v}_i\)</span> (since <span class="math notranslate nohighlight">\(\mathbf{v}_1,\ldots,\mathbf{v}_{d-1}\)</span> forms an orthonormal basis of it; why?) and it follows that</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, \mathbf{u} \rangle
= \sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle^2
\]</div>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, A \mathbf{u} \rangle
= \left\langle \mathbf{u}, \sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle \lambda_i \mathbf{v}_i \right\rangle
= \sum_{i=1}^{d-1} \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2.
\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle}
= \frac{\sum_{i=1}^{d-1} \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
\geq \lambda_{d-1} \frac{\sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=1}^{d-1} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
= \lambda_{d-1},
\]</div>
<p>where we used <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_{d-1}\)</span> and the fact that <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{v}_i \rangle^2 \geq 0\)</span>. Moreover <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}_{d-1}) = \lambda_{d-1}\)</span> and of course <span class="math notranslate nohighlight">\(\mathbf{v}_{d-1} \in \mathcal{V}_{d-1}\)</span>. So we have established</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-1} 
= \min_{\mathbf{0} \neq \mathbf{u} \in \mathcal{V}_{d-1}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p>Now what is <span class="math notranslate nohighlight">\(\mathcal{V}_{d-1}\)</span> ? It is spanned by the orthonormal list <span class="math notranslate nohighlight">\(\mathbf{v}_1,\ldots,\mathbf{v}_{d-1}\)</span>, each of  which is orthogonal to <span class="math notranslate nohighlight">\(\mathbf{v}_d\)</span>. So (Why?)</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V}_{d-1}
= \mathrm{span}(\mathbf{v}_d)^\perp.
\]</div>
<p>So, equivalently,</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-1} 
= \min\left\{\mathcal{R}_A(\mathbf{u})\,:\ \mathbf{u} \neq \mathbf{0}, \langle \mathbf{u}, \mathbf{v}_d\rangle = 0 \right\}.
\]</div>
<p>In fact, for any <span class="math notranslate nohighlight">\(\mathbf{u} \neq \mathbf{0}\)</span>, we can normalize it by defining <span class="math notranslate nohighlight">\(\mathbf{z} = \mathbf{u}/\|\mathbf{u}\|\)</span> and we note that</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u}\rangle}{\langle \mathbf{u},\mathbf{u}\rangle}
= \frac{\langle \mathbf{u}, A \mathbf{u}\rangle}{\|\mathbf{u}\|^2}
= \left\langle \frac{\mathbf{u}}{\|\mathbf{u}\|}, A \frac{\mathbf{u}}{\|\mathbf{u}\|}\right\rangle
= \langle \mathbf{z}, A \mathbf{z}\rangle.
\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-1} 
= \min\left\{\langle \mathbf{z}, A \mathbf{z}\rangle\,:\ \|\mathbf{z}\|=1, \langle \mathbf{z}, \mathbf{v}_d\rangle = 0 \right\}.
\]</div>
&#13;

<h2><span class="section-number">5.3.3. </span>General statement: Courant-Fischer<a class="headerlink" href="#general-statement-courant-fischer" title="Link to this heading">#</a></h2>
<p>Before stating a general result, we give one more example.</p>
<p><em>The second smallest eigenvalue (take two):</em> Interestingly, there is a second characterization of the second smallest eigenvalue. Indeed, suppose we restrict the optimization to the space <span class="math notranslate nohighlight">\(\mathcal{W}_{2} = \mathrm{span}(\mathbf{v}_{d-1},\mathbf{v}_d)\)</span> instead. If <span class="math notranslate nohighlight">\(\mathbf{u}\)</span> is in the linear subspace <span class="math notranslate nohighlight">\(\mathcal{W}_{2}\)</span>, it can be written as <span class="math notranslate nohighlight">\(\mathbf{u} = \sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle \mathbf{v}_i\)</span> (since <span class="math notranslate nohighlight">\(\mathbf{v}_{d-1},\mathbf{v}_{d}\)</span> forms an orthonormal basis of it) and it follows that</p>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, \mathbf{u} \rangle
= \sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle^2
\]</div>
<div class="math notranslate nohighlight">
\[
\langle \mathbf{u}, A \mathbf{u} \rangle
= \left\langle \mathbf{u}, \sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle \lambda_i \mathbf{v}_i \right\rangle
= \sum_{i=d-1}^{d} \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2.
\]</div>
<p>Thus,</p>
<div class="math notranslate nohighlight">
\[
\mathcal{R}_A(\mathbf{u})
= \frac{\langle \mathbf{u}, A \mathbf{u} \rangle}{\langle \mathbf{u}, \mathbf{u} \rangle}
= \frac{\sum_{i=d-1}^{d} \lambda_i \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
\leq \lambda_{d-1} \frac{\sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}{\sum_{i=d-1}^{d} \langle \mathbf{u}, \mathbf{v}_i \rangle^2}
= \lambda_{d-1},
\]</div>
<p>where we used <span class="math notranslate nohighlight">\(\lambda_{d-1} \geq \lambda_{d}\)</span> and the fact that <span class="math notranslate nohighlight">\(\langle \mathbf{u}, \mathbf{v}_i \rangle^2 \geq 0\)</span>. Moreover <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}_{d-1}) = \lambda_{d-1}\)</span>. So we have established</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-1} 
= \max_{\mathbf{0} \neq \mathbf{u} \in \mathcal{W}_{2}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p><strong>THEOREM</strong> <strong>(Courant-Fischer)</strong> <span class="math notranslate nohighlight">\(\idx{Courant-Fischer theorem}\xdi\)</span> Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span> be a symmetric matrix with spectral decomposition <span class="math notranslate nohighlight">\(A = \sum_{i=1}^d \lambda_i \mathbf{v}_i \mathbf{v}_i^T\)</span> where <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span>. For each <span class="math notranslate nohighlight">\(k = 1,\ldots,d\)</span>, define the subspace</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V}_k = \mathrm{span}(\mathbf{v}_1, \ldots,  \mathbf{v}_k)
\quad\text{and}\quad
\mathcal{W}_{d-k+1} = \mathrm{span}(\mathbf{v}_k, \ldots,  \mathbf{v}_d).
\]</div>
<p>Then, for all <span class="math notranslate nohighlight">\(k = 1,\ldots,d\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\lambda_k 
= \min_{\mathbf{u} \in \mathcal{V}_k} \mathcal{R}_A(\mathbf{u})
= \max_{\mathbf{u} \in \mathcal{W}_{d-k+1}} \mathcal{R}_A(\mathbf{u}),
\]</div>
<p>which are referred to as the local formulas. Furthermore we have the following min-max (or global) formulas, which do not depend on the choice of a spectral decomposition: for all <span class="math notranslate nohighlight">\(k = 1,\ldots,d\)</span>,</p>
<div class="math notranslate nohighlight">
\[
\lambda_k 
= \max_{\mathrm{dim}(\mathcal{V}) = k} \min_{\mathbf{u} \in \mathcal{V}} \mathcal{R}_A(\mathbf{u})
= \min_{\mathrm{dim}(\mathcal{W}) = d-k+1} \max_{\mathbf{u} \in \mathcal{W}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p><span class="math notranslate nohighlight">\(\sharp\)</span></p>
<p><em>Proof idea:</em> For the local formula, we expand a vector in <span class="math notranslate nohighlight">\(\mathcal{V}_k\)</span> into the basis <span class="math notranslate nohighlight">\(\mathbf{v}_1,\ldots,\mathbf{v}_k\)</span> and use the fact that <span class="math notranslate nohighlight">\(\mathcal{R}_A(\mathbf{v}_i) = \lambda_i\)</span> and that eigenvalues are in non-increasing order. The global formulas follow from a dimension argument.</p>
<p><strong>EXAMPLE:</strong> <strong>(Third Smallest Eigenvalue)</strong> The <em>Courant-Fischer theorem</em> can be used to recover the special cases of the previous subsection. One also gets new cases of interest. We give a characterization of the third smallest eigenvalue next. Let <span class="math notranslate nohighlight">\(A \in \mathbb{R}^{d \times d}\)</span> be a symmetric matrix with spectral decomposition <span class="math notranslate nohighlight">\(A = \sum_{i=1}^d \lambda_i \mathbf{v}_i \mathbf{v}_i^T\)</span> where <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span>. Using <span class="math notranslate nohighlight">\(k=d-2\)</span> in the <em>Courant-Fischer theorem</em> gives</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-2} 
= \min_{\mathbf{u} \in \mathcal{V}_{d-2}} \mathcal{R}_A(\mathbf{u}),
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathcal{V}_{d-2} = \mathrm{span}(\mathbf{v}_1, \ldots,  \mathbf{v}_{d-2}).
\]</div>
<p>It can be seen that</p>
<div class="math notranslate nohighlight">
\[
\mathrm{span}(\mathbf{v}_1, \ldots,  \mathbf{v}_{d-2})
= \mathrm{span}(\mathbf{v}_{d-1}, \mathbf{v}_{d})^\perp.
\]</div>
<p>So we finally arrive at the following characterization of the third smallest eigenvalue</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-2} 
= \min\{\mathcal{R}_A(\mathbf{u})\,:\, \mathbf{0} \neq \mathbf{u}, \langle \mathbf{u},\mathbf{v}_d\rangle = 0, 
\langle \mathbf{u},\mathbf{v}_{d-1}\rangle = 0\}.
\]</div>
<p>Using the same argument as we used for <span class="math notranslate nohighlight">\(\lambda_{d-1}\)</span>, we get also</p>
<div class="math notranslate nohighlight">
\[
\lambda_{d-2} 
= \min\left\{\langle \mathbf{z}, A \mathbf{z}\rangle\,:\ \|\mathbf{z}\|=1, \langle \mathbf{z}, \mathbf{v}_d\rangle = 0, \langle \mathbf{z}, \mathbf{v}_{d-1}\rangle = 0 \right\}.
\]</div>
<p><span class="math notranslate nohighlight">\(\lhd\)</span></p>
<p>We now give a proof of the <em>Courant-Fischer theorem</em>. The local formulas follows from the same argument used to derive the special cases above so omit the general proof (but try to prove it!). The global formulas require a new idea.</p>
<p><em>Proof:</em> <em>(Courant-Fischer)</em> <span class="math notranslate nohighlight">\(\idx{Courant-Fischer theorem}\xdi\)</span> Since <span class="math notranslate nohighlight">\(\mathcal{V}_k\)</span> has dimension <span class="math notranslate nohighlight">\(k\)</span>, it follows from the local formula that</p>
<div class="math notranslate nohighlight">
\[
\lambda_k 
= \min_{\mathbf{u} \in \mathcal{V}_k} \mathcal{R}_A(\mathbf{u})
\leq \max_{\mathrm{dim}(\mathcal{V}) = k} \min_{\mathbf{u} \in \mathcal{V}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{V}\)</span> be any subspace with dimension <span class="math notranslate nohighlight">\(k\)</span>. Because <span class="math notranslate nohighlight">\(\mathcal{W}_{d-k+1}\)</span> has dimension <span class="math notranslate nohighlight">\(d - k + 1\)</span>, we have that <span class="math notranslate nohighlight">\(\dim(\mathcal{V}) + \mathrm{dim}(\mathcal{W}_{d-k+1}) &gt; d\)</span> and there must be non-zero vector <span class="math notranslate nohighlight">\(\mathbf{u}_0\)</span> in the intersection <span class="math notranslate nohighlight">\(\mathcal{V} \cap \mathcal{W}_{d-k+1}\)</span> (Prove it!).</p>
<p>We then have by the other local formula that</p>
<div class="math notranslate nohighlight">
\[
\lambda_k 
= \max_{\mathbf{u} \in \mathcal{W}_{d-k+1}} \mathcal{R}_A(\mathbf{u})
\geq \mathcal{R}_A(\mathbf{u}_0)
\geq \min_{\mathbf{u} \in \mathcal{V}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p>Since this inequality holds for any subspace of dimension <span class="math notranslate nohighlight">\(k\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[
\lambda_k 
\geq \max_{\mathrm{dim}(\mathcal{V}) = k} \min_{\mathbf{u} \in \mathcal{V}} \mathcal{R}_A(\mathbf{u}).
\]</div>
<p>Combining with the inequality in the other direction above gives the claim. The other global formula is proved similarly. <span class="math notranslate nohighlight">\(\square\)</span></p>
<p><em><strong>Self-assessment quiz</strong></em> <em>(with help from Claude, Gemini, and ChatGPT)</em></p>
<p><strong>1</strong> According to the variational characterization of the largest eigenvalue <span class="math notranslate nohighlight">\(\lambda_1\)</span> of a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span>, which of the following is true?</p>
<p>a) <span class="math notranslate nohighlight">\(\lambda_1 = \min_{\mathbf{u} \neq 0} R_A(\mathbf{u})\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\lambda_1 = \max_{\mathbf{u} \neq 0} R_A(\mathbf{u})\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\lambda_1 = \min_{\|\mathbf{u}\| = 0} R_A(\mathbf{u})\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\lambda_1 = \max_{\|\mathbf{u}\| = 0} R_A(\mathbf{u})\)</span></p>
<p><strong>2</strong> Let <span class="math notranslate nohighlight">\(\mathcal{V}_{d-1} = \mathrm{span}(\mathbf{v}_1, \ldots, \mathbf{v}_{d-1})\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{v}_1, \ldots, \mathbf{v}_d\)</span> are the eigenvectors of a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span> with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span>. Which of the following characterizes the second smallest eigenvalue <span class="math notranslate nohighlight">\(\lambda_{d-1}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \min_{0 \neq \mathbf{u} \in \mathcal{V}_{d-1}} R_A(\mathbf{u})\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \max_{0 \neq \mathbf{u} \in \mathcal{V}_{d-1}} R_A(\mathbf{u})\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \min_{\|\mathbf{u}\| = 0, \mathbf{u} \in \mathcal{V}_{d-1}} R_A(\mathbf{u})\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \max_{\|\mathbf{u}\| = 0, \mathbf{u} \in \mathcal{V}_{d-1}} R_A(\mathbf{u})\)</span></p>
<p><strong>3</strong> Let <span class="math notranslate nohighlight">\(\mathcal{W}_2 = \mathrm{span}(\mathbf{v}_{d-1}, \mathbf{v}_d)\)</span>, where <span class="math notranslate nohighlight">\(\mathbf{v}_1, \ldots, \mathbf{v}_d\)</span> are the eigenvectors of a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span> with eigenvalues <span class="math notranslate nohighlight">\(\lambda_1 \geq \cdots \geq \lambda_d\)</span>. Which of the following characterizes the second smallest eigenvalue <span class="math notranslate nohighlight">\(\lambda_{d-1}\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \min_{0 \neq \mathbf{u} \in \mathcal{W}_2} R_A(\mathbf{u})\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \max_{0 \neq \mathbf{u} \in \mathcal{W}_2} R_A(\mathbf{u})\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \min_{\|\mathbf{u}\| = 0, \mathbf{u} \in \mathcal{W}_2} R_A(\mathbf{u})\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\lambda_{d-1} = \max_{\|\mathbf{u}\| = 0, \mathbf{u} \in \mathcal{W}_2} R_A(\mathbf{u})\)</span></p>
<p><strong>4</strong> According to the <em>Courant-Fischer Theorem</em>, which of the following is the global formula for the <span class="math notranslate nohighlight">\(k\)</span>-th eigenvalue <span class="math notranslate nohighlight">\(\lambda_k\)</span> of a symmetric matrix <span class="math notranslate nohighlight">\(A\)</span>?</p>
<p>a) <span class="math notranslate nohighlight">\(\lambda_k = \min_{\mathbf{u} \in \mathcal{V}_k} R_A(\mathbf{u}) = \max_{\mathbf{u} \in \mathcal{W}_{d-k+1}} R_A(\mathbf{u})\)</span></p>
<p>b) <span class="math notranslate nohighlight">\(\lambda_k = \max_{\mathbf{u} \in \mathcal{V}_k} R_A(\mathbf{u}) = \min_{\mathbf{u} \in \mathcal{W}_{d-k+1}} R_A(\mathbf{u})\)</span></p>
<p>c) <span class="math notranslate nohighlight">\(\lambda_k = \max_{\mathrm{dim}(\mathcal{V}) = k} \min_{\mathbf{u} \in \mathcal{V}} R_A(\mathbf{u}) = \min_{\mathrm{dim}(\mathcal{W}) = d-k+1} \max_{\mathbf{u} \in \mathcal{W}} R_A(\mathbf{u})\)</span></p>
<p>d) <span class="math notranslate nohighlight">\(\lambda_k = \max_{\mathrm{dim}(\mathcal{V}) = k} \max_{\mathbf{u} \in \mathcal{V}} R_A(\mathbf{u}) = \min_{\mathrm{dim}(\mathcal{W}) = d-k+1} \min_{\mathbf{u} \in \mathcal{W}} R_A(\mathbf{u})\)</span></p>
<p><strong>5</strong> What is the main difference between the local and global formulas in the <em>Courant-Fischer Theorem</em>?</p>
<p>a) The local formulas are easier to compute than the global formulas.</p>
<p>b) The local formulas depend on a specific choice of eigenvectors, while the global formulas do not.</p>
<p>c) The local formulas apply only to symmetric matrices, while the global formulas apply to any matrix.</p>
<p>d) The local formulas provide upper bounds on the eigenvalues, while the global formulas provide lower bounds.</p>
<p>Answer for 1: b. Justification: The text establishes that <span class="math notranslate nohighlight">\(\lambda_1 = \max_{\mathbf{u} \neq 0} R_A(\mathbf{u})\)</span>.</p>
<p>Answer for 2: a. Justification: The text establishes that <span class="math notranslate nohighlight">\(\lambda_{d-1} = \min_{0 \neq \mathbf{u} \in \mathcal{V}_{d-1}} R_A(\mathbf{u})\)</span>.</p>
<p>Answer for 3: b. Justification: The text establishes that <span class="math notranslate nohighlight">\(\lambda_{d-1} = \max_{0 \neq \mathbf{u} \in \mathcal{W}_2} R_A(\mathbf{u})\)</span>.</p>
<p>Answer for 4: c. Justification: The <em>Courant-Fischer Theorem</em> states that the global formula for the <span class="math notranslate nohighlight">\(k\)</span>-th eigenvalue is <span class="math notranslate nohighlight">\(\lambda_k = \max_{\mathrm{dim}(\mathcal{V}) = k} \min_{\mathbf{u} \in \mathcal{V}} R_A(\mathbf{u}) = \min_{\mathrm{dim}(\mathcal{W}) = d-k+1} \max_{\mathbf{u} \in \mathcal{W}} R_A(\mathbf{u})\)</span>.</p>
<p>Answer for 5: b. Justification: The text highlights that the global formulas “do not depend on the choice of spectral decomposition,” unlike the local formulas, which rely on a specific set of eigenvectors.</p>
    
</body>
</html>