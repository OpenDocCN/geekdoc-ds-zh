- en: Chapter 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章
- en: Understanding Parallelism with GPUs
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解GPU的并行性
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: This chapter aims to provide a broad introduction to the concepts of parallel
    programming and how these relate to GPU technology. It’s primarily aimed at those
    people reading this text with a background in serial programming, but a lack of
    familiarity with parallel processing concepts. We look at these concepts in the
    primary context of GPUs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在提供并行编程概念的广泛介绍，并说明这些概念如何与GPU技术相关。它主要面向那些具有串行编程背景，但对并行处理概念不熟悉的读者。我们将在GPU的主要背景下讨论这些概念。
- en: Traditional Serial Code
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 传统串行代码
- en: A significant number of programmers graduated when serial programs dominated
    the landscape and parallel programming attracted just a handful of enthusiasts.
    Most people who go to university get a degree related to IT because they are interested
    in technology. However, they also appreciate they need to have a job or career
    that pays a reasonable salary. Thus, in specializing, at least some consideration
    is given to the likely availability of positions after university. With the exception
    of research or academic posts, the number of commercial roles in parallel programming
    has always been, at best, small. Most programmers developed applications in a
    simple serial fashion based broadly on how universities taught them to program,
    which in turn was driven by market demand.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 很多程序员是在串行程序主导的时代毕业的，当时并行编程只吸引了一小部分爱好者。大多数上大学的人选择与IT相关的专业，因为他们对技术感兴趣。然而，他们也意识到自己需要一份能支付合理薪水的工作或职业。因此，在专业化时，至少会考虑大学毕业后可能获得职位的情况。除了研究或学术职位外，并行编程的商业岗位数量一直很少，最多只能算是少数。大多数程序员开发应用程序时采用简单的串行方式，基本上是根据大学教授他们的编程方式来进行的，而这种方式又是由市场需求推动的。
- en: The landscape of parallel programming is scattered, with many technologies and
    languages that never quite made it to the mainstream. There was never really the
    large-scale market need for parallel hardware and, as a consequence, significant
    numbers of parallel programmers. Every year or two the various CPU vendors would
    bring out a new processor generation that executed code faster than the previous
    generation, thereby perpetuating serial code.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 并行编程的格局是支离破碎的，存在着许多从未真正进入主流的技术和语言。并行硬件从未真正存在大规模市场需求，因此也就没有大量的并行程序员。每年或两年，CPU厂商都会推出比前一代执行速度更快的新处理器，继而推动了串行代码的延续。
- en: Parallel programs by comparison were often linked closely to the hardware. Their
    goal was to achieve faster performance and often that was at the cost of portability.
    Feature X was implemented differently, or was not available in the next generation
    of parallel hardware. Periodically a revolutionary new architecture would appear
    that required a complete rewrite of all code. If your knowledge as a programmer
    was centered around processor X, it was valuable in the marketplace only so long
    as processor X was in use. Therefore, it made a lot more commercial sense to learn
    to program x86-type architecture than some exotic parallel architecture that would
    only be around for a few years.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，并行程序通常与硬件紧密相关。它们的目标是实现更快的性能，而这种性能往往是以可移植性为代价的。特性X的实现方式可能不同，或者在下一代并行硬件中根本不存在。周期性地，某种革命性的新架构出现，需要彻底重写所有代码。如果你作为程序员的知识集中在处理器X上，那么这份知识在市场上的价值仅限于处理器X还在使用的时候。因此，学习编程x86架构比学习某种仅在短短几年内存在的异国并行架构更具商业意义。
- en: However, over this time, a couple of standards did evolve that we still have
    today. The OpenMP standard addresses parallelism within a single node and is designed
    for shared memory machines that contain multicore processors. It does not have
    any concept of anything outside a single node or box. Thus, you are limited to
    problems that fit within a single box in terms of processing power, memory capacity,
    and storage space. Programming, however, is relatively easy as most of the low-level
    threading code (otherwise written using Windows threads or POSIX threads) is taken
    care of for you by OpenMP.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这一过程中，还是有一些标准逐渐发展出来，直到今天我们仍然在使用。OpenMP标准处理单个节点内的并行性，并且设计用于包含多核处理器的共享内存机器。它没有涉及单个节点或盒子外部的任何概念。因此，你受到处理能力、内存容量和存储空间方面的限制，只能解决适合在单个盒子中运行的问题。然而，编程相对容易，因为大多数底层线程代码（如果用Windows线程或POSIX线程编写）都由OpenMP为你处理了。
- en: The MPI (Message Passing Interface) standard addresses parallelism between nodes
    and is aimed at clusters of machines within well-defined networks. It is often
    used in supercomputer installations where there may be many thousands of individual
    nodes. Each node holds a small section of the problem. Thus, common resources
    (CPU, cache, memory, storage, etc.) are multiplied by the number of nodes in the
    network. The Achilles’ heel of any network is the various interconnects, the parts
    that connect the networked machines together. Internode communication is usually
    the dominating factor determining the maximum speed in any cluster-based solution.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: MPI（消息传递接口）标准解决了节点之间的并行性，旨在处理位于明确网络中的计算机集群。它通常用于超级计算机的安装中，可能涉及成千上万的独立节点。每个节点保存问题的一小部分。因此，常见的资源（CPU、缓存、内存、存储等）是通过网络中的节点数来倍增的。任何网络的“阿基里斯之踵”是各种互连部分，即将网络中的机器连接在一起的部分。节点间的通信通常是决定任何基于集群的解决方案最大速度的主要因素。
- en: Both OpenMP and MPI can be used together to exploit parallelism within nodes
    as well as across a network of machines. However, the APIs and the approaches
    used are entirely different, meaning they are often not used together. The OpenMP
    directives allow the programmer to take a high-level view of parallelism via specifying
    parallel regions. MPI by contrast uses an explicit interprocess communication
    model making the programmer do a lot more work.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP 和 MPI 可以一起使用，以便在节点内部以及跨机器网络利用并行性。然而，它们的 API 和使用的方法完全不同，这意味着它们通常不会一起使用。OpenMP
    指令允许程序员通过指定并行区域来以高层次的视角看待并行性。相比之下，MPI 使用显式的进程间通信模型，需要程序员做更多的工作。
- en: Having invested the time to become familiar with one API, programmers are often
    loathe to learn another. Thus, problems that fit within one computer are often
    implemented with OpenMP solutions, whereas really large problems are implemented
    with cluster-based solutions such as MPI.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在花时间熟悉一种 API 后，程序员通常不愿学习另一种。因此，适用于单台计算机的问题通常会采用 OpenMP 解决方案，而真正大规模的问题则采用基于集群的解决方案，如
    MPI。
- en: CUDA, the GPU programming language we’ll explore in this text, can be used in
    conjunction with both OpenMP and MPI. There is also an OpenMP-like directive version
    of CUDA (OpenACC) that may be somewhat easier for those familiar with OpenMP to
    pick up. OpenMP, MPI, and CUDA are increasingly taught at undergraduate and graduate
    levels in many university computer courses.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 是我们在本文中将要探索的 GPU 编程语言，可以与 OpenMP 和 MPI 一起使用。CUDA 还有一个类似 OpenMP 的指令版本（OpenACC），对于熟悉
    OpenMP 的人来说，可能更容易上手。OpenMP、MPI 和 CUDA 正在许多大学的计算机课程中逐渐成为本科生和研究生的教学内容。
- en: However, the first experience most serial programmers had with parallel programming
    was the introduction of multicore CPUs. These, like the parallel environments
    before them, were largely ignored by all but a few enthusiasts. The primary use
    of multicore CPUs was for OS-based parallelism. This is a model based on *task
    parallelism* that we’ll look at a little later.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数串行程序员首次接触并行编程时，是在多核 CPU 的引入下。像之前的并行环境一样，这些 CPU 也被大多数人忽视，只有少数热衷者在关注。多核
    CPU 的主要用途是基于操作系统的并行性。这是一个基于*任务并行性*的模型，我们稍后会详细讨论。
- en: As it became obvious that technology was marching toward the multicore route,
    more and more programmers started to take notice of the multicore era. Almost
    all desktops ship today with either a dual- or quad-core processor. Thus, programmers
    started using threads to allow the multiple cores on the CPU to be exploited.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着技术明显朝着多核路线发展，越来越多的程序员开始关注多核时代。几乎所有桌面计算机现在都配备了双核或四核处理器。因此，程序员开始使用线程来利用 CPU
    上的多个核心。
- en: A thread is a separate execution flow within a program that may diverge and
    converge as and when required with the main execution flow. Typically, CPU programs
    will have no more than twice the number of threads active than the number of physical
    processor cores. As with single-core processors, typically each OS task is time-sliced,
    given a small amount of time in turn, to give the illusion of running more tasks
    than there are physical CPU cores.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 线程是程序中一个独立的执行流，可以根据需要与主执行流分开或合并。通常，CPU 程序的线程数量不会超过物理处理器核心数量的两倍。与单核处理器类似，通常每个操作系统任务会被时间分片，轮流分配一个小的时间片，从而给人一种运行更多任务的假象，尽管物理
    CPU 核心数量有限。
- en: However, as the number of threads grows, this becomes more obvious to the end
    user. In the background the OS is having to context switch (swap in and out a
    set of registers) every time it needs to switch between tasks. As context switching
    is an expensive operation, typically thousands of cycles, CPU applications tend
    to have a fairly low number of threads compared with GPUs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着线程数量的增加，这个问题变得更加明显。后台操作系统每次需要在任务之间切换时，都必须进行上下文切换（即交换一组寄存器）。由于上下文切换是一项昂贵的操作，通常需要几千个周期，CPU应用程序的线程数量通常相较于GPU较少。
- en: Serial/Parallel Problems
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 串行/并行问题
- en: Threads brought with them many of the issues of parallel programming, such as
    sharing resources. Typically, this is done with a semaphore, which is simply a
    lock or token. Whoever has the token can use the resource and everyone else has
    to wait for the user of the token to release it. As long as there is only a single
    token, everything works fine.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 线程带来了许多并行编程的问题，例如资源共享。通常，这通过信号量来完成，信号量其实就是一个锁或令牌。拥有令牌的人可以使用资源，其他人则必须等待令牌持有者释放令牌。只要只有一个令牌，一切都能正常工作。
- en: Problems occur when there are two or more tokens that must be shared by the
    same threads. In such situations, thread 0 grabs token 0, while thread 1 grabs
    token 1\. Thread 0 now tries to grab token 1, while thread 1 tries to grab token
    0\. As the tokens are unavailable, both thread 0 and thread 1 sleep until the
    token becomes available. As neither thread ever releases the one token they already
    own, all threads wait forever. This is known as a deadlock, and it is something
    that can and will happen without proper design.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当有两个或更多令牌必须由相同线程共享时，问题就会出现。在这种情况下，线程0获取令牌0，而线程1获取令牌1。然后，线程0尝试获取令牌1，而线程1尝试获取令牌0。由于令牌不可用，线程0和线程1都会进入休眠状态，直到令牌变得可用。由于两个线程都没有释放自己已经拥有的令牌，所有线程会永远等待下去。这就是死锁，它是在没有正确设计的情况下可以且会发生的。
- en: The opposite also happens—sharing of resources by chance. With any sort of locking
    system, all parties to a resource must behave correctly. That is, they must request
    the token, wait if necessary, and, only when they have the token, perform the
    operation. This relies on the programmer to identify shared resources and specifically
    put in place mechanisms to coordinate updates by multiple threads. However, there
    are usually several programmers in any given team. If just one of them doesn’t
    follow this convention, or simply does not know this is a shared resource, you
    may appear to have a working program, but only by chance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，也会发生资源共享的偶然情况。在任何锁定系统中，所有参与资源访问的方都必须正确行为。也就是说，它们必须请求令牌，必要时等待，并且只有在获得令牌后，才能执行操作。这依赖于程序员识别共享资源，并专门制定机制来协调多个线程的更新。然而，任何团队中通常都有多个程序员。如果其中一个程序员没有遵循这个惯例，或者根本不知道这是一个共享资源，那么你可能看似有一个正常工作的程序，但这只是偶然的情况。
- en: One of the projects I worked on for a large company had exactly this problem.
    All threads requested a lock, waited, and updated the shared resource. Everything
    worked fine and the particular code passed quality assurance and all tests. However,
    in the field occasionally users would report the value of a certain field being
    reset to 0, seemingly randomly. Random bugs are always terrible to track down,
    because being able to consistently reproduce a problem is often the starting point
    of tracking down the error.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾为一家大公司工作时参与的一个项目就遇到了这个问题。所有线程请求锁、等待并更新共享资源。一切都正常，该代码通过了质量保证和所有测试。然而，在实际使用中，偶尔会有用户报告某个字段的值被重置为0，似乎是随机的。随机的bug总是很难追踪，因为能否一致地重现问题通常是追踪错误的起点。
- en: An intern who happened to be working for the company actually found the issue.
    In a completely unrelated section of the code a pointer was not initialized under
    certain conditions. Due to the way the program ran, some of the time, depending
    on the thread execution order, the pointer would point to our protected data.
    The other code would then initialize “its variable” by writing 0 to the pointer,
    thus eliminating the contents of our “protected” and thread-shared parameter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一名恰好在公司实习的员工发现了这个问题。在代码中一个完全无关的部分，在某些条件下指针未初始化。由于程序运行的方式，根据线程执行顺序的不同，有时候指针会指向我们受保护的数据。然后，其他代码会通过将0写入指针来初始化“它的变量”，从而消除了我们“受保护”的线程共享参数的内容。
- en: This is one of the unfortunate areas of thread-based operations; they operate
    with a shared memory space. This can be both an advantage in terms of not having
    to formally exchange data via messages, and a disadvantage in the lack of protection
    of shared data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于线程的操作中不幸的一个方面；它们在共享内存空间中运行。这在不需要通过消息交换数据的情况下是一种优势，但在缺乏对共享数据的保护方面也是一种劣势。
- en: The alternative to threads is processes. These are somewhat heavier in terms
    of OS load in that both code *and* data contexts must be maintained by the OS.
    A thread by contrast needs to only maintain a code context (the program/instruction
    counter plus a set of registers) and shares the same data space. Both threads
    and processes may be executing entirely different sections of a program at any
    point in time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 线程的替代方案是进程。就操作系统负载而言，进程相对较重，因为操作系统必须维护代码*和*数据上下文。相比之下，线程只需要维护代码上下文（程序/指令计数器加上一组寄存器），并且共享相同的数据空间。无论何时，线程和进程都可能在执行程序的完全不同部分。
- en: Processes by default operate in an independent memory area. This usually is
    enough to ensure one process is unable to affect the data of other processes.
    Thus, the stray pointer issue should result in an exception for out-of-bounds
    memory access, or at the very least localize the bug to the particular process.
    Data consequently has to be transferred by formally passing messages to or from
    processes.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 进程默认在独立的内存区域中运行。通常，这足以确保一个进程无法影响其他进程的数据。因此，指针错误问题应该导致越界内存访问异常，或者至少将错误局限于特定进程。数据因此必须通过正式的消息传递机制进行进程间的传递。
- en: In many respects the threading model sits well with OpenMP, while the process
    model sits well with MPI. In terms of GPUs, they map to a hybrid of both approaches.
    CUDA uses a grid of blocks. This can be thought of as a queue (or a grid) of processes
    (blocks) with no interprocess communication. Within each block there are many
    threads which operate cooperatively in batches called *warps*. We will look at
    this further in the coming chapters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，线程模型与OpenMP非常契合，而进程模型则与MPI契合。就GPU而言，它们映射到两种方法的混合体。CUDA使用块的网格。可以将其视为一个没有进程间通信的进程（块）队列（或网格）。在每个块内，有许多线程以协作方式批量执行，称为*warp*。我们将在接下来的章节中进一步讨论这一点。
- en: Concurrency
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并发
- en: The first aspect of concurrency is to think about the particular problem, without
    regard for any implementation, and consider what aspects of it could run in parallel.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 并发的第一个方面是考虑特定问题，不考虑任何实现方式，并思考其中哪些方面可以并行运行。
- en: If possible, try to think of a formula that represents each output point as
    some function of the input data. This may be too cumbersome for some algorithms,
    for example, those that iterate over a large number of steps. For these, consider
    each step or iteration individually. Can the data points for the step be represented
    as a transformation of the input dataset? If so, then you simply have a set of
    kernels (steps) that run in sequence. These can simply be pushed into a queue
    (or stream) that the hardware will schedule sequentially.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能，尽量考虑一个公式，表示每个输出点是输入数据的某种函数。这对于某些算法来说可能过于繁琐，例如那些迭代大量步骤的算法。对于这些算法，考虑逐步或逐次的迭代。每一步的数据点能否表示为输入数据集的转换？如果可以，那么你就有了一组按顺序执行的内核（步骤）。这些可以简单地推送到队列（或流）中，由硬件顺序调度。
- en: A significant number of problems are known as “embarrassingly parallel,” a term
    that rather underplays what is being achieved. If you can construct a formula
    where the output data points can be represented without relation to each other—for
    example, a matrix multiplication—be very happy. These types of problems can be
    implemented extremely well on GPUs and are easy to code.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 很多问题被称为“令人尴尬的并行问题”，这个词语有些低估了所实现的内容。如果你能构造出一个公式，使得输出数据点之间没有相互关系——例如，矩阵乘法——那就非常幸运。这类问题可以在GPU上实现得非常好，并且容易编程。
- en: If one or more steps of the algorithm can be represented in this way, but maybe
    one stage cannot, also be very happy. This single stage may turn out to be a bottleneck
    and may require a little thought, but the rest of the problem will usually be
    quite easy to code on a GPU.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果算法的某些步骤可以以这种方式表示，但某个阶段可能无法表示，那也没关系。这一阶段可能会成为瓶颈，并可能需要一些思考，但问题的其余部分通常可以很容易地在GPU上编程实现。
- en: If the problem requires every data point to know about the value of its surrounding
    neighbors then the speedup will ultimately be limited. In such cases, throwing
    more processors at the problem works up to a point. At this point the computation
    slows down due to the processors (or threads) spending more time sharing data
    than doing any useful work. The point at which you hit this will depend largely
    on the amount and cost of the communication overhead.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问题要求每个数据点都知道其周围邻居的值，那么加速最终会受到限制。在这种情况下，向问题投放更多的处理器会在一定程度上有效，但最终，计算会因为处理器（或线程）花更多时间共享数据而不是做有用的工作而变慢。达到这一点的时刻，很大程度上取决于通信开销的数量和成本。
- en: CUDA is ideal for an embarrassingly parallel problem, where little or no interthread
    or interblock communication is required. It supports interthread communication
    with explicit primitives using on-chip resources. Interblock communication is,
    however, only supported by invoking multiple kernels in series, communicating
    between kernel runs using off-chip global memory. It can also be performed in
    a somewhat restricted way through atomic operations to or from global memory.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 非常适合于典型的并行问题，其中几乎不需要线程间或块间的通信。它支持使用片上资源进行显式原语的线程间通信。然而，块间通信仅通过串行调用多个内核来支持，通过内核运行之间使用片外全局内存进行通信。它也可以通过原子操作以某种受限的方式在全局内存中执行。
- en: CUDA splits problems into grids of blocks, each containing multiple threads.
    The blocks may run in any order. Only a subset of the blocks will ever execute
    at any one point in time. A block must execute from start to completion and may
    be run on one of *N* SMs (symmetrical multiprocessors). Blocks are allocated from
    the grid of blocks to any SM that has free slots. Initially this is done on a
    round-robin basis so each SM gets an equal distribution of blocks. For most kernels,
    the number of blocks needs to be in the order of eight or more times the number
    of physical SMs on the GPU.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 将问题拆分成多个块的网格，每个块包含多个线程。块可以以任何顺序运行。在任何时刻，只有一部分块会执行。一个块必须从开始到完成地执行，并且可能会在
    *N* 个 SMs（对称多处理器）之一上运行。块从块的网格中分配给任何有空闲槽的 SM。最初，这通过轮询的方式进行分配，以确保每个 SM 得到相等的块分配。对于大多数内核，块的数量通常需要是
    GPU 上物理 SM 数量的八倍或更多。
- en: To use a military analogy, we have an army (a grid) of soldiers (threads). The
    army is split into a number of units (blocks), each commanded by a lieutenant.
    The unit is split into squads of 32 soldiers (a warp), each commanded by a sergeant
    (See [Figure 2.1](#F0010)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 用军事类比来说，我们有一支军队（网格）由士兵（线程）组成。军队被分成若干个单位（块），每个单位由一名上尉指挥。单位又分为 32 名士兵一组的小队（warp），每个小队由一名中士指挥（见
    [图 2.1](#F0010)）。
- en: '![image](../images/F000028f02-01-9780124159334.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000028f02-01-9780124159334.jpg)'
- en: Figure 2.1 GPU-based view of threads.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1 基于 GPU 的线程视图。
- en: To perform some action, central command (the kernel/host program) must provide
    some action plus some data. Each soldier (thread) works on his or her individual
    part of the problem. Threads may from time to time swap data with one another
    under the coordination of either the sergeant (the warp) or the lieutenant (the
    block). However, any coordination with other units (blocks) has to be performed
    by central command (the kernel/host program).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行某个操作，中央指挥（内核/主机程序）必须提供某些操作以及一些数据。每个士兵（线程）在自己负责的部分上工作。线程可能会不时在中士（warp）或上尉（block）的协调下交换数据。然而，与其他单元（块）的任何协调必须由中央指挥（内核/主机程序）来执行。
- en: Thus, it’s necessary to think of orchestrating thousands of threads in this
    very hierarchical manner when you think about how a CUDA program will implement
    concurrency. This may sound quite complex at first. However, for most embarrassingly
    parallel programs it’s just a case of thinking of one thread generating a single
    output data point. A typical GPU has on the order of 24 K *active* threads. On
    Fermi GPUs you can define 65,535 × 65,535 × 1536 threads in total, 24 K of which
    are active at any time. This is usually enough to cover most problems within a
    single node.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当你考虑 CUDA 程序如何实现并发时，必须以这种非常分层的方式来考虑如何协调成千上万的线程。这听起来一开始可能相当复杂。然而，对于大多数典型的并行程序来说，只需要考虑一个线程生成一个单一的输出数据点。一个典型的
    GPU 有大约 24 K *活跃*线程。在 Fermi GPU 上，你最多可以定义 65,535 × 65,535 × 1536 个线程，其中 24 K 个线程在任何时刻都是活跃的。这通常足以覆盖单个节点内的大多数问题。
- en: Locality
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局部性
- en: Computing has, over the last decade or so, moved from one limited by computational
    throughput of the processor, to one where moving the data is the primary limiting
    factor. When designing a processor in terms of processor real estate, compute
    units (or ALUs—algorithmic logic units) are cheap. They can run at high speed,
    and consume little power and physical die space. However, ALUs are of little use
    without operands. Considerable amounts of power and time are consumed in moving
    the operands to and from these functional units.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 近十年来，计算机技术的发展已经从以处理器计算吞吐量为限制，转变为数据传输成为主要限制因素。在设计处理器时，计算单元（或ALU—算法逻辑单元）是便宜的。它们可以高速运行，消耗的功率和物理芯片空间很小。然而，没有操作数，ALU几乎没有用。大量的能量和时间被用于在这些功能单元之间移动操作数。
- en: In modern computer designs this is addressed by the use of multilevel caches.
    Caches work on the principle of either spatial (close in the address space) or
    temporal (close in time) locality. Thus, data that has been accessed before, will
    likely be accessed again (temporal locality), and data that is close to the last
    accessed data will likely be accessed in the future (spatial locality).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代计算机设计中，这个问题通过使用多级缓存来解决。缓存的工作原理基于空间局部性（地址空间接近）或时间局部性（时间接近）。因此，之前访问过的数据可能会再次被访问（时间局部性），而与最后一次访问的数据接近的数据，未来也可能会被访问（空间局部性）。
- en: Caches work well where the task is repeated many times. Consider for the moment
    a tradesperson, a plumber with a toolbox (a cache) that can hold four tools. A
    number of the jobs he will attend are similar, so the same four tools are repeatedly
    used (a cache hit).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存在任务重复多次的情况下效果很好。暂时考虑一下一个工匠，一个带有工具箱（缓存）的管道工，工具箱可以容纳四个工具。他将参与的一些工作是相似的，所以这四个工具会被反复使用（缓存命中）。
- en: However, a significant number of jobs require additional tools. If the tradesperson
    does not know in advance what the job will entail, he arrives and starts work.
    Partway through the job he needs an additional tool. As it’s not in his toolbox
    (L1 cache), he retrieves the item from the van (L2 cache).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，许多工作需要额外的工具。如果工匠无法提前知道工作内容，他到达后就开始工作。在工作过程中，他需要额外的工具。由于工具不在他的工具箱（L1缓存）中，他从货车（L2缓存）中取出所需的工具。
- en: Occasionally he needs a special tool or part and must leave the job, drive down
    to the local hardware store (global memory), fetch the needed item, and return.
    Neither the tradesperson nor the client knows how long (the latency) this operation
    will actually take. There may be congestion on the freeway and/or queues at the
    hardware store (other processes competing for main memory access).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有时他需要一个特殊工具或零件，就必须离开工作现场，开车到本地五金店（全球内存），取来所需物品，然后返回。工匠和客户都不知道这一操作实际需要多久（延迟）。可能在高速公路上有交通堵塞，或者五金店有排队情况（其他进程在争夺主内存访问）。
- en: Clearly, this is not a very efficient use of the tradesperson’s time. Each time
    a different tool or part is needed, it needs to be fetched by the tradesperson
    from either the van or the hardware store. While fetching new tools the tradesperson
    is not working on the problem at hand.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这对工匠的时间来说并不是一种高效的利用。每当需要不同的工具或零件时，工匠需要从货车或五金店取来。而在取新工具的时候，工匠并没有在解决当前的问题。
- en: While this might seem bad, fetching data from a hard drive or SSD (solid-state
    drive) is akin to ordering an item at the hardware store. In comparative form,
    data from a hard drive arrives by regular courier several days later. Data from
    the SSD may arrive by overnight courier, but it’s still very slow compared to
    accessing data in global memory.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这看起来可能不太好，但从硬盘或SSD（固态硬盘）中提取数据就像是在五金店订购商品一样。做一个比较，硬盘中的数据通常需要几天后通过常规快递送达。SSD中的数据可能通过隔夜快递送达，但与访问全球内存中的数据相比，它仍然非常慢。
- en: In some more modern processor designs we have hardware threads. Some Intel processors
    feature hyperthreading, with two hardware threads per CPU core. To keep with the
    same analogy, this is equivalent to the tradesperson having an assistant and starting
    two jobs. Every time a new tool/part is required, the assistant is sent to fetch
    the new tool/part and the tradesperson switches to the alternate job. Providing
    the assistant is able to return with the necessary tool/part before the alternate
    job also needs an additional tool/part, the tradesperson continues to work.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些现代的处理器设计中，我们有了硬件线程。某些Intel处理器具备超线程技术，每个CPU核心有两个硬件线程。为了保持相同的类比，这相当于工人有了一个助手，并开始了两个工作。每当需要新工具/部件时，助手会去获取新的工具/部件，而工人则切换到另一个工作。只要助手能够在另一个工作也需要额外工具/部件之前将所需工具/部件带回来，工人就能继续工作。
- en: Although an improvement, this has not solved the latency issue—how long it takes
    to fetch new tools/parts from the hardware store (global memory). Typical latencies
    to global memory are in the order of hundreds of clocks. Increasingly, the answer
    to this problem from traditional processor design has been to increase the size
    of the cache. In effect, arrive with a bigger van so fewer trips to the hardware
    store are necessary.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有所改进，这仍未解决延迟问题——从硬件商店（全局内存）获取新工具/部件所需的时间。通常访问全局内存的延迟是以数百个时钟周期为单位的。传统处理器设计对这一问题的回应是增大缓存的大小。实际上，就是配备更大的货车，减少前往硬件商店的次数。
- en: There is, however, an increasing cost to this approach, both in terms of capital
    outlay for a larger van and the time it takes to search a bigger van for the tool/part.
    Thus, the approach taken by most designs today is to arrive with a van (L2 cache)
    and a truck (L3 cache). In the extreme case of the server processors, a huge 18-wheeler
    is brought in to try to ensure the tradesperson is kept busy for just that little
    bit longer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法的成本在不断增加，无论是在更大货车的资本支出方面，还是在搜索更大货车中的工具/部件所需的时间方面。因此，今天大多数设计采取的方法是配备一辆货车（L2缓存）和一辆卡车（L3缓存）。在服务器处理器的极端情况下，甚至会引入一辆巨大的18轮卡车，试图确保工人能忙碌更长时间。
- en: All of this work is necessary because of one fundamental reason. The CPUs are
    designed to run software where the programmer does not have to care about locality.
    Locality is an issue, regardless of whether the processor tries to hide it from
    the programmer or not. The denial that this is an issue is what leads to the huge
    amount of hardware necessary to deal with memory latency.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些工作都是由于一个根本原因。CPU被设计用来运行不需要程序员关心局部性的软件。局部性是一个问题，无论处理器是否试图将其隐藏在程序员面前。否认这是一个问题的做法导致了需要大量硬件来处理内存延迟。
- en: The design of GPUs takes a different approach. It places the GPU programmer
    in charge of dealing with locality and instead of an 18-wheeler truck gives him
    or her a number of small vans and a very large number of tradespeople.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: GPU的设计采取了不同的方法。它将GPU程序员置于处理局部性问题的责任中，并且不再给他或她一辆18轮卡车，而是提供了若干辆小货车和大量的工人。
- en: Thus, in the first instance the programmer must deal with locality. He or she
    needs to think in advance about what tools/parts (memory locations/data structures)
    will be needed for a given job. These then need to be collected in a single trip
    to the hardware store (global memory) and placed in the correct van (on chip memory)
    for a given job at the outset. Given that this data has been collected, as much
    work as possible needs to be performed with the data to avoid having to fetch
    and return it only to fetch it again later for another purpose.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，首先程序员必须处理局部性问题。他或她需要提前考虑在某个工作中需要哪些工具/部件（内存位置/数据结构）。这些工具/部件需要在一次去硬件商店（全局内存）时收集，并在开始时放置在正确的货车（片上内存）中。既然这些数据已经收集好，那么尽可能多地使用这些数据来完成工作，避免必须再次获取并返回数据，只是为了之后为了另一个目的再次获取它。
- en: Thus, the continual cycle of work-stall-fetch from global memory, work-stall-fetch
    from global memory, etc. is broken. We can see the same analogy on a production
    line. Workers are supplied with baskets of parts to process, rather than each
    worker individually fetching widgets one at a time from the store manager’s desk.
    To do otherwise is simply a hugely inefficient use of the available workers’ time.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，工作-停顿-从全局内存获取的循环被打破了。我们可以看到生产线上的相同类比。工人们获得了装满零件的篮子来处理，而不是每个工人单独从仓库经理的桌子上一个接一个地获取小部件。否则就是极其低效地浪费了工人的时间。
- en: This simple process of planning ahead allows the programmer to schedule memory
    loads into the on-chip memory before they are needed. This works well with both
    an explicit local memory model such as the GPU’s shared memory as well as a CPU-based
    cache. In the shared memory case you tell the memory management unit to request
    this data and then go off and perform useful work on another piece of data. In
    the cache case you can use special cache instructions that allow prefilling of
    the cache with data you expect the program to use later.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种提前规划的简单过程允许程序员在需要之前将内存加载到芯片内存中。这对于显式的本地内存模型（如GPU的共享内存）和基于CPU的缓存都能很好地工作。在共享内存的情况下，你可以告诉内存管理单元请求此数据，然后去执行另一块数据的有用工作。而在缓存的情况下，你可以使用特殊的缓存指令，预先填充缓存，以便程序稍后使用这些数据。
- en: The downside of the cache approach over the shared memory approach is eviction
    and dirty data. Data in a cache is said to be dirty if it has been written by
    the program. To free up the space in the cache for new useful data, the dirty
    data has to be written back to global memory before the cache space can be used
    again. This means instead of one trip to global memory of an unknown latency,
    we now have two—one to write the old data and one to get the new data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存方法相比共享内存方法的缺点是缓存置换和脏数据。缓存中的数据被称为脏数据，如果它已经被程序写入。为了腾出缓存空间给新的有用数据，脏数据必须先写回全局内存，然后缓存空间才能再次使用。这意味着，与访问一次全局内存并且其延迟未知相比，我们现在有两次访问——一次写入旧数据，一次读取新数据。
- en: The big advantage of the programmer-controlled on-chip memory is that the programmer
    is in control of when the writes happen. If you are performing some local transformation
    of the data, there may be no need to write the intermediate transformation back
    to global memory. With a cache, the cache controller does not know what needs
    to be written and what can be discarded. Thus, it writes everything, potentially
    creating lots of useless memory traffic that may in turn cause unnecessary congestion
    on the memory interface.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 程序员控制的片上内存的一个大优势是，程序员可以控制何时进行写操作。如果你在执行某些本地数据转换，可能不需要将中间转换结果写回全局内存。使用缓存时，缓存控制器不知道哪些需要写入，哪些可以丢弃。因此，它会写入所有内容，这可能会产生大量无用的内存流量，进而导致内存接口上的不必要的拥塞。
- en: Although many do, not every algorithm lends itself to this type of “known in
    advance” memory pattern that the programmer can optimize for. At the same time,
    not every programmer wants to deal with locality issues, either initially or sometimes
    at all. It’s a perfectly valid approach to develop a program, prove the concept,
    and then deal with locality issues.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管很多算法都适用，但并非所有算法都适合这种“提前已知”的内存模式，程序员可以为其进行优化。同时，并非所有程序员一开始就想处理局部性问题，甚至有时根本不想处理。开发一个程序，验证其概念后，再去处理局部性问题，完全是一个有效的做法。
- en: To facilitate such an approach and to deal with the issues of algorithms that
    did not have a well-defined data/execution pattern, later generations of GPUs
    (compute 2.x onward) have both L1 and L2 caches. These can be configured with
    a preference toward cache or shared memory, allowing the programmer flexibility
    to configure the hardware for a given problem.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进这种方法并解决那些没有明确数据/执行模式的算法问题，后续几代GPU（从compute 2.x开始）都配备了L1和L2缓存。这些缓存可以配置为偏向缓存或共享内存，从而允许程序员根据特定问题灵活配置硬件。
- en: Types of Parallelism
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 并行类型
- en: Task-based parallelism
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于任务的并行
- en: If we look at a typical operating system, we see it exploit a type of parallelism
    called *task parallelism*. The processes are diverse and unrelated. A user might
    be reading an article on a website while playing music from his or her music library
    in the background. More than one CPU core can be exploited by running each application
    on a different core.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看一个典型的操作系统，我们会看到它利用了一种被称为*任务并行*的并行类型。进程是多样的且互不相关。一个用户可能在浏览网站上的文章，同时在后台播放音乐库中的音乐。通过在不同的核心上运行每个应用程序，可以利用多个CPU核心。
- en: In terms of parallel programming, this can be exploited by writing a program
    as a number of sections that “pipe” (send via messages) the information from one
    application to another. The Linux pipe operator (the | symbol) does just this,
    via the operating system. The output of one program, such as `grep`, is the input
    of the next, such as `sort`. Thus, a set of input files can be easily scanned
    for a certain set of characters (the `grep` program) and that output set then
    sorted (the `sort` program). Each program can be scheduled to a separate CPU core.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行编程方面，可以通过将程序写成多个部分来利用这一点，这些部分“管道”信息（通过消息传递）从一个应用程序传递到另一个应用程序。Linux 的管道操作符（|
    符号）就是这样做的，通过操作系统完成。一个程序的输出，例如`grep`，是下一个程序的输入，例如`sort`。因此，可以很容易地扫描一组输入文件，查找一组特定的字符（`grep`程序），然后将该输出集合进行排序（`sort`程序）。每个程序都可以调度到不同的
    CPU 核心上。
- en: This pattern of parallelism is known as *pipeline parallelism*. The output on
    one program provides the input for the next. With a diverse set of components,
    such as the various text-based tools in Linux, a huge variety of useful functions
    can be performed by the user. As the programmer cannot know at the outset everyone’s
    needs, by providing components that operate together and can be connected easily,
    the programmer can target a very wide and diverse user base.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种并行模式被称为*管道并行性*。一个程序的输出为下一个程序提供输入。通过像 Linux 中各种基于文本的工具这样多样化的组件，用户可以执行各种各样有用的功能。由于程序员无法一开始就知道每个人的需求，通过提供可以协同工作的组件并且易于连接，程序员可以面向非常广泛和多样化的用户群体。
- en: This type of parallelism is very much geared toward *coarse-grained parallelism*.
    That is, there are a number of powerful processors, each of which can perform
    a significant chunk of work.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种并行性主要针对*粗粒度并行性*。也就是说，有许多强大的处理器，每个处理器可以执行相当一部分工作。
- en: In terms of GPUs we see coarse-grained parallelism only in terms of a GPU card
    and the execution of GPU kernels. GPUs support the pipeline parallelism pattern
    in two ways. First, kernels can be pushed into a single stream and separate streams
    executed concurrently. Second, multiple GPUs can work together directly through
    either passing data via the host or passing data via messages directly to one
    another over the PCI-E bus. This latter approach, the peer-to-peer (P2P) mechanism,
    was introduced in the CUDA 4.x SDK and requires certain OS/hardware/driver-level
    support.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 的领域，我们只能在 GPU 卡和 GPU 内核执行的层面上看到粗粒度的并行性。GPU 支持管道并行模式有两种方式。首先，内核可以推送到单一流中，并且不同流可以并行执行。其次，多个
    GPU 可以直接通过主机传递数据或者通过 PCI-E 总线直接互相传递消息来协同工作。这种后者的方法，即点对点（P2P）机制，是在 CUDA 4.x SDK
    中引入的，并且需要特定的操作系统/硬件/驱动级别的支持。
- en: One of the issues with a pipeline-based pattern is, like any production line,
    it can only run as fast as the slowest component. Thus, if the pipeline consists
    of five elements, each of which takes one second, we can produce one output per
    second. However, if just one of these elements takes two seconds, the throughput
    of the entire pipeline is reduced to one output every two seconds.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于管道的模式的一个问题是，像任何生产线一样，它的运行速度取决于最慢的组件。因此，如果管道由五个元素组成，每个元素需要一秒钟，我们每秒钟可以生成一个输出。然而，如果其中一个元素需要两秒钟，那么整个管道的吞吐量就会降低至每两秒钟输出一个结果。
- en: The approach to solving this is twofold. Let’s consider the production line
    analogy for a moment. Fred’s station takes two seconds because his task is complex.
    If we provide Fred with an assistant, Tim, and split his task in half with Tim,
    we’re back to one second per stage. We now have six stages instead of five, but
    the throughput of the pipeline is now again one widget per second.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是双管齐下。让我们暂时考虑生产线的类比。弗雷德的工作站需要两秒钟，因为他的任务较为复杂。如果我们为弗雷德配一个助手，蒂姆，并且把他的任务一分为二，那么每个阶段的时间又回到了每秒钟一件事。我们现在有了六个阶段，而不是五个，但管道的吞吐量现在又恢复到每秒钟一件产品。
- en: You can put up to four GPUs into a desktop PC with some thought and care about
    the design (see [Chapter 11](CHP011.html) on designing GPU systems). Thus, if
    we have a single GPU and it’s taking too long to process a particular workflow,
    we can simply add another one and increase the overall processing power of the
    node. However, we then have to think about the division of work between the two
    GPUs. There may not be an easy 50/50 split. If we can only extract a 70/30 split,
    clearly the maximum benefit will be 7/10 (70%) of the existing runtime. If we
    could introduce another GPU and then maybe move another task, which occupied say
    20% of the time, we’d end up with a 50/30/20 split. Again the speedup compared
    to one GPU would be 1/2 or 50% of the original time. We’re still left with the
    worst-case time dominating the overall execution time.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将最多四个GPU安装到台式电脑中，只要在设计时稍加考虑和用心（见[第11章](CHP011.html)关于GPU系统设计）。因此，如果我们只有一个GPU，而它在处理特定工作流时速度太慢，我们可以简单地添加另一个GPU，增加节点的整体处理能力。然而，接下来我们需要考虑两个GPU之间的工作分配。可能不会有一个简单的50/50分配。如果我们只能做到70/30的分配，显然，最大收益将是现有运行时间的7/10（70%）。如果我们能再增加一个GPU，并可能移动另一个占用20%时间的任务，我们将得到50/30/20的分配。再次强调，相比于单个GPU，速度提升将是原始时间的一半或50%。我们仍然面临最差情况下的时间主导了整体执行时间。
- en: The same issue applies to providing a speedup when using a single CPU/GPU combination.
    If we move 80% of the work off the CPU and onto the GPU, with the GPU computing
    this in just 10% of the time, what is the speedup? Well the CPU now takes 20%
    of the original time and the GPU 10% of the original time, but in parallel. Thus,
    the dominating factor is still the CPU. As the GPU is running in parallel and
    consumes less time than the CPU fraction, we can discount this time entirely.
    Thus, the maximum speedup is one divided by the fraction of the program that takes
    the longest time to execute.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用单个CPU/GPU组合时提供加速也面临相同的问题。如果我们将80%的工作从CPU转移到GPU，而GPU仅用10%的时间完成这些工作，那么加速效果如何？实际上，CPU现在需要20%的原始时间，GPU则需要10%的原始时间，但它们是并行执行的。因此，主导因素仍然是CPU。由于GPU是并行运行并且消耗的时间少于CPU部分，我们可以完全忽略这段时间。因此，最大加速比是程序中执行时间最长部分的倒数。
- en: This is known as Amdahl’s law and is often quoted as the limiting factor in
    any speedup. It allows you to know at the outset what the maximum speedup achievable
    is, without writing a single line of code. Ultimately, you will have serial operations.
    Even if you move everything onto the GPU, you will still have to use the CPU to
    load and store data to and from storage devices. You will also have to transfer
    data to and from the GPU to facilitate input and output (I/O). Thus, maximum theoretical
    speedup is determined by the fraction of the program that performs the computation/algorithmic
    part, plus the remaining serial fraction.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为阿姆达尔定律（Amdahl's law），并且通常被引用为任何加速中的限制因素。它使你在开始时就能知道最大可能的加速比，而无需编写一行代码。最终，你仍然会有串行操作。即使你将所有操作移到GPU上，仍然需要使用CPU来加载和存储数据到存储设备之间。你还需要在GPU与其他设备之间传输数据，以便进行输入输出（I/O）。因此，最大理论加速比由执行计算/算法部分的程序比例加上剩余的串行部分决定。
- en: Data-based parallelism
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于数据的并行性
- en: Computation power has been greatly increasing over the past couple of decades.
    We now have teraflop-capable GPUs. However, what has not kept pace with this evolution
    of compute power is the access time for data. The idea of data-based parallelism
    is that instead of concentrating on what tasks have to be performed, we look first
    to the data and how it needs to be transformed.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 计算能力在过去几十年中大幅增加。现在我们拥有能够支持TeraFlop的GPU。然而，随着计算能力的进步，数据访问时间并没有同步提升。基于数据的并行性理念是，重点不再仅仅是执行哪些任务，而是首先关注数据及其需要如何转换。
- en: Task-based parallelism tends to fit more with coarse-grained parallelism approaches.
    Let’s use an example of performing four different transformations on four separate,
    unrelated, and similarly sized arrays. We have four CPU cores, and a GPU with
    four SMs. In a task-based decomposition of the problem, we would assign one array
    to each of the CPU cores or SMs in the GPU. The parallel decomposition of the
    problem is driven by thinking about the tasks or transformations, not the data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基于任务的并行性更适合粗粒度并行方法。让我们用一个例子来说明，假设我们要对四个独立、不相关且大小相似的数组执行四种不同的转换。我们有四个CPU核心和一个有四个SM的GPU。在任务分解方法中，我们将每个数组分配给一个CPU核心或GPU中的一个SM。问题的并行分解是由任务或转换的思维方式驱动，而非数据。
- en: On the CPU side we could create four threads or processes to achieve this. On
    the GPU side we would need to use four blocks and pass the address of every array
    to every block. On the newer Fermi and Kepler devices, we could also create four
    separate kernels, one to process each array and run it concurrently.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CPU 端，我们可以创建四个线程或进程来实现这一点。在 GPU 端，我们需要使用四个块，并将每个数组的地址传递给每个块。在较新的 Fermi 和 Kepler
    设备上，我们还可以创建四个独立的内核，一个处理每个数组并并行运行。
- en: A data-based decomposition would instead split the first array into four blocks
    and assign one CPU core or one GPU SM to each section of the array. Once completed,
    the remaining three arrays would be processed in a similar way. In terms of the
    GPU implementation, this would be four kernels, each of which contained four or
    more blocks. The parallel decomposition here is driven by thinking about the data
    first and the transformations second.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基于数据的分解方法会将第一个数组拆分成四个块，并将每个块分配给一个 CPU 核心或一个 GPU SM。完成后，剩余的三个数组将以类似的方式处理。在 GPU
    实现方面，这将是四个内核，每个内核包含四个或更多块。这里的并行分解是通过先考虑数据，再进行转换来驱动的。
- en: As our CPU has only four cores, it makes a lot of sense to decompose the data
    into four blocks. We could have thread 0 process element 0, thread 1 process element
    1, thread 2 process element 2, thread 3 process element 3, and so on. Alternatively,
    the array could be split into four parts and each thread could start processing
    its section of the array.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的 CPU 只有四个核心，将数据分解为四个块是非常合理的。我们可以让线程 0 处理元素 0，线程 1 处理元素 1，线程 2 处理元素 2，线程
    3 处理元素 3，依此类推。或者，数组可以分成四个部分，每个线程可以开始处理自己部分的数组。
- en: In the first case, thread 0 fetches element 0\. As CPUs contain multiple levels
    of cache, this brings the data into the device. Typically the L3 cache is shared
    by all cores. Thus, the memory access from the first fetch is distributed to all
    cores in the CPU. By contrast in the second case, four separate memory fetches
    are needed and four separate L3 cache lines are utilized. The latter approach
    is often better where the CPU cores need to write data back to memory. Interleaving
    the data elements by core means the cache has to coordinate and combine the writes
    from different cores, which is usually a bad idea.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，线程 0 获取元素 0。由于 CPU 包含多个级别的缓存，这会将数据带入设备。通常，L3 缓存是所有核心共享的。因此，第一次获取的内存访问会分配给
    CPU 中的所有核心。相反，在第二种情况下，需要进行四次独立的内存获取，并使用四个独立的 L3 缓存行。后一种方法在 CPU 核心需要将数据写回内存时通常更好。将数据元素按核心交错意味着缓存必须协调并合并来自不同核心的写入，这通常不是一个好主意。
- en: If the algorithm permits, we can exploit a certain type of data parallelism,
    the SIMD (single instruction, multiple data) model. This would make use of special
    SIMD instructions such as MMX, SSE, AVX, etc. present in many x86-based CPUs.
    Thus, thread 0 could actually fetch multiple adjacent elements and process them
    with a single SIMD instruction.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果算法允许，我们可以利用某种类型的数据并行性，即 SIMD（单指令多数据）模型。这将利用许多基于 x86 的 CPU 中存在的特殊 SIMD 指令，如
    MMX、SSE、AVX 等。因此，线程 0 实际上可以获取多个相邻的元素，并通过单个 SIMD 指令进行处理。
- en: If we consider the same problem on the GPU, each array needs to have a separate
    transformation performed on it. This naturally maps such that one transformation
    equates to a single GPU kernel (or program). Each SM, unlike a CPU core, is designed
    to run multiple blocks of data with each block split into multiple threads. Thus,
    we need a further level of decomposition to use the GPU efficiently. We’d typically
    allocate, at least initially, a combination of blocks and threads such that a
    single thread processed a single element of data. As with the CPU, there are benefits
    from processing multiple elements per thread. This is somewhat limited on GPUs
    as only load/store/move explicit SIMD primitives are supported, but this in turn
    allows for enhanced levels of *instruction-level parallelism* (ILP), which we’ll
    see later is actually quite beneficial.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑在 GPU 上解决相同的问题，则每个数组需要单独进行转换。这自然地映射为每个转换对应一个 GPU 内核（或程序）。与 CPU 核心不同，每个
    SM 设计为运行多个数据块，每个块又被拆分为多个线程。因此，我们需要进一步的分解级别，以高效使用 GPU。通常，至少在最初阶段，我们会分配块和线程的组合，使得每个线程处理一个数据元素。与
    CPU 一样，通过每个线程处理多个元素是有好处的。尽管 GPU 上对 SIMD 原语的支持仅限于加载/存储/移动显式指令，但这反而允许更高水平的*指令级并行性*（ILP），如我们稍后所见，这实际上是相当有益的。
- en: With a Fermi and Kepler GPUs, we have a shared L2 cache that replicates the
    L3 cache function on the CPU. Thus, as with the CPU, a memory fetch from one thread
    can be distributed to other threads directly from the cache. On older hardware,
    there is no cache. However, on GPUs adjacent memory locations are coalesced (combined)
    together by the hardware, resulting in a single and more efficient memory fetch.
    We look at this in detail in [Chapter 6](CHP006.html) on memory.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Fermi和Kepler GPU时，我们有一个共享的L2缓存，它复制了CPU上的L3缓存功能。因此，像CPU一样，来自一个线程的内存访问请求可以直接从缓存分发到其他线程。在较旧的硬件上没有缓存。然而，在GPU上，相邻的内存位置会被硬件合并（组合）在一起，结果是一次更高效的内存访问。我们在[第六章](CHP006.html)中详细讨论了内存的这一点。
- en: One important distinction between the caches found in GPUs and CPUs is cache
    coherency. In a cache-coherent system a write to a memory location needs to be
    communicated to all levels of cache in all cores. Thus, all processor cores see
    the same view of memory at any point in time. This is one of the key factors that
    limits the number of cores in a processor. Communication becomes increasingly
    more expensive in terms of time as the processor core count increases. The worst
    case in a cache-coherent system is where each core writes adjacent memory locations
    as each write forces a global update to every core’s cache.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: GPU和CPU中的缓存之间有一个重要的区别，那就是缓存一致性。在一个缓存一致性的系统中，对内存位置的写操作需要被传达给所有核心的所有级别的缓存。因此，所有处理器核心在任何时刻看到的都是相同的内存视图。这是限制处理器核心数的关键因素之一。当处理器核心数量增加时，通信在时间上的开销也变得越来越大。在缓存一致性系统中，最糟糕的情况是每个核心写入相邻的内存位置，因为每次写操作都会强制全局更新每个核心的缓存。
- en: A non cache-coherent system by comparison does not automatically update the
    other core’s caches. It relies on the programmer to write the output of each processor
    core to separate areas/addresses. This supports the view of a program where a
    single core is responsible for a single or small set of outputs. CPUs follow the
    cache-coherent approach whereas the GPU does not and thus is able to scale to
    a far larger number of cores (SMs) per device.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，非缓存一致性的系统不会自动更新其他核心的缓存。它依赖程序员将每个处理器核心的输出写入不同的区域/地址。这支持一种程序的视图，其中单个核心负责一个单独的或少量的输出。CPU采用缓存一致性方法，而GPU则不采用，因此能够在每个设备上扩展到更多的核心（SMs）。
- en: Let’s assume for simplicity that we implement a kernel as four blocks. Thus,
    we have four kernels on the GPU and four processes or threads on the CPU. The
    CPU may support mechanisms such as hyperthreading to enable processing of additional
    threads/processes due to a stall event, a cache miss, for example. Thus, we could
    increase this number to eight and we might see an increase in performance. However,
    at some point, sometimes even at less than the number of cores, the CPU hits a
    point where there are just too many threads.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，假设我们将内核实现为四个块。因此，我们在GPU上有四个内核，在CPU上有四个进程或线程。CPU可能支持如超线程之类的机制，以便在发生停滞事件时（例如，缓存未命中）处理额外的线程/进程。因此，我们可以将此数字增加到八，可能会看到性能提升。然而，在某些情况下，即使核心数还不够，CPU也会达到一个点，线程数过多时，性能反而下降。
- en: At this point the memory bandwidth becomes flooded and cache utilization drops
    off, resulting in less performance, not more.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个时候，内存带宽会被淹没，缓存的利用率下降，导致性能降低，而不是提升。
- en: On the GPU side, four blocks is nowhere near enough to satisfy four SMs. Each
    SM can actually schedule up to eight blocks (16 on Kepler). Thus, we’d need 8
    × 4 = 32 blocks to load the four SMs correctly. As we have four independent operations,
    we can launch four simultaneous kernels on Fermi hardware via the streams feature
    (see [Chapter 8](CHP008.html) on using multiple GPUs). Consequently, we can launch
    16 blocks in total and work on the four arrays in parallel. As with the CPU, however,
    it would be more efficient to work on one array at a time as this would likely
    result in better cache utilization. Thus, on the GPU we need to ensure we always
    have enough blocks (typically a minimum of 8 to 16 times the number of SMs on
    the GPU device).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU方面，四个块远不足以满足四个SM的需求。每个SM实际上可以调度最多八个块（在Kepler上为16）。因此，我们需要8 × 4 = 32个块来正确加载四个SM。由于我们有四个独立的操作，我们可以通过流特性在Fermi硬件上启动四个同时运行的内核（参见[第八章](CHP008.html)关于使用多个GPU）。因此，我们总共可以启动16个块，并行处理四个数组。然而，像CPU一样，更高效的方法是一次处理一个数组，因为这样可能会带来更好的缓存利用率。因此，在GPU上，我们需要确保始终有足够的块（通常是GPU设备上SM数的8到16倍）。
- en: Flynn’s Taxonomy
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弗林分类法
- en: 'We mentioned the term SIMD earlier. This classification comes from Flynn’s
    taxonomy, a classification of different computer architectures. The various types
    are as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到了SIMD这个术语。这一分类来自Flynn的分类法，这是对不同计算机架构的分类。各种类型如下：
- en: • SIMD—single instruction, multiple data
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: • SIMD—单指令，多数据
- en: • MIMD—multiple instructions, multiple data
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: • MIMD—多指令，多数据
- en: • SISD—single instruction, single data
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: • SISD—单指令，单数据
- en: • MISD—multiple instructions, single data
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: • MISD—多指令，单数据
- en: The standard serial programming most people will be familiar with follows the
    SISD model. That is, there is a single instruction stream working on a single
    data item at any one point in time. This equates to a single-core CPU able to
    perform one task at a time. Of course it’s quite possible to provide the illusion
    of being able to perform more than a single task by simply switching between tasks
    very quickly, so-called time-slicing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人熟悉的标准串行编程遵循SISD模型。也就是说，在任何时刻，只有单个指令流作用于单个数据项。这等同于一个单核CPU一次只能执行一个任务。当然，也可以通过非常快速地在任务之间切换，从而提供能够执行多个任务的假象，这就是所谓的时间分片。
- en: MIMD systems are what we see today in dual- or quad-core desktop machines. They
    have a work pool of threads/processes that the OS will allocate to one of *N*
    CPU cores. Each thread/process has an independent stream of instructions, and
    thus the hardware contains all the control logic for decoding many separate instruction
    streams.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: MIMD系统是我们今天在双核或四核桌面计算机中看到的。它们拥有一个线程/进程的工作池，操作系统会将其分配给*N*个CPU核心中的一个。每个线程/进程都有一个独立的指令流，因此硬件包含所有解码多个独立指令流的控制逻辑。
- en: SIMD systems try to simplify this approach, in particular with the data parallelism
    model. They follow a single instruction stream at any one point in time. Thus,
    they require a single set of logic inside the device to decode and execute the
    instruction stream, rather than multiple-instruction decode paths. By removing
    this silicon real estate from the device, they can be smaller, cheaper, consume
    less power, and run at higher clock rates than their MIMD cousins.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: SIMD系统尝试简化这种方法，特别是在数据并行模型中。它们在任何时刻都遵循单一的指令流。因此，它们只需要设备内部的一套逻辑来解码并执行指令流，而不是多个指令解码路径。通过从设备中去除这种硅片空间，它们可以比MIMD系统更小、更便宜、功耗更低，并且能够以更高的时钟频率运行。
- en: Many algorithms make use of a small number of data points in one way or another.
    The data points can often be arranged as a SIMD instruction. Thus, all data points
    may have some fixed offset added, followed by a multiplication, a gain factor
    for example. This can be easily implemented as SIMD instructions. In effect, you
    are programming “for this range of data, perform this operation” instead of “for
    this data point, perform this operation.” As the data operation or transformation
    is constant for all elements in the range, it can be fetched and decoded from
    the program memory only once. As the range is defined and contiguous, the data
    can be loaded en masse from the memory, rather than one word at a time.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法以某种方式利用少量数据点。这些数据点通常可以安排为SIMD指令。因此，所有数据点可能会加上一些固定的偏移量，然后进行乘法运算，例如增益因子。这可以很容易地实现为SIMD指令。实际上，你是在为“这范围的数据，执行此操作”编程，而不是“为此数据点，执行此操作”。由于数据操作或变换对于该范围内的所有元素都是恒定的，它可以只从程序存储器中获取并解码一次。由于该范围是已定义并且连续的，因此数据可以从存储器中批量加载，而不是一次加载一个字。
- en: However, algorithms where one element has transformation A applied while another
    element has transformation B applied, and all others have transformation C applied,
    are difficult to implement using SIMD. The exception is where this algorithm is
    hard-coded into the hardware because it’s very common. Such examples include AES
    (Advanced Encryption Standard) and H.264 (a video compression standard).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些算法中，一个元素应用变换A，另一个元素应用变换B，而其他所有元素应用变换C，这种情况使用SIMD很难实现。唯一的例外是当这个算法被硬编码到硬件中，因为它非常常见。例如AES（高级加密标准）和H.264（视频压缩标准）。
- en: The GPU takes a slightly different approach to SIMD. It implements a model NVIDIA
    calls SIMT (single instruction, multiple thread). In this model the instruction
    side of the SIMD instruction is not a fixed function as it is within the CPU hardware.
    The programmer instead defines, through a kernel, what each thread will do. Thus,
    the kernel will read the data uniformly and the kernel code will execute transformation
    A, B, or C as necessary. In practice, what happens is that A, B, and C are executed
    in sequence by repeating the instruction stream and masking out the nonparticipating
    threads. However, conceptually this is a much easier model to work with than one
    that only supports SIMD.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: GPU对SIMD采用了稍有不同的方法。它实现了NVIDIA称为SIMT（单指令，多线程）的一种模型。在这种模型中，SIMD指令的指令侧并不是像CPU硬件中那样的固定功能。相反，程序员通过内核定义每个线程将做什么。因此，内核将统一读取数据，并根据需要执行变换A、B或C。实际上，发生的情况是A、B和C按顺序执行，通过重复指令流并屏蔽掉不参与的线程。然而，从概念上讲，这比仅支持SIMD的模型更易于使用。
- en: Some Common Parallel Patterns
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些常见的并行模式
- en: A number of parallel problems can be thought of as patterns. We see patterns
    in many software programs, although not everyone is aware of them. Thinking in
    terms of patterns allows us to broadly deconstruct or abstract a problem, and
    therefore more easily think about how to solve it.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 许多并行问题可以被看作是模式。我们在许多软件程序中看到这些模式，尽管并非每个人都意识到它们的存在。以模式的方式思考让我们能够广泛地拆解或抽象一个问题，因此更容易考虑如何解决它。
- en: Loop-based patterns
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于循环的模式
- en: Almost anyone who has done any programming is familiar with loops. They vary
    primarily in terms of entry and exit conditions (`for`, `do…while`, `while`),
    and whether they create dependencies between loop iterations or not.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎任何做过编程的人都熟悉循环。它们主要在进入和退出条件（`for`、`do…while`、`while`）以及是否在循环迭代之间创建依赖方面有所不同。
- en: A loop-based iteration dependency is where one iteration of the loop depends
    on one or more previous iterations. We want to remove these if at all possible
    as they make implementing parallel algorithms more difficult. If in fact this
    can’t be done, the loop is typically broken into a number of blocks that are executed
    in parallel. The result from block 0 is then retrospectively applied to block
    1, then to block 2, and so on. There is an example later in this text where we
    adopt just such an approach when handling the prefix-sum algorithm.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基于循环的迭代依赖是指循环中的一个迭代依赖于一个或多个先前的迭代。我们希望尽可能去除这些依赖，因为它们使得实现并行算法变得更加困难。如果实际上无法去除这些依赖，通常会将循环分成多个块并行执行。块0的结果会被追溯地应用到块1，然后是块2，以此类推。本文后面会有一个示例，我们在处理前缀和算法时采用了这种方法。
- en: Loop-based iteration is one of the easiest patterns to parallelize. With inter-loop
    dependencies removed, it’s then simply a matter of deciding how to split, or partition,
    the work between the available processors. This should be done with a view to
    minimizing communication between processors and maximizing the use of on-chip
    resources (registers and shared memory on a GPU; L1/L2/L3 cache on a CPU). Communication
    overhead typically scales badly and is often the bottleneck in poorly designed
    systems.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 基于循环的迭代是最容易并行化的模式之一。当去除循环间的依赖后，剩下的就是如何将工作划分或分配给可用的处理器。这样做应该考虑到最小化处理器之间的通信并最大化芯片内资源的使用（GPU上的寄存器和共享内存；CPU上的L1/L2/L3缓存）。通信开销通常扩展性差，并且在设计不良的系统中常常成为瓶颈。
- en: The macro-level decomposition should be based on the number of logical processing
    units available. For the CPU, this is simply the number of logical hardware threads
    available. For the GPU, this is the number of SMs multiplied by the maximum load
    we can give to each SM, 1 to 16 blocks depending on resource usage and GPU model.
    Notice we use the term *logical* and not physical hardware thread. Some Intel
    CPUs in particular support more than one logical thread per physical CPU core,
    so-called hyperthreading. GPUs run multiple blocks on a single SM, so we have
    to at least multiply the number of SMs by the maximum number of blocks each SM
    can support.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 宏观级别的分解应该基于可用的逻辑处理单元数量。对于CPU来说，这只是可用的逻辑硬件线程数量。对于GPU来说，这等于SM的数量乘以我们可以给每个SM分配的最大负载，通常是1到16个块，具体取决于资源使用和GPU型号。注意，我们使用的是“逻辑”而不是物理硬件线程这个术语。特别是一些英特尔CPU支持每个物理核心有多个逻辑线程，即所谓的超线程技术。GPU在单个SM上运行多个块，因此我们至少需要将SM的数量乘以每个SM可以支持的最大块数。
- en: Using more than one thread per physical device maximizes the throughput of such
    devices, in terms of giving them something to do while they may be waiting on
    either a memory fetch or I/O-type operation. Selecting some multiple of this minimum
    number can also be useful in terms of load balancing on the GPU and allows for
    improvements when new GPUs are released. This is particularly the case when the
    partition of the data would generate an uneven workload, where some blocks take
    much longer than others. In this case, using many times the number of SMs as the
    basis of the partitioning of the data allows slack SMs to take work from a pool
    of available blocks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个物理设备使用多个线程可以最大化这些设备的吞吐量，尤其是在它们可能需要等待内存获取或I/O操作时，给它们一些事情做。选择这个最小线程数的倍数对于GPU的负载平衡也很有用，并且在新GPU发布时可以带来改进。尤其是在数据划分会产生不均匀工作负载的情况下，一些块的执行时间远长于其他块。此时，使用多个SM的数量作为数据划分的基础，可以让空闲的SM从可用块池中获取工作。
- en: However, on the CPU side, over subscribing the number of threads tends to lead
    to poor performance. This is largely due to context switching being performed
    in software by the OS. Increased contention for the cache and memory bandwidth
    also contributes significantly should you try to run too many threads. Thus, an
    existing multicore CPU solution, taken as is, typically has far too large a granularity
    for a GPU. You will almost always have to repartition the data into many smaller
    blocks to solve the same problem on the GPU.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在CPU方面，超额订阅线程数量往往会导致性能下降。这主要是因为操作系统在软件中执行上下文切换。如果你尝试运行过多线程，增加对缓存和内存带宽的争用也会显著影响性能。因此，现有的多核CPU解决方案在直接使用时，通常对GPU来说粒度过大。你几乎总是需要将数据重新划分为多个较小的块，才能在GPU上解决相同的问题。
- en: When considering loop parallelism and porting an existing serial implementation,
    be critically aware of hidden dependencies. Look carefully at the loop to ensure
    one iteration does not calculate a value used later. Be wary of loops that count
    down as opposed to the standard zero to max value construct, which is the most
    common type of loop found. Why did the original programmer count backwards? It
    is likely this may be because there is some dependency in the loop and parallelizing
    it without understanding the dependencies will likely break it.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑循环并行性以及移植现有串行实现时，必须批判性地识别隐藏的依赖关系。仔细查看循环，确保一个迭代不会计算一个稍后使用的值。要小心倒计时的循环，与常见的从零到最大值的标准构造不同。为什么原程序员要倒数？这很可能是因为循环中存在某些依赖关系，如果没有理解这些依赖关系就进行并行化，可能会导致程序出错。
- en: We also have to consider loops where we have an inner loop and one or more outer
    loops. How should these be parallelized? On a CPU the approach would be to parallelize
    only the outer loop as you have only a limited number of threads. This works well,
    but as before it depends on there being no loop iteration dependencies.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须考虑有内循环和一个或多个外循环的情况。这些应该如何并行化？在CPU上，方法是仅并行化外循环，因为你只有有限数量的线程。这种方法效果不错，但和之前一样，它取决于循环迭代之间没有依赖关系。
- en: On the GPU the inner loop, provided it is small, is typically implemented by
    threads within a single block. As the loop iterations are grouped, adjacent threads
    usually access adjacent memory locations. This often allows us to exploit locality,
    something very important in CUDA programming. Any outer loop(s) are then implemented
    as blocks of the threads. These are concepts we cover in detail in [Chapter 5](CHP005.html).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU上，内循环（只要它较小）通常由单个块中的线程实现。由于循环迭代是分组的，相邻的线程通常访问相邻的内存位置。这通常让我们能够利用局部性，这是CUDA编程中非常重要的一点。任何外循环则作为线程块来实现。这些概念我们将在[第5章](CHP005.html)中详细讨论。
- en: Consider also that most loops can be flattened, thus reducing an inner and outer
    loop to a single loop. Think about an image processing algorithm that iterates
    along the *X* pixel axis in the inner loop and the *Y* pixel axis in the outer
    loop. It’s possible to flatten this loop by considering all pixels as a single-dimensional
    array and iterating over pixels as opposed to image coordinates. This requires
    a little more thought on the programming side, but it may be useful if one or
    more loops contain a very small number of iterations. Such small loops present
    considerable loop overhead compared to the work done per iteration. They are,
    thus, typically not efficient.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 还要考虑到，大多数循环可以被“扁平化”，即将内循环和外循环合并为一个单一循环。举个例子，假设有一个图像处理算法，在内循环中沿*X*像素轴进行迭代，在外循环中沿*Y*像素轴进行迭代。通过将所有像素视为一个一维数组并按像素迭代，而不是按图像坐标迭代，可以扁平化这个循环。这样做需要在编程时多加思考，但如果某些循环迭代次数很少，这种方式可能会有帮助。因为这些小的循环相比每次迭代的工作量，会带来相当大的循环开销，因此通常不高效。
- en: Fork/join pattern
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Fork/join模式
- en: The fork/join pattern is a common pattern in serial programming where there
    are synchronization points and only certain aspects of the program are parallel.
    The serial code runs and at some point hits a section where the work can be distributed
    to *P* processors in some manner. It then “forks” or spawns *N* threads/processes
    that perform the calculation in parallel. These then execute independently and
    finally converge or join once *all* the calculations are complete. This is typically
    the approach found in OpenMP, where you define a parallel region with pragma statements.
    The code then splits into *N* threads and later converges to a single thread again.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: fork/join模式是在串行编程中常见的一种模式，其中存在同步点，且程序的某些部分是并行执行的。串行代码运行到某一时刻时，会遇到一个可以将工作以某种方式分配给*P*处理器的区域。它然后“分叉”或生成*N*个线程/进程，执行并行计算。这些线程独立执行，最后在*所有*计算完成后汇聚或合并。典型的实现方式是OpenMP，在其中你通过pragma语句定义并行区域。代码随后会分裂成*N*个线程，并最终再次合并为单个线程。
- en: In [Figure 2.2](#F0015), we see a queue of data items. As we have three processing
    elements (e.g., CPU cores), these are split into three queues of data, one per
    processing element. Each is processed independently and then written to the appropriate
    place in the destination queue.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2.2](#F0015)中，我们看到一个数据项队列。由于我们有三个处理元素（例如CPU核心），这些数据被分割成三个数据队列，每个处理元素一个。每个队列独立处理，然后将结果写入目标队列中的相应位置。
- en: '![image](../images/F000028f02-02-9780124159334.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000028f02-02-9780124159334.jpg)'
- en: Figure 2.2 A queue of data processed by *N* threads.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 *N*个线程处理的数据队列。
- en: The fork/join pattern is typically implemented with static partitioning of the
    data. That is, the serial code will launch *N* threads and divide the dataset
    equally between the *N* threads. If each packet of data takes the same time to
    process, then this works well. However, as the overall time to execute is the
    time of the slowest thread, giving one thread too much work means it becomes the
    single factor determining the total time.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: fork/join模式通常通过数据的静态分区来实现。也就是说，串行代码将启动*N*个线程，并在*N*个线程之间均匀分配数据集。如果每个数据包的处理时间相同，这种方式效果很好。然而，由于执行的总时间由最慢的线程决定，因此给某个线程分配过多的工作会导致它成为决定总时间的单一因素。
- en: Systems such as OpenMP also have dynamic scheduling allocation, which mirrors
    the approach taken by GPUs. Here a thread pool is created (a block pool for GPUs)
    and only once one task is completed is more work allocated. Thus, if 1 task takes
    10x time and 20 tasks take just 1x time each, they are allocated only to free
    cores. With a dual-core CPU, core 1 gets the big 10x task and five of the smaller
    1x tasks. Core 2 gets 15 of the smaller 1x tasks, and therefore both CPU core
    1 and 2 complete around the same time.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 像OpenMP这样的系统还具有动态调度分配机制，类似于GPU的处理方式。在这种方式中，创建一个线程池（GPU的块池），并且只有当一个任务完成后，才会分配更多的工作。因此，如果一个任务需要10倍的时间，而20个任务只需1倍的时间，则这些任务只会分配给空闲的核心。在双核CPU中，核心1会处理一个大约10倍时间的任务，以及五个小的1倍时间任务。核心2则会处理15个小的1倍时间任务，因此CPU核心1和核心2会几乎同时完成任务。
- en: In this particular example, we’ve chosen to fork three threads, yet there are
    six data items in the queue. Why not fork six threads? The reality is that in
    most problems there can actually be millions of data items and attempting to fork
    a million threads will cause almost all OSs to fail in one way or another.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的示例中，我们选择了派生三个线程，但队列中有六个数据项。为什么不派生六个线程呢？现实情况是，在大多数问题中，实际上可能有数百万的数据项，尝试派生一百万个线程将导致几乎所有操作系统以某种方式失败。
- en: Typically an OS will apply a “fair” scheduling policy. Thus, each of the million
    threads would need to be processed in turn by one of perhaps four available processor
    cores. Each thread also requires its own memory space. In Windows a thread can
    come with a 1 MB stack allocation, meaning we’d rapidly run out of memory prior
    to being able to fork enough threads.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，操作系统会应用一种“公平”的调度策略。因此，每个百万级线程需要依次由可能的四个处理器核心中的一个来处理。每个线程还需要自己的内存空间。在 Windows
    中，一个线程可能会分配 1MB 的栈空间，这意味着我们在能够派生足够线程之前，内存就会迅速耗尽。
- en: Therefore on CPUs, typically programmers and many multithreaded libraries will
    use the number of logical processor threads available as the number of processes
    to fork. As CPU threads are typically also expensive to create and destroy, and
    also to limit maximum utilization, often a thread pool of workers is used who
    then fetch work from a queue of possible tasks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 CPU 上，通常程序员和许多多线程库会使用可用的逻辑处理器线程数作为需要 fork 的进程数。由于创建和销毁 CPU 线程通常也很昂贵，而且为了限制最大利用率，通常会使用一个线程池，工作线程从可能任务的队列中获取工作。
- en: On GPUs we have the opposite problem, in that we in fact need thousands or tens
    of thousands of threads. We have exactly the thread pool concept we find on more
    advanced CPU schedulers, except it’s more like a block pool than a thread pool.
    The GPU has an upper limit on the number of *concurrent* blocks it can execute.
    Each block contains a number of threads. Both the number of threads per block
    and the overall number of concurrently running blocks vary by GPU generation.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GPU 上，我们面临相反的问题，因为我们实际上需要成千上万的线程。我们拥有的正是我们在更高级的 CPU 调度程序中看到的线程池概念，不过它更像是一个块池，而不是线程池。GPU
    对其可以执行的*并发*块数有上限。每个块包含一定数量的线程。每个块的线程数和并发运行的块总数会根据 GPU 的不同代数而有所变化。
- en: The fork/join pattern is often used when there is an unknown amount of concurrency
    in a problem. Traversing a tree structure or a path exploration type algorithm
    may spawn (fork) additional threads when it encounters another node or path. When
    the path has been fully explored these threads may then join back into the pool
    of threads or simply complete to be respawned later.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: fork/join 模式通常在问题中存在未知数量并发时使用。遍历树结构或路径探索类型的算法可能会在遇到另一个节点或路径时生成（fork）额外的线程。当路径完全探索完毕后，这些线程可以重新加入线程池，或者完成任务后稍后重新生成。
- en: This pattern is not natively supported on a GPU, as it uses a fixed number of
    blocks/threads at kernel launch time. Additional blocks cannot be launched by
    the kernel, only the host program. Thus, such algorithms on the GPU side are typically
    implemented as a series of GPU kernel launches, each of which needs to generate
    the next state. An alternative is to coordinate or signal the host and have it
    launch additional, concurrent kernels. Neither solution works particularly well,
    as GPUs are designed for a static amount of concurrency. Kepler introduces a concept,
    dynamic parallelism, which addresses this issue. See [Chapter 12](CHP012.html)
    for more information on this.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式在 GPU 上并不原生支持，因为它在内核启动时使用固定数量的块/线程。内核不能启动额外的块，只有主机程序可以。因此，这类算法在 GPU 端通常是通过一系列
    GPU 内核启动来实现的，每次启动都需要生成下一个状态。另一种替代方法是协调或发信号给主机，由它来启动额外的并发内核。然而，这两种解决方案都不是特别有效，因为
    GPU 是为静态的并发数量设计的。Kepler 引入了一个概念——动态并行性，它解决了这个问题。有关更多信息，请参见[第12章](CHP012.html)。
- en: Within a block of threads on a GPU there are a number of methods to communication
    between threads and to coordinate a certain amount of problem growth or varying
    levels of concurrency within a kernel. For example, if you have an 8 × 8 matrix
    you may have many places where just 64 threads are active. However, there may
    be others where 256 threads can be used. You can launch 256 threads and leave
    most of them idle until such time as needed. Such idle threads occupy resources
    and may limit the overall throughput, but do not consume any execution time on
    the GPU whilst idle. This allows the use of shared memory, fast memory close to
    the processor, rather than creating a number of distinct steps that need to be
    synchronized by using the much slower global memory and multiple kernel launches.
    We look at memory types in [Chapter 6](CHP006.html).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在GPU的线程块内，有多种方法可以让线程之间进行通信，并协调问题增长或内核内不同并发性级别的调整。例如，如果你有一个8 × 8的矩阵，可能有很多地方只有64个线程在活动。然而，其他地方可能可以使用256个线程。你可以启动256个线程，并将大部分线程保持空闲，直到需要它们。虽然这些空闲线程占用了资源，并可能限制整体吞吐量，但在空闲时它们不会消耗GPU的执行时间。这使得你可以使用共享内存，即靠近处理器的快速内存，而不是通过使用速度较慢的全局内存和多个内核启动来创建需要同步的多个步骤。我们将在[第6章](CHP006.html)中讨论内存类型。
- en: Finally, the later-generation GPUs support fast atomic operations and synchronization
    primitives that communicate data between threads in addition to simply synchronizing.
    We look at some examples of this later in the text.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，后代GPU支持快速的原子操作和同步原语，这些操作不仅可以同步线程，还能在线程之间传递数据。我们将在本文后面查看一些相关示例。
- en: Tiling/grids
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平铺/网格
- en: The approach CUDA uses with all problems is to require the programmer to break
    the problem into smaller parts. Most parallel approaches make use of this concept
    in one way or another. Even in huge supercomputers problems such climate models
    must be broken down into hundreds of thousands of blocks, each of which is then
    allocated to one of the thousands of processing elements present in the machine.
    This type of parallel decomposition has the huge advantage that it scales really
    well.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA在处理所有问题时的做法是要求程序员将问题分解为更小的部分。大多数并行方法都以某种方式使用了这一概念。即使是在巨大的超级计算机中，像气候模型这样的任务也必须被分解为数十万个块，每个块分配给机器中的一个处理单元。这种并行分解的巨大优势在于它的扩展性非常好。
- en: A GPU is in many ways similar to a symmetrical multiprocessor system on a single
    processor. Each SM is a processor in its own right, capable of running up multiple
    blocks of threads, typically 256 or 512 threads per block. A number of SMs exist
    on a single GPU and share a common global memory space. Together as a single GPU
    they can operate at peak speeds of up to 3 teraflops/s (GTX680).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多方面，GPU类似于一个单处理器上的对称多处理系统。每个SM（流处理器单元）本身就是一个处理器，能够同时运行多个线程块，通常每个线程块有256或512个线程。单个GPU上存在多个SM，它们共享一个公共的全局内存空间。作为一个单一的GPU，它们可以在最高速度下达到每秒3万亿次浮点运算（GTX680）。
- en: While peak performance may be impressive, achieving anything like this is not
    possible without specially crafted programs, as this peak performance does not
    include things such as memory access, which is somewhat key to any real program.
    To achieve good performance on any platform requires a good knowledge of the hardware
    and the understanding of two key concepts—concurrency and locality.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管峰值性能可能令人印象深刻，但没有特别设计的程序，这种性能是无法实现的，因为这种峰值性能并不包括诸如内存访问等内容，而内存访问对于任何实际程序来说都是至关重要的。要在任何平台上实现良好的性能，需要对硬件有深入了解，并理解两个关键概念——并发性和局部性。
- en: There is concurrency in many problems. It’s just that as someone who may come
    from a serial background, you may not immediately see the concurrency in a problem.
    The tiling model is thus an easy model to conceptualize. Imagine the problem in
    two dimensions—a flat arrangement of data—and simply overlay a grid onto the problem
    space. For a three-dimensional problem imagine the problem as a Rubik’s Cube—a
    set of blocks that map onto the problem space.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 许多问题中都有并发性。只是作为可能来自串行背景的开发者，你可能不会立即在问题中看到并发性。因此，平铺模型是一个容易理解的模型。想象这个问题是二维的——一个平面的数据排列——然后简单地将一个网格覆盖在问题空间上。对于三维问题，想象问题像一个魔方——一组块映射到问题空间上。
- en: CUDA provides the simple two-dimensional grid model. For a significant number
    of problems this is entirely sufficient. If you have a linear distribution of
    work within a single block, you have an ideal decomposition into CUDA blocks.
    As we can assign up to sixteen blocks per SM and we can have up to 16 SMs (32
    on some GPUs), any number of blocks of 256 or larger is fine. In practice, we’d
    like to limit the number of elements within the block to 128, 256, or 512, so
    this in itself may drive much larger numbers of blocks with a typical dataset.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA提供了简单的二维网格模型。对于许多问题来说，这完全足够。如果在单个块内有线性分布的工作量，那么这就是一个理想的CUDA块分解。由于每个SM最多可以分配16个块，且我们最多可以有16个SM（在某些GPU上为32个SM），因此256或更大数量的块都是可以的。在实践中，我们希望将块内的元素数量限制为128、256或512，因此这本身可能会推动典型数据集中的块数量更大。
- en: When considering concurrency, consider also if there is ILP that can be exploited.
    Conceptually it’s easier to think about a single thread being associated with
    a single output data item. If, however, we can fill the GPU with threads on this
    basis and there is still more data that could be processed, can we still improve
    the throughput? The answer is yes, but only through the use of ILP.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑并发性时，也要考虑是否有ILP可以被利用。从概念上讲，更容易认为每个线程都与一个输出数据项相关联。然而，如果我们能够在这种基础上填充GPU的线程，并且仍然有更多数据可以处理，是否还能提高吞吐量呢？答案是肯定的，但这只能通过利用ILP来实现。
- en: ILP exploits the fact that instruction streams can be pipelined within the processor.
    Thus, it is more efficient to push four add operations into the queue, wait, and
    then collect them one at a time (push-push-push-push-wait), rather than perform
    them one at a time (push-wait-push-wait-push-wait-push-wait). For most GPUs, you’ll
    find an ILP level of four operations per thread works best. There are some detailed
    studies and examples of this in [Chapter 9](CHP009.html). Thus, if possible we’d
    like to process *N* elements per thread, but not to the extent that it reduces
    the overall number of active threads.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ILP利用指令流可以在处理器内进行流水线操作的事实。因此，将四个加法操作推入队列，等待，然后一次一个地收集它们（推-推-推-推-等待），比起一次一个地执行它们（推-等待-推-等待-推-等待-推-等待）要更高效。对于大多数GPU，通常每个线程的ILP水平为四个操作是最理想的。关于这一点，有一些详细的研究和示例可以参考[第9章](CHP009.html)。因此，如果可能的话，我们希望每个线程处理*N*个元素，但不要减少整体活跃线程的数量。
- en: Divide and conquer
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分治法
- en: The divide-and-conquer pattern is also a pattern for breaking down large problems
    into smaller sections, each of which can be conquered. Taken together these individual
    computations allow a much larger problem to be solved.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 分治法模式也是一种将大问题分解成更小部分的模式，每个部分都可以被解决。将这些单独的计算结果结合起来，就能解决一个更大的问题。
- en: Typically you see divide-and-conquer algorithms used with recursion. Quick sort
    is a classic example of this. It recursively partitions the data into two sets,
    those above a pivot point and those below the pivot point. When the partition
    finally consists of just two items, they are compared and swapped.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们看到分治法算法与递归一起使用。快速排序就是一个经典的例子。它递归地将数据分为两个集合，一个在基准点之上，另一个在基准点之下。当分区最终只包含两个元素时，它们会被比较并交换。
- en: Most recursive algorithms can also be represented as an iterative model, which
    is usually somewhat easier to map onto the GPU as it fits better into the primary
    tile-based decomposition model of the GPU.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数递归算法也可以表示为迭代模型，通常这种表示方式更容易映射到GPU上，因为它更适合GPU的主要基于瓦片的分解模型。
- en: Recursive algorithms are also supported on Fermi-class GPUs, although as with
    the CPU you have to be aware of the maximum call depth and translate this into
    stack usage. The available stack can be queried with API call `cudaDeviceGetLimit()`.
    It can also be set with the API call `cudaDeviceSetLimit()`. Failure to allocate
    enough stack space, as with CPUs, will result in the program failing. Some debugging
    tools such as Parallel Nsight and CUDA-GDB can detect such stack overflow issues.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 递归算法也在Fermi类GPU上得到支持，尽管与CPU一样，必须注意最大调用深度并将其转化为堆栈使用。可以通过API调用`cudaDeviceGetLimit()`查询可用的堆栈大小，也可以通过API调用`cudaDeviceSetLimit()`进行设置。与CPU一样，如果分配的堆栈空间不足，程序将失败。像Parallel
    Nsight和CUDA-GDB这样的调试工具可以检测到此类堆栈溢出问题。
- en: In selecting a recursive algorithm be aware that you are making a tradeoff of
    development time versus performance. It may be easier to conceptualize and therefore
    code a recursive algorithm than to try to convert such an approach to an iterative
    one. However, each recursive call causes any formal parameters to be pushed onto
    the stack along with any local variables. GPUs and CPUs implement a stack in the
    same way, simply an area of memory from the global memory space. Although CPUs
    and the Fermi-class GPUs cache this area, compared to passing values using registers,
    this is slow. Use iterative solutions where possible as they will generally perform
    much better and run on a wider range of GPU hardware.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择递归算法时，要意识到你正在权衡开发时间与性能之间的关系。递归算法可能比将这种方法转换为迭代算法更容易构思，因此也更容易编码。然而，每一次递归调用都会导致任何形式参数以及局部变量被压入栈中。GPU和CPU的栈实现方式相同，都是简单地将内存区域从全局内存空间分配出来。尽管CPU和Fermi级GPU会缓存这个区域，但与使用寄存器传递值相比，这仍然是较慢的。尽量使用迭代解决方案，因为它们通常性能更好，并且可以在更广泛的GPU硬件上运行。
- en: Conclusion
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: We’ve looked here at a broad overview of some parallel processing concepts and
    how these are applied to the GPU industry in particular. It’s not the purpose
    of this text to write a volume on parallel processing, for there are entire books
    devoted to just this subject. We want readers to have some feeling for the issues
    that parallel programming bring to the table that would not otherwise be thought
    about in a serial programming environment.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里概述了一些并行处理的概念，以及这些概念如何特别应用于GPU行业。本篇文章的目的并不是写一本关于并行处理的专著，因为有整本书专门讨论这个主题。我们希望读者能对并行编程带来的挑战有一定的了解，这些挑战在串行编程环境下是不会被考虑到的。
- en: In subsequent chapters we cover some of these concepts in detail in terms of
    practical examples. We also look at parallel prefix-sum, an algorithm that allows
    multiple writers of data to share a common array without writing over one another’s
    data. Such algorithms are never needed for serial based programming.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将结合实际例子详细讨论一些这些概念。我们还将探讨并行前缀和算法，这是一种允许多个数据写入者共享公共数组而不互相覆盖数据的算法。对于基于串行编程的程序，这类算法是完全不需要的。
- en: With parallelism comes a certain amount of complexity and the need for a programmer
    to think and plan ahead to consider the key issues of concurrency and locality.
    Always keep these two key concepts in mind when designing any software for the
    GPU.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 并行性带来了一定的复杂性，需要程序员提前思考并规划，考虑并发性和局部性这两个关键问题。在设计GPU软件时，始终牢记这两个关键概念。
