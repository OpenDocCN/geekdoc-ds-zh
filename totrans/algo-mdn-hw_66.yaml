- en: Moving Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://en.algorithmica.org/hpc/simd/moving/](https://en.algorithmica.org/hpc/simd/moving/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you took some time to study [the reference](https://software.intel.com/sites/landingpage/IntrinsicsGuide),
    you may have noticed that there are essentially two major groups of vector operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Instructions that perform some elementwise operation (`+`, `*`, `<`, `acos`,
    etc.).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instructions that load, store, mask, shuffle, and generally move data around.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While using the elementwise instructions is easy, the largest challenge with
    SIMD is getting the data in vector registers in the first place, with low enough
    overhead so that the whole endeavor is worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/simd/moving/#aligned-loads-and-stores)Aligned
    Loads and Stores'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operations of reading and writing the contents of a SIMD register into memory
    have two versions each: `load` / `loadu` and `store` / `storeu`. The letter “u”
    here stands for “unaligned.” The difference is that the former ones only work
    correctly when the read / written block fits inside a single [cache line](/hpc/cpu-cache/cache-lines)
    (and crash otherwise), while the latter work either way, but with a slight performance
    penalty if the block crosses a cache line.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, especially when the “inner” operation is very lightweight, the performance
    difference becomes significant (at least because you need to fetch two cache lines
    instead of one). As an extreme example, this way of adding two arrays together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '…is ~30% slower than its aligned version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the first version, assuming that arrays `a`, `b` and `c` are all 64-byte
    *aligned* (the addresses of their first elements are divisible by 64, and so they
    start at the beginning of a cache line), roughly half of reads and writes will
    be “bad” because they cross a cache line boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the performance difference is caused by the cache system and not by
    the instructions themselves. On most modern architectures, the `loadu` / `storeu`
    intrinsics should be equally as fast as `load` / `store` given that in both cases
    the blocks only span one cache line. The advantage of the latter is that they
    can act as free run time assertions that all reads and writes are aligned.
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes it important to properly [align](/hpc/cpu-cache/alignment) arrays
    and other data on allocation, and it is also one of the reasons why compilers
    can’t always [auto-vectorize](../auto-vectorization) efficiently. For most purposes,
    we only need to guarantee that any 32-byte SIMD block will not cross a cache line
    boundary, and we can specify this alignment with the `alignas` specifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The [built-in vector types](../intrinsics) already have corresponding alignment
    requirements and assume aligned memory reads and writes — so you are always safe
    when allocating an array of `v8si`, but when converting it from `int*` you have
    to make sure it is aligned.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the scalar case, many arithmetic instructions take memory addresses
    as operands — [vector addition](../intrinsics) is an example — although you can’t
    explicitly use it as an intrinsic and have to rely on the compiler. There are
    also a few other instructions for reading a SIMD block from memory, notably the
    [non-temporal](/hpc/cpu-cache/bandwidth#bypassing-the-cache) load and store operations
    that don’t lift accessed data in the cache hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/simd/moving/#register-aliasing)Register
    Aliasing'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first SIMD extension, MMX, started quite small. It only used 64-bit vectors,
    which were conveniently aliased to the mantissa part of a [80-bit float](/hpc/arithmetic/ieee-754)
    so that there is no need to introduce a separate set of registers. As the vector
    size grew with later extensions, the same [register aliasing](/hpc/architecture/assembly#instructions-and-registers)
    mechanism used in general-purpose registers was adopted for the vector registers
    to maintain backward compatibility: `xmm0` is the first half (128 bits) of `ymm0`,
    `xmm1` is the first half of `ymm1`, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: This feature, combined with the fact that the vector registers are located in
    the FPU, makes moving data between them and the general-purpose registers slightly
    complicated.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/simd/moving/#extract-and-insert)Extract
    and Insert'
  prefs: []
  type: TYPE_NORMAL
- en: To *extract* a specific value from a vector, you can use `_mm256_extract_epi32`
    and similar intrinsics. It takes the index of the integer to be extracted as the
    second parameter and generates different instruction sequences depending on its
    value.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to extract the first element, it generates the `vmovd` instruction
    (for `xmm0`, the first half of the vector):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For other elements of an SSE vector, it generates possibly slightly slower
    `vpextrd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: To extract anything from the second half of an AVX vector, it first has to extract
    that second half, and then the scalar itself. For example, here is how it extracts
    the last (eighth) element,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'There is a similar `_mm256_insert_epi32` intrinsic for overwriting specific
    elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Takeaway: moving scalar data to and from vector registers is slow, especially
    when this isn’t the first element.'
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/simd/moving/#making-constants)Making
    Constants'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to populate not just one element but the entire vector, you can
    use the `_mm256_setr_epi32` intrinsic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The “r” here stands for “reversed” — from [the CPU point of view](/hpc/arithmetic/integer#integer-types),
    not for humans. There is also the `_mm256_set_epi32` (without “r”) that fills
    the values from the opposite direction. Both are mostly used to create compile-time
    constants that are then fetched into the register with a block load. If your use
    case is filling a vector with zeros, use the `_mm256_setzero_si256` instead: it
    `xor`-s the register with itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In built-in vector types, you can just use normal braced initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '### [#](https://en.algorithmica.org/hpc/simd/moving/#broadcast)Broadcast'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of modifying just one element, you can also *broadcast* a single value
    into all its positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a frequently used operation, so you can also use a memory location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When using built-in vector types, you can create a zero vector and add a scalar
    to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '### [#](https://en.algorithmica.org/hpc/simd/moving/#mapping-to-arrays)Mapping
    to Arrays'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to avoid all this complexity, you can just dump the vector in memory
    and read its values back as scalars:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This may not be fast or technically legal (the C++ standard doesn’t specify
    what happens when you cast data like this), but it is simple, and I frequently
    use this code to print out the contents of a vector during debugging.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/simd/moving/#non-contiguous-load)Non-Contiguous
    Load'
  prefs: []
  type: TYPE_NORMAL
- en: Later SIMD extensions added special “gather” and “scatter instructions that
    read/write data non-sequentially using arbitrary array indices. These don’t work
    8 times faster though and are usually limited by the memory rather than the CPU,
    but they are still helpful for certain applications such as sparse linear algebra.
  prefs: []
  type: TYPE_NORMAL
- en: Gather is available since AVX2, and various scatter instructions are available
    since AVX512.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/1a34046ab00a6375ab2bb5e5cef95a93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s see if they work faster than scalar reads. First, we create an array
    of size $N$ and $Q$ random read queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the scalar code, we add the elements specified by the queries to a checksum
    one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'And in the SIMD code, we use the `gather` instruction to do that for 8 different
    indexes in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'They perform roughly the same, except when the array fits into the L1 cache:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/aa064e69926e477a9b25997890fce027.png)'
  prefs: []
  type: TYPE_IMG
- en: The purpose of `gather` and `scatter` is not to perform memory operations faster,
    but to get the data into registers to perform heavy computations on them. For
    anything costlier than just one addition, they are hugely favorable.
  prefs: []
  type: TYPE_NORMAL
- en: The lack of (fast) gather and scatter instructions makes SIMD programming on
    CPUs very different from proper parallel computing environments that support independent
    memory access. You have to always engineer around it and employ various ways of
    organizing your data sequentially so that it be loaded into registers. [← Intrinsics
    and Vector Types](https://en.algorithmica.org/hpc/simd/intrinsics/)[Reductions
    →](https://en.algorithmica.org/hpc/simd/reduction/)
  prefs: []
  type: TYPE_NORMAL
