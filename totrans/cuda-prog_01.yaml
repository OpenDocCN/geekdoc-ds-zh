- en: Chapter 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章
- en: A Short History of Supercomputing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超级计算机的简短历史
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: So why in a book about CUDA are we looking at supercomputers? Supercomputers
    are typically at the leading edge of the technology curve. What we see here is
    what will be commonplace on the desktop in 5 to 10 years. In 2010, the annual
    International Supercomputer Conference in Hamburg, Germany, announced that a NVIDIA
    GPU-based machine had been listed as the second most powerful computer in the
    world, according to the top 500 list ([http://www.top500.org](http://www.top500.org)).
    Theoretically, it had more peak performance than the mighty IBM Roadrunner, or
    the then-leader, the Cray Jaguar, peaking at near to 3 petaflops of performance.
    In 2011, NVIDIA CUDA-powered GPUs went on to claim the title of the fastest supercomputer
    in the world. It was suddenly clear to everyone that GPUs had arrived in a very
    big way on the high-performance computing landscape, as well as the humble desktop
    PC.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么在一本关于CUDA的书中，我们要关注超级计算机呢？超级计算机通常处于技术曲线的前沿。我们现在看到的技术，未来五到十年内将成为桌面计算机的普遍配置。在2010年，德国汉堡举行的年度国际超级计算机大会宣布，基于NVIDIA
    GPU的计算机被列为全球第二大最强计算机，依据的是《TOP500》榜单([http://www.top500.org](http://www.top500.org))。理论上，它的峰值性能超过了强大的IBM
    Roadrunner，或当时的领头者Cray Jaguar，后者的峰值性能接近3拍拉弗。在2011年，NVIDIA CUDA驱动的GPU赢得了全球最快超级计算机的称号。突然间，大家都明白了，GPU已经在高性能计算领域以及普通桌面PC中大放异彩。
- en: Supercomputing is the driver of many of the technologies we see in modern-day
    processors. Thanks to the need for ever-faster processors to process ever-larger
    datasets, the industry produces ever-faster computers. It is through some of these
    evolutions that GPU CUDA technology has come about today.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 超级计算是现代处理器中许多技术的推动力。由于需要更快的处理器来处理越来越大的数据集，工业界不断生产更快的计算机。正是在这些技术发展的过程中，GPU CUDA技术才得以诞生。
- en: Both supercomputers and desktop computing are moving toward a heterogeneous
    computing route—that is, they are trying to achieve performance with a mix of
    CPU (Central Processor Unit) and GPU (Graphics Processor Unit) technology. Two
    of the largest worldwide projects using GPUs are BOINC and Folding@Home, both
    of which are distributed computing projects. They allow ordinary people to make
    a real contribution to specific scientific projects. Contributions from CPU/GPU
    hosts on projects supporting GPU accelerators hugely outweigh contributions from
    CPU-only hosts. As of November 2011, there were some 5.5 million hosts contributing
    a total of around 5.3 petaflops, around half that of the world’s fastest supercomputer,
    in 2011, the Fujitsu “K computer” in Japan.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是超级计算机还是桌面计算，都朝着异构计算的方向发展——也就是说，它们试图通过结合CPU（中央处理单元）和GPU（图形处理单元）技术来实现更高的性能。全球最大的两个使用GPU的项目是BOINC和Folding@Home，两个都是分布式计算项目。它们让普通人能够为特定的科学项目做出实际贡献。支持GPU加速器的项目中，CPU/GPU主机的贡献远远超过了仅支持CPU的主机。截至2011年11月，约有550万个主机贡献了总计约5.3拍拉弗的计算能力，这相当于2011年全球最快超级计算机——日本富士通的“K计算机”一半的性能。
- en: The replacement for Jaguar, currently the fastest U.S. supercomputer, code-named
    Titan, is planned for 2013\. It will use almost 300,000 CPU cores and up to 18,000
    GPU boards to achieve between 10 and 20 petaflops per second of performance. With
    support like this from around the world, GPU programming is set to jump into the
    mainstream, both in the HPC industry and also on the desktop.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 目前美国最快的超级计算机——代号“Titan”的Jaguar替代品，计划于2013年投入使用。它将使用近30万个CPU核心和最多18,000块GPU板卡，达到每秒10到20拍拉弗的性能。在全球范围内的支持下，GPU编程有望迅速进入主流，无论是在HPC行业，还是在桌面领域。
- en: You can now put together or purchase a desktop supercomputer with several teraflops
    of performance. At the beginning of 2000, some 12 years ago, this would have given
    you first place in the top 500 list, beating IBM ASCI Red with its 9632 Pentium
    processors. This just shows how much a little over a decade of computing progress
    has achieved and opens up the question about where we will be a decade from now.
    You can be fairly certain GPUs will be at the forefront of this trend for some
    time to come. Thus, learning how to program GPUs effectively is a key skill any
    good developer needs to acquire.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Von Neumann Architecture
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost all processors work on the basis of the process developed by Von Neumann,
    considered one of the fathers of computing. In this approach, the processor fetches
    instructions from memory, decodes, and then executes that instruction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: A modern processor typically runs at anything up to 4 GHz in speed. Modern DDR-3
    memory, when paired with say a standard Intel I7 device, can run at anything up
    to 2 GHz. However, the I7 has at least four processors or cores in one device,
    or double that if you count its hyperthreading ability as a real processor.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: A DDR-3 triple-channel memory setup on a I7 Nehalem system would produce the
    theoretical bandwidth figures shown in [Table 1.1](#T0010). Depending on the motherboard,
    and exact memory pattern, the actual bandwidth could be considerably less.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.1 Bandwidth on I7 Nehalem Processor
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '| QPI Clock | Theoretical Bandwidth | Per Core |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
- en: '| 4.8 GT/s (standard part) | 19.2 GB/s | 4.8 GB/s |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
- en: '| 6.4 GT/s (extreme edition) | 25.6 GB/s | 6.4 GB/s |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
- en: 'Note: QPI = Quick Path Interconnect.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: You run into the first problem with memory bandwidth when you consider the processor
    clock speed. If you take a processor running at 4 GHz, you need to potentially
    fetch, every cycle, an instruction (an operator) plus some data (an operand).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Each instruction is typically 32 bits, so if you execute nothing but a set of
    linear instructions, with no data, on every core, you get 4.8 GB/s ÷ 4 = 1.2 GB
    instructions per second. This assumes the processor can dispatch one instruction
    per clock on average^([∗](#FN1)). However, you typically also need to fetch and
    write back data, which if we say is on a 1:1 ratio with instructions, means we
    effectively halve our throughput.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The ratio of clock speed to memory is an important limiter for both CPU and
    GPU throughput and something we’ll look at later. We find when you look into it,
    most applications, with a few exceptions on both CPU and GPU, are often memory
    bound and not processor cycle or processor clock/load bound.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'CPU vendors try to solve this problem by using cache memory and burst memory
    access. This exploits the principle of locality. It you look at a typical C program,
    you might see the following type of operation in a function:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '` int i = 0;`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you look at how the processor would typically implement this, you would see
    the address of `array` loaded into some memory access register. The parameter
    `i` would be loaded into another register. The loop exit condition, 100, is loaded
    into another register or possibly encoded into the instruction stream as a literal
    value. The computer would then iterate around the same instructions, over and
    over again 100 times. For each value calculated, we have control, memory, and
    calculation instructions, fetched and executed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看处理器通常如何实现这一点，你会看到`array`的地址被加载到某个内存访问寄存器中。参数`i`会被加载到另一个寄存器中。循环退出条件100会被加载到另一个寄存器，或者可能以字面值的形式编码到指令流中。计算机随后会重复执行相同的指令，反复执行100次。对于每个计算值，我们都有控制、内存和计算指令被提取并执行。
- en: This is clearly inefficient, as the computer is executing the same instructions,
    but with different data values. Thus, the hardware designers implement into just
    about all processors a small amount of cache, and in more complex processors,
    many levels of cache ([Figure 1.1](#F0010)). When the processor would fetch something
    from memory, the processor first queries the cache, and if the data or instructions
    are present there, the high-speed cache provides them to the processor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种做法显然效率低下，因为计算机正在执行相同的指令，但数据值不同。因此，硬件设计师在几乎所有处理器中实现了一小部分缓存，在更复杂的处理器中，有许多级别的缓存（[图1.1](#F0010)）。当处理器需要从内存中提取数据时，首先查询缓存，如果数据或指令已存在缓存中，高速缓存就会将其提供给处理器。
- en: '![image](../images/F000016f01-01-9780124159334.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-01-9780124159334.jpg)'
- en: Figure 1.1 Typical modern CPU cache organization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 典型的现代CPU缓存组织。
- en: 'If the data is not in the first level (L1) cache, then a fetch from the second
    or third level (L2 or L3) cache is required, or from the main memory if no cache
    line has this data already. The first level cache typically runs at or near the
    processor clock speed, so for the execution of our loop, potentially we do get
    near the full processor speed, assuming we write cache as well as read cache.
    However, there is a cost for this: The size of the L1 cache is typically only
    16 K or 32 K in size. The L2 cache is somewhat slower, but much larger, typically
    around 256 K. The L3 cache is much larger, usually several megabytes in size,
    but again much slower than the L2 cache.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据不在第一级（L1）缓存中，则需要从第二级或第三级（L2或L3）缓存中提取，或者如果没有缓存行包含该数据，则从主内存中提取。第一级缓存通常与处理器时钟速度相匹配，因此对于我们的循环执行，理论上我们能接近处理器的全速，假设我们同时写入和读取缓存。然而，这也有代价：L1缓存的大小通常只有16K或32K。L2缓存较慢，但更大，通常为256K左右。L3缓存更大，通常为数兆字节，但比L2缓存慢得多。
- en: With real-life examples, the loop iterations are much, much larger, maybe many
    megabytes in size. Even if the program can remain in cache memory, the dataset
    usually cannot, so the processor, despite all this cache trickery, is quite often
    limited by the memory throughput or bandwidth.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，循环的迭代次数通常要大得多，可能达到数兆字节大小。即使程序能保持在缓存内存中，数据集通常也不能。因此，尽管有这些缓存技巧，处理器通常会受到内存吞吐量或带宽的限制。
- en: When the processor fetches an instruction or data item from the cache instead
    of the main memory, it’s called a cache hit. The incremental benefit of using
    progressively larger caches drops off quite rapidly. This in turn means the ever-larger
    caches we see on modern processors are a less and less useful means to improve
    performance, unless they manage to encompass the *entire* dataset of the problem.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理器从缓存而非主内存中提取指令或数据项时，这被称为缓存命中。使用逐步增大的缓存所带来的增益会迅速下降。这也意味着，我们在现代处理器上看到的越来越大的缓存，除非它们能够涵盖问题的*整个*数据集，否则在提高性能方面的效果越来越小。
- en: The Intel I7-920 processor has some 8 MB of internal L3 cache. This cache memory
    is not free, and if we look at the die for the Intel I7 processor, we see around
    30% of the size of the chip is dedicated to the L3 cache memory ([Figure 1.2](#F0015)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔I7-920处理器具有大约8MB的内部L3缓存。这些缓存内存并非免费的，如果我们查看英特尔I7处理器的芯片，我们会看到大约30%的芯片面积被用于L3缓存内存（[图1.2](#F0015)）。
- en: '![image](../images/F000016f01-02-9780124159334.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-02-9780124159334.jpg)'
- en: Figure 1.2 Layout of I7 Nehalem processor on processor die.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 I7 Nehalem处理器在处理器芯片上的布局。
- en: As cache sizes grow, so does the physical size of the silicon used to make the
    processors. The larger the chip, the more expensive it is to manufacture and the
    higher the likelihood that it will contain an error and be discarded during the
    manufacturing process. Sometimes these faulty devices are sold cheaply as either
    triple- or dual-core devices, with the faulty cores disabled. However, the effect
    of larger, progressively more inefficient caches ultimately results in higher
    costs to the end user.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着缓存大小的增长，用来制造处理器的硅片物理尺寸也在增大。芯片越大，制造成本就越高，而且它包含错误并在制造过程中被丢弃的可能性也越大。有时，这些有缺陷的设备会被便宜出售，作为三核或双核设备，其中有缺陷的核心被禁用。然而，越来越大、效率越来越低的缓存最终会导致终端用户的成本上升。
- en: Cray
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cray
- en: The computing revolution that we all know today started back in the 1950s with
    the advent of the first microprocessors. These devices, by today’s standards,
    are slow and you most likely have a far more powerful processor in your smartphone.
    However, these led to the evolution of supercomputers, which are machines usually
    owned by governments, large academic institutions, or corporations. They are thousands
    of times more powerful than the computers in general use today. They cost millions
    of dollars to produce, occupy huge amounts of space, usually have special cooling
    requirements, and require a team of engineers to look after them. They consume
    huge amounts of power, to the extent they are often as expensive to run each year
    as they cost to build. In fact, power is one of the key considerations when planning
    such an installation and one of the main limiting factors in the growth of today’s
    supercomputers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天所知的计算革命始于1950年代，随着第一批微处理器的问世。这些设备按照今天的标准，运行速度很慢，你现在的智能手机可能拥有比它更强大的处理器。然而，这些设备为超级计算机的演变奠定了基础，超级计算机通常由政府、大型学术机构或公司拥有。它们比今天常规使用的计算机强大数千倍。它们的生产成本数百万美元，占用大量空间，通常需要特殊的冷却条件，并且需要一个工程师团队来维护。它们消耗巨量的电力，甚至每年的运行费用可能与建造成本相当。事实上，电力是规划此类设施时需要重点考虑的因素之一，也是今天超级计算机发展中的主要限制因素之一。
- en: One of the founders of modern supercomputers was Seymour Cray with his Cray-1,
    produced by Cray Research back in 1976\. It had many thousands of individual cables
    required to connect everything together—so much so they used to employ women because
    their hands were smaller than those of most men and they could therefore more
    easily wire up all the thousands of individual cables.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现代超级计算机的创始人之一是 Seymour Cray，他的Cray-1由Cray Research于1976年生产。这款计算机需要数千根电缆来连接所有组件——以至于他们曾经雇佣女性员工，因为女性的手比大多数男性的小，因此能够更轻松地连接这些数千根电缆。
- en: These machines would typically have an uptime (the actual running time between
    breakdowns) measured in hours. Keeping them running for a whole day at a time
    would be considered a huge
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机器通常的正常运行时间（即故障间的实际运行时间）是按小时计算的。能够连续运行一天的时间被认为是一个巨大的成就。
- en: '![image](../images/F000016f01-03-9780124159334.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-03-9780124159334.jpg)'
- en: Figure 1.3 Wiring inside the Cray-2 supercomputer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 Cray-2 超级计算机内部线路图。
- en: achievement. This seems quite backward by today’s standards. However, we owe
    a lot of what we have today to research carried out by Seymour Cray and other
    individuals of this era.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 成就。以今天的标准来看，这似乎相当落后。然而，我们今天所拥有的许多成就，归功于 Seymour Cray 以及那个时代其他研究人员的努力。
- en: Cray went on to produce some of the most groundbreaking supercomputers of his
    time under various Cray names. The original Cray-1 cost some $8.8 million USD
    and achieved a massive 160 MFLOPS (million floating-point operations per second).
    Computing speed today is measured in TFLOPS (tera floating-point operations per
    second), a million times larger than the old MFLOPS measurement (10^(12) vs. 10⁶).
    A single Fermi GPU card today has a theoretical peak in excess of 1 teraflop of
    performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Cray继续以多个Cray品牌生产一些当时最具突破性的超级计算机。原始的Cray-1售价约为880万美元，达到了160 MFLOPS（百万浮点运算每秒）。如今，计算速度以TFLOPS（万亿浮点运算每秒）为单位衡量，比旧的MFLOPS标准大一百万倍（10^(12)
    对比 10⁶）。今天一张Fermi GPU卡的理论峰值性能超过1 teraflop。
- en: The Cray-2 was a significant improvement on the Cray-1\. It used a shared memory
    architecture, split into banks. These were connected to one, two, or four processors.
    It led the way for the creation of today’s server-based symmetrical multiprocessor
    (SMP) systems in which multiple CPUs shared the same memory space. Like many machines
    of its era, it was a vector-based machine. In a vector machine the same operation
    acts on many operands. These still exist today, in part as processor extensions
    such as MMX, SSE, and AVX. GPU devices are, at their heart, vector processors
    that share many similarities with the older supercomputer designs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: The Cray also had hardware support for scatter- and gather-type primitives,
    something we’ll see is quite important in parallel computing and something we
    look at in subsequent chapters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Cray still exists today in the supercomputer market, and as of 2010 held the
    top 500 position with their Jaguar supercomputer at the Oak Ridge National Laboratory
    ([http://www.nccs.gov/computing-resources/jaguar/](http://www.nccs.gov/computing-resources/jaguar/)).
    I encourage you to read about the history of this great company, which you can
    find on Cray’s website ([http://www.cray.com](http://www.cray.com)), as it gives
    some insight into the evolution of computers and as to where we are today.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Connection Machine
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Back in 1982 a corporation called Thinking Machines came up with a very interesting
    design, that of the Connection Machine.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: It was a relatively simple concept that led to a revolution in today’s parallel
    computers. They used a few simple parts over and over again. They created a 16-core
    CPU, and then installed some 4096 of these devices in one machine. The concept
    was different. Instead of one fast processor churning through a dataset, there
    were 64 K processors doing this task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take the simple example of manipulating the color of an RGB (red, green,
    blue) image. Each color is made up of a single byte, with 3 bytes representing
    the color of a single pixel. Let’s suppose we want to reduce the blue level to
    zero.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume the memory is configured in three banks of red, blue, and green,
    rather than being interleaved. With a conventional processor, we would have a
    loop running through the blue memory and decrement every pixel color level by
    one. The operation is the same on each item of data, yet each time we fetch, decode,
    and execute the instruction stream on each loop iteration.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The Connection Machine used something called SIMD (single instruction, multiple
    data), which is used today in modern processors and known by names such as SSE
    (Streaming SIMD Extensions), MMX (Multi-Media eXtension), and AVX (Advanced Vector
    eXtensions). The concept is to define a data range and then have the processor
    apply that operation to the data range. However, SSE and MMX are based on having
    one processor core. The Connection Machine had 64 K processor cores, each executing
    SIMD instructions on its dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Processors such as the Intel I7 are 64-bit processors, meaning they can process
    up to 64 bits at a time (8 bytes). The SSE SIMD instruction set extends this to
    128 bits. With SIMD instructions on such a processor, we eliminate all redundant
    instruction memory fetches, and generate one sixteenth of the memory read and
    write cycles compared with fetching and writing 1 byte at a time. AVX extends
    this to 256 bits, making it even more effective.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Intel I7 这样的处理器是 64 位处理器，这意味着它们一次可以处理最多 64 位（8 字节）。SSE SIMD 指令集将其扩展到 128 位。使用这样的处理器上的
    SIMD 指令，我们消除了所有冗余的指令内存获取，并且与逐次获取和写入 1 字节相比，生成的内存读写周期仅为其 1/16。AVX 将这一扩展到 256 位，效果更加显著。
- en: For a high-definition (HD) video image of 1920 × 1080 resolution, the data size
    is 2,073,600 bytes, or around 2 MB per color plane. Thus, we generate around 260,000
    SIMD cycles for a single conventional processor using SSE/MMX. By SIMD cycle,
    we mean one read, compute, and write cycle. The actual number of processor clocks
    may be considerably different than this, depending on the particular processor
    architecture.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 1920 × 1080 分辨率的高清（HD）视频图像，数据大小为 2,073,600 字节，约为每个颜色平面 2 MB。因此，使用 SSE/MMX
    的单一常规处理器将生成约 260,000 个 SIMD 周期。我们所说的 SIMD 周期，是指一次读取、计算和写入的周期。实际的处理器时钟数量可能与此相差较大，具体取决于特定的处理器架构。
- en: The Connection Machine used 64 K processors. Thus, the 2 MB frame would have
    resulted in about 32 SIMD cycles for each processor. Clearly, this type of approach
    is vastly superior to the modern processor SIMD approach. However, there is of
    course a caveat. Synchronizing and communication between processors becomes the
    major issue when moving from a rather coarse-threaded approach of today’s CPUs
    to a hugely parallel approach used by such machines.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Connection Machine 使用了 64 K 个处理器。因此，2 MB 的帧数据将为每个处理器生成大约 32 个 SIMD 周期。显然，这种方法远远优于现代处理器的
    SIMD 方法。然而，当然也有一个警告。当从今天 CPU 的粗线程化方法转向这种机器使用的极度并行方法时，处理器之间的同步和通信将成为主要问题。
- en: Cell Processor
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cell 处理器
- en: Another interesting development in supercomputers stemmed from IBM’s invention
    of the Cell processor ([Figure 1.4](#F0025)). This worked on the idea of having
    a regular processor act as a supervisory processor, connected to a number of high-speed
    stream processors. The regular PowerPC (PPC)processor in the Cell acts as an interface
    to the stream processors and the outside world. The stream SIMD processors, or
    SPEs as IBM called them, would process datasets managed by the regular processor.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 超级计算机中的另一个有趣发展来源于 IBM 发明的 Cell 处理器（[图 1.4](#F0025)）。这一设计的思路是让一个常规处理器充当监督处理器，连接到多个高速流处理器。Cell
    中的常规 PowerPC（PPC）处理器作为流处理器和外部世界之间的接口。流 SIMD 处理器，或 IBM 所称的 SPE，将处理由常规处理器管理的数据集。
- en: '![image](../images/F000016f01-04-9780124159334.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-04-9780124159334.jpg)'
- en: Figure 1.4 IBM cell processor die layout (8 SPE version).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 IBM Cell 处理器芯片布局（8 SPE 版本）。
- en: The Cell is a particularly interesting processor for us, as it’s a similar design
    to what NVIDIA later used in the G80 and subsequent GPUs. Sony also used it in
    their PS3 console machines in the games industry, a very similar field to the
    main use of GPUs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，Cell 是一个特别有趣的处理器，因为它的设计与 NVIDIA 后来在 G80 和随后的 GPU 中使用的设计类似。索尼也在其 PS3 游戏主机中使用了它，这是一个与
    GPU 主要用途非常相似的领域。
- en: To program the Cell, you write a program to execute on the PowerPC core processor.
    It then invokes a program, using an entirely different binary, on each of the
    stream processing elements (SPEs). Each SPE is actually a core in itself. It can
    execute an independent program from its own local memory, which is different from
    the SPE next to it. In addition, the SPEs can communicate with one another and
    the PowerPC core over a shared interconnect. However, this type of hybrid architecture
    is not easy to program. The programmer must explicitly manage the eight SPEs,
    both in terms of programs and data, as well as the serial program running on the
    PowerPC core.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要编程 Cell，你需要编写一个程序在 PowerPC 核心处理器上执行。然后，它会在每个流处理单元（SPE）上调用一个完全不同的二进制程序。每个 SPE
    实际上都是一个独立的核心。它可以从自己的本地内存中执行独立的程序，这与旁边的 SPE 不同。此外，SPE 之间以及与 PowerPC 核心之间可以通过共享互连进行通信。然而，这种混合架构并不容易编程。程序员必须明确管理八个
    SPE，无论是程序还是数据，以及在 PowerPC 核心上运行的串行程序。
- en: With the ability to talk directly to the coordinating processor, a series of
    simple steps can be achieved. With our RGB example earlier, the PPC core fetches
    a chunk of data to work on. It allocates these to the eight SPEs. As we do the
    same thing in each SPE, each SPE fetches the byte, decrements it, and writes its
    bit back to its local memory. When all SPEs are done, the PC core fetches the
    data from each SPE. It then writes its chunk of data (or tile) to the memory area
    where the whole image is being assembled. The Cell processor is designed to be
    used in groups, thus repeating the design of the Connection Machine we covered
    earlier.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过能够直接与协调处理器通信，可以实现一系列简单的步骤。以我们之前的 RGB 示例为例，PPC 核心获取一块数据进行处理。它将这些数据分配给八个 SPE。当每个
    SPE 执行相同的操作时，每个 SPE 获取字节，递减该字节，并将其位写回到本地内存。当所有 SPE 完成后，PC 核心从每个 SPE 获取数据。然后它将数据块（或图块）写入正在组装整个图像的内存区域。Cell
    处理器设计为以组方式使用，因此它重复了我们之前讨论过的连接机器的设计。
- en: The SPEs could also be ordered to perform a stream operation, involving multiple
    steps, as each SPE is connected to a high-speed ring ([Figure 1.5](#F0030)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: SPEs 也可以被指令执行流操作，涉及多个步骤，因为每个 SPE 都连接到一个高速环形网络（[图 1.5](#F0030)）。
- en: '![image](../images/F000016f01-05-9780124159334.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-05-9780124159334.jpg)'
- en: Figure 1.5 Example routing stream processor routing on Cell.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 示例路由流处理器在 Cell 上的路由。
- en: The problem with this sort of streaming or pipelining approach is it runs only
    as fast as the slowest node. It mirrors a production line in a factory. The whole
    line can only run as fast as the slowest point. Each SPE (worker) only has a small
    set of tasks to perform, so just like the assembly line worker, it can do this
    very quickly and efficiently. However, just like any processor, there is a bandwidth
    limit and overhead of passing data to the next stage. Thus, while you gain efficiencies
    from executing a consistent program on each SPE, you lose on interprocessor communication
    and are ultimately limited by the slowest process step. This is a common problem
    with any pipeline-based model of execution.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种流处理或流水线处理方法的问题在于，它的运行速度仅受限于最慢的节点。这就像工厂里的生产线一样，整条生产线只能按照最慢的环节运行。每个 SPE（工人）只需执行一小部分任务，因此就像装配线工人一样，它可以非常快速和高效地完成这些任务。然而，就像任何处理器一样，它也存在带宽限制和将数据传递到下一个阶段的开销。因此，虽然你可以通过在每个
    SPE 上执行一致的程序来提高效率，但在处理器之间的通信上则会有所损失，并且最终会受到最慢步骤的限制。这是任何基于流水线的执行模型中的常见问题。
- en: The alternative approach of putting everything on one SPE and then having each
    SPE process a small chunk of data is often a more efficient approach. This is
    the equivalent to training all assembly line workers to assemble a complete widget.
    For simple tasks, this is easy, but each SPE has limits on available program and
    data memory. The PowerPC core must now also deliver and collect data from eight
    SPEs, instead of just two, so the management overhead and communication between
    host and SPEs increases.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将所有任务放在一个 SPE 上，然后让每个 SPE 处理一小块数据，这通常是一种更高效的做法。这相当于训练所有装配线工人组装一个完整的组件。对于简单的任务来说，这很容易，但每个
    SPE 对可用程序和数据内存都有一定限制。PowerPC 核心现在必须同时处理来自八个 SPE 的数据，而不是仅仅两个，因此管理开销和主机与 SPE 之间的通信会增加。
- en: IBM used a high-powered version of the Cell processor in their Roadrunner supercomputer,
    which as of 2010 was the third fastest computer on the top 500 list. It consists
    of 12,960 PowerPC cores, plus a total of 103,680 stream processors. Each PowerPC
    board is supervised by a dual-core AMD (Advanced Micro Devices) Opteron processor,
    of which there are 6912 in total. The Opteron processors act as coordinators among
    the nodes. Roadrunner has a theoretical throughput of 1.71 petaflops, cost $125
    million USD to build, occupies 560 square meters, and consumes 2.35 MW of electricity
    when operating!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: IBM 在其 Roadrunner 超级计算机中使用了高性能版本的 Cell 处理器，该计算机截至 2010 年排名全球超级计算机 TOP 500 的第三快计算机。它由
    12,960 个 PowerPC 核心以及 103,680 个流处理器组成。每个 PowerPC 板由一个双核的 AMD（高级微设备公司）Opteron 处理器进行监督，总共有
    6912 个 Opteron 处理器。这些 Opteron 处理器充当节点之间的协调者。Roadrunner 的理论吞吐量为 1.71 拍浮点运算每秒（petaflops），建设成本为
    1.25 亿美元，占地 560 平方米，运行时消耗 2.35 兆瓦的电力！
- en: Multinode Computing
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多节点计算
- en: As you increase the requirements (CPU, memory, storage space) needed on a single
    machine, costs rapidly increase. While a 2.6 GHz processor may cost you $250 USD,
    the same processor at 3.4 GHz may be $1400 for less than a 1 GHz increase in clock
    speed. A similar relationship is seen for both speed and size memory, and storage
    capacity.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当你提高单台机器所需的要求（如CPU、内存、存储空间）时，成本会迅速增加。比如一个2.6 GHz的处理器可能需要250美元，而一个3.4 GHz的同款处理器可能要1400美元，时钟频率增加不到1
    GHz，却导致成本大幅上涨。类似的情况也出现在速度、内存大小和存储容量上。
- en: Not only do costs scale as computing requirements scale, but so do the power
    requirements and the consequential heat dissipation issues. Processors can hit
    4–5 GHz, given sufficient supply of power and cooling.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算需求的增加，成本不仅增加，电力需求和由此产生的散热问题也会增加。只要提供足够的电力和冷却，处理器的频率可以达到4–5 GHz。
- en: In computing you often find the law of diminishing returns. There is only so
    much you can put into a single case. You are limited by cost, space, power, and
    heat. The solution is to select a reasonable balance of each and to replicate
    this many times.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算中，你常常会遇到收益递减法则。单一设备能承载的东西是有限的，你会受到成本、空间、电力和热量的限制。解决方法是选择每个因素的合理平衡，并将其多次复制。
- en: Cluster computing became popular in 1990s along with ever-increasing clock rates.
    The concept was a very simple one. Take a number of commodity PCs bought or made
    from off-the-shelf parts and connect them to an off-the-shelf 8-, 16-, 24-, or
    32-port Ethernet switch and you had up to 32 times the performance of a single
    box. Instead of paying $1600 for a high performance processor, you paid $250 and
    bought six medium performance processors. If your application needed huge memory
    capacity, the chances were that maxing out the DIMMs on many machines and adding
    them together was more than sufficient. Used together, the combined power of many
    machines hugely outperformed any single machine you could possible buy with a
    similar budget.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 集群计算在1990年代随着时钟频率的不断提升而变得流行。这个概念非常简单：用若干台从商用部件购买或制造的普通PC，将它们连接到一个8、16、24或32端口的以太网交换机上，你就能得到单台计算机32倍的性能。你不再需要为一个高性能处理器支付1600美元，而是花250美元买六个中等性能的处理器。如果你的应用需要巨大的内存容量，最大化多台机器的DIMM内存并将它们相加往往已经足够。多台机器的合力性能远远超过了你用相同预算所能买到的任何单台机器。
- en: All of a sudden universities, schools, offices, and computer departments could
    build machines much more powerful than before and were not locked out of the high-speed
    computing market due to lack of funds. Cluster computing back then was like GPU
    computing today—a disruptive technology that changed the face of computing. Combined
    with the ever-increasing single-core clock speeds it provided a cheap way to achieve
    parallel processing within single-core CPUs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 突然之间，大学、学校、办公室和计算机部门可以构建比以往更强大的机器，不再因为资金不足而无法进入高速计算市场。那时候的集群计算就像今天的GPU计算一样，是一种颠覆性的技术，改变了计算的面貌。再加上不断提高的单核时钟频率，它提供了一种廉价的方式，在单核CPU上实现并行计算。
- en: Clusters of PCs typically ran a variation of LINUX with each node usually fetching
    its boot instructions and operating system (OS) from a central master node. For
    example, at CudaDeveloper we have a tiny cluster of low-powered, atom-based PCs
    with embedded CUDA GPUs. It’s very cheap to buy and set up a cluster. Sometimes
    they can simply be made from a number of old PCs that are being replaced, so the
    hardware is effectively free.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: PC集群通常运行着某种版本的LINUX，每个节点通常从中央主节点获取启动指令和操作系统（OS）。例如，在CudaDeveloper，我们有一个由低功耗、基于Atom的PC和嵌入式CUDA
    GPU组成的小型集群。购买和搭建一个集群非常便宜。有时这些集群甚至可以由一些正在被替换掉的旧PC组成，因此硬件实际上是免费的。
- en: However, the problem with cluster computing is it’s only as fast as the amount
    of internode communication that is necessary for the problem. If you have 32 nodes
    and the problem breaks down into 32 nice chunks and requires no internode communication,
    you have an application that is ideal for a cluster. If every data point takes
    data from every node, you have a terrible problem to put into a cluster.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，集群计算的问题在于，它的速度取决于问题所需的节点间通信量。如果你有32个节点，而问题能被分解成32个相互独立的部分，并且不需要节点间的通信，那么你有一个非常适合集群的应用。如果每个数据点都需要从每个节点获取数据，那么将这样的应用放入集群中就会遇到很大的问题。
- en: Clusters are seen inside modern CPUs and GPUs. Look back at [Figure 1.1](#F0010),
    the CPU cache hierarchy. If we consider each CPU core as a node, the L2 cache
    as DRAM (Dynamic Random Access Memory), the L3 cache as the network switch, and
    the DRAM as mass storage, we have a cluster in miniature ([Figure 1.6](#F0035)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 集群出现在现代CPU和GPU中。回顾[图1.1](#F0010)，CPU缓存层次结构。如果我们将每个CPU核心视为一个节点，将L2缓存视为DRAM（动态随机存取存储器），将L3缓存视为网络交换机，将DRAM视为大容量存储，那么我们就得到了一个微型集群（参见[图1.6](#F0035)）。
- en: '![image](../images/F000016f01-06-9780124159334.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-06-9780124159334.jpg)'
- en: Figure 1.6 Typical cluster layout.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 典型集群布局。
- en: The architecture inside a modern GPU is really no different. You have a number
    of streaming multiprocessors (SMs) that are akin to CPU cores. These are connected
    to a shared memory/L1 cache. This is connected to an L2 cache that acts as an
    inter-SM switch. Data can be held in global memory storage where it’s then extracted
    and used by the host, or sent via the PCI-E switch directly to the memory on another
    GPU. The PCI-E switch is many times faster than any network’s interconnect.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现代GPU内部的架构与此并无太大区别。你会有多个类似于CPU核心的流式多处理器（SM）。这些处理器连接到共享内存/L1缓存。然后，它们连接到作为SM间交换机的L2缓存。数据可以保存在全局内存存储中，然后由主机提取并使用，或者通过PCI-E交换机直接发送到另一个GPU的内存。PCI-E交换机的速度远远超过任何网络的互连速度。
- en: The node may itself be replicated many times, as shown in [Figure 1.7](#F0040).
    This replication within a controlled environment forms a cluster. One evolution
    of the cluster designs are distributed applications. Distributed applications
    run on many nodes, each of which may contain many processing elements including
    GPUs. Distributed applications may, but do not need to, run in a controlled environment
    of a managed cluster. They can connect arbitrary machines together to work on
    some common problem, BOINC and Folding@Home being two of the largest examples
    of such applications that connect machines together over the Internet.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 节点本身可以多次复制，如[图1.7](#F0040)所示。这种在受控环境中的复制形成了集群。集群设计的一个演变是分布式应用程序。分布式应用程序在多个节点上运行，每个节点可能包含多个处理单元，包括GPU。分布式应用程序可以，也不一定需要，在管理集群的受控环境中运行。它们可以将任意机器连接在一起，共同处理某个问题，BOINC和Folding@Home是这类通过互联网将机器连接在一起的应用程序中的两个最大例子。
- en: '![image](../images/F000016f01-07-9780124159334.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-07-9780124159334.jpg)'
- en: Figure 1.7 GPUs compared to a cluster.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 GPUs与集群的对比。
- en: The Early Days of Gpgpu Coding
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gpgpu编码的早期
- en: Graphics processing units (GPUs) are devices present in most modern PCs. They
    provide a number of basic operations to the CPU, such as rendering an image in
    memory and then displaying that image onto the screen. A GPU will typically process
    a complex set of polygons, a map of the scene to be rendered. It then applies
    textures to the polygons and then performs shading and lighting calculations.
    The NVIDIA 5000 series cards brought for the first time photorealistic effects,
    such as shown in the Dawn Fairy demo from 2003.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图形处理单元（GPU）是大多数现代PC中存在的设备。它们为CPU提供一系列基本操作，例如在内存中渲染图像，并将该图像显示在屏幕上。GPU通常会处理一组复杂的多边形，这些多边形是要渲染的场景的映射。然后，它会将纹理应用于这些多边形，并进行着色和光照计算。NVIDIA
    5000系列显卡首次带来了照片级真实感效果，如2003年“黎明仙子”演示中所展示的效果。
- en: Have a look at [http://www.nvidia.com/object/cool_stuff.html#/demos](http://www.nvidia.com/object/cool_stuff.html%23/demos)
    and download some of the older demos and you’ll see just how much GPUs have evolved
    over the past decade. See [Table 1.2](#T0015).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[http://www.nvidia.com/object/cool_stuff.html#/demos](http://www.nvidia.com/object/cool_stuff.html%23/demos)，并下载一些较旧的演示，你将看到GPU在过去十年中是如何发展的。请参见[表1.2](#T0015)。
- en: Table 1.2 GPU Technology Demonstrated over the Years
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.2 多年来展示的GPU技术
- en: '| Demo | Card | Year |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 演示 | 显卡 | 年份 |'
- en: '| --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Dawn | GeForce FX | 2003 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 黎明 | GeForce FX | 2003 |'
- en: '| Dusk Ultra | GeForce FX | 2003 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 黄昏Ultra | GeForce FX | 2003 |'
- en: '| Nalu | GeForce 6 | 2004 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Nalu | GeForce 6 | 2004 |'
- en: '| Luna | GeForce 7 | 2005 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 月亮 | GeForce 7 | 2005 |'
- en: '| Froggy | GeForce 8 | 2006 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 青蛙 | GeForce 8 | 2006 |'
- en: '| Human Head | GeForce 8 | 2007 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 人头 | GeForce 8 | 2007 |'
- en: '| Medusa | GeForce 200 | 2008 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 美杜莎 | GeForce 200 | 2008 |'
- en: '| Supersonic Sled | GeForce 400 | 2010 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 超音速雪橇 | GeForce 400 | 2010 |'
- en: '| A New Dawn | GeForce 600 | 2012 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 新黎明 | GeForce 600 | 2012 |'
- en: One of the important steps was the development of programmable shaders. These
    were effectively little programs that the GPU ran to calculate different effects.
    No longer was the rendering fixed in the GPU; through downloadable shaders, it
    could be manipulated. This was the first evolution of general-purpose graphical
    processor unit (GPGPU) programming, in that the design had taken its first steps
    in moving away from fixed function units.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: However, these shaders were operations that by their very nature took a set
    of 3D points that represented a polygon map. The shaders applied the same operation
    to many such datasets, in a hugely parallel manner, giving huge throughput of
    computing power.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Now although polygons are sets of three points, and some other datasets such
    as RGB photos can be represented by sets of three points, a lot of datasets are
    not. A few brave researchers made use of GPU technology to try and speed up general-purpose
    computing. This led to the development of a number of initiatives (e.g., BrookGPU,
    Cg, CTM, etc.), all of which were aimed at making the GPU a real programmable
    device in the same way as the CPU. Unfortunately, each had its own advantages
    and problems. None were particularly easy to learn or program in and were never
    taught to people in large numbers. In short, there was never a critical mass of
    programmers or a critical mass of interest from programmers in this hard-to-learn
    technology. They never succeeded in hitting the mass market, something CUDA has
    for the first time managed to do, and at the same time provided programmers with
    a truly general-purpose language for GPUs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: The Death of the Single-Core Solution
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the problems with today’s modern processors is they have hit a clock
    rate limit at around 4 GHz. At this point they just generate too much heat for
    the current technology and require special and expensive cooling solutions. This
    is because as we increase the clock rate, the power consumption rises. In fact,
    the power consumption of a CPU, if you fix the voltage, is approximately the cube
    of its clock rate. To make this worse, as you increase the heat generated by the
    CPU, for the same clock rate, the power consumption also increases due to the
    properties of the silicon. This conversion of power into heat is a complete waste
    of energy. This increasingly inefficient use of power eventually means you are
    unable to either power or cool the processor sufficiently and you reach the thermal
    limits of the device or its housing, the so-called power wall.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Faced with not being able to increase the clock rate, making forever-faster
    processors, the processor manufacturers had to come up with another game plan.
    The two main PC processor manufacturers, Intel and AMD, have had to adopt a different
    approach. They have been forced down the route of adding more cores to processors,
    rather than continuously trying to increase CPU clock rates and/or extract more
    instructions per clock through instruction-level parallelism. We have dual, tri,
    quad, hex, 8, 12, and soon even 16 and 32 cores and so on. This is the future
    of where computing is now going for everyone, the GPU and CPU communities. The
    Fermi GPU is effectively already a 16-core device in CPU terms.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: There is a big problem with this approach—it requires programmers to switch
    from their traditional serial, single-thread approach, to dealing with multiple
    threads all executing at once. Now the programmer has to think about two, four,
    six, or eight program threads and how they interact and communicate with one another.
    When dual-core CPUs arrived, it was fairly easy, in that there were usually some
    background tasks being done that could be offloaded onto a second core. When quad-core
    CPUs arrived, not many programs were changed to support it. They just carried
    on being sold as single-thread applications. Even the games industry didn’t really
    move to quad-core programming very quickly, which is the one industry you’d expect
    to want to get the absolute most out of today’s technology.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: In some ways the processor manufacturers are to blame for this, because the
    single-core application runs just fine on one-quarter of the quad-core device.
    Some devices even increase the clock rate dynamically when only one core is active,
    encouraging programmers to be lazy and not make use of the available hardware.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: There are economic reasons too. The software development companies need to get
    the product to market as soon as possible. Developing a better quad-core solution
    is all well and good, but not if the market is being grabbed by a competitor who
    got there first. As manufacturers still continue to make single- and dual-core
    devices, the market naturally settles on the lowest configuration, with the widest
    scope for sales. Until the time that quad-core CPUs are the minimum produced,
    market forces work against the move to multicore programming in the CPU market.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Nvidia and Cuda
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you look at the relative computational power in GPUs and CPUs, we get an
    interesting graph ([Figure 1.8](#F0045)). We start to see a divergence of CPU
    and GPU computational power until 2009 when we see the GPU finally break the 1000
    gigaflops or 1 teraflop barrier. At this point we were moving from the G80 hardware
    to the G200 and then in 2010 to the Fermi evolution. This is driven by the introduction
    of massively parallel hardware. The G80 is a 128 CUDA core device, the G200 is
    a 256 CUDA core device, and the Fermi is a 512 CUDA core device.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![image](../images/F000016f01-08-9780124159334.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 CPU and GPU peak performance in gigaflops.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: We see NVIDIA GPUs make a leap of 300 gigaflops from the G200 architecture to
    the Fermi architecture, nearly a 30% improvement in throughput. By comparison,
    Intel’s leap from their core 2 architecture to the Nehalem architecture sees only
    a minor improvement. Only with the change to Sandy Bridge architecture do we see
    significant leaps in CPU performance. This is not to say one is better than the
    other, for the traditional CPUs are aimed at serial code execution and are extremely
    good at it. They contain special hardware such as branch prediction units, multiple
    caches, etc., all of which target serial code execution. The GPUs are not designed
    for this serial execution flow and only achieve their peak performance when fully
    utilized in a parallel manner.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: In 2007, NVIDIA saw an opportunity to bring GPUs into the mainstream by adding
    an easy-to-use programming interface, which it dubbed CUDA, or Compute Unified
    Device Architecture. This opened up the possibility to program GPUs without having
    to learn complex shader languages, or to think only in terms of graphics primitives.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: CUDA is an extension to the C language that allows GPU code to be written in
    regular C. The code is either targeted for the host processor (the CPU) or targeted
    at the device processor (the GPU). The host processor spawns multithread tasks
    (or kernels as they are known in CUDA) onto the GPU device. The GPU has its own
    internal scheduler that will then allocate the kernels to whatever GPU hardware
    is present. We’ll cover scheduling in detail later. Provided there is enough parallelism
    in the task, as the number of SMs in the GPU grows, so should the speed of the
    program.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: However, herein hides a big problem. You have to ask what percentage of the
    code can be run in parallel. The maximum speedup possible is limited by the amount
    of serial code. If you have an infinite amount of processing power and could do
    the parallel tasks in zero time, you would still be left with the time from the
    serial code part. Therefore, we have to consider at the outset if we can indeed
    parallelize a significant amount of the workload.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA is committed to providing support to CUDA. Considerable information,
    examples, and tools to help with development are available from its website at
    [http://www.nvidia.com](http://www.nvidia.com) under CudaZone.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: CUDA, unlike its predecessors, has now actually started to gain momentum and
    for the first time it looks like there will be a programming language that will
    emerge as the one of choice for GPU programming. Given that the number of CUDA-enabled
    GPUs now number in the millions, there is a huge market out there waiting for
    CUDA-enabled applications.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: There are currently many CUDA-enabled applications and the list grows monthly.
    NVIDIA showcases many of these on its community website at [http://www.nvidia.com/object/cuda_apps_flash_new.html](http://www.nvidia.com/object/cuda_apps_flash_new.html).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: In areas where programs have to do a lot of computational work—for example,
    making a DVD from your home movies (video transcoding)—we see most mainstream
    video packages now supporting CUDA. The average speedup is 5 to 10 times in this
    domain.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Along with the introduction of CUDA came the Tesla series of cards. These cards
    are not graphics cards, and in fact they have no DVI or VGA connectors on them.
    They are dedicated compute cards aimed at scientific computing. Here we see huge
    speedups in scientific calculations. These cards can either be installed in a
    regular desktop PC or in dedicated server racks. NVIDIA provides such a system
    at [http://www.nvidia.com/object/preconfigured_clusters.html](http://www.nvidia.com/object/preconfigured_clusters.html),
    which claims to provide up to 30 times the power of a conventional cluster. CUDA
    and GPUs are reshaping the world of high-performance computing.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Gpu Hardware
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The NVIDIA G80 series processor and beyond implemented a design that is similar
    to both the Connection Machine and IBM’s Cell processor. Each graphics card consists
    of a number of SMs. To each SM is attached eight or more SPs (Stream Processors).
    The original 9800 GTX card has eight SMs, giving a total of 128 SPs. However,
    unlike the Roadrunner, each GPU board can be purchased for a few hundred USD and
    it doesn’t take 2.35 MW to power it. Power considerations are not to be overlooked,
    as we’ll discuss later when we talk about building GPU servers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: The GPU cards can broadly be considered as accelerator or coprocessor cards.
    A GPU card, currently, must operate in conjunction with a CPU-based host. In this
    regard it follows very much the approach of the Cell processor with the regular
    serial core and N SIMD SPE cores. Each GPU device contains a set of SMs, each
    of which contain a set of SPs or CUDA cores. The SPs execute work as parallel
    sets of up to 32 units. They eliminate a lot of the complex circuitry needed on
    CPUs to achieve high-speed serial execution through instruction-level parallelism.
    They replace this with a programmer-specified explicit parallelism model, allowing
    more compute capacity to be squeezed onto the same area of silicon.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The overall throughput of GPUs is largely determined by the number of SPs present,
    the bandwidth to the global memory, and how well the programmer makes use of the
    parallel architecture he or she is working with. See [Table 1.3](#T0020) for a
    listing of current NVIDIA GPU cards.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.3 Current Series of NVIDIA GPU Cards
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](../images/T000016tabT0020.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: Which board is correct for a given application is a balance between memory and
    GPU processing power needed for a given application. Note the 9800 GX2, 295, 590,
    690, and K10 cards are actually dual cards, so to make full use of these they
    need to be programmed as two devices not one. The one caveat GPU here is that
    the figures quoted are for single-precision (32-bit) floating-point performance,
    not double-precision (64-bit) precision. Also be careful with the GF100 (Fermi)
    series, as the Tesla variant has double the number of double-precision units found
    in the standard desktop units, so achieves significantly better double-precision
    throughput. The Kepler K 20, yet to be released, will also have significant double
    precision performance over and above its already released K10 cousin.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Note also, although not shown here, as the generations have evolved, the power
    consumption, clock for clock, per SM has come down. However, the overall power
    consumption has increased considerably and this is one of the key considerations
    in any multi-GPU-based solution. Typically, we see dual-GPU-based cards (9800
    GX2, 295, 590, 690) having marginally lower power consumption figures than the
    equivalent two single cards due to the use of shared circuitry and/or reduced
    clock frequencies.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA provides various racks (the M series computing modules) containing two
    to four Tesla cards connected on a shared PCI-E bus for high-density computing.
    It’s quite possible to build your own GPU cluster or microsupercomputer from standard
    PC parts, and we show you how to do this later in the book.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about CUDA is that, despite all the variability in hardware,
    programs written for the original CUDA devices can run on today’s CUDA devices.
    The CUDA compilation model applies the same principle as used in Java—runtime
    compilation of a virtual instruction set. This allows modern GPUs to execute code
    from even the oldest generation GPUs. In many cases they benefit significantly
    from the original programmer reworking the program for the features of the newer
    GPUs. In fact, there is considerable scope for tuning for the various hardware
    revisions, which we’ll cover toward the end of the book.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Alternatives to Cuda
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenCL
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So what of the other GPU manufacturers, ATI (now AMD) being the prime example?
    AMD’s product range is as impressive as the NVIDIA range in terms of raw computer
    power. However, AMD brought its stream computing technology to the marketplace
    a long time after NVIDIA brought out CUDA. As a consequence, NVIDA has far more
    applications available for CUDA than AMD/ATI does for its competing stream technology.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL and Direct compute is not something we’ll cover in this book, but they
    deserve a mention in terms of alternatives to CUDA. CUDA is currently only officially
    executable on NVIDIA hardware. While NVIDIA has a sizeable chunk of the GPU market,
    its competitors also hold a sizeable chunk. As developers, we want to develop
    products for as large a market as possible, especially if we’re talking about
    the consumer market. As such, people should be aware there are alternatives to
    CUDA, which support both NVIDIA’s and others’ hardware.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: OpenCL is an open and royalty-free standard supported by NVIDIA, AMD, and others.
    The OpenCL trademark is owned by Apple. It sets out an open standard that allows
    the use of compute devices. A compute device can be a GPU, CPU, or other specialist
    device for which an OpenCL driver exists. As of 2012, OpenCL supports all major
    brands of GPU devices, including CPUs with at least SSE3 support.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Anyone who is familiar with CUDA can pick up OpenCL relatively easily, as the
    fundamental concepts are quite similar. However, OpenCL is somewhat more complex
    to use than CUDA, in that much of the work the CUDA runtime API does for the programmer
    needs to be explicitly performed in OpenCL.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about OpenCL at [http://www.khronos.org/opencl/](http://www.khronos.org/opencl/).
    There are also now a number of books written on OpenCL. I’d personally recommend
    learning CUDA prior to OpenCL as CUDA is somewhat of a higher-level language extension
    than OpenCL.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: DirectCompute
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DirectCompute is Microsoft’s alternative to CUDA and OpenCL. It is a proprietary
    product linked to the Windows operating system, and in particular, the DirectX
    11 API. The DirectX API was a huge leap forward for any of those who remember
    programming video cards before it. It meant the developers had to learn only one
    library API to program all graphics cards, rather than write or license drivers
    for each major video card manufacturer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: DirectX 11 is the latest standard and supported under Windows 7\. With Microsoft’s
    name behind the standard, you might expect to see some quite rapid adoption among
    the developer community. This is especially the case with developers already familiar
    with DirectX APIs. If you are familiar with CUDA and DirectCompute, then it is
    quite an easy task to port a CUDA application over to DirectCompute. According
    to Microsoft, this is something you can typically do in an afternoon’s work if
    you are familiar with both systems. However, being Windows centric, we’ll exclude
    DirectCompute from many high-end systems where the various flavors of UNIX dominate.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft are also set to launch C++ AMP, an additional set of standard template
    libraries (STLs), which may appeal more to programmers already familiar with C++-style
    STLs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: CPU alternatives
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main parallel processing languages extensions are MPI, OpenMP, and pthreads
    if you are developing for Linux. For Windows there is the Windows threading model
    and OpenMP. MPI and pthreads are supported as various ports from the Unix world.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '**MPI** (Message Passing Interface) is perhaps the most widely known messaging
    interface. It is process-based and generally found in large computing labs. It
    requires an administrator to configure the installation correctly and is best
    suited to controlled environments. Parallelism is expressed by spawning hundreds
    of processes over a cluster of nodes and explicitly exchanging messages, typically
    over high-speed network-based communication links (Ethernet or InfiniBand). MPI
    is widely used and taught. It’s a good solution within a controlled cluster environment.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenMP** (Open Multi-Processing) is a system designed for parallelism within
    a node or computer system. It works entirely differently, in that the programmer
    specifies various parallel directives through compiler pragmas. The compiler then
    attempts to automatically split the problem into *N* parts, according to the number
    of available processor cores. OpenMP support is built into many compilers, including
    the NVCC compiler used for CUDA. OpenMP tends to hit problems with scaling due
    to the underlying CPU architecture. Often the memory bandwidth in the CPU is just
    not large enough for all the cores continuously streaming data to or from memory.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '**Pthreads** is a library that is used significantly for multithread applications
    on Linux. As with OpenMP, pthreads uses threads and not processes as it is designed
    for parallelism within a single node. However, unlike OpenMP, the programmer is
    responsible for thread management and synchronization. This provides more flexibility
    and consequently better performance for well-written programs.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '**ZeroMQ** (0MQ) is also something that deserves a mention. This is a simple
    library that you link to, and we will use it later in the book for developing
    a multinode, multi-GPU example. ZeroMQ supports thread-, process-, and network-based
    communications models with a single cross-platform API. It is also available on
    both Linux and Windows platforms. It’s designed for distributed computing, so
    the connections are dynamic and nodes fail gracefully.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '**Hadoop** is also something that you may consider. Hadoop is an open-source
    version of Google’s MapReduce framework. It’s aimed primarily at the Linux platform.
    The concept is that you take a huge dataset and break (or map) it into a number
    of chunks. However, instead of sending the data to the node, the dataset is already
    split over hundreds or thousands of nodes using a parallel file system. Thus,
    the program, the reduce step, is instead sent to the node that contains the data.
    The output is written to the local node and remains there. Subsequent MapReduce
    programs take the previous output and again transform it in some way. As data
    is in fact mirrored to multiple nodes, this allows for a highly fault-tolerant
    as well as high-throughput system.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Directives and libraries
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are a number of compiler vendors, PGI, CAPS, and Cray being the most well-known,
    that support the recently announced OpenACC set of compiler directives for GPUs.
    These, in essence, replicate the approach of OpenMP, in that the programmer inserts
    a number of compiler directives marking regions as “to be executed on the GPU.”
    The compiler then does the grunt work of moving data to or from the GPU, invoking
    kernels, etc.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: As with the use of pthreads over OpenMP, with the lower level of control pthreads
    provides, you can achieve higher performance. The same is true of CUDA versus
    OpenACC. This extra level of control comes with a much higher level of required
    programming knowledge, a higher risk of errors, and the consequential time impact
    that may have on a development schedule. Currently, OpenACC requires directives
    to specify what areas of code should be run on the GPU, but also in which type
    of memory data should exist. NVIDIA claims you can get in the order of 5×-plus
    speedup using such directives. It’s a good solution for those programmers who
    need to get something working quickly. It’s also great for those people for whom
    programming is a secondary consideration who just want the answer to their problem
    in a reasonable timeframe.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: The use of libraries is also another key area where you can obtain some serious
    productivity gains, as well as execution time speedups. Libraries like SDK provide
    Thrust, which provides common functions implemented in a very efficient way. Libraries
    like CUBLAS are some of the best around for linear algebra. Libraries exist for
    many well-known applications such as Matlab and Mathematica. Language bindings
    exist for Python, Perl, Java, and many others. CUDA can even be integrated with
    Excel.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: As with many aspects of software development in the modern age, the chances
    are that someone has done what you are about to develop already. Search the Internet
    and see what is already there before you spend weeks developing a library that,
    unless you are a CUDA expert, is unlikely to be faster than one that is already
    available.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So maybe you’re thinking, why develop in CUDA? The answer is that CUDA is currently
    the easiest language to develop in, in terms of support, debugging tools, and
    drivers. CUDA has a head start on everything else and has a huge lead in terms
    of maturity. If your application needs to support hardware other than NVIDIA’s,
    then the best route currently is to develop under CUDA and then port the application
    to one of the other APIs. As such, we’ll concentrate on CUDA, for if you become
    an expert with CUDA, it’s easy to pick up alternative APIs should you need to.
    Understanding how CUDA works will allow you to better exploit and understand the
    limitations of any higher-level API.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The journey from a single-thread CPU programmer to a fully fledged parallel
    programmer on GPUs is one that I hope you will find interesting. Even if you never
    program a GPU in the future, the insight you gain will be of tremendous help in
    allowing you to design multithread programs. If you, like us, see the world changing
    to a parallel programming model, you’ll want to be at the forefront of that wave
    of innovation and technological challenge. The single-thread industry is one that
    is slowly moving to obsolescence. To be a valuable asset and an employable individual,
    you need to have skills that reflect where the computing world is headed to, not
    those that are becoming progressively obsolete.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are changing the face of computing. All of a sudden the computing power
    of supercomputers from a decade ago can be slotted under your desk. No longer
    must you wait in a queue to submit work batches and wait months for a committee
    to approve your request to use limited computer resources at overstretched computing
    installations. You can go out, spend up to 5000–10,000 USD, and have a supercomputer
    on your desk, or a development machine that runs CUDA for a fraction of that.
    GPUs are a disruptive technological change that will make supercomputer-like levels
    of performance available for everyone.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[^∗](#CFN1)The actual achieved dispatch rate can be higher or lower than one,
    which we use here for simplicity.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
