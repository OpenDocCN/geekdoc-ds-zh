- en: Chapter 1
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章
- en: A Short History of Supercomputing
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超级计算机的简短历史
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: So why in a book about CUDA are we looking at supercomputers? Supercomputers
    are typically at the leading edge of the technology curve. What we see here is
    what will be commonplace on the desktop in 5 to 10 years. In 2010, the annual
    International Supercomputer Conference in Hamburg, Germany, announced that a NVIDIA
    GPU-based machine had been listed as the second most powerful computer in the
    world, according to the top 500 list ([http://www.top500.org](http://www.top500.org)).
    Theoretically, it had more peak performance than the mighty IBM Roadrunner, or
    the then-leader, the Cray Jaguar, peaking at near to 3 petaflops of performance.
    In 2011, NVIDIA CUDA-powered GPUs went on to claim the title of the fastest supercomputer
    in the world. It was suddenly clear to everyone that GPUs had arrived in a very
    big way on the high-performance computing landscape, as well as the humble desktop
    PC.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么在一本关于CUDA的书中，我们要关注超级计算机呢？超级计算机通常处于技术曲线的前沿。我们现在看到的技术，未来五到十年内将成为桌面计算机的普遍配置。在2010年，德国汉堡举行的年度国际超级计算机大会宣布，基于NVIDIA
    GPU的计算机被列为全球第二大最强计算机，依据的是《TOP500》榜单([http://www.top500.org](http://www.top500.org))。理论上，它的峰值性能超过了强大的IBM
    Roadrunner，或当时的领头者Cray Jaguar，后者的峰值性能接近3拍拉弗。在2011年，NVIDIA CUDA驱动的GPU赢得了全球最快超级计算机的称号。突然间，大家都明白了，GPU已经在高性能计算领域以及普通桌面PC中大放异彩。
- en: Supercomputing is the driver of many of the technologies we see in modern-day
    processors. Thanks to the need for ever-faster processors to process ever-larger
    datasets, the industry produces ever-faster computers. It is through some of these
    evolutions that GPU CUDA technology has come about today.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 超级计算是现代处理器中许多技术的推动力。由于需要更快的处理器来处理越来越大的数据集，工业界不断生产更快的计算机。正是在这些技术发展的过程中，GPU CUDA技术才得以诞生。
- en: Both supercomputers and desktop computing are moving toward a heterogeneous
    computing route—that is, they are trying to achieve performance with a mix of
    CPU (Central Processor Unit) and GPU (Graphics Processor Unit) technology. Two
    of the largest worldwide projects using GPUs are BOINC and Folding@Home, both
    of which are distributed computing projects. They allow ordinary people to make
    a real contribution to specific scientific projects. Contributions from CPU/GPU
    hosts on projects supporting GPU accelerators hugely outweigh contributions from
    CPU-only hosts. As of November 2011, there were some 5.5 million hosts contributing
    a total of around 5.3 petaflops, around half that of the world’s fastest supercomputer,
    in 2011, the Fujitsu “K computer” in Japan.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是超级计算机还是桌面计算，都朝着异构计算的方向发展——也就是说，它们试图通过结合CPU（中央处理单元）和GPU（图形处理单元）技术来实现更高的性能。全球最大的两个使用GPU的项目是BOINC和Folding@Home，两个都是分布式计算项目。它们让普通人能够为特定的科学项目做出实际贡献。支持GPU加速器的项目中，CPU/GPU主机的贡献远远超过了仅支持CPU的主机。截至2011年11月，约有550万个主机贡献了总计约5.3拍拉弗的计算能力，这相当于2011年全球最快超级计算机——日本富士通的“K计算机”一半的性能。
- en: The replacement for Jaguar, currently the fastest U.S. supercomputer, code-named
    Titan, is planned for 2013\. It will use almost 300,000 CPU cores and up to 18,000
    GPU boards to achieve between 10 and 20 petaflops per second of performance. With
    support like this from around the world, GPU programming is set to jump into the
    mainstream, both in the HPC industry and also on the desktop.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 目前美国最快的超级计算机——代号“Titan”的Jaguar替代品，计划于2013年投入使用。它将使用近30万个CPU核心和最多18,000块GPU板卡，达到每秒10到20拍拉弗的性能。在全球范围内的支持下，GPU编程有望迅速进入主流，无论是在HPC行业，还是在桌面领域。
- en: You can now put together or purchase a desktop supercomputer with several teraflops
    of performance. At the beginning of 2000, some 12 years ago, this would have given
    you first place in the top 500 list, beating IBM ASCI Red with its 9632 Pentium
    processors. This just shows how much a little over a decade of computing progress
    has achieved and opens up the question about where we will be a decade from now.
    You can be fairly certain GPUs will be at the forefront of this trend for some
    time to come. Thus, learning how to program GPUs effectively is a key skill any
    good developer needs to acquire.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以组装或购买一台桌面超级计算机，性能达到几个 TFLOPS。大约在 2000 年初，12 年前，这样的性能可以让你在全球500强榜单上位居第一，超过
    IBM ASCI Red 的 9632 个 Pentium 处理器。这仅仅展示了十多年来计算机技术的进步，也引发了一个问题：十年后我们会处于什么位置？你可以相当确定，GPU
    将在这个趋势中占据前沿位置一段时间。因此，学习如何有效地编程 GPU 是任何优秀开发者需要掌握的关键技能。
- en: Von Neumann Architecture
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 冯·诺依曼架构
- en: Almost all processors work on the basis of the process developed by Von Neumann,
    considered one of the fathers of computing. In this approach, the processor fetches
    instructions from memory, decodes, and then executes that instruction.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有处理器都基于冯·诺依曼开发的流程工作，他被认为是计算机科学的奠基人之一。在这种方法中，处理器从内存中获取指令，解码，然后执行该指令。
- en: A modern processor typically runs at anything up to 4 GHz in speed. Modern DDR-3
    memory, when paired with say a standard Intel I7 device, can run at anything up
    to 2 GHz. However, the I7 has at least four processors or cores in one device,
    or double that if you count its hyperthreading ability as a real processor.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现代处理器的运行速度通常可以达到 4 GHz。现代的 DDR-3 内存，当与像标准 Intel I7 设备配对时，速度可以达到 2 GHz。然而，I7
    至少有四个处理器或核心，如果你将其超线程能力算作真实的处理器，数量则是其两倍。
- en: A DDR-3 triple-channel memory setup on a I7 Nehalem system would produce the
    theoretical bandwidth figures shown in [Table 1.1](#T0010). Depending on the motherboard,
    and exact memory pattern, the actual bandwidth could be considerably less.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 I7 Nehalem 系统上，DDR-3 三通道内存配置将产生 [表 1.1](#T0010) 中显示的理论带宽数据。根据主板和内存模式的不同，实际带宽可能会低得多。
- en: Table 1.1 Bandwidth on I7 Nehalem Processor
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1 I7 Nehalem 处理器带宽
- en: '| QPI Clock | Theoretical Bandwidth | Per Core |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| QPI 时钟 | 理论带宽 | 每核心 |'
- en: '| --- | --- | --- |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 4.8 GT/s (standard part) | 19.2 GB/s | 4.8 GB/s |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 4.8 GT/s（标准版） | 19.2 GB/s | 4.8 GB/s |'
- en: '| 6.4 GT/s (extreme edition) | 25.6 GB/s | 6.4 GB/s |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 6.4 GT/s（极限版） | 25.6 GB/s | 6.4 GB/s |'
- en: 'Note: QPI = Quick Path Interconnect.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注：QPI = 快速路径互联。
- en: You run into the first problem with memory bandwidth when you consider the processor
    clock speed. If you take a processor running at 4 GHz, you need to potentially
    fetch, every cycle, an instruction (an operator) plus some data (an operand).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑处理器时钟速度时，你会遇到内存带宽的第一个问题。如果你有一个运行在 4 GHz 的处理器，你可能需要在每个周期获取一条指令（运算符）以及一些数据（操作数）。
- en: Each instruction is typically 32 bits, so if you execute nothing but a set of
    linear instructions, with no data, on every core, you get 4.8 GB/s ÷ 4 = 1.2 GB
    instructions per second. This assumes the processor can dispatch one instruction
    per clock on average^([∗](#FN1)). However, you typically also need to fetch and
    write back data, which if we say is on a 1:1 ratio with instructions, means we
    effectively halve our throughput.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 每条指令通常为 32 位，因此如果你只执行一组线性指令，且每个核心上都没有数据，你将得到 4.8 GB/s ÷ 4 = 1.2 GB 每秒的指令吞吐量。这假设处理器平均每时钟周期可以调度一条指令^([∗](#FN1))。然而，你通常还需要获取和写回数据，如果我们假设数据与指令的比例是
    1:1，那么实际上我们将吞吐量减半。
- en: The ratio of clock speed to memory is an important limiter for both CPU and
    GPU throughput and something we’ll look at later. We find when you look into it,
    most applications, with a few exceptions on both CPU and GPU, are often memory
    bound and not processor cycle or processor clock/load bound.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 时钟速度与内存的比率是 CPU 和 GPU 吞吐量的一个重要限制因素，我们稍后会讨论。当你深入研究时，会发现大多数应用程序，无论是 CPU 还是 GPU，都往往受限于内存，而不是处理器周期或处理器时钟/负载。
- en: 'CPU vendors try to solve this problem by using cache memory and burst memory
    access. This exploits the principle of locality. It you look at a typical C program,
    you might see the following type of operation in a function:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 厂商通过使用缓存内存和突发内存访问来解决这个问题。这利用了局部性原理。如果你查看一个典型的 C 程序，你可能会在一个函数中看到以下类型的操作：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '` int i = 0;`'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`int i = 0;`'
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you look at how the processor would typically implement this, you would see
    the address of `array` loaded into some memory access register. The parameter
    `i` would be loaded into another register. The loop exit condition, 100, is loaded
    into another register or possibly encoded into the instruction stream as a literal
    value. The computer would then iterate around the same instructions, over and
    over again 100 times. For each value calculated, we have control, memory, and
    calculation instructions, fetched and executed.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看处理器通常如何实现这一点，你会看到`array`的地址被加载到某个内存访问寄存器中。参数`i`会被加载到另一个寄存器中。循环退出条件100会被加载到另一个寄存器，或者可能以字面值的形式编码到指令流中。计算机随后会重复执行相同的指令，反复执行100次。对于每个计算值，我们都有控制、内存和计算指令被提取并执行。
- en: This is clearly inefficient, as the computer is executing the same instructions,
    but with different data values. Thus, the hardware designers implement into just
    about all processors a small amount of cache, and in more complex processors,
    many levels of cache ([Figure 1.1](#F0010)). When the processor would fetch something
    from memory, the processor first queries the cache, and if the data or instructions
    are present there, the high-speed cache provides them to the processor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这种做法显然效率低下，因为计算机正在执行相同的指令，但数据值不同。因此，硬件设计师在几乎所有处理器中实现了一小部分缓存，在更复杂的处理器中，有许多级别的缓存（[图1.1](#F0010)）。当处理器需要从内存中提取数据时，首先查询缓存，如果数据或指令已存在缓存中，高速缓存就会将其提供给处理器。
- en: '![image](../images/F000016f01-01-9780124159334.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-01-9780124159334.jpg)'
- en: Figure 1.1 Typical modern CPU cache organization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 典型的现代CPU缓存组织。
- en: 'If the data is not in the first level (L1) cache, then a fetch from the second
    or third level (L2 or L3) cache is required, or from the main memory if no cache
    line has this data already. The first level cache typically runs at or near the
    processor clock speed, so for the execution of our loop, potentially we do get
    near the full processor speed, assuming we write cache as well as read cache.
    However, there is a cost for this: The size of the L1 cache is typically only
    16 K or 32 K in size. The L2 cache is somewhat slower, but much larger, typically
    around 256 K. The L3 cache is much larger, usually several megabytes in size,
    but again much slower than the L2 cache.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据不在第一级（L1）缓存中，则需要从第二级或第三级（L2或L3）缓存中提取，或者如果没有缓存行包含该数据，则从主内存中提取。第一级缓存通常与处理器时钟速度相匹配，因此对于我们的循环执行，理论上我们能接近处理器的全速，假设我们同时写入和读取缓存。然而，这也有代价：L1缓存的大小通常只有16K或32K。L2缓存较慢，但更大，通常为256K左右。L3缓存更大，通常为数兆字节，但比L2缓存慢得多。
- en: With real-life examples, the loop iterations are much, much larger, maybe many
    megabytes in size. Even if the program can remain in cache memory, the dataset
    usually cannot, so the processor, despite all this cache trickery, is quite often
    limited by the memory throughput or bandwidth.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，循环的迭代次数通常要大得多，可能达到数兆字节大小。即使程序能保持在缓存内存中，数据集通常也不能。因此，尽管有这些缓存技巧，处理器通常会受到内存吞吐量或带宽的限制。
- en: When the processor fetches an instruction or data item from the cache instead
    of the main memory, it’s called a cache hit. The incremental benefit of using
    progressively larger caches drops off quite rapidly. This in turn means the ever-larger
    caches we see on modern processors are a less and less useful means to improve
    performance, unless they manage to encompass the *entire* dataset of the problem.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理器从缓存而非主内存中提取指令或数据项时，这被称为缓存命中。使用逐步增大的缓存所带来的增益会迅速下降。这也意味着，我们在现代处理器上看到的越来越大的缓存，除非它们能够涵盖问题的*整个*数据集，否则在提高性能方面的效果越来越小。
- en: The Intel I7-920 processor has some 8 MB of internal L3 cache. This cache memory
    is not free, and if we look at the die for the Intel I7 processor, we see around
    30% of the size of the chip is dedicated to the L3 cache memory ([Figure 1.2](#F0015)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔I7-920处理器具有大约8MB的内部L3缓存。这些缓存内存并非免费的，如果我们查看英特尔I7处理器的芯片，我们会看到大约30%的芯片面积被用于L3缓存内存（[图1.2](#F0015)）。
- en: '![image](../images/F000016f01-02-9780124159334.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-02-9780124159334.jpg)'
- en: Figure 1.2 Layout of I7 Nehalem processor on processor die.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 I7 Nehalem处理器在处理器芯片上的布局。
- en: As cache sizes grow, so does the physical size of the silicon used to make the
    processors. The larger the chip, the more expensive it is to manufacture and the
    higher the likelihood that it will contain an error and be discarded during the
    manufacturing process. Sometimes these faulty devices are sold cheaply as either
    triple- or dual-core devices, with the faulty cores disabled. However, the effect
    of larger, progressively more inefficient caches ultimately results in higher
    costs to the end user.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着缓存大小的增长，用来制造处理器的硅片物理尺寸也在增大。芯片越大，制造成本就越高，而且它包含错误并在制造过程中被丢弃的可能性也越大。有时，这些有缺陷的设备会被便宜出售，作为三核或双核设备，其中有缺陷的核心被禁用。然而，越来越大、效率越来越低的缓存最终会导致终端用户的成本上升。
- en: Cray
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cray
- en: The computing revolution that we all know today started back in the 1950s with
    the advent of the first microprocessors. These devices, by today’s standards,
    are slow and you most likely have a far more powerful processor in your smartphone.
    However, these led to the evolution of supercomputers, which are machines usually
    owned by governments, large academic institutions, or corporations. They are thousands
    of times more powerful than the computers in general use today. They cost millions
    of dollars to produce, occupy huge amounts of space, usually have special cooling
    requirements, and require a team of engineers to look after them. They consume
    huge amounts of power, to the extent they are often as expensive to run each year
    as they cost to build. In fact, power is one of the key considerations when planning
    such an installation and one of the main limiting factors in the growth of today’s
    supercomputers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天所知的计算革命始于1950年代，随着第一批微处理器的问世。这些设备按照今天的标准，运行速度很慢，你现在的智能手机可能拥有比它更强大的处理器。然而，这些设备为超级计算机的演变奠定了基础，超级计算机通常由政府、大型学术机构或公司拥有。它们比今天常规使用的计算机强大数千倍。它们的生产成本数百万美元，占用大量空间，通常需要特殊的冷却条件，并且需要一个工程师团队来维护。它们消耗巨量的电力，甚至每年的运行费用可能与建造成本相当。事实上，电力是规划此类设施时需要重点考虑的因素之一，也是今天超级计算机发展中的主要限制因素之一。
- en: One of the founders of modern supercomputers was Seymour Cray with his Cray-1,
    produced by Cray Research back in 1976\. It had many thousands of individual cables
    required to connect everything together—so much so they used to employ women because
    their hands were smaller than those of most men and they could therefore more
    easily wire up all the thousands of individual cables.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现代超级计算机的创始人之一是 Seymour Cray，他的Cray-1由Cray Research于1976年生产。这款计算机需要数千根电缆来连接所有组件——以至于他们曾经雇佣女性员工，因为女性的手比大多数男性的小，因此能够更轻松地连接这些数千根电缆。
- en: These machines would typically have an uptime (the actual running time between
    breakdowns) measured in hours. Keeping them running for a whole day at a time
    would be considered a huge
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机器通常的正常运行时间（即故障间的实际运行时间）是按小时计算的。能够连续运行一天的时间被认为是一个巨大的成就。
- en: '![image](../images/F000016f01-03-9780124159334.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-03-9780124159334.jpg)'
- en: Figure 1.3 Wiring inside the Cray-2 supercomputer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 Cray-2 超级计算机内部线路图。
- en: achievement. This seems quite backward by today’s standards. However, we owe
    a lot of what we have today to research carried out by Seymour Cray and other
    individuals of this era.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 成就。以今天的标准来看，这似乎相当落后。然而，我们今天所拥有的许多成就，归功于 Seymour Cray 以及那个时代其他研究人员的努力。
- en: Cray went on to produce some of the most groundbreaking supercomputers of his
    time under various Cray names. The original Cray-1 cost some $8.8 million USD
    and achieved a massive 160 MFLOPS (million floating-point operations per second).
    Computing speed today is measured in TFLOPS (tera floating-point operations per
    second), a million times larger than the old MFLOPS measurement (10^(12) vs. 10⁶).
    A single Fermi GPU card today has a theoretical peak in excess of 1 teraflop of
    performance.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Cray继续以多个Cray品牌生产一些当时最具突破性的超级计算机。原始的Cray-1售价约为880万美元，达到了160 MFLOPS（百万浮点运算每秒）。如今，计算速度以TFLOPS（万亿浮点运算每秒）为单位衡量，比旧的MFLOPS标准大一百万倍（10^(12)
    对比 10⁶）。今天一张Fermi GPU卡的理论峰值性能超过1 teraflop。
- en: The Cray-2 was a significant improvement on the Cray-1\. It used a shared memory
    architecture, split into banks. These were connected to one, two, or four processors.
    It led the way for the creation of today’s server-based symmetrical multiprocessor
    (SMP) systems in which multiple CPUs shared the same memory space. Like many machines
    of its era, it was a vector-based machine. In a vector machine the same operation
    acts on many operands. These still exist today, in part as processor extensions
    such as MMX, SSE, and AVX. GPU devices are, at their heart, vector processors
    that share many similarities with the older supercomputer designs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Cray-2是对Cray-1的重大改进。它采用了共享内存架构，分为多个内存区块。这些内存区块连接到一个、两个或四个处理器上。它为今天的基于服务器的对称多处理器（SMP）系统的创建奠定了基础，在这种系统中，多个CPU共享同一内存空间。像其时代的许多机器一样，它是一个基于向量的机器。在向量机中，相同的操作作用于多个操作数。如今，这些机器仍然存在，部分以处理器扩展的形式出现，例如MMX、SSE和AVX。GPU设备本质上是向量处理器，与早期的超级计算机设计有许多相似之处。
- en: The Cray also had hardware support for scatter- and gather-type primitives,
    something we’ll see is quite important in parallel computing and something we
    look at in subsequent chapters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Cray还为散射和收集类型的原语提供硬件支持，这在并行计算中非常重要，我们将在后续章节中讨论这一点。
- en: Cray still exists today in the supercomputer market, and as of 2010 held the
    top 500 position with their Jaguar supercomputer at the Oak Ridge National Laboratory
    ([http://www.nccs.gov/computing-resources/jaguar/](http://www.nccs.gov/computing-resources/jaguar/)).
    I encourage you to read about the history of this great company, which you can
    find on Cray’s website ([http://www.cray.com](http://www.cray.com)), as it gives
    some insight into the evolution of computers and as to where we are today.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Cray至今仍活跃于超级计算机市场，并且截至2010年，其Jaguar超级计算机在奥克里奇国家实验室（[http://www.nccs.gov/computing-resources/jaguar/](http://www.nccs.gov/computing-resources/jaguar/)）位列全球500强。我鼓励你去了解这家伟大公司的历史，你可以在Cray的官网上找到相关信息（[http://www.cray.com](http://www.cray.com)），这将帮助你了解计算机的发展历程，以及我们今天所处的位置。
- en: Connection Machine
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接机
- en: Back in 1982 a corporation called Thinking Machines came up with a very interesting
    design, that of the Connection Machine.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 早在1982年，一家名为Thinking Machines的公司提出了一个非常有趣的设计——连接机。
- en: It was a relatively simple concept that led to a revolution in today’s parallel
    computers. They used a few simple parts over and over again. They created a 16-core
    CPU, and then installed some 4096 of these devices in one machine. The concept
    was different. Instead of one fast processor churning through a dataset, there
    were 64 K processors doing this task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相对简单的概念，促成了今天并行计算机的革命。它们反复使用一些简单的部件。它们创建了一个16核CPU，然后将这些设备中的4096个安装在一台机器中。这个概念有所不同。与单个快速处理器处理数据集不同，64
    K个处理器共同执行这个任务。
- en: Let’s take the simple example of manipulating the color of an RGB (red, green,
    blue) image. Each color is made up of a single byte, with 3 bytes representing
    the color of a single pixel. Let’s suppose we want to reduce the blue level to
    zero.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以操控RGB（红、绿、蓝）图像的颜色为例。每种颜色由一个字节组成，每个像素的颜色由3个字节表示。假设我们想将蓝色的值减少到零。
- en: Let’s assume the memory is configured in three banks of red, blue, and green,
    rather than being interleaved. With a conventional processor, we would have a
    loop running through the blue memory and decrement every pixel color level by
    one. The operation is the same on each item of data, yet each time we fetch, decode,
    and execute the instruction stream on each loop iteration.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假设内存被配置为红色、蓝色和绿色三个内存区块，而不是交错排列。对于常规处理器，我们会有一个循环遍历蓝色内存并将每个像素的颜色值减去1。每个数据项执行的操作是相同的，但每次我们都需要提取、解码并在每次循环迭代中执行指令流。
- en: The Connection Machine used something called SIMD (single instruction, multiple
    data), which is used today in modern processors and known by names such as SSE
    (Streaming SIMD Extensions), MMX (Multi-Media eXtension), and AVX (Advanced Vector
    eXtensions). The concept is to define a data range and then have the processor
    apply that operation to the data range. However, SSE and MMX are based on having
    one processor core. The Connection Machine had 64 K processor cores, each executing
    SIMD instructions on its dataset.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 连接机使用了一种叫做SIMD（单指令，多数据）的技术，这在现代处理器中得到了应用，并以SSE（流式SIMD扩展）、MMX（多媒体扩展）和AVX（高级向量扩展）等名称被人们熟知。这个概念是定义一个数据范围，然后让处理器将该操作应用到数据范围上。然而，SSE和MMX基于只有一个处理器核心的架构。连接机拥有64
    K个处理器核心，每个核心在其数据集上执行SIMD指令。
- en: Processors such as the Intel I7 are 64-bit processors, meaning they can process
    up to 64 bits at a time (8 bytes). The SSE SIMD instruction set extends this to
    128 bits. With SIMD instructions on such a processor, we eliminate all redundant
    instruction memory fetches, and generate one sixteenth of the memory read and
    write cycles compared with fetching and writing 1 byte at a time. AVX extends
    this to 256 bits, making it even more effective.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Intel I7 这样的处理器是 64 位处理器，这意味着它们一次可以处理最多 64 位（8 字节）。SSE SIMD 指令集将其扩展到 128 位。使用这样的处理器上的
    SIMD 指令，我们消除了所有冗余的指令内存获取，并且与逐次获取和写入 1 字节相比，生成的内存读写周期仅为其 1/16。AVX 将这一扩展到 256 位，效果更加显著。
- en: For a high-definition (HD) video image of 1920 × 1080 resolution, the data size
    is 2,073,600 bytes, or around 2 MB per color plane. Thus, we generate around 260,000
    SIMD cycles for a single conventional processor using SSE/MMX. By SIMD cycle,
    we mean one read, compute, and write cycle. The actual number of processor clocks
    may be considerably different than this, depending on the particular processor
    architecture.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 1920 × 1080 分辨率的高清（HD）视频图像，数据大小为 2,073,600 字节，约为每个颜色平面 2 MB。因此，使用 SSE/MMX
    的单一常规处理器将生成约 260,000 个 SIMD 周期。我们所说的 SIMD 周期，是指一次读取、计算和写入的周期。实际的处理器时钟数量可能与此相差较大，具体取决于特定的处理器架构。
- en: The Connection Machine used 64 K processors. Thus, the 2 MB frame would have
    resulted in about 32 SIMD cycles for each processor. Clearly, this type of approach
    is vastly superior to the modern processor SIMD approach. However, there is of
    course a caveat. Synchronizing and communication between processors becomes the
    major issue when moving from a rather coarse-threaded approach of today’s CPUs
    to a hugely parallel approach used by such machines.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Connection Machine 使用了 64 K 个处理器。因此，2 MB 的帧数据将为每个处理器生成大约 32 个 SIMD 周期。显然，这种方法远远优于现代处理器的
    SIMD 方法。然而，当然也有一个警告。当从今天 CPU 的粗线程化方法转向这种机器使用的极度并行方法时，处理器之间的同步和通信将成为主要问题。
- en: Cell Processor
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cell 处理器
- en: Another interesting development in supercomputers stemmed from IBM’s invention
    of the Cell processor ([Figure 1.4](#F0025)). This worked on the idea of having
    a regular processor act as a supervisory processor, connected to a number of high-speed
    stream processors. The regular PowerPC (PPC)processor in the Cell acts as an interface
    to the stream processors and the outside world. The stream SIMD processors, or
    SPEs as IBM called them, would process datasets managed by the regular processor.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 超级计算机中的另一个有趣发展来源于 IBM 发明的 Cell 处理器（[图 1.4](#F0025)）。这一设计的思路是让一个常规处理器充当监督处理器，连接到多个高速流处理器。Cell
    中的常规 PowerPC（PPC）处理器作为流处理器和外部世界之间的接口。流 SIMD 处理器，或 IBM 所称的 SPE，将处理由常规处理器管理的数据集。
- en: '![image](../images/F000016f01-04-9780124159334.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-04-9780124159334.jpg)'
- en: Figure 1.4 IBM cell processor die layout (8 SPE version).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.4 IBM Cell 处理器芯片布局（8 SPE 版本）。
- en: The Cell is a particularly interesting processor for us, as it’s a similar design
    to what NVIDIA later used in the G80 and subsequent GPUs. Sony also used it in
    their PS3 console machines in the games industry, a very similar field to the
    main use of GPUs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们来说，Cell 是一个特别有趣的处理器，因为它的设计与 NVIDIA 后来在 G80 和随后的 GPU 中使用的设计类似。索尼也在其 PS3 游戏主机中使用了它，这是一个与
    GPU 主要用途非常相似的领域。
- en: To program the Cell, you write a program to execute on the PowerPC core processor.
    It then invokes a program, using an entirely different binary, on each of the
    stream processing elements (SPEs). Each SPE is actually a core in itself. It can
    execute an independent program from its own local memory, which is different from
    the SPE next to it. In addition, the SPEs can communicate with one another and
    the PowerPC core over a shared interconnect. However, this type of hybrid architecture
    is not easy to program. The programmer must explicitly manage the eight SPEs,
    both in terms of programs and data, as well as the serial program running on the
    PowerPC core.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要编程 Cell，你需要编写一个程序在 PowerPC 核心处理器上执行。然后，它会在每个流处理单元（SPE）上调用一个完全不同的二进制程序。每个 SPE
    实际上都是一个独立的核心。它可以从自己的本地内存中执行独立的程序，这与旁边的 SPE 不同。此外，SPE 之间以及与 PowerPC 核心之间可以通过共享互连进行通信。然而，这种混合架构并不容易编程。程序员必须明确管理八个
    SPE，无论是程序还是数据，以及在 PowerPC 核心上运行的串行程序。
- en: With the ability to talk directly to the coordinating processor, a series of
    simple steps can be achieved. With our RGB example earlier, the PPC core fetches
    a chunk of data to work on. It allocates these to the eight SPEs. As we do the
    same thing in each SPE, each SPE fetches the byte, decrements it, and writes its
    bit back to its local memory. When all SPEs are done, the PC core fetches the
    data from each SPE. It then writes its chunk of data (or tile) to the memory area
    where the whole image is being assembled. The Cell processor is designed to be
    used in groups, thus repeating the design of the Connection Machine we covered
    earlier.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过能够直接与协调处理器通信，可以实现一系列简单的步骤。以我们之前的 RGB 示例为例，PPC 核心获取一块数据进行处理。它将这些数据分配给八个 SPE。当每个
    SPE 执行相同的操作时，每个 SPE 获取字节，递减该字节，并将其位写回到本地内存。当所有 SPE 完成后，PC 核心从每个 SPE 获取数据。然后它将数据块（或图块）写入正在组装整个图像的内存区域。Cell
    处理器设计为以组方式使用，因此它重复了我们之前讨论过的连接机器的设计。
- en: The SPEs could also be ordered to perform a stream operation, involving multiple
    steps, as each SPE is connected to a high-speed ring ([Figure 1.5](#F0030)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: SPEs 也可以被指令执行流操作，涉及多个步骤，因为每个 SPE 都连接到一个高速环形网络（[图 1.5](#F0030)）。
- en: '![image](../images/F000016f01-05-9780124159334.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-05-9780124159334.jpg)'
- en: Figure 1.5 Example routing stream processor routing on Cell.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 示例路由流处理器在 Cell 上的路由。
- en: The problem with this sort of streaming or pipelining approach is it runs only
    as fast as the slowest node. It mirrors a production line in a factory. The whole
    line can only run as fast as the slowest point. Each SPE (worker) only has a small
    set of tasks to perform, so just like the assembly line worker, it can do this
    very quickly and efficiently. However, just like any processor, there is a bandwidth
    limit and overhead of passing data to the next stage. Thus, while you gain efficiencies
    from executing a consistent program on each SPE, you lose on interprocessor communication
    and are ultimately limited by the slowest process step. This is a common problem
    with any pipeline-based model of execution.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这种流处理或流水线处理方法的问题在于，它的运行速度仅受限于最慢的节点。这就像工厂里的生产线一样，整条生产线只能按照最慢的环节运行。每个 SPE（工人）只需执行一小部分任务，因此就像装配线工人一样，它可以非常快速和高效地完成这些任务。然而，就像任何处理器一样，它也存在带宽限制和将数据传递到下一个阶段的开销。因此，虽然你可以通过在每个
    SPE 上执行一致的程序来提高效率，但在处理器之间的通信上则会有所损失，并且最终会受到最慢步骤的限制。这是任何基于流水线的执行模型中的常见问题。
- en: The alternative approach of putting everything on one SPE and then having each
    SPE process a small chunk of data is often a more efficient approach. This is
    the equivalent to training all assembly line workers to assemble a complete widget.
    For simple tasks, this is easy, but each SPE has limits on available program and
    data memory. The PowerPC core must now also deliver and collect data from eight
    SPEs, instead of just two, so the management overhead and communication between
    host and SPEs increases.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将所有任务放在一个 SPE 上，然后让每个 SPE 处理一小块数据，这通常是一种更高效的做法。这相当于训练所有装配线工人组装一个完整的组件。对于简单的任务来说，这很容易，但每个
    SPE 对可用程序和数据内存都有一定限制。PowerPC 核心现在必须同时处理来自八个 SPE 的数据，而不是仅仅两个，因此管理开销和主机与 SPE 之间的通信会增加。
- en: IBM used a high-powered version of the Cell processor in their Roadrunner supercomputer,
    which as of 2010 was the third fastest computer on the top 500 list. It consists
    of 12,960 PowerPC cores, plus a total of 103,680 stream processors. Each PowerPC
    board is supervised by a dual-core AMD (Advanced Micro Devices) Opteron processor,
    of which there are 6912 in total. The Opteron processors act as coordinators among
    the nodes. Roadrunner has a theoretical throughput of 1.71 petaflops, cost $125
    million USD to build, occupies 560 square meters, and consumes 2.35 MW of electricity
    when operating!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: IBM 在其 Roadrunner 超级计算机中使用了高性能版本的 Cell 处理器，该计算机截至 2010 年排名全球超级计算机 TOP 500 的第三快计算机。它由
    12,960 个 PowerPC 核心以及 103,680 个流处理器组成。每个 PowerPC 板由一个双核的 AMD（高级微设备公司）Opteron 处理器进行监督，总共有
    6912 个 Opteron 处理器。这些 Opteron 处理器充当节点之间的协调者。Roadrunner 的理论吞吐量为 1.71 拍浮点运算每秒（petaflops），建设成本为
    1.25 亿美元，占地 560 平方米，运行时消耗 2.35 兆瓦的电力！
- en: Multinode Computing
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多节点计算
- en: As you increase the requirements (CPU, memory, storage space) needed on a single
    machine, costs rapidly increase. While a 2.6 GHz processor may cost you $250 USD,
    the same processor at 3.4 GHz may be $1400 for less than a 1 GHz increase in clock
    speed. A similar relationship is seen for both speed and size memory, and storage
    capacity.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当你提高单台机器所需的要求（如CPU、内存、存储空间）时，成本会迅速增加。比如一个2.6 GHz的处理器可能需要250美元，而一个3.4 GHz的同款处理器可能要1400美元，时钟频率增加不到1
    GHz，却导致成本大幅上涨。类似的情况也出现在速度、内存大小和存储容量上。
- en: Not only do costs scale as computing requirements scale, but so do the power
    requirements and the consequential heat dissipation issues. Processors can hit
    4–5 GHz, given sufficient supply of power and cooling.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算需求的增加，成本不仅增加，电力需求和由此产生的散热问题也会增加。只要提供足够的电力和冷却，处理器的频率可以达到4–5 GHz。
- en: In computing you often find the law of diminishing returns. There is only so
    much you can put into a single case. You are limited by cost, space, power, and
    heat. The solution is to select a reasonable balance of each and to replicate
    this many times.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算中，你常常会遇到收益递减法则。单一设备能承载的东西是有限的，你会受到成本、空间、电力和热量的限制。解决方法是选择每个因素的合理平衡，并将其多次复制。
- en: Cluster computing became popular in 1990s along with ever-increasing clock rates.
    The concept was a very simple one. Take a number of commodity PCs bought or made
    from off-the-shelf parts and connect them to an off-the-shelf 8-, 16-, 24-, or
    32-port Ethernet switch and you had up to 32 times the performance of a single
    box. Instead of paying $1600 for a high performance processor, you paid $250 and
    bought six medium performance processors. If your application needed huge memory
    capacity, the chances were that maxing out the DIMMs on many machines and adding
    them together was more than sufficient. Used together, the combined power of many
    machines hugely outperformed any single machine you could possible buy with a
    similar budget.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 集群计算在1990年代随着时钟频率的不断提升而变得流行。这个概念非常简单：用若干台从商用部件购买或制造的普通PC，将它们连接到一个8、16、24或32端口的以太网交换机上，你就能得到单台计算机32倍的性能。你不再需要为一个高性能处理器支付1600美元，而是花250美元买六个中等性能的处理器。如果你的应用需要巨大的内存容量，最大化多台机器的DIMM内存并将它们相加往往已经足够。多台机器的合力性能远远超过了你用相同预算所能买到的任何单台机器。
- en: All of a sudden universities, schools, offices, and computer departments could
    build machines much more powerful than before and were not locked out of the high-speed
    computing market due to lack of funds. Cluster computing back then was like GPU
    computing today—a disruptive technology that changed the face of computing. Combined
    with the ever-increasing single-core clock speeds it provided a cheap way to achieve
    parallel processing within single-core CPUs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 突然之间，大学、学校、办公室和计算机部门可以构建比以往更强大的机器，不再因为资金不足而无法进入高速计算市场。那时候的集群计算就像今天的GPU计算一样，是一种颠覆性的技术，改变了计算的面貌。再加上不断提高的单核时钟频率，它提供了一种廉价的方式，在单核CPU上实现并行计算。
- en: Clusters of PCs typically ran a variation of LINUX with each node usually fetching
    its boot instructions and operating system (OS) from a central master node. For
    example, at CudaDeveloper we have a tiny cluster of low-powered, atom-based PCs
    with embedded CUDA GPUs. It’s very cheap to buy and set up a cluster. Sometimes
    they can simply be made from a number of old PCs that are being replaced, so the
    hardware is effectively free.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: PC集群通常运行着某种版本的LINUX，每个节点通常从中央主节点获取启动指令和操作系统（OS）。例如，在CudaDeveloper，我们有一个由低功耗、基于Atom的PC和嵌入式CUDA
    GPU组成的小型集群。购买和搭建一个集群非常便宜。有时这些集群甚至可以由一些正在被替换掉的旧PC组成，因此硬件实际上是免费的。
- en: However, the problem with cluster computing is it’s only as fast as the amount
    of internode communication that is necessary for the problem. If you have 32 nodes
    and the problem breaks down into 32 nice chunks and requires no internode communication,
    you have an application that is ideal for a cluster. If every data point takes
    data from every node, you have a terrible problem to put into a cluster.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，集群计算的问题在于，它的速度取决于问题所需的节点间通信量。如果你有32个节点，而问题能被分解成32个相互独立的部分，并且不需要节点间的通信，那么你有一个非常适合集群的应用。如果每个数据点都需要从每个节点获取数据，那么将这样的应用放入集群中就会遇到很大的问题。
- en: Clusters are seen inside modern CPUs and GPUs. Look back at [Figure 1.1](#F0010),
    the CPU cache hierarchy. If we consider each CPU core as a node, the L2 cache
    as DRAM (Dynamic Random Access Memory), the L3 cache as the network switch, and
    the DRAM as mass storage, we have a cluster in miniature ([Figure 1.6](#F0035)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 集群出现在现代CPU和GPU中。回顾[图1.1](#F0010)，CPU缓存层次结构。如果我们将每个CPU核心视为一个节点，将L2缓存视为DRAM（动态随机存取存储器），将L3缓存视为网络交换机，将DRAM视为大容量存储，那么我们就得到了一个微型集群（参见[图1.6](#F0035)）。
- en: '![image](../images/F000016f01-06-9780124159334.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-06-9780124159334.jpg)'
- en: Figure 1.6 Typical cluster layout.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.6 典型集群布局。
- en: The architecture inside a modern GPU is really no different. You have a number
    of streaming multiprocessors (SMs) that are akin to CPU cores. These are connected
    to a shared memory/L1 cache. This is connected to an L2 cache that acts as an
    inter-SM switch. Data can be held in global memory storage where it’s then extracted
    and used by the host, or sent via the PCI-E switch directly to the memory on another
    GPU. The PCI-E switch is many times faster than any network’s interconnect.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现代GPU内部的架构与此并无太大区别。你会有多个类似于CPU核心的流式多处理器（SM）。这些处理器连接到共享内存/L1缓存。然后，它们连接到作为SM间交换机的L2缓存。数据可以保存在全局内存存储中，然后由主机提取并使用，或者通过PCI-E交换机直接发送到另一个GPU的内存。PCI-E交换机的速度远远超过任何网络的互连速度。
- en: The node may itself be replicated many times, as shown in [Figure 1.7](#F0040).
    This replication within a controlled environment forms a cluster. One evolution
    of the cluster designs are distributed applications. Distributed applications
    run on many nodes, each of which may contain many processing elements including
    GPUs. Distributed applications may, but do not need to, run in a controlled environment
    of a managed cluster. They can connect arbitrary machines together to work on
    some common problem, BOINC and Folding@Home being two of the largest examples
    of such applications that connect machines together over the Internet.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 节点本身可以多次复制，如[图1.7](#F0040)所示。这种在受控环境中的复制形成了集群。集群设计的一个演变是分布式应用程序。分布式应用程序在多个节点上运行，每个节点可能包含多个处理单元，包括GPU。分布式应用程序可以，也不一定需要，在管理集群的受控环境中运行。它们可以将任意机器连接在一起，共同处理某个问题，BOINC和Folding@Home是这类通过互联网将机器连接在一起的应用程序中的两个最大例子。
- en: '![image](../images/F000016f01-07-9780124159334.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-07-9780124159334.jpg)'
- en: Figure 1.7 GPUs compared to a cluster.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.7 GPUs与集群的对比。
- en: The Early Days of Gpgpu Coding
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Gpgpu编码的早期
- en: Graphics processing units (GPUs) are devices present in most modern PCs. They
    provide a number of basic operations to the CPU, such as rendering an image in
    memory and then displaying that image onto the screen. A GPU will typically process
    a complex set of polygons, a map of the scene to be rendered. It then applies
    textures to the polygons and then performs shading and lighting calculations.
    The NVIDIA 5000 series cards brought for the first time photorealistic effects,
    such as shown in the Dawn Fairy demo from 2003.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图形处理单元（GPU）是大多数现代PC中存在的设备。它们为CPU提供一系列基本操作，例如在内存中渲染图像，并将该图像显示在屏幕上。GPU通常会处理一组复杂的多边形，这些多边形是要渲染的场景的映射。然后，它会将纹理应用于这些多边形，并进行着色和光照计算。NVIDIA
    5000系列显卡首次带来了照片级真实感效果，如2003年“黎明仙子”演示中所展示的效果。
- en: Have a look at [http://www.nvidia.com/object/cool_stuff.html#/demos](http://www.nvidia.com/object/cool_stuff.html%23/demos)
    and download some of the older demos and you’ll see just how much GPUs have evolved
    over the past decade. See [Table 1.2](#T0015).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[http://www.nvidia.com/object/cool_stuff.html#/demos](http://www.nvidia.com/object/cool_stuff.html%23/demos)，并下载一些较旧的演示，你将看到GPU在过去十年中是如何发展的。请参见[表1.2](#T0015)。
- en: Table 1.2 GPU Technology Demonstrated over the Years
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.2 多年来展示的GPU技术
- en: '| Demo | Card | Year |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 演示 | 显卡 | 年份 |'
- en: '| --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Dawn | GeForce FX | 2003 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 黎明 | GeForce FX | 2003 |'
- en: '| Dusk Ultra | GeForce FX | 2003 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 黄昏Ultra | GeForce FX | 2003 |'
- en: '| Nalu | GeForce 6 | 2004 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Nalu | GeForce 6 | 2004 |'
- en: '| Luna | GeForce 7 | 2005 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 月亮 | GeForce 7 | 2005 |'
- en: '| Froggy | GeForce 8 | 2006 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 青蛙 | GeForce 8 | 2006 |'
- en: '| Human Head | GeForce 8 | 2007 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 人头 | GeForce 8 | 2007 |'
- en: '| Medusa | GeForce 200 | 2008 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 美杜莎 | GeForce 200 | 2008 |'
- en: '| Supersonic Sled | GeForce 400 | 2010 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 超音速雪橇 | GeForce 400 | 2010 |'
- en: '| A New Dawn | GeForce 600 | 2012 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 新黎明 | GeForce 600 | 2012 |'
- en: One of the important steps was the development of programmable shaders. These
    were effectively little programs that the GPU ran to calculate different effects.
    No longer was the rendering fixed in the GPU; through downloadable shaders, it
    could be manipulated. This was the first evolution of general-purpose graphical
    processor unit (GPGPU) programming, in that the design had taken its first steps
    in moving away from fixed function units.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个重要的步骤是可编程着色器的发展。这些实际上是小型程序，由GPU运行，用于计算不同的效果。渲染不再固定在GPU中；通过可下载的着色器，它可以被操控。这是通用图形处理单元（GPGPU）编程的第一次演变，因为设计已迈出了远离固定功能单元的第一步。
- en: However, these shaders were operations that by their very nature took a set
    of 3D points that represented a polygon map. The shaders applied the same operation
    to many such datasets, in a hugely parallel manner, giving huge throughput of
    computing power.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些着色器本质上是对表示多边形地图的3D点集进行操作的。这些着色器以极高的并行方式对许多此类数据集应用相同的操作，从而提供了巨大的计算能力吞吐量。
- en: Now although polygons are sets of three points, and some other datasets such
    as RGB photos can be represented by sets of three points, a lot of datasets are
    not. A few brave researchers made use of GPU technology to try and speed up general-purpose
    computing. This led to the development of a number of initiatives (e.g., BrookGPU,
    Cg, CTM, etc.), all of which were aimed at making the GPU a real programmable
    device in the same way as the CPU. Unfortunately, each had its own advantages
    and problems. None were particularly easy to learn or program in and were never
    taught to people in large numbers. In short, there was never a critical mass of
    programmers or a critical mass of interest from programmers in this hard-to-learn
    technology. They never succeeded in hitting the mass market, something CUDA has
    for the first time managed to do, and at the same time provided programmers with
    a truly general-purpose language for GPUs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尽管多边形是由三个点组成的集合，一些其他数据集（如RGB照片）也可以通过三个点的集合来表示，但许多数据集并非如此。一些勇敢的研究人员利用GPU技术尝试加速通用计算。这导致了许多倡议的开发（例如BrookGPU、Cg、CTM等），这些倡议的目标是使GPU成为一个像CPU一样的真正可编程设备。不幸的是，每个方案都有自己的优缺点。它们都不容易学习或编程，并且从未大规模地教授给人们。简而言之，程序员群体从未对这种难以学习的技术产生关键的兴趣，也未能形成关键的规模。它们从未成功打入大众市场，而CUDA首次做到了这一点，并且同时为程序员提供了一个真正的通用GPU语言。
- en: The Death of the Single-Core Solution
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单核解决方案的终结
- en: One of the problems with today’s modern processors is they have hit a clock
    rate limit at around 4 GHz. At this point they just generate too much heat for
    the current technology and require special and expensive cooling solutions. This
    is because as we increase the clock rate, the power consumption rises. In fact,
    the power consumption of a CPU, if you fix the voltage, is approximately the cube
    of its clock rate. To make this worse, as you increase the heat generated by the
    CPU, for the same clock rate, the power consumption also increases due to the
    properties of the silicon. This conversion of power into heat is a complete waste
    of energy. This increasingly inefficient use of power eventually means you are
    unable to either power or cool the processor sufficiently and you reach the thermal
    limits of the device or its housing, the so-called power wall.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 当今现代处理器的一个问题是，它们已经达到了大约4 GHz的时钟频率极限。在这个频率下，它们产生的热量对于当前技术来说太大，需要特殊且昂贵的冷却解决方案。这是因为随着时钟频率的提高，功耗也会上升。事实上，如果固定电压，CPU的功耗大致与时钟频率的立方成正比。更糟糕的是，随着CPU产生的热量增加，在相同时钟频率下，由于硅的特性，功耗也会增加。这种功率转化为热量是完全浪费能源的。功率越来越低效的使用最终意味着你无法为处理器提供足够的电力或冷却，并且达到了设备或其外壳的热极限，这就是所谓的功率墙。
- en: Faced with not being able to increase the clock rate, making forever-faster
    processors, the processor manufacturers had to come up with another game plan.
    The two main PC processor manufacturers, Intel and AMD, have had to adopt a different
    approach. They have been forced down the route of adding more cores to processors,
    rather than continuously trying to increase CPU clock rates and/or extract more
    instructions per clock through instruction-level parallelism. We have dual, tri,
    quad, hex, 8, 12, and soon even 16 and 32 cores and so on. This is the future
    of where computing is now going for everyone, the GPU and CPU communities. The
    Fermi GPU is effectively already a 16-core device in CPU terms.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 面对无法提高时钟频率、制造更快处理器的困境，处理器制造商不得不想出另一种应对策略。两大主流PC处理器制造商，Intel和AMD，不得不采取不同的方法。它们被迫走上了给处理器增加更多核心的道路，而不是不断地尝试通过提高CPU时钟频率和/或通过指令级并行性提取每时钟更多指令。现在我们有双核、三核、四核、六核、8核、12核，甚至很快就会有16核和32核等等。这是计算技术的未来方向，不仅适用于GPU，还适用于CPU社区。Fermi
    GPU在CPU术语中实际上已经是一个16核设备。
- en: There is a big problem with this approach—it requires programmers to switch
    from their traditional serial, single-thread approach, to dealing with multiple
    threads all executing at once. Now the programmer has to think about two, four,
    six, or eight program threads and how they interact and communicate with one another.
    When dual-core CPUs arrived, it was fairly easy, in that there were usually some
    background tasks being done that could be offloaded onto a second core. When quad-core
    CPUs arrived, not many programs were changed to support it. They just carried
    on being sold as single-thread applications. Even the games industry didn’t really
    move to quad-core programming very quickly, which is the one industry you’d expect
    to want to get the absolute most out of today’s technology.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法存在一个大问题——它要求程序员从传统的串行、单线程方法转变为处理多个同时执行的线程。现在程序员必须考虑两个、四个、六个或八个程序线程以及它们之间如何相互作用和通信。当双核CPU出现时，这个转变相对容易，因为通常有一些后台任务可以被转移到第二个核心上。当四核CPU出现时，并不是很多程序做出了改变来支持它。它们只是继续作为单线程应用程序出售。即使是游戏行业，也并没有很快转向四核编程，然而这是一个本应希望充分利用当今技术的行业。
- en: In some ways the processor manufacturers are to blame for this, because the
    single-core application runs just fine on one-quarter of the quad-core device.
    Some devices even increase the clock rate dynamically when only one core is active,
    encouraging programmers to be lazy and not make use of the available hardware.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些方面，处理器制造商也要为此负责，因为单核应用程序在四核设备的四分之一核心上运行得相当不错。一些设备甚至在只有一个核心活动时动态提高时钟频率，促使程序员变得懒惰，不去充分利用可用的硬件。
- en: There are economic reasons too. The software development companies need to get
    the product to market as soon as possible. Developing a better quad-core solution
    is all well and good, but not if the market is being grabbed by a competitor who
    got there first. As manufacturers still continue to make single- and dual-core
    devices, the market naturally settles on the lowest configuration, with the widest
    scope for sales. Until the time that quad-core CPUs are the minimum produced,
    market forces work against the move to multicore programming in the CPU market.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 也有经济方面的原因。软件开发公司需要尽快将产品推向市场。开发一个更好的四核解决方案固然好，但如果市场被先行的竞争对手抢占，那就得不偿失了。由于制造商仍然继续生产单核和双核设备，市场自然会倾向于选择最低配置的设备，且销售范围最广。直到四核CPU成为生产的最低配置时，市场力量才会推动CPU市场向多核编程转变。
- en: Nvidia and Cuda
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Nvidia和Cuda
- en: If you look at the relative computational power in GPUs and CPUs, we get an
    interesting graph ([Figure 1.8](#F0045)). We start to see a divergence of CPU
    and GPU computational power until 2009 when we see the GPU finally break the 1000
    gigaflops or 1 teraflop barrier. At this point we were moving from the G80 hardware
    to the G200 and then in 2010 to the Fermi evolution. This is driven by the introduction
    of massively parallel hardware. The G80 is a 128 CUDA core device, the G200 is
    a 256 CUDA core device, and the Fermi is a 512 CUDA core device.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看一下GPU和CPU在计算能力上的相对差异，我们会得到一个有趣的图表（[图1.8](#F0045)）。我们开始看到CPU和GPU的计算能力开始分化，直到2009年，GPU终于突破了1000吉帕（gigaflops）或1泰帕（teraflop）障碍。此时，我们正从G80硬件过渡到G200，然后在2010年进入Fermi的演进阶段。这一切都得益于大规模并行硬件的引入。G80是一个128
    CUDA核心的设备，G200是一个256 CUDA核心的设备，而Fermi则是一个512 CUDA核心的设备。
- en: '![image](../images/F000016f01-08-9780124159334.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F000016f01-08-9780124159334.jpg)'
- en: Figure 1.8 CPU and GPU peak performance in gigaflops.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.8 CPU 和 GPU 在千亿次浮点运算中的峰值性能。
- en: We see NVIDIA GPUs make a leap of 300 gigaflops from the G200 architecture to
    the Fermi architecture, nearly a 30% improvement in throughput. By comparison,
    Intel’s leap from their core 2 architecture to the Nehalem architecture sees only
    a minor improvement. Only with the change to Sandy Bridge architecture do we see
    significant leaps in CPU performance. This is not to say one is better than the
    other, for the traditional CPUs are aimed at serial code execution and are extremely
    good at it. They contain special hardware such as branch prediction units, multiple
    caches, etc., all of which target serial code execution. The GPUs are not designed
    for this serial execution flow and only achieve their peak performance when fully
    utilized in a parallel manner.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 NVIDIA 的 GPU 从 G200 架构到 Fermi 架构实现了 300 gigaflops 的飞跃，吞吐量提升了近 30%。相比之下，英特尔从
    Core 2 架构到 Nehalem 架构的飞跃仅仅是一个小幅提升。只有在转向 Sandy Bridge 架构时，我们才看到 CPU 性能的显著提升。这并不是说一个比另一个好，因为传统的
    CPU 是为了串行代码执行而设计的，在这方面非常优秀。它们包含了特殊的硬件，例如分支预测单元、多个缓存等，所有这些硬件都针对串行代码执行优化。GPU 并非为串行执行流程而设计，只有在完全并行使用时，才能达到它们的峰值性能。
- en: In 2007, NVIDIA saw an opportunity to bring GPUs into the mainstream by adding
    an easy-to-use programming interface, which it dubbed CUDA, or Compute Unified
    Device Architecture. This opened up the possibility to program GPUs without having
    to learn complex shader languages, or to think only in terms of graphics primitives.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 2007 年，NVIDIA 看到了将 GPU 引入主流的机会，通过添加一个易于使用的编程接口，命名为 CUDA（计算统一设备架构）。这为编程 GPU 打开了可能性，无需学习复杂的着色器语言，也不必仅仅考虑图形原语。
- en: CUDA is an extension to the C language that allows GPU code to be written in
    regular C. The code is either targeted for the host processor (the CPU) or targeted
    at the device processor (the GPU). The host processor spawns multithread tasks
    (or kernels as they are known in CUDA) onto the GPU device. The GPU has its own
    internal scheduler that will then allocate the kernels to whatever GPU hardware
    is present. We’ll cover scheduling in detail later. Provided there is enough parallelism
    in the task, as the number of SMs in the GPU grows, so should the speed of the
    program.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 是 C 语言的扩展，允许用常规 C 语言编写 GPU 代码。代码可以针对主处理器（CPU）或者设备处理器（GPU）编写。主处理器将多线程任务（或在
    CUDA 中称为内核）分派到 GPU 设备。GPU 有自己的内部调度器，然后将内核分配给存在的 GPU 硬件。稍后我们将详细讨论调度问题。只要任务中有足够的并行性，随着
    GPU 中 SM 单元数量的增加，程序的速度也应该增加。
- en: However, herein hides a big problem. You have to ask what percentage of the
    code can be run in parallel. The maximum speedup possible is limited by the amount
    of serial code. If you have an infinite amount of processing power and could do
    the parallel tasks in zero time, you would still be left with the time from the
    serial code part. Therefore, we have to consider at the outset if we can indeed
    parallelize a significant amount of the workload.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，隐藏在这里的一个大问题是，你必须问一下代码中有多少百分比可以并行运行。最大加速的可能性受到串行代码部分的限制。如果你有无限的处理能力，且可以在零时间内完成并行任务，你仍然会受到串行代码部分所需时间的限制。因此，我们必须从一开始就考虑是否能够有效地并行化大量工作负载。
- en: NVIDIA is committed to providing support to CUDA. Considerable information,
    examples, and tools to help with development are available from its website at
    [http://www.nvidia.com](http://www.nvidia.com) under CudaZone.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA 致力于提供对 CUDA 的支持。从其网站上的 [http://www.nvidia.com](http://www.nvidia.com)
    的 CudaZone 页面可以获取大量的信息、示例和开发工具来帮助开发。
- en: CUDA, unlike its predecessors, has now actually started to gain momentum and
    for the first time it looks like there will be a programming language that will
    emerge as the one of choice for GPU programming. Given that the number of CUDA-enabled
    GPUs now number in the millions, there is a huge market out there waiting for
    CUDA-enabled applications.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与前辈们不同，CUDA 现在实际上已经开始获得动力，并且首次看起来会有一种编程语言成为 GPU 编程的首选。考虑到目前支持 CUDA 的 GPU 数量已达数百万，市场上有一个巨大的空间在等待着支持
    CUDA 的应用程序。
- en: There are currently many CUDA-enabled applications and the list grows monthly.
    NVIDIA showcases many of these on its community website at [http://www.nvidia.com/object/cuda_apps_flash_new.html](http://www.nvidia.com/object/cuda_apps_flash_new.html).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有许多支持 CUDA 的应用程序，而且每月都有新的应用程序加入。NVIDIA 在其社区网站上展示了其中的许多应用，地址是 [http://www.nvidia.com/object/cuda_apps_flash_new.html](http://www.nvidia.com/object/cuda_apps_flash_new.html)。
- en: In areas where programs have to do a lot of computational work—for example,
    making a DVD from your home movies (video transcoding)—we see most mainstream
    video packages now supporting CUDA. The average speedup is 5 to 10 times in this
    domain.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要进行大量计算的领域，例如将家庭电影制作成 DVD（视频转码），我们看到大多数主流视频软件现在都支持 CUDA。在这个领域，平均加速比为 5 到 10
    倍。
- en: Along with the introduction of CUDA came the Tesla series of cards. These cards
    are not graphics cards, and in fact they have no DVI or VGA connectors on them.
    They are dedicated compute cards aimed at scientific computing. Here we see huge
    speedups in scientific calculations. These cards can either be installed in a
    regular desktop PC or in dedicated server racks. NVIDIA provides such a system
    at [http://www.nvidia.com/object/preconfigured_clusters.html](http://www.nvidia.com/object/preconfigured_clusters.html),
    which claims to provide up to 30 times the power of a conventional cluster. CUDA
    and GPUs are reshaping the world of high-performance computing.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 CUDA 的推出，Tesla 系列卡也随之推出。这些卡不是显卡，实际上它们没有 DVI 或 VGA 连接器。它们是专门针对科学计算的计算卡。在这里，我们看到了科学计算方面的巨大加速。这些卡可以安装在普通的桌面
    PC 中，也可以安装在专用的服务器机架中。NVIDIA 提供了这样的系统，网址为 [http://www.nvidia.com/object/preconfigured_clusters.html](http://www.nvidia.com/object/preconfigured_clusters.html)，该系统宣称提供的计算能力是传统集群的
    30 倍。CUDA 和 GPU 正在重新定义高性能计算的世界。
- en: Gpu Hardware
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU 硬件
- en: The NVIDIA G80 series processor and beyond implemented a design that is similar
    to both the Connection Machine and IBM’s Cell processor. Each graphics card consists
    of a number of SMs. To each SM is attached eight or more SPs (Stream Processors).
    The original 9800 GTX card has eight SMs, giving a total of 128 SPs. However,
    unlike the Roadrunner, each GPU board can be purchased for a few hundred USD and
    it doesn’t take 2.35 MW to power it. Power considerations are not to be overlooked,
    as we’ll discuss later when we talk about building GPU servers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA G80 系列处理器及其以后的设计，类似于 Connection Machine 和 IBM 的 Cell 处理器。每个显卡由多个 SM 组成，每个
    SM 上连接着八个或更多的 SP（流处理器）。原始的 9800 GTX 卡有八个 SM，总共有 128 个 SP。然而，与 Roadrunner 不同的是，每个
    GPU 卡的价格只有几百美元，并且它的功耗也没有达到 2.35 MW。电力消耗问题不容忽视，正如我们稍后在讨论构建 GPU 服务器时将会提到的那样。
- en: The GPU cards can broadly be considered as accelerator or coprocessor cards.
    A GPU card, currently, must operate in conjunction with a CPU-based host. In this
    regard it follows very much the approach of the Cell processor with the regular
    serial core and N SIMD SPE cores. Each GPU device contains a set of SMs, each
    of which contain a set of SPs or CUDA cores. The SPs execute work as parallel
    sets of up to 32 units. They eliminate a lot of the complex circuitry needed on
    CPUs to achieve high-speed serial execution through instruction-level parallelism.
    They replace this with a programmer-specified explicit parallelism model, allowing
    more compute capacity to be squeezed onto the same area of silicon.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 卡可以广泛地视为加速器或协处理器卡。目前，GPU 卡必须与基于 CPU 的主机协同工作。在这方面，它非常类似于 Cell 处理器的设计，后者有一个常规的串行核心和多个
    SIMD SPE 核心。每个 GPU 设备包含一组 SM，每个 SM 又包含一组 SP 或 CUDA 核心。SP 以最多 32 个单元的并行集执行工作。它们消除了
    CPU 上为了实现高速串行执行而需要的复杂电路，这些电路通过指令级并行实现。它们用程序员指定的显式并行模型取而代之，使得更多的计算能力可以被压缩到同一片硅片上。
- en: The overall throughput of GPUs is largely determined by the number of SPs present,
    the bandwidth to the global memory, and how well the programmer makes use of the
    parallel architecture he or she is working with. See [Table 1.3](#T0020) for a
    listing of current NVIDIA GPU cards.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 的整体吞吐量在很大程度上取决于 SP 的数量、全局内存的带宽，以及程序员如何有效利用他们所使用的并行架构。请参阅 [表 1.3](#T0020)
    以查看当前 NVIDIA GPU 卡的列表。
- en: Table 1.3 Current Series of NVIDIA GPU Cards
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.3 当前 NVIDIA GPU 卡系列
- en: '![Image](../images/T000016tabT0020.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/T000016tabT0020.jpg)'
- en: Which board is correct for a given application is a balance between memory and
    GPU processing power needed for a given application. Note the 9800 GX2, 295, 590,
    690, and K10 cards are actually dual cards, so to make full use of these they
    need to be programmed as two devices not one. The one caveat GPU here is that
    the figures quoted are for single-precision (32-bit) floating-point performance,
    not double-precision (64-bit) precision. Also be careful with the GF100 (Fermi)
    series, as the Tesla variant has double the number of double-precision units found
    in the standard desktop units, so achieves significantly better double-precision
    throughput. The Kepler K 20, yet to be released, will also have significant double
    precision performance over and above its already released K10 cousin.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 针对特定应用，选择哪个显卡是一个在内存和GPU处理能力之间的平衡。需要注意的是，9800 GX2、295、590、690和K10卡实际上是双卡，因此要充分利用这些卡，需要将其作为两个设备而不是一个设备来编程。这里有一个警告：GPU的性能数据是针对单精度（32位）浮点性能的，而不是双精度（64位）性能。另外，需要小心GF100（Fermi）系列，因为Tesla版本的双精度单元数量是标准桌面单元的两倍，因此在双精度吞吐量上表现显著更好。尚未发布的Kepler
    K20相比已发布的K10版本，双精度性能也会有显著提升。
- en: Note also, although not shown here, as the generations have evolved, the power
    consumption, clock for clock, per SM has come down. However, the overall power
    consumption has increased considerably and this is one of the key considerations
    in any multi-GPU-based solution. Typically, we see dual-GPU-based cards (9800
    GX2, 295, 590, 690) having marginally lower power consumption figures than the
    equivalent two single cards due to the use of shared circuitry and/or reduced
    clock frequencies.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要注意的是，虽然这里没有展示，但随着代际发展，每个SM的功耗逐时减少。然而，整体功耗却大幅增加，这也是任何基于多GPU解决方案的关键考虑因素之一。通常情况下，我们看到双GPU卡（如9800
    GX2、295、590、690）的功耗略低于等效的两个单卡，因为它们采用了共享电路和/或降低了时钟频率。
- en: NVIDIA provides various racks (the M series computing modules) containing two
    to four Tesla cards connected on a shared PCI-E bus for high-density computing.
    It’s quite possible to build your own GPU cluster or microsupercomputer from standard
    PC parts, and we show you how to do this later in the book.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA提供了各种机架（M系列计算模块），这些机架包含了两到四个Tesla卡，连接在共享的PCI-E总线上，用于高密度计算。完全可以使用标准PC零部件来搭建自己的GPU集群或微型超级计算机，稍后在书中我们将介绍如何实现这一点。
- en: The great thing about CUDA is that, despite all the variability in hardware,
    programs written for the original CUDA devices can run on today’s CUDA devices.
    The CUDA compilation model applies the same principle as used in Java—runtime
    compilation of a virtual instruction set. This allows modern GPUs to execute code
    from even the oldest generation GPUs. In many cases they benefit significantly
    from the original programmer reworking the program for the features of the newer
    GPUs. In fact, there is considerable scope for tuning for the various hardware
    revisions, which we’ll cover toward the end of the book.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA的伟大之处在于，尽管硬件存在诸多差异，针对原始CUDA设备编写的程序仍然可以在今天的CUDA设备上运行。CUDA的编译模型采用了与Java相同的原理——虚拟指令集的运行时编译。这使得现代GPU能够执行来自即使是最旧一代GPU的代码。在许多情况下，程序通过原始程序员根据新GPU的特性进行改进，从而显著获益。事实上，对于各种硬件版本的调优空间非常大，我们将在书的后面部分讨论这一点。
- en: Alternatives to Cuda
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CUDA的替代方案
- en: OpenCL
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenCL
- en: So what of the other GPU manufacturers, ATI (now AMD) being the prime example?
    AMD’s product range is as impressive as the NVIDIA range in terms of raw computer
    power. However, AMD brought its stream computing technology to the marketplace
    a long time after NVIDIA brought out CUDA. As a consequence, NVIDA has far more
    applications available for CUDA than AMD/ATI does for its competing stream technology.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 那么其他GPU厂商如何呢，ATI（现在的AMD）作为典型的代表？就原始计算能力而言，AMD的产品系列与NVIDIA的系列一样令人印象深刻。然而，AMD将流计算技术推向市场的时间远晚于NVIDIA推出CUDA。因此，NVIDIA为CUDA提供了远多于AMD/ATI的应用程序。
- en: OpenCL and Direct compute is not something we’ll cover in this book, but they
    deserve a mention in terms of alternatives to CUDA. CUDA is currently only officially
    executable on NVIDIA hardware. While NVIDIA has a sizeable chunk of the GPU market,
    its competitors also hold a sizeable chunk. As developers, we want to develop
    products for as large a market as possible, especially if we’re talking about
    the consumer market. As such, people should be aware there are alternatives to
    CUDA, which support both NVIDIA’s and others’ hardware.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不会深入讨论OpenCL和DirectCompute，但它们作为CUDA的替代方案值得一提。CUDA目前只能在NVIDIA硬件上官方执行。虽然NVIDIA在GPU市场上占有相当大的份额，但它的竞争对手也占有不小的市场份额。作为开发者，我们希望为尽可能大的市场开发产品，尤其是在消费市场中。因此，大家应该意识到，除了CUDA外，还有其他支持NVIDIA及其他硬件的替代方案。
- en: OpenCL is an open and royalty-free standard supported by NVIDIA, AMD, and others.
    The OpenCL trademark is owned by Apple. It sets out an open standard that allows
    the use of compute devices. A compute device can be a GPU, CPU, or other specialist
    device for which an OpenCL driver exists. As of 2012, OpenCL supports all major
    brands of GPU devices, including CPUs with at least SSE3 support.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCL是一个开放且免版税的标准，得到了NVIDIA、AMD等公司的支持。OpenCL商标由苹果公司拥有。它制定了一个开放标准，允许使用计算设备。计算设备可以是GPU、CPU，或者是有OpenCL驱动的其他专业设备。截至2012年，OpenCL支持所有主要品牌的GPU设备，包括至少支持SSE3的CPU。
- en: Anyone who is familiar with CUDA can pick up OpenCL relatively easily, as the
    fundamental concepts are quite similar. However, OpenCL is somewhat more complex
    to use than CUDA, in that much of the work the CUDA runtime API does for the programmer
    needs to be explicitly performed in OpenCL.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 任何熟悉CUDA的人都可以相对容易地掌握OpenCL，因为它们的基本概念非常相似。然而，OpenCL的使用比CUDA稍微复杂，因为CUDA运行时API为程序员做的许多工作，在OpenCL中需要显式地执行。
- en: You can read more about OpenCL at [http://www.khronos.org/opencl/](http://www.khronos.org/opencl/).
    There are also now a number of books written on OpenCL. I’d personally recommend
    learning CUDA prior to OpenCL as CUDA is somewhat of a higher-level language extension
    than OpenCL.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[http://www.khronos.org/opencl/](http://www.khronos.org/opencl/)阅读更多关于OpenCL的内容。现在也有许多关于OpenCL的书籍。我个人推荐在学习OpenCL之前先学习CUDA，因为CUDA相较于OpenCL更像是一个更高层次的语言扩展。
- en: DirectCompute
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DirectCompute
- en: DirectCompute is Microsoft’s alternative to CUDA and OpenCL. It is a proprietary
    product linked to the Windows operating system, and in particular, the DirectX
    11 API. The DirectX API was a huge leap forward for any of those who remember
    programming video cards before it. It meant the developers had to learn only one
    library API to program all graphics cards, rather than write or license drivers
    for each major video card manufacturer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: DirectCompute是微软对CUDA和OpenCL的替代方案。它是一个专有产品，链接到Windows操作系统，特别是DirectX 11 API。对于那些记得在DirectX出现之前编写显卡程序的人来说，DirectX
    API是一次巨大的飞跃。它意味着开发者只需要学习一个库API就可以编程所有显卡，而不需要为每个主要显卡制造商编写或授权驱动程序。
- en: DirectX 11 is the latest standard and supported under Windows 7\. With Microsoft’s
    name behind the standard, you might expect to see some quite rapid adoption among
    the developer community. This is especially the case with developers already familiar
    with DirectX APIs. If you are familiar with CUDA and DirectCompute, then it is
    quite an easy task to port a CUDA application over to DirectCompute. According
    to Microsoft, this is something you can typically do in an afternoon’s work if
    you are familiar with both systems. However, being Windows centric, we’ll exclude
    DirectCompute from many high-end systems where the various flavors of UNIX dominate.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: DirectX 11是最新的标准，并在Windows 7下得到支持。微软的背书意味着你可能会看到开发者社区对该标准的迅速采纳。特别是对于那些已经熟悉DirectX
    API的开发者来说。如果你熟悉CUDA和DirectCompute，那么将CUDA应用程序移植到DirectCompute是一个相对容易的任务。根据微软的说法，如果你熟悉这两个系统，通常可以在一个下午完成这个工作。然而，由于DirectCompute以Windows为中心，我们将排除那些以UNIX各种版本为主的高端系统。
- en: Microsoft are also set to launch C++ AMP, an additional set of standard template
    libraries (STLs), which may appeal more to programmers already familiar with C++-style
    STLs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 微软也将推出C++ AMP，这是一个额外的标准模板库（STL）集，可能更吸引那些已经熟悉C++风格STL的程序员。
- en: CPU alternatives
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU替代方案
- en: The main parallel processing languages extensions are MPI, OpenMP, and pthreads
    if you are developing for Linux. For Windows there is the Windows threading model
    and OpenMP. MPI and pthreads are supported as various ports from the Unix world.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在开发 Linux 程序，主要的并行处理语言扩展包括 MPI、OpenMP 和 pthreads。对于 Windows 来说，则有 Windows
    线程模型和 OpenMP。MPI 和 pthreads 作为来自 Unix 世界的各种移植版本被支持。
- en: '**MPI** (Message Passing Interface) is perhaps the most widely known messaging
    interface. It is process-based and generally found in large computing labs. It
    requires an administrator to configure the installation correctly and is best
    suited to controlled environments. Parallelism is expressed by spawning hundreds
    of processes over a cluster of nodes and explicitly exchanging messages, typically
    over high-speed network-based communication links (Ethernet or InfiniBand). MPI
    is widely used and taught. It’s a good solution within a controlled cluster environment.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**MPI**（消息传递接口）可能是最广为人知的消息传递接口。它是基于进程的，通常出现在大型计算实验室中。它需要管理员正确配置安装，并且最适用于受控环境。并行性通过在节点集群中启动数百个进程并显式地交换消息来表达，通常是通过高速网络通信链接（如以太网或
    InfiniBand）。MPI 被广泛使用并教授，在受控的集群环境中，它是一个不错的解决方案。'
- en: '**OpenMP** (Open Multi-Processing) is a system designed for parallelism within
    a node or computer system. It works entirely differently, in that the programmer
    specifies various parallel directives through compiler pragmas. The compiler then
    attempts to automatically split the problem into *N* parts, according to the number
    of available processor cores. OpenMP support is built into many compilers, including
    the NVCC compiler used for CUDA. OpenMP tends to hit problems with scaling due
    to the underlying CPU architecture. Often the memory bandwidth in the CPU is just
    not large enough for all the cores continuously streaming data to or from memory.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenMP**（开放多处理）是一个为节点或计算机系统内的并行性设计的系统。它的工作方式完全不同，程序员通过编译器指令指定各种并行指令。然后，编译器会根据可用的处理器核心数自动尝试将问题分割成
    *N* 个部分。许多编译器（包括用于 CUDA 的 NVCC 编译器）都内建了 OpenMP 支持。由于底层 CPU 架构的原因，OpenMP 常常面临扩展性问题。通常，CPU
    的内存带宽不足以让所有核心持续从内存中读取或写入数据。'
- en: '**Pthreads** is a library that is used significantly for multithread applications
    on Linux. As with OpenMP, pthreads uses threads and not processes as it is designed
    for parallelism within a single node. However, unlike OpenMP, the programmer is
    responsible for thread management and synchronization. This provides more flexibility
    and consequently better performance for well-written programs.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pthreads** 是一个广泛用于 Linux 多线程应用的库。与 OpenMP 类似，pthreads 使用线程而不是进程，因为它是为单节点内的并行性设计的。然而，与
    OpenMP 不同，程序员需要负责线程的管理和同步。这提供了更多的灵活性，从而使得编写良好的程序能够获得更好的性能。'
- en: '**ZeroMQ** (0MQ) is also something that deserves a mention. This is a simple
    library that you link to, and we will use it later in the book for developing
    a multinode, multi-GPU example. ZeroMQ supports thread-, process-, and network-based
    communications models with a single cross-platform API. It is also available on
    both Linux and Windows platforms. It’s designed for distributed computing, so
    the connections are dynamic and nodes fail gracefully.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**ZeroMQ**（0MQ）也是值得一提的。这是一个简单的库，你只需链接它，我们将在本书稍后使用它来开发一个多节点、多 GPU 示例。ZeroMQ
    支持线程、进程和基于网络的通信模型，并且提供一个跨平台的 API。它也可以在 Linux 和 Windows 平台上使用。它是为分布式计算设计的，因此连接是动态的，节点可以优雅地失败。'
- en: '**Hadoop** is also something that you may consider. Hadoop is an open-source
    version of Google’s MapReduce framework. It’s aimed primarily at the Linux platform.
    The concept is that you take a huge dataset and break (or map) it into a number
    of chunks. However, instead of sending the data to the node, the dataset is already
    split over hundreds or thousands of nodes using a parallel file system. Thus,
    the program, the reduce step, is instead sent to the node that contains the data.
    The output is written to the local node and remains there. Subsequent MapReduce
    programs take the previous output and again transform it in some way. As data
    is in fact mirrored to multiple nodes, this allows for a highly fault-tolerant
    as well as high-throughput system.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hadoop**也是你可以考虑的一个选择。Hadoop是Google的MapReduce框架的开源版本，主要面向Linux平台。其概念是将一个庞大的数据集分解（或映射）成多个小块。然而，与将数据发送到节点不同，数据集已经通过并行文件系统分割到数百个或数千个节点上。因此，程序的“reduce”步骤被发送到包含数据的节点上。输出结果会写入本地节点并保留在那里。随后的MapReduce程序会接收前一个输出并以某种方式进行转换。由于数据实际上是镜像到多个节点上，这使得系统具有高度的容错性以及高吞吐量。'
- en: Directives and libraries
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指令和库
- en: There are a number of compiler vendors, PGI, CAPS, and Cray being the most well-known,
    that support the recently announced OpenACC set of compiler directives for GPUs.
    These, in essence, replicate the approach of OpenMP, in that the programmer inserts
    a number of compiler directives marking regions as “to be executed on the GPU.”
    The compiler then does the grunt work of moving data to or from the GPU, invoking
    kernels, etc.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多编译器供应商，其中PGI、CAPS和Cray是最著名的，它们支持最近发布的用于GPU的OpenACC编译器指令集。实质上，这些指令集复制了OpenMP的方法，程序员插入一些编译器指令，标记出“需要在GPU上执行”的区域。然后，编译器负责将数据移动到GPU或从GPU移动，调用内核等繁琐的工作。
- en: As with the use of pthreads over OpenMP, with the lower level of control pthreads
    provides, you can achieve higher performance. The same is true of CUDA versus
    OpenACC. This extra level of control comes with a much higher level of required
    programming knowledge, a higher risk of errors, and the consequential time impact
    that may have on a development schedule. Currently, OpenACC requires directives
    to specify what areas of code should be run on the GPU, but also in which type
    of memory data should exist. NVIDIA claims you can get in the order of 5×-plus
    speedup using such directives. It’s a good solution for those programmers who
    need to get something working quickly. It’s also great for those people for whom
    programming is a secondary consideration who just want the answer to their problem
    in a reasonable timeframe.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用pthread而非OpenMP类似，pthread提供的更低级的控制可以实现更高的性能。CUDA与OpenACC的情况也是如此。额外的控制级别带来了更高的编程知识要求、更高的错误风险，以及可能对开发进度造成的时间影响。目前，OpenACC要求使用指令指定哪些代码区域应在GPU上运行，还需要指定数据应该存储在哪种类型的内存中。NVIDIA声称使用这些指令可以获得大约5倍以上的加速。这对于需要快速实现某些功能的程序员来说是一个很好的解决方案。对于那些编程是次要考虑的人，他们只想在合理的时间框架内解决问题，这也是一个很棒的选择。
- en: The use of libraries is also another key area where you can obtain some serious
    productivity gains, as well as execution time speedups. Libraries like SDK provide
    Thrust, which provides common functions implemented in a very efficient way. Libraries
    like CUBLAS are some of the best around for linear algebra. Libraries exist for
    many well-known applications such as Matlab and Mathematica. Language bindings
    exist for Python, Perl, Java, and many others. CUDA can even be integrated with
    Excel.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用库也是获得显著生产力提升以及执行时间加速的另一个关键领域。像SDK这样的库提供了Thrust，它以非常高效的方式实现了常见的功能。像CUBLAS这样的库是线性代数领域最出色的库之一。许多知名的应用程序，如Matlab和Mathematica，也有相应的库。Python、Perl、Java等语言也有相关绑定。CUDA甚至可以与Excel集成。
- en: As with many aspects of software development in the modern age, the chances
    are that someone has done what you are about to develop already. Search the Internet
    and see what is already there before you spend weeks developing a library that,
    unless you are a CUDA expert, is unlikely to be faster than one that is already
    available.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 就像现代软件开发的许多方面一样，很有可能有人已经开发了你即将开发的内容。在你花费数周时间开发一个库之前，先在互联网上搜索，看看是否已经有类似的东西，除非你是CUDA专家，否则它可能不会比现有的库更快。
- en: Conclusion
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: So maybe you’re thinking, why develop in CUDA? The answer is that CUDA is currently
    the easiest language to develop in, in terms of support, debugging tools, and
    drivers. CUDA has a head start on everything else and has a huge lead in terms
    of maturity. If your application needs to support hardware other than NVIDIA’s,
    then the best route currently is to develop under CUDA and then port the application
    to one of the other APIs. As such, we’ll concentrate on CUDA, for if you become
    an expert with CUDA, it’s easy to pick up alternative APIs should you need to.
    Understanding how CUDA works will allow you to better exploit and understand the
    limitations of any higher-level API.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，也许你会想，为什么要开发 CUDA？答案是，CUDA 目前是开发中最容易的语言，特别是在支持、调试工具和驱动程序方面。CUDA 在所有方面都抢占了先机，并且在成熟度上遥遥领先。如果你的应用需要支持
    NVIDIA 以外的硬件，那么目前最好的路线是先在 CUDA 下开发，然后再将应用移植到其他 API 上。因此，我们将集中讨论 CUDA，因为如果你成为 CUDA
    的专家，那么如果需要，你很容易掌握其他的 API。了解 CUDA 的工作原理将帮助你更好地利用并理解任何更高层次 API 的局限性。
- en: The journey from a single-thread CPU programmer to a fully fledged parallel
    programmer on GPUs is one that I hope you will find interesting. Even if you never
    program a GPU in the future, the insight you gain will be of tremendous help in
    allowing you to design multithread programs. If you, like us, see the world changing
    to a parallel programming model, you’ll want to be at the forefront of that wave
    of innovation and technological challenge. The single-thread industry is one that
    is slowly moving to obsolescence. To be a valuable asset and an employable individual,
    you need to have skills that reflect where the computing world is headed to, not
    those that are becoming progressively obsolete.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 从单线程 CPU 程序员到完全成熟的 GPU 并行程序员的旅程是我希望你能感兴趣的。即使你将来从不编写 GPU 程序，你所获得的见解也将极大帮助你设计多线程程序。如果你像我们一样看到世界正在转向并行编程模型，你会希望站在这波创新和技术挑战的前沿。单线程产业正慢慢走向过时。为了成为一个有价值的资产和具备就业能力的个人，你需要具备反映计算世界发展方向的技能，而不是那些正在逐渐过时的技能。
- en: GPUs are changing the face of computing. All of a sudden the computing power
    of supercomputers from a decade ago can be slotted under your desk. No longer
    must you wait in a queue to submit work batches and wait months for a committee
    to approve your request to use limited computer resources at overstretched computing
    installations. You can go out, spend up to 5000–10,000 USD, and have a supercomputer
    on your desk, or a development machine that runs CUDA for a fraction of that.
    GPUs are a disruptive technological change that will make supercomputer-like levels
    of performance available for everyone.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 正在改变计算的面貌。突然间，十年前超级计算机的计算能力可以放在你的桌子下面。你不再需要排队提交工作批次，也不必等待几个月才能通过委员会审批使用超负荷计算设施的有限计算资源。你可以出去花费
    5000 到 10,000 美元，拥有一台桌面上的超级计算机，或者一台运行 CUDA 的开发机器，价格只是其中的一小部分。GPU 是一种颠覆性的技术变革，它将使超级计算机般的性能水平对每个人开放。
- en: '[^∗](#CFN1)The actual achieved dispatch rate can be higher or lower than one,
    which we use here for simplicity.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '[^∗](#CFN1)实际达到的调度速率可能高于或低于 1，这里为了简便我们使用 1。'
