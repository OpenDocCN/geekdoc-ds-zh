- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SOME SIMPLE ALGORITHMS AND DATA STRUCTURES
  prefs: []
  type: TYPE_NORMAL
- en: Though we expend a fair number of pages in this book talking about efficiency,
    the goal is not to make you expert in designing efficient programs. There are
    many long books (and even some good long books) devoted exclusively to that topic.[^(68)](#c12-fn-0001)
    In Chapter 11, we introduced some of the basic concepts underlying complexity
    analysis. In this chapter, we use those concepts to look at the complexity of
    a few classic algorithms. The goal of this chapter is to help you develop some
    general intuitions about how to approach questions of efficiency. By the time
    you get through this chapter, you should understand why some programs complete
    in the blink of an eye, why some need to run overnight, and why some wouldn't
    complete in your lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: The first algorithms we looked at in this book were based on brute-force exhaustive
    enumeration. We argued that modern computers are so fast that it is often the
    case that employing clever algorithms is a waste of time. Writing code that is
    simple and obviously correct is often the right way to go.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at some problems (e.g., finding an approximation to the roots
    of a polynomial) where the search space was too large to make brute force practical.
    This led us to consider more efficient algorithms such as bisection search and
    Newton–Raphson. The major point was that the key to efficiency is a good algorithm,
    not clever coding tricks.
  prefs: []
  type: TYPE_NORMAL
- en: In the sciences (physical, life, and social), programmers often start by quickly
    coding a simple algorithm to test the plausibility of a hypothesis about a data
    set, and then run it on a small amount of data. If this yields encouraging results,
    the hard work of producing an implementation that can be run (perhaps over and
    over again) on large data sets begins. Such implementations need to be based on
    efficient algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Efficient algorithms are hard to invent. Successful professional computer scientists
    might invent one algorithm during their whole career—if they are lucky. Most of
    us never invent a novel algorithm. What we do instead is learn to reduce the most
    complex aspects of the problems we are faced with to previously solved problems.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, we
  prefs: []
  type: TYPE_NORMAL
- en: Develop an understanding of the inherent complexity of the problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Think about how to break that problem up into subproblems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relate those subproblems to other problems for which efficient algorithms already
    exist.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter contains a few examples intended to give you some intuition about
    algorithm design. Many other algorithms appear elsewhere in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that the most efficient algorithm is not always the algorithm of
    choice. A program that does everything in the most efficient possible way is often
    needlessly difficult to understand. It is often a good strategy to start by solving
    the problem at hand in the most straightforward manner possible, instrument it
    to find any computational bottlenecks, and then look for ways to improve the computational
    complexity of those parts of the program contributing to the bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1 Search Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **search algorithm** is a method for finding an item or group of items with
    specific properties within a collection of items. We refer to the collection of
    items as a **search space**. The search space might be something concrete, such
    as a set of electronic medical records, or something abstract, such as the set
    of all integers. A large number of problems that occur in practice can be formulated
    as search problems.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the algorithms presented earlier in this book can be viewed as search
    algorithms. In Chapter 3, we formulated finding an approximation to the roots
    of a polynomial as a search problem and looked at three algorithms—exhaustive
    enumeration, bisection search, and Newton–Raphson—for searching the space of possible
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will examine two algorithms for searching a list. Each meets
    the specification
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The astute reader might wonder if this is not semantically equivalent to the
    Python expression `e in L`. The answer is yes, it is. And if you are unconcerned
    about the efficiency of discovering whether `e` is in `L`, you should simply write
    that expression.
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.1 Linear Search and Using Indirection to Access Elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Python uses the following algorithm to determine if an element is in a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If the element `e` is not in the list, the algorithm will perform *θ*`(len(L))`
    tests, i.e., the complexity is at best linear in the length of `L`. Why “at best”
    linear? It will be linear only if each operation inside the loop can be done in
    constant time. That raises the question of whether Python retrieves the `i`^(th)
    element of a list in constant time. Since our model of computation assumes that
    fetching the contents of an address is a constant-time operation, the question
    becomes whether we can compute the address of the `i`^(th) element of a list in
    constant time.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start by considering the simple case where each element of the list is
    an integer. This implies that each element of the list is the same size, e.g.,
    four units of memory (four 8-bit bytes[^(69)](#c12-fn-0002)). Assuming that the
    elements of the list are stored contiguously, the address in memory of the `i`^(th)
    element of the list is simply `start + 4`*`i`, where `start` is the address of
    the start of the list. Therefore we can assume that Python could compute the address
    of the `i`^(th) element of a list of integers in constant time.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, we know that Python lists can contain objects of types other than
    `int`, and that the same list can contain objects of many types and sizes. You
    might think that this would present a problem, but it does not.
  prefs: []
  type: TYPE_NORMAL
- en: In Python, a list is represented as a length (the number of objects in the list)
    and a sequence of fixed-size **pointers**[^(70)](#c12-fn-0003) to objects. [Figure
    12-1](#c12-fig-0001) illustrates the use of these pointers.
  prefs: []
  type: TYPE_NORMAL
- en: '![c12-fig-0001.jpg](../images/c12-fig-0001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12-1](#c12-fig-0001a) Implementing lists'
  prefs: []
  type: TYPE_NORMAL
- en: The shaded region represents a list containing four elements. The leftmost shaded
    box contains a pointer to an integer indicating the length of the list. Each of
    the other shaded boxes contains a pointer to an object in the list.
  prefs: []
  type: TYPE_NORMAL
- en: If the length field occupies four units of memory, and each pointer (address)
    occupies four units of memory, the address of the `i`^(th) element of the list
    is stored at the address `start + 4 + 4`*`i`. Again, this address can be found
    in constant time, and then the value stored at that address can be used to access
    the `i`^(th)element. This access too is a constant-time operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example illustrates one of the most important implementation techniques
    used in computing: **indirection**.[^(71)](#c12-fn-0004) Generally speaking, indirection
    involves accessing something by first accessing something else that contains a
    reference to the thing initially sought. This is what happens each time we use
    a variable to refer to the object to which that variable is bound. When we use
    a variable to access a list and then a reference stored in that list to access
    another object, we are going through two levels of indirection.[^(72)](#c12-fn-0005)'
  prefs: []
  type: TYPE_NORMAL
- en: 12.1.2 Binary Search and Exploiting Assumptions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Getting back to the problem of implementing `search(L, e)`, is *θ*`(len(L))`
    the best we can do? Yes, if we know nothing about the relationship of the values
    of the elements in the list and the order in which they are stored. In the worst
    case, we have to look at each element in `L` to determine whether `L` contains
    `e`.
  prefs: []
  type: TYPE_NORMAL
- en: But suppose we know something about the order in which elements are stored,
    e.g., suppose we know that we have a list of integers stored in ascending order.
    We could change the implementation so that the search stops when it reaches a
    number larger than the number for which it is searching, as in [Figure 12-2](#c12-fig-0002).
  prefs: []
  type: TYPE_NORMAL
- en: '![c12-fig-0002.jpg](../images/c12-fig-0002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12-2](#c12-fig-0002a) Linear search of a sorted list'
  prefs: []
  type: TYPE_NORMAL
- en: This would improve the average running time. However, it would not change the
    worst-case complexity of the algorithm, since in the worst case each element of
    `L` is examined.
  prefs: []
  type: TYPE_NORMAL
- en: We can, however, get a considerable improvement in the worst-case complexity
    by using an algorithm, **binary search**, that is similar to the bisection search
    algorithm used in Chapter 3 to find an approximation to the square root of a floating-point
    number. There we relied upon the fact that there is an intrinsic total ordering
    on floating-point numbers. Here we rely on the assumption that the list is ordered.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Pick an index, `i`, that divides the list `L` roughly in half.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. Ask if `L[i] == e`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. If not, ask whether `L[i]` is larger or smaller than `e`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 4\. Depending upon the answer, search either the left or right half of `L` for
    `e`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the structure of this algorithm, it is not surprising that the most straightforward
    implementation of binary search uses recursion, as shown in [Figure 12-3](#c12-fig-0003).
  prefs: []
  type: TYPE_NORMAL
- en: The outer function in [Figure 12-3](#c12-fig-0003), `search(L, e)`, has the
    same arguments and specification as the function defined in [Figure 12-2](#c12-fig-0002).
    The specification says that the implementation may assume that `L` is sorted in
    ascending order. The burden of making sure that this assumption is satisfied lies
    with the caller of `search`. If the assumption is not satisfied, the implementation
    has no obligation to behave well. It could work, but it could also crash or return
    an incorrect answer. Should `search` be modified to check that the assumption
    is satisfied? This might eliminate a source of errors, but it would defeat the
    purpose of using binary search, since checking the assumption would itself take
    `O(len(L))` time.
  prefs: []
  type: TYPE_NORMAL
- en: '![c12-fig-0003.jpg](../images/c12-fig-0003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12-3](#c12-fig-0003a) Recursive binary search'
  prefs: []
  type: TYPE_NORMAL
- en: Functions such as `search` are often called **wrapper functions**. The function
    provides a nice interface for client code, but is essentially a pass-through that
    does no serious computation. Instead, it calls the helper function `bSearch` with
    appropriate arguments. This raises the question of why not eliminate `search`
    and have clients call `bin_search` directly? The reason is that the parameters
    `low` and `high` have nothing to do with the abstraction of searching a list for
    an element. They are implementation details that should be hidden from those writing
    programs that call `search`.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now analyze the complexity of `bin_search`. We showed in the last section
    that list access takes constant time. Therefore, we can see that excluding the
    recursive call, each instance of `bSearch` is *θ*`(1)`. Therefore, the complexity
    of `bin_search` depends only upon the number of recursive calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'If this were a book about algorithms, we would now dive into a careful analysis
    using something called a recurrence relation. But since it isn''t, we will take
    a much less formal approach that starts with the question “How do we know that
    the program terminates?” Recall that in Chapter 3 we asked the same question about
    a `while` loop. We answered the question by providing a decrementing function
    for the loop. We do the same thing here. In this context, the decrementing function
    has the properties:'
  prefs: []
  type: TYPE_NORMAL
- en: It maps the values to which the formal parameters are bound to a nonnegative
    integer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When its value is `0`, the recursion terminates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each recursive call, the value of the decrementing function is less than
    the value of the decrementing function on entry to the instance of the function
    making the call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decrementing function for `bin_search` is `high`–`low`. The `if` statement
    in `search` ensures that the value of this decrementing function is at least `0`
    the first time `bSearch` is called (decrementing function property 1).
  prefs: []
  type: TYPE_NORMAL
- en: When `bin_search` is entered, if `high`–`low` is exactly `0`, the function makes
    no recursive call—simply returning the value `L[low] == e` (satisfying decrementing
    function property 2).
  prefs: []
  type: TYPE_NORMAL
- en: The function `bin_search` contains two recursive calls. One call uses arguments
    that cover all the elements to the left of `mid`, and the other call uses arguments
    that cover all the elements to the right of `mid`. In either case, the value of
    `high`–`low` is cut in half (satisfying decrementing function property 3).
  prefs: []
  type: TYPE_NORMAL
- en: We now understand why the recursion terminates. The next question is how many
    times can the value of `high–low` be cut in half before `high–low == 0`? Recall
    that `log`[y]`(x)` is the number of times that `y` has to be multiplied by itself
    to reach `x`. Conversely, if `x` is divided by `y log`[y]`(x)` times, the result
    is `1`. This implies that `high–low` can be cut in half using floor division at
    most `log`[2]`(``high–low``)` times before it reaches `0`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can answer the question, what is the algorithmic complexity of binary
    search? Since when `search` calls `bSearch` the value of `high`–`low` is equal
    to `len(L)-1`, the complexity of `search` is *θ*`(log(len(``L``)))`.[^(73)](#c12-fn-0006)
  prefs: []
  type: TYPE_NORMAL
- en: '**Finger exercise:** Why does the code use `mid+1` rather than `mid` in the
    second recursive call?'
  prefs: []
  type: TYPE_NORMAL
- en: 12.2 Sorting Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have just seen that if we happen to know a list is sorted, we can exploit
    that information to greatly reduce the time needed to search a list. Does this
    mean that when asked to search a list we should first sort it and then perform
    the search?
  prefs: []
  type: TYPE_NORMAL
- en: Let *θ*`(sortComplexity(L))` be a tight bound on the complexity of sorting a
    list. Since we know that we can search an unsorted list in *θ*`(len(L))` time,
    the question of whether we should first sort and then search boils down to the
    question, is `sortComplexity(L) + log(len(L))` less than `len(L)`? The answer,
    sadly, is no. It is impossible sort a list without looking at each element in
    the list at least once, so it is not possible to sort a list in sub-linear time.
  prefs: []
  type: TYPE_NORMAL
- en: Does this mean that binary search is an intellectual curiosity of no practical
    import? Happily, no. Suppose that we expect to search the same list many times.
    It might well make sense to pay the overhead of sorting the list once, and then
    **amortize** the cost of the sort over many searches. If we expect to search the
    list `k` times, the relevant question becomes, is `sortComplexity(L) + k`*`log(len(L))`
    less than `k`*`len(L)`?
  prefs: []
  type: TYPE_NORMAL
- en: As `k` becomes large, the time required to sort the list becomes increasingly
    irrelevant. How big `k` needs to be depends upon how long it takes to sort a list.
    If, for example, sorting were exponential in the size of the list, `k` would have
    to be quite large.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, sorting can be done rather efficiently. For example, the standard
    implementation of sorting in most Python implementations runs in roughly `O(n`*`log(n))`
    time, where `n` is the length of the list. In practice, you will rarely need to
    implement your own sort function. In most cases, the right thing to do is to use
    either Python's built-in `sort` method (`L.sort()` sorts the list `L`) or its
    built-in function `sorted` (`sorted(L)` returns a list with the same elements
    as `L`, but does not mutate `L`). We present sorting algorithms here primarily
    to provide some practice in thinking about algorithm design and complexity analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We begin with a simple but inefficient algorithm, **selection sort**. Selection
    sort, [Figure 12-4](#c12-fig-0004), works by maintaining the **loop invariant**
    that, given a partitioning of the list into a prefix (`L[0:i]`) and a suffix (`L[i+1:len(L)]`),
    the prefix is sorted and no element in the prefix is larger than the smallest
    element in the suffix.
  prefs: []
  type: TYPE_NORMAL
- en: We use induction to reason about loop invariants.
  prefs: []
  type: TYPE_NORMAL
- en: 'Base case: At the start of the first iteration, the prefix is empty, i.e.,
    the suffix is the entire list. Therefore, the invariant is (trivially) true.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Induction step: At each step of the algorithm, we move one element from the
    suffix to the prefix. We do this by appending a minimum element of the suffix
    to the end of the prefix. Because the invariant held before we moved the element,
    we know that after we append the element the prefix is still sorted. We also know
    that since we removed the smallest element in the suffix, no element in the prefix
    is larger than the smallest element in the suffix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Termination: When the loop is exited, the prefix includes the entire list,
    and the suffix is empty. Therefore, the entire list is now sorted in ascending
    order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![c12-fig-0004.jpg](../images/c12-fig-0004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12-4](#c12-fig-0004a) Selection sort'
  prefs: []
  type: TYPE_NORMAL
- en: It's hard to imagine a simpler or more obviously correct sorting algorithm.
    Unfortunately, it is rather inefficient.[^(74)](#c12-fn-0007) The complexity of
    the inner loop is *θ*`(len(L))`. The complexity of the outer loop is also *θ*`(len(L))`.
    So, the complexity of the entire function is *θ*`(len(L)`²`)`. I.e., it is quadratic
    in the length of `L`.[^(75)](#c12-fn-0008)
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.1 Merge Sort
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fortunately, we can do a lot better than quadratic time using a **divide-and-conquer**
    **algorithm**. The basic idea is to combine solutions of simpler instances of
    the original problem. In general, a divide-and-conquer algorithm is characterized
    by
  prefs: []
  type: TYPE_NORMAL
- en: A threshold input size, below which the problem is not subdivided
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size and number of sub-instances into which an instance is split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm used to combine sub-solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The threshold is sometimes called the **recursive base**. For item 2, it is
    usual to consider the ratio of initial problem size to the sub-instance size.
    In most of the examples we've seen so far, the ratio was `2`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Merge sort** is a prototypical divide-and-conquer algorithm. It was invented
    in 1945, by John von Neumann, and is still widely used. Like many divide-and-conquer
    algorithms it is most easily described recursively:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. If the list is of length `0` or `1`, it is already sorted.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. If the list has more than one element, split the list into two lists, and
    use merge sort to sort each of them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. Merge the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The key observation made by von Neumann is that two sorted lists can be efficiently
    merged into a single sorted list. The idea is to look at the first element of
    each list and move the smaller of the two to the end of the result list. When
    one of the lists is empty, all that remains is to copy the remaining items from
    the other list. Consider, for example, merging the two lists `L_1 =` `[1,5,12,18,19,20]`
    and `L_2 =` `[2,3,4,17]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Remaining in L_1 | Remaining in L_2 | Result |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `[1,5,12,18,19,20]` | `[2,3,4,17]` | `[]` |'
  prefs: []
  type: TYPE_TB
- en: '| `[5,12,18,19,20]` | `[2,3,4,17]` | `[1]` |'
  prefs: []
  type: TYPE_TB
- en: '| `[5,12,18,19,20]` | `[3,4,17]` | `[1,2]` |'
  prefs: []
  type: TYPE_TB
- en: '| `[5,12,18,19,20]` | `[4,17]` | `[1,2,3]` |'
  prefs: []
  type: TYPE_TB
- en: '| `[5,12,18,19,20]` | `[17]` | `[1,2,3,4]` |'
  prefs: []
  type: TYPE_TB
- en: '| `[12,18,19,20]` | `[17]` | `[1,2,3,4,5]` |'
  prefs: []
  type: TYPE_TB
- en: '| `[18,19,20]` | `[17]` | `[1,2,3,4,5,12]` |'
  prefs: []
  type: TYPE_TB
- en: '| `[18,19,20]` | `[]` | `[1,2,3,4,5,12,17]` |'
  prefs: []
  type: TYPE_TB
- en: '| `[]` | `[]` | `[1,2,3,4,5,12,17,18,19,20]` |'
  prefs: []
  type: TYPE_TB
- en: What is the complexity of the merge process? It involves two constant-time operations,
    comparing the values of elements and copying elements from one list to another.
    The number of comparisons is order *θ*`(len(L))`, where `L` is the longer of the
    two lists. The number of copy operations is order *θ*`(len(L1) + len(L2))`, because
    each element is copied exactly once. (The time to copy an element depends on the
    size of the element. However, this does not affect the order of the growth of
    sort as a function of the number of elements in the list.) Therefore, merging
    two sorted lists is linear in the length of the lists.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-5](#c12-fig-0005) contains an implementation of the merge sort algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '![c12-fig-0005.jpg](../images/c12-fig-0005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12-5](#c12-fig-0005a) Merge sort'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we have made the comparison operator a parameter of the `merge_sort`
    function and written a lambda expression to supply a default value. So, for example,
    the code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's analyze the complexity of `merge_sort`. We already know that the time
    complexity of `merge` is order *θ*`(len(L))`. At each level of recursion the total
    number of elements to be merged is `len(L)`. Therefore, the time complexity of
    `merge_sort` is order *θ*`(len(``L``))` multiplied by the number of levels of
    recursion. Since `merge_sort` divides the list in half each time, we know that
    the number of levels of recursion is `order` *θ*`(log(len(``L``))`. Therefore,
    the time complexity of `merge_sort` is *θ*`(n`*`log(n))`, where `n` is `len(L)`.[^(76)](#c12-fn-0009)
  prefs: []
  type: TYPE_NORMAL
- en: This is a lot better than selection sort's *θ*`(len(``L``)`²`)`. For example,
    if `L` has `10,000` elements, `len(``L``)`² is `100` million but `len(``L``)`*`log`[2]`(len(``L``))`
    is about `130,000`.
  prefs: []
  type: TYPE_NORMAL
- en: This improvement in time complexity comes with a price. Selection sort is an
    example of an **in-place** sorting algorithm. Because it works by swapping the
    place of elements within the list, it uses only a constant amount of extra storage
    (one element in our implementation). In contrast, the merge sort algorithm involves
    making copies of the list. This means that its space complexity is order *θ*`(len(``L``))`.
    This can be an issue for large lists.[^(77)](#c12-fn-0010)
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we want to sort a list of names written as first name followed by last
    name, e.g., the list `['Chris Terman', ‘Tom Brady', 'Eric Grimson', 'Gisele Bundchen']`.
    [Figure 12-6](#c12-fig-0006) defines two ordering functions, and then uses these
    to sort a list in two different ways. Each function uses the `split` method of
    type `str`.
  prefs: []
  type: TYPE_NORMAL
- en: '![c12-fig-0006.jpg](../images/c12-fig-0006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12-6](#c12-fig-0006a) Sorting a list of names'
  prefs: []
  type: TYPE_NORMAL
- en: When the code in [Figure 12-6](#c12-fig-0006) is run, it prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Finger exercise**: Use `merge_sort` to sort a list to tuples of integers.
    The sorting order should be determined by the sum of the integers in the tuple.
    For example, `(5, 2)` should precede `(1, 8)` and follow `(1, 2, 3)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 12.2.2 Sorting in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sorting algorithm used in most Python implementations is called **timsort**.[^(78)](#c12-fn-0011)
    The key idea is to take advantage of the fact that in a lot of data sets, the
    data are already partially sorted. Timsort's worst-case performance is the same
    as merge sort's, but on average it performs considerably better.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, the Python method `list.sort` takes a list as its first
    argument and modifies that list. In contrast, the Python function `sorted` takes
    an iterable object (e.g., a list or a view) as its first argument and returns
    a new sorted list. For example, the code
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: will print
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice that when the `sorted` function is applied to a dictionary, it returns
    a sorted list of the keys of the dictionary. In contrast, when the `sort` method
    is applied to a dictionary, it causes an exception to be raised since there is
    no method `dict.sort`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the `list.sort` method and the `sorted` function can have two additional
    parameters. The `key` parameter plays the same role as `compare` in our implementation
    of merge sort: it supplies the comparison function to be used. The `reverse` parameter
    specifies whether the list is to be sorted in ascending or descending order relative
    to the comparison function. For example, the code'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: sorts the elements of `L` in reverse order of length and prints
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Both the `list.sort` method and the `sorted` function provide **stable sorts**.
    This means that if two elements are equal with respect to the comparison (`len`
    in this example) used in the sort, their relative ordering in the original list
    (or other iterable object) is preserved in the final list. (Since no key can occur
    more than once in a `dict`, the question of whether `sorted` is stable when applied
    to a `dict` is moot.)
  prefs: []
  type: TYPE_NORMAL
- en: '**Finger exercise**: Is `merge_sort` a stable sort?'
  prefs: []
  type: TYPE_NORMAL
- en: 12.3 Hash Tables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we put merge sort together with binary search, we have a nice way to search
    lists. We use merge sort to preprocess the list in order *θ*`(n`*`log(n))` time,
    and then we use binary search to test whether elements are in the list in order
    *θ*`log(n))` time. If we search the list `k` times, the overall time complexity
    is order *θ*`(n`*`log(n) + k`*`log(n))`.
  prefs: []
  type: TYPE_NORMAL
- en: This is good, but we can still ask, is logarithmic the best that we can do for
    search when we are willing to do some preprocessing?
  prefs: []
  type: TYPE_NORMAL
- en: When we introduced the type `dict` in Chapter 5, we said that dictionaries use
    a technique called hashing to do the lookup in time that is nearly independent
    of the size of the dictionary. The basic idea behind a **hash table** is simple.
    We convert the key to an integer, and then use that integer to index into a list,
    which can be done in constant time. In principle, values of any type can be easily
    converted to an integer. After all, we know that the internal representation of
    each object is a sequence of bits, and any sequence of bits can be viewed as representing
    an integer. For example, the internal representation of the string `'abc'` is
    the sequence of bits `011000010110001001100011`, which can be viewed as a representation
    of the decimal integer `6,382,179`. Of course, if we want to use the internal
    representation of strings as indices into a list, the list is going to have to
    be pretty darn long.
  prefs: []
  type: TYPE_NORMAL
- en: What about situations where the keys are already integers? Imagine, for the
    moment, that we are implementing a dictionary all of whose keys are U.S. Social
    Security numbers, which are nine-digit integers. If we represented the dictionary
    by a list with `10`⁹ elements and used Social Security numbers to index into the
    list, we could do lookups in constant time. Of course, if the dictionary contained
    entries for only ten thousand (`10`⁴) people, this would waste quite a lot of
    space.
  prefs: []
  type: TYPE_NORMAL
- en: Which gets us to the subject of hash functions. A **hash function** maps a large
    space of inputs (e.g., all natural numbers) to a smaller space of outputs (e.g.,
    the natural numbers between `0` and `5000`). Hash functions can be used to convert
    a large space of keys to a smaller space of integer indices.
  prefs: []
  type: TYPE_NORMAL
- en: Since the space of possible outputs is smaller than the space of possible inputs,
    a hash function is a **many-to-one mapping**, i.e., multiple different inputs
    may be mapped to the same output. When two inputs are mapped to the same output,
    it is called a **collision**—a topic we will return to shortly. A good hash function
    produces a **uniform distribution**; i.e., every output in the range is equally
    probable, which minimizes the probability of collisions.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-7](#c12-fig-0007) uses a simple hash function (recall that `i%j`
    returns the remainder when the integer `i` is divided by the integer `j`) to implement
    a dictionary with integers as keys.'
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is to represent an instance of class `Int_dict` by a list of
    **hash** **buckets**, where each bucket is a list of key/value pairs implemented
    as tuples. By making each bucket a list, we handle collisions by storing all of
    the values that hash to the same bucket in the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hash table works as follows: The instance variable `buckets` is initialized
    to a list of `num_buckets` empty lists. To store or look up an entry with key
    `key`, we use the hash function `%` to convert `key` into an integer and use that
    integer to index into `buckets` to find the hash bucket associated with `key`.
    We then search that bucket (which, remember, is a list) linearly to see if there
    is an entry with the key `key`. If we are doing a lookup and there is an entry
    with the key, we simply return the value stored with that key. If there is no
    entry with that key, we return `None`. If a value is to be stored, we first check
    if there is already an entry with that key in the hash bucket. If so, we replace
    the entry with a new tuple; otherwise we append a new entry to the bucket.'
  prefs: []
  type: TYPE_NORMAL
- en: There are many other ways to handle collisions, some considerably more efficient
    than using lists. But this is probably the simplest mechanism, and it works fine
    if the hash table is large enough relative to the number of elements stored in
    it, and the hash function provides a good enough approximation to a uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![c12-fig-0007.jpg](../images/c12-fig-0007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12-7](#c12-fig-0007a) Implementing dictionaries using hashing'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the `__str__` method produces a representation of a dictionary that
    is unrelated to the order in which elements were added to it, but is instead ordered
    by the values to which the keys happen to hash.
  prefs: []
  type: TYPE_NORMAL
- en: The following code first constructs an `Int_dict` with 17 buckets and 20 entries.
    The values of the entries are the integers `0` to `19`. The keys are chosen at
    random, using `random.choice`, from integers in the range `0` to `10`⁵ `- 1`.
    (We discuss the `random` module in Chapters 16 and 17.) The code then prints the
    `Int_dict` using the `__str__` method defined in the class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: When we ran this code it printed[^(79)](#c12-fn-0012)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The following code prints the individual hash buckets by iterating over `D.buckets`.
    (This is a terrible violation of information hiding, but pedagogically useful.)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: When we ran this code it printed
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: When we violate the abstraction barrier and peek at the representation of the
    `Int_dict`, we see that some of the hash buckets are empty. Others contain one
    or more entries—depending upon the number of collisions that occurred.
  prefs: []
  type: TYPE_NORMAL
- en: What is the complexity of `get_value`? If there were no collisions it would
    be `constant time b`ecause each hash bucket would be of length 0 or 1\. But, of
    course, there might be collisions. If everything hashed to the same bucket, it
    would be `linear in` the number of entries in the dictionary, because the code
    would perform a linear search on that hash bucket. By making the hash table large
    enough, we can reduce the number of collisions sufficiently to allow us to treat
    the complexity as `constant time.` That is, we can trade space for time. But what
    is the tradeoff? To answer this question, we need to use a tiny bit of probability,
    so we defer the answer to Chapter 17.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4 Terms Introduced in Chapter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: search algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: search space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pointer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: indirection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: binary search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: wrapper function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: amortized complexity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: selection sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: loop invariant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: divide and conquer algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: recursive base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: merge sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: in-place sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: quicksort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: timsort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: stable sort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hash table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hash function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: many-to-one mapping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: collision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: uniform distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: hash bucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
