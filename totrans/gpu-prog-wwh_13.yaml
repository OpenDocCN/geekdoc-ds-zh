- en: Multiple GPU programming with MPI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://enccs.github.io/gpu-programming/10-multiple_gpu/](https://enccs.github.io/gpu-programming/10-multiple_gpu/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*[GPU programming: why, when and how?](../)* **   Multiple GPU programming
    with MPI'
  prefs: []
  type: TYPE_NORMAL
- en: '[Edit on GitHub](https://github.com/ENCCS/gpu-programming/blob/main/content/10-multiple_gpu.rst)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs: []
  type: TYPE_NORMAL
- en: What approach should be adopted to extend the synchronous OpenACC and OpenMP
    offloading models to utilise multiple GPUs across multiple nodes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: To learn about combining MPI with either OpenACC or OpenMP offloading models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn about implementing GPU-awareness MPI approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 30 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 30 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exploring multiple GPUs (Graphics Processing Units) across distributed nodes
    offers the potential to fully leveraging the capacity of modern HPC (High-Performance
    Computing) systems at a large scale. Here one of the approaches to accelerate
    computing on distributed systems is to combine MPI (Message Passing Interface)
    with a GPU programming model such as OpenACC and OpenMP application programming
    interfaces (APIs). This combination is motivated by both the simplicity of these
    APIs, and the widespread use of MPI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this guide we provide readers, who are familiar with MPI, with insights
    on implementing a hybrid model in which the MPI communication framework is combined
    with either OpenACC or OpenMP APIs. A special focus will be on performing point-to-point
    (e.g. MPI_Send and MPI_Recv) and collective operations (e.g. MPI_Allreduce) from
    OpenACC and OpenMP APIs. Here we address two scenarios: (i) a scenario in which
    MPI operations are performed in the CPU-host followed by an offload to the GPU-device;
    and (ii) a scenario in which MPI operations are performed between a pair of GPUs
    without involving the CPU-host memory. The latter scenario is referred to as GPU-awareness
    MPI, and has the advantage of reducing the computing time caused by transferring
    data via the host-memory during heterogeneous communications, thus rendering HPC
    applications efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide is organized as follows: we first introduce how to assign each MPI
    rank to a GPU device within the same node. We consider a situation in which the
    host and the device have a distinct memory. This is followed by a presentation
    on the hybrid MPI-OpenACC/OpenMP offloading with and without the GPU-awareness
    MPI. Exercises to help understanding these concepts are provided at the end.'
  prefs: []
  type: TYPE_NORMAL
- en: Assigning MPI-ranks to GPU-devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Accelerating MPI applications to utilise multiple GPUs on distributed nodes
    requires as a first step assigning each MPI rank to a GPU device, such that two
    MPI ranks do not use the same GPU device. This is necessarily in order to prevent
    the application from a potential crash. This is because GPUs are designed to handle
    multiple threading tasks, but not multiple MPI ranks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the way to ensure that two MPI ranks do not use the same GPU, is to determine
    which MPI processes run on the same node, such that each process can be assigned
    to a GPU device within the same node. This can be done, for instance, by splitting
    the world communicator into sub-groups of communicators (or sub-communicators)
    using the routine MPI_COMM_SPLIT_TYPE().
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Here, the size of each sub-communicator corresponds to the number of GPUs per
    node (which is also the number of tasks per node), and each sub-communicator contains
    a list of processes indicated by a rank. These processes have a shared-memory
    region defined by the argument MPI_COMM_TYPE_SHARED (see the [MPI report](https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf))
    for more details). Calling the routine MPI_COMM_SPLIT_TYPE() returns a sub-communicator
    labelled in the code above *”host_comm”*, and in which MPI-ranks are ranked from
    0 to number of processes per node -1\. These MPI ranks are in turn assigned to
    different GPU devices within the same node. This procedure is done according to
    which directive-based model is implemented. The retrieved MPI ranks are then stored
    in the variable **myDevice**. The variable is passed to an OpenACC or OpenMP routine
    as indicated in the code below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Assign device`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Another useful function for retrieving the device number of a specific device,
    which is useful, e.g., to map data to a specific device is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The syntax of assigning MPI ranks to GPU devices is summarised below
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Set device`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Hybrid MPI-OpenACC/OpenMP without GPU-awareness approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After covering how to assign each MPI-rank to a GPU device, we now address the
    concept of combining MPI with either OpenACC or OpenMP offloading. In this approach,
    calling an MPI routine from an OpenACC or OpenMP API requires updating the data
    in the CPU host before and after an MPI call. In this scenario, the data is copied
    back and forth between the host and the device before and after each MPI call.
    In the hybrid MPI-OpenACC model, the procedure is defined by specifying the directive
    update host() for copying the data from the device to the host before an MPI call;
    and by the directive update device() specified after an MPI call for copying the
    data back to the device. Similarly in the hybrid MPI-OpenMP. Here, updating the
    data in the host can be done by specifying the OpenMP directives update device()
    from() and update device() to(), respectively, for copying the data from the device
    to the host and back to the device.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the concept of the hybrid MPI-OpenACC/OpenMP, we show below an
    example of an implementation that involves the MPI functions MPI_Send() and MPI_Recv().
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Update host/device directives`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here we present a code example that combines MPI with OpenACC/OpenMP API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Update host/device directives`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Despite the simplicity of implementing the hybrid MPI-OpenACC/OpenMP offloading,
    it suffers from a low performance caused by an explicit transfer of data between
    the host and the device before and after calling an MPI routine. This constitutes
    a bottleneck in GPU-programming. To improve the performance affected by the host
    staging during the data transfer, one can implement the GPU-awareness MPI approach
    as described in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid MPI-OpenACC/OpenMP with GPU-awareness approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of the GPU-aware MPI enables an MPI library to directly access the
    GPU-device memory without necessarily using the CPU-host memory as an intermediate
    buffer (see e.g. [OpenMPI documentation](https://docs.open-mpi.org/en/v5.0.1/tuning-apps/networking/cuda.html)).
    This offers the benefit of transferring data from one GPU to another GPU without
    the involvement of the CPU-host memory.
  prefs: []
  type: TYPE_NORMAL
- en: To be specific, in the GPU-awareness approach, the device pointers point to
    the data allocated in the GPU memory space (data should be present in the GPU
    device). Here, the pointers are passed as arguments to an MPI routine that is
    supported by the GPU memory. As MPI routines can directly access GPU memory, it
    offers the possibility of communicating between pairs of GPUs without transferring
    data back to the host.
  prefs: []
  type: TYPE_NORMAL
- en: In the hybrid MPI-OpenACC model, the concept is defined by combining the directive
    host_data together with the clause use_device(list_array). This combination enables
    the access to the arrays listed in the clause use_device(list_array) from the
    host (see [here](https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.2-final.pdf)).
    The list of arrays, which are already present in the GPU-device memory, are directly
    passed to an MPI routine without a need of a staging host-memory for copying the
    data. Note that for initially copying data to GPU, we use unstructured data blocks
    characterized by the directives enter data and exit data. The unstructured data
    has the advantage of allowing to allocate and deallocate arrays within a data
    region.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the concept of the GPU-awareness MPI, we show below two examples
    that make use of point-to-point and collective operations from OpenACC and OpenMP
    APIs. In the first code example, the device pointer **f** is passed to the MPI
    functions MPI_Send() and MPI_Recv(); and in the second one, the pointer **SumToT**
    is passed to the MPI function MPI_Allreduce. Here, the MPI operations MPI_Send
    and MPI_Recv as well as MPI_Allreduce are performed between a pair of GPUs without
    passing through the CPU-host memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `GPU-awareness: MPI_Send & MPI_Recv`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Example: `GPU-awareness: MPI_Allreduce`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We provide below a code example that illustrates the implementation of the MPI
    functions MPI_Send(), MPI_Recv() and MPI_Allreduce() within an OpenACC/OpenMP
    API. This implementation is specifically designed to support GPU-aware MPI operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `GPU-awareness approach`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The GPU-aware MPI with OpenACC/OpenMP APIs has the capability of directly communicating
    between a pair of GPUs within a single node. However, performing the GPU-to-GPU
    communication across multiple nodes requires the the GPUDirect RDMA (Remote Direct
    Memory Access) technology. This technology can further improve performance by
    reducing latency.
  prefs: []
  type: TYPE_NORMAL
- en: Compilation process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The compilation process of the hybrid MPI-OpenACC and MPI-OpenMP offloading
    is described below. This description is given for a Cray compiler of the wrapper
    ftn. On LUMI-G, the following modules may be necessary before compiling (see the
    [LUMI documentation](https://docs.lumi-supercomputer.eu/development/compiling/prgenv/)
    for further details about the available programming environments):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Example: `Compilation process`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, the flags hacc and homp enable the OpenACC and OpenMP directives in the
    hybrid MPI-OpenACC and MPI-OpenMP applications, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enabling GPU-aware support**'
  prefs: []
  type: TYPE_NORMAL
- en: To enable the GPU-aware support in MPICH library, one needs to set the following
    environment variable before running the application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, we have presented an overview of a GPU-hybrid programming by
    integrating GPU-directive models, specifically OpenACC and OpenMP APIs, with the
    MPI library. The approach adopted here allows us to utilise multiple GPU-devices
    not only within a single node but it extends to distributed nodes. In particular,
    we have addressed GPU-aware MPI approach, which has the advantage of enabling
    a direct interaction between an MPI library and a GPU-device memory. In other
    words, it permits performing MPI operations between a pair of GPUs, thus reducing
    the computing time caused by the data locality.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We consider an MPI fortran code that solves a 2D-Laplace equation, and which
    is partially accelerated. The focus of the exercises is to complete the acceleration
    using either OpenACC or OpenMP API by following these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Access exercise material
  prefs: []
  type: TYPE_NORMAL
- en: 'Code examples for the exercises below can be accessed in the content/examples/exercise_multipleGPU
    subdirectory of this repository. To access them, you need to clone the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Exercise I: Set a GPU device'
  prefs: []
  type: TYPE_NORMAL
- en: Implement OpenACC/OpenMP functions that enable assigning each MPI rank to a
    GPU device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1.1 Compile and run the code on multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise II: Apply traditional MPI-OpenACC/OpenMP'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Incorporate the OpenACC directives *update host()* and *update device()*
    before and after calling an MPI function, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The OpenACC directive *update host()* is used to transfer data from GPU to CPU
    within a data region; while the directive *update device()* is used to transfer
    the data from CPU to GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Incorporate the OpenMP directives *update device() from()* and *update device()
    to()* before and after calling an MPI function, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The OpenMP directive *update device() from()* is used to transfer data from
    GPU to CPU within a data region; while the directive *update device() to()* is
    used to transfer the data from CPU to GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Compile and run the code on multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise III: Implement GPU-aware support'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Incorporate the OpenACC directive *host_data use_device()* to pass a device
    pointer to an MPI function.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Incorporate the OpenMP directive *data use_device_ptr()* to pass a device
    pointer to an MPI function.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Compile and run the code on multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise IV: Evaluate the performance'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the execution time of the accelerated codes in the exercises **II**
    and **III**, and compare it with that of a pure MPI implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[GPU-aware MPI](https://documentation.sigma2.no/code_development/guides/gpuaware_mpi.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MPI documentation](https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenACC specification](https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.2-final.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenMP specification](https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-2.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LUMI documentation](https://docs.lumi-supercomputer.eu/development/compiling/prgenv/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenACC vs OpenMP offloading](https://documentation.sigma2.no/code_development/guides/converting_acc2omp/openacc2openmp.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenACC course](https://github.com/HichamAgueny/GPU-course). [Previous](../9-language-support/
    "High-level language support") [Next](../11-gpu-porting/ "Preparing code for GPU
    porting")'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: © Copyright 2023-2024, The contributors.
  prefs: []
  type: TYPE_NORMAL
- en: Built with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme)
    provided by [Read the Docs](https://readthedocs.org). Questions
  prefs: []
  type: TYPE_NORMAL
- en: What approach should be adopted to extend the synchronous OpenACC and OpenMP
    offloading models to utilise multiple GPUs across multiple nodes?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: To learn about combining MPI with either OpenACC or OpenMP offloading models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn about implementing GPU-awareness MPI approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 30 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 30 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exploring multiple GPUs (Graphics Processing Units) across distributed nodes
    offers the potential to fully leveraging the capacity of modern HPC (High-Performance
    Computing) systems at a large scale. Here one of the approaches to accelerate
    computing on distributed systems is to combine MPI (Message Passing Interface)
    with a GPU programming model such as OpenACC and OpenMP application programming
    interfaces (APIs). This combination is motivated by both the simplicity of these
    APIs, and the widespread use of MPI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this guide we provide readers, who are familiar with MPI, with insights
    on implementing a hybrid model in which the MPI communication framework is combined
    with either OpenACC or OpenMP APIs. A special focus will be on performing point-to-point
    (e.g. MPI_Send and MPI_Recv) and collective operations (e.g. MPI_Allreduce) from
    OpenACC and OpenMP APIs. Here we address two scenarios: (i) a scenario in which
    MPI operations are performed in the CPU-host followed by an offload to the GPU-device;
    and (ii) a scenario in which MPI operations are performed between a pair of GPUs
    without involving the CPU-host memory. The latter scenario is referred to as GPU-awareness
    MPI, and has the advantage of reducing the computing time caused by transferring
    data via the host-memory during heterogeneous communications, thus rendering HPC
    applications efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide is organized as follows: we first introduce how to assign each MPI
    rank to a GPU device within the same node. We consider a situation in which the
    host and the device have a distinct memory. This is followed by a presentation
    on the hybrid MPI-OpenACC/OpenMP offloading with and without the GPU-awareness
    MPI. Exercises to help understanding these concepts are provided at the end.'
  prefs: []
  type: TYPE_NORMAL
- en: Assigning MPI-ranks to GPU-devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Accelerating MPI applications to utilise multiple GPUs on distributed nodes
    requires as a first step assigning each MPI rank to a GPU device, such that two
    MPI ranks do not use the same GPU device. This is necessarily in order to prevent
    the application from a potential crash. This is because GPUs are designed to handle
    multiple threading tasks, but not multiple MPI ranks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the way to ensure that two MPI ranks do not use the same GPU, is to determine
    which MPI processes run on the same node, such that each process can be assigned
    to a GPU device within the same node. This can be done, for instance, by splitting
    the world communicator into sub-groups of communicators (or sub-communicators)
    using the routine MPI_COMM_SPLIT_TYPE().
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Here, the size of each sub-communicator corresponds to the number of GPUs per
    node (which is also the number of tasks per node), and each sub-communicator contains
    a list of processes indicated by a rank. These processes have a shared-memory
    region defined by the argument MPI_COMM_TYPE_SHARED (see the [MPI report](https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf))
    for more details). Calling the routine MPI_COMM_SPLIT_TYPE() returns a sub-communicator
    labelled in the code above *”host_comm”*, and in which MPI-ranks are ranked from
    0 to number of processes per node -1\. These MPI ranks are in turn assigned to
    different GPU devices within the same node. This procedure is done according to
    which directive-based model is implemented. The retrieved MPI ranks are then stored
    in the variable **myDevice**. The variable is passed to an OpenACC or OpenMP routine
    as indicated in the code below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Assign device`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Another useful function for retrieving the device number of a specific device,
    which is useful, e.g., to map data to a specific device is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The syntax of assigning MPI ranks to GPU devices is summarised below
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Set device`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Hybrid MPI-OpenACC/OpenMP without GPU-awareness approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After covering how to assign each MPI-rank to a GPU device, we now address the
    concept of combining MPI with either OpenACC or OpenMP offloading. In this approach,
    calling an MPI routine from an OpenACC or OpenMP API requires updating the data
    in the CPU host before and after an MPI call. In this scenario, the data is copied
    back and forth between the host and the device before and after each MPI call.
    In the hybrid MPI-OpenACC model, the procedure is defined by specifying the directive
    update host() for copying the data from the device to the host before an MPI call;
    and by the directive update device() specified after an MPI call for copying the
    data back to the device. Similarly in the hybrid MPI-OpenMP. Here, updating the
    data in the host can be done by specifying the OpenMP directives update device()
    from() and update device() to(), respectively, for copying the data from the device
    to the host and back to the device.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the concept of the hybrid MPI-OpenACC/OpenMP, we show below an
    example of an implementation that involves the MPI functions MPI_Send() and MPI_Recv().
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Update host/device directives`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Here we present a code example that combines MPI with OpenACC/OpenMP API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Update host/device directives`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Despite the simplicity of implementing the hybrid MPI-OpenACC/OpenMP offloading,
    it suffers from a low performance caused by an explicit transfer of data between
    the host and the device before and after calling an MPI routine. This constitutes
    a bottleneck in GPU-programming. To improve the performance affected by the host
    staging during the data transfer, one can implement the GPU-awareness MPI approach
    as described in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid MPI-OpenACC/OpenMP with GPU-awareness approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of the GPU-aware MPI enables an MPI library to directly access the
    GPU-device memory without necessarily using the CPU-host memory as an intermediate
    buffer (see e.g. [OpenMPI documentation](https://docs.open-mpi.org/en/v5.0.1/tuning-apps/networking/cuda.html)).
    This offers the benefit of transferring data from one GPU to another GPU without
    the involvement of the CPU-host memory.
  prefs: []
  type: TYPE_NORMAL
- en: To be specific, in the GPU-awareness approach, the device pointers point to
    the data allocated in the GPU memory space (data should be present in the GPU
    device). Here, the pointers are passed as arguments to an MPI routine that is
    supported by the GPU memory. As MPI routines can directly access GPU memory, it
    offers the possibility of communicating between pairs of GPUs without transferring
    data back to the host.
  prefs: []
  type: TYPE_NORMAL
- en: In the hybrid MPI-OpenACC model, the concept is defined by combining the directive
    host_data together with the clause use_device(list_array). This combination enables
    the access to the arrays listed in the clause use_device(list_array) from the
    host (see [here](https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.2-final.pdf)).
    The list of arrays, which are already present in the GPU-device memory, are directly
    passed to an MPI routine without a need of a staging host-memory for copying the
    data. Note that for initially copying data to GPU, we use unstructured data blocks
    characterized by the directives enter data and exit data. The unstructured data
    has the advantage of allowing to allocate and deallocate arrays within a data
    region.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the concept of the GPU-awareness MPI, we show below two examples
    that make use of point-to-point and collective operations from OpenACC and OpenMP
    APIs. In the first code example, the device pointer **f** is passed to the MPI
    functions MPI_Send() and MPI_Recv(); and in the second one, the pointer **SumToT**
    is passed to the MPI function MPI_Allreduce. Here, the MPI operations MPI_Send
    and MPI_Recv as well as MPI_Allreduce are performed between a pair of GPUs without
    passing through the CPU-host memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `GPU-awareness: MPI_Send & MPI_Recv`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Example: `GPU-awareness: MPI_Allreduce`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: We provide below a code example that illustrates the implementation of the MPI
    functions MPI_Send(), MPI_Recv() and MPI_Allreduce() within an OpenACC/OpenMP
    API. This implementation is specifically designed to support GPU-aware MPI operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `GPU-awareness approach`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The GPU-aware MPI with OpenACC/OpenMP APIs has the capability of directly communicating
    between a pair of GPUs within a single node. However, performing the GPU-to-GPU
    communication across multiple nodes requires the the GPUDirect RDMA (Remote Direct
    Memory Access) technology. This technology can further improve performance by
    reducing latency.
  prefs: []
  type: TYPE_NORMAL
- en: Compilation process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The compilation process of the hybrid MPI-OpenACC and MPI-OpenMP offloading
    is described below. This description is given for a Cray compiler of the wrapper
    ftn. On LUMI-G, the following modules may be necessary before compiling (see the
    [LUMI documentation](https://docs.lumi-supercomputer.eu/development/compiling/prgenv/)
    for further details about the available programming environments):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Example: `Compilation process`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Here, the flags hacc and homp enable the OpenACC and OpenMP directives in the
    hybrid MPI-OpenACC and MPI-OpenMP applications, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enabling GPU-aware support**'
  prefs: []
  type: TYPE_NORMAL
- en: To enable the GPU-aware support in MPICH library, one needs to set the following
    environment variable before running the application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, we have presented an overview of a GPU-hybrid programming by
    integrating GPU-directive models, specifically OpenACC and OpenMP APIs, with the
    MPI library. The approach adopted here allows us to utilise multiple GPU-devices
    not only within a single node but it extends to distributed nodes. In particular,
    we have addressed GPU-aware MPI approach, which has the advantage of enabling
    a direct interaction between an MPI library and a GPU-device memory. In other
    words, it permits performing MPI operations between a pair of GPUs, thus reducing
    the computing time caused by the data locality.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We consider an MPI fortran code that solves a 2D-Laplace equation, and which
    is partially accelerated. The focus of the exercises is to complete the acceleration
    using either OpenACC or OpenMP API by following these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Access exercise material
  prefs: []
  type: TYPE_NORMAL
- en: 'Code examples for the exercises below can be accessed in the content/examples/exercise_multipleGPU
    subdirectory of this repository. To access them, you need to clone the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Exercise I: Set a GPU device'
  prefs: []
  type: TYPE_NORMAL
- en: Implement OpenACC/OpenMP functions that enable assigning each MPI rank to a
    GPU device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1.1 Compile and run the code on multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise II: Apply traditional MPI-OpenACC/OpenMP'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Incorporate the OpenACC directives *update host()* and *update device()*
    before and after calling an MPI function, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The OpenACC directive *update host()* is used to transfer data from GPU to CPU
    within a data region; while the directive *update device()* is used to transfer
    the data from CPU to GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Incorporate the OpenMP directives *update device() from()* and *update device()
    to()* before and after calling an MPI function, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The OpenMP directive *update device() from()* is used to transfer data from
    GPU to CPU within a data region; while the directive *update device() to()* is
    used to transfer the data from CPU to GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Compile and run the code on multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise III: Implement GPU-aware support'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Incorporate the OpenACC directive *host_data use_device()* to pass a device
    pointer to an MPI function.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Incorporate the OpenMP directive *data use_device_ptr()* to pass a device
    pointer to an MPI function.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Compile and run the code on multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise IV: Evaluate the performance'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the execution time of the accelerated codes in the exercises **II**
    and **III**, and compare it with that of a pure MPI implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[GPU-aware MPI](https://documentation.sigma2.no/code_development/guides/gpuaware_mpi.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MPI documentation](https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenACC specification](https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.2-final.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenMP specification](https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-2.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LUMI documentation](https://docs.lumi-supercomputer.eu/development/compiling/prgenv/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenACC vs OpenMP offloading](https://documentation.sigma2.no/code_development/guides/converting_acc2omp/openacc2openmp.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenACC course](https://github.com/HichamAgueny/GPU-course).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Exploring multiple GPUs (Graphics Processing Units) across distributed nodes
    offers the potential to fully leveraging the capacity of modern HPC (High-Performance
    Computing) systems at a large scale. Here one of the approaches to accelerate
    computing on distributed systems is to combine MPI (Message Passing Interface)
    with a GPU programming model such as OpenACC and OpenMP application programming
    interfaces (APIs). This combination is motivated by both the simplicity of these
    APIs, and the widespread use of MPI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this guide we provide readers, who are familiar with MPI, with insights
    on implementing a hybrid model in which the MPI communication framework is combined
    with either OpenACC or OpenMP APIs. A special focus will be on performing point-to-point
    (e.g. MPI_Send and MPI_Recv) and collective operations (e.g. MPI_Allreduce) from
    OpenACC and OpenMP APIs. Here we address two scenarios: (i) a scenario in which
    MPI operations are performed in the CPU-host followed by an offload to the GPU-device;
    and (ii) a scenario in which MPI operations are performed between a pair of GPUs
    without involving the CPU-host memory. The latter scenario is referred to as GPU-awareness
    MPI, and has the advantage of reducing the computing time caused by transferring
    data via the host-memory during heterogeneous communications, thus rendering HPC
    applications efficient.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This guide is organized as follows: we first introduce how to assign each MPI
    rank to a GPU device within the same node. We consider a situation in which the
    host and the device have a distinct memory. This is followed by a presentation
    on the hybrid MPI-OpenACC/OpenMP offloading with and without the GPU-awareness
    MPI. Exercises to help understanding these concepts are provided at the end.'
  prefs: []
  type: TYPE_NORMAL
- en: Assigning MPI-ranks to GPU-devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Accelerating MPI applications to utilise multiple GPUs on distributed nodes
    requires as a first step assigning each MPI rank to a GPU device, such that two
    MPI ranks do not use the same GPU device. This is necessarily in order to prevent
    the application from a potential crash. This is because GPUs are designed to handle
    multiple threading tasks, but not multiple MPI ranks.
  prefs: []
  type: TYPE_NORMAL
- en: One of the way to ensure that two MPI ranks do not use the same GPU, is to determine
    which MPI processes run on the same node, such that each process can be assigned
    to a GPU device within the same node. This can be done, for instance, by splitting
    the world communicator into sub-groups of communicators (or sub-communicators)
    using the routine MPI_COMM_SPLIT_TYPE().
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Here, the size of each sub-communicator corresponds to the number of GPUs per
    node (which is also the number of tasks per node), and each sub-communicator contains
    a list of processes indicated by a rank. These processes have a shared-memory
    region defined by the argument MPI_COMM_TYPE_SHARED (see the [MPI report](https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf))
    for more details). Calling the routine MPI_COMM_SPLIT_TYPE() returns a sub-communicator
    labelled in the code above *”host_comm”*, and in which MPI-ranks are ranked from
    0 to number of processes per node -1\. These MPI ranks are in turn assigned to
    different GPU devices within the same node. This procedure is done according to
    which directive-based model is implemented. The retrieved MPI ranks are then stored
    in the variable **myDevice**. The variable is passed to an OpenACC or OpenMP routine
    as indicated in the code below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Assign device`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Another useful function for retrieving the device number of a specific device,
    which is useful, e.g., to map data to a specific device is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The syntax of assigning MPI ranks to GPU devices is summarised below
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Set device`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Hybrid MPI-OpenACC/OpenMP without GPU-awareness approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After covering how to assign each MPI-rank to a GPU device, we now address the
    concept of combining MPI with either OpenACC or OpenMP offloading. In this approach,
    calling an MPI routine from an OpenACC or OpenMP API requires updating the data
    in the CPU host before and after an MPI call. In this scenario, the data is copied
    back and forth between the host and the device before and after each MPI call.
    In the hybrid MPI-OpenACC model, the procedure is defined by specifying the directive
    update host() for copying the data from the device to the host before an MPI call;
    and by the directive update device() specified after an MPI call for copying the
    data back to the device. Similarly in the hybrid MPI-OpenMP. Here, updating the
    data in the host can be done by specifying the OpenMP directives update device()
    from() and update device() to(), respectively, for copying the data from the device
    to the host and back to the device.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the concept of the hybrid MPI-OpenACC/OpenMP, we show below an
    example of an implementation that involves the MPI functions MPI_Send() and MPI_Recv().
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Update host/device directives`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Here we present a code example that combines MPI with OpenACC/OpenMP API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `Update host/device directives`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Despite the simplicity of implementing the hybrid MPI-OpenACC/OpenMP offloading,
    it suffers from a low performance caused by an explicit transfer of data between
    the host and the device before and after calling an MPI routine. This constitutes
    a bottleneck in GPU-programming. To improve the performance affected by the host
    staging during the data transfer, one can implement the GPU-awareness MPI approach
    as described in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid MPI-OpenACC/OpenMP with GPU-awareness approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The concept of the GPU-aware MPI enables an MPI library to directly access the
    GPU-device memory without necessarily using the CPU-host memory as an intermediate
    buffer (see e.g. [OpenMPI documentation](https://docs.open-mpi.org/en/v5.0.1/tuning-apps/networking/cuda.html)).
    This offers the benefit of transferring data from one GPU to another GPU without
    the involvement of the CPU-host memory.
  prefs: []
  type: TYPE_NORMAL
- en: To be specific, in the GPU-awareness approach, the device pointers point to
    the data allocated in the GPU memory space (data should be present in the GPU
    device). Here, the pointers are passed as arguments to an MPI routine that is
    supported by the GPU memory. As MPI routines can directly access GPU memory, it
    offers the possibility of communicating between pairs of GPUs without transferring
    data back to the host.
  prefs: []
  type: TYPE_NORMAL
- en: In the hybrid MPI-OpenACC model, the concept is defined by combining the directive
    host_data together with the clause use_device(list_array). This combination enables
    the access to the arrays listed in the clause use_device(list_array) from the
    host (see [here](https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.2-final.pdf)).
    The list of arrays, which are already present in the GPU-device memory, are directly
    passed to an MPI routine without a need of a staging host-memory for copying the
    data. Note that for initially copying data to GPU, we use unstructured data blocks
    characterized by the directives enter data and exit data. The unstructured data
    has the advantage of allowing to allocate and deallocate arrays within a data
    region.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the concept of the GPU-awareness MPI, we show below two examples
    that make use of point-to-point and collective operations from OpenACC and OpenMP
    APIs. In the first code example, the device pointer **f** is passed to the MPI
    functions MPI_Send() and MPI_Recv(); and in the second one, the pointer **SumToT**
    is passed to the MPI function MPI_Allreduce. Here, the MPI operations MPI_Send
    and MPI_Recv as well as MPI_Allreduce are performed between a pair of GPUs without
    passing through the CPU-host memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `GPU-awareness: MPI_Send & MPI_Recv`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Example: `GPU-awareness: MPI_Allreduce`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: We provide below a code example that illustrates the implementation of the MPI
    functions MPI_Send(), MPI_Recv() and MPI_Allreduce() within an OpenACC/OpenMP
    API. This implementation is specifically designed to support GPU-aware MPI operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: `GPU-awareness approach`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: The GPU-aware MPI with OpenACC/OpenMP APIs has the capability of directly communicating
    between a pair of GPUs within a single node. However, performing the GPU-to-GPU
    communication across multiple nodes requires the the GPUDirect RDMA (Remote Direct
    Memory Access) technology. This technology can further improve performance by
    reducing latency.
  prefs: []
  type: TYPE_NORMAL
- en: Compilation process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The compilation process of the hybrid MPI-OpenACC and MPI-OpenMP offloading
    is described below. This description is given for a Cray compiler of the wrapper
    ftn. On LUMI-G, the following modules may be necessary before compiling (see the
    [LUMI documentation](https://docs.lumi-supercomputer.eu/development/compiling/prgenv/)
    for further details about the available programming environments):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Example: `Compilation process`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Here, the flags hacc and homp enable the OpenACC and OpenMP directives in the
    hybrid MPI-OpenACC and MPI-OpenMP applications, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Enabling GPU-aware support**'
  prefs: []
  type: TYPE_NORMAL
- en: To enable the GPU-aware support in MPICH library, one needs to set the following
    environment variable before running the application.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, we have presented an overview of a GPU-hybrid programming by
    integrating GPU-directive models, specifically OpenACC and OpenMP APIs, with the
    MPI library. The approach adopted here allows us to utilise multiple GPU-devices
    not only within a single node but it extends to distributed nodes. In particular,
    we have addressed GPU-aware MPI approach, which has the advantage of enabling
    a direct interaction between an MPI library and a GPU-device memory. In other
    words, it permits performing MPI operations between a pair of GPUs, thus reducing
    the computing time caused by the data locality.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We consider an MPI fortran code that solves a 2D-Laplace equation, and which
    is partially accelerated. The focus of the exercises is to complete the acceleration
    using either OpenACC or OpenMP API by following these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Access exercise material
  prefs: []
  type: TYPE_NORMAL
- en: 'Code examples for the exercises below can be accessed in the content/examples/exercise_multipleGPU
    subdirectory of this repository. To access them, you need to clone the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Exercise I: Set a GPU device'
  prefs: []
  type: TYPE_NORMAL
- en: Implement OpenACC/OpenMP functions that enable assigning each MPI rank to a
    GPU device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1.1 Compile and run the code on multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise II: Apply traditional MPI-OpenACC/OpenMP'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Incorporate the OpenACC directives *update host()* and *update device()*
    before and after calling an MPI function, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The OpenACC directive *update host()* is used to transfer data from GPU to CPU
    within a data region; while the directive *update device()* is used to transfer
    the data from CPU to GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Incorporate the OpenMP directives *update device() from()* and *update device()
    to()* before and after calling an MPI function, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The OpenMP directive *update device() from()* is used to transfer data from
    GPU to CPU within a data region; while the directive *update device() to()* is
    used to transfer the data from CPU to GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Compile and run the code on multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise III: Implement GPU-aware support'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Incorporate the OpenACC directive *host_data use_device()* to pass a device
    pointer to an MPI function.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Incorporate the OpenMP directive *data use_device_ptr()* to pass a device
    pointer to an MPI function.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Compile and run the code on multiple GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise IV: Evaluate the performance'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluate the execution time of the accelerated codes in the exercises **II**
    and **III**, and compare it with that of a pure MPI implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[GPU-aware MPI](https://documentation.sigma2.no/code_development/guides/gpuaware_mpi.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MPI documentation](https://www.mpi-forum.org/docs/mpi-4.0/mpi40-report.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenACC specification](https://www.openacc.org/sites/default/files/inline-images/Specification/OpenACC-3.2-final.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenMP specification](https://www.openmp.org/wp-content/uploads/OpenMP-API-Specification-5-2.pdf).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[LUMI documentation](https://docs.lumi-supercomputer.eu/development/compiling/prgenv/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenACC vs OpenMP offloading](https://documentation.sigma2.no/code_development/guides/converting_acc2omp/openacc2openmp.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[OpenACC course](https://github.com/HichamAgueny/GPU-course).*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
