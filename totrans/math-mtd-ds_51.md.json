["```py\ndata = pd.read_csv('twitter-sentiment.csv', encoding='latin-1')\ndata.head() \n```", "```py\nlen(data.index) \n```", "```py\n14640 \n```", "```py\ncorpus = data['text']\nprint(corpus) \n```", "```py\n0                      @VirginAmerica What @dhepburn said.\n1        @VirginAmerica plus you've added commercials t...\n2        @VirginAmerica I didn't today... Must mean I n...\n3        @VirginAmerica it's really aggressive to blast...\n4        @VirginAmerica and it's a really big bad thing...\n                               ...                        \n14635    @AmericanAir thank you we got on a different f...\n14636    @AmericanAir leaving over 20 minutes Late Flig...\n14637    @AmericanAir Please bring American Airlines to...\n14638    @AmericanAir you have my money, you change my ...\n14639    @AmericanAir we have 8 ppl so we need 2 know h...\nName: text, Length: 14640, dtype: object \n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer \n```", "```py\nvectorizer = CountVectorizer()\ncount = vectorizer.fit_transform(corpus)\nprint(count[:2,]) \n```", "```py\n (0, 14376)\t1\n  (0, 14654)\t1\n  (0, 4872)\t1\n  (0, 11739)\t1\n  (1, 14376)\t1\n  (1, 10529)\t1\n  (1, 15047)\t1\n  (1, 14296)\t1\n  (1, 2025)\t1\n  (1, 4095)\t1\n  (1, 13425)\t1\n  (1, 13216)\t1\n  (1, 5733)\t1\n  (1, 13021)\t1 \n```", "```py\nterms = np.array(vectorizer.get_feature_names_out())\nprint(terms) \n```", "```py\n['00' '000' '000114' ... 'ü_ù__' 'üi' 'ýã'] \n```", "```py\nX = (count > 0).astype(int)\nprint(X[:2,]) \n```", "```py\n (0, 4872)\t1\n  (0, 11739)\t1\n  (0, 14376)\t1\n  (0, 14654)\t1\n  (1, 2025)\t1\n  (1, 4095)\t1\n  (1, 5733)\t1\n  (1, 10529)\t1\n  (1, 13021)\t1\n  (1, 13216)\t1\n  (1, 13425)\t1\n  (1, 14296)\t1\n  (1, 14376)\t1\n  (1, 15047)\t1 \n```", "```py\ny = data['sentiment'].to_numpy()\nprint(y) \n```", "```py\n['neutral' 'positive' 'neutral' ... 'neutral' 'negative' 'neutral'] \n```", "```py\nfrom sklearn.model_selection import train_test_split \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=535) \n```", "```py\nlabel_set = ['positive', 'negative', 'neutral']\nN_km = np.zeros((len(label_set), len(terms)))\nN_k = np.zeros(len(label_set))\nfor i, k in enumerate(label_set):\n    k_rows = (y_train == k)\n    N_km[i, :] = np.sum(X_train[k_rows, :] > 0, axis=0)\n    N_k[i] = np.sum(k_rows) \n```", "```py\npi_k, p_km = mmids.nb_fit_table(N_km, N_k)\nprint(pi_k) \n```", "```py\n[0.16071022 0.62849989 0.21078989] \n```", "```py\nprint(p_km) \n```", "```py\n[[0.00047192 0.00330345 0.00047192 ... 0.00094384 0.00094384 0.00047192]\n [0.00144858 0.00229358 0.00012071 ... 0.00012071 0.00012071 0.00024143]\n [0.00071968 0.00143937 0.00071968 ... 0.00035984 0.00035984 0.00071968]] \n```", "```py\nfig, (ax1,ax2,ax3) = plt.subplots(3, 1, sharex=True)\nax1.plot(p_km[0,:], c='k')\nax2.plot(p_km[1,:], c='k')\nax3.plot(p_km[2,:], c='k')\nplt.show() \n```", "```py\nmmids.nb_predict(pi_k, p_km, X_test[4,:].toarray(), label_set) \n```", "```py\n'positive' \n```", "```py\nacc = 0\nfor i in range(len(y_test)):\n    if mmids.nb_predict(pi_k, p_km, X_test[i,:].toarray(), label_set) == y_test[i]:\n        acc += 1\nacc/(len(y_test)) \n```", "```py\n0.7670765027322405 \n```", "```py\npos_terms = (p_km[0,:] > 0.02) & (p_km[1,:] < 0.02)\nprint(terms[pos_terms]) \n```", "```py\n['_ù' 'amazing' 'appreciate' 'awesome' 'best' 'crew' 'first' 'flying'\n 'good' 'great' 'll' 'love' 'made' 'much' 'new' 'response' 'see' 'thank'\n 'thx' 'very' 'well' 'work'] \n```", "```py\nneg_terms = (p_km[1,:] > 0.02) & (p_km[0,:] < 0.02)\nprint(terms[neg_terms]) \n```", "```py\n['about' 'after' 'again' 'agent' 'airport' 'am' 'another' 'any' 'bag'\n 'bags' 'because' 'by' 'call' 'cancelled' 'change' 'check' 'days' 'delay'\n 'delayed' 'did' 'don' 'due' 'even' 'ever' 'flighted' 'flightled' 'go'\n 'going' 'has' 'here' 'hold' 'hour' 'hours' 'how' 'hrs' 'if' 'last' 'late'\n 'lost' 'luggage' 'make' 'minutes' 'more' 'need' 'never' 'off' 'only' 'or'\n 'over' 'people' 'phone' 'really' 'should' 'sitting' 'someone' 'still'\n 'take' 'than' 'them' 'then' 'told' 'trying' 've' 'wait' 'waiting' 'want'\n 'weather' 'what' 'when' 'why' 'worst'] \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\nss = 4\nos = 2\nF = np.array([[1., 0., 1., 0.],\n              [0., 1., 0., 1.],\n              [0., 0., 1., 0.],\n              [0., 0., 0., 1.]]) \nH = np.array([[1., 0., 0., 0.],\n              [0., 1, 0., 0.]])\nQ = 0.01 * np.diag(np.ones(ss))\nR = 10 * np.diag(np.ones(os))\ninit_mu = np.array([0., 0., 1., 1.])\ninit_Sig = Q\nT = 30\nx, y = mmids.lgSamplePath(rng, ss, os, F, H, Q, R, init_mu, init_Sig, T) \n```", "```py\nfor i in range(10,20):\n    y[0,i] = np.nan\n    y[1,i] = np.nan \n```", "```py\nplt.scatter(y[0,:], y[1,:], s=5, c='r', alpha=0.5)\nplt.plot(x[0,:], x[1,:], c='g', linestyle='dotted')\nplt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \nplt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\nplt.show() \n```", "```py\ndef kalmanUpdate(ss, F, H, Q, R, y_t, mu_prev, Sig_prev):\n    mu_pred = F @ mu_prev\n    Sig_pred = F @ Sig_prev @ F.T + Q\n    if np.isnan(y_t[0]) or np.isnan(y_t[1]):\n        return mu_pred, Sig_pred\n    else:\n        e_t = y_t - H @ mu_pred\n        S = H @ Sig_pred @ H.T + R\n        Sinv = LA.inv(S)\n        K = Sig_pred @ H.T @ Sinv\n        mu_new = mu_pred + K @ e_t\n        Sig_new = (np.diag(np.ones(ss)) - K @ H) @ Sig_pred\n        return mu_new, Sig_new \n```", "```py\ninit_mu = np.array([0., 0., 1., 1.])\ninit_Sig = 1 * np.diag(np.ones(ss))\nmu, Sig = mmids.kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T) \n```", "```py\nplt.plot(mu[0,:], mu[1,:], c='b', marker='s', markersize=2, linewidth=1)\nplt.scatter(y[0,:], y[1,:], s=5, c='r', alpha=0.5)\nplt.plot(x[0,:], x[1,:], c='g', linestyle='dotted', alpha=0.5)\nplt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \nplt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\nplt.show() \n```", "```py\nfrom scipy.stats import norm\n\ndef generate_standard_normal_samples_using_inverse_cdf(n, mu, sigma2):\n    # Step 1: Generate uniform [0,1] random variables\n    U = rng.uniform(0, 1, n)\n\n    # Step 2: Apply the inverse CDF (ppf) of the standard normal distribution\n    Z = norm.ppf(U)\n\n    return mu + sigma2 * Z \n```", "```py\n# Generate 1000 standard normal samples\nsamples = generate_standard_normal_samples_using_inverse_cdf(1000, 0 , 1) \n```", "```py\n# Plot the empirical PDF\nplt.figure(figsize=(5, 3))\n\n# Plot histogram of the samples with density=True to normalize the histogram\ncount, bins, ignored = plt.hist(samples, bins=30, density=True, alpha=0.6, color='g', edgecolor='black')\n\n# Plot the theoretical standard normal PDF for comparison\nx = np.linspace(-4, 4, 1000)\nplt.plot(x, norm.pdf(x), 'k', linewidth=2)\n\nplt.title('Empirical PDF of Generated Samples')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.show() \n```", "```py\ndef cholesky(B):\n    n = B.shape[0] \n    L = np.zeros((n, n))\n    for j in range(n):\n        L[j,0:j] = mmids.forwardsubs(L[0:j,0:j],B[j,0:j])\n        L[j,j] = np.sqrt(B[j,j] - LA.norm(L[j,0:j])**2)\n    return L \n```", "```py\nB = np.array([[2., 1.],[1., 2.]])\nprint(B) \n```", "```py\n[[2\\. 1.]\n [1\\. 2.]] \n```", "```py\nL = cholesky(B)\nprint(L) \n```", "```py\n[[1.41421356 0\\.        ]\n [0.70710678 1.22474487]] \n```", "```py\nprint(L @ L.T) \n```", "```py\n[[2\\. 1.]\n [1\\. 2.]] \n```", "```py\ndef generate_multivariate_normal_samples_using_cholesky(n, d, mu, Sig):\n\n    # Compute Cholesky decomposition\n    L = cholesky(Sig)\n\n    # Initialization\n    X = np.zeros((n,d))\n    for i in range(n):\n\n            # Generate standard normal vector\n            Z = generate_standard_normal_samples_using_inverse_cdf(d, 0 , 1)\n\n            # Apply the inverse CDF (ppf) of the standard normal distribution\n            X[i,:] = mu + L @ Z \n\n    return X \n```", "```py\nmu = np.array([-1.,0.,1.])\nSig = np.array([[1., 2., 1.],[2., 8., 0.],[1., 0., 3.]])\nX = generate_multivariate_normal_samples_using_cholesky(10, 3, mu, Sig) \n```", "```py\nprint(X) \n```", "```py\n[[-0.47926185  1.97223283  2.73780609]\n [-2.69005319 -4.19788834 -0.43130768]\n [ 0.41957285  3.91719212  2.08604427]\n [-2.11532949 -5.34557983  0.69521104]\n [-2.41203356 -1.84032486 -0.82207565]\n [-1.46121329  0.4821332   0.55005982]\n [-0.84981594  0.67074839  0.16360931]\n [-2.19097155 -1.98022929 -1.06365711]\n [-2.75113597 -3.47560492 -0.26607926]\n [ 0.130848    6.07312936 -0.08800829]] \n```", "```py\ndef ls_by_chol(A, b):\n    L = cholesky(A.T @ A)\n    z = mmids.forwardsubs(L, A.T @ b)\n    return mmids.backsubs(L.T, z) \n```", "```py\ndata = pd.read_csv('twitter-sentiment.csv', encoding='latin-1')\ndata.head() \n```", "```py\nlen(data.index) \n```", "```py\n14640 \n```", "```py\ncorpus = data['text']\nprint(corpus) \n```", "```py\n0                      @VirginAmerica What @dhepburn said.\n1        @VirginAmerica plus you've added commercials t...\n2        @VirginAmerica I didn't today... Must mean I n...\n3        @VirginAmerica it's really aggressive to blast...\n4        @VirginAmerica and it's a really big bad thing...\n                               ...                        \n14635    @AmericanAir thank you we got on a different f...\n14636    @AmericanAir leaving over 20 minutes Late Flig...\n14637    @AmericanAir Please bring American Airlines to...\n14638    @AmericanAir you have my money, you change my ...\n14639    @AmericanAir we have 8 ppl so we need 2 know h...\nName: text, Length: 14640, dtype: object \n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer \n```", "```py\nvectorizer = CountVectorizer()\ncount = vectorizer.fit_transform(corpus)\nprint(count[:2,]) \n```", "```py\n (0, 14376)\t1\n  (0, 14654)\t1\n  (0, 4872)\t1\n  (0, 11739)\t1\n  (1, 14376)\t1\n  (1, 10529)\t1\n  (1, 15047)\t1\n  (1, 14296)\t1\n  (1, 2025)\t1\n  (1, 4095)\t1\n  (1, 13425)\t1\n  (1, 13216)\t1\n  (1, 5733)\t1\n  (1, 13021)\t1 \n```", "```py\nterms = np.array(vectorizer.get_feature_names_out())\nprint(terms) \n```", "```py\n['00' '000' '000114' ... 'ü_ù__' 'üi' 'ýã'] \n```", "```py\nX = (count > 0).astype(int)\nprint(X[:2,]) \n```", "```py\n (0, 4872)\t1\n  (0, 11739)\t1\n  (0, 14376)\t1\n  (0, 14654)\t1\n  (1, 2025)\t1\n  (1, 4095)\t1\n  (1, 5733)\t1\n  (1, 10529)\t1\n  (1, 13021)\t1\n  (1, 13216)\t1\n  (1, 13425)\t1\n  (1, 14296)\t1\n  (1, 14376)\t1\n  (1, 15047)\t1 \n```", "```py\ny = data['sentiment'].to_numpy()\nprint(y) \n```", "```py\n['neutral' 'positive' 'neutral' ... 'neutral' 'negative' 'neutral'] \n```", "```py\nfrom sklearn.model_selection import train_test_split \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=535) \n```", "```py\nlabel_set = ['positive', 'negative', 'neutral']\nN_km = np.zeros((len(label_set), len(terms)))\nN_k = np.zeros(len(label_set))\nfor i, k in enumerate(label_set):\n    k_rows = (y_train == k)\n    N_km[i, :] = np.sum(X_train[k_rows, :] > 0, axis=0)\n    N_k[i] = np.sum(k_rows) \n```", "```py\npi_k, p_km = mmids.nb_fit_table(N_km, N_k)\nprint(pi_k) \n```", "```py\n[0.16071022 0.62849989 0.21078989] \n```", "```py\nprint(p_km) \n```", "```py\n[[0.00047192 0.00330345 0.00047192 ... 0.00094384 0.00094384 0.00047192]\n [0.00144858 0.00229358 0.00012071 ... 0.00012071 0.00012071 0.00024143]\n [0.00071968 0.00143937 0.00071968 ... 0.00035984 0.00035984 0.00071968]] \n```", "```py\nfig, (ax1,ax2,ax3) = plt.subplots(3, 1, sharex=True)\nax1.plot(p_km[0,:], c='k')\nax2.plot(p_km[1,:], c='k')\nax3.plot(p_km[2,:], c='k')\nplt.show() \n```", "```py\nmmids.nb_predict(pi_k, p_km, X_test[4,:].toarray(), label_set) \n```", "```py\n'positive' \n```", "```py\nacc = 0\nfor i in range(len(y_test)):\n    if mmids.nb_predict(pi_k, p_km, X_test[i,:].toarray(), label_set) == y_test[i]:\n        acc += 1\nacc/(len(y_test)) \n```", "```py\n0.7670765027322405 \n```", "```py\npos_terms = (p_km[0,:] > 0.02) & (p_km[1,:] < 0.02)\nprint(terms[pos_terms]) \n```", "```py\n['_ù' 'amazing' 'appreciate' 'awesome' 'best' 'crew' 'first' 'flying'\n 'good' 'great' 'll' 'love' 'made' 'much' 'new' 'response' 'see' 'thank'\n 'thx' 'very' 'well' 'work'] \n```", "```py\nneg_terms = (p_km[1,:] > 0.02) & (p_km[0,:] < 0.02)\nprint(terms[neg_terms]) \n```", "```py\n['about' 'after' 'again' 'agent' 'airport' 'am' 'another' 'any' 'bag'\n 'bags' 'because' 'by' 'call' 'cancelled' 'change' 'check' 'days' 'delay'\n 'delayed' 'did' 'don' 'due' 'even' 'ever' 'flighted' 'flightled' 'go'\n 'going' 'has' 'here' 'hold' 'hour' 'hours' 'how' 'hrs' 'if' 'last' 'late'\n 'lost' 'luggage' 'make' 'minutes' 'more' 'need' 'never' 'off' 'only' 'or'\n 'over' 'people' 'phone' 'really' 'should' 'sitting' 'someone' 'still'\n 'take' 'than' 'them' 'then' 'told' 'trying' 've' 'wait' 'waiting' 'want'\n 'weather' 'what' 'when' 'why' 'worst'] \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\nss = 4\nos = 2\nF = np.array([[1., 0., 1., 0.],\n              [0., 1., 0., 1.],\n              [0., 0., 1., 0.],\n              [0., 0., 0., 1.]]) \nH = np.array([[1., 0., 0., 0.],\n              [0., 1, 0., 0.]])\nQ = 0.01 * np.diag(np.ones(ss))\nR = 10 * np.diag(np.ones(os))\ninit_mu = np.array([0., 0., 1., 1.])\ninit_Sig = Q\nT = 30\nx, y = mmids.lgSamplePath(rng, ss, os, F, H, Q, R, init_mu, init_Sig, T) \n```", "```py\nfor i in range(10,20):\n    y[0,i] = np.nan\n    y[1,i] = np.nan \n```", "```py\nplt.scatter(y[0,:], y[1,:], s=5, c='r', alpha=0.5)\nplt.plot(x[0,:], x[1,:], c='g', linestyle='dotted')\nplt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \nplt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\nplt.show() \n```", "```py\ndef kalmanUpdate(ss, F, H, Q, R, y_t, mu_prev, Sig_prev):\n    mu_pred = F @ mu_prev\n    Sig_pred = F @ Sig_prev @ F.T + Q\n    if np.isnan(y_t[0]) or np.isnan(y_t[1]):\n        return mu_pred, Sig_pred\n    else:\n        e_t = y_t - H @ mu_pred\n        S = H @ Sig_pred @ H.T + R\n        Sinv = LA.inv(S)\n        K = Sig_pred @ H.T @ Sinv\n        mu_new = mu_pred + K @ e_t\n        Sig_new = (np.diag(np.ones(ss)) - K @ H) @ Sig_pred\n        return mu_new, Sig_new \n```", "```py\ninit_mu = np.array([0., 0., 1., 1.])\ninit_Sig = 1 * np.diag(np.ones(ss))\nmu, Sig = mmids.kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T) \n```", "```py\nplt.plot(mu[0,:], mu[1,:], c='b', marker='s', markersize=2, linewidth=1)\nplt.scatter(y[0,:], y[1,:], s=5, c='r', alpha=0.5)\nplt.plot(x[0,:], x[1,:], c='g', linestyle='dotted', alpha=0.5)\nplt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \nplt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\nplt.show() \n```", "```py\nfrom scipy.stats import norm\n\ndef generate_standard_normal_samples_using_inverse_cdf(n, mu, sigma2):\n    # Step 1: Generate uniform [0,1] random variables\n    U = rng.uniform(0, 1, n)\n\n    # Step 2: Apply the inverse CDF (ppf) of the standard normal distribution\n    Z = norm.ppf(U)\n\n    return mu + sigma2 * Z \n```", "```py\n# Generate 1000 standard normal samples\nsamples = generate_standard_normal_samples_using_inverse_cdf(1000, 0 , 1) \n```", "```py\n# Plot the empirical PDF\nplt.figure(figsize=(5, 3))\n\n# Plot histogram of the samples with density=True to normalize the histogram\ncount, bins, ignored = plt.hist(samples, bins=30, density=True, alpha=0.6, color='g', edgecolor='black')\n\n# Plot the theoretical standard normal PDF for comparison\nx = np.linspace(-4, 4, 1000)\nplt.plot(x, norm.pdf(x), 'k', linewidth=2)\n\nplt.title('Empirical PDF of Generated Samples')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.show() \n```", "```py\ndef cholesky(B):\n    n = B.shape[0] \n    L = np.zeros((n, n))\n    for j in range(n):\n        L[j,0:j] = mmids.forwardsubs(L[0:j,0:j],B[j,0:j])\n        L[j,j] = np.sqrt(B[j,j] - LA.norm(L[j,0:j])**2)\n    return L \n```", "```py\nB = np.array([[2., 1.],[1., 2.]])\nprint(B) \n```", "```py\n[[2\\. 1.]\n [1\\. 2.]] \n```", "```py\nL = cholesky(B)\nprint(L) \n```", "```py\n[[1.41421356 0\\.        ]\n [0.70710678 1.22474487]] \n```", "```py\nprint(L @ L.T) \n```", "```py\n[[2\\. 1.]\n [1\\. 2.]] \n```", "```py\ndef generate_multivariate_normal_samples_using_cholesky(n, d, mu, Sig):\n\n    # Compute Cholesky decomposition\n    L = cholesky(Sig)\n\n    # Initialization\n    X = np.zeros((n,d))\n    for i in range(n):\n\n            # Generate standard normal vector\n            Z = generate_standard_normal_samples_using_inverse_cdf(d, 0 , 1)\n\n            # Apply the inverse CDF (ppf) of the standard normal distribution\n            X[i,:] = mu + L @ Z \n\n    return X \n```", "```py\nmu = np.array([-1.,0.,1.])\nSig = np.array([[1., 2., 1.],[2., 8., 0.],[1., 0., 3.]])\nX = generate_multivariate_normal_samples_using_cholesky(10, 3, mu, Sig) \n```", "```py\nprint(X) \n```", "```py\n[[-0.47926185  1.97223283  2.73780609]\n [-2.69005319 -4.19788834 -0.43130768]\n [ 0.41957285  3.91719212  2.08604427]\n [-2.11532949 -5.34557983  0.69521104]\n [-2.41203356 -1.84032486 -0.82207565]\n [-1.46121329  0.4821332   0.55005982]\n [-0.84981594  0.67074839  0.16360931]\n [-2.19097155 -1.98022929 -1.06365711]\n [-2.75113597 -3.47560492 -0.26607926]\n [ 0.130848    6.07312936 -0.08800829]] \n```", "```py\ndef ls_by_chol(A, b):\n    L = cholesky(A.T @ A)\n    z = mmids.forwardsubs(L, A.T @ b)\n    return mmids.backsubs(L.T, z) \n```", "```py\ndata = pd.read_csv('twitter-sentiment.csv', encoding='latin-1')\ndata.head() \n```", "```py\nlen(data.index) \n```", "```py\n14640 \n```", "```py\ncorpus = data['text']\nprint(corpus) \n```", "```py\n0                      @VirginAmerica What @dhepburn said.\n1        @VirginAmerica plus you've added commercials t...\n2        @VirginAmerica I didn't today... Must mean I n...\n3        @VirginAmerica it's really aggressive to blast...\n4        @VirginAmerica and it's a really big bad thing...\n                               ...                        \n14635    @AmericanAir thank you we got on a different f...\n14636    @AmericanAir leaving over 20 minutes Late Flig...\n14637    @AmericanAir Please bring American Airlines to...\n14638    @AmericanAir you have my money, you change my ...\n14639    @AmericanAir we have 8 ppl so we need 2 know h...\nName: text, Length: 14640, dtype: object \n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer \n```", "```py\nvectorizer = CountVectorizer()\ncount = vectorizer.fit_transform(corpus)\nprint(count[:2,]) \n```", "```py\n (0, 14376)\t1\n  (0, 14654)\t1\n  (0, 4872)\t1\n  (0, 11739)\t1\n  (1, 14376)\t1\n  (1, 10529)\t1\n  (1, 15047)\t1\n  (1, 14296)\t1\n  (1, 2025)\t1\n  (1, 4095)\t1\n  (1, 13425)\t1\n  (1, 13216)\t1\n  (1, 5733)\t1\n  (1, 13021)\t1 \n```", "```py\nterms = np.array(vectorizer.get_feature_names_out())\nprint(terms) \n```", "```py\n['00' '000' '000114' ... 'ü_ù__' 'üi' 'ýã'] \n```", "```py\nX = (count > 0).astype(int)\nprint(X[:2,]) \n```", "```py\n (0, 4872)\t1\n  (0, 11739)\t1\n  (0, 14376)\t1\n  (0, 14654)\t1\n  (1, 2025)\t1\n  (1, 4095)\t1\n  (1, 5733)\t1\n  (1, 10529)\t1\n  (1, 13021)\t1\n  (1, 13216)\t1\n  (1, 13425)\t1\n  (1, 14296)\t1\n  (1, 14376)\t1\n  (1, 15047)\t1 \n```", "```py\ny = data['sentiment'].to_numpy()\nprint(y) \n```", "```py\n['neutral' 'positive' 'neutral' ... 'neutral' 'negative' 'neutral'] \n```", "```py\nfrom sklearn.model_selection import train_test_split \n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=535) \n```", "```py\nlabel_set = ['positive', 'negative', 'neutral']\nN_km = np.zeros((len(label_set), len(terms)))\nN_k = np.zeros(len(label_set))\nfor i, k in enumerate(label_set):\n    k_rows = (y_train == k)\n    N_km[i, :] = np.sum(X_train[k_rows, :] > 0, axis=0)\n    N_k[i] = np.sum(k_rows) \n```", "```py\npi_k, p_km = mmids.nb_fit_table(N_km, N_k)\nprint(pi_k) \n```", "```py\n[0.16071022 0.62849989 0.21078989] \n```", "```py\nprint(p_km) \n```", "```py\n[[0.00047192 0.00330345 0.00047192 ... 0.00094384 0.00094384 0.00047192]\n [0.00144858 0.00229358 0.00012071 ... 0.00012071 0.00012071 0.00024143]\n [0.00071968 0.00143937 0.00071968 ... 0.00035984 0.00035984 0.00071968]] \n```", "```py\nfig, (ax1,ax2,ax3) = plt.subplots(3, 1, sharex=True)\nax1.plot(p_km[0,:], c='k')\nax2.plot(p_km[1,:], c='k')\nax3.plot(p_km[2,:], c='k')\nplt.show() \n```", "```py\nmmids.nb_predict(pi_k, p_km, X_test[4,:].toarray(), label_set) \n```", "```py\n'positive' \n```", "```py\nacc = 0\nfor i in range(len(y_test)):\n    if mmids.nb_predict(pi_k, p_km, X_test[i,:].toarray(), label_set) == y_test[i]:\n        acc += 1\nacc/(len(y_test)) \n```", "```py\n0.7670765027322405 \n```", "```py\npos_terms = (p_km[0,:] > 0.02) & (p_km[1,:] < 0.02)\nprint(terms[pos_terms]) \n```", "```py\n['_ù' 'amazing' 'appreciate' 'awesome' 'best' 'crew' 'first' 'flying'\n 'good' 'great' 'll' 'love' 'made' 'much' 'new' 'response' 'see' 'thank'\n 'thx' 'very' 'well' 'work'] \n```", "```py\nneg_terms = (p_km[1,:] > 0.02) & (p_km[0,:] < 0.02)\nprint(terms[neg_terms]) \n```", "```py\n['about' 'after' 'again' 'agent' 'airport' 'am' 'another' 'any' 'bag'\n 'bags' 'because' 'by' 'call' 'cancelled' 'change' 'check' 'days' 'delay'\n 'delayed' 'did' 'don' 'due' 'even' 'ever' 'flighted' 'flightled' 'go'\n 'going' 'has' 'here' 'hold' 'hour' 'hours' 'how' 'hrs' 'if' 'last' 'late'\n 'lost' 'luggage' 'make' 'minutes' 'more' 'need' 'never' 'off' 'only' 'or'\n 'over' 'people' 'phone' 'really' 'should' 'sitting' 'someone' 'still'\n 'take' 'than' 'them' 'then' 'told' 'trying' 've' 'wait' 'waiting' 'want'\n 'weather' 'what' 'when' 'why' 'worst'] \n```", "```py\nseed = 535\nrng = np.random.default_rng(seed)\nss = 4\nos = 2\nF = np.array([[1., 0., 1., 0.],\n              [0., 1., 0., 1.],\n              [0., 0., 1., 0.],\n              [0., 0., 0., 1.]]) \nH = np.array([[1., 0., 0., 0.],\n              [0., 1, 0., 0.]])\nQ = 0.01 * np.diag(np.ones(ss))\nR = 10 * np.diag(np.ones(os))\ninit_mu = np.array([0., 0., 1., 1.])\ninit_Sig = Q\nT = 30\nx, y = mmids.lgSamplePath(rng, ss, os, F, H, Q, R, init_mu, init_Sig, T) \n```", "```py\nfor i in range(10,20):\n    y[0,i] = np.nan\n    y[1,i] = np.nan \n```", "```py\nplt.scatter(y[0,:], y[1,:], s=5, c='r', alpha=0.5)\nplt.plot(x[0,:], x[1,:], c='g', linestyle='dotted')\nplt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \nplt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\nplt.show() \n```", "```py\ndef kalmanUpdate(ss, F, H, Q, R, y_t, mu_prev, Sig_prev):\n    mu_pred = F @ mu_prev\n    Sig_pred = F @ Sig_prev @ F.T + Q\n    if np.isnan(y_t[0]) or np.isnan(y_t[1]):\n        return mu_pred, Sig_pred\n    else:\n        e_t = y_t - H @ mu_pred\n        S = H @ Sig_pred @ H.T + R\n        Sinv = LA.inv(S)\n        K = Sig_pred @ H.T @ Sinv\n        mu_new = mu_pred + K @ e_t\n        Sig_new = (np.diag(np.ones(ss)) - K @ H) @ Sig_pred\n        return mu_new, Sig_new \n```", "```py\ninit_mu = np.array([0., 0., 1., 1.])\ninit_Sig = 1 * np.diag(np.ones(ss))\nmu, Sig = mmids.kalmanFilter(ss, os, y, F, H, Q, R, init_mu, init_Sig, T) \n```", "```py\nplt.plot(mu[0,:], mu[1,:], c='b', marker='s', markersize=2, linewidth=1)\nplt.scatter(y[0,:], y[1,:], s=5, c='r', alpha=0.5)\nplt.plot(x[0,:], x[1,:], c='g', linestyle='dotted', alpha=0.5)\nplt.xlim((np.min(x[0,:])-5, np.max(x[0,:])+5)) \nplt.ylim((np.min(x[1,:])-5, np.max(x[1,:])+5))\nplt.show() \n```", "```py\nfrom scipy.stats import norm\n\ndef generate_standard_normal_samples_using_inverse_cdf(n, mu, sigma2):\n    # Step 1: Generate uniform [0,1] random variables\n    U = rng.uniform(0, 1, n)\n\n    # Step 2: Apply the inverse CDF (ppf) of the standard normal distribution\n    Z = norm.ppf(U)\n\n    return mu + sigma2 * Z \n```", "```py\n# Generate 1000 standard normal samples\nsamples = generate_standard_normal_samples_using_inverse_cdf(1000, 0 , 1) \n```", "```py\n# Plot the empirical PDF\nplt.figure(figsize=(5, 3))\n\n# Plot histogram of the samples with density=True to normalize the histogram\ncount, bins, ignored = plt.hist(samples, bins=30, density=True, alpha=0.6, color='g', edgecolor='black')\n\n# Plot the theoretical standard normal PDF for comparison\nx = np.linspace(-4, 4, 1000)\nplt.plot(x, norm.pdf(x), 'k', linewidth=2)\n\nplt.title('Empirical PDF of Generated Samples')\nplt.xlabel('Value')\nplt.ylabel('Density')\nplt.show() \n```", "```py\ndef cholesky(B):\n    n = B.shape[0] \n    L = np.zeros((n, n))\n    for j in range(n):\n        L[j,0:j] = mmids.forwardsubs(L[0:j,0:j],B[j,0:j])\n        L[j,j] = np.sqrt(B[j,j] - LA.norm(L[j,0:j])**2)\n    return L \n```", "```py\nB = np.array([[2., 1.],[1., 2.]])\nprint(B) \n```", "```py\n[[2\\. 1.]\n [1\\. 2.]] \n```", "```py\nL = cholesky(B)\nprint(L) \n```", "```py\n[[1.41421356 0\\.        ]\n [0.70710678 1.22474487]] \n```", "```py\nprint(L @ L.T) \n```", "```py\n[[2\\. 1.]\n [1\\. 2.]] \n```", "```py\ndef generate_multivariate_normal_samples_using_cholesky(n, d, mu, Sig):\n\n    # Compute Cholesky decomposition\n    L = cholesky(Sig)\n\n    # Initialization\n    X = np.zeros((n,d))\n    for i in range(n):\n\n            # Generate standard normal vector\n            Z = generate_standard_normal_samples_using_inverse_cdf(d, 0 , 1)\n\n            # Apply the inverse CDF (ppf) of the standard normal distribution\n            X[i,:] = mu + L @ Z \n\n    return X \n```", "```py\nmu = np.array([-1.,0.,1.])\nSig = np.array([[1., 2., 1.],[2., 8., 0.],[1., 0., 3.]])\nX = generate_multivariate_normal_samples_using_cholesky(10, 3, mu, Sig) \n```", "```py\nprint(X) \n```", "```py\n[[-0.47926185  1.97223283  2.73780609]\n [-2.69005319 -4.19788834 -0.43130768]\n [ 0.41957285  3.91719212  2.08604427]\n [-2.11532949 -5.34557983  0.69521104]\n [-2.41203356 -1.84032486 -0.82207565]\n [-1.46121329  0.4821332   0.55005982]\n [-0.84981594  0.67074839  0.16360931]\n [-2.19097155 -1.98022929 -1.06365711]\n [-2.75113597 -3.47560492 -0.26607926]\n [ 0.130848    6.07312936 -0.08800829]] \n```", "```py\ndef ls_by_chol(A, b):\n    L = cholesky(A.T @ A)\n    z = mmids.forwardsubs(L, A.T @ b)\n    return mmids.backsubs(L.T, z) \n```"]