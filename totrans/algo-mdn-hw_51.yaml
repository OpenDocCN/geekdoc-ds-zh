- en: Spatial and Temporal Locality
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 空间和时间局部性
- en: 原文：[https://en.algorithmica.org/hpc/external-memory/locality/](https://en.algorithmica.org/hpc/external-memory/locality/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '[原文链接](https://en.algorithmica.org/hpc/external-memory/locality/)'
- en: 'To precisely assess the performance of an algorithm in terms of its memory
    operations, we need to take into account multiple characteristics of the cache
    system: the number of cache layers, the [memory and block sizes](../hierarchy)
    of each layer, the exact [strategy](../policies) used for cache eviction by each
    layer, and sometimes even the details of the [memory paging](../virtual) mechanism.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 要精确评估算法在内存操作方面的性能，我们需要考虑缓存系统的多个特性：缓存层数量、每层的[内存和块大小](../hierarchy)、每层用于缓存淘汰的确切[策略](../policies)，有时甚至包括[内存分页](../virtual)机制的细节。
- en: Abstracting away from all these minor details helps a lot in the first stages
    of designing algorithms. Instead of calculating theoretical cache hit rates, it
    often makes more sense to reason about cache performance in more qualitative terms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象掉所有这些细节对于算法设计的初期阶段非常有帮助。与其计算理论上的缓存命中率，不如从更定性的角度来推理缓存性能。
- en: 'In this context, we can talk about the degree of cache reuse primarily in two
    ways:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，我们可以主要从两个方面来讨论缓存复用的程度：
- en: '*Temporal locality* refers to the repeated access of the same data within a
    relatively small time period, such that the data likely remains cached between
    the requests.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间局部性*指的是在相对较短的时间内重复访问相同的数据，这样数据很可能在请求之间仍然保留在缓存中。'
- en: '*Spatial locality* refers to the use of elements relatively close to each other
    in terms of their memory locations, such that they are likely fetched in the same
    memory block.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*空间局部性*指的是使用在内存位置上相对接近的元素，这样它们很可能会被一起从同一内存块中加载。'
- en: In other words, temporal locality is when it is likely that this same memory
    location will soon be requested again, while spatial locality is when it is likely
    that a nearby location will be requested right after.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，时间局部性是指这个相同的内存位置很快会被再次请求，而空间局部性是指紧随其后的很可能请求的是附近的位置。
- en: In this section, we will do some case studies to show how these high-level concepts
    can help in practical optimization.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进行一些案例研究，以展示这些高级概念如何有助于实际优化。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/locality/#depth-first-vs-breadth-first)Depth-First
    vs. Breadth-First'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '### [深度优先与广度优先](https://en.algorithmica.org/hpc/external-memory/locality/#depth-first-vs-breadth-first)'
- en: 'Consider a divide-and-conquer algorithm such as merge sorting. There are two
    approaches to implementing it:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一种分而治之的算法，比如归并排序。实现它的方法有两种：
- en: 'We can implement it recursively, or “depth-first,” the way it is normally implemented:
    sort the left half, sort the right half and then merge the results.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以递归地实现它，或者说是“深度优先”，这是它通常的实现方式：先对左半部分进行排序，然后对右半部分进行排序，最后合并结果。
- en: We can implement it iteratively, or “breadth-first:” do the lowest “layer” first,
    looping through the entire dataset and comparing odd elements with even elements,
    then merge the first two elements with the second two elements, the third two
    elements with the fourth two elements and so on.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以迭代地实现它，或者说是“广度优先”：首先处理最低的“层”，遍历整个数据集，比较奇数元素和偶数元素，然后将前两个元素与后两个元素合并，第三个元素与第四个元素合并，依此类推。
- en: It seems like the second approach is more cumbersome, but faster — because recursion
    is always slow, right?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法似乎更繁琐，但更快——因为递归总是很慢，对吧？
- en: 'Generally, recursion is [indeed slow](/hpc/architecture/functions), but this
    is not the case for this and many similar divide-and-conquer algorithms. Although
    the iterative approach has the advantage of only doing sequential I/O, the recursive
    approach has much better temporal locality: when a segment fully fits into the
    cache, it stays there for all lower layers of recursion, resulting in better access
    times later on.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，递归确实很慢（/hpc/architecture/functions），但这种情况并不适用于这个和许多类似的分而治之算法。虽然迭代方法具有仅进行顺序I/O的优势，但递归方法具有更好的时间局部性：当一个段完全适合缓存时，它将保留在所有递归的较低层，从而在稍后获得更好的访问时间。
- en: In fact, since we only need to split the array $O(\log \frac{N}{M})$ times until
    this happens, we would only need to read $O(\frac{N}{B} \log \frac{N}{M})$ blocks
    in total, while in the iterative approach the entire array will be read from scratch
    $O(\log N)$ times no matter what. This results in the speedup of $O(\frac{\log
    N}{\log N - \log M})$, which may be up to an order of magnitude.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，由于我们只需要将数组分割$O(\log \frac{N}{M})$次，我们总共只需要读取$O(\frac{N}{B} \log \frac{N}{M})$个块，而在迭代方法中，无论什么情况，整个数组都需要从头开始读取$O(\log
    N)$次。这导致速度提升为$O(\frac{\log N}{\log N - \log M})$，可能高达一个数量级。
- en: In practice, there is still some overhead associated with the recursion, and
    for this reason, it makes sense to use hybrid algorithms where we don’t go all
    the way down to the base case and instead switch to the iterative code on the
    lower levels of recursion.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，递归仍然有一些开销，因此使用混合算法是有意义的，在这种算法中，我们不会一直递归到基本情况，而是在递归的较低级别切换到迭代代码。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/locality/#dynamic-programming)Dynamic
    Programming'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/external-memory/locality/#dynamic-programming)动态规划'
- en: Similar reasoning can be applied to the implementations of dynamic programming
    algorithms but leading to the reverse result. Consider the classic *knapsack problem:*
    given $N$ items with positive integer costs $c_i$, pick a subset of items with
    the maximum total cost that does not exceed a given constant $W$.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的推理可以应用于动态规划算法的实现，但会导致相反的结果。考虑经典的*背包问题*：给定$N$个具有正整数成本$c_i$的物品，选择一个物品子集，其总成本最大，不超过给定的常数$W$。
- en: The way to solve it is to introduce the *state* $f[n, w]$, which corresponds
    to the maximum total cost not exceeding $w$ that can be achieved using only the
    first $n$ items. These values can be computed in $O(1)$ time per entry if we consider
    either taking or not taking the $n$-th item and using the previous states of the
    dynamic to make the optimal decision.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 解决它的方法是引入*状态* $f[n, w]$，它对应于使用前$n$个物品所能达到的最大总成本，不超过$w$。如果考虑是否取第$n$个物品以及使用动态的先前状态来做出最优决策，这些值可以在$O(1)$时间内计算每个条目。
- en: 'Python has a handy `lru_cache` decorator which can be used for implementing
    it with memoized recursion:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Python有一个方便的`lru_cache`装饰器，可以用于实现带有记忆化递归的它：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When computing $f[N, W]$, the recursion may visit up to $O(N \cdot W)$ different
    states, which is asymptotically efficient, but rather slow in reality. Even after
    nullifying the overhead of Python recursion and all the [hash table queries](../policies/#implementing-caching)
    required for the LRU cache to work, it would still be slow because it does random
    I/O throughout most of the execution.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算$f[N, W]$时，递归可能访问多达$O(N \cdot W)$个不同的状态，这在渐近上是有效的，但在现实中相当慢。即使在消除了Python递归的开销以及LRU缓存工作所需的全部[哈希表查询](../policies/#implementing-caching)之后，它仍然会很慢，因为它在执行的大部分过程中都会进行随机的I/O操作。
- en: 'What we can do instead is to create a two-dimensional array for the dynamic
    and replace the recursion with a nice nested loop like this:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的另一件事是为动态创建一个二维数组，并用一个漂亮的嵌套循环替换递归，如下所示：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that we are only using the previous layer of the dynamic to calculate
    the next one. This means that if we can store one layer in the cache, we would
    only need to write $O(\frac{N \cdot W}{B})$ blocks in external memory.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们只使用前一层动态来计算下一层。这意味着如果我们能在缓存中存储一层，我们只需要在外部内存中写入$O(\frac{N \cdot W}{B})$个块。
- en: 'Moreover, if we only need the answer, we don’t actually have to store the whole
    2d array but only the last layer. This lets us use just $O(W)$ memory by maintaining
    a single array of $W$ values. To simplify the code, we can slightly change the
    dynamic to store a binary value: whether it is possible to get the sum of exactly
    $w$ using the items that we have already considered. This dynamic is even faster
    to compute:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们只需要答案，实际上我们不需要存储整个二维数组，只需要存储最后一层。这使得我们只需通过维护一个包含$W$个值的单个数组来使用$O(W)$的内存。为了简化代码，我们可以稍微改变动态，以存储一个二进制值：是否可以使用我们已考虑的物品得到恰好$w$的和。这个动态计算得更快：
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As a side note, now that it only uses simple bitwise operations, it can be
    optimized further by using a bitset:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作为旁注，现在它只使用简单的位操作，可以通过使用bitset进一步优化：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Surprisingly, there is still some room for improvement, and we will come back
    to this problem later.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，仍有改进的空间，我们稍后会回到这个问题。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/locality/#sparse-table)Sparse
    Table'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/external-memory/locality/#sparse-table)稀疏表'
- en: '*Sparse table* is a *static* data structure that is often used for solving
    the *static RMQ* problem and computing any similar *idempotent range reductions*
    in general. It can be formally defined as a two-dimensional array of size $\log
    n \times n$:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*稀疏表*是一种*静态*数据结构，常用于解决*静态RMQ*问题以及计算任何类似的*幂等范围缩减*。它可以正式定义为大小为$\log n \times
    n$的二维数组：'
- en: $$ t[k][i] = \min \{ a_i, a_{i+1}, \ldots, a_{i+2^k-1} \} $$
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: $$ t[k][i] = \min \{ a_i, a_{i+1}, \ldots, a_{i+2^k-1} \} $$
- en: 'In plain English: we store the minimum on each segment whose length is a power
    of two.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 用简单的话说：我们在每个长度为2的幂的段上存储最小值。
- en: Such array can be used for calculating minima on arbitrary segments in constant
    time because for each segment we can always find two possibly overlapping segments
    whose sizes are the same power of two, the union of which gives the whole segment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的数组可以用于在常数时间内计算任意段的最小值，因为对于每个段，我们总能找到两个可能重叠的段，它们的尺寸是相同的2的幂，它们的并集给出了整个段。
- en: '![](../Images/5079b4cf64174e850156b3c1eb56af58.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/5079b4cf64174e850156b3c1eb56af58.png)'
- en: 'This means that we can just take the minimum of these two precomputed minimums
    as the answer:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以直接取这两个预计算最小值中的最小值作为答案：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `__lg` function is an intrinsic available in GCC that calculates the binary
    logarithm of a number rounded down. Internally it uses the `clz` (“count leading
    zeros”) instruction and subtracts this count from 32 (in case of a 32-bit integer),
    and thus takes just a few cycles.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`__lg`函数是GCC中可用的一个内建函数，用于计算一个数的二进制对数并向下取整。内部它使用`clz`（“计算最高位零”）指令，并从32（针对32位整数）中减去这个计数，因此只需要几个周期。'
- en: 'The reason why I bring it up in this article is that there are multiple alternative
    ways it can be built, with different efficiencies in terms of memory operations.
    In general, a sparse table can be built in $O(n \log n)$ time in dynamic programming
    fashion by iterating in the order of increasing $i$ or $k$ and applying the following
    identity:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我之所以在这篇文章中提到它，是因为它有多个替代构建方法，在内存操作效率方面有不同的表现。一般来说，稀疏表可以通过动态规划的方式在$O(n \log n)$时间内构建，通过按增加的$i$或$k$的顺序迭代，并应用以下恒等式：
- en: $$ t[k][i] = \min(t[k-1][i], t[k-1][i+2^{k-1}]) $$
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: $$ t[k][i] = \min(t[k-1][i], t[k-1][i+2^{k-1}]) $$
- en: 'Now, there are two design choices to make: whether the log-size $k$ should
    be the first or the second dimension, and whether to iterate over $k$ and then
    $i$ or the other way around. This means that there are $2×2=4$ ways to build it,
    and here is the optimal one:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有两个设计选择要做：是否将对数大小的$k$作为第一维或第二维，以及是否先迭代$k$然后迭代$i$或相反。这意味着有$2×2=4$种构建它的方法，这里是最优的一种：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is the only combination of the memory layout and the iteration order that
    results in beautiful linear passes that work ~3x faster. As an exercise, consider
    the other three variants and think about *why* they are slower.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是唯一一种内存布局和迭代顺序的组合，它会产生漂亮的线性遍历，速度大约快3倍。作为一个练习，考虑其他三种变体，并思考一下*为什么*它们会慢。
- en: '### [#](https://en.algorithmica.org/hpc/external-memory/locality/#array-of-structs-vs-struct-of-arrays)Array-of-Structs
    vs. Struct-of-Arrays'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/external-memory/locality/#array-of-structs-vs-struct-of-arrays)数组结构
    vs. 结构数组'
- en: 'Suppose that you want to implement a binary tree and store its fields in separate
    arrays like this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想实现一个二叉树，并将它的字段存储在像这样的单独数组中：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Such memory layout, when we store each field separately from others, is called
    *struct-of-arrays* (SoA). In most cases, when implementing tree operations, you
    access a node and then shortly after all or most of its internal data. If these
    fields are stored separately, this would mean that they are also located in different
    memory blocks. If some of the requested fields happen to be are cached while the
    others are not, you would still have to wait for the slowest of them to be fetched.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将每个字段与其他字段分开存储时，这种内存布局被称为*数组结构*（SoA）。在大多数情况下，当实现树操作时，你会访问一个节点，然后很快就会访问其所有或大部分内部数据。如果这些字段是分开存储的，这意味着它们也位于不同的内存块中。如果请求的一些字段被缓存了，而其他字段没有被缓存，你仍然需要等待最慢的那个字段被读取。
- en: 'In contrast, if it was instead stored as an array-of-structs (AoS), you would
    need ~4 times fewer block reads as all the data of a node is stored in the same
    block and fetched at once:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果它被存储为结构数组（AoS），你将需要大约少4倍的块读取，因为一个节点的所有数据都存储在同一个块中，并且一次性读取：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The AoS layout is usually preferred for data structures, but SoA still has
    good uses: while it is worse for searching, it is much better for linear scanning.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: AoS布局通常被用于数据结构，但SoA仍然有很好的用途：虽然它在搜索方面表现较差，但在线性扫描方面则要好得多。
- en: 'This difference in design is important in data processing applications. For
    example, databases can be either *row-* or *column-oriented* (also called *columnar*):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种设计上的差异在数据处理应用中非常重要。例如，数据库可以是*行导向*或*列导向*（也称为*列格式*）：
- en: '*Row-oriented* storage formats are used when you need to search for a limited
    number of objects in a large dataset and/or fetch all or most of their fields.
    Examples: PostgreSQL, MongoDB.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你需要在大数据集中搜索有限数量的对象，并且/或者获取它们的所有或大部分字段时，会使用*行导向*的存储格式。例如：PostgreSQL, MongoDB。
- en: '*Columnar* storage formats are used for big data processing and analytics,
    where you need to scan through everything anyway to calculate certain statistics.
    Examples: ClickHouse, Hbase.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*列导向*的存储格式用于大数据处理和分析，在这些情况下，你需要扫描所有内容来计算某些统计信息。例如：ClickHouse, Hbase。'
- en: Columnar formats have the additional advantage that you can only read the fields
    that you need, as different fields are stored in separate external memory regions.
    [← Cache-Oblivious Algorithms](https://en.algorithmica.org/hpc/external-memory/oblivious/)[../RAM
    & CPU Caches →](https://en.algorithmica.org/hpc/cpu-cache/)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列格式有额外的优势，即你只能读取你需要的字段，因为不同的字段存储在不同的外部内存区域中。[← 无缓存算法](https://en.algorithmica.org/hpc/external-memory/oblivious/)[../RAM
    & CPU Caches →](https://en.algorithmica.org/hpc/cpu-cache/)
