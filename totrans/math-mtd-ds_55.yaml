- en: '7.3\. Limit behavior 1: stationary distributions#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap07_rwmc/03_stat/roch-mmids-rwmc-stat.html](https://mmids-textbook.github.io/chap07_rwmc/03_stat/roch-mmids-rwmc-stat.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We continue our exploration of basic Markov chain theory. In this section, we
    begin our study of the long-term behavior of a chain. As we did in the previous
    section, we restrict ourselves to finite-space discrete-time Markov chains that
    are also time-homogeneous.
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1\. Definitions[#](#definitions "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An important property of Markov chains is that, when run for long enough, they
    converge to a sort of “equilibrium.” We develop parts of this theory here. What
    do we mean by “equilibrium”? Here is the key definition.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Stationary Distribution)** \(\idx{stationary distribution}\xdi\)
    Let \((X_t)_{t \geq 0}\) be a Markov chain on \(\mathcal{S} = [n]\) with transition
    matrix \(P = (p_{i,j})_{i,j=1}^n\). A probability distribution \(\bpi = (\pi_i)_{i=1}^n\)
    over \([n]\) is a stationary distribution of \((X_t)_{t \geq 0}\) (or of \(P\))
    if:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \pi_i p_{i,j} = \pi_j, \qquad \forall j \in \mathcal{S}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: In matrix form, this condition can be stated as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi P = \bpi, \]
  prefs: []
  type: TYPE_NORMAL
- en: where recall that we think of \(\bpi\) as a row vector. One way to put this
    is that \(\bpi\) is a fixed point of \(P\) (through multiplication from the left).
    Another way to put it is that \(\bpi\) is a left (row) eigenvector of \(P\) with
    eigenvalue \(1\).
  prefs: []
  type: TYPE_NORMAL
- en: To see why a stationary distribution is indeed an equilibrium, we note the following.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Stationarity)** \(\idx{stationarity lemma}\xdi\) Let \(\mathbf{z}
    \in \mathbb{R}^n\) be a left eigenvector of transition matrix \(P \in \mathbb{R}^{n
    \times n}\) with eigenvalue \(1\). Then \(\mathbf{z} P^s = \mathbf{z}\) for all
    integers \(s \geq 0\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Indeed,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z} P^s = (\mathbf{z} P)P^{s-1} = \mathbf{z} P^{s-1} = (\mathbf{z}
    P) P^{s-2} = \mathbf{z} P^{s-2} = \cdots = \mathbf{z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the initial distribution is equal to a stationary distribution \(\bpi\).
    Then, from the *Time Marginals Theorem* and the *Stationarity Lemma*, the distribution
    at *any time \(s \geq 1\)* is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi P^s = \bpi. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, the distribution at all times indeed remains stationary.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section we will derive a remarkable fact: under certain conditions,
    a Markov chain started from an arbitrary initial distribution converges in the
    limit of \(t \to +\infty\) to a stationary distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Weather Model, continued)** Going back to the weather model,
    we compute a stationary distribution. We need \(\bpi = (\pi_1, \pi_2)^T\) to satisfy'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} (\pi_1, \pi_2)^T \begin{pmatrix} 3/4 & 1/4\\ 1/4 & 3/4 \end{pmatrix}
    = (\pi_1, \pi_2)^T \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: that is,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\frac{3}{4} \pi_1 + \frac{1}{4} \pi_2 = \pi_1\\ &\frac{1}{4}
    \pi_1 + \frac{3}{4} \pi_2 = \pi_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: You can check that, after rearranging, these two equations are in fact the same
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note, however, that we have some further restrictions: \(\bpi\) is a probability
    distribution. So \(\pi_1, \pi_2 \geq 0\) and \(\pi_1 + \pi_2 = 1\). Replacing
    the latter in the first equation we get'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{3}{4} \pi_1 + \frac{1}{4} (1 - \pi_1) = \pi_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: so that we require
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_1 = \frac{1/4}{1/2} = \frac{1}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: And \(\pi_2 = 1 - \pi_1 = 1/2\). The second equation above is also automatically
    satisfied. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The previous example is quite special. It generalizes to all doubly stochastic
    matrices (including the *Random Walk on the Petersen Graph* for instance). Indeed,
    we claim that the uniform distribution is always a stationary distribution in
    the doubly stochastic case. Let \(P = (p_{i,j})_{i,j=1}^n\) be doubly stochastic
    over \([n]\) and let \(\bpi = (\pi_i)_{i=1}^n\) be the uniform distribution on
    \([n]\). Then for all \(j \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \pi_i p_{i,j} = \sum_{i=1}^n \frac{1}{n} p_{i,j} = \frac{1}{n}
    \sum_{i=1}^n p_{i,j} = \frac{1}{n} = \pi_j \]
  prefs: []
  type: TYPE_NORMAL
- en: because the columns sum to \(1\). That proves the claim.
  prefs: []
  type: TYPE_NORMAL
- en: Is a stationary distribution guaranteed to exist? Is it unique? To answer this
    question, we first need some graph-theoretic concepts relevant to the long-term
    behavior of the chain.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(\(x \to y\))** A state \(x \in \S\) is said to communicate
    with a state \(y \in \S\) if there exists a sequence of states \(z_0 = x, z_1,
    z_2, \ldots, z_{r-1}, z_r = y\) such that for all \(\ell = 1,\ldots,r\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{z_{\ell-1},z_\ell} > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: We denote this property as \(x \to y\). \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the transition graph of the chain, the condition \(x \to y\) says
    that there exists a directed path from \(x\) to \(y\). It is important to see
    the difference between: (1) the existence of a direct edge from \(x\) to \(y\)
    (which implies \(x \to y\) but is not necessary) and (2) the existence of a directed
    path from \(x\) to \(y\). See the next example.'
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Robot Vacuum, continued)** Going back to the *Robot Vacuum
    Example*, recall the transition graph. While there is no direct edge from \(4\)
    to \(3\), we do have \(4 \to 3\) through the path \((4,2), (2,3)\). Do we have
    \(3 \to 4\)? \(\lhd\)'
  prefs: []
  type: TYPE_NORMAL
- en: Here is an important consequence of this graph-theoretic notion on the long-term
    behavior of the chain.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Communication)** \(\idx{communication lemma}\xdi\) If \(x \to
    y\), then there is an integer \(r \geq 1\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_r = y\,|\,X_0 = x] = (\mathbf{e}_x^T P^r)_y = (P^r)_{x,y} > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We lower bound the probability in the statement with the probability
    of visiting the particular sequence of states in the definition of \(x \to y\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By definition of \(x \to y\), there exists a sequence of states \(z_0
    = x, z_1, z_2, \ldots, z_{r-1}, z_r = y\) such that, for all \(\ell = 1,\ldots,r\),
    \(p_{z_{\ell-1},z_\ell} > 0\). Hence,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X_r = y\,|\,X_0 = x]\\ &\geq \P[X_1=z_1,X_2=z_2,\ldots,X_{r-1}=
    z_{r-1}, X_r = y\,|\,X_0 = x]\\ &= \prod_{\ell=1}^r \P[X_\ell = z_\ell\,|\,X_{\ell-1}
    = z_{\ell-1}]\\ &= \prod_{\ell=1}^r p_{z_{\ell-1},z_\ell} > 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows that the probability in the lemma is positive, but
    may not be \(1\). It also gives some insights about the question of the uniqueness
    of the stationary distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Consider random walk on the following digraph, which
    we refer to as the *Two Sinks Example* (why do you think?).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/86bc17d610f4fed2c1433a9a5219026a19898dc5e0f71d3184ac4fa4d5dd9dd6.png](../Images/2989fecad0172b2af16f97f29db93d7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we have \(1 \to 4\) (Why?). The *Communication Lemma* implies that, when
    started at \(1\), \((X_t)_{t \geq 0}\) visits \(4\) with positive probability.
    But that probability is not one. Indeed we also have \(1 \to 3\) (Why?), so there
    is a positive probability of visiting \(3\) as well. But if we do so before visiting
    \(4\), we stay at \(3\) forever hence cannot subsequently reach \(4\).
  prefs: []
  type: TYPE_NORMAL
- en: In fact, intuitively, if we run this chain long enough we will either get stuck
    at \(3\) or get stuck at \(4\). These give rise to different stationary distributions.
    The transition matrix is the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It is easy to check that \(\bpi = (0,0,1,0,0)^T\) and \(\bpi' = (0,0,0,1,0)^T\)
    are both stationary distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In fact, there are infinitely many stationary distributions in this case.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the behavior in the previous example, we introduce the following assumption.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Irreducibility)** \(\idx{irreducibility}\xdi\) A Markov chain
    on \(\mathcal{S}\) is irreducible if for all \(x, y \in \mathcal{S}\) with \(x
    \neq y\) we have \(x \to y\) and \(y \to x\). We also refer to the transition
    matrix as irreducible in that case. \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: In graphical terms, a Markov chain is irreducible if and only if its transition
    graph is strongly connected.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Because irreducibility is ultimately a graph-theoretic
    property, it is easy to check using NetworkX. For this, we use the function [`is_strongly_connected()`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.is_strongly_connected.html).
    Revisiting the *Robot Vacuum Example*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider again the *Two Sinks Example*. It turns out not to be irreducible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2\. Existence[#](#existence "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the irreducible case, it turns out that a stationary distribution always
    exists – and is in fact unique. (The presence of a single strongly connected component
    suffices for uniqueness to hold in this case, but we will not derive this here.
    Also, in the finite case, a stationary distribution always exists, but again we
    will not prove this here.)
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Existence of Stationary Distribution)** \(\idx{existence of
    stationary distribution}\xdi\) Let \(P\) be an irreducible transition matrix on
    \([n]\). Then there exists a unique stationary distribution \(\bpi\). Further
    all entries of \(\bpi\) are strictly positive. \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* The proof is not straightforward. We make a series of claims to establish
    existence.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 1)** There is a non-zero row vector \(\mathbf{z} \in \mathbb{R}^n\)
    such that \(\mathbf{z} P = \mathbf{z}\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 2)** Let \(\mathbf{z} \in \mathbb{R}^n\) be a non-zero row
    vector \(\mathbf{z} P = \mathbf{z}\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi = \frac{1}{\sum_{x} z_x} \mathbf{z} \]
  prefs: []
  type: TYPE_NORMAL
- en: is a strictly positive stationary distribution of \(P\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 3)** Let \(\bpi_1\) and \(\bpi_2\) be stationary distributions
    of \(P\). Then \(\bpi_1 = \bpi_2\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Lemma (Step 1))* Because \(P\) is stochastic, we have by definition
    that \(P \mathbf{1} = \mathbf{1}\), where \(\mathbf{1}\) is the column all-one
    vector of dimension \(n\). Put differently,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (P - I) \mathbf{1} = \mathbf{0} \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, the columns of \(P - I\) are linearly dependent. In particular \(\mathrm{rk}(P-I)
    < n\). That in turn implies that the rows of \(P - I\) are linearly dependent
    by the *Row Rank Equals Column Rank Theorem*. So there exists a non-zero row vector
    \(\mathbf{z} \in \mathbb{R}^n\) such that \(\mathbf{z}(P-I) = \mathbf{0}\), or
    after rearranging
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}P = \mathbf{z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Lemma (Step 2))* We break up the proof into several claims.'
  prefs: []
  type: TYPE_NORMAL
- en: To take advantage of irreducibility, we first construct a positive stochastic
    matrix with \(\mathbf{z}\) as a left eigenvector of eigenvalue \(1\). We then
    show that all entries of \(\mathbf{z}\) have the same sign. Finally, we normalize
    \(\mathbf{z}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 2a)** There exists a non-negative integer \(h\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R = \frac{1}{h+1}[I + P + P^2 + \cdots + P^h] \]
  prefs: []
  type: TYPE_NORMAL
- en: has only strictly positive entries and satisfies \(\mathbf{z} R = \mathbf{z}\).
    \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 2b)** The entries of \(\mathbf{z}\) are either all non-negative
    or all non-positive. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 2c)** Let \(\bpi = \frac{\mathbf{z}}{\mathbf{z}\mathbf{1}}\).
    Then \(\bpi\) is a strictly positive stationary distribution. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: We prove the claims next.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Lemma (Step 2a))* By irreducibility and the *Communication Lemma*,
    for any \(x, y \in [n]\) there is \(h_{x,y}\) such that \((P^{h_{x,y}})_{x,y}
    > 0\). Now define'
  prefs: []
  type: TYPE_NORMAL
- en: \[ h = \max_{x,y \in [n]} h_{x,y}. \]
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown (Try it!) that \(P^s\) (as a product of stochastic matrices)
    is itself a stochastic matrix for all \(s\). In particular, it has nonnegative
    entries. Hence, for each \(x,y\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ R_{x,y} = \frac{1}{h+1}[I_{x,y} + P_{x,y} + (P^2)_{x,y} + \cdots + (P^h)_{x,y}]
    \geq \frac{1}{h+1} (P^{h_{x,y}})_{x,y} > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown (Try it!) that \(R\) (as a convex combination of stochastic
    matrices) is itself a stochastic matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, by the *Stationarity Lemma*, since \(\mathbf{z} P = \mathbf{z}\) it
    follows that \(\mathbf{z} P^s = \mathbf{z}\) for all \(s\). Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z} R = \frac{1}{h+1}[\mathbf{z}I + \mathbf{z}P + \mathbf{z}P^2 +
    \cdots + \mathbf{z}P^h] = \frac{1}{h+1}[\mathbf{z} + \mathbf{z} + \mathbf{z} +
    \cdots + \mathbf{z}] = \mathbf{z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: That concludes the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Lemma (Step 2b))* We argue by contradiction. Suppose that two entries
    of \(\mathbf{z} = (z_x)_{x \in [n]}\) have different signs. Say \(z_i > 0\) while
    \(z_j < 0\). Let \(R = (r_{x,y})_{x,y=1}^n\). By *Step 2a*,'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ |z_y| = \left|\sum_{x} z_x r_{x,y}\right| = \left|\sum_{x: z_x \geq 0} z_x
    r_{x,y} + \sum_{x: z_x < 0} z_x r_{x,y}\right|. \]'
  prefs: []
  type: TYPE_NORMAL
- en: Because \(r_{x,y} > 0\) for all \(x,y\), the first term in the rightmost expression
    is strictly positive (since it is at least \(z_i r_{i,y} > 0\)) while the second
    term is strictly negative (since it is at most \(z_j r_{j,y} < 0\)). Hence, because
    of cancellations, this expression is strictly smaller than the sum of the absolute
    values
  prefs: []
  type: TYPE_NORMAL
- en: \[ |z_y| < \sum_{x} |z_x| r_{x,y}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Since \(R\) is stochastic by the proof of the previous claim, we deduce after
    summing over \(y\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{y} |z_y| < \sum_{y} \sum_{x} |z_x| r_{x,y} = \sum_{x} |z_x| \sum_{y}
    r_{x,y} = \sum_{x} |z_x|, \]
  prefs: []
  type: TYPE_NORMAL
- en: a contradiction, proving the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Lemma (Step 2c))* Now define \(\bpi = (\pi_x)_{x \in [n]}\) as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_ x = \frac{z_x}{\sum_{i} z_i} = \frac{|z_x|}{\sum_{i} |z_i|} \geq 0,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where the second equality comes from *Step 2b*. We also used the fact that \(\mathbf{z}
    \neq \mathbf{0}\). For all \(y\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{x} \pi_x p_{x,y} = \sum_{x} \frac{z_x}{\sum_{i} z_i} p_{x,y} = \frac{1}{\sum_{i}
    z_i} \sum_{x} z_x p_{x,y} = \frac{z_y}{\sum_{i} z_i} = \pi_y. \]
  prefs: []
  type: TYPE_NORMAL
- en: The same holds with \(p_{x,y}\) replaced with \(r_{x,y}\) by *Step 2a*. Since
    \(r_{x,y} > 0\) and \(\mathbf{z} \neq \mathbf{0}\) it follows that \(\pi_y > 0\)
    for all \(y\). That proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: That concludes the proof of *Lemma (Step 2)*. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: It remains to prove uniqueness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose there are two distinct stationary distributions \(\bpi_1\) and \(\bpi_2\).
    Since they are distinct, they are not a multiple of each other and therefore are
    linearly independent. Apply the Gram-Schmidt algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{q}_1 = \frac{\bpi_1}{\|\bpi_1\|} \qquad \text{and} \qquad\mathbf{q}_2
    = \frac{\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1}{\|\bpi_2 -
    \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1\|}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{q}_1 P = \frac{\bpi_1}{\|\bpi_1\|} P = \frac{\bpi_1 P}{\|\bpi_1\|}
    = \frac{\bpi_1}{\|\bpi_1\|} = \mathbf{q}_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: and all entries of \(\mathbf{q}_1\) are strictly positive.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{q}_2 P &= \frac{\bpi_2 - \langle \bpi_2, \mathbf{q}_1
    \rangle \mathbf{q}_1}{\|\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1\|}
    P\\ &= \frac{\bpi_2 P - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1 P}{\|\bpi_2
    - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1\|}\\ &= \frac{\bpi_2 - \langle
    \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1}{\|\bpi_2 - \langle \bpi_2, \mathbf{q}_1
    \rangle \mathbf{q}_1\|}\\ &= \mathbf{q}_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By *Steps 2a-2c*, there is a multiple of \(\mathbf{q}_2\), say \(\mathbf{q}_2'
    = \alpha \mathbf{q}_2\) with \(\alpha \neq 0\), such that \(\mathbf{q}_2' P =
    \mathbf{q}_2'\) and all entries of \(\mathbf{q}_2'\) are strictly positive.
  prefs: []
  type: TYPE_NORMAL
- en: By the properties of the Gram-Schmidt algorithm,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{q}_1, \mathbf{q}_2' \rangle = \langle \mathbf{q}_1, \alpha
    \mathbf{q}_2 \rangle = \alpha \langle \mathbf{q}_1, \mathbf{q}_2 \rangle = 0.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: But this is a contradiction – both vectors are strictly positive. That concludes
    the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: A few more observations about the eigenvalues of \(P\).
  prefs: []
  type: TYPE_NORMAL
- en: (1) Whenever \(\lambda\) is a left eigenvalue of \(P\) (i.e. \(\mathbf{z} P
    = \lambda \mathbf{z}\) for some \(\mathbf{z} \in \mathbb{R}^n\) as a row vector),
    it is also a right eigenvalue of \(P\) (i.e. \(P \mathbf{y} = \lambda \mathbf{y}\)
    for some \(\mathbf{y} \in \mathbb{R}^n\)). One way to see this using our previous
    results is to note that \(\mathbf{z} P = \lambda \mathbf{z}\) is equivalent to
    \(P^T \mathbf{z}^T = \lambda \mathbf{z}^T\), or put differently \((P^T - \lambda
    I) \mathbf{z}^T = \mathbf{0}\), so that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}^T \in \mathrm{null}(P^T - \lambda I). \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, \(P \mathbf{y} = \lambda \mathbf{y}\) is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{y} \in \mathrm{null}(P - \lambda I). \]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Rank-Nullity Theorem*, these two null spaces have the same dimension
    since \((P - \lambda I)^T = P^T - \lambda I^T = P^T - \lambda I\). In particular,
    when one of them has dimension greater than \(0\) (i.e., it contains non-zero
    vectors), so does the other.
  prefs: []
  type: TYPE_NORMAL
- en: That is not to say that they are the same space – only their dimension match!
    In other words, the left and right eigenvalues are the same, but the left and
    right eigenvectors *are not*.
  prefs: []
  type: TYPE_NORMAL
- en: (2) What we have shown in the previous theorem is that, if \(P\) is irreducible
    then it has a unique (up to scaling) left eigenvector of eigenvalue \(1\). By
    the first observation, \(\mathbf{1}\) is also the unique right eigenvector of
    \(P\) with eigenvalue \(1\) in this case. That is, the [geometric multiplicity](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenspaces,_geometric_multiplicity,_and_the_eigenbasis_for_matrices)
    of \(1\) is \(1\).
  prefs: []
  type: TYPE_NORMAL
- en: (3) What about the other eigenvalues? Suppose that \(\mathbf{z} P = \lambda
    \mathbf{z}\) for a non-zero row vector \(\mathbf{z}\), then taking the \(\ell_1\)-norm
    of the left-hand side we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{z} P\|_1 &= \sum_{j=1}^n \left|\sum_{i=1}^n z_i p_{i,j}
    \right| \leq \sum_{j=1}^n \sum_{i=1}^n |z_i| p_{i,j} = \sum_{i=1}^n |z_i| \sum_{j=1}^n
    p_{i,j} = \sum_{i=1}^n |z_i| = \|\mathbf{z}\|_1, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(P\) is stochastic.
  prefs: []
  type: TYPE_NORMAL
- en: The \(\ell_1\)-norm of the leftmost expression is \(\|\lambda \mathbf{z} \|_1
    = |\lambda| \|\mathbf{z} \|_1\). Hence \(|\lambda| \|\mathbf{z} \|_1 \leq \|\mathbf{z}\|_1\),
    which after simplifying implies \(|\lambda| \leq 1\).
  prefs: []
  type: TYPE_NORMAL
- en: (3) So all left and right eigenvalues of \(P\) are smaller or equal than \(1\)
    in absolute value. In the irreducible case, we know that \(1\) is achieved and
    has geometric multiplicity \(1\). What about \(-1\)? Suppose \(\mathbf{z} P =
    - \mathbf{z}\). Then applying \(P\) again to both sides we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z} P^2 = - \mathbf{z} P = \mathbf{z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So *if* \(P^2\) (which is stochastic) is irreducible, then there is a unique
    such \(\mathbf{z}\).
  prefs: []
  type: TYPE_NORMAL
- en: But we already know of one. Indeed, the unique stationary distribution \(\bpi\)
    of \(P\) satisfies
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi P^2 = \bpi P = \bpi. \]
  prefs: []
  type: TYPE_NORMAL
- en: But it does not satisfy \(\bpi P = - \bpi\). Hence there is no eigenvector with
    eigenvalue \(-1\) in that case.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In general, computing stationary distributions is not
    as straigthforward as in the simple example we considered above. We conclude this
    subsection with some numerical recipes.'
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the *Robot Vacuum*, finding a solution to \(\bpi P =\bpi\) in
    this case is not obvious. One way to do this is to note that, taking transposes,
    this condition is equivalent to \(P^T \bpi^T = \bpi^T\). That is, \(\bpi^T\) is
    an eigenvector of \(P^T\) with eigenvalue \(1\). (Or, as we noted previously,
    the row vector \(\bpi\) is a left eigenvector of \(P\) with eigenvalue \(1\).)
    It must also satisfy \(\bpi \geq 0\) with at least one entry non-zero. Here, we
    use NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The first eigenvalue is approximately \(1\), as seen below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The corresponding eigenvector is approximately non-negative.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: To obtain a stationary distribution, we remove the imaginary part and normalize
    it to sum to \(1\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, we can solve the linear system
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \pi_i p_{i,j} = \pi_j, \qquad \forall j \in [n]. \]
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the last equation is a linear combination over the other equations
    (see *Problem 7.21*), so we remove it and replace it instead with the condition
    \(\sum_{i=1}^n \pi_i = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The left-hand side of the resulting linear system is (after taking the transpose
    to work with column vectors):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The right-hand side of the resulting linear system is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We solve the linear system using [`numpy.linalg.solve()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This last approach is known as “Replace an Equation”.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The Perron-Frobenius theorem is a powerful result about the
    eigenvalues and eigenvectors of certain types of matrices, including irreducible
    stochastic matrices. Ask your favorite AI chatbot to explain the Perron-Frobenius
    theorem and how it relates to the material in this section. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following is the correct condition for a probability distribution
    \(\boldsymbol{\pi} = (\pi_i)_{i=1}^n\) to be a stationary distribution of a Markov
    chain with transition matrix \(P = (p_{i,j})_{i,j=1}^n\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\sum_{j=1}^n \pi_i p_{i,j} = \pi_j\) for all \(i \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\sum_{i=1}^n \pi_i p_{i,j} = \pi_j\) for all \(j \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\sum_{j=1}^n \pi_i p_{i,j} = \pi_i\) for all \(i \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\sum_{i=1}^n \pi_i p_{i,j} = \pi_i\) for all \(j \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Which of the following is the matrix form of the condition for a probability
    distribution \(\pi\) to be a stationary distribution of a Markov chain with transition
    matrix \(P\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\boldsymbol{\pi} P = \boldsymbol{\pi}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(P \boldsymbol{\pi} = \boldsymbol{\pi}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\boldsymbol{\pi} P^T = \boldsymbol{\pi}^T\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(P^T \boldsymbol{\pi}^T = \boldsymbol{\pi}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** A Markov chain is irreducible if:'
  prefs: []
  type: TYPE_NORMAL
- en: a) Every state communicates with every other state.
  prefs: []
  type: TYPE_NORMAL
- en: b) There exists a state that communicates with every other state.
  prefs: []
  type: TYPE_NORMAL
- en: c) The transition graph of the chain is strongly connected.
  prefs: []
  type: TYPE_NORMAL
- en: d) Both a and c.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Consider the following transition graph of a Markov chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Is this Markov chain irreducible?
  prefs: []
  type: TYPE_NORMAL
- en: a) Yes
  prefs: []
  type: TYPE_NORMAL
- en: b) No
  prefs: []
  type: TYPE_NORMAL
- en: '**5** In an irreducible Markov chain, the left and right eigenvectors corresponding
    to eigenvalue 1 are:'
  prefs: []
  type: TYPE_NORMAL
- en: a) The same up to scaling.
  prefs: []
  type: TYPE_NORMAL
- en: b) Always different.
  prefs: []
  type: TYPE_NORMAL
- en: c) Transposes of each other.
  prefs: []
  type: TYPE_NORMAL
- en: d) Not necessarily related to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states that a probability distribution
    \(\boldsymbol{\pi} = (\pi_i)_{i=1}^n\) over \([n]\) is a stationary distribution
    of a Markov chain with transition matrix \(P = (p_{i,j})_{i,j=1}^n\) if \(\sum_{i=1}^n
    \pi_i p_{i,j} = \pi_j\) for all \(j \in [n]\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: a. Justification: The text states that the condition for a probability
    distribution \(\boldsymbol{\pi}\) to be a stationary distribution of a Markov
    chain with transition matrix \(P\) can be written in matrix form as \(\boldsymbol{\pi}
    P = \boldsymbol{\pi}\), where \(\boldsymbol{\pi}\) is thought of as a row vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: d. Justification: The text states that a Markov chain on \(S\)
    is irreducible if for all \(x, y \in S\) with \(x \neq y\), we have \(x \to y\)
    and \(y \to x\). It also mentions that a Markov chain is irreducible if and only
    if its transition graph is strongly connected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: b. Justification: The Markov chain is not irreducible because
    there is no way to get back to state 1 from state 3.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: d. Justification: The text states that for an irreducible Markov
    chain, the left and right eigenvalues are the same, but the left and right eigenvectors
    are not necessarily the same. It also mentions that the geometric multiplicity
    of eigenvalue 1 is 1, implying that the left and right eigenvectors corresponding
    to eigenvalue 1 are unique up to scaling, but they are not necessarily related
    to each other.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.1\. Definitions[#](#definitions "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An important property of Markov chains is that, when run for long enough, they
    converge to a sort of “equilibrium.” We develop parts of this theory here. What
    do we mean by “equilibrium”? Here is the key definition.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Stationary Distribution)** \(\idx{stationary distribution}\xdi\)
    Let \((X_t)_{t \geq 0}\) be a Markov chain on \(\mathcal{S} = [n]\) with transition
    matrix \(P = (p_{i,j})_{i,j=1}^n\). A probability distribution \(\bpi = (\pi_i)_{i=1}^n\)
    over \([n]\) is a stationary distribution of \((X_t)_{t \geq 0}\) (or of \(P\))
    if:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \pi_i p_{i,j} = \pi_j, \qquad \forall j \in \mathcal{S}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: In matrix form, this condition can be stated as
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi P = \bpi, \]
  prefs: []
  type: TYPE_NORMAL
- en: where recall that we think of \(\bpi\) as a row vector. One way to put this
    is that \(\bpi\) is a fixed point of \(P\) (through multiplication from the left).
    Another way to put it is that \(\bpi\) is a left (row) eigenvector of \(P\) with
    eigenvalue \(1\).
  prefs: []
  type: TYPE_NORMAL
- en: To see why a stationary distribution is indeed an equilibrium, we note the following.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Stationarity)** \(\idx{stationarity lemma}\xdi\) Let \(\mathbf{z}
    \in \mathbb{R}^n\) be a left eigenvector of transition matrix \(P \in \mathbb{R}^{n
    \times n}\) with eigenvalue \(1\). Then \(\mathbf{z} P^s = \mathbf{z}\) for all
    integers \(s \geq 0\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Indeed,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z} P^s = (\mathbf{z} P)P^{s-1} = \mathbf{z} P^{s-1} = (\mathbf{z}
    P) P^{s-2} = \mathbf{z} P^{s-2} = \cdots = \mathbf{z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the initial distribution is equal to a stationary distribution \(\bpi\).
    Then, from the *Time Marginals Theorem* and the *Stationarity Lemma*, the distribution
    at *any time \(s \geq 1\)* is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi P^s = \bpi. \]
  prefs: []
  type: TYPE_NORMAL
- en: That is, the distribution at all times indeed remains stationary.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section we will derive a remarkable fact: under certain conditions,
    a Markov chain started from an arbitrary initial distribution converges in the
    limit of \(t \to +\infty\) to a stationary distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Weather Model, continued)** Going back to the weather model,
    we compute a stationary distribution. We need \(\bpi = (\pi_1, \pi_2)^T\) to satisfy'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} (\pi_1, \pi_2)^T \begin{pmatrix} 3/4 & 1/4\\ 1/4 & 3/4 \end{pmatrix}
    = (\pi_1, \pi_2)^T \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: that is,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\frac{3}{4} \pi_1 + \frac{1}{4} \pi_2 = \pi_1\\ &\frac{1}{4}
    \pi_1 + \frac{3}{4} \pi_2 = \pi_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: You can check that, after rearranging, these two equations are in fact the same
    one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note, however, that we have some further restrictions: \(\bpi\) is a probability
    distribution. So \(\pi_1, \pi_2 \geq 0\) and \(\pi_1 + \pi_2 = 1\). Replacing
    the latter in the first equation we get'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{3}{4} \pi_1 + \frac{1}{4} (1 - \pi_1) = \pi_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: so that we require
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_1 = \frac{1/4}{1/2} = \frac{1}{2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: And \(\pi_2 = 1 - \pi_1 = 1/2\). The second equation above is also automatically
    satisfied. \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: The previous example is quite special. It generalizes to all doubly stochastic
    matrices (including the *Random Walk on the Petersen Graph* for instance). Indeed,
    we claim that the uniform distribution is always a stationary distribution in
    the doubly stochastic case. Let \(P = (p_{i,j})_{i,j=1}^n\) be doubly stochastic
    over \([n]\) and let \(\bpi = (\pi_i)_{i=1}^n\) be the uniform distribution on
    \([n]\). Then for all \(j \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \pi_i p_{i,j} = \sum_{i=1}^n \frac{1}{n} p_{i,j} = \frac{1}{n}
    \sum_{i=1}^n p_{i,j} = \frac{1}{n} = \pi_j \]
  prefs: []
  type: TYPE_NORMAL
- en: because the columns sum to \(1\). That proves the claim.
  prefs: []
  type: TYPE_NORMAL
- en: Is a stationary distribution guaranteed to exist? Is it unique? To answer this
    question, we first need some graph-theoretic concepts relevant to the long-term
    behavior of the chain.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(\(x \to y\))** A state \(x \in \S\) is said to communicate
    with a state \(y \in \S\) if there exists a sequence of states \(z_0 = x, z_1,
    z_2, \ldots, z_{r-1}, z_r = y\) such that for all \(\ell = 1,\ldots,r\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[ p_{z_{\ell-1},z_\ell} > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: We denote this property as \(x \to y\). \(\natural\)
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of the transition graph of the chain, the condition \(x \to y\) says
    that there exists a directed path from \(x\) to \(y\). It is important to see
    the difference between: (1) the existence of a direct edge from \(x\) to \(y\)
    (which implies \(x \to y\) but is not necessary) and (2) the existence of a directed
    path from \(x\) to \(y\). See the next example.'
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** **(Robot Vacuum, continued)** Going back to the *Robot Vacuum
    Example*, recall the transition graph. While there is no direct edge from \(4\)
    to \(3\), we do have \(4 \to 3\) through the path \((4,2), (2,3)\). Do we have
    \(3 \to 4\)? \(\lhd\)'
  prefs: []
  type: TYPE_NORMAL
- en: Here is an important consequence of this graph-theoretic notion on the long-term
    behavior of the chain.
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Communication)** \(\idx{communication lemma}\xdi\) If \(x \to
    y\), then there is an integer \(r \geq 1\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \P[X_r = y\,|\,X_0 = x] = (\mathbf{e}_x^T P^r)_y = (P^r)_{x,y} > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* We lower bound the probability in the statement with the probability
    of visiting the particular sequence of states in the definition of \(x \to y\).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* By definition of \(x \to y\), there exists a sequence of states \(z_0
    = x, z_1, z_2, \ldots, z_{r-1}, z_r = y\) such that, for all \(\ell = 1,\ldots,r\),
    \(p_{z_{\ell-1},z_\ell} > 0\). Hence,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &\P[X_r = y\,|\,X_0 = x]\\ &\geq \P[X_1=z_1,X_2=z_2,\ldots,X_{r-1}=
    z_{r-1}, X_r = y\,|\,X_0 = x]\\ &= \prod_{\ell=1}^r \P[X_\ell = z_\ell\,|\,X_{\ell-1}
    = z_{\ell-1}]\\ &= \prod_{\ell=1}^r p_{z_{\ell-1},z_\ell} > 0, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: as claimed. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: The following example shows that the probability in the lemma is positive, but
    may not be \(1\). It also gives some insights about the question of the uniqueness
    of the stationary distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Consider random walk on the following digraph, which
    we refer to as the *Two Sinks Example* (why do you think?).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/86bc17d610f4fed2c1433a9a5219026a19898dc5e0f71d3184ac4fa4d5dd9dd6.png](../Images/2989fecad0172b2af16f97f29db93d7d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here we have \(1 \to 4\) (Why?). The *Communication Lemma* implies that, when
    started at \(1\), \((X_t)_{t \geq 0}\) visits \(4\) with positive probability.
    But that probability is not one. Indeed we also have \(1 \to 3\) (Why?), so there
    is a positive probability of visiting \(3\) as well. But if we do so before visiting
    \(4\), we stay at \(3\) forever hence cannot subsequently reach \(4\).
  prefs: []
  type: TYPE_NORMAL
- en: In fact, intuitively, if we run this chain long enough we will either get stuck
    at \(3\) or get stuck at \(4\). These give rise to different stationary distributions.
    The transition matrix is the following.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: It is easy to check that \(\bpi = (0,0,1,0,0)^T\) and \(\bpi' = (0,0,0,1,0)^T\)
    are both stationary distributions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In fact, there are infinitely many stationary distributions in this case.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the behavior in the previous example, we introduce the following assumption.
  prefs: []
  type: TYPE_NORMAL
- en: '**DEFINITION** **(Irreducibility)** \(\idx{irreducibility}\xdi\) A Markov chain
    on \(\mathcal{S}\) is irreducible if for all \(x, y \in \mathcal{S}\) with \(x
    \neq y\) we have \(x \to y\) and \(y \to x\). We also refer to the transition
    matrix as irreducible in that case. \(\natural\)'
  prefs: []
  type: TYPE_NORMAL
- en: In graphical terms, a Markov chain is irreducible if and only if its transition
    graph is strongly connected.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** Because irreducibility is ultimately a graph-theoretic
    property, it is easy to check using NetworkX. For this, we use the function [`is_strongly_connected()`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.is_strongly_connected.html).
    Revisiting the *Robot Vacuum Example*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider again the *Two Sinks Example*. It turns out not to be irreducible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: 7.3.2\. Existence[#](#existence "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the irreducible case, it turns out that a stationary distribution always
    exists – and is in fact unique. (The presence of a single strongly connected component
    suffices for uniqueness to hold in this case, but we will not derive this here.
    Also, in the finite case, a stationary distribution always exists, but again we
    will not prove this here.)
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Existence of Stationary Distribution)** \(\idx{existence of
    stationary distribution}\xdi\) Let \(P\) be an irreducible transition matrix on
    \([n]\). Then there exists a unique stationary distribution \(\bpi\). Further
    all entries of \(\bpi\) are strictly positive. \(\sharp\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* The proof is not straightforward. We make a series of claims to establish
    existence.'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 1)** There is a non-zero row vector \(\mathbf{z} \in \mathbb{R}^n\)
    such that \(\mathbf{z} P = \mathbf{z}\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 2)** Let \(\mathbf{z} \in \mathbb{R}^n\) be a non-zero row
    vector \(\mathbf{z} P = \mathbf{z}\). Then'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi = \frac{1}{\sum_{x} z_x} \mathbf{z} \]
  prefs: []
  type: TYPE_NORMAL
- en: is a strictly positive stationary distribution of \(P\). \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 3)** Let \(\bpi_1\) and \(\bpi_2\) be stationary distributions
    of \(P\). Then \(\bpi_1 = \bpi_2\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Lemma (Step 1))* Because \(P\) is stochastic, we have by definition
    that \(P \mathbf{1} = \mathbf{1}\), where \(\mathbf{1}\) is the column all-one
    vector of dimension \(n\). Put differently,'
  prefs: []
  type: TYPE_NORMAL
- en: \[ (P - I) \mathbf{1} = \mathbf{0} \]
  prefs: []
  type: TYPE_NORMAL
- en: that is, the columns of \(P - I\) are linearly dependent. In particular \(\mathrm{rk}(P-I)
    < n\). That in turn implies that the rows of \(P - I\) are linearly dependent
    by the *Row Rank Equals Column Rank Theorem*. So there exists a non-zero row vector
    \(\mathbf{z} \in \mathbb{R}^n\) such that \(\mathbf{z}(P-I) = \mathbf{0}\), or
    after rearranging
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}P = \mathbf{z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: That proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Lemma (Step 2))* We break up the proof into several claims.'
  prefs: []
  type: TYPE_NORMAL
- en: To take advantage of irreducibility, we first construct a positive stochastic
    matrix with \(\mathbf{z}\) as a left eigenvector of eigenvalue \(1\). We then
    show that all entries of \(\mathbf{z}\) have the same sign. Finally, we normalize
    \(\mathbf{z}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 2a)** There exists a non-negative integer \(h\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ R = \frac{1}{h+1}[I + P + P^2 + \cdots + P^h] \]
  prefs: []
  type: TYPE_NORMAL
- en: has only strictly positive entries and satisfies \(\mathbf{z} R = \mathbf{z}\).
    \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 2b)** The entries of \(\mathbf{z}\) are either all non-negative
    or all non-positive. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Step 2c)** Let \(\bpi = \frac{\mathbf{z}}{\mathbf{z}\mathbf{1}}\).
    Then \(\bpi\) is a strictly positive stationary distribution. \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: We prove the claims next.
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Lemma (Step 2a))* By irreducibility and the *Communication Lemma*,
    for any \(x, y \in [n]\) there is \(h_{x,y}\) such that \((P^{h_{x,y}})_{x,y}
    > 0\). Now define'
  prefs: []
  type: TYPE_NORMAL
- en: \[ h = \max_{x,y \in [n]} h_{x,y}. \]
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown (Try it!) that \(P^s\) (as a product of stochastic matrices)
    is itself a stochastic matrix for all \(s\). In particular, it has nonnegative
    entries. Hence, for each \(x,y\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ R_{x,y} = \frac{1}{h+1}[I_{x,y} + P_{x,y} + (P^2)_{x,y} + \cdots + (P^h)_{x,y}]
    \geq \frac{1}{h+1} (P^{h_{x,y}})_{x,y} > 0. \]
  prefs: []
  type: TYPE_NORMAL
- en: It can be shown (Try it!) that \(R\) (as a convex combination of stochastic
    matrices) is itself a stochastic matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, by the *Stationarity Lemma*, since \(\mathbf{z} P = \mathbf{z}\) it
    follows that \(\mathbf{z} P^s = \mathbf{z}\) for all \(s\). Therefore,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z} R = \frac{1}{h+1}[\mathbf{z}I + \mathbf{z}P + \mathbf{z}P^2 +
    \cdots + \mathbf{z}P^h] = \frac{1}{h+1}[\mathbf{z} + \mathbf{z} + \mathbf{z} +
    \cdots + \mathbf{z}] = \mathbf{z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: That concludes the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Lemma (Step 2b))* We argue by contradiction. Suppose that two entries
    of \(\mathbf{z} = (z_x)_{x \in [n]}\) have different signs. Say \(z_i > 0\) while
    \(z_j < 0\). Let \(R = (r_{x,y})_{x,y=1}^n\). By *Step 2a*,'
  prefs: []
  type: TYPE_NORMAL
- en: '\[ |z_y| = \left|\sum_{x} z_x r_{x,y}\right| = \left|\sum_{x: z_x \geq 0} z_x
    r_{x,y} + \sum_{x: z_x < 0} z_x r_{x,y}\right|. \]'
  prefs: []
  type: TYPE_NORMAL
- en: Because \(r_{x,y} > 0\) for all \(x,y\), the first term in the rightmost expression
    is strictly positive (since it is at least \(z_i r_{i,y} > 0\)) while the second
    term is strictly negative (since it is at most \(z_j r_{j,y} < 0\)). Hence, because
    of cancellations, this expression is strictly smaller than the sum of the absolute
    values
  prefs: []
  type: TYPE_NORMAL
- en: \[ |z_y| < \sum_{x} |z_x| r_{x,y}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Since \(R\) is stochastic by the proof of the previous claim, we deduce after
    summing over \(y\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{y} |z_y| < \sum_{y} \sum_{x} |z_x| r_{x,y} = \sum_{x} |z_x| \sum_{y}
    r_{x,y} = \sum_{x} |z_x|, \]
  prefs: []
  type: TYPE_NORMAL
- en: a contradiction, proving the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* *(Lemma (Step 2c))* Now define \(\bpi = (\pi_x)_{x \in [n]}\) as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \pi_ x = \frac{z_x}{\sum_{i} z_i} = \frac{|z_x|}{\sum_{i} |z_i|} \geq 0,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where the second equality comes from *Step 2b*. We also used the fact that \(\mathbf{z}
    \neq \mathbf{0}\). For all \(y\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{x} \pi_x p_{x,y} = \sum_{x} \frac{z_x}{\sum_{i} z_i} p_{x,y} = \frac{1}{\sum_{i}
    z_i} \sum_{x} z_x p_{x,y} = \frac{z_y}{\sum_{i} z_i} = \pi_y. \]
  prefs: []
  type: TYPE_NORMAL
- en: The same holds with \(p_{x,y}\) replaced with \(r_{x,y}\) by *Step 2a*. Since
    \(r_{x,y} > 0\) and \(\mathbf{z} \neq \mathbf{0}\) it follows that \(\pi_y > 0\)
    for all \(y\). That proves the claim. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: That concludes the proof of *Lemma (Step 2)*. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: It remains to prove uniqueness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose there are two distinct stationary distributions \(\bpi_1\) and \(\bpi_2\).
    Since they are distinct, they are not a multiple of each other and therefore are
    linearly independent. Apply the Gram-Schmidt algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{q}_1 = \frac{\bpi_1}{\|\bpi_1\|} \qquad \text{and} \qquad\mathbf{q}_2
    = \frac{\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1}{\|\bpi_2 -
    \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1\|}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{q}_1 P = \frac{\bpi_1}{\|\bpi_1\|} P = \frac{\bpi_1 P}{\|\bpi_1\|}
    = \frac{\bpi_1}{\|\bpi_1\|} = \mathbf{q}_1 \]
  prefs: []
  type: TYPE_NORMAL
- en: and all entries of \(\mathbf{q}_1\) are strictly positive.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{q}_2 P &= \frac{\bpi_2 - \langle \bpi_2, \mathbf{q}_1
    \rangle \mathbf{q}_1}{\|\bpi_2 - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1\|}
    P\\ &= \frac{\bpi_2 P - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1 P}{\|\bpi_2
    - \langle \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1\|}\\ &= \frac{\bpi_2 - \langle
    \bpi_2, \mathbf{q}_1 \rangle \mathbf{q}_1}{\|\bpi_2 - \langle \bpi_2, \mathbf{q}_1
    \rangle \mathbf{q}_1\|}\\ &= \mathbf{q}_2. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By *Steps 2a-2c*, there is a multiple of \(\mathbf{q}_2\), say \(\mathbf{q}_2'
    = \alpha \mathbf{q}_2\) with \(\alpha \neq 0\), such that \(\mathbf{q}_2' P =
    \mathbf{q}_2'\) and all entries of \(\mathbf{q}_2'\) are strictly positive.
  prefs: []
  type: TYPE_NORMAL
- en: By the properties of the Gram-Schmidt algorithm,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \langle \mathbf{q}_1, \mathbf{q}_2' \rangle = \langle \mathbf{q}_1, \alpha
    \mathbf{q}_2 \rangle = \alpha \langle \mathbf{q}_1, \mathbf{q}_2 \rangle = 0.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: But this is a contradiction – both vectors are strictly positive. That concludes
    the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: A few more observations about the eigenvalues of \(P\).
  prefs: []
  type: TYPE_NORMAL
- en: (1) Whenever \(\lambda\) is a left eigenvalue of \(P\) (i.e. \(\mathbf{z} P
    = \lambda \mathbf{z}\) for some \(\mathbf{z} \in \mathbb{R}^n\) as a row vector),
    it is also a right eigenvalue of \(P\) (i.e. \(P \mathbf{y} = \lambda \mathbf{y}\)
    for some \(\mathbf{y} \in \mathbb{R}^n\)). One way to see this using our previous
    results is to note that \(\mathbf{z} P = \lambda \mathbf{z}\) is equivalent to
    \(P^T \mathbf{z}^T = \lambda \mathbf{z}^T\), or put differently \((P^T - \lambda
    I) \mathbf{z}^T = \mathbf{0}\), so that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z}^T \in \mathrm{null}(P^T - \lambda I). \]
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, \(P \mathbf{y} = \lambda \mathbf{y}\) is equivalent to
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{y} \in \mathrm{null}(P - \lambda I). \]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Rank-Nullity Theorem*, these two null spaces have the same dimension
    since \((P - \lambda I)^T = P^T - \lambda I^T = P^T - \lambda I\). In particular,
    when one of them has dimension greater than \(0\) (i.e., it contains non-zero
    vectors), so does the other.
  prefs: []
  type: TYPE_NORMAL
- en: That is not to say that they are the same space – only their dimension match!
    In other words, the left and right eigenvalues are the same, but the left and
    right eigenvectors *are not*.
  prefs: []
  type: TYPE_NORMAL
- en: (2) What we have shown in the previous theorem is that, if \(P\) is irreducible
    then it has a unique (up to scaling) left eigenvector of eigenvalue \(1\). By
    the first observation, \(\mathbf{1}\) is also the unique right eigenvector of
    \(P\) with eigenvalue \(1\) in this case. That is, the [geometric multiplicity](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors#Eigenspaces,_geometric_multiplicity,_and_the_eigenbasis_for_matrices)
    of \(1\) is \(1\).
  prefs: []
  type: TYPE_NORMAL
- en: (3) What about the other eigenvalues? Suppose that \(\mathbf{z} P = \lambda
    \mathbf{z}\) for a non-zero row vector \(\mathbf{z}\), then taking the \(\ell_1\)-norm
    of the left-hand side we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \|\mathbf{z} P\|_1 &= \sum_{j=1}^n \left|\sum_{i=1}^n z_i p_{i,j}
    \right| \leq \sum_{j=1}^n \sum_{i=1}^n |z_i| p_{i,j} = \sum_{i=1}^n |z_i| \sum_{j=1}^n
    p_{i,j} = \sum_{i=1}^n |z_i| = \|\mathbf{z}\|_1, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used that \(P\) is stochastic.
  prefs: []
  type: TYPE_NORMAL
- en: The \(\ell_1\)-norm of the leftmost expression is \(\|\lambda \mathbf{z} \|_1
    = |\lambda| \|\mathbf{z} \|_1\). Hence \(|\lambda| \|\mathbf{z} \|_1 \leq \|\mathbf{z}\|_1\),
    which after simplifying implies \(|\lambda| \leq 1\).
  prefs: []
  type: TYPE_NORMAL
- en: (3) So all left and right eigenvalues of \(P\) are smaller or equal than \(1\)
    in absolute value. In the irreducible case, we know that \(1\) is achieved and
    has geometric multiplicity \(1\). What about \(-1\)? Suppose \(\mathbf{z} P =
    - \mathbf{z}\). Then applying \(P\) again to both sides we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{z} P^2 = - \mathbf{z} P = \mathbf{z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So *if* \(P^2\) (which is stochastic) is irreducible, then there is a unique
    such \(\mathbf{z}\).
  prefs: []
  type: TYPE_NORMAL
- en: But we already know of one. Indeed, the unique stationary distribution \(\bpi\)
    of \(P\) satisfies
  prefs: []
  type: TYPE_NORMAL
- en: \[ \bpi P^2 = \bpi P = \bpi. \]
  prefs: []
  type: TYPE_NORMAL
- en: But it does not satisfy \(\bpi P = - \bpi\). Hence there is no eigenvector with
    eigenvalue \(-1\) in that case.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In general, computing stationary distributions is not
    as straigthforward as in the simple example we considered above. We conclude this
    subsection with some numerical recipes.'
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the *Robot Vacuum*, finding a solution to \(\bpi P =\bpi\) in
    this case is not obvious. One way to do this is to note that, taking transposes,
    this condition is equivalent to \(P^T \bpi^T = \bpi^T\). That is, \(\bpi^T\) is
    an eigenvector of \(P^T\) with eigenvalue \(1\). (Or, as we noted previously,
    the row vector \(\bpi\) is a left eigenvector of \(P\) with eigenvalue \(1\).)
    It must also satisfy \(\bpi \geq 0\) with at least one entry non-zero. Here, we
    use NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The first eigenvalue is approximately \(1\), as seen below.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The corresponding eigenvector is approximately non-negative.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: To obtain a stationary distribution, we remove the imaginary part and normalize
    it to sum to \(1\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, we can solve the linear system
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sum_{i=1}^n \pi_i p_{i,j} = \pi_j, \qquad \forall j \in [n]. \]
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the last equation is a linear combination over the other equations
    (see *Problem 7.21*), so we remove it and replace it instead with the condition
    \(\sum_{i=1}^n \pi_i = 1\).
  prefs: []
  type: TYPE_NORMAL
- en: 'The left-hand side of the resulting linear system is (after taking the transpose
    to work with column vectors):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The right-hand side of the resulting linear system is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We solve the linear system using [`numpy.linalg.solve()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: This last approach is known as “Replace an Equation”.
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The Perron-Frobenius theorem is a powerful result about the
    eigenvalues and eigenvectors of certain types of matrices, including irreducible
    stochastic matrices. Ask your favorite AI chatbot to explain the Perron-Frobenius
    theorem and how it relates to the material in this section. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** Which of the following is the correct condition for a probability distribution
    \(\boldsymbol{\pi} = (\pi_i)_{i=1}^n\) to be a stationary distribution of a Markov
    chain with transition matrix \(P = (p_{i,j})_{i,j=1}^n\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\sum_{j=1}^n \pi_i p_{i,j} = \pi_j\) for all \(i \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\sum_{i=1}^n \pi_i p_{i,j} = \pi_j\) for all \(j \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\sum_{j=1}^n \pi_i p_{i,j} = \pi_i\) for all \(i \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\sum_{i=1}^n \pi_i p_{i,j} = \pi_i\) for all \(j \in [n]\)
  prefs: []
  type: TYPE_NORMAL
- en: '**2** Which of the following is the matrix form of the condition for a probability
    distribution \(\pi\) to be a stationary distribution of a Markov chain with transition
    matrix \(P\)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\boldsymbol{\pi} P = \boldsymbol{\pi}\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(P \boldsymbol{\pi} = \boldsymbol{\pi}\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\boldsymbol{\pi} P^T = \boldsymbol{\pi}^T\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(P^T \boldsymbol{\pi}^T = \boldsymbol{\pi}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**3** A Markov chain is irreducible if:'
  prefs: []
  type: TYPE_NORMAL
- en: a) Every state communicates with every other state.
  prefs: []
  type: TYPE_NORMAL
- en: b) There exists a state that communicates with every other state.
  prefs: []
  type: TYPE_NORMAL
- en: c) The transition graph of the chain is strongly connected.
  prefs: []
  type: TYPE_NORMAL
- en: d) Both a and c.
  prefs: []
  type: TYPE_NORMAL
- en: '**4** Consider the following transition graph of a Markov chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Is this Markov chain irreducible?
  prefs: []
  type: TYPE_NORMAL
- en: a) Yes
  prefs: []
  type: TYPE_NORMAL
- en: b) No
  prefs: []
  type: TYPE_NORMAL
- en: '**5** In an irreducible Markov chain, the left and right eigenvectors corresponding
    to eigenvalue 1 are:'
  prefs: []
  type: TYPE_NORMAL
- en: a) The same up to scaling.
  prefs: []
  type: TYPE_NORMAL
- en: b) Always different.
  prefs: []
  type: TYPE_NORMAL
- en: c) Transposes of each other.
  prefs: []
  type: TYPE_NORMAL
- en: d) Not necessarily related to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: b. Justification: The text states that a probability distribution
    \(\boldsymbol{\pi} = (\pi_i)_{i=1}^n\) over \([n]\) is a stationary distribution
    of a Markov chain with transition matrix \(P = (p_{i,j})_{i,j=1}^n\) if \(\sum_{i=1}^n
    \pi_i p_{i,j} = \pi_j\) for all \(j \in [n]\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: a. Justification: The text states that the condition for a probability
    distribution \(\boldsymbol{\pi}\) to be a stationary distribution of a Markov
    chain with transition matrix \(P\) can be written in matrix form as \(\boldsymbol{\pi}
    P = \boldsymbol{\pi}\), where \(\boldsymbol{\pi}\) is thought of as a row vector.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: d. Justification: The text states that a Markov chain on \(S\)
    is irreducible if for all \(x, y \in S\) with \(x \neq y\), we have \(x \to y\)
    and \(y \to x\). It also mentions that a Markov chain is irreducible if and only
    if its transition graph is strongly connected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: b. Justification: The Markov chain is not irreducible because
    there is no way to get back to state 1 from state 3.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: d. Justification: The text states that for an irreducible Markov
    chain, the left and right eigenvalues are the same, but the left and right eigenvectors
    are not necessarily the same. It also mentions that the geometric multiplicity
    of eigenvalue 1 is 1, implying that the left and right eigenvectors corresponding
    to eigenvalue 1 are unique up to scaling, but they are not necessarily related
    to each other.'
  prefs: []
  type: TYPE_NORMAL
