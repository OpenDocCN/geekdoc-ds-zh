<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>2.1. Motivating example: predicting sales#</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>2.1. Motivating example: predicting sales#</h1>
<blockquote>原文：<a href="https://mmids-textbook.github.io/chap02_ls/01_motiv/roch-mmids-ls-motiv.html">https://mmids-textbook.github.io/chap02_ls/01_motiv/roch-mmids-ls-motiv.html</a></blockquote>

<p><strong>Figure:</strong> Helpful map of ML by scitkit-learn (<a class="reference external" href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html">Source</a>)</p>
<p><img alt="ml-cheat-sheet" src="../Images/19ac9e49b2f297976e40fee63e1c4ba0.png" data-original-src="https://scikit-learn.org/1.4/_static/ml_map.png"/></p>
<p><span class="math notranslate nohighlight">\(\bowtie\)</span></p>
<p>The following dataset is from the excellent textbook <a class="reference external" href="https://www.statlearning.com/">[ISLP]</a>. Quoting [ISLP, Section 2.1]:</p>
<blockquote>
<div><p>Suppose that we are statistical consultants hired by a client to provide advice on how to improve sales of a particular product. The <code class="docutils literal notranslate"><span class="pre">advertising</span></code> data set consists of the <code class="docutils literal notranslate"><span class="pre">sales</span></code> of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: <code class="docutils literal notranslate"><span class="pre">TV</span></code>, <code class="docutils literal notranslate"><span class="pre">radio</span></code>, and <code class="docutils literal notranslate"><span class="pre">newspaper</span></code>. […] It is not possible for our client to directly increase sales of the product. On the other hand, they can control the advertising expenditure in each of the three media. Therefore, if we determine that there is an association between advertising and sales, then we can instruct our client to adjust advertising budgets, thereby indirectly increasing sales. In other words, our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets.</p>
</div></blockquote>
<p>This a <a class="reference external" href="https://en.wikipedia.org/wiki/Regression_analysis">regression</a> problem. That is, we want to estimate the relationship between an outcome variable and one or more predictors (or features). We load the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">'advertising.csv'</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th/>
      <th>TV</th>
      <th>radio</th>
      <th>newspaper</th>
      <th>sales</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>230.1</td>
      <td>37.8</td>
      <td>69.2</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>44.5</td>
      <td>39.3</td>
      <td>45.1</td>
      <td>10.4</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17.2</td>
      <td>45.9</td>
      <td>69.3</td>
      <td>9.3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>151.5</td>
      <td>41.3</td>
      <td>58.5</td>
      <td>18.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>180.8</td>
      <td>10.8</td>
      <td>58.4</td>
      <td>12.9</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We will focus for now on the TV budget.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">TV</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'TV'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">sales</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">'sales'</span><span class="p">]</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We make a scatterplot showing the relation between those two quantities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">TV</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'k'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'TV'</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'sales'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/513c36bbfdc95bb1193feb1a81faf6b12f49523a3e5972c61f8ebcbd7f20c997.png" src="../Images/b4db7dd46ce45bb3079256c749e70bf5.png" data-original-src="https://mmids-textbook.github.io/_images/513c36bbfdc95bb1193feb1a81faf6b12f49523a3e5972c61f8ebcbd7f20c997.png"/>
</div>
</div>
<p>There does seem to be a relationship between the two. Roughly a higher TV budget is linked to higher sales, although the correspondence is not perfect. To express the relationship more quantitatively, we seek a function <span class="math notranslate nohighlight">\(f\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
y \approx f(x)
\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> denotes a sample TV budget and <span class="math notranslate nohighlight">\(y\)</span> is the corresponding observed sales. We might posit for instance that there exists a true <span class="math notranslate nohighlight">\(f\)</span> and that each observation is disrupted by some noise <span class="math notranslate nohighlight">\(\varepsilon\)</span></p>
<div class="math notranslate nohighlight">
\[
y = f(x) + \varepsilon.
\]</div>
<p>A natural way to estimate such an <span class="math notranslate nohighlight">\(f\)</span> from data is <a class="reference external" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#k-NN_regression"><span class="math notranslate nohighlight">\(k\)</span>-nearest-neighbors (<span class="math notranslate nohighlight">\(k\)</span>-NN) regression</a><span class="math notranslate nohighlight">\(\idx{k-NN regression}\xdi\)</span>. Let the data be of the form <span class="math notranslate nohighlight">\(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span> where <span class="math notranslate nohighlight">\(\mathbf{x}_i \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(y_i \in \mathbb{R}\)</span>. In our case <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> is the (real-valued) TV budget of the <span class="math notranslate nohighlight">\(i\)</span>-th sample (so <span class="math notranslate nohighlight">\(d=1\)</span>) and <span class="math notranslate nohighlight">\(y_i\)</span> is the corresponding sales. For each <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> (not necessarily in the data), we do the following:</p>
<ul class="simple">
<li><p>find the <span class="math notranslate nohighlight">\(k\)</span> nearest <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>’s to <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></p></li>
<li><p>take an average of the corresponding <span class="math notranslate nohighlight">\(y_i\)</span>’s.</p></li>
</ul>
<p>We implement this method in Python. We use the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.argsort.html"><code class="docutils literal notranslate"><span class="pre">numpy.argsort</span></code></a> to sort an array and the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.absolute.html"><code class="docutils literal notranslate"><span class="pre">numpy.absolute</span></code></a> to compute the absolute deviation. Our quick implementation here assumes that the <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>’s are scalars.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="k">def</span> <span class="nf">knnregression</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">xnew</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">closest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">absolute</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">xnew</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">closest</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">k</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>For <span class="math notranslate nohighlight">\(k=3\)</span> and a grid of <span class="math notranslate nohighlight">\(1000\)</span> points, we get the following approximation <span class="math notranslate nohighlight">\(\hat{f}\)</span>. Here the function <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linspace.html"><code class="docutils literal notranslate"><span class="pre">numpy.linspace</span></code></a> creates an array of equally spaced points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">xgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">TV</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">TV</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="p">[</span><span class="n">knnregression</span><span class="p">(</span><span class="n">TV</span><span class="p">,</span><span class="n">sales</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">xnew</span><span class="p">)</span> <span class="k">for</span> <span class="n">xnew</span> <span class="ow">in</span> <span class="n">xgrid</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">TV</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'TV'</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'sales'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/30b183f214399989ff56602bf85c4b553a4721e53dab5b33add8516d6bcb98ea.png" src="../Images/9d112f3fd54d45fb5d476e64c7a0950c.png" data-original-src="https://mmids-textbook.github.io/_images/30b183f214399989ff56602bf85c4b553a4721e53dab5b33add8516d6bcb98ea.png"/>
</div>
</div>
<p>A higher <span class="math notranslate nohighlight">\(k\)</span> produces something less wiggly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span/><span class="n">k</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">xgrid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">TV</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">TV</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">num</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="p">[</span><span class="n">knnregression</span><span class="p">(</span><span class="n">TV</span><span class="p">,</span><span class="n">sales</span><span class="p">,</span><span class="n">k</span><span class="p">,</span><span class="n">xnew</span><span class="p">)</span> <span class="k">for</span> <span class="n">xnew</span> <span class="ow">in</span> <span class="n">xgrid</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">TV</span><span class="p">,</span> <span class="n">sales</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">'b'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xgrid</span><span class="p">,</span> <span class="n">yhat</span><span class="p">,</span> <span class="s1">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">'TV'</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">'sales'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/7182fdeb5a509496e08f6c00d370ca685553b74659c2124a83e0903a410a97f2.png" src="../Images/26d6f8c976ff1dbb81a5d207dab76ad6.png" data-original-src="https://mmids-textbook.github.io/_images/7182fdeb5a509496e08f6c00d370ca685553b74659c2124a83e0903a410a97f2.png"/>
</div>
</div>
<p>One downside of <span class="math notranslate nohighlight">\(k\)</span>-NN regression is that it does not give an easily interpretable relationship: if I increase my TV budget by <span class="math notranslate nohighlight">\(\Delta\)</span> dollars, how is it expected to affect the sales? Another issue arises in high dimension where the counter-intuitive phenomena we discussed in a previous section can have a significant impact. Recall in particular the <em>High-dimensional Cube Theorem</em>. If we have <span class="math notranslate nohighlight">\(d\)</span> predictors – where <span class="math notranslate nohighlight">\(d\)</span> is large – and our data is distributed uniformly in a bounded region, then any given <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> will be far from any of our data points. In that case, the <span class="math notranslate nohighlight">\(y\)</span>-values of the closest <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span>’s may not be predictive. This is referred to as the <a class="reference external" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of Dimensionality</a><span class="math notranslate nohighlight">\(\idx{curse of dimensionality}\xdi\)</span>.</p>
<p><strong>CHAT &amp; LEARN</strong> Ask your favorite AI chatbot for more details on the Curse of Dimensionality and how it arises in data science. <span class="math notranslate nohighlight">\(\ddagger\)</span></p>
<p>One way out is to make stronger assumptions on the function <span class="math notranslate nohighlight">\(f\)</span>. For instance, we can assume that the true relationship is (approximately) affine, that is, <span class="math notranslate nohighlight">\(y \approx \beta_0 + \beta_1 x\)</span>, or if we have <span class="math notranslate nohighlight">\(d\)</span> predictors</p>
<div class="math notranslate nohighlight">
\[
y \approx \beta_0 + \sum_{j=1}^d \beta_j x_j.
\]</div>
<p>How do we estimate appropriate intercept and coefficients? The standard approach is to minimize the sum of the squared errors</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \left(y_i - \left\{\beta_0 + \sum_{j=1}^d \beta_j (\mathbf{x}_{i})_j\right\}\right)^2,
\]</div>
<p>where <span class="math notranslate nohighlight">\((\mathbf{x}_{i})_j\)</span> is the <span class="math notranslate nohighlight">\(j\)</span>-th entry of input vector <span class="math notranslate nohighlight">\(\mathbf{x}_i \in \mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(y_i \in \mathbb{R}\)</span> is the corresponding <span class="math notranslate nohighlight">\(y\)</span>-value. This is called <a class="reference external" href="https://en.wikipedia.org/wiki/Linear_regression">multiple linear regression</a>.</p>
<p>It is a <a class="reference external" href="https://en.wikipedia.org/wiki/Least_squares">least-squares problem</a>. We re-write it in a more convenient matrix form and combine <span class="math notranslate nohighlight">\(\beta_0\)</span> with the other <span class="math notranslate nohighlight">\(\beta_i\)</span>’s by adding a dummy predictor to each sample. Let</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{y} = 
\begin{pmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{pmatrix},
\quad\quad
A =
\begin{pmatrix}
1 &amp; \mathbf{x}_1^T \\
1 &amp; \mathbf{x}_2^T \\
\vdots &amp; \vdots \\
1 &amp; \mathbf{x}_n^T
\end{pmatrix}
\quad\text{and}\quad
\boldsymbol{\beta} = 
\begin{pmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_d
\end{pmatrix}.
\end{split}\]</div>
<p>Then observe that</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\|\mathbf{y} 
- A \boldsymbol{\beta}\|^2
&amp;= \sum_{i=1}^n \left(y_i - (A \boldsymbol{\beta})_i\right)^2\\
&amp;= \sum_{i=1}^n \left(y_i - \left\{\beta_0 + \sum_{j=1}^d \beta_j (\mathbf{x}_{i})_j\right\}\right)^2.
\end{align*}\]</div>
<p>The linear least-squares problem is then formulated as</p>
<div class="math notranslate nohighlight">
\[
\min_{\boldsymbol{\beta} \in \mathbb{R}^{d+1}} 
\|\mathbf{y} 
- A \boldsymbol{\beta}\|^2.
\]</div>
<p>In words, we are looking for a linear combination of the columns of <span class="math notranslate nohighlight">\(A\)</span> that is closest to <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> in Euclidean distance. Indeed, minimizing the squared Euclidean distance is equivalent to minimizing its square root, as the latter in an increasing function.</p>
<p>One could solve this optimization problem through calculus (and we will come back to this approach later in the book), but understanding the geometric and algebraic structure of the problem turns out to provide powerful insights into its solution – and that of many of problems in data science. It will also be an opportunity to review some basic linear-algebraic concepts along the way.</p>
<p>We will come back to the <code class="docutils literal notranslate"><span class="pre">advertising</span></code> dataset later in the chapter.</p>
    
</body>
</html>