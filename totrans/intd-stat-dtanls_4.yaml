- en: 4 Statistical Inference - Case Study Satisfaction with Government
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://bookdown.org/conradziller/introstatistics/statistical-inference---case-study-satisfaction-with-government.html](https://bookdown.org/conradziller/introstatistics/statistical-inference---case-study-satisfaction-with-government.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 4.1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Western democracies, citizens’ satisfaction with the government is critically
    related to the electoral success of governing parties in an upcoming election.
    At the same time, satisfied citizens are more willing to support non-preferred
    policies and are less likely to support anti-system or populist parties.
  prefs: []
  type: TYPE_NORMAL
- en: If we are interested in the degree of satisfaction with the government in Germany,
    for example, we first need to define the population we aim to make statements
    about (e.g., people over 18 years old who live in Germany). Conducting interviews
    with each person that belongs to the population would be pointless. Instead, we
    can rely on the “shortcut” of random sampling, in which we randomly select observational
    units from this population (i.e., randomly selected Germans) and survey them.
    The idea is that the results (e.g., mean approval of the government using a survey
    item) from the random sample are representative of the underlying population -
    that is, a close match between the parameter estimated from the sample (e.g.,
    mean approval) and the (unobserved) parameter of the population.
  prefs: []
  type: TYPE_NORMAL
- en: '![Federal Chancellery (Bundeskanzleramt)](../Images/9d094f2d7b0391808b604a030ca9f903.png)Federal
    Chancellery (Bundeskanzleramt)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: [https://www.bundesregierung.de/breg-de/bundesregierung/bundeskanzleramt/dienstsitze-bundeskanzleramt-466834](https://www.bundesregierung.de/breg-de/bundesregierung/bundeskanzleramt/dienstsitze-bundeskanzleramt-466834)'
  prefs: []
  type: TYPE_NORMAL
- en: In statistical inference, we are primarily interested in how accurately an estimate
    from a sample reflects the true population parameter. Thus, the goal is to find
    a suitable measure of precision (or uncertainty, as the other “side of the coin”).
    We do this by matching the estimate from the sample with a test distribution that
    reflects likely or unlikely values in terms of probabilities. How this is done
    will be discussed in this case study.
  prefs: []
  type: TYPE_NORMAL
- en: Before that, we take a short excursion into the realm of probability theory.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Probabilities correspond to the ratio of the number of observations that meet
    a certain criterion (i.e., an “event”) to the total number of observations. A
    classic example is the coin toss and the ratio between heads and the total number
    of tosses. Probabilities are always positive (\(P(A)\ge0\)). The probability of
    all possible events (e.g., heads and tails in the case of a coin toss) is 1 (\(P(\Omega)=1\)).
    If events A and B cannot occur simultaneously, then the probability of A or B
    occurring is the summed probability for both events (\(P(A or B)=P(A)+P(B)\)).
  prefs: []
  type: TYPE_NORMAL
- en: The probabilities of events that occur in a chance experiment (e.g., a coin
    toss) can be represented as so-called random variables. Random variables have
    a probability distribution, for binary variables (0/1) this is the Bernoulli distribution,
    for continuous variables this is the normal distribution. (There are other distributions
    like the F-distribution or chi-square distribution, which we will use later for
    further purposes).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we see a probability distribution for 10 balls (7 blue, 3 red) randomly
    drawn with replacement, in total 10,000 draws were made. Accordingly, the probability
    distribution is approximately 0.7 for blue and 0.3 for red:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/503726bbe43a1bb8173b7e34e6f07530.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Bernoulli distribution is characterized by the probability of occurrence
    of an event \(p\). The probability for the counter-event automatically follows
    from this: \(1-p\). **We will need this distribution for the calculation of test
    statistics of proportions.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The normal distribution for random variables is a bell-shaped probability density
    function. It gives the probabilities for a **continuous random variable** with
    infinite intermediate values. Think of time as a continuous concept. Theoretically,
    we can measure time in infinitely small units (milliseconds, nanoseconds, etc.).
    However, in applied research, we would assign discrete values to the variable
    time. Hence, continuous variables are rather a theoretical concept. Regarding
    the distribution of a continuous random variable: The normal distribution is characterized
    by a mean \(\mu\) and a standard deviation \(\sigma\). The notation for the normal
    distribution of a random variable is \(X \sim N(\mu,\sigma)\). Each combination
    of \(\mu\) and \(\sigma\) gives a differently shaped normal distribution. The
    mean indicates where the center of the distribution is on the x-axis and \(\sigma\)
    indicates how flat or steep the distribution is (the higher, the more spread out
    and thus the flatter the curve becomes).'
  prefs: []
  type: TYPE_NORMAL
- en: The so-called *standard normal distribution* has a mean of 0 and a standard
    deviation of 1 (\(X \sim N(0,1)\)). As with all other normal distributions, (1)
    the ends of the curve spread out to the left and right, approaching the x-axis
    without ever touching it. Thus, the values can range from - to + infinity. (2)
    The area under the curve is equal to 1 in total. (3) The so-called “Empirical
    Rule” states that for a standard normal distribution, 68% of the observations
    lie between -1 and +1 standard deviations. 95% of the observations lie between
    approx. -2 and +2 standard deviations (to be exact -1.96 and +1.96), and 99.7%
    of the observations lie between approximately -3 and +3 standard deviations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7e4896d3aa2bb86af5da0464107f5bff.png)'
  prefs: []
  type: TYPE_IMG
- en: With probability distributions such as the standard normal distribution, we
    do not calculate the probability of a specific value but rather a range of values.
    For example, the probability that a value is greater than 1.96 standard deviations,
    i.e., \(P(Z\ge1.96)\), is 2.5%. And the probability that a value is less than
    -1.96 standard deviations, i.e., \(P(Z\le-1.96)\), is 2.5% (see the grey areas
    in the plot above, both areas sum up to 5%).
  prefs: []
  type: TYPE_NORMAL
- en: '**We will use the standard normal distribution (also referred to as Z-distribution)
    for various statistical tests. The rule that 95% of observable values are located
    between -1.96 and 1.96, and 5% at the outer margins will also become important
    in Null Hypothesis Testing.**'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Basics of statistical inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Inference means that we use facts we know to learn about facts we don’t know
    (yet). Statistical inference means that we take information from a (random) sample
    and infer from it (not yet known) properties of the population (from which the
    sample was drawn). Basically, only random samples are suitable for this purpose,
    since they represent the underlying population in all properties in the best possible
    way (plus minus some random variation).
  prefs: []
  type: TYPE_NORMAL
- en: While random samples are the best way to conduct statistical inference, the
    process is not flawless. There is still a chance that the units in our sample
    do not perfectly match all properties of the population due to random deviations.
    It may be that a sample was drawn in which disproportionately many women accidentally
    “slipped in”. These random deviations are called **sampling variability**. The
    following sections should illustrate that sampling variability decreases with
    the number of observations in a sample (“more data is better”) and that if we
    did the process of random sampling over and over again (with a sufficiently large
    number of observations), we would obtain a distribution of sampled estimates that
    are shaped like a normal distribution. This will be tremendously useful when we
    conduct statistical tests, in which we use the normal distribution as if it was
    the sampling distribution of a quantity of interest we can match and test our
    estimate with.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Law of Large Numbers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, a small number of cases leads to higher sampling variability. You
    may be familiar with this from the coin toss experiment. A coin is often tossed
    in succession. After 10 tosses, it is quite possible that heads were tossed 7
    times and tails only 3 times. Nevertheless, the underlying probability is always
    0.5/0.5 (or 50%/50%) and in the population of all coin tosses, heads and tails
    should occur equally often. If we now toss the coin even more often, after 30,
    300, 3000, etc. the observed coin tosses will increasingly match the 0.5/0.5 probability
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The law of large numbers states that as the sample size increases, the mean
    of the sample will increasingly approxiamte that of the population. (Eventually,
    the sample would be identical to the entire population.) This means that more
    data is better, and the larger our sample, the more confident we can be that it
    matches or at least approximates the population parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '![Law of large numbers: Coin toss](../Images/e067d3d1c6e50c1aaeedc4b319f705e7.png)Law
    of large numbers: Coin toss'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://link.springer.com/chapter/10.1007/978-3-030-45553-8_5](https://link.springer.com/chapter/10.1007/978-3-030-45553-8_5)'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Central Limit Theorem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we draw many (or even infinite) samples from a population and are interested
    in, for example, the mean of satisfaction with government, then plotting the means
    of each sample as a histogram will lead to a distribution that (with added samples
    increasingly) follows a normal distribution. The mean of the sample means thereby
    corresponds to the population parameter we are essentially interested in (i.e.,
    satisfaction with the government of all over-18-year-olds living in Germany).
  prefs: []
  type: TYPE_NORMAL
- en: The implications of the central limit theorem are illustrated in the following
    figure. The mean of the means from 100,000 simulated samples is 4.2 and corresponds
    to the “true” population mean of government satisfaction. Note that in reality,
    we do not have this information and we are not able to draw these many samples
    as that would be too costly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/78fad6f32fbeef73ffde823b7f7cf4d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Another property of the central limit theorem is that the larger the number
    of cases (\(n\)) of each of the individual samples, the better the distribution
    of sample estimates corresponds to a normal distribution. This works very well
    with \(n>30\). Moreover, it is not important how the distribution of the characteristic
    to be examined is in the population (e.g., right- or left-skewed). In any case,
    with increasing samples (and \(n>30\)), the distribution of sample estimates will
    (with added samples increasingly) correspond to a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.3 Why statistical laws are important
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.3.3.1 Only one sample
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We calculate everything on the basis of **one sample**, everything else would
    be too costly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can never be completely sure that the ONE specific sample and the statistics
    estimated using it correspond to those of the population, but:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **larger the sample**, the steeper the distribution of sample estimates
    and the more certain we are to get an estimated parameter **near the population
    parameter**, even with only one sample (Law of Large Numbers).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For a **sample of 30 or more observations**, we know that the distribution of
    sample estimates is normal no matter how the distribution of the population looks
    like; this gives us the rationale that most samples mingle around the population
    mean and that there is quite a high probability that the estimate from our one
    sample **is near the center of the distribution (i.e., close to the population
    parameter)**; getting an estimate that is far away from the center of the distribution
    is rather unlikely (Central Limit Theorem).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.3.3.2 A Primer on Null Hypothesis Testing
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Central Limit Theorem also gives us a rationale for **quantifying uncertainty**,
    because if repeated sample estimates can be represented as a **normal distribution**,
    then we can use the normal distribution as a frame of reference for how credible
    the results from our one sample are.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To do so, we make specific assumptions about the distribution we test our sample
    estimate against (e.g., a distribution that assumes no difference in means between
    two groups).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This procedure is called Null Hypothesis Testing (or, NHT):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We develop a research hypothesis or a so-called **alternative hypothesis (e.g.,
    women are more satisfied with the government than men)**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a random sample and estimate the difference in means of government satisfaction
    between men and women.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We test our result against a (theoretical) distribution under which we assume
    the **null hypothesis (-> there is NO difference in government satisfaction between
    men and women)** to be true (i.e., the test distribution).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the estimate from our sample is located near the center of the distribution
    that assumes the null hypothesis, this would be not much evidence for the alternative
    hypothesis; however: If the estimate is at the margins of that distribution, we
    would conclude that there is evidence that favor the alternative hypothesis, which
    states that **in the population, women are more satisfied with the government
    than men**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The prerequisite for this testing procedure is that we standardize our sample
    estimate to make it comparable to the test distribution (For example, this can
    be achieved by: \(z = \frac{estimate}{{standard error}}\)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.3.3.3 Standard error
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To determine the variation around a population parameter, we could use the standard
    deviation of the sampling distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, since in the applied case we do not have many samples, but only one,
    we need to estimate this variation from the one sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This estimated variation of an estimate is called the standard error, and the
    formula for calculating it differs depending on the estimator (usually we normalize
    a **measure of variance or standard deviation** using a measure that involves
    the **number of observations**; for example, the standard error of mean the formula
    is given as: \(\bar x = \frac{s}{\sqrt{n}}\)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4.4 Confidence intervals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.4.1 Basic idea
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In theory, the confidence intervals (CIs) for a point estimate (e.g., a mean
    or a correlation coefficient) shows a range of values that contain the true population
    parameter with a certain probability. It is thus a measure of uncertainty (or,
    conversely, precision) of an estimate. There are conventionally three types of
    confidence intervals: 90%, 95%, and 99% confidence intervals. These three types
    reflect the level of probability that the true population value lies within the
    interval for a very large number of samples. For example, if one is interested
    in the population mean and conducts repeated (“infinite”) sampling, 95% of the
    confidence intervals of the respective samples would contain the true population
    mean, and 5% would not contain it.'
  prefs: []
  type: TYPE_NORMAL
- en: This can be illustrated with the following figures.
  prefs: []
  type: TYPE_NORMAL
- en: '![Confidence interval of sample means](../Images/f3474453e6d0423693806d67d7090428.png)Confidence
    interval of sample means'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://seeing-theory.brown.edu/frequentist-inference/index.html](https://seeing-theory.brown.edu/frequentist-inference/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: In the figure, samples were repeatedly drawn from a population with five observations
    each (orange dots). Each time, the mean is calculated (or estimated because we
    are using a sample). These point estimates of the mean from each sample are represented
    by the green or red dots, the confidence interval is the corresponding line to
    the left and right of it. For the green estimates, the population mean (dashed
    line) is within the confidence interval, for the red it is not.
  prefs: []
  type: TYPE_NORMAL
- en: '![Confidence intervals of sample correlations](../Images/e8646d9f0396c286662592c32225e9da.png)Confidence
    intervals of sample correlations'
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [https://github.com/simonheb/metrics1-public](https://github.com/simonheb/metrics1-public)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, instead of the mean value, results of a bivariate regression (similar
    to a correlation) are shown. The bivariate relationship between x and y in the
    population is represented by the black slope line (regression coefficient \(\beta_1=1.2\)).
    The gray dots are the units in the population. From this population, four random
    samples are drawn and the observations in the sample are represented by the red
    dots. The point estimate of the correlation is represented by the red slope line.
    The confidence interval is the light red area around the estimate. In samples
    1, 2, and 4, the confidence interval includes the population parameter; only in
    sample 3 (bottom left) is this no longer the case for parts of the confidence
    interval. Analogously to the thought experiment regarding means above, 95% of
    estimates from repeated samples would include the population mean, and 5% would
    not.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2 How to calculate and interpret the confidence interval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The idea of confidence intervals refers to the sampling distribution, i.e. many
    (up to infinitely many) repeated random samples from the population. For a 95%
    confidence interval, the interpretation would be: for repeated samples, 95% of
    the samples will contain the population parameter, 5% will not.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we only have one sample and can therefore only calculate confidence
    intervals once. Whether the population parameter actually falls within the value
    range of the confidence interval of our one sample or not cannot be answered conclusively.
    What can be said is that the likelihood of this is high. For example, with the
    95% confidence interval, 95 out of 100 samples would have to contain the population
    parameter and it is likely that our one sample is one of the 95 and not one of
    the 5\. (At the same time, some interpretations made in textbooks such as “with
    95% probability the population parameter is in the interval” are strictly speaking
    not true).
  prefs: []
  type: TYPE_NORMAL
- en: We can therefore interpret the confidence intervals primarily as **the likely
    or plausible range of values of the unknown population parameter we are interested
    in (given the characteristics of our sample).** The confidence interval thus gives
    us, at the same time as the point estimate, a range of variability to be expected
    if we were to repeat studies similar to ours more often. This is indeed important
    information, and for many researchers, it is also more intuitive than significance
    values from null hypothesis tests.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there is a relationship between confidence intervals and statistical
    significance, a concept related to null hypothesis tests. We will look at this
    relationship at the end of this case study. In short, confidence intervals show
    a possible range of values of the population parameter *and* at the same time
    provide information about statistical significance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute a confidence interval, we generally use the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: \(point estimate \pm z_\alpha \times standard error\).
  prefs: []
  type: TYPE_NORMAL
- en: \(z_\alpha\) refers to the critical Z-value (i.e., a given value of the z-distribution
    that corresponds a set probability of error, this will be explained in greater
    detail below).
  prefs: []
  type: TYPE_NORMAL
- en: 4.4.2.1 Mean
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the lower and upper 95% confidence intervals of a mean, we use: \(95\%
    CIs=[\bar x - 1.96 \times {\frac{s}{\sqrt n}}, \bar x + 1.96 \times {\frac{s}{\sqrt
    n}}]\).'
  prefs: []
  type: TYPE_NORMAL
- en: Let us look into this with real data from the European Social Survey (German
    sample). First, we read the data and get an overview of the distribution of the
    variable satisfaction with government `stfgov`. `stfgov` was surveyed using an
    11-point scale ranging from 0 “extremely dissatisfied” to 10 “extremely satisfied.”
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8f622a96d1c89b332e19274bde795f99.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we calculate the 95% confidence interval in several steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Computing confidence intervals is even easier with the “t.test” command. While
    we are interested in the z-distribution, t- and z-distribution correspond to each
    other when the number of observations is high enough (usually already >30).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question:** Interpret the confidence interval.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Your answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: <details><summary>Solution:</summary>
  prefs: []
  type: TYPE_NORMAL
- en: Typically, it is easier to start with formulating the alternative hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: '\(H_A\): High-income individuals are more satisfied with government than low-income
    individuals (i.e., positive relationship between income and satisfaction with
    government).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal: \(\mu(satisfaction|highincome)>\mu(satisfaction|lowincome)\) or (equivalent)
    \(\mu(satisfaction|highincome)-\mu(satisfaction|lowincome)>0\)'
  prefs: []
  type: TYPE_NORMAL
- en: '\(H_0\): High-income individuals are equally or less satisfied with government
    than low-income individuals (i.e., null or negative relationship between income
    and satisfaction with government).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal: \(\mu(satisfaction|highincome)\le\mu(satisfaction|lowincome)\) or (equivalent)
    \(\mu(satisfaction|highincome)-\mu(satisfaction|lowincome)\le0\)'
  prefs: []
  type: TYPE_NORMAL
- en: Note that it is important that all possible outcomes are covered by both hypotheses.
    This means that when formulating a directed hypothesis, the null hypothesis must
    include the case of no relationship (i.e., “\(\le\)” instead of just “\(<\)”).</details>
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us then assume an error probability of 5% or 0.05 (Step 2). Next, we calculate
    the test statistic (Step 3). Generally, this is given as \(\frac{estimate - true
    value}{standard error}\). Since we test against the null hypothesis, the true
    value is 0\. The formula for the so-called z-test statistic is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: \(z=\frac{Estimation}{Standard Error}\).
  prefs: []
  type: TYPE_NORMAL
- en: 'Steps 4 and 5 are about the actual test. Remember that per the alternative
    hypothesis, we assumed a positive mean difference (i.e., high-income earners are
    more satisfied with the government than low-income earners). Next, we want to
    know where the mean difference is located on the null distribution. If it is close
    to the center of the distribution, we have no reason to reject the null hypothesis.
    However, if it is at the margins of the distribution, this would provide a reason
    for rejecting the null hypothesis (and favoring the alternative hypothesis). We
    use the z-distribution with a mean of 0 and a standard deviation of 1\. Remember
    the properties of this distribution from above. If we set the critical value to
    be lower than -1.96 and higher than +1.96, we would be in the 5% range of the
    distribution if our test statistic exceeds those critical values. If this was
    the case, we can conclude that it would be very unlikely to sample such a value
    even though \(H_0\) holds. Note that we set up a positive and negative area of
    rejection if we formulate undirected hypotheses (i.e., \(H_A\): the mean difference
    can be higher or lower than zero, \(H_0\): the mean difference equals zero) and
    conduct two-sided tests. For a directed hypothesis (and one-sided tests), like
    in our example, the rejection area is only in the negative or positive range of
    the distribution (depending on whether \(H_A\) expects a pos. or neg. statistic).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/adbfd17d562ccc396d1fe0c9bea7d598.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/03648c89dbdb157687b7943bf457c54f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/faa33b90ef9f0d91a5d9bbba48e44b09.png)'
  prefs: []
  type: TYPE_IMG
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question:** How would the rejection area change if we test one-sided? What
    would change if we set a 1% probability of error (two-sided test)?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Your answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: <details><summary>Solution:</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '\(H_A\): Verbal: Income is either positively or negatively related to satisfaction
    with government.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal: \(r \ne 0\)'
  prefs: []
  type: TYPE_NORMAL
- en: '\(H_0\): Verbal: Income is unrelated to satisfaction with government.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Formal: \(r = 0\)</details>'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: We now calculate the test statistic and the p-value.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '**Question:** Interpret the result of the statistical test (two-sided test)
    with reference to the t-value and p-value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Your answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
