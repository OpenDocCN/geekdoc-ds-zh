- en: High-level language support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://enccs.github.io/gpu-programming/9-language-support/](https://enccs.github.io/gpu-programming/9-language-support/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*[GPU programming: why, when and how?](../)* **   High-level language support'
  prefs: []
  type: TYPE_NORMAL
- en: '[Edit on GitHub](https://github.com/ENCCS/gpu-programming/blob/main/content/9-language-support.rst)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs: []
  type: TYPE_NORMAL
- en: Can I port code in high-level languages to run on GPUs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: Get an overview of libraries for GPU programming in Python and Julia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 40 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Julia has first-class support for GPU programming through the following packages
    that target GPUs from all three major vendors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CUDA.jl` is the most mature, `AMDGPU.jl` is somewhat behind but still ready
    for general use, while `oneAPI.jl` and `Metal.jl` are functional but might contain
    bugs, miss some features and provide suboptimal performance.'
  prefs: []
  type: TYPE_NORMAL
- en: The APIs of these libraries are completely analogous and translation between
    them is normally straightforward. The libraries offer both user-friendly **high-level
    abstractions** (the array interface and higher-level abstractions) that require
    little programming effort, and a **lower level** approach for writing kernels
    for fine-grained control.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing these packages is done with the Julia package manager:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing `CUDA.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing `AMDGPU.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing `oneAPI.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing `Metal.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the Julia GPU stack, one needs to have the relevant GPU drivers and
    programming toolkits installed. GPU drivers are already installed on HPC systems
    while on your own machine you will need to install them yourself (see e.g. these
    [instructions from NVIDIA](https://www.nvidia.com/Download/index.aspx)). Programming
    toolkits for CUDA can be installed automatically through Julia’s artifact system
    upon the first usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The array interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GPU programming with Julia can be as simple as using a different array type
    instead of regular `Base.Array` arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CuArray` from CUDA.jl for NVIDIA GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ROCArray` from AMDGPU.jl for AMD GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`oneArray` from oneAPI.jl for Intel GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MtlArray` from Metal.jl for Apple GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These array types closely resemble `Base.Array` which enables us to write generic
    code which works on both types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code copies an array to the GPU and executes a simple operation
    on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Moving an array back from the GPU to the CPU is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s have a look at a more realistic example: matrix multiplication. We create
    two random arrays, one on the CPU and one on the GPU, and compare the performance
    using the [BenchmarkTools package](https://github.com/JuliaCI/BenchmarkTools.jl):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Vendor libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Support for using GPU vendor libraries from Julia is currently most mature
    on NVIDIA GPUs. NVIDIA libraries contain precompiled kernels for common operations
    like matrix multiplication (cuBLAS), fast Fourier transforms (cuFFT), linear solvers
    (cuSOLVER), etc. These kernels are wrapped in `CUDA.jl` and can be used directly
    with `CuArrays`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`AMDGPU.jl` currently supports some of the ROCm libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: rocBLAS for BLAS support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rocFFT for FFT support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rocRAND for RNG support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MIOpen for DNN support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher-order abstractions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A powerful way to program GPUs with arrays is through Julia’s higher-order
    array abstractions. The simple element-wise addition we saw above, `a .+= 1`,
    is an example of this, but more general constructs can be created with `broadcast`,
    `map`, `reduce`, `accumulate` etc:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Writing your own kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not all algorithms can be made to work with the higher-level abstractions in
    `CUDA.jl`. In such cases it’s necessary to explicitly write our own GPU kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to writing kernels in CUDA or HIP, we use a special function to return
    the index of the GPU thread which executes it (e.g., `threadIdx().x` for NVIDIA
    and `workitemIdx().x` for AMD), and two additional functions to parallelise over
    multiple blocks (e.g., `blockDim().x()` and `blockIdx().x()` for NVIDIA, and `workgroupDim().x()`
    and `workgroupIdx().x()` for AMD).
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s an example of vector addition kernels for NVIDIA, AMD, Intel and Apple
    GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Restrictions in kernel programming
  prefs: []
  type: TYPE_NORMAL
- en: Within kernels, most of the Julia language is supported with the exception of
    functionality that requires the Julia runtime library. This means one cannot allocate
    memory or perform dynamic function calls, both of which are easy to do accidentally!
  prefs: []
  type: TYPE_NORMAL
- en: 1D, 2D and 3D
  prefs: []
  type: TYPE_NORMAL
- en: CUDA.jl and AMDGPU.jl support indexing in up to 3 dimensions (x, y and z, e.g.
    `threadIdx().x` and `workitemIdx().x`). This is convenient for multidimensional
    data where thread blocks can be organised into 1D, 2D or 3D arrays of threads.
  prefs: []
  type: TYPE_NORMAL
- en: Writing protable kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    allows you to write generic GPU code and run it on GPUs from Nvidia, AMD, Intel
    or Apple, similar to alpaka and Kokkos for C++. The backend is the object that
    decides where the code will be executed. A specific backend such as `ROCBackend()`
    becomes available when the corresponding package is loaded.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There has been a lot of progress in GPU programming using Python and the ecosystem
    is still evolving. There are a couple of options available to work with GPU.
  prefs: []
  type: TYPE_NORMAL
- en: CuPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CuPy is a NumPy/SciPy-compatible data array library used on GPU. It has been
    developed for NVIDIA GPUs but as experimental support for AMD GPUs. CuPy has a
    highly compatible interface with NumPy and SciPy. As stated on its official website,
    “All you need to do is just replace `numpy` and `scipy` with `cupy` and `cupyx.scipy`
    in your Python code.” If you know NumPy, CuPy is a very easy way to get started
    on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: cuDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAPIDS is a high level packages collections which implement CUDA functionalities
    and API with Python bindings. It only supports NVIDIA GPUs. cuDF belongs to RAPIDS
    and is the library for manipulating data frames on GPU. cuDF provides a pandas-like
    API, so if you are familiar with Pandas, you can accelerate your work without
    knowing too much CUDA programming.
  prefs: []
  type: TYPE_NORMAL
- en: PyCUDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyCUDA is a Python programming environment for CUDA. It allows users to access
    to NVIDIA’s CUDA API from Python. PyCUDA is powerful library but only runs on
    NVIDIA GPUs. Knowledge of CUDA programming is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Numba
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Numba allows users to just-in-time (JIT) compile Python code to run fast on
    CPUs, but can also be used for JIT compiling for GPUs. In the following we will
    focus on using Numba, which supports GPUs from both NVIDIA and AMD.
  prefs: []
  type: TYPE_NORMAL
- en: Using Numba in AMD GPUs
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Numba with AMD GPUs `numba-hip` extension can be used. By adding the
    following lines in the beginning of the code, a single-source code can be made
    to work in both Nvidia and AMD GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Read more how to install and use it [here](https://github.com/ROCm/numba-hip).
  prefs: []
  type: TYPE_NORMAL
- en: Numba supports GPU programming by directly compiling a restricted subset of
    Python code into kernels and device functions following the execution model. Kernels
    written in Numba appear to have direct access to NumPy arrays. NumPy arrays are
    transferred between the CPU and the GPU automatically.
  prefs: []
  type: TYPE_NORMAL
- en: ufunc (gufunc) decorator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using ufuncs (and generalized ufuncs) is the easiest way to run on a GPU with
    Numba, and it requires minimal understanding of GPU programming. Numba `@vectorize`
    will produce a ufunc-like object. This object is a close analog but not fully
    compatible with a regular NumPy ufunc. Generating a ufunc for GPU requires the
    explicit type signature and target attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Demo: Numba ufunc'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simple mathematical problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark, first initialize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Numba `@vectorize` is limited to scalar arguments in the core function, for
    multi-dimensional arrays arguments, `@guvectorize` is used. Consider the following
    example which does matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: One should never implement things like matrix multiplication by oneself since
    there are plenty of highly optimized libraries available!
  prefs: []
  type: TYPE_NORMAL
- en: Numba gufunc
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark, first, initialize some arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Numba automatically did a lot of things for us:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory was allocated on GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was copied from CPU and GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel was configured and launched
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was copied back from GPU to CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ufuncs (or gfuncs) for GPU processing can be straightforward, but this
    approach may not always yield optimal performance due to automatic handling of
    data transfer to and from the GPU, as well as kernel launching. Additionally,
    in practice, not every function can be constructed as a ufunc.
  prefs: []
  type: TYPE_NORMAL
- en: To gain greater control and flexibility, one may need to craft their own kernels
    and manually manage data transfer. Refer to the *Python for HPDA* resource linked
    below for guidance on implementing such techniques using Numba.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Play around yourself
  prefs: []
  type: TYPE_NORMAL
- en: Are you a Julian or a Pythonista? Maybe neither, but take a pick between Python
    and Julia and play around with the code examples provided above.
  prefs: []
  type: TYPE_NORMAL
- en: You can find instructions for running Julia on LUMI and Python on LUMI / Google
    Colab in the [Setup](../0-setup/) episode.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Introduction to programming in Julia (ENCCS)](https://enccs.github.io/julia-intro/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Julia for High-Performance Scientific Computing (ENCCS)](https://enccs.github.io/julia-for-hpc/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Julia for high-performance data analytics (ENCCS)](https://enccs.github.io/julia-for-hpda/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to running R, Python, Julia, and Matlab in HPC (NAISS-LUNARC-HPC2N-UPPMAX)](https://uppmax.github.io/R-python-julia-matlab-HPC/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[High Performance Data Analytics in Python (ENCCS)](https://enccs.github.io/hpda-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Practical Intro to GPU Programming using Python (ENCCS)](https://github.com/ENCCS/webinar_documents/tree/main/2024-oct-24-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Python in an HPC environment (UPPMAX-HPC2N)](https://uppmax.github.io/HPC-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python for Scientific Computing (Aalto Scientific Computing)](https://aaltoscicomp.github.io/python-for-scicomp/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Previous](../8-portable-kernel-models/ "Portable kernel-based models") [Next](../10-multiple_gpu/
    "Multiple GPU programming with MPI")'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: © Copyright 2023-2024, The contributors.
  prefs: []
  type: TYPE_NORMAL
- en: Built with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme)
    provided by [Read the Docs](https://readthedocs.org).
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs: []
  type: TYPE_NORMAL
- en: Can I port code in high-level languages to run on GPUs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objectives
  prefs: []
  type: TYPE_NORMAL
- en: Get an overview of libraries for GPU programming in Python and Julia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instructor note
  prefs: []
  type: TYPE_NORMAL
- en: 40 min teaching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20 min exercises
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Julia has first-class support for GPU programming through the following packages
    that target GPUs from all three major vendors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CUDA.jl` is the most mature, `AMDGPU.jl` is somewhat behind but still ready
    for general use, while `oneAPI.jl` and `Metal.jl` are functional but might contain
    bugs, miss some features and provide suboptimal performance.'
  prefs: []
  type: TYPE_NORMAL
- en: The APIs of these libraries are completely analogous and translation between
    them is normally straightforward. The libraries offer both user-friendly **high-level
    abstractions** (the array interface and higher-level abstractions) that require
    little programming effort, and a **lower level** approach for writing kernels
    for fine-grained control.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing these packages is done with the Julia package manager:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing `CUDA.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing `AMDGPU.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing `oneAPI.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing `Metal.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the Julia GPU stack, one needs to have the relevant GPU drivers and
    programming toolkits installed. GPU drivers are already installed on HPC systems
    while on your own machine you will need to install them yourself (see e.g. these
    [instructions from NVIDIA](https://www.nvidia.com/Download/index.aspx)). Programming
    toolkits for CUDA can be installed automatically through Julia’s artifact system
    upon the first usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: The array interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GPU programming with Julia can be as simple as using a different array type
    instead of regular `Base.Array` arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CuArray` from CUDA.jl for NVIDIA GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ROCArray` from AMDGPU.jl for AMD GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`oneArray` from oneAPI.jl for Intel GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MtlArray` from Metal.jl for Apple GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These array types closely resemble `Base.Array` which enables us to write generic
    code which works on both types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code copies an array to the GPU and executes a simple operation
    on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Moving an array back from the GPU to the CPU is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s have a look at a more realistic example: matrix multiplication. We create
    two random arrays, one on the CPU and one on the GPU, and compare the performance
    using the [BenchmarkTools package](https://github.com/JuliaCI/BenchmarkTools.jl):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Vendor libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Support for using GPU vendor libraries from Julia is currently most mature
    on NVIDIA GPUs. NVIDIA libraries contain precompiled kernels for common operations
    like matrix multiplication (cuBLAS), fast Fourier transforms (cuFFT), linear solvers
    (cuSOLVER), etc. These kernels are wrapped in `CUDA.jl` and can be used directly
    with `CuArrays`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '`AMDGPU.jl` currently supports some of the ROCm libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: rocBLAS for BLAS support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rocFFT for FFT support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rocRAND for RNG support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MIOpen for DNN support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher-order abstractions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A powerful way to program GPUs with arrays is through Julia’s higher-order
    array abstractions. The simple element-wise addition we saw above, `a .+= 1`,
    is an example of this, but more general constructs can be created with `broadcast`,
    `map`, `reduce`, `accumulate` etc:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Writing your own kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not all algorithms can be made to work with the higher-level abstractions in
    `CUDA.jl`. In such cases it’s necessary to explicitly write our own GPU kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to writing kernels in CUDA or HIP, we use a special function to return
    the index of the GPU thread which executes it (e.g., `threadIdx().x` for NVIDIA
    and `workitemIdx().x` for AMD), and two additional functions to parallelise over
    multiple blocks (e.g., `blockDim().x()` and `blockIdx().x()` for NVIDIA, and `workgroupDim().x()`
    and `workgroupIdx().x()` for AMD).
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s an example of vector addition kernels for NVIDIA, AMD, Intel and Apple
    GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Restrictions in kernel programming
  prefs: []
  type: TYPE_NORMAL
- en: Within kernels, most of the Julia language is supported with the exception of
    functionality that requires the Julia runtime library. This means one cannot allocate
    memory or perform dynamic function calls, both of which are easy to do accidentally!
  prefs: []
  type: TYPE_NORMAL
- en: 1D, 2D and 3D
  prefs: []
  type: TYPE_NORMAL
- en: CUDA.jl and AMDGPU.jl support indexing in up to 3 dimensions (x, y and z, e.g.
    `threadIdx().x` and `workitemIdx().x`). This is convenient for multidimensional
    data where thread blocks can be organised into 1D, 2D or 3D arrays of threads.
  prefs: []
  type: TYPE_NORMAL
- en: Writing protable kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    allows you to write generic GPU code and run it on GPUs from Nvidia, AMD, Intel
    or Apple, similar to alpaka and Kokkos for C++. The backend is the object that
    decides where the code will be executed. A specific backend such as `ROCBackend()`
    becomes available when the corresponding package is loaded.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There has been a lot of progress in GPU programming using Python and the ecosystem
    is still evolving. There are a couple of options available to work with GPU.
  prefs: []
  type: TYPE_NORMAL
- en: CuPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CuPy is a NumPy/SciPy-compatible data array library used on GPU. It has been
    developed for NVIDIA GPUs but as experimental support for AMD GPUs. CuPy has a
    highly compatible interface with NumPy and SciPy. As stated on its official website,
    “All you need to do is just replace `numpy` and `scipy` with `cupy` and `cupyx.scipy`
    in your Python code.” If you know NumPy, CuPy is a very easy way to get started
    on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: cuDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAPIDS is a high level packages collections which implement CUDA functionalities
    and API with Python bindings. It only supports NVIDIA GPUs. cuDF belongs to RAPIDS
    and is the library for manipulating data frames on GPU. cuDF provides a pandas-like
    API, so if you are familiar with Pandas, you can accelerate your work without
    knowing too much CUDA programming.
  prefs: []
  type: TYPE_NORMAL
- en: PyCUDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyCUDA is a Python programming environment for CUDA. It allows users to access
    to NVIDIA’s CUDA API from Python. PyCUDA is powerful library but only runs on
    NVIDIA GPUs. Knowledge of CUDA programming is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Numba
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Numba allows users to just-in-time (JIT) compile Python code to run fast on
    CPUs, but can also be used for JIT compiling for GPUs. In the following we will
    focus on using Numba, which supports GPUs from both NVIDIA and AMD.
  prefs: []
  type: TYPE_NORMAL
- en: Using Numba in AMD GPUs
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Numba with AMD GPUs `numba-hip` extension can be used. By adding the
    following lines in the beginning of the code, a single-source code can be made
    to work in both Nvidia and AMD GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Read more how to install and use it [here](https://github.com/ROCm/numba-hip).
  prefs: []
  type: TYPE_NORMAL
- en: Numba supports GPU programming by directly compiling a restricted subset of
    Python code into kernels and device functions following the execution model. Kernels
    written in Numba appear to have direct access to NumPy arrays. NumPy arrays are
    transferred between the CPU and the GPU automatically.
  prefs: []
  type: TYPE_NORMAL
- en: ufunc (gufunc) decorator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using ufuncs (and generalized ufuncs) is the easiest way to run on a GPU with
    Numba, and it requires minimal understanding of GPU programming. Numba `@vectorize`
    will produce a ufunc-like object. This object is a close analog but not fully
    compatible with a regular NumPy ufunc. Generating a ufunc for GPU requires the
    explicit type signature and target attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Demo: Numba ufunc'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simple mathematical problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark, first initialize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Numba `@vectorize` is limited to scalar arguments in the core function, for
    multi-dimensional arrays arguments, `@guvectorize` is used. Consider the following
    example which does matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: One should never implement things like matrix multiplication by oneself since
    there are plenty of highly optimized libraries available!
  prefs: []
  type: TYPE_NORMAL
- en: Numba gufunc
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark, first, initialize some arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Numba automatically did a lot of things for us:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory was allocated on GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was copied from CPU and GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel was configured and launched
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was copied back from GPU to CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ufuncs (or gfuncs) for GPU processing can be straightforward, but this
    approach may not always yield optimal performance due to automatic handling of
    data transfer to and from the GPU, as well as kernel launching. Additionally,
    in practice, not every function can be constructed as a ufunc.
  prefs: []
  type: TYPE_NORMAL
- en: To gain greater control and flexibility, one may need to craft their own kernels
    and manually manage data transfer. Refer to the *Python for HPDA* resource linked
    below for guidance on implementing such techniques using Numba.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Play around yourself
  prefs: []
  type: TYPE_NORMAL
- en: Are you a Julian or a Pythonista? Maybe neither, but take a pick between Python
    and Julia and play around with the code examples provided above.
  prefs: []
  type: TYPE_NORMAL
- en: You can find instructions for running Julia on LUMI and Python on LUMI / Google
    Colab in the [Setup](../0-setup/) episode.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Introduction to programming in Julia (ENCCS)](https://enccs.github.io/julia-intro/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Julia for High-Performance Scientific Computing (ENCCS)](https://enccs.github.io/julia-for-hpc/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Julia for high-performance data analytics (ENCCS)](https://enccs.github.io/julia-for-hpda/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to running R, Python, Julia, and Matlab in HPC (NAISS-LUNARC-HPC2N-UPPMAX)](https://uppmax.github.io/R-python-julia-matlab-HPC/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[High Performance Data Analytics in Python (ENCCS)](https://enccs.github.io/hpda-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Practical Intro to GPU Programming using Python (ENCCS)](https://github.com/ENCCS/webinar_documents/tree/main/2024-oct-24-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Python in an HPC environment (UPPMAX-HPC2N)](https://uppmax.github.io/HPC-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python for Scientific Computing (Aalto Scientific Computing)](https://aaltoscicomp.github.io/python-for-scicomp/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Julia
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Julia has first-class support for GPU programming through the following packages
    that target GPUs from all three major vendors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CUDA.jl` is the most mature, `AMDGPU.jl` is somewhat behind but still ready
    for general use, while `oneAPI.jl` and `Metal.jl` are functional but might contain
    bugs, miss some features and provide suboptimal performance.'
  prefs: []
  type: TYPE_NORMAL
- en: The APIs of these libraries are completely analogous and translation between
    them is normally straightforward. The libraries offer both user-friendly **high-level
    abstractions** (the array interface and higher-level abstractions) that require
    little programming effort, and a **lower level** approach for writing kernels
    for fine-grained control.
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing these packages is done with the Julia package manager:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Installing `CUDA.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing `AMDGPU.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing `oneAPI.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Installing `Metal.jl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'To use the Julia GPU stack, one needs to have the relevant GPU drivers and
    programming toolkits installed. GPU drivers are already installed on HPC systems
    while on your own machine you will need to install them yourself (see e.g. these
    [instructions from NVIDIA](https://www.nvidia.com/Download/index.aspx)). Programming
    toolkits for CUDA can be installed automatically through Julia’s artifact system
    upon the first usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: The array interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GPU programming with Julia can be as simple as using a different array type
    instead of regular `Base.Array` arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CuArray` from CUDA.jl for NVIDIA GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ROCArray` from AMDGPU.jl for AMD GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`oneArray` from oneAPI.jl for Intel GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MtlArray` from Metal.jl for Apple GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These array types closely resemble `Base.Array` which enables us to write generic
    code which works on both types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code copies an array to the GPU and executes a simple operation
    on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Moving an array back from the GPU to the CPU is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s have a look at a more realistic example: matrix multiplication. We create
    two random arrays, one on the CPU and one on the GPU, and compare the performance
    using the [BenchmarkTools package](https://github.com/JuliaCI/BenchmarkTools.jl):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Vendor libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Support for using GPU vendor libraries from Julia is currently most mature
    on NVIDIA GPUs. NVIDIA libraries contain precompiled kernels for common operations
    like matrix multiplication (cuBLAS), fast Fourier transforms (cuFFT), linear solvers
    (cuSOLVER), etc. These kernels are wrapped in `CUDA.jl` and can be used directly
    with `CuArrays`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '`AMDGPU.jl` currently supports some of the ROCm libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: rocBLAS for BLAS support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rocFFT for FFT support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rocRAND for RNG support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MIOpen for DNN support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher-order abstractions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A powerful way to program GPUs with arrays is through Julia’s higher-order
    array abstractions. The simple element-wise addition we saw above, `a .+= 1`,
    is an example of this, but more general constructs can be created with `broadcast`,
    `map`, `reduce`, `accumulate` etc:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: Writing your own kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not all algorithms can be made to work with the higher-level abstractions in
    `CUDA.jl`. In such cases it’s necessary to explicitly write our own GPU kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to writing kernels in CUDA or HIP, we use a special function to return
    the index of the GPU thread which executes it (e.g., `threadIdx().x` for NVIDIA
    and `workitemIdx().x` for AMD), and two additional functions to parallelise over
    multiple blocks (e.g., `blockDim().x()` and `blockIdx().x()` for NVIDIA, and `workgroupDim().x()`
    and `workgroupIdx().x()` for AMD).
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s an example of vector addition kernels for NVIDIA, AMD, Intel and Apple
    GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: Restrictions in kernel programming
  prefs: []
  type: TYPE_NORMAL
- en: Within kernels, most of the Julia language is supported with the exception of
    functionality that requires the Julia runtime library. This means one cannot allocate
    memory or perform dynamic function calls, both of which are easy to do accidentally!
  prefs: []
  type: TYPE_NORMAL
- en: 1D, 2D and 3D
  prefs: []
  type: TYPE_NORMAL
- en: CUDA.jl and AMDGPU.jl support indexing in up to 3 dimensions (x, y and z, e.g.
    `threadIdx().x` and `workitemIdx().x`). This is convenient for multidimensional
    data where thread blocks can be organised into 1D, 2D or 3D arrays of threads.
  prefs: []
  type: TYPE_NORMAL
- en: Writing protable kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    allows you to write generic GPU code and run it on GPUs from Nvidia, AMD, Intel
    or Apple, similar to alpaka and Kokkos for C++. The backend is the object that
    decides where the code will be executed. A specific backend such as `ROCBackend()`
    becomes available when the corresponding package is loaded.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: The array interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GPU programming with Julia can be as simple as using a different array type
    instead of regular `Base.Array` arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CuArray` from CUDA.jl for NVIDIA GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ROCArray` from AMDGPU.jl for AMD GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`oneArray` from oneAPI.jl for Intel GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MtlArray` from Metal.jl for Apple GPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These array types closely resemble `Base.Array` which enables us to write generic
    code which works on both types.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code copies an array to the GPU and executes a simple operation
    on the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'Moving an array back from the GPU to the CPU is simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s have a look at a more realistic example: matrix multiplication. We create
    two random arrays, one on the CPU and one on the GPU, and compare the performance
    using the [BenchmarkTools package](https://github.com/JuliaCI/BenchmarkTools.jl):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Vendor libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Support for using GPU vendor libraries from Julia is currently most mature
    on NVIDIA GPUs. NVIDIA libraries contain precompiled kernels for common operations
    like matrix multiplication (cuBLAS), fast Fourier transforms (cuFFT), linear solvers
    (cuSOLVER), etc. These kernels are wrapped in `CUDA.jl` and can be used directly
    with `CuArrays`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '`AMDGPU.jl` currently supports some of the ROCm libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: rocBLAS for BLAS support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rocFFT for FFT support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rocRAND for RNG support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MIOpen for DNN support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Higher-order abstractions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A powerful way to program GPUs with arrays is through Julia’s higher-order
    array abstractions. The simple element-wise addition we saw above, `a .+= 1`,
    is an example of this, but more general constructs can be created with `broadcast`,
    `map`, `reduce`, `accumulate` etc:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: Writing your own kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Not all algorithms can be made to work with the higher-level abstractions in
    `CUDA.jl`. In such cases it’s necessary to explicitly write our own GPU kernel.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly to writing kernels in CUDA or HIP, we use a special function to return
    the index of the GPU thread which executes it (e.g., `threadIdx().x` for NVIDIA
    and `workitemIdx().x` for AMD), and two additional functions to parallelise over
    multiple blocks (e.g., `blockDim().x()` and `blockIdx().x()` for NVIDIA, and `workgroupDim().x()`
    and `workgroupIdx().x()` for AMD).
  prefs: []
  type: TYPE_NORMAL
- en: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here’s an example of vector addition kernels for NVIDIA, AMD, Intel and Apple
    GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: Restrictions in kernel programming
  prefs: []
  type: TYPE_NORMAL
- en: Within kernels, most of the Julia language is supported with the exception of
    functionality that requires the Julia runtime library. This means one cannot allocate
    memory or perform dynamic function calls, both of which are easy to do accidentally!
  prefs: []
  type: TYPE_NORMAL
- en: 1D, 2D and 3D
  prefs: []
  type: TYPE_NORMAL
- en: CUDA.jl and AMDGPU.jl support indexing in up to 3 dimensions (x, y and z, e.g.
    `threadIdx().x` and `workitemIdx().x`). This is convenient for multidimensional
    data where thread blocks can be organised into 1D, 2D or 3D arrays of threads.
  prefs: []
  type: TYPE_NORMAL
- en: Writing protable kernels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    allows you to write generic GPU code and run it on GPUs from Nvidia, AMD, Intel
    or Apple, similar to alpaka and Kokkos for C++. The backend is the object that
    decides where the code will be executed. A specific backend such as `ROCBackend()`
    becomes available when the corresponding package is loaded.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There has been a lot of progress in GPU programming using Python and the ecosystem
    is still evolving. There are a couple of options available to work with GPU.
  prefs: []
  type: TYPE_NORMAL
- en: CuPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CuPy is a NumPy/SciPy-compatible data array library used on GPU. It has been
    developed for NVIDIA GPUs but as experimental support for AMD GPUs. CuPy has a
    highly compatible interface with NumPy and SciPy. As stated on its official website,
    “All you need to do is just replace `numpy` and `scipy` with `cupy` and `cupyx.scipy`
    in your Python code.” If you know NumPy, CuPy is a very easy way to get started
    on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: cuDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAPIDS is a high level packages collections which implement CUDA functionalities
    and API with Python bindings. It only supports NVIDIA GPUs. cuDF belongs to RAPIDS
    and is the library for manipulating data frames on GPU. cuDF provides a pandas-like
    API, so if you are familiar with Pandas, you can accelerate your work without
    knowing too much CUDA programming.
  prefs: []
  type: TYPE_NORMAL
- en: PyCUDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyCUDA is a Python programming environment for CUDA. It allows users to access
    to NVIDIA’s CUDA API from Python. PyCUDA is powerful library but only runs on
    NVIDIA GPUs. Knowledge of CUDA programming is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Numba
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Numba allows users to just-in-time (JIT) compile Python code to run fast on
    CPUs, but can also be used for JIT compiling for GPUs. In the following we will
    focus on using Numba, which supports GPUs from both NVIDIA and AMD.
  prefs: []
  type: TYPE_NORMAL
- en: Using Numba in AMD GPUs
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Numba with AMD GPUs `numba-hip` extension can be used. By adding the
    following lines in the beginning of the code, a single-source code can be made
    to work in both Nvidia and AMD GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: Read more how to install and use it [here](https://github.com/ROCm/numba-hip).
  prefs: []
  type: TYPE_NORMAL
- en: Numba supports GPU programming by directly compiling a restricted subset of
    Python code into kernels and device functions following the execution model. Kernels
    written in Numba appear to have direct access to NumPy arrays. NumPy arrays are
    transferred between the CPU and the GPU automatically.
  prefs: []
  type: TYPE_NORMAL
- en: ufunc (gufunc) decorator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using ufuncs (and generalized ufuncs) is the easiest way to run on a GPU with
    Numba, and it requires minimal understanding of GPU programming. Numba `@vectorize`
    will produce a ufunc-like object. This object is a close analog but not fully
    compatible with a regular NumPy ufunc. Generating a ufunc for GPU requires the
    explicit type signature and target attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Demo: Numba ufunc'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simple mathematical problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark, first initialize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: Numba `@vectorize` is limited to scalar arguments in the core function, for
    multi-dimensional arrays arguments, `@guvectorize` is used. Consider the following
    example which does matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: One should never implement things like matrix multiplication by oneself since
    there are plenty of highly optimized libraries available!
  prefs: []
  type: TYPE_NORMAL
- en: Numba gufunc
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark, first, initialize some arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Numba automatically did a lot of things for us:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory was allocated on GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was copied from CPU and GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel was configured and launched
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was copied back from GPU to CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ufuncs (or gfuncs) for GPU processing can be straightforward, but this
    approach may not always yield optimal performance due to automatic handling of
    data transfer to and from the GPU, as well as kernel launching. Additionally,
    in practice, not every function can be constructed as a ufunc.
  prefs: []
  type: TYPE_NORMAL
- en: To gain greater control and flexibility, one may need to craft their own kernels
    and manually manage data transfer. Refer to the *Python for HPDA* resource linked
    below for guidance on implementing such techniques using Numba.
  prefs: []
  type: TYPE_NORMAL
- en: CuPy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CuPy is a NumPy/SciPy-compatible data array library used on GPU. It has been
    developed for NVIDIA GPUs but as experimental support for AMD GPUs. CuPy has a
    highly compatible interface with NumPy and SciPy. As stated on its official website,
    “All you need to do is just replace `numpy` and `scipy` with `cupy` and `cupyx.scipy`
    in your Python code.” If you know NumPy, CuPy is a very easy way to get started
    on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: cuDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RAPIDS is a high level packages collections which implement CUDA functionalities
    and API with Python bindings. It only supports NVIDIA GPUs. cuDF belongs to RAPIDS
    and is the library for manipulating data frames on GPU. cuDF provides a pandas-like
    API, so if you are familiar with Pandas, you can accelerate your work without
    knowing too much CUDA programming.
  prefs: []
  type: TYPE_NORMAL
- en: PyCUDA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PyCUDA is a Python programming environment for CUDA. It allows users to access
    to NVIDIA’s CUDA API from Python. PyCUDA is powerful library but only runs on
    NVIDIA GPUs. Knowledge of CUDA programming is needed.
  prefs: []
  type: TYPE_NORMAL
- en: Numba
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Numba allows users to just-in-time (JIT) compile Python code to run fast on
    CPUs, but can also be used for JIT compiling for GPUs. In the following we will
    focus on using Numba, which supports GPUs from both NVIDIA and AMD.
  prefs: []
  type: TYPE_NORMAL
- en: Using Numba in AMD GPUs
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Numba with AMD GPUs `numba-hip` extension can be used. By adding the
    following lines in the beginning of the code, a single-source code can be made
    to work in both Nvidia and AMD GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: Read more how to install and use it [here](https://github.com/ROCm/numba-hip).
  prefs: []
  type: TYPE_NORMAL
- en: Numba supports GPU programming by directly compiling a restricted subset of
    Python code into kernels and device functions following the execution model. Kernels
    written in Numba appear to have direct access to NumPy arrays. NumPy arrays are
    transferred between the CPU and the GPU automatically.
  prefs: []
  type: TYPE_NORMAL
- en: ufunc (gufunc) decorator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using ufuncs (and generalized ufuncs) is the easiest way to run on a GPU with
    Numba, and it requires minimal understanding of GPU programming. Numba `@vectorize`
    will produce a ufunc-like object. This object is a close analog but not fully
    compatible with a regular NumPy ufunc. Generating a ufunc for GPU requires the
    explicit type signature and target attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Demo: Numba ufunc'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simple mathematical problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark, first initialize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: Numba `@vectorize` is limited to scalar arguments in the core function, for
    multi-dimensional arrays arguments, `@guvectorize` is used. Consider the following
    example which does matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: One should never implement things like matrix multiplication by oneself since
    there are plenty of highly optimized libraries available!
  prefs: []
  type: TYPE_NORMAL
- en: Numba gufunc
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark, first, initialize some arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Numba automatically did a lot of things for us:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory was allocated on GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was copied from CPU and GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel was configured and launched
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was copied back from GPU to CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ufuncs (or gfuncs) for GPU processing can be straightforward, but this
    approach may not always yield optimal performance due to automatic handling of
    data transfer to and from the GPU, as well as kernel launching. Additionally,
    in practice, not every function can be constructed as a ufunc.
  prefs: []
  type: TYPE_NORMAL
- en: To gain greater control and flexibility, one may need to craft their own kernels
    and manually manage data transfer. Refer to the *Python for HPDA* resource linked
    below for guidance on implementing such techniques using Numba.
  prefs: []
  type: TYPE_NORMAL
- en: ufunc (gufunc) decorator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Using ufuncs (and generalized ufuncs) is the easiest way to run on a GPU with
    Numba, and it requires minimal understanding of GPU programming. Numba `@vectorize`
    will produce a ufunc-like object. This object is a close analog but not fully
    compatible with a regular NumPy ufunc. Generating a ufunc for GPU requires the
    explicit type signature and target attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Demo: Numba ufunc'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at a simple mathematical problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark, first initialize:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: Numba `@vectorize` is limited to scalar arguments in the core function, for
    multi-dimensional arrays arguments, `@guvectorize` is used. Consider the following
    example which does matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs: []
  type: TYPE_NORMAL
- en: One should never implement things like matrix multiplication by oneself since
    there are plenty of highly optimized libraries available!
  prefs: []
  type: TYPE_NORMAL
- en: Numba gufunc
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: 'To benchmark, first, initialize some arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Numba automatically did a lot of things for us:'
  prefs: []
  type: TYPE_NORMAL
- en: Memory was allocated on GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was copied from CPU and GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kernel was configured and launched
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data was copied back from GPU to CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using ufuncs (or gfuncs) for GPU processing can be straightforward, but this
    approach may not always yield optimal performance due to automatic handling of
    data transfer to and from the GPU, as well as kernel launching. Additionally,
    in practice, not every function can be constructed as a ufunc.
  prefs: []
  type: TYPE_NORMAL
- en: To gain greater control and flexibility, one may need to craft their own kernels
    and manually manage data transfer. Refer to the *Python for HPDA* resource linked
    below for guidance on implementing such techniques using Numba.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Play around yourself
  prefs: []
  type: TYPE_NORMAL
- en: Are you a Julian or a Pythonista? Maybe neither, but take a pick between Python
    and Julia and play around with the code examples provided above.
  prefs: []
  type: TYPE_NORMAL
- en: You can find instructions for running Julia on LUMI and Python on LUMI / Google
    Colab in the [Setup](../0-setup/) episode.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Introduction to programming in Julia (ENCCS)](https://enccs.github.io/julia-intro/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Julia for High-Performance Scientific Computing (ENCCS)](https://enccs.github.io/julia-for-hpc/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Julia for high-performance data analytics (ENCCS)](https://enccs.github.io/julia-for-hpda/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Introduction to running R, Python, Julia, and Matlab in HPC (NAISS-LUNARC-HPC2N-UPPMAX)](https://uppmax.github.io/R-python-julia-matlab-HPC/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[High Performance Data Analytics in Python (ENCCS)](https://enccs.github.io/hpda-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Practical Intro to GPU Programming using Python (ENCCS)](https://github.com/ENCCS/webinar_documents/tree/main/2024-oct-24-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Using Python in an HPC environment (UPPMAX-HPC2N)](https://uppmax.github.io/HPC-python/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Python for Scientific Computing (Aalto Scientific Computing)](https://aaltoscicomp.github.io/python-for-scicomp/)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
