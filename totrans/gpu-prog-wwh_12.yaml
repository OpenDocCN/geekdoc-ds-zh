- en: High-level language support
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级语言支持
- en: 原文：[https://enccs.github.io/gpu-programming/9-language-support/](https://enccs.github.io/gpu-programming/9-language-support/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://enccs.github.io/gpu-programming/9-language-support/](https://enccs.github.io/gpu-programming/9-language-support/)
- en: '*[GPU programming: why, when and how?](../)* **   High-level language support'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*[GPU编程：为什么、何时以及如何？](../)* **   高级语言支持'
- en: '[Edit on GitHub](https://github.com/ENCCS/gpu-programming/blob/main/content/9-language-support.rst)'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在GitHub上编辑](https://github.com/ENCCS/gpu-programming/blob/main/content/9-language-support.rst)'
- en: '* * *'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Questions
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 问题
- en: Can I port code in high-level languages to run on GPUs?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能否将高级语言中的代码移植到GPU上运行？
- en: Objectives
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Get an overview of libraries for GPU programming in Python and Julia
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取Python和Julia中GPU编程库的概述
- en: Instructor note
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 教师备注
- en: 40 min teaching
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 40分钟教学
- en: 20 min exercises
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 20分钟练习
- en: Julia
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Julia
- en: 'Julia has first-class support for GPU programming through the following packages
    that target GPUs from all three major vendors:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Julia通过以下针对所有三个主要供应商的GPU的包，为GPU编程提供了一级支持：
- en: '[CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CUDA.jl](https://cuda.juliagpu.org/stable/) 用于NVIDIA GPU'
- en: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) 用于AMD GPU'
- en: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) 用于Intel GPU'
- en: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) 用于Apple M系列GPU'
- en: '`CUDA.jl` is the most mature, `AMDGPU.jl` is somewhat behind but still ready
    for general use, while `oneAPI.jl` and `Metal.jl` are functional but might contain
    bugs, miss some features and provide suboptimal performance.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA.jl` 是最成熟的，`AMDGPU.jl` 稍微落后但仍然可以通用，而 `oneAPI.jl` 和 `Metal.jl` 功能齐全但可能包含错误，缺少一些功能，并提供次优性能。'
- en: The APIs of these libraries are completely analogous and translation between
    them is normally straightforward. The libraries offer both user-friendly **high-level
    abstractions** (the array interface and higher-level abstractions) that require
    little programming effort, and a **lower level** approach for writing kernels
    for fine-grained control.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库的API完全类似，它们之间的转换通常很简单。这些库提供了用户友好的**高级抽象**（数组接口和更高级的抽象），这需要很少的编程工作，以及**低级**方法来编写内核，以实现精细的控制。
- en: 'Installing these packages is done with the Julia package manager:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Julia包管理器安装这些包：
- en: 'Installing `CUDA.jl`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `CUDA.jl`：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Installing `AMDGPU.jl`:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `AMDGPU.jl`：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Installing `oneAPI.jl`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `oneAPI.jl`：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Installing `Metal.jl`:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 `Metal.jl`：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To use the Julia GPU stack, one needs to have the relevant GPU drivers and
    programming toolkits installed. GPU drivers are already installed on HPC systems
    while on your own machine you will need to install them yourself (see e.g. these
    [instructions from NVIDIA](https://www.nvidia.com/Download/index.aspx)). Programming
    toolkits for CUDA can be installed automatically through Julia’s artifact system
    upon the first usage:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Julia GPU堆栈，需要安装相关的GPU驱动程序和编程工具包。GPU驱动程序已在HPC系统上安装，而在您的个人机器上，您需要自行安装（例如，请参阅NVIDIA的这些[说明](https://www.nvidia.com/Download/index.aspx)）。CUDA的编程工具包可以在首次使用时通过Julia的artifact系统自动安装：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The array interface
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数组接口
- en: 'GPU programming with Julia can be as simple as using a different array type
    instead of regular `Base.Array` arrays:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Julia进行GPU编程可以像使用不同的数组类型而不是常规的 `Base.Array` 数组一样简单：
- en: '`CuArray` from CUDA.jl for NVIDIA GPUs'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CuArray` 来自 CUDA.jl，用于NVIDIA GPU'
- en: '`ROCArray` from AMDGPU.jl for AMD GPUs'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ROCArray` 来自 AMDGPU.jl，用于AMD GPU'
- en: '`oneArray` from oneAPI.jl for Intel GPUs'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`oneArray` 来自 oneAPI.jl，用于Intel GPU'
- en: '`MtlArray` from Metal.jl for Apple GPUs'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MtlArray` 来自 Metal.jl，用于Apple GPU'
- en: These array types closely resemble `Base.Array` which enables us to write generic
    code which works on both types.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数组类型与 `Base.Array` 非常相似，这使得我们可以编写通用的代码，这些代码可以在两种类型上运行。
- en: 'The following code copies an array to the GPU and executes a simple operation
    on the GPU:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将数组复制到GPU并在GPU上执行简单操作：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Moving an array back from the GPU to the CPU is simple:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 将数组从GPU移动回CPU很简单：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s have a look at a more realistic example: matrix multiplication. We create
    two random arrays, one on the CPU and one on the GPU, and compare the performance
    using the [BenchmarkTools package](https://github.com/JuliaCI/BenchmarkTools.jl):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个更实际的例子：矩阵乘法。我们创建两个随机数组，一个在CPU上，一个在GPU上，并使用[BenchmarkTools包](https://github.com/JuliaCI/BenchmarkTools.jl)比较性能：
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Vendor libraries
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 供应商库
- en: 'Support for using GPU vendor libraries from Julia is currently most mature
    on NVIDIA GPUs. NVIDIA libraries contain precompiled kernels for common operations
    like matrix multiplication (cuBLAS), fast Fourier transforms (cuFFT), linear solvers
    (cuSOLVER), etc. These kernels are wrapped in `CUDA.jl` and can be used directly
    with `CuArrays`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，从 Julia 使用 GPU 供应商库的支持在 NVIDIA GPU 上最为成熟。NVIDIA 库包含预编译的内核，用于常见操作，如矩阵乘法（cuBLAS）、快速傅里叶变换（cuFFT）、线性求解器（cuSOLVER）等。这些内核被封装在
    `CUDA.jl` 中，可以直接与 `CuArrays` 一起使用：
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`AMDGPU.jl` currently supports some of the ROCm libraries:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`AMDGPU.jl` 目前支持一些 ROCm 库：'
- en: rocBLAS for BLAS support
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocBLAS 用于 BLAS 支持
- en: rocFFT for FFT support
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocFFT 用于 FFT 支持
- en: rocRAND for RNG support
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocRAND 用于 RNG 支持
- en: MIOpen for DNN support
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MIOpen 用于 DNN 支持
- en: Higher-order abstractions
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高阶抽象
- en: 'A powerful way to program GPUs with arrays is through Julia’s higher-order
    array abstractions. The simple element-wise addition we saw above, `a .+= 1`,
    is an example of this, but more general constructs can be created with `broadcast`,
    `map`, `reduce`, `accumulate` etc:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Julia 的高阶数组抽象来编程 GPU 是一种强大的方式。上面看到的简单元素级加法 `a .+= 1` 是一个例子，但可以使用 `broadcast`、`map`、`reduce`、`accumulate`
    等创建更通用的结构：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Writing your own kernels
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写自己的内核
- en: Not all algorithms can be made to work with the higher-level abstractions in
    `CUDA.jl`. In such cases it’s necessary to explicitly write our own GPU kernel.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有算法都能与 `CUDA.jl` 中的高级抽象一起工作。在这种情况下，需要显式编写我们自己的 GPU 内核。
- en: Similarly to writing kernels in CUDA or HIP, we use a special function to return
    the index of the GPU thread which executes it (e.g., `threadIdx().x` for NVIDIA
    and `workitemIdx().x` for AMD), and two additional functions to parallelise over
    multiple blocks (e.g., `blockDim().x()` and `blockIdx().x()` for NVIDIA, and `workgroupDim().x()`
    and `workgroupIdx().x()` for AMD).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于在 CUDA 或 HIP 中编写内核，我们使用一个特殊函数来返回执行它的 GPU 线程的索引（例如，NVIDIA 的 `threadIdx().x`
    和 AMD 的 `workitemIdx().x`），以及两个额外的函数来并行处理多个块（例如，NVIDIA 的 `blockDim().x()` 和 `blockIdx().x()`，AMD
    的 `workgroupDim().x()` 和 `workgroupIdx().x()`）。
- en: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
- en: 'Here’s an example of vector addition kernels for NVIDIA, AMD, Intel and Apple
    GPUs:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个为 NVIDIA、AMD、Intel 和 Apple GPU 编写的向量加法内核示例：
- en: '[PRE19]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Restrictions in kernel programming
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 内核编程的限制
- en: Within kernels, most of the Julia language is supported with the exception of
    functionality that requires the Julia runtime library. This means one cannot allocate
    memory or perform dynamic function calls, both of which are easy to do accidentally!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中，除了需要 Julia 运行时库的功能之外，大多数 Julia 语言都受到支持。这意味着不能分配内存或执行动态函数调用，这两者都很容易意外发生！
- en: 1D, 2D and 3D
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 1D、2D 和 3D
- en: CUDA.jl and AMDGPU.jl support indexing in up to 3 dimensions (x, y and z, e.g.
    `threadIdx().x` and `workitemIdx().x`). This is convenient for multidimensional
    data where thread blocks can be organised into 1D, 2D or 3D arrays of threads.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA.jl 和 AMDGPU.jl 支持最多 3 维的索引（x, y 和 z，例如 `threadIdx().x` 和 `workitemIdx().x`）。这对于可以将线程块组织成
    1D、2D 或 3D 线程数组的多维数据来说很方便。
- en: Writing protable kernels
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写可移植的内核
- en: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    allows you to write generic GPU code and run it on GPUs from Nvidia, AMD, Intel
    or Apple, similar to alpaka and Kokkos for C++. The backend is the object that
    decides where the code will be executed. A specific backend such as `ROCBackend()`
    becomes available when the corresponding package is loaded.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    允许您编写通用的 GPU 代码，并在 Nvidia、AMD、Intel 或 Apple 的 GPU 上运行，类似于 C++ 的 alpaka 和 Kokkos。后端是决定代码将在何处执行的对象。当加载相应的包时，特定的后端（如
    `ROCBackend()`）变得可用。'
- en: '[PRE23]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Python
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python
- en: There has been a lot of progress in GPU programming using Python and the ecosystem
    is still evolving. There are a couple of options available to work with GPU.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 进行 GPU 编程已经取得了很大的进展，生态系统仍在不断发展。有几个选项可用于与 GPU 一起工作。
- en: CuPy
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CuPy
- en: CuPy is a NumPy/SciPy-compatible data array library used on GPU. It has been
    developed for NVIDIA GPUs but as experimental support for AMD GPUs. CuPy has a
    highly compatible interface with NumPy and SciPy. As stated on its official website,
    “All you need to do is just replace `numpy` and `scipy` with `cupy` and `cupyx.scipy`
    in your Python code.” If you know NumPy, CuPy is a very easy way to get started
    on the GPU.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: CuPy 是一个与 NumPy/SciPy 兼容的数据数组库，用于 GPU。它为 NVIDIA GPU 开发，但提供了对 AMD GPU 的实验性支持。CuPy
    与 NumPy 和 SciPy 具有高度兼容的接口。正如其官方网站所述，“你只需要在 Python 代码中将 `numpy` 和 `scipy` 替换为 `cupy`
    和 `cupyx.scipy`。”如果你熟悉 NumPy，CuPy 是一个在 GPU 上开始的好方法。
- en: cuDF
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cuDF
- en: RAPIDS is a high level packages collections which implement CUDA functionalities
    and API with Python bindings. It only supports NVIDIA GPUs. cuDF belongs to RAPIDS
    and is the library for manipulating data frames on GPU. cuDF provides a pandas-like
    API, so if you are familiar with Pandas, you can accelerate your work without
    knowing too much CUDA programming.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: RAPIDS 是一个高级包集合，它使用 Python 绑定实现了 CUDA 功能和 API。它仅支持 NVIDIA GPU。cuDF 属于 RAPIDS，是用于在
    GPU 上操作数据帧的库。cuDF 提供了类似 Pandas 的 API，因此如果你熟悉 Pandas，你可以在不了解太多 CUDA 编程的情况下加速你的工作。
- en: PyCUDA
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyCUDA
- en: PyCUDA is a Python programming environment for CUDA. It allows users to access
    to NVIDIA’s CUDA API from Python. PyCUDA is powerful library but only runs on
    NVIDIA GPUs. Knowledge of CUDA programming is needed.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: PyCUDA 是一个用于 CUDA 的 Python 编程环境。它允许用户从 Python 访问 NVIDIA 的 CUDA API。PyCUDA 是一个强大的库，但只能在
    NVIDIA GPU 上运行。需要了解 CUDA 编程知识。
- en: Numba
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Numba
- en: Numba allows users to just-in-time (JIT) compile Python code to run fast on
    CPUs, but can also be used for JIT compiling for GPUs. In the following we will
    focus on using Numba, which supports GPUs from both NVIDIA and AMD.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Numba 允许用户即时（JIT）编译 Python 代码以在 CPU 上快速运行，但也可以用于 GPU 的 JIT 编译。在以下内容中，我们将重点介绍使用
    Numba，它支持 NVIDIA 和 AMD 的 GPU。
- en: Using Numba in AMD GPUs
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AMD GPU 上使用 Numba
- en: 'To use Numba with AMD GPUs `numba-hip` extension can be used. By adding the
    following lines in the beginning of the code, a single-source code can be made
    to work in both Nvidia and AMD GPUs:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Numba 与 AMD GPU，可以使用 `numba-hip` 扩展。通过在代码开头添加以下几行，可以使单源代码在 Nvidia 和 AMD
    GPU 上运行：
- en: '[PRE27]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Read more how to install and use it [here](https://github.com/ROCm/numba-hip).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何安装和使用的更多信息，请参阅[此处](https://github.com/ROCm/numba-hip)。
- en: Numba supports GPU programming by directly compiling a restricted subset of
    Python code into kernels and device functions following the execution model. Kernels
    written in Numba appear to have direct access to NumPy arrays. NumPy arrays are
    transferred between the CPU and the GPU automatically.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Numba 通过直接将 Python 代码的子集编译成内核和设备函数来支持 GPU 编程，遵循执行模型。用 Numba 编写的内核似乎可以直接访问 NumPy
    数组。NumPy 数组在 CPU 和 GPU 之间自动传输。
- en: ufunc (gufunc) decorator
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ufunc（通用函数）装饰器
- en: Using ufuncs (and generalized ufuncs) is the easiest way to run on a GPU with
    Numba, and it requires minimal understanding of GPU programming. Numba `@vectorize`
    will produce a ufunc-like object. This object is a close analog but not fully
    compatible with a regular NumPy ufunc. Generating a ufunc for GPU requires the
    explicit type signature and target attribute.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ufunc（和广义 ufunc）是使用 Numba 在 GPU 上运行的最简单方法，并且它需要最小程度的 GPU 编程理解。Numba 的 `@vectorize`
    将生成一个类似 ufunc 的对象。此对象是一个接近的类似物，但与常规 NumPy ufunc 不完全兼容。为 GPU 生成 ufunc 需要显式的类型签名和目标属性。
- en: Examples
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: 'Demo: Numba ufunc'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：Numba ufunc
- en: 'Let’s look at a simple mathematical problem:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的数学问题：
- en: '[PRE28]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To benchmark, first initialize:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行基准测试，首先初始化：
- en: '[PRE31]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Numba `@vectorize` is limited to scalar arguments in the core function, for
    multi-dimensional arrays arguments, `@guvectorize` is used. Consider the following
    example which does matrix multiplication.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Numba 的 `@vectorize` 仅限于核心函数中的标量参数，对于多维数组参数，使用 `@guvectorize`。考虑以下示例，它执行矩阵乘法。
- en: Warning
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: One should never implement things like matrix multiplication by oneself since
    there are plenty of highly optimized libraries available!
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要自己实现像矩阵乘法这样的功能，因为已经有大量高度优化的库可供使用！
- en: Numba gufunc
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Numba gufunc
- en: '[PRE35]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To benchmark, first, initialize some arrays:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行基准测试，首先初始化一些数组：
- en: '[PRE38]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Numba automatically did a lot of things for us:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Numba 自动为我们做了很多事情：
- en: Memory was allocated on GPU
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存已在 GPU 上分配
- en: Data was copied from CPU and GPU
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据已从 CPU 和 GPU 复制
- en: The kernel was configured and launched
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核已配置并启动
- en: Data was copied back from GPU to CPU
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据已从 GPU 复制回 CPU
- en: Using ufuncs (or gfuncs) for GPU processing can be straightforward, but this
    approach may not always yield optimal performance due to automatic handling of
    data transfer to and from the GPU, as well as kernel launching. Additionally,
    in practice, not every function can be constructed as a ufunc.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ufunc（或 gfunc）进行 GPU 处理可能很简单，但由于自动处理数据在 GPU 之间传输以及内核启动，这种方法可能并不总是产生最佳性能。此外，在实践中，并非每个函数都可以构建为
    ufunc。
- en: To gain greater control and flexibility, one may need to craft their own kernels
    and manually manage data transfer. Refer to the *Python for HPDA* resource linked
    below for guidance on implementing such techniques using Numba.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更大的控制和灵活性，可能需要自己编写内核并手动管理数据传输。有关使用 Numba 实现此类技术的指导，请参考下面的 *Python for HPDA*
    资源。
- en: Exercises
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Play around yourself
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Are you a Julian or a Pythonista? Maybe neither, but take a pick between Python
    and Julia and play around with the code examples provided above.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你是 Julian 还是 Pythonista？也许都不是，但请从 Python 和 Julia 中选择一个，并尝试上面的代码示例。
- en: You can find instructions for running Julia on LUMI and Python on LUMI / Google
    Colab in the [Setup](../0-setup/) episode.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [设置](../0-setup/) 部分找到在 LUMI 上运行 Julia 和 Python 以及在 Google Colab 上运行的说明。
- en: See also
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Introduction to programming in Julia (ENCCS)](https://enccs.github.io/julia-intro/)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Julia 编程入门 (ENCCS)](https://enccs.github.io/julia-intro/)'
- en: '[Julia for High-Performance Scientific Computing (ENCCS)](https://enccs.github.io/julia-for-hpc/)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Julia 高性能科学计算 (ENCCS)](https://enccs.github.io/julia-for-hpc/)'
- en: '[Julia for high-performance data analytics (ENCCS)](https://enccs.github.io/julia-for-hpda/)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Julia 高性能数据分析 (ENCCS)](https://enccs.github.io/julia-for-hpda/)'
- en: '[Introduction to running R, Python, Julia, and Matlab in HPC (NAISS-LUNARC-HPC2N-UPPMAX)](https://uppmax.github.io/R-python-julia-matlab-HPC/)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 HPC 中运行 R、Python、Julia 和 Matlab 的入门 (NAISS-LUNARC-HPC2N-UPPMAX)](https://uppmax.github.io/R-python-julia-matlab-HPC/)'
- en: '[High Performance Data Analytics in Python (ENCCS)](https://enccs.github.io/hpda-python/)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 中的高性能数据分析 (ENCCS)](https://enccs.github.io/hpda-python/)'
- en: '[Practical Intro to GPU Programming using Python (ENCCS)](https://github.com/ENCCS/webinar_documents/tree/main/2024-oct-24-python)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用 Python 进行 GPU 编程的实用入门 (ENCCS)](https://github.com/ENCCS/webinar_documents/tree/main/2024-oct-24-python)'
- en: '[Using Python in an HPC environment (UPPMAX-HPC2N)](https://uppmax.github.io/HPC-python/)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在 HPC 环境中使用 Python (UPPMAX-HPC2N)](https://uppmax.github.io/HPC-python/)'
- en: '[Python for Scientific Computing (Aalto Scientific Computing)](https://aaltoscicomp.github.io/python-for-scicomp/)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python 科学计算 (Aalto 科学计算)](https://aaltoscicomp.github.io/python-for-scicomp/)'
- en: '[Previous](../8-portable-kernel-models/ "Portable kernel-based models") [Next](../10-multiple_gpu/
    "Multiple GPU programming with MPI")'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '[上一节](../8-portable-kernel-models/ "基于内核的可移植模型") [下一节](../10-multiple_gpu/
    "使用 MPI 的多 GPU 编程")'
- en: '* * *'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: © Copyright 2023-2024, The contributors.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: © 版权 2023-2024，贡献者。
- en: Built with [Sphinx](https://www.sphinx-doc.org/) using a [theme](https://github.com/readthedocs/sphinx_rtd_theme)
    provided by [Read the Docs](https://readthedocs.org).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 [Sphinx](https://www.sphinx-doc.org/) 和由 [Read the Docs](https://readthedocs.org)
    提供的 [主题](https://github.com/readthedocs/sphinx_rtd_theme) 构建
- en: Questions
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 问题
- en: Can I port code in high-level languages to run on GPUs?
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我能否将高级语言编写的代码移植到 GPU 上运行？
- en: Objectives
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 目标
- en: Get an overview of libraries for GPU programming in Python and Julia
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 Python 和 Julia 中用于 GPU 编程的库概述
- en: Instructor note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 教师备注
- en: 40 min teaching
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 40 分钟教学
- en: 20 min exercises
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 20 分钟练习
- en: Julia
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Julia
- en: 'Julia has first-class support for GPU programming through the following packages
    that target GPUs from all three major vendors:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Julia 通过以下针对所有三个主要供应商的 GPU 的包提供一流的 GPU 编程支持：
- en: '[CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CUDA.jl](https://cuda.juliagpu.org/stable/) 用于 NVIDIA GPU'
- en: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) 用于 AMD GPU'
- en: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) 用于英特尔 GPU'
- en: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) 用于 Apple M 系列GPU'
- en: '`CUDA.jl` is the most mature, `AMDGPU.jl` is somewhat behind but still ready
    for general use, while `oneAPI.jl` and `Metal.jl` are functional but might contain
    bugs, miss some features and provide suboptimal performance.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA.jl` 是最成熟的，`AMDGPU.jl` 稍微落后但仍然适用于通用用途，而 `oneAPI.jl` 和 `Metal.jl` 功能齐全但可能存在错误，缺少一些功能，并且性能可能不是最优的。'
- en: The APIs of these libraries are completely analogous and translation between
    them is normally straightforward. The libraries offer both user-friendly **high-level
    abstractions** (the array interface and higher-level abstractions) that require
    little programming effort, and a **lower level** approach for writing kernels
    for fine-grained control.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库的API完全类似，它们之间的转换通常是直接的。这些库提供了用户友好的**高级抽象**（数组接口和高级抽象），这需要很少的编程工作，以及用于编写内核的**低级**方法，以实现细粒度控制。
- en: 'Installing these packages is done with the Julia package manager:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Julia包管理器安装这些包：
- en: 'Installing `CUDA.jl`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`CUDA.jl`：
- en: '[PRE42]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Installing `AMDGPU.jl`:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`AMDGPU.jl`：
- en: '[PRE43]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Installing `oneAPI.jl`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`oneAPI.jl`：
- en: '[PRE44]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Installing `Metal.jl`:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`Metal.jl`：
- en: '[PRE45]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'To use the Julia GPU stack, one needs to have the relevant GPU drivers and
    programming toolkits installed. GPU drivers are already installed on HPC systems
    while on your own machine you will need to install them yourself (see e.g. these
    [instructions from NVIDIA](https://www.nvidia.com/Download/index.aspx)). Programming
    toolkits for CUDA can be installed automatically through Julia’s artifact system
    upon the first usage:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Julia GPU堆栈，需要安装相关的GPU驱动程序和编程工具包。GPU驱动程序已经在HPC系统上安装，而在您的个人机器上，您需要自行安装它们（例如，请参阅NVIDIA的这些[说明](https://www.nvidia.com/Download/index.aspx))。CUDA的编程工具包可以在第一次使用时通过Julia的artifact系统自动安装：
- en: '[PRE46]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The array interface
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数组接口
- en: 'GPU programming with Julia can be as simple as using a different array type
    instead of regular `Base.Array` arrays:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Julia进行GPU编程可以像使用不同的数组类型而不是常规的`Base.Array`数组一样简单：
- en: '`CuArray` from CUDA.jl for NVIDIA GPUs'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CuArray`来自CUDA.jl，适用于NVIDIA GPU'
- en: '`ROCArray` from AMDGPU.jl for AMD GPUs'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ROCArray`来自AMDGPU.jl，适用于AMD GPU'
- en: '`oneArray` from oneAPI.jl for Intel GPUs'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`oneArray`来自oneAPI.jl，适用于Intel GPU'
- en: '`MtlArray` from Metal.jl for Apple GPUs'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MtlArray`来自Metal.jl，适用于Apple GPU'
- en: These array types closely resemble `Base.Array` which enables us to write generic
    code which works on both types.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数组类型与`Base.Array`非常相似，这使得我们可以编写通用的代码，这些代码可以在两种类型上运行。
- en: 'The following code copies an array to the GPU and executes a simple operation
    on the GPU:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将数组复制到GPU上并在GPU上执行简单操作：
- en: '[PRE47]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Moving an array back from the GPU to the CPU is simple:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 将数组从GPU移回CPU的操作很简单：
- en: '[PRE51]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s have a look at a more realistic example: matrix multiplication. We create
    two random arrays, one on the CPU and one on the GPU, and compare the performance
    using the [BenchmarkTools package](https://github.com/JuliaCI/BenchmarkTools.jl):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个更实际的例子：矩阵乘法。我们创建两个随机数组，一个在CPU上，一个在GPU上，并使用[BenchmarkTools包](https://github.com/JuliaCI/BenchmarkTools.jl)比较性能：
- en: '[PRE52]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Vendor libraries
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 供应商库
- en: 'Support for using GPU vendor libraries from Julia is currently most mature
    on NVIDIA GPUs. NVIDIA libraries contain precompiled kernels for common operations
    like matrix multiplication (cuBLAS), fast Fourier transforms (cuFFT), linear solvers
    (cuSOLVER), etc. These kernels are wrapped in `CUDA.jl` and can be used directly
    with `CuArrays`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，从Julia使用GPU供应商库在NVIDIA GPU上最为成熟。NVIDIA库包含预编译的内核，用于常见操作，如矩阵乘法（cuBLAS）、快速傅里叶变换（cuFFT）、线性求解器（cuSOLVER）等。这些内核被封装在`CUDA.jl`中，可以直接与`CuArrays`一起使用：
- en: '[PRE56]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '`AMDGPU.jl` currently supports some of the ROCm libraries:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`AMDGPU.jl`目前支持一些ROCm库：'
- en: rocBLAS for BLAS support
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocBLAS用于BLAS支持
- en: rocFFT for FFT support
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocFFT用于快速傅里叶变换支持
- en: rocRAND for RNG support
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocRAND用于随机数生成器支持
- en: MIOpen for DNN support
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MIOpen用于深度神经网络支持
- en: Higher-order abstractions
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级抽象
- en: 'A powerful way to program GPUs with arrays is through Julia’s higher-order
    array abstractions. The simple element-wise addition we saw above, `a .+= 1`,
    is an example of this, but more general constructs can be created with `broadcast`,
    `map`, `reduce`, `accumulate` etc:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数组编程GPU的一种强大方式是通过Julia的高级数组抽象。我们上面看到的简单元素级加法`a .+= 1`就是这种抽象的一个例子，但可以使用`broadcast`、`map`、`reduce`、`accumulate`等创建更通用的结构：
- en: '[PRE57]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Writing your own kernels
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写自己的内核
- en: Not all algorithms can be made to work with the higher-level abstractions in
    `CUDA.jl`. In such cases it’s necessary to explicitly write our own GPU kernel.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有算法都可以通过`CUDA.jl`中的高级抽象来实现。在这种情况下，需要明确编写我们自己的GPU内核。
- en: Similarly to writing kernels in CUDA or HIP, we use a special function to return
    the index of the GPU thread which executes it (e.g., `threadIdx().x` for NVIDIA
    and `workitemIdx().x` for AMD), and two additional functions to parallelise over
    multiple blocks (e.g., `blockDim().x()` and `blockIdx().x()` for NVIDIA, and `workgroupDim().x()`
    and `workgroupIdx().x()` for AMD).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于在CUDA或HIP中编写内核，我们使用一个特殊函数来返回执行它的GPU线程的索引（例如，NVIDIA的`threadIdx().x`和AMD的`workitemIdx().x`），以及两个额外的函数来并行处理多个块（例如，NVIDIA的`blockDim().x()`和`blockIdx().x`，AMD的`workgroupDim().x()`和`workgroupIdx().x`）。
- en: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
- en: 'Here’s an example of vector addition kernels for NVIDIA, AMD, Intel and Apple
    GPUs:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个为NVIDIA、AMD、Intel和Apple GPU的向量加法内核示例：
- en: '[PRE61]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Restrictions in kernel programming
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 内核编程的限制
- en: Within kernels, most of the Julia language is supported with the exception of
    functionality that requires the Julia runtime library. This means one cannot allocate
    memory or perform dynamic function calls, both of which are easy to do accidentally!
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中，除了需要Julia运行时库的功能外，大多数Julia语言的功能都得到了支持。这意味着不能分配内存或执行动态函数调用，这两者都很容易意外发生！
- en: 1D, 2D and 3D
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 1D、2D和3D
- en: CUDA.jl and AMDGPU.jl support indexing in up to 3 dimensions (x, y and z, e.g.
    `threadIdx().x` and `workitemIdx().x`). This is convenient for multidimensional
    data where thread blocks can be organised into 1D, 2D or 3D arrays of threads.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA.jl和AMDGPU.jl支持最多3维度的索引（x、y和z，例如`threadIdx().x`和`workitemIdx().x`）。这对于多维数据很有用，其中线程块可以组织成1D、2D或3D的线程数组。
- en: Writing protable kernels
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写可移植内核
- en: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    allows you to write generic GPU code and run it on GPUs from Nvidia, AMD, Intel
    or Apple, similar to alpaka and Kokkos for C++. The backend is the object that
    decides where the code will be executed. A specific backend such as `ROCBackend()`
    becomes available when the corresponding package is loaded.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)允许你编写通用的GPU代码，并在Nvidia、AMD、Intel或Apple的GPU上运行，类似于C++的alpaka和Kokkos。后端是决定代码将在何处执行的对象。当加载相应的包时，特定的后端（如`ROCBackend()`）将变得可用。'
- en: '[PRE65]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Python
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python
- en: There has been a lot of progress in GPU programming using Python and the ecosystem
    is still evolving. There are a couple of options available to work with GPU.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python和其生态系统进行GPU编程已经取得了很大的进展，该生态系统仍在不断发展。目前有几个选项可用于与GPU一起工作。
- en: CuPy
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CuPy
- en: CuPy is a NumPy/SciPy-compatible data array library used on GPU. It has been
    developed for NVIDIA GPUs but as experimental support for AMD GPUs. CuPy has a
    highly compatible interface with NumPy and SciPy. As stated on its official website,
    “All you need to do is just replace `numpy` and `scipy` with `cupy` and `cupyx.scipy`
    in your Python code.” If you know NumPy, CuPy is a very easy way to get started
    on the GPU.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: CuPy是一个与NumPy/SciPy兼容的GPU数据数组库。它为NVIDIA GPU开发，但作为对AMD GPU的实验性支持。CuPy与NumPy和SciPy具有高度兼容的接口。正如其官方网站所述，“你只需要在你的Python代码中将`numpy`和`scipy`替换为`cupy`和`cupyx.scipy`。”如果你熟悉NumPy，CuPy是开始GPU编程的一个非常简单的方法。
- en: cuDF
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cuDF
- en: RAPIDS is a high level packages collections which implement CUDA functionalities
    and API with Python bindings. It only supports NVIDIA GPUs. cuDF belongs to RAPIDS
    and is the library for manipulating data frames on GPU. cuDF provides a pandas-like
    API, so if you are familiar with Pandas, you can accelerate your work without
    knowing too much CUDA programming.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: RAPIDS是一个高级包集合，它实现了CUDA功能性和API，并带有Python绑定。它仅支持NVIDIA GPU。cuDF属于RAPIDS，是用于在GPU上操作数据帧的库。cuDF提供了一个类似于Pandas的API，因此如果你熟悉Pandas，你可以在不深入了解CUDA编程的情况下加速你的工作。
- en: PyCUDA
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyCUDA
- en: PyCUDA is a Python programming environment for CUDA. It allows users to access
    to NVIDIA’s CUDA API from Python. PyCUDA is powerful library but only runs on
    NVIDIA GPUs. Knowledge of CUDA programming is needed.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: PyCUDA是一个用于CUDA的Python编程环境。它允许用户从Python访问NVIDIA的CUDA API。PyCUDA是一个功能强大的库，但只能在NVIDIA
    GPU上运行。需要了解CUDA编程知识。
- en: Numba
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Numba
- en: Numba allows users to just-in-time (JIT) compile Python code to run fast on
    CPUs, but can also be used for JIT compiling for GPUs. In the following we will
    focus on using Numba, which supports GPUs from both NVIDIA and AMD.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Numba允许用户即时（JIT）编译Python代码以在CPU上快速运行，但也可以用于GPU的JIT编译。在以下内容中，我们将重点介绍使用Numba，它支持NVIDIA和AMD的GPU。
- en: Using Numba in AMD GPUs
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在AMD GPU上使用Numba
- en: 'To use Numba with AMD GPUs `numba-hip` extension can be used. By adding the
    following lines in the beginning of the code, a single-source code can be made
    to work in both Nvidia and AMD GPUs:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Numba与AMD GPU，可以使用`numba-hip`扩展。通过在代码开头添加以下几行，可以将单源代码配置为在Nvidia和AMD GPU上运行：
- en: '[PRE69]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Read more how to install and use it [here](https://github.com/ROCm/numba-hip).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在[这里](https://github.com/ROCm/numba-hip)了解更多如何安装和使用它。
- en: Numba supports GPU programming by directly compiling a restricted subset of
    Python code into kernels and device functions following the execution model. Kernels
    written in Numba appear to have direct access to NumPy arrays. NumPy arrays are
    transferred between the CPU and the GPU automatically.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Numba通过直接将Python代码的子集编译成内核和设备函数来支持GPU编程，遵循执行模型。用Numba编写的内核似乎可以直接访问NumPy数组。NumPy数组在CPU和GPU之间自动传输。
- en: ufunc (gufunc) decorator
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ufunc (gufunc) 装饰器
- en: Using ufuncs (and generalized ufuncs) is the easiest way to run on a GPU with
    Numba, and it requires minimal understanding of GPU programming. Numba `@vectorize`
    will produce a ufunc-like object. This object is a close analog but not fully
    compatible with a regular NumPy ufunc. Generating a ufunc for GPU requires the
    explicit type signature and target attribute.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ufunc（和广义ufunc）是使用Numba在GPU上运行的最简单方法，并且它需要最小程度的GPU编程理解。Numba `@vectorize`
    将生成一个类似于ufunc的对象。此对象是一个近似物，但与常规NumPy ufunc不完全兼容。为GPU生成ufunc需要显式的类型签名和目标属性。
- en: Examples
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: 'Demo: Numba ufunc'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 演示：Numba ufunc
- en: 'Let’s look at a simple mathematical problem:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的数学问题：
- en: '[PRE70]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'To benchmark, first initialize:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行基准测试，首先初始化：
- en: '[PRE73]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Numba `@vectorize` is limited to scalar arguments in the core function, for
    multi-dimensional arrays arguments, `@guvectorize` is used. Consider the following
    example which does matrix multiplication.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Numba `@vectorize` 在核心函数中仅限于标量参数，对于多维数组参数，使用`@guvectorize`。考虑以下示例，它执行矩阵乘法。
- en: Warning
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: One should never implement things like matrix multiplication by oneself since
    there are plenty of highly optimized libraries available!
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要自己实现诸如矩阵乘法之类的事情，因为已经有大量高度优化的库可用！
- en: Numba gufunc
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: Numba gufunc
- en: '[PRE77]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'To benchmark, first, initialize some arrays:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行基准测试，首先初始化一些数组：
- en: '[PRE80]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Note
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Numba automatically did a lot of things for us:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Numba为我们自动做了很多事情：
- en: Memory was allocated on GPU
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GPU上分配了内存
- en: Data was copied from CPU and GPU
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据从CPU和GPU复制
- en: The kernel was configured and launched
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核已配置并启动
- en: Data was copied back from GPU to CPU
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据从GPU复制回CPU
- en: Using ufuncs (or gfuncs) for GPU processing can be straightforward, but this
    approach may not always yield optimal performance due to automatic handling of
    data transfer to and from the GPU, as well as kernel launching. Additionally,
    in practice, not every function can be constructed as a ufunc.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ufunc（或gfunc）进行GPU处理可能很简单，但由于自动处理数据在GPU之间传输以及内核启动，这种方法可能并不总是产生最佳性能。此外，在实践中，并非每个函数都可以构造为ufunc。
- en: To gain greater control and flexibility, one may need to craft their own kernels
    and manually manage data transfer. Refer to the *Python for HPDA* resource linked
    below for guidance on implementing such techniques using Numba.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更大的控制和灵活性，可能需要自己编写内核并手动管理数据传输。有关使用Numba实现此类技术的指导，请参考下面的*Python for HPDA*资源。
- en: Exercises
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Play around yourself
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Are you a Julian or a Pythonista? Maybe neither, but take a pick between Python
    and Julia and play around with the code examples provided above.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你是Julian还是Pythonista？也许都不是，但请从Python和Julia中选择，并尝试上面的代码示例。
- en: You can find instructions for running Julia on LUMI and Python on LUMI / Google
    Colab in the [Setup](../0-setup/) episode.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[设置](../0-setup/)部分找到有关在LUMI上运行Julia和在LUMI / Google Colab上运行Python的说明。
- en: See also
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Introduction to programming in Julia (ENCCS)](https://enccs.github.io/julia-intro/)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Julia编程入门 (ENCCS)](https://enccs.github.io/julia-intro/)'
- en: '[Julia for High-Performance Scientific Computing (ENCCS)](https://enccs.github.io/julia-for-hpc/)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Julia用于高性能科学计算 (ENCCS)](https://enccs.github.io/julia-for-hpc/)'
- en: '[Julia for high-performance data analytics (ENCCS)](https://enccs.github.io/julia-for-hpda/)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Julia用于高性能数据分析 (ENCCS)](https://enccs.github.io/julia-for-hpda/)'
- en: '[Introduction to running R, Python, Julia, and Matlab in HPC (NAISS-LUNARC-HPC2N-UPPMAX)](https://uppmax.github.io/R-python-julia-matlab-HPC/)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在HPC中运行R、Python、Julia和Matlab的入门 (NAISS-LUNARC-HPC2N-UPPMAX)](https://uppmax.github.io/R-python-julia-matlab-HPC/)'
- en: '[High Performance Data Analytics in Python (ENCCS)](https://enccs.github.io/hpda-python/)'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python中的高性能数据分析 (ENCCS)](https://enccs.github.io/hpda-python/)'
- en: '[Practical Intro to GPU Programming using Python (ENCCS)](https://github.com/ENCCS/webinar_documents/tree/main/2024-oct-24-python)'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Python进行GPU编程的实际入门（ENCCS）](https://github.com/ENCCS/webinar_documents/tree/main/2024-oct-24-python)'
- en: '[Using Python in an HPC environment (UPPMAX-HPC2N)](https://uppmax.github.io/HPC-python/)'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在HPC环境中使用Python（UPPMAX-HPC2N）](https://uppmax.github.io/HPC-python/)'
- en: '[Python for Scientific Computing (Aalto Scientific Computing)](https://aaltoscicomp.github.io/python-for-scicomp/)'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[科学计算中的Python（Aalto科学计算）](https://aaltoscicomp.github.io/python-for-scicomp/)'
- en: Julia
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Julia
- en: 'Julia has first-class support for GPU programming through the following packages
    that target GPUs from all three major vendors:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: Julia通过以下针对所有三个主要供应商的GPU的包提供了一级GPU编程支持：
- en: '[CUDA.jl](https://cuda.juliagpu.org/stable/) for NVIDIA GPUs'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CUDA.jl](https://cuda.juliagpu.org/stable/) 用于NVIDIA GPU'
- en: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) for AMD GPUs'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[AMDGPU.jl](https://amdgpu.juliagpu.org/stable/) 用于AMD GPU'
- en: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) 用于Intel GPU'
- en: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Metal.jl](https://github.com/JuliaGPU/Metal.jl) 用于Apple M系列GPU'
- en: '`CUDA.jl` is the most mature, `AMDGPU.jl` is somewhat behind but still ready
    for general use, while `oneAPI.jl` and `Metal.jl` are functional but might contain
    bugs, miss some features and provide suboptimal performance.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`CUDA.jl`是最成熟的，`AMDGPU.jl`稍微落后但仍然适用于通用用途，而`oneAPI.jl`和`Metal.jl`功能齐全但可能包含错误，缺少一些功能，并提供次优性能。'
- en: The APIs of these libraries are completely analogous and translation between
    them is normally straightforward. The libraries offer both user-friendly **high-level
    abstractions** (the array interface and higher-level abstractions) that require
    little programming effort, and a **lower level** approach for writing kernels
    for fine-grained control.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这些库的API完全类似，它们之间的转换通常是直接的。这些库提供用户友好的**高级抽象**（数组接口和高级抽象），需要很少的编程工作，以及**低级**方法来编写内核以进行精细控制。
- en: 'Installing these packages is done with the Julia package manager:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 安装这些包使用Julia包管理器：
- en: 'Installing `CUDA.jl`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`CUDA.jl`：
- en: '[PRE84]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Installing `AMDGPU.jl`:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`AMDGPU.jl`：
- en: '[PRE85]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Installing `oneAPI.jl`:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`oneAPI.jl`：
- en: '[PRE86]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Installing `Metal.jl`:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`Metal.jl`：
- en: '[PRE87]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'To use the Julia GPU stack, one needs to have the relevant GPU drivers and
    programming toolkits installed. GPU drivers are already installed on HPC systems
    while on your own machine you will need to install them yourself (see e.g. these
    [instructions from NVIDIA](https://www.nvidia.com/Download/index.aspx)). Programming
    toolkits for CUDA can be installed automatically through Julia’s artifact system
    upon the first usage:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Julia GPU堆栈，需要安装相关的GPU驱动程序和编程工具包。GPU驱动程序已经安装在HPC系统上，而在您的个人机器上，您需要自行安装它们（例如，请参阅NVIDIA的这些[说明](https://www.nvidia.com/Download/index.aspx)）。CUDA的编程工具包可以在第一次使用时通过Julia的artifact系统自动安装：
- en: '[PRE88]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: The array interface
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数组接口
- en: 'GPU programming with Julia can be as simple as using a different array type
    instead of regular `Base.Array` arrays:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Julia进行GPU编程可以像使用不同的数组类型而不是常规的`Base.Array`数组一样简单：
- en: '`CuArray` from CUDA.jl for NVIDIA GPUs'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CuArray`来自CUDA.jl，用于NVIDIA GPU'
- en: '`ROCArray` from AMDGPU.jl for AMD GPUs'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ROCArray`来自AMDGPU.jl，用于AMD GPU'
- en: '`oneArray` from oneAPI.jl for Intel GPUs'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`oneArray`来自oneAPI.jl，用于Intel GPU'
- en: '`MtlArray` from Metal.jl for Apple GPUs'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MtlArray`来自`Metal.jl`，用于Apple GPU'
- en: These array types closely resemble `Base.Array` which enables us to write generic
    code which works on both types.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数组类型与`Base.Array`非常相似，这使得我们能够编写通用的代码，这些代码可以在两种类型上运行。
- en: 'The following code copies an array to the GPU and executes a simple operation
    on the GPU:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将数组复制到GPU上并在GPU上执行简单操作：
- en: '[PRE89]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Moving an array back from the GPU to the CPU is simple:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 将数组从GPU移回到CPU上很简单：
- en: '[PRE93]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Let’s have a look at a more realistic example: matrix multiplication. We create
    two random arrays, one on the CPU and one on the GPU, and compare the performance
    using the [BenchmarkTools package](https://github.com/JuliaCI/BenchmarkTools.jl):'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个更实际的例子：矩阵乘法。我们创建两个随机数组，一个在CPU上，一个在GPU上，并使用[BenchmarkTools包](https://github.com/JuliaCI/BenchmarkTools.jl)比较性能：
- en: '[PRE94]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Vendor libraries
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 供应商库
- en: 'Support for using GPU vendor libraries from Julia is currently most mature
    on NVIDIA GPUs. NVIDIA libraries contain precompiled kernels for common operations
    like matrix multiplication (cuBLAS), fast Fourier transforms (cuFFT), linear solvers
    (cuSOLVER), etc. These kernels are wrapped in `CUDA.jl` and can be used directly
    with `CuArrays`:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在 Julia 中使用 GPU 供应商库的支持最成熟的是 NVIDIA GPU。NVIDIA 库包含预编译的内核，用于常见操作，如矩阵乘法（cuBLAS）、快速傅里叶变换（cuFFT）、线性求解器（cuSOLVER）等。这些内核在
    `CUDA.jl` 中被封装，可以直接与 `CuArrays` 一起使用：
- en: '[PRE98]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '`AMDGPU.jl` currently supports some of the ROCm libraries:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '`AMDGPU.jl` 目前支持一些 ROCm 库：'
- en: rocBLAS for BLAS support
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocBLAS 用于 BLAS 支持
- en: rocFFT for FFT support
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocFFT 用于 FFT 支持
- en: rocRAND for RNG support
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocRAND 用于 RNG 支持
- en: MIOpen for DNN support
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MIOpen 用于 DNN 支持
- en: Higher-order abstractions
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高阶抽象
- en: 'A powerful way to program GPUs with arrays is through Julia’s higher-order
    array abstractions. The simple element-wise addition we saw above, `a .+= 1`,
    is an example of this, but more general constructs can be created with `broadcast`,
    `map`, `reduce`, `accumulate` etc:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数组编程 GPU 的强大方式是通过 Julia 的高阶数组抽象。上面看到的简单元素级加法 `a .+= 1` 是一个例子，但可以使用 `broadcast`、`map`、`reduce`、`accumulate`
    等创建更通用的结构：
- en: '[PRE99]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: Writing your own kernels
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写你自己的内核
- en: Not all algorithms can be made to work with the higher-level abstractions in
    `CUDA.jl`. In such cases it’s necessary to explicitly write our own GPU kernel.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有算法都可以在 `CUDA.jl` 的高级抽象中工作。在这种情况下，有必要明确编写我们自己的 GPU 内核。
- en: Similarly to writing kernels in CUDA or HIP, we use a special function to return
    the index of the GPU thread which executes it (e.g., `threadIdx().x` for NVIDIA
    and `workitemIdx().x` for AMD), and two additional functions to parallelise over
    multiple blocks (e.g., `blockDim().x()` and `blockIdx().x()` for NVIDIA, and `workgroupDim().x()`
    and `workgroupIdx().x()` for AMD).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于在 CUDA 或 HIP 中编写内核，我们使用一个特殊函数来返回执行它的 GPU 线程的索引（例如，NVIDIA 的 `threadIdx().x`
    和 AMD 的 `workitemIdx().x`），以及两个额外的函数来并行处理多个块（例如，NVIDIA 的 `blockDim().x()` 和 `blockIdx().x()`，AMD
    的 `workgroupDim().x()` 和 `workgroupIdx().x()`）。
- en: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
- en: 'Here’s an example of vector addition kernels for NVIDIA, AMD, Intel and Apple
    GPUs:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个为 NVIDIA、AMD、Intel 和 Apple GPU 编写的向量加法内核的示例：
- en: '[PRE103]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: Restrictions in kernel programming
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 内核编程中的限制
- en: Within kernels, most of the Julia language is supported with the exception of
    functionality that requires the Julia runtime library. This means one cannot allocate
    memory or perform dynamic function calls, both of which are easy to do accidentally!
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中，除了需要 Julia 运行时库的功能外，大多数 Julia 语言都受到支持。这意味着不能分配内存或执行动态函数调用，这两者都很容易意外发生！
- en: 1D, 2D and 3D
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 1D、2D 和 3D
- en: CUDA.jl and AMDGPU.jl support indexing in up to 3 dimensions (x, y and z, e.g.
    `threadIdx().x` and `workitemIdx().x`). This is convenient for multidimensional
    data where thread blocks can be organised into 1D, 2D or 3D arrays of threads.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA.jl 和 AMDGPU.jl 支持最多 3 维的索引（x、y 和 z，例如 `threadIdx().x` 和 `workitemIdx().x`）。这对于多维数据很有用，其中线程块可以组织成
    1D、2D 或 3D 的线程数组。
- en: Writing protable kernels
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写可移植内核
- en: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    allows you to write generic GPU code and run it on GPUs from Nvidia, AMD, Intel
    or Apple, similar to alpaka and Kokkos for C++. The backend is the object that
    decides where the code will be executed. A specific backend such as `ROCBackend()`
    becomes available when the corresponding package is loaded.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    允许你编写通用的 GPU 代码并在 Nvidia、AMD、Intel 或 Apple 的 GPU 上运行，类似于 C++ 的 alpaka 和 Kokkos。后端是决定代码将在何处执行的对象。当加载相应的包时，特定的后端，如
    `ROCBackend()`，将变得可用。'
- en: '[PRE107]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: The array interface
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数组接口
- en: 'GPU programming with Julia can be as simple as using a different array type
    instead of regular `Base.Array` arrays:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Julia 进行 GPU 编程可以像使用不同的数组类型而不是常规的 `Base.Array` 数组一样简单：
- en: '`CuArray` from CUDA.jl for NVIDIA GPUs'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 CUDA.jl 的 `CuArray` 用于 NVIDIA GPU
- en: '`ROCArray` from AMDGPU.jl for AMD GPUs'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 AMDGPU.jl 的 `ROCArray` 用于 AMD GPU
- en: '`oneArray` from oneAPI.jl for Intel GPUs'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 oneAPI.jl 的 `oneArray` 用于 Intel GPU
- en: '`MtlArray` from Metal.jl for Apple GPUs'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 Metal.jl 的 `MtlArray` 用于 Apple GPU
- en: These array types closely resemble `Base.Array` which enables us to write generic
    code which works on both types.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数组类型与 `Base.Array` 非常相似，这使我们能够编写通用的代码，这些代码可以在两种类型上运行。
- en: 'The following code copies an array to the GPU and executes a simple operation
    on the GPU:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将数组复制到 GPU 并在 GPU 上执行一个简单操作：
- en: '[PRE111]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Moving an array back from the GPU to the CPU is simple:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 将数组从GPU移回到CPU很简单：
- en: '[PRE115]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Let’s have a look at a more realistic example: matrix multiplication. We create
    two random arrays, one on the CPU and one on the GPU, and compare the performance
    using the [BenchmarkTools package](https://github.com/JuliaCI/BenchmarkTools.jl):'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个更实际的例子：矩阵乘法。我们创建两个随机数组，一个在CPU上，一个在GPU上，并使用[BenchmarkTools包](https://github.com/JuliaCI/BenchmarkTools.jl)比较性能：
- en: '[PRE116]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Vendor libraries
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 供应商库
- en: 'Support for using GPU vendor libraries from Julia is currently most mature
    on NVIDIA GPUs. NVIDIA libraries contain precompiled kernels for common operations
    like matrix multiplication (cuBLAS), fast Fourier transforms (cuFFT), linear solvers
    (cuSOLVER), etc. These kernels are wrapped in `CUDA.jl` and can be used directly
    with `CuArrays`:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，在Julia中使用GPU供应商库的支持在NVIDIA GPU上最为成熟。NVIDIA库包含预编译的内核，用于常见操作，如矩阵乘法（cuBLAS）、快速傅里叶变换（cuFFT）、线性求解器（cuSOLVER）等。这些内核被封装在`CUDA.jl`中，可以直接与`CuArrays`一起使用：
- en: '[PRE120]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '`AMDGPU.jl` currently supports some of the ROCm libraries:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '`AMDGPU.jl`目前支持一些ROCm库：'
- en: rocBLAS for BLAS support
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocBLAS用于BLAS支持
- en: rocFFT for FFT support
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocFFT用于FFT支持
- en: rocRAND for RNG support
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rocRAND用于RNG支持
- en: MIOpen for DNN support
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MIOpen用于DNN支持
- en: Higher-order abstractions
  id: totrans-385
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高阶抽象
- en: 'A powerful way to program GPUs with arrays is through Julia’s higher-order
    array abstractions. The simple element-wise addition we saw above, `a .+= 1`,
    is an example of this, but more general constructs can be created with `broadcast`,
    `map`, `reduce`, `accumulate` etc:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数组编程GPU的一种强大方式是通过Julia的高阶数组抽象。我们上面看到的简单元素级加法`a .+= 1`是这种抽象的一个例子，但可以使用`broadcast`、`map`、`reduce`、`accumulate`等创建更通用的结构：
- en: '[PRE121]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: Writing your own kernels
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写自己的内核
- en: Not all algorithms can be made to work with the higher-level abstractions in
    `CUDA.jl`. In such cases it’s necessary to explicitly write our own GPU kernel.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有算法都可以在`CUDA.jl`的高级抽象中工作。在这种情况下，需要显式编写我们自己的GPU内核。
- en: Similarly to writing kernels in CUDA or HIP, we use a special function to return
    the index of the GPU thread which executes it (e.g., `threadIdx().x` for NVIDIA
    and `workitemIdx().x` for AMD), and two additional functions to parallelise over
    multiple blocks (e.g., `blockDim().x()` and `blockIdx().x()` for NVIDIA, and `workgroupDim().x()`
    and `workgroupIdx().x()` for AMD).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于在CUDA或HIP中编写内核，我们使用一个特殊函数来返回执行它的GPU线程的索引（例如，NVIDIA的`threadIdx().x`和AMD的`workitemIdx().x`），以及两个额外的函数来并行处理多个块（例如，NVIDIA的`blockDim().x()`和`blockIdx().x()`，AMD的`workgroupDim().x()`和`workgroupIdx().x()`）。
- en: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![../_images/MappingBlocksToSMs.png](../Images/c863a79411878993272b8922dc54e15c.png)'
- en: 'Here’s an example of vector addition kernels for NVIDIA, AMD, Intel and Apple
    GPUs:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个为NVIDIA、AMD、Intel和Apple GPU编写的向量加法内核的示例：
- en: '[PRE125]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: Restrictions in kernel programming
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 内核编程的限制
- en: Within kernels, most of the Julia language is supported with the exception of
    functionality that requires the Julia runtime library. This means one cannot allocate
    memory or perform dynamic function calls, both of which are easy to do accidentally!
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在内核中，除了需要Julia运行时库的功能外，大多数Julia语言都是支持的。这意味着不能分配内存或执行动态函数调用，这两者都很容易意外发生！
- en: 1D, 2D and 3D
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 1D、2D和3D
- en: CUDA.jl and AMDGPU.jl support indexing in up to 3 dimensions (x, y and z, e.g.
    `threadIdx().x` and `workitemIdx().x`). This is convenient for multidimensional
    data where thread blocks can be organised into 1D, 2D or 3D arrays of threads.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA.jl和AMDGPU.jl支持最多3维的索引（x、y和z，例如`threadIdx().x`和`workitemIdx().x`）。这对于多维数据很有用，其中线程块可以组织成1D、2D或3D的线程数组。
- en: Writing protable kernels
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写可移植的内核
- en: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    allows you to write generic GPU code and run it on GPUs from Nvidia, AMD, Intel
    or Apple, similar to alpaka and Kokkos for C++. The backend is the object that
    decides where the code will be executed. A specific backend such as `ROCBackend()`
    becomes available when the corresponding package is loaded.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '[KernelAbstractions.jl](https://github.com/JuliaGPU/KernelAbstractions.jl)
    允许您编写通用的GPU代码，并在Nvidia、AMD、Intel或Apple的GPU上运行，类似于C++的alpaka和Kokkos。后端是决定代码将在何处执行的对象。当加载相应的包时，特定的后端（如`ROCBackend()`）变得可用。'
- en: '[PRE129]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: '[PRE130]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: Python
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python
- en: There has been a lot of progress in GPU programming using Python and the ecosystem
    is still evolving. There are a couple of options available to work with GPU.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python进行GPU编程已经取得了很大的进展，生态系统仍在不断发展。有几个选项可用于与GPU一起工作。
- en: CuPy
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CuPy
- en: CuPy is a NumPy/SciPy-compatible data array library used on GPU. It has been
    developed for NVIDIA GPUs but as experimental support for AMD GPUs. CuPy has a
    highly compatible interface with NumPy and SciPy. As stated on its official website,
    “All you need to do is just replace `numpy` and `scipy` with `cupy` and `cupyx.scipy`
    in your Python code.” If you know NumPy, CuPy is a very easy way to get started
    on the GPU.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: CuPy 是一个用于 GPU 的 NumPy/SciPy 兼容的数据数组库。它为 NVIDIA GPU 开发，但作为对 AMD GPU 的实验性支持。CuPy
    与 NumPy 和 SciPy 具有高度兼容的接口。正如其官方网站上所述，“你只需要在你的 Python 代码中将 `numpy` 和 `scipy` 替换为
    `cupy` 和 `cupyx.scipy`。”如果你熟悉 NumPy，CuPy 是一个在 GPU 上开始的好方法。
- en: cuDF
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cuDF
- en: RAPIDS is a high level packages collections which implement CUDA functionalities
    and API with Python bindings. It only supports NVIDIA GPUs. cuDF belongs to RAPIDS
    and is the library for manipulating data frames on GPU. cuDF provides a pandas-like
    API, so if you are familiar with Pandas, you can accelerate your work without
    knowing too much CUDA programming.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: RAPIDS 是一个高级包集合，它使用 Python 绑定实现了 CUDA 功能和 API。它仅支持 NVIDIA GPU。cuDF 属于 RAPIDS，是用于在
    GPU 上操作数据帧的库。cuDF 提供了类似于 Pandas 的 API，因此如果你熟悉 Pandas，你可以加速你的工作而无需深入了解 CUDA 编程。
- en: PyCUDA
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyCUDA
- en: PyCUDA is a Python programming environment for CUDA. It allows users to access
    to NVIDIA’s CUDA API from Python. PyCUDA is powerful library but only runs on
    NVIDIA GPUs. Knowledge of CUDA programming is needed.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: PyCUDA 是一个用于 CUDA 的 Python 编程环境。它允许用户从 Python 访问 NVIDIA 的 CUDA API。PyCUDA 是一个强大的库，但只能在
    NVIDIA GPU 上运行。需要了解 CUDA 编程知识。
- en: Numba
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Numba
- en: Numba allows users to just-in-time (JIT) compile Python code to run fast on
    CPUs, but can also be used for JIT compiling for GPUs. In the following we will
    focus on using Numba, which supports GPUs from both NVIDIA and AMD.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: Numba 允许用户即时（JIT）编译 Python 代码以在 CPU 上快速运行，但也可以用于 GPU 的 JIT 编译。在以下内容中，我们将重点关注使用
    Numba，它支持 NVIDIA 和 AMD 的 GPU。
- en: Using Numba in AMD GPUs
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AMD GPU 上使用 Numba
- en: 'To use Numba with AMD GPUs `numba-hip` extension can be used. By adding the
    following lines in the beginning of the code, a single-source code can be made
    to work in both Nvidia and AMD GPUs:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 Numba 与 AMD GPU，可以使用 `numba-hip` 扩展。通过在代码开头添加以下行，可以使单源代码在 Nvidia 和 AMD GPU
    上运行：
- en: '[PRE133]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: Read more how to install and use it [here](https://github.com/ROCm/numba-hip).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何安装和使用它的更多信息，请参阅[这里](https://github.com/ROCm/numba-hip)。
- en: Numba supports GPU programming by directly compiling a restricted subset of
    Python code into kernels and device functions following the execution model. Kernels
    written in Numba appear to have direct access to NumPy arrays. NumPy arrays are
    transferred between the CPU and the GPU automatically.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: Numba 通过直接将 Python 代码的子集编译成内核和设备函数来支持 GPU 编程，遵循执行模型。用 Numba 编写的内核似乎可以直接访问 NumPy
    数组。NumPy 数组在 CPU 和 GPU 之间自动传输。
- en: ufunc (gufunc) decorator
  id: totrans-425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ufunc (gufunc) 装饰器
- en: Using ufuncs (and generalized ufuncs) is the easiest way to run on a GPU with
    Numba, and it requires minimal understanding of GPU programming. Numba `@vectorize`
    will produce a ufunc-like object. This object is a close analog but not fully
    compatible with a regular NumPy ufunc. Generating a ufunc for GPU requires the
    explicit type signature and target attribute.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ufunc（和广义 ufunc）是使用 Numba 在 GPU 上运行的最简单方法，并且它需要最小程度的 GPU 编程理解。Numba `@vectorize`
    将生成一个类似于 ufunc 的对象。此对象是一个接近的类似物，但与常规 NumPy ufunc 不完全兼容。为 GPU 生成 ufunc 需要显式的类型签名和目标属性。
- en: Examples
  id: totrans-427
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: 'Demo: Numba ufunc'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 演示：Numba ufunc
- en: 'Let’s look at a simple mathematical problem:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个简单的数学问题：
- en: '[PRE134]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: '[PRE136]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'To benchmark, first initialize:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行基准测试，首先初始化：
- en: '[PRE137]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '[PRE140]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: Numba `@vectorize` is limited to scalar arguments in the core function, for
    multi-dimensional arrays arguments, `@guvectorize` is used. Consider the following
    example which does matrix multiplication.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: Numba `@vectorize` 在核心函数中仅限于标量参数，对于多维数组参数，使用 `@guvectorize`。考虑以下示例，它执行矩阵乘法。
- en: Warning
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: One should never implement things like matrix multiplication by oneself since
    there are plenty of highly optimized libraries available!
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要自己实现像矩阵乘法这样的东西，因为已经有大量高度优化的库可用！
- en: Numba gufunc
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: Numba gufunc
- en: '[PRE141]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: '[PRE142]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: '[PRE143]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: 'To benchmark, first, initialize some arrays:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行基准测试，首先初始化一些数组：
- en: '[PRE144]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: '[PRE145]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: '[PRE146]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: '[PRE147]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: Note
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Numba automatically did a lot of things for us:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: Numba 自动为我们做了很多事情：
- en: Memory was allocated on GPU
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 GPU 上分配了内存
- en: Data was copied from CPU and GPU
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 CPU 和 GPU 复制了数据
- en: The kernel was configured and launched
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核已配置并启动
- en: Data was copied back from GPU to CPU
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据已从 GPU 复制回 CPU
- en: Using ufuncs (or gfuncs) for GPU processing can be straightforward, but this
    approach may not always yield optimal performance due to automatic handling of
    data transfer to and from the GPU, as well as kernel launching. Additionally,
    in practice, not every function can be constructed as a ufunc.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ufunc（或gfunc）进行GPU处理可能很简单，但由于自动处理数据在GPU之间传输以及内核启动，这种方法可能并不总是能带来最佳性能。此外，在实践中，并非每个函数都可以构建为ufunc。
- en: To gain greater control and flexibility, one may need to craft their own kernels
    and manually manage data transfer. Refer to the *Python for HPDA* resource linked
    below for guidance on implementing such techniques using Numba.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更大的控制和灵活性，可能需要自己编写内核并手动管理数据传输。请参考下述链接中的*Python for HPDA*资源，以获取使用Numba实现此类技术的指导。
- en: CuPy
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CuPy
- en: CuPy is a NumPy/SciPy-compatible data array library used on GPU. It has been
    developed for NVIDIA GPUs but as experimental support for AMD GPUs. CuPy has a
    highly compatible interface with NumPy and SciPy. As stated on its official website,
    “All you need to do is just replace `numpy` and `scipy` with `cupy` and `cupyx.scipy`
    in your Python code.” If you know NumPy, CuPy is a very easy way to get started
    on the GPU.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: CuPy是一个与NumPy/SciPy兼容的GPU数据数组库。它为NVIDIA GPU开发，但同时也提供了对AMD GPU的实验性支持。CuPy与NumPy和SciPy具有高度兼容的接口。正如其官方网站上所述，“你只需要在你的Python代码中将`numpy`和`scipy`替换为`cupy`和`cupyx.scipy`。”如果你熟悉NumPy，CuPy是开始GPU编程的一个非常简单的方式。
- en: cuDF
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: cuDF
- en: RAPIDS is a high level packages collections which implement CUDA functionalities
    and API with Python bindings. It only supports NVIDIA GPUs. cuDF belongs to RAPIDS
    and is the library for manipulating data frames on GPU. cuDF provides a pandas-like
    API, so if you are familiar with Pandas, you can accelerate your work without
    knowing too much CUDA programming.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: RAPIDS是一个高级包集合，它使用Python绑定实现了CUDA功能性和API。它仅支持NVIDIA GPU。cuDF属于RAPIDS，是用于在GPU上操作数据帧的库。cuDF提供了一个类似于Pandas的API，因此如果你熟悉Pandas，你可以在不了解太多CUDA编程的情况下加速你的工作。
- en: PyCUDA
  id: totrans-462
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyCUDA
- en: PyCUDA is a Python programming environment for CUDA. It allows users to access
    to NVIDIA’s CUDA API from Python. PyCUDA is powerful library but only runs on
    NVIDIA GPUs. Knowledge of CUDA programming is needed.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: PyCUDA是一个用于CUDA的Python编程环境。它允许用户从Python访问NVIDIA的CUDA API。PyCUDA是一个强大的库，但只能在NVIDIA
    GPU上运行。需要了解CUDA编程知识。
- en: Numba
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Numba
- en: Numba allows users to just-in-time (JIT) compile Python code to run fast on
    CPUs, but can also be used for JIT compiling for GPUs. In the following we will
    focus on using Numba, which supports GPUs from both NVIDIA and AMD.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: Numba允许用户即时（JIT）编译Python代码以在CPU上快速运行，但也可以用于GPU的JIT编译。在以下内容中，我们将重点关注使用Numba，它支持NVIDIA和AMD的GPU。
- en: Using Numba in AMD GPUs
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在AMD GPU上使用Numba
- en: 'To use Numba with AMD GPUs `numba-hip` extension can be used. By adding the
    following lines in the beginning of the code, a single-source code can be made
    to work in both Nvidia and AMD GPUs:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Numba与AMD GPU，可以使用`numba-hip`扩展。通过在代码开头添加以下行，可以使单源代码在Nvidia和AMD GPU上运行：
- en: '[PRE148]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Read more how to install and use it [here](https://github.com/ROCm/numba-hip).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在[这里](https://github.com/ROCm/numba-hip)了解更多如何安装和使用它。
- en: Numba supports GPU programming by directly compiling a restricted subset of
    Python code into kernels and device functions following the execution model. Kernels
    written in Numba appear to have direct access to NumPy arrays. NumPy arrays are
    transferred between the CPU and the GPU automatically.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: Numba支持通过直接编译Python代码的子集为内核和设备函数来执行GPU编程。用Numba编写的内核似乎可以直接访问NumPy数组。NumPy数组会在CPU和GPU之间自动传输。
- en: ufunc (gufunc) decorator
  id: totrans-471
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ufunc（gufunc）装饰器
- en: Using ufuncs (and generalized ufuncs) is the easiest way to run on a GPU with
    Numba, and it requires minimal understanding of GPU programming. Numba `@vectorize`
    will produce a ufunc-like object. This object is a close analog but not fully
    compatible with a regular NumPy ufunc. Generating a ufunc for GPU requires the
    explicit type signature and target attribute.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ufunc（和泛化ufunc）是使用Numba在GPU上运行的最简单方式，并且它需要最小程度的GPU编程理解。Numba的`@vectorize`将生成一个类似ufunc的对象。此对象是一个接近的类似物，但与常规NumPy
    ufunc不完全兼容。为GPU生成ufunc需要显式的类型签名和目标属性。
- en: Examples
  id: totrans-473
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: 'Demo: Numba ufunc'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 演示：Numba ufunc
- en: 'Let’s look at a simple mathematical problem:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的数学问题：
- en: '[PRE149]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: '[PRE150]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: '[PRE151]'
  id: totrans-478
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'To benchmark, first initialize:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行基准测试，首先初始化：
- en: '[PRE152]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: '[PRE153]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: '[PRE154]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: '[PRE155]'
  id: totrans-483
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: Numba `@vectorize` is limited to scalar arguments in the core function, for
    multi-dimensional arrays arguments, `@guvectorize` is used. Consider the following
    example which does matrix multiplication.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: Numba `@vectorize` 仅限于核心函数中的标量参数，对于多维数组参数，使用`@guvectorize`。考虑以下示例，它执行矩阵乘法。
- en: Warning
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: One should never implement things like matrix multiplication by oneself since
    there are plenty of highly optimized libraries available!
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要自己实现诸如矩阵乘法之类的事情，因为已经有大量高度优化的库可用！
- en: Numba gufunc
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: Numba gufunc
- en: '[PRE156]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: '[PRE157]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: '[PRE158]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: 'To benchmark, first, initialize some arrays:'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行基准测试，首先初始化一些数组：
- en: '[PRE159]'
  id: totrans-492
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: '[PRE160]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: '[PRE161]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: '[PRE162]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: Note
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Numba automatically did a lot of things for us:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: Numba自动为我们做了很多事情：
- en: Memory was allocated on GPU
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GPU上分配了内存
- en: Data was copied from CPU and GPU
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据从CPU和GPU复制
- en: The kernel was configured and launched
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核已配置并启动
- en: Data was copied back from GPU to CPU
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据从GPU复制回CPU
- en: Using ufuncs (or gfuncs) for GPU processing can be straightforward, but this
    approach may not always yield optimal performance due to automatic handling of
    data transfer to and from the GPU, as well as kernel launching. Additionally,
    in practice, not every function can be constructed as a ufunc.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ufunc（或gfunc）进行GPU处理可能很简单，但由于自动处理数据在GPU之间传输以及内核启动，这种方法可能不会总是产生最佳性能。此外，在实践中，并非每个函数都可以构造为ufunc。
- en: To gain greater control and flexibility, one may need to craft their own kernels
    and manually manage data transfer. Refer to the *Python for HPDA* resource linked
    below for guidance on implementing such techniques using Numba.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更大的控制和灵活性，可能需要自己编写内核并手动管理数据传输。请参考以下链接的 *Python for HPDA* 资源，以获取使用Numba实现此类技术的指导。
- en: ufunc (gufunc) decorator
  id: totrans-504
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ufunc（gufunc）装饰器
- en: Using ufuncs (and generalized ufuncs) is the easiest way to run on a GPU with
    Numba, and it requires minimal understanding of GPU programming. Numba `@vectorize`
    will produce a ufunc-like object. This object is a close analog but not fully
    compatible with a regular NumPy ufunc. Generating a ufunc for GPU requires the
    explicit type signature and target attribute.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ufunc（以及广义ufunc）是使用Numba在GPU上运行的最简单方法，并且它需要最小程度的GPU编程理解。Numba `@vectorize`
    将生成一个类似于ufunc的对象。此对象是一个接近的类似物，但与常规NumPy ufunc不完全兼容。为GPU生成ufunc需要显式的类型签名和目标属性。
- en: Examples
  id: totrans-506
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 示例
- en: 'Demo: Numba ufunc'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 演示：Numba ufunc
- en: 'Let’s look at a simple mathematical problem:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的数学问题：
- en: '[PRE163]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: '[PRE164]'
  id: totrans-510
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: '[PRE165]'
  id: totrans-511
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'To benchmark, first initialize:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行基准测试，首先初始化：
- en: '[PRE166]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: '[PRE167]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: '[PRE168]'
  id: totrans-515
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: '[PRE169]'
  id: totrans-516
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: Numba `@vectorize` is limited to scalar arguments in the core function, for
    multi-dimensional arrays arguments, `@guvectorize` is used. Consider the following
    example which does matrix multiplication.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: Numba `@vectorize` 仅限于核心函数中的标量参数，对于多维数组参数，使用`@guvectorize`。考虑以下示例，它执行矩阵乘法。
- en: Warning
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: One should never implement things like matrix multiplication by oneself since
    there are plenty of highly optimized libraries available!
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要自己实现诸如矩阵乘法之类的事情，因为已经有大量高度优化的库可用！
- en: Numba gufunc
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: Numba gufunc
- en: '[PRE170]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: '[PRE171]'
  id: totrans-522
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: '[PRE172]'
  id: totrans-523
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'To benchmark, first, initialize some arrays:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行基准测试，首先初始化一些数组：
- en: '[PRE173]'
  id: totrans-525
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: '[PRE174]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: '[PRE175]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: '[PRE176]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: Note
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'Numba automatically did a lot of things for us:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: Numba自动为我们做了很多事情：
- en: Memory was allocated on GPU
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GPU上分配了内存
- en: Data was copied from CPU and GPU
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据从CPU和GPU复制
- en: The kernel was configured and launched
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内核已配置并启动
- en: Data was copied back from GPU to CPU
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据从GPU复制回CPU
- en: Using ufuncs (or gfuncs) for GPU processing can be straightforward, but this
    approach may not always yield optimal performance due to automatic handling of
    data transfer to and from the GPU, as well as kernel launching. Additionally,
    in practice, not every function can be constructed as a ufunc.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ufunc（或gfunc）进行GPU处理可能很简单，但由于自动处理数据在GPU之间传输以及内核启动，这种方法可能不会总是产生最佳性能。此外，在实践中，并非每个函数都可以构造为ufunc。
- en: To gain greater control and flexibility, one may need to craft their own kernels
    and manually manage data transfer. Refer to the *Python for HPDA* resource linked
    below for guidance on implementing such techniques using Numba.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更大的控制和灵活性，可能需要自己编写内核并手动管理数据传输。请参考以下链接的 *Python for HPDA* 资源，以获取使用Numba实现此类技术的指导。
- en: Exercises
  id: totrans-537
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Play around yourself
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 亲自尝试
- en: Are you a Julian or a Pythonista? Maybe neither, but take a pick between Python
    and Julia and play around with the code examples provided above.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 你是Julian还是Pythonista？也许两者都不是，但请从Python和Julia中选择一个，并尝试上面的代码示例。
- en: You can find instructions for running Julia on LUMI and Python on LUMI / Google
    Colab in the [Setup](../0-setup/) episode.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[设置](../0-setup/)章节中找到在LUMI上运行Julia和在LUMI / Google Colab上运行Python的说明。
- en: See also
  id: totrans-541
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Introduction to programming in Julia (ENCCS)](https://enccs.github.io/julia-intro/)'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Julia编程入门 (ENCCS)](https://enccs.github.io/julia-intro/)'
- en: '[Julia for High-Performance Scientific Computing (ENCCS)](https://enccs.github.io/julia-for-hpc/)'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高性能科学计算中的Julia (ENCCS)](https://enccs.github.io/julia-for-hpc/)'
- en: '[Julia for high-performance data analytics (ENCCS)](https://enccs.github.io/julia-for-hpda/)'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[高性能数据分析中的Julia (ENCCS)](https://enccs.github.io/julia-for-hpda/)'
- en: '[Introduction to running R, Python, Julia, and Matlab in HPC (NAISS-LUNARC-HPC2N-UPPMAX)](https://uppmax.github.io/R-python-julia-matlab-HPC/)'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在HPC中运行R、Python、Julia和Matlab的入门 (NAISS-LUNARC-HPC2N-UPPMAX)](https://uppmax.github.io/R-python-julia-matlab-HPC/)'
- en: '[High Performance Data Analytics in Python (ENCCS)](https://enccs.github.io/hpda-python/)'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Python中的高性能数据分析 (ENCCS)](https://enccs.github.io/hpda-python/)'
- en: '[Practical Intro to GPU Programming using Python (ENCCS)](https://github.com/ENCCS/webinar_documents/tree/main/2024-oct-24-python)'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[使用Python进行GPU编程的实际入门 (ENCCS)](https://github.com/ENCCS/webinar_documents/tree/main/2024-oct-24-python)'
- en: '[Using Python in an HPC environment (UPPMAX-HPC2N)](https://uppmax.github.io/HPC-python/)'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在HPC环境中使用Python (UPPMAX-HPC2N)](https://uppmax.github.io/HPC-python/)'
- en: '[Python for Scientific Computing (Aalto Scientific Computing)](https://aaltoscicomp.github.io/python-for-scicomp/)*'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[科学计算中的Python (Aalto科学计算)](https://aaltoscicomp.github.io/python-for-scicomp/)*'
