- en: Part III
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分
- en: Chapter 11\. Streaming Workloads
  id: totrans-1
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11章 流式工作负载
- en: 'Streaming workloads are among the simplest that can be ported to CUDA: computations
    where each data element can be computed independently of the others, often with
    such low computational density that the workload is bandwidth-bound. Streaming
    workloads do not use many of the hardware resources of the GPU, such as caches
    and shared memory, that are designed to optimize reuse of data.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 流式工作负载是最简单的 CUDA 移植类型之一：每个数据元素可以独立计算，通常计算密度如此低，以至于工作负载受带宽限制。流式工作负载不会使用 GPU 的许多硬件资源，如缓存和共享内存，这些资源旨在优化数据的重用。
- en: Since GPUs give the biggest benefits on workloads with *high* computational
    density, it might be useful to review some cases when it still makes sense for
    streaming workloads to port to GPUs.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GPU 在具有*高*计算密度的工作负载上提供最大的性能提升，因此回顾一些情况下流式工作负载仍然值得迁移到 GPU 上可能会很有用。
- en: • If the input and output are in device memory, it doesn’t make sense to transfer
    the data back to the CPU just to perform one operation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: • 如果输入和输出都在设备内存中，那么仅为了执行一个操作将数据传输回 CPU 是没有意义的。
- en: • If the GPU has much better instruction-level support than the CPU for the
    operation (e.g., Black-Scholes options computation, which uses Special Function
    Unit instructions intensively), the GPU can outperform the CPU despite memory
    transfer overhead.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: • 如果 GPU 在操作方面提供比 CPU 更强的指令级支持（例如，Black-Scholes 期权计算，它大量使用特殊功能单元指令），那么即便有内存传输开销，GPU
    也能超过 CPU 的表现。
- en: • The GPU operating concurrently with the CPU can approximately double performance,
    even if they are the same speed.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: • 即使 CPU 和 GPU 的速度相同，GPU 与 CPU 并行操作时，性能也能大致翻倍。
- en: • The CUDA code for a given workload may be more readable or maintainable than
    highly optimized CPU code for the same computation.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: • 对于给定的工作负载，CUDA 代码可能比针对同一计算进行高度优化的 CPU 代码更易读或更易维护。
- en: • On integrated systems (i.e., systems-on-a-chip with CPU and CUDA-capable GPU
    operating on the same memory), there is no transfer overhead. CUDA can use “zero-copy”
    methods and avoid the copy entirely.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: • 在集成系统（即，CPU 和支持 CUDA 的 GPU 在同一内存上运行的系统）上，数据传输开销不存在。CUDA 可以使用“零复制”方法，完全避免复制。
- en: This chapter covers every aspect of streaming workloads, giving different formulations
    of the same workload to highlight the different issues that arise. The workload
    in question—the SAXPY operation from the BLAS library—performs a scalar multiplication
    and vector addition together in a single operation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了流式工作负载的各个方面，给出了同一工作负载的不同表述，以突出不同问题的出现。所讨论的工作负载——来自 BLAS 库的 SAXPY 操作——将标量乘法和向量加法结合在一个操作中进行。
- en: '[Listing 11.1](ch11.html#ch11lis01) gives a trivial C implementation of SAXPY.
    For corresponding elements in the two input arrays, one element is scaled by a
    constant, added to the other, and written to the output array. Both input arrays
    and the output arrays consist of *N* elements. Since GPUs have a native multiply-add
    instruction, the innermost loop of SAXPY has an extremely modest number of instructions
    per memory access.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 11.1](ch11.html#ch11lis01) 给出了一个简单的 C 实现的 SAXPY。对于两个输入数组中的对应元素，一个元素由常数缩放后加到另一个元素上，并写入输出数组。输入数组和输出数组都包含
    *N* 个元素。由于 GPU 有原生的乘加指令，SAXPY 的最内层循环在每次内存访问时的指令数非常少。'
- en: '*Listing 11.1.* saxpyCPU.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11.1.* saxpyCPU。'
- en: '[Click here to view code image](ch11_images.html#p11lis01a)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch11_images.html#p11lis01a)'
- en: '* * *'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: void
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: saxpyCPU(
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: saxpyCPU(
- en: float *out,
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: float *out，
- en: const float *x,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: const float *x，
- en: const float *y,
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: const float *y，
- en: size_t N,
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N，
- en: float alpha )
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: float alpha )
- en: '{'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = 0; i < N; i++ ) {
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = 0; i < N; i++ ) {
- en: out[i] += alpha*x[i]+y[i];
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] += alpha*x[i]+y[i];
- en: '}'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 11.2](ch11.html#ch11lis02) gives a trivial CUDA implementation of
    SAXPY. This version works for any grid or block size, and it performs adequately
    for most applications. This kernel is so bandwidth-bound that most applications
    would benefit more from restructuring the application to increase the computational
    density than from optimizing this tiny kernel.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 11.2](ch11.html#ch11lis02) 给出了一个简单的 CUDA 实现的 SAXPY。这个版本适用于任何网格或块大小，并且对于大多数应用程序来说，它的表现是足够的。这个内核的带宽需求非常高，因此大多数应用程序通过重构应用程序来增加计算密度，而不是优化这个小内核，会得到更多的性能提升。'
- en: '*Listing 11.2.* saxpyGPU.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11.2.* saxpyGPU。'
- en: '[Click here to view code image](ch11_images.html#p11lis02a)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch11_images.html#p11lis02a)'
- en: '* * *'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: saxpyGPU(
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: saxpyGPU(
- en: float *out,
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: float *out，
- en: const float *x,
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: const float *x，
- en: const float *y,
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: const float *y，
- en: size_t N,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N，
- en: float alpha )
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: float alpha )
- en: '{'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x；
- en: i < N;
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: i < N；
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: out[i] = alpha*x[i]+y[i];
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] = alpha*x[i]+y[i];
- en: '}'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The bulk of this chapter discusses how to move data to and from host memory
    efficiently, but first we’ll spend a moment examining how to improve this kernel’s
    performance when operating on device memory.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要内容讨论如何高效地将数据移动到主机内存和设备内存之间，但首先我们将花一点时间来检查如何在操作设备内存时提高此内核的性能。
- en: 11.1\. Device Memory
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1\. 设备内存
- en: 'If the input and output data are in device memory, optimizing a low-density
    computation such as SAXPY is a matter of optimizing the global memory access.
    Besides alignment and coalescing constraints that inform performance, CUDA kernels
    are sensitive to the number of blocks and threads per block. The `globalRead`,
    `globalWrite`, `globalCopy`, and `globalCopy2` applications (in the `memory/`
    subdirectory of the source code) generate reports for the bandwidths achieved
    for a variety of operand sizes, block sizes, and loop unroll factors. A sample
    report generated by `globalCopy2` (which follows a memory access pattern similar
    to SAXPY: two reads and one write per loop iteration) is given in [Listing 11.3](ch11.html#ch11lis03).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入和输出数据位于设备内存中，优化像SAXPY这样的低密度计算就是优化全局内存访问的问题。除了对性能有影响的对齐和合并约束外，CUDA内核还对每个块中的块数和线程数敏感。`globalRead`、`globalWrite`、`globalCopy`
    和 `globalCopy2` 应用程序（位于源代码的 `memory/` 子目录中）会生成关于不同操作数大小、块大小和循环展开因子所达到的带宽的报告。由
    `globalCopy2` 生成的示例报告（它遵循类似SAXPY的内存访问模式：每次循环迭代读取两次并写一次）见于[Listing 11.3](ch11.html#ch11lis03)。
- en: If we reference the `globalCopy2.cu` application from [Chapter 5](ch05.html#ch05)
    (see [Listing 5.8](ch05.html#ch05lis08)), running it on a GK104 gets us the output
    in [Listing 11.3](ch11.html#ch11lis03) for 4-byte operands. The top row (unroll
    factor of 1) corresponds to the naïve implementation (similar to [Listing 11.2](ch11.html#ch11lis02));
    a slight performance benefit is observed when the loop is unrolled. An unroll
    factor of 4 gives a speedup of about 10%, delivering 128 GiB/s of bandwidth as
    opposed to the naïve implementation’s 116 GiB/s.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们参考[第5章](ch05.html#ch05)中的 `globalCopy2.cu` 应用程序（见[Listing 5.8](ch05.html#ch05lis08)），在GK104上运行它，我们可以得到[Listing
    11.3](ch11.html#ch11lis03)中对于4字节操作数的输出。最上面一行（展开因子为1）对应的是简单实现（类似于[Listing 11.2](ch11.html#ch11lis02)）；当循环展开时，观察到轻微的性能提升。展开因子为4时，性能提升大约10%，带宽从简单实现的116
    GiB/s提升至128 GiB/s。
- en: Interestingly, using the `#pragma` `unroll` compiler directive only increases
    performance to about 118 GiB/s, while modifying the templated kernel from `globalCopy2.cu`
    to perform SAXPY increases performance to 135 GiB/s. [Listing 11.4](ch11.html#ch11lis04)
    gives the resulting kernel, which is implemented in the `stream1Device.cu` application
    (`cudahandbook/streaming/`).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，使用 `#pragma` `unroll` 编译器指令仅将性能提升到大约118 GiB/s，而将 `globalCopy2.cu` 的模板化内核修改为执行SAXPY计算，则性能提升至135
    GiB/s。[Listing 11.4](ch11.html#ch11lis04)给出了最终的内核实现，该内核在 `stream1Device.cu` 应用程序中实现（位于`cudahandbook/streaming/`）。
- en: For most applications, these small performance differences don’t justify rewriting
    kernels in this way. But if kernels are written to be “blocking-agnostic” (i.e.,
    to work correctly for any grid or block size), then the optimal settings can be
    determined empirically without too much effort.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数应用而言，这些小的性能差异不足以通过这种方式重写内核。但是，如果内核被编写成“与块无关”（即能够正确地适应任何网格或块大小），那么最优设置可以通过经验方法轻松确定。
- en: '*Listing 11.3.* `globalCopy2` output (GK104).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 11.3.* `globalCopy2` 输出（GK104）。'
- en: '[Click here to view code image](ch11_images.html#p11lis03a)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch11_images.html#p11lis03a)'
- en: '* * *'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Operand size: 4 bytes'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 操作数大小：4 字节
- en: 'Input size: 16M operands'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 输入大小：16M 操作数
- en: Block Size
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 块大小
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 展开  32      64      128     256     512     最大带宽   最大线程数
- en: 1       63.21   90.89   104.64  113.45  116.06  116.06  512
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 1       63.21   90.89   104.64  113.45  116.06  116.06  512
- en: 2       66.43   92.89   105.09  116.35  120.66  120.66  512
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 2       66.43   92.89   105.09  116.35  120.66  120.66  512
- en: 3       87.23   100.70  112.07  110.85  121.36  121.36  512
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 3       87.23   100.70  112.07  110.85  121.36  121.36  512
- en: 4       99.54   103.53  113.58  119.52  128.64  128.64  512
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 4       99.54   103.53  113.58  119.52  128.64  128.64  512
- en: 5       94.27   103.56  108.02  122.82  124.88  124.88  512
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 5       94.27   103.56  108.02  122.82  124.88  124.88  512
- en: 6       100.67  104.18  115.10  122.05  122.46  122.46  512
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 6       100.67  104.18  115.10  122.05  122.46  122.46  512
- en: 7       94.56   106.09  116.30  117.63  114.50  117.63  256
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 7       94.56   106.09  116.30  117.63  114.50  117.63  256
- en: 8       58.27   45.10   47.07   46.29   45.18   58.27   32
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 8       58.27   45.10   47.07   46.29   45.18   58.27   32
- en: 9       41.20   34.74   35.87   35.49   34.58   41.20   32
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 9       41.20   34.74   35.87   35.49   34.58   41.20   32
- en: 10      33.59   31.97   32.42   31.43   30.61   33.59   32
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 10      33.59   31.97   32.42   31.43   30.61   33.59   32
- en: 11      27.76   28.17   28.46   27.83   26.79   28.46   128
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 11      27.76   28.17   28.46   27.83   26.79   28.46   128
- en: 12      25.59   26.42   26.54   25.72   24.51   26.54   128
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 12      25.59   26.42   26.54   25.72   24.51   26.54   128
- en: 13      22.69   23.07   23.54   22.50   20.71   23.54   128
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 13      22.69   23.07   23.54   22.50   20.71   23.54   128
- en: 14      22.19   22.40   22.23   21.10   19.00   22.40   64
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 14      22.19   22.40   22.23   21.10   19.00   22.40   64
- en: 15      20.94   21.14   20.98   19.62   17.31   21.14   64
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 15      20.94   21.14   20.98   19.62   17.31   21.14   64
- en: 16      18.86   19.01   18.97   17.66   15.40   19.01   64
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 16      18.86   19.01   18.97   17.66   15.40   19.01   64
- en: '* * *'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 11.4.* saxpyGPU (templated unroll).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 11.4.* saxpyGPU (模板展开).'
- en: '[Click here to view code image](ch11_images.html#p11lis04a)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch11_images.html#p11lis04a)'
- en: '* * *'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<const int n>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: template<const int n>
- en: __device__ void
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ void
- en: saxpy_unrolled(
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: saxpy_unrolled(
- en: float *out,
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: float *out,
- en: const float *px,
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: const float *px,
- en: const float *py,
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: const float *py,
- en: size_t N,
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: float alpha )
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: float alpha )
- en: '{'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float x[n], y[n];
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: float x[n], y[n];
- en: size_t i;
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i;
- en: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
- en: i < N-n*blockDim.x*gridDim.x;
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: i < N-n*blockDim.x*gridDim.x;
- en: i += n*blockDim.x*gridDim.x ) {
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: i += n*blockDim.x*gridDim.x ) {
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: x[j] = px[index];
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: x[j] = px[index];
- en: y[j] = py[index];
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: y[j] = py[index];
- en: '}'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: out[index] = alpha*x[j]+y[j];
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: out[index] = alpha*x[j]+y[j];
- en: '}'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // to avoid the (index<N) conditional in the inner loop,
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: // 为了避免内层循环中的 (index<N) 条件判断，
- en: // we left off some work at the end
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: // 我们在最后留下了一些工作
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: if ( index<N ) {
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index<N ) {
- en: x[j] = px[index];
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: x[j] = px[index];
- en: y[j] = py[index];
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: y[j] = py[index];
- en: '}'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: size_t index = i+j*blockDim.x;
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+j*blockDim.x;
- en: if ( index<N ) out[index] = alpha*x[j]+y[j];
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( index<N ) out[index] = alpha*x[j]+y[j];
- en: '}'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __global__ void
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: saxpyGPU( float *out, const float *px, const float *py, size_t N, float alpha
    )
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: saxpyGPU( float *out, const float *px, const float *py, size_t N, float alpha
    )
- en: '{'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: saxpy_unrolled<4>( out, px, py, N, alpha );
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: saxpy_unrolled<4>( out, px, py, N, alpha );
- en: '}'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The `stream1Device.cu` application reports the total wall clock time needed
    to transfer data from pageable system memory to device memory, operate on the
    data with the kernel in [Listing 11.4](ch11.html#ch11lis04), and transfer the
    data back. On a test system with an Intel i7 running Windows 7 on a GeForce GTX
    680, the output of this application is as follows.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`stream1Device.cu`应用程序报告了从可分页系统内存到设备内存传输数据、使用[第11.4节](ch11.html#ch11lis04)中的内核对数据进行操作以及将数据传回的总墙钟时间。在一台配备Intel
    i7处理器、运行Windows 7并使用GeForce GTX 680显卡的测试系统上，该应用程序的输出如下。'
- en: '[Click here to view code image](ch11_images.html#p357pro02a)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch11_images.html#p357pro02a)'
- en: Measuring times with 128M floats (use --N to specify number of Mfloats)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 测量128M浮点数的时间（使用--N指定浮点数的数量）
- en: 'Memcpy( host->device ): 365.95 ms (2934.15 MB/s)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'Memcpy( host->device ): 365.95 毫秒 (2934.15 MB/s)'
- en: 'Kernel processing : 11.94 ms (134920.75 MB/s)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 内核处理：11.94 毫秒 (134920.75 MB/s)
- en: 'Memcpy (device->host ): 188.72 ms (2844.73 MB/s)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 'Memcpy (device->host ): 188.72 毫秒 (2844.73 MB/s)'
- en: 'Total time (wall clock): 570.22 ms (2815.30 MB/s)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 总时间（墙钟时间）：570.22 毫秒 (2815.30 MB/s)
- en: The kernel takes a tiny amount of the overall execution time—about 2% of the
    wall clock time. The other 98% of time is spent transferring data to and from
    the GPU! For transfer-bound workloads like this one, if some or all of the data
    being operated on is in host memory, the best way to optimize the application
    is to improve CPU/GPU overlap and transfer performance.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 内核只占用总体执行时间的一小部分——大约2%的墙钟时间。其余的98%的时间用于将数据传输到GPU和从GPU传输回来！对于像这样的传输绑定型工作负载，如果一些或全部操作的数据位于主机内存中，优化应用程序的最佳方法是改善CPU/GPU的重叠性和传输性能。
- en: 11.2\. Asynchronous Memcpy
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2\. 异步Memcpy
- en: Unless the input and output data can stay resident on the GPU, the logistics
    of streaming the data through the GPU—copying the input and output data to and
    from device memory—become the primary consideration. The two tools best suited
    to improve transfer performance are pinned memory and asynchronous memcpy (which
    can only operate on pinned memory).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 除非输入和输出数据可以保持驻留在GPU上，否则通过GPU流式传输数据——将输入和输出数据复制到设备内存并从设备内存复制回来——就成了主要的考虑因素。两种最适合提高传输性能的工具是固定内存和异步memcpy（它只能在固定内存上操作）。
- en: The `stream2Async.cu` application illustrates the effect of moving the pageable
    memory of `stream1Device.cu` to pinned memory and invoking the memcpys asynchronously.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`stream2Async.cu`应用程序演示了将`stream1Device.cu`的可分页内存移至固定内存并异步调用memcpys的效果。'
- en: '[Click here to view code image](ch11_images.html#p358pro01a)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch11_images.html#p358pro01a)'
- en: Measuring times with 128M floats (use --N to specify number of Mfloats)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 测量128M浮点数的时间（使用--N指定浮点数的数量）
- en: 'Memcpy( host->device ): 181.03 ms (5931.33 MB/s)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 'Memcpy( host->device ): 181.03 毫秒 (5931.33 MB/s)'
- en: 'Kernel processing : 13.87 ms (116152.99 MB/s)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 内核处理：13.87 毫秒 (116152.99 MB/s)
- en: 'Memcpy (device->host ): 90.07 ms (5960.35 MB/s)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 'Memcpy (device->host ): 90.07 毫秒 (5960.35 MB/s)'
- en: 'Total time (wall clock): 288.68 ms (5579.29 MB/s)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '总时间 (实际时钟): 288.68 毫秒 (5579.29 MB/s)'
- en: '[Listing 11.5](ch11.html#ch11lis05) contrasts the difference between the timed
    portions of `stream1-``Device.cu` (which performs synchronous transfers) and `stream2Async.cu`
    (which performs asynchronous transfers).^([1](ch11.html#ch11fn1)) In both cases,
    four CUDA events are used to record the times at the start, after the host→device
    transfers, after the kernel launch, and at the end. For `stream2Async.cu`, all
    of these operations are requested of the GPU in quick succession, and the GPU
    records the event times as it performs them. For `stream1Device.cu`, the GPU event-based
    times are a bit suspect, since for any `cudaMemcpy()`, calls must wait for the
    GPU to complete before proceeding, causing a pipeline bubble before the `cudaEventRecord()`
    calls for `evHtoD` and `evDtoH` are processed.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 11.5](ch11.html#ch11lis05) 对比了 `stream1Device.cu`（执行同步传输）与 `stream2Async.cu`（执行异步传输）的时间段差异。^([1](ch11.html#ch11fn1))
    在这两种情况下，都使用了四个 CUDA 事件来记录开始时、主机→设备传输后、内核启动后以及结束时的时间。对于 `stream2Async.cu`，所有这些操作都迅速依次请求
    GPU，并由 GPU 在执行时记录事件时间。对于 `stream1Device.cu`，由于任何 `cudaMemcpy()` 调用都必须等待 GPU 完成后才能继续，因此
    GPU 事件的时间可能不太准确，导致在 `evHtoD` 和 `evDtoH` 的 `cudaEventRecord()` 调用之前会出现管道气泡。'
- en: '[1](ch11.html#ch11fn1a). Error checking has been removed for clarity.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch11.html#ch11fn1a). 为了简洁起见，已删除错误检查。'
- en: 'Note that despite using the slower, naïve implementation of `saxpyGPU` (from
    [Listing 11.2](ch11.html#ch11lis02)), the wall clock time from this application
    shows that it completes the computation almost twice as fast: 289 ms versus 570.22
    ms. The combination of faster transfers and asynchronous execution delivers much
    better performance.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管使用了较慢的、天真的 `saxpyGPU` 实现（来自 [示例 11.2](ch11.html#ch11lis02)），该应用程序的实际时钟时间表明，它几乎以两倍的速度完成了计算：289
    毫秒与 570.22 毫秒。更快的数据传输和异步执行的结合大大提高了性能。
- en: 'Despite the improved performance, the application output highlights another
    performance opportunity: Some of the kernel processing can be performed concurrently
    with transfers. The next two sections describe two different methods to overlap
    kernel execution with transfers.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管性能有所提升，应用程序的输出仍然突出了另一个性能机会：部分内核处理可以与数据传输并行执行。接下来的两个部分描述了两种不同的方法，用于将内核执行与数据传输重叠。
- en: '*Listing 11.5.* Synchronous (`stream1Device.cu`) versus asynchronous (`stream2Async.cu`).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例 11.5.* 同步 (`stream1Device.cu`) 与异步 (`stream2Async.cu`)。'
- en: '[Click here to view code image](ch11_images.html#p11lis05a)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch11_images.html#p11lis05a)'
- en: '* * *'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: //
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // from stream1Device.cu
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: // 来自 stream1Device.cu
- en: //
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: cudaEventRecord( evStart, 0 );
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( evStart, 0 );
- en: cudaMemcpy( dptrX, hptrX, ..., cudaMemcpyHostToDevice );
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpy( dptrX, hptrX, ..., cudaMemcpyHostToDevice );
- en: cudaMemcpy( dptrY, hptrY, ..., cudaMemcpyHostToDevice );
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpy( dptrY, hptrY, ..., cudaMemcpyHostToDevice );
- en: cudaEventRecord( evHtoD, 0 );
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( evHtoD, 0 );
- en: saxpyGPU<<<nBlocks, nThreads>>>( dptrOut, dptrX, dptrY, N, alpha;
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: saxpyGPU<<<nBlocks, nThreads>>>( dptrOut, dptrX, dptrY, N, alpha );
- en: cudaEventRecord( evKernel, 0 );
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( evKernel, 0 );
- en: cudaMemcpy( hptrOut, dptrOut, N*sizeof(float), cudaMemcpyDeviceToHost );
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpy( hptrOut, dptrOut, N*sizeof(float), cudaMemcpyDeviceToHost );
- en: cudaEventRecord( evDtoH, 0 );
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( evDtoH, 0 );
- en: cudaDeviceSynchronize();
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceSynchronize();
- en: //
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // from stream2Async.cu
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: // 来自stream2Async.cu
- en: //
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: cudaEventRecord( evStart, 0 );
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( evStart, 0 );
- en: cudaMemcpyAsync( dptrX, hptrX, ..., cudaMemcpyHostToDevice, NULL );
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync( dptrX, hptrX, ..., cudaMemcpyHostToDevice, NULL );
- en: cudaMemcpyAsync( dptrY, hptrY, ..., cudaMemcpyHostToDevice, NULL );
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync( dptrY, hptrY, ..., cudaMemcpyHostToDevice, NULL );
- en: cudaEventRecord( evHtoD, 0 );
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( evHtoD, 0 );
- en: saxpyGPU<<<nBlocks, nThreads>>>( dptrOut, dptrX, dptrY, N, alpha;
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: saxpyGPU<<<nBlocks, nThreads>>>( dptrOut, dptrX, dptrY, N, alpha );
- en: cudaEventRecord( evKernel, 0 );
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( evKernel, 0 );
- en: cudaMemcpyAsync( hptrOut, dptrOut, N*sizeof(float), ... , NULL );
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyAsync( hptrOut, dptrOut, N*sizeof(float), ... , NULL );
- en: cudaEventRecord( evDtoH, 0 );
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( evDtoH, 0 );
- en: cudaDeviceSynchronize();
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceSynchronize();
- en: '* * *'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 11.3\. Streams
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3\. 流
- en: For workloads that benefit from concurrent memcpy and kernel execution (GPU/GPU
    overlap), CUDA streams can be used to coordinate execution. The `stream3Streams.cu`
    application splits the input and output arrays into k streams and then invokes
    k host→device memcpys, kernels, and device→host memcpys, each in their own stream.
    Associating the transfers and computations with different streams lets CUDA know
    that the computations are completely independent, and CUDA will exploit whatever
    parallelism opportunities the hardware can support. On GPUs with multiple copy
    engines, the GPU may be transferring data both to and from device memory while
    processing other data with the SMs.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些可以从并发的memcpy和内核执行中获益的工作负载（GPU/GPU重叠），可以使用CUDA流来协调执行。`stream3Streams.cu`应用程序将输入和输出数组分成k个流，然后分别在各自的流中调用k个主机→设备的memcpy、内核和设备→主机的memcpy。将传输和计算关联到不同的流，让CUDA知道计算是完全独立的，CUDA将利用硬件可以支持的任何并行机会。在具有多个复制引擎的GPU上，GPU可能在处理其他数据时，既向设备内存传输数据，又从设备内存传输数据。
- en: '[Listing 11.6](ch11.html#ch11lis06) shows an excerpt from `stream3Streams.cu`,
    the same portion of the application as shown in [Listing 11.5](ch11.html#ch11lis05).
    On the test system, the output from this application reads as follows.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单11.6](ch11.html#ch11lis06)展示了`stream3Streams.cu`中的一个片段，它与[清单11.5](ch11.html#ch11lis05)中的应用部分相同。在测试系统上，来自该应用程序的输出如下所示。'
- en: '[Click here to view code image](ch11_images.html#p360pro01a)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch11_images.html#p360pro01a)'
- en: Measuring times with 128M floats
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用128M浮点数测量时间
- en: Testing with default max of 8 streams (set with --maxStreams <count>)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用默认最大值8个流进行测试（使用--maxStreams <count>设置）
- en: Streams  Time (ms)  MB/s
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 流         时间 (ms)     MB/s
- en: 1        290.77 ms  5471.45
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 1        290.77 ms  5471.45
- en: 2        273.46 ms  5820.34
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 2        273.46 ms  5820.34
- en: 3        277.14 ms  5744.49
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 3        277.14 ms  5744.49
- en: 4        278.06 ms  5725.76
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 4        278.06 ms  5725.76
- en: 5        277.44 ms  5736.52
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 5        277.44 毫秒  5736.52
- en: 6        276.56 ms  5751.87
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 6        276.56 毫秒  5751.87
- en: 7        274.75 ms  5793.43
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 7        274.75 毫秒  5793.43
- en: 8        275.41 ms  5779.51
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 8        275.41 毫秒  5779.51
- en: The GPU in question has only one copy engine, so it is not surprising that the
    case with 2 streams delivers the highest performance. If the kernel execution
    time were more in line with the transfer time, it would likely be beneficial to
    split the arrays into more than 2 subarrays. As things stand, the first kernel
    launch cannot begin processing until the first host→device memcpy is done, and
    the final device→host memcpy cannot begin until the last kernel launch is done.
    If the kernel processing took more time, this “overhang” would be more pronounced.
    For our application, the wall clock time of 273 ms shows that most of the kernel
    processing (13.87 ms) has been hidden.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 该GPU只有一个复制引擎，因此，两个流的情况能够提供最高性能并不令人惊讶。如果内核执行时间与传输时间更为匹配，可能有益于将数组拆分成多个子数组，而不仅仅是两个。就目前情况而言，第一次内核启动不能开始处理，直到第一次主机→设备的memcpy完成，最后一次设备→主机的memcpy也不能开始，直到最后一次内核启动完成。如果内核处理时间更长，这种“悬挂”现象会更加明显。对于我们的应用程序，273毫秒的时钟时间表明，大部分内核处理时间（13.87毫秒）已经被隐藏了。
- en: Note that in this formulation, partly due to hardware limitations, we are not
    trying to insert any `cudaEventRecord()` calls between operations, as we did in
    [Listing 11.5](ch11.html#ch11lis05). On most CUDA hardware, trying to record events
    between the streamed operations in [Listing 11.6](ch11.html#ch11lis06) would break
    concurrency and reduce performance. Instead, we bracket the operations with one
    `cudaEventRecord()` before and one `cudaEventRecord()` after.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种表达方式中，部分由于硬件限制，我们没有像在[列表11.5](ch11.html#ch11lis05)中那样尝试在操作之间插入任何`cudaEventRecord()`调用。在大多数CUDA硬件上，尝试在[列表11.6](ch11.html#ch11lis06)中的流式操作之间记录事件会破坏并发性并降低性能。相反，我们在操作之前和之后分别用一个`cudaEventRecord()`来括起来。
- en: '*Listing 11.6.* `stream3Streams.cu` excerpt.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表11.6.* `stream3Streams.cu`摘录。'
- en: '[Click here to view code image](ch11_images.html#p11lis06a)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch11_images.html#p11lis06a)'
- en: '* * *'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: for ( int iStream = 0; iStream < nStreams; iStream++ ) {
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int iStream = 0; iStream < nStreams; iStream++ ) {
- en: CUDART_CHECK( cudaMemcpyAsync(
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpyAsync(
- en: dptrX+iStream*streamStep,
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: dptrX+iStream*streamStep,
- en: hptrX+iStream*streamStep,
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: hptrX+iStream*streamStep,
- en: streamStep*sizeof(float),
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: streamStep*sizeof(float),
- en: cudaMemcpyHostToDevice,
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice,
- en: streams[iStream] ) );
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: streams[iStream] ) );
- en: CUDART_CHECK( cudaMemcpyAsync(
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpyAsync(
- en: dptrY+iStream*streamStep,
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: dptrY+iStream*streamStep,
- en: hptrY+iStream*streamStep,
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: hptrY+iStream*streamStep,
- en: streamStep*sizeof(float),
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: streamStep*sizeof(float),
- en: cudaMemcpyHostToDevice,
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyHostToDevice,
- en: streams[iStream] ) );
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: streams[iStream] ) );
- en: '}'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int iStream = 0; iStream < nStreams; iStream++ ) {
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int iStream = 0; iStream < nStreams; iStream++ ) {
- en: saxpyGPU<<<nBlocks, nThreads, 0, streams[iStream]>>>(
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: saxpyGPU<<<nBlocks, nThreads, 0, streams[iStream]>>>(
- en: dptrOut+iStream*streamStep,
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: dptrOut+iStream*streamStep,
- en: dptrX+iStream*streamStep,
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: dptrX+iStream*streamStep,
- en: dptrY+iStream*streamStep,
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: dptrY+iStream*streamStep,
- en: streamStep,
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: streamStep,
- en: alpha );
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: alpha );
- en: '}'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int iStream = 0; iStream < nStreams; iStream++ ) {
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int iStream = 0; iStream < nStreams; iStream++ ) {
- en: CUDART_CHECK( cudaMemcpyAsync(
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpyAsync(
- en: hptrOut+iStream*streamStep,
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: hptrOut+iStream*streamStep，
- en: dptrOut+iStream*streamStep,
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: dptrOut+iStream*streamStep，
- en: streamStep*sizeof(float),
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: streamStep*sizeof(float)，
- en: cudaMemcpyDeviceToHost,
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToHost，
- en: streams[iStream] ) );
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: streams[iStream] ) );
- en: '}'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 11.4\. Mapped Pinned Memory
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4\. 映射固定内存
- en: For transfer-bound, streaming workloads such as SAXPY, reformulating the application
    to use mapped pinned memory for both the input and output confers a number of
    benefits.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于传输绑定的流式工作负载，如SAXPY，将应用程序重构为使用映射固定内存来处理输入和输出，会带来许多好处。
- en: • As shown by the excerpt from `stream4Mapped.cu` ([Listing 11.7](ch11.html#ch11lis07)),
    it eliminates the need to call `cudaMemcpy()`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: • 如`stream4Mapped.cu`中的摘录所示（[清单 11.7](ch11.html#ch11lis07)），它消除了调用`cudaMemcpy()`的需求。
- en: • It eliminates the need to allocate device memory.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: • 它消除了分配设备内存的需求。
- en: • For discrete GPUs, mapped pinned memory performs bus transfers but minimizes
    the amount of “overhang” alluded to in the previous section. Instead of waiting
    for a host→device memcpy to finish, input data can be processed by the SMs as
    soon as it arrives. Instead of waiting for a kernel to complete before initiating
    a device →host transfer, the data is posted to the bus as soon as the SMs are
    done processing.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: • 对于离散GPU，映射固定内存执行总线传输，但最大限度地减少了前一节提到的“悬挂”现象。输入数据可以在到达后立即由SM处理，而无需等待主机→设备的内存拷贝完成。在内核完成之前，数据也会立即传输到总线上，而无需等待设备→主机的传输。
- en: • For integrated GPUs, host and device memory exist in the same memory pool,
    so mapped pinned memory enables “zero copy” and eliminates any need to transfer
    data over the bus at all.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: • 对于集成GPU，主机和设备内存位于同一内存池中，因此映射固定内存实现了“零拷贝”，并完全消除了通过总线传输数据的需求。
- en: '*Listing 11.7.* `stream4Mapped` excerpt.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 11.7.* `stream4Mapped`摘录。'
- en: '[Click here to view code image](ch11_images.html#p11lis07a)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch11_images.html#p11lis07a)'
- en: '* * *'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: chTimerGetTime( &chStart );
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &chStart );
- en: cudaEventRecord( evStart, 0 );
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( evStart, 0 );
- en: saxpyGPU<<<nBlocks, nThreads>>>( dptrOut, dptrX, dptrY, N, alpha );
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: saxpyGPU<<<nBlocks, nThreads>>>( dptrOut, dptrX, dptrY, N, alpha );
- en: cudaEventRecord( evStop, 0 );
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventRecord( evStop, 0 );
- en: cudaDeviceSynchronize();
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceSynchronize();
- en: '* * *'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Mapped pinned memory works especially well when writing to host memory (for
    example, to deliver the result of a reduction to the host) because unlike reads,
    there is no need to wait until writes arrive before continuing execution.^([2](ch11.html#ch11fn2))
    Workloads that read mapped pinned memory are more problematic. If the GPU cannot
    sustain full bus performance while reading from mapped pinned memory, the smaller
    transfer performance may overwhelm the benefits of a smaller overhang. Also, for
    some workloads, the SMs have better things to do than drive (and wait for) PCI
    Express bus traffic.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 映射的固定内存在写入主机内存时尤其有效（例如，将归约结果传送给主机），因为与读取不同，写入无需等待读取完成就可以继续执行。^([2](ch11.html#ch11fn2))
    读取映射的固定内存的工作负载更具挑战性。如果GPU在读取映射的固定内存时无法维持完整的总线性能，则较小的传输性能可能会压倒较小悬挂的好处。此外，对于某些工作负载，SM可能有更重要的任务要做，而不是驱动（和等待）PCI
    Express总线流量。
- en: '[2](ch11.html#ch11fn2a). Hardware designers call this “covering the latency.”'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch11.html#ch11fn2a)。硬件设计师称之为“覆盖延迟”。'
- en: In the case of our application, on our test system, mapped pinned memory is
    a definite win.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的应用程序中，在我们的测试系统上，映射的固定内存确实是一个优势。
- en: '[Click here to view code image](ch11_images.html#p362lis02a)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch11_images.html#p362lis02a)'
- en: Measuring times with 128M floats (use --N to specify number of Mfloats)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用128M浮点数测量时间（使用--N指定浮点数数量）
- en: 'Total time: 204.54 ms (7874.45 MB/s)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 总时间：204.54毫秒（7874.45 MB/s）
- en: It completes the computation in 204.54 ms, significantly faster than the 273
    ms of the second-fastest implementation. The effective bandwidth of 7.9 GiB/s
    shows that the GPU is pushing both directions of PCI Express.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 它在204.54毫秒内完成计算，显著快于第二快实现的273毫秒。有效带宽为7.9 GiB/s，表明GPU在推动PCI Express的双向数据流。
- en: Not all combinations of systems and GPUs can sustain such high levels of performance
    with mapped pinned memory. If there’s any doubt, keep the data in device memory
    and use the asynchronous memcpy formulations, similar to `stream2Async.cu`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有系统和GPU的组合都能在映射的固定内存下维持如此高的性能。如果有任何疑虑，保持数据在设备内存中，并使用异步内存拷贝方法，类似于`stream2Async.cu`。
- en: 11.5\. Performance and Summary
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5\. 性能和总结
- en: This chapter covers four different implementations of SAXPY, emphasizing different
    strategies of data movement.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了SAXPY的四种不同实现，重点强调了不同的数据移动策略。
- en: • Synchronous memcpy to and from device memory
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: • 同步内存拷贝到设备内存和从设备内存拷贝
- en: • Asynchronous memcpy to and from device memory
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: • 异步内存拷贝到设备内存和从设备内存拷贝
- en: • Asynchronous memcpy using streams
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用流的异步内存拷贝
- en: • Mapped pinned memory
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: • 映射的固定内存
- en: '[Table 11.1](ch11.html#ch11tab01) and [Figure 11.1](ch11.html#ch11fig01) summarize
    the relative performance of these implementations for 128M floats on GK104s plugged
    into two different test systems: the Intel i7 system (PCI Express 2.0) and an
    Intel Xeon E5-2670 (PCI Express 3.0). The benefits of PCI Express 3.0 are evident,
    as they are about twice as fast. Additionally, the overhead of CPU/GPU synchronization
    is higher on the E5-2670, since the pageable memcpy operations are slower.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 11.1](ch11.html#ch11tab01) 和 [图 11.1](ch11.html#ch11fig01) 总结了这几种实现方法在 128M
    浮点数上的相对性能，测试系统为两种不同的系统：Intel i7 系统（PCI Express 2.0）和 Intel Xeon E5-2670（PCI Express
    3.0）。PCI Express 3.0 的优势非常明显，速度约为其两倍。此外，由于分页的 memcpy 操作较慢，E5-2670 上的 CPU/GPU 同步开销较高。'
- en: '![Image](graphics/11tab01.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/11tab01.jpg)'
- en: '*Table 11.1* Streaming Performance'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 11.1* 流式性能'
- en: '![Image](graphics/11fig01.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/11fig01.jpg)'
- en: '*Figure 11.1* Bandwidth (GeForce GTX 680 on Intel i7 versus Sandy Bridge)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 11.1* 带宽（GeForce GTX 680 在 Intel i7 与 Sandy Bridge 上的对比）'
- en: Chapter 12\. Reduction
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 12 章\. 归约
- en: Reduction is a class of parallel algorithms that pass over *O*(N) input data
    and generate a *O*(1) result computed with a binary associative operator ![Image](graphics/plus.jpg).
    Examples of such operations include minimum, maximum, sum, sum of squares, AND,
    OR, and the dot product of two vectors. Reduction is also an important primitive
    used as a subroutine in other operations, such as Scan (covered in the next chapter).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 归约是一类并行算法，遍历 *O*(N) 输入数据并生成 *O*(1) 结果，该结果是通过二元结合运算符 ![图片](graphics/plus.jpg)
    计算的。此类操作的例子包括最小值、最大值、和、平方和、与、或以及两个向量的点积。归约还是其他操作中的重要原语，例如扫描（将在下一章介绍）。
- en: Unless the operator ![Image](graphics/plus.jpg) is extremely expensive to evaluate,
    reduction tends to be bandwidth-bound. Our treatment of reduction begins with
    several two-pass implementations based on the `reduction` SDK sample. Next, the
    `threadFenceReduction` SDK sample shows how to perform reduction in a single pass
    so only one kernel must be invoked to perform the operation. Finally, the chapter
    concludes with a discussion of fast binary reduction with the `__syncthreads_count()`
    intrinsic (added with SM 2.0) and how to perform reduction using the warp shuffle
    instruction (added with SM 3.0).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 除非运算符 ![图片](graphics/plus.jpg) 计算代价非常高，否则归约通常受带宽限制。我们对归约的讨论从基于 `reduction` SDK
    示例的几种双遍实现开始。接下来，`threadFenceReduction` SDK 示例展示了如何在单遍操作中执行归约，这样只需要调用一个内核来执行操作。最后，本章以快速二元归约为结尾，介绍了如何使用
    `__syncthreads_count()` 内建函数（SM 2.0 添加）以及如何使用 warp shuffle 指令（SM 3.0 添加）执行归约。
- en: 12.1\. Overview
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.1\. 概述
- en: Since the binary operator is associative, the *O*(*N*) operations to compute
    a reduction may be performed in any order.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 由于二元运算符是结合性的，计算一个归约的*O*(*N*)操作可以按任意顺序执行。
- en: '![Image](graphics/365equ01.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/365equ01.jpg)'
- en: '[Figure 12.1](ch12.html#ch12fig01) shows some different options to process
    an 8-element array. The serial implementation is shown for contrast. Only one
    execution unit that can perform the ![Image](graphics/plus.jpg) operator is needed,
    but performance is poor because it takes 7 steps to complete the computation.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12.1](ch12.html#ch12fig01) 显示了处理一个 8 元素数组的不同选项。为了对比，展示了串行实现。只需要一个能够执行![Image](graphics/plus.jpg)运算符的执行单元，但性能较差，因为完成计算需要
    7 步。'
- en: '![Image](graphics/12fig01.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/12fig01.jpg)'
- en: '*Figure 12.1* Reduction of 8 elements.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.1* 8 元素的归约。'
- en: The pairwise formulation is intuitive and only requires *O*(lg*N*) steps (3
    in this case) to compute the result, but it exhibits poor performance in CUDA.
    When reading global memory, having a single thread access adjacent memory locations
    causes uncoalesced memory transactions. When reading shared memory, the pattern
    shown will cause bank conflicts.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 成对公式直观且仅需要 *O*(lg*N*) 步骤（在此例中为 3 步）来计算结果，但在 CUDA 中表现较差。读取全局内存时，单个线程访问相邻的内存位置会导致未合并的内存事务。读取共享内存时，显示的模式会导致银行冲突。
- en: For both global memory and shared memory, an interleaving-based strategy works
    better. In [Figure 12.1](ch12.html#ch12fig01), the interleaving factor is 4; for
    global memory, interleaving by a multiple of `blockDim.x *gridDim.x` has good
    performance because all memory transactions are coalesced. For shared memory,
    best performance is achieved by accumulating the partial sums with an interleaving
    factor chosen to avoid bank conflicts and to keep adjacent threads in the thread
    block active.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 对于全局内存和共享内存，基于交错的策略效果更好。在[图 12.1](ch12.html#ch12fig01)中，交错因子为 4；对于全局内存，按 `blockDim.x
    * gridDim.x` 的倍数交错具有良好的性能，因为所有内存事务都能合并。对于共享内存，最佳性能是通过选择一个交错因子来积累部分和，从而避免银行冲突，并保持线程块中的相邻线程处于活动状态。
- en: Once a thread block has finished processing its interleaved subarray, it writes
    the result to global memory for further processing by a subsequent kernel launch.
    It may seem expensive to launch multiple kernels, but kernel launches are asynchronous,
    so the CPU can request the next kernel launch while the GPU is executing the first;
    every kernel launch represents an opportunity to specify different launch configurations.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦一个线程块完成了其交错子数组的处理，它会将结果写入全局内存，以便后续的内核启动进行进一步处理。启动多个内核可能看起来很昂贵，但内核启动是异步的，因此
    CPU 可以在 GPU 执行第一个内核时请求下一个内核启动；每次内核启动都代表着一个机会，可以指定不同的启动配置。
- en: Since the performance of a kernel can vary with different thread and block sizes,
    it’s a good idea to write the kernel so it will work correctly for any valid combination
    of thread and block sizes. The optimal thread/block configuration then can be
    determined empirically.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内核的性能可能随着线程和块大小的不同而变化，因此编写内核时最好确保它能在任何有效的线程和块大小组合下正确工作。然后可以通过实验证明最佳的线程/块配置。
- en: The initial reduction kernels in this chapter illustrate some important CUDA
    programming concepts that may be familiar.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的初始归约核展示了一些可能熟悉的重要CUDA编程概念。
- en: • *Coalesced memory operations* to maximize bandwidth
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: • *合并内存操作*以最大化带宽
- en: • *Variable-sized shared memory* to facilitate collaboration between threads
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: • *可变大小共享内存*以促进线程间的协作
- en: • Avoiding shared memory *bank conflicts*
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: • 避免共享内存*银行冲突*
- en: The optimized reduction kernels illustrate more advanced CUDA programming idioms.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 优化后的归约核展示了更高级的CUDA编程习惯。
- en: • *Warp synchronous* coding avoids unneeded thread synchronization.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: • *Warp同步*编码避免了不必要的线程同步。
- en: • *Atomic operations* and *memory fences* eliminate the need to invoke multiple
    kernels.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: • *原子操作*和*内存屏障*消除了调用多个内核的需要。
- en: • The *shuffle* instruction enables warp-level reductions without the use of
    shared memory.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: • *shuffle*指令使得warp级别的归约得以实现，而无需使用共享内存。
- en: 12.2\. Two-Pass Reduction
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.2\. 两遍归约
- en: This algorithm operates in two stages. A kernel performs *NumBlocks* reductions
    in parallel, where *NumBlocks* is the number of blocks used to invoke the kernel;
    the results are written to an intermediate array. The final result is generated
    by invoking the same kernel to perform a second pass on the intermediate array
    with a single block. [Listing 12.1](ch12.html#ch12lis01) gives a two-pass reduction
    kernel that computes the sum of an array of integers.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法分为两个阶段。一个内核并行执行*NumBlocks*次归约，其中*NumBlocks*是调用内核时使用的块数；结果写入一个中间数组。最终结果通过调用相同的内核在中间数组上执行第二遍处理，使用一个块生成。[Listing
    12.1](ch12.html#ch12lis01)给出了一个两遍归约核，用于计算一个整数数组的和。
- en: '*Listing 12.1.* Two-pass reduction kernel.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 12.1.* 两遍归约核。'
- en: '[Click here to view code image](ch12_images.html#p12lis01a)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch12_images.html#p12lis01a)'
- en: '* * *'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: Reduction1_kernel( int *out, const int *in, size_t N )
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction1_kernel( int *out, const int *in, size_t N )
- en: '{'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ int sPartials[];
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ int sPartials[];
- en: int sum = 0;
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: int sum = 0;
- en: const int tid = threadIdx.x;
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: for ( size_t i = blockIdx.x*blockDim.x + tid;
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + tid;
- en: i < N;
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: sum += in[i];
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: sum += in[i];
- en: '}'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sPartials[tid] = sum;
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] = sum;
- en: __syncthreads();
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: for ( int activeThreads = blockDim.x>>1;
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int activeThreads = blockDim.x>>1;
- en: activeThreads;
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: activeThreads;
- en: activeThreads >>= 1 ) {
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: activeThreads >>= 1 ) {
- en: if ( tid < activeThreads ) {
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid < activeThreads ) {
- en: sPartials[tid] += sPartials[tid+activeThreads];
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] += sPartials[tid+activeThreads];
- en: '}'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if ( tid == 0 ) {
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid == 0 ) {
- en: out[blockIdx.x] = sPartials[0];
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: out[blockIdx.x] = sPartials[0];
- en: '}'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: Reduction1( int *answer, int *partial,
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction1( int *answer, int *partial,
- en: const int *in, size_t N,
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: const int *in, size_t N,
- en: int numBlocks, int numThreads )
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: int numBlocks, int numThreads )
- en: '{'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: unsigned int sharedSize = numThreads*sizeof(int);
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int sharedSize = numThreads*sizeof(int);
- en: Reduction1_kernel<<<
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction1_kernel<<<
- en: numBlocks, numThreads, sharedSize>>>(
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: numBlocks, numThreads, sharedSize>>>(
- en: partial, in, N );
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: partial, in, N );
- en: Reduction1_kernel<<<
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction1_kernel<<<
- en: 1, numThreads, sharedSize>>>(
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 1, numThreads, sharedSize>>>（
- en: answer, partial, numBlocks );
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: answer, partial, numBlocks );
- en: '}'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The shared memory array is used to accumulate the reduction within each thread
    block. Its size depends on the number of threads in the block, so it must be specified
    when the kernel is launched. *Note:* The number of threads in the block must be
    a power of 2!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存数组用于在每个线程块内积累归约结果。它的大小取决于线程块中的线程数量，因此在启动内核时必须指定。*注意：* 线程块中的线程数必须是 2 的幂！
- en: The first `for` loop computes the thread’s sum over the input array. If the
    input pointer is properly aligned, all of the memory transactions by this code
    are coalesced, and it will maximize the memory bandwidth. Each thread then writes
    its accumulated sum to shared memory and synchronizes before starting the log-step
    reduction.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个 `for` 循环计算线程在输入数组上的和。如果输入指针正确对齐，这段代码的所有内存事务都将合并，从而最大化内存带宽。然后每个线程将其累计和写入共享内存，并在开始对数步长归约前进行同步。
- en: The second `for` loop performs a log-step reduction over the values in shared
    memory. The values in the upper half of shared memory are added to the values
    in the lower half, and the number of participating threads is successively halved
    until one value in `shared_sum[0]` contains the output for that block. This part
    of the kernel is the one that requires that the thread block size be a power of
    2.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个 `for` 循环在共享内存中的值上执行对数步长归约。共享内存上半部分的值与下半部分的值相加，参与的线程数依次减半，直到 `shared_sum[0]`
    中的一个值包含该块的输出。这个内核部分要求线程块的大小是 2 的幂。
- en: 'Finally, the output value of the thread block is written to global memory.
    This kernel is intended to be invoked twice, as shown in the host function: once
    with *N* blocks, where *N* is chosen for maximum performance in performing the
    reduction over the input array, and then with 1 block to accumulate the final
    output. [Listing 12.2](ch12.html#ch12lis02) shows the host function that invokes
    `Reduction1_kernel()`. Note that an array for the partial sums is allocated and
    passed in separately. Also note that since the kernel uses an unsized shared memory
    array, the amount of shared memory needed by the kernel must be specified as the
    third parameter in the `<<< >>>` syntax.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，线程块的输出值被写入全局内存。该内核设计为调用两次，如主机函数中所示：第一次使用 *N* 个块，其中 *N* 选择为在对输入数组执行归约时获得最佳性能，然后使用
    1 个块来累积最终输出。[Listing 12.2](ch12.html#ch12lis02) 显示了调用 `Reduction1_kernel()` 的主机函数。请注意，部分和数组是单独分配并传递的。还要注意，由于内核使用的是无大小的共享内存数组，内核所需的共享内存量必须作为
    `<<< >>>` 语法中的第三个参数指定。
- en: The CUDA SDK discusses several optimizations of this kernel that focus on reducing
    the amount of conditional code in the log-step reduction. Part of the `for` loop
    that performs the log-step reduction—the later part, when the thread count is
    32 or fewer—can be implemented with *warp-synchronous* code. Since the warps in
    each thread block execute each instruction in lockstep, the `__syncthreads()`
    intrinsics are no longer needed when the number of active threads in a block drops
    below the hardware’s warp size of 32\. The resulting kernel, located in the `reduction2.cu`
    source code file, is shown in [Listing 12.2](ch12.html#ch12lis02).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA SDK 讨论了该内核的几种优化，重点减少对数步骤归约中的条件代码量。执行对数步骤归约的`for`循环的后半部分，当线程数小于等于32时，可以通过*warp同步*代码来实现。由于每个线程块中的warp以锁步方式执行每条指令，当一个块中的活跃线程数小于硬件的warp大小32时，就不再需要`__syncthreads()`内建函数。最终的内核代码位于`reduction2.cu`源文件中，见[第12.2列表](ch12.html#ch12lis02)。
- en: '* * *'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Important Note
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: When writing warp synchronous code, the `volatile` keyword must be used for
    the pointers into shared memory. Otherwise, the compiler may introduce optimizations
    that change the order of memory operations and the code will not work correctly.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 编写warp同步代码时，必须使用`volatile`关键字来声明指向共享内存的指针。否则，编译器可能会引入优化，改变内存操作的顺序，从而导致代码无法正确工作。
- en: '* * *'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 12.2.* Reduction with unrolled, warp-synchronous finish.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*第12.2列表* 使用展开的、warp同步的结束进行归约。'
- en: '[Click here to view code image](ch12_images.html#p12lis02a)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch12_images.html#p12lis02a)'
- en: '* * *'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: Reduction2_kernel( int *out, const int *in, size_t N )
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction2_kernel( int *out, const int *in, size_t N )
- en: '{'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ int sPartials[];
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ int sPartials[];
- en: int sum = 0;
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: int sum = 0;
- en: const int tid = threadIdx.x;
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: for ( size_t i = blockIdx.x*blockDim.x + tid;
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + tid;
- en: i < N;
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: sum += in[i];
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: sum += in[i];
- en: '}'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sPartials[tid] = sum;
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] = sum;
- en: __syncthreads();
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: for ( int activeThreads = blockDim.x>>1;
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int activeThreads = blockDim.x>>1;
- en: activeThreads > 32;
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: activeThreads > 32;
- en: activeThreads >>= 1 ) {
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: activeThreads >>= 1 ) {
- en: if ( tid < activeThreads ) {
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid < activeThreads ) {
- en: sPartials[tid] += sPartials[tid+activeThreads];
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] += sPartials[tid+activeThreads];
- en: '}'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if ( threadIdx.x < 32 ) {
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: if ( threadIdx.x < 32 ) {
- en: volatile int *wsSum = sPartials;
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: volatile int *wsSum = sPartials;
- en: if ( blockDim.x > 32 ) wsSum[tid] += wsSum[tid + 32];
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: if ( blockDim.x > 32 ) wsSum[tid] += wsSum[tid + 32];
- en: wsSum[tid] += wsSum[tid + 16];
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: wsSum[tid] += wsSum[tid + 16];
- en: wsSum[tid] += wsSum[tid + 8];
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: wsSum[tid] += wsSum[tid + 8];
- en: wsSum[tid] += wsSum[tid + 4];
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: wsSum[tid] += wsSum[tid + 4];
- en: wsSum[tid] += wsSum[tid + 2];
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: wsSum[tid] += wsSum[tid + 2];
- en: wsSum[tid] += wsSum[tid + 1];
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: wsSum[tid] += wsSum[tid + 1];
- en: if ( tid == 0 ) {
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid == 0 ) {
- en: volatile int *wsSum = sPartials;
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: volatile int *wsSum = sPartials;
- en: out[blockIdx.x] = wsSum[0];
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: out[blockIdx.x] = wsSum[0];
- en: '}'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The warp synchronous optimization can be taken a step further by lofting the
    thread count into a template parameter, enabling the log-step reduction to be
    unrolled completely. [Listing 12.3](ch12.html#ch12lis03) gives the complete optimized
    kernel. Following Mark Harris’s reduction presentation,^([1](ch12.html#ch12fn1))
    the code evaluated at compile time is italicized.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: warp 同步优化可以通过将线程数提升为模板参数进一步推进，从而实现对数步长归约的完全展开。[列表 12.3](ch12.html#ch12lis03)
    给出了完整的优化内核。根据 Mark Harris 的归约演讲，^([1](ch12.html#ch12fn1)) 在编译时评估的代码用斜体表示。
- en: '[1](ch12.html#ch12fn1a). [http://bit.ly/WNmH9Z](http://bit.ly/WNmH9Z)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch12.html#ch12fn1a). [http://bit.ly/WNmH9Z](http://bit.ly/WNmH9Z)'
- en: '*Listing 12.3.* Templatized, fully unrolled log-step reduction.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 12.3.* 模板化的、完全展开的对数步长归约。'
- en: '[Click here to view code image](ch12_images.html#p12lis03a)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch12_images.html#p12lis03a)'
- en: '* * *'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<unsigned int numThreads>
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: template<unsigned int numThreads>
- en: __global__ void
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: Reduction3_kernel( int *out, const int *in, size_t N )
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction3_kernel( int *out, const int *in, size_t N )
- en: '{'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ int sPartials[];
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ int sPartials[];
- en: const unsigned int tid = threadIdx.x;
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: const unsigned int tid = threadIdx.x;
- en: int sum = 0;
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: int sum = 0;
- en: for ( size_t i = blockIdx.x*numThreads + tid;
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*numThreads + tid;
- en: i < N;
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += numThreads*gridDim.x )
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: i += numThreads*gridDim.x )
- en: '{'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: sum += in[i];
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: sum += in[i];
- en: '}'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sPartials[tid] = sum;
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] = sum;
- en: __syncthreads();
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: if (numThreads >= 1024) {
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: if (numThreads >= 1024) {
- en: if (tid < 512) {
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: if (tid < 512) {
- en: sPartials[tid] += sPartials[tid + 512];
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] += sPartials[tid + 512];
- en: '}'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if (numThreads >= 512) {
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: if (numThreads >= 512) {
- en: if (tid < 256) {
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: if (tid < 256) {
- en: sPartials[tid] += sPartials[tid + 256];
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] += sPartials[tid + 256];
- en: '}'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if (numThreads >= 256) {
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: if (numThreads >= 256) {
- en: if (tid < 128) {
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: if (tid < 128) {
- en: sPartials[tid] += sPartials[tid + 128];
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] += sPartials[tid + 128];
- en: '}'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if (numThreads >= 128) {
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: if (numThreads >= 128) {
- en: if (tid <  64) {
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: if (tid <  64) {
- en: sPartials[tid] += sPartials[tid +  64];
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] += sPartials[tid +  64];
- en: '}'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // warp synchronous at the end
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: // 在最后进行 warp 同步
- en: if ( tid < 32 ) {
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid < 32 ) {
- en: volatile int *wsSum = sPartials;
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: volatile int *wsSum = sPartials;
- en: if (numThreads >=  64) { wsSum[tid] += wsSum[tid + 32]; }
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: if (numThreads >=  64) { wsSum[tid] += wsSum[tid + 32]; }
- en: if (numThreads >=  32) { wsSum[tid] += wsSum[tid + 16]; }
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: if (numThreads >=   32) { wsSum[tid] += wsSum[tid + 16]; }
- en: if (numThreads >=  16) { wsSum[tid] += wsSum[tid +  8]; }
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: if (numThreads >=  16) { wsSum[tid] += wsSum[tid +  8]; }
- en: if (numThreads >=   8) { wsSum[tid] += wsSum[tid +  4]; }
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: if (numThreads >=   8) { wsSum[tid] += wsSum[tid +  4]; }
- en: if (numThreads >=   4) { wsSum[tid] += wsSum[tid +  2]; }
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: if (numThreads >=   4) { wsSum[tid] += wsSum[tid +  2]; }
- en: if (numThreads >=   2) { wsSum[tid] += wsSum[tid +  1]; }
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: if (numThreads >=   2) { wsSum[tid] += wsSum[tid +  1]; }
- en: if ( tid == 0 ) {
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid == 0 ) {
- en: out[blockIdx.x] = wsSum[0];
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: out[blockIdx.x] = wsSum[0];
- en: '}'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: To instantiate the function template in [Listing 12.3](ch12.html#ch12lis03),
    it must be invoked explicitly in a separate host function. [Listing 12.4](ch12.html#ch12lis04)
    shows how `Reduction3_kernel` is invoked by another function template, and the
    host function uses a `switch` statement to invoke that template for each possible
    block size.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 要在[Listing 12.3](ch12.html#ch12lis03)中实例化函数模板，必须在单独的主机函数中显式调用它。[Listing 12.4](ch12.html#ch12lis04)展示了如何通过另一个函数模板调用`Reduction3_kernel`，而主机函数使用`switch`语句来为每个可能的块大小调用该模板。
- en: '*Listing 12.4.* Template instantiations for unrolled reduction.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 12.4.* 用于展开归约的模板实例化。'
- en: '[Click here to view code image](ch12_images.html#p12lis04a)'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '[Click here to view code image](ch12_images.html#p12lis04a)'
- en: '* * *'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<unsigned int numThreads>
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: template<unsigned int numThreads>
- en: void
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: Reduction3_template( int *answer, int *partial,
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction3_template( int *answer, int *partial,
- en: const int *in, size_t N,
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: const int *in, size_t N,
- en: int numBlocks )
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: int numBlocks )
- en: '{'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: Reduction3_kernel<numThreads><<<
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction3_kernel<numThreads><<<
- en: numBlocks, numThreads, numThreads*sizeof(int)>>>(
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: numBlocks, numThreads, numThreads*sizeof(int)>>>(
- en: partial, in, N );
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: partial, in, N );
- en: Reduction3_kernel<numThreads><<<
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction3_kernel<numThreads><<<
- en: 1, numThreads, numThreads*sizeof(int)>>>(
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 1, numThreads, numThreads*sizeof(int)>>>(
- en: answer, partial, numBlocks );
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: answer, partial, numBlocks );
- en: '}'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: Reduction3( int *out, int *partial,
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction3( int *out, int *partial,
- en: const int *in, size_t N,
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: const int *in, size_t N,
- en: int numBlocks, int numThreads )
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: int numBlocks, int numThreads )
- en: '{'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: switch ( numThreads ) {
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: switch ( numThreads ) {
- en: 'case    1: return Reduction3_template<   1>( ... );'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 1: return Reduction3_template< 1>( ... );'
- en: 'case    2: return Reduction3_template<   2>( ... );'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 2: return Reduction3_template< 2>( ... );'
- en: 'case    4: return Reduction3_template<   4>( ... );'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 4: return Reduction3_template< 4>( ... );'
- en: 'case    8: return Reduction3_template<   8>( ... );'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 8: return Reduction3_template< 8>( ... );'
- en: 'case   16: return Reduction3_template<  16>( ... );'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 16: return Reduction3_template< 16>( ... );'
- en: 'case   32: return Reduction3_template<  32>( ... );'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 32: return Reduction3_template< 32>( ... );'
- en: 'case   64: return Reduction3_template<  64>( ... );'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 64: return Reduction3_template< 64>( ... );'
- en: 'case  128: return Reduction3_template< 128>( ... );'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 128: return Reduction3_template< 128>( ... );'
- en: 'case  256: return Reduction3_template< 256>( ... );'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 256: return Reduction3_template< 256>( ... );'
- en: 'case  512: return Reduction3_template< 512>( ... );'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 512: return Reduction3_template< 512>( ... );'
- en: 'case 1024: return Reduction3_template<1024>( ... );'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 1024: return Reduction3_template<1024>( ... );'
- en: '}'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 12.3\. Single-Pass Reduction
  id: totrans-468
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.3\. 单次归约
- en: The two-pass reduction approach is in part a workaround for the inability of
    CUDA blocks to synchronize with one another. In the absence of interblock synchronization
    to determine when processing of the final output can begin, a second kernel invocation
    is needed.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 双次归约方法在一定程度上是对CUDA块无法相互同步的一个变通方法。由于缺乏跨块同步来确定何时可以开始处理最终输出，因此需要第二次内核调用。
- en: The second kernel invocation can be avoided by using a combination of atomic
    operations and shared memory, as described in the `threadfenceReduction` sample
    in the CUDA SDK. A single device memory location tracks which thread blocks have
    finished writing their partial sums. Once all blocks have finished, one block
    performs the final log-step reduction to write the output.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次内核调用可以通过使用原子操作和共享内存的组合来避免，具体方法可参考CUDA SDK中的`threadfenceReduction`示例。一个设备内存位置跟踪哪些线程块已经完成了部分和的写入。所有线程块完成后，一个线程块执行最终的对数步骤归约，写入输出。
- en: Since this kernel performs several log-step reductions from shared memory, the
    code in [Listing 12.3](ch12.html#ch12lis03) that conditionally adds based on the
    templated thread count is pulled into a separate device function for reuse.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 由于此内核执行多个来自共享内存的对数步骤归约，代码在[列表 12.3](ch12.html#ch12lis03)中根据模板化的线程数条件性地添加，被提取到一个单独的设备函数中以供重用。
- en: '*Listing 12.5.* `Reduction4_LogStepShared.`'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 12.5.* `Reduction4_LogStepShared.`'
- en: '[Click here to view code image](ch12_images.html#p12lis05a)'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch12_images.html#p12lis05a)'
- en: '* * *'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<unsigned int numThreads>
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: template<unsigned int numThreads>
- en: __device__ void
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ void
- en: Reduction4_LogStepShared( int *out, volatile int *partials )
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction4_LogStepShared( int *out, volatile int *partials )
- en: '{'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const int tid = threadIdx.x;
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: if (numThreads >= 1024) {
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(numThreads >= 1024) {
- en: if (tid < 512) {
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(tid < 512) {
- en: partials[tid] += partials[tid + 512];
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: partials[tid] += partials[tid + 512];
- en: '}'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if (numThreads >= 512) {
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(numThreads >= 512) {
- en: if (tid < 256) {
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(tid < 256) {
- en: partials[tid] += partials[tid + 256];
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: partials[tid] += partials[tid + 256];
- en: '}'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if (numThreads >= 256) {
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(numThreads >= 256) {
- en: if (tid < 128) {
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(tid < 128) {
- en: partials[tid] += partials[tid + 128];
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: partials[tid] += partials[tid + 128];
- en: '}'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if (numThreads >= 128) {
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(numThreads >= 128) {
- en: if (tid <  64) {
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(tid < 64) {
- en: partials[tid] += partials[tid +  64];
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: partials[tid] += partials[tid + 64];
- en: '}'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // warp synchronous at the end
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: // warp同步在末尾
- en: if ( tid < 32 ) {
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(tid < 32) {
- en: if (numThreads >= 64) { partials[tid] += partials[tid + 32]; }
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(numThreads >= 64) { partials[tid] += partials[tid + 32]; }
- en: if (numThreads >= 32) { partials[tid] += partials[tid + 16]; }
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(numThreads >= 32) { partials[tid] += partials[tid + 16]; }
- en: if (numThreads >= 16) { partials[tid] += partials[tid +  8]; }
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(numThreads >= 16) { partials[tid] += partials[tid + 8]; }
- en: if (numThreads >=  8) { partials[tid] += partials[tid +  4]; }
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(numThreads >= 8) { partials[tid] += partials[tid + 4]; }
- en: if (numThreads >=  4) { partials[tid] += partials[tid +  2]; }
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(numThreads >= 4) { partials[tid] += partials[tid + 2]; }
- en: if (numThreads >=  2) { partials[tid] += partials[tid +  1]; }
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(numThreads >= 2) { partials[tid] += partials[tid + 1]; }
- en: if ( tid == 0 ) {
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(tid == 0) {
- en: '*out = partials[0];'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '*out = partials[0];'
- en: '}'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The `Reduction4_LogStepShared()` function, shown in [Listing 12.5](ch12.html#ch12lis05),
    writes the reduction for the thread block, whose partial sums are given by `partials`
    to the pointer to the memory location specified by `out`. [Listing 12.6](ch12.html#ch12lis06)
    gives the single-pass reduction using `Reduction4_LogStepShared()` as a subroutine.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '`Reduction4_LogStepShared()` 函数，如[清单 12.5](ch12.html#ch12lis05)所示，将线程块的部分和
    `partials` 写入由 `out` 指定的内存位置。[清单 12.6](ch12.html#ch12lis06)提供了使用 `Reduction4_LogStepShared()`
    作为子例程的单次减法。'
- en: '*Listing 12.6.* Single-pass reduction kernel (`reduction4SinglePass.cuh`).'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 12.6.* 单次减法内核 (`reduction4SinglePass.cuh`)。'
- en: '[Click here to view code image](ch12_images.html#p12lis06a)'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch12_images.html#p12lis06a)'
- en: '* * *'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: // Global variable used by reduceSinglePass to count blocks
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: // reduceSinglePass 使用的全局变量，用于计数块
- en: __device__ unsigned int retirementCount = 0;
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ unsigned int retirementCount = 0;
- en: template <unsigned int numThreads>
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: template <unsigned int numThreads>
- en: __global__ void
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: reduceSinglePass( int *out, int *partial,
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: reduceSinglePass( int *out, int *partial,
- en: const int *in, unsigned int N )
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: const int *in, unsigned int N )
- en: '{'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ int sPartials[];
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ int sPartials[];
- en: unsigned int tid = threadIdx.x;
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int tid = threadIdx.x;
- en: int sum = 0;
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: int sum = 0;
- en: for ( size_t i = blockIdx.x*numThreads + tid;
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*numThreads + tid;
- en: i < N;
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += numThreads*gridDim.x ) {
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: i += numThreads*gridDim.x ) {
- en: sum += in[i];
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: sum += in[i];
- en: '}'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sPartials[tid] = sum;
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] = sum;
- en: __syncthreads();
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: if (gridDim.x == 1) {
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: if (gridDim.x == 1) {
- en: Reduction4_LogStepShared<numThreads>( &out[blockIdx.x],
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction4_LogStepShared<numThreads>( &out[blockIdx.x],
- en: sPartials );
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials );
- en: return;
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: return;
- en: '}'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Reduction4_LogStepShared<numThreads>( &partial[blockIdx.x],
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction4_LogStepShared<numThreads>( &partial[blockIdx.x],
- en: sPartials );
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials );
- en: __shared__ bool lastBlock;
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: __shared__ bool lastBlock;
- en: // wait for outstanding memory instructions in this thread
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: // 等待此线程中的未完成内存指令
- en: __threadfence();
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: __threadfence();
- en: // Thread 0 takes a ticket
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: // 线程 0 获取一张票
- en: if( tid==0 ) {
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: if( tid==0 ) {
- en: unsigned int ticket = atomicAdd(&retirementCount, 1);
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int ticket = atomicAdd(&retirementCount, 1);
- en: //
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // If the ticket ID is equal to the number of blocks,
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: // 如果票据 ID 等于块的数量，
- en: // we are the last block!
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: // 我们是最后一个块！
- en: //
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: lastBlock = (ticket == gridDim.x-1);
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: lastBlock = (ticket == gridDim.x-1);
- en: '}'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: // One block performs the final log-step reduction
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: // 一个块执行最终的日志步减法
- en: if( lastBlock ) {
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: if( lastBlock ) {
- en: int sum = 0;
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: int sum = 0;
- en: for ( size_t i = tid;
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = tid;
- en: i < gridDim.x;
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: i < gridDim.x;
- en: i += numThreads ) {
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: i += numThreads ) {
- en: sum += partial[i];
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: sum += partial[i];
- en: '}'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sPartials[threadIdx.x] = sum;
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[threadIdx.x] = sum;
- en: __syncthreads();
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: Reduction4_LogStepShared<numThreads>( out, sPartials );
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction4_LogStepShared<numThreads>( out, sPartials );
- en: retirementCount = 0;
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: retirementCount = 0;
- en: '}'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The kernel starts out with familiar code that has each thread compute a partial
    reduction across the input array and write the results to shared memory. Once
    this is done, the single-block case is treated specially, since the output of
    the log-step reduction from shared memory can be written directly and not to the
    array of partial sums. The remainder of the kernel is executed only on kernels
    with multiple thread blocks.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 内核一开始使用的是常见的代码，每个线程计算输入数组的部分归约并将结果写入共享内存。完成后，单块情况会被特别处理，因为来自共享内存的日志步长归约的输出可以直接写入，而不是写入部分和数组。其余的内核只会在有多个线程块的情况下执行。
- en: The shared Boolean `lastBlock` is used to evaluate a predicate that must be
    communicated to all threads in the final block. The `__threadfence()` causes all
    threads in the block to wait until any pending memory transactions have been posted
    to device memory. When `__threadfence()` is executed, writes to global memory
    are visible to all threads, not just the calling thread or threads in the block.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 共享布尔值`lastBlock`用于评估必须传递给最终块中所有线程的谓词。`__threadfence()`使块中的所有线程等待，直到所有待处理的内存事务已经提交到设备内存。当执行`__threadfence()`时，写入全局内存的内容对所有线程可见，而不仅仅是调用线程或块中的线程。
- en: As each block exits, it performs an `atomicAdd()` to check whether it is the
    one block that needs to perform the final log-step reduction. Since `atomicAdd()`
    returns the *previous* value of the memory location, the block that increments
    `retirementCount` and gets a value equal to `gridDim.x-1` can be deemed the “last
    thread” and can perform the final reduction. The `lastBlock` shared memory location
    communicates that result to all threads in the block, and `__syncthreads()` then
    must be called so the write to `lastBlock` will be visible to all threads in the
    block. The final block performs the final log-step reduction of the partial sums
    and writes the result. Finally, `retirementCount` is set back to 0 for subsequent
    invocations of `reduceSinglePass()`.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 每个块退出时，都会执行`atomicAdd()`来检查它是否是需要执行最终日志步长归约的那个块。由于`atomicAdd()`返回的是内存位置的*先前*值，递增`retirementCount`并且得到值为`gridDim.x-1`的块可以被认为是“最后一个线程”，并可以执行最终的归约。`lastBlock`共享内存位置将该结果传递给块中的所有线程，然后必须调用`__syncthreads()`，这样写入`lastBlock`的结果才能对块中的所有线程可见。最后一个块执行部分和的最终日志步长归约并写入结果。最后，`retirementCount`被重置为0，以便后续调用`reduceSinglePass()`。
- en: 12.4\. Reduction with Atomics
  id: totrans-577
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.4\. 使用原子操作的归约
- en: 'For reductions whose ![Image](graphics/plus.jpg) operator is supported natively
    by an atomic operator implemented in hardware, a simpler approach to reduction
    is possible: Just loop over the input data and “fire and forget” the inputs into
    the output memory location to receive the output value. The `Reduction5` kernel
    given in [Listing 12.7](ch12.html#ch12lis07) is much simpler than previous formulations.
    Each thread computes a partial sum over the inputs and performs an `atomicAdd`
    on the output at the end.'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些由硬件中实现的原子操作原生支持的![Image](graphics/plus.jpg)操作符的归约操作，可以采用更简单的归约方法：只需遍历输入数据，并将输入数据“发射并忘记”到输出内存位置，以接收输出值。给定的`Reduction5`内核比之前的实现要简单得多。每个线程计算输入的部分和，并在最后对输出进行`atomicAdd`操作。
- en: Note that `Reduction5_kernel` does not work properly unless the memory location
    pointed to by `out` is initialized to 0.^([2](ch12.html#ch12fn2)) Like the `threadFenceReduction`
    sample, this kernel has the advantage that only one kernel invocation is needed
    to perform the operation.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除非`out`指向的内存位置被初始化为0，`Reduction5_kernel`不会正常工作。^([2](ch12.html#ch12fn2))
    像`threadFenceReduction`示例一样，这个内核的优点是只需要一次内核调用就能完成操作。
- en: '[2](ch12.html#ch12fn2a). The kernel itself cannot perform this initialization
    because CUDA’s execution model does not enable the race condition to be resolved
    between thread blocks. See [Section 7.3.1](ch07.html#ch07lev2sec7).'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch12.html#ch12fn2a)。内核本身无法执行此初始化，因为CUDA的执行模型不支持在线程块之间解决竞态条件。参见[第7.3.1节](ch07.html#ch07lev2sec7)。'
- en: '*Listing 12.7.* Reduction with global atomics (`reduction5Atomics.cuh`).'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '*示例12.7.* 使用全局原子操作的归约（`reduction5Atomics.cuh`）。'
- en: '[Click here to view code image](ch12_images.html#p12lis07a)'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch12_images.html#p12lis07a)'
- en: '* * *'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: Reduction5_kernel( int *out, const int *in, size_t N )
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction5_kernel( int *out, const int *in, size_t N )
- en: '{'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const int tid = threadIdx.x;
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: int partialSum = 0;
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: int partialSum = 0;
- en: for ( size_t i = blockIdx.x*blockDim.x + tid;
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + tid;
- en: i < N;
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: partialSum += in[i];
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: partialSum += in[i];
- en: '}'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: atomicAdd( out, partialSum );
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: atomicAdd( out, partialSum );
- en: '}'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: void
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: Reduction5( int *answer, int *partial,
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction5( int *answer, int *partial,
- en: const int *in, size_t N,
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: const int *in, size_t N,
- en: int numBlocks, int numThreads )
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: int numBlocks, int numThreads )
- en: '{'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaMemset( answer, 0, sizeof(int) );
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemset( answer, 0, sizeof(int) );
- en: Reduction5_kernel<<< numBlocks, numThreads>>>( answer, in, N );
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction5_kernel<<< numBlocks, numThreads>>>( answer, in, N );
- en: '}'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 12.5\. Arbitrary Block Sizes
  id: totrans-605
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.5\. 任意块大小
- en: So far, all of the reduction implementations that use shared memory require
    the block size to be a power of 2\. With a small amount of additional code, the
    reduction can be made to work on arbitrary block sizes. [Listing 12.8](ch12.html#ch12lis08)
    gives a kernel derived from the very first two-pass kernel given in [Listing 12.1](ch12.html#ch12lis01),
    modified to operate on any block size. The `floorPow2` variable computes the power
    of 2 that is less than or equal to the block size, and the contribution from any
    threads above that power of 2 is added before continuing on to the loop that implements
    the log-step reduction.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，所有使用共享内存的归约实现都要求块大小是 2 的幂。通过少量的额外代码，可以使归约在任意块大小上工作。[清单 12.8](ch12.html#ch12lis08)
    给出了一个内核，该内核源自[清单 12.1](ch12.html#ch12lis01)中给出的第一个两遍内核，经过修改以在任何块大小上运行。`floorPow2`
    变量计算小于或等于块大小的最大 2 的幂，并且大于该 2 的幂的线程贡献会在继续进行实现对数步归约的循环之前被加上。
- en: '*Listing 12.8.* Reduction (arbitrary block size) (`reduction6AnyBlockSize.cuh`).'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 12.8.* 归约（任意块大小） (`reduction6AnyBlockSize.cuh`)。'
- en: '[Click here to view code image](ch12_images.html#p12lis08a)'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch12_images.html#p12lis08a)'
- en: '* * *'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: Reduction6_kernel( int *out, const int *in, size_t N )
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction6_kernel( int *out, const int *in, size_t N )
- en: '{'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ int sPartials[];
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ int sPartials[];
- en: int sum = 0;
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: int sum = 0;
- en: const int tid = threadIdx.x;
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: for ( size_t i = blockIdx.x*blockDim.x + tid;
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + tid;
- en: i < N;
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: sum += in[i];
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: sum += in[i];
- en: '}'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sPartials[tid] = sum;
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] = sum;
- en: __syncthreads();
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: // start the shared memory loop on the next power of 2 less
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: // 在下一个小于的 2 的幂上启动共享内存循环
- en: // than the block size.  If block size is not a power of 2,
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: // 比块大小更大。如果块大小不是 2 的幂，
- en: // accumulate the intermediate sums in the remainder range.
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: // 在剩余范围内累加中间结果。
- en: int floorPow2 = blockDim.x;
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: int floorPow2 = blockDim.x;
- en: if ( floorPow2 & (floorPow2-1) ) {
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: if ( floorPow2 & (floorPow2-1) ) {
- en: while ( floorPow2 & (floorPow2-1) ) {
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: while ( floorPow2 & (floorPow2-1) ) {
- en: floorPow2 &= floorPow2-1;
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: floorPow2 &= floorPow2-1;
- en: '}'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if ( tid >= floorPow2 ) {
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid >= floorPow2 ) {
- en: sPartials[tid - floorPow2] += sPartials[tid];
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid - floorPow2] += sPartials[tid];
- en: '}'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( int activeThreads = floorPow2>>1;
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int activeThreads = floorPow2>>1;
- en: activeThreads;
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: activeThreads;
- en: activeThreads >>= 1 ) {
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: activeThreads >>= 1 ) {
- en: if ( tid < activeThreads ) {
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid < activeThreads ) {
- en: sPartials[tid] += sPartials[tid+activeThreads];
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] += sPartials[tid+activeThreads];
- en: '}'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if ( tid == 0 ) {
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid == 0 ) {
- en: out[blockIdx.x] = sPartials[0];
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: out[blockIdx.x] = sPartials[0];
- en: '}'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 12.6\. Reduction Using Arbitrary Data Types
  id: totrans-649
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.6\. 使用任意数据类型的归约
- en: So far, we have only developed reduction kernels that can compute the sum of
    an array of integers. To generalize these kernels to perform a broader set of
    operations, we turn to C++ templates. With the exception of the algorithms that
    use atomics, all of the kernels that have appeared so far can be adapted to use
    templates. In the source code accompanying the book, they are in the CUDA headers
    `reduction1Templated.cuh`, `reduction2Templated.cuh`, and so on. [Listing 12.9](ch12.html#ch12lis09)
    gives the templated version of the reduction kernel from [Listing 12.1](ch12.html#ch12lis01).
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只开发了能够计算整数数组总和的归约内核。为了将这些内核推广到执行更广泛操作的能力，我们转向 C++ 模板。除了使用原子操作的算法外，到目前为止所有出现过的内核都可以适配为使用模板。在书籍附带的源代码中，它们位于
    CUDA 头文件 `reduction1Templated.cuh`、`reduction2Templated.cuh` 等中。[列表 12.9](ch12.html#ch12lis09)
    给出了来自 [列表 12.1](ch12.html#ch12lis01) 的归约内核的模板版本。
- en: '*Listing 12.9.* Templated reduction kernel.'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 12.9.* 模板化归约内核。'
- en: '[Click here to view code image](ch12_images.html#p12lis09a)'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch12_images.html#p12lis09a)'
- en: '* * *'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<typename ReductionType, typename T>
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: template<typename ReductionType, typename T>
- en: __global__ void
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: Reduction_templated( ReductionType *out, const T *in, size_t N )
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction_templated( ReductionType *out, const T *in, size_t N )
- en: '{'
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: SharedMemory<ReductionType> sPartials;
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: SharedMemory<ReductionType> sPartials;
- en: ReductionType sum;
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: ReductionType sum;
- en: const int tid = threadIdx.x;
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: for ( size_t i = blockIdx.x*blockDim.x + tid;
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x + tid;
- en: i < N;
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: sum += in[i];
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: sum += in[i];
- en: '}'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sPartials[tid] = sum;
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] = sum;
- en: __syncthreads();
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: for ( int activeThreads = blockDim.x>>1;
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int activeThreads = blockDim.x>>1;
- en: activeThreads;
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: activeThreads;
- en: activeThreads >>= 1 ) {
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: activeThreads >>= 1 ) {
- en: if ( tid < activeThreads ) {
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid < activeThreads ) {
- en: sPartials[tid] += sPartials[tid+activeThreads];
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] += sPartials[tid+activeThreads];
- en: '}'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if ( tid == 0 ) {
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: if ( tid == 0 ) {
- en: out[blockIdx.x] = sPartials[0];
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: out[blockIdx.x] = sPartials[0];
- en: '}'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 'Note that since we want to be able to compute a variety of output types for
    a given type of input (for example, we would like to build kernels that compute
    any combination of the minimum, maximum, sum, or the sum of squares of an array
    of integers), we’ve used two different template parameters: `T` is the type being
    reduced, and `ReductionType` is the type used for partial sums and for the final
    result.'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于我们希望能够为给定类型的输入计算多种输出类型（例如，我们希望构建可以计算整数数组的最小值、最大值、总和或平方和的内核），因此我们使用了两个不同的模板参数：`T`
    是被归约的类型，而 `ReductionType` 是用于部分和以及最终结果的类型。
- en: The first few lines of code use the `+=` operator to “rake” through the input,
    accumulating a partial sum for each thread in the block.^([3](ch12.html#ch12fn3))
    Execution then proceeds exactly as in [Listing 12.1](ch12.html#ch12lis01), except
    that the code is operating on `ReductionType` instead of `int`. To avoid alignment-related
    compilation errors, this kernel uses an idiom from the CUDA SDK to declare the
    variable-sized shared memory.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的前几行使用`+=`运算符“扫过”输入数据，为每个线程在块中累加部分和。^([3](ch12.html#ch12fn3)) 执行接着与[清单 12.1](ch12.html#ch12lis01)完全相同，不同之处在于代码操作的是`ReductionType`而不是`int`。为了避免与对齐相关的编译错误，该内核使用CUDA
    SDK中的一种惯用法来声明可变大小的共享内存。
- en: '[3](ch12.html#ch12fn3a). We just as easily could have defined a function to
    wrap the binary operator being evaluated by the reduction. The Thrust library
    defines a functor `plus`.'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch12.html#ch12fn3a)。我们同样可以定义一个函数来包装由reduction评估的二元运算符。Thrust库定义了一个函数对象`plus`。'
- en: '[Click here to view code image](ch12_images.html#p379pro01a)'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch12_images.html#p379pro01a)'
- en: template<class T>
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: struct SharedMemory
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: struct SharedMemory
- en: '{'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: __device__ inline operator       T*()
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ inline operator       T*()
- en: '{'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ int __smem[];
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ int __smem[];
- en: return (T*) (void *) __smem;
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: return (T*) (void *) __smem;
- en: '}'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __device__ inline operator const T*() const
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ inline operator const T*() const
- en: '{'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ int __smem[];
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ int __smem[];
- en: return (T*) (void *) __smem;
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: return (T*) (void *) __smem;
- en: '}'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '};'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '[Listing 12.10](ch12.html#ch12lis10) shows an example of a class intended to
    be used with templated reduction functions such as `Reduction_templated`. This
    class computes both the sum and the sum of squares of an array of integers.^([4](ch12.html#ch12fn4))
    Besides defining `operator+=`, a specialization of the `SharedMemory` template
    must be declared; otherwise, the compiler will generate the following error.'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 12.10](ch12.html#ch12lis10)展示了一个旨在与模板化的reduction函数（如`Reduction_templated`）一起使用的类的示例。该类计算整数数组的和及其平方和。^([4](ch12.html#ch12fn4))
    除了定义`operator+=`外，还必须声明`SharedMemory`模板的特化；否则，编译器会生成以下错误。'
- en: '[4](ch12.html#ch12fn4a). You could compute a whole suite of statistics on the
    input array in a single pass, but we are keeping things simple here for illustrative
    purposes.'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch12.html#ch12fn4a)。你可以在一次遍历中计算输入数组的所有统计数据，但为了说明问题，我们在此保持简单。'
- en: 'Error: Unaligned memory accesses not supported'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：不支持未对齐的内存访问
- en: The `reductionTemplated.cu` program in the accompanying source code shows how
    the function templates from the CUDA headers can be invoked.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 附带源代码中的`reductionTemplated.cu`程序展示了如何调用CUDA头文件中的函数模板。
- en: Reduction1<CReduction_Sumi_isq, int>( ... );
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: Reduction1<CReduction_Sumi_isq, int>( ... );
- en: '*Listing 12.10.* `CReduction_Sumi_isq` class.'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 12.10.* `CReduction_Sumi_isq` 类。'
- en: '[Click here to view code image](ch12_images.html#p12lis10a)'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch12_images.html#p12lis10a)'
- en: '* * *'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: struct CReduction_Sumi_isq {
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: struct CReduction_Sumi_isq {
- en: 'public:'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 'public:'
- en: CReduction_Sumi_isq();
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: CReduction_Sumi_isq();
- en: int sum;
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: int sum;
- en: long long sumsq;
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: long long sumsq;
- en: CReduction_Sumi_isq& operator +=( int a );
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: CReduction_Sumi_isq& operator +=( int a );
- en: volatile CReduction_Sumi_isq& operator +=( int a ) volatile;
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: volatile CReduction_Sumi_isq& operator +=( int a ) volatile;
- en: CReduction_Sumi_isq& operator +=( const CReduction_Sumi_isq& a );
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: CReduction_Sumi_isq& operator +=( const CReduction_Sumi_isq& a );
- en: volatile CReduction_Sumi_isq& operator +=(
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: volatile CReduction_Sumi_isq& operator +=(
- en: volatile CReduction_Sumi_isq& a ) volatile;
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: volatile CReduction_Sumi_isq& a ) volatile;
- en: '};'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: inline __device__ __host__
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ __host__
- en: CReduction_Sumi_isq::CReduction_Sumi_isq()
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: CReduction_Sumi_isq::CReduction_Sumi_isq()
- en: '{'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: sum = 0;
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: sum = 0;
- en: sumsq = 0;
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: sumsq = 0;
- en: '}'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: inline __device__ __host__
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ __host__
- en: CReduction_Sumi_isq&
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: CReduction_Sumi_isq&
- en: CReduction_Sumi_isq::operator +=( int a )
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: CReduction_Sumi_isq::operator +=( int a )
- en: '{'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: sum += a;
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: sum += a;
- en: sumsq += (long long) a*a;
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: sumsq += (long long) a*a;
- en: return *this;
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: return *this;
- en: '}'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: inline __device__ __host__
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ __host__
- en: volatile CReduction_Sumi_isq&
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: volatile CReduction_Sumi_isq&
- en: CReduction_Sumi_isq::operator +=( int a ) volatile
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: CReduction_Sumi_isq::operator +=( int a ) volatile
- en: '{'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: sum += a;
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: sum += a;
- en: sumsq += (long long) a*a;
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: sumsq += (long long) a*a;
- en: return *this;
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: return *this;
- en: '}'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: inline __device__ __host__
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ __host__
- en: CReduction_Sumi_isq&
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: CReduction_Sumi_isq&
- en: CReduction_Sumi_isq::operator +=( const CReduction_Sumi_isq& a )
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: CReduction_Sumi_isq::operator +=( const CReduction_Sumi_isq& a )
- en: '{'
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: sum += a.sum;
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: sum += a.sum;
- en: sumsq += a.sumsq;
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: sumsq += a.sumsq;
- en: return *this;
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: return *this;
- en: '}'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: inline __device__ __host__
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ __host__
- en: volatile CReduction_Sumi_isq&
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: volatile CReduction_Sumi_isq&
- en: CReduction_Sumi_isq::operator +=(
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: CReduction_Sumi_isq::operator +=(
- en: volatile CReduction_Sumi_isq& a ) volatile
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: volatile CReduction_Sumi_isq& a ) volatile
- en: '{'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: sum += a.sum;
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: sum += a.sum;
- en: sumsq += a.sumsq;
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: sumsq += a.sumsq;
- en: return *this;
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: return *this;
- en: '}'
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: inline int
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: inline int
- en: operator!=( const CReduction_Sumi_isq& a,
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: operator!=( const CReduction_Sumi_isq& a,
- en: const CReduction_Sumi_isq& b )
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: const CReduction_Sumi_isq& b )
- en: '{'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: return a.sum != b.sum && a.sumsq != b.sumsq;
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: return a.sum != b.sum && a.sumsq != b.sumsq;
- en: '}'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: //
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: '// from Reduction SDK sample:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: // 来自Reduction SDK示例：
- en: // specialize to avoid unaligned memory
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: // 为避免未对齐的内存进行专门化处理
- en: // access compile errors
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: // 访问编译错误
- en: //
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: template<>
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: template<>
- en: struct SharedMemory<CReduction_Sumi_isq>
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: struct SharedMemory<CReduction_Sumi_isq>
- en: '{'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: __device__ inline operator       CReduction_Sumi_isq*()
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ inline operator       CReduction_Sumi_isq*()
- en: '{'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ CReduction_Sumi_isq
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ CReduction_Sumi_isq
- en: __smem_CReduction_Sumi_isq[];
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: __smem_CReduction_Sumi_isq[];
- en: return (CReduction_Sumi_isq*)__smem_CReduction_Sumi_isq;
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: return (CReduction_Sumi_isq*)__smem_CReduction_Sumi_isq;
- en: '}'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __device__ inline operator const CReduction_Sumi_isq*() const
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ inline operator const CReduction_Sumi_isq*() const
- en: '{'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ CReduction_Sumi_isq
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ CReduction_Sumi_isq
- en: __smem_CReduction_Sumi_isq[];
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: __smem_CReduction_Sumi_isq[];
- en: return (CReduction_Sumi_isq*)__smem_CReduction_Sumi_isq;
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: return (CReduction_Sumi_isq*)__smem_CReduction_Sumi_isq;
- en: '}'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '};'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: '* * *'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 12.7\. Predicate Reduction
  id: totrans-785
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.7\. 谓词归约
- en: Predicates or truth values (true/false) can be represented compactly, since
    each predicate only occupies 1 bit. In SM 2.0, NVIDIA added a number of instructions
    to make predicate manipulation more efficient. The `__ballot()` and `__popc()`
    intrinsics can be used for warp-level reduction, and the `__syncthreads_count()`
    intrinsic can be used for block-level reduction.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 谓词或布尔值（真/假）可以紧凑表示，因为每个谓词只占用1个位。在SM 2.0中，NVIDIA增加了一些指令来提高谓词操作的效率。`__ballot()`和`__popc()`内建函数可以用于warp级别的归约，而`__syncthreads_count()`内建函数可以用于块级别的归约。
- en: int __ballot( int p );
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: int __ballot( int p );
- en: '`__ballot()` evaluates a condition for all threads in the warp and returns
    a 32-bit word, where each bit gives the condition for the corresponding thread
    in the warp. Since `__ballot()` broadcasts its result to every thread in the warp,
    it is effectively a reduction across the warp. Any thread that wants to count
    the number of threads in the warp for which the condition was true can call the
    `__popc()` intrinsic'
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: '`__ballot()`会对warp中所有线程评估一个条件，并返回一个32位字，其中每一位表示warp中相应线程的条件。由于`__ballot()`将结果广播到warp中的每个线程，它实际上是对warp进行归约。任何希望计算warp中满足条件的线程数量的线程都可以调用`__popc()`内建函数。'
- en: int __popc( int i );
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: int __popc( int i );
- en: which returns the number of set bits in the input word.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 它返回输入字中的置位位数。
- en: SM 2.0 also introduced `__syncthreads_count()`.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: SM 2.0 还引入了`__syncthreads_count()`。
- en: int __syncthreads_count( int p );
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: int __syncthreads_count( int p );
- en: This intrinsic waits until all warps in the threadblock have arrived, then broadcasts
    to all threads in the block the number of threads for which the input condition
    was true.
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: 这个内建函数会等待线程块中的所有warp到达后，再将输入条件为真的线程数量广播给线程块中的所有线程。
- en: Since the 1-bit predicates immediately turn into 5- and 9- or 10-bit values
    after a warp- or block-level reduction, these intrinsics only serve to reduce
    the amount of shared memory needed for the lowest-level evaluation and reduction.
    Still, they greatly amplify the number of elements that can be considered by a
    single thread block.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 由于1位的谓词在warp级或块级归约后立即变为5位、9位或10位值，这些内建函数只用于减少最低级评估和归约所需的共享内存量。不过，它们极大地放大了单个线程块可以考虑的元素数量。
- en: 12.8\. Warp Reduction with Shuffle
  id: totrans-795
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 12.8\. 使用 Shuffle 进行 Warp 归约
- en: SM 3.0 introduced the “shuffle” instruction, described in [Section 8.6.1](ch08.html#ch08lev2sec20),
    that can be used to perform a reduction across the 32 threads in a warp. By using
    the “butterfly” variant of the shuffle instruction, the final 5 steps of the log-step
    reduction
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: SM 3.0 引入了“shuffle”指令，如[第8.6.1节](ch08.html#ch08lev2sec20)所述，可以用于在一个warp的32个线程之间进行归约。通过使用“蝴蝶”变种的shuffle指令，可以执行最终5步的对数步长归约。
- en: wsSum[tid] += wsSum[tid+16];
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: wsSum[tid] += wsSum[tid+16];
- en: wsSum[tid] += wsSum[tid+8];
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: wsSum[tid] += wsSum[tid+8];
- en: wsSum[tid] += wsSum[tid+4];
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: wsSum[tid] += wsSum[tid+4];
- en: wsSum[tid] += wsSum[tid+2];
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: wsSum[tid] += wsSum[tid+2];
- en: wsSum[tid] += wsSum[tid+1];
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: wsSum[tid] += wsSum[tid+1];
- en: can be rewritten as
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 可以重写为
- en: int mySum = wsSum[tid];
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: int mySum = wsSum[tid];
- en: mySum += __shuf_xor( mySum, 16 );
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: mySum += __shuf_xor( mySum, 16 );
- en: mySum += __shuf_xor( mySum, 8 );
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: mySum += __shuf_xor( mySum, 8 );
- en: mySum += __shuf_xor( mySum, 4 );
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: mySum += __shuf_xor( mySum, 4 );
- en: mySum += __shuf_xor( mySum, 2 );
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: mySum += __shuf_xor( mySum, 2 );
- en: mySum += __shuf_xor( mySum, 1 );
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: mySum += __shuf_xor( mySum, 1 );
- en: All threads in the warp then contain the reduction in `mySum`. [Figure 12.2](ch12.html#ch12fig02)
    illustrates the operation of this warp scan primitive. Each thread’s sum is shown
    as a 4W × 8H rectangle, with a dark square showing which threads have contributed
    to each thread’s partial sum. (Besides the inset, the top row shows which squares
    correspond to each thread’s contribution.) With each step in the log-step reduction,
    the number of contributions doubles until every thread has a full reduction.^([5](ch12.html#ch12fn5))
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，warp 中的所有线程都包含 `mySum` 中的归约值。[图 12.2](ch12.html#ch12fig02)展示了这个 warp 扫描原语的操作。每个线程的和显示为一个
    4W × 8H 的矩形，深色方块显示了哪些线程对每个线程的部分和做出了贡献。（除了插图，顶部行显示了哪些方块对应于每个线程的贡献。）在每一步对数步长归约中，贡献的数量翻倍，直到每个线程都有一个完整的归约。^([5](ch12.html#ch12fn5))
- en: '[5](ch12.html#ch12fn5a). The shuffle-up or shuffle-down variants can be used
    to implement reduction, but they take just as long as the butterfly (XOR) variant
    and only make the reduction value available to a single thread.'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch12.html#ch12fn5a)。shuffle-up 或 shuffle-down 变体可以用于实现归约，但它们与蝴蝶（XOR）变体一样耗时，只能将归约值提供给单个线程。'
- en: '![Image](graphics/12fig02.jpg)'
  id: totrans-811
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/12fig02.jpg)'
- en: '*Figure 12.2* Reduction using shuffle instruction.'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 12.2* 使用 shuffle 指令的归约。'
- en: Chapter 13\. Scan
  id: totrans-813
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 13 章 扫描
- en: Scan, also known as *prefix scan*, *prefix sum*, or *parallel prefix sum*, is
    an important primitive in parallel programming and is used as a building block
    for many different algorithms, including but not limited to the following.
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描（Scan），也称为 *前缀扫描*、*前缀和* 或 *并行前缀和*，是并行编程中的一个重要原语，作为许多不同算法的构建块，以下列举了一些，包括但不限于：
- en: • Radix sort
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: • 基数排序
- en: • Quicksort
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: • 快速排序
- en: • Stream compaction and stream splitting
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: • 流压缩和流分割
- en: • Sparse matrix-vector multiplication
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: • 稀疏矩阵-向量乘法
- en: • Minimum spanning tree construction
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: • 最小生成树构建
- en: • Computation of summed area tables
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: • 求和区域表的计算
- en: This chapter starts with a description of the algorithm and a few variations,
    discusses an early implementation strategy and how Scan algorithms can be described
    in terms of circuit diagrams, and then provides detailed descriptions of Scan
    implementations for CUDA. The References section covers both the Scan algorithm
    and the parallel prefix sum circuit problem in hardware design.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先描述了算法及其几种变体，讨论了早期的实现策略以及如何通过电路图描述扫描算法，接着提供了 CUDA 中扫描实现的详细描述。参考文献部分涵盖了扫描算法以及硬件设计中的并行前缀和电路问题。
- en: 13.1\. Definition and Variations
  id: totrans-822
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1 定义和变体
- en: '*Inclusive scan* takes a binary associative operator ![Image](graphics/plus.jpg)
    and an array of length *N*'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: '*包含扫描* 使用二元结合运算符 ![Image](graphics/plus.jpg) 和一个长度为 *N* 的数组。'
- en: '[a[0], a[1], . . . a[N-1]]'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: '[a[0], a[1], . . . a[N-1]]'
- en: and returns the array
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 并返回数组
- en: '[a[0], (a[0]![Image](graphics/plus.jpg)a[1]), . . . (a[0]![Image](graphics/plus.jpg)a[1]![Image](graphics/plus.jpg)
    . . . ![Image](graphics/plus.jpg)a[N-1])].'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: '[a[0], (a[0]![Image](graphics/plus.jpg)a[1]), . . . (a[0]![Image](graphics/plus.jpg)a[1]![Image](graphics/plus.jpg)
    . . . ![Image](graphics/plus.jpg)a[N-1])].'
- en: Each element of the output depends on the preceding elements in the input.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的每个元素依赖于输入中的前一个元素。
- en: '*Exclusive scan* is defined similarly but shifts the output and uses an *identity
    element* *id*[![Image](graphics/plus.jpg)] that has no effect on a value when
    ![Image](graphics/plus.jpg) is performed with it (for example, 0 for integer addition,
    1 for multiplication, etc.).'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: '*排除扫描*的定义类似，但它移位输出并使用*恒等元素* *id*[![Image](graphics/plus.jpg)]，在与![Image](graphics/plus.jpg)进行操作时对值没有影响（例如，整数加法中的0，乘法中的1等）。'
- en: '[*id*[![Image](graphics/plus.jpg)], a[0], a[0]![Image](graphics/plus.jpg)a[1],
    . . . a[0]![Image](graphics/plus.jpg)a[1]![Image](graphics/plus.jpg) . . . ![Image](graphics/plus.jpg)a[N-2]].'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: '[*id*[![Image](graphics/plus.jpg)], a[0], a[0]![Image](graphics/plus.jpg)a[1],
    . . . a[0]![Image](graphics/plus.jpg)a[1]![Image](graphics/plus.jpg) . . . ![Image](graphics/plus.jpg)a[N-2]].'
- en: Inclusive and exclusive scans can be transformed between each other by adding
    or subtracting the input array element by element, as shown in [Figure 13.1](ch13.html#ch13fig01).
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 包括扫描和排除扫描可以通过逐元素地加或减输入数组来相互转换，如[图 13.1](ch13.html#ch13fig01)所示。
- en: '![Image](graphics/13fig01.jpg)'
  id: totrans-831
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/13fig01.jpg)'
- en: '*Figure 13.1* Inclusive and exclusive scan.'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.1* 包括扫描和排除扫描。'
- en: '*Stream compaction* is an operation that separates elements in an array according
    to a criterion. If a predicate (0 or 1) is computed for each element of the input
    array to determine whether it should be included in the output stream, then an
    exclusive scan on the predicates computes the indices of the output elements.
    A variation of stream compaction, known as *stream splitting*, writes the compact
    output separately for each value of the predicate. *Segmented scan* is a variation
    that takes a set of input flags (one per array element) in addition to the array
    and performs scans on the subarrays delineated by the flags.'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: '*流压缩*是一种根据标准将数组中的元素分开的操作。如果对输入数组中的每个元素计算一个谓词（0或1），以确定该元素是否应包含在输出流中，则对这些谓词进行排除扫描可以计算输出元素的索引。流压缩的一种变体，称为*流分割*，为谓词的每个值单独写入压缩后的输出。*分段扫描*是一种变体，它除了数组外，还需要一组输入标志（每个数组元素一个），并对由这些标志划定的子数组进行扫描。'
- en: Due to the importance of the Scan primitive, an enormous amount of effort has
    been put into developing optimized scan implementations for CUDA. A list of references
    is given at the end of this chapter. Both the CUDPP and Thrust libraries include
    families of optimized Scan primitives that use templates for the best tradeoff
    between generality and performance. All that said, however, applications that
    use Scan as a primitive usually can benefit from custom implementations that take
    advantage of specific knowledge about the problem.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 由于扫描原语的重要性，已经投入了大量精力开发针对CUDA的优化扫描实现。参考文献列表见本章末尾。CUDPP和Thrust库都包括一系列优化过的扫描原语，它们使用模板来实现最佳的通用性与性能折衷。尽管如此，使用扫描作为原语的应用通常可以通过定制实现来获得更多好处，这些实现能够利用对问题的特定了解。
- en: 13.2\. Overview
  id: totrans-835
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2\. 概述
- en: A simple implementation in C++ looks like [Listing 13.1](ch13.html#ch13lis01).
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 在C++中的一个简单实现如[列表13.1](ch13.html#ch13lis01)所示。
- en: '*Listing 13.1.* Inclusive scan (in C++).'
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表13.1.* 包含扫描（在C++中）。'
- en: '[Click here to view code image](ch13_images.html#p13lis01a)'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch13_images.html#p13lis01a)'
- en: '* * *'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: T
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: T
- en: InclusiveScan( T *out, const T *in, size_t N )
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: InclusiveScan( T *out, const T *in, size_t N )
- en: '{'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: T sum(0);
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: T sum(0);
- en: for ( size_t i = 0; i < N; i++ ) {
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = 0; i < N; i++ ) {
- en: sum += in[i];
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: sum += in[i];
- en: out[i] = sum;
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] = sum;
- en: '}'
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return sum;
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: return sum;
- en: '}'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: For these serial implementations in [Listings 13.1](ch13.html#ch13lis01) and
    [13.2](ch13.html#ch13lis02), the only difference between inclusive and exclusive
    scan is that the lines
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 对于[列表13.1](ch13.html#ch13lis01)和[13.2](ch13.html#ch13lis02)中的这些串行实现，包含扫描和独占扫描之间唯一的区别在于以下行
- en: out[i] = sum;
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] = sum;
- en: and
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: sum += in[i];
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: sum += in[i];
- en: are swapped.^([1](ch13.html#ch13fn1))
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 被交换了。^([1](ch13.html#ch13fn1))
- en: '[1](ch13.html#ch13fn1a). As written, the implementation of exclusive scan does
    not support an in-place computation. To enable the input and output arrays to
    be the same, `in[i]` must be saved in a temporary variable.'
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch13.html#ch13fn1a)。如所写，独占扫描的实现不支持原地计算。为了使输入和输出数组相同，`in[i]` 必须保存在一个临时变量中。'
- en: '*Listing 13.2.* Exclusive scan (in C++).'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表13.2.* 独占扫描（在C++中）。'
- en: '[Click here to view code image](ch13_images.html#p13lis02a)'
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch13_images.html#p13lis02a)'
- en: '* * *'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: T
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: T
- en: ExclusiveScan( T *out, const T *in, size_t N )
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: ExclusiveScan( T *out, const T *in, size_t N )
- en: '{'
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: T sum(0);
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: T sum(0);
- en: for ( size_t i = 0; i < N; i++ ) {
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = 0; i < N; i++ ) {
- en: out[i] = sum;
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: out[i] = sum;
- en: sum += in[i];
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: sum += in[i];
- en: '}'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return sum;
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: return sum;
- en: '}'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The serial implementations of Scan are so obvious and trivial that you are forgiven
    if you’re wondering what a parallel implementation would look like! The so-called
    *prefix dependency,* where each output depends on all of the preceding inputs,
    may have some wondering if it’s even possible. But, upon reflection, you can see
    that the operations for neighboring pairs (a*[i]*![Image](graphics/plus.jpg)a*[i]*[+1]
    for 0 ≤ *i* < *N –* 1) could be computed in parallel; for *i* = 0, a*[i]*![Image](graphics/plus.jpg)a*[i]*[+1]
    computes a final output of the Scan, and otherwise these pairwise operations compute
    partial sums that can be used to contribute to the final output, much as we used
    partial sums in [Chapter 12](ch12.html#ch12).
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描的串行实现非常显而易见且简单，如果你在想并行实现会是什么样子，是可以理解的！所谓的*前缀依赖*，即每个输出都依赖于所有前面的输入，可能会让人怀疑这是否可能。但经过深思熟虑，你可以看到，相邻的操作对（a*[i]*![图片](graphics/plus.jpg)a*[i]*[+1]，其中0
    ≤ *i* < *N –* 1）可以并行计算；对于*i* = 0，a*[i]*![图片](graphics/plus.jpg)a*[i]*[+1]计算扫描的最终输出，其他情况下这些配对操作计算的部分和可以用于贡献最终输出，就像我们在[第12章](ch12.html#ch12)中使用部分和一样。
- en: Blelloch^([2](ch13.html#ch13fn2)) describes a two-pass algorithm with an *upsweep*
    phase that computes the reduction of the array, storing intermediate results along
    the way, and followed by a *downsweep* that computes the final output of the scan.
    Pseudocode for the upsweep as is follows.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: Blelloch^([2](ch13.html#ch13fn2)) 描述了一种两次传递算法，其中包括一个*向上扫掠*阶段，计算数组的归约，并在过程中存储中间结果，随后是一个*向下扫掠*阶段，计算扫描的最终输出。向上扫掠的伪代码如下所示。
- en: '[2](ch13.html#ch13fn2a). [http://bit.ly/YmTmGP](http://bit.ly/YmTmGP)'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch13.html#ch13fn2a). [http://bit.ly/YmTmGP](http://bit.ly/YmTmGP)'
- en: '[Click here to view code image](ch13_images.html#p388pro01a)'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch13_images.html#p388pro01a)'
- en: upsweep(a, N)
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 向上扫掠(a, N)
- en: for d from 0 to (lg N) – 1
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 d 从 0 到 (lg N) – 1
- en: in parallel for i from 0 to N – 1 by 2^(d+1)
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 并行执行，i 从 0 到 N – 1，以 2^(d+1) 为步长
- en: a[i + 2^(d+1) – 1] += a[i + 2^(d – 1)]
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: a[i + 2^(d+1) – 1] += a[i + 2^(d – 1)]
- en: The operation resembles the log-step reduction we have discussed before, except
    intermediate sums are stored for later use in generating the final output of the
    scan.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 该操作类似于我们之前讨论的对数步归约，唯一不同的是，中间的和被存储起来，以便稍后用于生成扫描的最终输出。
- en: After Blelloch, [Figure 13.2](ch13.html#ch13fig02) shows an example run of this
    upsweep algorithm on an 8-element array using addition on integers. The “upsweep”
    terminology stems from thinking of the array as a balanced tree ([Figure 13.3](ch13.html#ch13fig03)).
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Blelloch的描述，[图13.2](ch13.html#ch13fig02)展示了这个向上扫掠算法在一个包含8个元素的数组上使用整数加法的示例运行。*向上扫掠*一词来自于将数组看作一个平衡树的思路（参见[图13.3](ch13.html#ch13fig03)）。
- en: '![Image](graphics/13fig02.jpg)'
  id: totrans-883
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/13fig02.jpg)'
- en: '*Figure 13.2* Upsweep pass (array view).'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.2* 向上扫掠传递（数组视图）。'
- en: '![Image](graphics/13fig03.jpg)'
  id: totrans-885
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/13fig03.jpg)'
- en: '*Figure 13.3* Upsweep pass (tree view).'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.3* 向上扫掠传递（树形视图）。'
- en: Once the upsweep has been completed, a *downsweep* propagates intermediate sums
    into the leaves of the tree. Pseudocode for the downsweep is as follows.
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦向上扫描完成，*向下扫描* 就会将中间的和传播到树的叶子节点。向下扫描的伪代码如下。
- en: '[Click here to view code image](ch13_images.html#p389pro01a)'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch13_images.html#p389pro01a)'
- en: downsweep(a, N)
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: downsweep(a, N)
- en: a[N-1] = 0
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: a[N-1] = 0
- en: for d from (lg N)-1 downto 0
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 从(dg N)-1 开始，d 一直减小到 0
- en: in parallel for i from 0 to N-1 by 2^(d+1)
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: 对 i 从 0 到 N-1 并行执行，步长为 2^(d+1)
- en: t := a[i+2^d-1]
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: t := a[i+2^d-1]
- en: a[i+2^d-1] = a[i + 2^(d+1)-1]
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: a[i+2^d-1] = a[i + 2^(d+1)-1]
- en: a[i+2^(d+1)-1] += t
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: a[i+2^(d+1)-1] += t
- en: '[Figure 13.4](ch13.html#ch13fig04) shows how the example array is transformed
    during the downsweep, and [Figure 13.5](ch13.html#ch13fig05) shows the downsweep
    in tree form. Early implementations of Scan for CUDA followed this algorithm closely,
    and it does make a good introduction to thinking about possible parallel implementations.
    Unfortunately, it is not a great match to CUDA’s architecture; a naïve implementation
    suffers from shared memory bank conflicts, and addressing schemes to compensate
    for the bank conflicts incurs enough overhead that the costs outweigh the benefits.'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13.4](ch13.html#ch13fig04) 显示了在向下扫描过程中示例数组的变换，[图 13.5](ch13.html#ch13fig05)
    显示了树形结构中的向下扫描。CUDA 的早期扫描实现紧密跟随了这一算法，并且它确实为思考并行实现提供了很好的引导。不幸的是，它并不非常适合 CUDA 的架构；一个简单的实现容易受到共享内存银行冲突的影响，而为了弥补这些银行冲突所采取的寻址方案，会带来足够的开销，使得成本超过了收益。'
- en: '![Image](graphics/13fig04.jpg)'
  id: totrans-897
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/13fig04.jpg)'
- en: '*Figure 13.4* Downsweep (array view).'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.4* 向下扫描（数组视图）。'
- en: '![Image](graphics/13fig05.jpg)'
  id: totrans-899
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/13fig05.jpg)'
- en: '*Figure 13.5* Downsweep (tree view).'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.5* 向下扫描（树形视图）。'
- en: 13.3\. Scan and Circuit Design
  id: totrans-901
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3\. 扫描与电路设计
- en: 'Having explored one possible parallel algorithm for Scan, it may now be clear
    that there are many different ways to implement parallel Scan algorithms. In reasoning
    about other possible implementations, we can take advantage of design methodologies
    for integer addition hardware that performs a similar function: Instead of propagating
    an arbitrary binary associative operator ![Image](graphics/plus.jpg) across an
    array, culminating in the output from a reduction, hardware adders propagate partial
    addition results, culminating in the carry bit to be propagated for multiprecision
    arithmetic.'
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了一个可能的并行算法之后，现在可能已经清楚，实际上有很多不同的方式可以实现并行的扫描算法。在思考其他可能的实现方式时，我们可以借鉴类似功能的整数加法硬件的设计方法：与其在数组中传播一个任意的二进制结合运算符
    ![Image](graphics/plus.jpg)，并在最终的归约中得出输出，不如让硬件加法器传播部分加法结果，最终将进位位传播到多精度运算中。
- en: Hardware designers use directed acyclic, oriented graphs to represent different
    implementations of Scan “circuits.” These diagrams compactly express both the
    data flow and the parallelism. A diagram of the serial implementation of [Listing
    13.1](ch13.html#ch13lis01) is given in [Figure 13.6](ch13.html#ch13fig06). The
    steps proceed downward as time advances; the vertical lines denote wires where
    the signal is propagated. Nodes of in-degree 2 (“operation nodes”) apply the operator
    ![Image](graphics/plus.jpg) to their inputs.
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件设计师使用有向无环图来表示Scan“电路”的不同实现方式。这些图示紧凑地表达了数据流和并行性。[列表13.1](ch13.html#ch13lis01)的串行实现图示见[图13.6](ch13.html#ch13fig06)。步骤随时间向下进行；垂直线表示信号传播的线路。具有2个输入度的节点（“操作节点”）对其输入应用运算符
    ![图片](graphics/plus.jpg)。
- en: '![Image](graphics/13fig06.jpg)'
  id: totrans-904
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/13fig06.jpg)'
- en: '*Figure 13.6* Serial scan.'
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.6* 串行扫描。'
- en: Note that the circuit diagrams show inclusive scans, not exclusive ones. For
    circuit diagrams, the difference is minor; to turn the inclusive scan in [Figure
    13.6](ch13.html#ch13fig06) into an exclusive scan, a 0 is wired into the first
    output and the sum is wired into the output, as shown in [Figure 13.7](ch13.html#ch13fig07).
    Note that both inclusive and exclusive scans generate the reduction of the input
    array as output, a characteristic that we will exploit in building efficient scan
    algorithms. (For purposes of clarity, all circuit diagrams other than [Figure
    13.7](ch13.html#ch13fig07) will depict inclusive scans.)
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，电路图显示的是包含扫描，而不是排他扫描。对于电路图而言，二者的差异很小；要将[图13.6](ch13.html#ch13fig06)中的包含扫描转换为排他扫描，需要将0接入第一个输出端，并将和接入输出端，如[图13.7](ch13.html#ch13fig07)所示。请注意，包含扫描和排他扫描都会将输入数组的减少作为输出，这是我们在构建高效扫描算法时将会利用的特性。（为了清晰起见，除了[图13.7](ch13.html#ch13fig07)之外，所有电路图都将展示包含扫描。）
- en: '![Image](graphics/13fig07.jpg)'
  id: totrans-907
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/13fig07.jpg)'
- en: '*Figure 13.7* Serial scan (inclusive and exclusive).'
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.7* 串行扫描（包含和排他）。'
- en: The Scan algorithm described by Blelloch corresponds to a circuit design known
    as Brent-Kung, a recursive decomposition in which every second output is fed into
    a Brent-Kung circuit of half the width. [Figure 13.8](ch13.html#ch13fig08) illustrates
    a Brent-Kung circuit operating on our example length of 8, along with Blelloch’s
    upsweep and downsweep phases. Nodes that broadcast their output to multiple nodes
    in the next stage are known as *fans.* Brent-Kung circuits are notable for having
    a constant fan-out of 2.
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: Blelloch描述的Scan算法对应于一种称为Brent-Kung的电路设计，这是一种递归分解方式，其中每个第二个输出被送入一个宽度为一半的Brent-Kung电路。[图13.8](ch13.html#ch13fig08)展示了一个Brent-Kung电路在我们的示例长度8上的工作情况，并包括Blelloch的向上扫描和向下扫描阶段。将输出广播到下一阶段多个节点的节点被称为*风扇*。Brent-Kung电路的一个显著特点是其具有恒定的风扇输出为2。
- en: '![Image](graphics/13fig08.jpg)'
  id: totrans-910
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/13fig08.jpg)'
- en: '*Figure 13.8* Brent-Kung circuit.'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.8* Brent-Kung电路。'
- en: The structure of a Brent-Kung circuit becomes clearer on larger circuits; see,
    for example, the circuit that processes 16 inputs in [Figure 13.9](ch13.html#ch13fig09).
    [Figure 13.9](ch13.html#ch13fig09) also highlights the *spine* of the circuit,
    the longest subgraph that generates the last element of the scan output.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 在更大的电路中，Brent-Kung电路的结构变得更加清晰；例如，见处理16个输入的电路[图13.9](ch13.html#ch13fig09)。[图13.9](ch13.html#ch13fig09)还突出显示了电路的*主干*，即生成扫描输出最后一个元素的最长子图。
- en: '![Image](graphics/13fig09.jpg)'
  id: totrans-913
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/13fig09.jpg)'
- en: '*Figure 13.9* Brent-Kung circuit (16 inputs).'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.9* Brent-Kung电路（16输入）。'
- en: The depth of the Brent-Kung circuit grows logarithmically in the number of inputs,
    illustrating its greater efficiency than (for example) the serial algorithm. But
    because each stage in the recursive decomposition increases the depth by 2, the
    Brent-Kung circuit is not of minimum depth. Sklansky described a method to build
    circuits of minimum depth by recursively decomposing them as shown in [Figure
    13.10](ch13.html#ch13fig10).
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: Brent-Kung电路的深度随输入数量呈对数增长，说明它比（例如）串行算法更高效。但由于递归分解中的每个阶段都会将深度增加2，因此Brent-Kung电路的深度并非最小深度。Sklansky描述了一种方法，通过递归分解电路来构建最小深度电路，如[图13.10](ch13.html#ch13fig10)所示。
- en: '![Image](graphics/13fig10.jpg)'
  id: totrans-916
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/13fig10.jpg)'
- en: '*Figure 13.10* Sklansky (minimum-depth) circuit.'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.10* Sklansky（最小深度）电路。'
- en: Two (*N*/2)-input circuits are run in parallel, and the output of the spine
    of the left circuit is added to each element of the right circuit. For our 16-element
    example, the left-hand subgraph of the recursion is highlighted in [Figure 13.10](ch13.html#ch13fig10).
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 两个(*N*/2)输入电路并行运行，左侧电路的主干输出与右侧电路的每个元素相加。对于我们的16元素示例，递归的左侧子图在[图13.10](ch13.html#ch13fig10)中被突出显示。
- en: Another minimum-depth scan circuit, known as Kogge-Stone, has a constant fan-out
    of 2, which is a desirable characteristic for hardware implementation, but, as
    you can see in [Figure 13.11](ch13.html#ch13fig11), it has many operation nodes;
    software implementations analogous to Kogge-Stone are work-inefficient.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种最小深度扫描电路，称为Kogge-Stone电路，具有2的常数扇出，这是硬件实现的理想特性。然而，正如你在[图13.11](ch13.html#ch13fig11)中看到的，它有许多操作节点；与Kogge-Stone类似的软件实现效率较低。
- en: '![Image](graphics/13fig11.jpg)'
  id: totrans-920
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/13fig11.jpg)'
- en: '*Figure 13.11* Kogge-Stone circuit.'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.11* Kogge-Stone电路。'
- en: Any scan circuit can be constructed from a combination of *scans* (which perform
    the parallel prefix computation and generate the sum of the input array as output)
    and *fans* (which add an input to each of their remaining outputs). The minimum-depth
    circuit in [Figure 13.10](ch13.html#ch13fig10) makes heavy use of fans in its
    recursive definition.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 任何扫描电路都可以通过*扫描*（执行并行前缀计算并生成输入数组的总和作为输出）和*扇出*（将输入添加到每个剩余输出）组合构建。[图13.10](ch13.html#ch13fig10)中的最小深度电路在递归定义中大量使用了扇出。
- en: For an optimized CUDA implementation, a key insight is that a fan doesn’t need
    to take its input from a Scan *per se*; any reduction will do. And from [Chapter
    12](ch12.html#ch12), we have a highly optimized reduction algorithm.
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 对于优化的 CUDA 实现，一个关键的见解是，风扇不一定需要从扫描*本身*获取输入；任何归约都可以。并且从[第 12 章](ch12.html#ch12)中，我们有一个高度优化的归约算法。
- en: If, for example, we split the input array into subarrays of length *b* and compute
    the sum of each subarray using our optimized reduction routine, we end up with
    an array of ![Image](graphics/393equ01.jpg) reduction values. *If we then perform
    an exclusive scan on* *that array, it becomes an array of fan inputs (*seeds*)
    for scans of each subarray.* The number of values that can be efficiently scanned
    in one pass over global memory is limited by CUDA’s thread block and shared memory
    size, so for larger inputs, the approach must be applied recursively.
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 举例来说，如果我们将输入数组拆分成长度为*b*的子数组，并使用优化过的归约例程计算每个子数组的和，我们最终会得到一个包含![Image](graphics/393equ01.jpg)归约值的数组。*如果我们接着对*
    *该数组执行独占扫描，它将成为每个子数组扫描的风扇输入（*种子*）数组。* 可以在全局内存中一次扫描的值的数量受限于 CUDA 的线程块和共享内存大小，因此对于较大的输入，必须递归地应用此方法。
- en: 13.4\. CUDA Implementations
  id: totrans-925
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.4\. CUDA 实现
- en: Designing Scan algorithms and studying circuit diagrams is instructive, but
    in order to implement Scan for CUDA, we need to map the algorithms onto registers,
    memory and addressing schemes, and correct synchronization. The optimal CUDA implementation
    of Scan depends on the size of the scan being performed. Different schemes are
    best for warp-sized scans, scans that can fit in shared memory, and scans that
    must spill to global memory. Because blocks cannot reliably exchange data through
    global memory, scans too large to fit in shared memory *must* perform multiple
    kernel invocations.^([3](ch13.html#ch13fn3))
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 设计扫描算法并研究电路图是很有启发性的，但为了在 CUDA 中实现扫描，我们需要将算法映射到寄存器、内存和寻址方案，并进行正确的同步。扫描的最优 CUDA
    实现取决于正在执行的扫描的大小。不同的方案适用于符合 warp 大小的扫描、能够适配共享内存的扫描以及必须溢出到全局内存的扫描。因为块不能通过全局内存可靠地交换数据，因此过大的扫描必须执行多个内核调用。^([3](ch13.html#ch13fn3))
- en: '[3](ch13.html#ch13fn3a). With CUDA 5.0 and SM 3.5 hardware, dynamic parallelism
    can move most of the kernel launches to be “child grids” as opposed to kernel
    launches initiated by the host.'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch13.html#ch13fn3a). 使用 CUDA 5.0 和 SM 3.5 硬件，动态并行性可以将大多数内核启动移至“子网格”，而不是由主机启动的内核调用。'
- en: Before examining special cases (such as scanning of predicates), we will examine
    three (3) approaches to doing Scan on CUDA.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 在考察特殊情况（例如谓词的扫描）之前，我们将探讨三种在 CUDA 上执行扫描的方法。
- en: • Scan-then-fan (recursive)
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: • 扫描-再风扇（递归）
- en: • Reduce-then-scan (recursive)
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: • 减少-再扫描（递归）
- en: • Two-level reduce-then-scan
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: • 两级减少-再扫描
- en: 13.4.1\. Scan-Then-Fan
  id: totrans-932
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.4.1\. 扫描-再风扇
- en: 'The scan-then-fan approach uses a similar decomposition for global and shared
    memory. [Figure 13.12](ch13.html#ch13fig12) shows the approach used to scan a
    threadblock: A scan is performed on each 32-thread warp, and the reduction of
    that 32-element subarray is written to shared memory. A single warp then scans
    the array of partial sums. A single warp is sufficient because CUDA does not support
    threadblocks with more than 1024 threads. Finally, the base sums are fanned out
    to each warp’s output elements. Note that [Figure 13.12](ch13.html#ch13fig12)
    shows an inclusive scan being performed in step 2, so the first element of its
    output must be fanned out to the second warp, and so on.'
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: scan-then-fan方法对全局内存和共享内存使用了类似的分解。[图13.12](ch13.html#ch13fig12)展示了用于扫描线程块的方法：对每个32线程warp执行扫描，并将该32元素子数组的归约结果写入共享内存。然后，一个warp扫描部分和的数组。由于CUDA不支持超过1024个线程的线程块，因此一个warp足以完成扫描。最后，基础和会分配到每个warp的输出元素。注意，[图13.12](ch13.html#ch13fig12)显示了在第2步执行的是包含扫描，因此其输出的第一个元素必须分配到第二个warp，依此类推。
- en: '![Image](graphics/13fig12.jpg)'
  id: totrans-934
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/13fig12.jpg)'
- en: '*Figure 13.12* Scan-then-fan (shared memory).'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: '*图13.12* Scan-then-fan（共享内存）。'
- en: The code to implement this algorithm is given in [Listing 13.3](ch13.html#ch13lis03).
    The input array is assumed to have been loaded into shared memory already, and
    the parameters `sharedPartials` and `idx` specify the base address and index of
    the warp to scan, respectively. (In our first implementation, `threadIdx.x` is
    passed as the parameter `idx`.) Lines 9–13 implement step 1 in [Figure 13.12](ch13.html#ch13fig12);
    lines 16–21 implement step 2; and lines 31–45 implement step 3\. The output value
    written by this thread is returned to the caller but is used only if it happens
    to be the thread block’s reduction.
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 实现此算法的代码见于[清单 13.3](ch13.html#ch13lis03)。假设输入数组已加载到共享内存中，参数`sharedPartials`和`idx`分别指定要扫描的warp的基地址和索引。（在我们第一次实现中，`threadIdx.x`作为参数`idx`传递。）第9到13行实现了[图13.12](ch13.html#ch13fig12)中的第1步；第16到21行实现了第2步；第31到45行实现了第3步。此线程写入的输出值会返回给调用者，但只有在它恰好是线程块的归约时才会使用。
- en: '*Listing 13.3.* scanBlock: Block portion of scan-then-fan for thread blocks.'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 13.3.* scanBlock：线程块的scan-then-fan部分。'
- en: '[Click here to view code image](ch13_images.html#p13lis03a)'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch13_images.html#p13lis03a)'
- en: '* * *'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: inline __device__ T
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ T
- en: scanBlock( volatile T *sPartials )
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: scanBlock( volatile T *sPartials )
- en: '{'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern __shared__ T warpPartials[];
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ T warpPartials[];
- en: const int tid = threadIdx.x;
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: const int lane = tid & 31;
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: const int lane = tid & 31;
- en: const int warpid = tid >> 5;
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: const int warpid = tid >> 5;
- en: //
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // Compute this thread's partial sum
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: // 计算此线程的部分和
- en: //
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: T sum = scanWarp<T>( sPartials );
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: T sum = scanWarp<T>( sPartials );
- en: __syncthreads();
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: //
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // Write each warp's reduction to shared memory
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: // 将每个warp的归约结果写入共享内存
- en: //
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: if ( lane == 31 ) {
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: if ( lane == 31 ) {
- en: warpPartials[16+warpid] = sum;
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: warpPartials[16+warpid] = sum;
- en: '}'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: //
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // Have one warp scan reductions
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: // 让一个warp进行归约扫描
- en: //
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: if ( warpid==0 ) {
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: if ( warpid==0 ) {
- en: scanWarp<T>( 16+warpPartials+tid );
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: scanWarp<T>( 16+warpPartials+tid );
- en: '}'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: //
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // Fan out the exclusive scan element (obtained
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: // 扩展独占扫描元素（获得
- en: // by the conditional and the decrement by 1)
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: // 由条件判断和减去1所致）
- en: // to this warp's pending output
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: // 到此warp的待处理输出
- en: //
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: if ( warpid > 0 ) {
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: if ( warpid > 0 ) {
- en: sum += warpPartials[16+warpid-1];
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: sum += warpPartials[16+warpid-1];
- en: '}'
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: //
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // Write this thread's scan output
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: // 写入此线程的扫描输出
- en: //
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: '*sPartials = sum;'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: '*sPartials = sum;'
- en: __syncthreads();
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: //
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // The return value will only be used by caller if it
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: // 返回值仅在调用者需要时使用
- en: // contains the spine value (i.e., the reduction
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: // 包含主干值（即约简
- en: // of the array we just scanned).
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: // 我们刚刚扫描的数组的）。
- en: //
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: return sum;
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: return sum;
- en: '}'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Figure 13.13](ch13.html#ch13fig13) shows how this approach is adapted to global
    memory. A kernel scans *b*-element subarrays, where *b* is the block size. The
    partial sums are written to global memory, and another 1-block kernel invocation
    scans these partial sums, which are then fanned into the final output in global
    memory.'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 13.13](ch13.html#ch13fig13) 显示了这种方法如何适应全局内存。一个内核扫描*b*元素的子数组，其中*b*是块大小。部分和被写入全局内存，另一个1块内核调用扫描这些部分和，然后将它们扩展为全局内存中的最终输出。'
- en: '![Image](graphics/13fig13.jpg)'
  id: totrans-990
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/13fig13.jpg)'
- en: '*Figure 13.13* Scan-then-fan (global memory).'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: '*Figure 13.13* 扫描然后扩展（全局内存）。'
- en: '[Listing 13.4](ch13.html#ch13lis04) gives the CUDA code for the Scan kernel
    in step 1 in [Figure 13.13](ch13.html#ch13fig13). It loops over the threadblocks
    to process, staging the input array into and out of shared memory. The kernel
    then optionally writes the spine value to global memory at the end. At the bottom
    level of the recursion, there is no need to record spine values, so the `bWriteSpine`
    template parameter enables the kernel to avoid dynamically checking the value
    of `partialsOut`.'
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 13.4](ch13.html#ch13lis04) 给出了步骤1中CUDA扫描内核的代码，在[Figure 13.13](ch13.html#ch13fig13)中展示。它循环遍历线程块进行处理，将输入数组进出共享内存。该内核在结束时可选择性地将主干值写入全局内存。在递归的最底层，不需要记录主干值，因此`bWriteSpine`模板参数使得内核能够避免动态检查`partialsOut`的值。'
- en: '*Listing 13.4.* scanAndWritePartials.'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 13.4.* scanAndWritePartials.'
- en: '[Click here to view code image](ch13_images.html#p13lis04a)'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: '[Click here to view code image](ch13_images.html#p13lis04a)'
- en: '* * *'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, bool bWriteSpine>
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, bool bWriteSpine>
- en: __global__ void
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: scanAndWritePartials(
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: scanAndWritePartials(
- en: T *out,
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: T *out,
- en: T *gPartials,
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: T *gPartials,
- en: const T *in,
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: const T *in,
- en: size_t N,
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: size_t numBlocks )
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: size_t numBlocks )
- en: '{'
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern volatile __shared__ T sPartials[];
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: extern volatile __shared__ T sPartials[];
- en: const int tid = threadIdx.x;
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: volatile T *myShared = sPartials+tid;
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: volatile T *myShared = sPartials+tid;
- en: for ( size_t iBlock = blockIdx.x;
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t iBlock = blockIdx.x;
- en: iBlock < numBlocks;
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock < numBlocks;
- en: iBlock += gridDim.x ) {
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock += gridDim.x ) {
- en: size_t index = iBlock*blockDim.x+tid;
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = iBlock*blockDim.x+tid;
- en: '*myShared = (index < N) ? in[index] : 0;'
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: '*myShared = (index < N) ? in[index] : 0;'
- en: __syncthreads();
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: T sum = scanBlock( myShared );
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: T sum = scanBlock( myShared );
- en: __syncthreads();
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: if ( index < N ) {
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index < N ) {
- en: out[index] = *myShared;
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: out[index] = *myShared;
- en: '}'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: //
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // write the spine value to global memory
  id: totrans-1020
  prefs: []
  type: TYPE_NORMAL
  zh: // 将主干值写入全局内存
- en: //
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: if ( bWriteSpine && (threadIdx.x==(blockDim.x-1)) )
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: if ( bWriteSpine && (threadIdx.x==(blockDim.x-1)) )
- en: '{'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: gPartials[iBlock] = sum;
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: gPartials[iBlock] = sum;
- en: '}'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 13.5](ch13.html#ch13lis05) gives the host function that uses [Listings
    13.3](ch13.html#ch13lis03) to [13.4](ch13.html#ch13lis04) to implement an inclusive
    scan on an array in global memory. Note that the function recurses for scans too
    large to perform in shared memory. The first conditional in the function serves
    both as the base case for the recursion and to short-circuit scans small enough
    to perform in shared memory alone, avoiding any need to allocate global memory.
    Note how the amount of shared memory needed by the kernel (`b*sizeof(T)`) is specified
    at kernel invocation time.'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单13.5](ch13.html#ch13lis05)给出了主机函数，利用[清单13.3](ch13.html#ch13lis03)到[13.4](ch13.html#ch13lis04)来实现对全局内存中数组的包容性扫描。注意，该函数对无法在共享内存中执行的扫描进行递归。函数中的第一个条件既是递归的基本情况，也是为了对足够小的扫描直接在共享内存中执行，避免分配全局内存。请注意，在内核调用时指定了内核所需的共享内存量（`b*sizeof(T)`）。'
- en: For larger scans, the function computes the number of partial sums needed ![Image](graphics/398equ01.jpg),
    allocates global memory to hold them, and follows the pattern in Figure 3.13,
    writing partial sums to the global array for later use by the `scanAndWritePartials`
    kernel in [Listing 13.4](ch13.html#ch13lis04).
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较大的扫描，该函数计算所需的部分和的数量 ![Image](graphics/398equ01.jpg)，为它们分配全局内存，并遵循图3.13中的模式，将部分和写入全局数组，以供`scanAndWritePartials`内核在[清单13.4](ch13.html#ch13lis04)中稍后使用。
- en: 'Each level of recursion reduces the number of elements being processed by a
    factor of *b*, so for *b* = 128 and *N* = 1048576, for example, two levels of
    recursion are required: one of size 8192 and one of size 64.'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层递归都会将处理的元素数量减少一个*b*倍，因此，例如，当*b* = 128且*N* = 1048576时，需进行两层递归：第一层大小为8192，第二层大小为64。
- en: '*Listing 13.5.* scanFan host function.'
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单13.5.* scanFan主机函数。'
- en: '[Click here to view code image](ch13_images.html#p13lis05a)'
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch13_images.html#p13lis05a)'
- en: '* * *'
  id: totrans-1034
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: void
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: scanFan( T *out, const T *in, size_t N, int b )
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: scanFan( T *out, const T *in, size_t N, int b )
- en: '{'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: if ( N <= b ) {
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: if ( N <= b ) {
- en: scanAndWritePartials<T, false><<<1,b,b*sizeof(T)>>>(
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: scanAndWritePartials<T, false><<<1,b,b*sizeof(T)>>>(
- en: out, 0, in, N, 1 );
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: out, 0, in, N, 1 );
- en: return;
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: return;
- en: '}'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: //
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // device pointer to array of partial sums in global memory
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: // 指向全局内存中部分和数组的设备指针
- en: //
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: T *gPartials = 0;
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: T *gPartials = 0;
- en: //
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // ceil(N/b)
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: // ceil(N/b)
- en: //
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: size_t numPartials = (N1)/b;
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: size_t numPartials = (N1)/b;
- en: //
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // number of CUDA threadblocks to use.  The kernels are
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: // 要使用的CUDA线程块数。内核是
- en: // blocking agnostic, so we can clamp to any number
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: // 阻塞无关，因此我们可以将其限制为任意数字
- en: // within CUDA's limits and the code will work.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: // 在CUDA的限制内，代码将正常工作。
- en: //
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: const unsigned int maxBlocks = 150;   // maximum blocks to launch
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: const unsigned int maxBlocks = 150;   // 启动的最大块数
- en: unsigned int numBlocks = min( numPartials, maxBlocks );
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 无符号整数 numBlocks = min( numPartials, maxBlocks );
- en: CUDART_CHECK( cudaMalloc( &gPartials,
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMalloc( &gPartials,
- en: numPartials*sizeof(T) ) );
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: numPartials*sizeof(T) ) );
- en: scanAndWritePartials<T, true><<<numBlocks,b,b*sizeof(T)>>>(
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: scanAndWritePartials<T, true><<<numBlocks,b,b*sizeof(T)>>>(
- en: out, gPartials, in, N, numPartials );
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: out, gPartials, in, N, numPartials );
- en: scanFan<T>( gPartials, gPartials, numPartials, b );
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: scanFan<T>( gPartials, gPartials, numPartials, b );
- en: scanAddBaseSums<T><<<numBlocks, b>>>( out, gPartials, N,
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: scanAddBaseSums<T><<<numBlocks, b>>>( out, gPartials, N,
- en: numPartials );
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: numPartials );
- en: 'Error:'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: cudaFree( gPartials );
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFree( gPartials );
- en: '}'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 13.6](ch13.html#ch13lis06) completes the picture with a very simple
    kernel to fan-out results from global memory to global memory.'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.6](ch13.html#ch13lis06) 用一个非常简单的内核完成了从全局内存到全局内存的结果分发。'
- en: '*Listing 13.6.* scanAddBaseSums kernel.'
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.6.* scanAddBaseSums 内核。'
- en: '[Click here to view code image](ch13_images.html#p13lis06a)'
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch13_images.html#p13lis06a)'
- en: '* * *'
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: __global__ void
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: scanAddBaseSums(
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: scanAddBaseSums(
- en: T *out,
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: T *out,
- en: T *gBaseSums,
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: T *gBaseSums,
- en: size_t N,
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: size_t numBlocks )
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: size_t numBlocks )
- en: '{'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const int tid = threadIdx.x;
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: T fan_value = 0;
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: T fan_value = 0;
- en: for ( size_t iBlock = blockIdx.x;
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t iBlock = blockIdx.x;
- en: iBlock < numBlocks;
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock < numBlocks;
- en: iBlock += gridDim.x ) {
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock += gridDim.x ) {
- en: size_t index = iBlock*blockDim.x+tid;
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = iBlock*blockDim.x+tid;
- en: if ( iBlock > 0 ) {
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: if ( iBlock > 0 ) {
- en: fan_value = gBaseSums[iBlock-1];
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: fan_value = gBaseSums[iBlock-1];
- en: '}'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: out[index] += fan_value;
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: out[index] += fan_value;
- en: '}'
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: At the highest level of recursion, the scan-then-fan strategy performs 4*N*
    global memory operations. The initial scan performs one read and one write, and
    then the fan in [Listing 13.4](ch13.html#ch13lis04) performs another read and
    write. We can decrease the number of global memory writes by first computing only
    reductions on the input array.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高层次的递归中，扫描-再分发策略执行了 4*N* 次全局内存操作。初始扫描执行一次读取和一次写入，然后在[列表 13.4](ch13.html#ch13lis04)中执行的分发又进行了一次读取和写入。我们可以通过首先仅对输入数组进行归约来减少全局内存写入的次数。
- en: 13.4.2\. Reduce-Then-Scan (Recursive)
  id: totrans-1097
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.4.2\. 归约-再扫描（递归）
- en: '[Figure 13.14](ch13.html#ch13fig14) shows how this strategy works. As before,
    an array of ![Image](graphics/398equ01.jpg) partial sums of the input is computed
    and scanned to compute an array of base sums. But instead of doing the scan in
    the first pass, we compute only the partial sums in the first pass. The scan of
    the final output is then performed, adding the base sum along the way.'
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13.14](ch13.html#ch13fig14)展示了这一策略的工作原理。与之前一样，计算输入的部分和数组 ![Image](graphics/398equ01.jpg)
    并扫描以计算基和数组。但我们在第一次传递中只计算部分和，而不是进行扫描。然后执行最终输出的扫描，同时添加基和。'
- en: '![Image](graphics/13fig14.jpg)'
  id: totrans-1099
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/13fig14.jpg)'
- en: '*Figure 13.14* Reduce-then-scan.'
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.14* 归约-再扫描。'
- en: '[Listing 13.7](ch13.html#ch13lis07) gives the code used to compute the array
    of partial sums, which uses the reduction code from [Listing 12.3](ch12.html#ch12lis03)
    as a subroutine. As with the reduction code, the kernel is templatized according
    to block size, and a wrapper template uses a `switch` statement to invoke specializations
    of the template.'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 13.7](ch13.html#ch13lis07) 给出了用于计算部分和数组的代码，该代码使用 [Listing 12.3](ch12.html#ch12lis03)
    中的归约代码作为子程序。与归约代码一样，内核根据块大小进行了模板化，包装模板使用 `switch` 语句来调用模板的特化版本。'
- en: '*Listing 13.7.* scanReduceBlocks.'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 13.7.* scanReduceBlocks.'
- en: '[Click here to view code image](ch13_images.html#p13lis07a)'
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch13_images.html#p13lis07a)'
- en: '* * *'
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, int numThreads>
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, int numThreads>
- en: __global__ void
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: scanReduceBlocks( T *gPartials, const T *in, size_t N )
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: scanReduceBlocks( T *gPartials, const T *in, size_t N )
- en: '{'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern volatile __shared__ T sPartials[];
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: extern volatile __shared__ T sPartials[];
- en: const int tid = threadIdx.x;
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: gPartials += blockIdx.x;
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: gPartials += blockIdx.x;
- en: for ( size_t i = blockIdx.x*blockDim.x;
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = blockIdx.x*blockDim.x;
- en: i < N;
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x ) {
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x ) {
- en: size_t index = i+tid;
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = i+tid;
- en: 'sPartials[tid] = (index < N) ? in[index] : 0;'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 'sPartials[tid] = (index < N) ? in[index] : 0;'
- en: __syncthreads();
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: reduceBlock<T,numThreads>( gPartials, sPartials );
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: reduceBlock<T,numThreads>( gPartials, sPartials );
- en: __syncthreads();
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: gPartials += gridDim.x;
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: gPartials += gridDim.x;
- en: '}'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: template<class T>
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: void
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: scanReduceBlocks(
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: scanReduceBlocks(
- en: T *gPartials,
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: T *gPartials,
- en: const T *in,
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: const T *in,
- en: size_t N,
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: int numThreads,
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: int numThreads,
- en: int numBlocks )
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: int numBlocks )
- en: '{'
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: switch ( numThreads ) {
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: switch ( numThreads ) {
- en: 'case  128: return scanReduceBlocks<T, 128> ... ( ... );'
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 128: return scanReduceBlocks<T, 128> ... ( ... );'
- en: 'case  256: return scanReduceBlocks<T, 256> ... ( ... );'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 256: return scanReduceBlocks<T, 256> ... ( ... );'
- en: 'case  512: return scanReduceBlocks<T, 512> ... ( ... );'
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 512: return scanReduceBlocks<T, 512> ... ( ... );'
- en: 'case 1024: return scanReduceBlocks<T,1024> ... ( ... );'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: 'case 1024: return scanReduceBlocks<T,1024> ... ( ... );'
- en: '}'
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 13.8](ch13.html#ch13lis08) gives the kernel used to perform the scans.
    The main difference from [Listing 13.4](ch13.html#ch13lis04) is that instead of
    writing the sum of the input subarrays to global memory, the kernel adds the base
    sum corresponding to each subarray to the output elements before writing them.'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 13.8](ch13.html#ch13lis08) 给出了用于执行扫描的内核。与 [Listing 13.4](ch13.html#ch13lis04)
    的主要区别在于，内核不是将输入子数组的和写入全局内存，而是在写入之前将每个子数组对应的基准和加到输出元素中。'
- en: '*Listing 13.8.* scanWithBaseSums.'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 13.8.* scanWithBaseSums.'
- en: '[Click here to view code image](ch13_images.html#p13lis08a)'
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch13_images.html#p13lis08a)'
- en: '* * *'
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: __global__ void
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: scanWithBaseSums(
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: scanWithBaseSums(
- en: T *out,
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: T *out,
- en: const T *gBaseSums,
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: const T *gBaseSums,
- en: const T *in,
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: const T *in,
- en: size_t N,
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: size_t numBlocks )
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: size_t numBlocks )
- en: '{'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern volatile __shared__ T sPartials[];
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: extern volatile __shared__ T sPartials[];
- en: const int tid = threadIdx.x;
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: for ( size_t iBlock = blockIdx.x;
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t iBlock = blockIdx.x;
- en: iBlock < numBlocks;
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock < numBlocks;
- en: iBlock += gridDim.x ) {
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock += gridDim.x ) {
- en: T base_sum = 0;
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: T base_sum = 0;
- en: size_t index = iBlock*blockDim.x+tid;
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = iBlock*blockDim.x+tid;
- en: if ( iBlock > 0 && gBaseSums ) {
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: if ( iBlock > 0 && gBaseSums ) {
- en: base_sum = gBaseSums[iBlock-1];
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: base_sum = gBaseSums[iBlock-1];
- en: '}'
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'sPartials[tid] = (index < N) ? in[index] : 0;'
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 'sPartials[tid] = (index < N) ? in[index] : 0;'
- en: __syncthreads();
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: scanBlock( sPartials+tid );
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: scanBlock( sPartials+tid );
- en: __syncthreads();
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: if ( index < N ) {
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index < N ) {
- en: out[index] = sPartials[tid]+base_sum;
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: out[index] = sPartials[tid]+base_sum;
- en: '}'
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The host code for the reduce-then-scan strategy is given in [Listing 13.9](ch13.html#ch13lis09).
    At the highest level of recursion, the reduce-then-scan strategy performs 3*N*
    global memory operations. The initial reduction pass performs one read per element,
    and then the scan in [Listing 13.9](ch13.html#ch13lis09) performs another read
    and a write. As with fan-then-scan, each level of recursion reduces the number
    of elements being processed by a factor of *b*.
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 reduce-then-scan 策略的主机代码见 [Listing 13.9](ch13.html#ch13lis09)。在递归的最高层次，reduce-then-scan
    策略执行 3*N* 次全局内存操作。初始的归约步骤对每个元素执行一次读取，然后 [Listing 13.9](ch13.html#ch13lis09) 中的扫描操作会再执行一次读取和一次写入。与
    fan-then-scan 相似，每一级递归都会将处理的元素数量减少一个 *b* 的倍数。
- en: '*Listing 13.9.* scanReduceThenScan.'
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 13.9.* scanReduceThenScan.'
- en: '[Click here to view code image](ch13_images.html#p13lis09a)'
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch13_images.html#p13lis09a)'
- en: '* * *'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: void
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: scanReduceThenScan( T *out, const T *in, size_t N, int b )
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: scanReduceThenScan( T *out, const T *in, size_t N, int b )
- en: '{'
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: if ( N <= b ) {
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: if ( N <= b ) {
- en: return scanWithBaseSums<T><<<1,b,b*sizeof(T)>>>(
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: return scanWithBaseSums<T><<<1,b,b*sizeof(T)>>>(
- en: out, 0, in, N, 1 );
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: out, 0, in, N, 1 );
- en: '}'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: //
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // device pointer to array of partial sums in global memory
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: // 指向全局内存中部分和数组的设备指针
- en: //
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: T *gPartials = 0;
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: T *gPartials = 0;
- en: //
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // ceil(N/b) = number of partial sums to compute
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: // ceil(N/b) = 要计算的部分和的数量
- en: //
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: size_t numPartials = (N1)/b;
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: size_t numPartials = (N1)/b;
- en: //
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // number of CUDA threadblocks to use.  The kernels are blocking
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: // 使用的 CUDA 线程块数。内核是阻塞式的
- en: // agnostic, so we can clamp to any number within CUDA's limits
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: // 无关紧要，因此我们可以将其限制为 CUDA 限制内的任何数值
- en: // and the code will work.
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: // 代码将正常工作。
- en: //
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: const unsigned int maxBlocks = 150;
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: const unsigned int maxBlocks = 150;
- en: unsigned int numBlocks = min( numPartials, maxBlocks );
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int numBlocks = min( numPartials, maxBlocks );
- en: CUDART_CHECK( cudaMalloc( &gPartials, numPartials*sizeof(T) ) );
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMalloc( &gPartials, numPartials*sizeof(T) ) );
- en: scanReduceBlocks<T>( gPartials, in, N, b, numBlocks );
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: scanReduceBlocks<T>( gPartials, in, N, b, numBlocks );
- en: scanReduceThenScan<T>( gPartials, gPartials, numPartials, b );
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: scanReduceThenScan<T>( gPartials, gPartials, numPartials, b );
- en: scanWithBaseSums<T><<<numBlocks,b,b*sizeof(T)>>>(
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: scanWithBaseSums<T><<<numBlocks,b,b*sizeof(T)>>>(
- en: out,
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: out,
- en: gPartials,
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: gPartials,
- en: in,
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: in,
- en: N,
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: N,
- en: numPartials );
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: numPartials );
- en: 'Error:'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: cudaFree( gPartials );
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: cudaFree( gPartials );
- en: '}'
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 13.4.3\. Reduce-Then-Scan (Two Pass)
  id: totrans-1214
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.4.3\. Reduce-Then-Scan（两次传递）
- en: Merrill^([4](ch13.html#ch13fn4)) describes another formulation of Scan that
    uses a small, fixed-size number of base sums. The algorithm is the same as [Figure
    13.14](ch13.html#ch13fig14), except that the array in step 2 is a relatively small,
    fixed size of perhaps a few hundred instead of ![Image](graphics/398equ01.jpg)
    partial sums. The number of partial sums is the same as the number of threadblocks
    to use, both for the reduction pass and for the Scan pass. [Listing 13.10](ch13.html#ch13lis10)
    shows the code to compute these partial sums, which is updated to compute reductions
    for subarrays of size `elementsPerPartial` as opposed to the thread block size.
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: Merrill^([4](ch13.html#ch13fn4)) 描述了另一种扫描的公式，它使用了一个小的、固定大小的基本和数目。该算法与[图13.14](ch13.html#ch13fig14)相同，唯一的区别是步骤2中的数组大小相对较小，固定为几百个，而不是![Image](graphics/398equ01.jpg)个部分和。部分和的数量与需要使用的线程块数量相同，既用于归约过程，也用于扫描过程。[代码清单13.10](ch13.html#ch13lis10)展示了计算这些部分和的代码，代码更新为计算大小为`elementsPerPartial`的子数组的归约，而不是线程块的大小。
- en: '[4](ch13.html#ch13fn4a). [http://bit.ly/ZKtlh1](http://bit.ly/ZKtlh1)'
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch13.html#ch13fn4a). [http://bit.ly/ZKtlh1](http://bit.ly/ZKtlh1)'
- en: '*Listing 13.10.* scanReduceSubarrays.'
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: '*代码清单13.10*。 scanReduceSubarrays。'
- en: '[Click here to view code image](ch13_images.html#p13lis10a)'
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch13_images.html#p13lis10a)'
- en: '* * *'
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, int numThreads>
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, int numThreads>
- en: __device__ void
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ void
- en: scanReduceSubarray(
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
  zh: scanReduceSubarray(
- en: T *gPartials,
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: T *gPartials,
- en: const T *in,
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: const T *in,
- en: size_t iBlock,
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: size_t iBlock,
- en: size_t N,
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: int elementsPerPartial )
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: int elementsPerPartial )
- en: '{'
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern volatile __shared__ T sPartials[];
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
  zh: extern volatile __shared__ T sPartials[];
- en: const int tid = threadIdx.x;
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: size_t baseIndex = iBlock*elementsPerPartial;
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: size_t baseIndex = iBlock*elementsPerPartial;
- en: T sum = 0;
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: T sum = 0;
- en: for ( int i = tid; i < elementsPerPartial; i += blockDim.x ) {
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = tid; i < elementsPerPartial; i += blockDim.x ) {
- en: size_t index = baseIndex+i;
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = baseIndex+i;
- en: if ( index < N )
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index < N )
- en: sum += in[index];
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
  zh: sum += in[index];
- en: '}'
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sPartials[tid] = sum;
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] = sum;
- en: __syncthreads();
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: reduceBlock<T,numThreads>( &gPartials[iBlock], sPartials );
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: reduceBlock<T,numThreads>( &gPartials[iBlock], sPartials );
- en: '}'
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: /*
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: /*
- en: '* Compute the reductions of each subarray of size'
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: '* 计算每个大小为子数组的归约'
- en: '* elementsPerPartial, and write them to gPartials.'
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: '* elementsPerPartial，并将它们写入gPartials。'
- en: '*/'
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
  zh: '*/'
- en: template<class T, int numThreads>
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, int numThreads>
- en: __global__ void
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: scanReduceSubarrays(
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
  zh: scanReduceSubarrays(
- en: T *gPartials,
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: T *gPartials,
- en: const T *in,
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
  zh: const T *in,
- en: size_t N,
  id: totrans-1251
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: int elementsPerPartial )
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
  zh: int elementsPerPartial )
- en: '{'
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern volatile __shared__ T sPartials[];
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
  zh: extern volatile __shared__ T sPartials[];
- en: for ( int iBlock = blockIdx.x;
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int iBlock = blockIdx.x;
- en: iBlock*elementsPerPartial < N;
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock*elementsPerPartial < N;
- en: iBlock += gridDim.x )
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock += gridDim.x )
- en: '{'
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: scanReduceSubarray<T,numThreads>(
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: scanReduceSubarray<T,numThreads>(
- en: gPartials,
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: gPartials,
- en: in,
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: in,
- en: iBlock,
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock,
- en: N,
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: N,
- en: elementsPerPartial );
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: elementsPerPartial );
- en: '}'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 13.11](ch13.html#ch13lis11) gives the Scan code, which has been modified
    to carry over each block’s sum as the Scan of that block is completed. The `bZeroPad`
    template parameter in [Listing 13.11](ch13.html#ch13lis11) and the utility function
    `scanSharedIndex` that uses it are described in more detail in [Section 13.5.1](ch13.html#ch13lev2sec4).'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.11](ch13.html#ch13lis11) 给出了扫描代码，已经修改为在每个块的扫描完成时传递该块的和。[列表 13.11](ch13.html#ch13lis11)中的`bZeroPad`模板参数和使用它的实用函数`scanSharedIndex`在[第13.5.1节](ch13.html#ch13lev2sec4)中有更详细的描述。'
- en: '*Listing 13.11.* scan2Level_kernel.'
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.11.* scan2Level_kernel.'
- en: '[Click here to view code image](ch13_images.html#p13lis11a)'
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch13_images.html#p13lis11a)'
- en: '* * *'
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, bool bZeroPad>
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, bool bZeroPad>
- en: __global__ void
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: scan2Level_kernel(
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: scan2Level_kernel(
- en: T *out,
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: T *out,
- en: const T *gBaseSums,
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
  zh: const T *gBaseSums,
- en: const T *in,
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: const T *in,
- en: size_t N,
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: size_t elementsPerPartial )
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: size_t elementsPerPartial )
- en: '{'
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern volatile __shared__ T sPartials[];
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: extern volatile __shared__ T sPartials[];
- en: const int tid = threadIdx.x;
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: int sIndex = scanSharedIndex<bZeroPad>( threadIdx.x );
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: int sIndex = scanSharedIndex<bZeroPad>( threadIdx.x );
- en: if ( bZeroPad ) {
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: if ( bZeroPad ) {
- en: sPartials[sIndex-16] = 0;
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[sIndex-16] = 0;
- en: '}'
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: T base_sum = 0;
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: T base_sum = 0;
- en: if ( blockIdx.x && gBaseSums ) {
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: if ( blockIdx.x && gBaseSums ) {
- en: base_sum = gBaseSums[blockIdx.x-1];
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: base_sum = gBaseSums[blockIdx.x-1];
- en: '}'
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( size_t i = 0;
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = 0;
- en: i < elementsPerPartial;
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: i < elementsPerPartial;
- en: i += blockDim.x ) {
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x ) {
- en: size_t index = blockIdx.x*elementsPerPartial + i + tid;
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = blockIdx.x*elementsPerPartial + i + tid;
- en: 'sPartials[sIndex] = (index < N) ? in[index] : 0;'
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
  zh: 'sPartials[sIndex] = (index < N) ? in[index] : 0;'
- en: __syncthreads();
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: scanBlock<T,bZeroPad>( sPartials+sIndex );
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: scanBlock<T,bZeroPad>( sPartials+sIndex );
- en: __syncthreads();
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: if ( index < N ) {
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: if ( index < N ) {
- en: out[index] = sPartials[sIndex]+base_sum;
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
  zh: out[index] = sPartials[sIndex]+base_sum;
- en: '}'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: // carry forward from this block to the next.
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: // 从当前块传递到下一个块。
- en: base_sum += sPartials[
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: base_sum += sPartials[
- en: scanSharedIndex<bZeroPad>( blockDim.x-1 ) ];
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
  zh: scanSharedIndex<bZeroPad>( blockDim.x-1 ) ];
- en: __syncthreads();
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 13.12](ch13.html#ch13lis12) gives the host code for Merrill’s two-pass
    reduce-then-scan algorithm. Since the number of partials computed is small and
    never varies, the host code never has to allocate global memory in order to perform
    the scan; instead, we declare a `__device__` array that is allocated at module
    load time'
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.12](ch13.html#ch13lis12) 提供了Merrill的两遍归约-扫描算法的主机代码。由于计算的部分数量较少且始终不变，主机代码不需要分配全局内存来执行扫描；相反，我们声明了一个`__device__`数组，该数组在模块加载时分配。'
- en: __device__ int g_globalPartials[MAX_PARTIALS];
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ int g_globalPartials[MAX_PARTIALS];
- en: and obtain its address by calling `cudaGetSymbolAddress()`.
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: 并通过调用`cudaGetSymbolAddress()`来获取其地址。
- en: status = cudaGetSymbolAddress(
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
  zh: status = cudaGetSymbolAddress(
- en: (void **) &globalPartials,
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: (void **) &globalPartials,
- en: g_globalPartials );
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: g_globalPartials );
- en: The routine then computes the number of elements per partial and number of threadblocks
    to use and invokes the three (3) kernels needed to perform the computation.
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
  zh: 该程序接着计算每个部分的元素数量和使用的线程块数量，并调用执行计算所需的三个（3）内核。
- en: '*Listing 13.12.* scan2Level.'
  id: totrans-1317
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.12.* scan2Level。'
- en: '[Click here to view code image](ch13_images.html#p13lis12a)'
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch13_images.html#p13lis12a)'
- en: '* * *'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, bool bZeroPad>
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
  zh: 模板<class T, bool bZeroPad>
- en: void
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: scan2Level( T *out, const T *in, size_t N, int b )
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: scan2Level( T *输出, const T *输入, size_t N, int b )
- en: '{'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int sBytes = scanSharedMemory<T,bZeroPad>( b );
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
  zh: int sBytes = scanSharedMemory<T,bZeroPad>( b );
- en: if ( N <= b ) {
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( N <= b ) {
- en: return scan2Level_kernel<T, bZeroPad><<<1,b,sBytes>>>(
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
  zh: 返回 scan2Level_kernel<T, bZeroPad><<<1,b,sBytes>>>(
- en: out, 0, in, N, N );
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，0，输入，N，N );
- en: '}'
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: cudaError_t status;
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t 状态；
- en: T *gPartials = 0;
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
  zh: T *gPartials = 0；
- en: status = cudaGetSymbolAddress(
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 = cudaGetSymbolAddress(
- en: (void **) &gPartials,
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
  zh: (void **) &gPartials，
- en: g_globalPartials );
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: g_globalPartials );
- en: if ( cudaSuccess ==  status )
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( cudaSuccess == 状态 )
- en: '{'
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: //
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // ceil(N/b) = number of partial sums to compute
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: // ceil(N/b) = 计算要计算的部分和的数量
- en: //
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: size_t numPartials = (N+b-1)/b;
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
  zh: size_t numPartials = (N+b-1)/b；
- en: if ( numPartials > MAX_PARTIALS ) {
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( numPartials > MAX_PARTIALS ) {
- en: numPartials = MAX_PARTIALS;
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
  zh: numPartials = MAX_PARTIALS;
- en: '}'
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: //
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // elementsPerPartial has to be a multiple of b
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: // elementsPerPartial 必须是 b 的倍数
- en: //
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: unsigned int elementsPerPartial =
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int elementsPerPartial =
- en: (N+numPartials-1)/numPartials;
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: (N+numPartials-1)/numPartials;
- en: elementsPerPartial = b * ((elementsPerPartial+b-1)/b);
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: elementsPerPartial = b * ((elementsPerPartial+b-1)/b);
- en: numPartials = (N+elementsPerPartial-1)/elementsPerPartial;
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: numPartials = (N+elementsPerPartial-1)/elementsPerPartial；
- en: //
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: // number of CUDA threadblocks to use.  The kernels are
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: // 要使用的 CUDA 线程块数量。内核是
- en: // blocking agnostic, so we can clamp to any number within
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
  zh: // 与阻塞无关，因此我们可以限制为任意数量的线程块
- en: // CUDA's limits and the code will work.
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
  zh: // CUDA 的限制，代码会正常工作。
- en: //
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: //
- en: const unsigned int maxBlocks = MAX_PARTIALS;
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
  zh: const unsigned int maxBlocks = MAX_PARTIALS;
- en: unsigned int numBlocks = min( numPartials, maxBlocks );
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned int numBlocks = min( numPartials, maxBlocks );
- en: scanReduceSubarrays<T>(
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: scanReduceSubarrays<T>(
- en: gPartials,
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
  zh: gPartials，
- en: in,
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: 输入，
- en: N,
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: N，
- en: elementsPerPartial,
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
  zh: elementsPerPartial，
- en: numBlocks,
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
  zh: numBlocks，
- en: b );
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: b );
- en: scan2Level_kernel<T, bZeroPad><<<1,b,sBytes>>>(
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
  zh: scan2Level_kernel<T, bZeroPad><<<1,b,sBytes>>>(
- en: gPartials,
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: gPartials，
- en: 0,
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
  zh: 0，
- en: gPartials,
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
  zh: gPartials，
- en: numPartials,
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
  zh: numPartials，
- en: numPartials );
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: numPartials );
- en: scan2Level_kernel<T, bZeroPad><<<numBlocks,b,sBytes>>>(
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: scan2Level_kernel<T, bZeroPad><<<numBlocks,b,sBytes>>>(
- en: out,
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: 输出，
- en: gPartials,
  id: totrans-1372
  prefs: []
  type: TYPE_NORMAL
  zh: gPartials，
- en: in,
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
  zh: 输入，
- en: N,
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
  zh: N，
- en: elementsPerPartial );
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
  zh: elementsPerPartial );
- en: '}'
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 13.5\. Warp Scans
  id: totrans-1379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.5\. Warp 扫描
- en: So far, we’ve focused on constructing our Scan implementations from the top
    down. At the bottom of all three of our Scan implementations, however, lurks an
    entirely different software approach to Scan. For subarrays of size 32 or less,
    we use a special *warp scan* modeled on the Kogge-Stone circuit ([Figure 13.11](ch13.html#ch13fig11)).
    Kogge-Stone circuits are *work-inefficient*, meaning they perform many operations
    despite their small depth, but at the warp level, where execution resources of
    CUDA hardware are available whether or not the developer uses them, Kogge-Stone
    works well on CUDA hardware.
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于从上到下构建我们的扫描实现。然而，在我们所有三种扫描实现的底部，隐藏着一种完全不同的软件方法。对于大小为 32 或更小的子数组，我们使用一种特殊的
    *warp 扫描*，该方法基于 Kogge-Stone 电路（[图 13.11](ch13.html#ch13fig11)）。Kogge-Stone 电路是
    *工作低效* 的，意味着它们尽管深度较小，但会执行许多操作，但在 warp 级别，CUDA 硬件的执行资源无论开发者是否使用它们，都可用，因此 Kogge-Stone
    在 CUDA 硬件上表现良好。
- en: '[Listing 13.13](ch13.html#ch13lis13) gives a `__device__` routine that is designed
    to operate on shared memory, the fastest way for threads to exchange data with
    one another. Because there are no shared memory conflicts and the routine executes
    at warp granularity, no thread synchronization is needed during updates to the
    shared memory.'
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.13](ch13.html#ch13lis13) 给出了一个 `__device__` 例程，旨在操作共享内存，这是线程之间交换数据的最快方式。由于没有共享内存冲突，且该例程在
    warp 粒度下执行，因此在更新共享内存时无需线程同步。'
- en: '*Listing 13.13.* scanWarp.'
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.13.* scanWarp.'
- en: '[Click here to view code image](ch13_images.html#p13lis13a)'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch13_images.html#p13lis13a)'
- en: '* * *'
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-1385
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: inline __device__ T
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: 内联 __device__ T
- en: scanWarp( volatile T *sPartials )
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
  zh: scanWarp( volatile T *sPartials )
- en: '{'
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const int tid = threadIdx.x;
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: const int lane = tid & 31;
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
  zh: const int lane = tid & 31;
- en: if ( lane >=  1 ) sPartials[0] += sPartials[- 1];
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( lane >= 1 ) sPartials[0] += sPartials[-1];
- en: if ( lane >=  2 ) sPartials[0] += sPartials[- 2];
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( lane >= 2 ) sPartials[0] += sPartials[-2];
- en: if ( lane >=  4 ) sPartials[0] += sPartials[- 4];
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( lane >= 4 ) sPartials[0] += sPartials[-4];
- en: if ( lane >=  8 ) sPartials[0] += sPartials[- 8];
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( lane >= 8 ) sPartials[0] += sPartials[-8];
- en: if ( lane >= 16 ) sPartials[0] += sPartials[-16];
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( lane >= 16 ) sPartials[0] += sPartials[-16];
- en: return sPartials[0];
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: return sPartials[0];
- en: '}'
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 13.5.1\. Zero Padding
  id: totrans-1399
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.5.1\. 零填充
- en: We can reduce the number of machine instructions needed to implement the warp
    scan by interleaving the warps’ data with 16-element arrays of 0’s, enabling the
    conditionals to be removed. [Listing 13.14](ch13.html#ch13lis14) gives a version
    of `scanWarp` that assumes 16 zero elements preceding the base address in shared
    memory.
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将 warp 的数据与 16 元素的 0 数组交错，减少实现 warp 扫描所需的机器指令，从而移除条件语句。[列表 13.14](ch13.html#ch13lis14)
    给出了一个假设共享内存中基址前有 16 个零元素的 `scanWarp` 版本。
- en: '*Listing 13.14.* scanWarp0.'
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.14.* scanWarp0.'
- en: '[Click here to view code image](ch13_images.html#p13lis14a)'
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch13_images.html#p13lis14a)'
- en: '* * *'
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: __device__ T scanWarp0( volatile T *sharedPartials, int idx )
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ T scanWarp0( volatile T *sharedPartials, int idx )
- en: '{'
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const int tid = threadIdx.x;
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: const int lane = tid & 31;
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: const int lane = tid & 31;
- en: sharedPartials[idx] += sharedPartials[idx -  1];
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
  zh: sharedPartials[idx] += sharedPartials[idx - 1];
- en: sharedPartials[idx] += sharedPartials[idx -  2];
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: sharedPartials[idx] += sharedPartials[idx - 2];
- en: sharedPartials[idx] += sharedPartials[idx -  4];
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
  zh: sharedPartials[idx] += sharedPartials[idx - 4];
- en: sharedPartials[idx] += sharedPartials[idx -  8];
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
  zh: sharedPartials[idx] += sharedPartials[idx - 8];
- en: sharedPartials[idx] += sharedPartials[idx - 16];
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
  zh: sharedPartials[idx] += sharedPartials[idx - 16];
- en: return sharedPartials[idx];
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
  zh: return sharedPartials[idx];
- en: '}'
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Figure 13.15](ch13.html#ch13fig15) shows how the interleaving works for a
    256-thread block, which contains 8 warps. The shared memory index is computed
    as follows.'
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13.15](ch13.html#ch13fig15) 显示了 256 线程块的交错工作方式，该块包含 8 个 warp。共享内存索引按如下方式计算。'
- en: '[Click here to view code image](ch13_images.html#p408pro01a)'
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch13_images.html#p408pro01a)'
- en: const int tid = threadIdx.x;
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: const int warp = tid >> 5;
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
  zh: const int warp = tid >> 5;
- en: const int lane = tid & 31;
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
  zh: const int lane = tid & 31;
- en: const int sharedIndex = 49 * warp + 32 + lane;
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
  zh: const int sharedIndex = 49 * warp + 32 + lane;
- en: '![Image](graphics/13fig15.jpg)'
  id: totrans-1423
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/13fig15.jpg)'
- en: '*Figure 13.15* Interleaved zeros for warp scan.'
  id: totrans-1424
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.15* 交错零填充用于 warp 扫描。'
- en: The initialization to 0 is then done as follows.
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化为 0 如下所示。
- en: partials[sharedIndex-16] = 0;
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
  zh: partials[sharedIndex-16] = 0;
- en: The other area where this change affects the shared memory addressing is in
    the block scan subroutine. The index for each partial sum for each warp must be
    offset by 16 to enable the single warp scan that computes the base sums to work.
    Finally, the kernel invocation must reserve enough shared memory to hold both
    the partial sums and the zeros.
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个受此更改影响的共享内存寻址区域是块扫描子程序。每个 warp 的每个部分和的索引必须偏移 16，以使计算基本和的单个 warp 扫描能够工作。最后，内核调用必须预留足够的共享内存，以容纳部分和与零填充。
- en: 13.5.2\. Templated Formulations
  id: totrans-1428
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.5.2\. 模板化公式
- en: The faster, zero-padded implementation of Scan requires more shared memory,
    a resource requirement that not all applications can accommodate. To enable our
    code to support both versions, [Listing 13.15](ch13.html#ch13lis15) shows utility
    functions that take a `bool` template parameter `bZeroPad`. The `scanSharedMemory`
    function returns the amount of shared memory needed for a given block size. `scanSharedIndex`
    returns the shared memory index corresponding to a given thread. In turn, [Listing
    13.16](ch13.html#ch13lis16) gives the templated version of `scanWarp` that works
    for both the zero-padded and non-zero-padded cases.
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: 更快的零填充扫描实现需要更多的共享内存，这是并非所有应用程序都能满足的资源要求。为了使我们的代码支持这两种版本，[列表 13.15](ch13.html#ch13lis15)
    显示了一个包含 `bool` 模板参数 `bZeroPad` 的实用函数。`scanSharedMemory` 函数返回给定块大小所需的共享内存量。`scanSharedIndex`
    返回与给定线程对应的共享内存索引。接着，[列表 13.16](ch13.html#ch13lis16) 给出了适用于零填充和非零填充情况的 `scanWarp`
    模板版本。
- en: '*Listing 13.15.* Shared memory utilities for zero padding.'
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.15.* 零填充的共享内存实用工具。'
- en: '[Click here to view code image](ch13_images.html#p13lis15a)'
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch13_images.html#p13lis15a)'
- en: '* * *'
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<bool bZeroPad>
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: template<bool bZeroPad>
- en: inline __device__ int
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ int
- en: scanSharedIndex( int tid )
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: scanSharedIndex( int tid )
- en: '{'
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: if ( bZeroPad ) {
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: if ( bZeroPad ) {
- en: const int warp = tid >> 5;
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
  zh: const int warp = tid >> 5;
- en: const int lane = tid & 31;
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
  zh: const int lane = tid & 31;
- en: return 49 * warp + 16 + lane;
  id: totrans-1440
  prefs: []
  type: TYPE_NORMAL
  zh: return 49 * warp + 16 + lane;
- en: '}'
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else {
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: else {
- en: return tid;
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
  zh: return tid;
- en: '}'
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: template<typename T, bool bZeroPad>
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
  zh: template<typename T, bool bZeroPad>
- en: inline __device__ __host__ int
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ __host__ int
- en: scanSharedMemory( int numThreads )
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
  zh: scanSharedMemory( int numThreads )
- en: '{'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: if ( bZeroPad ) {
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
  zh: if ( bZeroPad ) {
- en: const int warpcount = numThreads>>5;
  id: totrans-1451
  prefs: []
  type: TYPE_NORMAL
  zh: const int warpcount = numThreads>>5;
- en: return (49 * warpcount + 16)*sizeof(T);
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
  zh: return (49 * warpcount + 16)*sizeof(T);
- en: '}'
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else {
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: else {
- en: return numThreads*sizeof(T);
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: return numThreads*sizeof(T);
- en: '}'
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 13.16.* scanWarp (templated).'
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 13.16.* scanWarp (模板化)。'
- en: '[Click here to view code image](ch13_images.html#p13lis16a)'
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch13_images.html#p13lis16a)'
- en: '* * *'
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, bool bZeroPadded>
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, bool bZeroPadded>
- en: inline __device__ T
  id: totrans-1463
  prefs: []
  type: TYPE_NORMAL
  zh: inline __device__ T
- en: scanWarp( volatile T *sPartials )
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
  zh: scanWarp( volatile T *sPartials )
- en: '{'
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: T t = sPartials[0];
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
  zh: T t = sPartials[0];
- en: if ( bZeroPadded ) {
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: if ( bZeroPadded ) {
- en: t += sPartials[- 1]; sPartials[0] = t;
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
  zh: t += sPartials[- 1]; sPartials[0] = t;
- en: t += sPartials[- 2]; sPartials[0] = t;
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
  zh: t += sPartials[- 2]; sPartials[0] = t;
- en: t += sPartials[- 4]; sPartials[0] = t;
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
  zh: t += sPartials[- 4]; sPartials[0] = t;
- en: t += sPartials[- 8]; sPartials[0] = t;
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: t += sPartials[- 8]; sPartials[0] = t;
- en: t += sPartials[-16]; sPartials[0] = t;
  id: totrans-1472
  prefs: []
  type: TYPE_NORMAL
  zh: t += sPartials[-16]; sPartials[0] = t;
- en: '}'
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else {
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: else {
- en: const int tid = threadIdx.x;
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: const int lane = tid & 31;
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: const int lane = tid & 31;
- en: if ( lane >=  1 ) { t += sPartials[- 1]; sPartials[0] = t; }
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: if ( lane >=  1 ) { t += sPartials[- 1]; sPartials[0] = t; }
- en: if ( lane >=  2 ) { t += sPartials[- 2]; sPartials[0] = t; }
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
  zh: if ( lane >=  2 ) { t += sPartials[- 2]; sPartials[0] = t; }
- en: if ( lane >=  4 ) { t += sPartials[- 4]; sPartials[0] = t; }
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: if ( lane >=  4 ) { t += sPartials[- 4]; sPartials[0] = t; }
- en: if ( lane >=  8 ) { t += sPartials[- 8]; sPartials[0] = t; }
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: if ( lane >=  8 ) { t += sPartials[- 8]; sPartials[0] = t; }
- en: if ( lane >= 16 ) { t += sPartials[-16]; sPartials[0] = t; }
  id: totrans-1481
  prefs: []
  type: TYPE_NORMAL
  zh: if ( lane >= 16 ) { t += sPartials[-16]; sPartials[0] = t; }
- en: '}'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return t;
  id: totrans-1483
  prefs: []
  type: TYPE_NORMAL
  zh: return t;
- en: '}'
  id: totrans-1484
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1485
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 13.5.3\. Warp Shuffle
  id: totrans-1486
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.5.3\. Warp Shuffle
- en: The SM 3.0 instruction set added the warp shuffle instruction, which enables
    registers to be exchanged within the 32 threads of a warp. The “up” and “down”
    variants of the warp shuffle can be used to implement scan and reverse scan, respectively.
    The shuffle instruction takes a register to exchange and an offset to apply to
    the lane ID. It returns a predicate that is false for inactive threads or threads
    whose offset is outside the warp.
  id: totrans-1487
  prefs: []
  type: TYPE_NORMAL
  zh: SM 3.0 指令集添加了 warp shuffle 指令，允许在一个 warp 的 32 个线程之间交换寄存器。warp shuffle 的“向上”和“向下”变种可以分别用于实现扫描和逆向扫描。shuffle
    指令需要一个寄存器进行交换，并且应用一个偏移量到 lane ID。它返回一个谓词，对于非活动线程或偏移量超出 warp 的线程，该谓词为 false。
- en: '[Listing 13.17](ch13.html#ch13lis17) gives `scanWarpShuffle`, a device function
    that implements an inclusive warp scan with the shuffle instruction. The template
    parameter is an integer, and typically the value 5 is passed because 5 is the
    base 2 logarithm of the warp size of 32\. `scanWarpShuffle` uses a utility function
    `scanWarpShuffle_step`, implemented in inline PTX, because the compiler does not
    emit efficient code to deal with the predicate returned by the shuffle instruction.'
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.17](ch13.html#ch13lis17) 给出了 `scanWarpShuffle`，一个实现包括 warp 扫描的设备函数，使用
    shuffle 指令。模板参数是一个整数，通常传递值 5，因为 5 是 warp 大小 32 的二进制对数。`scanWarpShuffle` 使用一个实用函数
    `scanWarpShuffle_step`，该函数在内联 PTX 中实现，因为编译器不会生成高效的代码来处理 shuffle 指令返回的谓词。'
- en: '*Listing 13.17.* scanWarpShuffle device function.'
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.17.* scanWarpShuffle 设备函数。'
- en: '[Click here to view code image](ch13_images.html#p13lis17a)'
  id: totrans-1490
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch13_images.html#p13lis17a)'
- en: '* * *'
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __device__ __forceinline__
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ __forceinline__
- en: int
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
  zh: int
- en: scanWarpShuffle_step(int partial, int offset)
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: scanWarpShuffle_step(int partial, int offset)
- en: '{'
  id: totrans-1495
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int result;
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
  zh: int result;
- en: asm(
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
  zh: asm(
- en: '"{.reg .u32 r0;"'
  id: totrans-1498
  prefs: []
  type: TYPE_NORMAL
  zh: '"{.reg .u32 r0;"'
- en: '".reg .pred p;"'
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
  zh: '".reg .pred p;"'
- en: '"shfl.up.b32 r0|p, %1, %2, 0;"'
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: '"shfl.up.b32 r0|p, %1, %2, 0;"'
- en: '"@p add.u32 r0, r0, %3;"'
  id: totrans-1501
  prefs: []
  type: TYPE_NORMAL
  zh: '"@p add.u32 r0, r0, %3;"'
- en: '"mov.u32 %0, r0;}"'
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
  zh: '"mov.u32 %0, r0;}"'
- en: ': "=r"(result) : "r"(partial), "r"(offset), "r"(partial));'
  id: totrans-1503
  prefs: []
  type: TYPE_NORMAL
  zh: ': "=r"(result) : "r"(partial), "r"(offset), "r"(partial));'
- en: return result;
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
  zh: return result;
- en: '}'
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: template <int levels>
  id: totrans-1506
  prefs: []
  type: TYPE_NORMAL
  zh: template <int levels>
- en: __device__ __forceinline__
  id: totrans-1507
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ __forceinline__
- en: int
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
  zh: int
- en: scanWarpShuffle(int mysum)
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: scanWarpShuffle(int mysum)
- en: '{'
  id: totrans-1510
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for(int i = 0; i < levels; ++i)
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
  zh: for(int i = 0; i < levels; ++i)
- en: mysum = scanWarpShuffle_step(mysum, 1 << i);
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
  zh: mysum = scanWarpShuffle_step(mysum, 1 << i);
- en: return mysum;
  id: totrans-1513
  prefs: []
  type: TYPE_NORMAL
  zh: return mysum;
- en: '}'
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 13.18](ch13.html#ch13lis18) illustrates how to extend `scanWarpShuffle`
    to scan the values across a thread block using shared memory. Following the same
    pattern as the block scan in [Listing 13.3](ch13.html#ch13lis03), `scanBlockShuffle`
    uses the warp shuffle to scan each warp. Each warp writes its partial sum to shared
    memory, and then the warp shuffle is used again, this time by a single warp, to
    scan these base sums. Finally, each warp adds its corresponding base sum to compute
    the final output value.'
  id: totrans-1516
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 13.18](ch13.html#ch13lis18) 说明了如何扩展 `scanWarpShuffle`，通过共享内存扫描线程块中的值。按照与
    [列表 13.3](ch13.html#ch13lis03) 中块扫描相同的模式，`scanBlockShuffle` 使用 warp shuffle 来扫描每个
    warp。每个 warp 将其部分和写入共享内存，然后再次使用 warp shuffle，这次由单个 warp 执行，扫描这些基础和。最后，每个 warp
    将其对应的基础和相加，计算最终输出值。'
- en: '*Listing 13.18.* scanBlockShuffle device function.'
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.18.* scanBlockShuffle 设备函数。'
- en: '[Click here to view code image](ch13_images.html#p13lis18a)'
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch13_images.html#p13lis18a)'
- en: '* * *'
  id: totrans-1519
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template <int logBlockSize>
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
  zh: template <int logBlockSize>
- en: __device__
  id: totrans-1521
  prefs: []
  type: TYPE_NORMAL
  zh: __device__
- en: int
  id: totrans-1522
  prefs: []
  type: TYPE_NORMAL
  zh: int
- en: scanBlockShuffle(int val, const unsigned int idx)
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
  zh: scanBlockShuffle(int val, const unsigned int idx)
- en: '{'
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const unsigned int lane   = idx & 31;
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
  zh: const unsigned int lane   = idx & 31;
- en: const unsigned int warpid = idx >> 5;
  id: totrans-1526
  prefs: []
  type: TYPE_NORMAL
  zh: const unsigned int warpid = idx >> 5;
- en: __shared__ int sPartials[32];
  id: totrans-1527
  prefs: []
  type: TYPE_NORMAL
  zh: __shared__ int sPartials[32];
- en: // Intra-warp scan in each warp
  id: totrans-1528
  prefs: []
  type: TYPE_NORMAL
  zh: // 每个 warp 内部的扫描
- en: val = scanWarpShuffle<5>(val);
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
  zh: val = scanWarpShuffle<5>(val);
- en: // Collect per-warp results
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
  zh: // 收集每个 warp 的结果
- en: if (lane == 31) sPartials[warpid] = val;
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 lane == 31，则 sPartials[warpid] = val;
- en: __syncthreads();
  id: totrans-1532
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: // Use first warp to scan per-warp results
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
  zh: // 使用第一个 warp 扫描每个 warp 的结果
- en: if (warpid == 0) {
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 warpid == 0 {
- en: int t = sPartials[lane];
  id: totrans-1535
  prefs: []
  type: TYPE_NORMAL
  zh: int t = sPartials[lane];
- en: t = scanWarpShuffle<logBlockSize-5>( t );
  id: totrans-1536
  prefs: []
  type: TYPE_NORMAL
  zh: t = scanWarpShuffle<logBlockSize-5>( t );
- en: sPartials[lane] = t;
  id: totrans-1537
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[lane] = t;
- en: '}'
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-1539
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: // Add scanned base sum for final result
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
  zh: // 将扫描的基准和加到最终结果中
- en: if (warpid > 0) {
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 warpid > 0 {
- en: val += sPartials[warpid - 1];
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
  zh: val += sPartials[warpid - 1];
- en: '}'
  id: totrans-1543
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: return val;
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
  zh: return val;
- en: '}'
  id: totrans-1545
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1546
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 13.5.4\. Instruction Counts
  id: totrans-1547
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 13.5.4\. 指令计数
- en: To examine the tradeoffs between the different variations of warp scan discussed
    in this section, we compiled for SM 3.0 and used `cuobjdump` to disassemble the
    three implementations.
  id: totrans-1548
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检验本节讨论的不同 warp 扫描变种之间的权衡，我们针对 SM 3.0 进行了编译，并使用 `cuobjdump` 来反汇编这三种实现。
- en: • The non-zero-padded implementation given in [Listing 13.19](ch13.html#ch13lis19)
    is 30 instructions and includes a great deal of branching (the `SSY/.S` instruction
    pairs push and pop the divergence stack, as described in [Section 8.4.2](ch08.html#ch08lev2sec18)).
  id: totrans-1549
  prefs: []
  type: TYPE_NORMAL
  zh: • 在[列表 13.19](ch13.html#ch13lis19)中给出的非零填充实现包含30条指令，并且包含大量的分支（`SSY/.S` 指令对用于推送和弹出分支栈，详见[第8.4.2节](ch08.html#ch08lev2sec18)）。
- en: • The zero-padded implementation given in [Listing 13.20](ch13.html#ch13lis20)
    is 17 instructions because it does not check the lane ID before performing its
    shared memory reads. Note that because the shared memory operations are guaranteed
    to be contained within a warp, there is no need for barrier synchronization via
    the `__syncthreads()` intrinsic, which compiles to `BAR.SYNC` instructions in
    SASS.
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
  zh: • 在[列表 13.20](ch13.html#ch13lis20)中给出的零填充实现包含17条指令，因为它在执行共享内存读取之前没有检查 lane ID。请注意，由于共享内存操作保证是在同一个
    warp 内进行，因此无需通过 `__syncthreads()` 内建函数进行屏障同步，该函数在 SASS 中编译为 `BAR.SYNC` 指令。
- en: • The shuffle-based implementation given in [Listing 13.21](ch13.html#ch13lis21)
    is only 11 instructions.
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
  zh: • 在[列表 13.21](ch13.html#ch13lis21)中给出的基于 shuffle 的实现只有11条指令。
- en: We confirmed that the shuffle-based implementation is, in fact, significantly
    faster (about 2x) than the general case given in [Listing 13.19](ch13.html#ch13lis19),
    running on a synthetic workload that isolates the warp scan.
  id: totrans-1552
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确认基于 shuffle 的实现实际上显著更快（大约 2 倍）与[列表 13.19](ch13.html#ch13lis19)中给出的通用实现，在一个隔离
    warp 扫描的合成工作负载下运行。
- en: '*Listing 13.19.* SASS for warp scan (no zero padding).'
  id: totrans-1553
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.19.* 适用于 warp 扫描的 SASS（无零填充）。'
- en: '[Click here to view code image](ch13_images.html#p13lis19a)'
  id: totrans-1554
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch13_images.html#p13lis19a)'
- en: '* * *'
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: /*0070*/     SSY 0xa0;
  id: totrans-1556
  prefs: []
  type: TYPE_NORMAL
  zh: /*0070*/     SSY 0xa0;
- en: /*0078*/     @P0 NOP.S CC.T;
  id: totrans-1557
  prefs: []
  type: TYPE_NORMAL
  zh: /*0078*/     @P0 NOP.S CC.T;
- en: /*0088*/     LDS R5, [R3+-0x4];
  id: totrans-1558
  prefs: []
  type: TYPE_NORMAL
  zh: /*0088*/     LDS R5, [R3+-0x4];
- en: /*0090*/     IADD R0, R5, R0;
  id: totrans-1559
  prefs: []
  type: TYPE_NORMAL
  zh: /*0090*/     IADD R0, R5, R0;
- en: /*0098*/     STS.S [R3], R0;
  id: totrans-1560
  prefs: []
  type: TYPE_NORMAL
  zh: /*0098*/     STS.S [R3], R0;
- en: /*00a0*/     ISETP.LT.U32.AND P0, pt, R4, 0x2, pt;
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
  zh: /*00a0*/     ISETP.LT.U32.AND P0, pt, R4, 0x2, pt;
- en: /*00a8*/     SSY 0xd8;
  id: totrans-1562
  prefs: []
  type: TYPE_NORMAL
  zh: /*00a8*/     SSY 0xd8;
- en: /*00b0*/     @P0 NOP.S CC.T;
  id: totrans-1563
  prefs: []
  type: TYPE_NORMAL
  zh: /*00b0*/     @P0 NOP.S CC.T;
- en: /*00b8*/     LDS R5, [R3+-0x8];
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
  zh: /*00b8*/     LDS R5, [R3+-0x8];
- en: /*00c8*/     IADD R0, R5, R0;
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
  zh: /*00c8*/     IADD R0, R5, R0;
- en: /*00d0*/     STS.S [R3], R0;
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
  zh: /*00d0*/     STS.S [R3], R0;
- en: /*00d8*/     ISETP.LT.U32.AND P0, pt, R4, 0x4, pt;
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
  zh: /*00d8*/     ISETP.LT.U32.AND P0, pt, R4, 0x4, pt;
- en: /*00e0*/     SSY 0x110;
  id: totrans-1568
  prefs: []
  type: TYPE_NORMAL
  zh: /*00e0*/     SSY 0x110;
- en: /*00e8*/     @P0 NOP.S CC.T;
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: /*00e8*/     @P0 NOP.S CC.T;
- en: /*00f0*/     LDS R5, [R3+-0x10];
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
  zh: /*00f0*/     LDS R5, [R3+-0x10];
- en: /*00f8*/     IADD R0, R5, R0;
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
  zh: /*00f8*/     IADD R0, R5, R0;
- en: /*0108*/     STS.S [R3], R0;
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
  zh: /*0108*/     STS.S [R3], R0;
- en: /*0110*/     ISETP.LT.U32.AND P0, pt, R4, 0x8, pt;
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
  zh: /*0110*/     ISETP.LT.U32.AND P0, pt, R4, 0x8, pt;
- en: /*0118*/     SSY 0x140;
  id: totrans-1574
  prefs: []
  type: TYPE_NORMAL
  zh: /*0118*/     SSY 0x140;
- en: /*0120*/     @P0 NOP.S CC.T;
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
  zh: /*0120*/     @P0 NOP.S CC.T;
- en: /*0128*/     LDS R5, [R3+-0x20];
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
  zh: /*0128*/     LDS R5, [R3+-0x20];
- en: /*0130*/     IADD R0, R5, R0;
  id: totrans-1577
  prefs: []
  type: TYPE_NORMAL
  zh: /*0130*/     IADD R0, R5, R0;
- en: /*0138*/     STS.S [R3], R0;
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
  zh: /*0138*/     STS.S [R3], R0;
- en: /*0148*/     ISETP.LT.U32.AND P0, pt, R4, 0x10, pt;
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
  zh: /*0148*/     ISETP.LT.U32.AND P0, pt, R4, 0x10, pt;
- en: /*0150*/     SSY 0x178;
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
  zh: /*0150*/     SSY 0x178;
- en: /*0158*/     @P0 NOP.S CC.T;
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
  zh: /*0158*/     @P0 NOP.S CC.T;
- en: /*0160*/     LDS R4, [R3+-0x40];
  id: totrans-1582
  prefs: []
  type: TYPE_NORMAL
  zh: /*0160*/     LDS R4, [R3+-0x40];
- en: /*0168*/     IADD R0, R4, R0;
  id: totrans-1583
  prefs: []
  type: TYPE_NORMAL
  zh: /*0168*/     IADD R0, R4, R0;
- en: /*0170*/     STS.S [R3], R0;
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
  zh: /*0170*/     STS.S [R3], R0;
- en: /*0178*/     BAR.SYNC 0x0;
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
  zh: /*0178*/     BAR.SYNC 0x0;
- en: '* * *'
  id: totrans-1586
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 13.20.* SASS for warp scan (with zero padding).'
  id: totrans-1587
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.20.* 用于波束扫描的 SASS（带零填充）。'
- en: '[Click here to view code image](ch13_images.html#p13lis20a)'
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch13_images.html#p13lis20a)'
- en: '* * *'
  id: totrans-1589
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: /*0058*/     LDS R4, [R3+-0x4];
  id: totrans-1590
  prefs: []
  type: TYPE_NORMAL
  zh: /*0058*/     LDS R4, [R3+-0x4];
- en: /*0060*/     LDS R0, [R3];
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
  zh: /*0060*/     LDS R0, [R3];
- en: /*0068*/     IADD R4, R4, R0;
  id: totrans-1592
  prefs: []
  type: TYPE_NORMAL
  zh: /*0068*/     IADD R4, R4, R0;
- en: /*0070*/     STS [R3], R4;
  id: totrans-1593
  prefs: []
  type: TYPE_NORMAL
  zh: /*0070*/     STS [R3], R4;
- en: /*0078*/     LDS R0, [R3+-0x8];
  id: totrans-1594
  prefs: []
  type: TYPE_NORMAL
  zh: /*0078*/     LDS R0, [R3+-0x8];
- en: /*0088*/     IADD R4, R4, R0;
  id: totrans-1595
  prefs: []
  type: TYPE_NORMAL
  zh: /*0088*/     IADD R4, R4, R0;
- en: /*0090*/     STS [R3], R4;
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
  zh: /*0090*/     STS [R3], R4;
- en: /*0098*/     LDS R0, [R3+-0x10];
  id: totrans-1597
  prefs: []
  type: TYPE_NORMAL
  zh: /*0098*/     LDS R0, [R3+-0x10];
- en: /*00a0*/     IADD R4, R4, R0;
  id: totrans-1598
  prefs: []
  type: TYPE_NORMAL
  zh: /*00a0*/     IADD R4, R4, R0;
- en: /*00a8*/     STS [R3], R4;
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
  zh: /*00a8*/     STS [R3], R4;
- en: /*00b0*/     LDS R0, [R3+-0x20];
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
  zh: /*00b0*/     LDS R0, [R3+-0x20];
- en: /*00b8*/     IADD R4, R4, R0;
  id: totrans-1601
  prefs: []
  type: TYPE_NORMAL
  zh: /*00b8*/     IADD R4, R4, R0;
- en: /*00c8*/     STS [R3], R4;
  id: totrans-1602
  prefs: []
  type: TYPE_NORMAL
  zh: /*00c8*/     STS [R3], R4;
- en: /*00d0*/     LDS R0, [R3+-0x40];
  id: totrans-1603
  prefs: []
  type: TYPE_NORMAL
  zh: /*00d0*/     LDS R0, [R3+-0x40];
- en: /*00d8*/     IADD R0, R4, R0;
  id: totrans-1604
  prefs: []
  type: TYPE_NORMAL
  zh: /*00d8*/     IADD R0, R4, R0;
- en: /*00e0*/     STS [R3], R0;
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
  zh: /*00e0*/     STS [R3], R0;
- en: /*00e8*/     BAR.SYNC 0x0;
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
  zh: /*00e8*/     BAR.SYNC 0x0;
- en: '* * *'
  id: totrans-1607
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '*Listing 13.21.* SASS for warp scan (using shuffle).'
  id: totrans-1608
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 13.21.* 用于波束扫描的 SASS（使用洗牌）。'
- en: '[Click here to view code image](ch13_images.html#p13lis21a)'
  id: totrans-1609
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch13_images.html#p13lis21a)'
- en: '* * *'
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: /*0050*/     SHFL.UP P0, R4, R0, 0x1, 0x0;
  id: totrans-1611
  prefs: []
  type: TYPE_NORMAL
  zh: /*0050*/     SHFL.UP P0, R4, R0, 0x1, 0x0;
- en: /*0058*/     IADD.X R3, R3, c [0x0] [0x144];
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
  zh: /*0058*/     IADD.X R3, R3, c [0x0] [0x144];
- en: /*0060*/     @P0 IADD R4, R4, R0;
  id: totrans-1613
  prefs: []
  type: TYPE_NORMAL
  zh: /*0060*/     @P0 IADD R4, R4, R0;
- en: /*0068*/     SHFL.UP P0, R0, R4, 0x2, 0x0;
  id: totrans-1614
  prefs: []
  type: TYPE_NORMAL
  zh: /*0068*/     SHFL.UP P0, R0, R4, 0x2, 0x0;
- en: /*0070*/     @P0 IADD R0, R0, R4;
  id: totrans-1615
  prefs: []
  type: TYPE_NORMAL
  zh: /*0070*/     @P0 IADD R0, R0, R4;
- en: /*0078*/     SHFL.UP P0, R4, R0, 0x4, 0x0;
  id: totrans-1616
  prefs: []
  type: TYPE_NORMAL
  zh: /*0078*/     SHFL.UP P0, R4, R0, 0x4, 0x0;
- en: /*0088*/     @P0 IADD R4, R4, R0;
  id: totrans-1617
  prefs: []
  type: TYPE_NORMAL
  zh: /*0088*/     @P0 IADD R4, R4, R0;
- en: /*0090*/     SHFL.UP P0, R0, R4, 0x8, 0x0;
  id: totrans-1618
  prefs: []
  type: TYPE_NORMAL
  zh: /*0090*/     SHFL.UP P0, R0, R4, 0x8, 0x0;
- en: /*0098*/     @P0 IADD R0, R0, R4;
  id: totrans-1619
  prefs: []
  type: TYPE_NORMAL
  zh: /*0098*/     @P0 IADD R0, R0, R4;
- en: /*00a0*/     SHFL.UP P0, R4, R0, 0x10, 0x0;
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
  zh: /*00a0*/     SHFL.UP P0, R4, R0, 0x10, 0x0;
- en: /*00a8*/     @P0 IADD R4, R4, R0;
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
  zh: /*00a8*/     @P0 IADD R4, R4, R0;
- en: '* * *'
  id: totrans-1622
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 13.6\. Stream Compaction
  id: totrans-1623
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.6\. 流压缩
- en: Scan implementations often operate on *predicates*—truth values (0 or 1) computed
    by evaluating a condition. As mentioned at the beginning of the chapter, an exclusive
    scan of predicates can be used to implement *stream compaction*, a class of parallel
    problems where only the “interesting” elements of an input array are written to
    the output. For predicate values where the predicate is equal to 1 for “interesting”
    elements, the exclusive scan computes the output index of the element.
  id: totrans-1624
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描实现通常操作于*谓词*——通过评估条件计算出的真假值（0或1）。如章节开头所提到，谓词的独占扫描可用于实现*流压缩*，这是一类并行问题，在这些问题中，只有输入数组中“有趣”的元素被写入输出。对于谓词值为1的“有趣”元素，独占扫描计算该元素的输出索引。
- en: As an example, let’s write a Scan implementation that operates on an array of
    `int` and emits all `ints` that are odd.^([5](ch13.html#ch13fn5)) Our implementation
    is based on Merrill’s reduce-then-scan with a fixed number of blocks *b.*
  id: totrans-1625
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们编写一个扫描实现，操作一个`int`数组，并输出所有奇数`int`。^([5](ch13.html#ch13fn5)) 我们的实现基于Merrill的先归约后扫描方法，使用固定数量的块*b*。
- en: '[5](ch13.html#ch13fn5a). The code is easily modified to evaluate more complicated
    predicates.'
  id: totrans-1626
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch13.html#ch13fn5a)。该代码可以轻松修改，以评估更复杂的谓词。'
- en: '**1.** A first reduction pass over the input data gives the number of elements
    in each ![Image](graphics/398equ01.jpg) subarray that meets the criteria.'
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
  zh: '**1.** 对输入数据进行第一次归约，得到每个![Image](graphics/398equ01.jpg)子数组中符合标准的元素个数。'
- en: '**2.** A scan is performed on the array of *b* counts, giving the base index
    for the output of each subarray.'
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
  zh: '**2.** 对*b*个计数的数组执行扫描，得到每个子数组输出的基索引。'
- en: '**3.** A scan is performed on the input array, evaluating the criteria and
    using the “seed” value as the base index for each subarray’s output.'
  id: totrans-1629
  prefs: []
  type: TYPE_NORMAL
  zh: '**3.** 对输入数组执行扫描，评估标准，并使用“种子”值作为每个子数组输出的基索引。'
- en: '[Listing 13.22](ch13.html#ch13lis22) shows the code for step 1: `predicateReduceSubarrays_odd()`
    function invokes subroutines `predicateReduceSubarray_odd()` and `isOdd()` to
    evaluate the predicate for each array element, compute the reduction, and write
    it to the array of base sums.'
  id: totrans-1630
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 13.22](ch13.html#ch13lis22) 展示了第1步的代码：`predicateReduceSubarrays_odd()`函数调用子程序`predicateReduceSubarray_odd()`和`isOdd()`，用于评估每个数组元素的谓词，计算归约并将其写入基和数组。'
- en: '*Listing 13.22.* `predicateReduceSubarrays_odd`.'
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 13.22.* `predicateReduceSubarrays_odd`。'
- en: '[Click here to view code image](ch13_images.html#p13lis22a)'
  id: totrans-1632
  prefs: []
  type: TYPE_NORMAL
  zh: '[Click here to view code image](ch13_images.html#p13lis22a)'
- en: '* * *'
  id: totrans-1633
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T>
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T>
- en: __host__ __device__ bool
  id: totrans-1635
  prefs: []
  type: TYPE_NORMAL
  zh: __host__ __device__ bool
- en: isOdd( T x )
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
  zh: isOdd( T x )
- en: '{'
  id: totrans-1637
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: return x & 1;
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
  zh: return x & 1;
- en: '}'
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: template<class T, int numThreads>
  id: totrans-1640
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, int numThreads>
- en: __device__ void
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ void
- en: predicateReduceSubarray_odd(
  id: totrans-1642
  prefs: []
  type: TYPE_NORMAL
  zh: predicateReduceSubarray_odd(
- en: int *gPartials,
  id: totrans-1643
  prefs: []
  type: TYPE_NORMAL
  zh: int *gPartials,
- en: const T *in,
  id: totrans-1644
  prefs: []
  type: TYPE_NORMAL
  zh: const T *in,
- en: size_t iBlock,
  id: totrans-1645
  prefs: []
  type: TYPE_NORMAL
  zh: size_t iBlock,
- en: size_t N,
  id: totrans-1646
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: int elementsPerPartial )
  id: totrans-1647
  prefs: []
  type: TYPE_NORMAL
  zh: int elementsPerPartial )
- en: '{'
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern volatile __shared__ int sPartials[];
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
  zh: extern volatile __shared__ int sPartials[];
- en: const int tid = threadIdx.x;
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: size_t baseIndex = iBlock*elementsPerPartial;
  id: totrans-1651
  prefs: []
  type: TYPE_NORMAL
  zh: size_t baseIndex = iBlock*elementsPerPartial;
- en: int sum = 0;
  id: totrans-1652
  prefs: []
  type: TYPE_NORMAL
  zh: int sum = 0;
- en: for ( int i = tid; i < elementsPerPartial; i += blockDim.x ) {
  id: totrans-1653
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ( int i = tid; i < elementsPerPartial; i += blockDim.x ) {
- en: size_t index = baseIndex+i;
  id: totrans-1654
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = baseIndex+i;
- en: if ( index < N )
  id: totrans-1655
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( index < N )
- en: sum += isOdd( in[index] );
  id: totrans-1656
  prefs: []
  type: TYPE_NORMAL
  zh: sum += isOdd( in[index] );
- en: '}'
  id: totrans-1657
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: sPartials[tid] = sum;
  id: totrans-1658
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[tid] = sum;
- en: __syncthreads();
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: reduceBlock<int,numThreads>( &gPartials[iBlock], sPartials );
  id: totrans-1660
  prefs: []
  type: TYPE_NORMAL
  zh: reduceBlock<int,numThreads>( &gPartials[iBlock], sPartials );
- en: '}'
  id: totrans-1661
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: /*
  id: totrans-1662
  prefs: []
  type: TYPE_NORMAL
  zh: /*
- en: '* Compute the reductions of each subarray of size'
  id: totrans-1663
  prefs: []
  type: TYPE_NORMAL
  zh: '* 计算每个大小为的子数组的归约'
- en: '* elementsPerPartial, and write them to gPartials.'
  id: totrans-1664
  prefs: []
  type: TYPE_NORMAL
  zh: '* elementsPerPartial，并将它们写入 gPartials。'
- en: '*/'
  id: totrans-1665
  prefs: []
  type: TYPE_NORMAL
  zh: '*/'
- en: template<class T, int numThreads>
  id: totrans-1666
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, int numThreads>
- en: __global__ void
  id: totrans-1667
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: predicateReduceSubarrays_odd(
  id: totrans-1668
  prefs: []
  type: TYPE_NORMAL
  zh: predicateReduceSubarrays_odd(
- en: int *gPartials,
  id: totrans-1669
  prefs: []
  type: TYPE_NORMAL
  zh: int *gPartials,
- en: const T *in,
  id: totrans-1670
  prefs: []
  type: TYPE_NORMAL
  zh: const T *in,
- en: size_t N,
  id: totrans-1671
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: int elementsPerPartial )
  id: totrans-1672
  prefs: []
  type: TYPE_NORMAL
  zh: int elementsPerPartial )
- en: '{'
  id: totrans-1673
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern volatile __shared__ int sPartials[];
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
  zh: extern volatile __shared__ int sPartials[];
- en: for ( int iBlock = blockIdx.x;
  id: totrans-1675
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ( int iBlock = blockIdx.x;
- en: iBlock*elementsPerPartial < N;
  id: totrans-1676
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock*elementsPerPartial < N;
- en: iBlock += gridDim.x )
  id: totrans-1677
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock += gridDim.x )
- en: '{'
  id: totrans-1678
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: predicateReduceSubarray_odd<T,numThreads>(
  id: totrans-1679
  prefs: []
  type: TYPE_NORMAL
  zh: predicateReduceSubarray_odd<T,numThreads>(
- en: gPartials,
  id: totrans-1680
  prefs: []
  type: TYPE_NORMAL
  zh: gPartials,
- en: in,
  id: totrans-1681
  prefs: []
  type: TYPE_NORMAL
  zh: in,
- en: iBlock,
  id: totrans-1682
  prefs: []
  type: TYPE_NORMAL
  zh: iBlock,
- en: N,
  id: totrans-1683
  prefs: []
  type: TYPE_NORMAL
  zh: N,
- en: elementsPerPartial );
  id: totrans-1684
  prefs: []
  type: TYPE_NORMAL
  zh: elementsPerPartial );
- en: '}'
  id: totrans-1685
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1686
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Computing the scan of the array of base sums is done by invoking the kernel
    in [Listing 13.23](ch13.html#ch13lis23). Once this is done, each base sum element
    contains the number of preceding array elements for which the predicate is true,
    which also is the start index of the corresponding block’s output array.
  id: totrans-1688
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用[第13.23节](ch13.html#ch13lis23)中的核心函数来计算基本和数组的扫描。一旦完成，每个基本和元素包含满足谓词条件的前面数组元素的数量，也就是对应块输出数组的起始索引。
- en: '*Listing 13.23.* `streamCompact_odd` kernel.'
  id: totrans-1689
  prefs: []
  type: TYPE_NORMAL
  zh: '*第13.23节.* `streamCompact_odd` 核心函数。'
- en: '[Click here to view code image](ch13_images.html#p13lis23a)'
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch13_images.html#p13lis23a)'
- en: '* * *'
  id: totrans-1691
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<class T, bool bZeroPad>
  id: totrans-1692
  prefs: []
  type: TYPE_NORMAL
  zh: template<class T, bool bZeroPad>
- en: __global__ void
  id: totrans-1693
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: streamCompact_odd(
  id: totrans-1694
  prefs: []
  type: TYPE_NORMAL
  zh: streamCompact_odd(
- en: T *out,
  id: totrans-1695
  prefs: []
  type: TYPE_NORMAL
  zh: T *out,
- en: int *outCount,
  id: totrans-1696
  prefs: []
  type: TYPE_NORMAL
  zh: int *outCount,
- en: const int *gBaseSums,
  id: totrans-1697
  prefs: []
  type: TYPE_NORMAL
  zh: const int *gBaseSums,
- en: const T *in,
  id: totrans-1698
  prefs: []
  type: TYPE_NORMAL
  zh: const T *in,
- en: size_t N,
  id: totrans-1699
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: size_t elementsPerPartial )
  id: totrans-1700
  prefs: []
  type: TYPE_NORMAL
  zh: size_t elementsPerPartial )
- en: '{'
  id: totrans-1701
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: extern volatile __shared__ int sPartials[];
  id: totrans-1702
  prefs: []
  type: TYPE_NORMAL
  zh: extern volatile __shared__ int sPartials[];
- en: const int tid = threadIdx.x;
  id: totrans-1703
  prefs: []
  type: TYPE_NORMAL
  zh: const int tid = threadIdx.x;
- en: int sIndex = scanSharedIndex<bZeroPad>( threadIdx.x );
  id: totrans-1704
  prefs: []
  type: TYPE_NORMAL
  zh: int sIndex = scanSharedIndex<bZeroPad>( threadIdx.x );
- en: if ( bZeroPad ) {
  id: totrans-1705
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( bZeroPad ) {
- en: sPartials[sIndex-16] = 0;
  id: totrans-1706
  prefs: []
  type: TYPE_NORMAL
  zh: sPartials[sIndex-16] = 0;
- en: '}'
  id: totrans-1707
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // exclusive scan element gBaseSums[blockIdx.x]
  id: totrans-1708
  prefs: []
  type: TYPE_NORMAL
  zh: // 排他扫描元素 gBaseSums[blockIdx.x]
- en: int base_sum = 0;
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
  zh: int base_sum = 0;
- en: if ( blockIdx.x && gBaseSums ) {
  id: totrans-1710
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( blockIdx.x && gBaseSums ) {
- en: base_sum = gBaseSums[blockIdx.x-1];
  id: totrans-1711
  prefs: []
  type: TYPE_NORMAL
  zh: base_sum = gBaseSums[blockIdx.x-1];
- en: '}'
  id: totrans-1712
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( size_t i = 0;
  id: totrans-1713
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ( size_t i = 0;
- en: i < elementsPerPartial;
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
  zh: i < elementsPerPartial;
- en: i += blockDim.x ) {
  id: totrans-1715
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x ) {
- en: size_t index = blockIdx.x*elementsPerPartial + i + tid;
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
  zh: size_t index = blockIdx.x*elementsPerPartial + i + tid;
- en: 'int value = (index < N) ? in[index] : 0;'
  id: totrans-1717
  prefs: []
  type: TYPE_NORMAL
  zh: 'int value = (index < N) ? in[index] : 0;'
- en: 'sPartials[sIndex] = (index < N) ? isOdd( value ) : 0;'
  id: totrans-1718
  prefs: []
  type: TYPE_NORMAL
  zh: 'sPartials[sIndex] = (index < N) ? isOdd( value ) : 0;'
- en: __syncthreads();
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: scanBlock<int,bZeroPad>( sPartials+sIndex );
  id: totrans-1720
  prefs: []
  type: TYPE_NORMAL
  zh: scanBlock<int,bZeroPad>( sPartials+sIndex );
- en: __syncthreads();
  id: totrans-1721
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: if ( index < N && isOdd( value ) ) {
  id: totrans-1722
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( index < N && isOdd( value ) ) {
- en: int outIndex = base_sum;
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
  zh: int outIndex = base_sum;
- en: if ( tid ) {
  id: totrans-1724
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( tid ) {
- en: outIndex += sPartials[
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
  zh: outIndex += sPartials[
- en: scanSharedIndex<bZeroPad>(tid-1)];
  id: totrans-1726
  prefs: []
  type: TYPE_NORMAL
  zh: scanSharedIndex<bZeroPad>(tid-1)];
- en: '}'
  id: totrans-1727
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: out[outIndex] = value;
  id: totrans-1728
  prefs: []
  type: TYPE_NORMAL
  zh: out[outIndex] = value;
- en: '}'
  id: totrans-1729
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-1730
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: // carry forward from this block to the next.
  id: totrans-1731
  prefs: []
  type: TYPE_NORMAL
  zh: // 从当前块传递到下一个块。
- en: '{'
  id: totrans-1732
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int inx = scanSharedIndex<bZeroPad>( blockDim.x-1 );
  id: totrans-1733
  prefs: []
  type: TYPE_NORMAL
  zh: int inx = scanSharedIndex<bZeroPad>( blockDim.x-1 );
- en: base_sum += sPartials[ inx ];
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
  zh: base_sum += sPartials[ inx ];
- en: '}'
  id: totrans-1735
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-1736
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-1737
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if ( threadIdx.x == 0 && blockIdx.x == 0 ) {
  id: totrans-1738
  prefs: []
  type: TYPE_NORMAL
  zh: if ( threadIdx.x == 0 && blockIdx.x == 0 ) {
- en: if ( gBaseSums ) {
  id: totrans-1739
  prefs: []
  type: TYPE_NORMAL
  zh: if ( gBaseSums ) {
- en: '*outCount = gBaseSums[gridDim.x-1];'
  id: totrans-1740
  prefs: []
  type: TYPE_NORMAL
  zh: '*outCount = gBaseSums[gridDim.x-1];'
- en: '}'
  id: totrans-1741
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else {
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
  zh: else {
- en: int inx = scanSharedIndex<bZeroPad>( blockDim.x-1 );
  id: totrans-1743
  prefs: []
  type: TYPE_NORMAL
  zh: int inx = scanSharedIndex<bZeroPad>( blockDim.x-1 );
- en: '*outCount = sPartials[ inx ];'
  id: totrans-1744
  prefs: []
  type: TYPE_NORMAL
  zh: '*outCount = sPartials[ inx ];'
- en: '}'
  id: totrans-1745
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1746
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1747
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1748
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 13.23](ch13.html#ch13lis23) shows the code for step 3, which takes
    the input array and the array of base sums, evaluates the predicate again for
    each input array element, and writes the element to the correctly indexed output
    element if the predicate is true. The host code is analogous to [Listing 13.12](ch13.html#ch13lis12),
    with minor changes, and is not shown here.'
  id: totrans-1749
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 13.23](ch13.html#ch13lis23) 显示了步骤3的代码，该步骤接受输入数组和基数和数组，针对每个输入数组元素再次评估谓词，如果谓词为真，则将元素写入正确索引的输出元素。主机代码与
    [Listing 13.12](ch13.html#ch13lis12) 类似，只有一些小改动，这里不再展示。'
- en: 13.7\. References (Parallel Scan Algorithms)
  id: totrans-1750
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.7\. 参考文献（并行扫描算法）
- en: The recursive scan-then-fan is described in the NVIDIA Technical Report NVR-2008-003
    by Sengupta et al. The recursive reduce-then-scan algorithm is described by Dotsenko
    et al. The two-level reduce-then-scan algorithm is due to Merrill. Merrill’s paper
    is extremely valuable reading, both for background and for an overview of negative
    results—for example, an attempted formulation of Scan modeled on Sklansky’s minimum-depth
    circuit whose performance was disappointing.
  id: totrans-1751
  prefs: []
  type: TYPE_NORMAL
  zh: 递归扫描然后扇形方法在NVIDIA技术报告NVR-2008-003中由Sengupta等人描述。递归减少然后扫描算法由Dotsenko等人描述。两级减少然后扫描算法由Merrill提出。Merrill的论文对于背景知识和负面结果的概述非常有价值——例如，Sklansky的最小深度电路模型中尝试的Scan公式，其性能令人失望。
- en: Blelloch, Guy E. Prefix sums and their applications. Technical Report CMU-CS-90-190.
  id: totrans-1752
  prefs: []
  type: TYPE_NORMAL
  zh: Blelloch, Guy E. 前缀和及其应用. 技术报告 CMU-CS-90-190.
- en: Dotsenko, Yuri, Naga K. Govindaraju, Peter-Pike Sloan, Charles Boyd, and John
    Manferdelli. Fast scan algorithms in graphics processors. In *Proceedings of the
    22nd Annual International Conference on Supercomputing*, ACM, 2008, pp. 205–213.
  id: totrans-1753
  prefs: []
  type: TYPE_NORMAL
  zh: Dotsenko, Yuri, Naga K. Govindaraju, Peter-Pike Sloan, Charles Boyd, 和 John
    Manferdelli. 图形处理器中的快速扫描算法. 载于 *第22届国际超级计算大会论文集*，ACM，2008年，页码205–213。
- en: Fellner, D., and S. Spender, eds. SIGGRAPH/Eurographics Conference on Graphics
    Hardware. Eurographics Association, Aire-la-Ville, Switzerland, pp. 97–106.
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
  zh: Fellner, D., 和 S. Spender, 编. SIGGRAPH/Eurographics 图形硬件会议. Eurographics协会，瑞士Aire-la-Ville，页码97–106。
- en: Harris, Mark, and Michael Garland. Optimizing parallel prefix operations for
    the Fermi architecture. In *GPU Computing Gems, Jade Edition*, Wen-Mei Hwu, ed.
    Morgan Kaufmann, Waltham, MA, 2012, pp. 29–38.
  id: totrans-1755
  prefs: []
  type: TYPE_NORMAL
  zh: Harris, Mark, 和 Michael Garland. 为Fermi架构优化并行前缀操作。在*GPU计算宝石, Jade版*中，Wen-Mei
    Hwu主编。摩根·考夫曼，马萨诸塞州沃尔瑟姆，2012年，第29-38页。
- en: Harris, Mark, Shubhabrata Sengupta, and John Owens. Parallel prefix sum (scan)
    with CUDA. In *GPU Gems 3*, H. Nguyen, ed. Addison-Wesley, Boston, MA, Aug. 2007.
  id: totrans-1756
  prefs: []
  type: TYPE_NORMAL
  zh: Harris, Mark, Shubhabrata Sengupta 和 John Owens. 使用CUDA的并行前缀和（扫描）。在*GPU Gems
    3*中，H. Nguyen主编。阿迪森-韦斯利，马萨诸塞州波士顿，2007年8月。
- en: Merrill, Duane, and Andrew Grimshaw. Parallel scan for stream architectures.
    Technical Report CS2009-14\. Department of Computer Science, University of Virginia.
  id: totrans-1757
  prefs: []
  type: TYPE_NORMAL
  zh: Merrill, Duane, 和 Andrew Grimshaw. 流架构的并行扫描。技术报告CS2009-14。弗吉尼亚大学计算机科学系。
- en: Sengupta, Shubhabrata, Mark Harris, and Michael Garland. Efficient parallel
    scan algorithms for GPUs. NVIDIA Technical Report NVR-2008-003\. December 2008.
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
  zh: Sengupta, Shubhabrata, Mark Harris 和 Michael Garland. 高效的并行扫描算法针对GPU。NVIDIA技术报告NVR-2008-003。2008年12月。
- en: '[http://research.nvidia.com/publication/efficient-parallel-scan-algorithms-gpus](http://research.nvidia.com/publication/efficient-parallel-scan-algorithms-gpus)'
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://research.nvidia.com/publication/efficient-parallel-scan-algorithms-gpus](http://research.nvidia.com/publication/efficient-parallel-scan-algorithms-gpus)'
- en: Sengupta, Shubhabrata, Mark Harris, ZhangYao Zhang, and John D. Owens. Scan
    primitives for GPU computing. In *Proceedings of the 22nd ACM SIGGRAPH/Eurographics
    Symposium on Graphics Hardware.* San Diego, CA, August 4–5, 2007.
  id: totrans-1760
  prefs: []
  type: TYPE_NORMAL
  zh: Sengupta, Shubhabrata, Mark Harris, ZhangYao Zhang 和 John D. Owens. GPU计算的扫描原语。在*第22届ACM
    SIGGRAPH/欧洲图形学硬件研讨会论文集*中。加利福尼亚州圣地亚哥，2007年8月4-5日。
- en: 13.8\. Further Reading (Parallel Prefix Sum Circuits)
  id: totrans-1761
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.8. 进一步阅读（并行前缀和电路）
- en: There is a rich literature on circuits to compute parallel prefix sums. Besides
    the Brent-Kung, Sklansky, and Kogge-Stone formulations, other examples of scan
    circuits include Ladner-Fischer and more recent work by Lin and Hsiao. Hinze describes
    an algebra of scans that can be used to reason about Scan implementations. The
    details of his work are outside the scope of this book, but his paper is highly
    recommended reading.
  id: totrans-1762
  prefs: []
  type: TYPE_NORMAL
  zh: 有关计算并行前缀和的电路有丰富的文献。除了Brent-Kung、Sklansky和Kogge-Stone的公式外，其他扫描电路的例子还包括Ladner-Fischer和Lin与Hsiao的最新工作。Hinze描述了一种扫描代数，可以用来推理扫描实现的相关问题。他的工作细节超出了本书的范围，但他的论文非常值得一读。
- en: Sean Baxter’s Web site, [http://www.moderngpu.com](http://www.moderngpu.com),
    is an excellent resource for optimized Scan and its applications.
  id: totrans-1763
  prefs: []
  type: TYPE_NORMAL
  zh: Sean Baxter的官方网站，[http://www.moderngpu.com](http://www.moderngpu.com)，是优化扫描及其应用的绝佳资源。
- en: Brent, Richard P., and H.T. Kung. A regular layout for parallel adders. *IEEE
    Transactions on Computers* C-31, 1982, pp. 260–264.
  id: totrans-1764
  prefs: []
  type: TYPE_NORMAL
  zh: Brent, Richard P., 和 H.T. Kung. 并行加法器的常规布局。*IEEE计算机学报* C-31，1982年，第260-264页。
- en: Hinze, Ralf. An algebra of scans. In *Mathematics of Program Construction*,
    Springer, 2004, Stirling, Scotland, pp. 186–210.
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
  zh: Hinze, Ralf. 扫描的代数。在*程序构造的数学*中，Springer，2004年，苏格兰斯特灵，第186-210页。
- en: Kogge, Peter M., and Harold S. Stone. A parallel algorithm for the efficient
    solution of a general class of recurrence equations. *IEEE Transactions on Computers*
    C-22, 1973, pp. 783–791.
  id: totrans-1766
  prefs: []
  type: TYPE_NORMAL
  zh: Kogge, Peter M., 和 Harold S. Stone. 一种用于高效解决一般类型递归方程的并行算法。*IEEE Transactions
    on Computers* C-22，1973年，第783-791页。
- en: Sklansky, J. Conditional sum addition logic. *IRE Trans. Electron. Comput*.
    9 (2), June 1960, pp. 226–231.
  id: totrans-1767
  prefs: []
  type: TYPE_NORMAL
  zh: Sklansky, J. 条件和加法逻辑。*IRE Trans. Electron. Comput*。9 (2)，1960年6月，第226-231页。
- en: Chapter 14\. N-Body
  id: totrans-1768
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第14章 N-体问题
- en: N-Body computations are a family of computation that models a set of particles
    (known as *bodies*), each of which must consider all the other bodies during the
    computation. Example applications of N-Body include (but are not limited to) the
    following.
  id: totrans-1769
  prefs: []
  type: TYPE_NORMAL
  zh: N-体计算是一类模拟一组粒子（称为*物体*）的计算，每个粒子在计算过程中必须考虑其他所有粒子。N-体问题的典型应用包括（但不限于）以下几种。
- en: • Gravitational simulation in which stars exert gravitational forces
  id: totrans-1770
  prefs: []
  type: TYPE_NORMAL
  zh: • 引力模拟，其中恒星施加引力
- en: • Molecular modeling in which ions exert electrostatic forces
  id: totrans-1771
  prefs: []
  type: TYPE_NORMAL
  zh: • 分子建模，其中离子施加静电力
- en: • Particle systems in computer graphics to simulate water and fire
  id: totrans-1772
  prefs: []
  type: TYPE_NORMAL
  zh: • 计算机图形学中的粒子系统，用于模拟水和火
- en: • “Boids,” a technique for computer animation designed to simulate flocking
    behavior
  id: totrans-1773
  prefs: []
  type: TYPE_NORMAL
  zh: • “Boids”，一种用于计算机动画的技术，旨在模拟群体行为
- en: Typically the paths of the bodies are being simulated per timestep, and computing
    each timestep costs O(*N*²) operations for *N* bodies. In most formulations, the
    forces quickly decrease with distance, leading to hierarchical algorithms in which
    (for example) the mass and location of the center-of-mass for a collection of
    bodies are used to avoid performing the full O(*N*²) computations needed otherwise.
    Barnes-Hut algorithms reduce the runtime to O(*N*lg*N*) by introducing a spatial
    hierarchy that approximates the forces between clusters of objects; for applications
    where the “leaf nodes” of the computation contain *k* bodies, O(*k*²) computations
    must be performed in a given leaf. It is this O(*k*²) portion of the computation
    at which GPUs excel.
  id: totrans-1774
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，物体的路径是按时间步长进行模拟的，每个时间步的计算对于*N*个物体来说需要O(*N*²)次运算。在大多数情况下，力随着距离的增加迅速减小，这导致了层次化算法（例如，通过物体集合的质心质量和位置来避免进行完整的O(*N*²)计算）。Barnes-Hut算法通过引入空间层次结构，近似物体簇之间的力，将运行时间降低到O(*N*lg*N*)；对于计算的“叶节点”包含*k*个物体的应用，必须在给定的叶节点中执行O(*k*²)次计算。在这个O(*k*²)部分的计算中，GPU表现得尤为出色。
- en: N-Body workloads have proven the most effective way for GPUs to approach their
    theoretical limit in processing power. In their GPU Gems 3 paper “Fast N-Body
    Simulation with CUDA,”^([1](ch14.html#ch14fn1)) Harris et al. frequently cite
    this theoretical limit in explaining why further performance improvements are
    not possible. The GPU in question, NVIDIA GeForce 8800 GTX, was so effective at
    N-Body computations that it outperformed custom GRAPE-6 hardware that had been
    specifically designed to perform astrophysics computation.
  id: totrans-1775
  prefs: []
  type: TYPE_NORMAL
  zh: N-Body 工作负载已被证明是 GPU 达到其理论处理能力极限的最有效方式。在他们的《GPU Gems 3》论文《CUDA 快速 N-Body 仿真》中，Harris
    等人^([1](ch14.html#ch14fn1)) 经常引用这个理论极限来解释为何进一步的性能提升已经不可能。所涉及的 GPU，NVIDIA GeForce
    8800 GTX，在 N-Body 计算中的表现如此出色，以至于超越了专为天体物理计算设计的定制 GRAPE-6 硬件。
- en: '[1](ch14.html#ch14fn1a). [http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html](http://developer.nvidia.com/GPUGems3/gpugems3_ch31.html)'
  id: totrans-1776
  prefs: []
  type: TYPE_NORMAL
  zh: '[1](ch14.html#ch14fn1a). [http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html](http://developer.nvidia.com/GPUGems3/gpugems3_ch31.html)'
- en: In the hopes that readers will be able to “plug in” their computation and find
    the fastest method, this chapter illustrates several different ways to implement
    N-Body and related computations using CUDA.
  id: totrans-1777
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在帮助读者能够“插入”他们的计算并找到最快的方法，展示了多种使用 CUDA 实现 N-Body 和相关计算的方式。
- en: • A naïve implementation illustrates the technique and underscores the effectiveness
    of caches and the importance of loop unrolling.
  id: totrans-1778
  prefs: []
  type: TYPE_NORMAL
  zh: • 一个简单的实现展示了该技术，并强调了缓存的有效性和循环展开的重要性。
- en: • A shared memory implementation (for our gravitational computation, the fastest)
    duplicates Harris et al.’s result, tiling the computation over threadblock-sized
    collections of bodies to minimize memory latencies in the innermost loop.
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
  zh: • 一个共享内存实现（对于我们的引力计算来说是最快的）复制了 Harris 等人的结果，将计算分块到线程块大小的物体集合上，以最小化最内层循环中的内存延迟。
- en: • A constant memory implementation, inspired by Stone et al.’s implementation
    of Direct Coulomb Summation (DCS),^([2](ch14.html#ch14fn2)) uses constant memory
    to hold body descriptions, freeing shared memory for other uses.
  id: totrans-1780
  prefs: []
  type: TYPE_NORMAL
  zh: • 一个常量内存实现，灵感来自 Stone 等人对直接库伦求和（DCS）实现的研究^([2](ch14.html#ch14fn2))，使用常量内存保存物体描述，从而将共享内存释放给其他用途。
- en: '[2](ch14.html#ch14fn2a). [www.ncbi.nlm.nih.gov/pubmed/17894371](http://www.ncbi.nlm.nih.gov/pubmed/17894371)'
  id: totrans-1781
  prefs: []
  type: TYPE_NORMAL
  zh: '[2](ch14.html#ch14fn2a). [www.ncbi.nlm.nih.gov/pubmed/17894371](http://www.ncbi.nlm.nih.gov/pubmed/17894371)'
- en: Because readers’ applications may not happen to be gravitational N-Body, these
    different implementations are not presented with the singular goal of optimizing
    that particular computation. It may make sense to adapt a different implementation,
    depending on the target SM architecture, problem size, and details of the central
    calculation.
  id: totrans-1782
  prefs: []
  type: TYPE_NORMAL
  zh: 因为读者的应用程序可能不涉及引力 N-Body 计算，这些不同的实现并非以优化该特定计算为唯一目标。根据目标 SM 架构、问题规模和中心计算的细节，采用不同的实现可能更为合适。
- en: Since gravitational N-Body has been presented as a poster child for theoretical
    performance of GPUs, with speedups of up to 400x reported, the chapter concludes
    by presenting an implementation optimized for CPUs. By rewriting the calculation
    to use SSE (Streaming SIMD Extensions) and multithreading, a speedup of more than
    300x is obtained. Nevertheless, as reported in [Section 14.9](ch14.html#ch14lev1sec9),
    a GK104-based GPU is significantly faster than a high-end server with a pair of
    Intel Xeon E2670 CPUs. The CUDA implementation is faster, more readable, and more
    maintainable than the optimized CPU implementation.
  id: totrans-1783
  prefs: []
  type: TYPE_NORMAL
  zh: 由于引力 N 体问题被作为理论性能展示的典型案例，报告中提到速度提升可达 400 倍，本章最后通过展示一个针对 CPU 优化的实现来总结。通过重写计算方式，使用
    SSE（流式 SIMD 扩展）和多线程，获得了超过 300 倍的加速。然而，正如在[第 14.9 节](ch14.html#ch14lev1sec9)中所报告的，基于
    GK104 的 GPU 显著快于配备一对 Intel Xeon E2670 CPU 的高端服务器。CUDA 实现比优化过的 CPU 实现更快、更易读且更易维护。
- en: Throughout the chapter, performance results are reported using a server-class
    machine with two Xeon E2670 “Sandy Bridge” CPUs and up to four GK104 GPUs that
    are underclocked to conserve power and minimize heat dissipation. Rather than
    reporting results in GFLOPS, we report performance results in terms of body-body
    interactions per second.
  id: totrans-1784
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，性能结果使用一台服务器级别的机器进行报告，该机器配备了两颗 Xeon E2670 “Sandy Bridge” CPU 和最多四个降频以节省电力并最小化散热的
    GK104 GPU。我们并未以 GFLOPS 为单位报告结果，而是以每秒体体相互作用次数来报告性能结果。
- en: 14.1\. Introduction
  id: totrans-1785
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1\. 引言
- en: Given *N* bodies with positions **x**[i] and velocities **v**[i] for 1 ≤ *i*
    ≤ *N*, the force vector **f**[ij] on body *i* caused by body *j* is given by
  id: totrans-1786
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *N* 个物体，其中每个物体的位置信息为 **x**[i]，速度信息为 **v**[i]，其中 1 ≤ *i* ≤ *N*，物体 *i* 受物体
    *j* 影响的力向量 **f**[ij] 为
- en: '![Image](graphics/423equ01.jpg)'
  id: totrans-1787
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/423equ01.jpg)'
- en: where *m*[i] and *m*[j] are the masses of bodies *i* and *j*, respectively;
    **d**[ij] is the difference vector from body *i* to body *j*; and *G* is the gravitational
    constant. Due to divide overflow, this express diverges for **d**[ij] with small
    magnitude; to compensate, it is common practice to apply a *softening factor*
    that models the interaction between two Plummer masses—masses that behave as if
    they were spherical galaxies. For a softening factor ε, the resulting expression
    is
  id: totrans-1788
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *m*[i] 和 *m*[j] 分别是物体 *i* 和物体 *j* 的质量；**d**[ij] 是从物体 *i* 到物体 *j* 的差向量；*G*
    是引力常数。由于除法溢出，当 **d**[ij] 的大小很小时，该表达式会发散；为了解决这个问题，通常会应用一个 *软化因子* 来模拟两个普朗默质量体之间的相互作用——这些质量体的行为类似于球形星系。对于一个软化因子
    ε，得到的表达式为
- en: '![Image](graphics/423equ02.jpg)'
  id: totrans-1789
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/423equ02.jpg)'
- en: The total force **F**[i] on body *i*, due to its interactions with the other
    *N* – 1 bodies, is obtained by summing all interactions.
  id: totrans-1790
  prefs: []
  type: TYPE_NORMAL
  zh: 物体 *i* 受到其他 *N* – 1 个物体的相互作用产生的总力 **F**[i] 由所有相互作用的合力得出。
- en: '![Image](graphics/423equ03.jpg)'
  id: totrans-1791
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/423equ03.jpg)'
- en: To update the position and velocity of each body, the force (acceleration) applied
    for body *i* is **a**[i] = **F**[i]/*m*[i], so the **m**[i] term can be removed
    from the numerator, as follows.
  id: totrans-1792
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新每个物体的位置和速度，施加在物体*i*上的力（加速度）是**a**[i] = **F**[i]/*m*[i]，因此**m**[i]项可以从分子中去除，具体如下。
- en: '![Image](graphics/423equ04.jpg)'
  id: totrans-1793
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/423equ04.jpg)'
- en: Like Nyland et al., we use a leapfrog Verlet algorithm to apply a timestep to
    the simulation. The values of the positions and velocities are offset by half
    a timestep from one another, a characteristic that is not obvious from the code
    because in our sample, the positions and velocities are initially assigned random
    values. Our leapfrog Verlet integration updates the velocity, then the position.
  id: totrans-1794
  prefs: []
  type: TYPE_NORMAL
  zh: 像Nyland等人一样，我们使用跳跃 Verlet 算法为模拟应用时间步长。位置和速度的值相互偏移半个时间步长，这一特性在代码中并不明显，因为在我们的示例中，位置和速度初始时被赋予了随机值。我们的跳跃
    Verlet 积分法首先更新速度，然后更新位置。
- en: '![Image](graphics/424equ01.jpg)![Image](graphics/424equ02.jpg)'
  id: totrans-1795
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/424equ01.jpg)![Image](graphics/424equ02.jpg)'
- en: This method is just one of many different integration algorithms that can be
    used to update the simulation, but an extensive discussion is outside the scope
    of this book.
  id: totrans-1796
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法仅是许多不同的积分算法之一，这些算法可用于更新模拟，但详细讨论超出了本书的范围。
- en: Since the integration has a runtime of O(*N*) and computing the forces has a
    runtime of *O*(*N*²), the biggest performance benefits from porting to CUDA stem
    from optimizing the force computation. Optimizing this portion of the calculation
    is the primary focus of this chapter.
  id: totrans-1797
  prefs: []
  type: TYPE_NORMAL
  zh: 由于积分的运行时间为O(*N*)，而计算力的运行时间为O(*N*²)，将计算迁移到CUDA中最显著的性能提升来自于优化力的计算。优化这部分计算是本章的主要重点。
- en: 14.1.1\. A Matrix of Forces
  id: totrans-1798
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 14.1.1\. 力矩阵
- en: A naïve implementation of the N-Body algorithm consists of a doubly nested loop
    that, for each body, computes the sum of the forces exerted on that body by every
    other body. The O(*N*²) body-body forces can be thought of as an *N*×*N* matrix,
    where the sum of each row *i* is the total gravitational force exerted on body
    *i.*
  id: totrans-1799
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的N-Body算法实现包括一个双重嵌套的循环，在每次迭代中，对于每个物体，计算所有其他物体施加在该物体上的力的总和。O(*N*²)的物体间力可以被视为一个*N*×*N*的矩阵，其中每一行*i*的总和就是施加在物体*i*上的总引力。
- en: '![Image](graphics/424equ03.jpg)'
  id: totrans-1800
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/424equ03.jpg)'
- en: The diagonals of this “matrix” are zeroes corresponding to each body’s influence
    on itself, which can be ignored.
  id: totrans-1801
  prefs: []
  type: TYPE_NORMAL
  zh: 该“矩阵”的对角线为零，表示每个物体对自身的影响，可以忽略。
- en: Because each element in the matrix may be computed independently, there is a
    tremendous amount of potential parallelism. The sum of each row is a reduction
    that may be computed by a single thread or by combining results from multiple
    threads as described in [Chapter 12](ch12.html#ch12).
  id: totrans-1802
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵中的每个元素可以独立计算，因此具有巨大的潜在并行性。每行的总和是一个可以通过单个线程计算的归约操作，也可以通过结合多个线程的结果来计算，正如在[第12章](ch12.html#ch12)中所描述的那样。
- en: '[Figure 14.1](ch14.html#ch14fig01) shows an 8-body “matrix” being processed.
    The rows correspond to the sums that are the output of the computation. When using
    CUDA, N-Body implementations typically have one thread compute the sum for each
    row.'
  id: totrans-1803
  prefs: []
  type: TYPE_NORMAL
  zh: '[图14.1](ch14.html#ch14fig01)展示了一个8物体“矩阵”的处理过程。行对应于计算结果的和。在使用CUDA时，N-Body实现通常让每个线程计算每行的和。'
- en: '![Image](graphics/14fig01.jpg)'
  id: totrans-1804
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/14fig01.jpg)'
- en: '*Figure 14.1* “Matrix” of forces (8 bodies).'
  id: totrans-1805
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.1* 力的“矩阵”（8个物体）。'
- en: 'Since every element in the “matrix” is independent, they also may be computed
    in parallel *within* a given row: Compute the sum of every fourth element, for
    example, and then add the four partial sums to compute the final output. Harris
    et al. describe using this method for small *N* where there are not enough threads
    to cover the latencies in the GPU: Launch more threads, compute partial sums in
    each thread, and accumulate the final sums with reductions in shared memory. Harris
    et al. reported a benefit for *N* ≤ 4096.'
  id: totrans-1806
  prefs: []
  type: TYPE_NORMAL
  zh: 由于“矩阵”中的每个元素是独立的，因此它们也可以在给定的行内并行计算：例如，计算每四个元素的和，然后将四个部分和相加得到最终结果。Harris等人描述了这种方法用于小*N*的情况，在这种情况下，线程不足以覆盖GPU的延迟：启动更多线程，在每个线程中计算部分和，并在共享内存中通过归约累加最终和。Harris等人报告了当*N*
    ≤ 4096时的好处。
- en: For physical forces that are symmetric (i.e., the force exerted on body *i*
    by body *j* is equal in magnitude but has the opposite sign of the force exerted
    by body *j* on *i*), such as gravitation, the transpose “elements” of the matrix
    have the opposite sign.
  id: totrans-1807
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有对称性的物理力（即，物体*i*受到物体*j*作用的力与物体*j*受到物体*i*作用的力大小相等但方向相反），例如引力，矩阵的转置“元素”具有相反的符号。
- en: '![Image](graphics/425equ01.jpg)'
  id: totrans-1808
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/425equ01.jpg)'
- en: In this case, the “matrix” takes the form shown in [Figure 14.2](ch14.html#ch14fig02).
    When exploiting the symmetry, an implementation need only compute the upper right
    triangle of the “matrix,” performing about half as many body-body computations.^([3](ch14.html#ch14fn3))
  id: totrans-1809
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，“矩阵”的形式如[图14.2](ch14.html#ch14fig02)所示。利用对称性时，实施时只需计算“矩阵”的右上三角部分，执行的体间计算大约是原来的一半。^([3](ch14.html#ch14fn3))
- en: '[3](ch14.html#ch14fn3a). ![Image](graphics/425fig02.jpg), to be exact.'
  id: totrans-1810
  prefs: []
  type: TYPE_NORMAL
  zh: '[3](ch14.html#ch14fn3a)。准确来说，![Image](graphics/425fig02.jpg)。'
- en: '![Image](graphics/14fig02.jpg)'
  id: totrans-1811
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/14fig02.jpg)'
- en: '*Figure 14.2* Matrix with symmetric forces.'
  id: totrans-1812
  prefs: []
  type: TYPE_NORMAL
  zh: '*图14.2* 带有对称力的矩阵。'
- en: The problem is that unlike the brute force method outlined in [Figure 14.1](ch14.html#ch14fig01),
    when exploiting symmetric forces, different threads may have contributions to
    add to a given output sum. Partial sums must be accumulated and either written
    to temporary locations for eventual reduction or the system must protect the final
    sums with mutual exclusion (by using atomics or thread synchronization). Since
    the body-body computation is about 20 FLOPS (for single precision) or 30 FLOPS
    (for double precision), subtracting from a sum would seem like a decisive performance
    win.
  id: totrans-1813
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，与[图 14.1](ch14.html#ch14fig01)中概述的暴力方法不同，当利用对称力时，不同的线程可能需要对给定的输出和进行加法操作。必须累积部分和，并将其写入临时位置以便最终归约，或者系统必须通过互斥保护最终的和（通过使用原子操作或线程同步）。由于体-体计算约为20
    FLOPS（单精度）或30 FLOPS（双精度），从和中减去将看起来是一个决定性的性能提升。
- en: Unfortunately, the overhead often overwhelms the benefit of performing half
    as many body-body computations. For example, a completely naïve implementation
    that does two (2) floating-point atomic adds per body-body computation is prohibitively
    slower than the brute force method.
  id: totrans-1814
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，开销往往压倒了进行一半数量体-体计算所带来的好处。例如，一个完全天真的实现，每次体-体计算执行两个（2）浮点原子加法，比暴力方法慢得多，几乎无法接受。
- en: '[Figure 14.3](ch14.html#ch14fig03) shows a compromise between the two extremes:
    By tiling the computation, only the upper right diagonal of *tiles* needs to be
    computed. For a tile size of *k*, this method performs *k*² body-body computations
    each on ![Image](graphics/426fig01.jpg) nondiagonal tiles, plus *k*(*k* –1) body-body
    computations each on *N/k* diagonal tiles. For large *N*, the savings in body-body
    computations are about the same,^([4](ch14.html#ch14fn4)) but because the tiles
    can locally accumulate partial sums to contribute to the final answer, the synchronization
    overhead is reduced. [Figure 14.3](ch14.html#ch14fig03) shows a tile size with
    *k* = 2, but a tile size corresponding to the warp size (*k* = 32) is more practical.'
  id: totrans-1815
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14.3](ch14.html#ch14fig03)展示了两种极端情况之间的折中：通过分块计算，只需要计算*tiles*的右上对角线部分。对于大小为*k*的块，该方法每个非对角块执行*k*²次体-体计算，另外对*N/k*个对角块，每个执行*k*(*k*–1)次体-体计算。对于大*N*，体-体计算的节省大致相同，^([4](ch14.html#ch14fn4))，但由于块可以局部累积部分和以贡献最终答案，减少了同步开销。[图
    14.3](ch14.html#ch14fig03)显示了一个块大小为*k* = 2，但一个与warp大小相对应的块大小（*k* = 32）更加实用。'
- en: '[4](ch14.html#ch14fn4a). For example, with *N* = 65536 and *k* = 32, the tiled
    approach performs 51.5% of the body-body computations performed by the brute force
    algorithm, or 3% more than the ideal symmetric algorithm.'
  id: totrans-1816
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](ch14.html#ch14fn4a)。例如，当*N* = 65536 和 *k* = 32 时，分块方法执行的体-体计算为暴力算法的51.5%，比理想的对称算法多出3%。'
- en: '![Image](graphics/14fig03.jpg)'
  id: totrans-1817
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/14fig03.jpg)'
- en: '*Figure 14.3* Tiled N-Body (*k* = 2).'
  id: totrans-1818
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14.3* 分块 N 体问题（*k* = 2）。'
- en: '[Figure 14.4](ch14.html#ch14fig04) shows how the partial sums for a given tile
    are computed. The partial sums for the rows and columns are computed—adding and
    subtracting, respectively—in order to arrive at partial sums that must be added
    to the corresponding output sums.'
  id: totrans-1819
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 14.4](ch14.html#ch14fig04) 显示了如何计算给定瓦片的部分和。分别通过加法和减法计算行和列的部分和，以得到必须添加到相应输出和的部分和。'
- en: '![Image](graphics/14fig04.jpg)'
  id: totrans-1820
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/14fig04.jpg)'
- en: '*Figure 14.4* N-Body tile.'
  id: totrans-1821
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 14.4* N-Body 瓦片。'
- en: The popular AMBER application for molecular modeling exploits symmetry of forces,
    performing the work on tiles tuned to the warp size of 32,^([5](ch14.html#ch14fn5))
    but in extensive testing, the approach has not proven fruitful for the more lightweight
    computation described here.
  id: totrans-1822
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的分子建模应用 AMBER 利用力的对称性，在调整到 32 的 Warp 大小的瓦片上进行计算，^([5](ch14.html#ch14fn5))，但在广泛的测试中，这种方法对于此处描述的更轻量的计算并未取得良好的效果。
- en: '[5](ch14.html#ch14fn5a). Götz, Andreas, Mark J. Williamson, Dong Xu, Duncan
    Poole, Scott Le Grand, and Ross C. Walker. Routine microsecond molecular dynamics
    simulations with AMBER on GPUs—Part I: Generalized Born, *J. Chem. Theory Comput*.
    8, no. 5 (2012), pp. 1542–1555.'
  id: totrans-1823
  prefs: []
  type: TYPE_NORMAL
  zh: '[5](ch14.html#ch14fn5a). Götz, Andreas, Mark J. Williamson, Dong Xu, Duncan
    Poole, Scott Le Grand, 和 Ross C. Walker. 使用 AMBER 在 GPU 上进行常规微秒分子动力学模拟—第 I 部分：广义
    Born，*J. Chem. Theory Comput*. 8, 第 5 期 (2012)，第 1542–1555 页。'
- en: 14.2\. Naïve Implementation
  id: totrans-1824
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2\. 简单实现
- en: '[Listing 14.1](ch14.html#ch14lis01) gives a function that implements the body-body
    interaction described in the previous section; by annotating it with both the
    `__host__` and `__device__` keywords, the CUDA compiler knows it is valid for
    both the CPU and GPU. The function is templated so it may be invoked for both
    `float` and `double` values (though for this book, only `float` is fully implemented).
    It passes back the 3D force vector in the `(fx, fy, fz)` tuple.'
  id: totrans-1825
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 14.1](ch14.html#ch14lis01) 给出了实现前述章节中描述的体-体相互作用的函数；通过在函数前标注 `__host__`
    和 `__device__` 关键字，CUDA 编译器就知道该函数适用于 CPU 和 GPU。该函数是模板化的，因此可以在 `float` 和 `double`
    类型上调用（尽管在本书中，只有 `float` 类型得到了完全实现）。它会返回 3D 力向量，格式为 `(fx, fy, fz)` 元组。'
- en: '*Listing 14.1.* `bodyBodyInteraction`.'
  id: totrans-1826
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 14.1.* `bodyBodyInteraction`。'
- en: '[Click here to view code image](ch14_images.html#p14lis01a)'
  id: totrans-1827
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch14_images.html#p14lis01a)'
- en: '* * *'
  id: totrans-1828
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template <typename T>
  id: totrans-1829
  prefs: []
  type: TYPE_NORMAL
  zh: template <typename T>
- en: __host__ __device__ void bodyBodyInteraction(
  id: totrans-1830
  prefs: []
  type: TYPE_NORMAL
  zh: __host__ __device__ void bodyBodyInteraction(
- en: T& ax, T& ay, T& az,
  id: totrans-1831
  prefs: []
  type: TYPE_NORMAL
  zh: T& ax, T& ay, T& az,
- en: T x0, T y0, T z0,
  id: totrans-1832
  prefs: []
  type: TYPE_NORMAL
  zh: T x0, T y0, T z0,
- en: T x1, T y1, T z1, T mass1,
  id: totrans-1833
  prefs: []
  type: TYPE_NORMAL
  zh: T x1, T y1, T z1, T mass1,
- en: T softeningSquared)
  id: totrans-1834
  prefs: []
  type: TYPE_NORMAL
  zh: T softeningSquared)
- en: '{'
  id: totrans-1835
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: T dx = x1 - x0;
  id: totrans-1836
  prefs: []
  type: TYPE_NORMAL
  zh: T dx = x1 - x0;
- en: T dy = y1 - y0;
  id: totrans-1837
  prefs: []
  type: TYPE_NORMAL
  zh: T dy = y1 - y0;
- en: T dz = z1 - z0;
  id: totrans-1838
  prefs: []
  type: TYPE_NORMAL
  zh: T dz = z1 - z0;
- en: T distSqr = dx*dx + dy*dy + dz*dz;
  id: totrans-1839
  prefs: []
  type: TYPE_NORMAL
  zh: T distSqr = dx*dx + dy*dy + dz*dz;
- en: distSqr += softeningSquared;
  id: totrans-1840
  prefs: []
  type: TYPE_NORMAL
  zh: distSqr += softeningSquared;
- en: T invDist = (T)1.0 / (T)sqrt(distSqr);
  id: totrans-1841
  prefs: []
  type: TYPE_NORMAL
  zh: T invDist = (T)1.0 / (T)sqrt(distSqr);
- en: T invDistCube =  invDist * invDist * invDist;
  id: totrans-1842
  prefs: []
  type: TYPE_NORMAL
  zh: T invDistCube = invDist * invDist * invDist;
- en: T s = mass1 * invDistCube;
  id: totrans-1843
  prefs: []
  type: TYPE_NORMAL
  zh: T s = mass1 * invDistCube;
- en: ax = dx * s;
  id: totrans-1844
  prefs: []
  type: TYPE_NORMAL
  zh: ax = dx * s;
- en: ay = dy * s;
  id: totrans-1845
  prefs: []
  type: TYPE_NORMAL
  zh: ay = dy * s;
- en: az = dz * s;
  id: totrans-1846
  prefs: []
  type: TYPE_NORMAL
  zh: az = dz * s;
- en: '}'
  id: totrans-1847
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1848
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 14.2](ch14.html#ch14lis02) gives the function that computes the total
    gravitational force exerted on each body. For each body, it loads that body’s
    position into `(myX, myY, myZ)` and then, for every other body, calls `bodyBodyInteraction<float>`
    to compute the force exerted between the two. The “AOS” in the function name denotes
    that the input data comes in the form of an “array of structures”: four packed
    `float` values that give the *(x, y, z, mass)* tuple that specifies a body’s position
    and mass. The `float4` representation is a convenient size for GPU implementation,
    with native hardware support for loads and stores. Our optimized CPU implementations,
    described in [Section 14.9](ch14.html#ch14lev1sec9), use so-called “structure
    of arrays” (SOA) representation where four arrays of float contain packed *x*,
    *y*, *z*, and *mass* elements for easier processing by SIMD instruction sets.
    SOA is not a good fit for GPU implementation because the 4 base pointers needed
    by an SOA representation cost too many registers.'
  id: totrans-1849
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 14.2](ch14.html#ch14lis02) 给出了计算每个物体所受总引力的函数。对于每个物体，它将该物体的位置加载到`(myX, myY,
    myZ)`中，然后对于每个其他物体，调用`bodyBodyInteraction<float>`来计算两者之间施加的力。函数名中的“AOS”表示输入数据采用“结构体数组”（array
    of structures）形式：四个打包的`float`值，表示一个物体的*(x, y, z, mass)*元组，指定物体的位置和质量。`float4`表示法是一个适合GPU实现的便捷大小，原生硬件支持加载和存储。我们优化过的CPU实现，如[第14.9节](ch14.html#ch14lev1sec9)所述，采用了所谓的“数组结构”（SOA）表示法，其中四个`float`数组包含打包的*x*、*y*、*z*和*mass*元素，以便通过SIMD指令集进行更容易的处理。SOA不适合GPU实现，因为SOA表示法所需的4个基础指针需要消耗过多的寄存器。'
- en: '*Listing 14.2.* `ComputeGravitation_AOS` (CPU implementation).'
  id: totrans-1850
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 14.2.* `ComputeGravitation_AOS`（CPU实现）。'
- en: '[Click here to view code image](ch14_images.html#p14lis02a)'
  id: totrans-1851
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch14_images.html#p14lis02a)'
- en: '* * *'
  id: totrans-1852
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: float
  id: totrans-1853
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: ComputeGravitation_AOS(
  id: totrans-1854
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeGravitation_AOS(
- en: float *force,
  id: totrans-1855
  prefs: []
  type: TYPE_NORMAL
  zh: float *force,
- en: float *posMass,
  id: totrans-1856
  prefs: []
  type: TYPE_NORMAL
  zh: float *posMass,
- en: float softeningSquared,
  id: totrans-1857
  prefs: []
  type: TYPE_NORMAL
  zh: float softeningSquared,
- en: size_t N
  id: totrans-1858
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N
- en: )
  id: totrans-1859
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '{'
  id: totrans-1860
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: chTimerTimestamp start, end;
  id: totrans-1861
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerTimestamp start, end;
- en: chTimerGetTime( &start );
  id: totrans-1862
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: for ( size_t i = 0; i < N; i++ )
  id: totrans-1863
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = 0; i < N; i++ )
- en: '{'
  id: totrans-1864
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float ax = 0.0f;
  id: totrans-1865
  prefs: []
  type: TYPE_NORMAL
  zh: float ax = 0.0f;
- en: float ay = 0.0f;
  id: totrans-1866
  prefs: []
  type: TYPE_NORMAL
  zh: float ay = 0.0f;
- en: float az = 0.0f;
  id: totrans-1867
  prefs: []
  type: TYPE_NORMAL
  zh: float az = 0.0f;
- en: float myX = posMass[i*4+0];
  id: totrans-1868
  prefs: []
  type: TYPE_NORMAL
  zh: float myX = posMass[i*4+0];
- en: float myY = posMass[i*4+1];
  id: totrans-1869
  prefs: []
  type: TYPE_NORMAL
  zh: float myY = posMass[i*4+1];
- en: float myZ = posMass[i*4+2];
  id: totrans-1870
  prefs: []
  type: TYPE_NORMAL
  zh: float myZ = posMass[i*4+2];
- en: for ( size_t j = 0; j < N; j++ ) {
  id: totrans-1871
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t j = 0; j < N; j++ ) {
- en: float acc[3];
  id: totrans-1872
  prefs: []
  type: TYPE_NORMAL
  zh: float acc[3];
- en: float bodyX = posMass[j*4+0];
  id: totrans-1873
  prefs: []
  type: TYPE_NORMAL
  zh: float bodyX = posMass[j*4+0];
- en: float bodyY = posMass[j*4+1];
  id: totrans-1874
  prefs: []
  type: TYPE_NORMAL
  zh: float bodyY = posMass[j*4+1];
- en: float bodyZ = posMass[j*4+2];
  id: totrans-1875
  prefs: []
  type: TYPE_NORMAL
  zh: float bodyZ = posMass[j*4+2];
- en: float bodyMass = posMass[j*4+3];
  id: totrans-1876
  prefs: []
  type: TYPE_NORMAL
  zh: float bodyMass = posMass[j*4+3];
- en: bodyBodyInteraction<float>(
  id: totrans-1877
  prefs: []
  type: TYPE_NORMAL
  zh: bodyBodyInteraction<float>(
- en: ax, ay, az,
  id: totrans-1878
  prefs: []
  type: TYPE_NORMAL
  zh: ax, ay, az,
- en: myX, myY, myZ,
  id: totrans-1879
  prefs: []
  type: TYPE_NORMAL
  zh: myX, myY, myZ,
- en: bodyX, bodyY, bodyZ, bodyMass,
  id: totrans-1880
  prefs: []
  type: TYPE_NORMAL
  zh: bodyX, bodyY, bodyZ, bodyMass,
- en: softeningSquared );
  id: totrans-1881
  prefs: []
  type: TYPE_NORMAL
  zh: softeningSquared );
- en: ax += acc[0];
  id: totrans-1882
  prefs: []
  type: TYPE_NORMAL
  zh: ax += acc[0];
- en: ay += acc[1];
  id: totrans-1883
  prefs: []
  type: TYPE_NORMAL
  zh: ay += acc[1];
- en: az += acc[2];
  id: totrans-1884
  prefs: []
  type: TYPE_NORMAL
  zh: az += acc[2];
- en: '}'
  id: totrans-1885
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: force[3*i+0] = ax;
  id: totrans-1886
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+0] = ax;
- en: force[3*i+1] = ay;
  id: totrans-1887
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+1] = ay;
- en: force[3*i+2] = az;
  id: totrans-1888
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+2] = az;
- en: '}'
  id: totrans-1889
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: chTimerGetTime( &end );
  id: totrans-1890
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &end );
- en: return (float) chTimerElapsedTime( &start, &end ) * 1000.0f;
  id: totrans-1891
  prefs: []
  type: TYPE_NORMAL
  zh: return (float) chTimerElapsedTime( &start, &end ) * 1000.0f;
- en: '}'
  id: totrans-1892
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1893
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 14.3](ch14.html#ch14lis03) gives the GPU equivalent to [Listing 14.2](ch14.html#ch14lis02).
    For each body, it sums the accelerations due to every other body, then writes
    that value out to the force array. The L1 and L2 caches in SM 2.x and later GPUs
    accelerate this workload well, since there is a great deal of reuse in the innermost
    loop.'
  id: totrans-1894
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 14.3](ch14.html#ch14lis03) 给出了与[列表 14.2](ch14.html#ch14lis02)相对应的 GPU 版本。对于每个物体，它将所有其他物体的加速度求和，然后将该值写入力数组。SM
    2.x 及更高版本的 GPU 中的 L1 和 L2 缓存能够很好地加速此工作负载，因为在最内层的循环中有大量的重用。'
- en: 'Both the outer loop and the inner loop cast the input array `posMass` to `float4`
    to ensure that the compiler correctly emits a single 16-byte load instruction.
    Loop unrolling is an oft-cited optimization for N-Body calculations on GPUs, and
    it’s not hard to imagine why: Branch overhead is much higher on GPUs than CPUs,
    so the reduced instruction count per loop iteration has a bigger benefit, and
    the unrolled loop exposes more opportunities for ILP (instruction level parallelism),
    in which the GPU covers latency of instruction execution as well as memory latency.'
  id: totrans-1895
  prefs: []
  type: TYPE_NORMAL
  zh: 外层循环和内层循环都将输入数组`posMass`转换为`float4`，以确保编译器正确发出单个 16 字节的加载指令。循环展开是针对 GPU 上 N-Body
    计算的常见优化，而且不难理解为什么：GPU 上的分支开销比 CPU 高得多，因此每次循环迭代减少的指令数带来了更大的好处，展开的循环还揭示了更多的指令级并行（ILP）机会，其中
    GPU 覆盖了指令执行延迟以及内存延迟。
- en: To get the benefits of loop unrolling in our N-Body application, we need only
    insert the line
  id: totrans-1896
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在我们的 N-Body 应用程序中获得循环展开的好处，我们只需要插入这一行
- en: '#pragma unroll <factor>'
  id: totrans-1897
  prefs: []
  type: TYPE_NORMAL
  zh: '#pragma unroll <因子>'
- en: in front of the `for` loop over `j`. Unfortunately, the optimal loop unrolling
    factor must be determined empirically. [Table 14.1](ch14.html#ch14tab01) summarizes
    the effects of unrolling the loop in this kernel.
  id: totrans-1898
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `j` 的 `for` 循环前插入。遗憾的是，最佳的循环展开因子必须通过经验来确定。[表 14.1](ch14.html#ch14tab01) 总结了在此内核中展开循环的效果。
- en: '![Image](graphics/14tab01.jpg)'
  id: totrans-1899
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/14tab01.jpg)'
- en: '*Table 14.1* Loop Unrolling in the Naïve Kernel'
  id: totrans-1900
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 14.1* 原始内核中的循环展开'
- en: 'In the case of this kernel, in the absence of unrolling, it only delivers 25
    billion body-body interactions per second. Even an unroll factor of 2 increases
    this performance to 30 billion; increasing the unroll factor to 16 delivers the
    highest performance observed with this kernel: 34.3 billion body-body interactions
    per second, a 37% performance improvement.'
  id: totrans-1901
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个内核的情况下，如果没有展开，它每秒只能提供 250 亿次物体-物体交互。即使是展开因子为 2，也能将性能提高到 300 亿次；将展开因子增加到 16
    会提供此内核观察到的最高性能：每秒 343 亿次物体-物体交互，性能提升了 37%。
- en: '*Listing 14.3.* `ComputeNBodyGravitation_GPU_AOS`.'
  id: totrans-1902
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 14.3.* `ComputeNBodyGravitation_GPU_AOS`。'
- en: '[Click here to view code image](ch14_images.html#p14lis03a)'
  id: totrans-1903
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch14_images.html#p14lis03a)'
- en: '* * *'
  id: totrans-1904
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: template<typename T>
  id: totrans-1905
  prefs: []
  type: TYPE_NORMAL
  zh: template<typename T>
- en: __global__ void
  id: totrans-1906
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: ComputeNBodyGravitation_GPU_AOS(
  id: totrans-1907
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeNBodyGravitation_GPU_AOS(
- en: T *force,
  id: totrans-1908
  prefs: []
  type: TYPE_NORMAL
  zh: T *force,
- en: T *posMass,
  id: totrans-1909
  prefs: []
  type: TYPE_NORMAL
  zh: T *posMass,
- en: size_t N,
  id: totrans-1910
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N,
- en: T softeningSquared )
  id: totrans-1911
  prefs: []
  type: TYPE_NORMAL
  zh: T softeningSquared )
- en: '{'
  id: totrans-1912
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-1913
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
- en: i < N;
  id: totrans-1914
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x )
  id: totrans-1915
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x )
- en: '{'
  id: totrans-1916
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: T acc[3] = {0};
  id: totrans-1917
  prefs: []
  type: TYPE_NORMAL
  zh: T acc[3] = {0};
- en: float4 me = ((float4 *) posMass)[i];
  id: totrans-1918
  prefs: []
  type: TYPE_NORMAL
  zh: float4 me = ((float4 *) posMass)[i];
- en: T myX = me.x;
  id: totrans-1919
  prefs: []
  type: TYPE_NORMAL
  zh: T myX = me.x;
- en: T myY = me.y;
  id: totrans-1920
  prefs: []
  type: TYPE_NORMAL
  zh: T myY = me.y;
- en: T myZ = me.z;
  id: totrans-1921
  prefs: []
  type: TYPE_NORMAL
  zh: T myZ = me.z;
- en: for ( int j = 0; j < N; j++ ) {
  id: totrans-1922
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < N; j++ ) {
- en: float4 body = ((float4 *) posMass)[j];
  id: totrans-1923
  prefs: []
  type: TYPE_NORMAL
  zh: float4 body = ((float4 *) posMass)[j];
- en: float fx, fy, fz;
  id: totrans-1924
  prefs: []
  type: TYPE_NORMAL
  zh: float fx, fy, fz;
- en: bodyBodyInteraction(
  id: totrans-1925
  prefs: []
  type: TYPE_NORMAL
  zh: bodyBodyInteraction(
- en: '&fx, &fy, &fz,'
  id: totrans-1926
  prefs: []
  type: TYPE_NORMAL
  zh: '&fx, &fy, &fz,'
- en: myX, myY, myZ,
  id: totrans-1927
  prefs: []
  type: TYPE_NORMAL
  zh: myX, myY, myZ,
- en: body.x, body.y, body.z, body.w,
  id: totrans-1928
  prefs: []
  type: TYPE_NORMAL
  zh: body.x, body.y, body.z, body.w,
- en: softeningSquared);
  id: totrans-1929
  prefs: []
  type: TYPE_NORMAL
  zh: softeningSquared);
- en: acc[0] += fx;
  id: totrans-1930
  prefs: []
  type: TYPE_NORMAL
  zh: acc[0] += fx;
- en: acc[1] += fy;
  id: totrans-1931
  prefs: []
  type: TYPE_NORMAL
  zh: acc[1] += fy;
- en: acc[2] += fz;
  id: totrans-1932
  prefs: []
  type: TYPE_NORMAL
  zh: acc[2] += fz;
- en: '}'
  id: totrans-1933
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: force[3*i+0] = acc[0];
  id: totrans-1934
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+0] = acc[0];
- en: force[3*i+1] = acc[1];
  id: totrans-1935
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+1] = acc[1];
- en: force[3*i+2] = acc[2];
  id: totrans-1936
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+2] = acc[2];
- en: '}'
  id: totrans-1937
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1938
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1939
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 14.3\. Shared Memory
  id: totrans-1940
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.3\. 共享内存
- en: 'There is enough locality and reuse in the innermost loop of the N-Body calculation
    that caches work well without any involvement from the programmer; but on CUDA
    architectures, there is a benefit to using shared memory to explicitly cache the
    data^([6](ch14.html#ch14fn6)), as shown in [Listing 14-4](ch14.html#ch14lis04).
    The inner loop is *tiled* using two loops: an outer one that strides through the
    *N* bodies, a thread block at a time, loading shared memory, and an inner one
    that iterates through the body descriptions in shared memory. Shared memory always
    has been optimized to broadcast to threads within a warp if they are reading the
    same shared memory location, so this usage pattern is a good fit with the hardware
    architecture.'
  id: totrans-1941
  prefs: []
  type: TYPE_NORMAL
  zh: N-Body计算的最内层循环有足够的局部性和重用性，因此缓存能够很好地工作，而无需程序员的任何干预；但是在CUDA架构中，使用共享内存显式地缓存数据是有益的^([6](ch14.html#ch14fn6))，如[Listing
    14-4](ch14.html#ch14lis04)所示。内层循环使用两个循环进行“平铺”操作：外层循环按线程块逐步遍历*N*个体，加载共享内存；内层循环遍历共享内存中的体描述。共享内存始终经过优化，以便在warp内的线程读取相同共享内存位置时进行广播，因此这一使用模式与硬件架构非常契合。
- en: '[6](ch14.html#ch14fn6a). Shared memory is a must on SM 1.x architectures, which
    did not include caches. But it turns out to be a win on all CUDA architectures,
    albeit a slight one on SM 2.x and SM 3.x.'
  id: totrans-1942
  prefs: []
  type: TYPE_NORMAL
  zh: '[6](ch14.html#ch14fn6a). 在SM 1.x架构中，共享内存是必须的，因为这些架构没有包含缓存。但事实证明，在所有CUDA架构中使用共享内存都是有益的，尽管在SM
    2.x和SM 3.x架构上这一效果较为微弱。'
- en: This approach is the same one reported by Harris et al. that achieved the highest
    performance for large *N* and that approached the theoretical limits of the GPU’s
    performance.
  id: totrans-1943
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与Harris等人报告的相同，后者为大型*N*问题实现了最高性能，并接近GPU性能的理论极限。
- en: '*Listing 14.4.* `ComputeNBodyGravitation_Shared`.'
  id: totrans-1944
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 14.4.* `ComputeNBodyGravitation_Shared`。'
- en: '[Click here to view code image](ch14_images.html#p14lis04a)'
  id: totrans-1945
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch14_images.html#p14lis04a)'
- en: '* * *'
  id: totrans-1946
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-1947
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: ComputeNBodyGravitation_Shared(
  id: totrans-1948
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeNBodyGravitation_Shared(
- en: float *force,
  id: totrans-1949
  prefs: []
  type: TYPE_NORMAL
  zh: float *force,
- en: float *posMass,
  id: totrans-1950
  prefs: []
  type: TYPE_NORMAL
  zh: float *posMass,
- en: float softeningSquared,
  id: totrans-1951
  prefs: []
  type: TYPE_NORMAL
  zh: float softeningSquared,
- en: size_t N )
  id: totrans-1952
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N )
- en: '{'
  id: totrans-1953
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float4 *posMass4 = posMass;
  id: totrans-1954
  prefs: []
  type: TYPE_NORMAL
  zh: float4 *posMass4 = posMass;
- en: extern __shared__ float4 shPosMass[];
  id: totrans-1955
  prefs: []
  type: TYPE_NORMAL
  zh: extern __shared__ float4 shPosMass[];
- en: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-1956
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
- en: i < N;
  id: totrans-1957
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x )
  id: totrans-1958
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x )
- en: '{'
  id: totrans-1959
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float acc[3] = {0};
  id: totrans-1960
  prefs: []
  type: TYPE_NORMAL
  zh: float acc[3] = {0};
- en: float4 myPosMass = posMass4[i];
  id: totrans-1961
  prefs: []
  type: TYPE_NORMAL
  zh: float4 myPosMass = posMass4[i];
- en: '#pragma unroll 32'
  id: totrans-1962
  prefs: []
  type: TYPE_NORMAL
  zh: '#pragma unroll 32'
- en: for ( int j = 0; j < N; j += blockDim.x ) {
  id: totrans-1963
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < N; j += blockDim.x ) {
- en: shPosMass[threadIdx.x] = posMass4[j+threadIdx.x];
  id: totrans-1964
  prefs: []
  type: TYPE_NORMAL
  zh: shPosMass[threadIdx.x] = posMass4[j+threadIdx.x];
- en: __syncthreads();
  id: totrans-1965
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: for ( size_t k = 0; k < blockDim.x; k++ ) {
  id: totrans-1966
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t k = 0; k < blockDim.x; k++ ) {
- en: float fx, fy, fz;
  id: totrans-1967
  prefs: []
  type: TYPE_NORMAL
  zh: float fx, fy, fz;
- en: float4 bodyPosMass = shPosMass[k];
  id: totrans-1968
  prefs: []
  type: TYPE_NORMAL
  zh: float4 bodyPosMass = shPosMass[k];
- en: bodyBodyInteraction(
  id: totrans-1969
  prefs: []
  type: TYPE_NORMAL
  zh: bodyBodyInteraction(
- en: '&fx, &fy, &fz,'
  id: totrans-1970
  prefs: []
  type: TYPE_NORMAL
  zh: '&fx, &fy, &fz,'
- en: myPosMass.x, myPosMass.y, myPosMass.z,
  id: totrans-1971
  prefs: []
  type: TYPE_NORMAL
  zh: myPosMass.x, myPosMass.y, myPosMass.z,
- en: bodyPosMass.x,
  id: totrans-1972
  prefs: []
  type: TYPE_NORMAL
  zh: bodyPosMass.x,
- en: bodyPosMass.y,
  id: totrans-1973
  prefs: []
  type: TYPE_NORMAL
  zh: bodyPosMass.y,
- en: bodyPosMass.z,
  id: totrans-1974
  prefs: []
  type: TYPE_NORMAL
  zh: bodyPosMass.z,
- en: bodyPosMass.w,
  id: totrans-1975
  prefs: []
  type: TYPE_NORMAL
  zh: bodyPosMass.w,
- en: softeningSquared );
  id: totrans-1976
  prefs: []
  type: TYPE_NORMAL
  zh: softeningSquared );
- en: acc[0] += fx;
  id: totrans-1977
  prefs: []
  type: TYPE_NORMAL
  zh: acc[0] += fx;
- en: acc[1] += fy;
  id: totrans-1978
  prefs: []
  type: TYPE_NORMAL
  zh: acc[1] += fy;
- en: acc[2] += fz;
  id: totrans-1979
  prefs: []
  type: TYPE_NORMAL
  zh: acc[2] += fz;
- en: '}'
  id: totrans-1980
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-1981
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-1982
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: force[3*i+0] = acc[0];
  id: totrans-1983
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+0] = acc[0];
- en: force[3*i+1] = acc[1];
  id: totrans-1984
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+1] = acc[1];
- en: force[3*i+2] = acc[2];
  id: totrans-1985
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+2] = acc[2];
- en: '}'
  id: totrans-1986
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-1987
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-1988
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As with the previous kernel, loop unrolling delivers higher performance. [Table
    14.2](ch14.html#ch14tab02) summarizes the effects of loop unrolling in the shared
    memory implementation. The optimal unroll factor of 4 delivers 18% higher performance.
  id: totrans-1989
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的内核一样，循环展开提高了性能。[表 14.2](ch14.html#ch14tab02)总结了在共享内存实现中循环展开的效果。最佳的展开因子为
    4，可以提高 18% 的性能。
- en: '![Image](graphics/14tab02.jpg)'
  id: totrans-1990
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/14tab02.jpg)'
- en: '*Table 14.2* Loop Unrolling in the Shared Memory Kernel'
  id: totrans-1991
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 14.2* 在共享内存内核中的循环展开'
- en: 14.4\. Constant Memory
  id: totrans-1992
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.4\. 常量内存
- en: Stone et al. describe a method of Direct Coulomb Summation (DCS) that uses shared
    memory to hold potential map lattice points for a molecular modeling application^([7](ch14.html#ch14fn7))
    so it must use constant memory to hold body descriptions. [Listing 14.5](ch14.html#ch14lis05)
    shows a CUDA kernel that uses the same method for our gravitational simulation.
    Since only 64K of constant memory is available to developers for a given kernel,
    each kernel invocation can only process about 4000 16-byte body descriptions.
    The constant `g_bodiesPerPass` specifies the number of bodies that can be considered
    by the innermost loop.
  id: totrans-1993
  prefs: []
  type: TYPE_NORMAL
  zh: Stone 等人描述了一种使用共享内存存储分子建模应用中潜力地图格点的直接库仑求和（DCS）方法^([7](ch14.html#ch14fn7))，因此必须使用常量内存来保存物体描述。[列表
    14.5](ch14.html#ch14lis05)展示了一个使用相同方法进行重力模拟的 CUDA 内核。由于每个内核只有 64K 的常量内存可用，每次内核调用最多只能处理约
    4000 个 16 字节的物体描述。常量 `g_bodiesPerPass` 指定了内层循环中可以处理的物体数量。
- en: '[7](ch14.html#ch14fn7a). [www.ncbi.nlm.nih.gov/pubmed/17894371](http://www.ncbi.nlm.nih.gov/pubmed/17894371)'
  id: totrans-1994
  prefs: []
  type: TYPE_NORMAL
  zh: '[7](ch14.html#ch14fn7a). [www.ncbi.nlm.nih.gov/pubmed/17894371](http://www.ncbi.nlm.nih.gov/pubmed/17894371)'
- en: Since every thread in the innermost loop is reading the same body description,
    constant memory works well because it is optimized to broadcast reads to all threads
    in a warp.
  id: totrans-1995
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内层循环中的每个线程都读取相同的物体描述，常量内存非常有效，因为它被优化为将读取广播到 warp 中的所有线程。
- en: '*Listing 14.5.* N-Body (constant memory).'
  id: totrans-1996
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 14.5* N-Body（常量内存）。'
- en: '[Click here to view code image](ch14_images.html#p14lis05a)'
  id: totrans-1997
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch14_images.html#p14lis05a)'
- en: '* * *'
  id: totrans-1998
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: const int g_bodiesPerPass = 4000;
  id: totrans-1999
  prefs: []
  type: TYPE_NORMAL
  zh: const int g_bodiesPerPass = 4000;
- en: __constant__ __device__ float4 g_constantBodies[g_bodiesPerPass];
  id: totrans-2000
  prefs: []
  type: TYPE_NORMAL
  zh: __constant__ __device__ float4 g_constantBodies[g_bodiesPerPass];
- en: template<typename T>
  id: totrans-2001
  prefs: []
  type: TYPE_NORMAL
  zh: template<typename T>
- en: __global__ void
  id: totrans-2002
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: ComputeNBodyGravitation_GPU_AOS_const(
  id: totrans-2003
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeNBodyGravitation_GPU_AOS_const(
- en: T *force,
  id: totrans-2004
  prefs: []
  type: TYPE_NORMAL
  zh: T *force,
- en: T *posMass,
  id: totrans-2005
  prefs: []
  type: TYPE_NORMAL
  zh: T *posMass,
- en: T softeningSquared,
  id: totrans-2006
  prefs: []
  type: TYPE_NORMAL
  zh: T softeningSquared,
- en: size_t n,
  id: totrans-2007
  prefs: []
  type: TYPE_NORMAL
  zh: size_t n,
- en: size_t N )
  id: totrans-2008
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N )
- en: '{'
  id: totrans-2009
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-2010
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
- en: i < N;
  id: totrans-2011
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x )
  id: totrans-2012
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x )
- en: '{'
  id: totrans-2013
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: T acc[3] = {0};
  id: totrans-2014
  prefs: []
  type: TYPE_NORMAL
  zh: T acc[3] = {0};
- en: float4 me = ((float4 *) posMass)[i];
  id: totrans-2015
  prefs: []
  type: TYPE_NORMAL
  zh: float4 me = ((float4 *) posMass)[i];
- en: T myX = me.x;
  id: totrans-2016
  prefs: []
  type: TYPE_NORMAL
  zh: T myX = me.x;
- en: T myY = me.y;
  id: totrans-2017
  prefs: []
  type: TYPE_NORMAL
  zh: T myY = me.y;
- en: T myZ = me.z;
  id: totrans-2018
  prefs: []
  type: TYPE_NORMAL
  zh: T myZ = me.z;
- en: for ( int j = 0; j < n; j++ ) {
  id: totrans-2019
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < n; j++ ) {
- en: float4 body = g_constantBodies[j];
  id: totrans-2020
  prefs: []
  type: TYPE_NORMAL
  zh: float4 body = g_constantBodies[j];
- en: float fx, fy, fz;
  id: totrans-2021
  prefs: []
  type: TYPE_NORMAL
  zh: float fx, fy, fz;
- en: bodyBodyInteraction(
  id: totrans-2022
  prefs: []
  type: TYPE_NORMAL
  zh: bodyBodyInteraction(
- en: '&fx, &fy, &fz,'
  id: totrans-2023
  prefs: []
  type: TYPE_NORMAL
  zh: '&fx, &fy, &fz,'
- en: myX, myY, myZ,
  id: totrans-2024
  prefs: []
  type: TYPE_NORMAL
  zh: myX, myY, myZ,
- en: body.x, body.y, body.z, body.w,
  id: totrans-2025
  prefs: []
  type: TYPE_NORMAL
  zh: body.x, body.y, body.z, body.w,
- en: softeningSquared);
  id: totrans-2026
  prefs: []
  type: TYPE_NORMAL
  zh: softeningSquared);
- en: acc[0] += fx;
  id: totrans-2027
  prefs: []
  type: TYPE_NORMAL
  zh: acc[0] += fx;
- en: acc[1] += fy;
  id: totrans-2028
  prefs: []
  type: TYPE_NORMAL
  zh: acc[1] += fy;
- en: acc[2] += fz;
  id: totrans-2029
  prefs: []
  type: TYPE_NORMAL
  zh: acc[2] += fz;
- en: '}'
  id: totrans-2030
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: force[3*i+0] += acc[0];
  id: totrans-2031
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+0] += acc[0];
- en: force[3*i+1] += acc[1];
  id: totrans-2032
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+1] += acc[1];
- en: force[3*i+2] += acc[2];
  id: totrans-2033
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+2] += acc[2];
- en: '}'
  id: totrans-2034
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2035
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2036
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: As shown in [Listing 14.6](ch14.html#ch14lis06), the host code must loop over
    the bodies, calling `cudaMemcpyToSymbolAsync()` to load the constant memory before
    each kernel invocation.
  id: totrans-2037
  prefs: []
  type: TYPE_NORMAL
  zh: 如[列表 14.6](ch14.html#ch14lis06)所示，主机代码必须循环遍历各个天体，在每次内核调用前使用`cudaMemcpyToSymbolAsync()`加载常量内存。
- en: '*Listing 14.6.* Host code (constant memory N-Body).'
  id: totrans-2038
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 14.6.* 主机代码（常量内存N体）。'
- en: '[Click here to view code image](ch14_images.html#p14lis06a)'
  id: totrans-2039
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch14_images.html#p14lis06a)'
- en: '* * *'
  id: totrans-2040
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: float
  id: totrans-2041
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: ComputeNBodyGravitation_GPU_AOS_const(
  id: totrans-2042
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeNBodyGravitation_GPU_AOS_const(
- en: float *force,
  id: totrans-2043
  prefs: []
  type: TYPE_NORMAL
  zh: float *force,
- en: float *posMass,
  id: totrans-2044
  prefs: []
  type: TYPE_NORMAL
  zh: float *posMass,
- en: float softeningSquared,
  id: totrans-2045
  prefs: []
  type: TYPE_NORMAL
  zh: float softeningSquared,
- en: size_t N
  id: totrans-2046
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N
- en: )
  id: totrans-2047
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '{'
  id: totrans-2048
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-2049
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: cudaEvent_t evStart = 0, evStop = 0;
  id: totrans-2050
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEvent_t evStart = 0, evStop = 0;
- en: float ms = 0.0;
  id: totrans-2051
  prefs: []
  type: TYPE_NORMAL
  zh: float ms = 0.0;
- en: size_t bodiesLeft = N;
  id: totrans-2052
  prefs: []
  type: TYPE_NORMAL
  zh: size_t bodiesLeft = N;
- en: void *p;
  id: totrans-2053
  prefs: []
  type: TYPE_NORMAL
  zh: void *p;
- en: CUDART_CHECK( cudaGetSymbolAddress( &p, g_constantBodies ) );
  id: totrans-2054
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaGetSymbolAddress( &p, g_constantBodies ) );
- en: CUDART_CHECK( cudaEventCreate( &evStart ) );
  id: totrans-2055
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventCreate( &evStart ) );
- en: CUDART_CHECK( cudaEventCreate( &evStop ) );
  id: totrans-2056
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventCreate( &evStop ) );
- en: CUDART_CHECK( cudaEventRecord( evStart, NULL ) );
  id: totrans-2057
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( evStart, NULL ) );
- en: for ( size_t i = 0; i < N; i += g_bodiesPerPass ) {
  id: totrans-2058
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = 0; i < N; i += g_bodiesPerPass ) {
- en: // bodiesThisPass = max(bodiesLeft, g_bodiesPerPass);
  id: totrans-2059
  prefs: []
  type: TYPE_NORMAL
  zh: // bodiesThisPass = max(bodiesLeft, g_bodiesPerPass);
- en: size_t bodiesThisPass = bodiesLeft;
  id: totrans-2060
  prefs: []
  type: TYPE_NORMAL
  zh: size_t bodiesThisPass = bodiesLeft;
- en: if ( bodiesThisPass > g_bodiesPerPass ) {
  id: totrans-2061
  prefs: []
  type: TYPE_NORMAL
  zh: if ( bodiesThisPass > g_bodiesPerPass ) {
- en: bodiesThisPass = g_bodiesPerPass;
  id: totrans-2062
  prefs: []
  type: TYPE_NORMAL
  zh: bodiesThisPass = g_bodiesPerPass;
- en: '}'
  id: totrans-2063
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: CUDART_CHECK( cudaMemcpyToSymbolAsync(
  id: totrans-2064
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaMemcpyToSymbolAsync(
- en: g_constantBodies,
  id: totrans-2065
  prefs: []
  type: TYPE_NORMAL
  zh: g_constantBodies,
- en: ((float4 *) posMass)+i,
  id: totrans-2066
  prefs: []
  type: TYPE_NORMAL
  zh: ((float4 *) posMass)+i,
- en: bodiesThisPass*sizeof(float4),
  id: totrans-2067
  prefs: []
  type: TYPE_NORMAL
  zh: bodiesThisPass*sizeof(float4),
- en: 0,
  id: totrans-2068
  prefs: []
  type: TYPE_NORMAL
  zh: 0,
- en: cudaMemcpyDeviceToDevice,
  id: totrans-2069
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToDevice,
- en: NULL ) );
  id: totrans-2070
  prefs: []
  type: TYPE_NORMAL
  zh: NULL ) );
- en: ComputeNBodyGravitation_GPU_AOS_const<float> <<<300,256>>>(
  id: totrans-2071
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeNBodyGravitation_GPU_AOS_const<float> <<<300,256>>>(
- en: force, posMass, softeningSquared, bodiesThisPass, N );
  id: totrans-2072
  prefs: []
  type: TYPE_NORMAL
  zh: force, posMass, softeningSquared, bodiesThisPass, N );
- en: bodiesLeft -= bodiesThisPass;
  id: totrans-2073
  prefs: []
  type: TYPE_NORMAL
  zh: bodiesLeft -= bodiesThisPass;
- en: '}'
  id: totrans-2074
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: CUDART_CHECK( cudaEventRecord( evStop, NULL ) );
  id: totrans-2075
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventRecord( evStop, NULL ) );
- en: CUDART_CHECK( cudaDeviceSynchronize() );
  id: totrans-2076
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaDeviceSynchronize() );
- en: CUDART_CHECK( cudaEventElapsedTime( &ms, evStart, evStop ) );
  id: totrans-2077
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaEventElapsedTime( &ms, evStart, evStop ) );
- en: 'Error:'
  id: totrans-2078
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: cudaEventDestroy( evStop );
  id: totrans-2079
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventDestroy( evStop );
- en: cudaEventDestroy( evStart );
  id: totrans-2080
  prefs: []
  type: TYPE_NORMAL
  zh: cudaEventDestroy( evStart );
- en: return ms;
  id: totrans-2081
  prefs: []
  type: TYPE_NORMAL
  zh: return ms;
- en: '}'
  id: totrans-2082
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2083
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 14.5\. Warp Shuffle
  id: totrans-2084
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.5\. Warp Shuffle
- en: SM 3.x added a warp shuffle instruction (described in [Section 8.6.1](ch08.html#ch08lev2sec20))
    that enables threads to interchange data between registers without writing the
    data to shared memory. The `__shfl()` intrinsic can be used to broadcast one thread’s
    register value to all other threads in the warp. As shown in [Listing 14.4](ch14.html#ch14lis04),
    instead of using tiles sized to the threadblock and using shared memory, we can
    use tiles of size 32 (corresponding to the warp size) and broadcast the body description
    read by each thread to the other threads within the warp.
  id: totrans-2085
  prefs: []
  type: TYPE_NORMAL
  zh: SM 3.x 添加了一个 warp shuffle 指令（在[第 8.6.1 节](ch08.html#ch08lev2sec20)中描述），它使线程能够在不将数据写入共享内存的情况下，在寄存器之间交换数据。`__shfl()`
    内建函数可以用来将一个线程的寄存器值广播到 warp 中的所有其他线程。如[清单 14.4](ch14.html#ch14lis04)所示， 我们可以使用大小为
    32 的瓦片（对应 warp 大小），并将每个线程读取的体描述广播给 warp 中的其他线程，而不是使用与线程块大小匹配的瓦片并使用共享内存。
- en: Interestingly, this strategy has 25% lower performance than the shared memory
    implementation (34 billion as opposed to 45.2 billion interactions per second).
    The warp shuffle instruction takes about as long as a read from shared memory,
    and the computation is tiled at the warp size (32 threads) rather than a thread
    block size. So it seems the benefits of warp shuffle are best realized when replacing
    both a write and a read to shared memory, not just a read. Warp shuffle should
    only be used if the kernel needs shared memory for other purposes.
  id: totrans-2086
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这种策略的性能比共享内存实现低 25%（每秒 34 亿次交互，相比之下共享内存实现为 45.2 亿次交互）。warp shuffle 指令的执行时间大约与读取共享内存相当，且计算是以
    warp 大小（32 线程）而非线程块大小进行瓦片化的。因此，warp shuffle 的好处似乎在于替换共享内存的读写操作，而不仅仅是读取操作。只有当内核需要共享内存用于其他用途时，才应使用
    warp shuffle。
- en: '*Listing 14.7.* `ComputeNBodyGravitation_Shuffle`.'
  id: totrans-2087
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 14.7.* `ComputeNBodyGravitation_Shuffle`。'
- en: '[Click here to view code image](ch14_images.html#p14lis07a)'
  id: totrans-2088
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch14_images.html#p14lis07a)'
- en: '* * *'
  id: totrans-2089
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-2090
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: ComputeNBodyGravitation_Shuffle(
  id: totrans-2091
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeNBodyGravitation_Shuffle(
- en: float *force,
  id: totrans-2092
  prefs: []
  type: TYPE_NORMAL
  zh: float *force,
- en: float *posMass,
  id: totrans-2093
  prefs: []
  type: TYPE_NORMAL
  zh: float *posMass,
- en: float softeningSquared,
  id: totrans-2094
  prefs: []
  type: TYPE_NORMAL
  zh: float softeningSquared,
- en: size_t N )
  id: totrans-2095
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N )
- en: '{'
  id: totrans-2096
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const int laneid = threadIdx.x & 31;
  id: totrans-2097
  prefs: []
  type: TYPE_NORMAL
  zh: const int laneid = threadIdx.x & 31;
- en: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-2098
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
- en: i < N;
  id: totrans-2099
  prefs: []
  type: TYPE_NORMAL
  zh: i < N;
- en: i += blockDim.x*gridDim.x )
  id: totrans-2100
  prefs: []
  type: TYPE_NORMAL
  zh: i += blockDim.x*gridDim.x )
- en: '{'
  id: totrans-2101
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float acc[3] = {0};
  id: totrans-2102
  prefs: []
  type: TYPE_NORMAL
  zh: float acc[3] = {0};
- en: float4 myPosMass = ((float4 *) posMass)[i];
  id: totrans-2103
  prefs: []
  type: TYPE_NORMAL
  zh: float4 myPosMass = ((float4 *) posMass)[i];
- en: for ( int j = 0; j < N; j += 32 ) {
  id: totrans-2104
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < N; j += 32 ) {
- en: float4 shufSrcPosMass = ((float4 *) posMass)[j+laneid];
  id: totrans-2105
  prefs: []
  type: TYPE_NORMAL
  zh: float4 shufSrcPosMass = ((float4 *) posMass)[j+laneid];
- en: '#pragma unroll 32'
  id: totrans-2106
  prefs: []
  type: TYPE_NORMAL
  zh: '#pragma unroll 32'
- en: for ( int k = 0; k < 32; k++ ) {
  id: totrans-2107
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int k = 0; k < 32; k++ ) {
- en: float fx, fy, fz;
  id: totrans-2108
  prefs: []
  type: TYPE_NORMAL
  zh: float fx, fy, fz;
- en: float4 shufDstPosMass;
  id: totrans-2109
  prefs: []
  type: TYPE_NORMAL
  zh: float4 shufDstPosMass;
- en: shufDstPosMass.x = __shfl( shufSrcPosMass.x, k );
  id: totrans-2110
  prefs: []
  type: TYPE_NORMAL
  zh: shufDstPosMass.x = __shfl( shufSrcPosMass.x, k );
- en: shufDstPosMass.y = __shfl( shufSrcPosMass.y, k );
  id: totrans-2111
  prefs: []
  type: TYPE_NORMAL
  zh: shufDstPosMass.y = __shfl( shufSrcPosMass.y, k );
- en: shufDstPosMass.z = __shfl( shufSrcPosMass.z, k );
  id: totrans-2112
  prefs: []
  type: TYPE_NORMAL
  zh: shufDstPosMass.z = __shfl( shufSrcPosMass.z, k );
- en: shufDstPosMass.w = __shfl( shufSrcPosMass.w, k );
  id: totrans-2113
  prefs: []
  type: TYPE_NORMAL
  zh: shufDstPosMass.w = __shfl( shufSrcPosMass.w, k );
- en: bodyBodyInteraction(
  id: totrans-2114
  prefs: []
  type: TYPE_NORMAL
  zh: bodyBodyInteraction(
- en: '&fx, &fy, &fz,'
  id: totrans-2115
  prefs: []
  type: TYPE_NORMAL
  zh: '&fx, &fy, &fz,'
- en: myPosMass.x, myPosMass.y, myPosMass.z,
  id: totrans-2116
  prefs: []
  type: TYPE_NORMAL
  zh: myPosMass.x, myPosMass.y, myPosMass.z,
- en: shufDstPosMass.x,
  id: totrans-2117
  prefs: []
  type: TYPE_NORMAL
  zh: shufDstPosMass.x,
- en: shufDstPosMass.y,
  id: totrans-2118
  prefs: []
  type: TYPE_NORMAL
  zh: shufDstPosMass.y,
- en: shufDstPosMass.z,
  id: totrans-2119
  prefs: []
  type: TYPE_NORMAL
  zh: shufDstPosMass.z,
- en: shufDstPosMass.w,
  id: totrans-2120
  prefs: []
  type: TYPE_NORMAL
  zh: shufDstPosMass.w,
- en: softeningSquared);
  id: totrans-2121
  prefs: []
  type: TYPE_NORMAL
  zh: softeningSquared);
- en: acc[0] += fx;
  id: totrans-2122
  prefs: []
  type: TYPE_NORMAL
  zh: acc[0] += fx;
- en: acc[1] += fy;
  id: totrans-2123
  prefs: []
  type: TYPE_NORMAL
  zh: acc[1] += fy;
- en: acc[2] += fz;
  id: totrans-2124
  prefs: []
  type: TYPE_NORMAL
  zh: acc[2] += fz;
- en: '}'
  id: totrans-2125
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2126
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: force[3*i+0] = acc[0];
  id: totrans-2127
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+0] = acc[0];
- en: force[3*i+1] = acc[1];
  id: totrans-2128
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+1] = acc[1];
- en: force[3*i+2] = acc[2];
  id: totrans-2129
  prefs: []
  type: TYPE_NORMAL
  zh: force[3*i+2] = acc[2];
- en: '}'
  id: totrans-2130
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2131
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2132
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 14.6\. Multiple GPUs and Scalability
  id: totrans-2133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.6\. 多GPU和可扩展性
- en: Because the computational density is so high, N-Body scales well across multiple
    GPUs. Portable pinned memory is used to hold the body descriptions so they can
    easily be referenced by all GPUs in the system. For a system containing *k* GPUs,
    each GPU is assigned *N*/*k* forces to compute.^([8](ch14.html#ch14fn8)) Our multi-GPU
    implementation of N-Body is featured in [Chapter 9](ch09.html#ch09). The rows
    are evenly divided among GPUs, the input data is broadcast to all GPUs via portable
    pinned memory, and each GPU computes its output independently. CUDA applications
    that use multiple GPUs can be multithreaded or single-threaded. [Chapter 9](ch09.html#ch09)
    includes optimized N-Body implementations that illustrate both approaches.
  id: totrans-2134
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算密度非常高，N-Body 在多个GPU之间具有良好的扩展性。便携式固定内存用于存储物体描述，以便所有GPU都能轻松访问。对于一个包含*k*个GPU的系统，每个GPU被分配*N*/*k*个力进行计算。^([8](ch14.html#ch14fn8))
    我们的多GPU实现的N-Body算法在[第9章](ch09.html#ch09)中进行了介绍。各行数据在GPU之间均匀分配，输入数据通过便携式固定内存广播到所有GPU，每个GPU独立计算其输出。使用多个GPU的CUDA应用可以是多线程或单线程的。[第9章](ch09.html#ch09)包含了优化过的N-Body实现，展示了这两种方法。
- en: '[8](ch14.html#ch14fn8a). Our implementation requires that *N* be evenly divisible
    by *k*.'
  id: totrans-2135
  prefs: []
  type: TYPE_NORMAL
  zh: '[8](ch14.html#ch14fn8a). 我们的实现要求*N*能被*k*整除。'
- en: For N-Body, the single- and multithreaded implementations have the same performance,
    since there is little work for the CPU to do. [Table 14.3](ch14.html#ch14tab03)
    summarizes the scalability of the multithreaded implementation for a problem size
    of 96K bodies and up to 4 GPUs. The efficiency is the percentage of measured performance
    as compared to perfect scaling. There is room for improvement over this result,
    since the performance results reported here include allocation and freeing of
    device memory on each GPU for each timestep.
  id: totrans-2136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于N-Body，单线程和多线程的实现具有相同的性能，因为CPU的工作量很小。[表14.3](ch14.html#ch14tab03)总结了96K个物体在多达4个GPU下的多线程实现的可扩展性。效率是与完美扩展相比的性能百分比。由于这里报告的性能结果包括在每个时间步上为每个GPU分配和释放设备内存，因此该结果还有改进的空间。
- en: '![Image](graphics/14tab03.jpg)'
  id: totrans-2137
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/14tab03.jpg)'
- en: '*Table 14.3* N-Body Scalability'
  id: totrans-2138
  prefs: []
  type: TYPE_NORMAL
  zh: '*表14.3* N-Body 可扩展性'
- en: 14.7\. CPU Optimizations
  id: totrans-2139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.7\. CPU优化
- en: Papers on CUDA ports often compare against CPU implementations that are not
    optimized for highest performance. Although CUDA hardware generally is faster
    than CPUs at the workloads described in these papers, the reported speedup is
    often higher than it would be if the CPU implementation had been optimized properly.
  id: totrans-2140
  prefs: []
  type: TYPE_NORMAL
  zh: 许多关于 CUDA 移植的论文通常会与那些没有经过最高性能优化的 CPU 实现进行比较。尽管 CUDA 硬件在这些论文中描述的工作负载下通常比 CPU
    更快，但报告的加速比往往高于如果 CPU 实现得到了适当优化时的情况。
- en: To gain some insight into the tradeoffs between CUDA and modern CPU optimizations,
    we optimized the N-Body computation using two key strategies that are necessary
    for multicore CPUs to achieve peak performance.
  id: totrans-2141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 CUDA 与现代 CPU 优化之间的权衡，我们使用了两种关键策略来优化 N-Body 计算，这些策略对于多核 CPU 达到峰值性能是必要的。
- en: • SIMD (“single instruction multiple data”) instructions can perform multiple
    single-precision floating-point operations in a single instruction.
  id: totrans-2142
  prefs: []
  type: TYPE_NORMAL
  zh: • SIMD（“单指令多数据”）指令可以在一条指令中执行多个单精度浮点运算。
- en: • Multithreading achieves near-linear speedups in the number of execution cores
    available in the CPU. Multicore CPUs have been widely available since 2006, and
    N-Body computations are expected to scale almost linearly in the number of cores.
  id: totrans-2143
  prefs: []
  type: TYPE_NORMAL
  zh: • 多线程能够在可用的 CPU 执行核心数目上实现接近线性的加速。自 2006 年以来，多核 CPU 已广泛普及，N-Body 计算预计将在核心数目上几乎线性扩展。
- en: Since N-Body computations have such high computational density, we will not
    concern ourselves with affinity (for example, trying to use NUMA APIs to associate
    memory buffers with certain CPUs). There is so much reuse in this computation
    that caches in the CPU keep external memory traffic to a trickle.
  id: totrans-2144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 N-Body 计算具有非常高的计算密度，我们不会关注亲和性（例如，尝试使用 NUMA API 将内存缓冲区与特定的 CPU 关联）。在此计算中有大量重用，CPU
    缓存将外部内存流量限制到极低的水平。
- en: The Streaming SIMD Extensions (SSE) instructions were added to Intel’s x86 architecture
    in the late 1990s, starting with the Pentium III. They added a set of eight 128-bit
    XMM registers that could operate on four packed 32-bit floating-point values.^([9](ch14.html#ch14fn9))
    For example, the ADDPS instruction performs four floating-point additions in parallel
    on corresponding packed floats in XMM registers.
  id: totrans-2145
  prefs: []
  type: TYPE_NORMAL
  zh: 流式 SIMD 扩展（SSE）指令在 1990 年代末期被添加到 Intel 的 x86 架构中，从 Pentium III 开始。它们增加了一组八个
    128 位的 XMM 寄存器，可以在四个打包的 32 位浮点值上执行操作。^([9](ch14.html#ch14fn9)) 例如，ADDPS 指令可以并行地对
    XMM 寄存器中相应的打包浮点数执行四次浮点加法。
- en: '[9](ch14.html#ch14fn9a). Intel later added instructions that could consider
    the XMM registers as packed integers (up to 16 bytes) or two packed double-precision
    floating-point values, but we do not use any of those features. We also do not
    use the AVX (“Advanced Vector Extensions”) instruction set. AVX features registers
    and instructions that support SIMD operations that are twice as wide (256-bit),
    so it potentially could double performance.'
  id: totrans-2146
  prefs: []
  type: TYPE_NORMAL
  zh: '[9](ch14.html#ch14fn9a)。后来，英特尔添加了可以将XMM寄存器视为打包整数（最多16字节）或两个打包的双精度浮点值的指令，但我们并未使用这些功能。我们也没有使用AVX（“高级向量扩展”）指令集。AVX特性包括支持SIMD操作的寄存器和指令，宽度是SSE的两倍（256位），因此它可能使性能翻倍。'
- en: 'When porting N-Body to the SSE instruction set, the AOS (array of structures)
    memory layout that we have been using becomes problematic. Although the body descriptions
    are 16 bytes, just like XMM registers, the instruction set requires us to rearrange
    the data such that the X, Y, Z, and Mass components are packed into separate registers.
    Rather than perform this operation when computing the body-body interactions,
    we rearrange the memory layout as structure of arrays: Instead of a single array
    of `float4` (each element being the X, Y, Z, and Mass values for a given body),
    we use four arrays of `float`, with an array of X values, an array of Y values,
    and so on. With the data rearranged in this way, four bodies’ descriptions can
    be loaded into XMM registers with just 4 machine instructions; the difference
    vectors between four bodies’ positions can be computed with just 3 `SUBPS` instructions;
    and so on.'
  id: totrans-2147
  prefs: []
  type: TYPE_NORMAL
  zh: 在将N-Body移植到SSE指令集时，我们一直使用的AOS（结构数组）内存布局变得有问题。尽管物体描述是16字节，就像XMM寄存器一样，但指令集要求我们重新排列数据，使得X、Y、Z和质量组件分别打包到不同的寄存器中。我们不在计算物体间交互时执行此操作，而是将内存布局重组为数组结构：我们不使用单一的`float4`数组（每个元素包含给定物体的X、Y、Z和质量值），而是使用四个`float`数组，分别存储X值数组、Y值数组等等。数据按这种方式重新排列后，四个物体的描述可以通过仅仅4条机器指令加载到XMM寄存器中；四个物体位置之间的差向量可以通过仅仅3条`SUBPS`指令计算出来；等等。
- en: To simplify SSE coding, Intel has worked with compiler vendors to add cross-platform
    support for the SSE instruction set. A special data type `__m128` corresponds
    to the 128-bit register and operand size and intrinsic functions such as `_mm_sub_ps()`
    that correspond to the SUBPS instruction.
  id: totrans-2148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化SSE编码，英特尔与编译器供应商合作，添加了对SSE指令集的跨平台支持。一个特殊的数据类型`__m128`对应于128位寄存器和操作数大小，以及诸如`_mm_sub_ps()`的内联函数，它们对应于SUBPS指令。
- en: For purposes of our N-Body implementation, we also need a full-precision reciprocal
    square root implementation. The SSE instruction set has an instruction RSQRTPS
    that computes an approximation of the reciprocal square root, but its 12-bit estimate
    must be refined by a Newton-Raphson iteration to achieve full float precision.^([10](ch14.html#ch14fn10))
  id: totrans-2149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我们的N体模拟，我们还需要一个高精度的倒数平方根实现。SSE指令集提供了一个RSQRTPS指令，用于计算倒数平方根的近似值，但其12位估算值必须通过牛顿-拉夫森迭代来细化，以达到完整的浮点精度。^([10](ch14.html#ch14fn10))
- en: '[10](ch14.html#ch14fn10a). This code is not present in the SSE compiler support
    and is surprisingly difficult to find. Our implementation is from [http://nume.googlecode.com/svn/trunk/fosh/src/sse_approx.h](http://nume.googlecode.com/svn/trunk/fosh/src/sse_approx.h).'
  id: totrans-2150
  prefs: []
  type: TYPE_NORMAL
  zh: '[10](ch14.html#ch14fn10a)。这个代码在SSE编译器支持中并不存在，且令人惊讶地很难找到。我们的实现来自于[http://nume.googlecode.com/svn/trunk/fosh/src/sse_approx.h](http://nume.googlecode.com/svn/trunk/fosh/src/sse_approx.h)。'
- en: '![Image](graphics/440equ01.jpg)'
  id: totrans-2151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/440equ01.jpg)'
- en: '[Listing 14.8](ch14.html#ch14lis08) gives an SSE implementation of the body-body
    computation that takes the 2 bodies’ descriptions as `__m128` variables, computes
    the 4 body-body forces in parallel, and passes back the 3 resulting force vectors.
    [Listing 14.8](ch14.html#ch14lis08) is functionally equivalent to [Listings 14.1](ch14.html#ch14lis01)
    and [14.2](ch14.html#ch14lis02), though markedly less readable. Note that the
    `x0`, `y0`, and `z0` variables contain descriptions of the same body, replicated
    across the `__m128` variable four times.'
  id: totrans-2152
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 14.8](ch14.html#ch14lis08) 给出了一个SSE版本的体-体计算实现，它将两个物体的描述作为`__m128`变量，平行计算四个体-体力，并返回三个结果力向量。[Listing
    14.8](ch14.html#ch14lis08) 在功能上等价于[Listings 14.1](ch14.html#ch14lis01)和[14.2](ch14.html#ch14lis02)，尽管可读性明显较差。请注意，`x0`、`y0`和`z0`变量包含相同物体的描述，这些描述被四次复制到`__m128`变量中。'
- en: '*Listing 14.8.* Body-body interaction (SSE version).'
  id: totrans-2153
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 14.8.* 体-体交互（SSE版本）。'
- en: '[Click here to view code image](ch14_images.html#p14lis08a)'
  id: totrans-2154
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch14_images.html#p14lis08a)'
- en: '* * *'
  id: totrans-2155
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: static inline __m128
  id: totrans-2156
  prefs: []
  type: TYPE_NORMAL
  zh: static inline __m128
- en: rcp_sqrt_nr_ps(const __m128 x)
  id: totrans-2157
  prefs: []
  type: TYPE_NORMAL
  zh: rcp_sqrt_nr_ps(const __m128 x)
- en: '{'
  id: totrans-2158
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const __m128
  id: totrans-2159
  prefs: []
  type: TYPE_NORMAL
  zh: const __m128
- en: nr      = _mm_rsqrt_ps(x),
  id: totrans-2160
  prefs: []
  type: TYPE_NORMAL
  zh: nr      = _mm_rsqrt_ps(x),
- en: muls    = _mm_mul_ps(_mm_mul_ps(nr, nr), x),
  id: totrans-2161
  prefs: []
  type: TYPE_NORMAL
  zh: muls    = _mm_mul_ps(_mm_mul_ps(nr, nr), x),
- en: beta    = _mm_mul_ps(_mm_set_ps1(0.5f), nr),
  id: totrans-2162
  prefs: []
  type: TYPE_NORMAL
  zh: beta    = _mm_mul_ps(_mm_set_ps1(0.5f), nr),
- en: gamma   = _mm_sub_ps(_mm_set_ps1(3.0f), muls);
  id: totrans-2163
  prefs: []
  type: TYPE_NORMAL
  zh: gamma   = _mm_sub_ps(_mm_set_ps1(3.0f), muls);
- en: return _mm_mul_ps(beta, gamma);
  id: totrans-2164
  prefs: []
  type: TYPE_NORMAL
  zh: return _mm_mul_ps(beta, gamma);
- en: '}'
  id: totrans-2165
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: static inline __m128
  id: totrans-2166
  prefs: []
  type: TYPE_NORMAL
  zh: static inline __m128
- en: horizontal_sum_ps( const __m128 x )
  id: totrans-2167
  prefs: []
  type: TYPE_NORMAL
  zh: horizontal_sum_ps( const __m128 x )
- en: '{'
  id: totrans-2168
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: const __m128 t = _mm_add_ps(x, _mm_movehl_ps(x, x));
  id: totrans-2169
  prefs: []
  type: TYPE_NORMAL
  zh: const __m128 t = _mm_add_ps(x, _mm_movehl_ps(x, x));
- en: return _mm_add_ss(t, _mm_shuffle_ps(t, t, 1));
  id: totrans-2170
  prefs: []
  type: TYPE_NORMAL
  zh: return _mm_add_ss(t, _mm_shuffle_ps(t, t, 1));
- en: '}'
  id: totrans-2171
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: inline void
  id: totrans-2172
  prefs: []
  type: TYPE_NORMAL
  zh: inline void
- en: bodyBodyInteraction(
  id: totrans-2173
  prefs: []
  type: TYPE_NORMAL
  zh: bodyBodyInteraction(
- en: __m128& f0,
  id: totrans-2174
  prefs: []
  type: TYPE_NORMAL
  zh: __m128& f0,
- en: __m128& f1,
  id: totrans-2175
  prefs: []
  type: TYPE_NORMAL
  zh: __m128& f1,
- en: __m128& f2,
  id: totrans-2176
  prefs: []
  type: TYPE_NORMAL
  zh: __m128& f2,
- en: const __m128& x0,
  id: totrans-2177
  prefs: []
  type: TYPE_NORMAL
  zh: const __m128& x0,
- en: const __m128& y0,
  id: totrans-2178
  prefs: []
  type: TYPE_NORMAL
  zh: const __m128& y0,
- en: const __m128& z0,
  id: totrans-2179
  prefs: []
  type: TYPE_NORMAL
  zh: const __m128& z0,
- en: const __m128& x1,
  id: totrans-2180
  prefs: []
  type: TYPE_NORMAL
  zh: const __m128& x1,
- en: const __m128& y1,
  id: totrans-2181
  prefs: []
  type: TYPE_NORMAL
  zh: const __m128& y1,
- en: const __m128& z1,
  id: totrans-2182
  prefs: []
  type: TYPE_NORMAL
  zh: const __m128& z1,
- en: const __m128& mass1,
  id: totrans-2183
  prefs: []
  type: TYPE_NORMAL
  zh: const __m128& mass1,
- en: const __m128& softeningSquared )
  id: totrans-2184
  prefs: []
  type: TYPE_NORMAL
  zh: const __m128& softeningSquared )
- en: '{'
  id: totrans-2185
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: __m128 dx = _mm_sub_ps( x1, x0 );
  id: totrans-2186
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 dx = _mm_sub_ps( x1, x0 );
- en: __m128 dy = _mm_sub_ps( y1, y0 );
  id: totrans-2187
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 dy = _mm_sub_ps( y1, y0 );
- en: __m128 dz = _mm_sub_ps( z1, z0 );
  id: totrans-2188
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 dz = _mm_sub_ps( z1, z0 );
- en: __m128 distSq =
  id: totrans-2189
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 distSq =
- en: _mm_add_ps(
  id: totrans-2190
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_add_ps(
- en: _mm_add_ps(
  id: totrans-2191
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_add_ps(
- en: _mm_mul_ps( dx, dx ),
  id: totrans-2192
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_mul_ps( dx, dx ),
- en: _mm_mul_ps( dy, dy )
  id: totrans-2193
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_mul_ps( dy, dy )
- en: ),
  id: totrans-2194
  prefs: []
  type: TYPE_NORMAL
  zh: ),
- en: _mm_mul_ps( dz, dz )
  id: totrans-2195
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_mul_ps( dz, dz )
- en: );
  id: totrans-2196
  prefs: []
  type: TYPE_NORMAL
  zh: );
- en: distSq = _mm_add_ps( distSq, softeningSquared );
  id: totrans-2197
  prefs: []
  type: TYPE_NORMAL
  zh: distSq = _mm_add_ps( distSq, softeningSquared );
- en: __m128 invDist = rcp_sqrt_nr_ps( distSq );
  id: totrans-2198
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 invDist = rcp_sqrt_nr_ps( distSq );
- en: __m128 invDistCube =
  id: totrans-2199
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 invDistCube =
- en: _mm_mul_ps(
  id: totrans-2200
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_mul_ps(
- en: invDist,
  id: totrans-2201
  prefs: []
  type: TYPE_NORMAL
  zh: invDist,
- en: _mm_mul_ps(
  id: totrans-2202
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_mul_ps(
- en: invDist, invDist )
  id: totrans-2203
  prefs: []
  type: TYPE_NORMAL
  zh: invDist, invDist )
- en: );
  id: totrans-2204
  prefs: []
  type: TYPE_NORMAL
  zh: );
- en: __m128 s = _mm_mul_ps( mass1, invDistCube );
  id: totrans-2205
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 s = _mm_mul_ps( mass1, invDistCube );
- en: f0 = _mm_add_ps( a0, _mm_mul_ps( dx, s ) );
  id: totrans-2206
  prefs: []
  type: TYPE_NORMAL
  zh: f0 = _mm_add_ps( a0, _mm_mul_ps( dx, s ) );
- en: f1 = _mm_add_ps( a1, _mm_mul_ps( dy, s ) );
  id: totrans-2207
  prefs: []
  type: TYPE_NORMAL
  zh: f1 = _mm_add_ps( a1, _mm_mul_ps( dy, s ) );
- en: f2 = _mm_add_ps( a2, _mm_mul_ps( dz, s ) );
  id: totrans-2208
  prefs: []
  type: TYPE_NORMAL
  zh: f2 = _mm_add_ps( a2, _mm_mul_ps( dz, s ) );
- en: '}'
  id: totrans-2209
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2210
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: To take advantage of multiple cores, we must spawn multiple threads and have
    each thread perform part of the computation. The same strategy is used for multiple
    CPU cores as for multiple GPUs:^([11](ch14.html#ch14fn11)) Just evenly divide
    the output rows among threads (one per CPU core) and, for each timestep, have
    the “parent” thread signal the worker threads to perform their work and then wait
    for them to finish. Since thread creation can be expensive and can fail, our application
    creates a pool of CPU threads at initialization time and uses thread synchronization
    to make the worker threads wait for work and signal completion.
  id: totrans-2211
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用多核处理器，我们必须生成多个线程，并让每个线程执行部分计算。对于多个 CPU 核心的情况，采用与多个 GPU 相同的策略：^([11](ch14.html#ch14fn11))
    将输出行均匀分配给线程（每个 CPU 核心一个），并在每个时间步中，让“父”线程通知工作线程执行任务，然后等待它们完成。由于线程创建可能比较耗费资源并且可能失败，我们的应用程序在初始化时创建了一个
    CPU 线程池，并使用线程同步使工作线程等待工作并在完成后发出信号。
- en: '[11](ch14.html#ch14fn11a). In fact, we used the same platform-independent threading
    library to implement the multithreaded multi-GPU support in [Chapter 9](ch09.html#ch09).'
  id: totrans-2212
  prefs: []
  type: TYPE_NORMAL
  zh: '[11](ch14.html#ch14fn11a)。事实上，我们使用了相同的跨平台线程库来实现[第 9 章](ch09.html#ch09)中的多线程多
    GPU 支持。'
- en: The portable CUDA handbook threading library, described in [Section A.2](app01.html#app01lev1sec2),
    implements a function `processorCount()` that returns the number of CPU cores
    on the system and a C++ class `workerThread` with methods to create and destroy
    CPU threads and delegate work synchronously or asynchronously. After delegating
    asynchronous work with the `delegateAsynchronous()` member function, the static
    function `waitAll()` is used to wait until the worker threads are finished.
  id: totrans-2213
  prefs: []
  type: TYPE_NORMAL
  zh: 可移植的 CUDA 手册线程库，详见[第 A.2 节](app01.html#app01lev1sec2)，实现了一个函数`processorCount()`，该函数返回系统上的
    CPU 核心数量，并且提供了一个 C++ 类`workerThread`，它包含创建和销毁 CPU 线程的方法，以及同步或异步地委派工作。在使用`delegateAsynchronous()`成员函数委派异步工作后，静态函数`waitAll()`用于等待所有工作线程完成。
- en: '[Listing 14.9](ch14.html#ch14lis09) gives the code that dispatches the N-Body
    calculation to worker CPU threads. The `sseDelegation` structures are used to
    communicate the delegation to each worker CPU thread; the `delegateSynchronous`
    function takes a pointer-to-function to execute and a `void *` that will be passed
    to that function (in this case, the `void *` points to the corresponding CPU thread’s
    `sseDelegation` structure).'
  id: totrans-2214
  prefs: []
  type: TYPE_NORMAL
  zh: '[清单 14.9](ch14.html#ch14lis09) 给出了将 N-Body 计算分派到工作 CPU 线程的代码。`sseDelegation`
    结构体用于将委托传递给每个工作 CPU 线程；`delegateSynchronous` 函数接受一个函数指针来执行，并传递一个 `void *` 类型的参数，该参数会被传递给该函数（在此例中，`void
    *` 指向对应 CPU 线程的 `sseDelegation` 结构体）。'
- en: '*Listing 14.9.* Multithreaded SSE (master thread code).'
  id: totrans-2215
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 14.9.* 多线程 SSE（主线程代码）。'
- en: '[Click here to view code image](ch14_images.html#p14lis09a)'
  id: totrans-2216
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch14_images.html#p14lis09a)'
- en: '* * *'
  id: totrans-2217
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: float
  id: totrans-2218
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: ComputeGravitation_SSE_threaded(
  id: totrans-2219
  prefs: []
  type: TYPE_NORMAL
  zh: ComputeGravitation_SSE_threaded(
- en: float *force[3],
  id: totrans-2220
  prefs: []
  type: TYPE_NORMAL
  zh: float *force[3],
- en: float *pos[4],
  id: totrans-2221
  prefs: []
  type: TYPE_NORMAL
  zh: float *pos[4],
- en: float *mass,
  id: totrans-2222
  prefs: []
  type: TYPE_NORMAL
  zh: float *mass,
- en: float softeningSquared,
  id: totrans-2223
  prefs: []
  type: TYPE_NORMAL
  zh: float softeningSquared,
- en: size_t N
  id: totrans-2224
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N
- en: )
  id: totrans-2225
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '{'
  id: totrans-2226
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: chTimerTimestamp start, end;
  id: totrans-2227
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerTimestamp start, end;
- en: chTimerGetTime( &start );
  id: totrans-2228
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &start );
- en: '{'
  id: totrans-2229
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: sseDelegation *psse = new sseDelegation[g_numCPUCores];
  id: totrans-2230
  prefs: []
  type: TYPE_NORMAL
  zh: sseDelegation *psse = new sseDelegation[g_numCPUCores];
- en: size_t bodiesPerCore = N / g_numCPUCores;
  id: totrans-2231
  prefs: []
  type: TYPE_NORMAL
  zh: size_t bodiesPerCore = N / g_numCPUCores;
- en: if ( N % g_numCPUCores ) {
  id: totrans-2232
  prefs: []
  type: TYPE_NORMAL
  zh: if ( N % g_numCPUCores ) {
- en: return 0.0f;
  id: totrans-2233
  prefs: []
  type: TYPE_NORMAL
  zh: return 0.0f;
- en: '}'
  id: totrans-2234
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for ( size_t i = 0; i < g_numCPUCores; i++ ) {
  id: totrans-2235
  prefs: []
  type: TYPE_NORMAL
  zh: for ( size_t i = 0; i < g_numCPUCores; i++ ) {
- en: psse[i].hostPosSOA[0] = pos[0];
  id: totrans-2236
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].hostPosSOA[0] = pos[0];
- en: psse[i].hostPosSOA[1] = pos[1];
  id: totrans-2237
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].hostPosSOA[1] = pos[1];
- en: psse[i].hostPosSOA[2] = pos[2];
  id: totrans-2238
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].hostPosSOA[2] = pos[2];
- en: psse[i].hostMassSOA = mass;
  id: totrans-2239
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].hostMassSOA = mass;
- en: psse[i].hostForceSOA[0] = force[0];
  id: totrans-2240
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].hostForceSOA[0] = force[0];
- en: psse[i].hostForceSOA[1] = force[1];
  id: totrans-2241
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].hostForceSOA[1] = force[1];
- en: psse[i].hostForceSOA[2] = force[2];
  id: totrans-2242
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].hostForceSOA[2] = force[2];
- en: psse[i].softeningSquared = softeningSquared;
  id: totrans-2243
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].softeningSquared = softeningSquared;
- en: psse[i].i = bodiesPerCore*i;
  id: totrans-2244
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].i = bodiesPerCore*i;
- en: psse[i].n = bodiesPerCore;
  id: totrans-2245
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].n = bodiesPerCore;
- en: psse[i].N = N;
  id: totrans-2246
  prefs: []
  type: TYPE_NORMAL
  zh: psse[i].N = N;
- en: g_CPUThreadPool[i].delegateAsynchronous(
  id: totrans-2247
  prefs: []
  type: TYPE_NORMAL
  zh: g_CPUThreadPool[i].delegateAsynchronous(
- en: sseWorkerThread,
  id: totrans-2248
  prefs: []
  type: TYPE_NORMAL
  zh: sseWorkerThread,
- en: '&psse[i] );'
  id: totrans-2249
  prefs: []
  type: TYPE_NORMAL
  zh: '&psse[i] );'
- en: '}'
  id: totrans-2250
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: workerThread::waitAll( g_CPUThreadPool, g_numCPUCores );
  id: totrans-2251
  prefs: []
  type: TYPE_NORMAL
  zh: workerThread::waitAll( g_CPUThreadPool, g_numCPUCores );
- en: delete[] psse;
  id: totrans-2252
  prefs: []
  type: TYPE_NORMAL
  zh: delete[] psse;
- en: '}'
  id: totrans-2253
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: chTimerGetTime( &end );
  id: totrans-2254
  prefs: []
  type: TYPE_NORMAL
  zh: chTimerGetTime( &end );
- en: return (float) chTimerElapsedTime( &start, &end ) * 1000.0f;
  id: totrans-2255
  prefs: []
  type: TYPE_NORMAL
  zh: return (float) chTimerElapsedTime( &start, &end ) * 1000.0f;
- en: '}'
  id: totrans-2256
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2257
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Finally, [Listing 14.10](ch14.html#ch14lis10) gives the `sseDelegation` structure
    and the delegation function invoked by `ComputeGravitation_SSE_threaded` in [Listing
    14.9](ch14.html#ch14lis09). It performs the body-body calculations four at a time,
    accumulating four partial sums that are added together with `horizontal_sum_ps()`
    before storing the final output forces. This function, along with all the functions
    that it calls, uses the SOA memory layout for all inputs and outputs.
  id: totrans-2258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，[清单 14.10](ch14.html#ch14lis10) 给出了 `sseDelegation` 结构体和由 `ComputeGravitation_SSE_threaded`
    在[清单 14.9](ch14.html#ch14lis09)中调用的委托函数。它一次执行四个物体间的计算，累加四个部分和，在存储最终输出力之前，使用 `horizontal_sum_ps()`
    将其加总。这个函数以及它调用的所有函数，使用面向对象的内存布局（SOA）来处理所有输入和输出。
- en: '*Listing 14.10.* sseWorkerThread.'
  id: totrans-2259
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 14.10.* sseWorkerThread.'
- en: '[Click here to view code image](ch14_images.html#p14lis10a)'
  id: totrans-2260
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch14_images.html#p14lis10a)'
- en: '* * *'
  id: totrans-2261
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: struct sseDelegation {
  id: totrans-2262
  prefs: []
  type: TYPE_NORMAL
  zh: struct sseDelegation {
- en: size_t i;   // base offset for this thread to process
  id: totrans-2263
  prefs: []
  type: TYPE_NORMAL
  zh: size_t i;   // 当前线程处理的基准偏移量
- en: size_t n;   // size of this thread's problem
  id: totrans-2264
  prefs: []
  type: TYPE_NORMAL
  zh: size_t n;   // 当前线程问题的大小
- en: size_t N;   // total number of bodies
  id: totrans-2265
  prefs: []
  type: TYPE_NORMAL
  zh: size_t N;   // 物体总数
- en: float *hostPosSOA[3];
  id: totrans-2266
  prefs: []
  type: TYPE_NORMAL
  zh: float *hostPosSOA[3];
- en: float *hostMassSOA;
  id: totrans-2267
  prefs: []
  type: TYPE_NORMAL
  zh: float *hostMassSOA;
- en: float *hostForceSOA[3];
  id: totrans-2268
  prefs: []
  type: TYPE_NORMAL
  zh: float *hostForceSOA[3];
- en: float softeningSquared;
  id: totrans-2269
  prefs: []
  type: TYPE_NORMAL
  zh: float softeningSquared;
- en: '};'
  id: totrans-2270
  prefs: []
  type: TYPE_NORMAL
  zh: '};'
- en: void
  id: totrans-2271
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: sseWorkerThread( void *_p )
  id: totrans-2272
  prefs: []
  type: TYPE_NORMAL
  zh: sseWorkerThread( void *_p )
- en: '{'
  id: totrans-2273
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: sseDelegation *p = (sseDelegation *) _p;
  id: totrans-2274
  prefs: []
  type: TYPE_NORMAL
  zh: sseDelegation *p = (sseDelegation *) _p;
- en: for (int k = 0; k < p->n; k++)
  id: totrans-2275
  prefs: []
  type: TYPE_NORMAL
  zh: for (int k = 0; k < p->n; k++)
- en: '{'
  id: totrans-2276
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int i = p->i + k;
  id: totrans-2277
  prefs: []
  type: TYPE_NORMAL
  zh: int i = p->i + k;
- en: __m128 ax = _mm_setzero_ps();
  id: totrans-2278
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 ax = _mm_setzero_ps();
- en: __m128 ay = _mm_setzero_ps();
  id: totrans-2279
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 ay = _mm_setzero_ps();
- en: __m128 az = _mm_setzero_ps();
  id: totrans-2280
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 az = _mm_setzero_ps();
- en: __m128 *px = (__m128 *) p->hostPosSOA[0];
  id: totrans-2281
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 *px = (__m128 *) p->hostPosSOA[0];
- en: __m128 *py = (__m128 *) p->hostPosSOA[1];
  id: totrans-2282
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 *py = (__m128 *) p->hostPosSOA[1];
- en: __m128 *pz = (__m128 *) p->hostPosSOA[2];
  id: totrans-2283
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 *pz = (__m128 *) p->hostPosSOA[2];
- en: __m128 *pmass = (__m128 *) p->hostMassSOA;
  id: totrans-2284
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 *pmass = (__m128 *) p->hostMassSOA;
- en: __m128 x0 = _mm_set_ps1( p->hostPosSOA[0][i] );
  id: totrans-2285
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 x0 = _mm_set_ps1( p->hostPosSOA[0][i] );
- en: __m128 y0 = _mm_set_ps1( p->hostPosSOA[1][i] );
  id: totrans-2286
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 y0 = _mm_set_ps1( p->hostPosSOA[1][i] );
- en: __m128 z0 = _mm_set_ps1( p->hostPosSOA[2][i] );
  id: totrans-2287
  prefs: []
  type: TYPE_NORMAL
  zh: __m128 z0 = _mm_set_ps1( p->hostPosSOA[2][i] );
- en: for ( int j = 0; j < p->N/4; j++ ) {
  id: totrans-2288
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < p->N/4; j++ ) {
- en: bodyBodyInteraction(
  id: totrans-2289
  prefs: []
  type: TYPE_NORMAL
  zh: bodyBodyInteraction(
- en: ax, ay, az,
  id: totrans-2290
  prefs: []
  type: TYPE_NORMAL
  zh: ax, ay, az,
- en: x0, y0, z0,
  id: totrans-2291
  prefs: []
  type: TYPE_NORMAL
  zh: x0, y0, z0,
- en: px[j], py[j], pz[j], pmass[j],
  id: totrans-2292
  prefs: []
  type: TYPE_NORMAL
  zh: px[j], py[j], pz[j], pmass[j],
- en: _mm_set_ps1( p->softeningSquared ) );
  id: totrans-2293
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_set_ps1( p->softeningSquared ) );
- en: '}'
  id: totrans-2294
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: // Accumulate sum of four floats in the SSE register
  id: totrans-2295
  prefs: []
  type: TYPE_NORMAL
  zh: // 累加SSE寄存器中的四个浮点数和
- en: ax = horizontal_sum_ps( ax );
  id: totrans-2296
  prefs: []
  type: TYPE_NORMAL
  zh: ax = horizontal_sum_ps( ax );
- en: ay = horizontal_sum_ps( ay );
  id: totrans-2297
  prefs: []
  type: TYPE_NORMAL
  zh: ay = horizontal_sum_ps( ay );
- en: az = horizontal_sum_ps( az );
  id: totrans-2298
  prefs: []
  type: TYPE_NORMAL
  zh: az = horizontal_sum_ps( az );
- en: _mm_store_ss( (float *) &p->hostForceSOA[0][i], ax );
  id: totrans-2299
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_store_ss( (float *) &p->hostForceSOA[0][i], ax );
- en: _mm_store_ss( (float *) &p->hostForceSOA[1][i], ay );
  id: totrans-2300
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_store_ss( (float *) &p->hostForceSOA[1][i], ay );
- en: _mm_store_ss( (float *) &p->hostForceSOA[2][i], az );
  id: totrans-2301
  prefs: []
  type: TYPE_NORMAL
  zh: _mm_store_ss( (float *) &p->hostForceSOA[2][i], az );
- en: '}'
  id: totrans-2302
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2303
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2304
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 14.8\. Conclusion
  id: totrans-2305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.8\. 结论
- en: Since instruction sets and architectures differ, performance is measured in
    body-body interactions per second rather than GFLOPS. Performance was measured
    on a dual-socket Sandy Bridge system with two E5-2670 CPUs (similar to Amazon’s
    `cc2.8xlarge` instance type), 64GB of RAM, and four (4) GK104 GPUs clocked at
    about 800MHz. The GK104s are on two dual-GPU boards plugged into 16-lane PCI Express
    3.0 slots.
  id: totrans-2306
  prefs: []
  type: TYPE_NORMAL
  zh: 由于指令集和架构不同，性能是通过每秒钟的物体-物体交互次数来衡量的，而不是GFLOPS。性能测试是在一台双插槽的Sandy Bridge系统上进行的，配备了两颗E5-2670
    CPU（类似于亚马逊的`cc2.8xlarge`实例类型），64GB内存，以及四个（4）个工作频率约为800MHz的GK104 GPU。GK104安装在两块双GPU板卡上，插入16通道的PCI
    Express 3.0插槽中。
- en: '[Table 14.4](ch14.html#ch14tab04) summarizes the speedups due to CPU optimizations.
    All measurements were performed on a server with dual Xeon E2670 CPUs (2.6GHz).
    On this system, the generic CPU code in [Listing 14.2](ch14.html#ch14lis02) performs
    17.2M interactions per second; the single-threaded SSE code performs 307M interactions
    per second, some 17.8x faster! As expected, multithreading the SSE code achieves
    good speedups, with 32 CPU threads delivering 5650M interactions per second, about
    18x as fast as one thread. Between porting to SSE and multithreading, the total
    speedup on this platform for CPUs is more than 300x.'
  id: totrans-2307
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 14.4](ch14.html#ch14tab04)总结了由于CPU优化带来的加速效果。所有测量都在配备双Xeon E2670 CPU（2.6GHz）的服务器上进行。在该系统上，[列表
    14.2](ch14.html#ch14lis02)中的通用CPU代码每秒执行17.2M次交互；单线程SSE代码每秒执行307M次交互，速度快了约17.8倍！正如预期的那样，SSE代码通过多线程化实现了良好的加速效果，32个CPU线程每秒执行5650M次交互，约为单线程的18倍。通过将代码移植到SSE并进行多线程处理，在该平台上CPU的总加速比超过了300倍。'
- en: '![Image](graphics/14tab04.jpg)'
  id: totrans-2308
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/14tab04.jpg)'
- en: '*Table 14.4* Speedups Due to CPU Optimizations'
  id: totrans-2309
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 14.4* 由于CPU优化带来的加速效果'
- en: Because we got such a huge performance improvement from our CPU optimizations,
    the performance comparisons aren’t as pronounced in favor of GPUs as most.^([12](ch14.html#ch14fn12))
    The highest-performing kernel in our testing (the shared memory implementation
    in [Listing 14.4](ch14.html#ch14lis04), with a loop unroll factor of 4) delivered
    45.2 billion body-body interactions per second, exactly 8x faster than the fastest
    multithreaded SSE implementation. This result understates the performance advantages
    of CUDA in some ways, since the server used for testing had two high-end CPUs,
    and the GPUs are derated to reduce power consumption and heat dissipation.
  id: totrans-2310
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通过CPU优化获得了如此巨大的性能提升，性能比较在GPU方面的优势不像大多数人预期的那样明显。我们测试中的最高性能内核（[列表 14.4](ch14.html#ch14lis04)中的共享内存实现，循环展开因子为4）每秒执行45.2亿次体对体交互，速度正好是最快的多线程SSE实现的8倍。这个结果在某些方面低估了CUDA的性能优势，因为用于测试的服务器配备了两颗高端CPU，而GPU则进行了降额处理以减少功耗和散热。
- en: '[12](ch14.html#ch14fn12a). In fairness, that would be true of many other workloads
    in this book, like the SAXPY implementation in [Chapter 11](ch11.html#ch11) and
    the normalized cross-correlation implementation in [Chapter 15](ch15.html#ch15).
    Porting those workloads to multithreaded SIMD would proffer similar tradeoffs
    in performance versus engineering investment, readability, and maintainability
    as compared to the CUDA version.'
  id: totrans-2311
  prefs: []
  type: TYPE_NORMAL
  zh: '[12](ch14.html#ch14fn12a)。公平地说，书中许多其他工作负载也适用这一点，比如[第11章](ch11.html#ch11)中的SAXPY实现和[第15章](ch15.html#ch15)中的标准化互相关实现。将这些工作负载移植到多线程SIMD上，会带来与CUDA版本在性能、工程投入、可读性和可维护性上的类似权衡。'
- en: 'Furthermore, future improvements can be had for both technologies: For CPUs,
    porting this workload to the AVX (“Advanced Vector eXtensions”) instruction set
    would potentially double performance, but it would run only on Sandy Bridge and
    later chips, and the optimized CPU implementation does not exploit symmetry. For
    GPUs, NVIDIA’s GK110 is about twice as big (and presumably about twice as fast)
    as the GK104\. Comparing the source code for [Listings 14.1](ch14.html#ch14lis01)
    and [14.9](ch14.html#ch14lis09) (the GPU and SSE implementations of the core body-body
    interaction code), though, it becomes clear that performance isn’t the only reason
    to favor CUDA over optimizing CPU code. Dr. Vincent Natoli alluded to this tradeoff
    in his June 2010 article “Kudos for CUDA.”^([13](ch14.html#ch14fn13))'
  id: totrans-2312
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，未来两种技术都可以进行改进：对于CPU，将此工作负载移植到AVX（“高级向量扩展”）指令集可能会使性能翻倍，但它仅能在Sandy Bridge及之后的芯片上运行，而且优化后的CPU实现并没有利用对称性。对于GPU，NVIDIA的GK110大约是GK104的两倍大（并且假设其速度也大约是GK104的两倍）。然而，通过比较[Listings
    14.1](ch14.html#ch14lis01)和[14.9](ch14.html#ch14lis09)（核心体-体交互代码的GPU和SSE实现）的源代码，可以明显看出，性能并不是偏向CUDA而不是优化CPU代码的唯一原因。Vincent
    Natoli博士在他2010年6月的文章《CUDA的称赞》中提到了这一权衡。^([13](ch14.html#ch14fn13))
- en: '[13](ch14.html#ch14fn13a). [www.hpcwire.com/hpcwire/2010-07-06/kudos_for_cuda.html](http://www.hpcwire.com/hpcwire/2010-07-06/kudos_for_cuda.html)'
  id: totrans-2313
  prefs: []
  type: TYPE_NORMAL
  zh: '[13](ch14.html#ch14fn13a). [www.hpcwire.com/hpcwire/2010-07-06/kudos_for_cuda.html](http://www.hpcwire.com/hpcwire/2010-07-06/kudos_for_cuda.html)'
- en: Similarly, we have found in many cases that the expression of algorithmic parallelism
    in CUDA in fields as diverse as oil and gas, bioinformatics, and finance is more
    elegant, compact, and readable than equivalently optimized CPU code, preserving
    and more clearly presenting the underlying algorithm. In a recent project we reduced
    3500 lines of highly optimized C code to a CUDA kernel of about 800 lines. The
    optimized C was peppered with inline assembly, SSE macros, unrolled loops, and
    special cases, making it difficult to read, extract algorithmic meaning, and extend
    in the future. By comparison, the CUDA code was cleaner and more readable. Ultimately
    it will be easier to maintain.
  id: totrans-2314
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们在许多情况下发现，在油气、生物信息学和金融等领域，CUDA中算法并行性的表达比等效优化的CPU代码更加优雅、简洁且易读，保留并更清晰地呈现了底层算法。在一个最近的项目中，我们将3500行高度优化的C代码缩减为约800行的CUDA内核。优化过的C代码充满了内联汇编、SSE宏、展开的循环和特定的特殊情况，使得阅读、提取算法含义和未来扩展都变得困难。而相比之下，CUDA代码更加简洁易读，最终也会更容易维护。
- en: Although it was feasible to develop an SSE implementation of this application,
    with a core body-body computation that takes about 50 lines of code to express
    ([Listing 14.8](ch14.html#ch14lis08)), it’s hard to imagine what the source code
    would look like for an SSE-optimized implementation of something like Boids, where
    each body must evaluate conditions and, when running on CUDA hardware, the code
    winds up being divergent. SSE supports divergence both in the form of predication
    (using masks and Boolean instruction sequences such as `ANDPS/ANDNOTPS/ORPS` to
    construct the result) and branching (often using `MOVMSKPS` to extract evaluated
    conditions), but getting the theoretical speedups on such workloads would require
    large engineering investments unless they can be extracted automatically by a
    vectorizing compiler.
  id: totrans-2315
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管开发该应用程序的SSE实现是可行的，其中核心的体-体计算大约需要50行代码来表示（[Listing 14.8](ch14.html#ch14lis08)），但很难想象像Boids这样的程序的SSE优化实现的源代码会是什么样的，因为每个物体都必须评估条件，并且在CUDA硬件上运行时，代码往往会出现分歧。SSE支持分歧，可以通过预测（使用掩码和布尔指令序列，如`ANDPS/ANDNOTPS/ORPS`构造结果）和分支（通常使用`MOVMSKPS`提取评估的条件）来实现，但在这种工作负载上获得理论加速，除非可以通过矢量化编译器自动提取，否则需要大量的工程投入。
- en: 14.9\. References and Further Reading
  id: totrans-2316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.9\. 参考文献与进一步阅读
- en: N-Body and algorithms with similarly high computational density are a source
    of many high-profile speedups, since they can approach the theoretical limits
    of the GPU’s computing capabilities. The following are just a sample of the numerous
    papers on compute-intensive methods such as N-Body.
  id: totrans-2317
  prefs: []
  type: TYPE_NORMAL
  zh: N-Body及类似的高计算密度算法是许多高性能加速的来源，因为它们能够接近GPU计算能力的理论极限。以下只是关于计算密集型方法（如N-Body）的一些文献示例。
- en: '*Gravitational Simulation*'
  id: totrans-2318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*引力模拟*'
- en: Burtscher, Martin, and Keshav Pingali. An efficient CUDA implementation of the
    tree-based Barnes-Hut n-body algorithm. In *GPU Gems Emerald Edition*, Wen-Mei
    Hwu, ed., Morgan-Kaufmann, 2011, Burlington, MA, pp. 75–92.
  id: totrans-2319
  prefs: []
  type: TYPE_NORMAL
  zh: Burtscher, Martin, 和 Keshav Pingali. 基于树的Barnes-Hut N体算法的高效CUDA实现。见 *GPU Gems
    Emerald Edition*，Wen-Mei Hwu编辑，Morgan-Kaufmann，2011年，马萨诸塞州伯灵顿，第75-92页。
- en: '[http://cs.txstate.edu/~burtscher/papers/gcg11.pdf](http://cs.txstate.edu/~burtscher/papers/gcg11.pdf)'
  id: totrans-2320
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://cs.txstate.edu/~burtscher/papers/gcg11.pdf](http://cs.txstate.edu/~burtscher/papers/gcg11.pdf)'
- en: Harris, Mark, Lars Nyland, and Jan Prins. Fast n-body simulation with CUDA.
    In *GPU Gems 3*, Addison-Wesley, Boston, MA, 2007, pp. 677–695.
  id: totrans-2321
  prefs: []
  type: TYPE_NORMAL
  zh: Harris, Mark, Lars Nyland, 和 Jan Prins. 使用CUDA的快速N体模拟。见 *GPU Gems 3*，Addison-Wesley，波士顿，马萨诸塞州，2007年，第677-695页。
- en: '[http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html](http://developer.nvidia.com/GPUGems3/gpugems3_ch31.html)'
  id: totrans-2322
  prefs: []
  type: TYPE_NORMAL
  zh: '[http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html](http://developer.nvidia.com/GPUGems3/gpugems3_ch31.html)'
- en: '*Molecular Modeling*'
  id: totrans-2323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*分子建模*'
- en: 'Götz, Andreas, Mark J. Williamson, Dong Xu, Duncan Poole, Scott Le Grand, and
    Ross C. Walker. Routine microsecond molecular dynamics simulations with AMBER
    on GPUs—Part I: Generalized Born, *J. Chem. Theory Comput*. 8 (5), 2012, pp. 1542–1555.'
  id: totrans-2324
  prefs: []
  type: TYPE_NORMAL
  zh: Götz, Andreas, Mark J. Williamson, Dong Xu, Duncan Poole, Scott Le Grand, 和
    Ross C. Walker. 使用AMBER在GPU上进行常规微秒分子动力学模拟——第一部分：广义Born，*J. Chem. Theory Comput*，8
    (5)，2012，第1542–1555页.
- en: Hwu, Wen-Mei, and David Kirk. *Programming Massively Parallel Processors*. Morgan-Kaufmann,
    2010, pp. 173–188.
  id: totrans-2325
  prefs: []
  type: TYPE_NORMAL
  zh: Hwu, Wen-Mei, 和 David Kirk. *大规模并行处理器编程*. Morgan-Kaufmann, 2010，第173–188页.
- en: Hardy, David J., John E. Stone, Kirby L. Vandivort, David Gohara, Christopher
    Rodrigues, and Klaus Schulten. Fast molecular electrostatics algorithms on GPUs.
    In *GPU Computing Gems*, Elsevier, Burlington, MA, 2011, pp. 43–58.
  id: totrans-2326
  prefs: []
  type: TYPE_NORMAL
  zh: Hardy, David J., John E. Stone, Kirby L. Vandivort, David Gohara, Christopher
    Rodrigues, 和 Klaus Schulten. 在GPU上的快速分子电荷算法. 见于 *GPU计算宝石*，Elsevier，Burlington,
    MA, 2011，第43–58页.
- en: Stone, John E., James C. Phillips, Peter L. Freddolino, David J. Hardy, Leonardo
    G. Trabuco, and Klaus Schulten. Accelerating molecular modeling applications with
    graphics processors. *Journal of Computational Chemistry* 28 (2007), pp. 2618–2640.
  id: totrans-2327
  prefs: []
  type: TYPE_NORMAL
  zh: Stone, John E., James C. Phillips, Peter L. Freddolino, David J. Hardy, Leonardo
    G. Trabuco, 和 Klaus Schulten. 使用图形处理器加速分子建模应用. *计算化学杂志* 28 (2007)，第2618–2640页.
- en: '[http://cacs.usc.edu/education/cs653/Stone-MDGPU-JCC07.pdf](http://cacs.usc.edu/education/cs653/Stone-MDGPU-JCC07.pdf)'
  id: totrans-2328
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://cacs.usc.edu/education/cs653/Stone-MDGPU-JCC07.pdf](http://cacs.usc.edu/education/cs653/Stone-MDGPU-JCC07.pdf)'
- en: Stone, John E., David J. Hardy, Barry Isralewitz, and Klaus Schulten. GPU algorithms
    for molecular modeling. In *Scientific Computing with Multicore and Accelerators*,
    Jack Dongarra, David A. Bader, and Jakob Kurzak, eds. Chapman & Hall/CRC Press,
    London, UK, 2010, pp. 351–371.
  id: totrans-2329
  prefs: []
  type: TYPE_NORMAL
  zh: Stone, John E., David J. Hardy, Barry Isralewitz, 和 Klaus Schulten. 用于分子建模的GPU算法.
    见于 *多核和加速器科学计算*，Jack Dongarra, David A. Bader, 和 Jakob Kurzak 编著. Chapman & Hall/CRC
    Press, 伦敦，英国, 2010，第351–371页.
- en: '*Boids*'
  id: totrans-2330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '*Boids*'
- en: 'da Silva, A.R., W.S. Lages, and L. Chaimowicz. Boids that see: Using self-occlusion
    for simulating large groups on GPUs. *ACM Comput. Entertain*. 7 (4), 2009.'
  id: totrans-2331
  prefs: []
  type: TYPE_NORMAL
  zh: da Silva, A.R., W.S. Lages, 和 L. Chaimowicz. 会看的Boids：利用自遮挡模拟GPU上的大型群体. *ACM计算娱乐*，7
    (4), 2009.
- en: '[http://doi.acm.org/10.1145/1658866.1658870](http://doi.acm.org/10.1145/1658866.1658870)'
  id: totrans-2332
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://doi.acm.org/10.1145/1658866.1658870](http://doi.acm.org/10.1145/1658866.1658870)'
- en: 'Chapter 15\. Image Processing: Normalized Correlation'
  id: totrans-2333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第15章. 图像处理：归一化相关
- en: Normalized cross-correlation is a popular template-matching algorithm in image
    processing and computer vision. The template typically is an image that depicts
    a sought-after feature; by repeatedly computing a statistic between the template
    image and corresponding pixels of a subset of an input image, a search algorithm
    can locate instances of the template that are present in the input image.
  id: totrans-2334
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化互相关是图像处理和计算机视觉中常用的模板匹配算法。模板通常是描绘所寻特征的图像；通过反复计算模板图像与输入图像子集对应像素之间的统计值，搜索算法可以定位输入图像中存在的模板实例。
- en: The popularity of normalized cross-correlation for this application stems from
    its *amplitude independence*, which, in the context of image processing, essentially
    means that the statistic is robust in the face of lighting changes between the
    image and the template. Normalized correlation is popular enough, and sufficiently
    compute-intensive enough, that it has prompted companies to build custom hardware.
    This chapter develops an optimized implementation of normalized cross-correlation
    for 8-bit grayscale images, but many of the concepts can be extended to other
    types of image processing or computer vision algorithms.
  id: totrans-2335
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化互相关在这一应用中的流行源于其*幅度独立性*，在图像处理的背景下，实际上意味着该统计量对图像和模板之间的光照变化具有鲁棒性。归一化相关方法足够流行且计算密集，以至于促使公司开发定制硬件。本章将开发一种针对8位灰度图像的归一化互相关优化实现，但许多概念可以扩展到其他类型的图像处理或计算机视觉算法。
- en: 15.1\. Overview
  id: totrans-2336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1\. 概述
- en: Two 2D images, the image and the template, are compared by computing a correlation
    coefficient as follows.
  id: totrans-2337
  prefs: []
  type: TYPE_NORMAL
  zh: 两幅二维图像，图像和模板，通过计算相关系数进行比较，如下所示。
- en: '![Image](graphics/450equ01.jpg)'
  id: totrans-2338
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/450equ01.jpg)'
- en: where *I* and *T* are the image and template, respectively; ![image](graphics/t-bar.jpg)
    is the average value of the template; and ![image](graphics/i-bar.jpg) is the
    average value of the image pixels corresponding to the template.
  id: totrans-2339
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *I* 和 *T* 分别是图像和模板；![image](graphics/t-bar.jpg) 是模板的平均值；而![image](graphics/i-bar.jpg)
    是与模板对应的图像像素的平均值。
- en: 'The value of this coefficient falls into the range [–1.0, 1.0]; a value of
    1.0 corresponds to a perfect match. An optimized implementation of normalized
    correlation factors out the statistics that may be precomputed and computes sums
    instead of averages to avoid a separate pass over the input data. If *N* pixels
    are being compared, replacing ![image](graphics/i-bar.jpg) with ![image](graphics/450equ02.jpg)
    and multiplying the numerator and denominator by *N* yields a coefficient that
    can be expressed entirely in terms of sums. Rewriting without the coordinate notation:'
  id: totrans-2340
  prefs: []
  type: TYPE_NORMAL
  zh: 该系数的值范围在[–1.0, 1.0]之间；1.0的值对应完美匹配。归一化相关的优化实现将可以预先计算的统计量提取出来，并计算总和而非平均值，以避免对输入数据进行单独的遍历。如果比较的是
    *N* 个像素，替换![image](graphics/i-bar.jpg)为![image](graphics/450equ02.jpg)，并将分子和分母都乘以
    *N*，就能得到一个完全由总和表示的系数。重写时去掉坐标表示法：
- en: '![Image](graphics/450equ03.jpg)'
  id: totrans-2341
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/450equ03.jpg)'
- en: Assuming the template will be the same for many correlation computations, the
    statistics on the template ![image](graphics/450fig01.jpg) and ![image](graphics/450fig02.jpg)
    can be precomputed, as can the subexpression ![Image](graphics/450equ04.jpg) in
    the denominator. Translating this notation to C variable names gives the following.
  id: totrans-2342
  prefs: []
  type: TYPE_NORMAL
  zh: 假设模板在多次相关计算中保持不变，可以预先计算模板的统计数据![image](graphics/450fig01.jpg) 和![image](graphics/450fig02.jpg)，以及分母中的子表达式![Image](graphics/450equ04.jpg)。将此符号转换为C语言变量名得到如下结果。
- en: '![Image](graphics/450tab01.jpg)'
  id: totrans-2343
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/450tab01.jpg)'
- en: Then a normalized correlation value may be computed using this function.
  id: totrans-2344
  prefs: []
  type: TYPE_NORMAL
  zh: 然后可以使用此函数计算标准化的相关值。
- en: '[Click here to view code image](ch15_images.html#p451pro01a)'
  id: totrans-2345
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch15_images.html#p451pro01a)'
- en: float
  id: totrans-2346
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: CorrelationValue( float SumI, float SumISq,
  id: totrans-2347
  prefs: []
  type: TYPE_NORMAL
  zh: CorrelationValue( float SumI, float SumISq,
- en: float SumT, float SumTSq, float SumIT,
  id: totrans-2348
  prefs: []
  type: TYPE_NORMAL
  zh: float SumT, float SumTSq, float SumIT,
- en: float N )
  id: totrans-2349
  prefs: []
  type: TYPE_NORMAL
  zh: float N )
- en: '{'
  id: totrans-2350
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float Numerator = N*SumIT - SumI*SumT;
  id: totrans-2351
  prefs: []
  type: TYPE_NORMAL
  zh: float Numerator = N*SumIT - SumI*SumT;
- en: float Denominator = (N*SumISq - SumI*SumI)*
  id: totrans-2352
  prefs: []
  type: TYPE_NORMAL
  zh: float Denominator = (N*SumISq - SumI*SumI)*
- en: (N*SumTSq – SumT*SumT);
  id: totrans-2353
  prefs: []
  type: TYPE_NORMAL
  zh: (N*SumTSq – SumT*SumT);
- en: return Numerator / sqrtf(Denominator);
  id: totrans-2354
  prefs: []
  type: TYPE_NORMAL
  zh: return Numerator / sqrtf(Denominator);
- en: '}'
  id: totrans-2355
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: In practical applications for this algorithm, the template is kept fixed across
    many invocations, matching against different offsets into an image. Then it makes
    sense to precompute the template statistics and the denominator subexpression
  id: totrans-2356
  prefs: []
  type: TYPE_NORMAL
  zh: 在该算法的实际应用中，模板在多次调用中保持固定，并与图像中的不同偏移量进行匹配。因此，预计算模板统计数据和分母子表达式是有意义的。
- en: float fDenomExp = N*SumSqT - SumT*SumT;
  id: totrans-2357
  prefs: []
  type: TYPE_NORMAL
  zh: float fDenomExp = N*SumSqT - SumT*SumT;
- en: In practice, it’s best to use double precision to compute `fDenomExp.`
  id: totrans-2358
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，最好使用双精度计算 `fDenomExp`。
- en: float fDenomExp = (float) ((double) N*SumSqT – (double) SumT*SumT);
  id: totrans-2359
  prefs: []
  type: TYPE_NORMAL
  zh: float fDenomExp = (float) ((double) N*SumSqT – (double) SumT*SumT);
- en: '*Note:* This computation is done on the CPU, once per template.'
  id: totrans-2360
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意：* 该计算在 CPU 上执行，每个模板计算一次。'
- en: It is faster to multiply by the reciprocal square root than to divide by the
    square root, which results in the following `CorrelationValue()` function.
  id: totrans-2361
  prefs: []
  type: TYPE_NORMAL
  zh: 通过倒数平方根乘法比通过平方根除法更快，这就得到了以下 `CorrelationValue()` 函数。
- en: '[Click here to view code image](ch15_images.html#p451pro02a)'
  id: totrans-2362
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch15_images.html#p451pro02a)'
- en: float
  id: totrans-2363
  prefs: []
  type: TYPE_NORMAL
  zh: float
- en: CorrelationValue( float SumI, float SumISq, float SumIT,
  id: totrans-2364
  prefs: []
  type: TYPE_NORMAL
  zh: CorrelationValue( float SumI, float SumISq, float SumIT,
- en: float N, float fDenomExp )
  id: totrans-2365
  prefs: []
  type: TYPE_NORMAL
  zh: float N, float fDenomExp )
- en: '{'
  id: totrans-2366
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: float Numerator = cPixels*SumIT - SumI*SumT;
  id: totrans-2367
  prefs: []
  type: TYPE_NORMAL
  zh: float Numerator = cPixels*SumIT - SumI*SumT;
- en: float Denominator = fDenomExp*(cPixels*SumISq - SumI*SumI);
  id: totrans-2368
  prefs: []
  type: TYPE_NORMAL
  zh: float Denominator = fDenomExp*(cPixels*SumISq - SumI*SumI);
- en: return Numerator * rsqrtf(Denominator);
  id: totrans-2369
  prefs: []
  type: TYPE_NORMAL
  zh: return Numerator * rsqrtf(Denominator);
- en: '}'
  id: totrans-2370
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'Hence, an optimized implementation of this algorithm need only compute three
    sums over the pixels to compute a given correlation coefficient: ![image](graphics/451fig01.jpg),
    ![image](graphics/451fig02.jpg), and ![image](graphics/451fig03.jpg). Since the
    SMs include hardware support for integer multiply-add, NVIDIA GPUs are able to
    perform this computation extremely fast.'
  id: totrans-2371
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个算法的优化实现只需要计算三个关于像素的和来计算给定的相关系数：![image](graphics/451fig01.jpg)、![image](graphics/451fig02.jpg)
    和 ![image](graphics/451fig03.jpg)。由于 SM 单元包含对整数乘加运算的硬件支持，NVIDIA GPU 能够非常快速地执行此计算。
- en: CUDA offers a number of paths that could be used to deliver the data to the
    streaming multiprocessors.
  id: totrans-2372
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 提供了多条路径，可以用来将数据传送到流式多处理器。
- en: • Global memory or texture memory for the image, the template, or both
  id: totrans-2373
  prefs: []
  type: TYPE_NORMAL
  zh: • 图像、模板或两者的全局内存或纹理内存
- en: • Constant memory for the template and possibly other template-specific parameters
    (up to 64K)
  id: totrans-2374
  prefs: []
  type: TYPE_NORMAL
  zh: • 模板的常量内存以及可能的其他模板特定参数（最多 64K）
- en: • Shared memory to hold image and/or template values for reuse
  id: totrans-2375
  prefs: []
  type: TYPE_NORMAL
  zh: • 使用共享内存存储图像和/或模板值以便重用。
- en: This chapter assumes the pixels are 8-bit grayscale. The hardware works very
    well on images with higher precision, but if anything, that simplifies the problem
    by making it easier to efficiently address global and shared memory.
  id: totrans-2376
  prefs: []
  type: TYPE_NORMAL
  zh: 本章假设像素为 8 位灰度图像。硬件在处理高精度图像时表现非常好，但实际上，这通过简化问题，使得能够更高效地访问全局内存和共享内存，从而简化了问题。
- en: All of the CUDA implementations in this chapter use texture for the image that
    is being compared with the template. There are several reasons for this.
  id: totrans-2377
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有 CUDA 实现都使用纹理来存储与模板进行比较的图像。这样做有几个原因。
- en: • The texture units deal with boundary conditions gracefully and efficiently.
  id: totrans-2378
  prefs: []
  type: TYPE_NORMAL
  zh: • 纹理单元优雅而高效地处理边界条件。
- en: • The texture cache aggregates external bandwidth on reuse, which will occur
    as nearby correlation values are computed.
  id: totrans-2379
  prefs: []
  type: TYPE_NORMAL
  zh: • 纹理缓存通过重用聚合外部带宽，这将在计算附近的相关值时发生。
- en: • The 2D locality of the texture cache is a good fit with the access patterns
    exhibited by correlation search algorithms.
  id: totrans-2380
  prefs: []
  type: TYPE_NORMAL
  zh: • 纹理缓存的二维局部性与相关性搜索算法展示的访问模式非常契合。
- en: We’ll explore the tradeoffs of using texture versus constant memory for the
    template.
  id: totrans-2381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨使用纹理与常量内存存储模板的权衡。
- en: 15.2\. Naïve Texture-Texture Implementation
  id: totrans-2382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2\. 天真的纹理-纹理实现
- en: Our first implementation of normalized cross-correlation uses the texture unit
    to read both image and template values. This implementation is not optimized;
    it does not even include the optimization to precompute the template statistics.
    But it is simple to understand and will serve as a good basis for more highly
    optimized (but more byzantine) implementations.
  id: totrans-2383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个归一化互相关实现使用纹理单元读取图像和模板值。这个实现并没有经过优化；它甚至没有包括预计算模板统计数据的优化。但它简单易懂，将为更高效（但更加复杂）的实现提供良好的基础。
- en: '[Listing 15.1](ch15.html#ch15lis01) gives the kernel that performs this computation.
    It computes the five sums, then uses the `CorrelationValue()` utility function
    given earlier to write the `float`-valued correlation coefficients into the output
    array. Note that the expression computing `fDenomExp` will issue a warning on
    pre–SM 1.3 architectures, which do not include double precision support. The kernel
    will still work as long as the number of pixels in the template is not too large.'
  id: totrans-2384
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 15.1](ch15.html#ch15lis01)给出了执行此计算的内核。它计算五个和，然后使用之前给出的 `CorrelationValue()`
    工具函数将 `float` 类型的相关系数写入输出数组。请注意，计算 `fDenomExp` 的表达式会在 pre–SM 1.3 架构上发出警告，因为该架构不支持双精度计算。只要模板中的像素数不是特别大，内核仍然可以正常工作。'
- en: The upper left corner of the image is given by `(xUL, yUL)`; the width and height
    of the search window, and thus the output array of coefficients, is given by `w`
    and `h`. If the template is in a texture, the upper left corner of the template
    in the texture image is given by `(xTemplate, yTemplate)`.
  id: totrans-2385
  prefs: []
  type: TYPE_NORMAL
  zh: 图像的左上角由`(xUL, yUL)`给出；搜索窗口的宽度和高度，以及输出系数数组的宽度和高度分别由`w`和`h`给出。如果模板在纹理中，模板在纹理图像中的左上角由`(xTemplate,
    yTemplate)`给出。
- en: Finally, an offset `(xOffset, yOffset)` specifies how the template will be overlaid
    with the image for comparison purposes. When fetching image pixels, this offset
    is added to the coordinates of the search rectangle whose upper left corner is
    `(xUL, yUL)`.
  id: totrans-2386
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，偏移量`(xOffset, yOffset)`指定了模板如何与图像叠加进行比较。当获取图像像素时，偏移量会被加到搜索矩形的坐标上，该矩形的左上角为`(xUL,
    yUL)`。
- en: 'It’s instructive to look at how the correlation function “falls off” in the
    neighborhood of the image from which a template is extracted. The sample program
    `normalizedCrossCorrelation.cu` writes out the neighborhood around the template:'
  id: totrans-2387
  prefs: []
  type: TYPE_NORMAL
  zh: 观察相关函数在提取模板的图像邻域中的“衰减”情况非常有帮助。示例程序`normalizedCrossCorrelation.cu`输出模板周围的邻域：
- en: '[Click here to view code image](ch15_images.html#p453pro01a)'
  id: totrans-2388
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch15_images.html#p453pro01a)'
- en: 'Neighborhood around template:'
  id: totrans-2389
  prefs: []
  type: TYPE_NORMAL
  zh: 模板周围的邻域：
- en: 0.71 0.75 0.79 0.81 0.82 0.81 0.81 0.80 0.78
  id: totrans-2390
  prefs: []
  type: TYPE_NORMAL
  zh: 0.71 0.75 0.79 0.81 0.82 0.81 0.81 0.80 0.78
- en: 0.72 0.77 0.81 0.84 0.84 0.84 0.83 0.81 0.79
  id: totrans-2391
  prefs: []
  type: TYPE_NORMAL
  zh: 0.72 0.77 0.81 0.84 0.84 0.84 0.83 0.81 0.79
- en: 0.74 0.79 0.84 0.88 0.88 0.87 0.85 0.82 0.79
  id: totrans-2392
  prefs: []
  type: TYPE_NORMAL
  zh: 0.74 0.79 0.84 0.88 0.88 0.87 0.85 0.82 0.79
- en: 0.75 0.80 0.86 0.93 0.95 0.91 0.86 0.83 0.80
  id: totrans-2393
  prefs: []
  type: TYPE_NORMAL
  zh: 0.75 0.80 0.86 0.93 0.95 0.91 0.86 0.83 0.80
- en: 0.75 0.80 0.87 0.95 1.00 0.95 0.88 0.83 0.81
  id: totrans-2394
  prefs: []
  type: TYPE_NORMAL
  zh: 0.75 0.80 0.87 0.95 1.00 0.95 0.88 0.83 0.81
- en: 0.75 0.80 0.86 0.91 0.95 0.93 0.87 0.82 0.80
  id: totrans-2395
  prefs: []
  type: TYPE_NORMAL
  zh: 0.75 0.80 0.86 0.91 0.95 0.93 0.87 0.82 0.80
- en: 0.75 0.80 0.84 0.87 0.89 0.88 0.85 0.81 0.78
  id: totrans-2396
  prefs: []
  type: TYPE_NORMAL
  zh: 0.75 0.80 0.84 0.87 0.89 0.88 0.85 0.81 0.78
- en: 0.73 0.78 0.81 0.83 0.85 0.85 0.82 0.79 0.76
  id: totrans-2397
  prefs: []
  type: TYPE_NORMAL
  zh: 0.73 0.78 0.81 0.83 0.85 0.85 0.82 0.79 0.76
- en: 0.71 0.75 0.78 0.81 0.82 0.82 0.80 0.77 0.75
  id: totrans-2398
  prefs: []
  type: TYPE_NORMAL
  zh: 0.71 0.75 0.78 0.81 0.82 0.82 0.80 0.77 0.75
- en: In the coins image included in the book, the default template is a 52x52 subimage
    around the dime in the lower right corner ([Figure 15.1](ch15.html#ch15fig01)).
    The default program optionally can write a PGM file as output, with the correlation
    values converted to pixel values in the range 0..255\. For the template highlighted
    in [Figure 15.1](ch15.html#ch15fig01), the resulting image is given in [Figure
    15.2](ch15.html#ch15fig02). The other dimes are very bright, with strong matches,
    while the other coins get less intense responses.
  id: totrans-2399
  prefs: []
  type: TYPE_NORMAL
  zh: 在书中的硬币图像中，默认模板是一个52x52的子图像，位于右下角的角币周围（[图15.1](ch15.html#ch15fig01)）。默认程序可以选择性地将相关值转换为像素值范围0..255并写入PGM文件作为输出。对于[图15.1](ch15.html#ch15fig01)中高亮显示的模板，结果图像如[图15.2](ch15.html#ch15fig02)所示。其他角币非常亮，匹配度强，而其他硬币则响应较弱。
- en: '![Image](graphics/15fig01.jpg)'
  id: totrans-2400
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/15fig01.jpg)'
- en: '*Figure 15.1* Coins.pgm (with default template highlighted).'
  id: totrans-2401
  prefs: []
  type: TYPE_NORMAL
  zh: '*图15.1* Coins.pgm（高亮显示的默认模板）。'
- en: '![Image](graphics/15fig02.jpg)'
  id: totrans-2402
  prefs: []
  type: TYPE_IMG
  zh: '![图像](graphics/15fig02.jpg)'
- en: '*Figure 15.2* Correlation image with default template.'
  id: totrans-2403
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.2* 使用默认模板的相关性图像。'
- en: '*Listing 15.1.* `corrTexTex2D_kernel.`'
  id: totrans-2404
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15.1.* `corrTexTex2D_kernel.`'
- en: '[Click here to view code image](ch15_images.html#p15lis01a)'
  id: totrans-2405
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch15_images.html#p15lis01a)'
- en: '* * *'
  id: totrans-2406
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-2407
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: corrTexTex2D_kernel(
  id: totrans-2408
  prefs: []
  type: TYPE_NORMAL
  zh: corrTexTex2D_kernel(
- en: float *pCorr, size_t CorrPitch,
  id: totrans-2409
  prefs: []
  type: TYPE_NORMAL
  zh: float *pCorr, size_t CorrPitch,
- en: float cPixels,
  id: totrans-2410
  prefs: []
  type: TYPE_NORMAL
  zh: float cPixels,
- en: int xOffset, int yOffset,
  id: totrans-2411
  prefs: []
  type: TYPE_NORMAL
  zh: int xOffset, int yOffset,
- en: int xTemplate, int yTemplate,
  id: totrans-2412
  prefs: []
  type: TYPE_NORMAL
  zh: int xTemplate, int yTemplate,
- en: int wTemplate, int hTemplate,
  id: totrans-2413
  prefs: []
  type: TYPE_NORMAL
  zh: int wTemplate, int hTemplate,
- en: float xUL, float yUL, int w, int h )
  id: totrans-2414
  prefs: []
  type: TYPE_NORMAL
  zh: float xUL, float yUL, int w, int h )
- en: '{'
  id: totrans-2415
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t row = blockIdx.y*blockDim.y + threadIdx.y;
  id: totrans-2416
  prefs: []
  type: TYPE_NORMAL
  zh: size_t row = blockIdx.y*blockDim.y + threadIdx.y;
- en: size_t col = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-2417
  prefs: []
  type: TYPE_NORMAL
  zh: size_t col = blockIdx.x*blockDim.x + threadIdx.x;
- en: // adjust pCorr to point to row
  id: totrans-2418
  prefs: []
  type: TYPE_NORMAL
  zh: // 调整 pCorr 指向行
- en: pCorr = (float *) ((char *) pCorr+row*CorrPitch);
  id: totrans-2419
  prefs: []
  type: TYPE_NORMAL
  zh: pCorr = (float *) ((char *) pCorr+row*CorrPitch);
- en: // No __syncthreads in this kernel, so we can early-out
  id: totrans-2420
  prefs: []
  type: TYPE_NORMAL
  zh: // 本内核没有 __syncthreads，因此我们可以提前退出
- en: // without worrying about the effects of divergence.
  id: totrans-2421
  prefs: []
  type: TYPE_NORMAL
  zh: // 无需担心发散的影响。
- en: if ( col >= w || row >= h )
  id: totrans-2422
  prefs: []
  type: TYPE_NORMAL
  zh: if ( col >= w || row >= h )
- en: return;
  id: totrans-2423
  prefs: []
  type: TYPE_NORMAL
  zh: return;
- en: int SumI = 0;
  id: totrans-2424
  prefs: []
  type: TYPE_NORMAL
  zh: int SumI = 0;
- en: int SumT = 0;
  id: totrans-2425
  prefs: []
  type: TYPE_NORMAL
  zh: int SumT = 0;
- en: int SumISq = 0;
  id: totrans-2426
  prefs: []
  type: TYPE_NORMAL
  zh: int SumISq = 0;
- en: int SumTSq = 0;
  id: totrans-2427
  prefs: []
  type: TYPE_NORMAL
  zh: int SumTSq = 0;
- en: int SumIT = 0;
  id: totrans-2428
  prefs: []
  type: TYPE_NORMAL
  zh: int SumIT = 0;
- en: for ( int y = 0; y < hTemplate; y++ ) {
  id: totrans-2429
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int y = 0; y < hTemplate; y++ ) {
- en: for ( int x = 0; x < wTemplate; x++ ) {
  id: totrans-2430
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int x = 0; x < wTemplate; x++ ) {
- en: unsigned char I = tex2D( texImage,
  id: totrans-2431
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char I = tex2D( texImage,
- en: (float) col+xUL+xOffset+x, (float) row+yUL+yOffset+y );
  id: totrans-2432
  prefs: []
  type: TYPE_NORMAL
  zh: (float) col+xUL+xOffset+x, (float) row+yUL+yOffset+y );
- en: unsigned char T = tex2D( texTemplate,
  id: totrans-2433
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char T = tex2D( texTemplate,
- en: (float) xTemplate+x, (float) yTemplate+y);
  id: totrans-2434
  prefs: []
  type: TYPE_NORMAL
  zh: (float) xTemplate+x, (float) yTemplate+y);
- en: SumI += I;
  id: totrans-2435
  prefs: []
  type: TYPE_NORMAL
  zh: SumI += I;
- en: SumT += T;
  id: totrans-2436
  prefs: []
  type: TYPE_NORMAL
  zh: SumT += T;
- en: SumISq += I*I;
  id: totrans-2437
  prefs: []
  type: TYPE_NORMAL
  zh: SumISq += I*I;
- en: SumTSq += T*T;
  id: totrans-2438
  prefs: []
  type: TYPE_NORMAL
  zh: SumTSq += T*T;
- en: SumIT += I*T;
  id: totrans-2439
  prefs: []
  type: TYPE_NORMAL
  zh: SumIT += I*T;
- en: '}'
  id: totrans-2440
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: float fDenomExp = (float) ( (double) cPixels*SumTSq -
  id: totrans-2441
  prefs: []
  type: TYPE_NORMAL
  zh: float fDenomExp = (float) ( (double) cPixels*SumTSq -
- en: (double) SumT*SumT);
  id: totrans-2442
  prefs: []
  type: TYPE_NORMAL
  zh: (double) SumT*SumT);
- en: pCorr[col] = CorrelationValue(
  id: totrans-2443
  prefs: []
  type: TYPE_NORMAL
  zh: pCorr[col] = CorrelationValue(
- en: SumI, SumISq, SumIT, SumT, cPixels, fDenomExp );
  id: totrans-2444
  prefs: []
  type: TYPE_NORMAL
  zh: SumI, SumISq, SumIT, SumT, cPixels, fDenomExp );
- en: '}'
  id: totrans-2445
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2446
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2447
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[Listing 15.2](ch15.html#ch15lis02) gives the host code to invoke `corrTexTex2D_kernel()`.
    It is designed to work with the testing and performance measurement code in the
    sample source file `normalizedCrossCorrelation.cu`, which is why it has so many
    parameters. This host function just turns around and launches the kernel with
    the needed parameters, but later implementations of this function will check the
    device properties and launch different kernels, depending on what it finds. For
    images of a useful size, the cost of doing such checks is negligible compared
    to the GPU runtime.'
  id: totrans-2448
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 15.2](ch15.html#ch15lis02) 给出了调用 `corrTexTex2D_kernel()` 的主机代码。它是为了配合示例源文件
    `normalizedCrossCorrelation.cu` 中的测试和性能测量代码设计的，这也是它参数众多的原因。这个主机函数只是简单地调用内核并传递所需的参数，但该函数的后续实现将会检查设备属性，并根据结果启动不同的内核。对于有用大小的图像，进行这些检查的成本与
    GPU 运行时相比几乎可以忽略不计。'
- en: '*Listing 15.2.* `corrTexTex2D()` (host code).'
  id: totrans-2449
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15.2.* `corrTexTex2D()`（主机代码）。'
- en: '[Click here to view code image](ch15_images.html#p15lis02a)'
  id: totrans-2450
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch15_images.html#p15lis02a)'
- en: '* * *'
  id: totrans-2451
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: void
  id: totrans-2452
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: corrTexTex2D(
  id: totrans-2453
  prefs: []
  type: TYPE_NORMAL
  zh: corrTexTex2D(
- en: float *dCorr, int CorrPitch,
  id: totrans-2454
  prefs: []
  type: TYPE_NORMAL
  zh: float *dCorr, int CorrPitch,
- en: int wTile,
  id: totrans-2455
  prefs: []
  type: TYPE_NORMAL
  zh: int wTile,
- en: int wTemplate, int hTemplate,
  id: totrans-2456
  prefs: []
  type: TYPE_NORMAL
  zh: int wTemplate, int hTemplate,
- en: float cPixels,
  id: totrans-2457
  prefs: []
  type: TYPE_NORMAL
  zh: float cPixels,
- en: float fDenomExp,
  id: totrans-2458
  prefs: []
  type: TYPE_NORMAL
  zh: float fDenomExp,
- en: int sharedPitch,
  id: totrans-2459
  prefs: []
  type: TYPE_NORMAL
  zh: int sharedPitch,
- en: int xOffset, int yOffset,
  id: totrans-2460
  prefs: []
  type: TYPE_NORMAL
  zh: int xOffset, int yOffset,
- en: int xTemplate, int yTemplate,
  id: totrans-2461
  prefs: []
  type: TYPE_NORMAL
  zh: int xTemplate, int yTemplate,
- en: int xUL, int yUL, int w, int h,
  id: totrans-2462
  prefs: []
  type: TYPE_NORMAL
  zh: int xUL, int yUL, int w, int h,
- en: dim3 threads, dim3 blocks,
  id: totrans-2463
  prefs: []
  type: TYPE_NORMAL
  zh: dim3 threads, dim3 blocks,
- en: int sharedMem )
  id: totrans-2464
  prefs: []
  type: TYPE_NORMAL
  zh: int sharedMem )
- en: '{'
  id: totrans-2465
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: corrTexTex2D_kernel<<<blocks, threads>>>(
  id: totrans-2466
  prefs: []
  type: TYPE_NORMAL
  zh: corrTexTex2D_kernel<<<blocks, threads>>>(
- en: dCorr, CorrPitch,
  id: totrans-2467
  prefs: []
  type: TYPE_NORMAL
  zh: dCorr, CorrPitch,
- en: cPixels,
  id: totrans-2468
  prefs: []
  type: TYPE_NORMAL
  zh: cPixels,
- en: xOffset, yOffset,
  id: totrans-2469
  prefs: []
  type: TYPE_NORMAL
  zh: xOffset, yOffset,
- en: xTemplate+xOffset, yTemplate+yOffset,
  id: totrans-2470
  prefs: []
  type: TYPE_NORMAL
  zh: xTemplate + xOffset, yTemplate + yOffset,
- en: wTemplate, hTemplate,
  id: totrans-2471
  prefs: []
  type: TYPE_NORMAL
  zh: wTemplate, hTemplate,
- en: (float) xUL, (float) yUL, w, h );
  id: totrans-2472
  prefs: []
  type: TYPE_NORMAL
  zh: (float) xUL, (float) yUL, w, h );
- en: '}'
  id: totrans-2473
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2474
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: A texture-texture formulation is a very good fit if the application is choosing
    different templates as well as different images during its search—for example,
    applying transformations to the template data while comparing to the image. But
    for most applications, the template is chosen once and compared against many different
    offsets within the image. The remainder of the chapter will examine implementations
    that are optimized for that case.
  id: totrans-2475
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序在搜索过程中选择不同的模板以及不同的图像，那么纹理-纹理公式非常合适——例如，在与图像比较时对模板数据应用变换。但对于大多数应用程序，模板在开始时选择一次，并在图像中与多个不同偏移位置进行比较。本章其余部分将讨论为这种情况优化的实现。
- en: 15.3\. Template in Constant Memory
  id: totrans-2476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3\. 常量内存中的模板
- en: Most template-matching applications perform many correlation computations with
    the same template at different offsets of the input image. In that case, the template
    statistics (`SumT` and `fDenomExp`) can be precomputed, and the template data
    can be moved to special memory or otherwise premassaged. For CUDA, the obvious
    place to put the template data is in constant memory so each template pixel can
    be broadcast to the threads computing correlation values for different image locations.
  id: totrans-2477
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模板匹配应用程序在输入图像的不同偏移位置使用相同的模板执行许多相关性计算。在这种情况下，模板统计数据（`SumT` 和 `fDenomExp`）可以预先计算，模板数据可以移到特殊内存或以其他方式进行预处理。对于
    CUDA，显然应该将模板数据放入常量内存，以便每个模板像素可以广播到计算不同图像位置相关性值的线程中。
- en: The `CopyToTemplate` function given in [Listing 15.3](ch15.html#ch15lis03) pulls
    a rectangular area of pixels out of the input image, computes the statistics,
    and copies the data and statistics to `__constant__` memory.
  id: totrans-2478
  prefs: []
  type: TYPE_NORMAL
  zh: '`CopyToTemplate` 函数在[清单 15.3](ch15.html#ch15lis03)中提到，它从输入图像中提取一个矩形区域的像素，计算统计数据，并将数据和统计信息复制到
    `__constant__` 内存。'
- en: '*Listing 15.3.* `CopyToTemplate` function (error handling removed).'
  id: totrans-2479
  prefs: []
  type: TYPE_NORMAL
  zh: '*清单 15.3.* `CopyToTemplate` 函数（错误处理已移除）。'
- en: '[Click here to view code image](ch15_images.html#p15lis03a)'
  id: totrans-2480
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch15_images.html#p15lis03a)'
- en: '* * *'
  id: totrans-2481
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: cudaError_t
  id: totrans-2482
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t
- en: CopyToTemplate(
  id: totrans-2483
  prefs: []
  type: TYPE_NORMAL
  zh: CopyToTemplate(
- en: unsigned char *img, size_t imgPitch,
  id: totrans-2484
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char *img, size_t imgPitch,
- en: int xTemplate, int yTemplate,
  id: totrans-2485
  prefs: []
  type: TYPE_NORMAL
  zh: int xTemplate, int yTemplate,
- en: int wTemplate, int hTemplate,
  id: totrans-2486
  prefs: []
  type: TYPE_NORMAL
  zh: int wTemplate, int hTemplate,
- en: int OffsetX, int OffsetY
  id: totrans-2487
  prefs: []
  type: TYPE_NORMAL
  zh: int OffsetX, int OffsetY
- en: )
  id: totrans-2488
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: '{'
  id: totrans-2489
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: cudaError_t status;
  id: totrans-2490
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: unsigned char pixels[maxTemplatePixels];
  id: totrans-2491
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char pixels[maxTemplatePixels];
- en: int inx = 0;
  id: totrans-2492
  prefs: []
  type: TYPE_NORMAL
  zh: int inx = 0;
- en: int SumT = 0;
  id: totrans-2493
  prefs: []
  type: TYPE_NORMAL
  zh: int SumT = 0;
- en: int SumTSq = 0;
  id: totrans-2494
  prefs: []
  type: TYPE_NORMAL
  zh: int SumTSq = 0;
- en: int cPixels = wTemplate*hTemplate;
  id: totrans-2495
  prefs: []
  type: TYPE_NORMAL
  zh: int cPixels = wTemplate*hTemplate;
- en: size_t sizeOffsets = cPixels*sizeof(int);
  id: totrans-2496
  prefs: []
  type: TYPE_NORMAL
  zh: size_t sizeOffsets = cPixels*sizeof(int);
- en: float fSumT, fDenomExp, fcPixels;
  id: totrans-2497
  prefs: []
  type: TYPE_NORMAL
  zh: float fSumT, fDenomExp, fcPixels;
- en: cudaMemcpy2D(
  id: totrans-2498
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpy2D(
- en: pixels, wTemplate,
  id: totrans-2499
  prefs: []
  type: TYPE_NORMAL
  zh: pixels, wTemplate,
- en: img+yTemplate*imgPitch+xTemplate, imgPitch,
  id: totrans-2500
  prefs: []
  type: TYPE_NORMAL
  zh: img+yTemplate*imgPitch+xTemplate, imgPitch,
- en: wTemplate, hTemplate,
  id: totrans-2501
  prefs: []
  type: TYPE_NORMAL
  zh: wTemplate, hTemplate,
- en: cudaMemcpyDeviceToHost );
  id: totrans-2502
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyDeviceToHost );
- en: cudaMemcpyToSymbol( g_Tpix, pixels, cPixels );
  id: totrans-2503
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyToSymbol( g_Tpix, pixels, cPixels );
- en: for ( int i = OffsetY; i < OffsetY+hTemplate; i++ ) {
  id: totrans-2504
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = OffsetY; i < OffsetY+hTemplate; i++ ) {
- en: for ( int j = OffsetX; j < OffsetX+wTemplate; j++) {
  id: totrans-2505
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = OffsetX; j < OffsetX+wTemplate; j++) {
- en: SumT += pixels[inx];
  id: totrans-2506
  prefs: []
  type: TYPE_NORMAL
  zh: SumT += pixels[inx];
- en: SumTSq += pixels[inx]*pixels[inx];
  id: totrans-2507
  prefs: []
  type: TYPE_NORMAL
  zh: SumTSq += pixels[inx]*pixels[inx];
- en: poffsetx[inx] = j;
  id: totrans-2508
  prefs: []
  type: TYPE_NORMAL
  zh: poffsetx[inx] = j;
- en: poffsety[inx] = i;
  id: totrans-2509
  prefs: []
  type: TYPE_NORMAL
  zh: poffsety[inx] = i;
- en: inx += 1;
  id: totrans-2510
  prefs: []
  type: TYPE_NORMAL
  zh: inx += 1;
- en: '}'
  id: totrans-2511
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2512
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: g_cpuSumT = SumT;
  id: totrans-2513
  prefs: []
  type: TYPE_NORMAL
  zh: g_cpuSumT = SumT;
- en: g_cpuSumTSq = SumTSq;
  id: totrans-2514
  prefs: []
  type: TYPE_NORMAL
  zh: g_cpuSumTSq = SumTSq;
- en: cudaMemcpyToSymbol(g_xOffset, poffsetx, sizeOffsets);
  id: totrans-2515
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyToSymbol(g_xOffset, poffsetx, sizeOffsets);
- en: cudaMemcpyToSymbol(g_yOffset, poffsety, sizeOffsets);
  id: totrans-2516
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyToSymbol(g_yOffset, poffsety, sizeOffsets);
- en: fSumT = (float) SumT;
  id: totrans-2517
  prefs: []
  type: TYPE_NORMAL
  zh: fSumT = (float) SumT;
- en: cudaMemcpyToSymbol(g_SumT, &fSumT, sizeof(float));
  id: totrans-2518
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyToSymbol(g_SumT, &fSumT, sizeof(float));
- en: fDenomExp = float( (double)cPixels*SumTSq – (double) SumT*SumT);
  id: totrans-2519
  prefs: []
  type: TYPE_NORMAL
  zh: fDenomExp = float( (double)cPixels*SumTSq – (double) SumT*SumT);
- en: cudaMemcpyToSymbol(g_fDenomExp, &fDenomExp, sizeof(float));
  id: totrans-2520
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyToSymbol(g_fDenomExp, &fDenomExp, sizeof(float));
- en: fcPixels = (float) cPixels;
  id: totrans-2521
  prefs: []
  type: TYPE_NORMAL
  zh: fcPixels = (float) cPixels;
- en: cudaMemcpyToSymbol(g_cPixels, &fcPixels, sizeof(float));
  id: totrans-2522
  prefs: []
  type: TYPE_NORMAL
  zh: cudaMemcpyToSymbol(g_cPixels, &fcPixels, sizeof(float));
- en: 'Error:'
  id: totrans-2523
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: return status;
  id: totrans-2524
  prefs: []
  type: TYPE_NORMAL
  zh: return status;
- en: '}'
  id: totrans-2525
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2526
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The `corrTemplate2D()` kernel given in [Listing 15.4](ch15.html#ch15lis04) then
    can read the template values from `g_TPix[]`, which resides in constant memory.
    `corrTemplate2D()` is even simpler and shorter than `corrTexTex2D()`, since it
    does not have to compute the template statistics.
  id: totrans-2527
  prefs: []
  type: TYPE_NORMAL
  zh: '`corrTemplate2D()` 核函数给出的[列表 15.4](ch15.html#ch15lis04)可以从 `g_TPix[]` 中读取模板值，该值存在于常量内存中。`corrTemplate2D()`
    比 `corrTexTex2D()` 更简洁和更短，因为它不需要计算模板统计数据。'
- en: '*Listing 15.4.* `corrTemplate2D` kernel.'
  id: totrans-2528
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15.4.* `corrTemplate2D` 核函数。'
- en: '[Click here to view code image](ch15_images.html#p15lis04a)'
  id: totrans-2529
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch15_images.html#p15lis04a)'
- en: '* * *'
  id: totrans-2530
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-2531
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: corrTemplate2D_kernel(
  id: totrans-2532
  prefs: []
  type: TYPE_NORMAL
  zh: corrTemplate2D_kernel(
- en: float *pCorr, size_t CorrPitch,
  id: totrans-2533
  prefs: []
  type: TYPE_NORMAL
  zh: float *pCorr, size_t CorrPitch,
- en: float cPixels, float fDenomExp,
  id: totrans-2534
  prefs: []
  type: TYPE_NORMAL
  zh: float cPixels, float fDenomExp,
- en: float xUL, float yUL, int w, int h,
  id: totrans-2535
  prefs: []
  type: TYPE_NORMAL
  zh: float xUL, float yUL, int w, int h,
- en: int xOffset, int yOffset,
  id: totrans-2536
  prefs: []
  type: TYPE_NORMAL
  zh: int xOffset, int yOffset,
- en: int wTemplate, int hTemplate )
  id: totrans-2537
  prefs: []
  type: TYPE_NORMAL
  zh: int wTemplate, int hTemplate )
- en: '{'
  id: totrans-2538
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: size_t row = blockIdx.y*blockDim.y + threadIdx.y;
  id: totrans-2539
  prefs: []
  type: TYPE_NORMAL
  zh: size_t row = blockIdx.y*blockDim.y + threadIdx.y;
- en: size_t col = blockIdx.x*blockDim.x + threadIdx.x;
  id: totrans-2540
  prefs: []
  type: TYPE_NORMAL
  zh: size_t col = blockIdx.x*blockDim.x + threadIdx.x;
- en: // adjust pointers to row
  id: totrans-2541
  prefs: []
  type: TYPE_NORMAL
  zh: // 调整指针到行
- en: pCorr = (float *) ((char *) pCorr+row*CorrPitch);
  id: totrans-2542
  prefs: []
  type: TYPE_NORMAL
  zh: pCorr = (float *) ((char *) pCorr+row*CorrPitch);
- en: // No __syncthreads in this kernel, so we can early-out
  id: totrans-2543
  prefs: []
  type: TYPE_NORMAL
  zh: // 此核函数没有 __syncthreads，因此可以提前退出
- en: // without worrying about the effects of divergence.
  id: totrans-2544
  prefs: []
  type: TYPE_NORMAL
  zh: // 无需担心分支失效的影响。
- en: if ( col >= w || row >= h )
  id: totrans-2545
  prefs: []
  type: TYPE_NORMAL
  zh: if ( col >= w || row >= h )
- en: return;
  id: totrans-2546
  prefs: []
  type: TYPE_NORMAL
  zh: return;
- en: int SumI = 0;
  id: totrans-2547
  prefs: []
  type: TYPE_NORMAL
  zh: int SumI = 0;
- en: int SumISq = 0;
  id: totrans-2548
  prefs: []
  type: TYPE_NORMAL
  zh: int SumISq = 0;
- en: int SumIT = 0;
  id: totrans-2549
  prefs: []
  type: TYPE_NORMAL
  zh: int SumIT = 0;
- en: int inx = 0;
  id: totrans-2550
  prefs: []
  type: TYPE_NORMAL
  zh: int inx = 0;
- en: for ( int j = 0; j < hTemplate; j++ ) {
  id: totrans-2551
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < hTemplate; j++ ) {
- en: for ( int i = 0; i < wTemplate; i++ ) {
  id: totrans-2552
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < wTemplate; i++ ) {
- en: unsigned char I = tex2D( texImage,
  id: totrans-2553
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char I = tex2D( texImage,
- en: (float) col+xUL+xOffset+i,
  id: totrans-2554
  prefs: []
  type: TYPE_NORMAL
  zh: (float) col+xUL+xOffset+i,
- en: (float) row+yUL+yOffset+j );
  id: totrans-2555
  prefs: []
  type: TYPE_NORMAL
  zh: (float) row+yUL+yOffset+j );
- en: unsigned char T = g_Tpix[inx++];
  id: totrans-2556
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char T = g_Tpix[inx++];
- en: SumI += I;
  id: totrans-2557
  prefs: []
  type: TYPE_NORMAL
  zh: SumI += I;
- en: SumISq += I*I;
  id: totrans-2558
  prefs: []
  type: TYPE_NORMAL
  zh: SumISq += I*I;
- en: SumIT += I*T;
  id: totrans-2559
  prefs: []
  type: TYPE_NORMAL
  zh: SumIT += I*T;
- en: '}'
  id: totrans-2560
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2561
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: pCorr[col] =
  id: totrans-2562
  prefs: []
  type: TYPE_NORMAL
  zh: pCorr[col] =
- en: CorrelationValue(
  id: totrans-2563
  prefs: []
  type: TYPE_NORMAL
  zh: 相关值(
- en: SumI, SumISq, SumIT, g_SumT, cPixels, fDenomExp );
  id: totrans-2564
  prefs: []
  type: TYPE_NORMAL
  zh: SumI, SumISq, SumIT, g_SumT, cPixels, fDenomExp );
- en: '}'
  id: totrans-2565
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2566
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 15.4\. Image in Shared Memory
  id: totrans-2567
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4\. 共享内存中的图像
- en: For rectangles of correlation values such as the ones computed by our sample
    program, the CUDA kernel exhibits a tremendous amount of reuse of the image data
    as the template matches are swept across the image. So far, our code has relied
    on the texture caches to service these redundant reads without going to external
    memory. For smaller templates, however, shared memory can be used to further increase
    performance by making the image data available with lower latency.
  id: totrans-2568
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像我们的示例程序计算出的相关值矩形，CUDA 内核在模板匹配穿过图像时展示了大量的图像数据重用。到目前为止，我们的代码依赖纹理缓存来处理这些冗余读取，而不需要访问外部内存。然而，对于较小的模板，可以使用共享内存通过降低延迟进一步提高性能，使图像数据更容易访问。
- en: The kernels in [Listings 15.1](ch15.html#ch15lis01) and [15.3](ch15.html#ch15lis03)
    implicitly divided the input image into tiles that were the same size as the threadblocks.
    For our shared memory implementation shown in [Listing 15.5](ch15.html#ch15lis05),
    we’ll use the height of the threadblock (`blockDim.y`) but specify an explicit
    tile width of `wTile`. In our sample program, `wTile` is 32\. [Figure 15.4](ch15.html#ch15fig04)
    shows how the kernel “overfetches” a rectangle of `wTemplate×hTemplate` pixels
    outside the tile; boundary conditions are handled by the texture addressing mode.
    Once the shared memory has been populated with image data, the kernel does `__syncthreads()`
    and computes and writes out the tile’s correlation coefficients.
  id: totrans-2569
  prefs: []
  type: TYPE_NORMAL
  zh: '[Listing 15.1](ch15.html#ch15lis01) 和 [15.3](ch15.html#ch15lis03) 中的内核隐式地将输入图像划分为与线程块相同大小的瓦片。对于我们在
    [Listing 15.5](ch15.html#ch15lis05) 中展示的共享内存实现，我们将使用线程块的高度（`blockDim.y`），但指定一个显式的瓦片宽度
    `wTile`。在我们的示例程序中，`wTile` 为 32\. [图 15.4](ch15.html#ch15fig04) 显示了内核如何“超取”一个 `wTemplate×hTemplate`
    像素的矩形区域，超出了瓦片的范围；边界条件通过纹理寻址模式处理。共享内存填充了图像数据后，内核执行 `__syncthreads()` 并计算并写出瓦片的相关系数。'
- en: '*Listing 15.5.* `corrShared_kernel().`'
  id: totrans-2570
  prefs: []
  type: TYPE_NORMAL
  zh: '*Listing 15.5.* `corrShared_kernel().`'
- en: '[Click here to view code image](ch15_images.html#p15lis05a)'
  id: totrans-2571
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图像](ch15_images.html#p15lis05a)'
- en: '* * *'
  id: totrans-2572
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: __global__ void
  id: totrans-2573
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: corrShared_kernel(
  id: totrans-2574
  prefs: []
  type: TYPE_NORMAL
  zh: corrShared_kernel(
- en: float *pCorr, size_t CorrPitch,
  id: totrans-2575
  prefs: []
  type: TYPE_NORMAL
  zh: float *pCorr, size_t CorrPitch,
- en: int wTile,
  id: totrans-2576
  prefs: []
  type: TYPE_NORMAL
  zh: int wTile,
- en: int wTemplate, int hTemplate,
  id: totrans-2577
  prefs: []
  type: TYPE_NORMAL
  zh: int wTemplate, int hTemplate,
- en: float xOffset, float yOffset,
  id: totrans-2578
  prefs: []
  type: TYPE_NORMAL
  zh: float xOffset, float yOffset,
- en: float cPixels, float fDenomExp, int SharedPitch,
  id: totrans-2579
  prefs: []
  type: TYPE_NORMAL
  zh: float cPixels, float fDenomExp, int SharedPitch,
- en: float xUL, float yUL, int w, int h )
  id: totrans-2580
  prefs: []
  type: TYPE_NORMAL
  zh: float xUL, float yUL, int w, int h )
- en: '{'
  id: totrans-2581
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int uTile = blockIdx.x*wTile;
  id: totrans-2582
  prefs: []
  type: TYPE_NORMAL
  zh: int uTile = blockIdx.x*wTile;
- en: int vTile = blockIdx.y*blockDim.y;
  id: totrans-2583
  prefs: []
  type: TYPE_NORMAL
  zh: int vTile = blockIdx.y*blockDim.y;
- en: int v = vTile + threadIdx.y;
  id: totrans-2584
  prefs: []
  type: TYPE_NORMAL
  zh: int v = vTile + threadIdx.y;
- en: float *pOut = (float *) (((char *) pCorr)+v*CorrPitch);
  id: totrans-2585
  prefs: []
  type: TYPE_NORMAL
  zh: float *pOut = (float *) (((char *) pCorr)+v*CorrPitch);
- en: for ( int row = threadIdx.y;
  id: totrans-2586
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int row = threadIdx.y;
- en: row < blockDim.y+hTemplate;
  id: totrans-2587
  prefs: []
  type: TYPE_NORMAL
  zh: row < blockDim.y+hTemplate;
- en: row += blockDim.y ) {
  id: totrans-2588
  prefs: []
  type: TYPE_NORMAL
  zh: row += blockDim.y ) {
- en: int SharedIdx = row * SharedPitch;
  id: totrans-2589
  prefs: []
  type: TYPE_NORMAL
  zh: int SharedIdx = row * SharedPitch;
- en: for ( int col = threadIdx.x;
  id: totrans-2590
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int col = threadIdx.x;
- en: col < wTile+wTemplate;
  id: totrans-2591
  prefs: []
  type: TYPE_NORMAL
  zh: col < wTile+wTemplate;
- en: col += blockDim.x ) {
  id: totrans-2592
  prefs: []
  type: TYPE_NORMAL
  zh: col += blockDim.x ) {
- en: LocalBlock[SharedIdx+col] =
  id: totrans-2593
  prefs: []
  type: TYPE_NORMAL
  zh: LocalBlock[SharedIdx+col] =
- en: tex2D( texImage,
  id: totrans-2594
  prefs: []
  type: TYPE_NORMAL
  zh: tex2D( texImage,
- en: (float) (uTile+col+xUL+xOffset),
  id: totrans-2595
  prefs: []
  type: TYPE_NORMAL
  zh: (float) (uTile+col+xUL+xOffset),
- en: (float) (vTile+row+yUL+yOffset) );
  id: totrans-2596
  prefs: []
  type: TYPE_NORMAL
  zh: (float) (vTile+row+yUL+yOffset) );
- en: '}'
  id: totrans-2597
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2598
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-2599
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: for ( int col = threadIdx.x;
  id: totrans-2600
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int col = threadIdx.x;
- en: col < wTile;
  id: totrans-2601
  prefs: []
  type: TYPE_NORMAL
  zh: col < wTile;
- en: col += blockDim.x ) {
  id: totrans-2602
  prefs: []
  type: TYPE_NORMAL
  zh: col += blockDim.x ) {
- en: int SumI = 0;
  id: totrans-2603
  prefs: []
  type: TYPE_NORMAL
  zh: int SumI = 0;
- en: int SumISq = 0;
  id: totrans-2604
  prefs: []
  type: TYPE_NORMAL
  zh: int SumISq = 0;
- en: int SumIT = 0;
  id: totrans-2605
  prefs: []
  type: TYPE_NORMAL
  zh: int SumIT = 0;
- en: int idx = 0;
  id: totrans-2606
  prefs: []
  type: TYPE_NORMAL
  zh: int idx = 0;
- en: int SharedIdx = threadIdx.y * SharedPitch + col;
  id: totrans-2607
  prefs: []
  type: TYPE_NORMAL
  zh: int SharedIdx = threadIdx.y * SharedPitch + col;
- en: for ( int j = 0; j < hTemplate; j++ ) {
  id: totrans-2608
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < hTemplate; j++ ) {
- en: for ( int i = 0; i < wTemplate; i++) {
  id: totrans-2609
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < wTemplate; i++) {
- en: unsigned char I = LocalBlock[SharedIdx+i];
  id: totrans-2610
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char I = LocalBlock[SharedIdx+i];
- en: unsigned char T = g_Tpix[idx++];
  id: totrans-2611
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char T = g_Tpix[idx++];
- en: SumI += I;
  id: totrans-2612
  prefs: []
  type: TYPE_NORMAL
  zh: SumI += I;
- en: SumISq += I*I;
  id: totrans-2613
  prefs: []
  type: TYPE_NORMAL
  zh: SumISq += I*I;
- en: SumIT += I*T;
  id: totrans-2614
  prefs: []
  type: TYPE_NORMAL
  zh: SumIT += I*T;
- en: '}'
  id: totrans-2615
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: SharedIdx += SharedPitch;
  id: totrans-2616
  prefs: []
  type: TYPE_NORMAL
  zh: SharedIdx += SharedPitch;
- en: '}'
  id: totrans-2617
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: if ( uTile+col < w && v < h ) {
  id: totrans-2618
  prefs: []
  type: TYPE_NORMAL
  zh: if ( uTile+col < w && v < h ) {
- en: pOut[uTile+col] =
  id: totrans-2619
  prefs: []
  type: TYPE_NORMAL
  zh: pOut[uTile+col] =
- en: CorrelationValue( SumI, SumISq, SumIT, g_SumT,
  id: totrans-2620
  prefs: []
  type: TYPE_NORMAL
  zh: CorrelationValue( SumI, SumISq, SumIT, g_SumT,
- en: cPixels, fDenomExp );
  id: totrans-2621
  prefs: []
  type: TYPE_NORMAL
  zh: cPixels, fDenomExp );
- en: '}'
  id: totrans-2622
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2623
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: __syncthreads();
  id: totrans-2624
  prefs: []
  type: TYPE_NORMAL
  zh: __syncthreads();
- en: '}'
  id: totrans-2625
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2626
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: To ensure that shared memory references will avoid bank conflicts from one row
    to the next, the amount of shared memory per row is padded to the next multiple
    of 64.
  id: totrans-2627
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保共享内存引用能避免从一行到下一行的银行冲突，每行的共享内存量被填充到下一个64的倍数。
- en: sharedPitch = ~63&(((wTile+wTemplate)+63));
  id: totrans-2628
  prefs: []
  type: TYPE_NORMAL
  zh: sharedPitch = ~63&(((wTile+wTemplate)+63));
- en: The total amount of shared memory needed per block is then the pitch multiplied
    by the number of rows (block height plus template height).
  id: totrans-2629
  prefs: []
  type: TYPE_NORMAL
  zh: 每个块所需的共享内存总量是通过将宽度乘以行数（块高度加上模板高度）得到的。
- en: sharedMem = sharedPitch*(threads.y+hTemplate);
  id: totrans-2630
  prefs: []
  type: TYPE_NORMAL
  zh: sharedMem = sharedPitch*(threads.y+hTemplate);
- en: The host code to launch `corrShared_kernel()`, shown in [Listing 15.6](ch15.html#ch15lis06),
    detects whether the kernel launch will require more shared memory than is available.
    If that is the case, it calls `corrTexTex2D()`, which will work for any template
    size.
  id: totrans-2631
  prefs: []
  type: TYPE_NORMAL
  zh: 启动 `corrShared_kernel()` 的主机代码，如[列表 15.6](ch15.html#ch15lis06)，检测内核启动是否会需要比可用共享内存更多的内存。如果是这种情况，它会调用
    `corrTexTex2D()`，该函数适用于任何模板大小。
- en: '*Listing 15.6.* `corrShared()` (host code).'
  id: totrans-2632
  prefs: []
  type: TYPE_NORMAL
  zh: '*列表 15.6.* `corrShared()` (主机代码)。'
- en: '[Click here to view code image](ch15_images.html#p15lis06a)'
  id: totrans-2633
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击这里查看代码图片](ch15_images.html#p15lis06a)'
- en: '* * *'
  id: totrans-2634
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: void
  id: totrans-2635
  prefs: []
  type: TYPE_NORMAL
  zh: void
- en: corrShared(
  id: totrans-2636
  prefs: []
  type: TYPE_NORMAL
  zh: corrShared(
- en: float *dCorr, int CorrPitch,
  id: totrans-2637
  prefs: []
  type: TYPE_NORMAL
  zh: float *dCorr, int CorrPitch,
- en: int wTile,
  id: totrans-2638
  prefs: []
  type: TYPE_NORMAL
  zh: int wTile,
- en: int wTemplate, int hTemplate,
  id: totrans-2639
  prefs: []
  type: TYPE_NORMAL
  zh: int wTemplate, int hTemplate,
- en: float cPixels,
  id: totrans-2640
  prefs: []
  type: TYPE_NORMAL
  zh: float cPixels,
- en: float fDenomExp,
  id: totrans-2641
  prefs: []
  type: TYPE_NORMAL
  zh: float fDenomExp,
- en: int sharedPitch,
  id: totrans-2642
  prefs: []
  type: TYPE_NORMAL
  zh: int sharedPitch,
- en: int xOffset, int yOffset,
  id: totrans-2643
  prefs: []
  type: TYPE_NORMAL
  zh: int xOffset, int yOffset,
- en: int xTemplate, int yTemplate,
  id: totrans-2644
  prefs: []
  type: TYPE_NORMAL
  zh: int xTemplate, int yTemplate,
- en: int xUL, int yUL, int w, int h,
  id: totrans-2645
  prefs: []
  type: TYPE_NORMAL
  zh: int xUL, int yUL, int w, int h,
- en: dim3 threads, dim3 blocks,
  id: totrans-2646
  prefs: []
  type: TYPE_NORMAL
  zh: dim3 threads, dim3 blocks,
- en: int sharedMem )
  id: totrans-2647
  prefs: []
  type: TYPE_NORMAL
  zh: int sharedMem )
- en: '{'
  id: totrans-2648
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: int device;
  id: totrans-2649
  prefs: []
  type: TYPE_NORMAL
  zh: int device;
- en: cudaDeviceProp props;
  id: totrans-2650
  prefs: []
  type: TYPE_NORMAL
  zh: cudaDeviceProp props;
- en: cudaError_t status;
  id: totrans-2651
  prefs: []
  type: TYPE_NORMAL
  zh: cudaError_t status;
- en: CUDART_CHECK( cudaGetDevice( &device ) );
  id: totrans-2652
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaGetDevice( &device ) );
- en: CUDART_CHECK( cudaGetDeviceProperties( &props, device ) );
  id: totrans-2653
  prefs: []
  type: TYPE_NORMAL
  zh: CUDART_CHECK( cudaGetDeviceProperties( &props, device ) );
- en: if ( sharedMem > props.sharedMemPerBlock ) {
  id: totrans-2654
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( sharedMem > props.sharedMemPerBlock ) {
- en: dim3 threads88(8, 8, 1);
  id: totrans-2655
  prefs: []
  type: TYPE_NORMAL
  zh: dim3 threads88(8, 8, 1);
- en: dim3 blocks88;
  id: totrans-2656
  prefs: []
  type: TYPE_NORMAL
  zh: dim3 blocks88;
- en: blocks88.x = INTCEIL(w,8);
  id: totrans-2657
  prefs: []
  type: TYPE_NORMAL
  zh: blocks88.x = INTCEIL(w,8);
- en: blocks88.y = INTCEIL(h,8);
  id: totrans-2658
  prefs: []
  type: TYPE_NORMAL
  zh: blocks88.y = INTCEIL(h,8);
- en: blocks88.z = 1;
  id: totrans-2659
  prefs: []
  type: TYPE_NORMAL
  zh: blocks88.z = 1;
- en: return corrTexTex2D(
  id: totrans-2660
  prefs: []
  type: TYPE_NORMAL
  zh: return corrTexTex2D(
- en: dCorr, CorrPitch,
  id: totrans-2661
  prefs: []
  type: TYPE_NORMAL
  zh: dCorr, CorrPitch,
- en: wTile,
  id: totrans-2662
  prefs: []
  type: TYPE_NORMAL
  zh: wTile,
- en: wTemplate, hTemplate,
  id: totrans-2663
  prefs: []
  type: TYPE_NORMAL
  zh: wTemplate, hTemplate,
- en: cPixels,
  id: totrans-2664
  prefs: []
  type: TYPE_NORMAL
  zh: cPixels,
- en: fDenomExp,
  id: totrans-2665
  prefs: []
  type: TYPE_NORMAL
  zh: fDenomExp,
- en: sharedPitch,
  id: totrans-2666
  prefs: []
  type: TYPE_NORMAL
  zh: sharedPitch,
- en: xOffset, yOffset,
  id: totrans-2667
  prefs: []
  type: TYPE_NORMAL
  zh: xOffset, yOffset,
- en: xTemplate, yTemplate,
  id: totrans-2668
  prefs: []
  type: TYPE_NORMAL
  zh: xTemplate, yTemplate,
- en: xUL, yUL, w, h,
  id: totrans-2669
  prefs: []
  type: TYPE_NORMAL
  zh: xUL, yUL, w, h,
- en: threads88, blocks88,
  id: totrans-2670
  prefs: []
  type: TYPE_NORMAL
  zh: threads88, blocks88,
- en: sharedMem );
  id: totrans-2671
  prefs: []
  type: TYPE_NORMAL
  zh: sharedMem );
- en: '}'
  id: totrans-2672
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: corrShared_kernel<<<blocks, threads, sharedMem>>>(
  id: totrans-2673
  prefs: []
  type: TYPE_NORMAL
  zh: corrShared_kernel<<<blocks, threads, sharedMem>>>(
- en: dCorr, CorrPitch,
  id: totrans-2674
  prefs: []
  type: TYPE_NORMAL
  zh: dCorr, CorrPitch,
- en: wTile,
  id: totrans-2675
  prefs: []
  type: TYPE_NORMAL
  zh: wTile,
- en: wTemplate, hTemplate,
  id: totrans-2676
  prefs: []
  type: TYPE_NORMAL
  zh: wTemplate, hTemplate,
- en: (float) xOffset, (float) yOffset,
  id: totrans-2677
  prefs: []
  type: TYPE_NORMAL
  zh: (float) xOffset, (float) yOffset,
- en: cPixels, fDenomExp,
  id: totrans-2678
  prefs: []
  type: TYPE_NORMAL
  zh: cPixels, fDenomExp,
- en: sharedPitch,
  id: totrans-2679
  prefs: []
  type: TYPE_NORMAL
  zh: sharedPitch,
- en: (float) xUL, (float) yUL, w, h );
  id: totrans-2680
  prefs: []
  type: TYPE_NORMAL
  zh: (float) xUL, (float) yUL, w, h );
- en: 'Error:'
  id: totrans-2681
  prefs: []
  type: TYPE_NORMAL
  zh: 错误：
- en: return;
  id: totrans-2682
  prefs: []
  type: TYPE_NORMAL
  zh: return;
- en: '}'
  id: totrans-2683
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '* * *'
  id: totrans-2684
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 15.5\. Further Optimizations
  id: totrans-2685
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.5\. 进一步优化
- en: 'Two more optimizations are implemented in the sample source code: SM-aware
    kernel invocation (since SM 1.x has different instruction set support for multiplication,
    which is in the innermost loop of this computation) and an unrolled inner loop
    of the kernel.'
  id: totrans-2686
  prefs: []
  type: TYPE_NORMAL
  zh: 示例源代码中还实现了两个优化：基于SM的内核调用（因为SM 1.x对于乘法操作有不同的指令集支持，而该乘法操作位于计算的最内层循环中），以及内核的内层循环展开。
- en: 15.5.1\. SM-Aware Coding
  id: totrans-2687
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.5.1\. 基于SM的编码
- en: SM 1.x hardware uses a 24-bit multiplier (plenty wide enough to do the multiplications
    in the inner loop of this computation), yet SM 2.x and SM 3.x hardware use 32-bit
    multipliers. Sometimes the compiler can detect when the participating integers
    are narrow enough that it can use the 24-bit multiply on SM 1.x–class hardware,
    but that does not seem to be the case for `corrShared_kernel()`. To work around
    the issue, we can use a template on the kernel declaration.
  id: totrans-2688
  prefs: []
  type: TYPE_NORMAL
  zh: SM 1.x 硬件使用的是 24 位乘法器（足够宽以完成计算中的最内层循环的乘法操作），而 SM 2.x 和 SM 3.x 硬件使用的是 32 位乘法器。有时编译器可以检测到当参与的整数足够窄时，它可以在
    SM 1.x 类硬件上使用 24 位乘法，但这似乎不适用于 `corrShared_kernel()`。为了解决这个问题，我们可以在内核声明中使用模板。
- en: template<bool bSM1>
  id: totrans-2689
  prefs: []
  type: TYPE_NORMAL
  zh: template<bool bSM1>
- en: __global__ void
  id: totrans-2690
  prefs: []
  type: TYPE_NORMAL
  zh: __global__ void
- en: corrSharedSM_kernel( ... )
  id: totrans-2691
  prefs: []
  type: TYPE_NORMAL
  zh: corrSharedSM_kernel( ... )
- en: The inner loop of the kernel then becomes
  id: totrans-2692
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，内核的内层循环变为
- en: '[Click here to view code image](ch15_images.html#p463pro01a)'
  id: totrans-2693
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch15_images.html#p463pro01a)'
- en: for ( int j = 0; j < hTemplate; j++ ) {
  id: totrans-2694
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < hTemplate; j++ ) {
- en: for ( int i = 0; i < wTemplate; i++) {
  id: totrans-2695
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < wTemplate; i++) {
- en: unsigned char I = LocalBlock[SharedIdx+i];
  id: totrans-2696
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char I = LocalBlock[SharedIdx+i];
- en: unsigned char T = g_Tpix[idx++];
  id: totrans-2697
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char T = g_Tpix[idx++];
- en: SumI += I;
  id: totrans-2698
  prefs: []
  type: TYPE_NORMAL
  zh: SumI += I;
- en: if ( bSM1 ) {
  id: totrans-2699
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ( bSM1 ) {
- en: SumISq += __umul24(I, I);
  id: totrans-2700
  prefs: []
  type: TYPE_NORMAL
  zh: SumISq += __umul24(I, I);
- en: SumIT += __umul24(I, T);
  id: totrans-2701
  prefs: []
  type: TYPE_NORMAL
  zh: SumIT += __umul24(I, T);
- en: '}'
  id: totrans-2702
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else {
  id: totrans-2703
  prefs: []
  type: TYPE_NORMAL
  zh: 否则 {
- en: SumISq += I*I;
  id: totrans-2704
  prefs: []
  type: TYPE_NORMAL
  zh: SumISq += I*I;
- en: SumIT += I*T;
  id: totrans-2705
  prefs: []
  type: TYPE_NORMAL
  zh: SumIT += I*T;
- en: '}'
  id: totrans-2706
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2707
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: SharedIdx += SharedPitch;
  id: totrans-2708
  prefs: []
  type: TYPE_NORMAL
  zh: SharedIdx += SharedPitch;
- en: '}'
  id: totrans-2709
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: And the host function that invokes the kernel must detect whether the device
    is SM 1.x and, if so, invoke the kernel with `bSM1=true`. In the sample source
    code, this implementation is given in the `corrSharedSM.cuh` and `corrSharedSMSums.cuh`
    header files.
  id: totrans-2710
  prefs: []
  type: TYPE_NORMAL
  zh: 而调用内核的主机函数必须检测设备是否为 SM 1.x，如果是，则用 `bSM1=true` 来调用内核。在示例源代码中，这一实现给出了 `corrSharedSM.cuh`
    和 `corrSharedSMSums.cuh` 头文件。
- en: 15.5.2\. Loop Unrolling
  id: totrans-2711
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 15.5.2\. 循环展开
- en: Since each thread is accessing adjacent bytes in shared memory, the innermost
    loop of these kernels generates 4-way bank conflicts on SM 1.x-class hardware.
    If we rewrite
  id: totrans-2712
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个线程都在访问共享内存中的相邻字节，这些内核的最内层循环会在 SM 1.x 类硬件上产生 4 路 bank 冲突。如果我们重写
- en: '[Click here to view code image](ch15_images.html#p464pro01a)'
  id: totrans-2713
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch15_images.html#p464pro01a)'
- en: for ( int j = 0; j < hTemplate; j++ ) {
  id: totrans-2714
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < hTemplate; j++ ) {
- en: for ( int i = 0; i < wTemplate; i++) {
  id: totrans-2715
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < wTemplate; i++) {
- en: unsigned char I = LocalBlock[SharedIdx+i];
  id: totrans-2716
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char I = LocalBlock[SharedIdx+i];
- en: unsigned char T = g_Tpix[idx++];
  id: totrans-2717
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char T = g_Tpix[idx++];
- en: SumI += I;
  id: totrans-2718
  prefs: []
  type: TYPE_NORMAL
  zh: SumI += I;
- en: SumISq += I*I;
  id: totrans-2719
  prefs: []
  type: TYPE_NORMAL
  zh: SumISq += I*I;
- en: SumIT += I*T;
  id: totrans-2720
  prefs: []
  type: TYPE_NORMAL
  zh: SumIT += I*T;
- en: '}'
  id: totrans-2721
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: SharedIdx += SharedPitch;
  id: totrans-2722
  prefs: []
  type: TYPE_NORMAL
  zh: SharedIdx += SharedPitch;
- en: '}'
  id: totrans-2723
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: as follows
  id: totrans-2724
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示
- en: '[Click here to view code image](ch15_images.html#p464pro02a)'
  id: totrans-2725
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch15_images.html#p464pro02a)'
- en: for ( int j = 0; j < hTemplate; j++ ) {
  id: totrans-2726
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int j = 0; j < hTemplate; j++ ) {
- en: for ( int i = 0; i < wTemplate/4; i++) {
  id: totrans-2727
  prefs: []
  type: TYPE_NORMAL
  zh: for ( int i = 0; i < wTemplate/4; i++) {
- en: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+0], );
  id: totrans-2728
  prefs: []
  type: TYPE_NORMAL
  zh: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+0], );
- en: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+1], );
  id: totrans-2729
  prefs: []
  type: TYPE_NORMAL
  zh: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+1], );
- en: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+2], );
  id: totrans-2730
  prefs: []
  type: TYPE_NORMAL
  zh: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+2], );
- en: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+3], );
  id: totrans-2731
  prefs: []
  type: TYPE_NORMAL
  zh: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+3], );
- en: '}'
  id: totrans-2732
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: SharedIdx += SharedPitch;
  id: totrans-2733
  prefs: []
  type: TYPE_NORMAL
  zh: SharedIdx += SharedPitch;
- en: '}'
  id: totrans-2734
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: where the `corrSharedAccumulate()` function encapsulates the template parameter
    bSM1
  id: totrans-2735
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `corrSharedAccumulate()` 函数封装了模板参数 bSM1
- en: '[Click here to view code image](ch15_images.html#p464pro03a)'
  id: totrans-2736
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图像](ch15_images.html#p464pro03a)'
- en: template<bool bSM1>
  id: totrans-2737
  prefs: []
  type: TYPE_NORMAL
  zh: template<bool bSM1>
- en: __device__ void
  id: totrans-2738
  prefs: []
  type: TYPE_NORMAL
  zh: __device__ void
- en: corrSharedAccumulate(
  id: totrans-2739
  prefs: []
  type: TYPE_NORMAL
  zh: corrSharedAccumulate(
- en: int& SumI, int& SumISq, int& SumIT,
  id: totrans-2740
  prefs: []
  type: TYPE_NORMAL
  zh: int& SumI, int& SumISq, int& SumIT,
- en: unsigned char I, unsigned char T )
  id: totrans-2741
  prefs: []
  type: TYPE_NORMAL
  zh: unsigned char I, unsigned char T )
- en: '{'
  id: totrans-2742
  prefs: []
  type: TYPE_NORMAL
  zh: '{'
- en: SumI += I;
  id: totrans-2743
  prefs: []
  type: TYPE_NORMAL
  zh: SumI += I;
- en: if ( bSM1 ) {
  id: totrans-2744
  prefs: []
  type: TYPE_NORMAL
  zh: if ( bSM1 ) {
- en: SumISq += __umul24(I,I);
  id: totrans-2745
  prefs: []
  type: TYPE_NORMAL
  zh: SumISq += __umul24(I,I);
- en: SumIT += __umul24(I,T);
  id: totrans-2746
  prefs: []
  type: TYPE_NORMAL
  zh: SumIT += __umul24(I,T);
- en: '}'
  id: totrans-2747
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: else {
  id: totrans-2748
  prefs: []
  type: TYPE_NORMAL
  zh: else {
- en: SumISq += I*I;
  id: totrans-2749
  prefs: []
  type: TYPE_NORMAL
  zh: SumISq += I*I;
- en: SumIT += I*T;
  id: totrans-2750
  prefs: []
  type: TYPE_NORMAL
  zh: SumIT += I*T;
- en: '}'
  id: totrans-2751
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '}'
  id: totrans-2752
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Although the primary motivation is to decrease bank conflicts due to byte reads—an
    effect that only occurs on SM 1.x hardware—the resulting kernel is faster on all
    CUDA hardware.
  id: totrans-2753
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管主要的动机是减少由于字节读取而导致的 bank 冲突——这种效应只在 SM 1.x 硬件上发生——但最终的内核在所有 CUDA 硬件上都更快。
- en: 15.6\. Source Code
  id: totrans-2754
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.6\. 源代码
- en: When working on optimized normalized cross-correlation code, it does not take
    long to realize that it’s surprisingly difficult and error-prone. Converting the
    sums to correlation coefficients, as described in [Section 15.1](ch15.html#ch15lev1sec1),
    must be done carefully due to the precision characteristics of `float` versus
    `int` (`float` has a greater dynamic range, but only 24 bits of precision). It
    is good practice to develop separate subroutines that report the computed sums
    to root cause whether a given implementation is reporting incorrect coefficients
    due to incorrect sums or an incorrect coefficient computation. Also, the sums
    can be bitwise-compared with CPU results, while the float-valued coefficients
    must be fuzzily compared against an epsilon value.
  id: totrans-2755
  prefs: []
  type: TYPE_NORMAL
  zh: 在优化过的标准化互相关代码中，工作时不难意识到，它既出乎意料地困难又容易出错。将求和转换为相关系数，如[第15.1节](ch15.html#ch15lev1sec1)所述，必须小心处理，因为`float`与`int`的精度特性不同（`float`具有更大的动态范围，但精度只有24位）。一个好的做法是开发单独的子程序，将计算出的求和报告给根本原因，以确定给定实现是否因为错误的求和或错误的系数计算而报告了不正确的系数。此外，求和可以与CPU结果逐位比较，而浮动值的系数则必须与一个ε值进行模糊比较。
- en: The different implementations of correlation are broken out into separate header
    (`.cuh`) files, and the kernels that emit sums as well as correlation coefficients
    are separate.
  id: totrans-2756
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的相关性实现被拆分成单独的头文件（`.cuh`），发出求和以及相关系数的内核是分开的。
- en: '![Image](graphics/465tab01.jpg)'
  id: totrans-2757
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/465tab01.jpg)'
- en: 'The `normalizedCrossCorrelation.cu` program tests both the functionality and
    the performance of the kernels. By default, it loads `coins.pgm` and detects the
    dime in the lower right corner. The dime is located at (210,148) and is 52×52
    pixels in size. The program also writes the performance measurements to `stdout`—for
    example:'
  id: totrans-2758
  prefs: []
  type: TYPE_NORMAL
  zh: '`normalizedCrossCorrelation.cu` 程序测试了内核的功能和性能。默认情况下，它加载 `coins.pgm` 并检测右下角的硬币。硬币位于（210,148），大小为52×52像素。程序还将性能测量写入
    `stdout`——例如：'
- en: '[Click here to view code image](ch15_images.html#p466pro01a)'
  id: totrans-2759
  prefs: []
  type: TYPE_NORMAL
  zh: '[点击此处查看代码图片](ch15_images.html#p466pro01a)'
- en: $ normalizedCrossCorrelation --padWidth 1024 --padHeight 1024 -wTemplate 16
    -hTemplate 16
  id: totrans-2760
  prefs: []
  type: TYPE_NORMAL
  zh: $ normalizedCrossCorrelation --padWidth 1024 --padHeight 1024 -wTemplate 16
    -hTemplate 16
- en: 'corrTexTex2D: 54.86 Mpix/s 14.05Gtpix/s'
  id: totrans-2761
  prefs: []
  type: TYPE_NORMAL
  zh: 'corrTexTex2D: 54.86 Mpix/s 14.05Gtpix/s'
- en: 'corrTemplate2D: 72.87 Mpix/s 18.65Gtpix/s'
  id: totrans-2762
  prefs: []
  type: TYPE_NORMAL
  zh: 'corrTemplate2D: 72.87 Mpix/s 18.65Gtpix/s'
- en: 'corrShared: 69.66 Mpix/s 17.83Gtpix/s'
  id: totrans-2763
  prefs: []
  type: TYPE_NORMAL
  zh: 'corrShared: 69.66 Mpix/s 17.83Gtpix/s'
- en: 'corrSharedSM: 78.66 Mpix/s 20.14Gtpix/s'
  id: totrans-2764
  prefs: []
  type: TYPE_NORMAL
  zh: 'corrSharedSM: 78.66 Mpix/s 20.14Gtpix/s'
- en: 'corrShared4: 97.02 Mpix/s 24.84Gtpix/s'
  id: totrans-2765
  prefs: []
  type: TYPE_NORMAL
  zh: 'corrShared4: 97.02 Mpix/s 24.84Gtpix/s'
- en: The program supports the following command line options.
  id: totrans-2766
  prefs: []
  type: TYPE_NORMAL
  zh: 程序支持以下命令行选项。
- en: '![Image](graphics/466tab01.jpg)'
  id: totrans-2767
  prefs: []
  type: TYPE_IMG
  zh: '![图片](graphics/466tab01.jpg)'
- en: 15.7\. Performance and Further Reading
  id: totrans-2768
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.7\. 性能与进一步阅读
- en: Our sample program uses CUDA events to report the performance of some number
    of consecutive kernel launches (default 100) and reports the rates of both output
    coefficients (which varies with the template size) and the “template-pixel” rate,
    or the number of inner loop iterations per unit time.
  id: totrans-2769
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例程序使用 CUDA 事件报告若干连续内核启动的性能（默认为 100 次），并报告输出系数的速率（根据模板大小变化）和“模板像素”速率，即每单位时间内的内循环迭代次数。
- en: The raw performance of GPUs at performing this computation is astonishing. A
    GeForce GTX 280 (GT200) can perform almost 25 billion template-pixel calculations
    per second (Gtpix/s), and the GeForce 680 GTX (GK104) delivers well over 100 Gtpix/s.
  id: totrans-2770
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 执行此计算的原始性能令人惊叹。GeForce GTX 280（GT200）每秒可以执行近 250 亿个模板像素计算（Gtpix/s），而 GeForce
    680 GTX（GK104）则可以达到超过 100 Gtpix/s。
- en: The default parameters of the program are not ideal for performance measurement.
    They are set to detect the dime in the lower right corner and optionally write
    out the image in [Figure 15.3](ch15.html#ch15fig03). In particular, the image
    is too small to keep the GPU fully busy. The image is only 300×246 pixels (74K
    in size), so only 310 blocks are needed by the shared memory implementation to
    perform the computation. The `--padWidth` and `--padHeight` command-line options
    can be used in the sample program to increase the size of the image and thus the
    number of correlation coefficients computed (there are no data dependencies in
    the code, so the padding can be filled with arbitrary data); a 1024×1024 image
    is both more realistic and gets best utilization out of all GPUs tested.
  id: totrans-2771
  prefs: []
  type: TYPE_NORMAL
  zh: 程序的默认参数并不适合性能测量。它们被设置为检测右下角的暗影，并可选择性地输出图像 [图 15.3](ch15.html#ch15fig03)。特别是，图像过小，无法让
    GPU 完全忙碌。图像的大小只有 300×246 像素（74K 大小），因此共享内存实现只需要 310 个块来执行计算。可以使用示例程序中的 `--padWidth`
    和 `--padHeight` 命令行选项来增大图像大小，从而增加计算的相关系数数量（代码中没有数据依赖性，因此填充可以使用任意数据）；1024×1024
    的图像既更为现实，也能更好地利用所有测试过的 GPU。
- en: '![Image](graphics/15fig03.jpg)'
  id: totrans-2772
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/15fig03.jpg)'
- en: '*Figure 15.3* Template in `__constant__` memory.'
  id: totrans-2773
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.3* `__constant__` 内存中的模板。'
- en: '![Image](graphics/15fig04.jpg)'
  id: totrans-2774
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/15fig04.jpg)'
- en: '*Figure 15.4* Image in shared memory.'
  id: totrans-2775
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.4* 共享内存中的图像。'
- en: '[Figure 15.5](ch15.html#ch15fig05) summarizes the relative performance of our
    5 implementations.'
  id: totrans-2776
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 15.5](ch15.html#ch15fig05) 总结了我们 5 种实现的相对性能。'
- en: '• `corrTexTex`: template and image both in texture memory'
  id: totrans-2777
  prefs: []
  type: TYPE_NORMAL
  zh: '• `corrTexTex`: 模板和图像都在纹理内存中'
- en: '• `corrTexConstant`: template in constant memory'
  id: totrans-2778
  prefs: []
  type: TYPE_NORMAL
  zh: '• `corrTexConstant`: 常量内存中的模板'
- en: '• `corrShared`: template in constant memory and image in shared memory'
  id: totrans-2779
  prefs: []
  type: TYPE_NORMAL
  zh: '• `corrShared`: 常量内存中的模板和共享内存中的图像'
- en: '• `corrSharedSM`: `corrShared` with SM-aware kernel invocations'
  id: totrans-2780
  prefs: []
  type: TYPE_NORMAL
  zh: '• `corrSharedSM`: `corrShared`，带有 SM 感知的内核调用'
- en: '• `corrShared4`: `corrSharedSM` with the inner loop unrolled 4x'
  id: totrans-2781
  prefs: []
  type: TYPE_NORMAL
  zh: '• `corrShared4`: `corrSharedSM`，将内循环展开 4 倍'
- en: '![Image](graphics/15fig05.jpg)'
  id: totrans-2782
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/15fig05.jpg)'
- en: '*Figure 15.5* Performance comparison.'
  id: totrans-2783
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.5* 性能对比。'
- en: The various optimizations did improve performance, to varying degrees, as shown
    in [Figure 15.6](ch15.html#ch15fig06). Moving the template to constant memory
    had the biggest impact on GK104, increasing performance by 80%; moving the image
    to shared memory had the biggest impact on GF100, increasing performance by 70%.
    The SM-aware kernel launches had the most muted impact, increasing performance
    on GT200 by 14% (it does not affect performance on the other architectures, since
    using the built-in multiplication operator is also fastest).
  id: totrans-2784
  prefs: []
  type: TYPE_NORMAL
  zh: 各种优化确实在不同程度上提升了性能，如[图 15.6](ch15.html#ch15fig06)所示。将模板移到常量内存对GK104的影响最大，性能提高了80%；将图像移到共享内存对GF100的影响最大，性能提高了70%。SM感知内核启动的影响最小，GT200的性能提高了14%（对其他架构没有影响，因为使用内建的乘法运算符也是最快的）。
- en: '![Image](graphics/15fig06.jpg)'
  id: totrans-2785
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/15fig06.jpg)'
- en: '*Figure 15.6* Correlation rate versus template size.'
  id: totrans-2786
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 15.6* 相关性速率与模板大小的关系。'
- en: On GT200, `corrShared` suffered from bank conflicts in shared memory, so much
    so that `corrShared` is slower than `corrTexConstant`; `corrShared4` alleviates
    these bank conflicts, increasing performance by 23%.
  id: totrans-2787
  prefs: []
  type: TYPE_NORMAL
  zh: 在GT200上，`corrShared`在共享内存中存在银行冲突，以至于`corrShared`比`corrTexConstant`还要慢；`corrShared4`缓解了这些银行冲突，性能提高了23%。
- en: 'The size of the template also has a bearing on the efficiency of this algorithm:
    The larger the template, the more efficient the computation on a per-template-pixel
    basis. [Figure 15.6](ch15.html#ch15fig06) illustrates how the template size affects
    performance of the `corrShared4` formulation.'
  id: totrans-2788
  prefs: []
  type: TYPE_NORMAL
  zh: 模板的大小也会影响该算法的效率：模板越大，每个模板像素的计算效率越高。[图 15.6](ch15.html#ch15fig06)展示了模板大小如何影响`corrShared4`公式的性能。
- en: As the template grows from 8×8 to 28×28, GT200 performance improves 36% (19.6
    Gtpix/s to 26.8 Gtpix/s), GF100 improves 57% (46.5 Gtpix/s to 72.9 Gtpix/s), and
    GK104 improves 30% (93.9 Gtpix/s to 120.72 Gtpix/s).
  id: totrans-2789
  prefs: []
  type: TYPE_NORMAL
  zh: 当模板从8×8增长到28×28时，GT200的性能提升了36%（从19.6 Gtpix/s提升到26.8 Gtpix/s），GF100提升了57%（从46.5
    Gtpix/s提升到72.9 Gtpix/s），GK104提升了30%（从93.9 Gtpix/s提升到120.72 Gtpix/s）。
- en: For small templates, the compiler generates faster code if the template size
    is known at compile time. Moving `wTemplate` and `hTemplate` to be template parameters
    and specializing for an 8×8 template improved performance as follows.
  id: totrans-2790
  prefs: []
  type: TYPE_NORMAL
  zh: 对于小模板，如果在编译时已知模板大小，编译器会生成更快的代码。将`wTemplate`和`hTemplate`作为模板参数，并为8×8模板进行专门化，性能有所提升。
- en: '![Image](graphics/469tab01.jpg)'
  id: totrans-2791
  prefs: []
  type: TYPE_IMG
  zh: '![Image](graphics/469tab01.jpg)'
- en: 15.8\. Further Reading
  id: totrans-2792
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.8\. 进一步阅读
- en: '*Digital Image Processing* includes both a discussion of normalized correlation
    (pp. 583–586) and the logarithmic transform used to compute the output pixels
    in our sample program (pp. 168–169).'
  id: totrans-2793
  prefs: []
  type: TYPE_NORMAL
  zh: '*数字图像处理* 包括了归一化相关性（第583–586页）和用于计算我们示例程序中输出像素的对数变换（第168–169页）的讨论。'
- en: Gonzalez, Rafael C., and Richard E. Woods. *Digital image processing*. Addison-Wesley,
    Reading, MA, 1992.
  id: totrans-2794
  prefs: []
  type: TYPE_NORMAL
  zh: Gonzalez, Rafael C., 和 Richard E. Woods. *数字图像处理*。Addison-Wesley, Reading, MA,
    1992.
- en: '[www.imageprocessingplace.com/root_files_V3/publications.htm](http://www.imageprocessingplace.com/root_files_V3/publications.htm)'
  id: totrans-2795
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.imageprocessingplace.com/root_files_V3/publications.htm](http://www.imageprocessingplace.com/root_files_V3/publications.htm)'
- en: J.P. Lewis has an excellent discussion, including a more asymptotically efficient
    way to accelerate the type of correlation operation implemented by our sample
    program, where a template match against every pixel in the input image is desired.
    Lewis uses FFTs to compute the numerators and summed area tables to compute the
    denominators of the coefficients.
  id: totrans-2796
  prefs: []
  type: TYPE_NORMAL
  zh: J.P. Lewis 进行了出色的讨论，其中包括一种更渐进高效的加速方法，用于加速我们示例程序中实现的那种相关操作，在该操作中，需要对输入图像中的每个像素进行模板匹配。Lewis
    使用FFT来计算系数的分子，使用求和区域表来计算分母。
- en: Lewis, J.P. Fast template matching. *Vision Interface 10*, 1995, pp. 120–123\.
    An expanded version entitled “Fast Normalized Correlation” may be found online
    at [http://bit.ly/NJnZPI](http://bit.ly/NJnZPI).
  id: totrans-2797
  prefs: []
  type: TYPE_NORMAL
  zh: Lewis, J.P. 快速模板匹配。*Vision Interface 10*, 1995年，第120–123页。一个扩展版本题为“快速归一化相关”可以在[http://bit.ly/NJnZPI](http://bit.ly/NJnZPI)在线找到。
