- en: Part III
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 11\. Streaming Workloads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Streaming workloads are among the simplest that can be ported to CUDA: computations
    where each data element can be computed independently of the others, often with
    such low computational density that the workload is bandwidth-bound. Streaming
    workloads do not use many of the hardware resources of the GPU, such as caches
    and shared memory, that are designed to optimize reuse of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Since GPUs give the biggest benefits on workloads with *high* computational
    density, it might be useful to review some cases when it still makes sense for
    streaming workloads to port to GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: • If the input and output are in device memory, it doesn’t make sense to transfer
    the data back to the CPU just to perform one operation.
  prefs: []
  type: TYPE_NORMAL
- en: • If the GPU has much better instruction-level support than the CPU for the
    operation (e.g., Black-Scholes options computation, which uses Special Function
    Unit instructions intensively), the GPU can outperform the CPU despite memory
    transfer overhead.
  prefs: []
  type: TYPE_NORMAL
- en: • The GPU operating concurrently with the CPU can approximately double performance,
    even if they are the same speed.
  prefs: []
  type: TYPE_NORMAL
- en: • The CUDA code for a given workload may be more readable or maintainable than
    highly optimized CPU code for the same computation.
  prefs: []
  type: TYPE_NORMAL
- en: • On integrated systems (i.e., systems-on-a-chip with CPU and CUDA-capable GPU
    operating on the same memory), there is no transfer overhead. CUDA can use “zero-copy”
    methods and avoid the copy entirely.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter covers every aspect of streaming workloads, giving different formulations
    of the same workload to highlight the different issues that arise. The workload
    in question—the SAXPY operation from the BLAS library—performs a scalar multiplication
    and vector addition together in a single operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 11.1](ch11.html#ch11lis01) gives a trivial C implementation of SAXPY.
    For corresponding elements in the two input arrays, one element is scaled by a
    constant, added to the other, and written to the output array. Both input arrays
    and the output arrays consist of *N* elements. Since GPUs have a native multiply-add
    instruction, the innermost loop of SAXPY has an extremely modest number of instructions
    per memory access.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 11.1.* saxpyCPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p11lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: saxpyCPU(
  prefs: []
  type: TYPE_NORMAL
- en: float *out,
  prefs: []
  type: TYPE_NORMAL
- en: const float *x,
  prefs: []
  type: TYPE_NORMAL
- en: const float *y,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: float alpha )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0; i < N; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[i] += alpha*x[i]+y[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 11.2](ch11.html#ch11lis02) gives a trivial CUDA implementation of
    SAXPY. This version works for any grid or block size, and it performs adequately
    for most applications. This kernel is so bandwidth-bound that most applications
    would benefit more from restructuring the application to increase the computational
    density than from optimizing this tiny kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 11.2.* saxpyGPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p11lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: saxpyGPU(
  prefs: []
  type: TYPE_NORMAL
- en: float *out,
  prefs: []
  type: TYPE_NORMAL
- en: const float *x,
  prefs: []
  type: TYPE_NORMAL
- en: const float *y,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: float alpha )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[i] = alpha*x[i]+y[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The bulk of this chapter discusses how to move data to and from host memory
    efficiently, but first we’ll spend a moment examining how to improve this kernel’s
    performance when operating on device memory.
  prefs: []
  type: TYPE_NORMAL
- en: 11.1\. Device Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If the input and output data are in device memory, optimizing a low-density
    computation such as SAXPY is a matter of optimizing the global memory access.
    Besides alignment and coalescing constraints that inform performance, CUDA kernels
    are sensitive to the number of blocks and threads per block. The `globalRead`,
    `globalWrite`, `globalCopy`, and `globalCopy2` applications (in the `memory/`
    subdirectory of the source code) generate reports for the bandwidths achieved
    for a variety of operand sizes, block sizes, and loop unroll factors. A sample
    report generated by `globalCopy2` (which follows a memory access pattern similar
    to SAXPY: two reads and one write per loop iteration) is given in [Listing 11.3](ch11.html#ch11lis03).'
  prefs: []
  type: TYPE_NORMAL
- en: If we reference the `globalCopy2.cu` application from [Chapter 5](ch05.html#ch05)
    (see [Listing 5.8](ch05.html#ch05lis08)), running it on a GK104 gets us the output
    in [Listing 11.3](ch11.html#ch11lis03) for 4-byte operands. The top row (unroll
    factor of 1) corresponds to the naïve implementation (similar to [Listing 11.2](ch11.html#ch11lis02));
    a slight performance benefit is observed when the loop is unrolled. An unroll
    factor of 4 gives a speedup of about 10%, delivering 128 GiB/s of bandwidth as
    opposed to the naïve implementation’s 116 GiB/s.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, using the `#pragma` `unroll` compiler directive only increases
    performance to about 118 GiB/s, while modifying the templated kernel from `globalCopy2.cu`
    to perform SAXPY increases performance to 135 GiB/s. [Listing 11.4](ch11.html#ch11lis04)
    gives the resulting kernel, which is implemented in the `stream1Device.cu` application
    (`cudahandbook/streaming/`).
  prefs: []
  type: TYPE_NORMAL
- en: For most applications, these small performance differences don’t justify rewriting
    kernels in this way. But if kernels are written to be “blocking-agnostic” (i.e.,
    to work correctly for any grid or block size), then the optimal settings can be
    determined empirically without too much effort.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 11.3.* `globalCopy2` output (GK104).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p11lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Operand size: 4 bytes'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input size: 16M operands'
  prefs: []
  type: TYPE_NORMAL
- en: Block Size
  prefs: []
  type: TYPE_NORMAL
- en: Unroll  32      64      128     256     512     maxBW   maxThreads
  prefs: []
  type: TYPE_NORMAL
- en: 1       63.21   90.89   104.64  113.45  116.06  116.06  512
  prefs: []
  type: TYPE_NORMAL
- en: 2       66.43   92.89   105.09  116.35  120.66  120.66  512
  prefs: []
  type: TYPE_NORMAL
- en: 3       87.23   100.70  112.07  110.85  121.36  121.36  512
  prefs: []
  type: TYPE_NORMAL
- en: 4       99.54   103.53  113.58  119.52  128.64  128.64  512
  prefs: []
  type: TYPE_NORMAL
- en: 5       94.27   103.56  108.02  122.82  124.88  124.88  512
  prefs: []
  type: TYPE_NORMAL
- en: 6       100.67  104.18  115.10  122.05  122.46  122.46  512
  prefs: []
  type: TYPE_NORMAL
- en: 7       94.56   106.09  116.30  117.63  114.50  117.63  256
  prefs: []
  type: TYPE_NORMAL
- en: 8       58.27   45.10   47.07   46.29   45.18   58.27   32
  prefs: []
  type: TYPE_NORMAL
- en: 9       41.20   34.74   35.87   35.49   34.58   41.20   32
  prefs: []
  type: TYPE_NORMAL
- en: 10      33.59   31.97   32.42   31.43   30.61   33.59   32
  prefs: []
  type: TYPE_NORMAL
- en: 11      27.76   28.17   28.46   27.83   26.79   28.46   128
  prefs: []
  type: TYPE_NORMAL
- en: 12      25.59   26.42   26.54   25.72   24.51   26.54   128
  prefs: []
  type: TYPE_NORMAL
- en: 13      22.69   23.07   23.54   22.50   20.71   23.54   128
  prefs: []
  type: TYPE_NORMAL
- en: 14      22.19   22.40   22.23   21.10   19.00   22.40   64
  prefs: []
  type: TYPE_NORMAL
- en: 15      20.94   21.14   20.98   19.62   17.31   21.14   64
  prefs: []
  type: TYPE_NORMAL
- en: 16      18.86   19.01   18.97   17.66   15.40   19.01   64
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 11.4.* saxpyGPU (templated unroll).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p11lis04a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<const int n>
  prefs: []
  type: TYPE_NORMAL
- en: __device__ void
  prefs: []
  type: TYPE_NORMAL
- en: saxpy_unrolled(
  prefs: []
  type: TYPE_NORMAL
- en: float *out,
  prefs: []
  type: TYPE_NORMAL
- en: const float *px,
  prefs: []
  type: TYPE_NORMAL
- en: const float *py,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: float alpha )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float x[n], y[n];
  prefs: []
  type: TYPE_NORMAL
- en: size_t i;
  prefs: []
  type: TYPE_NORMAL
- en: for ( i = n*blockIdx.x*blockDim.x+threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N-n*blockDim.x*gridDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: i += n*blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+j*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: x[j] = px[index];
  prefs: []
  type: TYPE_NORMAL
- en: y[j] = py[index];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+j*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: out[index] = alpha*x[j]+y[j];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // to avoid the (index<N) conditional in the inner loop,
  prefs: []
  type: TYPE_NORMAL
- en: // we left off some work at the end
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+j*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: if ( index<N ) {
  prefs: []
  type: TYPE_NORMAL
- en: x[j] = px[index];
  prefs: []
  type: TYPE_NORMAL
- en: y[j] = py[index];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+j*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: if ( index<N ) out[index] = alpha*x[j]+y[j];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: saxpyGPU( float *out, const float *px, const float *py, size_t N, float alpha
    )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: saxpy_unrolled<4>( out, px, py, N, alpha );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The `stream1Device.cu` application reports the total wall clock time needed
    to transfer data from pageable system memory to device memory, operate on the
    data with the kernel in [Listing 11.4](ch11.html#ch11lis04), and transfer the
    data back. On a test system with an Intel i7 running Windows 7 on a GeForce GTX
    680, the output of this application is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p357pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring times with 128M floats (use --N to specify number of Mfloats)
  prefs: []
  type: TYPE_NORMAL
- en: 'Memcpy( host->device ): 365.95 ms (2934.15 MB/s)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel processing : 11.94 ms (134920.75 MB/s)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memcpy (device->host ): 188.72 ms (2844.73 MB/s)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Total time (wall clock): 570.22 ms (2815.30 MB/s)'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel takes a tiny amount of the overall execution time—about 2% of the
    wall clock time. The other 98% of time is spent transferring data to and from
    the GPU! For transfer-bound workloads like this one, if some or all of the data
    being operated on is in host memory, the best way to optimize the application
    is to improve CPU/GPU overlap and transfer performance.
  prefs: []
  type: TYPE_NORMAL
- en: 11.2\. Asynchronous Memcpy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unless the input and output data can stay resident on the GPU, the logistics
    of streaming the data through the GPU—copying the input and output data to and
    from device memory—become the primary consideration. The two tools best suited
    to improve transfer performance are pinned memory and asynchronous memcpy (which
    can only operate on pinned memory).
  prefs: []
  type: TYPE_NORMAL
- en: The `stream2Async.cu` application illustrates the effect of moving the pageable
    memory of `stream1Device.cu` to pinned memory and invoking the memcpys asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p358pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring times with 128M floats (use --N to specify number of Mfloats)
  prefs: []
  type: TYPE_NORMAL
- en: 'Memcpy( host->device ): 181.03 ms (5931.33 MB/s)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kernel processing : 13.87 ms (116152.99 MB/s)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memcpy (device->host ): 90.07 ms (5960.35 MB/s)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Total time (wall clock): 288.68 ms (5579.29 MB/s)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 11.5](ch11.html#ch11lis05) contrasts the difference between the timed
    portions of `stream1-``Device.cu` (which performs synchronous transfers) and `stream2Async.cu`
    (which performs asynchronous transfers).^([1](ch11.html#ch11fn1)) In both cases,
    four CUDA events are used to record the times at the start, after the host→device
    transfers, after the kernel launch, and at the end. For `stream2Async.cu`, all
    of these operations are requested of the GPU in quick succession, and the GPU
    records the event times as it performs them. For `stream1Device.cu`, the GPU event-based
    times are a bit suspect, since for any `cudaMemcpy()`, calls must wait for the
    GPU to complete before proceeding, causing a pipeline bubble before the `cudaEventRecord()`
    calls for `evHtoD` and `evDtoH` are processed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch11.html#ch11fn1a). Error checking has been removed for clarity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that despite using the slower, naïve implementation of `saxpyGPU` (from
    [Listing 11.2](ch11.html#ch11lis02)), the wall clock time from this application
    shows that it completes the computation almost twice as fast: 289 ms versus 570.22
    ms. The combination of faster transfers and asynchronous execution delivers much
    better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the improved performance, the application output highlights another
    performance opportunity: Some of the kernel processing can be performed concurrently
    with transfers. The next two sections describe two different methods to overlap
    kernel execution with transfers.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 11.5.* Synchronous (`stream1Device.cu`) versus asynchronous (`stream2Async.cu`).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p11lis05a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // from stream1Device.cu
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( evStart, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpy( dptrX, hptrX, ..., cudaMemcpyHostToDevice );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpy( dptrY, hptrY, ..., cudaMemcpyHostToDevice );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( evHtoD, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: saxpyGPU<<<nBlocks, nThreads>>>( dptrOut, dptrX, dptrY, N, alpha;
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( evKernel, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpy( hptrOut, dptrOut, N*sizeof(float), cudaMemcpyDeviceToHost );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( evDtoH, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // from stream2Async.cu
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( evStart, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( dptrX, hptrX, ..., cudaMemcpyHostToDevice, NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( dptrY, hptrY, ..., cudaMemcpyHostToDevice, NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( evHtoD, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: saxpyGPU<<<nBlocks, nThreads>>>( dptrOut, dptrX, dptrY, N, alpha;
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( evKernel, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyAsync( hptrOut, dptrOut, N*sizeof(float), ... , NULL );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( evDtoH, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 11.3\. Streams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For workloads that benefit from concurrent memcpy and kernel execution (GPU/GPU
    overlap), CUDA streams can be used to coordinate execution. The `stream3Streams.cu`
    application splits the input and output arrays into k streams and then invokes
    k host→device memcpys, kernels, and device→host memcpys, each in their own stream.
    Associating the transfers and computations with different streams lets CUDA know
    that the computations are completely independent, and CUDA will exploit whatever
    parallelism opportunities the hardware can support. On GPUs with multiple copy
    engines, the GPU may be transferring data both to and from device memory while
    processing other data with the SMs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 11.6](ch11.html#ch11lis06) shows an excerpt from `stream3Streams.cu`,
    the same portion of the application as shown in [Listing 11.5](ch11.html#ch11lis05).
    On the test system, the output from this application reads as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p360pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring times with 128M floats
  prefs: []
  type: TYPE_NORMAL
- en: Testing with default max of 8 streams (set with --maxStreams <count>)
  prefs: []
  type: TYPE_NORMAL
- en: Streams  Time (ms)  MB/s
  prefs: []
  type: TYPE_NORMAL
- en: 1        290.77 ms  5471.45
  prefs: []
  type: TYPE_NORMAL
- en: 2        273.46 ms  5820.34
  prefs: []
  type: TYPE_NORMAL
- en: 3        277.14 ms  5744.49
  prefs: []
  type: TYPE_NORMAL
- en: 4        278.06 ms  5725.76
  prefs: []
  type: TYPE_NORMAL
- en: 5        277.44 ms  5736.52
  prefs: []
  type: TYPE_NORMAL
- en: 6        276.56 ms  5751.87
  prefs: []
  type: TYPE_NORMAL
- en: 7        274.75 ms  5793.43
  prefs: []
  type: TYPE_NORMAL
- en: 8        275.41 ms  5779.51
  prefs: []
  type: TYPE_NORMAL
- en: The GPU in question has only one copy engine, so it is not surprising that the
    case with 2 streams delivers the highest performance. If the kernel execution
    time were more in line with the transfer time, it would likely be beneficial to
    split the arrays into more than 2 subarrays. As things stand, the first kernel
    launch cannot begin processing until the first host→device memcpy is done, and
    the final device→host memcpy cannot begin until the last kernel launch is done.
    If the kernel processing took more time, this “overhang” would be more pronounced.
    For our application, the wall clock time of 273 ms shows that most of the kernel
    processing (13.87 ms) has been hidden.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this formulation, partly due to hardware limitations, we are not
    trying to insert any `cudaEventRecord()` calls between operations, as we did in
    [Listing 11.5](ch11.html#ch11lis05). On most CUDA hardware, trying to record events
    between the streamed operations in [Listing 11.6](ch11.html#ch11lis06) would break
    concurrency and reduce performance. Instead, we bracket the operations with one
    `cudaEventRecord()` before and one `cudaEventRecord()` after.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 11.6.* `stream3Streams.cu` excerpt.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p11lis06a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iStream = 0; iStream < nStreams; iStream++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync(
  prefs: []
  type: TYPE_NORMAL
- en: dptrX+iStream*streamStep,
  prefs: []
  type: TYPE_NORMAL
- en: hptrX+iStream*streamStep,
  prefs: []
  type: TYPE_NORMAL
- en: streamStep*sizeof(float),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice,
  prefs: []
  type: TYPE_NORMAL
- en: streams[iStream] ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync(
  prefs: []
  type: TYPE_NORMAL
- en: dptrY+iStream*streamStep,
  prefs: []
  type: TYPE_NORMAL
- en: hptrY+iStream*streamStep,
  prefs: []
  type: TYPE_NORMAL
- en: streamStep*sizeof(float),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyHostToDevice,
  prefs: []
  type: TYPE_NORMAL
- en: streams[iStream] ) );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iStream = 0; iStream < nStreams; iStream++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: saxpyGPU<<<nBlocks, nThreads, 0, streams[iStream]>>>(
  prefs: []
  type: TYPE_NORMAL
- en: dptrOut+iStream*streamStep,
  prefs: []
  type: TYPE_NORMAL
- en: dptrX+iStream*streamStep,
  prefs: []
  type: TYPE_NORMAL
- en: dptrY+iStream*streamStep,
  prefs: []
  type: TYPE_NORMAL
- en: streamStep,
  prefs: []
  type: TYPE_NORMAL
- en: alpha );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iStream = 0; iStream < nStreams; iStream++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyAsync(
  prefs: []
  type: TYPE_NORMAL
- en: hptrOut+iStream*streamStep,
  prefs: []
  type: TYPE_NORMAL
- en: dptrOut+iStream*streamStep,
  prefs: []
  type: TYPE_NORMAL
- en: streamStep*sizeof(float),
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost,
  prefs: []
  type: TYPE_NORMAL
- en: streams[iStream] ) );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 11.4\. Mapped Pinned Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For transfer-bound, streaming workloads such as SAXPY, reformulating the application
    to use mapped pinned memory for both the input and output confers a number of
    benefits.
  prefs: []
  type: TYPE_NORMAL
- en: • As shown by the excerpt from `stream4Mapped.cu` ([Listing 11.7](ch11.html#ch11lis07)),
    it eliminates the need to call `cudaMemcpy()`.
  prefs: []
  type: TYPE_NORMAL
- en: • It eliminates the need to allocate device memory.
  prefs: []
  type: TYPE_NORMAL
- en: • For discrete GPUs, mapped pinned memory performs bus transfers but minimizes
    the amount of “overhang” alluded to in the previous section. Instead of waiting
    for a host→device memcpy to finish, input data can be processed by the SMs as
    soon as it arrives. Instead of waiting for a kernel to complete before initiating
    a device →host transfer, the data is posted to the bus as soon as the SMs are
    done processing.
  prefs: []
  type: TYPE_NORMAL
- en: • For integrated GPUs, host and device memory exist in the same memory pool,
    so mapped pinned memory enables “zero copy” and eliminates any need to transfer
    data over the bus at all.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 11.7.* `stream4Mapped` excerpt.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p11lis07a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &chStart );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( evStart, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: saxpyGPU<<<nBlocks, nThreads>>>( dptrOut, dptrX, dptrY, N, alpha );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventRecord( evStop, 0 );
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceSynchronize();
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Mapped pinned memory works especially well when writing to host memory (for
    example, to deliver the result of a reduction to the host) because unlike reads,
    there is no need to wait until writes arrive before continuing execution.^([2](ch11.html#ch11fn2))
    Workloads that read mapped pinned memory are more problematic. If the GPU cannot
    sustain full bus performance while reading from mapped pinned memory, the smaller
    transfer performance may overwhelm the benefits of a smaller overhang. Also, for
    some workloads, the SMs have better things to do than drive (and wait for) PCI
    Express bus traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch11.html#ch11fn2a). Hardware designers call this “covering the latency.”'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of our application, on our test system, mapped pinned memory is
    a definite win.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch11_images.html#p362lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring times with 128M floats (use --N to specify number of Mfloats)
  prefs: []
  type: TYPE_NORMAL
- en: 'Total time: 204.54 ms (7874.45 MB/s)'
  prefs: []
  type: TYPE_NORMAL
- en: It completes the computation in 204.54 ms, significantly faster than the 273
    ms of the second-fastest implementation. The effective bandwidth of 7.9 GiB/s
    shows that the GPU is pushing both directions of PCI Express.
  prefs: []
  type: TYPE_NORMAL
- en: Not all combinations of systems and GPUs can sustain such high levels of performance
    with mapped pinned memory. If there’s any doubt, keep the data in device memory
    and use the asynchronous memcpy formulations, similar to `stream2Async.cu`.
  prefs: []
  type: TYPE_NORMAL
- en: 11.5\. Performance and Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter covers four different implementations of SAXPY, emphasizing different
    strategies of data movement.
  prefs: []
  type: TYPE_NORMAL
- en: • Synchronous memcpy to and from device memory
  prefs: []
  type: TYPE_NORMAL
- en: • Asynchronous memcpy to and from device memory
  prefs: []
  type: TYPE_NORMAL
- en: • Asynchronous memcpy using streams
  prefs: []
  type: TYPE_NORMAL
- en: • Mapped pinned memory
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 11.1](ch11.html#ch11tab01) and [Figure 11.1](ch11.html#ch11fig01) summarize
    the relative performance of these implementations for 128M floats on GK104s plugged
    into two different test systems: the Intel i7 system (PCI Express 2.0) and an
    Intel Xeon E5-2670 (PCI Express 3.0). The benefits of PCI Express 3.0 are evident,
    as they are about twice as fast. Additionally, the overhead of CPU/GPU synchronization
    is higher on the E5-2670, since the pageable memcpy operations are slower.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/11tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 11.1* Streaming Performance'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/11fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 11.1* Bandwidth (GeForce GTX 680 on Intel i7 versus Sandy Bridge)'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 12\. Reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reduction is a class of parallel algorithms that pass over *O*(N) input data
    and generate a *O*(1) result computed with a binary associative operator ![Image](graphics/plus.jpg).
    Examples of such operations include minimum, maximum, sum, sum of squares, AND,
    OR, and the dot product of two vectors. Reduction is also an important primitive
    used as a subroutine in other operations, such as Scan (covered in the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Unless the operator ![Image](graphics/plus.jpg) is extremely expensive to evaluate,
    reduction tends to be bandwidth-bound. Our treatment of reduction begins with
    several two-pass implementations based on the `reduction` SDK sample. Next, the
    `threadFenceReduction` SDK sample shows how to perform reduction in a single pass
    so only one kernel must be invoked to perform the operation. Finally, the chapter
    concludes with a discussion of fast binary reduction with the `__syncthreads_count()`
    intrinsic (added with SM 2.0) and how to perform reduction using the warp shuffle
    instruction (added with SM 3.0).
  prefs: []
  type: TYPE_NORMAL
- en: 12.1\. Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the binary operator is associative, the *O*(*N*) operations to compute
    a reduction may be performed in any order.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/365equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 12.1](ch12.html#ch12fig01) shows some different options to process
    an 8-element array. The serial implementation is shown for contrast. Only one
    execution unit that can perform the ![Image](graphics/plus.jpg) operator is needed,
    but performance is poor because it takes 7 steps to complete the computation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/12fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12.1* Reduction of 8 elements.'
  prefs: []
  type: TYPE_NORMAL
- en: The pairwise formulation is intuitive and only requires *O*(lg*N*) steps (3
    in this case) to compute the result, but it exhibits poor performance in CUDA.
    When reading global memory, having a single thread access adjacent memory locations
    causes uncoalesced memory transactions. When reading shared memory, the pattern
    shown will cause bank conflicts.
  prefs: []
  type: TYPE_NORMAL
- en: For both global memory and shared memory, an interleaving-based strategy works
    better. In [Figure 12.1](ch12.html#ch12fig01), the interleaving factor is 4; for
    global memory, interleaving by a multiple of `blockDim.x *gridDim.x` has good
    performance because all memory transactions are coalesced. For shared memory,
    best performance is achieved by accumulating the partial sums with an interleaving
    factor chosen to avoid bank conflicts and to keep adjacent threads in the thread
    block active.
  prefs: []
  type: TYPE_NORMAL
- en: Once a thread block has finished processing its interleaved subarray, it writes
    the result to global memory for further processing by a subsequent kernel launch.
    It may seem expensive to launch multiple kernels, but kernel launches are asynchronous,
    so the CPU can request the next kernel launch while the GPU is executing the first;
    every kernel launch represents an opportunity to specify different launch configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Since the performance of a kernel can vary with different thread and block sizes,
    it’s a good idea to write the kernel so it will work correctly for any valid combination
    of thread and block sizes. The optimal thread/block configuration then can be
    determined empirically.
  prefs: []
  type: TYPE_NORMAL
- en: The initial reduction kernels in this chapter illustrate some important CUDA
    programming concepts that may be familiar.
  prefs: []
  type: TYPE_NORMAL
- en: • *Coalesced memory operations* to maximize bandwidth
  prefs: []
  type: TYPE_NORMAL
- en: • *Variable-sized shared memory* to facilitate collaboration between threads
  prefs: []
  type: TYPE_NORMAL
- en: • Avoiding shared memory *bank conflicts*
  prefs: []
  type: TYPE_NORMAL
- en: The optimized reduction kernels illustrate more advanced CUDA programming idioms.
  prefs: []
  type: TYPE_NORMAL
- en: • *Warp synchronous* coding avoids unneeded thread synchronization.
  prefs: []
  type: TYPE_NORMAL
- en: • *Atomic operations* and *memory fences* eliminate the need to invoke multiple
    kernels.
  prefs: []
  type: TYPE_NORMAL
- en: • The *shuffle* instruction enables warp-level reductions without the use of
    shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: 12.2\. Two-Pass Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This algorithm operates in two stages. A kernel performs *NumBlocks* reductions
    in parallel, where *NumBlocks* is the number of blocks used to invoke the kernel;
    the results are written to an intermediate array. The final result is generated
    by invoking the same kernel to perform a second pass on the intermediate array
    with a single block. [Listing 12.1](ch12.html#ch12lis01) gives a two-pass reduction
    kernel that computes the sum of an array of integers.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 12.1.* Two-pass reduction kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p12lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction1_kernel( int *out, const int *in, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ int sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: int sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + tid;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: sum += in[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: for ( int activeThreads = blockDim.x>>1;
  prefs: []
  type: TYPE_NORMAL
- en: activeThreads;
  prefs: []
  type: TYPE_NORMAL
- en: activeThreads >>= 1 ) {
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid < activeThreads ) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] += sPartials[tid+activeThreads];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid == 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[blockIdx.x] = sPartials[0];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction1( int *answer, int *partial,
  prefs: []
  type: TYPE_NORMAL
- en: const int *in, size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: int numBlocks, int numThreads )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int sharedSize = numThreads*sizeof(int);
  prefs: []
  type: TYPE_NORMAL
- en: Reduction1_kernel<<<
  prefs: []
  type: TYPE_NORMAL
- en: numBlocks, numThreads, sharedSize>>>(
  prefs: []
  type: TYPE_NORMAL
- en: partial, in, N );
  prefs: []
  type: TYPE_NORMAL
- en: Reduction1_kernel<<<
  prefs: []
  type: TYPE_NORMAL
- en: 1, numThreads, sharedSize>>>(
  prefs: []
  type: TYPE_NORMAL
- en: answer, partial, numBlocks );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The shared memory array is used to accumulate the reduction within each thread
    block. Its size depends on the number of threads in the block, so it must be specified
    when the kernel is launched. *Note:* The number of threads in the block must be
    a power of 2!
  prefs: []
  type: TYPE_NORMAL
- en: The first `for` loop computes the thread’s sum over the input array. If the
    input pointer is properly aligned, all of the memory transactions by this code
    are coalesced, and it will maximize the memory bandwidth. Each thread then writes
    its accumulated sum to shared memory and synchronizes before starting the log-step
    reduction.
  prefs: []
  type: TYPE_NORMAL
- en: The second `for` loop performs a log-step reduction over the values in shared
    memory. The values in the upper half of shared memory are added to the values
    in the lower half, and the number of participating threads is successively halved
    until one value in `shared_sum[0]` contains the output for that block. This part
    of the kernel is the one that requires that the thread block size be a power of
    2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the output value of the thread block is written to global memory.
    This kernel is intended to be invoked twice, as shown in the host function: once
    with *N* blocks, where *N* is chosen for maximum performance in performing the
    reduction over the input array, and then with 1 block to accumulate the final
    output. [Listing 12.2](ch12.html#ch12lis02) shows the host function that invokes
    `Reduction1_kernel()`. Note that an array for the partial sums is allocated and
    passed in separately. Also note that since the kernel uses an unsized shared memory
    array, the amount of shared memory needed by the kernel must be specified as the
    third parameter in the `<<< >>>` syntax.'
  prefs: []
  type: TYPE_NORMAL
- en: The CUDA SDK discusses several optimizations of this kernel that focus on reducing
    the amount of conditional code in the log-step reduction. Part of the `for` loop
    that performs the log-step reduction—the later part, when the thread count is
    32 or fewer—can be implemented with *warp-synchronous* code. Since the warps in
    each thread block execute each instruction in lockstep, the `__syncthreads()`
    intrinsics are no longer needed when the number of active threads in a block drops
    below the hardware’s warp size of 32\. The resulting kernel, located in the `reduction2.cu`
    source code file, is shown in [Listing 12.2](ch12.html#ch12lis02).
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: When writing warp synchronous code, the `volatile` keyword must be used for
    the pointers into shared memory. Otherwise, the compiler may introduce optimizations
    that change the order of memory operations and the code will not work correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 12.2.* Reduction with unrolled, warp-synchronous finish.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p12lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction2_kernel( int *out, const int *in, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ int sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: int sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + tid;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: sum += in[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: for ( int activeThreads = blockDim.x>>1;
  prefs: []
  type: TYPE_NORMAL
- en: activeThreads > 32;
  prefs: []
  type: TYPE_NORMAL
- en: activeThreads >>= 1 ) {
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid < activeThreads ) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] += sPartials[tid+activeThreads];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if ( threadIdx.x < 32 ) {
  prefs: []
  type: TYPE_NORMAL
- en: volatile int *wsSum = sPartials;
  prefs: []
  type: TYPE_NORMAL
- en: if ( blockDim.x > 32 ) wsSum[tid] += wsSum[tid + 32];
  prefs: []
  type: TYPE_NORMAL
- en: wsSum[tid] += wsSum[tid + 16];
  prefs: []
  type: TYPE_NORMAL
- en: wsSum[tid] += wsSum[tid + 8];
  prefs: []
  type: TYPE_NORMAL
- en: wsSum[tid] += wsSum[tid + 4];
  prefs: []
  type: TYPE_NORMAL
- en: wsSum[tid] += wsSum[tid + 2];
  prefs: []
  type: TYPE_NORMAL
- en: wsSum[tid] += wsSum[tid + 1];
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid == 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: volatile int *wsSum = sPartials;
  prefs: []
  type: TYPE_NORMAL
- en: out[blockIdx.x] = wsSum[0];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The warp synchronous optimization can be taken a step further by lofting the
    thread count into a template parameter, enabling the log-step reduction to be
    unrolled completely. [Listing 12.3](ch12.html#ch12lis03) gives the complete optimized
    kernel. Following Mark Harris’s reduction presentation,^([1](ch12.html#ch12fn1))
    the code evaluated at compile time is italicized.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch12.html#ch12fn1a). [http://bit.ly/WNmH9Z](http://bit.ly/WNmH9Z)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 12.3.* Templatized, fully unrolled log-step reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p12lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<unsigned int numThreads>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction3_kernel( int *out, const int *in, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ int sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: const unsigned int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: int sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*numThreads + tid;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += numThreads*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: sum += in[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 1024) {
  prefs: []
  type: TYPE_NORMAL
- en: if (tid < 512) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] += sPartials[tid + 512];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 512) {
  prefs: []
  type: TYPE_NORMAL
- en: if (tid < 256) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] += sPartials[tid + 256];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 256) {
  prefs: []
  type: TYPE_NORMAL
- en: if (tid < 128) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] += sPartials[tid + 128];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 128) {
  prefs: []
  type: TYPE_NORMAL
- en: if (tid <  64) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] += sPartials[tid +  64];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // warp synchronous at the end
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid < 32 ) {
  prefs: []
  type: TYPE_NORMAL
- en: volatile int *wsSum = sPartials;
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >=  64) { wsSum[tid] += wsSum[tid + 32]; }
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >=  32) { wsSum[tid] += wsSum[tid + 16]; }
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >=  16) { wsSum[tid] += wsSum[tid +  8]; }
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >=   8) { wsSum[tid] += wsSum[tid +  4]; }
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >=   4) { wsSum[tid] += wsSum[tid +  2]; }
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >=   2) { wsSum[tid] += wsSum[tid +  1]; }
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid == 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[blockIdx.x] = wsSum[0];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: To instantiate the function template in [Listing 12.3](ch12.html#ch12lis03),
    it must be invoked explicitly in a separate host function. [Listing 12.4](ch12.html#ch12lis04)
    shows how `Reduction3_kernel` is invoked by another function template, and the
    host function uses a `switch` statement to invoke that template for each possible
    block size.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 12.4.* Template instantiations for unrolled reduction.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p12lis04a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<unsigned int numThreads>
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction3_template( int *answer, int *partial,
  prefs: []
  type: TYPE_NORMAL
- en: const int *in, size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: int numBlocks )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: Reduction3_kernel<numThreads><<<
  prefs: []
  type: TYPE_NORMAL
- en: numBlocks, numThreads, numThreads*sizeof(int)>>>(
  prefs: []
  type: TYPE_NORMAL
- en: partial, in, N );
  prefs: []
  type: TYPE_NORMAL
- en: Reduction3_kernel<numThreads><<<
  prefs: []
  type: TYPE_NORMAL
- en: 1, numThreads, numThreads*sizeof(int)>>>(
  prefs: []
  type: TYPE_NORMAL
- en: answer, partial, numBlocks );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction3( int *out, int *partial,
  prefs: []
  type: TYPE_NORMAL
- en: const int *in, size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: int numBlocks, int numThreads )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: switch ( numThreads ) {
  prefs: []
  type: TYPE_NORMAL
- en: 'case    1: return Reduction3_template<   1>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case    2: return Reduction3_template<   2>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case    4: return Reduction3_template<   4>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case    8: return Reduction3_template<   8>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case   16: return Reduction3_template<  16>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case   32: return Reduction3_template<  32>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case   64: return Reduction3_template<  64>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case  128: return Reduction3_template< 128>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case  256: return Reduction3_template< 256>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case  512: return Reduction3_template< 512>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case 1024: return Reduction3_template<1024>( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 12.3\. Single-Pass Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The two-pass reduction approach is in part a workaround for the inability of
    CUDA blocks to synchronize with one another. In the absence of interblock synchronization
    to determine when processing of the final output can begin, a second kernel invocation
    is needed.
  prefs: []
  type: TYPE_NORMAL
- en: The second kernel invocation can be avoided by using a combination of atomic
    operations and shared memory, as described in the `threadfenceReduction` sample
    in the CUDA SDK. A single device memory location tracks which thread blocks have
    finished writing their partial sums. Once all blocks have finished, one block
    performs the final log-step reduction to write the output.
  prefs: []
  type: TYPE_NORMAL
- en: Since this kernel performs several log-step reductions from shared memory, the
    code in [Listing 12.3](ch12.html#ch12lis03) that conditionally adds based on the
    templated thread count is pulled into a separate device function for reuse.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 12.5.* `Reduction4_LogStepShared.`'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p12lis05a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<unsigned int numThreads>
  prefs: []
  type: TYPE_NORMAL
- en: __device__ void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction4_LogStepShared( int *out, volatile int *partials )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 1024) {
  prefs: []
  type: TYPE_NORMAL
- en: if (tid < 512) {
  prefs: []
  type: TYPE_NORMAL
- en: partials[tid] += partials[tid + 512];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 512) {
  prefs: []
  type: TYPE_NORMAL
- en: if (tid < 256) {
  prefs: []
  type: TYPE_NORMAL
- en: partials[tid] += partials[tid + 256];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 256) {
  prefs: []
  type: TYPE_NORMAL
- en: if (tid < 128) {
  prefs: []
  type: TYPE_NORMAL
- en: partials[tid] += partials[tid + 128];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 128) {
  prefs: []
  type: TYPE_NORMAL
- en: if (tid <  64) {
  prefs: []
  type: TYPE_NORMAL
- en: partials[tid] += partials[tid +  64];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // warp synchronous at the end
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid < 32 ) {
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 64) { partials[tid] += partials[tid + 32]; }
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 32) { partials[tid] += partials[tid + 16]; }
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >= 16) { partials[tid] += partials[tid +  8]; }
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >=  8) { partials[tid] += partials[tid +  4]; }
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >=  4) { partials[tid] += partials[tid +  2]; }
  prefs: []
  type: TYPE_NORMAL
- en: if (numThreads >=  2) { partials[tid] += partials[tid +  1]; }
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid == 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: '*out = partials[0];'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The `Reduction4_LogStepShared()` function, shown in [Listing 12.5](ch12.html#ch12lis05),
    writes the reduction for the thread block, whose partial sums are given by `partials`
    to the pointer to the memory location specified by `out`. [Listing 12.6](ch12.html#ch12lis06)
    gives the single-pass reduction using `Reduction4_LogStepShared()` as a subroutine.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 12.6.* Single-pass reduction kernel (`reduction4SinglePass.cuh`).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p12lis06a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: // Global variable used by reduceSinglePass to count blocks
  prefs: []
  type: TYPE_NORMAL
- en: __device__ unsigned int retirementCount = 0;
  prefs: []
  type: TYPE_NORMAL
- en: template <unsigned int numThreads>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: reduceSinglePass( int *out, int *partial,
  prefs: []
  type: TYPE_NORMAL
- en: const int *in, unsigned int N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ int sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: int sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*numThreads + tid;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += numThreads*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: sum += in[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: if (gridDim.x == 1) {
  prefs: []
  type: TYPE_NORMAL
- en: Reduction4_LogStepShared<numThreads>( &out[blockIdx.x],
  prefs: []
  type: TYPE_NORMAL
- en: sPartials );
  prefs: []
  type: TYPE_NORMAL
- en: return;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Reduction4_LogStepShared<numThreads>( &partial[blockIdx.x],
  prefs: []
  type: TYPE_NORMAL
- en: sPartials );
  prefs: []
  type: TYPE_NORMAL
- en: __shared__ bool lastBlock;
  prefs: []
  type: TYPE_NORMAL
- en: // wait for outstanding memory instructions in this thread
  prefs: []
  type: TYPE_NORMAL
- en: __threadfence();
  prefs: []
  type: TYPE_NORMAL
- en: // Thread 0 takes a ticket
  prefs: []
  type: TYPE_NORMAL
- en: if( tid==0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int ticket = atomicAdd(&retirementCount, 1);
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // If the ticket ID is equal to the number of blocks,
  prefs: []
  type: TYPE_NORMAL
- en: // we are the last block!
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: lastBlock = (ticket == gridDim.x-1);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: // One block performs the final log-step reduction
  prefs: []
  type: TYPE_NORMAL
- en: if( lastBlock ) {
  prefs: []
  type: TYPE_NORMAL
- en: int sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = tid;
  prefs: []
  type: TYPE_NORMAL
- en: i < gridDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: i += numThreads ) {
  prefs: []
  type: TYPE_NORMAL
- en: sum += partial[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[threadIdx.x] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: Reduction4_LogStepShared<numThreads>( out, sPartials );
  prefs: []
  type: TYPE_NORMAL
- en: retirementCount = 0;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel starts out with familiar code that has each thread compute a partial
    reduction across the input array and write the results to shared memory. Once
    this is done, the single-block case is treated specially, since the output of
    the log-step reduction from shared memory can be written directly and not to the
    array of partial sums. The remainder of the kernel is executed only on kernels
    with multiple thread blocks.
  prefs: []
  type: TYPE_NORMAL
- en: The shared Boolean `lastBlock` is used to evaluate a predicate that must be
    communicated to all threads in the final block. The `__threadfence()` causes all
    threads in the block to wait until any pending memory transactions have been posted
    to device memory. When `__threadfence()` is executed, writes to global memory
    are visible to all threads, not just the calling thread or threads in the block.
  prefs: []
  type: TYPE_NORMAL
- en: As each block exits, it performs an `atomicAdd()` to check whether it is the
    one block that needs to perform the final log-step reduction. Since `atomicAdd()`
    returns the *previous* value of the memory location, the block that increments
    `retirementCount` and gets a value equal to `gridDim.x-1` can be deemed the “last
    thread” and can perform the final reduction. The `lastBlock` shared memory location
    communicates that result to all threads in the block, and `__syncthreads()` then
    must be called so the write to `lastBlock` will be visible to all threads in the
    block. The final block performs the final log-step reduction of the partial sums
    and writes the result. Finally, `retirementCount` is set back to 0 for subsequent
    invocations of `reduceSinglePass()`.
  prefs: []
  type: TYPE_NORMAL
- en: 12.4\. Reduction with Atomics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For reductions whose ![Image](graphics/plus.jpg) operator is supported natively
    by an atomic operator implemented in hardware, a simpler approach to reduction
    is possible: Just loop over the input data and “fire and forget” the inputs into
    the output memory location to receive the output value. The `Reduction5` kernel
    given in [Listing 12.7](ch12.html#ch12lis07) is much simpler than previous formulations.
    Each thread computes a partial sum over the inputs and performs an `atomicAdd`
    on the output at the end.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that `Reduction5_kernel` does not work properly unless the memory location
    pointed to by `out` is initialized to 0.^([2](ch12.html#ch12fn2)) Like the `threadFenceReduction`
    sample, this kernel has the advantage that only one kernel invocation is needed
    to perform the operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch12.html#ch12fn2a). The kernel itself cannot perform this initialization
    because CUDA’s execution model does not enable the race condition to be resolved
    between thread blocks. See [Section 7.3.1](ch07.html#ch07lev2sec7).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 12.7.* Reduction with global atomics (`reduction5Atomics.cuh`).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p12lis07a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction5_kernel( int *out, const int *in, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: int partialSum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + tid;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: partialSum += in[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: atomicAdd( out, partialSum );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction5( int *answer, int *partial,
  prefs: []
  type: TYPE_NORMAL
- en: const int *in, size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: int numBlocks, int numThreads )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemset( answer, 0, sizeof(int) );
  prefs: []
  type: TYPE_NORMAL
- en: Reduction5_kernel<<< numBlocks, numThreads>>>( answer, in, N );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 12.5\. Arbitrary Block Sizes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, all of the reduction implementations that use shared memory require
    the block size to be a power of 2\. With a small amount of additional code, the
    reduction can be made to work on arbitrary block sizes. [Listing 12.8](ch12.html#ch12lis08)
    gives a kernel derived from the very first two-pass kernel given in [Listing 12.1](ch12.html#ch12lis01),
    modified to operate on any block size. The `floorPow2` variable computes the power
    of 2 that is less than or equal to the block size, and the contribution from any
    threads above that power of 2 is added before continuing on to the loop that implements
    the log-step reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 12.8.* Reduction (arbitrary block size) (`reduction6AnyBlockSize.cuh`).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p12lis08a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction6_kernel( int *out, const int *in, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ int sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: int sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + tid;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: sum += in[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: // start the shared memory loop on the next power of 2 less
  prefs: []
  type: TYPE_NORMAL
- en: // than the block size.  If block size is not a power of 2,
  prefs: []
  type: TYPE_NORMAL
- en: // accumulate the intermediate sums in the remainder range.
  prefs: []
  type: TYPE_NORMAL
- en: int floorPow2 = blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: if ( floorPow2 & (floorPow2-1) ) {
  prefs: []
  type: TYPE_NORMAL
- en: while ( floorPow2 & (floorPow2-1) ) {
  prefs: []
  type: TYPE_NORMAL
- en: floorPow2 &= floorPow2-1;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid >= floorPow2 ) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid - floorPow2] += sPartials[tid];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int activeThreads = floorPow2>>1;
  prefs: []
  type: TYPE_NORMAL
- en: activeThreads;
  prefs: []
  type: TYPE_NORMAL
- en: activeThreads >>= 1 ) {
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid < activeThreads ) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] += sPartials[tid+activeThreads];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid == 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[blockIdx.x] = sPartials[0];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 12.6\. Reduction Using Arbitrary Data Types
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have only developed reduction kernels that can compute the sum of
    an array of integers. To generalize these kernels to perform a broader set of
    operations, we turn to C++ templates. With the exception of the algorithms that
    use atomics, all of the kernels that have appeared so far can be adapted to use
    templates. In the source code accompanying the book, they are in the CUDA headers
    `reduction1Templated.cuh`, `reduction2Templated.cuh`, and so on. [Listing 12.9](ch12.html#ch12lis09)
    gives the templated version of the reduction kernel from [Listing 12.1](ch12.html#ch12lis01).
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 12.9.* Templated reduction kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p12lis09a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<typename ReductionType, typename T>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: Reduction_templated( ReductionType *out, const T *in, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: SharedMemory<ReductionType> sPartials;
  prefs: []
  type: TYPE_NORMAL
- en: ReductionType sum;
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x + tid;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: sum += in[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: for ( int activeThreads = blockDim.x>>1;
  prefs: []
  type: TYPE_NORMAL
- en: activeThreads;
  prefs: []
  type: TYPE_NORMAL
- en: activeThreads >>= 1 ) {
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid < activeThreads ) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] += sPartials[tid+activeThreads];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid == 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[blockIdx.x] = sPartials[0];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that since we want to be able to compute a variety of output types for
    a given type of input (for example, we would like to build kernels that compute
    any combination of the minimum, maximum, sum, or the sum of squares of an array
    of integers), we’ve used two different template parameters: `T` is the type being
    reduced, and `ReductionType` is the type used for partial sums and for the final
    result.'
  prefs: []
  type: TYPE_NORMAL
- en: The first few lines of code use the `+=` operator to “rake” through the input,
    accumulating a partial sum for each thread in the block.^([3](ch12.html#ch12fn3))
    Execution then proceeds exactly as in [Listing 12.1](ch12.html#ch12lis01), except
    that the code is operating on `ReductionType` instead of `int`. To avoid alignment-related
    compilation errors, this kernel uses an idiom from the CUDA SDK to declare the
    variable-sized shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch12.html#ch12fn3a). We just as easily could have defined a function to
    wrap the binary operator being evaluated by the reduction. The Thrust library
    defines a functor `plus`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p379pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: struct SharedMemory
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: __device__ inline operator       T*()
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ int __smem[];
  prefs: []
  type: TYPE_NORMAL
- en: return (T*) (void *) __smem;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __device__ inline operator const T*() const
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ int __smem[];
  prefs: []
  type: TYPE_NORMAL
- en: return (T*) (void *) __smem;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 12.10](ch12.html#ch12lis10) shows an example of a class intended to
    be used with templated reduction functions such as `Reduction_templated`. This
    class computes both the sum and the sum of squares of an array of integers.^([4](ch12.html#ch12fn4))
    Besides defining `operator+=`, a specialization of the `SharedMemory` template
    must be declared; otherwise, the compiler will generate the following error.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch12.html#ch12fn4a). You could compute a whole suite of statistics on the
    input array in a single pass, but we are keeping things simple here for illustrative
    purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Error: Unaligned memory accesses not supported'
  prefs: []
  type: TYPE_NORMAL
- en: The `reductionTemplated.cu` program in the accompanying source code shows how
    the function templates from the CUDA headers can be invoked.
  prefs: []
  type: TYPE_NORMAL
- en: Reduction1<CReduction_Sumi_isq, int>( ... );
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 12.10.* `CReduction_Sumi_isq` class.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch12_images.html#p12lis10a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: struct CReduction_Sumi_isq {
  prefs: []
  type: TYPE_NORMAL
- en: 'public:'
  prefs: []
  type: TYPE_NORMAL
- en: CReduction_Sumi_isq();
  prefs: []
  type: TYPE_NORMAL
- en: int sum;
  prefs: []
  type: TYPE_NORMAL
- en: long long sumsq;
  prefs: []
  type: TYPE_NORMAL
- en: CReduction_Sumi_isq& operator +=( int a );
  prefs: []
  type: TYPE_NORMAL
- en: volatile CReduction_Sumi_isq& operator +=( int a ) volatile;
  prefs: []
  type: TYPE_NORMAL
- en: CReduction_Sumi_isq& operator +=( const CReduction_Sumi_isq& a );
  prefs: []
  type: TYPE_NORMAL
- en: volatile CReduction_Sumi_isq& operator +=(
  prefs: []
  type: TYPE_NORMAL
- en: volatile CReduction_Sumi_isq& a ) volatile;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ __host__
  prefs: []
  type: TYPE_NORMAL
- en: CReduction_Sumi_isq::CReduction_Sumi_isq()
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: sumsq = 0;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ __host__
  prefs: []
  type: TYPE_NORMAL
- en: CReduction_Sumi_isq&
  prefs: []
  type: TYPE_NORMAL
- en: CReduction_Sumi_isq::operator +=( int a )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: sum += a;
  prefs: []
  type: TYPE_NORMAL
- en: sumsq += (long long) a*a;
  prefs: []
  type: TYPE_NORMAL
- en: return *this;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ __host__
  prefs: []
  type: TYPE_NORMAL
- en: volatile CReduction_Sumi_isq&
  prefs: []
  type: TYPE_NORMAL
- en: CReduction_Sumi_isq::operator +=( int a ) volatile
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: sum += a;
  prefs: []
  type: TYPE_NORMAL
- en: sumsq += (long long) a*a;
  prefs: []
  type: TYPE_NORMAL
- en: return *this;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ __host__
  prefs: []
  type: TYPE_NORMAL
- en: CReduction_Sumi_isq&
  prefs: []
  type: TYPE_NORMAL
- en: CReduction_Sumi_isq::operator +=( const CReduction_Sumi_isq& a )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: sum += a.sum;
  prefs: []
  type: TYPE_NORMAL
- en: sumsq += a.sumsq;
  prefs: []
  type: TYPE_NORMAL
- en: return *this;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ __host__
  prefs: []
  type: TYPE_NORMAL
- en: volatile CReduction_Sumi_isq&
  prefs: []
  type: TYPE_NORMAL
- en: CReduction_Sumi_isq::operator +=(
  prefs: []
  type: TYPE_NORMAL
- en: volatile CReduction_Sumi_isq& a ) volatile
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: sum += a.sum;
  prefs: []
  type: TYPE_NORMAL
- en: sumsq += a.sumsq;
  prefs: []
  type: TYPE_NORMAL
- en: return *this;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: inline int
  prefs: []
  type: TYPE_NORMAL
- en: operator!=( const CReduction_Sumi_isq& a,
  prefs: []
  type: TYPE_NORMAL
- en: const CReduction_Sumi_isq& b )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: return a.sum != b.sum && a.sumsq != b.sumsq;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: '// from Reduction SDK sample:'
  prefs: []
  type: TYPE_NORMAL
- en: // specialize to avoid unaligned memory
  prefs: []
  type: TYPE_NORMAL
- en: // access compile errors
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: template<>
  prefs: []
  type: TYPE_NORMAL
- en: struct SharedMemory<CReduction_Sumi_isq>
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: __device__ inline operator       CReduction_Sumi_isq*()
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ CReduction_Sumi_isq
  prefs: []
  type: TYPE_NORMAL
- en: __smem_CReduction_Sumi_isq[];
  prefs: []
  type: TYPE_NORMAL
- en: return (CReduction_Sumi_isq*)__smem_CReduction_Sumi_isq;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __device__ inline operator const CReduction_Sumi_isq*() const
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ CReduction_Sumi_isq
  prefs: []
  type: TYPE_NORMAL
- en: __smem_CReduction_Sumi_isq[];
  prefs: []
  type: TYPE_NORMAL
- en: return (CReduction_Sumi_isq*)__smem_CReduction_Sumi_isq;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 12.7\. Predicate Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Predicates or truth values (true/false) can be represented compactly, since
    each predicate only occupies 1 bit. In SM 2.0, NVIDIA added a number of instructions
    to make predicate manipulation more efficient. The `__ballot()` and `__popc()`
    intrinsics can be used for warp-level reduction, and the `__syncthreads_count()`
    intrinsic can be used for block-level reduction.
  prefs: []
  type: TYPE_NORMAL
- en: int __ballot( int p );
  prefs: []
  type: TYPE_NORMAL
- en: '`__ballot()` evaluates a condition for all threads in the warp and returns
    a 32-bit word, where each bit gives the condition for the corresponding thread
    in the warp. Since `__ballot()` broadcasts its result to every thread in the warp,
    it is effectively a reduction across the warp. Any thread that wants to count
    the number of threads in the warp for which the condition was true can call the
    `__popc()` intrinsic'
  prefs: []
  type: TYPE_NORMAL
- en: int __popc( int i );
  prefs: []
  type: TYPE_NORMAL
- en: which returns the number of set bits in the input word.
  prefs: []
  type: TYPE_NORMAL
- en: SM 2.0 also introduced `__syncthreads_count()`.
  prefs: []
  type: TYPE_NORMAL
- en: int __syncthreads_count( int p );
  prefs: []
  type: TYPE_NORMAL
- en: This intrinsic waits until all warps in the threadblock have arrived, then broadcasts
    to all threads in the block the number of threads for which the input condition
    was true.
  prefs: []
  type: TYPE_NORMAL
- en: Since the 1-bit predicates immediately turn into 5- and 9- or 10-bit values
    after a warp- or block-level reduction, these intrinsics only serve to reduce
    the amount of shared memory needed for the lowest-level evaluation and reduction.
    Still, they greatly amplify the number of elements that can be considered by a
    single thread block.
  prefs: []
  type: TYPE_NORMAL
- en: 12.8\. Warp Reduction with Shuffle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SM 3.0 introduced the “shuffle” instruction, described in [Section 8.6.1](ch08.html#ch08lev2sec20),
    that can be used to perform a reduction across the 32 threads in a warp. By using
    the “butterfly” variant of the shuffle instruction, the final 5 steps of the log-step
    reduction
  prefs: []
  type: TYPE_NORMAL
- en: wsSum[tid] += wsSum[tid+16];
  prefs: []
  type: TYPE_NORMAL
- en: wsSum[tid] += wsSum[tid+8];
  prefs: []
  type: TYPE_NORMAL
- en: wsSum[tid] += wsSum[tid+4];
  prefs: []
  type: TYPE_NORMAL
- en: wsSum[tid] += wsSum[tid+2];
  prefs: []
  type: TYPE_NORMAL
- en: wsSum[tid] += wsSum[tid+1];
  prefs: []
  type: TYPE_NORMAL
- en: can be rewritten as
  prefs: []
  type: TYPE_NORMAL
- en: int mySum = wsSum[tid];
  prefs: []
  type: TYPE_NORMAL
- en: mySum += __shuf_xor( mySum, 16 );
  prefs: []
  type: TYPE_NORMAL
- en: mySum += __shuf_xor( mySum, 8 );
  prefs: []
  type: TYPE_NORMAL
- en: mySum += __shuf_xor( mySum, 4 );
  prefs: []
  type: TYPE_NORMAL
- en: mySum += __shuf_xor( mySum, 2 );
  prefs: []
  type: TYPE_NORMAL
- en: mySum += __shuf_xor( mySum, 1 );
  prefs: []
  type: TYPE_NORMAL
- en: All threads in the warp then contain the reduction in `mySum`. [Figure 12.2](ch12.html#ch12fig02)
    illustrates the operation of this warp scan primitive. Each thread’s sum is shown
    as a 4W × 8H rectangle, with a dark square showing which threads have contributed
    to each thread’s partial sum. (Besides the inset, the top row shows which squares
    correspond to each thread’s contribution.) With each step in the log-step reduction,
    the number of contributions doubles until every thread has a full reduction.^([5](ch12.html#ch12fn5))
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch12.html#ch12fn5a). The shuffle-up or shuffle-down variants can be used
    to implement reduction, but they take just as long as the butterfly (XOR) variant
    and only make the reduction value available to a single thread.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/12fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 12.2* Reduction using shuffle instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 13\. Scan
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scan, also known as *prefix scan*, *prefix sum*, or *parallel prefix sum*, is
    an important primitive in parallel programming and is used as a building block
    for many different algorithms, including but not limited to the following.
  prefs: []
  type: TYPE_NORMAL
- en: • Radix sort
  prefs: []
  type: TYPE_NORMAL
- en: • Quicksort
  prefs: []
  type: TYPE_NORMAL
- en: • Stream compaction and stream splitting
  prefs: []
  type: TYPE_NORMAL
- en: • Sparse matrix-vector multiplication
  prefs: []
  type: TYPE_NORMAL
- en: • Minimum spanning tree construction
  prefs: []
  type: TYPE_NORMAL
- en: • Computation of summed area tables
  prefs: []
  type: TYPE_NORMAL
- en: This chapter starts with a description of the algorithm and a few variations,
    discusses an early implementation strategy and how Scan algorithms can be described
    in terms of circuit diagrams, and then provides detailed descriptions of Scan
    implementations for CUDA. The References section covers both the Scan algorithm
    and the parallel prefix sum circuit problem in hardware design.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1\. Definition and Variations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Inclusive scan* takes a binary associative operator ![Image](graphics/plus.jpg)
    and an array of length *N*'
  prefs: []
  type: TYPE_NORMAL
- en: '[a[0], a[1], . . . a[N-1]]'
  prefs: []
  type: TYPE_NORMAL
- en: and returns the array
  prefs: []
  type: TYPE_NORMAL
- en: '[a[0], (a[0]![Image](graphics/plus.jpg)a[1]), . . . (a[0]![Image](graphics/plus.jpg)a[1]![Image](graphics/plus.jpg)
    . . . ![Image](graphics/plus.jpg)a[N-1])].'
  prefs: []
  type: TYPE_NORMAL
- en: Each element of the output depends on the preceding elements in the input.
  prefs: []
  type: TYPE_NORMAL
- en: '*Exclusive scan* is defined similarly but shifts the output and uses an *identity
    element* *id*[![Image](graphics/plus.jpg)] that has no effect on a value when
    ![Image](graphics/plus.jpg) is performed with it (for example, 0 for integer addition,
    1 for multiplication, etc.).'
  prefs: []
  type: TYPE_NORMAL
- en: '[*id*[![Image](graphics/plus.jpg)], a[0], a[0]![Image](graphics/plus.jpg)a[1],
    . . . a[0]![Image](graphics/plus.jpg)a[1]![Image](graphics/plus.jpg) . . . ![Image](graphics/plus.jpg)a[N-2]].'
  prefs: []
  type: TYPE_NORMAL
- en: Inclusive and exclusive scans can be transformed between each other by adding
    or subtracting the input array element by element, as shown in [Figure 13.1](ch13.html#ch13fig01).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.1* Inclusive and exclusive scan.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Stream compaction* is an operation that separates elements in an array according
    to a criterion. If a predicate (0 or 1) is computed for each element of the input
    array to determine whether it should be included in the output stream, then an
    exclusive scan on the predicates computes the indices of the output elements.
    A variation of stream compaction, known as *stream splitting*, writes the compact
    output separately for each value of the predicate. *Segmented scan* is a variation
    that takes a set of input flags (one per array element) in addition to the array
    and performs scans on the subarrays delineated by the flags.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the importance of the Scan primitive, an enormous amount of effort has
    been put into developing optimized scan implementations for CUDA. A list of references
    is given at the end of this chapter. Both the CUDPP and Thrust libraries include
    families of optimized Scan primitives that use templates for the best tradeoff
    between generality and performance. All that said, however, applications that
    use Scan as a primitive usually can benefit from custom implementations that take
    advantage of specific knowledge about the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 13.2\. Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A simple implementation in C++ looks like [Listing 13.1](ch13.html#ch13lis01).
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.1.* Inclusive scan (in C++).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: T
  prefs: []
  type: TYPE_NORMAL
- en: InclusiveScan( T *out, const T *in, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: T sum(0);
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0; i < N; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: sum += in[i];
  prefs: []
  type: TYPE_NORMAL
- en: out[i] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return sum;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: For these serial implementations in [Listings 13.1](ch13.html#ch13lis01) and
    [13.2](ch13.html#ch13lis02), the only difference between inclusive and exclusive
    scan is that the lines
  prefs: []
  type: TYPE_NORMAL
- en: out[i] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: sum += in[i];
  prefs: []
  type: TYPE_NORMAL
- en: are swapped.^([1](ch13.html#ch13fn1))
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch13.html#ch13fn1a). As written, the implementation of exclusive scan does
    not support an in-place computation. To enable the input and output arrays to
    be the same, `in[i]` must be saved in a temporary variable.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.2.* Exclusive scan (in C++).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: T
  prefs: []
  type: TYPE_NORMAL
- en: ExclusiveScan( T *out, const T *in, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: T sum(0);
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0; i < N; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[i] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: sum += in[i];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return sum;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The serial implementations of Scan are so obvious and trivial that you are forgiven
    if you’re wondering what a parallel implementation would look like! The so-called
    *prefix dependency,* where each output depends on all of the preceding inputs,
    may have some wondering if it’s even possible. But, upon reflection, you can see
    that the operations for neighboring pairs (a*[i]*![Image](graphics/plus.jpg)a*[i]*[+1]
    for 0 ≤ *i* < *N –* 1) could be computed in parallel; for *i* = 0, a*[i]*![Image](graphics/plus.jpg)a*[i]*[+1]
    computes a final output of the Scan, and otherwise these pairwise operations compute
    partial sums that can be used to contribute to the final output, much as we used
    partial sums in [Chapter 12](ch12.html#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: Blelloch^([2](ch13.html#ch13fn2)) describes a two-pass algorithm with an *upsweep*
    phase that computes the reduction of the array, storing intermediate results along
    the way, and followed by a *downsweep* that computes the final output of the scan.
    Pseudocode for the upsweep as is follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch13.html#ch13fn2a). [http://bit.ly/YmTmGP](http://bit.ly/YmTmGP)'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p388pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: upsweep(a, N)
  prefs: []
  type: TYPE_NORMAL
- en: for d from 0 to (lg N) – 1
  prefs: []
  type: TYPE_NORMAL
- en: in parallel for i from 0 to N – 1 by 2^(d+1)
  prefs: []
  type: TYPE_NORMAL
- en: a[i + 2^(d+1) – 1] += a[i + 2^(d – 1)]
  prefs: []
  type: TYPE_NORMAL
- en: The operation resembles the log-step reduction we have discussed before, except
    intermediate sums are stored for later use in generating the final output of the
    scan.
  prefs: []
  type: TYPE_NORMAL
- en: After Blelloch, [Figure 13.2](ch13.html#ch13fig02) shows an example run of this
    upsweep algorithm on an 8-element array using addition on integers. The “upsweep”
    terminology stems from thinking of the array as a balanced tree ([Figure 13.3](ch13.html#ch13fig03)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.2* Upsweep pass (array view).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.3* Upsweep pass (tree view).'
  prefs: []
  type: TYPE_NORMAL
- en: Once the upsweep has been completed, a *downsweep* propagates intermediate sums
    into the leaves of the tree. Pseudocode for the downsweep is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p389pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: downsweep(a, N)
  prefs: []
  type: TYPE_NORMAL
- en: a[N-1] = 0
  prefs: []
  type: TYPE_NORMAL
- en: for d from (lg N)-1 downto 0
  prefs: []
  type: TYPE_NORMAL
- en: in parallel for i from 0 to N-1 by 2^(d+1)
  prefs: []
  type: TYPE_NORMAL
- en: t := a[i+2^d-1]
  prefs: []
  type: TYPE_NORMAL
- en: a[i+2^d-1] = a[i + 2^(d+1)-1]
  prefs: []
  type: TYPE_NORMAL
- en: a[i+2^(d+1)-1] += t
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13.4](ch13.html#ch13fig04) shows how the example array is transformed
    during the downsweep, and [Figure 13.5](ch13.html#ch13fig05) shows the downsweep
    in tree form. Early implementations of Scan for CUDA followed this algorithm closely,
    and it does make a good introduction to thinking about possible parallel implementations.
    Unfortunately, it is not a great match to CUDA’s architecture; a naïve implementation
    suffers from shared memory bank conflicts, and addressing schemes to compensate
    for the bank conflicts incurs enough overhead that the costs outweigh the benefits.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.4* Downsweep (array view).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.5* Downsweep (tree view).'
  prefs: []
  type: TYPE_NORMAL
- en: 13.3\. Scan and Circuit Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Having explored one possible parallel algorithm for Scan, it may now be clear
    that there are many different ways to implement parallel Scan algorithms. In reasoning
    about other possible implementations, we can take advantage of design methodologies
    for integer addition hardware that performs a similar function: Instead of propagating
    an arbitrary binary associative operator ![Image](graphics/plus.jpg) across an
    array, culminating in the output from a reduction, hardware adders propagate partial
    addition results, culminating in the carry bit to be propagated for multiprecision
    arithmetic.'
  prefs: []
  type: TYPE_NORMAL
- en: Hardware designers use directed acyclic, oriented graphs to represent different
    implementations of Scan “circuits.” These diagrams compactly express both the
    data flow and the parallelism. A diagram of the serial implementation of [Listing
    13.1](ch13.html#ch13lis01) is given in [Figure 13.6](ch13.html#ch13fig06). The
    steps proceed downward as time advances; the vertical lines denote wires where
    the signal is propagated. Nodes of in-degree 2 (“operation nodes”) apply the operator
    ![Image](graphics/plus.jpg) to their inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.6* Serial scan.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the circuit diagrams show inclusive scans, not exclusive ones. For
    circuit diagrams, the difference is minor; to turn the inclusive scan in [Figure
    13.6](ch13.html#ch13fig06) into an exclusive scan, a 0 is wired into the first
    output and the sum is wired into the output, as shown in [Figure 13.7](ch13.html#ch13fig07).
    Note that both inclusive and exclusive scans generate the reduction of the input
    array as output, a characteristic that we will exploit in building efficient scan
    algorithms. (For purposes of clarity, all circuit diagrams other than [Figure
    13.7](ch13.html#ch13fig07) will depict inclusive scans.)
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.7* Serial scan (inclusive and exclusive).'
  prefs: []
  type: TYPE_NORMAL
- en: The Scan algorithm described by Blelloch corresponds to a circuit design known
    as Brent-Kung, a recursive decomposition in which every second output is fed into
    a Brent-Kung circuit of half the width. [Figure 13.8](ch13.html#ch13fig08) illustrates
    a Brent-Kung circuit operating on our example length of 8, along with Blelloch’s
    upsweep and downsweep phases. Nodes that broadcast their output to multiple nodes
    in the next stage are known as *fans.* Brent-Kung circuits are notable for having
    a constant fan-out of 2.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.8* Brent-Kung circuit.'
  prefs: []
  type: TYPE_NORMAL
- en: The structure of a Brent-Kung circuit becomes clearer on larger circuits; see,
    for example, the circuit that processes 16 inputs in [Figure 13.9](ch13.html#ch13fig09).
    [Figure 13.9](ch13.html#ch13fig09) also highlights the *spine* of the circuit,
    the longest subgraph that generates the last element of the scan output.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.9* Brent-Kung circuit (16 inputs).'
  prefs: []
  type: TYPE_NORMAL
- en: The depth of the Brent-Kung circuit grows logarithmically in the number of inputs,
    illustrating its greater efficiency than (for example) the serial algorithm. But
    because each stage in the recursive decomposition increases the depth by 2, the
    Brent-Kung circuit is not of minimum depth. Sklansky described a method to build
    circuits of minimum depth by recursively decomposing them as shown in [Figure
    13.10](ch13.html#ch13fig10).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.10* Sklansky (minimum-depth) circuit.'
  prefs: []
  type: TYPE_NORMAL
- en: Two (*N*/2)-input circuits are run in parallel, and the output of the spine
    of the left circuit is added to each element of the right circuit. For our 16-element
    example, the left-hand subgraph of the recursion is highlighted in [Figure 13.10](ch13.html#ch13fig10).
  prefs: []
  type: TYPE_NORMAL
- en: Another minimum-depth scan circuit, known as Kogge-Stone, has a constant fan-out
    of 2, which is a desirable characteristic for hardware implementation, but, as
    you can see in [Figure 13.11](ch13.html#ch13fig11), it has many operation nodes;
    software implementations analogous to Kogge-Stone are work-inefficient.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.11* Kogge-Stone circuit.'
  prefs: []
  type: TYPE_NORMAL
- en: Any scan circuit can be constructed from a combination of *scans* (which perform
    the parallel prefix computation and generate the sum of the input array as output)
    and *fans* (which add an input to each of their remaining outputs). The minimum-depth
    circuit in [Figure 13.10](ch13.html#ch13fig10) makes heavy use of fans in its
    recursive definition.
  prefs: []
  type: TYPE_NORMAL
- en: For an optimized CUDA implementation, a key insight is that a fan doesn’t need
    to take its input from a Scan *per se*; any reduction will do. And from [Chapter
    12](ch12.html#ch12), we have a highly optimized reduction algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: If, for example, we split the input array into subarrays of length *b* and compute
    the sum of each subarray using our optimized reduction routine, we end up with
    an array of ![Image](graphics/393equ01.jpg) reduction values. *If we then perform
    an exclusive scan on* *that array, it becomes an array of fan inputs (*seeds*)
    for scans of each subarray.* The number of values that can be efficiently scanned
    in one pass over global memory is limited by CUDA’s thread block and shared memory
    size, so for larger inputs, the approach must be applied recursively.
  prefs: []
  type: TYPE_NORMAL
- en: 13.4\. CUDA Implementations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Designing Scan algorithms and studying circuit diagrams is instructive, but
    in order to implement Scan for CUDA, we need to map the algorithms onto registers,
    memory and addressing schemes, and correct synchronization. The optimal CUDA implementation
    of Scan depends on the size of the scan being performed. Different schemes are
    best for warp-sized scans, scans that can fit in shared memory, and scans that
    must spill to global memory. Because blocks cannot reliably exchange data through
    global memory, scans too large to fit in shared memory *must* perform multiple
    kernel invocations.^([3](ch13.html#ch13fn3))
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch13.html#ch13fn3a). With CUDA 5.0 and SM 3.5 hardware, dynamic parallelism
    can move most of the kernel launches to be “child grids” as opposed to kernel
    launches initiated by the host.'
  prefs: []
  type: TYPE_NORMAL
- en: Before examining special cases (such as scanning of predicates), we will examine
    three (3) approaches to doing Scan on CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: • Scan-then-fan (recursive)
  prefs: []
  type: TYPE_NORMAL
- en: • Reduce-then-scan (recursive)
  prefs: []
  type: TYPE_NORMAL
- en: • Two-level reduce-then-scan
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.1\. Scan-Then-Fan
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The scan-then-fan approach uses a similar decomposition for global and shared
    memory. [Figure 13.12](ch13.html#ch13fig12) shows the approach used to scan a
    threadblock: A scan is performed on each 32-thread warp, and the reduction of
    that 32-element subarray is written to shared memory. A single warp then scans
    the array of partial sums. A single warp is sufficient because CUDA does not support
    threadblocks with more than 1024 threads. Finally, the base sums are fanned out
    to each warp’s output elements. Note that [Figure 13.12](ch13.html#ch13fig12)
    shows an inclusive scan being performed in step 2, so the first element of its
    output must be fanned out to the second warp, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.12* Scan-then-fan (shared memory).'
  prefs: []
  type: TYPE_NORMAL
- en: The code to implement this algorithm is given in [Listing 13.3](ch13.html#ch13lis03).
    The input array is assumed to have been loaded into shared memory already, and
    the parameters `sharedPartials` and `idx` specify the base address and index of
    the warp to scan, respectively. (In our first implementation, `threadIdx.x` is
    passed as the parameter `idx`.) Lines 9–13 implement step 1 in [Figure 13.12](ch13.html#ch13fig12);
    lines 16–21 implement step 2; and lines 31–45 implement step 3\. The output value
    written by this thread is returned to the caller but is used only if it happens
    to be the thread block’s reduction.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.3.* scanBlock: Block portion of scan-then-fan for thread blocks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ T
  prefs: []
  type: TYPE_NORMAL
- en: scanBlock( volatile T *sPartials )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ T warpPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: const int lane = tid & 31;
  prefs: []
  type: TYPE_NORMAL
- en: const int warpid = tid >> 5;
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // Compute this thread's partial sum
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: T sum = scanWarp<T>( sPartials );
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // Write each warp's reduction to shared memory
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane == 31 ) {
  prefs: []
  type: TYPE_NORMAL
- en: warpPartials[16+warpid] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // Have one warp scan reductions
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: if ( warpid==0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: scanWarp<T>( 16+warpPartials+tid );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // Fan out the exclusive scan element (obtained
  prefs: []
  type: TYPE_NORMAL
- en: // by the conditional and the decrement by 1)
  prefs: []
  type: TYPE_NORMAL
- en: // to this warp's pending output
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: if ( warpid > 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: sum += warpPartials[16+warpid-1];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // Write this thread's scan output
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: '*sPartials = sum;'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // The return value will only be used by caller if it
  prefs: []
  type: TYPE_NORMAL
- en: // contains the spine value (i.e., the reduction
  prefs: []
  type: TYPE_NORMAL
- en: // of the array we just scanned).
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: return sum;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13.13](ch13.html#ch13fig13) shows how this approach is adapted to global
    memory. A kernel scans *b*-element subarrays, where *b* is the block size. The
    partial sums are written to global memory, and another 1-block kernel invocation
    scans these partial sums, which are then fanned into the final output in global
    memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.13* Scan-then-fan (global memory).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.4](ch13.html#ch13lis04) gives the CUDA code for the Scan kernel
    in step 1 in [Figure 13.13](ch13.html#ch13fig13). It loops over the threadblocks
    to process, staging the input array into and out of shared memory. The kernel
    then optionally writes the spine value to global memory at the end. At the bottom
    level of the recursion, there is no need to record spine values, so the `bWriteSpine`
    template parameter enables the kernel to avoid dynamically checking the value
    of `partialsOut`.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.4.* scanAndWritePartials.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis04a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, bool bWriteSpine>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: scanAndWritePartials(
  prefs: []
  type: TYPE_NORMAL
- en: T *out,
  prefs: []
  type: TYPE_NORMAL
- en: T *gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: const T *in,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: size_t numBlocks )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern volatile __shared__ T sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: volatile T *myShared = sPartials+tid;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t iBlock = blockIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: iBlock < numBlocks;
  prefs: []
  type: TYPE_NORMAL
- en: iBlock += gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = iBlock*blockDim.x+tid;
  prefs: []
  type: TYPE_NORMAL
- en: '*myShared = (index < N) ? in[index] : 0;'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: T sum = scanBlock( myShared );
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: if ( index < N ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[index] = *myShared;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // write the spine value to global memory
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: if ( bWriteSpine && (threadIdx.x==(blockDim.x-1)) )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: gPartials[iBlock] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.5](ch13.html#ch13lis05) gives the host function that uses [Listings
    13.3](ch13.html#ch13lis03) to [13.4](ch13.html#ch13lis04) to implement an inclusive
    scan on an array in global memory. Note that the function recurses for scans too
    large to perform in shared memory. The first conditional in the function serves
    both as the base case for the recursion and to short-circuit scans small enough
    to perform in shared memory alone, avoiding any need to allocate global memory.
    Note how the amount of shared memory needed by the kernel (`b*sizeof(T)`) is specified
    at kernel invocation time.'
  prefs: []
  type: TYPE_NORMAL
- en: For larger scans, the function computes the number of partial sums needed ![Image](graphics/398equ01.jpg),
    allocates global memory to hold them, and follows the pattern in Figure 3.13,
    writing partial sums to the global array for later use by the `scanAndWritePartials`
    kernel in [Listing 13.4](ch13.html#ch13lis04).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each level of recursion reduces the number of elements being processed by a
    factor of *b*, so for *b* = 128 and *N* = 1048576, for example, two levels of
    recursion are required: one of size 8192 and one of size 64.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.5.* scanFan host function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis05a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: scanFan( T *out, const T *in, size_t N, int b )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: if ( N <= b ) {
  prefs: []
  type: TYPE_NORMAL
- en: scanAndWritePartials<T, false><<<1,b,b*sizeof(T)>>>(
  prefs: []
  type: TYPE_NORMAL
- en: out, 0, in, N, 1 );
  prefs: []
  type: TYPE_NORMAL
- en: return;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // device pointer to array of partial sums in global memory
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: T *gPartials = 0;
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // ceil(N/b)
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: size_t numPartials = (N1)/b;
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // number of CUDA threadblocks to use.  The kernels are
  prefs: []
  type: TYPE_NORMAL
- en: // blocking agnostic, so we can clamp to any number
  prefs: []
  type: TYPE_NORMAL
- en: // within CUDA's limits and the code will work.
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: const unsigned int maxBlocks = 150;   // maximum blocks to launch
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int numBlocks = min( numPartials, maxBlocks );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMalloc( &gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: numPartials*sizeof(T) ) );
  prefs: []
  type: TYPE_NORMAL
- en: scanAndWritePartials<T, true><<<numBlocks,b,b*sizeof(T)>>>(
  prefs: []
  type: TYPE_NORMAL
- en: out, gPartials, in, N, numPartials );
  prefs: []
  type: TYPE_NORMAL
- en: scanFan<T>( gPartials, gPartials, numPartials, b );
  prefs: []
  type: TYPE_NORMAL
- en: scanAddBaseSums<T><<<numBlocks, b>>>( out, gPartials, N,
  prefs: []
  type: TYPE_NORMAL
- en: numPartials );
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( gPartials );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.6](ch13.html#ch13lis06) completes the picture with a very simple
    kernel to fan-out results from global memory to global memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.6.* scanAddBaseSums kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis06a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: scanAddBaseSums(
  prefs: []
  type: TYPE_NORMAL
- en: T *out,
  prefs: []
  type: TYPE_NORMAL
- en: T *gBaseSums,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: size_t numBlocks )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: T fan_value = 0;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t iBlock = blockIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: iBlock < numBlocks;
  prefs: []
  type: TYPE_NORMAL
- en: iBlock += gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = iBlock*blockDim.x+tid;
  prefs: []
  type: TYPE_NORMAL
- en: if ( iBlock > 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: fan_value = gBaseSums[iBlock-1];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: out[index] += fan_value;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: At the highest level of recursion, the scan-then-fan strategy performs 4*N*
    global memory operations. The initial scan performs one read and one write, and
    then the fan in [Listing 13.4](ch13.html#ch13lis04) performs another read and
    write. We can decrease the number of global memory writes by first computing only
    reductions on the input array.
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.2\. Reduce-Then-Scan (Recursive)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Figure 13.14](ch13.html#ch13fig14) shows how this strategy works. As before,
    an array of ![Image](graphics/398equ01.jpg) partial sums of the input is computed
    and scanned to compute an array of base sums. But instead of doing the scan in
    the first pass, we compute only the partial sums in the first pass. The scan of
    the final output is then performed, adding the base sum along the way.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.14* Reduce-then-scan.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.7](ch13.html#ch13lis07) gives the code used to compute the array
    of partial sums, which uses the reduction code from [Listing 12.3](ch12.html#ch12lis03)
    as a subroutine. As with the reduction code, the kernel is templatized according
    to block size, and a wrapper template uses a `switch` statement to invoke specializations
    of the template.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.7.* scanReduceBlocks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis07a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, int numThreads>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: scanReduceBlocks( T *gPartials, const T *in, size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern volatile __shared__ T sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: gPartials += blockIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = blockIdx.x*blockDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = i+tid;
  prefs: []
  type: TYPE_NORMAL
- en: 'sPartials[tid] = (index < N) ? in[index] : 0;'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: reduceBlock<T,numThreads>( gPartials, sPartials );
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: gPartials += gridDim.x;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: scanReduceBlocks(
  prefs: []
  type: TYPE_NORMAL
- en: T *gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: const T *in,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: int numThreads,
  prefs: []
  type: TYPE_NORMAL
- en: int numBlocks )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: switch ( numThreads ) {
  prefs: []
  type: TYPE_NORMAL
- en: 'case  128: return scanReduceBlocks<T, 128> ... ( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case  256: return scanReduceBlocks<T, 256> ... ( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case  512: return scanReduceBlocks<T, 512> ... ( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: 'case 1024: return scanReduceBlocks<T,1024> ... ( ... );'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.8](ch13.html#ch13lis08) gives the kernel used to perform the scans.
    The main difference from [Listing 13.4](ch13.html#ch13lis04) is that instead of
    writing the sum of the input subarrays to global memory, the kernel adds the base
    sum corresponding to each subarray to the output elements before writing them.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.8.* scanWithBaseSums.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis08a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: scanWithBaseSums(
  prefs: []
  type: TYPE_NORMAL
- en: T *out,
  prefs: []
  type: TYPE_NORMAL
- en: const T *gBaseSums,
  prefs: []
  type: TYPE_NORMAL
- en: const T *in,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: size_t numBlocks )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern volatile __shared__ T sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t iBlock = blockIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: iBlock < numBlocks;
  prefs: []
  type: TYPE_NORMAL
- en: iBlock += gridDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: T base_sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = iBlock*blockDim.x+tid;
  prefs: []
  type: TYPE_NORMAL
- en: if ( iBlock > 0 && gBaseSums ) {
  prefs: []
  type: TYPE_NORMAL
- en: base_sum = gBaseSums[iBlock-1];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'sPartials[tid] = (index < N) ? in[index] : 0;'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: scanBlock( sPartials+tid );
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: if ( index < N ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[index] = sPartials[tid]+base_sum;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The host code for the reduce-then-scan strategy is given in [Listing 13.9](ch13.html#ch13lis09).
    At the highest level of recursion, the reduce-then-scan strategy performs 3*N*
    global memory operations. The initial reduction pass performs one read per element,
    and then the scan in [Listing 13.9](ch13.html#ch13lis09) performs another read
    and a write. As with fan-then-scan, each level of recursion reduces the number
    of elements being processed by a factor of *b*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.9.* scanReduceThenScan.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis09a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: scanReduceThenScan( T *out, const T *in, size_t N, int b )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: if ( N <= b ) {
  prefs: []
  type: TYPE_NORMAL
- en: return scanWithBaseSums<T><<<1,b,b*sizeof(T)>>>(
  prefs: []
  type: TYPE_NORMAL
- en: out, 0, in, N, 1 );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // device pointer to array of partial sums in global memory
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: T *gPartials = 0;
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // ceil(N/b) = number of partial sums to compute
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: size_t numPartials = (N1)/b;
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // number of CUDA threadblocks to use.  The kernels are blocking
  prefs: []
  type: TYPE_NORMAL
- en: // agnostic, so we can clamp to any number within CUDA's limits
  prefs: []
  type: TYPE_NORMAL
- en: // and the code will work.
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: const unsigned int maxBlocks = 150;
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int numBlocks = min( numPartials, maxBlocks );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMalloc( &gPartials, numPartials*sizeof(T) ) );
  prefs: []
  type: TYPE_NORMAL
- en: scanReduceBlocks<T>( gPartials, in, N, b, numBlocks );
  prefs: []
  type: TYPE_NORMAL
- en: scanReduceThenScan<T>( gPartials, gPartials, numPartials, b );
  prefs: []
  type: TYPE_NORMAL
- en: scanWithBaseSums<T><<<numBlocks,b,b*sizeof(T)>>>(
  prefs: []
  type: TYPE_NORMAL
- en: out,
  prefs: []
  type: TYPE_NORMAL
- en: gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: in,
  prefs: []
  type: TYPE_NORMAL
- en: N,
  prefs: []
  type: TYPE_NORMAL
- en: numPartials );
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: cudaFree( gPartials );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 13.4.3\. Reduce-Then-Scan (Two Pass)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Merrill^([4](ch13.html#ch13fn4)) describes another formulation of Scan that
    uses a small, fixed-size number of base sums. The algorithm is the same as [Figure
    13.14](ch13.html#ch13fig14), except that the array in step 2 is a relatively small,
    fixed size of perhaps a few hundred instead of ![Image](graphics/398equ01.jpg)
    partial sums. The number of partial sums is the same as the number of threadblocks
    to use, both for the reduction pass and for the Scan pass. [Listing 13.10](ch13.html#ch13lis10)
    shows the code to compute these partial sums, which is updated to compute reductions
    for subarrays of size `elementsPerPartial` as opposed to the thread block size.
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch13.html#ch13fn4a). [http://bit.ly/ZKtlh1](http://bit.ly/ZKtlh1)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.10.* scanReduceSubarrays.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis10a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, int numThreads>
  prefs: []
  type: TYPE_NORMAL
- en: __device__ void
  prefs: []
  type: TYPE_NORMAL
- en: scanReduceSubarray(
  prefs: []
  type: TYPE_NORMAL
- en: T *gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: const T *in,
  prefs: []
  type: TYPE_NORMAL
- en: size_t iBlock,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: int elementsPerPartial )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern volatile __shared__ T sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: size_t baseIndex = iBlock*elementsPerPartial;
  prefs: []
  type: TYPE_NORMAL
- en: T sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = tid; i < elementsPerPartial; i += blockDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = baseIndex+i;
  prefs: []
  type: TYPE_NORMAL
- en: if ( index < N )
  prefs: []
  type: TYPE_NORMAL
- en: sum += in[index];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: reduceBlock<T,numThreads>( &gPartials[iBlock], sPartials );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: /*
  prefs: []
  type: TYPE_NORMAL
- en: '* Compute the reductions of each subarray of size'
  prefs: []
  type: TYPE_NORMAL
- en: '* elementsPerPartial, and write them to gPartials.'
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, int numThreads>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: scanReduceSubarrays(
  prefs: []
  type: TYPE_NORMAL
- en: T *gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: const T *in,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: int elementsPerPartial )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern volatile __shared__ T sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iBlock = blockIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: iBlock*elementsPerPartial < N;
  prefs: []
  type: TYPE_NORMAL
- en: iBlock += gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: scanReduceSubarray<T,numThreads>(
  prefs: []
  type: TYPE_NORMAL
- en: gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: in,
  prefs: []
  type: TYPE_NORMAL
- en: iBlock,
  prefs: []
  type: TYPE_NORMAL
- en: N,
  prefs: []
  type: TYPE_NORMAL
- en: elementsPerPartial );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.11](ch13.html#ch13lis11) gives the Scan code, which has been modified
    to carry over each block’s sum as the Scan of that block is completed. The `bZeroPad`
    template parameter in [Listing 13.11](ch13.html#ch13lis11) and the utility function
    `scanSharedIndex` that uses it are described in more detail in [Section 13.5.1](ch13.html#ch13lev2sec4).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.11.* scan2Level_kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis11a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, bool bZeroPad>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: scan2Level_kernel(
  prefs: []
  type: TYPE_NORMAL
- en: T *out,
  prefs: []
  type: TYPE_NORMAL
- en: const T *gBaseSums,
  prefs: []
  type: TYPE_NORMAL
- en: const T *in,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: size_t elementsPerPartial )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern volatile __shared__ T sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: int sIndex = scanSharedIndex<bZeroPad>( threadIdx.x );
  prefs: []
  type: TYPE_NORMAL
- en: if ( bZeroPad ) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[sIndex-16] = 0;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: T base_sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: if ( blockIdx.x && gBaseSums ) {
  prefs: []
  type: TYPE_NORMAL
- en: base_sum = gBaseSums[blockIdx.x-1];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0;
  prefs: []
  type: TYPE_NORMAL
- en: i < elementsPerPartial;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = blockIdx.x*elementsPerPartial + i + tid;
  prefs: []
  type: TYPE_NORMAL
- en: 'sPartials[sIndex] = (index < N) ? in[index] : 0;'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: scanBlock<T,bZeroPad>( sPartials+sIndex );
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: if ( index < N ) {
  prefs: []
  type: TYPE_NORMAL
- en: out[index] = sPartials[sIndex]+base_sum;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: // carry forward from this block to the next.
  prefs: []
  type: TYPE_NORMAL
- en: base_sum += sPartials[
  prefs: []
  type: TYPE_NORMAL
- en: scanSharedIndex<bZeroPad>( blockDim.x-1 ) ];
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.12](ch13.html#ch13lis12) gives the host code for Merrill’s two-pass
    reduce-then-scan algorithm. Since the number of partials computed is small and
    never varies, the host code never has to allocate global memory in order to perform
    the scan; instead, we declare a `__device__` array that is allocated at module
    load time'
  prefs: []
  type: TYPE_NORMAL
- en: __device__ int g_globalPartials[MAX_PARTIALS];
  prefs: []
  type: TYPE_NORMAL
- en: and obtain its address by calling `cudaGetSymbolAddress()`.
  prefs: []
  type: TYPE_NORMAL
- en: status = cudaGetSymbolAddress(
  prefs: []
  type: TYPE_NORMAL
- en: (void **) &globalPartials,
  prefs: []
  type: TYPE_NORMAL
- en: g_globalPartials );
  prefs: []
  type: TYPE_NORMAL
- en: The routine then computes the number of elements per partial and number of threadblocks
    to use and invokes the three (3) kernels needed to perform the computation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.12.* scan2Level.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis12a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, bool bZeroPad>
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: scan2Level( T *out, const T *in, size_t N, int b )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int sBytes = scanSharedMemory<T,bZeroPad>( b );
  prefs: []
  type: TYPE_NORMAL
- en: if ( N <= b ) {
  prefs: []
  type: TYPE_NORMAL
- en: return scan2Level_kernel<T, bZeroPad><<<1,b,sBytes>>>(
  prefs: []
  type: TYPE_NORMAL
- en: out, 0, in, N, N );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: T *gPartials = 0;
  prefs: []
  type: TYPE_NORMAL
- en: status = cudaGetSymbolAddress(
  prefs: []
  type: TYPE_NORMAL
- en: (void **) &gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: g_globalPartials );
  prefs: []
  type: TYPE_NORMAL
- en: if ( cudaSuccess ==  status )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // ceil(N/b) = number of partial sums to compute
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: size_t numPartials = (N+b-1)/b;
  prefs: []
  type: TYPE_NORMAL
- en: if ( numPartials > MAX_PARTIALS ) {
  prefs: []
  type: TYPE_NORMAL
- en: numPartials = MAX_PARTIALS;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // elementsPerPartial has to be a multiple of b
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int elementsPerPartial =
  prefs: []
  type: TYPE_NORMAL
- en: (N+numPartials-1)/numPartials;
  prefs: []
  type: TYPE_NORMAL
- en: elementsPerPartial = b * ((elementsPerPartial+b-1)/b);
  prefs: []
  type: TYPE_NORMAL
- en: numPartials = (N+elementsPerPartial-1)/elementsPerPartial;
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: // number of CUDA threadblocks to use.  The kernels are
  prefs: []
  type: TYPE_NORMAL
- en: // blocking agnostic, so we can clamp to any number within
  prefs: []
  type: TYPE_NORMAL
- en: // CUDA's limits and the code will work.
  prefs: []
  type: TYPE_NORMAL
- en: //
  prefs: []
  type: TYPE_NORMAL
- en: const unsigned int maxBlocks = MAX_PARTIALS;
  prefs: []
  type: TYPE_NORMAL
- en: unsigned int numBlocks = min( numPartials, maxBlocks );
  prefs: []
  type: TYPE_NORMAL
- en: scanReduceSubarrays<T>(
  prefs: []
  type: TYPE_NORMAL
- en: gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: in,
  prefs: []
  type: TYPE_NORMAL
- en: N,
  prefs: []
  type: TYPE_NORMAL
- en: elementsPerPartial,
  prefs: []
  type: TYPE_NORMAL
- en: numBlocks,
  prefs: []
  type: TYPE_NORMAL
- en: b );
  prefs: []
  type: TYPE_NORMAL
- en: scan2Level_kernel<T, bZeroPad><<<1,b,sBytes>>>(
  prefs: []
  type: TYPE_NORMAL
- en: gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: 0,
  prefs: []
  type: TYPE_NORMAL
- en: gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: numPartials,
  prefs: []
  type: TYPE_NORMAL
- en: numPartials );
  prefs: []
  type: TYPE_NORMAL
- en: scan2Level_kernel<T, bZeroPad><<<numBlocks,b,sBytes>>>(
  prefs: []
  type: TYPE_NORMAL
- en: out,
  prefs: []
  type: TYPE_NORMAL
- en: gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: in,
  prefs: []
  type: TYPE_NORMAL
- en: N,
  prefs: []
  type: TYPE_NORMAL
- en: elementsPerPartial );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 13.5\. Warp Scans
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we’ve focused on constructing our Scan implementations from the top
    down. At the bottom of all three of our Scan implementations, however, lurks an
    entirely different software approach to Scan. For subarrays of size 32 or less,
    we use a special *warp scan* modeled on the Kogge-Stone circuit ([Figure 13.11](ch13.html#ch13fig11)).
    Kogge-Stone circuits are *work-inefficient*, meaning they perform many operations
    despite their small depth, but at the warp level, where execution resources of
    CUDA hardware are available whether or not the developer uses them, Kogge-Stone
    works well on CUDA hardware.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.13](ch13.html#ch13lis13) gives a `__device__` routine that is designed
    to operate on shared memory, the fastest way for threads to exchange data with
    one another. Because there are no shared memory conflicts and the routine executes
    at warp granularity, no thread synchronization is needed during updates to the
    shared memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.13.* scanWarp.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis13a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ T
  prefs: []
  type: TYPE_NORMAL
- en: scanWarp( volatile T *sPartials )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: const int lane = tid & 31;
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane >=  1 ) sPartials[0] += sPartials[- 1];
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane >=  2 ) sPartials[0] += sPartials[- 2];
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane >=  4 ) sPartials[0] += sPartials[- 4];
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane >=  8 ) sPartials[0] += sPartials[- 8];
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane >= 16 ) sPartials[0] += sPartials[-16];
  prefs: []
  type: TYPE_NORMAL
- en: return sPartials[0];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 13.5.1\. Zero Padding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can reduce the number of machine instructions needed to implement the warp
    scan by interleaving the warps’ data with 16-element arrays of 0’s, enabling the
    conditionals to be removed. [Listing 13.14](ch13.html#ch13lis14) gives a version
    of `scanWarp` that assumes 16 zero elements preceding the base address in shared
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.14.* scanWarp0.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis14a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: __device__ T scanWarp0( volatile T *sharedPartials, int idx )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: const int lane = tid & 31;
  prefs: []
  type: TYPE_NORMAL
- en: sharedPartials[idx] += sharedPartials[idx -  1];
  prefs: []
  type: TYPE_NORMAL
- en: sharedPartials[idx] += sharedPartials[idx -  2];
  prefs: []
  type: TYPE_NORMAL
- en: sharedPartials[idx] += sharedPartials[idx -  4];
  prefs: []
  type: TYPE_NORMAL
- en: sharedPartials[idx] += sharedPartials[idx -  8];
  prefs: []
  type: TYPE_NORMAL
- en: sharedPartials[idx] += sharedPartials[idx - 16];
  prefs: []
  type: TYPE_NORMAL
- en: return sharedPartials[idx];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13.15](ch13.html#ch13fig15) shows how the interleaving works for a
    256-thread block, which contains 8 warps. The shared memory index is computed
    as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p408pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: const int warp = tid >> 5;
  prefs: []
  type: TYPE_NORMAL
- en: const int lane = tid & 31;
  prefs: []
  type: TYPE_NORMAL
- en: const int sharedIndex = 49 * warp + 32 + lane;
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/13fig15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 13.15* Interleaved zeros for warp scan.'
  prefs: []
  type: TYPE_NORMAL
- en: The initialization to 0 is then done as follows.
  prefs: []
  type: TYPE_NORMAL
- en: partials[sharedIndex-16] = 0;
  prefs: []
  type: TYPE_NORMAL
- en: The other area where this change affects the shared memory addressing is in
    the block scan subroutine. The index for each partial sum for each warp must be
    offset by 16 to enable the single warp scan that computes the base sums to work.
    Finally, the kernel invocation must reserve enough shared memory to hold both
    the partial sums and the zeros.
  prefs: []
  type: TYPE_NORMAL
- en: 13.5.2\. Templated Formulations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The faster, zero-padded implementation of Scan requires more shared memory,
    a resource requirement that not all applications can accommodate. To enable our
    code to support both versions, [Listing 13.15](ch13.html#ch13lis15) shows utility
    functions that take a `bool` template parameter `bZeroPad`. The `scanSharedMemory`
    function returns the amount of shared memory needed for a given block size. `scanSharedIndex`
    returns the shared memory index corresponding to a given thread. In turn, [Listing
    13.16](ch13.html#ch13lis16) gives the templated version of `scanWarp` that works
    for both the zero-padded and non-zero-padded cases.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.15.* Shared memory utilities for zero padding.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis15a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<bool bZeroPad>
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ int
  prefs: []
  type: TYPE_NORMAL
- en: scanSharedIndex( int tid )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: if ( bZeroPad ) {
  prefs: []
  type: TYPE_NORMAL
- en: const int warp = tid >> 5;
  prefs: []
  type: TYPE_NORMAL
- en: const int lane = tid & 31;
  prefs: []
  type: TYPE_NORMAL
- en: return 49 * warp + 16 + lane;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else {
  prefs: []
  type: TYPE_NORMAL
- en: return tid;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: template<typename T, bool bZeroPad>
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ __host__ int
  prefs: []
  type: TYPE_NORMAL
- en: scanSharedMemory( int numThreads )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: if ( bZeroPad ) {
  prefs: []
  type: TYPE_NORMAL
- en: const int warpcount = numThreads>>5;
  prefs: []
  type: TYPE_NORMAL
- en: return (49 * warpcount + 16)*sizeof(T);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else {
  prefs: []
  type: TYPE_NORMAL
- en: return numThreads*sizeof(T);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.16.* scanWarp (templated).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis16a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, bool bZeroPadded>
  prefs: []
  type: TYPE_NORMAL
- en: inline __device__ T
  prefs: []
  type: TYPE_NORMAL
- en: scanWarp( volatile T *sPartials )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: T t = sPartials[0];
  prefs: []
  type: TYPE_NORMAL
- en: if ( bZeroPadded ) {
  prefs: []
  type: TYPE_NORMAL
- en: t += sPartials[- 1]; sPartials[0] = t;
  prefs: []
  type: TYPE_NORMAL
- en: t += sPartials[- 2]; sPartials[0] = t;
  prefs: []
  type: TYPE_NORMAL
- en: t += sPartials[- 4]; sPartials[0] = t;
  prefs: []
  type: TYPE_NORMAL
- en: t += sPartials[- 8]; sPartials[0] = t;
  prefs: []
  type: TYPE_NORMAL
- en: t += sPartials[-16]; sPartials[0] = t;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else {
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: const int lane = tid & 31;
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane >=  1 ) { t += sPartials[- 1]; sPartials[0] = t; }
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane >=  2 ) { t += sPartials[- 2]; sPartials[0] = t; }
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane >=  4 ) { t += sPartials[- 4]; sPartials[0] = t; }
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane >=  8 ) { t += sPartials[- 8]; sPartials[0] = t; }
  prefs: []
  type: TYPE_NORMAL
- en: if ( lane >= 16 ) { t += sPartials[-16]; sPartials[0] = t; }
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return t;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 13.5.3\. Warp Shuffle
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The SM 3.0 instruction set added the warp shuffle instruction, which enables
    registers to be exchanged within the 32 threads of a warp. The “up” and “down”
    variants of the warp shuffle can be used to implement scan and reverse scan, respectively.
    The shuffle instruction takes a register to exchange and an offset to apply to
    the lane ID. It returns a predicate that is false for inactive threads or threads
    whose offset is outside the warp.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.17](ch13.html#ch13lis17) gives `scanWarpShuffle`, a device function
    that implements an inclusive warp scan with the shuffle instruction. The template
    parameter is an integer, and typically the value 5 is passed because 5 is the
    base 2 logarithm of the warp size of 32\. `scanWarpShuffle` uses a utility function
    `scanWarpShuffle_step`, implemented in inline PTX, because the compiler does not
    emit efficient code to deal with the predicate returned by the shuffle instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.17.* scanWarpShuffle device function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis17a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __device__ __forceinline__
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: scanWarpShuffle_step(int partial, int offset)
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int result;
  prefs: []
  type: TYPE_NORMAL
- en: asm(
  prefs: []
  type: TYPE_NORMAL
- en: '"{.reg .u32 r0;"'
  prefs: []
  type: TYPE_NORMAL
- en: '".reg .pred p;"'
  prefs: []
  type: TYPE_NORMAL
- en: '"shfl.up.b32 r0|p, %1, %2, 0;"'
  prefs: []
  type: TYPE_NORMAL
- en: '"@p add.u32 r0, r0, %3;"'
  prefs: []
  type: TYPE_NORMAL
- en: '"mov.u32 %0, r0;}"'
  prefs: []
  type: TYPE_NORMAL
- en: ': "=r"(result) : "r"(partial), "r"(offset), "r"(partial));'
  prefs: []
  type: TYPE_NORMAL
- en: return result;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: template <int levels>
  prefs: []
  type: TYPE_NORMAL
- en: __device__ __forceinline__
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: scanWarpShuffle(int mysum)
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for(int i = 0; i < levels; ++i)
  prefs: []
  type: TYPE_NORMAL
- en: mysum = scanWarpShuffle_step(mysum, 1 << i);
  prefs: []
  type: TYPE_NORMAL
- en: return mysum;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.18](ch13.html#ch13lis18) illustrates how to extend `scanWarpShuffle`
    to scan the values across a thread block using shared memory. Following the same
    pattern as the block scan in [Listing 13.3](ch13.html#ch13lis03), `scanBlockShuffle`
    uses the warp shuffle to scan each warp. Each warp writes its partial sum to shared
    memory, and then the warp shuffle is used again, this time by a single warp, to
    scan these base sums. Finally, each warp adds its corresponding base sum to compute
    the final output value.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.18.* scanBlockShuffle device function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis18a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template <int logBlockSize>
  prefs: []
  type: TYPE_NORMAL
- en: __device__
  prefs: []
  type: TYPE_NORMAL
- en: int
  prefs: []
  type: TYPE_NORMAL
- en: scanBlockShuffle(int val, const unsigned int idx)
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: const unsigned int lane   = idx & 31;
  prefs: []
  type: TYPE_NORMAL
- en: const unsigned int warpid = idx >> 5;
  prefs: []
  type: TYPE_NORMAL
- en: __shared__ int sPartials[32];
  prefs: []
  type: TYPE_NORMAL
- en: // Intra-warp scan in each warp
  prefs: []
  type: TYPE_NORMAL
- en: val = scanWarpShuffle<5>(val);
  prefs: []
  type: TYPE_NORMAL
- en: // Collect per-warp results
  prefs: []
  type: TYPE_NORMAL
- en: if (lane == 31) sPartials[warpid] = val;
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: // Use first warp to scan per-warp results
  prefs: []
  type: TYPE_NORMAL
- en: if (warpid == 0) {
  prefs: []
  type: TYPE_NORMAL
- en: int t = sPartials[lane];
  prefs: []
  type: TYPE_NORMAL
- en: t = scanWarpShuffle<logBlockSize-5>( t );
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[lane] = t;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: // Add scanned base sum for final result
  prefs: []
  type: TYPE_NORMAL
- en: if (warpid > 0) {
  prefs: []
  type: TYPE_NORMAL
- en: val += sPartials[warpid - 1];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: return val;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 13.5.4\. Instruction Counts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To examine the tradeoffs between the different variations of warp scan discussed
    in this section, we compiled for SM 3.0 and used `cuobjdump` to disassemble the
    three implementations.
  prefs: []
  type: TYPE_NORMAL
- en: • The non-zero-padded implementation given in [Listing 13.19](ch13.html#ch13lis19)
    is 30 instructions and includes a great deal of branching (the `SSY/.S` instruction
    pairs push and pop the divergence stack, as described in [Section 8.4.2](ch08.html#ch08lev2sec18)).
  prefs: []
  type: TYPE_NORMAL
- en: • The zero-padded implementation given in [Listing 13.20](ch13.html#ch13lis20)
    is 17 instructions because it does not check the lane ID before performing its
    shared memory reads. Note that because the shared memory operations are guaranteed
    to be contained within a warp, there is no need for barrier synchronization via
    the `__syncthreads()` intrinsic, which compiles to `BAR.SYNC` instructions in
    SASS.
  prefs: []
  type: TYPE_NORMAL
- en: • The shuffle-based implementation given in [Listing 13.21](ch13.html#ch13lis21)
    is only 11 instructions.
  prefs: []
  type: TYPE_NORMAL
- en: We confirmed that the shuffle-based implementation is, in fact, significantly
    faster (about 2x) than the general case given in [Listing 13.19](ch13.html#ch13lis19),
    running on a synthetic workload that isolates the warp scan.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.19.* SASS for warp scan (no zero padding).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis19a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: /*0070*/     SSY 0xa0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0078*/     @P0 NOP.S CC.T;
  prefs: []
  type: TYPE_NORMAL
- en: /*0088*/     LDS R5, [R3+-0x4];
  prefs: []
  type: TYPE_NORMAL
- en: /*0090*/     IADD R0, R5, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0098*/     STS.S [R3], R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*00a0*/     ISETP.LT.U32.AND P0, pt, R4, 0x2, pt;
  prefs: []
  type: TYPE_NORMAL
- en: /*00a8*/     SSY 0xd8;
  prefs: []
  type: TYPE_NORMAL
- en: /*00b0*/     @P0 NOP.S CC.T;
  prefs: []
  type: TYPE_NORMAL
- en: /*00b8*/     LDS R5, [R3+-0x8];
  prefs: []
  type: TYPE_NORMAL
- en: /*00c8*/     IADD R0, R5, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*00d0*/     STS.S [R3], R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*00d8*/     ISETP.LT.U32.AND P0, pt, R4, 0x4, pt;
  prefs: []
  type: TYPE_NORMAL
- en: /*00e0*/     SSY 0x110;
  prefs: []
  type: TYPE_NORMAL
- en: /*00e8*/     @P0 NOP.S CC.T;
  prefs: []
  type: TYPE_NORMAL
- en: /*00f0*/     LDS R5, [R3+-0x10];
  prefs: []
  type: TYPE_NORMAL
- en: /*00f8*/     IADD R0, R5, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0108*/     STS.S [R3], R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0110*/     ISETP.LT.U32.AND P0, pt, R4, 0x8, pt;
  prefs: []
  type: TYPE_NORMAL
- en: /*0118*/     SSY 0x140;
  prefs: []
  type: TYPE_NORMAL
- en: /*0120*/     @P0 NOP.S CC.T;
  prefs: []
  type: TYPE_NORMAL
- en: /*0128*/     LDS R5, [R3+-0x20];
  prefs: []
  type: TYPE_NORMAL
- en: /*0130*/     IADD R0, R5, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0138*/     STS.S [R3], R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0148*/     ISETP.LT.U32.AND P0, pt, R4, 0x10, pt;
  prefs: []
  type: TYPE_NORMAL
- en: /*0150*/     SSY 0x178;
  prefs: []
  type: TYPE_NORMAL
- en: /*0158*/     @P0 NOP.S CC.T;
  prefs: []
  type: TYPE_NORMAL
- en: /*0160*/     LDS R4, [R3+-0x40];
  prefs: []
  type: TYPE_NORMAL
- en: /*0168*/     IADD R0, R4, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0170*/     STS.S [R3], R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0178*/     BAR.SYNC 0x0;
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.20.* SASS for warp scan (with zero padding).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis20a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: /*0058*/     LDS R4, [R3+-0x4];
  prefs: []
  type: TYPE_NORMAL
- en: /*0060*/     LDS R0, [R3];
  prefs: []
  type: TYPE_NORMAL
- en: /*0068*/     IADD R4, R4, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0070*/     STS [R3], R4;
  prefs: []
  type: TYPE_NORMAL
- en: /*0078*/     LDS R0, [R3+-0x8];
  prefs: []
  type: TYPE_NORMAL
- en: /*0088*/     IADD R4, R4, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0090*/     STS [R3], R4;
  prefs: []
  type: TYPE_NORMAL
- en: /*0098*/     LDS R0, [R3+-0x10];
  prefs: []
  type: TYPE_NORMAL
- en: /*00a0*/     IADD R4, R4, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*00a8*/     STS [R3], R4;
  prefs: []
  type: TYPE_NORMAL
- en: /*00b0*/     LDS R0, [R3+-0x20];
  prefs: []
  type: TYPE_NORMAL
- en: /*00b8*/     IADD R4, R4, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*00c8*/     STS [R3], R4;
  prefs: []
  type: TYPE_NORMAL
- en: /*00d0*/     LDS R0, [R3+-0x40];
  prefs: []
  type: TYPE_NORMAL
- en: /*00d8*/     IADD R0, R4, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*00e0*/     STS [R3], R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*00e8*/     BAR.SYNC 0x0;
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.21.* SASS for warp scan (using shuffle).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis21a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: /*0050*/     SHFL.UP P0, R4, R0, 0x1, 0x0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0058*/     IADD.X R3, R3, c [0x0] [0x144];
  prefs: []
  type: TYPE_NORMAL
- en: /*0060*/     @P0 IADD R4, R4, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0068*/     SHFL.UP P0, R0, R4, 0x2, 0x0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0070*/     @P0 IADD R0, R0, R4;
  prefs: []
  type: TYPE_NORMAL
- en: /*0078*/     SHFL.UP P0, R4, R0, 0x4, 0x0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0088*/     @P0 IADD R4, R4, R0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0090*/     SHFL.UP P0, R0, R4, 0x8, 0x0;
  prefs: []
  type: TYPE_NORMAL
- en: /*0098*/     @P0 IADD R0, R0, R4;
  prefs: []
  type: TYPE_NORMAL
- en: /*00a0*/     SHFL.UP P0, R4, R0, 0x10, 0x0;
  prefs: []
  type: TYPE_NORMAL
- en: /*00a8*/     @P0 IADD R4, R4, R0;
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 13.6\. Stream Compaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scan implementations often operate on *predicates*—truth values (0 or 1) computed
    by evaluating a condition. As mentioned at the beginning of the chapter, an exclusive
    scan of predicates can be used to implement *stream compaction*, a class of parallel
    problems where only the “interesting” elements of an input array are written to
    the output. For predicate values where the predicate is equal to 1 for “interesting”
    elements, the exclusive scan computes the output index of the element.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, let’s write a Scan implementation that operates on an array of
    `int` and emits all `ints` that are odd.^([5](ch13.html#ch13fn5)) Our implementation
    is based on Merrill’s reduce-then-scan with a fixed number of blocks *b.*
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch13.html#ch13fn5a). The code is easily modified to evaluate more complicated
    predicates.'
  prefs: []
  type: TYPE_NORMAL
- en: '**1.** A first reduction pass over the input data gives the number of elements
    in each ![Image](graphics/398equ01.jpg) subarray that meets the criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: '**2.** A scan is performed on the array of *b* counts, giving the base index
    for the output of each subarray.'
  prefs: []
  type: TYPE_NORMAL
- en: '**3.** A scan is performed on the input array, evaluating the criteria and
    using the “seed” value as the base index for each subarray’s output.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.22](ch13.html#ch13lis22) shows the code for step 1: `predicateReduceSubarrays_odd()`
    function invokes subroutines `predicateReduceSubarray_odd()` and `isOdd()` to
    evaluate the predicate for each array element, compute the reduction, and write
    it to the array of base sums.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.22.* `predicateReduceSubarrays_odd`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis22a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T>
  prefs: []
  type: TYPE_NORMAL
- en: __host__ __device__ bool
  prefs: []
  type: TYPE_NORMAL
- en: isOdd( T x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: return x & 1;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, int numThreads>
  prefs: []
  type: TYPE_NORMAL
- en: __device__ void
  prefs: []
  type: TYPE_NORMAL
- en: predicateReduceSubarray_odd(
  prefs: []
  type: TYPE_NORMAL
- en: int *gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: const T *in,
  prefs: []
  type: TYPE_NORMAL
- en: size_t iBlock,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: int elementsPerPartial )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern volatile __shared__ int sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: size_t baseIndex = iBlock*elementsPerPartial;
  prefs: []
  type: TYPE_NORMAL
- en: int sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = tid; i < elementsPerPartial; i += blockDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = baseIndex+i;
  prefs: []
  type: TYPE_NORMAL
- en: if ( index < N )
  prefs: []
  type: TYPE_NORMAL
- en: sum += isOdd( in[index] );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[tid] = sum;
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: reduceBlock<int,numThreads>( &gPartials[iBlock], sPartials );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: /*
  prefs: []
  type: TYPE_NORMAL
- en: '* Compute the reductions of each subarray of size'
  prefs: []
  type: TYPE_NORMAL
- en: '* elementsPerPartial, and write them to gPartials.'
  prefs: []
  type: TYPE_NORMAL
- en: '*/'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, int numThreads>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: predicateReduceSubarrays_odd(
  prefs: []
  type: TYPE_NORMAL
- en: int *gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: const T *in,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: int elementsPerPartial )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern volatile __shared__ int sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: for ( int iBlock = blockIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: iBlock*elementsPerPartial < N;
  prefs: []
  type: TYPE_NORMAL
- en: iBlock += gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: predicateReduceSubarray_odd<T,numThreads>(
  prefs: []
  type: TYPE_NORMAL
- en: gPartials,
  prefs: []
  type: TYPE_NORMAL
- en: in,
  prefs: []
  type: TYPE_NORMAL
- en: iBlock,
  prefs: []
  type: TYPE_NORMAL
- en: N,
  prefs: []
  type: TYPE_NORMAL
- en: elementsPerPartial );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Computing the scan of the array of base sums is done by invoking the kernel
    in [Listing 13.23](ch13.html#ch13lis23). Once this is done, each base sum element
    contains the number of preceding array elements for which the predicate is true,
    which also is the start index of the corresponding block’s output array.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 13.23.* `streamCompact_odd` kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch13_images.html#p13lis23a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<class T, bool bZeroPad>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: streamCompact_odd(
  prefs: []
  type: TYPE_NORMAL
- en: T *out,
  prefs: []
  type: TYPE_NORMAL
- en: int *outCount,
  prefs: []
  type: TYPE_NORMAL
- en: const int *gBaseSums,
  prefs: []
  type: TYPE_NORMAL
- en: const T *in,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: size_t elementsPerPartial )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: extern volatile __shared__ int sPartials[];
  prefs: []
  type: TYPE_NORMAL
- en: const int tid = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: int sIndex = scanSharedIndex<bZeroPad>( threadIdx.x );
  prefs: []
  type: TYPE_NORMAL
- en: if ( bZeroPad ) {
  prefs: []
  type: TYPE_NORMAL
- en: sPartials[sIndex-16] = 0;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // exclusive scan element gBaseSums[blockIdx.x]
  prefs: []
  type: TYPE_NORMAL
- en: int base_sum = 0;
  prefs: []
  type: TYPE_NORMAL
- en: if ( blockIdx.x && gBaseSums ) {
  prefs: []
  type: TYPE_NORMAL
- en: base_sum = gBaseSums[blockIdx.x-1];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0;
  prefs: []
  type: TYPE_NORMAL
- en: i < elementsPerPartial;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: size_t index = blockIdx.x*elementsPerPartial + i + tid;
  prefs: []
  type: TYPE_NORMAL
- en: 'int value = (index < N) ? in[index] : 0;'
  prefs: []
  type: TYPE_NORMAL
- en: 'sPartials[sIndex] = (index < N) ? isOdd( value ) : 0;'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: scanBlock<int,bZeroPad>( sPartials+sIndex );
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: if ( index < N && isOdd( value ) ) {
  prefs: []
  type: TYPE_NORMAL
- en: int outIndex = base_sum;
  prefs: []
  type: TYPE_NORMAL
- en: if ( tid ) {
  prefs: []
  type: TYPE_NORMAL
- en: outIndex += sPartials[
  prefs: []
  type: TYPE_NORMAL
- en: scanSharedIndex<bZeroPad>(tid-1)];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: out[outIndex] = value;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: // carry forward from this block to the next.
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int inx = scanSharedIndex<bZeroPad>( blockDim.x-1 );
  prefs: []
  type: TYPE_NORMAL
- en: base_sum += sPartials[ inx ];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if ( threadIdx.x == 0 && blockIdx.x == 0 ) {
  prefs: []
  type: TYPE_NORMAL
- en: if ( gBaseSums ) {
  prefs: []
  type: TYPE_NORMAL
- en: '*outCount = gBaseSums[gridDim.x-1];'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else {
  prefs: []
  type: TYPE_NORMAL
- en: int inx = scanSharedIndex<bZeroPad>( blockDim.x-1 );
  prefs: []
  type: TYPE_NORMAL
- en: '*outCount = sPartials[ inx ];'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 13.23](ch13.html#ch13lis23) shows the code for step 3, which takes
    the input array and the array of base sums, evaluates the predicate again for
    each input array element, and writes the element to the correctly indexed output
    element if the predicate is true. The host code is analogous to [Listing 13.12](ch13.html#ch13lis12),
    with minor changes, and is not shown here.'
  prefs: []
  type: TYPE_NORMAL
- en: 13.7\. References (Parallel Scan Algorithms)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The recursive scan-then-fan is described in the NVIDIA Technical Report NVR-2008-003
    by Sengupta et al. The recursive reduce-then-scan algorithm is described by Dotsenko
    et al. The two-level reduce-then-scan algorithm is due to Merrill. Merrill’s paper
    is extremely valuable reading, both for background and for an overview of negative
    results—for example, an attempted formulation of Scan modeled on Sklansky’s minimum-depth
    circuit whose performance was disappointing.
  prefs: []
  type: TYPE_NORMAL
- en: Blelloch, Guy E. Prefix sums and their applications. Technical Report CMU-CS-90-190.
  prefs: []
  type: TYPE_NORMAL
- en: Dotsenko, Yuri, Naga K. Govindaraju, Peter-Pike Sloan, Charles Boyd, and John
    Manferdelli. Fast scan algorithms in graphics processors. In *Proceedings of the
    22nd Annual International Conference on Supercomputing*, ACM, 2008, pp. 205–213.
  prefs: []
  type: TYPE_NORMAL
- en: Fellner, D., and S. Spender, eds. SIGGRAPH/Eurographics Conference on Graphics
    Hardware. Eurographics Association, Aire-la-Ville, Switzerland, pp. 97–106.
  prefs: []
  type: TYPE_NORMAL
- en: Harris, Mark, and Michael Garland. Optimizing parallel prefix operations for
    the Fermi architecture. In *GPU Computing Gems, Jade Edition*, Wen-Mei Hwu, ed.
    Morgan Kaufmann, Waltham, MA, 2012, pp. 29–38.
  prefs: []
  type: TYPE_NORMAL
- en: Harris, Mark, Shubhabrata Sengupta, and John Owens. Parallel prefix sum (scan)
    with CUDA. In *GPU Gems 3*, H. Nguyen, ed. Addison-Wesley, Boston, MA, Aug. 2007.
  prefs: []
  type: TYPE_NORMAL
- en: Merrill, Duane, and Andrew Grimshaw. Parallel scan for stream architectures.
    Technical Report CS2009-14\. Department of Computer Science, University of Virginia.
  prefs: []
  type: TYPE_NORMAL
- en: Sengupta, Shubhabrata, Mark Harris, and Michael Garland. Efficient parallel
    scan algorithms for GPUs. NVIDIA Technical Report NVR-2008-003\. December 2008.
  prefs: []
  type: TYPE_NORMAL
- en: '[http://research.nvidia.com/publication/efficient-parallel-scan-algorithms-gpus](http://research.nvidia.com/publication/efficient-parallel-scan-algorithms-gpus)'
  prefs: []
  type: TYPE_NORMAL
- en: Sengupta, Shubhabrata, Mark Harris, ZhangYao Zhang, and John D. Owens. Scan
    primitives for GPU computing. In *Proceedings of the 22nd ACM SIGGRAPH/Eurographics
    Symposium on Graphics Hardware.* San Diego, CA, August 4–5, 2007.
  prefs: []
  type: TYPE_NORMAL
- en: 13.8\. Further Reading (Parallel Prefix Sum Circuits)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a rich literature on circuits to compute parallel prefix sums. Besides
    the Brent-Kung, Sklansky, and Kogge-Stone formulations, other examples of scan
    circuits include Ladner-Fischer and more recent work by Lin and Hsiao. Hinze describes
    an algebra of scans that can be used to reason about Scan implementations. The
    details of his work are outside the scope of this book, but his paper is highly
    recommended reading.
  prefs: []
  type: TYPE_NORMAL
- en: Sean Baxter’s Web site, [http://www.moderngpu.com](http://www.moderngpu.com),
    is an excellent resource for optimized Scan and its applications.
  prefs: []
  type: TYPE_NORMAL
- en: Brent, Richard P., and H.T. Kung. A regular layout for parallel adders. *IEEE
    Transactions on Computers* C-31, 1982, pp. 260–264.
  prefs: []
  type: TYPE_NORMAL
- en: Hinze, Ralf. An algebra of scans. In *Mathematics of Program Construction*,
    Springer, 2004, Stirling, Scotland, pp. 186–210.
  prefs: []
  type: TYPE_NORMAL
- en: Kogge, Peter M., and Harold S. Stone. A parallel algorithm for the efficient
    solution of a general class of recurrence equations. *IEEE Transactions on Computers*
    C-22, 1973, pp. 783–791.
  prefs: []
  type: TYPE_NORMAL
- en: Sklansky, J. Conditional sum addition logic. *IRE Trans. Electron. Comput*.
    9 (2), June 1960, pp. 226–231.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter 14\. N-Body
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: N-Body computations are a family of computation that models a set of particles
    (known as *bodies*), each of which must consider all the other bodies during the
    computation. Example applications of N-Body include (but are not limited to) the
    following.
  prefs: []
  type: TYPE_NORMAL
- en: • Gravitational simulation in which stars exert gravitational forces
  prefs: []
  type: TYPE_NORMAL
- en: • Molecular modeling in which ions exert electrostatic forces
  prefs: []
  type: TYPE_NORMAL
- en: • Particle systems in computer graphics to simulate water and fire
  prefs: []
  type: TYPE_NORMAL
- en: • “Boids,” a technique for computer animation designed to simulate flocking
    behavior
  prefs: []
  type: TYPE_NORMAL
- en: Typically the paths of the bodies are being simulated per timestep, and computing
    each timestep costs O(*N*²) operations for *N* bodies. In most formulations, the
    forces quickly decrease with distance, leading to hierarchical algorithms in which
    (for example) the mass and location of the center-of-mass for a collection of
    bodies are used to avoid performing the full O(*N*²) computations needed otherwise.
    Barnes-Hut algorithms reduce the runtime to O(*N*lg*N*) by introducing a spatial
    hierarchy that approximates the forces between clusters of objects; for applications
    where the “leaf nodes” of the computation contain *k* bodies, O(*k*²) computations
    must be performed in a given leaf. It is this O(*k*²) portion of the computation
    at which GPUs excel.
  prefs: []
  type: TYPE_NORMAL
- en: N-Body workloads have proven the most effective way for GPUs to approach their
    theoretical limit in processing power. In their GPU Gems 3 paper “Fast N-Body
    Simulation with CUDA,”^([1](ch14.html#ch14fn1)) Harris et al. frequently cite
    this theoretical limit in explaining why further performance improvements are
    not possible. The GPU in question, NVIDIA GeForce 8800 GTX, was so effective at
    N-Body computations that it outperformed custom GRAPE-6 hardware that had been
    specifically designed to perform astrophysics computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[1](ch14.html#ch14fn1a). [http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html](http://developer.nvidia.com/GPUGems3/gpugems3_ch31.html)'
  prefs: []
  type: TYPE_NORMAL
- en: In the hopes that readers will be able to “plug in” their computation and find
    the fastest method, this chapter illustrates several different ways to implement
    N-Body and related computations using CUDA.
  prefs: []
  type: TYPE_NORMAL
- en: • A naïve implementation illustrates the technique and underscores the effectiveness
    of caches and the importance of loop unrolling.
  prefs: []
  type: TYPE_NORMAL
- en: • A shared memory implementation (for our gravitational computation, the fastest)
    duplicates Harris et al.’s result, tiling the computation over threadblock-sized
    collections of bodies to minimize memory latencies in the innermost loop.
  prefs: []
  type: TYPE_NORMAL
- en: • A constant memory implementation, inspired by Stone et al.’s implementation
    of Direct Coulomb Summation (DCS),^([2](ch14.html#ch14fn2)) uses constant memory
    to hold body descriptions, freeing shared memory for other uses.
  prefs: []
  type: TYPE_NORMAL
- en: '[2](ch14.html#ch14fn2a). [www.ncbi.nlm.nih.gov/pubmed/17894371](http://www.ncbi.nlm.nih.gov/pubmed/17894371)'
  prefs: []
  type: TYPE_NORMAL
- en: Because readers’ applications may not happen to be gravitational N-Body, these
    different implementations are not presented with the singular goal of optimizing
    that particular computation. It may make sense to adapt a different implementation,
    depending on the target SM architecture, problem size, and details of the central
    calculation.
  prefs: []
  type: TYPE_NORMAL
- en: Since gravitational N-Body has been presented as a poster child for theoretical
    performance of GPUs, with speedups of up to 400x reported, the chapter concludes
    by presenting an implementation optimized for CPUs. By rewriting the calculation
    to use SSE (Streaming SIMD Extensions) and multithreading, a speedup of more than
    300x is obtained. Nevertheless, as reported in [Section 14.9](ch14.html#ch14lev1sec9),
    a GK104-based GPU is significantly faster than a high-end server with a pair of
    Intel Xeon E2670 CPUs. The CUDA implementation is faster, more readable, and more
    maintainable than the optimized CPU implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, performance results are reported using a server-class
    machine with two Xeon E2670 “Sandy Bridge” CPUs and up to four GK104 GPUs that
    are underclocked to conserve power and minimize heat dissipation. Rather than
    reporting results in GFLOPS, we report performance results in terms of body-body
    interactions per second.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1\. Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given *N* bodies with positions **x**[i] and velocities **v**[i] for 1 ≤ *i*
    ≤ *N*, the force vector **f**[ij] on body *i* caused by body *j* is given by
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/423equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *m*[i] and *m*[j] are the masses of bodies *i* and *j*, respectively;
    **d**[ij] is the difference vector from body *i* to body *j*; and *G* is the gravitational
    constant. Due to divide overflow, this express diverges for **d**[ij] with small
    magnitude; to compensate, it is common practice to apply a *softening factor*
    that models the interaction between two Plummer masses—masses that behave as if
    they were spherical galaxies. For a softening factor ε, the resulting expression
    is
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/423equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The total force **F**[i] on body *i*, due to its interactions with the other
    *N* – 1 bodies, is obtained by summing all interactions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/423equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To update the position and velocity of each body, the force (acceleration) applied
    for body *i* is **a**[i] = **F**[i]/*m*[i], so the **m**[i] term can be removed
    from the numerator, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/423equ04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Like Nyland et al., we use a leapfrog Verlet algorithm to apply a timestep to
    the simulation. The values of the positions and velocities are offset by half
    a timestep from one another, a characteristic that is not obvious from the code
    because in our sample, the positions and velocities are initially assigned random
    values. Our leapfrog Verlet integration updates the velocity, then the position.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/424equ01.jpg)![Image](graphics/424equ02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This method is just one of many different integration algorithms that can be
    used to update the simulation, but an extensive discussion is outside the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Since the integration has a runtime of O(*N*) and computing the forces has a
    runtime of *O*(*N*²), the biggest performance benefits from porting to CUDA stem
    from optimizing the force computation. Optimizing this portion of the calculation
    is the primary focus of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 14.1.1\. A Matrix of Forces
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A naïve implementation of the N-Body algorithm consists of a doubly nested loop
    that, for each body, computes the sum of the forces exerted on that body by every
    other body. The O(*N*²) body-body forces can be thought of as an *N*×*N* matrix,
    where the sum of each row *i* is the total gravitational force exerted on body
    *i.*
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/424equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The diagonals of this “matrix” are zeroes corresponding to each body’s influence
    on itself, which can be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: Because each element in the matrix may be computed independently, there is a
    tremendous amount of potential parallelism. The sum of each row is a reduction
    that may be computed by a single thread or by combining results from multiple
    threads as described in [Chapter 12](ch12.html#ch12).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14.1](ch14.html#ch14fig01) shows an 8-body “matrix” being processed.
    The rows correspond to the sums that are the output of the computation. When using
    CUDA, N-Body implementations typically have one thread compute the sum for each
    row.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/14fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14.1* “Matrix” of forces (8 bodies).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since every element in the “matrix” is independent, they also may be computed
    in parallel *within* a given row: Compute the sum of every fourth element, for
    example, and then add the four partial sums to compute the final output. Harris
    et al. describe using this method for small *N* where there are not enough threads
    to cover the latencies in the GPU: Launch more threads, compute partial sums in
    each thread, and accumulate the final sums with reductions in shared memory. Harris
    et al. reported a benefit for *N* ≤ 4096.'
  prefs: []
  type: TYPE_NORMAL
- en: For physical forces that are symmetric (i.e., the force exerted on body *i*
    by body *j* is equal in magnitude but has the opposite sign of the force exerted
    by body *j* on *i*), such as gravitation, the transpose “elements” of the matrix
    have the opposite sign.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/425equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, the “matrix” takes the form shown in [Figure 14.2](ch14.html#ch14fig02).
    When exploiting the symmetry, an implementation need only compute the upper right
    triangle of the “matrix,” performing about half as many body-body computations.^([3](ch14.html#ch14fn3))
  prefs: []
  type: TYPE_NORMAL
- en: '[3](ch14.html#ch14fn3a). ![Image](graphics/425fig02.jpg), to be exact.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/14fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14.2* Matrix with symmetric forces.'
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that unlike the brute force method outlined in [Figure 14.1](ch14.html#ch14fig01),
    when exploiting symmetric forces, different threads may have contributions to
    add to a given output sum. Partial sums must be accumulated and either written
    to temporary locations for eventual reduction or the system must protect the final
    sums with mutual exclusion (by using atomics or thread synchronization). Since
    the body-body computation is about 20 FLOPS (for single precision) or 30 FLOPS
    (for double precision), subtracting from a sum would seem like a decisive performance
    win.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the overhead often overwhelms the benefit of performing half
    as many body-body computations. For example, a completely naïve implementation
    that does two (2) floating-point atomic adds per body-body computation is prohibitively
    slower than the brute force method.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14.3](ch14.html#ch14fig03) shows a compromise between the two extremes:
    By tiling the computation, only the upper right diagonal of *tiles* needs to be
    computed. For a tile size of *k*, this method performs *k*² body-body computations
    each on ![Image](graphics/426fig01.jpg) nondiagonal tiles, plus *k*(*k* –1) body-body
    computations each on *N/k* diagonal tiles. For large *N*, the savings in body-body
    computations are about the same,^([4](ch14.html#ch14fn4)) but because the tiles
    can locally accumulate partial sums to contribute to the final answer, the synchronization
    overhead is reduced. [Figure 14.3](ch14.html#ch14fig03) shows a tile size with
    *k* = 2, but a tile size corresponding to the warp size (*k* = 32) is more practical.'
  prefs: []
  type: TYPE_NORMAL
- en: '[4](ch14.html#ch14fn4a). For example, with *N* = 65536 and *k* = 32, the tiled
    approach performs 51.5% of the body-body computations performed by the brute force
    algorithm, or 3% more than the ideal symmetric algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/14fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14.3* Tiled N-Body (*k* = 2).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 14.4](ch14.html#ch14fig04) shows how the partial sums for a given tile
    are computed. The partial sums for the rows and columns are computed—adding and
    subtracting, respectively—in order to arrive at partial sums that must be added
    to the corresponding output sums.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/14fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 14.4* N-Body tile.'
  prefs: []
  type: TYPE_NORMAL
- en: The popular AMBER application for molecular modeling exploits symmetry of forces,
    performing the work on tiles tuned to the warp size of 32,^([5](ch14.html#ch14fn5))
    but in extensive testing, the approach has not proven fruitful for the more lightweight
    computation described here.
  prefs: []
  type: TYPE_NORMAL
- en: '[5](ch14.html#ch14fn5a). Götz, Andreas, Mark J. Williamson, Dong Xu, Duncan
    Poole, Scott Le Grand, and Ross C. Walker. Routine microsecond molecular dynamics
    simulations with AMBER on GPUs—Part I: Generalized Born, *J. Chem. Theory Comput*.
    8, no. 5 (2012), pp. 1542–1555.'
  prefs: []
  type: TYPE_NORMAL
- en: 14.2\. Naïve Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Listing 14.1](ch14.html#ch14lis01) gives a function that implements the body-body
    interaction described in the previous section; by annotating it with both the
    `__host__` and `__device__` keywords, the CUDA compiler knows it is valid for
    both the CPU and GPU. The function is templated so it may be invoked for both
    `float` and `double` values (though for this book, only `float` is fully implemented).
    It passes back the 3D force vector in the `(fx, fy, fz)` tuple.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 14.1.* `bodyBodyInteraction`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch14_images.html#p14lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template <typename T>
  prefs: []
  type: TYPE_NORMAL
- en: __host__ __device__ void bodyBodyInteraction(
  prefs: []
  type: TYPE_NORMAL
- en: T& ax, T& ay, T& az,
  prefs: []
  type: TYPE_NORMAL
- en: T x0, T y0, T z0,
  prefs: []
  type: TYPE_NORMAL
- en: T x1, T y1, T z1, T mass1,
  prefs: []
  type: TYPE_NORMAL
- en: T softeningSquared)
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: T dx = x1 - x0;
  prefs: []
  type: TYPE_NORMAL
- en: T dy = y1 - y0;
  prefs: []
  type: TYPE_NORMAL
- en: T dz = z1 - z0;
  prefs: []
  type: TYPE_NORMAL
- en: T distSqr = dx*dx + dy*dy + dz*dz;
  prefs: []
  type: TYPE_NORMAL
- en: distSqr += softeningSquared;
  prefs: []
  type: TYPE_NORMAL
- en: T invDist = (T)1.0 / (T)sqrt(distSqr);
  prefs: []
  type: TYPE_NORMAL
- en: T invDistCube =  invDist * invDist * invDist;
  prefs: []
  type: TYPE_NORMAL
- en: T s = mass1 * invDistCube;
  prefs: []
  type: TYPE_NORMAL
- en: ax = dx * s;
  prefs: []
  type: TYPE_NORMAL
- en: ay = dy * s;
  prefs: []
  type: TYPE_NORMAL
- en: az = dz * s;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 14.2](ch14.html#ch14lis02) gives the function that computes the total
    gravitational force exerted on each body. For each body, it loads that body’s
    position into `(myX, myY, myZ)` and then, for every other body, calls `bodyBodyInteraction<float>`
    to compute the force exerted between the two. The “AOS” in the function name denotes
    that the input data comes in the form of an “array of structures”: four packed
    `float` values that give the *(x, y, z, mass)* tuple that specifies a body’s position
    and mass. The `float4` representation is a convenient size for GPU implementation,
    with native hardware support for loads and stores. Our optimized CPU implementations,
    described in [Section 14.9](ch14.html#ch14lev1sec9), use so-called “structure
    of arrays” (SOA) representation where four arrays of float contain packed *x*,
    *y*, *z*, and *mass* elements for easier processing by SIMD instruction sets.
    SOA is not a good fit for GPU implementation because the 4 base pointers needed
    by an SOA representation cost too many registers.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 14.2.* `ComputeGravitation_AOS` (CPU implementation).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch14_images.html#p14lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: ComputeGravitation_AOS(
  prefs: []
  type: TYPE_NORMAL
- en: float *force,
  prefs: []
  type: TYPE_NORMAL
- en: float *posMass,
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerTimestamp start, end;
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &start );
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0; i < N; i++ )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float ax = 0.0f;
  prefs: []
  type: TYPE_NORMAL
- en: float ay = 0.0f;
  prefs: []
  type: TYPE_NORMAL
- en: float az = 0.0f;
  prefs: []
  type: TYPE_NORMAL
- en: float myX = posMass[i*4+0];
  prefs: []
  type: TYPE_NORMAL
- en: float myY = posMass[i*4+1];
  prefs: []
  type: TYPE_NORMAL
- en: float myZ = posMass[i*4+2];
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t j = 0; j < N; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: float acc[3];
  prefs: []
  type: TYPE_NORMAL
- en: float bodyX = posMass[j*4+0];
  prefs: []
  type: TYPE_NORMAL
- en: float bodyY = posMass[j*4+1];
  prefs: []
  type: TYPE_NORMAL
- en: float bodyZ = posMass[j*4+2];
  prefs: []
  type: TYPE_NORMAL
- en: float bodyMass = posMass[j*4+3];
  prefs: []
  type: TYPE_NORMAL
- en: bodyBodyInteraction<float>(
  prefs: []
  type: TYPE_NORMAL
- en: ax, ay, az,
  prefs: []
  type: TYPE_NORMAL
- en: myX, myY, myZ,
  prefs: []
  type: TYPE_NORMAL
- en: bodyX, bodyY, bodyZ, bodyMass,
  prefs: []
  type: TYPE_NORMAL
- en: softeningSquared );
  prefs: []
  type: TYPE_NORMAL
- en: ax += acc[0];
  prefs: []
  type: TYPE_NORMAL
- en: ay += acc[1];
  prefs: []
  type: TYPE_NORMAL
- en: az += acc[2];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+0] = ax;
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+1] = ay;
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+2] = az;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &end );
  prefs: []
  type: TYPE_NORMAL
- en: return (float) chTimerElapsedTime( &start, &end ) * 1000.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 14.3](ch14.html#ch14lis03) gives the GPU equivalent to [Listing 14.2](ch14.html#ch14lis02).
    For each body, it sums the accelerations due to every other body, then writes
    that value out to the force array. The L1 and L2 caches in SM 2.x and later GPUs
    accelerate this workload well, since there is a great deal of reuse in the innermost
    loop.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both the outer loop and the inner loop cast the input array `posMass` to `float4`
    to ensure that the compiler correctly emits a single 16-byte load instruction.
    Loop unrolling is an oft-cited optimization for N-Body calculations on GPUs, and
    it’s not hard to imagine why: Branch overhead is much higher on GPUs than CPUs,
    so the reduced instruction count per loop iteration has a bigger benefit, and
    the unrolled loop exposes more opportunities for ILP (instruction level parallelism),
    in which the GPU covers latency of instruction execution as well as memory latency.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the benefits of loop unrolling in our N-Body application, we need only
    insert the line
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma unroll <factor>'
  prefs: []
  type: TYPE_NORMAL
- en: in front of the `for` loop over `j`. Unfortunately, the optimal loop unrolling
    factor must be determined empirically. [Table 14.1](ch14.html#ch14tab01) summarizes
    the effects of unrolling the loop in this kernel.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/14tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 14.1* Loop Unrolling in the Naïve Kernel'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of this kernel, in the absence of unrolling, it only delivers 25
    billion body-body interactions per second. Even an unroll factor of 2 increases
    this performance to 30 billion; increasing the unroll factor to 16 delivers the
    highest performance observed with this kernel: 34.3 billion body-body interactions
    per second, a 37% performance improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 14.3.* `ComputeNBodyGravitation_GPU_AOS`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch14_images.html#p14lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: template<typename T>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_GPU_AOS(
  prefs: []
  type: TYPE_NORMAL
- en: T *force,
  prefs: []
  type: TYPE_NORMAL
- en: T *posMass,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N,
  prefs: []
  type: TYPE_NORMAL
- en: T softeningSquared )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: T acc[3] = {0};
  prefs: []
  type: TYPE_NORMAL
- en: float4 me = ((float4 *) posMass)[i];
  prefs: []
  type: TYPE_NORMAL
- en: T myX = me.x;
  prefs: []
  type: TYPE_NORMAL
- en: T myY = me.y;
  prefs: []
  type: TYPE_NORMAL
- en: T myZ = me.z;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < N; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: float4 body = ((float4 *) posMass)[j];
  prefs: []
  type: TYPE_NORMAL
- en: float fx, fy, fz;
  prefs: []
  type: TYPE_NORMAL
- en: bodyBodyInteraction(
  prefs: []
  type: TYPE_NORMAL
- en: '&fx, &fy, &fz,'
  prefs: []
  type: TYPE_NORMAL
- en: myX, myY, myZ,
  prefs: []
  type: TYPE_NORMAL
- en: body.x, body.y, body.z, body.w,
  prefs: []
  type: TYPE_NORMAL
- en: softeningSquared);
  prefs: []
  type: TYPE_NORMAL
- en: acc[0] += fx;
  prefs: []
  type: TYPE_NORMAL
- en: acc[1] += fy;
  prefs: []
  type: TYPE_NORMAL
- en: acc[2] += fz;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+0] = acc[0];
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+1] = acc[1];
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+2] = acc[2];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 14.3\. Shared Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There is enough locality and reuse in the innermost loop of the N-Body calculation
    that caches work well without any involvement from the programmer; but on CUDA
    architectures, there is a benefit to using shared memory to explicitly cache the
    data^([6](ch14.html#ch14fn6)), as shown in [Listing 14-4](ch14.html#ch14lis04).
    The inner loop is *tiled* using two loops: an outer one that strides through the
    *N* bodies, a thread block at a time, loading shared memory, and an inner one
    that iterates through the body descriptions in shared memory. Shared memory always
    has been optimized to broadcast to threads within a warp if they are reading the
    same shared memory location, so this usage pattern is a good fit with the hardware
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '[6](ch14.html#ch14fn6a). Shared memory is a must on SM 1.x architectures, which
    did not include caches. But it turns out to be a win on all CUDA architectures,
    albeit a slight one on SM 2.x and SM 3.x.'
  prefs: []
  type: TYPE_NORMAL
- en: This approach is the same one reported by Harris et al. that achieved the highest
    performance for large *N* and that approached the theoretical limits of the GPU’s
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 14.4.* `ComputeNBodyGravitation_Shared`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch14_images.html#p14lis04a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_Shared(
  prefs: []
  type: TYPE_NORMAL
- en: float *force,
  prefs: []
  type: TYPE_NORMAL
- en: float *posMass,
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float4 *posMass4 = posMass;
  prefs: []
  type: TYPE_NORMAL
- en: extern __shared__ float4 shPosMass[];
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float acc[3] = {0};
  prefs: []
  type: TYPE_NORMAL
- en: float4 myPosMass = posMass4[i];
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma unroll 32'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < N; j += blockDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: shPosMass[threadIdx.x] = posMass4[j+threadIdx.x];
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t k = 0; k < blockDim.x; k++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: float fx, fy, fz;
  prefs: []
  type: TYPE_NORMAL
- en: float4 bodyPosMass = shPosMass[k];
  prefs: []
  type: TYPE_NORMAL
- en: bodyBodyInteraction(
  prefs: []
  type: TYPE_NORMAL
- en: '&fx, &fy, &fz,'
  prefs: []
  type: TYPE_NORMAL
- en: myPosMass.x, myPosMass.y, myPosMass.z,
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.x,
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.y,
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.z,
  prefs: []
  type: TYPE_NORMAL
- en: bodyPosMass.w,
  prefs: []
  type: TYPE_NORMAL
- en: softeningSquared );
  prefs: []
  type: TYPE_NORMAL
- en: acc[0] += fx;
  prefs: []
  type: TYPE_NORMAL
- en: acc[1] += fy;
  prefs: []
  type: TYPE_NORMAL
- en: acc[2] += fz;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+0] = acc[0];
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+1] = acc[1];
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+2] = acc[2];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As with the previous kernel, loop unrolling delivers higher performance. [Table
    14.2](ch14.html#ch14tab02) summarizes the effects of loop unrolling in the shared
    memory implementation. The optimal unroll factor of 4 delivers 18% higher performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/14tab02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 14.2* Loop Unrolling in the Shared Memory Kernel'
  prefs: []
  type: TYPE_NORMAL
- en: 14.4\. Constant Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stone et al. describe a method of Direct Coulomb Summation (DCS) that uses shared
    memory to hold potential map lattice points for a molecular modeling application^([7](ch14.html#ch14fn7))
    so it must use constant memory to hold body descriptions. [Listing 14.5](ch14.html#ch14lis05)
    shows a CUDA kernel that uses the same method for our gravitational simulation.
    Since only 64K of constant memory is available to developers for a given kernel,
    each kernel invocation can only process about 4000 16-byte body descriptions.
    The constant `g_bodiesPerPass` specifies the number of bodies that can be considered
    by the innermost loop.
  prefs: []
  type: TYPE_NORMAL
- en: '[7](ch14.html#ch14fn7a). [www.ncbi.nlm.nih.gov/pubmed/17894371](http://www.ncbi.nlm.nih.gov/pubmed/17894371)'
  prefs: []
  type: TYPE_NORMAL
- en: Since every thread in the innermost loop is reading the same body description,
    constant memory works well because it is optimized to broadcast reads to all threads
    in a warp.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 14.5.* N-Body (constant memory).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch14_images.html#p14lis05a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: const int g_bodiesPerPass = 4000;
  prefs: []
  type: TYPE_NORMAL
- en: __constant__ __device__ float4 g_constantBodies[g_bodiesPerPass];
  prefs: []
  type: TYPE_NORMAL
- en: template<typename T>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_GPU_AOS_const(
  prefs: []
  type: TYPE_NORMAL
- en: T *force,
  prefs: []
  type: TYPE_NORMAL
- en: T *posMass,
  prefs: []
  type: TYPE_NORMAL
- en: T softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: size_t n,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: T acc[3] = {0};
  prefs: []
  type: TYPE_NORMAL
- en: float4 me = ((float4 *) posMass)[i];
  prefs: []
  type: TYPE_NORMAL
- en: T myX = me.x;
  prefs: []
  type: TYPE_NORMAL
- en: T myY = me.y;
  prefs: []
  type: TYPE_NORMAL
- en: T myZ = me.z;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < n; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: float4 body = g_constantBodies[j];
  prefs: []
  type: TYPE_NORMAL
- en: float fx, fy, fz;
  prefs: []
  type: TYPE_NORMAL
- en: bodyBodyInteraction(
  prefs: []
  type: TYPE_NORMAL
- en: '&fx, &fy, &fz,'
  prefs: []
  type: TYPE_NORMAL
- en: myX, myY, myZ,
  prefs: []
  type: TYPE_NORMAL
- en: body.x, body.y, body.z, body.w,
  prefs: []
  type: TYPE_NORMAL
- en: softeningSquared);
  prefs: []
  type: TYPE_NORMAL
- en: acc[0] += fx;
  prefs: []
  type: TYPE_NORMAL
- en: acc[1] += fy;
  prefs: []
  type: TYPE_NORMAL
- en: acc[2] += fz;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+0] += acc[0];
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+1] += acc[1];
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+2] += acc[2];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Listing 14.6](ch14.html#ch14lis06), the host code must loop over
    the bodies, calling `cudaMemcpyToSymbolAsync()` to load the constant memory before
    each kernel invocation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 14.6.* Host code (constant memory N-Body).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch14_images.html#p14lis06a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_GPU_AOS_const(
  prefs: []
  type: TYPE_NORMAL
- en: float *force,
  prefs: []
  type: TYPE_NORMAL
- en: float *posMass,
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: cudaEvent_t evStart = 0, evStop = 0;
  prefs: []
  type: TYPE_NORMAL
- en: float ms = 0.0;
  prefs: []
  type: TYPE_NORMAL
- en: size_t bodiesLeft = N;
  prefs: []
  type: TYPE_NORMAL
- en: void *p;
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaGetSymbolAddress( &p, g_constantBodies ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventCreate( &evStart ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventCreate( &evStop ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( evStart, NULL ) );
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0; i < N; i += g_bodiesPerPass ) {
  prefs: []
  type: TYPE_NORMAL
- en: // bodiesThisPass = max(bodiesLeft, g_bodiesPerPass);
  prefs: []
  type: TYPE_NORMAL
- en: size_t bodiesThisPass = bodiesLeft;
  prefs: []
  type: TYPE_NORMAL
- en: if ( bodiesThisPass > g_bodiesPerPass ) {
  prefs: []
  type: TYPE_NORMAL
- en: bodiesThisPass = g_bodiesPerPass;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaMemcpyToSymbolAsync(
  prefs: []
  type: TYPE_NORMAL
- en: g_constantBodies,
  prefs: []
  type: TYPE_NORMAL
- en: ((float4 *) posMass)+i,
  prefs: []
  type: TYPE_NORMAL
- en: bodiesThisPass*sizeof(float4),
  prefs: []
  type: TYPE_NORMAL
- en: 0,
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToDevice,
  prefs: []
  type: TYPE_NORMAL
- en: NULL ) );
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_GPU_AOS_const<float> <<<300,256>>>(
  prefs: []
  type: TYPE_NORMAL
- en: force, posMass, softeningSquared, bodiesThisPass, N );
  prefs: []
  type: TYPE_NORMAL
- en: bodiesLeft -= bodiesThisPass;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventRecord( evStop, NULL ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaDeviceSynchronize() );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaEventElapsedTime( &ms, evStart, evStop ) );
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventDestroy( evStop );
  prefs: []
  type: TYPE_NORMAL
- en: cudaEventDestroy( evStart );
  prefs: []
  type: TYPE_NORMAL
- en: return ms;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 14.5\. Warp Shuffle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SM 3.x added a warp shuffle instruction (described in [Section 8.6.1](ch08.html#ch08lev2sec20))
    that enables threads to interchange data between registers without writing the
    data to shared memory. The `__shfl()` intrinsic can be used to broadcast one thread’s
    register value to all other threads in the warp. As shown in [Listing 14.4](ch14.html#ch14lis04),
    instead of using tiles sized to the threadblock and using shared memory, we can
    use tiles of size 32 (corresponding to the warp size) and broadcast the body description
    read by each thread to the other threads within the warp.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, this strategy has 25% lower performance than the shared memory
    implementation (34 billion as opposed to 45.2 billion interactions per second).
    The warp shuffle instruction takes about as long as a read from shared memory,
    and the computation is tiled at the warp size (32 threads) rather than a thread
    block size. So it seems the benefits of warp shuffle are best realized when replacing
    both a write and a read to shared memory, not just a read. Warp shuffle should
    only be used if the kernel needs shared memory for other purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 14.7.* `ComputeNBodyGravitation_Shuffle`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch14_images.html#p14lis07a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: ComputeNBodyGravitation_Shuffle(
  prefs: []
  type: TYPE_NORMAL
- en: float *force,
  prefs: []
  type: TYPE_NORMAL
- en: float *posMass,
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: const int laneid = threadIdx.x & 31;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: i < N;
  prefs: []
  type: TYPE_NORMAL
- en: i += blockDim.x*gridDim.x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float acc[3] = {0};
  prefs: []
  type: TYPE_NORMAL
- en: float4 myPosMass = ((float4 *) posMass)[i];
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < N; j += 32 ) {
  prefs: []
  type: TYPE_NORMAL
- en: float4 shufSrcPosMass = ((float4 *) posMass)[j+laneid];
  prefs: []
  type: TYPE_NORMAL
- en: '#pragma unroll 32'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int k = 0; k < 32; k++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: float fx, fy, fz;
  prefs: []
  type: TYPE_NORMAL
- en: float4 shufDstPosMass;
  prefs: []
  type: TYPE_NORMAL
- en: shufDstPosMass.x = __shfl( shufSrcPosMass.x, k );
  prefs: []
  type: TYPE_NORMAL
- en: shufDstPosMass.y = __shfl( shufSrcPosMass.y, k );
  prefs: []
  type: TYPE_NORMAL
- en: shufDstPosMass.z = __shfl( shufSrcPosMass.z, k );
  prefs: []
  type: TYPE_NORMAL
- en: shufDstPosMass.w = __shfl( shufSrcPosMass.w, k );
  prefs: []
  type: TYPE_NORMAL
- en: bodyBodyInteraction(
  prefs: []
  type: TYPE_NORMAL
- en: '&fx, &fy, &fz,'
  prefs: []
  type: TYPE_NORMAL
- en: myPosMass.x, myPosMass.y, myPosMass.z,
  prefs: []
  type: TYPE_NORMAL
- en: shufDstPosMass.x,
  prefs: []
  type: TYPE_NORMAL
- en: shufDstPosMass.y,
  prefs: []
  type: TYPE_NORMAL
- en: shufDstPosMass.z,
  prefs: []
  type: TYPE_NORMAL
- en: shufDstPosMass.w,
  prefs: []
  type: TYPE_NORMAL
- en: softeningSquared);
  prefs: []
  type: TYPE_NORMAL
- en: acc[0] += fx;
  prefs: []
  type: TYPE_NORMAL
- en: acc[1] += fy;
  prefs: []
  type: TYPE_NORMAL
- en: acc[2] += fz;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+0] = acc[0];
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+1] = acc[1];
  prefs: []
  type: TYPE_NORMAL
- en: force[3*i+2] = acc[2];
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 14.6\. Multiple GPUs and Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because the computational density is so high, N-Body scales well across multiple
    GPUs. Portable pinned memory is used to hold the body descriptions so they can
    easily be referenced by all GPUs in the system. For a system containing *k* GPUs,
    each GPU is assigned *N*/*k* forces to compute.^([8](ch14.html#ch14fn8)) Our multi-GPU
    implementation of N-Body is featured in [Chapter 9](ch09.html#ch09). The rows
    are evenly divided among GPUs, the input data is broadcast to all GPUs via portable
    pinned memory, and each GPU computes its output independently. CUDA applications
    that use multiple GPUs can be multithreaded or single-threaded. [Chapter 9](ch09.html#ch09)
    includes optimized N-Body implementations that illustrate both approaches.
  prefs: []
  type: TYPE_NORMAL
- en: '[8](ch14.html#ch14fn8a). Our implementation requires that *N* be evenly divisible
    by *k*.'
  prefs: []
  type: TYPE_NORMAL
- en: For N-Body, the single- and multithreaded implementations have the same performance,
    since there is little work for the CPU to do. [Table 14.3](ch14.html#ch14tab03)
    summarizes the scalability of the multithreaded implementation for a problem size
    of 96K bodies and up to 4 GPUs. The efficiency is the percentage of measured performance
    as compared to perfect scaling. There is room for improvement over this result,
    since the performance results reported here include allocation and freeing of
    device memory on each GPU for each timestep.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/14tab03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 14.3* N-Body Scalability'
  prefs: []
  type: TYPE_NORMAL
- en: 14.7\. CPU Optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Papers on CUDA ports often compare against CPU implementations that are not
    optimized for highest performance. Although CUDA hardware generally is faster
    than CPUs at the workloads described in these papers, the reported speedup is
    often higher than it would be if the CPU implementation had been optimized properly.
  prefs: []
  type: TYPE_NORMAL
- en: To gain some insight into the tradeoffs between CUDA and modern CPU optimizations,
    we optimized the N-Body computation using two key strategies that are necessary
    for multicore CPUs to achieve peak performance.
  prefs: []
  type: TYPE_NORMAL
- en: • SIMD (“single instruction multiple data”) instructions can perform multiple
    single-precision floating-point operations in a single instruction.
  prefs: []
  type: TYPE_NORMAL
- en: • Multithreading achieves near-linear speedups in the number of execution cores
    available in the CPU. Multicore CPUs have been widely available since 2006, and
    N-Body computations are expected to scale almost linearly in the number of cores.
  prefs: []
  type: TYPE_NORMAL
- en: Since N-Body computations have such high computational density, we will not
    concern ourselves with affinity (for example, trying to use NUMA APIs to associate
    memory buffers with certain CPUs). There is so much reuse in this computation
    that caches in the CPU keep external memory traffic to a trickle.
  prefs: []
  type: TYPE_NORMAL
- en: The Streaming SIMD Extensions (SSE) instructions were added to Intel’s x86 architecture
    in the late 1990s, starting with the Pentium III. They added a set of eight 128-bit
    XMM registers that could operate on four packed 32-bit floating-point values.^([9](ch14.html#ch14fn9))
    For example, the ADDPS instruction performs four floating-point additions in parallel
    on corresponding packed floats in XMM registers.
  prefs: []
  type: TYPE_NORMAL
- en: '[9](ch14.html#ch14fn9a). Intel later added instructions that could consider
    the XMM registers as packed integers (up to 16 bytes) or two packed double-precision
    floating-point values, but we do not use any of those features. We also do not
    use the AVX (“Advanced Vector Extensions”) instruction set. AVX features registers
    and instructions that support SIMD operations that are twice as wide (256-bit),
    so it potentially could double performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When porting N-Body to the SSE instruction set, the AOS (array of structures)
    memory layout that we have been using becomes problematic. Although the body descriptions
    are 16 bytes, just like XMM registers, the instruction set requires us to rearrange
    the data such that the X, Y, Z, and Mass components are packed into separate registers.
    Rather than perform this operation when computing the body-body interactions,
    we rearrange the memory layout as structure of arrays: Instead of a single array
    of `float4` (each element being the X, Y, Z, and Mass values for a given body),
    we use four arrays of `float`, with an array of X values, an array of Y values,
    and so on. With the data rearranged in this way, four bodies’ descriptions can
    be loaded into XMM registers with just 4 machine instructions; the difference
    vectors between four bodies’ positions can be computed with just 3 `SUBPS` instructions;
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: To simplify SSE coding, Intel has worked with compiler vendors to add cross-platform
    support for the SSE instruction set. A special data type `__m128` corresponds
    to the 128-bit register and operand size and intrinsic functions such as `_mm_sub_ps()`
    that correspond to the SUBPS instruction.
  prefs: []
  type: TYPE_NORMAL
- en: For purposes of our N-Body implementation, we also need a full-precision reciprocal
    square root implementation. The SSE instruction set has an instruction RSQRTPS
    that computes an approximation of the reciprocal square root, but its 12-bit estimate
    must be refined by a Newton-Raphson iteration to achieve full float precision.^([10](ch14.html#ch14fn10))
  prefs: []
  type: TYPE_NORMAL
- en: '[10](ch14.html#ch14fn10a). This code is not present in the SSE compiler support
    and is surprisingly difficult to find. Our implementation is from [http://nume.googlecode.com/svn/trunk/fosh/src/sse_approx.h](http://nume.googlecode.com/svn/trunk/fosh/src/sse_approx.h).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/440equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Listing 14.8](ch14.html#ch14lis08) gives an SSE implementation of the body-body
    computation that takes the 2 bodies’ descriptions as `__m128` variables, computes
    the 4 body-body forces in parallel, and passes back the 3 resulting force vectors.
    [Listing 14.8](ch14.html#ch14lis08) is functionally equivalent to [Listings 14.1](ch14.html#ch14lis01)
    and [14.2](ch14.html#ch14lis02), though markedly less readable. Note that the
    `x0`, `y0`, and `z0` variables contain descriptions of the same body, replicated
    across the `__m128` variable four times.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 14.8.* Body-body interaction (SSE version).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch14_images.html#p14lis08a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: static inline __m128
  prefs: []
  type: TYPE_NORMAL
- en: rcp_sqrt_nr_ps(const __m128 x)
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: const __m128
  prefs: []
  type: TYPE_NORMAL
- en: nr      = _mm_rsqrt_ps(x),
  prefs: []
  type: TYPE_NORMAL
- en: muls    = _mm_mul_ps(_mm_mul_ps(nr, nr), x),
  prefs: []
  type: TYPE_NORMAL
- en: beta    = _mm_mul_ps(_mm_set_ps1(0.5f), nr),
  prefs: []
  type: TYPE_NORMAL
- en: gamma   = _mm_sub_ps(_mm_set_ps1(3.0f), muls);
  prefs: []
  type: TYPE_NORMAL
- en: return _mm_mul_ps(beta, gamma);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: static inline __m128
  prefs: []
  type: TYPE_NORMAL
- en: horizontal_sum_ps( const __m128 x )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: const __m128 t = _mm_add_ps(x, _mm_movehl_ps(x, x));
  prefs: []
  type: TYPE_NORMAL
- en: return _mm_add_ss(t, _mm_shuffle_ps(t, t, 1));
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: inline void
  prefs: []
  type: TYPE_NORMAL
- en: bodyBodyInteraction(
  prefs: []
  type: TYPE_NORMAL
- en: __m128& f0,
  prefs: []
  type: TYPE_NORMAL
- en: __m128& f1,
  prefs: []
  type: TYPE_NORMAL
- en: __m128& f2,
  prefs: []
  type: TYPE_NORMAL
- en: const __m128& x0,
  prefs: []
  type: TYPE_NORMAL
- en: const __m128& y0,
  prefs: []
  type: TYPE_NORMAL
- en: const __m128& z0,
  prefs: []
  type: TYPE_NORMAL
- en: const __m128& x1,
  prefs: []
  type: TYPE_NORMAL
- en: const __m128& y1,
  prefs: []
  type: TYPE_NORMAL
- en: const __m128& z1,
  prefs: []
  type: TYPE_NORMAL
- en: const __m128& mass1,
  prefs: []
  type: TYPE_NORMAL
- en: const __m128& softeningSquared )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: __m128 dx = _mm_sub_ps( x1, x0 );
  prefs: []
  type: TYPE_NORMAL
- en: __m128 dy = _mm_sub_ps( y1, y0 );
  prefs: []
  type: TYPE_NORMAL
- en: __m128 dz = _mm_sub_ps( z1, z0 );
  prefs: []
  type: TYPE_NORMAL
- en: __m128 distSq =
  prefs: []
  type: TYPE_NORMAL
- en: _mm_add_ps(
  prefs: []
  type: TYPE_NORMAL
- en: _mm_add_ps(
  prefs: []
  type: TYPE_NORMAL
- en: _mm_mul_ps( dx, dx ),
  prefs: []
  type: TYPE_NORMAL
- en: _mm_mul_ps( dy, dy )
  prefs: []
  type: TYPE_NORMAL
- en: ),
  prefs: []
  type: TYPE_NORMAL
- en: _mm_mul_ps( dz, dz )
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: distSq = _mm_add_ps( distSq, softeningSquared );
  prefs: []
  type: TYPE_NORMAL
- en: __m128 invDist = rcp_sqrt_nr_ps( distSq );
  prefs: []
  type: TYPE_NORMAL
- en: __m128 invDistCube =
  prefs: []
  type: TYPE_NORMAL
- en: _mm_mul_ps(
  prefs: []
  type: TYPE_NORMAL
- en: invDist,
  prefs: []
  type: TYPE_NORMAL
- en: _mm_mul_ps(
  prefs: []
  type: TYPE_NORMAL
- en: invDist, invDist )
  prefs: []
  type: TYPE_NORMAL
- en: );
  prefs: []
  type: TYPE_NORMAL
- en: __m128 s = _mm_mul_ps( mass1, invDistCube );
  prefs: []
  type: TYPE_NORMAL
- en: f0 = _mm_add_ps( a0, _mm_mul_ps( dx, s ) );
  prefs: []
  type: TYPE_NORMAL
- en: f1 = _mm_add_ps( a1, _mm_mul_ps( dy, s ) );
  prefs: []
  type: TYPE_NORMAL
- en: f2 = _mm_add_ps( a2, _mm_mul_ps( dz, s ) );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: To take advantage of multiple cores, we must spawn multiple threads and have
    each thread perform part of the computation. The same strategy is used for multiple
    CPU cores as for multiple GPUs:^([11](ch14.html#ch14fn11)) Just evenly divide
    the output rows among threads (one per CPU core) and, for each timestep, have
    the “parent” thread signal the worker threads to perform their work and then wait
    for them to finish. Since thread creation can be expensive and can fail, our application
    creates a pool of CPU threads at initialization time and uses thread synchronization
    to make the worker threads wait for work and signal completion.
  prefs: []
  type: TYPE_NORMAL
- en: '[11](ch14.html#ch14fn11a). In fact, we used the same platform-independent threading
    library to implement the multithreaded multi-GPU support in [Chapter 9](ch09.html#ch09).'
  prefs: []
  type: TYPE_NORMAL
- en: The portable CUDA handbook threading library, described in [Section A.2](app01.html#app01lev1sec2),
    implements a function `processorCount()` that returns the number of CPU cores
    on the system and a C++ class `workerThread` with methods to create and destroy
    CPU threads and delegate work synchronously or asynchronously. After delegating
    asynchronous work with the `delegateAsynchronous()` member function, the static
    function `waitAll()` is used to wait until the worker threads are finished.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 14.9](ch14.html#ch14lis09) gives the code that dispatches the N-Body
    calculation to worker CPU threads. The `sseDelegation` structures are used to
    communicate the delegation to each worker CPU thread; the `delegateSynchronous`
    function takes a pointer-to-function to execute and a `void *` that will be passed
    to that function (in this case, the `void *` points to the corresponding CPU thread’s
    `sseDelegation` structure).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 14.9.* Multithreaded SSE (master thread code).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch14_images.html#p14lis09a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: ComputeGravitation_SSE_threaded(
  prefs: []
  type: TYPE_NORMAL
- en: float *force[3],
  prefs: []
  type: TYPE_NORMAL
- en: float *pos[4],
  prefs: []
  type: TYPE_NORMAL
- en: float *mass,
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared,
  prefs: []
  type: TYPE_NORMAL
- en: size_t N
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerTimestamp start, end;
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &start );
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: sseDelegation *psse = new sseDelegation[g_numCPUCores];
  prefs: []
  type: TYPE_NORMAL
- en: size_t bodiesPerCore = N / g_numCPUCores;
  prefs: []
  type: TYPE_NORMAL
- en: if ( N % g_numCPUCores ) {
  prefs: []
  type: TYPE_NORMAL
- en: return 0.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: for ( size_t i = 0; i < g_numCPUCores; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].hostPosSOA[0] = pos[0];
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].hostPosSOA[1] = pos[1];
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].hostPosSOA[2] = pos[2];
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].hostMassSOA = mass;
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].hostForceSOA[0] = force[0];
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].hostForceSOA[1] = force[1];
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].hostForceSOA[2] = force[2];
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].softeningSquared = softeningSquared;
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].i = bodiesPerCore*i;
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].n = bodiesPerCore;
  prefs: []
  type: TYPE_NORMAL
- en: psse[i].N = N;
  prefs: []
  type: TYPE_NORMAL
- en: g_CPUThreadPool[i].delegateAsynchronous(
  prefs: []
  type: TYPE_NORMAL
- en: sseWorkerThread,
  prefs: []
  type: TYPE_NORMAL
- en: '&psse[i] );'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: workerThread::waitAll( g_CPUThreadPool, g_numCPUCores );
  prefs: []
  type: TYPE_NORMAL
- en: delete[] psse;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: chTimerGetTime( &end );
  prefs: []
  type: TYPE_NORMAL
- en: return (float) chTimerElapsedTime( &start, &end ) * 1000.0f;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, [Listing 14.10](ch14.html#ch14lis10) gives the `sseDelegation` structure
    and the delegation function invoked by `ComputeGravitation_SSE_threaded` in [Listing
    14.9](ch14.html#ch14lis09). It performs the body-body calculations four at a time,
    accumulating four partial sums that are added together with `horizontal_sum_ps()`
    before storing the final output forces. This function, along with all the functions
    that it calls, uses the SOA memory layout for all inputs and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 14.10.* sseWorkerThread.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch14_images.html#p14lis10a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: struct sseDelegation {
  prefs: []
  type: TYPE_NORMAL
- en: size_t i;   // base offset for this thread to process
  prefs: []
  type: TYPE_NORMAL
- en: size_t n;   // size of this thread's problem
  prefs: []
  type: TYPE_NORMAL
- en: size_t N;   // total number of bodies
  prefs: []
  type: TYPE_NORMAL
- en: float *hostPosSOA[3];
  prefs: []
  type: TYPE_NORMAL
- en: float *hostMassSOA;
  prefs: []
  type: TYPE_NORMAL
- en: float *hostForceSOA[3];
  prefs: []
  type: TYPE_NORMAL
- en: float softeningSquared;
  prefs: []
  type: TYPE_NORMAL
- en: '};'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: sseWorkerThread( void *_p )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: sseDelegation *p = (sseDelegation *) _p;
  prefs: []
  type: TYPE_NORMAL
- en: for (int k = 0; k < p->n; k++)
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int i = p->i + k;
  prefs: []
  type: TYPE_NORMAL
- en: __m128 ax = _mm_setzero_ps();
  prefs: []
  type: TYPE_NORMAL
- en: __m128 ay = _mm_setzero_ps();
  prefs: []
  type: TYPE_NORMAL
- en: __m128 az = _mm_setzero_ps();
  prefs: []
  type: TYPE_NORMAL
- en: __m128 *px = (__m128 *) p->hostPosSOA[0];
  prefs: []
  type: TYPE_NORMAL
- en: __m128 *py = (__m128 *) p->hostPosSOA[1];
  prefs: []
  type: TYPE_NORMAL
- en: __m128 *pz = (__m128 *) p->hostPosSOA[2];
  prefs: []
  type: TYPE_NORMAL
- en: __m128 *pmass = (__m128 *) p->hostMassSOA;
  prefs: []
  type: TYPE_NORMAL
- en: __m128 x0 = _mm_set_ps1( p->hostPosSOA[0][i] );
  prefs: []
  type: TYPE_NORMAL
- en: __m128 y0 = _mm_set_ps1( p->hostPosSOA[1][i] );
  prefs: []
  type: TYPE_NORMAL
- en: __m128 z0 = _mm_set_ps1( p->hostPosSOA[2][i] );
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < p->N/4; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: bodyBodyInteraction(
  prefs: []
  type: TYPE_NORMAL
- en: ax, ay, az,
  prefs: []
  type: TYPE_NORMAL
- en: x0, y0, z0,
  prefs: []
  type: TYPE_NORMAL
- en: px[j], py[j], pz[j], pmass[j],
  prefs: []
  type: TYPE_NORMAL
- en: _mm_set_ps1( p->softeningSquared ) );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: // Accumulate sum of four floats in the SSE register
  prefs: []
  type: TYPE_NORMAL
- en: ax = horizontal_sum_ps( ax );
  prefs: []
  type: TYPE_NORMAL
- en: ay = horizontal_sum_ps( ay );
  prefs: []
  type: TYPE_NORMAL
- en: az = horizontal_sum_ps( az );
  prefs: []
  type: TYPE_NORMAL
- en: _mm_store_ss( (float *) &p->hostForceSOA[0][i], ax );
  prefs: []
  type: TYPE_NORMAL
- en: _mm_store_ss( (float *) &p->hostForceSOA[1][i], ay );
  prefs: []
  type: TYPE_NORMAL
- en: _mm_store_ss( (float *) &p->hostForceSOA[2][i], az );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 14.8\. Conclusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since instruction sets and architectures differ, performance is measured in
    body-body interactions per second rather than GFLOPS. Performance was measured
    on a dual-socket Sandy Bridge system with two E5-2670 CPUs (similar to Amazon’s
    `cc2.8xlarge` instance type), 64GB of RAM, and four (4) GK104 GPUs clocked at
    about 800MHz. The GK104s are on two dual-GPU boards plugged into 16-lane PCI Express
    3.0 slots.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 14.4](ch14.html#ch14tab04) summarizes the speedups due to CPU optimizations.
    All measurements were performed on a server with dual Xeon E2670 CPUs (2.6GHz).
    On this system, the generic CPU code in [Listing 14.2](ch14.html#ch14lis02) performs
    17.2M interactions per second; the single-threaded SSE code performs 307M interactions
    per second, some 17.8x faster! As expected, multithreading the SSE code achieves
    good speedups, with 32 CPU threads delivering 5650M interactions per second, about
    18x as fast as one thread. Between porting to SSE and multithreading, the total
    speedup on this platform for CPUs is more than 300x.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/14tab04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 14.4* Speedups Due to CPU Optimizations'
  prefs: []
  type: TYPE_NORMAL
- en: Because we got such a huge performance improvement from our CPU optimizations,
    the performance comparisons aren’t as pronounced in favor of GPUs as most.^([12](ch14.html#ch14fn12))
    The highest-performing kernel in our testing (the shared memory implementation
    in [Listing 14.4](ch14.html#ch14lis04), with a loop unroll factor of 4) delivered
    45.2 billion body-body interactions per second, exactly 8x faster than the fastest
    multithreaded SSE implementation. This result understates the performance advantages
    of CUDA in some ways, since the server used for testing had two high-end CPUs,
    and the GPUs are derated to reduce power consumption and heat dissipation.
  prefs: []
  type: TYPE_NORMAL
- en: '[12](ch14.html#ch14fn12a). In fairness, that would be true of many other workloads
    in this book, like the SAXPY implementation in [Chapter 11](ch11.html#ch11) and
    the normalized cross-correlation implementation in [Chapter 15](ch15.html#ch15).
    Porting those workloads to multithreaded SIMD would proffer similar tradeoffs
    in performance versus engineering investment, readability, and maintainability
    as compared to the CUDA version.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, future improvements can be had for both technologies: For CPUs,
    porting this workload to the AVX (“Advanced Vector eXtensions”) instruction set
    would potentially double performance, but it would run only on Sandy Bridge and
    later chips, and the optimized CPU implementation does not exploit symmetry. For
    GPUs, NVIDIA’s GK110 is about twice as big (and presumably about twice as fast)
    as the GK104\. Comparing the source code for [Listings 14.1](ch14.html#ch14lis01)
    and [14.9](ch14.html#ch14lis09) (the GPU and SSE implementations of the core body-body
    interaction code), though, it becomes clear that performance isn’t the only reason
    to favor CUDA over optimizing CPU code. Dr. Vincent Natoli alluded to this tradeoff
    in his June 2010 article “Kudos for CUDA.”^([13](ch14.html#ch14fn13))'
  prefs: []
  type: TYPE_NORMAL
- en: '[13](ch14.html#ch14fn13a). [www.hpcwire.com/hpcwire/2010-07-06/kudos_for_cuda.html](http://www.hpcwire.com/hpcwire/2010-07-06/kudos_for_cuda.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we have found in many cases that the expression of algorithmic parallelism
    in CUDA in fields as diverse as oil and gas, bioinformatics, and finance is more
    elegant, compact, and readable than equivalently optimized CPU code, preserving
    and more clearly presenting the underlying algorithm. In a recent project we reduced
    3500 lines of highly optimized C code to a CUDA kernel of about 800 lines. The
    optimized C was peppered with inline assembly, SSE macros, unrolled loops, and
    special cases, making it difficult to read, extract algorithmic meaning, and extend
    in the future. By comparison, the CUDA code was cleaner and more readable. Ultimately
    it will be easier to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: Although it was feasible to develop an SSE implementation of this application,
    with a core body-body computation that takes about 50 lines of code to express
    ([Listing 14.8](ch14.html#ch14lis08)), it’s hard to imagine what the source code
    would look like for an SSE-optimized implementation of something like Boids, where
    each body must evaluate conditions and, when running on CUDA hardware, the code
    winds up being divergent. SSE supports divergence both in the form of predication
    (using masks and Boolean instruction sequences such as `ANDPS/ANDNOTPS/ORPS` to
    construct the result) and branching (often using `MOVMSKPS` to extract evaluated
    conditions), but getting the theoretical speedups on such workloads would require
    large engineering investments unless they can be extracted automatically by a
    vectorizing compiler.
  prefs: []
  type: TYPE_NORMAL
- en: 14.9\. References and Further Reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: N-Body and algorithms with similarly high computational density are a source
    of many high-profile speedups, since they can approach the theoretical limits
    of the GPU’s computing capabilities. The following are just a sample of the numerous
    papers on compute-intensive methods such as N-Body.
  prefs: []
  type: TYPE_NORMAL
- en: '*Gravitational Simulation*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Burtscher, Martin, and Keshav Pingali. An efficient CUDA implementation of the
    tree-based Barnes-Hut n-body algorithm. In *GPU Gems Emerald Edition*, Wen-Mei
    Hwu, ed., Morgan-Kaufmann, 2011, Burlington, MA, pp. 75–92.
  prefs: []
  type: TYPE_NORMAL
- en: '[http://cs.txstate.edu/~burtscher/papers/gcg11.pdf](http://cs.txstate.edu/~burtscher/papers/gcg11.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Harris, Mark, Lars Nyland, and Jan Prins. Fast n-body simulation with CUDA.
    In *GPU Gems 3*, Addison-Wesley, Boston, MA, 2007, pp. 677–695.
  prefs: []
  type: TYPE_NORMAL
- en: '[http.developer.nvidia.com/GPUGems3/gpugems3_ch31.html](http://developer.nvidia.com/GPUGems3/gpugems3_ch31.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Molecular Modeling*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Götz, Andreas, Mark J. Williamson, Dong Xu, Duncan Poole, Scott Le Grand, and
    Ross C. Walker. Routine microsecond molecular dynamics simulations with AMBER
    on GPUs—Part I: Generalized Born, *J. Chem. Theory Comput*. 8 (5), 2012, pp. 1542–1555.'
  prefs: []
  type: TYPE_NORMAL
- en: Hwu, Wen-Mei, and David Kirk. *Programming Massively Parallel Processors*. Morgan-Kaufmann,
    2010, pp. 173–188.
  prefs: []
  type: TYPE_NORMAL
- en: Hardy, David J., John E. Stone, Kirby L. Vandivort, David Gohara, Christopher
    Rodrigues, and Klaus Schulten. Fast molecular electrostatics algorithms on GPUs.
    In *GPU Computing Gems*, Elsevier, Burlington, MA, 2011, pp. 43–58.
  prefs: []
  type: TYPE_NORMAL
- en: Stone, John E., James C. Phillips, Peter L. Freddolino, David J. Hardy, Leonardo
    G. Trabuco, and Klaus Schulten. Accelerating molecular modeling applications with
    graphics processors. *Journal of Computational Chemistry* 28 (2007), pp. 2618–2640.
  prefs: []
  type: TYPE_NORMAL
- en: '[http://cacs.usc.edu/education/cs653/Stone-MDGPU-JCC07.pdf](http://cacs.usc.edu/education/cs653/Stone-MDGPU-JCC07.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Stone, John E., David J. Hardy, Barry Isralewitz, and Klaus Schulten. GPU algorithms
    for molecular modeling. In *Scientific Computing with Multicore and Accelerators*,
    Jack Dongarra, David A. Bader, and Jakob Kurzak, eds. Chapman & Hall/CRC Press,
    London, UK, 2010, pp. 351–371.
  prefs: []
  type: TYPE_NORMAL
- en: '*Boids*'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'da Silva, A.R., W.S. Lages, and L. Chaimowicz. Boids that see: Using self-occlusion
    for simulating large groups on GPUs. *ACM Comput. Entertain*. 7 (4), 2009.'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://doi.acm.org/10.1145/1658866.1658870](http://doi.acm.org/10.1145/1658866.1658870)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 15\. Image Processing: Normalized Correlation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Normalized cross-correlation is a popular template-matching algorithm in image
    processing and computer vision. The template typically is an image that depicts
    a sought-after feature; by repeatedly computing a statistic between the template
    image and corresponding pixels of a subset of an input image, a search algorithm
    can locate instances of the template that are present in the input image.
  prefs: []
  type: TYPE_NORMAL
- en: The popularity of normalized cross-correlation for this application stems from
    its *amplitude independence*, which, in the context of image processing, essentially
    means that the statistic is robust in the face of lighting changes between the
    image and the template. Normalized correlation is popular enough, and sufficiently
    compute-intensive enough, that it has prompted companies to build custom hardware.
    This chapter develops an optimized implementation of normalized cross-correlation
    for 8-bit grayscale images, but many of the concepts can be extended to other
    types of image processing or computer vision algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 15.1\. Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two 2D images, the image and the template, are compared by computing a correlation
    coefficient as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/450equ01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: where *I* and *T* are the image and template, respectively; ![image](graphics/t-bar.jpg)
    is the average value of the template; and ![image](graphics/i-bar.jpg) is the
    average value of the image pixels corresponding to the template.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value of this coefficient falls into the range [–1.0, 1.0]; a value of
    1.0 corresponds to a perfect match. An optimized implementation of normalized
    correlation factors out the statistics that may be precomputed and computes sums
    instead of averages to avoid a separate pass over the input data. If *N* pixels
    are being compared, replacing ![image](graphics/i-bar.jpg) with ![image](graphics/450equ02.jpg)
    and multiplying the numerator and denominator by *N* yields a coefficient that
    can be expressed entirely in terms of sums. Rewriting without the coordinate notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/450equ03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Assuming the template will be the same for many correlation computations, the
    statistics on the template ![image](graphics/450fig01.jpg) and ![image](graphics/450fig02.jpg)
    can be precomputed, as can the subexpression ![Image](graphics/450equ04.jpg) in
    the denominator. Translating this notation to C variable names gives the following.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/450tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Then a normalized correlation value may be computed using this function.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p451pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: CorrelationValue( float SumI, float SumISq,
  prefs: []
  type: TYPE_NORMAL
- en: float SumT, float SumTSq, float SumIT,
  prefs: []
  type: TYPE_NORMAL
- en: float N )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float Numerator = N*SumIT - SumI*SumT;
  prefs: []
  type: TYPE_NORMAL
- en: float Denominator = (N*SumISq - SumI*SumI)*
  prefs: []
  type: TYPE_NORMAL
- en: (N*SumTSq – SumT*SumT);
  prefs: []
  type: TYPE_NORMAL
- en: return Numerator / sqrtf(Denominator);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: In practical applications for this algorithm, the template is kept fixed across
    many invocations, matching against different offsets into an image. Then it makes
    sense to precompute the template statistics and the denominator subexpression
  prefs: []
  type: TYPE_NORMAL
- en: float fDenomExp = N*SumSqT - SumT*SumT;
  prefs: []
  type: TYPE_NORMAL
- en: In practice, it’s best to use double precision to compute `fDenomExp.`
  prefs: []
  type: TYPE_NORMAL
- en: float fDenomExp = (float) ((double) N*SumSqT – (double) SumT*SumT);
  prefs: []
  type: TYPE_NORMAL
- en: '*Note:* This computation is done on the CPU, once per template.'
  prefs: []
  type: TYPE_NORMAL
- en: It is faster to multiply by the reciprocal square root than to divide by the
    square root, which results in the following `CorrelationValue()` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p451pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: float
  prefs: []
  type: TYPE_NORMAL
- en: CorrelationValue( float SumI, float SumISq, float SumIT,
  prefs: []
  type: TYPE_NORMAL
- en: float N, float fDenomExp )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: float Numerator = cPixels*SumIT - SumI*SumT;
  prefs: []
  type: TYPE_NORMAL
- en: float Denominator = fDenomExp*(cPixels*SumISq - SumI*SumI);
  prefs: []
  type: TYPE_NORMAL
- en: return Numerator * rsqrtf(Denominator);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, an optimized implementation of this algorithm need only compute three
    sums over the pixels to compute a given correlation coefficient: ![image](graphics/451fig01.jpg),
    ![image](graphics/451fig02.jpg), and ![image](graphics/451fig03.jpg). Since the
    SMs include hardware support for integer multiply-add, NVIDIA GPUs are able to
    perform this computation extremely fast.'
  prefs: []
  type: TYPE_NORMAL
- en: CUDA offers a number of paths that could be used to deliver the data to the
    streaming multiprocessors.
  prefs: []
  type: TYPE_NORMAL
- en: • Global memory or texture memory for the image, the template, or both
  prefs: []
  type: TYPE_NORMAL
- en: • Constant memory for the template and possibly other template-specific parameters
    (up to 64K)
  prefs: []
  type: TYPE_NORMAL
- en: • Shared memory to hold image and/or template values for reuse
  prefs: []
  type: TYPE_NORMAL
- en: This chapter assumes the pixels are 8-bit grayscale. The hardware works very
    well on images with higher precision, but if anything, that simplifies the problem
    by making it easier to efficiently address global and shared memory.
  prefs: []
  type: TYPE_NORMAL
- en: All of the CUDA implementations in this chapter use texture for the image that
    is being compared with the template. There are several reasons for this.
  prefs: []
  type: TYPE_NORMAL
- en: • The texture units deal with boundary conditions gracefully and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: • The texture cache aggregates external bandwidth on reuse, which will occur
    as nearby correlation values are computed.
  prefs: []
  type: TYPE_NORMAL
- en: • The 2D locality of the texture cache is a good fit with the access patterns
    exhibited by correlation search algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll explore the tradeoffs of using texture versus constant memory for the
    template.
  prefs: []
  type: TYPE_NORMAL
- en: 15.2\. Naïve Texture-Texture Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our first implementation of normalized cross-correlation uses the texture unit
    to read both image and template values. This implementation is not optimized;
    it does not even include the optimization to precompute the template statistics.
    But it is simple to understand and will serve as a good basis for more highly
    optimized (but more byzantine) implementations.
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 15.1](ch15.html#ch15lis01) gives the kernel that performs this computation.
    It computes the five sums, then uses the `CorrelationValue()` utility function
    given earlier to write the `float`-valued correlation coefficients into the output
    array. Note that the expression computing `fDenomExp` will issue a warning on
    pre–SM 1.3 architectures, which do not include double precision support. The kernel
    will still work as long as the number of pixels in the template is not too large.'
  prefs: []
  type: TYPE_NORMAL
- en: The upper left corner of the image is given by `(xUL, yUL)`; the width and height
    of the search window, and thus the output array of coefficients, is given by `w`
    and `h`. If the template is in a texture, the upper left corner of the template
    in the texture image is given by `(xTemplate, yTemplate)`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, an offset `(xOffset, yOffset)` specifies how the template will be overlaid
    with the image for comparison purposes. When fetching image pixels, this offset
    is added to the coordinates of the search rectangle whose upper left corner is
    `(xUL, yUL)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s instructive to look at how the correlation function “falls off” in the
    neighborhood of the image from which a template is extracted. The sample program
    `normalizedCrossCorrelation.cu` writes out the neighborhood around the template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p453pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Neighborhood around template:'
  prefs: []
  type: TYPE_NORMAL
- en: 0.71 0.75 0.79 0.81 0.82 0.81 0.81 0.80 0.78
  prefs: []
  type: TYPE_NORMAL
- en: 0.72 0.77 0.81 0.84 0.84 0.84 0.83 0.81 0.79
  prefs: []
  type: TYPE_NORMAL
- en: 0.74 0.79 0.84 0.88 0.88 0.87 0.85 0.82 0.79
  prefs: []
  type: TYPE_NORMAL
- en: 0.75 0.80 0.86 0.93 0.95 0.91 0.86 0.83 0.80
  prefs: []
  type: TYPE_NORMAL
- en: 0.75 0.80 0.87 0.95 1.00 0.95 0.88 0.83 0.81
  prefs: []
  type: TYPE_NORMAL
- en: 0.75 0.80 0.86 0.91 0.95 0.93 0.87 0.82 0.80
  prefs: []
  type: TYPE_NORMAL
- en: 0.75 0.80 0.84 0.87 0.89 0.88 0.85 0.81 0.78
  prefs: []
  type: TYPE_NORMAL
- en: 0.73 0.78 0.81 0.83 0.85 0.85 0.82 0.79 0.76
  prefs: []
  type: TYPE_NORMAL
- en: 0.71 0.75 0.78 0.81 0.82 0.82 0.80 0.77 0.75
  prefs: []
  type: TYPE_NORMAL
- en: In the coins image included in the book, the default template is a 52x52 subimage
    around the dime in the lower right corner ([Figure 15.1](ch15.html#ch15fig01)).
    The default program optionally can write a PGM file as output, with the correlation
    values converted to pixel values in the range 0..255\. For the template highlighted
    in [Figure 15.1](ch15.html#ch15fig01), the resulting image is given in [Figure
    15.2](ch15.html#ch15fig02). The other dimes are very bright, with strong matches,
    while the other coins get less intense responses.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/15fig01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15.1* Coins.pgm (with default template highlighted).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/15fig02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15.2* Correlation image with default template.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15.1.* `corrTexTex2D_kernel.`'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p15lis01a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: corrTexTex2D_kernel(
  prefs: []
  type: TYPE_NORMAL
- en: float *pCorr, size_t CorrPitch,
  prefs: []
  type: TYPE_NORMAL
- en: float cPixels,
  prefs: []
  type: TYPE_NORMAL
- en: int xOffset, int yOffset,
  prefs: []
  type: TYPE_NORMAL
- en: int xTemplate, int yTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: int wTemplate, int hTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: float xUL, float yUL, int w, int h )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: size_t row = blockIdx.y*blockDim.y + threadIdx.y;
  prefs: []
  type: TYPE_NORMAL
- en: size_t col = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: // adjust pCorr to point to row
  prefs: []
  type: TYPE_NORMAL
- en: pCorr = (float *) ((char *) pCorr+row*CorrPitch);
  prefs: []
  type: TYPE_NORMAL
- en: // No __syncthreads in this kernel, so we can early-out
  prefs: []
  type: TYPE_NORMAL
- en: // without worrying about the effects of divergence.
  prefs: []
  type: TYPE_NORMAL
- en: if ( col >= w || row >= h )
  prefs: []
  type: TYPE_NORMAL
- en: return;
  prefs: []
  type: TYPE_NORMAL
- en: int SumI = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SumT = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SumISq = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SumTSq = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SumIT = 0;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int y = 0; y < hTemplate; y++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int x = 0; x < wTemplate; x++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char I = tex2D( texImage,
  prefs: []
  type: TYPE_NORMAL
- en: (float) col+xUL+xOffset+x, (float) row+yUL+yOffset+y );
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char T = tex2D( texTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: (float) xTemplate+x, (float) yTemplate+y);
  prefs: []
  type: TYPE_NORMAL
- en: SumI += I;
  prefs: []
  type: TYPE_NORMAL
- en: SumT += T;
  prefs: []
  type: TYPE_NORMAL
- en: SumISq += I*I;
  prefs: []
  type: TYPE_NORMAL
- en: SumTSq += T*T;
  prefs: []
  type: TYPE_NORMAL
- en: SumIT += I*T;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: float fDenomExp = (float) ( (double) cPixels*SumTSq -
  prefs: []
  type: TYPE_NORMAL
- en: (double) SumT*SumT);
  prefs: []
  type: TYPE_NORMAL
- en: pCorr[col] = CorrelationValue(
  prefs: []
  type: TYPE_NORMAL
- en: SumI, SumISq, SumIT, SumT, cPixels, fDenomExp );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: '[Listing 15.2](ch15.html#ch15lis02) gives the host code to invoke `corrTexTex2D_kernel()`.
    It is designed to work with the testing and performance measurement code in the
    sample source file `normalizedCrossCorrelation.cu`, which is why it has so many
    parameters. This host function just turns around and launches the kernel with
    the needed parameters, but later implementations of this function will check the
    device properties and launch different kernels, depending on what it finds. For
    images of a useful size, the cost of doing such checks is negligible compared
    to the GPU runtime.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15.2.* `corrTexTex2D()` (host code).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p15lis02a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: corrTexTex2D(
  prefs: []
  type: TYPE_NORMAL
- en: float *dCorr, int CorrPitch,
  prefs: []
  type: TYPE_NORMAL
- en: int wTile,
  prefs: []
  type: TYPE_NORMAL
- en: int wTemplate, int hTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: float cPixels,
  prefs: []
  type: TYPE_NORMAL
- en: float fDenomExp,
  prefs: []
  type: TYPE_NORMAL
- en: int sharedPitch,
  prefs: []
  type: TYPE_NORMAL
- en: int xOffset, int yOffset,
  prefs: []
  type: TYPE_NORMAL
- en: int xTemplate, int yTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: int xUL, int yUL, int w, int h,
  prefs: []
  type: TYPE_NORMAL
- en: dim3 threads, dim3 blocks,
  prefs: []
  type: TYPE_NORMAL
- en: int sharedMem )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: corrTexTex2D_kernel<<<blocks, threads>>>(
  prefs: []
  type: TYPE_NORMAL
- en: dCorr, CorrPitch,
  prefs: []
  type: TYPE_NORMAL
- en: cPixels,
  prefs: []
  type: TYPE_NORMAL
- en: xOffset, yOffset,
  prefs: []
  type: TYPE_NORMAL
- en: xTemplate+xOffset, yTemplate+yOffset,
  prefs: []
  type: TYPE_NORMAL
- en: wTemplate, hTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: (float) xUL, (float) yUL, w, h );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: A texture-texture formulation is a very good fit if the application is choosing
    different templates as well as different images during its search—for example,
    applying transformations to the template data while comparing to the image. But
    for most applications, the template is chosen once and compared against many different
    offsets within the image. The remainder of the chapter will examine implementations
    that are optimized for that case.
  prefs: []
  type: TYPE_NORMAL
- en: 15.3\. Template in Constant Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most template-matching applications perform many correlation computations with
    the same template at different offsets of the input image. In that case, the template
    statistics (`SumT` and `fDenomExp`) can be precomputed, and the template data
    can be moved to special memory or otherwise premassaged. For CUDA, the obvious
    place to put the template data is in constant memory so each template pixel can
    be broadcast to the threads computing correlation values for different image locations.
  prefs: []
  type: TYPE_NORMAL
- en: The `CopyToTemplate` function given in [Listing 15.3](ch15.html#ch15lis03) pulls
    a rectangular area of pixels out of the input image, computes the statistics,
    and copies the data and statistics to `__constant__` memory.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15.3.* `CopyToTemplate` function (error handling removed).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p15lis03a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t
  prefs: []
  type: TYPE_NORMAL
- en: CopyToTemplate(
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char *img, size_t imgPitch,
  prefs: []
  type: TYPE_NORMAL
- en: int xTemplate, int yTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: int wTemplate, int hTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: int OffsetX, int OffsetY
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char pixels[maxTemplatePixels];
  prefs: []
  type: TYPE_NORMAL
- en: int inx = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SumT = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SumTSq = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int cPixels = wTemplate*hTemplate;
  prefs: []
  type: TYPE_NORMAL
- en: size_t sizeOffsets = cPixels*sizeof(int);
  prefs: []
  type: TYPE_NORMAL
- en: float fSumT, fDenomExp, fcPixels;
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpy2D(
  prefs: []
  type: TYPE_NORMAL
- en: pixels, wTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: img+yTemplate*imgPitch+xTemplate, imgPitch,
  prefs: []
  type: TYPE_NORMAL
- en: wTemplate, hTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyDeviceToHost );
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyToSymbol( g_Tpix, pixels, cPixels );
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = OffsetY; i < OffsetY+hTemplate; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = OffsetX; j < OffsetX+wTemplate; j++) {
  prefs: []
  type: TYPE_NORMAL
- en: SumT += pixels[inx];
  prefs: []
  type: TYPE_NORMAL
- en: SumTSq += pixels[inx]*pixels[inx];
  prefs: []
  type: TYPE_NORMAL
- en: poffsetx[inx] = j;
  prefs: []
  type: TYPE_NORMAL
- en: poffsety[inx] = i;
  prefs: []
  type: TYPE_NORMAL
- en: inx += 1;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: g_cpuSumT = SumT;
  prefs: []
  type: TYPE_NORMAL
- en: g_cpuSumTSq = SumTSq;
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyToSymbol(g_xOffset, poffsetx, sizeOffsets);
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyToSymbol(g_yOffset, poffsety, sizeOffsets);
  prefs: []
  type: TYPE_NORMAL
- en: fSumT = (float) SumT;
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyToSymbol(g_SumT, &fSumT, sizeof(float));
  prefs: []
  type: TYPE_NORMAL
- en: fDenomExp = float( (double)cPixels*SumTSq – (double) SumT*SumT);
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyToSymbol(g_fDenomExp, &fDenomExp, sizeof(float));
  prefs: []
  type: TYPE_NORMAL
- en: fcPixels = (float) cPixels;
  prefs: []
  type: TYPE_NORMAL
- en: cudaMemcpyToSymbol(g_cPixels, &fcPixels, sizeof(float));
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: return status;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: The `corrTemplate2D()` kernel given in [Listing 15.4](ch15.html#ch15lis04) then
    can read the template values from `g_TPix[]`, which resides in constant memory.
    `corrTemplate2D()` is even simpler and shorter than `corrTexTex2D()`, since it
    does not have to compute the template statistics.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15.4.* `corrTemplate2D` kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p15lis04a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: corrTemplate2D_kernel(
  prefs: []
  type: TYPE_NORMAL
- en: float *pCorr, size_t CorrPitch,
  prefs: []
  type: TYPE_NORMAL
- en: float cPixels, float fDenomExp,
  prefs: []
  type: TYPE_NORMAL
- en: float xUL, float yUL, int w, int h,
  prefs: []
  type: TYPE_NORMAL
- en: int xOffset, int yOffset,
  prefs: []
  type: TYPE_NORMAL
- en: int wTemplate, int hTemplate )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: size_t row = blockIdx.y*blockDim.y + threadIdx.y;
  prefs: []
  type: TYPE_NORMAL
- en: size_t col = blockIdx.x*blockDim.x + threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: // adjust pointers to row
  prefs: []
  type: TYPE_NORMAL
- en: pCorr = (float *) ((char *) pCorr+row*CorrPitch);
  prefs: []
  type: TYPE_NORMAL
- en: // No __syncthreads in this kernel, so we can early-out
  prefs: []
  type: TYPE_NORMAL
- en: // without worrying about the effects of divergence.
  prefs: []
  type: TYPE_NORMAL
- en: if ( col >= w || row >= h )
  prefs: []
  type: TYPE_NORMAL
- en: return;
  prefs: []
  type: TYPE_NORMAL
- en: int SumI = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SumISq = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SumIT = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int inx = 0;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < hTemplate; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < wTemplate; i++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char I = tex2D( texImage,
  prefs: []
  type: TYPE_NORMAL
- en: (float) col+xUL+xOffset+i,
  prefs: []
  type: TYPE_NORMAL
- en: (float) row+yUL+yOffset+j );
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char T = g_Tpix[inx++];
  prefs: []
  type: TYPE_NORMAL
- en: SumI += I;
  prefs: []
  type: TYPE_NORMAL
- en: SumISq += I*I;
  prefs: []
  type: TYPE_NORMAL
- en: SumIT += I*T;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: pCorr[col] =
  prefs: []
  type: TYPE_NORMAL
- en: CorrelationValue(
  prefs: []
  type: TYPE_NORMAL
- en: SumI, SumISq, SumIT, g_SumT, cPixels, fDenomExp );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 15.4\. Image in Shared Memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For rectangles of correlation values such as the ones computed by our sample
    program, the CUDA kernel exhibits a tremendous amount of reuse of the image data
    as the template matches are swept across the image. So far, our code has relied
    on the texture caches to service these redundant reads without going to external
    memory. For smaller templates, however, shared memory can be used to further increase
    performance by making the image data available with lower latency.
  prefs: []
  type: TYPE_NORMAL
- en: The kernels in [Listings 15.1](ch15.html#ch15lis01) and [15.3](ch15.html#ch15lis03)
    implicitly divided the input image into tiles that were the same size as the threadblocks.
    For our shared memory implementation shown in [Listing 15.5](ch15.html#ch15lis05),
    we’ll use the height of the threadblock (`blockDim.y`) but specify an explicit
    tile width of `wTile`. In our sample program, `wTile` is 32\. [Figure 15.4](ch15.html#ch15fig04)
    shows how the kernel “overfetches” a rectangle of `wTemplate×hTemplate` pixels
    outside the tile; boundary conditions are handled by the texture addressing mode.
    Once the shared memory has been populated with image data, the kernel does `__syncthreads()`
    and computes and writes out the tile’s correlation coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15.5.* `corrShared_kernel().`'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p15lis05a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: corrShared_kernel(
  prefs: []
  type: TYPE_NORMAL
- en: float *pCorr, size_t CorrPitch,
  prefs: []
  type: TYPE_NORMAL
- en: int wTile,
  prefs: []
  type: TYPE_NORMAL
- en: int wTemplate, int hTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: float xOffset, float yOffset,
  prefs: []
  type: TYPE_NORMAL
- en: float cPixels, float fDenomExp, int SharedPitch,
  prefs: []
  type: TYPE_NORMAL
- en: float xUL, float yUL, int w, int h )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int uTile = blockIdx.x*wTile;
  prefs: []
  type: TYPE_NORMAL
- en: int vTile = blockIdx.y*blockDim.y;
  prefs: []
  type: TYPE_NORMAL
- en: int v = vTile + threadIdx.y;
  prefs: []
  type: TYPE_NORMAL
- en: float *pOut = (float *) (((char *) pCorr)+v*CorrPitch);
  prefs: []
  type: TYPE_NORMAL
- en: for ( int row = threadIdx.y;
  prefs: []
  type: TYPE_NORMAL
- en: row < blockDim.y+hTemplate;
  prefs: []
  type: TYPE_NORMAL
- en: row += blockDim.y ) {
  prefs: []
  type: TYPE_NORMAL
- en: int SharedIdx = row * SharedPitch;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int col = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: col < wTile+wTemplate;
  prefs: []
  type: TYPE_NORMAL
- en: col += blockDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: LocalBlock[SharedIdx+col] =
  prefs: []
  type: TYPE_NORMAL
- en: tex2D( texImage,
  prefs: []
  type: TYPE_NORMAL
- en: (float) (uTile+col+xUL+xOffset),
  prefs: []
  type: TYPE_NORMAL
- en: (float) (vTile+row+yUL+yOffset) );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: for ( int col = threadIdx.x;
  prefs: []
  type: TYPE_NORMAL
- en: col < wTile;
  prefs: []
  type: TYPE_NORMAL
- en: col += blockDim.x ) {
  prefs: []
  type: TYPE_NORMAL
- en: int SumI = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SumISq = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SumIT = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int idx = 0;
  prefs: []
  type: TYPE_NORMAL
- en: int SharedIdx = threadIdx.y * SharedPitch + col;
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < hTemplate; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < wTemplate; i++) {
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char I = LocalBlock[SharedIdx+i];
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char T = g_Tpix[idx++];
  prefs: []
  type: TYPE_NORMAL
- en: SumI += I;
  prefs: []
  type: TYPE_NORMAL
- en: SumISq += I*I;
  prefs: []
  type: TYPE_NORMAL
- en: SumIT += I*T;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: SharedIdx += SharedPitch;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: if ( uTile+col < w && v < h ) {
  prefs: []
  type: TYPE_NORMAL
- en: pOut[uTile+col] =
  prefs: []
  type: TYPE_NORMAL
- en: CorrelationValue( SumI, SumISq, SumIT, g_SumT,
  prefs: []
  type: TYPE_NORMAL
- en: cPixels, fDenomExp );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: __syncthreads();
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that shared memory references will avoid bank conflicts from one row
    to the next, the amount of shared memory per row is padded to the next multiple
    of 64.
  prefs: []
  type: TYPE_NORMAL
- en: sharedPitch = ~63&(((wTile+wTemplate)+63));
  prefs: []
  type: TYPE_NORMAL
- en: The total amount of shared memory needed per block is then the pitch multiplied
    by the number of rows (block height plus template height).
  prefs: []
  type: TYPE_NORMAL
- en: sharedMem = sharedPitch*(threads.y+hTemplate);
  prefs: []
  type: TYPE_NORMAL
- en: The host code to launch `corrShared_kernel()`, shown in [Listing 15.6](ch15.html#ch15lis06),
    detects whether the kernel launch will require more shared memory than is available.
    If that is the case, it calls `corrTexTex2D()`, which will work for any template
    size.
  prefs: []
  type: TYPE_NORMAL
- en: '*Listing 15.6.* `corrShared()` (host code).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p15lis06a)'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: void
  prefs: []
  type: TYPE_NORMAL
- en: corrShared(
  prefs: []
  type: TYPE_NORMAL
- en: float *dCorr, int CorrPitch,
  prefs: []
  type: TYPE_NORMAL
- en: int wTile,
  prefs: []
  type: TYPE_NORMAL
- en: int wTemplate, int hTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: float cPixels,
  prefs: []
  type: TYPE_NORMAL
- en: float fDenomExp,
  prefs: []
  type: TYPE_NORMAL
- en: int sharedPitch,
  prefs: []
  type: TYPE_NORMAL
- en: int xOffset, int yOffset,
  prefs: []
  type: TYPE_NORMAL
- en: int xTemplate, int yTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: int xUL, int yUL, int w, int h,
  prefs: []
  type: TYPE_NORMAL
- en: dim3 threads, dim3 blocks,
  prefs: []
  type: TYPE_NORMAL
- en: int sharedMem )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: int device;
  prefs: []
  type: TYPE_NORMAL
- en: cudaDeviceProp props;
  prefs: []
  type: TYPE_NORMAL
- en: cudaError_t status;
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaGetDevice( &device ) );
  prefs: []
  type: TYPE_NORMAL
- en: CUDART_CHECK( cudaGetDeviceProperties( &props, device ) );
  prefs: []
  type: TYPE_NORMAL
- en: if ( sharedMem > props.sharedMemPerBlock ) {
  prefs: []
  type: TYPE_NORMAL
- en: dim3 threads88(8, 8, 1);
  prefs: []
  type: TYPE_NORMAL
- en: dim3 blocks88;
  prefs: []
  type: TYPE_NORMAL
- en: blocks88.x = INTCEIL(w,8);
  prefs: []
  type: TYPE_NORMAL
- en: blocks88.y = INTCEIL(h,8);
  prefs: []
  type: TYPE_NORMAL
- en: blocks88.z = 1;
  prefs: []
  type: TYPE_NORMAL
- en: return corrTexTex2D(
  prefs: []
  type: TYPE_NORMAL
- en: dCorr, CorrPitch,
  prefs: []
  type: TYPE_NORMAL
- en: wTile,
  prefs: []
  type: TYPE_NORMAL
- en: wTemplate, hTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: cPixels,
  prefs: []
  type: TYPE_NORMAL
- en: fDenomExp,
  prefs: []
  type: TYPE_NORMAL
- en: sharedPitch,
  prefs: []
  type: TYPE_NORMAL
- en: xOffset, yOffset,
  prefs: []
  type: TYPE_NORMAL
- en: xTemplate, yTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: xUL, yUL, w, h,
  prefs: []
  type: TYPE_NORMAL
- en: threads88, blocks88,
  prefs: []
  type: TYPE_NORMAL
- en: sharedMem );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: corrShared_kernel<<<blocks, threads, sharedMem>>>(
  prefs: []
  type: TYPE_NORMAL
- en: dCorr, CorrPitch,
  prefs: []
  type: TYPE_NORMAL
- en: wTile,
  prefs: []
  type: TYPE_NORMAL
- en: wTemplate, hTemplate,
  prefs: []
  type: TYPE_NORMAL
- en: (float) xOffset, (float) yOffset,
  prefs: []
  type: TYPE_NORMAL
- en: cPixels, fDenomExp,
  prefs: []
  type: TYPE_NORMAL
- en: sharedPitch,
  prefs: []
  type: TYPE_NORMAL
- en: (float) xUL, (float) yUL, w, h );
  prefs: []
  type: TYPE_NORMAL
- en: 'Error:'
  prefs: []
  type: TYPE_NORMAL
- en: return;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: 15.5\. Further Optimizations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Two more optimizations are implemented in the sample source code: SM-aware
    kernel invocation (since SM 1.x has different instruction set support for multiplication,
    which is in the innermost loop of this computation) and an unrolled inner loop
    of the kernel.'
  prefs: []
  type: TYPE_NORMAL
- en: 15.5.1\. SM-Aware Coding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SM 1.x hardware uses a 24-bit multiplier (plenty wide enough to do the multiplications
    in the inner loop of this computation), yet SM 2.x and SM 3.x hardware use 32-bit
    multipliers. Sometimes the compiler can detect when the participating integers
    are narrow enough that it can use the 24-bit multiply on SM 1.x–class hardware,
    but that does not seem to be the case for `corrShared_kernel()`. To work around
    the issue, we can use a template on the kernel declaration.
  prefs: []
  type: TYPE_NORMAL
- en: template<bool bSM1>
  prefs: []
  type: TYPE_NORMAL
- en: __global__ void
  prefs: []
  type: TYPE_NORMAL
- en: corrSharedSM_kernel( ... )
  prefs: []
  type: TYPE_NORMAL
- en: The inner loop of the kernel then becomes
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p463pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < hTemplate; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < wTemplate; i++) {
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char I = LocalBlock[SharedIdx+i];
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char T = g_Tpix[idx++];
  prefs: []
  type: TYPE_NORMAL
- en: SumI += I;
  prefs: []
  type: TYPE_NORMAL
- en: if ( bSM1 ) {
  prefs: []
  type: TYPE_NORMAL
- en: SumISq += __umul24(I, I);
  prefs: []
  type: TYPE_NORMAL
- en: SumIT += __umul24(I, T);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else {
  prefs: []
  type: TYPE_NORMAL
- en: SumISq += I*I;
  prefs: []
  type: TYPE_NORMAL
- en: SumIT += I*T;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: SharedIdx += SharedPitch;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: And the host function that invokes the kernel must detect whether the device
    is SM 1.x and, if so, invoke the kernel with `bSM1=true`. In the sample source
    code, this implementation is given in the `corrSharedSM.cuh` and `corrSharedSMSums.cuh`
    header files.
  prefs: []
  type: TYPE_NORMAL
- en: 15.5.2\. Loop Unrolling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since each thread is accessing adjacent bytes in shared memory, the innermost
    loop of these kernels generates 4-way bank conflicts on SM 1.x-class hardware.
    If we rewrite
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p464pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < hTemplate; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < wTemplate; i++) {
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char I = LocalBlock[SharedIdx+i];
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char T = g_Tpix[idx++];
  prefs: []
  type: TYPE_NORMAL
- en: SumI += I;
  prefs: []
  type: TYPE_NORMAL
- en: SumISq += I*I;
  prefs: []
  type: TYPE_NORMAL
- en: SumIT += I*T;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: SharedIdx += SharedPitch;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: as follows
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p464pro02a)'
  prefs: []
  type: TYPE_NORMAL
- en: for ( int j = 0; j < hTemplate; j++ ) {
  prefs: []
  type: TYPE_NORMAL
- en: for ( int i = 0; i < wTemplate/4; i++) {
  prefs: []
  type: TYPE_NORMAL
- en: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+0], );
  prefs: []
  type: TYPE_NORMAL
- en: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+1], );
  prefs: []
  type: TYPE_NORMAL
- en: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+2], );
  prefs: []
  type: TYPE_NORMAL
- en: corrSharedAccumulate<bSM1>( ... LocalBlock[SharedIdx+i*4+3], );
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: SharedIdx += SharedPitch;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: where the `corrSharedAccumulate()` function encapsulates the template parameter
    bSM1
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p464pro03a)'
  prefs: []
  type: TYPE_NORMAL
- en: template<bool bSM1>
  prefs: []
  type: TYPE_NORMAL
- en: __device__ void
  prefs: []
  type: TYPE_NORMAL
- en: corrSharedAccumulate(
  prefs: []
  type: TYPE_NORMAL
- en: int& SumI, int& SumISq, int& SumIT,
  prefs: []
  type: TYPE_NORMAL
- en: unsigned char I, unsigned char T )
  prefs: []
  type: TYPE_NORMAL
- en: '{'
  prefs: []
  type: TYPE_NORMAL
- en: SumI += I;
  prefs: []
  type: TYPE_NORMAL
- en: if ( bSM1 ) {
  prefs: []
  type: TYPE_NORMAL
- en: SumISq += __umul24(I,I);
  prefs: []
  type: TYPE_NORMAL
- en: SumIT += __umul24(I,T);
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: else {
  prefs: []
  type: TYPE_NORMAL
- en: SumISq += I*I;
  prefs: []
  type: TYPE_NORMAL
- en: SumIT += I*T;
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: '}'
  prefs: []
  type: TYPE_NORMAL
- en: Although the primary motivation is to decrease bank conflicts due to byte reads—an
    effect that only occurs on SM 1.x hardware—the resulting kernel is faster on all
    CUDA hardware.
  prefs: []
  type: TYPE_NORMAL
- en: 15.6\. Source Code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When working on optimized normalized cross-correlation code, it does not take
    long to realize that it’s surprisingly difficult and error-prone. Converting the
    sums to correlation coefficients, as described in [Section 15.1](ch15.html#ch15lev1sec1),
    must be done carefully due to the precision characteristics of `float` versus
    `int` (`float` has a greater dynamic range, but only 24 bits of precision). It
    is good practice to develop separate subroutines that report the computed sums
    to root cause whether a given implementation is reporting incorrect coefficients
    due to incorrect sums or an incorrect coefficient computation. Also, the sums
    can be bitwise-compared with CPU results, while the float-valued coefficients
    must be fuzzily compared against an epsilon value.
  prefs: []
  type: TYPE_NORMAL
- en: The different implementations of correlation are broken out into separate header
    (`.cuh`) files, and the kernels that emit sums as well as correlation coefficients
    are separate.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/465tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `normalizedCrossCorrelation.cu` program tests both the functionality and
    the performance of the kernels. By default, it loads `coins.pgm` and detects the
    dime in the lower right corner. The dime is located at (210,148) and is 52×52
    pixels in size. The program also writes the performance measurements to `stdout`—for
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Click here to view code image](ch15_images.html#p466pro01a)'
  prefs: []
  type: TYPE_NORMAL
- en: $ normalizedCrossCorrelation --padWidth 1024 --padHeight 1024 -wTemplate 16
    -hTemplate 16
  prefs: []
  type: TYPE_NORMAL
- en: 'corrTexTex2D: 54.86 Mpix/s 14.05Gtpix/s'
  prefs: []
  type: TYPE_NORMAL
- en: 'corrTemplate2D: 72.87 Mpix/s 18.65Gtpix/s'
  prefs: []
  type: TYPE_NORMAL
- en: 'corrShared: 69.66 Mpix/s 17.83Gtpix/s'
  prefs: []
  type: TYPE_NORMAL
- en: 'corrSharedSM: 78.66 Mpix/s 20.14Gtpix/s'
  prefs: []
  type: TYPE_NORMAL
- en: 'corrShared4: 97.02 Mpix/s 24.84Gtpix/s'
  prefs: []
  type: TYPE_NORMAL
- en: The program supports the following command line options.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/466tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 15.7\. Performance and Further Reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our sample program uses CUDA events to report the performance of some number
    of consecutive kernel launches (default 100) and reports the rates of both output
    coefficients (which varies with the template size) and the “template-pixel” rate,
    or the number of inner loop iterations per unit time.
  prefs: []
  type: TYPE_NORMAL
- en: The raw performance of GPUs at performing this computation is astonishing. A
    GeForce GTX 280 (GT200) can perform almost 25 billion template-pixel calculations
    per second (Gtpix/s), and the GeForce 680 GTX (GK104) delivers well over 100 Gtpix/s.
  prefs: []
  type: TYPE_NORMAL
- en: The default parameters of the program are not ideal for performance measurement.
    They are set to detect the dime in the lower right corner and optionally write
    out the image in [Figure 15.3](ch15.html#ch15fig03). In particular, the image
    is too small to keep the GPU fully busy. The image is only 300×246 pixels (74K
    in size), so only 310 blocks are needed by the shared memory implementation to
    perform the computation. The `--padWidth` and `--padHeight` command-line options
    can be used in the sample program to increase the size of the image and thus the
    number of correlation coefficients computed (there are no data dependencies in
    the code, so the padding can be filled with arbitrary data); a 1024×1024 image
    is both more realistic and gets best utilization out of all GPUs tested.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/15fig03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15.3* Template in `__constant__` memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/15fig04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15.4* Image in shared memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 15.5](ch15.html#ch15fig05) summarizes the relative performance of our
    5 implementations.'
  prefs: []
  type: TYPE_NORMAL
- en: '• `corrTexTex`: template and image both in texture memory'
  prefs: []
  type: TYPE_NORMAL
- en: '• `corrTexConstant`: template in constant memory'
  prefs: []
  type: TYPE_NORMAL
- en: '• `corrShared`: template in constant memory and image in shared memory'
  prefs: []
  type: TYPE_NORMAL
- en: '• `corrSharedSM`: `corrShared` with SM-aware kernel invocations'
  prefs: []
  type: TYPE_NORMAL
- en: '• `corrShared4`: `corrSharedSM` with the inner loop unrolled 4x'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/15fig05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15.5* Performance comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: The various optimizations did improve performance, to varying degrees, as shown
    in [Figure 15.6](ch15.html#ch15fig06). Moving the template to constant memory
    had the biggest impact on GK104, increasing performance by 80%; moving the image
    to shared memory had the biggest impact on GF100, increasing performance by 70%.
    The SM-aware kernel launches had the most muted impact, increasing performance
    on GT200 by 14% (it does not affect performance on the other architectures, since
    using the built-in multiplication operator is also fastest).
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/15fig06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 15.6* Correlation rate versus template size.'
  prefs: []
  type: TYPE_NORMAL
- en: On GT200, `corrShared` suffered from bank conflicts in shared memory, so much
    so that `corrShared` is slower than `corrTexConstant`; `corrShared4` alleviates
    these bank conflicts, increasing performance by 23%.
  prefs: []
  type: TYPE_NORMAL
- en: 'The size of the template also has a bearing on the efficiency of this algorithm:
    The larger the template, the more efficient the computation on a per-template-pixel
    basis. [Figure 15.6](ch15.html#ch15fig06) illustrates how the template size affects
    performance of the `corrShared4` formulation.'
  prefs: []
  type: TYPE_NORMAL
- en: As the template grows from 8×8 to 28×28, GT200 performance improves 36% (19.6
    Gtpix/s to 26.8 Gtpix/s), GF100 improves 57% (46.5 Gtpix/s to 72.9 Gtpix/s), and
    GK104 improves 30% (93.9 Gtpix/s to 120.72 Gtpix/s).
  prefs: []
  type: TYPE_NORMAL
- en: For small templates, the compiler generates faster code if the template size
    is known at compile time. Moving `wTemplate` and `hTemplate` to be template parameters
    and specializing for an 8×8 template improved performance as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Image](graphics/469tab01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 15.8\. Further Reading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Digital Image Processing* includes both a discussion of normalized correlation
    (pp. 583–586) and the logarithmic transform used to compute the output pixels
    in our sample program (pp. 168–169).'
  prefs: []
  type: TYPE_NORMAL
- en: Gonzalez, Rafael C., and Richard E. Woods. *Digital image processing*. Addison-Wesley,
    Reading, MA, 1992.
  prefs: []
  type: TYPE_NORMAL
- en: '[www.imageprocessingplace.com/root_files_V3/publications.htm](http://www.imageprocessingplace.com/root_files_V3/publications.htm)'
  prefs: []
  type: TYPE_NORMAL
- en: J.P. Lewis has an excellent discussion, including a more asymptotically efficient
    way to accelerate the type of correlation operation implemented by our sample
    program, where a template match against every pixel in the input image is desired.
    Lewis uses FFTs to compute the numerators and summed area tables to compute the
    denominators of the coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Lewis, J.P. Fast template matching. *Vision Interface 10*, 1995, pp. 120–123\.
    An expanded version entitled “Fast Normalized Correlation” may be found online
    at [http://bit.ly/NJnZPI](http://bit.ly/NJnZPI).
  prefs: []
  type: TYPE_NORMAL
