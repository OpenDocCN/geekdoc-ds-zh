<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>6  The mechanics of least squares</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1>6  The mechanics of least squares</h1>
<blockquote>原文：<a href="https://mattblackwell.github.io/gov2002-book/least_squares.html">https://mattblackwell.github.io/gov2002-book/least_squares.html</a></blockquote>

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./linear_model.html">Regression</a></li><li class="breadcrumb-item"><a href="./least_squares.html"><span class="chapter-number">6</span>  <span class="chapter-title">The mechanics of least squares</span></a></li></ol></nav>
<div class="quarto-title">

</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter explores the most widely used estimator for population linear regressions: <strong>ordinary least squares</strong> (OLS). OLS is a plug-in estimator for the best linear projection (or population linear regression) described in the last chapter. Its popularity is partly due to its ease of interpretation, computational simplicity, and statistical efficiency. Because most people in the quantitative social sciences rely extensively on OLS for their own research, the time you spend developing deep familiarity with this approach will serve you well.</p>
<p>In this chapter, we focus on motivating the estimator and the mechanical or algebraic properties of the OLS estimator. In the next chapter, we will investigate its statistical assumptions. Textbooks often introduce OLS under the assumption of a linear model for the conditional expectation, but this is unnecessary if we view the inference target as the best linear predictor. We discuss this point more fully in the next chapter.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-ajr-scatter" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ajr-scatter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../Images/f17e01b7a19b165143302da9b3faaf34.png" class="img-fluid figure-img" width="672" data-original-src="https://mattblackwell.github.io/gov2002-book/least_squares_files/figure-html/fig-ajr-scatter-1.png"/>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ajr-scatter-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 6.1: Relationship between political institutions and economic development from Acemoglu, Johnson, and Robinson (2001).
</figcaption>
</figure>
</div>
</div>
</div>
<section id="deriving-the-ols-estimator" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="deriving-the-ols-estimator"><span class="header-section-number">6.1</span> Deriving the OLS estimator</h2>
<p>The last chapter on the linear model and the best linear projection operated purely in the population, not samples. We derived the population regression coefficients <span class="math inline">\(\bfbeta\)</span>, representing the coefficients on the line of best fit in the population. We now take these as our quantity of interest. We now focus on how to use a sample from the population to make inferences about the line of best fit in the population and the population coefficients. To do this, we will focus on the OLS estimator for these population quantities.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
Assumption
</div>
</div>
<div class="callout-body-container callout-body">
<p>The variables <span class="math inline">\(\{(Y_1, \X_1), \ldots, (Y_i,\X_i), \ldots, (Y_n, \X_n)\}\)</span> are i.i.d. draws from a common distribution <span class="math inline">\(F\)</span>.</p>
</div>
</div>
<p>Recall the population linear coefficients (or best linear predictor coefficients) that we derived in the last chapter, <span class="math display">\[
\bfbeta = \argmin_{\mb{b} \in \real^k}\; \E\bigl[ \bigl(Y_{i} - \mb{X}_{i}'\mb{b} \bigr)^2\bigr] = \left(\E[\X_{i}\X_{i}']\right)^{-1}\E[\X_{i}Y_{i}]
\]</span></p>
<p>We will consider two different ways to derive the OLS estimator for these coefficients, both of which are versions of the plug-in principle. The first approach is to use the closed-form representation of the coefficients and then to replace any expectations with sample means, <span class="math display">\[
\bhat = \left(\frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left(\frac{1}{n} \sum_{i=1}^n \X_{i}Y_{i} \right),
\]</span> which exists if <span class="math inline">\(\sum_{i=1}^n \X_i\X_i'\)</span> is <strong>positive definite</strong> and thus invertible. We will return to this assumption below.</p>
<p>In a simple bivariate linear projection model <span class="math inline">\(m(X_{i}) = \beta_0 + \beta_1X_{i}\)</span>, we saw that the population slope was <span class="math inline">\(\beta_1= \text{cov}(Y_{i},X_{i})/ \V[X_{i}]\)</span>. This approach means that the estimator for the slope should be the ratio of the sample covariance of <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(X_i\)</span> to the sample variance of <span class="math inline">\(X_i\)</span>, or <span class="math display">\[
\widehat{\beta}_{1} = \frac{\widehat{\sigma}_{Y,X}}{\widehat{\sigma}^{2}_{X}} = \frac{ \frac{1}{n-1}\sum_{i=1}^{n} (Y_{i} - \overline{Y})(X_{i} - \overline{X})}{\frac{1}{n-1} \sum_{i=1}^{n} (X_{i} - \Xbar)^{2}}.
\]</span></p>
<p>This plug-in approach is widely applicable and tends to have excellent properties in large samples under iid data. But the simplicity of the plug-in approach also hides some features of the estimator that become more apparent when deriving the estimator more explicitly using calculus. The second approach applies the plug-in principle not to the closed-form expression for the coefficients but to the optimization problem itself. We call this the <strong>least squares</strong> estimator because it minimizes the empirical (or sample) squared prediction error, <span class="math display">\[
\bhat = \argmin_{\mb{b} \in \real^k}\; \frac{1}{n} \sum_{i=1}^{n}\bigl(Y_{i} - \mb{X}_{i}'\mb{b} \bigr)^2 = \argmin_{\mb{b} \in \real^k}\; SSR(\mb{b}),
\]</span> where, <span class="math display">\[
SSR(\mb{b}) = \sum_{i=1}^{n}\bigl(Y_{i} - \mb{X}_{i}'\mb{b} \bigr)^2
\]</span> is the sum of the squared residuals. To distinguish it from other, more complicated least squares estimators, we call this the <strong>ordinary least squares</strong> estimator, or OLS.</p>
<p>Let’s solve this minimization problem. We write down the first-order conditions as <span class="math display">\[
0=\frac{\partial SSR(\bhat)}{\partial \bfbeta} = 2 \left(\sum_{i=1}^{n} \X_{i}Y_{i}\right) - 2\left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right)\bhat.
\]</span> We can rearrange this system of equations to <span class="math display">\[
\left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right)\bhat = \left(\sum_{i=1}^{n} \X_{i}Y_{i}\right).
\]</span> To obtain the solution for <span class="math inline">\(\bhat\)</span>, notice that <span class="math inline">\(\sum_{i=1}^{n}\X_{i}\X_{i}'\)</span> is a <span class="math inline">\((k+1) \times (k+1)\)</span> matrix and <span class="math inline">\(\bhat\)</span> and <span class="math inline">\(\sum_{i=1}^{n} \X_{i}Y_{i}\)</span> are both <span class="math inline">\(k+1\)</span> length column vectors. If <span class="math inline">\(\sum_{i=1}^{n}\X_{i}\X_{i}'\)</span> is invertible, then we can multiply both sides of this equation by that inverse to arrive at <span class="math display">\[
\bhat = \left(\sum_{i=1}^n \X_i\X_i' \right)^{-1} \left(\sum_{i=1}^n \X_{i}Y_{i} \right),
\]</span> which is the same expression as the plug-in estimator (after canceling the <span class="math inline">\(1/n\)</span> terms). To confirm that we have found a minimum, we also need to check the second-order condition, <span class="math display">\[
\frac{\partial^{2} SSR(\bhat)}{\partial \bfbeta\bfbeta'} = 2\left(\sum_{i=1}^{n}\X_{i}\X_{i}'\right) &gt; 0.
\]</span> What does the matrix being “positive” mean? In matrix algebra, this condition means that the matrix <span class="math inline">\(\sum_{i=1}^{n}\X_{i}\X_{i}'\)</span> is <strong>positive definite</strong>, a condition that we discuss in <a href="#sec-rank" class="quarto-xref"><span>Section 6.4</span></a>.</p>
<p>Both the plug-in or least squares approaches yield the same estimator for the best linear predictor/population linear regression coefficients.</p>
<div id="thm-ols" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.1</strong></span> If the <span class="math inline">\(\sum_{i=1}^{n}\X_{i}\X_{i}'\)</span> is positive definite, then the ordinary least squares estimator is <span class="math display">\[
\bhat = \left(\sum_{i=1}^n \X_i\X_i' \right)^{-1} \left(\sum_{i=1}^n \X_{i}Y_{i} \right).
\]</span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
Formula for the OLS slopes
</div>
</div>
<div class="callout-body-container callout-body">
<p>Almost all regression will contain an intercept term, usually represented as a constant 1 in the covariate vector. It is also possible to obtain expressions for the OLS estimates of the intercept and variable coefficients separately. We can rewrite the best linear predictor decomposition as <span class="math display">\[
Y_{i} = \alpha + \X_{i}'\bfbeta + \e_{i}.
\]</span> Defined this way, we can write the OLS estimator for the “slopes” on <span class="math inline">\(\X_i\)</span> as the OLS estimator with all variables demeaned: <span class="math display">\[
\bhat = \left(\frac{1}{n} \sum_{i=1}^{n} (\X_{i} - \overline{\X})(\X_{i} - \overline{\X})'\right) \left(\frac{1}{n} \sum_{i=1}^{n}(\X_{i} - \overline{\X})(Y_{i} - \overline{Y})\right)
\]</span> which is the inverse of the sample covariance matrix of <span class="math inline">\(\X_i\)</span> times the sample covariance of <span class="math inline">\(\X_i\)</span> and <span class="math inline">\(Y_i\)</span>. The intercept is <span class="math display">\[
\widehat{\alpha} = \overline{Y} - \overline{\X}'\bhat.
\]</span></p>
</div>
</div>
<p>When dealing with actual data and not the population, we refer to the prediction errors <span class="math inline">\(\widehat{e}_{i} = Y_i - \X_i'\bhat\)</span> as the <strong>residuals</strong>. The predicted value itself, <span class="math inline">\(\widehat{Y}_i = \X_{i}'\bhat\)</span>, is also called the <strong>fitted value</strong>. With the population linear regression, we saw that the projection errors, <span class="math inline">\(e_i = Y_i - \X_i'\bfbeta\)</span>, were mean zero and uncorrelated with the covariates <span class="math inline">\(\E[\X_{i}e_{i}] = 0\)</span>. The residuals have a similar property with respect to the covariates in the sample: <span class="math display">\[
\sum_{i=1}^n \X_i\widehat{e}_i = 0.
\]</span> The residuals are <em>exactly</em> uncorrelated with the covariates (when the covariates include a constant/intercept term), which is a mechanical artifact of the OLS estimator.</p>
<p><a href="#fig-ssr-comp" class="quarto-xref">Figure <span>6.2</span></a> shows how OLS works in the bivariate case. It displays three possible regression lines as well as the sum of the squared residuals for each line. OLS aims to find the line that minimizes the function on the right.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-ssr-comp" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ssr-comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../Images/e4f48f2833e1f11662843925d67cf6c8.png" class="img-fluid figure-img" width="672" data-original-src="https://mattblackwell.github.io/gov2002-book/least_squares_files/figure-html/fig-ssr-comp-1.png"/>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ssr-comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 6.2: Different possible lines and their corresponding sum of squared residuals.
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="model-fit" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="model-fit"><span class="header-section-number">6.2</span> Model fit</h2>
<p>We have learned how to use OLS to obtain an estimate of the best linear predictor, but an open question is whether that prediction is any good. Does using <span class="math inline">\(\X_i\)</span> help us predict <span class="math inline">\(Y_i\)</span>? To investigate this, we consider two different prediction errors: (1) those using covariates and (2) those that do not.</p>
<p>We have already seen the prediction error when using the covariates; it is just the <strong>sum of the squared residuals</strong>, <span class="math display">\[
SSR = \sum_{i=1}^n (Y_i - \X_{i}'\bhat)^2.
\]</span> Recall that the best predictor for <span class="math inline">\(Y_i\)</span> without any covariates is simply its sample mean <span class="math inline">\(\overline{Y}\)</span>. The prediction error without covariates is what we call the <strong>total sum of squares</strong>, <span class="math display">\[
TSS = \sum_{i=1}^n (Y_i - \overline{Y})^2.
\]</span> <a href="#fig-ssr-vs-tss" class="quarto-xref">Figure <span>6.3</span></a> shows the difference between these two types of prediction errors.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-ssr-vs-tss" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ssr-vs-tss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../Images/ab8599aca3c52599119edacf9f2d3737.png" class="img-fluid figure-img" width="672" data-original-src="https://mattblackwell.github.io/gov2002-book/least_squares_files/figure-html/fig-ssr-vs-tss-1.png"/>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ssr-vs-tss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 6.3: Total sum of squares vs. the sum of squared residuals.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can use the <strong>proportion reduction in prediction error</strong> from adding those covariates to measure how much those covariates improve the regression’s predictive ability. This value, called the <strong>coefficient of determination</strong> or <span class="math inline">\(R^2\)</span>, is simply <span class="math display">\[
R^2 = \frac{TSS - SSR}{TSS} = 1-\frac{SSR}{TSS}.
\]</span> The numerator, <span class="math inline">\(TSS - SSR\)</span>, is the reduction in prediction error moving from <span class="math inline">\(\overline{Y}\)</span> to <span class="math inline">\(\X_i'\bhat\)</span> as the predictor. The denominator is the prediction error using <span class="math inline">\(\overline{Y}\)</span>. Thus, the <span class="math inline">\(R^2\)</span> value is the fraction of the total prediction error eliminated by using <span class="math inline">\(\X_i\)</span> to predict <span class="math inline">\(Y_i\)</span>. Another way to think about this value is that it measures how much less noisy the residuals are relative to the overall variation in <span class="math inline">\(Y\)</span>. One thing to note is that OLS with covariates will <em>always</em> improve in-sample fit so that <span class="math inline">\(TSS \geq SSR\)</span> even if <span class="math inline">\(\X_i\)</span> is unrelated to <span class="math inline">\(Y_i\)</span>. This phantom improvement occurs because the point of OLS is to minimize the SSR, and it will do that even if it is just chasing noise.</p>
<p>Since regression always improves in-sample fit, <span class="math inline">\(R^2\)</span> will fall between 0 and 1. A value 0 zero would indicate exactly 0 estimated coefficients on all covariates (except the intercept) so that <span class="math inline">\(Y_i\)</span> and <span class="math inline">\(\X_i\)</span> are perfectly orthogonal in the data. (This is very unlikely to occur because there will likely be some minimal but nonzero relationship by random chance.) A value of 1 indicates a perfect linear fit, which occurs when all data points are perfectly predicted by the model with zero residuals.</p>
</section>
<section id="matrix-form-of-ols" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="matrix-form-of-ols"><span class="header-section-number">6.3</span> Matrix form of OLS</h2>
<p>We derived the OLS estimator above using simple algebra and calculus, but a more common representation of the estimator relies on vectors and matrices. We usually write the linear model for a generic unit, <span class="math inline">\(Y_i = \X_i'\bfbeta + e_i\)</span>, but obviously, there are <span class="math inline">\(n\)</span> of these equations, <span class="math display">\[
\begin{aligned}
  Y_1 &amp;= \X_1'\bfbeta + e_1 \\
  Y_2 &amp;= \X_2'\bfbeta + e_2 \\
  &amp;\vdots \\
  Y_n &amp;= \X_n'\bfbeta + e_n \\
\end{aligned}
\]</span> We can write this system of equations more compactly using matrix algebra. Combining the variables here into random vectors/matrices gives us: <span class="math display">\[
\mb{Y} = \begin{pmatrix}
Y_1 \\ Y_2 \\ \vdots \\ Y_n
  \end{pmatrix}, \quad
  \mathbb{X} = \begin{pmatrix}
\X'_1 \\
\X'_2 \\
\vdots \\
\X'_n
  \end{pmatrix} =
  \begin{pmatrix}
    1 &amp; X_{11} &amp; X_{12} &amp; \cdots &amp; X_{1k} \\
    1 &amp; X_{21} &amp; X_{22} &amp; \cdots &amp; X_{2k} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
    1 &amp; X_{n1} &amp; X_{n2} &amp; \cdots &amp; X_{nk} \\
  \end{pmatrix},
  \quad
  \mb{e} = \begin{pmatrix}
e_1 \\ e_2 \\ \vdots \\ e_n
  \end{pmatrix}
\]</span> We can write the above system of equations as <span class="math display">\[
\mb{Y} = \mathbb{X}\bfbeta + \mb{e},
\]</span> Note that <span class="math inline">\(\mathbb{X}\)</span> is an <span class="math inline">\(n \times (k+1)\)</span> matrix and <span class="math inline">\(\bfbeta\)</span> is a <span class="math inline">\(k+1\)</span> length column vector.</p>
<p>Representing sums in matrix form is the critical link between the definition of OLS and matrix notation. In particular, we have <span class="math display">\[
\begin{aligned}
  \sum_{i=1}^n \X_i\X_i' &amp;= \Xmat'\Xmat \\
  \sum_{i=1}^n \X_iY_i &amp;= \Xmat'\mb{Y},
\end{aligned}
\]</span> which means we can write the OLS estimator in the more recognizable form as <span class="math display">\[
\bhat = \left( \mathbb{X}'\mathbb{X} \right)^{-1} \mathbb{X}'\mb{Y}.
\]</span></p>
<p>We can of course also define the vector of residuals, <span class="math display">\[
\widehat{\mb{e}} = \mb{Y} - \mathbb{X}\bhat = \left[
\begin{array}{c}
    Y_1 \\
    Y_2 \\
    \vdots \\
    Y_n
    \end{array}
\right] -
\left[
\begin{array}{c}
   1\widehat{\beta}_0 + X_{11}\widehat{\beta}_1 + X_{12}\widehat{\beta}_2 + \dots + X_{1k}\widehat{\beta}_k \\
   1\widehat{\beta}_0 + X_{21}\widehat{\beta}_1 + X_{22}\widehat{\beta}_2 + \dots + X_{2k}\widehat{\beta}_k \\
   \vdots \\
   1\widehat{\beta}_0 + X_{n1}\widehat{\beta}_1 + X_{n2}\widehat{\beta}_2 + \dots + X_{nk}\widehat{\beta}_k
\end{array}
\right],
\]</span> and so the sum of the squared residuals in this case becomes <span class="math display">\[
SSR(\bfbeta) = \Vert\mb{Y} - \mathbb{X}\bfbeta\Vert^{2} = (\mb{Y} - \mathbb{X}\bfbeta)'(\mb{Y} - \mathbb{X}\bfbeta),
\]</span> where the double vertical lines are the Euclidean norm of the argument, <span class="math inline">\(\Vert \mb{z} \Vert = \sqrt{\sum_{i=1}^n z_i^{2}}\)</span>. The OLS minimization problem, then, is <span class="math display">\[
\bhat = \argmin_{\mb{b} \in \mathbb{R}^{(k+1)}}\; \Vert\mb{Y} - \mathbb{X}\mb{b}\Vert^{2}
\]</span> Finally, we can write the lack of correlation of the covariates and the residuals as <span class="math display">\[
\mathbb{X}'\widehat{\mb{e}} = \sum_{i=1}^{n} \X_{i}\widehat{e}_{i} = 0,
\]</span> which also implies these vectors are <strong>orthogonal</strong>.</p>
</section>
<section id="sec-rank" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="sec-rank"><span class="header-section-number">6.4</span> Rank, linear independence, and multicollinearity</h2>
<p>We noted that the OLS estimator exists when <span class="math inline">\(\sum_{i=1}^n \X_i\X_i'\)</span> is positive definite or that there is “no multicollinearity.” This assumption is equivalent to saying that the matrix <span class="math inline">\(\mathbb{X}\)</span> is full column rank, meaning that <span class="math inline">\(\text{rank}(\mathbb{X}) = (k+1)\)</span>, where <span class="math inline">\(k+1\)</span> is the number of columns of <span class="math inline">\(\mathbb{X}\)</span>. Recall from matrix algebra that the column rank is the number of linearly independent columns in the matrix, and <strong>linear independence</strong> means that <span class="math inline">\(\mathbb{X}\mb{b} = 0\)</span> if and only if <span class="math inline">\(\mb{b}\)</span> is a column vector of 0s. In other words, we have <span class="math display">\[
b_{1}\mathbb{X}_{1} + b_{2}\mathbb{X}_{2} + \cdots + b_{k+1}\mathbb{X}_{k+1} = 0 \quad\iff\quad b_{1} = b_{2} = \cdots = b_{k+1} = 0,
\]</span> where <span class="math inline">\(\mathbb{X}_j\)</span> is the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\mathbb{X}\)</span>. Thus, full column rank says that all the columns are linearly independent or that there is no “multicollinearity.”</p>
<p>Could this be violated? Suppose we accidentally included a linear function of one variable so that <span class="math inline">\(\mathbb{X}_2 = 2\mathbb{X}_1\)</span>. We then have <span class="math display">\[
\begin{aligned}
  \mathbb{X}\mb{b} &amp;= b_{1}\mathbb{X}_{1} + b_{2}2\mathbb{X}_1+ b_{3}\mathbb{X}_{3}+ \cdots + b_{k+1}\mathbb{X}_{k+1} \\
  &amp;= (b_{1} + 2b_{2})\mathbb{X}_{1} + b_{3}\mathbb{X}_{3} + \cdots + b_{k+1}\mathbb{X}_{k+1}
\end{aligned}
\]</span> In this case, this expression equals 0 when <span class="math inline">\(b_3 = b_4 = \cdots = b_{k+1} = 0\)</span> and <span class="math inline">\(b_1 = -2b_2\)</span>. Thus, the collection of columns is linearly dependent, so we know that the rank of <span class="math inline">\(\mathbb{X}\)</span> must be less than full column rank (that is, less than <span class="math inline">\(k+1\)</span>). Hopefully it is also clear that if we removed the problematic column <span class="math inline">\(\mathbb{X}_2\)</span>, the resulting matrix would have <span class="math inline">\(k\)</span> linearly independent columns, implying that <span class="math inline">\(\mathbb{X}\)</span> is rank <span class="math inline">\(k\)</span>.</p>
<p>Why does this rank condition matter for the OLS estimator? In short, linear independence of the columns of <span class="math inline">\(\Xmat\)</span> ensures that the inverse <span class="math inline">\((\Xmat'\Xmat)^{-1}\)</span> exists and so does <span class="math inline">\(\bhat\)</span>. This is because <span class="math inline">\(\Xmat\)</span> is of full column rank if and only if <span class="math inline">\(\Xmat'\Xmat\)</span> is non-singular and a matrix is invertible if and only if it is non-singular. This full rank condition further implies that <span class="math inline">\(\Xmat'\Xmat = \sum_{i=1}^{n}\X_{i}\X_{i}'\)</span> is positive definite, implying that the estimator is truly finding the minimal sum of squared residuals.</p>
<p>What are common situations that lead to violations of no multicollinearity? We have seen one above, with one variable being a linear function of another. But this problem can come out in more subtle ways. Suppose we have a set of dummy variables corresponding to a single categorical variable, like the region of the world. This might mean we have <span class="math inline">\(X_{i1} = 1\)</span> for units in Asia (0 otherwise), <span class="math inline">\(X_{i2} = 1\)</span> for units in Europe (0 otherwise), <span class="math inline">\(X_{i3} = 1\)</span> for units in Africa (0 otherwise), and <span class="math inline">\(X_{i4} = 1\)</span> for units in the Americas (0 otherwise), and <span class="math inline">\(X_{i5} = 1\)</span> for countries in Oceania (0 otherwise). Each unit has to be in exactly one of these five regions, so there is a linear dependence between these variables, <span class="math display">\[
X_{i5} = 1 - X_{i1} - X_{i2} - X_{i3} - X_{i4}.
\]</span> That is, if a unit is not in Asia, Europe, Africa, or the Americas, we know it is in Oceania. We would get a linear dependence by including all of these variables in our regression with an intercept. (Note the 1 in the relationship between <span class="math inline">\(X_{i5}\)</span> and the other variables, the reason why there will be linear dependence when including a constant.) Thus, we usually omit one dummy variable from each categorical variable. In that case, the coefficients on the remaining dummies are differences in means between that category and the omitted one (perhaps conditional on other variables included, if included). So if we omitted <span class="math inline">\(X_{i5}\)</span> (Oceania), then the coefficient on <span class="math inline">\(X_{i1}\)</span> would be the difference in mean outcomes between units in Asia and Oceania.</p>
<p>Collinearity can also occur when including both an intercept term and a variable that does not vary. This issue can often happen if we mistakenly subset our data, for example in this case if we subsetted the data to only the Asian units but still included the Asian dummy variable in the regression.</p>
<p>Finally, note that most statistical software packages will “solve” the multicollinearity by arbitrarily removing as many linearly dependent covariates as is necessary to achieve full rank. R will show the estimated coefficients as <code>NA</code> in those cases.</p>
</section>
<section id="ols-coefficients-for-binary-and-categorical-regressors" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="ols-coefficients-for-binary-and-categorical-regressors"><span class="header-section-number">6.5</span> OLS coefficients for binary and categorical regressors</h2>
<p>Suppose that the covariates include just the intercept and a single binary variable, <span class="math inline">\(\X_i = (1\; X_{i})'\)</span>, where <span class="math inline">\(X_i \in \{0,1\}\)</span>. In other words, the right-hand side contains only one covariate, an indicator variable. In this case, the OLS coefficient on <span class="math inline">\(X_i\)</span>, <span class="math inline">\(\widehat{\beta_{1}}\)</span>, is exactly equal to the difference in sample means of <span class="math inline">\(Y_i\)</span> in the <span class="math inline">\(X_i = 1\)</span> group and the <span class="math inline">\(X_i = 0\)</span> group: <span class="math display">\[
\widehat{\beta}_{1} = \frac{\sum_{i=1}^{n} X_{i}Y_{i}}{\sum_{i=1}^{n} X_{i}} - \frac{\sum_{i=1}^{n} (1 - X_{i})Y_{i}}{\sum_{i=1}^{n} 1- X_{i}} = \overline{Y}_{X =1} - \overline{Y}_{X=0}
\]</span> This very useful result is not an approximation: it holds exactly for any sample size.</p>
<p>We can generalize this idea to discrete variables more broadly. Suppose we have our region variables from the last section and include in our covariates a constant and the dummies for Asia, Europe, Africa, and the Americas (with Oceania again being the omitted variable/category). Then the coefficient on the West dummy will be <span class="math display">\[
\widehat{\beta}_{\text{Asia}} = \overline{Y}_{\text{Asia}} - \overline{Y}_{\text{Oceania}},
\]</span> which is exactly the difference in sample means of <span class="math inline">\(Y_i\)</span> between Asian units and units in Oceania.</p>
<p>Note that these interpretations only hold when the regression consists solely of the binary variable or the set of categorical dummy variables. These exact relationships fail when other covariates are added to the model.</p>
</section>
<section id="projection-and-geometry-of-least-squares" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="projection-and-geometry-of-least-squares"><span class="header-section-number">6.6</span> Projection and geometry of least squares</h2>
<p>OLS has a very nice geometric interpretation that adds a lot of intuition for various aspects of the method. In this geometric approach, we view <span class="math inline">\(\mb{Y}\)</span> as an <span class="math inline">\(n\)</span>-dimensional vector in <span class="math inline">\(\mathbb{R}^n\)</span>. As we saw above, OLS in matrix form is about finding a linear combination of the covariate matrix <span class="math inline">\(\Xmat\)</span> closest to this vector in terms of the Euclidean distance, which is just the sum of squares.</p>
<p>Let <span class="math inline">\(\mathcal{C}(\Xmat) = \{\Xmat\mb{b} : \mb{b} \in \mathbb{R}^(k+1)\}\)</span> be the <strong>column space</strong> of the matrix <span class="math inline">\(\Xmat\)</span>. This set is all linear combinations of the columns of <span class="math inline">\(\Xmat\)</span> or the set of all possible linear predictions we could obtain from <span class="math inline">\(\Xmat\)</span>. Note that the OLS fitted values, <span class="math inline">\(\Xmat\bhat\)</span>, are in this column space. If, as we assume, <span class="math inline">\(\Xmat\)</span> has full column rank of <span class="math inline">\(k+1\)</span>, then the column space <span class="math inline">\(\mathcal{C}(\Xmat)\)</span> will be a <span class="math inline">\(k+1\)</span>-dimensional surface inside of the larger <span class="math inline">\(n\)</span>-dimensional space. If <span class="math inline">\(\Xmat\)</span> has two columns, the column space will be a plane.</p>
<p>Another interpretation of the OLS estimator is that it finds the linear predictor as the closest point in the column space of <span class="math inline">\(\Xmat\)</span> to the outcome vector <span class="math inline">\(\mb{Y}\)</span>. This is called the <strong>projection</strong> of <span class="math inline">\(\mb{Y}\)</span> onto <span class="math inline">\(\mathcal{C}(\Xmat)\)</span>. <a href="#fig-projection" class="quarto-xref">Figure <span>6.4</span></a> shows this projection for a case with <span class="math inline">\(n=3\)</span> and 2 columns in <span class="math inline">\(\Xmat\)</span>. The shaded blue region represents the plane of the column space of <span class="math inline">\(\Xmat\)</span>, and <span class="math inline">\(\Xmat\bhat\)</span> is the closest point to <span class="math inline">\(\mb{Y}\)</span> in that space. This illustrates the whole idea of the OLS estimator: find the linear combination of the columns of <span class="math inline">\(\Xmat\)</span> (a point in the column space) that minimizes the Euclidean distance between that point and the outcome vector (the sum of squared residuals).</p>
<div id="fig-projection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-projection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../Images/d35d2015fb109b0c81bb2430d5ee3f58.png" class="img-fluid figure-img" data-original-src="https://mattblackwell.github.io/gov2002-book/assets/img/projection-drawing.png"/>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-projection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 6.4: Projection of Y on the column space of the covariates.
</figcaption>
</figure>
</div>
<p>This figure shows that the residual vector, which is the difference between the <span class="math inline">\(\mb{Y}\)</span> vector and the projection <span class="math inline">\(\Xmat\bhat\)</span>, is perpendicular or orthogonal to the column space of <span class="math inline">\(\Xmat\)</span>. This orthogonality is a consequence of the residuals being orthogonal to all the columns of <span class="math inline">\(\Xmat\)</span>, <span class="math display">\[
\Xmat'\mb{e} = 0,
\]</span> as we established above. Being orthogonal to all the columns means it will also be orthogonal to all linear combinations of the columns.</p>
</section>
<section id="projection-and-annihilator-matrices" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="projection-and-annihilator-matrices"><span class="header-section-number">6.7</span> Projection and annihilator matrices</h2>
<p>With the idea of projection to the column space of <span class="math inline">\(\Xmat\)</span> established, we can define a way to project any vector into that space. The <span class="math inline">\(n\times n\)</span> <strong>projection matrix,</strong> <span class="math display">\[
\mb{P}_{\Xmat} = \Xmat (\Xmat'\Xmat)^{-1} \Xmat',
\]</span> projects a vector into <span class="math inline">\(\mathcal{C}(\Xmat)\)</span>. In particular, we can see that this gives us the fitted values for <span class="math inline">\(\mb{Y}\)</span>: <span class="math display">\[
\mb{P}_{\Xmat}\mb{Y} = \Xmat (\Xmat'\Xmat)^{-1} \Xmat'\mb{Y} = \Xmat\bhat.
\]</span> Because we sometimes write the linear predictor as <span class="math inline">\(\widehat{\mb{Y}} = \Xmat\bhat\)</span>, the projection matrix is also called the <strong>hat matrix</strong>. With either name, multiplying a vector by <span class="math inline">\(\mb{P}_{\Xmat}\)</span> gives the best linear predictor of that vector as a function of <span class="math inline">\(\Xmat\)</span>. Intuitively, any vector that is already a linear combination of the columns of <span class="math inline">\(\Xmat\)</span> (so is in <span class="math inline">\(\mathcal{C}(\Xmat)\)</span>) should be unaffected by this projection: the closest point in <span class="math inline">\(\mathcal{C}(\Xmat)\)</span> to a point already in <span class="math inline">\(\mathcal{C}(\Xmat)\)</span> is itself. We can also see this algebraically for any linear combination <span class="math inline">\(\Xmat\mb{c}\)</span>, <span class="math display">\[
\mb{P}_{\Xmat}\Xmat\mb{c} = \Xmat (\Xmat'\Xmat)^{-1} \Xmat'\Xmat\mb{c} = \Xmat\mb{c},
\]</span> because <span class="math inline">\((\Xmat'\Xmat)^{-1} \Xmat'\Xmat\)</span> simplifies to the identity matrix. In particular, the projection of <span class="math inline">\(\Xmat\)</span> onto itself is just itself: <span class="math inline">\(\mb{P}_{\Xmat}\Xmat = \Xmat\)</span>.</p>
<p>The second matrix related to projection is the <strong>annihilator matrix</strong>, <span class="math display">\[
\mb{M}_{\Xmat} = \mb{I}_{n} - \mb{P}_{\Xmat},
\]</span> which projects any vector into the orthogonal complement to the column space of <span class="math inline">\(\Xmat\)</span>, <span class="math display">\[
\mathcal{C}^{\perp}(\Xmat) = \{\mb{c} \in \mathbb{R}^n\;:\; \Xmat\mb{c} = 0 \}.
\]</span> This matrix is called the annihilator matrix because applying it to any linear combination of <span class="math inline">\(\Xmat\)</span>, gives us 0: <span class="math display">\[
\mb{M}_{\Xmat}\Xmat\mb{c} = \Xmat\mb{c} - \mb{P}_{\Xmat}\Xmat\mb{c} = \Xmat\mb{c} - \Xmat\mb{c} = 0.
\]</span> Note that <span class="math inline">\(\mb{M}_{\Xmat}\Xmat = 0\)</span>. Why should we care about this matrix? Perhaps a more evocative name might be the <strong>residual maker</strong> since it makes residuals when applied to <span class="math inline">\(\mb{Y}\)</span>, <span class="math display">\[
\mb{M}_{\Xmat}\mb{Y} = (\mb{I}_{n} - \mb{P}_{\Xmat})\mb{Y} = \mb{Y} - \mb{P}_{\Xmat}\mb{Y} = \mb{Y} - \Xmat\bhat = \widehat{\mb{e}}.
\]</span></p>
<p>The projection matrix has several useful properties:</p>
<ul>
<li><p><span class="math inline">\(\mb{P}_{\Xmat}\)</span> and <span class="math inline">\(\mb{M}_{\Xmat}\)</span> are <strong>idempotent</strong>, which means that when applied to itself, it simply returns itself: <span class="math inline">\(\mb{P}_{\Xmat}\mb{P}_{\Xmat} = \mb{P}_{\Xmat}\)</span> and <span class="math inline">\(\mb{M}_{\Xmat}\mb{M}_{\Xmat} = \mb{M}_{\Xmat}\)</span>.</p></li>
<li><p><span class="math inline">\(\mb{P}_{\Xmat}\)</span> and <span class="math inline">\(\mb{M}_{\Xmat}\)</span> are symmetric <span class="math inline">\(n \times n\)</span> matrices so that <span class="math inline">\(\mb{P}_{\Xmat}' = \mb{P}_{\Xmat}\)</span> and <span class="math inline">\(\mb{M}_{\Xmat}' = \mb{M}_{\Xmat}\)</span>.</p></li>
<li><p>The rank of <span class="math inline">\(\mb{P}_{\Xmat}\)</span> is <span class="math inline">\(k+1\)</span> (the number of columns of <span class="math inline">\(\Xmat\)</span>) and the rank of <span class="math inline">\(\mb{M}_{\Xmat}\)</span> is <span class="math inline">\(n - k - 1\)</span>.</p></li>
</ul>
<p>We can use the projection and annihilator matrices to arrive at an orthogonal decomposition of the outcome vector: <span class="math display">\[
\mb{Y} = \Xmat\bhat + \widehat{\mb{e}} = \mb{P}_{\Xmat}\mb{Y} + \mb{M}_{\Xmat}\mb{Y}.
\]</span></p>
</section>
<section id="residual-regression" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="residual-regression"><span class="header-section-number">6.8</span> Residual regression</h2>
<p>There are many situations where we can partition the covariates into two groups, and we might wonder if it is possible to express or calculate the OLS coefficients for just one set of covariates. In particular, let the columns of <span class="math inline">\(\Xmat\)</span> be partitioned into <span class="math inline">\([\Xmat_{1} \Xmat_{2}]\)</span>, so that the linear prediction we are estimating is <span class="math display">\[
\mb{Y} = \Xmat_{1}\bfbeta_{1} + \Xmat_{2}\bfbeta_{2} + \mb{e},
\]</span> with estimated coefficients and residuals <span class="math display">\[
\mb{Y} = \Xmat_{1}\bhat_{1} + \Xmat_{2}\bhat_{2} + \widehat{\mb{e}}.
\]</span></p>
<p>We now document another way to obtain the estimator <span class="math inline">\(\bhat_1\)</span> from this regression using a technique called <strong>residual regression</strong>, <strong>partitioned regression</strong>, or the <strong>Frisch-Waugh-Lovell theorem</strong>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"/>
</div>
<div class="callout-title-container flex-fill">
Residual regression approach
</div>
</div>
<div class="callout-body-container callout-body">
<p>The residual regression approach is:</p>
<ol type="1">
<li>Use OLS to regress <span class="math inline">\(\mb{Y}\)</span> on <span class="math inline">\(\Xmat_2\)</span> and obtain residuals <span class="math inline">\(\widetilde{\mb{e}}_2\)</span>.</li>
<li>Use OLS to regress each column of <span class="math inline">\(\Xmat_1\)</span> on <span class="math inline">\(\Xmat_2\)</span> and obtain residuals <span class="math inline">\(\widetilde{\Xmat}_1\)</span>.</li>
<li>Use OLS to regress <span class="math inline">\(\widetilde{\mb{e}}_{2}\)</span> on <span class="math inline">\(\widetilde{\Xmat}_1\)</span>.</li>
</ol>
</div>
</div>
<div id="thm-fwl" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.2 (Frisch-Waugh-Lovell)</strong></span> The OLS coefficients from a regression of <span class="math inline">\(\widetilde{\mb{e}}_{2}\)</span> on <span class="math inline">\(\widetilde{\Xmat}_1\)</span> are equivalent to the coefficients on <span class="math inline">\(\Xmat_{1}\)</span> from the regression of <span class="math inline">\(\mb{Y}\)</span> on both <span class="math inline">\(\Xmat_{1}\)</span> and <span class="math inline">\(\Xmat_2\)</span>.</p>
</div>
<p>An implication of this theorem is that the regression coefficient for a given variable captures the relationship between the residual variation in the outcome and that variable after accounting for the other covariates. In particular, this coefficient focuses on the variation orthogonal to those other covariates.</p>
<p>While perhaps unexpected, this result may not appear particularly useful. We can just run the long regression, right? But this trick can be very handy when <span class="math inline">\(\Xmat_2\)</span> consists of dummy variables (or “fixed effects”) for a categorical variable with many categories. For example, suppose <span class="math inline">\(\Xmat_2\)</span> consists of indicators for the county of residence for a respondent. In that case, that will have over 3,000 columns, meaning that direct calculation of the <span class="math inline">\(\bhat = (\bhat_{1}, \bhat_{2})\)</span> will require inverting a matrix that is bigger than <span class="math inline">\(3,000 \times 3,000\)</span>. Computationally, this process will be very slow. But above, we saw that predictions of an outcome on a categorical variable are just the sample mean within each level of the variable. Thus, in this case, the residuals <span class="math inline">\(\widetilde{\mb{e}}_2\)</span> and <span class="math inline">\(\Xmat_1\)</span> can be computed by demeaning the outcome and <span class="math inline">\(\Xmat_1\)</span> within levels of the dummies in <span class="math inline">\(\Xmat_2\)</span>, which can be considerably faster computationally.</p>
<p>Finally, using residual regression allows researchers to visualize the conditional relationships between the outcome and a single independent variable after adjusting for other covariates. In particular, one can check the relationship using this approach with a scatterplot of <span class="math inline">\(\widetilde{\mb{e}}_2\)</span> on <span class="math inline">\(\Xmat_1\)</span> (when it is a single column). This residualized scatterplot allows researchers to check if this conditional relationship appears linear or should be modeled in another way.</p>
</section>
<section id="outliers-leverage-points-and-influential-observations" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="outliers-leverage-points-and-influential-observations"><span class="header-section-number">6.9</span> Outliers, leverage points, and influential observations</h2>
<p>Given that OLS finds the coefficients that minimize the sum of the squared residuals, asking how much impact each residual has on that solution is very helpful. Let <span class="math inline">\(\bhat_{(-i)}\)</span> be the OLS estimates if we omit unit <span class="math inline">\(i\)</span>. Intuitively, <strong>influential observations</strong> should significantly impact the estimated coefficients so that <span class="math inline">\(\bhat_{(-i)} - \bhat\)</span> is large in absolute value.</p>
<p>Under what conditions do we have influential observations? OLS tries to minimize the sum of <strong>squared</strong> residuals, so it will move more in order to shrink larger residuals versus smaller ones. Where are large residuals likely to occur? Well, notice that any OLS regression line with a constant will exactly pass through the means of the outcome and the covariates: <span class="math inline">\(\overline{Y} = \overline{\X}\bhat\)</span>. Thus, by definition, this means that, when an observation is close to the average of the covariates, <span class="math inline">\(\overline{\X}\)</span>, it cannot have that much influence because OLS forces the regression line to go through <span class="math inline">\(\overline{Y}\)</span>. Thus, influential points will have two properties:</p>
<ol type="1">
<li>Have high <strong>leverage</strong>, where leverage roughly measures how far <span class="math inline">\(\X_i\)</span> is from <span class="math inline">\(\overline{\X}\)</span>, and</li>
<li>Be an <strong>outlier</strong> in the sense of having a large residual (if left out of the regression).</li>
</ol>
<p>We’ll take each of these in turn.</p>
<section id="sec-leverage" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="sec-leverage"><span class="header-section-number">6.9.1</span> Leverage points</h3>
<p>We can define the <strong>leverage</strong> of an observation by <span class="math display">\[
h_{ii} = \X_{i}'\left(\Xmat'\Xmat\right)^{-1}\X_{i},
\]</span> which is the <span class="math inline">\(i\)</span>th diagonal entry of the projection matrix, <span class="math inline">\(\mb{P}_{\Xmat}\)</span>. Notice that <span class="math display">\[
\widehat{\mb{Y}} = \mb{P}_{\Xmat}\mb{Y} \qquad \implies \qquad \widehat{Y}_i = \sum_{j=1}^n h_{ij}Y_j,
\]</span> so that <span class="math inline">\(h_{ij}\)</span> is the importance of observation <span class="math inline">\(j\)</span> for the fitted value for observation <span class="math inline">\(i\)</span>. The leverage, then, is the importance of the observation for its own fitted value. We can also interpret these values in terms of the distribution of <span class="math inline">\(\X_{i}\)</span>. Roughly speaking, these values are the weighted distance between <span class="math inline">\(\X_i\)</span> and <span class="math inline">\(\overline{\X}\)</span>, where the weights normalize to the empirical variance/covariance structure of the covariates (so that the scale of each covariate is roughly the same). We can see this most clearly when we fit a simple linear regression (with one covariate and an intercept) with OLS when the leverage is <span class="math display">\[
h_{ii} = \frac{1}{n} + \frac{(X_i - \overline{X})^2}{\sum_{j=1}^n (X_j - \overline{X})^2}
\]</span></p>
<p>Leverage values have three key properties:</p>
<ol type="1">
<li><span class="math inline">\(0 \leq h_{ii} \leq 1\)</span></li>
<li><span class="math inline">\(h_{ii} \geq 1/n\)</span> if the model contains an intercept</li>
<li><span class="math inline">\(\sum_{i=1}^{n} h_{ii} = k + 1\)</span></li>
</ol>
</section>
<section id="outliers-and-leave-one-out-regression" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="outliers-and-leave-one-out-regression"><span class="header-section-number">6.9.2</span> Outliers and leave-one-out regression</h3>
<p>In the context of OLS, an <strong>outlier</strong> is an observation with a large prediction error for a particular OLS specification. <a href="#fig-outlier" class="quarto-xref">Figure <span>6.5</span></a> shows an example of an outlier.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-outlier" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-outlier-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../Images/e08bf8a216cac143b9649600deb911a8.png" class="img-fluid figure-img" width="672" data-original-src="https://mattblackwell.github.io/gov2002-book/least_squares_files/figure-html/fig-outlier-1.png"/>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-outlier-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 6.5: An example of an outlier.
</figcaption>
</figure>
</div>
</div>
</div>
<p>Intuitively, it seems as though we could use the residual <span class="math inline">\(\widehat{e}_i\)</span> to assess the prediction error for a given unit. But the residuals are not valid predictions because the OLS estimator is designed to make those as small as possible (in machine learning parlance, these were in the training set). In particular, if an outlier is influential, we already noted that it might “pull” the regression line toward it, and the resulting residual might be pretty small.</p>
<p>To assess prediction errors more cleanly, we can use <strong>leave-one-out regression</strong> (LOO), which regresses <span class="math inline">\(\mb{Y}_{(-i)}\)</span> on <span class="math inline">\(\Xmat_{(-i)}\)</span>, where these omit unit <span class="math inline">\(i\)</span>: <span class="math display">\[
\bhat_{(-i)} = \left(\Xmat'_{(-i)}\Xmat_{(-i)}\right)^{-1}\Xmat_{(-i)}\mb{Y}_{(-i)}.
\]</span> We can then calculate LOO prediction errors as <span class="math display">\[
\widetilde{e}_{i} = Y_{i} - \X_{i}'\bhat_{(-i)}.
\]</span> Calculating these LOO prediction errors for each unit appears to be computationally costly because it seems as though we have to fit OLS <span class="math inline">\(n\)</span> times. Fortunately, there is a closed-form expression for the LOO coefficients and prediction errors in terms of the original regression, <span id="eq-loo-coefs"><span class="math display">\[
\bhat_{(-i)} = \bhat - \left( \Xmat'\Xmat\right)^{-1}\X_i\widetilde{e}_i \qquad \widetilde{e}_i = \frac{\widehat{e}_i}{1 - h_{ii}}.
\tag{6.1}\]</span></span> This shows that the LOO prediction errors will differ from the residuals when the leverage of a unit is high. This makes sense! We said earlier that observations with low leverage would be close to <span class="math inline">\(\overline{\X}\)</span>, where the outcome values have relatively little impact on the OLS fit (because the regression line must go through <span class="math inline">\(\overline{Y}\)</span>).</p>
</section>
<section id="influential-observations" class="level3" data-number="6.9.3">
<h3 data-number="6.9.3" class="anchored" data-anchor-id="influential-observations"><span class="header-section-number">6.9.3</span> Influential observations</h3>
<p>An influential observation (also sometimes called an influential point) is a unit that has the power to change the coefficients and fitted values for a particular OLS specification. <a href="#fig-influence" class="quarto-xref">Figure <span>6.6</span></a> shows an example of such an influence point.</p>
<div class="cell">
<div class="cell-output-display">
<div id="fig-influence" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-influence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="../Images/fcabb6ae8553589d6ba01eae403b266f.png" class="img-fluid figure-img" width="672" data-original-src="https://mattblackwell.github.io/gov2002-book/least_squares_files/figure-html/fig-influence-1.png"/>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-influence-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure 6.6: An example of an influence point.
</figcaption>
</figure>
</div>
</div>
</div>
<p>One measure of influence, called DFBETA<span class="math inline">\(_i\)</span>, measures how much <span class="math inline">\(i\)</span> changes the estimated coefficient vector <span class="math display">\[
\bhat - \bhat_{(-i)} = \left( \Xmat'\Xmat\right)^{-1}\X_i\widetilde{e}_i,
\]</span> so there is one value for each observation-covariate pair. When divided by the standard error of the estimated coefficients, this is called DFBETA<strong>S</strong> (where the “S” is for standardized). These are helpful if we focus on a particular coefficient.</p>
<p>When we want to summarize how much an observation matters for the fit, we can use a compact measure of the influence of an observation by comparing the fitted value from the entire sample to the fitted value from the leave-one-out regression. Using the DFBETA above, we have <span class="math display">\[
\widehat{Y}_i - \X_{i}\bhat_{(-1)} = \X_{i}'(\bhat -\bhat_{(-1)}) = \X_{i}'\left( \Xmat'\Xmat\right)^{-1}\X_i\widetilde{e}_i = h_{ii}\widetilde{e}_i,
\]</span> so the influence of an observation is its leverage multiplied by how much of an outlier it is. This value is sometimes called DFFIT (difference in fit). One transformation of this quantity, <strong>Cook’s distance</strong>, standardizes this by the sum of the squared residuals: <span class="math display">\[
D_i = \frac{n-k-1}{k+1}\frac{h_{ii}\widetilde{e}_{i}^{2}}{\widehat{\mb{e}}'\widehat{\mb{e}}}.
\]</span> Different cutoffs exist for identifying “influential” observations, but they tend to be ad hoc. In any case, the more important question is “how much does this observation matter for my substantive interpretation” rather than the narrow question of a particular threshold.</p>
<p>It’s all well and good to find influential observations, but what should be done about them? The first thing to check is that the data is not corrupted somehow. Influence points sometimes occur because of a coding or data entry error. We may consider removing the observation if the error appears in the data acquired from another source but exercise transparency if this appears to be the case. Another approach is to consider a transformation of the dependent or independent variables, like taking the natural logarithm, that might dampen the effects of outliers. Finally, consider using methods that are robust to outliers such as least absolute deviations or least trimmed squares.</p>
</section>
</section>
<section id="summary" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="summary"><span class="header-section-number">6.10</span> Summary</h2>
<p>In this chapter, we introduced the <strong>ordinary least squares</strong> estimator, which finds the linear function of the <span class="math inline">\(\X_i\)</span> that minimizes the sum of the squared residuals and is the sample version of the best linear predictor in the last chapter. The <span class="math inline">\(R^2\)</span> statistic assesses the in-sample <strong>model fit</strong> of OLS by comparing how much better it predicts the outcome compared to a simple baseline predictor of the sample mean of the outcome. OLS can also be written in a very compact manner using matrix algebra, which allows us to understand the geometry of OLS as a <strong>projection</strong> of the outcome into space of linear functions of the independent variables. The <strong>Frisch-Waugh-Lovell theorem</strong> describes a residual regression approach to obtaining OLS estimates for subsets of coefficients, which can be helpful for computational efficiency or data visualization. Lastly, influential observations are those that alter the estimated coefficients when they are omitted from the OLS estimation, and there are several metrics that help to assess this. In the next chapter, we move from the mechanical properties to the statistical properties of OLS: unbiasedness, consistency, and asymptotic normality.</p>


</section>

    
</body>
</html>