- en: '2\. Least squares: geometric, algebraic, and numerical aspects#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2. 最小二乘法：几何、代数和数值方面#
- en: 原文：[https://mmids-textbook.github.io/chap02_ls/00_intro/roch-mmids-ls-intro.html](https://mmids-textbook.github.io/chap02_ls/00_intro/roch-mmids-ls-intro.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap02_ls/00_intro/roch-mmids-ls-intro.html](https://mmids-textbook.github.io/chap02_ls/00_intro/roch-mmids-ls-intro.html)
- en: 'In this chapter, we introduce the linear least squares problem, an optimization
    problem that aims to find the best-fitting linear model for a given set of data
    points. It is closely connected to regression analysis, one of the most fundamental
    and widely used statistical techniques. We develop the mathematical concepts at
    its foundation, in particular, emphasizing the various facets of the problem:
    the algebra, the geometry, and the numerics. Here is a more detailed overview
    of the main sections of the chapter.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了线性最小二乘问题，这是一个优化问题，旨在为给定的一组数据点找到最佳拟合的线性模型。它与回归分析密切相关，回归分析是最基本且最广泛使用的统计技术之一。我们发展了其基础上的数学概念，特别是强调问题的各个方面：代数、几何和数值。以下是本章主要部分的更详细概述。
- en: '*“Background: review of vector spaces and matrix inverses”* This section reviews
    fundamental concepts in linear algebra that are essential for data science. It
    begins by defining linear subspaces and related notions like span, linear independence,
    and bases. The dimension theorem is stated, establishing that the dimension of
    a subspace is well-defined. Finally, the section discusses inverses of square
    matrices, proving that a square matrix is invertible if and only if it is nonsingular,
    and that the solution to the equation \(A\mathbf{x}=\mathbf{b}\) is unique and
    given by \(\mathbf{x}=A^{-1}\mathbf{b}\) when \(A\) is nonsingular.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*“背景：向量空间和矩阵逆的回顾”* 本节回顾了线性代数中的基本概念，这些概念对于数据科学至关重要。它首先定义了线性子空间和相关概念，如张量积、线性无关性和基。陈述了维度定理，确立了子空间的维度是良好定义的。最后，本节讨论了方阵的逆，证明了方阵可逆当且仅当它是非奇异的，并且当\(A\)是非奇异的时，方程\(A\mathbf{x}=\mathbf{b}\)的解是唯一的，并且由\(\mathbf{x}=A^{-1}\mathbf{b}\)给出。'
- en: '*“The geometry of least squares”* This section discusses the geometry of least
    squares. The important concept of orthogonality is reviewed, along with key results
    like Pythagoras’ theorem, the Cauchy-Schwarz inequality, and the Gram-Schmidt
    Theorem regarding the existence of orthonormal bases. It then introduces the concept
    of orthogonal projection, which is the process of finding the closest vector in
    a linear subspace to a given vector. Finally, the section applies these concepts
    to solve overdetermined systems using the least squares method, showing that the
    solution satisfies the normal equations.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*“最小二乘法的几何”* 本节讨论了最小二乘法的几何。回顾了正交性的重要概念，以及诸如毕达哥拉斯定理、柯西-施瓦茨不等式和关于正交基存在性的Gram-Schmidt定理等关键结果。然后，它介绍了正交投影的概念，即找到线性子空间中与给定向量最近的向量的过程。最后，本节将这些概念应用于使用最小二乘法解决超定系统，表明解满足正则方程。'
- en: '*“QR decomposition and Householder transformations”* In this section, the Gram-Schmidt
    algorithm is introduced as a way to obtain an orthonormal basis from a set of
    linearly independent vectors, and its matrix factorization perspective is presented
    as the QR decomposition. The section then shows how the QR decomposition can be
    used to solve linear least squares problems. While the Gram-Schmidt algorithm
    is geometrically intuitive, it can have numerical issues. Householder reflections,
    on the other hand, provide a more numerically stable method for computing QR decompositions.
    These reflections are orthogonal transformations that can be used to introduce
    zeros below the diagonal of a matrix, ultimately leading to its triangularization.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*“QR分解和Householder变换”* 在本节中，介绍了Gram-Schmidt算法作为从一组线性无关的向量集中获得正交基的方法，并将其矩阵分解视角作为QR分解提出。然后，本节展示了如何使用QR分解来解决线性最小二乘问题。虽然Gram-Schmidt算法在几何上直观，但它可能存在数值问题。另一方面，Householder反射提供了计算QR分解的更数值稳定的方法。这些反射是正交变换，可以用来在矩阵的对角线下方引入零，最终导致其三角化。'
- en: '*“Application to regression analysis”* This section applies the least squares
    method to regression analysis. It begins by using the method to find the coefficients
    that best fit a linear model to a given dataset. It then extends this approach
    to polynomial regression, showing how to fit higher-degree polynomials by adding
    columns to the data matrix. The section also discusses the concept of overfitting,
    where a model with too many parameters may fit the noise in the data rather than
    the underlying relationship.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*“应用于回归分析”* 本节将最小二乘法应用于回归分析。它首先使用该方法来找到最佳拟合给定数据集的线性模型的系数。然后，它将这种方法扩展到多项式回归，展示了如何通过向数据矩阵中添加列来拟合更高次的多项式。本节还讨论了过拟合的概念，即具有过多参数的模型可能拟合数据的噪声而不是潜在关系。'
