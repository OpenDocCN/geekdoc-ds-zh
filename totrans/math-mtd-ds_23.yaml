- en: '3.6\. Application: logistic regression#'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap03_opt/06_logistic/roch-mmids-opt-logistic.html](https://mmids-textbook.github.io/chap03_opt/06_logistic/roch-mmids-opt-logistic.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'We return to logistic regression\(\idx{logistic regression}\xdi\), which we
    alluded to in the motivating example of this chapter. The input data is of the
    form \(\{(\boldsymbol{\alpha}_i, b_i) : i=1,\ldots, n\}\) where \(\boldsymbol{\alpha}_i
    = (\alpha_{i,1}, \ldots, \alpha_{i,d}) \in \mathbb{R}^d\) are the features and
    \(b_i \in \{0,1\}\) is the label. As before we use a matrix representation: \(A
    \in \mathbb{R}^{n \times d}\) has rows \(\boldsymbol{\alpha}_i^T\), \(i = 1,\ldots,
    n\) and \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.1\. Definitions[#](#definitions "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We summarize the logistic regression approach. Our goal is to find a function
    of the features that approximates the probability of the label \(1\). For this
    purpose, we model the [log-odds](https://en.wikipedia.org/wiki/Logit) (or logit
    function) of the probability of label \(1\) as a linear function of the features
    \(\boldsymbol{\alpha} \in \mathbb{R}^d\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \log \frac{p(\mathbf{x}; \boldsymbol{\alpha})}{1-p(\mathbf{x}; \boldsymbol{\alpha})}
    = \boldsymbol{\alpha}^T \mathbf{x} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{x} \in \mathbb{R}^d\) is the vector of coefficients (i.e., parameters).
    Inverting this expression gives
  prefs: []
  type: TYPE_NORMAL
- en: \[ p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where the [sigmoid](https://en.wikipedia.org/wiki/Logistic_function)\(\idx{sigmoid
    function}\xdi\) function is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma(z) = \frac{1}{1 + e^{-z}} \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(z \in \mathbb{R}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We plot the sigmoid function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/beb1329759d4211ded6c56cbd1366fe1f58d01d13f8f8dedb821b6c63b9982c9.png](../Images/0cd8936e1761530c96b5b7087ee4fd19.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: We seek to maximize the probability of observing the data (also known as [likelihood
    function](https://en.wikipedia.org/wiki/Likelihood_function)) assuming the labels
    are independent given the features, which is given by (see Chapter 6 for more
    details; for now we are merely setting up the relevant optimization problem)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L}(\mathbf{x}; A, \mathbf{b}) = \prod_{i=1}^n p(\boldsymbol{\alpha}_i;
    \mathbf{x})^{b_i} (1- p(\boldsymbol{\alpha}_i; \mathbf{x}))^{1-b_i}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Taking a logarithm, multiplying by \(-1/n\) and substituting the sigmoid function,
    we want to minimize the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)\(\idx{cross-entropy
    loss}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1-b_i) \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We used standard properties of the logarithm: for \(x, y > 0\), \(\log(xy)
    = \log x + \log y\) and \(\log(x^y) = y \log x\).'
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we want to solve the minimization problem
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}). \]
  prefs: []
  type: TYPE_NORMAL
- en: We are implicitly using here that the logarithm is a strictly increasing function
    and therefore does not change the global optimum of a function; multiplying by
    \(-1/n\) changed the global maximum into a global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: To use gradient descent, we need the gradient of \(\ell\). We use the *Chain
    Rule* and first compute the derivative of \(\sigma\) which is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}}\left(1
    - \frac{1}{1 + e^{-z}}\right) = \sigma(z) (1 - \sigma(z)). \]
  prefs: []
  type: TYPE_NORMAL
- en: The latter expression is known as the [logistic differential equation](https://en.wikipedia.org/wiki/Logistic_function#Logistic_differential_equation).
    It arises in a variety of applications, including the modeling of [population
    dynamics](https://towardsdatascience.com/covid-19-infection-in-italy-mathematical-models-and-predictions-7784b4d7dd8d).
    Here it will be a convenient way to compute the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Observe that, for \(\boldsymbol{\alpha} = (\alpha_{1}, \ldots, \alpha_{d}) \in
    \mathbb{R}^d\), by the *Chain Rule*
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \nabla (\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \boldsymbol{\alpha} \]
  prefs: []
  type: TYPE_NORMAL
- en: where, throughout, the gradient is with respect to \(\mathbf{x}\).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can obtain the same formula by applying the single-variable
    *Chain Rule*
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial}{\partial x_j} \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x}) \frac{\partial}{\partial
    x_j}(\boldsymbol{\alpha}^T \mathbf{x})\\ &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x})
    \frac{\partial}{\partial x_j}\left(\alpha_{j} x_{j} + \sum_{\ell=1, \ell \neq
    j}^d \alpha_{\ell} x_{\ell}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: so that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) &= \left(\sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{1}, \ldots,
    \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\,
    \alpha_{d}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}))\, (\alpha_{1}, \ldots, \alpha_{d})\\ &= \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By another application of the *Chain Rule*, since \(\frac{\mathrm{d}}{\mathrm{d}
    z} \log z = \frac{1}{z}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= \nabla\left[\frac{1}{n}
    \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) - (1-b_i)
    \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}\right]\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) - \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) + \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Using the expression for the gradient of the sigmoid functions, this is equal
    to
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &- \frac{1}{n} \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \,\boldsymbol{\alpha}_i\\ &\quad\quad + \frac{1}{n} \sum_{i=1}^n
    \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n \left( b_i (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))
    - (1-b_i)\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) \right)\,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    ) \,\boldsymbol{\alpha}_i. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this formula, it will be useful to re-write it in terms of the
    matrix representation \(A \in \mathbb{R}^{n \times d}\) (which has rows \(\boldsymbol{\alpha}_i^T\),
    \(i = 1,\ldots, n\)) and \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\). Let
    \(\bsigma : \mathbb{R}^n \to \mathbb{R}\) be the vector-valued function that applies
    the sigmoid \(\sigma\) entry-wise, i.e., \(\bsigma(\mathbf{z}) = (\sigma(z_1),\ldots,\sigma(z_n))\)
    where \(\mathbf{z} = (z_1,\ldots,z_n)\). Thinking of \(\sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})\,\boldsymbol{\alpha}_i\) as a linear combination of the columns of
    \(A^T\) with coefficients being the entries of the vector \(\mathbf{b} - \bsigma(A
    \mathbf{x})\), we that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla\ell(\mathbf{x}; A, \mathbf{b}) = - \frac{1}{n} \sum_{i=1}^n ( b_i
    - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i = -\frac{1}{n}
    A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \]
  prefs: []
  type: TYPE_NORMAL
- en: We turn to the Hessian. By symmetry, we can think of the \(j\)-th column of
    the Hessian as the gradient of the partial derivative with respect to \(x_j\).
    Hence we start by computing the gradient of the \(j\)-th entry of the summands
    in the gradient of \(\ell\). We note that, for \(\boldsymbol{\alpha} = (\alpha_{1},
    \ldots, \alpha_{d}) \in \mathbb{R}^d\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla [(b - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j}] = -
    \nabla [\sigma(\boldsymbol{\alpha}^T \mathbf{x})] \, \alpha_{j} = - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}\alpha_{j}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, using the fact that \(\boldsymbol{\alpha} \alpha_{j}\) is the \(j\)-th
    column of the matrix \(\boldsymbol{\alpha} \boldsymbol{\alpha}^T\), we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})\) indicates the Hessian
    with respect to the \(\mathbf{x}\) variables, for fixed \(A, \mathbf{b}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Convexity of logistic regression)** \(\idx{convexity of logistic
    regression}\xdi\) The function \(\ell(\mathbf{x}; A, \mathbf{b})\) is convex as
    a function of \(\mathbf{x} \in \mathbb{R}^d\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Indeed, the Hessian is positive semidefinite: for any \(\mathbf{z}
    \in \mathbb{R}^d\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \mathbf{z}^T \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \mathbf{z}\\ &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\geq 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: since \(\sigma(t) \in [0,1]\) for all \(t\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Convexity is one reason for working with the cross-entropy loss ([rather than
    the mean squared error](https://math.stackexchange.com/questions/1582452/logistic-regression-prove-that-the-cost-function-is-convex)
    for instance).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Smoothness of logistic regression)** The function \(\ell(\mathbf{x};
    A, \mathbf{b})\) is \(L\)-smooth for'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L= \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 = \frac{1}{4n} \|A\|_F^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We use convexity and the expression for the Hessian to derive that,
    for any unit vector \(\mathbf{z} \in \mathbb{R}^d\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} 0 \leq \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We need to find the maximum value that the factor \(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\) can take. Note
    that \(\sigma(t) \in [0,1]\) for all \(t\) and \(\sigma(t) + (1 - \sigma(t)) =
    1\). Taking the derivatives of the function \(f(w) = w (1 - w) = w - w^2\) we
    get \(f'(w) = 1 - 2 w\) and \(f''(w) = -2\). So \(f\) is concave and achieve its
    maximum at \(w^* = 1/2\) where it takes the value \(f(1/2) = 1/4\). We have proved
    that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \leq 1/4 \]
  prefs: []
  type: TYPE_NORMAL
- en: for any \(\mathbf{z}\).
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the upper bound on \(\mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x};
    A, \mathbf{b}) \,\mathbf{z}\) we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\leq \frac{1}{4n} \sum_{i=1}^n (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\mathbf{z}\|^2 \|\boldsymbol{\alpha}_i\|^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Cauchy-Schwarz inequality* on the third line and the fact
    that \(\mathbf{z}\) is a unit vector on the fourth one.
  prefs: []
  type: TYPE_NORMAL
- en: That implies \(L\)-smoothness. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: For step size \(\beta\), one step of gradient descent is therefore
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{n} \sum_{i=1}^n ( b_i -
    \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.2\. Implementation[#](#implementation "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We modify our implementation of gradient descent to take a dataset as input.
    Recall that to run gradient descent, we first implement a function computing a
    descent update. It takes as input a function `grad_fn` computing the gradient
    itself, as well as a current iterate and a step size. We now also feed a dataset
    as additional input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to implement gradient descent. Our function takes as input a function
    `loss_fn` computing the objective, a function `grad_fn` computing the gradient,
    the dataset `A` and `b`, and an initial guess `init_x`. Optional parameters are
    the step size and the number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: To implement `loss_fn` and `grad_fn`, we define the sigmoid as above. Below,
    `pred_fn` is \(\bsigma(A \mathbf{x})\). Here we write the loss function as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right), \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\odot\) is the Hadamard product, or element-wise product (for example
    \(\mathbf{u} \odot \mathbf{v} = (u_1 v_1, \ldots,u_n v_n)\)), the logarithm (denoted
    in bold) is applied element-wise and \(\mathrm{mean}(\mathbf{z})\) is the mean
    of the entries of \(\mathbf{z}\) (i.e., \(\mathrm{mean}(\mathbf{z}) = n^{-1} \sum_{i=1}^n
    z_i\)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We can choose a step size based on the smoothness of the objective as above.
    Recall that [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)
    computes the Frobenius norm by default.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We return to our original motivation, the [airline customer
    satisfaction](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction)
    dataset. We first load the dataset. We will need the column names later.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Our goal will be to predict the first column, `Satisfied`, from the rest of
    the columns. For this, we transform our data into NumPy arrays. We also standardize
    the columns by subtracting their mean and dividing by their standard deviation.
    This will allow to compare the influence of different features on the prediction.
    And we add a column of 1s to account for the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We use the functions `loss_fn` and `grad_fn` which were written for general
    logistic regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: To interpret the results, we plot the coefficients in decreasing order.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/4a66052e179e6e561e9711defd186394eac14fb13f59c0250fcce6f357c91144.png](../Images/f0f3c46cb4f17a7ae15de744d08dd196.png)'
  prefs: []
  type: TYPE_IMG
- en: We see from the first ten bars or so that, as might be expected, higher ratings
    on various aspects of the flight generally contribute to a higher predicted likelihood
    of satisfaction (with one exception being `Gate location` whose coefficient is
    negative but may not be [statistically significant](https://en.wikipedia.org/wiki/Statistical_significance)).
    `Inflight entertainment` seems particularly influential. `Age` also shows the
    same pattern, something we had noticed in the introductory section through a different
    analysis. On the other hand, `Departure Delay in Minutes` and `Arrival Delay in
    Minutes` contribute to a lower predicted likelihood of satisfaction, again an
    expected pattern. The most negative influence however appears to come from `Class_Eco`.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** There are faster methods for logistic regression. Ask your
    favorite AI chatbot for an explanation and implementation of the iteratively reweighted
    least squares method. Try it on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**TRY IT!** One can attempt to predict whether a new customer, whose feature
    vector is \(\boldsymbol{\alpha}\), will be satisfied by using the prediction function
    \(p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T \mathbf{x})\),
    where \(\mathbf{x}\) is the fitted coefficients. Say a customer is predicted to
    be satisfied if \(\sigma(\boldsymbol{\alpha}^T \mathbf{x}) > 0.5\). Implement
    this predictor and compute its accuracy on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Because of the issue of overfitting, computing the accuracy
    of a predictor on a dataset used to estimate the coefficients is problematic.
    Ask your favorite AI chatbot about scikit-learn’s `train_test_split` function
    and how it helps resolve this issue. Implement it on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** What is the primary goal of logistic regression?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To predict a continuous outcome variable.
  prefs: []
  type: TYPE_NORMAL
- en: b) To classify data points into multiple categories.
  prefs: []
  type: TYPE_NORMAL
- en: c) To model the probability of a binary outcome.
  prefs: []
  type: TYPE_NORMAL
- en: d) To find the optimal linear combination of features.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** What is the relationship between maximizing the likelihood function and
    minimizing the cross-entropy loss in logistic regression?'
  prefs: []
  type: TYPE_NORMAL
- en: a) They are unrelated concepts.
  prefs: []
  type: TYPE_NORMAL
- en: b) Maximizing the likelihood is equivalent to minimizing the cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: c) Minimizing the cross-entropy loss is a first step towards maximizing the
    likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: d) Maximizing the likelihood is a special case of minimizing the cross-entropy
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Which of the following is the correct formula for the logistic regression
    objective function (cross-entropy loss)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \log(1 + \exp(-b_i
    \boldsymbol{\alpha}_i^T \mathbf{x}))\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}))^2\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n b_i \boldsymbol{\alpha}_i^T
    \mathbf{x}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** What is the purpose of standardizing the input features in the airline
    customer satisfaction dataset example?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To ensure the objective function is convex.
  prefs: []
  type: TYPE_NORMAL
- en: b) To speed up the convergence of gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: c) To allow comparison of the influence of different features on the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: d) To handle missing data in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which of the following is a valid reason for adding a column of 1’s to
    the feature matrix in logistic regression?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To ensure the objective function is convex.
  prefs: []
  type: TYPE_NORMAL
- en: b) To allow for an intercept term in the model.
  prefs: []
  type: TYPE_NORMAL
- en: c) To standardize the input features.
  prefs: []
  type: TYPE_NORMAL
- en: d) To handle missing data in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: c. Justification: The text states that the goal of logistic regression
    is to “find a function of the features that approximates the probability of the
    label.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The text states: “We seek to maximize the probability
    of observing the data… which is given by… Taking a logarithm, multiplying by \(-1/n\)
    and substituting the sigmoid function, we want to minimize the cross-entropy loss.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text states: “Hence, we want to solve the
    minimization problem \(\min_{x \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b})\),”
    where \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: c. Justification: The text states: “We also standardize the columns
    by subtracting their mean and dividing by their standard deviation. This will
    allow to compare the influence of different features on the prediction.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text states: “To allow an affine function
    of the features, we add a column of 1’s as we have done before.”'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.1\. Definitions[#](#definitions "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We summarize the logistic regression approach. Our goal is to find a function
    of the features that approximates the probability of the label \(1\). For this
    purpose, we model the [log-odds](https://en.wikipedia.org/wiki/Logit) (or logit
    function) of the probability of label \(1\) as a linear function of the features
    \(\boldsymbol{\alpha} \in \mathbb{R}^d\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \log \frac{p(\mathbf{x}; \boldsymbol{\alpha})}{1-p(\mathbf{x}; \boldsymbol{\alpha})}
    = \boldsymbol{\alpha}^T \mathbf{x} \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{x} \in \mathbb{R}^d\) is the vector of coefficients (i.e., parameters).
    Inverting this expression gives
  prefs: []
  type: TYPE_NORMAL
- en: \[ p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where the [sigmoid](https://en.wikipedia.org/wiki/Logistic_function)\(\idx{sigmoid
    function}\xdi\) function is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma(z) = \frac{1}{1 + e^{-z}} \]
  prefs: []
  type: TYPE_NORMAL
- en: for \(z \in \mathbb{R}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We plot the sigmoid function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/beb1329759d4211ded6c56cbd1366fe1f58d01d13f8f8dedb821b6c63b9982c9.png](../Images/0cd8936e1761530c96b5b7087ee4fd19.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: We seek to maximize the probability of observing the data (also known as [likelihood
    function](https://en.wikipedia.org/wiki/Likelihood_function)) assuming the labels
    are independent given the features, which is given by (see Chapter 6 for more
    details; for now we are merely setting up the relevant optimization problem)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathcal{L}(\mathbf{x}; A, \mathbf{b}) = \prod_{i=1}^n p(\boldsymbol{\alpha}_i;
    \mathbf{x})^{b_i} (1- p(\boldsymbol{\alpha}_i; \mathbf{x}))^{1-b_i}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Taking a logarithm, multiplying by \(-1/n\) and substituting the sigmoid function,
    we want to minimize the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)\(\idx{cross-entropy
    loss}\xdi\)
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1-b_i) \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We used standard properties of the logarithm: for \(x, y > 0\), \(\log(xy)
    = \log x + \log y\) and \(\log(x^y) = y \log x\).'
  prefs: []
  type: TYPE_NORMAL
- en: Hence, we want to solve the minimization problem
  prefs: []
  type: TYPE_NORMAL
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}). \]
  prefs: []
  type: TYPE_NORMAL
- en: We are implicitly using here that the logarithm is a strictly increasing function
    and therefore does not change the global optimum of a function; multiplying by
    \(-1/n\) changed the global maximum into a global minimum.
  prefs: []
  type: TYPE_NORMAL
- en: To use gradient descent, we need the gradient of \(\ell\). We use the *Chain
    Rule* and first compute the derivative of \(\sigma\) which is
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}}\left(1
    - \frac{1}{1 + e^{-z}}\right) = \sigma(z) (1 - \sigma(z)). \]
  prefs: []
  type: TYPE_NORMAL
- en: The latter expression is known as the [logistic differential equation](https://en.wikipedia.org/wiki/Logistic_function#Logistic_differential_equation).
    It arises in a variety of applications, including the modeling of [population
    dynamics](https://towardsdatascience.com/covid-19-infection-in-italy-mathematical-models-and-predictions-7784b4d7dd8d).
    Here it will be a convenient way to compute the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Observe that, for \(\boldsymbol{\alpha} = (\alpha_{1}, \ldots, \alpha_{d}) \in
    \mathbb{R}^d\), by the *Chain Rule*
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \nabla (\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \boldsymbol{\alpha} \]
  prefs: []
  type: TYPE_NORMAL
- en: where, throughout, the gradient is with respect to \(\mathbf{x}\).
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can obtain the same formula by applying the single-variable
    *Chain Rule*
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \frac{\partial}{\partial x_j} \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x}) \frac{\partial}{\partial
    x_j}(\boldsymbol{\alpha}^T \mathbf{x})\\ &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x})
    \frac{\partial}{\partial x_j}\left(\alpha_{j} x_{j} + \sum_{\ell=1, \ell \neq
    j}^d \alpha_{\ell} x_{\ell}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: so that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) &= \left(\sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{1}, \ldots,
    \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\,
    \alpha_{d}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}))\, (\alpha_{1}, \ldots, \alpha_{d})\\ &= \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: By another application of the *Chain Rule*, since \(\frac{\mathrm{d}}{\mathrm{d}
    z} \log z = \frac{1}{z}\),
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= \nabla\left[\frac{1}{n}
    \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) - (1-b_i)
    \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}\right]\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) - \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) + \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}). \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: Using the expression for the gradient of the sigmoid functions, this is equal
    to
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} &- \frac{1}{n} \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \,\boldsymbol{\alpha}_i\\ &\quad\quad + \frac{1}{n} \sum_{i=1}^n
    \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n \left( b_i (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))
    - (1-b_i)\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) \right)\,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    ) \,\boldsymbol{\alpha}_i. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this formula, it will be useful to re-write it in terms of the
    matrix representation \(A \in \mathbb{R}^{n \times d}\) (which has rows \(\boldsymbol{\alpha}_i^T\),
    \(i = 1,\ldots, n\)) and \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\). Let
    \(\bsigma : \mathbb{R}^n \to \mathbb{R}\) be the vector-valued function that applies
    the sigmoid \(\sigma\) entry-wise, i.e., \(\bsigma(\mathbf{z}) = (\sigma(z_1),\ldots,\sigma(z_n))\)
    where \(\mathbf{z} = (z_1,\ldots,z_n)\). Thinking of \(\sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})\,\boldsymbol{\alpha}_i\) as a linear combination of the columns of
    \(A^T\) with coefficients being the entries of the vector \(\mathbf{b} - \bsigma(A
    \mathbf{x})\), we that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla\ell(\mathbf{x}; A, \mathbf{b}) = - \frac{1}{n} \sum_{i=1}^n ( b_i
    - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i = -\frac{1}{n}
    A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \]
  prefs: []
  type: TYPE_NORMAL
- en: We turn to the Hessian. By symmetry, we can think of the \(j\)-th column of
    the Hessian as the gradient of the partial derivative with respect to \(x_j\).
    Hence we start by computing the gradient of the \(j\)-th entry of the summands
    in the gradient of \(\ell\). We note that, for \(\boldsymbol{\alpha} = (\alpha_{1},
    \ldots, \alpha_{d}) \in \mathbb{R}^d\),
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla [(b - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j}] = -
    \nabla [\sigma(\boldsymbol{\alpha}^T \mathbf{x})] \, \alpha_{j} = - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}\alpha_{j}.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: Thus, using the fact that \(\boldsymbol{\alpha} \alpha_{j}\) is the \(j\)-th
    column of the matrix \(\boldsymbol{\alpha} \boldsymbol{\alpha}^T\), we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})\) indicates the Hessian
    with respect to the \(\mathbf{x}\) variables, for fixed \(A, \mathbf{b}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Convexity of logistic regression)** \(\idx{convexity of logistic
    regression}\xdi\) The function \(\ell(\mathbf{x}; A, \mathbf{b})\) is convex as
    a function of \(\mathbf{x} \in \mathbb{R}^d\). \(\flat\)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* Indeed, the Hessian is positive semidefinite: for any \(\mathbf{z}
    \in \mathbb{R}^d\)'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \mathbf{z}^T \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \mathbf{z}\\ &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\geq 0 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: since \(\sigma(t) \in [0,1]\) for all \(t\). \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: Convexity is one reason for working with the cross-entropy loss ([rather than
    the mean squared error](https://math.stackexchange.com/questions/1582452/logistic-regression-prove-that-the-cost-function-is-convex)
    for instance).
  prefs: []
  type: TYPE_NORMAL
- en: '**LEMMA** **(Smoothness of logistic regression)** The function \(\ell(\mathbf{x};
    A, \mathbf{b})\) is \(L\)-smooth for'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L= \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 = \frac{1}{4n} \|A\|_F^2.
    \]
  prefs: []
  type: TYPE_NORMAL
- en: \(\flat\)
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* We use convexity and the expression for the Hessian to derive that,
    for any unit vector \(\mathbf{z} \in \mathbb{R}^d\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} 0 \leq \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2.
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: We need to find the maximum value that the factor \(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\) can take. Note
    that \(\sigma(t) \in [0,1]\) for all \(t\) and \(\sigma(t) + (1 - \sigma(t)) =
    1\). Taking the derivatives of the function \(f(w) = w (1 - w) = w - w^2\) we
    get \(f'(w) = 1 - 2 w\) and \(f''(w) = -2\). So \(f\) is concave and achieve its
    maximum at \(w^* = 1/2\) where it takes the value \(f(1/2) = 1/4\). We have proved
    that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \leq 1/4 \]
  prefs: []
  type: TYPE_NORMAL
- en: for any \(\mathbf{z}\).
  prefs: []
  type: TYPE_NORMAL
- en: Going back to the upper bound on \(\mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x};
    A, \mathbf{b}) \,\mathbf{z}\) we get
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\leq \frac{1}{4n} \sum_{i=1}^n (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\mathbf{z}\|^2 \|\boldsymbol{\alpha}_i\|^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2, \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the *Cauchy-Schwarz inequality* on the third line and the fact
    that \(\mathbf{z}\) is a unit vector on the fourth one.
  prefs: []
  type: TYPE_NORMAL
- en: That implies \(L\)-smoothness. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: For step size \(\beta\), one step of gradient descent is therefore
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{n} \sum_{i=1}^n ( b_i -
    \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i. \]
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.2\. Implementation[#](#implementation "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We modify our implementation of gradient descent to take a dataset as input.
    Recall that to run gradient descent, we first implement a function computing a
    descent update. It takes as input a function `grad_fn` computing the gradient
    itself, as well as a current iterate and a step size. We now also feed a dataset
    as additional input.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to implement gradient descent. Our function takes as input a function
    `loss_fn` computing the objective, a function `grad_fn` computing the gradient,
    the dataset `A` and `b`, and an initial guess `init_x`. Optional parameters are
    the step size and the number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To implement `loss_fn` and `grad_fn`, we define the sigmoid as above. Below,
    `pred_fn` is \(\bsigma(A \mathbf{x})\). Here we write the loss function as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right), \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\odot\) is the Hadamard product, or element-wise product (for example
    \(\mathbf{u} \odot \mathbf{v} = (u_1 v_1, \ldots,u_n v_n)\)), the logarithm (denoted
    in bold) is applied element-wise and \(\mathrm{mean}(\mathbf{z})\) is the mean
    of the entries of \(\mathbf{z}\) (i.e., \(\mathrm{mean}(\mathbf{z}) = n^{-1} \sum_{i=1}^n
    z_i\)).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can choose a step size based on the smoothness of the objective as above.
    Recall that [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)
    computes the Frobenius norm by default.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**NUMERICAL CORNER:** We return to our original motivation, the [airline customer
    satisfaction](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction)
    dataset. We first load the dataset. We will need the column names later.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Our goal will be to predict the first column, `Satisfied`, from the rest of
    the columns. For this, we transform our data into NumPy arrays. We also standardize
    the columns by subtracting their mean and dividing by their standard deviation.
    This will allow to compare the influence of different features on the prediction.
    And we add a column of 1s to account for the intercept.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We use the functions `loss_fn` and `grad_fn` which were written for general
    logistic regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: To interpret the results, we plot the coefficients in decreasing order.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/4a66052e179e6e561e9711defd186394eac14fb13f59c0250fcce6f357c91144.png](../Images/f0f3c46cb4f17a7ae15de744d08dd196.png)'
  prefs: []
  type: TYPE_IMG
- en: We see from the first ten bars or so that, as might be expected, higher ratings
    on various aspects of the flight generally contribute to a higher predicted likelihood
    of satisfaction (with one exception being `Gate location` whose coefficient is
    negative but may not be [statistically significant](https://en.wikipedia.org/wiki/Statistical_significance)).
    `Inflight entertainment` seems particularly influential. `Age` also shows the
    same pattern, something we had noticed in the introductory section through a different
    analysis. On the other hand, `Departure Delay in Minutes` and `Arrival Delay in
    Minutes` contribute to a lower predicted likelihood of satisfaction, again an
    expected pattern. The most negative influence however appears to come from `Class_Eco`.
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** There are faster methods for logistic regression. Ask your
    favorite AI chatbot for an explanation and implementation of the iteratively reweighted
    least squares method. Try it on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '**TRY IT!** One can attempt to predict whether a new customer, whose feature
    vector is \(\boldsymbol{\alpha}\), will be satisfied by using the prediction function
    \(p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T \mathbf{x})\),
    where \(\mathbf{x}\) is the fitted coefficients. Say a customer is predicted to
    be satisfied if \(\sigma(\boldsymbol{\alpha}^T \mathbf{x}) > 0.5\). Implement
    this predictor and compute its accuracy on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** Because of the issue of overfitting, computing the accuracy
    of a predictor on a dataset used to estimate the coefficients is problematic.
    Ask your favorite AI chatbot about scikit-learn’s `train_test_split` function
    and how it helps resolve this issue. Implement it on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**1** What is the primary goal of logistic regression?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To predict a continuous outcome variable.
  prefs: []
  type: TYPE_NORMAL
- en: b) To classify data points into multiple categories.
  prefs: []
  type: TYPE_NORMAL
- en: c) To model the probability of a binary outcome.
  prefs: []
  type: TYPE_NORMAL
- en: d) To find the optimal linear combination of features.
  prefs: []
  type: TYPE_NORMAL
- en: '**2** What is the relationship between maximizing the likelihood function and
    minimizing the cross-entropy loss in logistic regression?'
  prefs: []
  type: TYPE_NORMAL
- en: a) They are unrelated concepts.
  prefs: []
  type: TYPE_NORMAL
- en: b) Maximizing the likelihood is equivalent to minimizing the cross-entropy loss.
  prefs: []
  type: TYPE_NORMAL
- en: c) Minimizing the cross-entropy loss is a first step towards maximizing the
    likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: d) Maximizing the likelihood is a special case of minimizing the cross-entropy
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: '**3** Which of the following is the correct formula for the logistic regression
    objective function (cross-entropy loss)?'
  prefs: []
  type: TYPE_NORMAL
- en: a) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \log(1 + \exp(-b_i
    \boldsymbol{\alpha}_i^T \mathbf{x}))\)
  prefs: []
  type: TYPE_NORMAL
- en: b) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}))^2\)
  prefs: []
  type: TYPE_NORMAL
- en: c) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\}\)
  prefs: []
  type: TYPE_NORMAL
- en: d) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n b_i \boldsymbol{\alpha}_i^T
    \mathbf{x}\)
  prefs: []
  type: TYPE_NORMAL
- en: '**4** What is the purpose of standardizing the input features in the airline
    customer satisfaction dataset example?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To ensure the objective function is convex.
  prefs: []
  type: TYPE_NORMAL
- en: b) To speed up the convergence of gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: c) To allow comparison of the influence of different features on the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: d) To handle missing data in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**5** Which of the following is a valid reason for adding a column of 1’s to
    the feature matrix in logistic regression?'
  prefs: []
  type: TYPE_NORMAL
- en: a) To ensure the objective function is convex.
  prefs: []
  type: TYPE_NORMAL
- en: b) To allow for an intercept term in the model.
  prefs: []
  type: TYPE_NORMAL
- en: c) To standardize the input features.
  prefs: []
  type: TYPE_NORMAL
- en: d) To handle missing data in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 1: c. Justification: The text states that the goal of logistic regression
    is to “find a function of the features that approximates the probability of the
    label.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 2: b. Justification: The text states: “We seek to maximize the probability
    of observing the data… which is given by… Taking a logarithm, multiplying by \(-1/n\)
    and substituting the sigmoid function, we want to minimize the cross-entropy loss.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 3: c. Justification: The text states: “Hence, we want to solve the
    minimization problem \(\min_{x \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b})\),”
    where \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\}\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 4: c. Justification: The text states: “We also standardize the columns
    by subtracting their mean and dividing by their standard deviation. This will
    allow to compare the influence of different features on the prediction.”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer for 5: b. Justification: The text states: “To allow an affine function
    of the features, we add a column of 1’s as we have done before.”'
  prefs: []
  type: TYPE_NORMAL
