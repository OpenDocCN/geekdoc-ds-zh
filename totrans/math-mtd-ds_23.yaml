- en: '3.6\. Application: logistic regression#'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3.6\. 应用：逻辑回归#
- en: 原文：[https://mmids-textbook.github.io/chap03_opt/06_logistic/roch-mmids-opt-logistic.html](https://mmids-textbook.github.io/chap03_opt/06_logistic/roch-mmids-opt-logistic.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mmids-textbook.github.io/chap03_opt/06_logistic/roch-mmids-opt-logistic.html](https://mmids-textbook.github.io/chap03_opt/06_logistic/roch-mmids-opt-logistic.html)
- en: 'We return to logistic regression\(\idx{logistic regression}\xdi\), which we
    alluded to in the motivating example of this chapter. The input data is of the
    form \(\{(\boldsymbol{\alpha}_i, b_i) : i=1,\ldots, n\}\) where \(\boldsymbol{\alpha}_i
    = (\alpha_{i,1}, \ldots, \alpha_{i,d}) \in \mathbb{R}^d\) are the features and
    \(b_i \in \{0,1\}\) is the label. As before we use a matrix representation: \(A
    \in \mathbb{R}^{n \times d}\) has rows \(\boldsymbol{\alpha}_i^T\), \(i = 1,\ldots,
    n\) and \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '我们回到逻辑回归\(\idx{逻辑回归}\xdi\)，这是我们在本章的动机示例中提到的。输入数据的形式为 \(\{(\boldsymbol{\alpha}_i,
    b_i) : i=1,\ldots, n\}\)，其中 \(\boldsymbol{\alpha}_i = (\alpha_{i,1}, \ldots, \alpha_{i,d})
    \in \mathbb{R}^d\) 是特征，\(b_i \in \{0,1\}\) 是标签。和之前一样，我们使用矩阵表示：\(A \in \mathbb{R}^{n
    \times d}\) 的行是 \(\boldsymbol{\alpha}_i^T\)，\(i = 1,\ldots, n\)，而 \(\mathbf{b}
    = (b_1, \ldots, b_n) \in \{0,1\}^n\)。'
- en: 3.6.1\. Definitions[#](#definitions "Link to this heading")
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6.1\. 定义[#](#definitions "链接到这个标题")
- en: We summarize the logistic regression approach. Our goal is to find a function
    of the features that approximates the probability of the label \(1\). For this
    purpose, we model the [log-odds](https://en.wikipedia.org/wiki/Logit) (or logit
    function) of the probability of label \(1\) as a linear function of the features
    \(\boldsymbol{\alpha} \in \mathbb{R}^d\)
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了逻辑回归方法。我们的目标是找到一个特征函数，该函数近似于标签 \(1\) 的概率。为此，我们将标签 \(1\) 的概率的对数几率（或logit函数）建模为特征
    \(\boldsymbol{\alpha} \in \mathbb{R}^d\) 的线性函数
- en: \[ \log \frac{p(\mathbf{x}; \boldsymbol{\alpha})}{1-p(\mathbf{x}; \boldsymbol{\alpha})}
    = \boldsymbol{\alpha}^T \mathbf{x} \]
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log \frac{p(\mathbf{x}; \boldsymbol{\alpha})}{1-p(\mathbf{x}; \boldsymbol{\alpha})}
    = \boldsymbol{\alpha}^T \mathbf{x} \]
- en: where \(\mathbf{x} \in \mathbb{R}^d\) is the vector of coefficients (i.e., parameters).
    Inverting this expression gives
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{x} \in \mathbb{R}^d\) 是系数向量（即参数）。通过反转这个表达式，我们得到
- en: \[ p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    \]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    \]
- en: where the [sigmoid](https://en.wikipedia.org/wiki/Logistic_function)\(\idx{sigmoid
    function}\xdi\) function is
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 其中sigmoid函数\(\idx{sigmoid函数}\xdi\)是
- en: \[ \sigma(z) = \frac{1}{1 + e^{-z}} \]
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma(z) = \frac{1}{1 + e^{-z}} \]
- en: for \(z \in \mathbb{R}\).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(z \in \mathbb{R}\)。
- en: '**NUMERICAL CORNER:** We plot the sigmoid function.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们绘制了S形函数。'
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![../../_images/beb1329759d4211ded6c56cbd1366fe1f58d01d13f8f8dedb821b6c63b9982c9.png](../Images/0cd8936e1761530c96b5b7087ee4fd19.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/beb1329759d4211ded6c56cbd1366fe1f58d01d13f8f8dedb821b6c63b9982c9.png](../Images/0cd8936e1761530c96b5b7087ee4fd19.png)'
- en: \(\unlhd\)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: We seek to maximize the probability of observing the data (also known as [likelihood
    function](https://en.wikipedia.org/wiki/Likelihood_function)) assuming the labels
    are independent given the features, which is given by (see Chapter 6 for more
    details; for now we are merely setting up the relevant optimization problem)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求最大化观察数据的概率（也称为[似然函数](https://en.wikipedia.org/wiki/Likelihood_function)），假设在给定特征的情况下标签是独立的，这由以下公式给出（详见第6章以获取更多详细信息；现在我们只是设置相关的优化问题）
- en: \[ \mathcal{L}(\mathbf{x}; A, \mathbf{b}) = \prod_{i=1}^n p(\boldsymbol{\alpha}_i;
    \mathbf{x})^{b_i} (1- p(\boldsymbol{\alpha}_i; \mathbf{x}))^{1-b_i}. \]
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}(\mathbf{x}; A, \mathbf{b}) = \prod_{i=1}^n p(\boldsymbol{\alpha}_i;
    \mathbf{x})^{b_i} (1- p(\boldsymbol{\alpha}_i; \mathbf{x}))^{1-b_i}. \]
- en: Taking a logarithm, multiplying by \(-1/n\) and substituting the sigmoid function,
    we want to minimize the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)\(\idx{cross-entropy
    loss}\xdi\)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 取对数，乘以 \(-1/n\) 并代入S形函数，我们希望最小化[交叉熵损失](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)\(\idx{交叉熵损失}\xdi\)
- en: \[ \ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1-b_i) \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}.
    \]
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1-b_i) \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}.
    \]
- en: 'We used standard properties of the logarithm: for \(x, y > 0\), \(\log(xy)
    = \log x + \log y\) and \(\log(x^y) = y \log x\).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了对数的标准性质：对于 \(x, y > 0\)，\(\log(xy) = \log x + \log y\) 和 \(\log(x^y) =
    y \log x\)。
- en: Hence, we want to solve the minimization problem
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望解决最小化问题
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}). \]
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}). \]
- en: We are implicitly using here that the logarithm is a strictly increasing function
    and therefore does not change the global optimum of a function; multiplying by
    \(-1/n\) changed the global maximum into a global minimum.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里隐含地使用了对数是严格单调递增的函数，因此不会改变函数的全局最优；乘以 \(-1/n\) 将全局最大值变为全局最小值。
- en: To use gradient descent, we need the gradient of \(\ell\). We use the *Chain
    Rule* and first compute the derivative of \(\sigma\) which is
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用梯度下降，我们需要 \(\ell\) 的梯度。我们使用链式法则，并首先计算 \(\sigma\) 的导数，即
- en: \[ \sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}}\left(1
    - \frac{1}{1 + e^{-z}}\right) = \sigma(z) (1 - \sigma(z)). \]
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}}\left(1
    - \frac{1}{1 + e^{-z}}\right) = \sigma(z) (1 - \sigma(z)). \]
- en: The latter expression is known as the [logistic differential equation](https://en.wikipedia.org/wiki/Logistic_function#Logistic_differential_equation).
    It arises in a variety of applications, including the modeling of [population
    dynamics](https://towardsdatascience.com/covid-19-infection-in-italy-mathematical-models-and-predictions-7784b4d7dd8d).
    Here it will be a convenient way to compute the gradient.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 后者表达式被称为[逻辑微分方程](https://en.wikipedia.org/wiki/Logistic_function#Logistic_differential_equation)。它在各种应用中出现，包括[人口动力学](https://towardsdatascience.com/covid-19-in-italy-mathematical-models-and-predictions-7784b4d7dd8d)的建模。在这里，它将是一种方便计算梯度的方法。
- en: Observe that, for \(\boldsymbol{\alpha} = (\alpha_{1}, \ldots, \alpha_{d}) \in
    \mathbb{R}^d\), by the *Chain Rule*
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，对于 \(\boldsymbol{\alpha} = (\alpha_{1}, \ldots, \alpha_{d}) \in \mathbb{R}^d\)，通过链式法则
- en: \[ \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \nabla (\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \boldsymbol{\alpha} \]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \nabla (\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \boldsymbol{\alpha} \]
- en: where, throughout, the gradient is with respect to \(\mathbf{x}\).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，梯度是相对于 \(\mathbf{x}\) 的。
- en: Alternatively, we can obtain the same formula by applying the single-variable
    *Chain Rule*
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过应用单变量链式法则得到相同的公式
- en: \[\begin{align*} \frac{\partial}{\partial x_j} \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x}) \frac{\partial}{\partial
    x_j}(\boldsymbol{\alpha}^T \mathbf{x})\\ &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x})
    \frac{\partial}{\partial x_j}\left(\alpha_{j} x_{j} + \sum_{\ell=1, \ell \neq
    j}^d \alpha_{\ell} x_{\ell}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j} \end{align*}\]
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial}{\partial x_j} \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x}) \frac{\partial}{\partial
    x_j}(\boldsymbol{\alpha}^T \mathbf{x})\\ &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x})
    \frac{\partial}{\partial x_j}\left(\alpha_{j} x_{j} + \sum_{\ell=1, \ell \neq
    j}^d \alpha_{\ell} x_{\ell}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j} \end{align*}\]
- en: so that
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以
- en: \[\begin{align*} \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) &= \left(\sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{1}, \ldots,
    \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\,
    \alpha_{d}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}))\, (\alpha_{1}, \ldots, \alpha_{d})\\ &= \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}.
    \end{align*}\]
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) &= \left(\sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{1}, \ldots,
    \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\,
    \alpha_{d}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}))\, (\alpha_{1}, \ldots, \alpha_{d})\\ &= \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}.
    \end{align*}\]
- en: By another application of the *Chain Rule*, since \(\frac{\mathrm{d}}{\mathrm{d}
    z} \log z = \frac{1}{z}\),
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用链式法则的另一个例子，因为 \(\frac{\mathrm{d}}{\mathrm{d} z} \log z = \frac{1}{z}\),
- en: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= \nabla\left[\frac{1}{n}
    \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) - (1-b_i)
    \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}\right]\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) - \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) + \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}). \end{align*}\]
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= \nabla\left[\frac{1}{n}
    \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) - (1-b_i)
    \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}\right]\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) - \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) + \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}). \end{align*}\]
- en: Using the expression for the gradient of the sigmoid functions, this is equal
    to
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 sigmoid 函数梯度的表达式，这等于
- en: \[\begin{align*} &- \frac{1}{n} \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \,\boldsymbol{\alpha}_i\\ &\quad\quad + \frac{1}{n} \sum_{i=1}^n
    \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n \left( b_i (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))
    - (1-b_i)\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) \right)\,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    ) \,\boldsymbol{\alpha}_i. \end{align*}\]
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &- \frac{1}{n} \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \,\boldsymbol{\alpha}_i\\ &\quad\quad + \frac{1}{n} \sum_{i=1}^n
    \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n \left( b_i (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))
    - (1-b_i)\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) \right)\,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    ) \,\boldsymbol{\alpha}_i. \end{align*}\]
- en: 'To implement this formula, it will be useful to re-write it in terms of the
    matrix representation \(A \in \mathbb{R}^{n \times d}\) (which has rows \(\boldsymbol{\alpha}_i^T\),
    \(i = 1,\ldots, n\)) and \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\). Let
    \(\bsigma : \mathbb{R}^n \to \mathbb{R}\) be the vector-valued function that applies
    the sigmoid \(\sigma\) entry-wise, i.e., \(\bsigma(\mathbf{z}) = (\sigma(z_1),\ldots,\sigma(z_n))\)
    where \(\mathbf{z} = (z_1,\ldots,z_n)\). Thinking of \(\sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})\,\boldsymbol{\alpha}_i\) as a linear combination of the columns of
    \(A^T\) with coefficients being the entries of the vector \(\mathbf{b} - \bsigma(A
    \mathbf{x})\), we that'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '要实现这个公式，将其用矩阵表示 \(A \in \mathbb{R}^{n \times d}\)（其行是 \(\boldsymbol{\alpha}_i^T\)，\(i
    = 1,\ldots, n\)）和 \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\) 来重新编写将是有用的。设
    \(\bsigma : \mathbb{R}^n \to \mathbb{R}\) 是一个向量值函数，它逐项应用 sigmoid \(\sigma\)，即
    \(\bsigma(\mathbf{z}) = (\sigma(z_1),\ldots,\sigma(z_n))\) 其中 \(\mathbf{z} = (z_1,\ldots,z_n)\)。将
    \(\sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})\,\boldsymbol{\alpha}_i\)
    视为 \(A^T\) 的列的线性组合，其系数是向量 \(\mathbf{b} - \bsigma(A \mathbf{x})\) 的项，我们得到'
- en: \[ \nabla\ell(\mathbf{x}; A, \mathbf{b}) = - \frac{1}{n} \sum_{i=1}^n ( b_i
    - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i = -\frac{1}{n}
    A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \]
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla\ell(\mathbf{x}; A, \mathbf{b}) = - \frac{1}{n} \sum_{i=1}^n ( b_i
    - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i = -\frac{1}{n}
    A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \]
- en: We turn to the Hessian. By symmetry, we can think of the \(j\)-th column of
    the Hessian as the gradient of the partial derivative with respect to \(x_j\).
    Hence we start by computing the gradient of the \(j\)-th entry of the summands
    in the gradient of \(\ell\). We note that, for \(\boldsymbol{\alpha} = (\alpha_{1},
    \ldots, \alpha_{d}) \in \mathbb{R}^d\),
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们转向 Hessian 矩阵。由于对称性，我们可以将 Hessian 矩阵的第 \(j\) 列视为关于 \(x_j\) 的偏导数的梯度。因此，我们首先计算
    \(\ell\) 的梯度中项的和的 \(j\) 项的梯度。我们注意到，对于 \(\boldsymbol{\alpha} = (\alpha_{1}, \ldots,
    \alpha_{d}) \in \mathbb{R}^d\)，
- en: \[ \nabla [(b - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j}] = -
    \nabla [\sigma(\boldsymbol{\alpha}^T \mathbf{x})] \, \alpha_{j} = - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}\alpha_{j}.
    \]
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla [(b - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j}] = -
    \nabla [\sigma(\boldsymbol{\alpha}^T \mathbf{x})] \, \alpha_{j} = - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}\alpha_{j}.
    \]
- en: Thus, using the fact that \(\boldsymbol{\alpha} \alpha_{j}\) is the \(j\)-th
    column of the matrix \(\boldsymbol{\alpha} \boldsymbol{\alpha}^T\), we get
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，利用 \(\boldsymbol{\alpha} \alpha_{j}\) 是矩阵 \(\boldsymbol{\alpha} \boldsymbol{\alpha}^T\)
    的第 \(j\) 列这一事实，我们得到
- en: \[ \mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \]
- en: where \(\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})\) indicates the Hessian
    with respect to the \(\mathbf{x}\) variables, for fixed \(A, \mathbf{b}\).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})\) 表示相对于 \(\mathbf{x}\) 变量的
    Hessian 矩阵，对于固定的 \(A, \mathbf{b}\)。
- en: '**LEMMA** **(Convexity of logistic regression)** \(\idx{convexity of logistic
    regression}\xdi\) The function \(\ell(\mathbf{x}; A, \mathbf{b})\) is convex as
    a function of \(\mathbf{x} \in \mathbb{R}^d\). \(\flat\)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(逻辑回归的凸性)** \(\idx{逻辑回归的凸性}\xdi\) 函数 \(\ell(\mathbf{x}; A, \mathbf{b})\)
    作为 \(\mathbf{x} \in \mathbb{R}^d\) 的函数是凸的。 \(\flat\)'
- en: '*Proof:* Indeed, the Hessian is positive semidefinite: for any \(\mathbf{z}
    \in \mathbb{R}^d\)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 事实上，Hessian 矩阵是正半定的：对于任何 \(\mathbf{z} \in \mathbb{R}^d\)'
- en: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \mathbf{z}^T \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \mathbf{z}\\ &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\geq 0 \end{align*}\]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \mathbf{z}^T \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \mathbf{z}\\ &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\geq 0 \end{align*}\]
- en: since \(\sigma(t) \in [0,1]\) for all \(t\). \(\square\)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对于所有 \(t\)，\(\sigma(t) \in [0,1]\)。 \(\square\)
- en: Convexity is one reason for working with the cross-entropy loss ([rather than
    the mean squared error](https://math.stackexchange.com/questions/1582452/logistic-regression-prove-that-the-cost-function-is-convex)
    for instance).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 凸性是使用交叉熵损失（例如，而不是均方误差 [https://math.stackexchange.com/questions/1582452/logistic-regression-prove-that-the-cost-function-is-convex]）的原因之一。
- en: '**LEMMA** **(Smoothness of logistic regression)** The function \(\ell(\mathbf{x};
    A, \mathbf{b})\) is \(L\)-smooth for'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(逻辑回归的平滑性)** 函数 \(\ell(\mathbf{x}; A, \mathbf{b})\) 对于'
- en: \[ L= \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 = \frac{1}{4n} \|A\|_F^2.
    \]
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L= \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 = \frac{1}{4n} \|A\|_F^2.
    \]
- en: \(\flat\)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* We use convexity and the expression for the Hessian to derive that,
    for any unit vector \(\mathbf{z} \in \mathbb{R}^d\),'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明:* 我们使用凸性和 Hessian 的表达式推导出，对于任何单位向量 \(\mathbf{z} \in \mathbb{R}^d\)，'
- en: \[\begin{align*} 0 \leq \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2.
    \end{align*}\]
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} 0 \leq \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2.
    \end{align*}\]
- en: We need to find the maximum value that the factor \(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\) can take. Note
    that \(\sigma(t) \in [0,1]\) for all \(t\) and \(\sigma(t) + (1 - \sigma(t)) =
    1\). Taking the derivatives of the function \(f(w) = w (1 - w) = w - w^2\) we
    get \(f'(w) = 1 - 2 w\) and \(f''(w) = -2\). So \(f\) is concave and achieve its
    maximum at \(w^* = 1/2\) where it takes the value \(f(1/2) = 1/4\). We have proved
    that
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找到因子 \(\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}))\) 可以取的最大值。注意，对于所有 \(t\)，\(\sigma(t) \in [0,1]\)，且 \(\sigma(t) + (1
    - \sigma(t)) = 1\)。对函数 \(f(w) = w (1 - w) = w - w^2\) 求导得到 \(f'(w) = 1 - 2 w\)
    和 \(f''(w) = -2\)。因此，\(f\) 是凹函数，并在 \(w^* = 1/2\) 处达到最大值，此时 \(f(1/2) = 1/4\)。我们已经证明了
- en: \[ \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \leq 1/4 \]
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \leq 1/4 \]
- en: for any \(\mathbf{z}\).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 \(\mathbf{z}\)。
- en: Going back to the upper bound on \(\mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x};
    A, \mathbf{b}) \,\mathbf{z}\) we get
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到 \(\mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b}) \,\mathbf{z}\)
    的上界，我们得到
- en: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\leq \frac{1}{4n} \sum_{i=1}^n (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\mathbf{z}\|^2 \|\boldsymbol{\alpha}_i\|^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2, \end{align*}\]
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\leq \frac{1}{4n} \sum_{i=1}^n (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\mathbf{z}\|^2 \|\boldsymbol{\alpha}_i\|^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2, \end{align*}\]
- en: where we used the *Cauchy-Schwarz inequality* on the third line and the fact
    that \(\mathbf{z}\) is a unit vector on the fourth one.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三行中，我们使用了 *柯西-施瓦茨不等式*，在第四行中，我们使用了 \(\mathbf{z}\) 是一个单位向量的事实。
- en: That implies \(L\)-smoothness. \(\square\)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 \(L\)-平滑性。 \(\square\)
- en: For step size \(\beta\), one step of gradient descent is therefore
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于步长 \(\beta\)，梯度下降的一步因此是
- en: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{n} \sum_{i=1}^n ( b_i -
    \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i. \]
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{n} \sum_{i=1}^n ( b_i -
    \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i. \]
- en: 3.6.2\. Implementation[#](#implementation "Link to this heading")
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6.2\. 实现方法[#](#implementation "链接到这个标题")
- en: We modify our implementation of gradient descent to take a dataset as input.
    Recall that to run gradient descent, we first implement a function computing a
    descent update. It takes as input a function `grad_fn` computing the gradient
    itself, as well as a current iterate and a step size. We now also feed a dataset
    as additional input.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修改了梯度下降的实现，使其接受数据集作为输入。回想一下，为了运行梯度下降，我们首先实现一个计算下降更新的函数。它接受一个计算梯度的函数 `grad_fn`，以及一个当前迭代和步长。我们现在还提供数据集作为额外的输入。
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We are ready to implement gradient descent. Our function takes as input a function
    `loss_fn` computing the objective, a function `grad_fn` computing the gradient,
    the dataset `A` and `b`, and an initial guess `init_x`. Optional parameters are
    the step size and the number of iterations.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好实现梯度下降。我们的函数接受一个计算目标函数的函数 `loss_fn`，一个计算梯度的函数 `grad_fn`，数据集 `A` 和 `b`，以及一个初始猜测
    `init_x`。可选参数是步长和迭代次数。
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To implement `loss_fn` and `grad_fn`, we define the sigmoid as above. Below,
    `pred_fn` is \(\bsigma(A \mathbf{x})\). Here we write the loss function as
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现 `loss_fn` 和 `grad_fn`，我们定义了上述的 sigmoid 函数。下面，`pred_fn` 是 \(\bsigma(A \mathbf{x})\)。这里我们将损失函数写为
- en: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right), \end{align*}\]
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right), \end{align*}\]
- en: where \(\odot\) is the Hadamard product, or element-wise product (for example
    \(\mathbf{u} \odot \mathbf{v} = (u_1 v_1, \ldots,u_n v_n)\)), the logarithm (denoted
    in bold) is applied element-wise and \(\mathrm{mean}(\mathbf{z})\) is the mean
    of the entries of \(\mathbf{z}\) (i.e., \(\mathrm{mean}(\mathbf{z}) = n^{-1} \sum_{i=1}^n
    z_i\)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\odot\) 是哈达玛积，或逐元素乘积（例如 \(\mathbf{u} \odot \mathbf{v} = (u_1 v_1, \ldots,u_n
    v_n)\)），对数（用粗体表示）是逐元素应用，\(\mathrm{mean}(\mathbf{z})\) 是 \(\mathbf{z}\) 的元素平均值（即，\(\mathrm{mean}(\mathbf{z})
    = n^{-1} \sum_{i=1}^n z_i\))。
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can choose a step size based on the smoothness of the objective as above.
    Recall that [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)
    computes the Frobenius norm by default.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据目标函数的平滑性选择步长，如上所述。回想一下，[`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)
    默认计算 Frobenius 范数。
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**NUMERICAL CORNER:** We return to our original motivation, the [airline customer
    satisfaction](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction)
    dataset. We first load the dataset. We will need the column names later.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**NUMERICAL CORNER:** 我们回到最初的动力，即[航空公司客户满意度](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction)数据集。我们首先加载数据集。稍后我们需要列名。'
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Our goal will be to predict the first column, `Satisfied`, from the rest of
    the columns. For this, we transform our data into NumPy arrays. We also standardize
    the columns by subtracting their mean and dividing by their standard deviation.
    This will allow to compare the influence of different features on the prediction.
    And we add a column of 1s to account for the intercept.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标将是预测第一列，即`Satisfied`，从其余列中。为此，我们将数据转换为NumPy数组。我们还通过减去均值并除以标准差来标准化列，这将允许我们比较不同特征对预测的影响。并且我们添加一列1s来考虑截距。
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We use the functions `loss_fn` and `grad_fn` which were written for general
    logistic regression problems.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用为一般逻辑回归问题编写的函数`loss_fn`和`grad_fn`。
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To interpret the results, we plot the coefficients in decreasing order.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释结果，我们按降序排列系数。
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![../../_images/4a66052e179e6e561e9711defd186394eac14fb13f59c0250fcce6f357c91144.png](../Images/f0f3c46cb4f17a7ae15de744d08dd196.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/4a66052e179e6e561e9711defd186394eac14fb13f59c0250fcce6f357c91144.png](../Images/f0f3c46cb4f17a7ae15de744d08dd196.png)'
- en: We see from the first ten bars or so that, as might be expected, higher ratings
    on various aspects of the flight generally contribute to a higher predicted likelihood
    of satisfaction (with one exception being `Gate location` whose coefficient is
    negative but may not be [statistically significant](https://en.wikipedia.org/wiki/Statistical_significance)).
    `Inflight entertainment` seems particularly influential. `Age` also shows the
    same pattern, something we had noticed in the introductory section through a different
    analysis. On the other hand, `Departure Delay in Minutes` and `Arrival Delay in
    Minutes` contribute to a lower predicted likelihood of satisfaction, again an
    expected pattern. The most negative influence however appears to come from `Class_Eco`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从前十个条形图或类似的位置我们可以看到，正如预期的那样，航班各方面的高评级通常有助于提高预测满意度的可能性（有一个例外是`Gate location`，其系数为负，但可能不是[统计上显著](https://en.wikipedia.org/wiki/Statistical_significance)）。`机上娱乐`似乎特别有影响力。`Age`也显示出相同的模式，这是我们通过不同的分析在介绍部分注意到的。另一方面，`Departure
    Delay in Minutes`和`Arrival Delay in Minutes`对预测满意度的可能性有负面影响，这也是一个预期的模式。然而，最负面的影响似乎来自`Class_Eco`。
- en: '**CHAT & LEARN** There are faster methods for logistic regression. Ask your
    favorite AI chatbot for an explanation and implementation of the iteratively reweighted
    least squares method. Try it on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 对于逻辑回归，有更快的方法。向你的心仪AI聊天机器人询问迭代加权最小二乘法的解释和实现。尝试在这个数据集上应用它。（[在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
- en: '**TRY IT!** One can attempt to predict whether a new customer, whose feature
    vector is \(\boldsymbol{\alpha}\), will be satisfied by using the prediction function
    \(p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T \mathbf{x})\),
    where \(\mathbf{x}\) is the fitted coefficients. Say a customer is predicted to
    be satisfied if \(\sigma(\boldsymbol{\alpha}^T \mathbf{x}) > 0.5\). Implement
    this predictor and compute its accuracy on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**TRY IT!** 可以尝试使用预测函数 \(p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T
    \mathbf{x})\) 来预测一个新客户（其特征向量为 \(\boldsymbol{\alpha}\)）是否会满意，其中 \(\mathbf{x}\)
    是拟合系数。如果一个客户被预测为满意，如果 \(\sigma(\boldsymbol{\alpha}^T \mathbf{x}) > 0.5\)。实现这个预测器并计算它在数据集上的准确率。（[在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))'
- en: '**CHAT & LEARN** Because of the issue of overfitting, computing the accuracy
    of a predictor on a dataset used to estimate the coefficients is problematic.
    Ask your favorite AI chatbot about scikit-learn’s `train_test_split` function
    and how it helps resolve this issue. Implement it on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 由于过拟合的问题，在用于估计系数的数据集上计算预测器的准确度是有问题的。向你的心仪AI聊天机器人询问scikit-learn的`train_test_split`函数以及它是如何帮助解决这个问题，并在该数据集上实现它。([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
- en: \(\unlhd\)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude, Gemini和ChatGPT协助)*'
- en: '**1** What is the primary goal of logistic regression?'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 逻辑回归的主要目标是什么？'
- en: a) To predict a continuous outcome variable.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: a) 预测连续的因变量。
- en: b) To classify data points into multiple categories.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: b) 将数据点分类到多个类别。
- en: c) To model the probability of a binary outcome.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: c) 建立二元结果的概率模型。
- en: d) To find the optimal linear combination of features.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: d) 找到特征的最佳线性组合。
- en: '**2** What is the relationship between maximizing the likelihood function and
    minimizing the cross-entropy loss in logistic regression?'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 在逻辑回归中，最大化似然函数和最小化交叉熵损失之间有什么关系？'
- en: a) They are unrelated concepts.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它们是无关的概念。
- en: b) Maximizing the likelihood is equivalent to minimizing the cross-entropy loss.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: b) 最大化似然等价于最小化交叉熵损失。
- en: c) Minimizing the cross-entropy loss is a first step towards maximizing the
    likelihood.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: c) 最小化交叉熵损失是最大化似然的第一步。
- en: d) Maximizing the likelihood is a special case of minimizing the cross-entropy
    loss.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: d) 最大化似然是最小化交叉熵损失的一种特殊情况。
- en: '**3** Which of the following is the correct formula for the logistic regression
    objective function (cross-entropy loss)?'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 以下哪个是逻辑回归目标函数（交叉熵损失）的正确公式？'
- en: a) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \log(1 + \exp(-b_i
    \boldsymbol{\alpha}_i^T \mathbf{x}))\)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \log(1 + \exp(-b_i
    \boldsymbol{\alpha}_i^T \mathbf{x}))\)
- en: b) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}))^2\)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}))^2\)
- en: c) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\}\)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\}\)
- en: d) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n b_i \boldsymbol{\alpha}_i^T
    \mathbf{x}\)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n b_i \boldsymbol{\alpha}_i^T
    \mathbf{x}\)
- en: '**4** What is the purpose of standardizing the input features in the airline
    customer satisfaction dataset example?'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在航空公司客户满意度数据集示例中，标准化输入特征的目的是什么？'
- en: a) To ensure the objective function is convex.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: a) 确保目标函数是凸的。
- en: b) To speed up the convergence of gradient descent.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: b) 加速梯度下降的收敛速度。
- en: c) To allow comparison of the influence of different features on the prediction.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: c) 允许比较不同特征对预测的影响。
- en: d) To handle missing data in the dataset.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: d) 处理数据集中的缺失数据。
- en: '**5** Which of the following is a valid reason for adding a column of 1’s to
    the feature matrix in logistic regression?'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 在逻辑回归中，向特征矩阵中添加一列1的以下哪个是有效的理由？'
- en: a) To ensure the objective function is convex.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: a) 确保目标函数是凸的。
- en: b) To allow for an intercept term in the model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: b) 在模型中允许有截距项。
- en: c) To standardize the input features.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: c) 标准化输入特征。
- en: d) To handle missing data in the dataset.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: d) 处理数据集中的缺失数据。
- en: 'Answer for 1: c. Justification: The text states that the goal of logistic regression
    is to “find a function of the features that approximates the probability of the
    label.”'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 1题答案：c. 理由：文本中提到逻辑回归的目标是“找到一个特征函数来近似标签的概率。”
- en: 'Answer for 2: b. Justification: The text states: “We seek to maximize the probability
    of observing the data… which is given by… Taking a logarithm, multiplying by \(-1/n\)
    and substituting the sigmoid function, we want to minimize the cross-entropy loss.”'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 2题答案：b. 理由：文本中提到：“我们寻求最大化观察数据的概率……这由……给出……取对数，乘以\(-1/n\)并替换sigmoid函数，我们想要最小化交叉熵损失。”
- en: 'Answer for 3: c. Justification: The text states: “Hence, we want to solve the
    minimization problem \(\min_{x \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b})\),”
    where \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\}\).'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 3 的答案：c. 理由：文本中提到：“因此，我们想要解决最小化问题 \(\min_{x \in \mathbb{R}^d} \ell(\mathbf{x};
    A, \mathbf{b})\)，” 其中 \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n
    \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}))\}\)。
- en: 'Answer for 4: c. Justification: The text states: “We also standardize the columns
    by subtracting their mean and dividing by their standard deviation. This will
    allow to compare the influence of different features on the prediction.”'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 4 的答案：c. 理由：文本中提到：“我们还通过减去它们的均值并除以它们的标准差来标准化列。这将允许比较不同特征对预测的影响。”
- en: 'Answer for 5: b. Justification: The text states: “To allow an affine function
    of the features, we add a column of 1’s as we have done before.”'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 5 的答案：b. 理由：文本中提到：“为了允许特征的一个仿射函数，我们添加了一个 1 的列，就像我们之前做的那样。”
- en: 3.6.1\. Definitions[#](#definitions "Link to this heading")
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6.1\. 定义[#](#definitions "链接到这个标题")
- en: We summarize the logistic regression approach. Our goal is to find a function
    of the features that approximates the probability of the label \(1\). For this
    purpose, we model the [log-odds](https://en.wikipedia.org/wiki/Logit) (or logit
    function) of the probability of label \(1\) as a linear function of the features
    \(\boldsymbol{\alpha} \in \mathbb{R}^d\)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结逻辑回归方法。我们的目标是找到一个近似标签 1 的概率的特征函数。为此，我们将标签 1 的概率的对数几率（或 logit 函数）建模为特征 \(\boldsymbol{\alpha}
    \in \mathbb{R}^d\) 的线性函数
- en: \[ \log \frac{p(\mathbf{x}; \boldsymbol{\alpha})}{1-p(\mathbf{x}; \boldsymbol{\alpha})}
    = \boldsymbol{\alpha}^T \mathbf{x} \]
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \log \frac{p(\mathbf{x}; \boldsymbol{\alpha})}{1-p(\mathbf{x}; \boldsymbol{\alpha})}
    = \boldsymbol{\alpha}^T \mathbf{x} \]
- en: where \(\mathbf{x} \in \mathbb{R}^d\) is the vector of coefficients (i.e., parameters).
    Inverting this expression gives
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{x} \in \mathbb{R}^d\) 是系数向量（即参数）。通过反转这个表达式给出
- en: \[ p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    \]
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: \[ p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    \]
- en: where the [sigmoid](https://en.wikipedia.org/wiki/Logistic_function)\(\idx{sigmoid
    function}\xdi\) function is
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 sigmoid 函数是
- en: \[ \sigma(z) = \frac{1}{1 + e^{-z}} \]
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma(z) = \frac{1}{1 + e^{-z}} \]
- en: for \(z \in \mathbb{R}\).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 \(z \in \mathbb{R}\)。
- en: '**NUMERICAL CORNER:** We plot the sigmoid function.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们绘制了 sigmoid 函数。'
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![../../_images/beb1329759d4211ded6c56cbd1366fe1f58d01d13f8f8dedb821b6c63b9982c9.png](../Images/0cd8936e1761530c96b5b7087ee4fd19.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/beb1329759d4211ded6c56cbd1366fe1f58d01d13f8f8dedb821b6c63b9982c9.png](../Images/0cd8936e1761530c96b5b7087ee4fd19.png)'
- en: \(\unlhd\)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: We seek to maximize the probability of observing the data (also known as [likelihood
    function](https://en.wikipedia.org/wiki/Likelihood_function)) assuming the labels
    are independent given the features, which is given by (see Chapter 6 for more
    details; for now we are merely setting up the relevant optimization problem)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们寻求最大化观察数据的概率（也称为 [似然函数](https://en.wikipedia.org/wiki/Likelihood_function)），假设在给定特征的情况下标签是独立的，这由以下公式给出（详见第
    6 章，目前我们只是在设置相关的优化问题）。
- en: \[ \mathcal{L}(\mathbf{x}; A, \mathbf{b}) = \prod_{i=1}^n p(\boldsymbol{\alpha}_i;
    \mathbf{x})^{b_i} (1- p(\boldsymbol{\alpha}_i; \mathbf{x}))^{1-b_i}. \]
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathcal{L}(\mathbf{x}; A, \mathbf{b}) = \prod_{i=1}^n p(\boldsymbol{\alpha}_i;
    \mathbf{x})^{b_i} (1- p(\boldsymbol{\alpha}_i; \mathbf{x}))^{1-b_i}. \]
- en: Taking a logarithm, multiplying by \(-1/n\) and substituting the sigmoid function,
    we want to minimize the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)\(\idx{cross-entropy
    loss}\xdi\)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 取对数，乘以 \(-1/n\) 并代入 sigmoid 函数，我们希望最小化 [交叉熵损失](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression)\(\idx{交叉熵损失}\xdi\)。
- en: \[ \ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1-b_i) \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}.
    \]
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1-b_i) \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}.
    \]
- en: 'We used standard properties of the logarithm: for \(x, y > 0\), \(\log(xy)
    = \log x + \log y\) and \(\log(x^y) = y \log x\).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了对数的标准性质：对于 \(x, y > 0\)，\(\log(xy) = \log x + \log y\) 和 \(\log(x^y) =
    y \log x\)。
- en: Hence, we want to solve the minimization problem
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们想要解决最小化问题
- en: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}). \]
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \min_{\mathbf{x} \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b}). \]
- en: We are implicitly using here that the logarithm is a strictly increasing function
    and therefore does not change the global optimum of a function; multiplying by
    \(-1/n\) changed the global maximum into a global minimum.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们隐含地使用了对数是一个严格递增函数的事实，因此它不会改变函数的全局最优解；乘以 \(-1/n\) 将全局最大值变成了全局最小值。
- en: To use gradient descent, we need the gradient of \(\ell\). We use the *Chain
    Rule* and first compute the derivative of \(\sigma\) which is
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用梯度下降，我们需要 \(\ell\) 的梯度。我们使用**链式法则**，首先计算 \(\sigma\) 的导数，它是
- en: \[ \sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}}\left(1
    - \frac{1}{1 + e^{-z}}\right) = \sigma(z) (1 - \sigma(z)). \]
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma'(z) = \frac{e^{-z}}{(1 + e^{-z})^2} = \frac{1}{1 + e^{-z}}\left(1
    - \frac{1}{1 + e^{-z}}\right) = \sigma(z) (1 - \sigma(z)). \]
- en: The latter expression is known as the [logistic differential equation](https://en.wikipedia.org/wiki/Logistic_function#Logistic_differential_equation).
    It arises in a variety of applications, including the modeling of [population
    dynamics](https://towardsdatascience.com/covid-19-infection-in-italy-mathematical-models-and-predictions-7784b4d7dd8d).
    Here it will be a convenient way to compute the gradient.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 后一个表达式被称为[逻辑微分方程](https://en.wikipedia.org/wiki/Logistic_function#Logistic_differential_equation)。它在各种应用中都有出现，包括[人口动力学](https://towardsdatascience.com/covid-19-in-italy-mathematical-models-and-predictions-7784b4d7dd8d)的建模。在这里，它将是一个方便计算梯度的方法。
- en: Observe that, for \(\boldsymbol{\alpha} = (\alpha_{1}, \ldots, \alpha_{d}) \in
    \mathbb{R}^d\), by the *Chain Rule*
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，对于 \(\boldsymbol{\alpha} = (\alpha_{1}, \ldots, \alpha_{d}) \in \mathbb{R}^d\)，通过**链式法则**
- en: \[ \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \nabla (\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \boldsymbol{\alpha} \]
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \nabla (\boldsymbol{\alpha}^T \mathbf{x}) = \sigma'(\boldsymbol{\alpha}^T
    \mathbf{x}) \boldsymbol{\alpha} \]
- en: where, throughout, the gradient is with respect to \(\mathbf{x}\).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，梯度始终是相对于 \(\mathbf{x}\) 的。
- en: Alternatively, we can obtain the same formula by applying the single-variable
    *Chain Rule*
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以通过应用单变量**链式法则**得到相同的公式
- en: \[\begin{align*} \frac{\partial}{\partial x_j} \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x}) \frac{\partial}{\partial
    x_j}(\boldsymbol{\alpha}^T \mathbf{x})\\ &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x})
    \frac{\partial}{\partial x_j}\left(\alpha_{j} x_{j} + \sum_{\ell=1, \ell \neq
    j}^d \alpha_{\ell} x_{\ell}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j} \end{align*}\]
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \frac{\partial}{\partial x_j} \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x}) \frac{\partial}{\partial
    x_j}(\boldsymbol{\alpha}^T \mathbf{x})\\ &= \sigma'(\boldsymbol{\alpha}^T \mathbf{x})
    \frac{\partial}{\partial x_j}\left(\alpha_{j} x_{j} + \sum_{\ell=1, \ell \neq
    j}^d \alpha_{\ell} x_{\ell}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j} \end{align*}\]
- en: so that
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 因此
- en: \[\begin{align*} \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) &= \left(\sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{1}, \ldots,
    \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\,
    \alpha_{d}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}))\, (\alpha_{1}, \ldots, \alpha_{d})\\ &= \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}.
    \end{align*}\]
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla\sigma(\boldsymbol{\alpha}^T \mathbf{x}) &= \left(\sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{1}, \ldots,
    \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\,
    \alpha_{d}\right)\\ &= \sigma(\boldsymbol{\alpha}^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}))\, (\alpha_{1}, \ldots, \alpha_{d})\\ &= \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}.
    \end{align*}\]
- en: By another application of the *Chain Rule*, since \(\frac{\mathrm{d}}{\mathrm{d}
    z} \log z = \frac{1}{z}\),
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**链式法则**的另一种应用，因为 \(\frac{\mathrm{d}}{\mathrm{d} z} \log z = \frac{1}{z}\)，
- en: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= \nabla\left[\frac{1}{n}
    \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) - (1-b_i)
    \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}\right]\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) - \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) + \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}). \end{align*}\]
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \nabla\ell(\mathbf{x}; A, \mathbf{b}) &= \nabla\left[\frac{1}{n}
    \sum_{i=1}^n \left\{- b_i \log(\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) - (1-b_i)
    \log(1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\right\}\right]\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) - \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\\ &= - \frac{1}{n}
    \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) + \frac{1}{n} \sum_{i=1}^n \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \nabla\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}). \end{align*}\]
- en: Using the expression for the gradient of the sigmoid functions, this is equal
    to
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用sigmoid函数的梯度表达式，这等于
- en: \[\begin{align*} &- \frac{1}{n} \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \,\boldsymbol{\alpha}_i\\ &\quad\quad + \frac{1}{n} \sum_{i=1}^n
    \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n \left( b_i (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))
    - (1-b_i)\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) \right)\,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    ) \,\boldsymbol{\alpha}_i. \end{align*}\]
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} &- \frac{1}{n} \sum_{i=1}^n \frac{b_i}{\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \,\boldsymbol{\alpha}_i\\ &\quad\quad + \frac{1}{n} \sum_{i=1}^n
    \frac{1-b_i}{1- \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})} \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) \,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n \left( b_i (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))
    - (1-b_i)\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) \right)\,\boldsymbol{\alpha}_i\\
    &= - \frac{1}{n} \sum_{i=1}^n ( b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    ) \,\boldsymbol{\alpha}_i. \end{align*}\]
- en: 'To implement this formula, it will be useful to re-write it in terms of the
    matrix representation \(A \in \mathbb{R}^{n \times d}\) (which has rows \(\boldsymbol{\alpha}_i^T\),
    \(i = 1,\ldots, n\)) and \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\). Let
    \(\bsigma : \mathbb{R}^n \to \mathbb{R}\) be the vector-valued function that applies
    the sigmoid \(\sigma\) entry-wise, i.e., \(\bsigma(\mathbf{z}) = (\sigma(z_1),\ldots,\sigma(z_n))\)
    where \(\mathbf{z} = (z_1,\ldots,z_n)\). Thinking of \(\sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})\,\boldsymbol{\alpha}_i\) as a linear combination of the columns of
    \(A^T\) with coefficients being the entries of the vector \(\mathbf{b} - \bsigma(A
    \mathbf{x})\), we that'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '为了实现这个公式，将其用矩阵表示 \(A \in \mathbb{R}^{n \times d}\)（其行是 \(\boldsymbol{\alpha}_i^T\)，\(i
    = 1,\ldots, n\)）和 \(\mathbf{b} = (b_1, \ldots, b_n) \in \{0,1\}^n\) 来重写将是有用的。设
    \(\bsigma : \mathbb{R}^n \to \mathbb{R}\) 是一个向量值函数，它逐项应用sigmoid \(\sigma\)，即 \(\bsigma(\mathbf{z})
    = (\sigma(z_1),\ldots,\sigma(z_n))\) 其中 \(\mathbf{z} = (z_1,\ldots,z_n)\)。将 \(\sum_{i=1}^n
    (b_i - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})\,\boldsymbol{\alpha}_i\) 视为
    \(A^T\) 的列的线性组合，其系数是向量 \(\mathbf{b} - \bsigma(A \mathbf{x})\) 的元素，我们得到'
- en: \[ \nabla\ell(\mathbf{x}; A, \mathbf{b}) = - \frac{1}{n} \sum_{i=1}^n ( b_i
    - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i = -\frac{1}{n}
    A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \]
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla\ell(\mathbf{x}; A, \mathbf{b}) = - \frac{1}{n} \sum_{i=1}^n ( b_i
    - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) ) \,\boldsymbol{\alpha}_i = -\frac{1}{n}
    A^T [\mathbf{b} - \bsigma(A \mathbf{x})]. \]
- en: We turn to the Hessian. By symmetry, we can think of the \(j\)-th column of
    the Hessian as the gradient of the partial derivative with respect to \(x_j\).
    Hence we start by computing the gradient of the \(j\)-th entry of the summands
    in the gradient of \(\ell\). We note that, for \(\boldsymbol{\alpha} = (\alpha_{1},
    \ldots, \alpha_{d}) \in \mathbb{R}^d\),
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们转向Hessian矩阵。由于对称性，我们可以将Hessian矩阵的第 \(j\) 列视为关于 \(x_j\) 的偏导数的梯度。因此，我们首先计算梯度中求和项的第
    \(j\) 个元素的梯度。我们注意到，对于 \(\boldsymbol{\alpha} = (\alpha_{1}, \ldots, \alpha_{d})
    \in \mathbb{R}^d\)，
- en: \[ \nabla [(b - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j}] = -
    \nabla [\sigma(\boldsymbol{\alpha}^T \mathbf{x})] \, \alpha_{j} = - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}\alpha_{j}.
    \]
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \nabla [(b - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \alpha_{j}] = -
    \nabla [\sigma(\boldsymbol{\alpha}^T \mathbf{x})] \, \alpha_{j} = - \sigma(\boldsymbol{\alpha}^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}^T \mathbf{x}))\, \boldsymbol{\alpha}\alpha_{j}.
    \]
- en: Thus, using the fact that \(\boldsymbol{\alpha} \alpha_{j}\) is the \(j\)-th
    column of the matrix \(\boldsymbol{\alpha} \boldsymbol{\alpha}^T\), we get
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，利用 \(\boldsymbol{\alpha} \alpha_{j}\) 是矩阵 \(\boldsymbol{\alpha} \boldsymbol{\alpha}^T\)
    的第 \(j\) 列这一事实，我们得到
- en: \[ \mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \]
- en: where \(\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})\) indicates the Hessian
    with respect to the \(\mathbf{x}\) variables, for fixed \(A, \mathbf{b}\).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 \(\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})\) 表示相对于 \(\mathbf{x}\) 变量的Hessian，对于固定的
    \(A, \mathbf{b}\)。
- en: '**LEMMA** **(Convexity of logistic regression)** \(\idx{convexity of logistic
    regression}\xdi\) The function \(\ell(\mathbf{x}; A, \mathbf{b})\) is convex as
    a function of \(\mathbf{x} \in \mathbb{R}^d\). \(\flat\)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(逻辑回归的凸性)** \(\idx{convexity of logistic regression}\xdi\) 函数 \(\ell(\mathbf{x};
    A, \mathbf{b})\) 作为 \(\mathbf{x} \in \mathbb{R}^d\) 的函数是凸的。\(\flat\)'
- en: '*Proof:* Indeed, the Hessian is positive semidefinite: for any \(\mathbf{z}
    \in \mathbb{R}^d\)'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*: 事实上，Hessian 是正半定的：对于任何 \(\mathbf{z} \in \mathbb{R}^d\)'
- en: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \mathbf{z}^T \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \mathbf{z}\\ &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\geq 0 \end{align*}\]
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, \mathbf{z}^T \boldsymbol{\alpha}_i
    \boldsymbol{\alpha}_i^T \mathbf{z}\\ &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\geq 0 \end{align*}\]
- en: since \(\sigma(t) \in [0,1]\) for all \(t\). \(\square\)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对于所有 \(t\)，\(\sigma(t) \in [0,1]\)。\(\square\)
- en: Convexity is one reason for working with the cross-entropy loss ([rather than
    the mean squared error](https://math.stackexchange.com/questions/1582452/logistic-regression-prove-that-the-cost-function-is-convex)
    for instance).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 凸性是使用交叉熵损失（例如，而不是均方误差 [https://math.stackexchange.com/questions/1582452/logistic-regression-prove-that-the-cost-function-is-convex]）的原因之一。
- en: '**LEMMA** **(Smoothness of logistic regression)** The function \(\ell(\mathbf{x};
    A, \mathbf{b})\) is \(L\)-smooth for'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**引理** **(逻辑回归的平滑性)** 函数 \(\ell(\mathbf{x}; A, \mathbf{b})\) 对于'
- en: \[ L= \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 = \frac{1}{4n} \|A\|_F^2.
    \]
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: \[ L= \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2 = \frac{1}{4n} \|A\|_F^2.
    \]
- en: \(\flat\)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \(\flat\)
- en: '*Proof:* We use convexity and the expression for the Hessian to derive that,
    for any unit vector \(\mathbf{z} \in \mathbb{R}^d\),'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*: 我们使用凸性和Hessian的表达式推导出，对于任何单位向量 \(\mathbf{z} \in \mathbb{R}^d\),'
- en: \[\begin{align*} 0 \leq \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2.
    \end{align*}\]
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} 0 \leq \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2.
    \end{align*}\]
- en: We need to find the maximum value that the factor \(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\) can take. Note
    that \(\sigma(t) \in [0,1]\) for all \(t\) and \(\sigma(t) + (1 - \sigma(t)) =
    1\). Taking the derivatives of the function \(f(w) = w (1 - w) = w - w^2\) we
    get \(f'(w) = 1 - 2 w\) and \(f''(w) = -2\). So \(f\) is concave and achieve its
    maximum at \(w^* = 1/2\) where it takes the value \(f(1/2) = 1/4\). We have proved
    that
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找到因子 \(\sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}))\) 可以取的最大值。注意，对于所有 \(t\)，\(\sigma(t) \in [0,1]\)，且 \(\sigma(t) + (1
    - \sigma(t)) = 1\)。对函数 \(f(w) = w (1 - w) = w - w^2\) 求导，我们得到 \(f'(w) = 1 - 2
    w\) 和 \(f''(w) = -2\)。因此，\(f\) 是凹函数，并在 \(w^* = 1/2\) 处达到最大值，此时 \(f(1/2) = 1/4\)。我们已经证明了
- en: \[ \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \leq 1/4 \]
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}) (1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) \leq 1/4 \]
- en: for any \(\mathbf{z}\).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 \(\mathbf{z}\)。
- en: Going back to the upper bound on \(\mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x};
    A, \mathbf{b}) \,\mathbf{z}\) we get
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 \(\mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b}) \,\mathbf{z}\)
    的上界，我们得到
- en: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\leq \frac{1}{4n} \sum_{i=1}^n (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\mathbf{z}\|^2 \|\boldsymbol{\alpha}_i\|^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2, \end{align*}\]
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \mathbf{z}^T \,\mathbf{H}_{\ell}(\mathbf{x}; A, \mathbf{b})
    \,\mathbf{z} &= \frac{1}{n} \sum_{i=1}^n \sigma(\boldsymbol{\alpha}_i^T \mathbf{x})
    (1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\, (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\
    &\leq \frac{1}{4n} \sum_{i=1}^n (\mathbf{z}^T \boldsymbol{\alpha}_i)^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\mathbf{z}\|^2 \|\boldsymbol{\alpha}_i\|^2\\ &\leq
    \frac{1}{4n} \sum_{i=1}^n \|\boldsymbol{\alpha}_i\|^2, \end{align*}\]
- en: where we used the *Cauchy-Schwarz inequality* on the third line and the fact
    that \(\mathbf{z}\) is a unit vector on the fourth one.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，我们在第三行使用了柯西-施瓦茨不等式，在第四行使用了 \(\mathbf{z}\) 是单位向量的事实。
- en: That implies \(L\)-smoothness. \(\square\)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 \(L\)-平滑性。 \(\square\)
- en: For step size \(\beta\), one step of gradient descent is therefore
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对于步长 \(\beta\)，梯度下降的一步因此是
- en: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{n} \sum_{i=1}^n ( b_i -
    \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i. \]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mathbf{x}^{t+1} = \mathbf{x}^{t} +\beta \frac{1}{n} \sum_{i=1}^n ( b_i -
    \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}^t) ) \,\boldsymbol{\alpha}_i. \]
- en: 3.6.2\. Implementation[#](#implementation "Link to this heading")
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6.2\. 实现方法[#](#implementation "链接到这个标题")
- en: We modify our implementation of gradient descent to take a dataset as input.
    Recall that to run gradient descent, we first implement a function computing a
    descent update. It takes as input a function `grad_fn` computing the gradient
    itself, as well as a current iterate and a step size. We now also feed a dataset
    as additional input.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修改了梯度下降的实现，使其接受一个数据集作为输入。回想一下，要运行梯度下降，我们首先实现一个计算下降更新的函数。它接受一个计算梯度的 `grad_fn`
    函数，以及当前迭代和步长。我们现在还提供数据集作为额外的输入。
- en: '[PRE12]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We are ready to implement gradient descent. Our function takes as input a function
    `loss_fn` computing the objective, a function `grad_fn` computing the gradient,
    the dataset `A` and `b`, and an initial guess `init_x`. Optional parameters are
    the step size and the number of iterations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备实现梯度下降。我们的函数接受一个计算目标函数的 `loss_fn` 函数，一个计算梯度的 `grad_fn` 函数，数据集 `A` 和 `b`，以及一个初始猜测
    `init_x`。可选参数是步长和迭代次数。
- en: '[PRE13]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To implement `loss_fn` and `grad_fn`, we define the sigmoid as above. Below,
    `pred_fn` is \(\bsigma(A \mathbf{x})\). Here we write the loss function as
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现 `loss_fn` 和 `grad_fn`，我们定义了上述的 sigmoid 函数。下面，`pred_fn` 是 \(\bsigma(A \mathbf{x})\)。在这里，我们将损失函数写成
- en: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right), \end{align*}\]
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: \[\begin{align*} \ell(\mathbf{x}; A, \mathbf{b}) &= \frac{1}{n} \sum_{i=1}^n
    \left\{- b_i \log(\sigma(\boldsymbol{\alpha_i}^T \mathbf{x})) - (1-b_i) \log(1-
    \sigma(\boldsymbol{\alpha_i}^T \mathbf{x}))\right\}\\ &= \mathrm{mean}\left(-\mathbf{b}
    \odot \mathbf{log}(\bsigma(A \mathbf{x})) - (\mathbf{1} - \mathbf{b}) \odot \mathbf{log}(\mathbf{1}
    - \bsigma(A \mathbf{x}))\right), \end{align*}\]
- en: where \(\odot\) is the Hadamard product, or element-wise product (for example
    \(\mathbf{u} \odot \mathbf{v} = (u_1 v_1, \ldots,u_n v_n)\)), the logarithm (denoted
    in bold) is applied element-wise and \(\mathrm{mean}(\mathbf{z})\) is the mean
    of the entries of \(\mathbf{z}\) (i.e., \(\mathrm{mean}(\mathbf{z}) = n^{-1} \sum_{i=1}^n
    z_i\)).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，\(\odot\) 是哈达玛积，或逐元素乘积（例如 \(\mathbf{u} \odot \mathbf{v} = (u_1 v_1, \ldots,u_n
    v_n)\)），对数（用粗体表示）是逐元素应用，\(\mathrm{mean}(\mathbf{z})\) 是 \(\mathbf{z}\) 的元素平均值（即，\(\mathrm{mean}(\mathbf{z})
    = n^{-1} \sum_{i=1}^n z_i\))。
- en: '[PRE14]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can choose a step size based on the smoothness of the objective as above.
    Recall that [`numpy.linalg.norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)
    computes the Frobenius norm by default.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据上述目标函数的平滑性选择一个步长。回想一下，`numpy.linalg.norm` 默认计算 Frobenius 范数。
- en: '[PRE15]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**NUMERICAL CORNER:** We return to our original motivation, the [airline customer
    satisfaction](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction)
    dataset. We first load the dataset. We will need the column names later.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值角**: 我们回归到我们的原始动机，即[航空公司客户满意度](https://www.kaggle.com/datasets/sjleshrac/airlines-customer-satisfaction)数据集。我们首先加载这个数据集。我们稍后会需要列名。'
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Our goal will be to predict the first column, `Satisfied`, from the rest of
    the columns. For this, we transform our data into NumPy arrays. We also standardize
    the columns by subtracting their mean and dividing by their standard deviation.
    This will allow to compare the influence of different features on the prediction.
    And we add a column of 1s to account for the intercept.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标将是预测第一列，即`Satisfied`，从其余的列中。为此，我们将数据转换为NumPy数组。我们还通过减去它们的平均值并除以它们的标准差来标准化列。这将允许我们比较不同特征对预测的影响。并且我们添加一列1来考虑截距。
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We use the functions `loss_fn` and `grad_fn` which were written for general
    logistic regression problems.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用为一般逻辑回归问题编写的函数 `loss_fn` 和 `grad_fn`。
- en: '[PRE19]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To interpret the results, we plot the coefficients in decreasing order.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释结果，我们按降序排列系数进行绘图。
- en: '[PRE21]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![../../_images/4a66052e179e6e561e9711defd186394eac14fb13f59c0250fcce6f357c91144.png](../Images/f0f3c46cb4f17a7ae15de744d08dd196.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![../../_images/4a66052e179e6e561e9711defd186394eac14fb13f59c0250fcce6f357c91144.png](../Images/f0f3c46cb4f17a7ae15de744d08dd196.png)'
- en: We see from the first ten bars or so that, as might be expected, higher ratings
    on various aspects of the flight generally contribute to a higher predicted likelihood
    of satisfaction (with one exception being `Gate location` whose coefficient is
    negative but may not be [statistically significant](https://en.wikipedia.org/wiki/Statistical_significance)).
    `Inflight entertainment` seems particularly influential. `Age` also shows the
    same pattern, something we had noticed in the introductory section through a different
    analysis. On the other hand, `Departure Delay in Minutes` and `Arrival Delay in
    Minutes` contribute to a lower predicted likelihood of satisfaction, again an
    expected pattern. The most negative influence however appears to come from `Class_Eco`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 从前十个条形图或更多可以看出，正如预期的那样，飞行各方面的高评分通常有助于提高预测的满意度可能性（有一个例外是`登机口位置`，其系数为负，但可能不是[统计上显著的](https://en.wikipedia.org/wiki/Statistical_significance)）。`机上娱乐`似乎特别有影响力。`年龄`也显示出相同的模式，这是我们通过不同的分析在介绍部分注意到的。另一方面，`出发延误时间（分钟）`和`到达延误时间（分钟）`对预测的满意度可能性有负面影响，这也是一个预期的模式。然而，最负面的影响似乎来自`Class_Eco`。
- en: '**CHAT & LEARN** There are faster methods for logistic regression. Ask your
    favorite AI chatbot for an explanation and implementation of the iteratively reweighted
    least squares method. Try it on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天与学习**: 对于逻辑回归，有更快的方法。向你的喜欢的AI聊天机器人询问迭代加权最小二乘法的解释和实现。尝试在这个数据集上使用它。([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
- en: '**TRY IT!** One can attempt to predict whether a new customer, whose feature
    vector is \(\boldsymbol{\alpha}\), will be satisfied by using the prediction function
    \(p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T \mathbf{x})\),
    where \(\mathbf{x}\) is the fitted coefficients. Say a customer is predicted to
    be satisfied if \(\sigma(\boldsymbol{\alpha}^T \mathbf{x}) > 0.5\). Implement
    this predictor and compute its accuracy on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**尝试一下！** 可以尝试使用预测函数 \(p(\mathbf{x}; \boldsymbol{\alpha}) = \sigma(\boldsymbol{\alpha}^T
    \mathbf{x})\) 来预测一个新客户（其特征向量为 \(\boldsymbol{\alpha}\)）是否会满意，其中 \(\mathbf{x}\)
    是拟合的系数。如果一个客户被预测为满意，如果 \(\sigma(\boldsymbol{\alpha}^T \mathbf{x}) > 0.5\)。实现这个预测器并计算其在数据集上的准确率。([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))'
- en: '**CHAT & LEARN** Because of the issue of overfitting, computing the accuracy
    of a predictor on a dataset used to estimate the coefficients is problematic.
    Ask your favorite AI chatbot about scikit-learn’s `train_test_split` function
    and how it helps resolve this issue. Implement it on this dataset. ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**CHAT & LEARN** 由于过拟合的问题，在用于估计系数的数据集上计算预测器的准确度是有问题的。向你的首选AI聊天机器人询问scikit-learn的`train_test_split`函数以及它是如何帮助解决这个问题，并在该数据集上实现它。([在Colab中打开](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_opt_notebook.ipynb))
    \(\ddagger\)'
- en: \(\unlhd\)
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: \(\unlhd\)
- en: '***Self-assessment quiz*** *(with help from Claude, Gemini, and ChatGPT)*'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '***自我评估测验*** *(由Claude, Gemini和ChatGPT协助)*'
- en: '**1** What is the primary goal of logistic regression?'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**1** 逻辑回归的主要目标是什么？'
- en: a) To predict a continuous outcome variable.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: a) 预测一个连续的因变量。
- en: b) To classify data points into multiple categories.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: b) 将数据点分类到多个类别。
- en: c) To model the probability of a binary outcome.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: c) 建模二元结果的概率。
- en: d) To find the optimal linear combination of features.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: d) 找到特征的最优线性组合。
- en: '**2** What is the relationship between maximizing the likelihood function and
    minimizing the cross-entropy loss in logistic regression?'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**2** 在逻辑回归中，最大化似然函数和最小化交叉熵损失之间的关系是什么？'
- en: a) They are unrelated concepts.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: a) 它们是无关的概念。
- en: b) Maximizing the likelihood is equivalent to minimizing the cross-entropy loss.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: b) 最大化似然等价于最小化交叉熵损失。
- en: c) Minimizing the cross-entropy loss is a first step towards maximizing the
    likelihood.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: c) 最小化交叉熵损失是最大化似然的第一步。
- en: d) Maximizing the likelihood is a special case of minimizing the cross-entropy
    loss.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: d) 最大化似然是最小化交叉熵损失的特殊情况。
- en: '**3** Which of the following is the correct formula for the logistic regression
    objective function (cross-entropy loss)?'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**3** 以下哪个是逻辑回归目标函数（交叉熵损失）的正确公式？'
- en: a) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \log(1 + \exp(-b_i
    \boldsymbol{\alpha}_i^T \mathbf{x}))\)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: a) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \log(1 + \exp(-b_i
    \boldsymbol{\alpha}_i^T \mathbf{x}))\)
- en: b) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}))^2\)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: b) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n (b_i - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}))^2\)
- en: c) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\}\)
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: c) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\}\)
- en: d) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n b_i \boldsymbol{\alpha}_i^T
    \mathbf{x}\)
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: d) \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n b_i \boldsymbol{\alpha}_i^T
    \mathbf{x}\)
- en: '**4** What is the purpose of standardizing the input features in the airline
    customer satisfaction dataset example?'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**4** 在航空公司客户满意度数据集示例中，标准化输入特征的目的是什么？'
- en: a) To ensure the objective function is convex.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: a) 确保目标函数是凸的。
- en: b) To speed up the convergence of gradient descent.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: b) 加速梯度下降的收敛速度。
- en: c) To allow comparison of the influence of different features on the prediction.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: c) 允许比较不同特征对预测的影响。
- en: d) To handle missing data in the dataset.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: d) 处理数据集中的缺失数据。
- en: '**5** Which of the following is a valid reason for adding a column of 1’s to
    the feature matrix in logistic regression?'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**5** 以下哪个是向逻辑回归特征矩阵中添加一列1的有效理由？'
- en: a) To ensure the objective function is convex.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: a) 确保目标函数是凸的。
- en: b) To allow for an intercept term in the model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: b) 在模型中允许截距项。
- en: c) To standardize the input features.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: c) 标准化输入特征。
- en: d) To handle missing data in the dataset.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: d) 处理数据集中的缺失数据。
- en: 'Answer for 1: c. Justification: The text states that the goal of logistic regression
    is to “find a function of the features that approximates the probability of the
    label.”'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 1题的答案：c. 理由：文本中提到逻辑回归的目的是“找到一个特征函数来近似标签的概率。”
- en: 'Answer for 2: b. Justification: The text states: “We seek to maximize the probability
    of observing the data… which is given by… Taking a logarithm, multiplying by \(-1/n\)
    and substituting the sigmoid function, we want to minimize the cross-entropy loss.”'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 2题的答案：b. 理由：文本中提到：“我们寻求最大化观察数据的概率……这由……给出……取对数，乘以 \(-1/n\) 并代入sigmoid函数，我们想要最小化交叉熵损失。”
- en: 'Answer for 3: c. Justification: The text states: “Hence, we want to solve the
    minimization problem \(\min_{x \in \mathbb{R}^d} \ell(\mathbf{x}; A, \mathbf{b})\),”
    where \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T \mathbf{x}))\}\).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 答案3：c. 证明：文本中提到：“因此，我们想要解决最小化问题 \(\min_{x \in \mathbb{R}^d} \ell(\mathbf{x};
    A, \mathbf{b})\),” 其中 \(\ell(\mathbf{x}; A, \mathbf{b}) = \frac{1}{n} \sum_{i=1}^n
    \{-b_i \log(\sigma(\boldsymbol{\alpha}_i^T \mathbf{x})) - (1 - b_i) \log(1 - \sigma(\boldsymbol{\alpha}_i^T
    \mathbf{x}))\}\).
- en: 'Answer for 4: c. Justification: The text states: “We also standardize the columns
    by subtracting their mean and dividing by their standard deviation. This will
    allow to compare the influence of different features on the prediction.”'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 答案4：c. 证明：文本中提到：“我们还通过减去它们的均值并除以标准差来标准化列。这将允许我们比较不同特征对预测的影响。”
- en: 'Answer for 5: b. Justification: The text states: “To allow an affine function
    of the features, we add a column of 1’s as we have done before.”'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 答案5：b. 证明：文本中提到：“为了允许特征的一个仿射函数，我们添加了一个1的列，就像我们之前做的那样。”
