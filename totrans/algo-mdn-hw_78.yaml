- en: Binary Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://en.algorithmica.org/hpc/data-structures/binary-search/](https://en.algorithmica.org/hpc/data-structures/binary-search/)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: While improving the speed of user-facing applications is the end goal of performance
    engineering, people don’t really get excited over 5-10% improvements in some databases.
    Yes, this is what software engineers are paid for, but these types of optimizations
    tend to be too intricate and system-specific to be readily generalized to other
    software.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, the most fascinating showcases of performance engineering are multifold
    optimizations of textbook algorithms: the kinds that everybody knows and deemed
    so simple that it would never even occur to try to optimize them in the first
    place. These optimizations are simple and instructive and can very much be adopted
    elsewhere. And they are surprisingly not as rare as you’d think.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we focus on one such fundamental algorithm — *binary search*
    — and implement two of its variants that are, depending on the problem size, up
    to 4x faster than `std::lower_bound`, while being under just 15 lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: The first algorithm achieves that by removing [branches](/hpc/pipelining/branching),
    and the second also optimizes the memory layout to achieve better [cache system](/hpc/cpu-cache)
    performance. This technically disqualifies it from being a drop-in replacement
    for `std::lower_bound` as it needs to permute the elements of the array before
    it can start answering queries — but I can’t recall a lot of scenarios where you
    obtain a sorted array but can’t afford to spend linear time on preprocessing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The usual disclaimer: the CPU is a [Zen 2](https://www.7-cpu.com/cpu/Zen2.html),
    the RAM is a [DDR4-2666](/hpc/cpu-cache/), and the compiler we will be using by
    default is Clang 10\. The performance on your machine may be different, so I highly
    encourage to [go and test it](https://godbolt.org/z/14rd5Pnve) for yourself.'
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#binary-search)Binary
    Search'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the standard way of searching for the first element not less than `x`
    in a sorted array `t` of `n` integers that you can find in any introductory computer
    science textbook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Find the middle element of the search range, compare it to `x`, shrink the range
    in half. Beautiful in its simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar approach is employed by `std::lower_bound`, except that it needs
    to be more generic to support containers with non-random-access iterators and
    thus uses the first element and the size of the search interval instead of the
    two of its ends. To this end, implementations from both [Clang](https://github.com/llvm-mirror/libcxx/blob/78d6a7767ed57b50122a161b91f59f19c9bd0d19/include/algorithm#L4169)
    and [GCC](https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algobase.h#L1023)
    use this metaprogramming monstrosity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If the compiler is successful in removing the abstractions, it compiles to
    roughly the same machine code and yields roughly the same average latency, which
    [expectedly](/hpc/cpu-cache/latency) grows with the array size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/0d8b6b224cf1d3bc705727ef6612ae2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Since most people don’t implement binary search by hand, we will use `std::lower_bound`
    from Clang as the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#the-bottleneck)The
    Bottleneck'
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping to the optimized implementations, let’s briefly discuss why binary
    search is slow in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run `std::lower_bound` with [perf](/hpc/profiling/events), you’ll see
    that it spends most of its time on a [conditional jump](/hpc/architecture/loops)
    instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This [pipeline stall](/hpc/) stops the search from progressing, and it is mainly
    caused by two [factors](/hpc/pipelining/hazards):'
  prefs: []
  type: TYPE_NORMAL
- en: We suffer a *control hazard* because we have a [branch](/hpc/pipelining/branching)
    that is impossible to predict (queries and keys are drawn independently at random),
    and the processor has to halt for 10-15 cycles to flush the pipeline and fill
    it back on each branch mispredict.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We suffer a *data hazard* because we have to wait for the preceding comparison
    to complete, which in turn waits for one of its operands to be fetched from the
    memory — and it [may take](/hpc/cpu-cache/latency) anywhere between 0 and 300
    cycles, depending on where it is located.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s try to get rid of these obstacles one by one.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#removing-branches)Removing
    Branches'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can replace branching with [predication](/hpc/pipelining/branchless). To
    make the task easier, we can adopt the STL approach and rewrite the loop using
    the first element and the size of the search interval (instead of its first and
    last element):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that, on each iteration, `len` is essentially just halved and then either
    floored or ceiled, depending on how the comparison went. This conditional update
    seems unnecessary; to avoid it, we can simply say that it’s always ceiled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This way, we only need to update the first element of the search interval with
    a [conditional move](/hpc/pipelining/branchless/) and halve its size on each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note that this loop is not always equivalent to the standard binary search.
    Since it always rounds *up* the size of the search interval, it accesses slightly
    different elements and may perform one comparison more than needed. Apart from
    simplifying computations on each iteration, it also makes the number of iterations
    constant if the array size is constant, removing branch mispredictions completely.
  prefs: []
  type: TYPE_NORMAL
- en: 'As typical for predication, this trick is very fragile to compiler optimizations
    — depending on the compiler and how the function is invoked, it may still leave
    a branch or generate suboptimal code. It works fine on Clang 10, yielding a 2.5-3x
    improvement on small arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d1de0a9e2468bfe1337d27ec18713602.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One interesting detail is that it performs worse on large arrays. It seems
    weird: the total delay is dominated by the RAM latency, and since it does roughly
    the same memory accesses as the standard binary search, it should be roughly the
    same or even slightly better.'
  prefs: []
  type: TYPE_NORMAL
- en: The real question you need to ask is not why the branchless implementation is
    worse but why the branchy version is better. It happens because when you have
    branching, the CPU can [speculate](/hpc/pipelining/branching/) on one of the branches
    and start fetching either the left or the right key before it can even confirm
    that it is the right one — which effectively acts as implicit [prefetching](/hpc/cpu-cache/prefetching).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the branchless implementation, this doesn’t happen, as `cmov` is treated
    as every other instruction, and the branch predictor doesn’t try to peek into
    its operands to predict the future. To compensate for this, we can prefetch the
    data in software by explicitly requesting the left and right child key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'With prefetching, the performance on large arrays becomes roughly the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/daa69a5238a8e64bfaf7b75f8ce5111a.png)'
  prefs: []
  type: TYPE_IMG
- en: The graph still grows faster as the branchy version also prefetches “grandchildren,”
    “great-grandchildren,” and so on — although the usefulness of each new speculative
    read diminishes exponentially as the prediction is less and less likely to be
    correct.
  prefs: []
  type: TYPE_NORMAL
- en: In the branchless version, we could also fetch ahead by more than one layer,
    but the number of fetches we’d need also grows exponentially. Instead, we will
    try a different approach to optimize memory operations.
  prefs: []
  type: TYPE_NORMAL
- en: '## [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#optimizing-the-layout)Optimizing
    the Layout'
  prefs: []
  type: TYPE_NORMAL
- en: 'The memory requests we perform during binary search form a very specific access
    pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/ba0f19f678d02d700f8434f1b6859735.png)'
  prefs: []
  type: TYPE_IMG
- en: How likely is it that the elements on each request are cached? How good is their
    [data locality](/hpc/external-memory/locality/)?
  prefs: []
  type: TYPE_NORMAL
- en: '*Spatial locality* seems to be okay for the last 3 to 4 requests that are likely
    to be on the same [cache line](/hpc/cpu-cache/cache-lines) — but all the previous
    requests require huge memory jumps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Temporal locality* seems to be okay for the first dozen or so requests — there
    aren’t that many different comparison sequences of this length, so we will be
    comparing against the same middle elements over and over, which are likely to
    be cached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To illustrate how important the second type of cache sharing is, let’s try
    to pick the element we will compare to on each iteration randomly among the elements
    of the search interval, instead of the middle one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Theoretically](#appendix-random-binary-search), this randomized binary search
    is expected to do 30-40% more comparisons than the normal one, but on a real computer,
    the running time goes ~6x on large arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/3d703a992e5db0ba9b92db99c332f6ce.png)'
  prefs: []
  type: TYPE_IMG
- en: This isn’t just caused by the `rand()` call being slow. You can clearly see
    the point on the L2-L3 boundary where memory latency outweighs the random number
    generation and [modulo](/hpc/arithmetic/division). The performance degrades because
    all of the fetched elements are unlikely to be cached and not just some small
    suffix of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another potential negative effect is that of [cache associativity](/hpc/cpu-cache/associativity).
    If the array size is a multiple of a large power of two, then the indices of these
    “hot” elements will also be divisible by some large powers of two and map to the
    same cache line, kicking each other out. For example, binary searching over arrays
    of size $2^{20}$ takes about ~360ns per query while searching over arrays of size
    $(2^{20} + 123)$ takes ~300ns — a 20% difference. There are [ways](https://en.wikipedia.org/wiki/Fibonacci_search_technique)
    to fix this problem, but to not get distracted from more pressing matters, we
    are just going to ignore it: all array sizes we use are in the form of $\lfloor
    1.17^k \rfloor$ for integer $k$ so that any cache side effects are unlikely.'
  prefs: []
  type: TYPE_NORMAL
- en: The real problem with our memory layout is that it doesn’t make the most efficient
    use of temporal locality because it groups hot and cold elements together. For
    example, we likely store the element $\lfloor n/2 \rfloor$, which we request the
    first thing on each query, in the same cache line with $\lfloor n/2 \rfloor +
    1$, which we almost never request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the heatmap visualizing the expected frequency of comparisons for a
    31-element array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/97149b2f72e5c7dd2e42f068f661650a.png)'
  prefs: []
  type: TYPE_IMG
- en: So, ideally, we’d want a memory layout where hot elements are grouped with hot
    elements, and cold elements are grouped with cold elements. And we can achieve
    this if we permute the array in a more cache-friendly way by renumbering them.
    The numeration we will use is actually half a millennium old, and chances are,
    you already know it.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#eytzinger-layout)Eytzinger
    Layout'
  prefs: []
  type: TYPE_NORMAL
- en: '**Michaël Eytzinger** is a 16th-century Austrian nobleman known for his work
    on genealogy, particularly for a system for numbering ancestors called *ahnentafel*
    (German for “ancestor table”).'
  prefs: []
  type: TYPE_NORMAL
- en: Ancestry mattered a lot back then, but writing down that data was expensive.
    *Ahnentafel* allows displaying a person’s genealogy compactly, without wasting
    extra space by drawing diagrams.
  prefs: []
  type: TYPE_NORMAL
- en: It lists a person’s direct ancestors in a fixed sequence of ascent. First, the
    person themselves is listed as number 1, and then, recursively, for each person
    numbered $k$, their father is listed as $2k$ and their mother as $(2k+1)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the example for [Paul I](https://en.wikipedia.org/wiki/Paul_I_of_Russia),
    the great-grandson of [Peter the Great](https://en.wikipedia.org/wiki/Peter_the_Great):'
  prefs: []
  type: TYPE_NORMAL
- en: Paul I
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Peter III (Paul’s father)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[Catherine II](https://en.wikipedia.org/wiki/Catherine_the_Great) (Paul’s mother)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Charles Frederick (Peter’s father, Paul’s paternal grandfather)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Anna Petrovna (Peter’s mother, Paul’s paternal grandmother)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Christian August (Catherine’s father, Paul’s maternal grandfather)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Johanna Elisabeth (Catherine’s mother, Paul’s maternal grandmother)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apart from being compact, it has some nice properties, like that all even-numbered
    persons are male and all odd-numbered (possibly except for 1) are female. One
    can also find the number of a particular ancestor only knowing the genders of
    their descendants. For example, Peter the Great’s bloodline is Paul I → Peter
    III → Anna Petrovna → Peter the Great, so his number should be $((1 \times 2)
    \times 2 + 1) \times 2 = 10$.
  prefs: []
  type: TYPE_NORMAL
- en: '**In computer science**, this enumeration has been widely used for implicit
    (pointer-free) implementations of heaps, segment trees, and other binary tree
    structures — where instead of names, it stores underlying array items.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how this layout looks when applied to binary search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/e9da87bd9fd823f9afa85785500ce7a4.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the tree is slightly imbalanced (because of the last layer is continuous)
  prefs: []
  type: TYPE_NORMAL
- en: 'When searching in this layout, we just need to start from the first element
    of the array, and then on each iteration jump to either $2 k$ or $(2k + 1)$, depending
    on how the comparison went:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/228f35b59f050f682d228f93babb8656.png)'
  prefs: []
  type: TYPE_IMG
- en: You can immediately see how its temporal locality is better (and, in fact, theoretically
    optimal) as the elements closer to the root are closer to the beginning of the
    array and thus are more likely to be fetched from the cache.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/d7f49b3b16fdf4021d6bf5a913e91021.png)'
  prefs: []
  type: TYPE_IMG
- en: Another way to look at it is that we write every even-indexed element to the
    end of the new array, then write every even-indexed element of the remaining ones
    right before them, and so on, until we place the root as the first element.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#construction)Construction'
  prefs: []
  type: TYPE_NORMAL
- en: 'To construct the Eytzinger array, we could do this even-odd [filtering](/hpc/simd/shuffling/#permutations-and-lookup-tables)
    $O(\log n)$ times — and, perhaps, this is the fastest approach — but for brevity,
    we will instead build it by traversing the original search tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes the current node number `k`, recursively writes out all
    elements to the left of the middle of the search interval, writes out the current
    element we’d compare against, and then recursively writes out all the elements
    on the right. It seems a bit complicated, but to convince yourself that it works,
    you only need three observations:'
  prefs: []
  type: TYPE_NORMAL
- en: It writes exactly `n` elements as we enter the body of `if` for each `k` from
    `1` to `n` just once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It writes out sequential elements from the original array as it increments the
    `i` pointer each time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the time we write the element at node `k`, we will have already written all
    the elements to its left (exactly `i`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Despite being recursive, it is actually quite fast as all the memory reads
    are sequential, and the memory writes are only in $O(\log n)$ different memory
    blocks at a time. Maintaining the permutation is both logically and computationally
    harder to maintain though: adding an element to a sorted array only requires shifting
    a suffix of its elements one position to the right, while Eytzinger array practically
    needs to be rebuilt from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this traversal and the resulting permutation are not exactly equivalent
    to the “tree” of vanilla binary search: for example, the left child subtree may
    be larger than the right child subtree — up to twice as large — but it doesn’t
    matter much since both approaches result in the same $\lceil \log_2 n \rceil$
    tree depth.'
  prefs: []
  type: TYPE_NORMAL
- en: Also note that the Eytzinger array is one-indexed — this will be important for
    performance later. You can put in the zeroth element the value that you want to
    be returned in the case when the lower bound doesn’t exist (similar to `a.end()`
    for `std::lower_bound`).
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#search-implementation)Search
    Implementation'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now descend this array using only indices: we just start with $k=1$
    and execute $k := 2k$ if we need to go left and $k := 2k + 1$ if we need to go
    right. We don’t even need to store and recalculate the search boundaries anymore.
    This simplicity also lets us avoid branching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The only problem arises when we need to restore the index of the resulting
    element, as $k$ does not directly point to it. Consider this example (its corresponding
    tree is listed above):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here we query the array of $[0, …, 9]$ for the lower bound of $x=3$. We compare
    it against $6$, $3$, $1$, and $2$, go left-left-right-right, and end up with $k
    = 19$, which isn’t even a valid array index.
  prefs: []
  type: TYPE_NORMAL
- en: The trick is to notice that, unless the answer is the last element of the array,
    we compare $x$ against it at some point, and after we’ve learned that it is not
    less than $x$, we go left exactly once and then keep going right until we reach
    a leaf (because we will only be comparing $x$ against lesser elements). Therefore,
    to restore the answer, we just need to “cancel” some number of right turns and
    then one more.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done in an elegant way by observing that the right turns are recorded
    in the binary representation of $k$ as 1-bits, and so we just need to find the
    number of trailing 1s in the binary representation and right-shift $k$ by exactly
    that number of bits plus one. To do this, we can invert the number (`~k`) and
    call the “find first set” instruction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We run it, and… well, it doesn’t look *that* good:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/968d4f0e9353467d25cbc991c2c23c85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The latency on smaller arrays is on par with the branchless binary search implementation
    — which isn’t surprising as it is just two lines of code — but it starts taking
    off much sooner. The reason is that the Eytzinger binary search doesn’t get the
    advantage of spatial locality: the last 3-4 elements we compare against are not
    in the same cache line anymore, and we have to fetch them separately.'
  prefs: []
  type: TYPE_NORMAL
- en: If you think about it deeper, you might object that the improved temporal locality
    should compensate for that. Before, we were using only about $\frac{1}{16}$-th
    of the cache line to store one hot element, and now we are using all of it, so
    the effective cache size is larger by a factor of 16, which lets us cover $\log_2
    16 = 4$ more first requests.
  prefs: []
  type: TYPE_NORMAL
- en: But if you think about it more, you understand that this isn’t enough compensation.
    Caching the other 15 elements wasn’t completely useless, and also, the hardware
    prefetcher could fetch the neighboring cache lines of our requests. If this was
    one of our last requests, the rest of what we will be reading will probably be
    cached elements. So actually, the last 6-7 accesses are likely to be cached, not
    3-4.
  prefs: []
  type: TYPE_NORMAL
- en: It seems like we did an overall stupid thing switching to this layout, but there
    is a way to make it worthwhile.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#prefetching)Prefetching'
  prefs: []
  type: TYPE_NORMAL
- en: 'To hide the memory latency, we can use software prefetching similar to how
    we did for branchless binary search. But instead of issuing two separate prefetch
    instructions for the left and right child nodes, we can notice that they are neighbors
    in the Eytzinger array: one has index $2 k$ and the other $(2k + 1)$, so they
    are likely in the same cache line, and we can use just one instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This observation extends to the grand-children of node $k$ — they are also
    stored sequentially:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Their cache line can also be fetched with one instruction. Interesting… what
    if we continue this, and instead of fetching direct children, we fetch ahead as
    many descendants as we can cramp into one cache line? That would be $\frac{64}{4}
    = 16$ elements, our great-great-grandchildren with indices from $16k$ to $(16k
    + 15)$.
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we prefetch just one of these 16 elements, we will probably only get
    some but not all of them, as they may cross a cache line boundary. We can prefetch
    the first *and* the last element, but to get away with just one memory request,
    we need to notice that the index of the first element, $16k$, is divisible by
    $16$, so its memory address will be the base address of the array plus something
    divisible by $16 \cdot 4 = 64$, the cache line size. If the array were to begin
    on a cache line, then these $16$ great-great-grandchildren elements will be guaranteed
    to be on a single cache line, which is just what we needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we only need to [align](/hpc/cpu-cache/alignment) the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And then prefetch the element indexed $16 k$ on each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance on large arrays improves 3-4x from the previous version and
    ~2x compared to `std::lower_bound`. Not bad for just two more lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/03052e594c2dc64bc4d19a90ee17b8cb.png)'
  prefs: []
  type: TYPE_IMG
- en: Essentially, what we do here is hide the latency by prefetching four steps ahead
    and overlapping memory requests. Theoretically, if the compute didn’t matter,
    we would expect a ~4x speedup, but in reality, we get a somewhat more moderate
    speedup.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also try to prefetch further than that four steps ahead, and we don’t
    even have to use more than one prefetch instruction for that: we can try to request
    only the first cache line and rely on the hardware to prefetch its neighbors.
    This trick may or may not improve actual performance — depends on the hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Also, note that the last few prefetch requests are actually not needed, and
    in fact, they may even be outside the memory region allocated for the program.
    On most modern CPUs, invalid prefetch instructions get converted into no-ops,
    so it isn’t a problem, but on some platforms, this may cause a slowdown, so it
    may make sense, for example, to split off the last ~4 iterations from the loop
    to try to remove them.
  prefs: []
  type: TYPE_NORMAL
- en: This prefetching technique allows us to read up to four elements ahead, but
    it doesn’t really come for free — we are effectively trading off excess memory
    [bandwidth](/hpc/cpu-cache/bandwidth) for reduced [latency](/hpc/cpu-cache/latency).
    If you run more than one instance at a time on separate hardware threads or just
    any other memory-intensive computation in the background, it will significantly
    [affect](/hpc/cpu-cache/sharing) the benchmark performance.
  prefs: []
  type: TYPE_NORMAL
- en: But we can do better. Instead of fetching four cache lines at a time, we could
    fetch four times *fewer* cache lines. And in the [next section](../s-tree), we
    will explore the approach.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#removing-the-last-branch)Removing
    the Last Branch'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just one finishing touch: did you notice the bumpiness of the Eytzinger search?
    This isn’t random noise — let’s zoom in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/98293c45b3cc644c79feae3d4ef2b8c9.png)'
  prefs: []
  type: TYPE_IMG
- en: The latency is ~10ns higher for the array sizes in the form of $1.5 \cdot 2^k$.
    These are mispredicted branches from the loop itself — the last branch, to be
    exact. When the array size is far from a power of two, it is hard to predict whether
    the loop will make $\lfloor \log_2 n \rfloor$ or $\lfloor \log_2 n \rfloor + 1$
    iterations, so we have a 50% chance to suffer exactly one branch mispredict.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to address it is to pad the array with infinities to the closest power
    of two, but this wastes memory. Instead, we get rid of that last branch by always
    executing a constant minimum number of iterations and then using predication to
    optionally make the last comparison against some dummy element — that is guaranteed
    to be less than $x$ so that its comparison will be canceled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph is now smooth, and on small arrays, it is just a couple of cycles
    slower than the branchless binary search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/361967de1a95a2729ca8023b6b8c060d.png)'
  prefs: []
  type: TYPE_IMG
- en: Interestingly, now GCC fails to replace the branch with `cmov`, but Clang doesn’t.
    1-1.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#appendix-random-binary-search)Appendix:
    Random Binary Search'
  prefs: []
  type: TYPE_NORMAL
- en: By the way, finding the exact expected number of comparisons for random binary
    search is quite an interesting math problem in and of itself. Try solving it yourself
    first!
  prefs: []
  type: TYPE_NORMAL
- en: 'The way to compute it *algorithmically* is through dynamic programming. If
    we denote $f_n$ as the expected number of comparisons to find a random lower bound
    on a search interval of size $n$, it can be calculated from the previous $f_n$
    by considering all the $(n - 1)$ possible splits:'
  prefs: []
  type: TYPE_NORMAL
- en: '$$ f_n = \sum_{l = 1}^{n - 1} \frac{1}{n-1} \cdot \left( f_l \cdot \frac{l}{n}
    + f_{n - l} \cdot \frac{n - l}{n} \right) + 1 $$ Directly applying this formula
    gives us an $O(n^2)$ algorithm, but we can optimize it by rearranging the sum
    like this: $$ \begin{aligned} f_n &= \sum_{i = 1}^{n - 1} \frac{ f_i \cdot i +
    f_{n - i} \cdot (n - i) }{ n \cdot (n - 1) } + 1 \\ &= \frac{2}{n \cdot (n - 1)}
    \cdot \sum_{i = 1}^{n - 1} f_i \cdot i + 1 \end{aligned} $$ To update $f_n$, we
    only need to calculate the sum of $f_i \cdot i$ for all $i < n$. To do that, let’s
    introduce two new variables: $$ g_n = f_n \cdot n, \;\; s_n = \sum_{i=1}^{n} g_n
    $$ Now they can be sequentially calculated as: $$ \begin{aligned} g_n &= f_n \cdot
    n = \frac{2}{n-1} \cdot \sum_{i = 1}^{n - 1} g_i + n = \frac{2}{n - 1} \cdot s_{n
    - 1} + n \\ s_n &= s_{n - 1} + g_n \end{aligned} $$ This way we get an $O(n)$
    algorithm, but we can do even better. Let’s substitute $g_n$ in the update formula
    for $s_n$: $$ \begin{aligned} s_n &= s_{n - 1} + \frac{2}{n - 1} \cdot s_{n -
    1} + n \\ &= (1 + \frac{2}{n - 1}) \cdot s_{n - 1} + n \\ &= \frac{n + 1}{n -
    1} \cdot s_{n - 1} + n \end{aligned} $$'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next trick is more complicated. We define $r_n$ like this: $$ \begin{aligned}
    r_n &= \frac{s_n}{n} \\ &= \frac{1}{n} \cdot \left(\frac{n + 1}{n - 1} \cdot s_{n
    - 1} + n\right) \\ &= \frac{n + 1}{n} \cdot \frac{s_{n - 1}}{n - 1} + 1 \\ &=
    \left(1 + \frac{1}{n}\right) \cdot r_{n - 1} + 1 \end{aligned} $$ We can substitute
    it into the formula we got for $g_n$ before: $$ g_n = \frac{2}{n - 1} \cdot s_{n
    - 1} + n = 2 \cdot r_{n - 1} + n $$ Recalling that $g_n = f_n \cdot n$, we can
    express $r_{n - 1}$ using $f_n$: $$ f_n \cdot n = 2 \cdot r_{n - 1} + n \implies
    r_{n - 1} = \frac{(f_n - 1) \cdot n}{2} $$ Final step. We’ve just expressed $r_n$
    through $r_{n - 1}$ and $r_{n - 1}$ through $f_n$. This lets us express $f_{n
    + 1}$ through $f_n$: $$ \begin{aligned} &&\quad r_n &= \left(1 + \frac{1}{n}\right)
    \cdot r_{n - 1} + 1 \\ &\Rightarrow & \frac{(f_{n + 1} - 1) \cdot (n + 1)}{2}
    &= \left(1 + \frac{1}{n}\right) \cdot \frac{(f_n - 1) \cdot n}{2} + 1 \\ &&&=
    \frac{n + 1}{2} \cdot (f_n - 1) + 1 \\ &\Rightarrow & (f_{n + 1} - 1) &= (f_{n}
    - 1) + \frac{2}{n + 1} \\ &\Rightarrow &f_{n + 1} &= f_{n} + \frac{2}{n + 1} \\
    &\Rightarrow &f_{n} &= f_{n - 1} + \frac{2}{n} \\ &\Rightarrow &f_{n} &= \sum_{k
    = 2}^{n} \frac{2}{k} \end{aligned} $$'
  prefs: []
  type: TYPE_NORMAL
- en: The last expression is double the [harmonic series](https://en.wikipedia.org/wiki/Harmonic_series_(mathematics)),
    which is well known to approximate $\ln n$ as $n \to \infty$. Therefore, the random
    binary search will perform $\frac{2 \ln n}{\log_2 n} = 2 \ln 2 \approx 1.386$
    more comparisons compared to the normal one.
  prefs: []
  type: TYPE_NORMAL
- en: '### [#](https://en.algorithmica.org/hpc/data-structures/binary-search/#acknowledgements)Acknowledgements'
  prefs: []
  type: TYPE_NORMAL
- en: The article is loosely based on “[Array Layouts for Comparison-Based Searching](https://arxiv.org/pdf/1509.05053.pdf)”
    by Paul-Virak Khuong and Pat Morin. It is 46 pages long and discusses these and
    many other (less successful) approaches in more detail. I highly recommend also
    checking it out — this is one of my favorite performance engineering papers.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to Marshall Lochbaum for [providing](https://github.com/algorithmica-org/algorithmica/issues/57)
    the proof for the random binary search. No way I could do it myself.
  prefs: []
  type: TYPE_NORMAL
- en: I also stole these lovely layout visualizations from some blog a long time ago,
    but I don’t remember the name of the blog and what license they had, and inverse
    image search doesn’t find them anymore. If you don’t sue me, thank you, whoever
    you are! [← ../Data Structures Case Studies](https://en.algorithmica.org/hpc/data-structures/)[Static
    B-Trees →](https://en.algorithmica.org/hpc/data-structures/s-tree/)
  prefs: []
  type: TYPE_NORMAL
