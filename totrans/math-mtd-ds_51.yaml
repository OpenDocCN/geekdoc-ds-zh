- en: 6.7\. Online supplementary materials#
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://mmids-textbook.github.io/chap06_prob/supp/roch-mmids-prob-supp.html](https://mmids-textbook.github.io/chap06_prob/supp/roch-mmids-prob-supp.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 6.7.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.7.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_prob_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 6.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 6.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 6.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 6.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.1** The probability of \(Y\) being in the second category is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}(Y = e_2) = \pi_2 = 0.5. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.3** Let \(X_1, \ldots, X_n\) be i.i.d. Bernoulli\((q^*)\). The MLE is
    \(\hat{q}_{\mathrm{MLE}} = \frac{1}{n} \sum_{i=1}^n X_i\). By the Law of Large
    Numbers, as \(n \to \infty\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{q}_{\mathrm{MLE}} \to \mathbb{E}[X_1] = q^* \]
  prefs: []
  type: TYPE_NORMAL
- en: almost surely.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.5** The gradient at \(w = 0\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla_w L_4(0; \{(x_i, y_i)\}_{i=1}^4) = -\sum_{i=1}^4 x_i(y_i - \sigma(0))
    = -\frac{1}{2}(1 - 2 + 3 - 4) = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated parameter after one step of gradient descent is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ w' = w - \eta \nabla_w L_4(w; \{(x_i, y_i)\}_{i=1}^4) = 0 - 0.1 \cdot 1 =
    -0.1. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.7** We must have \(\sum_{x=-1}^1 \mathbb{P}(X=x) = 1\). This implies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{Z(\theta)} (h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}) = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[ Z(\theta) = h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.9** The empirical frequency for each category is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\pi}_i = \frac{N_i}{n}, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(N_i\) is the number of times category \(i\) appears in the sample.
    The counts are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ N_1 = 1, \quad N_2 = 2, \quad N_3 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the empirical frequencies are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\pi}_1 = \frac{1}{4} = 0.25, \quad \hat{\pi}_2 = \frac{2}{4} = 0.5,
    \quad \hat{\pi}_3 = \frac{1}{4} = 0.25. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.11** The log-likelihood for a multivariate Gaussian distribution is
    given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \log \mathcal{L}(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{X}) = -\frac{1}{2}
    \left[ (\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X}
    - \boldsymbol{\mu}) + \log |\boldsymbol{\Sigma}| + 2 \log(2\pi) \right]. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'First, compute the inverse and determinant of \(\boldsymbol{\Sigma}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \boldsymbol{\Sigma}^{-1} = \frac{1}{3} \begin{pmatrix} 2 & -1
    \\ -1 & 2 \end{pmatrix}, \quad |\boldsymbol{\Sigma}| = 3. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{X} - \boldsymbol{\mu} = \begin{pmatrix} 1 \\ 3 \end{pmatrix}
    - \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} (\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X}
    - \boldsymbol{\mu}) = \begin{pmatrix} 0 & 1 \end{pmatrix} \frac{1}{3} \begin{pmatrix}
    2 & -1 \\ -1 & 2 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \frac{1}{3}
    \cdot 2 = \frac{2}{3}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'So the log-likelihood is: $\( \log \mathcal{L} = -\frac{1}{2} \left[ \frac{2}{3}
    + \log 3 + 2 \log (2\pi) \right] \approx -3.178. \)$'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.1** \(\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} =
    \frac{0.2}{0.5} = 0.4.\) This follows from the definition of conditional probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.3** \(\mathbb{E}[X|Y=y] = 1 \cdot 0.3 + 2 \cdot 0.7 = 0.3 + 1.4 = 1.7.\)
    This is the definition of the conditional expectation for discrete random variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.5**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[A \mid B \cap C] &= \frac{\mathbb{P}[A \cap (B \cap
    C)]}{\mathbb{P}[B \cap C]} \\ &= \frac{\mathbb{P}[A \cap B \cap C]}{\mathbb{P}[B
    \cap C]} \\ &= \frac{0.05}{0.1} \\ &= 0.5. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.7** If \(A\), \(B\), and \(C\) are pairwise independent, then they are
    also mutually independent. Therefore,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[A \cap B \cap C] &= \mathbb{P}[A] \mathbb{P}[B]
    \mathbb{P}[C] \\ &= 0.8 \cdot 0.6 \cdot 0.5 \\ &= 0.24. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.9**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} (\mathbb{P}[X=x, Z=z])_{x,z} &= \sum_{y} (\mathbb{P}[X=x, Y
    = y, Z=z])_{x,z} \\ &= \sum_{y} [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = y \mid
    X=x])_x] (\mathbb{P}[Z=z \mid Y = y])_z \\ &= [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y
    = 0 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 0])_z\\ & \quad + [(\mathbb{P}[X=x])_x
    \odot (\mathbb{P}[Y = 1 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 1])_z \\ &= \begin{pmatrix}
    0.3 \cdot 0.2 \cdot 0.5 & 0.3 \cdot 0.2 \cdot 0.5 \\ 0.7 \cdot 0.6 \cdot 0.5 &
    0.7 \cdot 0.6 \cdot 0.5 \end{pmatrix} + \begin{pmatrix} 0.3 \cdot 0.8 \cdot 0.1
    & 0.3 \cdot 0.8 \cdot 0.9 \\ 0.7 \cdot 0.4 \cdot 0.1 & 0.7 \cdot 0.4 \cdot 0.9
    \end{pmatrix} \\ &= \begin{pmatrix} 0.03 & 0.03 \\ 0.21 & 0.21 \end{pmatrix} +
    \begin{pmatrix} 0.024 & 0.216 \\ 0.028 & 0.252 \end{pmatrix} \\ &= \begin{pmatrix}
    0.054 & 0.246 \\ 0.238 & 0.462 \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.11**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \hat{\pi}_1 &= \frac{N_1}{N_1 + N_2} = \frac{50}{50 + 100}
    = \frac{1}{3}, \\ \hat{p}_{1,1} &= \frac{N_{1,1}}{N_1} = \frac{10}{50} = 0.2,
    \\ \hat{p}_{2,1} &= \frac{N_{2,1}}{N_2} = \frac{40}{100} = 0.4. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[X=x, Y=y, Z=z] = \mathbb{P}[X=x]\mathbb{P}[Y=y|X=x]\mathbb{P}[Z=z|X=x].
    \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.1**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}(X = 1) &= \sum_{i=1}^2 \pi_i p_i \\ &= (0.6)(0.3)
    + (0.4)(0.8) \\ &= 0.5 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.3** \(\mathbb{P}[X = (1, 0)] = \pi_1 p_{1,1} (1 - p_{1,2}) + \pi_2 p_{2,1}
    (1 - p_{2,2}) = 0.4 \cdot 0.7 \cdot 0.7 + 0.6 \cdot 0.2 \cdot 0.2 = 0.196 + 0.024
    = 0.22\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.5** \(r_{1,i} = \frac{\pi_1 p_{1,1} p_{1,2}}{\pi_1 p_{1,1} p_{1,2} +
    \pi_2 p_{2,1} p_{2,2}} = \frac{0.5 \cdot 0.8 \cdot 0.2}{0.5 \cdot 0.8 \cdot 0.2
    + 0.5 \cdot 0.1 \cdot 0.9} = \frac{0.08}{0.08 + 0.045} \approx 0.64\), \(r_{2,i}
    = 1 - r_{1,i} \approx 0.36\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.7** \(\pi_1 = \frac{\eta_1}{n} = \frac{r_{1,1} + r_{1,1}}{2} = \frac{0.8
    + 0.8}{2} = 0.8\), \(\pi_2 = 1 - \pi_1 = 0.2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.9**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} r_{1,2} &= \frac{\pi_1 \prod_{m=1}^3 p_{1,m}^{x_{2,m}} (1 -
    p_{1,m})^{1-x_{2,m}}}{\sum_{k=1}^2 \pi_k \prod_{m=1}^3 p_{k,m}^{x_{2,m}} (1 -
    p_{k,m})^{1-x_{2,m}}} \\ &= \frac{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1}{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1
    + (0.6)(0.8)^0(0.2)^1(0.5)^0(0.5)^1} \\ &= \frac{0.032}{0.032 + 0.06} \\ &= \frac{8}{23}
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.11**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[X] = \pi_1 \mu_1 + \pi_2 \mu_2 = 0.5 \times (-1) + 0.5 \times
    3 = -0.5 + 1.5 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{Var}(X) = \pi_1 (\sigma_1^2 + \mu_1^2) + \pi_2 (\sigma_2^2 + \mu_2^2)
    - \left(\pi_1 \mu_1 + \pi_2 \mu_2\right)^2, \]\[ \mathrm{Var}(X) = 0.4 (1 + 0^2)
    + 0.6 (2 + 4^2) - (0.4 \times 0 + 0.6 \times 4)^2 = 0.4 \times 1 + 0.6 \times
    18 - 2.4^2, \]\[ \mathrm{Var}(X) = 0.4 + 10.8 - 5.76 = 5.44. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.1**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B / B_{11} = B_{22} - B_{12}^T B_{11}^{-1} B_{12} = 3 - 1 \cdot \frac{1}{2}
    \cdot 1 = \frac{5}{2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: using the definition of the Schur complement.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.3** The Schur complement of \(A_{11}\) in \(A\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A/A_{11} = A_{22} - A_{21}A_{11}^{-1}A_{12} = 7 - \begin{pmatrix}
    0 & 6 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}^{-1} \begin{pmatrix}
    0 \\ 5 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'First, compute \(A_{11}^{-1}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A_{11}^{-1} = \frac{1}{1 \cdot 4 - 2 \cdot 3} \begin{pmatrix}
    4 & -2 \\ -3 & 1 \end{pmatrix} = \begin{pmatrix} -2 & 1 \\ 1.5 & -0.5 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A/A_{11} = 7 - \begin{pmatrix} 0 & 6 \end{pmatrix} \begin{pmatrix}
    -2 & 1 \\ 1.5 & -0.5 \end{pmatrix} \begin{pmatrix} 0 \\ 5 \end{pmatrix} = 7 +
    (6 \cdot 0.5 \cdot 5) = 22. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.5** The conditional mean of \(X_1\) given \(X_2 = 3\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_{1|2}(3) = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (3 - \bmu_2) = 1 +
    1 \cdot \frac{1}{3} (3 - 2) = \frac{4}{3}, \]
  prefs: []
  type: TYPE_NORMAL
- en: using the formula for the conditional mean of multivariate Gaussians.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.7** The conditional distribution of \(X_1\) given \(X_2 = 1\) is Gaussian
    with mean \(\mu_{1|2} = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (1 - \mu_2) = \frac{1}{2}\)
    and variance \(\bSigma_{1|2} = \bSigma_{11} - \bSigma_{12} \bSigma_{22}^{-1} \bSigma_{21}
    = \frac{7}{2}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.9** The distribution of \(\bY\) is Gaussian with mean vector \(A\bmu
    = \begin{pmatrix} -3 \\ -3 \end{pmatrix}\) and covariance matrix \(A\bSigma A^T
    = \begin{pmatrix} 8 & 1 \\ 1 & 6 \end{pmatrix}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.11** The mean of \(Y_t\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbb{E}[Y_t] = \begin{pmatrix} 1 & 0 \end{pmatrix} \mathbb{E}[\bX_t]
    = \begin{pmatrix} 1 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = 1,
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and the variance of \(Y_t\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathrm{Var}[Y_t] = \begin{pmatrix} 1 & 0 \end{pmatrix} \mathrm{Cov}[\bX_t]
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = \begin{pmatrix} 1 & 0 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = 2, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: using the properties of linear-Gaussian systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.13** The innovation is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} e_t = Y_t - H \bmu_{\text{pred}} = 3 - \begin{pmatrix} 1 & 0
    \end{pmatrix} \begin{pmatrix} 3 \\ 1 \end{pmatrix} = 3 - 3 = 0. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.15** The updated state estimate is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bmu_t = \bmu_{\text{pred}} + K_t e_t = \begin{pmatrix} 3 \\
    1 \end{pmatrix} + \begin{pmatrix} \frac{2}{3} \\ \frac{1}{3} \end{pmatrix} \cdot
    0 = \begin{pmatrix} 3 \\ 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define exponential families and give examples of common probability distributions
    that belong to this family.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the maximum likelihood estimator for exponential families and explain
    its properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that, under certain conditions, the maximum likelihood estimator is statistically
    consistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate generalized linear models using exponential families and express linear
    and logistic regression as special cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the gradient and Hessian of the negative log-likelihood for generalized
    linear models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the moment-matching equations for the maximum likelihood estimator
    in generalized linear models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the multiplication rule, the law of total probability, and Bayes’ rule
    to solve problems involving conditional probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate conditional probability mass functions and conditional expectations
    for discrete random variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define conditional independence and express it mathematically in terms of conditional
    probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiate between the fork, chain, and collider configurations in graphical
    models representing conditional independence relations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the joint probability distribution for the Naive Bayes model under the
    assumption of conditional independence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement maximum likelihood estimation to fit the parameters of the Naive Bayes
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Naive Bayes model for prediction and evaluate its accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement Laplace smoothing to address the issue of unseen words in the training
    data when fitting a Naive Bayes model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Naive Bayes model to perform sentiment analysis on a real-world dataset
    and interpret the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define mixtures as convex combinations of distributions and express the probability
    distribution of a mixture model using the law of total probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify examples of mixture models, such as mixtures of multinomials and Gaussian
    mixture models, and recognize their probability density functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of marginalizing out an unobserved random variable in the
    context of mixture models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the objective function for parameter estimation in mixtures of multivariate
    Bernoullis using the negative log-likelihood.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the majorization-minimization principle and its application in the
    Expectation-Maximization (EM) algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the E-step and M-step updates for the EM algorithm in the context of
    mixtures of multivariate Bernoullis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the EM algorithm for mixtures of multivariate Bernoullis and apply
    it to a real-world dataset, such as clustering handwritten digits from the MNIST
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify and address numerical issues that may arise during the implementation
    of the EM algorithm, such as underflow, by applying techniques like the log-sum-exp
    trick.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define block matrices and the Schur complement, and demonstrate their properties
    through examples and proofs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the marginal and conditional distributions of multivariate Gaussians
    using the properties of block matrices and the Schur complement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the linear-Gaussian system model and its components, including the
    state evolution and observation processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the purpose and key steps of the Kalman filter algorithm, including
    the prediction and update steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the Kalman filter algorithm in code, given the state evolution and
    observation models, and the initial state distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Kalman filter to a location tracking problem, and interpret the results
    in terms of the estimated object path and the algorithm’s performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.2\. Additional sections[#](#additional-sections "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.7.2.1\. Sentiment analysis[#](#sentiment-analysis "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As an application of the Naive Bayes model, we consider the task of sentiment
    analysis, which is a classification problem. We use a dataset available [here](https://www.kaggle.com/crowdflower/twitter-airline-sentiment).
    Quoting from there:'
  prefs: []
  type: TYPE_NORMAL
- en: A sentiment analysis job about the problems of each major U.S. airline. Twitter
    data was scraped from February of 2015 and contributors were asked to first classify
    positive, negative, and neutral tweets, followed by categorizing negative reasons
    (such as “late flight” or “rude service”).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We first load a cleaned-up version of the data and look at its summary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|  | time | user | sentiment | text |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2/24/15 11:35 | cairdin | neutral | @VirginAmerica What @dhepburn said.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2/24/15 11:15 | jnardino | positive | @VirginAmerica plus you''ve added
    commercials t... |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2/24/15 11:15 | yvonnalynn | neutral | @VirginAmerica I didn''t today...
    Must mean I n... |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2/24/15 11:15 | jnardino | negative | @VirginAmerica it''s really aggressive
    to blast... |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2/24/15 11:14 | jnardino | negative | @VirginAmerica and it''s a really
    big bad thing... |'
  prefs: []
  type: TYPE_TB
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We extract the text information in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we convert our dataset into a matrix by creating a document-term matrix
    using [`sklearn.feature_extraction.text.CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform).
    Quoting [Wikipedia](https://en.wikipedia.org/wiki/Document-term_matrix):'
  prefs: []
  type: TYPE_NORMAL
- en: A document-term matrix or term-document matrix is a mathematical matrix that
    describes the frequency of terms that occur in a collection of documents. In a
    document-term matrix, rows correspond to documents in the collection and columns
    correspond to terms.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'By default, it first preprocesses the data. In particular, it lower-cases all
    words and removes punctuation. A more careful pre-procsseing would also include
    stemming, although we do not do this here. Regarding the latter, quoting [Wikipedia](https://en.wikipedia.org/wiki/Stemming):'
  prefs: []
  type: TYPE_NORMAL
- en: In linguistic morphology and information retrieval, stemming is the process
    of reducing inflected (or sometimes derived) words to their word stem, base or
    root form—generally a written word form. […] A computer program or subroutine
    that stems word may be called a stemming program, stemming algorithm, or stemmer.
    […] A stemmer for English operating on the stem cat should identify such strings
    as cats, catlike, and catty. A stemming algorithm might also reduce the words
    fishing, fished, and fisher to the stem fish. The stem need not be a word, for
    example the Porter algorithm reduces, argue, argued, argues, arguing, and argus
    to the stem argu.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The list of all terms used can be accessed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Because of our use of the multivariate Bernoulli naive Bayes model, it will
    be more convenient to work with a variant of the document-term matrix where each
    word is either present or absent. Note that, in the context of tweet data which
    are very short documents with likely little word repetition, there is probably
    not much difference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We also extract the labels (`neutral`, `postive`, `negative`) from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We split the data into a training set and a test set using [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We use the Naive Bayes method. We first construct the matrix \(N_{k,m}\) and
    the vector \(N_k\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Above, the [`enumerate`](https://docs.python.org/3/library/functions.html#enumerate)
    function provides both the index and the value for each item in the `label_set`
    list during the loop. This allows the code to use the label’s value (`k`) and
    its numerical position (`i`) at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: We are ready to train on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Next, we plot the vector \(p_{k,m}\) for each label \(k\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e75f50e0059a06f686e844c95d499bf2dbc3ef78367a3fc95c0ed8f7df2202b1.png](../Images/dc490453944015ef54f86c78956d449e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compute a prediction on the test tweets. For example, for the 5th test
    tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The following computes the overall accuracy over the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: To get a better understanding of the differences uncovered by Naive Bayes between
    the different labels, we identify words that are particularly common in one label,
    but not on the other. Recall that label `1` corresponds to `positive` while label
    `2` corresponds to `negative`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'One notices that many positive words do appear in this list: `awesome`, `best`,
    `great`, `love`, `thank`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we notice: `bag`, `cancelled`, `delayed`, `hours`, `phone`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The bag-of-words representation used in the sentiment analysis
    example is a simple but limited way to represent text data. More advanced representations
    such as word embeddings and transformer models can capture more semantic information.
    Ask your favorite AI chatbot to explain these representations and how they can
    be used for text classification tasks. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '6.7.2.2\. Kalman filtering: missing data[#](#kalman-filtering-missing-data
    "Link to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Kalman filtering, we can also allow for the possibility that some observations
    are missing. Imagine for instance losing GPS signal while going through a tunnel.
    The recursions above are still valid, with the only modification that the *Update*
    equations involving \(\bY_t\) are dropped at those times \(t\) where there is
    no observation. In Numpy, we can use [`NaN`](https://numpy.org/doc/stable/reference/constants.html#numpy.nan)
    to indicate the lack of observation. (Alternatively, one can use the [numpy.ma](https://numpy.org/doc/stable/reference/maskedarray.generic.html)
    module.)
  prefs: []
  type: TYPE_NORMAL
- en: We use a same sample path as above, but mask observations at times \(t=10,\ldots,20\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Here is the sample we are aiming to infer.
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]</details> ![../../_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png](../Images/23e83372ecc5a5cde94ad7f021c21a15.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We modify the recursion accordingly, that is, skip the *Update* step when there
    is no observation to use for the update.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]</details> ![../../_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png](../Images/6633cdad5605633dacdc68a6a28fde12.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.2.3\. Cholesky decomposition[#](#cholesky-decomposition "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we derive an important matrix factorization and apply it to
    generating multivariate Gaussians. We also revisit the least-squares problem.
    We begin with the motivation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating multivariate Gaussians** Suppose we want to generate samples from
    a multivariate Gaussian \(\bX \sim N_d(\bmu, \bSigma)\) with given mean vector
    \(\bmu \in \mathbb{R}^d\) and positive definite covariance matrix \(\bSigma \in
    \mathbb{R}^{d \times d}\). Of course, in Numpy, we could use [`numpy.random.Generator.multivariate_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html).
    But what is behind it? More precisely, suppose we have access to unlimited samples
    \(U_1, U_2, U_3, etc.\) from uniform random variables in \([0,1]\). How do we
    transform them to obtain samples from \(N_d(\bmu, \bSigma)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the simplest case: \(d=1\), \(\mu = 0\), and \(\sigma^2 = 1\).
    That is, we first generate a univariate standard Normal. We have seen a recipe
    for doing this before, the inverse transform sampling method. Specifically, recall
    that the cumulative distribution function (CDF) of a random variable \(Z\) is
    defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ F_Z(z) = \mathbb{P}[Z \leq z], \qquad \forall z \in \mathbb{R}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathcal{Z}\) be the interval where \(F_Z(z) \in (0,1)\) and assume that
    \(F_X\) is strictly increasing on \(\mathcal{Z}\). Let \(U \sim \mathrm{U}[0,1]\).
    Then it can be shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[F_X^{-1}(U) \leq z] = F_X(z). \]
  prefs: []
  type: TYPE_NORMAL
- en: So take \(F_Z = \Phi\), the CDF of the standard Normal. Then \(Z = \Phi^{-1}(U)\)
    is \(N(0,1)\).
  prefs: []
  type: TYPE_NORMAL
- en: How do we generate a \(N(\mu, \sigma^2)\) variable, for arbitrary \(\mu \in
    \mathbb{R}\) and \(\sigma^2 > 0\)? We use the fact that the linear transformation
    of Gaussian is still Gaussian. In particular, if \(Z \sim N(0,1)\), then
  prefs: []
  type: TYPE_NORMAL
- en: \[ X = \mu + \sigma Z \]
  prefs: []
  type: TYPE_NORMAL
- en: is \(N(\mu, \sigma^2)\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Python, \(\Phi^{-1}\) can be accessed using [`scipy.stats.norm.ppf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html).
    We implement this next (with help from ChatGPT).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: We generate 1000 samples and plot the empirical distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]</details> ![../../_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png](../Images/c4d1f8c3493a1c88b3b5727f55a25bef.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** It turns out there is a neat trick to generate *two* independent
    samples from \(N(0,1)\) that does not rely on access to \(\Phi^{-1}\). It is called
    the Box-Muller transform. Ask your favorite AI chatbot about it. Modify our code
    above to implement it. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: We move on to the multivariate case. We proceed similarly as before. First,
    how do we generate a \(d\)-dimensional Gaussian with mean vector \(\bmu = \mathbf{0}\)
    and identity covariance matrix \(\bSigma = I_{d \times d}\)? Easy – it has \(d\)
    independent components, each of which is standard Normal. So letting \(U_1, \ldots,
    U_d\) be independent uniform \([0,1]\) variables, then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{Z} = (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d)) \]
  prefs: []
  type: TYPE_NORMAL
- en: is \(N(\mathbf{0}, I_{d \times d})\).
  prefs: []
  type: TYPE_NORMAL
- en: We now seek to generate a multivariate Gaussian with arbitrary mean vector \(\bmu\)
    and positive definite covariance matrix \(\bSigma\). Again, we use a linear transformation
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{X} = \mathbf{a} + A \mathbf{Z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: What are the right choices for \(a \in \mathbb{R}^d\) and \(A \in \mathbb{R}^{d
    \times d}\)? We need to match the obtained and desired mean and covariance. We
    start with the mean. By linearity of expectation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[\mathbf{X}] = \E[\mathbf{a} + A \mathbf{Z}] = \mathbf{a} + A \,\E[\mathbf{Z}]
    = \mathbf{a}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence we pick \(\mathbf{a} := \bmu\).
  prefs: []
  type: TYPE_NORMAL
- en: As for the covariance, using the *Covariance of a Linear Transformation*, we
    get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \cov[\mathbf{X}] = A \,\cov[\mathbf{Z}] A^T = A A^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a problem: what is a matrix \(A\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A A^T = \bSigma? \]
  prefs: []
  type: TYPE_NORMAL
- en: In some sense, we are looking for a sort of “square root” of the covariance
    matrix. There are several ways of doing this. The Cholesky decomposition is one
    of them. We return to generating samples from \(N(\bmu, \bSigma)\) after introducing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '**A matrix factorization** Our key linear-algebraic result of this section
    is the following. The matrix factorization in the next theorem is called a Cholesky
    decomposition. It has many [applications](https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications).'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Cholesky Decomposition)** Any positive definite matrix \(B \in
    \mathbb{R}^{n \times n}\) can be factorized uniquely as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = L L^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(L \in \mathbb{R}^{n \times n}\) is a lower triangular matrix with positive
    entries on the diagonal. \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: 'The proof is provided below. It is based on deriving an algorithm for computing
    the Cholesky decomposition: we grow \(L\) starting from its top-left corner by
    successively computing its next row based on the previously constructed submatrix.
    Note that, because \(L\) is lower triangular, it suffices to compute its elements
    on and below the diagonal. We first give the algorithm, then establish that it
    is well-defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Access pattern ([Source](https://en.wikipedia.org/wiki/File:Chol.gif))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Access pattern](../Images/20de5b823040500b0d04511959484e97.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Before proceeeding with the general method, we give a small example
    to provide some intuition as to how it operates. We need a positive definite matrix.
    Consider the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 2 & 1\\ 0 & -2 & 1\\ 0 & 0 & 1\\ 0 &
    0 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: It has full column rank (why?). Recall that, in that case, the \(B = A^T A\)
    is positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: That is, the matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = A^T A = \begin{pmatrix} 1 & 2 & 1\\ 2 & 8 & 0\\ 1 & 0 &
    3 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: is positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(L = (\ell_{i,j})_{i,j=1}^3\) be lower triangular. We seek to solve \(L
    L^T = B\) for the nonzero entries of \(L\). Observe that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \ell_{1,1} & 0 & 0\\ \ell_{2,1} & \ell_{2,2}
    & 0\\ \ell_{3,1} & \ell_{3,2} & \ell_{3,3} \end{pmatrix} \begin{pmatrix} \ell_{1,1}
    & \ell_{2,1} & \ell_{3,1}\\ 0 & \ell_{2,2} & \ell_{3,2}\\ 0 & 0 & \ell_{3,3} \end{pmatrix}
    = \begin{pmatrix} \ell_{1,1}^2 & \ell_{1,1}\ell_{2,1} & \ell_{1,1}\ell_{3,1}\\
    \ell_{1,1}\ell_{2,1} & \ell_{2,1}^2 + \ell_{2,2}^2 & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
    \ell_{1,1}\ell_{3,1} & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} & \ell_{3,1}^2
    + \ell_{3,2}^2 + \ell_{3,3} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The system
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \ell_{1,1}^2 & \ell_{1,1}\ell_{2,1} & \ell_{1,1}\ell_{3,1}\\
    \ell_{1,1}\ell_{2,1} & \ell_{2,1}^2 + \ell_{2,2}^2 & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
    \ell_{1,1}\ell_{3,1} & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} & \ell_{3,1}^2
    + \ell_{3,2}^2 + \ell_{3,3}^2 \end{pmatrix} = \begin{pmatrix} 1 & 2 & 1\\ 2 &
    8 & 0\\ 1 & 0 & 3 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: turns out to be fairly simple to solve.
  prefs: []
  type: TYPE_NORMAL
- en: From the first entry, we get \(\ell_{1,1} = 1\) (where we took the positive
    solution to \(\ell_{1,1}^2 = 1\)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given that \(\ell_{1,1}\) is known, entry \(\ell_{2,1}\) is determined from
    \(\ell_{1,1}\ell_{2,1} =2\) in the first entry of the second row. That is, \(\ell_{2,1}
    =2\). Then the second entry of the second row gives \(\ell_{2,2}\) through \(\ell_{2,1}^2
    + \ell_{2,2}^2 = 8\). So \(\ell_{2,2} = 2\) (again we take the positive solution).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We move to the third row. The first entry gives \(\ell_{3,1} = 1\), the second
    entry gives \(\ell_{3,2} = -1\) and finally the third entry leads to \(\ell_{3,3}
    = 1\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hence we have
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L = \begin{pmatrix} \ell_{1,1} & 0 & 0\\ \ell_{2,1} & \ell_{2,2}
    & 0\\ \ell_{3,1} & \ell_{3,2} & \ell_{3,3} \end{pmatrix} = \begin{pmatrix} 1 &
    0 & 0\\ 2 & 2 & 0\\ 1 & -1 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: To detail the computation of the Cholesky decomposition \(L L^T\) of \(B\),
    we will need some notation. Write \(B = (b_{i,j})_{i,j=1}^n\) and \(L = (\ell_{i,j})_{i,j=1}^n\).
    Let \(L_{(k)} = (\ell_{i,j})_{i,j=1}^k\) be the first \(k\) rows and columns of
    \(L\), let \(\bflambda_{(k)}^T = (\ell_{k,1},\ldots,\ell_{k,k-1})\) be the row
    vector corresponding to the first \(k-1\) entries of row \(k\) of \(L\), and let
    \(\bfbeta_{(k)}^T = (b_{k,1},\ldots,b_{k,k-1})\) be the row vector corresponding
    to the first \(k-1\) entries of row \(k\) of \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: The strategy is to compute \(L_{(1)}\), then \(L_{(2)}\), then \(L_{(3)}\) and
    so on. With the notation above, \(L_{(j)}\) can be written in block form as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L_{(j)} = \begin{pmatrix} L_{(j-1)} & \mathbf{0}\\ \bflambda_{(j)}^T
    & \ell_{j,j}. \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, once \(L_{(j-1)}\) is known, in order to compute \(L_{(j)}\) one only
    needs \(\bflambda_{(j)}\) and \(\ell_{j,j}\). We show next that they satisfy easily
    solvable systems of equations.
  prefs: []
  type: TYPE_NORMAL
- en: We first note that the \((1,1)\) entry of the matrix equation \(L L^T = B\)
    implies that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell_{1,1}^2 = b_{1,1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So we set
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{(1)} = \ell_{1,1} = \sqrt{b_{1,1}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For this step to be well-defined, it needs to be the case that \(b_{1,1} >
    0\). It is easy to see that it follows from the positive definiteness of \(B\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 < \langle \mathbf{e}_1, B \mathbf{e}_1\rangle = \mathbf{e}_1^T B_{\cdot,1}
    = b_{1,1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Proceeding by induction, assume \(L_{(j-1)}\) has been constructed. The first
    \(j-1\) elements of the \(j\)-th row of the matrix equation \(L L^T = B\) translate
    into
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{j,\cdot} (L^T)_{\cdot,1:j-1} = \bflambda_{(j)}^T L_{(j-1)}^T = \bfbeta_{(j)}^T,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \((L^T)_{\cdot,1:j-1}\) denotes the first \(j-1\) columns of \(L^T\).
    In the first equality above, we used the fact that \(L^T\) is upper triangular.
    Taking a transpose, the resulting linear system of equations
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{(j-1)} \bflambda_{(j)} = \bfbeta_{(j)} \]
  prefs: []
  type: TYPE_NORMAL
- en: can be solved by forward substitution (since \(\bfbeta_{(j)}\) is part of the
    input and \(L_{(j-1)}\) was previously computed). The fact that this system has
    a unique solution (more specifically, that the diagonal entries of \(L_{(j-1)}\)
    are strictly positive) is established in the proof of the *Cholesky Decomposition
    Theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: The \((j,j)\)-th entry of the matrix equation \(L L^T = B\) translates into
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{j,\cdot} (L^T)_{\cdot,j} = \sum_{k=1}^j \ell_{j,k}^2 = b_{j,j}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where again we used the fact that \(L^T\) is upper triangular. Since \(\ell_{j,1},
    \ldots, \ell_{j,j-1}\) are the elements of \(\bflambda_{(j)}\), they have already
    been determined. So we can set
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell_{j,j} = \sqrt{b_{j,j} - \sum_{k=1}^{j-1} \ell_{j,k}^2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The fact that we are taking the square root of a positive quantity is established
    in the proof of the *Cholesky Decomposition Theorem*. Finally, from \(L_{(j-1)}\),
    \(\bflambda_{(j)}\), and \(\ell_{j,j}\), we construct \(L_{(j)}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement the algorithm above. In our naive implementation,
    we assume that \(B\) is positive definite, and therefore that all steps are well-defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Here is a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We can check that it produces the right factorization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof of Cholesky decomposition theorem** We give a proof of the *Cholesky
    Decomposition Theorem*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* Assuming by induction that the upper-left corner of the matrix
    \(B\) has a Cholesky decomposition, one finds equations for the remaining row
    that can be solved uniquely by the properties established in the previous subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* If \(n=1\), we have shown previously that \(b_{1,1} > 0\), and hence
    we can take \(L = [\ell_{1,1}]\) where \(\ell_{1,1} = \sqrt{b_{1,1}}\). Assuming
    the result holds for positive definite matrices in \(\mathbb{R}^{(n-1) \times
    (n-1)}\), we first re-write \(B = L L^T\) in block form'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} B_{11} & \bfbeta_{12}\\ \bfbeta_{12}^T & \beta_{22}
    \end{pmatrix} = \begin{pmatrix} \Lambda_{11} & \mathbf{0}\\ \bflambda_{12}^T &
    \lambda_{22} \end{pmatrix} \begin{pmatrix} \Lambda_{11}^T & \bflambda_{12}\\ \mathbf{0}^T
    & \lambda_{22} \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{11}, \Lambda_{11} \in \mathbb{R}^{n-1 \times n-1}\), \(\bfbeta_{12},
    \bflambda_{12} \in \mathbb{R}^{n-1}\) and \(\beta_{22}, \lambda_{22} \in \mathbb{R}\).
    By block matrix algebra, we get the system
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B_{11} = \Lambda_{11} \Lambda_{11}^T\\ \bfbeta_{12} = \Lambda_{11}
    \bflambda_{12}\\ \beta_{22} = \bflambda_{12}^T \bflambda_{12} + \lambda_{22}^2.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Principal Submatrices Lemma*, the principal submatrix \(B_{11}\) is
    positive definite. Hence, by induction, there is a unique lower-triangular matrix
    \(\Lambda_{11}\) with positive diagonal elements satisfying the first equation.
    We can then obtain \(\bfbeta_{12}\) from the second equation by forward substitution.
    And finally we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{22} = \sqrt{\beta_{22} - \bflambda_{12}^T \bflambda_{12}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We do have to check that the square root above exists. That is, we need to
    argue that the expression inside the square root is non-negative. In fact, for
    the claim to go through, we need it to be strictly positive. We notice that the
    expression inside the square root is in fact the Schur complement of the block
    \(B_{11}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \beta_{22} - \bflambda_{12}^T \bflambda_{12} &= \beta_{22}
    - (\Lambda_{11}^{-1} \bfbeta_{12})^T (\Lambda_{11}^{-1} \bfbeta_{12})\\ &= \beta_{22}
    - \bfbeta_{12}^T (\Lambda_{11}^{-1})^T \Lambda_{11}^{-1} \bfbeta_{12}\\ &= \beta_{22}
    - \bfbeta_{12}^T (\Lambda_{11} \Lambda_{11}^T)^{-1} \bfbeta_{12}\\ &= \beta_{22}
    - \bfbeta_{12}^T (B_{11})^{-1} \bfbeta_{12} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the equation \(\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\) on
    the first line, the identities \((Q W)^{-1} = W^{-1} Q^{-1}\) and \((Q^T)^{-1}
    = (Q^{-1})^T\) (see the exercise below) on the third line and the equation \(B_{11}
    = \Lambda_{11} \Lambda_{11}^T\) on the fourth line. By the *Schur Complement Lemma*,
    the Schur complement is positive definite. Because it is a scalar in this case,
    it is strictly positive (prove it!), which concludes the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Back to multivariate Gaussians** Returning to our motivation, we can generate
    samples from a \(N(\bmu, \bSigma)\) by first generating'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{Z} = (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d)) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(U_1, \ldots, U_d\) are independent uniform \([0,1]\) variables, then
    setting
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{X} = \bmu + L \mathbf{Z}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bSigma = L L^T\) is a Cholesky decomposition of \(\bSigma\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement this method.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: We generate some samples as an example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Using a Cholesky decomposition to solve the least squares problem** Another
    application of the Cholesky decomposition is to solving the least squares problem.
    In this section, we restrict ourselves to the case where \(A \in \mathbb{R}^{n\times
    m}\) has full column rank. By the *Least Squares and Positive Semidefiniteness
    Lemma*, we then have that \(A^T A\) is positive definite. By the *Cholesky Decomposition
    Theorem*, we can factorize this matrix as \(A^T A = L L^T\) where \(L\) is lower
    triangular with positive diagonal elements. The normal equations then reduce to'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L L^T \mathbf{x} = A^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This system can be solved in two steps. We first obtain the solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ L \mathbf{z} = A^T \mathbf{b} \]
  prefs: []
  type: TYPE_NORMAL
- en: by forward substitution. Then we obtain the solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ L^T \mathbf{x} = \mathbf{z} \]
  prefs: []
  type: TYPE_NORMAL
- en: by back-substitution. Note that \(L^T\) is indeed an upper triangular matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement this algorithm below. In our naive implementation,
    we assume that \(A\) has full column rank, and therefore that all steps are well-defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Other applications of the Cholesky decomposition are briefly described [here](https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications).
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.1\. Quizzes, solutions, code, etc.[#](#quizzes-solutions-code-etc "Link
    to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.7.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_prob_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 6.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 6.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 6.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 6.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.1** The probability of \(Y\) being in the second category is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}(Y = e_2) = \pi_2 = 0.5. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.3** Let \(X_1, \ldots, X_n\) be i.i.d. Bernoulli\((q^*)\). The MLE is
    \(\hat{q}_{\mathrm{MLE}} = \frac{1}{n} \sum_{i=1}^n X_i\). By the Law of Large
    Numbers, as \(n \to \infty\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{q}_{\mathrm{MLE}} \to \mathbb{E}[X_1] = q^* \]
  prefs: []
  type: TYPE_NORMAL
- en: almost surely.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.5** The gradient at \(w = 0\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla_w L_4(0; \{(x_i, y_i)\}_{i=1}^4) = -\sum_{i=1}^4 x_i(y_i - \sigma(0))
    = -\frac{1}{2}(1 - 2 + 3 - 4) = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated parameter after one step of gradient descent is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ w' = w - \eta \nabla_w L_4(w; \{(x_i, y_i)\}_{i=1}^4) = 0 - 0.1 \cdot 1 =
    -0.1. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.7** We must have \(\sum_{x=-1}^1 \mathbb{P}(X=x) = 1\). This implies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{Z(\theta)} (h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}) = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[ Z(\theta) = h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.9** The empirical frequency for each category is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\pi}_i = \frac{N_i}{n}, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(N_i\) is the number of times category \(i\) appears in the sample.
    The counts are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ N_1 = 1, \quad N_2 = 2, \quad N_3 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the empirical frequencies are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\pi}_1 = \frac{1}{4} = 0.25, \quad \hat{\pi}_2 = \frac{2}{4} = 0.5,
    \quad \hat{\pi}_3 = \frac{1}{4} = 0.25. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.11** The log-likelihood for a multivariate Gaussian distribution is
    given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \log \mathcal{L}(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{X}) = -\frac{1}{2}
    \left[ (\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X}
    - \boldsymbol{\mu}) + \log |\boldsymbol{\Sigma}| + 2 \log(2\pi) \right]. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'First, compute the inverse and determinant of \(\boldsymbol{\Sigma}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \boldsymbol{\Sigma}^{-1} = \frac{1}{3} \begin{pmatrix} 2 & -1
    \\ -1 & 2 \end{pmatrix}, \quad |\boldsymbol{\Sigma}| = 3. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{X} - \boldsymbol{\mu} = \begin{pmatrix} 1 \\ 3 \end{pmatrix}
    - \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} (\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X}
    - \boldsymbol{\mu}) = \begin{pmatrix} 0 & 1 \end{pmatrix} \frac{1}{3} \begin{pmatrix}
    2 & -1 \\ -1 & 2 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \frac{1}{3}
    \cdot 2 = \frac{2}{3}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'So the log-likelihood is: $\( \log \mathcal{L} = -\frac{1}{2} \left[ \frac{2}{3}
    + \log 3 + 2 \log (2\pi) \right] \approx -3.178. \)$'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.1** \(\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} =
    \frac{0.2}{0.5} = 0.4.\) This follows from the definition of conditional probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.3** \(\mathbb{E}[X|Y=y] = 1 \cdot 0.3 + 2 \cdot 0.7 = 0.3 + 1.4 = 1.7.\)
    This is the definition of the conditional expectation for discrete random variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.5**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[A \mid B \cap C] &= \frac{\mathbb{P}[A \cap (B \cap
    C)]}{\mathbb{P}[B \cap C]} \\ &= \frac{\mathbb{P}[A \cap B \cap C]}{\mathbb{P}[B
    \cap C]} \\ &= \frac{0.05}{0.1} \\ &= 0.5. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.7** If \(A\), \(B\), and \(C\) are pairwise independent, then they are
    also mutually independent. Therefore,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[A \cap B \cap C] &= \mathbb{P}[A] \mathbb{P}[B]
    \mathbb{P}[C] \\ &= 0.8 \cdot 0.6 \cdot 0.5 \\ &= 0.24. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.9**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} (\mathbb{P}[X=x, Z=z])_{x,z} &= \sum_{y} (\mathbb{P}[X=x, Y
    = y, Z=z])_{x,z} \\ &= \sum_{y} [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = y \mid
    X=x])_x] (\mathbb{P}[Z=z \mid Y = y])_z \\ &= [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y
    = 0 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 0])_z\\ & \quad + [(\mathbb{P}[X=x])_x
    \odot (\mathbb{P}[Y = 1 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 1])_z \\ &= \begin{pmatrix}
    0.3 \cdot 0.2 \cdot 0.5 & 0.3 \cdot 0.2 \cdot 0.5 \\ 0.7 \cdot 0.6 \cdot 0.5 &
    0.7 \cdot 0.6 \cdot 0.5 \end{pmatrix} + \begin{pmatrix} 0.3 \cdot 0.8 \cdot 0.1
    & 0.3 \cdot 0.8 \cdot 0.9 \\ 0.7 \cdot 0.4 \cdot 0.1 & 0.7 \cdot 0.4 \cdot 0.9
    \end{pmatrix} \\ &= \begin{pmatrix} 0.03 & 0.03 \\ 0.21 & 0.21 \end{pmatrix} +
    \begin{pmatrix} 0.024 & 0.216 \\ 0.028 & 0.252 \end{pmatrix} \\ &= \begin{pmatrix}
    0.054 & 0.246 \\ 0.238 & 0.462 \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.11**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \hat{\pi}_1 &= \frac{N_1}{N_1 + N_2} = \frac{50}{50 + 100}
    = \frac{1}{3}, \\ \hat{p}_{1,1} &= \frac{N_{1,1}}{N_1} = \frac{10}{50} = 0.2,
    \\ \hat{p}_{2,1} &= \frac{N_{2,1}}{N_2} = \frac{40}{100} = 0.4. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[X=x, Y=y, Z=z] = \mathbb{P}[X=x]\mathbb{P}[Y=y|X=x]\mathbb{P}[Z=z|X=x].
    \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.1**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}(X = 1) &= \sum_{i=1}^2 \pi_i p_i \\ &= (0.6)(0.3)
    + (0.4)(0.8) \\ &= 0.5 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.3** \(\mathbb{P}[X = (1, 0)] = \pi_1 p_{1,1} (1 - p_{1,2}) + \pi_2 p_{2,1}
    (1 - p_{2,2}) = 0.4 \cdot 0.7 \cdot 0.7 + 0.6 \cdot 0.2 \cdot 0.2 = 0.196 + 0.024
    = 0.22\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.5** \(r_{1,i} = \frac{\pi_1 p_{1,1} p_{1,2}}{\pi_1 p_{1,1} p_{1,2} +
    \pi_2 p_{2,1} p_{2,2}} = \frac{0.5 \cdot 0.8 \cdot 0.2}{0.5 \cdot 0.8 \cdot 0.2
    + 0.5 \cdot 0.1 \cdot 0.9} = \frac{0.08}{0.08 + 0.045} \approx 0.64\), \(r_{2,i}
    = 1 - r_{1,i} \approx 0.36\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.7** \(\pi_1 = \frac{\eta_1}{n} = \frac{r_{1,1} + r_{1,1}}{2} = \frac{0.8
    + 0.8}{2} = 0.8\), \(\pi_2 = 1 - \pi_1 = 0.2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.9**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} r_{1,2} &= \frac{\pi_1 \prod_{m=1}^3 p_{1,m}^{x_{2,m}} (1 -
    p_{1,m})^{1-x_{2,m}}}{\sum_{k=1}^2 \pi_k \prod_{m=1}^3 p_{k,m}^{x_{2,m}} (1 -
    p_{k,m})^{1-x_{2,m}}} \\ &= \frac{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1}{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1
    + (0.6)(0.8)^0(0.2)^1(0.5)^0(0.5)^1} \\ &= \frac{0.032}{0.032 + 0.06} \\ &= \frac{8}{23}
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.11**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[X] = \pi_1 \mu_1 + \pi_2 \mu_2 = 0.5 \times (-1) + 0.5 \times
    3 = -0.5 + 1.5 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{Var}(X) = \pi_1 (\sigma_1^2 + \mu_1^2) + \pi_2 (\sigma_2^2 + \mu_2^2)
    - \left(\pi_1 \mu_1 + \pi_2 \mu_2\right)^2, \]\[ \mathrm{Var}(X) = 0.4 (1 + 0^2)
    + 0.6 (2 + 4^2) - (0.4 \times 0 + 0.6 \times 4)^2 = 0.4 \times 1 + 0.6 \times
    18 - 2.4^2, \]\[ \mathrm{Var}(X) = 0.4 + 10.8 - 5.76 = 5.44. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.1**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B / B_{11} = B_{22} - B_{12}^T B_{11}^{-1} B_{12} = 3 - 1 \cdot \frac{1}{2}
    \cdot 1 = \frac{5}{2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: using the definition of the Schur complement.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.3** The Schur complement of \(A_{11}\) in \(A\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A/A_{11} = A_{22} - A_{21}A_{11}^{-1}A_{12} = 7 - \begin{pmatrix}
    0 & 6 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}^{-1} \begin{pmatrix}
    0 \\ 5 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'First, compute \(A_{11}^{-1}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A_{11}^{-1} = \frac{1}{1 \cdot 4 - 2 \cdot 3} \begin{pmatrix}
    4 & -2 \\ -3 & 1 \end{pmatrix} = \begin{pmatrix} -2 & 1 \\ 1.5 & -0.5 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A/A_{11} = 7 - \begin{pmatrix} 0 & 6 \end{pmatrix} \begin{pmatrix}
    -2 & 1 \\ 1.5 & -0.5 \end{pmatrix} \begin{pmatrix} 0 \\ 5 \end{pmatrix} = 7 +
    (6 \cdot 0.5 \cdot 5) = 22. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.5** The conditional mean of \(X_1\) given \(X_2 = 3\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_{1|2}(3) = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (3 - \bmu_2) = 1 +
    1 \cdot \frac{1}{3} (3 - 2) = \frac{4}{3}, \]
  prefs: []
  type: TYPE_NORMAL
- en: using the formula for the conditional mean of multivariate Gaussians.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.7** The conditional distribution of \(X_1\) given \(X_2 = 1\) is Gaussian
    with mean \(\mu_{1|2} = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (1 - \mu_2) = \frac{1}{2}\)
    and variance \(\bSigma_{1|2} = \bSigma_{11} - \bSigma_{12} \bSigma_{22}^{-1} \bSigma_{21}
    = \frac{7}{2}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.9** The distribution of \(\bY\) is Gaussian with mean vector \(A\bmu
    = \begin{pmatrix} -3 \\ -3 \end{pmatrix}\) and covariance matrix \(A\bSigma A^T
    = \begin{pmatrix} 8 & 1 \\ 1 & 6 \end{pmatrix}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.11** The mean of \(Y_t\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbb{E}[Y_t] = \begin{pmatrix} 1 & 0 \end{pmatrix} \mathbb{E}[\bX_t]
    = \begin{pmatrix} 1 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = 1,
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and the variance of \(Y_t\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathrm{Var}[Y_t] = \begin{pmatrix} 1 & 0 \end{pmatrix} \mathrm{Cov}[\bX_t]
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = \begin{pmatrix} 1 & 0 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = 2, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: using the properties of linear-Gaussian systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.13** The innovation is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} e_t = Y_t - H \bmu_{\text{pred}} = 3 - \begin{pmatrix} 1 & 0
    \end{pmatrix} \begin{pmatrix} 3 \\ 1 \end{pmatrix} = 3 - 3 = 0. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.15** The updated state estimate is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bmu_t = \bmu_{\text{pred}} + K_t e_t = \begin{pmatrix} 3 \\
    1 \end{pmatrix} + \begin{pmatrix} \frac{2}{3} \\ \frac{1}{3} \end{pmatrix} \cdot
    0 = \begin{pmatrix} 3 \\ 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define exponential families and give examples of common probability distributions
    that belong to this family.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the maximum likelihood estimator for exponential families and explain
    its properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that, under certain conditions, the maximum likelihood estimator is statistically
    consistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate generalized linear models using exponential families and express linear
    and logistic regression as special cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the gradient and Hessian of the negative log-likelihood for generalized
    linear models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the moment-matching equations for the maximum likelihood estimator
    in generalized linear models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the multiplication rule, the law of total probability, and Bayes’ rule
    to solve problems involving conditional probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate conditional probability mass functions and conditional expectations
    for discrete random variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define conditional independence and express it mathematically in terms of conditional
    probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiate between the fork, chain, and collider configurations in graphical
    models representing conditional independence relations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the joint probability distribution for the Naive Bayes model under the
    assumption of conditional independence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement maximum likelihood estimation to fit the parameters of the Naive Bayes
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Naive Bayes model for prediction and evaluate its accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement Laplace smoothing to address the issue of unseen words in the training
    data when fitting a Naive Bayes model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Naive Bayes model to perform sentiment analysis on a real-world dataset
    and interpret the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define mixtures as convex combinations of distributions and express the probability
    distribution of a mixture model using the law of total probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify examples of mixture models, such as mixtures of multinomials and Gaussian
    mixture models, and recognize their probability density functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of marginalizing out an unobserved random variable in the
    context of mixture models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the objective function for parameter estimation in mixtures of multivariate
    Bernoullis using the negative log-likelihood.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the majorization-minimization principle and its application in the
    Expectation-Maximization (EM) algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the E-step and M-step updates for the EM algorithm in the context of
    mixtures of multivariate Bernoullis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the EM algorithm for mixtures of multivariate Bernoullis and apply
    it to a real-world dataset, such as clustering handwritten digits from the MNIST
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify and address numerical issues that may arise during the implementation
    of the EM algorithm, such as underflow, by applying techniques like the log-sum-exp
    trick.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define block matrices and the Schur complement, and demonstrate their properties
    through examples and proofs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the marginal and conditional distributions of multivariate Gaussians
    using the properties of block matrices and the Schur complement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the linear-Gaussian system model and its components, including the
    state evolution and observation processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the purpose and key steps of the Kalman filter algorithm, including
    the prediction and update steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the Kalman filter algorithm in code, given the state evolution and
    observation models, and the initial state distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Kalman filter to a location tracking problem, and interpret the results
    in terms of the estimated object path and the algorithm’s performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.1.1\. Just the code[#](#just-the-code "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An interactive Jupyter notebook featuring the code in this chapter can be accessed
    below (Google Colab recommended). You are encouraged to tinker with it. Some suggested
    computational exercises are scattered throughout. The notebook is also available
    as a slideshow.
  prefs: []
  type: TYPE_NORMAL
- en: '[Notebook](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/just_the_code/roch_mmids_chap_prob_notebook.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Slideshow](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/just_the_code/roch_mmids_chap_prob_notebook_slides.slides.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7.1.2\. Self-assessment quizzes[#](#self-assessment-quizzes "Link to this
    heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A more extensive web version of the self-assessment quizzes is available by
    following the links below.
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 6.2](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_2.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 6.3](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_3.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 6.4](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_4.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Section 6.5](https://raw.githack.com/MMiDS-textbook/MMiDS-textbook.github.io/main/quizzes/self-assessment/quiz_6_5.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7.1.3\. Auto-quizzes[#](#auto-quizzes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Automatically generated quizzes for this chapter can be accessed here (Google
    Colab recommended).
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-quizzes](https://github.com/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb)
    ([Open In Colab](https://colab.research.google.com/github/MMiDS-textbook/MMiDS-textbook.github.io/blob/main/quizzes/auto_quizzes/roch-mmids-prob-autoquiz.ipynb))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 6.7.1.4\. Solutions to odd-numbered warm-up exercises[#](#solutions-to-odd-numbered-warm-up-exercises
    "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*(with help from Claude, Gemini, and ChatGPT)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.1** The probability of \(Y\) being in the second category is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}(Y = e_2) = \pi_2 = 0.5. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.3** Let \(X_1, \ldots, X_n\) be i.i.d. Bernoulli\((q^*)\). The MLE is
    \(\hat{q}_{\mathrm{MLE}} = \frac{1}{n} \sum_{i=1}^n X_i\). By the Law of Large
    Numbers, as \(n \to \infty\),'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{q}_{\mathrm{MLE}} \to \mathbb{E}[X_1] = q^* \]
  prefs: []
  type: TYPE_NORMAL
- en: almost surely.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.5** The gradient at \(w = 0\) is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \nabla_w L_4(0; \{(x_i, y_i)\}_{i=1}^4) = -\sum_{i=1}^4 x_i(y_i - \sigma(0))
    = -\frac{1}{2}(1 - 2 + 3 - 4) = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated parameter after one step of gradient descent is:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ w' = w - \eta \nabla_w L_4(w; \{(x_i, y_i)\}_{i=1}^4) = 0 - 0.1 \cdot 1 =
    -0.1. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.7** We must have \(\sum_{x=-1}^1 \mathbb{P}(X=x) = 1\). This implies'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \frac{1}{Z(\theta)} (h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}) = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence,
  prefs: []
  type: TYPE_NORMAL
- en: \[ Z(\theta) = h(-1)e^{-\theta} + h(0) + h(1)e^{\theta}. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.9** The empirical frequency for each category is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\pi}_i = \frac{N_i}{n}, \]
  prefs: []
  type: TYPE_NORMAL
- en: 'where \(N_i\) is the number of times category \(i\) appears in the sample.
    The counts are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ N_1 = 1, \quad N_2 = 2, \quad N_3 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the empirical frequencies are:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \hat{\pi}_1 = \frac{1}{4} = 0.25, \quad \hat{\pi}_2 = \frac{2}{4} = 0.5,
    \quad \hat{\pi}_3 = \frac{1}{4} = 0.25. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.2.11** The log-likelihood for a multivariate Gaussian distribution is
    given by:'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \log \mathcal{L}(\boldsymbol{\mu}, \boldsymbol{\Sigma}; \mathbf{X}) = -\frac{1}{2}
    \left[ (\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X}
    - \boldsymbol{\mu}) + \log |\boldsymbol{\Sigma}| + 2 \log(2\pi) \right]. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'First, compute the inverse and determinant of \(\boldsymbol{\Sigma}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \boldsymbol{\Sigma}^{-1} = \frac{1}{3} \begin{pmatrix} 2 & -1
    \\ -1 & 2 \end{pmatrix}, \quad |\boldsymbol{\Sigma}| = 3. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbf{X} - \boldsymbol{\mu} = \begin{pmatrix} 1 \\ 3 \end{pmatrix}
    - \begin{pmatrix} 1 \\ 2 \end{pmatrix} = \begin{pmatrix} 0 \\ 1 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Thus,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} (\mathbf{X} - \boldsymbol{\mu})^T \boldsymbol{\Sigma}^{-1} (\mathbf{X}
    - \boldsymbol{\mu}) = \begin{pmatrix} 0 & 1 \end{pmatrix} \frac{1}{3} \begin{pmatrix}
    2 & -1 \\ -1 & 2 \end{pmatrix} \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \frac{1}{3}
    \cdot 2 = \frac{2}{3}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'So the log-likelihood is: $\( \log \mathcal{L} = -\frac{1}{2} \left[ \frac{2}{3}
    + \log 3 + 2 \log (2\pi) \right] \approx -3.178. \)$'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.1** \(\mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} =
    \frac{0.2}{0.5} = 0.4.\) This follows from the definition of conditional probability.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.3** \(\mathbb{E}[X|Y=y] = 1 \cdot 0.3 + 2 \cdot 0.7 = 0.3 + 1.4 = 1.7.\)
    This is the definition of the conditional expectation for discrete random variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.5**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[A \mid B \cap C] &= \frac{\mathbb{P}[A \cap (B \cap
    C)]}{\mathbb{P}[B \cap C]} \\ &= \frac{\mathbb{P}[A \cap B \cap C]}{\mathbb{P}[B
    \cap C]} \\ &= \frac{0.05}{0.1} \\ &= 0.5. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.7** If \(A\), \(B\), and \(C\) are pairwise independent, then they are
    also mutually independent. Therefore,'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}[A \cap B \cap C] &= \mathbb{P}[A] \mathbb{P}[B]
    \mathbb{P}[C] \\ &= 0.8 \cdot 0.6 \cdot 0.5 \\ &= 0.24. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.9**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} (\mathbb{P}[X=x, Z=z])_{x,z} &= \sum_{y} (\mathbb{P}[X=x, Y
    = y, Z=z])_{x,z} \\ &= \sum_{y} [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y = y \mid
    X=x])_x] (\mathbb{P}[Z=z \mid Y = y])_z \\ &= [(\mathbb{P}[X=x])_x \odot (\mathbb{P}[Y
    = 0 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 0])_z\\ & \quad + [(\mathbb{P}[X=x])_x
    \odot (\mathbb{P}[Y = 1 \mid X=x])_x] (\mathbb{P}[Z=z \mid Y = 1])_z \\ &= \begin{pmatrix}
    0.3 \cdot 0.2 \cdot 0.5 & 0.3 \cdot 0.2 \cdot 0.5 \\ 0.7 \cdot 0.6 \cdot 0.5 &
    0.7 \cdot 0.6 \cdot 0.5 \end{pmatrix} + \begin{pmatrix} 0.3 \cdot 0.8 \cdot 0.1
    & 0.3 \cdot 0.8 \cdot 0.9 \\ 0.7 \cdot 0.4 \cdot 0.1 & 0.7 \cdot 0.4 \cdot 0.9
    \end{pmatrix} \\ &= \begin{pmatrix} 0.03 & 0.03 \\ 0.21 & 0.21 \end{pmatrix} +
    \begin{pmatrix} 0.024 & 0.216 \\ 0.028 & 0.252 \end{pmatrix} \\ &= \begin{pmatrix}
    0.054 & 0.246 \\ 0.238 & 0.462 \end{pmatrix}. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.11**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \hat{\pi}_1 &= \frac{N_1}{N_1 + N_2} = \frac{50}{50 + 100}
    = \frac{1}{3}, \\ \hat{p}_{1,1} &= \frac{N_{1,1}}{N_1} = \frac{10}{50} = 0.2,
    \\ \hat{p}_{2,1} &= \frac{N_{2,1}}{N_2} = \frac{40}{100} = 0.4. \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.3.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[X=x, Y=y, Z=z] = \mathbb{P}[X=x]\mathbb{P}[Y=y|X=x]\mathbb{P}[Z=z|X=x].
    \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.1**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \mathbb{P}(X = 1) &= \sum_{i=1}^2 \pi_i p_i \\ &= (0.6)(0.3)
    + (0.4)(0.8) \\ &= 0.5 \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.3** \(\mathbb{P}[X = (1, 0)] = \pi_1 p_{1,1} (1 - p_{1,2}) + \pi_2 p_{2,1}
    (1 - p_{2,2}) = 0.4 \cdot 0.7 \cdot 0.7 + 0.6 \cdot 0.2 \cdot 0.2 = 0.196 + 0.024
    = 0.22\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.5** \(r_{1,i} = \frac{\pi_1 p_{1,1} p_{1,2}}{\pi_1 p_{1,1} p_{1,2} +
    \pi_2 p_{2,1} p_{2,2}} = \frac{0.5 \cdot 0.8 \cdot 0.2}{0.5 \cdot 0.8 \cdot 0.2
    + 0.5 \cdot 0.1 \cdot 0.9} = \frac{0.08}{0.08 + 0.045} \approx 0.64\), \(r_{2,i}
    = 1 - r_{1,i} \approx 0.36\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.7** \(\pi_1 = \frac{\eta_1}{n} = \frac{r_{1,1} + r_{1,1}}{2} = \frac{0.8
    + 0.8}{2} = 0.8\), \(\pi_2 = 1 - \pi_1 = 0.2\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.9**'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} r_{1,2} &= \frac{\pi_1 \prod_{m=1}^3 p_{1,m}^{x_{2,m}} (1 -
    p_{1,m})^{1-x_{2,m}}}{\sum_{k=1}^2 \pi_k \prod_{m=1}^3 p_{k,m}^{x_{2,m}} (1 -
    p_{k,m})^{1-x_{2,m}}} \\ &= \frac{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1}{(0.4)(0.2)^0(0.8)^1(0.9)^0(0.1)^1
    + (0.6)(0.8)^0(0.2)^1(0.5)^0(0.5)^1} \\ &= \frac{0.032}{0.032 + 0.06} \\ &= \frac{8}{23}
    \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.11**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{E}[X] = \pi_1 \mu_1 + \pi_2 \mu_2 = 0.5 \times (-1) + 0.5 \times
    3 = -0.5 + 1.5 = 1. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.4.13**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathrm{Var}(X) = \pi_1 (\sigma_1^2 + \mu_1^2) + \pi_2 (\sigma_2^2 + \mu_2^2)
    - \left(\pi_1 \mu_1 + \pi_2 \mu_2\right)^2, \]\[ \mathrm{Var}(X) = 0.4 (1 + 0^2)
    + 0.6 (2 + 4^2) - (0.4 \times 0 + 0.6 \times 4)^2 = 0.4 \times 1 + 0.6 \times
    18 - 2.4^2, \]\[ \mathrm{Var}(X) = 0.4 + 10.8 - 5.76 = 5.44. \]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.1**'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B / B_{11} = B_{22} - B_{12}^T B_{11}^{-1} B_{12} = 3 - 1 \cdot \frac{1}{2}
    \cdot 1 = \frac{5}{2}, \]
  prefs: []
  type: TYPE_NORMAL
- en: using the definition of the Schur complement.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.3** The Schur complement of \(A_{11}\) in \(A\) is given by'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A/A_{11} = A_{22} - A_{21}A_{11}^{-1}A_{12} = 7 - \begin{pmatrix}
    0 & 6 \end{pmatrix} \begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}^{-1} \begin{pmatrix}
    0 \\ 5 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 'First, compute \(A_{11}^{-1}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A_{11}^{-1} = \frac{1}{1 \cdot 4 - 2 \cdot 3} \begin{pmatrix}
    4 & -2 \\ -3 & 1 \end{pmatrix} = \begin{pmatrix} -2 & 1 \\ 1.5 & -0.5 \end{pmatrix}.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Then,
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A/A_{11} = 7 - \begin{pmatrix} 0 & 6 \end{pmatrix} \begin{pmatrix}
    -2 & 1 \\ 1.5 & -0.5 \end{pmatrix} \begin{pmatrix} 0 \\ 5 \end{pmatrix} = 7 +
    (6 \cdot 0.5 \cdot 5) = 22. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.5** The conditional mean of \(X_1\) given \(X_2 = 3\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mu_{1|2}(3) = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (3 - \bmu_2) = 1 +
    1 \cdot \frac{1}{3} (3 - 2) = \frac{4}{3}, \]
  prefs: []
  type: TYPE_NORMAL
- en: using the formula for the conditional mean of multivariate Gaussians.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.7** The conditional distribution of \(X_1\) given \(X_2 = 1\) is Gaussian
    with mean \(\mu_{1|2} = \mu_1 + \bSigma_{12} \bSigma_{22}^{-1} (1 - \mu_2) = \frac{1}{2}\)
    and variance \(\bSigma_{1|2} = \bSigma_{11} - \bSigma_{12} \bSigma_{22}^{-1} \bSigma_{21}
    = \frac{7}{2}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.9** The distribution of \(\bY\) is Gaussian with mean vector \(A\bmu
    = \begin{pmatrix} -3 \\ -3 \end{pmatrix}\) and covariance matrix \(A\bSigma A^T
    = \begin{pmatrix} 8 & 1 \\ 1 & 6 \end{pmatrix}\).'
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.11** The mean of \(Y_t\) is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathbb{E}[Y_t] = \begin{pmatrix} 1 & 0 \end{pmatrix} \mathbb{E}[\bX_t]
    = \begin{pmatrix} 1 & 0 \end{pmatrix} \begin{pmatrix} 1 \\ 2 \end{pmatrix} = 1,
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: and the variance of \(Y_t\) is
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \mathrm{Var}[Y_t] = \begin{pmatrix} 1 & 0 \end{pmatrix} \mathrm{Cov}[\bX_t]
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = \begin{pmatrix} 1 & 0 \end{pmatrix}
    \begin{pmatrix} 1 \\ 0 \end{pmatrix} + 1 = 2, \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: using the properties of linear-Gaussian systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.13** The innovation is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} e_t = Y_t - H \bmu_{\text{pred}} = 3 - \begin{pmatrix} 1 & 0
    \end{pmatrix} \begin{pmatrix} 3 \\ 1 \end{pmatrix} = 3 - 3 = 0. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: '**E6.5.15** The updated state estimate is'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \bmu_t = \bmu_{\text{pred}} + K_t e_t = \begin{pmatrix} 3 \\
    1 \end{pmatrix} + \begin{pmatrix} \frac{2}{3} \\ \frac{1}{3} \end{pmatrix} \cdot
    0 = \begin{pmatrix} 3 \\ 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.1.5\. Learning outcomes[#](#learning-outcomes "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Define exponential families and give examples of common probability distributions
    that belong to this family.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the maximum likelihood estimator for exponential families and explain
    its properties.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prove that, under certain conditions, the maximum likelihood estimator is statistically
    consistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate generalized linear models using exponential families and express linear
    and logistic regression as special cases.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute the gradient and Hessian of the negative log-likelihood for generalized
    linear models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpret the moment-matching equations for the maximum likelihood estimator
    in generalized linear models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the multiplication rule, the law of total probability, and Bayes’ rule
    to solve problems involving conditional probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate conditional probability mass functions and conditional expectations
    for discrete random variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define conditional independence and express it mathematically in terms of conditional
    probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiate between the fork, chain, and collider configurations in graphical
    models representing conditional independence relations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the joint probability distribution for the Naive Bayes model under the
    assumption of conditional independence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement maximum likelihood estimation to fit the parameters of the Naive Bayes
    model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Naive Bayes model for prediction and evaluate its accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement Laplace smoothing to address the issue of unseen words in the training
    data when fitting a Naive Bayes model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Naive Bayes model to perform sentiment analysis on a real-world dataset
    and interpret the results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define mixtures as convex combinations of distributions and express the probability
    distribution of a mixture model using the law of total probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify examples of mixture models, such as mixtures of multinomials and Gaussian
    mixture models, and recognize their probability density functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the concept of marginalizing out an unobserved random variable in the
    context of mixture models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formulate the objective function for parameter estimation in mixtures of multivariate
    Bernoullis using the negative log-likelihood.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the majorization-minimization principle and its application in the
    Expectation-Maximization (EM) algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the E-step and M-step updates for the EM algorithm in the context of
    mixtures of multivariate Bernoullis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the EM algorithm for mixtures of multivariate Bernoullis and apply
    it to a real-world dataset, such as clustering handwritten digits from the MNIST
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify and address numerical issues that may arise during the implementation
    of the EM algorithm, such as underflow, by applying techniques like the log-sum-exp
    trick.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define block matrices and the Schur complement, and demonstrate their properties
    through examples and proofs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Derive the marginal and conditional distributions of multivariate Gaussians
    using the properties of block matrices and the Schur complement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the linear-Gaussian system model and its components, including the
    state evolution and observation processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explain the purpose and key steps of the Kalman filter algorithm, including
    the prediction and update steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement the Kalman filter algorithm in code, given the state evolution and
    observation models, and the initial state distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply the Kalman filter to a location tracking problem, and interpret the results
    in terms of the estimated object path and the algorithm’s performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \(\aleph\)
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.2\. Additional sections[#](#additional-sections "Link to this heading")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.7.2.1\. Sentiment analysis[#](#sentiment-analysis "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As an application of the Naive Bayes model, we consider the task of sentiment
    analysis, which is a classification problem. We use a dataset available [here](https://www.kaggle.com/crowdflower/twitter-airline-sentiment).
    Quoting from there:'
  prefs: []
  type: TYPE_NORMAL
- en: A sentiment analysis job about the problems of each major U.S. airline. Twitter
    data was scraped from February of 2015 and contributors were asked to first classify
    positive, negative, and neutral tweets, followed by categorizing negative reasons
    (such as “late flight” or “rude service”).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We first load a cleaned-up version of the data and look at its summary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '|  | time | user | sentiment | text |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2/24/15 11:35 | cairdin | neutral | @VirginAmerica What @dhepburn said.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2/24/15 11:15 | jnardino | positive | @VirginAmerica plus you''ve added
    commercials t... |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2/24/15 11:15 | yvonnalynn | neutral | @VirginAmerica I didn''t today...
    Must mean I n... |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2/24/15 11:15 | jnardino | negative | @VirginAmerica it''s really aggressive
    to blast... |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2/24/15 11:14 | jnardino | negative | @VirginAmerica and it''s a really
    big bad thing... |'
  prefs: []
  type: TYPE_TB
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We extract the text information in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we convert our dataset into a matrix by creating a document-term matrix
    using [`sklearn.feature_extraction.text.CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform).
    Quoting [Wikipedia](https://en.wikipedia.org/wiki/Document-term_matrix):'
  prefs: []
  type: TYPE_NORMAL
- en: A document-term matrix or term-document matrix is a mathematical matrix that
    describes the frequency of terms that occur in a collection of documents. In a
    document-term matrix, rows correspond to documents in the collection and columns
    correspond to terms.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'By default, it first preprocesses the data. In particular, it lower-cases all
    words and removes punctuation. A more careful pre-procsseing would also include
    stemming, although we do not do this here. Regarding the latter, quoting [Wikipedia](https://en.wikipedia.org/wiki/Stemming):'
  prefs: []
  type: TYPE_NORMAL
- en: In linguistic morphology and information retrieval, stemming is the process
    of reducing inflected (or sometimes derived) words to their word stem, base or
    root form—generally a written word form. […] A computer program or subroutine
    that stems word may be called a stemming program, stemming algorithm, or stemmer.
    […] A stemmer for English operating on the stem cat should identify such strings
    as cats, catlike, and catty. A stemming algorithm might also reduce the words
    fishing, fished, and fisher to the stem fish. The stem need not be a word, for
    example the Porter algorithm reduces, argue, argued, argues, arguing, and argus
    to the stem argu.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The list of all terms used can be accessed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Because of our use of the multivariate Bernoulli naive Bayes model, it will
    be more convenient to work with a variant of the document-term matrix where each
    word is either present or absent. Note that, in the context of tweet data which
    are very short documents with likely little word repetition, there is probably
    not much difference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We also extract the labels (`neutral`, `postive`, `negative`) from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: We split the data into a training set and a test set using [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: We use the Naive Bayes method. We first construct the matrix \(N_{k,m}\) and
    the vector \(N_k\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Above, the [`enumerate`](https://docs.python.org/3/library/functions.html#enumerate)
    function provides both the index and the value for each item in the `label_set`
    list during the loop. This allows the code to use the label’s value (`k`) and
    its numerical position (`i`) at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: We are ready to train on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Next, we plot the vector \(p_{k,m}\) for each label \(k\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e75f50e0059a06f686e844c95d499bf2dbc3ef78367a3fc95c0ed8f7df2202b1.png](../Images/dc490453944015ef54f86c78956d449e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compute a prediction on the test tweets. For example, for the 5th test
    tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The following computes the overall accuracy over the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: To get a better understanding of the differences uncovered by Naive Bayes between
    the different labels, we identify words that are particularly common in one label,
    but not on the other. Recall that label `1` corresponds to `positive` while label
    `2` corresponds to `negative`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'One notices that many positive words do appear in this list: `awesome`, `best`,
    `great`, `love`, `thank`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we notice: `bag`, `cancelled`, `delayed`, `hours`, `phone`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The bag-of-words representation used in the sentiment analysis
    example is a simple but limited way to represent text data. More advanced representations
    such as word embeddings and transformer models can capture more semantic information.
    Ask your favorite AI chatbot to explain these representations and how they can
    be used for text classification tasks. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '6.7.2.2\. Kalman filtering: missing data[#](#kalman-filtering-missing-data
    "Link to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Kalman filtering, we can also allow for the possibility that some observations
    are missing. Imagine for instance losing GPS signal while going through a tunnel.
    The recursions above are still valid, with the only modification that the *Update*
    equations involving \(\bY_t\) are dropped at those times \(t\) where there is
    no observation. In Numpy, we can use [`NaN`](https://numpy.org/doc/stable/reference/constants.html#numpy.nan)
    to indicate the lack of observation. (Alternatively, one can use the [numpy.ma](https://numpy.org/doc/stable/reference/maskedarray.generic.html)
    module.)
  prefs: []
  type: TYPE_NORMAL
- en: We use a same sample path as above, but mask observations at times \(t=10,\ldots,20\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: Here is the sample we are aiming to infer.
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]</details> ![../../_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png](../Images/23e83372ecc5a5cde94ad7f021c21a15.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We modify the recursion accordingly, that is, skip the *Update* step when there
    is no observation to use for the update.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]</details> ![../../_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png](../Images/6633cdad5605633dacdc68a6a28fde12.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.2.3\. Cholesky decomposition[#](#cholesky-decomposition "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we derive an important matrix factorization and apply it to
    generating multivariate Gaussians. We also revisit the least-squares problem.
    We begin with the motivation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating multivariate Gaussians** Suppose we want to generate samples from
    a multivariate Gaussian \(\bX \sim N_d(\bmu, \bSigma)\) with given mean vector
    \(\bmu \in \mathbb{R}^d\) and positive definite covariance matrix \(\bSigma \in
    \mathbb{R}^{d \times d}\). Of course, in Numpy, we could use [`numpy.random.Generator.multivariate_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html).
    But what is behind it? More precisely, suppose we have access to unlimited samples
    \(U_1, U_2, U_3, etc.\) from uniform random variables in \([0,1]\). How do we
    transform them to obtain samples from \(N_d(\bmu, \bSigma)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the simplest case: \(d=1\), \(\mu = 0\), and \(\sigma^2 = 1\).
    That is, we first generate a univariate standard Normal. We have seen a recipe
    for doing this before, the inverse transform sampling method. Specifically, recall
    that the cumulative distribution function (CDF) of a random variable \(Z\) is
    defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ F_Z(z) = \mathbb{P}[Z \leq z], \qquad \forall z \in \mathbb{R}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathcal{Z}\) be the interval where \(F_Z(z) \in (0,1)\) and assume that
    \(F_X\) is strictly increasing on \(\mathcal{Z}\). Let \(U \sim \mathrm{U}[0,1]\).
    Then it can be shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[F_X^{-1}(U) \leq z] = F_X(z). \]
  prefs: []
  type: TYPE_NORMAL
- en: So take \(F_Z = \Phi\), the CDF of the standard Normal. Then \(Z = \Phi^{-1}(U)\)
    is \(N(0,1)\).
  prefs: []
  type: TYPE_NORMAL
- en: How do we generate a \(N(\mu, \sigma^2)\) variable, for arbitrary \(\mu \in
    \mathbb{R}\) and \(\sigma^2 > 0\)? We use the fact that the linear transformation
    of Gaussian is still Gaussian. In particular, if \(Z \sim N(0,1)\), then
  prefs: []
  type: TYPE_NORMAL
- en: \[ X = \mu + \sigma Z \]
  prefs: []
  type: TYPE_NORMAL
- en: is \(N(\mu, \sigma^2)\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Python, \(\Phi^{-1}\) can be accessed using [`scipy.stats.norm.ppf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html).
    We implement this next (with help from ChatGPT).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: We generate 1000 samples and plot the empirical distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]</details> ![../../_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png](../Images/c4d1f8c3493a1c88b3b5727f55a25bef.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** It turns out there is a neat trick to generate *two* independent
    samples from \(N(0,1)\) that does not rely on access to \(\Phi^{-1}\). It is called
    the Box-Muller transform. Ask your favorite AI chatbot about it. Modify our code
    above to implement it. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: We move on to the multivariate case. We proceed similarly as before. First,
    how do we generate a \(d\)-dimensional Gaussian with mean vector \(\bmu = \mathbf{0}\)
    and identity covariance matrix \(\bSigma = I_{d \times d}\)? Easy – it has \(d\)
    independent components, each of which is standard Normal. So letting \(U_1, \ldots,
    U_d\) be independent uniform \([0,1]\) variables, then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{Z} = (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d)) \]
  prefs: []
  type: TYPE_NORMAL
- en: is \(N(\mathbf{0}, I_{d \times d})\).
  prefs: []
  type: TYPE_NORMAL
- en: We now seek to generate a multivariate Gaussian with arbitrary mean vector \(\bmu\)
    and positive definite covariance matrix \(\bSigma\). Again, we use a linear transformation
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{X} = \mathbf{a} + A \mathbf{Z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: What are the right choices for \(a \in \mathbb{R}^d\) and \(A \in \mathbb{R}^{d
    \times d}\)? We need to match the obtained and desired mean and covariance. We
    start with the mean. By linearity of expectation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[\mathbf{X}] = \E[\mathbf{a} + A \mathbf{Z}] = \mathbf{a} + A \,\E[\mathbf{Z}]
    = \mathbf{a}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence we pick \(\mathbf{a} := \bmu\).
  prefs: []
  type: TYPE_NORMAL
- en: As for the covariance, using the *Covariance of a Linear Transformation*, we
    get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \cov[\mathbf{X}] = A \,\cov[\mathbf{Z}] A^T = A A^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a problem: what is a matrix \(A\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A A^T = \bSigma? \]
  prefs: []
  type: TYPE_NORMAL
- en: In some sense, we are looking for a sort of “square root” of the covariance
    matrix. There are several ways of doing this. The Cholesky decomposition is one
    of them. We return to generating samples from \(N(\bmu, \bSigma)\) after introducing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '**A matrix factorization** Our key linear-algebraic result of this section
    is the following. The matrix factorization in the next theorem is called a Cholesky
    decomposition. It has many [applications](https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications).'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Cholesky Decomposition)** Any positive definite matrix \(B \in
    \mathbb{R}^{n \times n}\) can be factorized uniquely as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = L L^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(L \in \mathbb{R}^{n \times n}\) is a lower triangular matrix with positive
    entries on the diagonal. \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: 'The proof is provided below. It is based on deriving an algorithm for computing
    the Cholesky decomposition: we grow \(L\) starting from its top-left corner by
    successively computing its next row based on the previously constructed submatrix.
    Note that, because \(L\) is lower triangular, it suffices to compute its elements
    on and below the diagonal. We first give the algorithm, then establish that it
    is well-defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Access pattern ([Source](https://en.wikipedia.org/wiki/File:Chol.gif))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Access pattern](../Images/20de5b823040500b0d04511959484e97.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Before proceeeding with the general method, we give a small example
    to provide some intuition as to how it operates. We need a positive definite matrix.
    Consider the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 2 & 1\\ 0 & -2 & 1\\ 0 & 0 & 1\\ 0 &
    0 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: It has full column rank (why?). Recall that, in that case, the \(B = A^T A\)
    is positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: That is, the matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = A^T A = \begin{pmatrix} 1 & 2 & 1\\ 2 & 8 & 0\\ 1 & 0 &
    3 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: is positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(L = (\ell_{i,j})_{i,j=1}^3\) be lower triangular. We seek to solve \(L
    L^T = B\) for the nonzero entries of \(L\). Observe that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \ell_{1,1} & 0 & 0\\ \ell_{2,1} & \ell_{2,2}
    & 0\\ \ell_{3,1} & \ell_{3,2} & \ell_{3,3} \end{pmatrix} \begin{pmatrix} \ell_{1,1}
    & \ell_{2,1} & \ell_{3,1}\\ 0 & \ell_{2,2} & \ell_{3,2}\\ 0 & 0 & \ell_{3,3} \end{pmatrix}
    = \begin{pmatrix} \ell_{1,1}^2 & \ell_{1,1}\ell_{2,1} & \ell_{1,1}\ell_{3,1}\\
    \ell_{1,1}\ell_{2,1} & \ell_{2,1}^2 + \ell_{2,2}^2 & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
    \ell_{1,1}\ell_{3,1} & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} & \ell_{3,1}^2
    + \ell_{3,2}^2 + \ell_{3,3} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The system
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \ell_{1,1}^2 & \ell_{1,1}\ell_{2,1} & \ell_{1,1}\ell_{3,1}\\
    \ell_{1,1}\ell_{2,1} & \ell_{2,1}^2 + \ell_{2,2}^2 & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
    \ell_{1,1}\ell_{3,1} & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} & \ell_{3,1}^2
    + \ell_{3,2}^2 + \ell_{3,3}^2 \end{pmatrix} = \begin{pmatrix} 1 & 2 & 1\\ 2 &
    8 & 0\\ 1 & 0 & 3 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: turns out to be fairly simple to solve.
  prefs: []
  type: TYPE_NORMAL
- en: From the first entry, we get \(\ell_{1,1} = 1\) (where we took the positive
    solution to \(\ell_{1,1}^2 = 1\)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given that \(\ell_{1,1}\) is known, entry \(\ell_{2,1}\) is determined from
    \(\ell_{1,1}\ell_{2,1} =2\) in the first entry of the second row. That is, \(\ell_{2,1}
    =2\). Then the second entry of the second row gives \(\ell_{2,2}\) through \(\ell_{2,1}^2
    + \ell_{2,2}^2 = 8\). So \(\ell_{2,2} = 2\) (again we take the positive solution).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We move to the third row. The first entry gives \(\ell_{3,1} = 1\), the second
    entry gives \(\ell_{3,2} = -1\) and finally the third entry leads to \(\ell_{3,3}
    = 1\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hence we have
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L = \begin{pmatrix} \ell_{1,1} & 0 & 0\\ \ell_{2,1} & \ell_{2,2}
    & 0\\ \ell_{3,1} & \ell_{3,2} & \ell_{3,3} \end{pmatrix} = \begin{pmatrix} 1 &
    0 & 0\\ 2 & 2 & 0\\ 1 & -1 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: To detail the computation of the Cholesky decomposition \(L L^T\) of \(B\),
    we will need some notation. Write \(B = (b_{i,j})_{i,j=1}^n\) and \(L = (\ell_{i,j})_{i,j=1}^n\).
    Let \(L_{(k)} = (\ell_{i,j})_{i,j=1}^k\) be the first \(k\) rows and columns of
    \(L\), let \(\bflambda_{(k)}^T = (\ell_{k,1},\ldots,\ell_{k,k-1})\) be the row
    vector corresponding to the first \(k-1\) entries of row \(k\) of \(L\), and let
    \(\bfbeta_{(k)}^T = (b_{k,1},\ldots,b_{k,k-1})\) be the row vector corresponding
    to the first \(k-1\) entries of row \(k\) of \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: The strategy is to compute \(L_{(1)}\), then \(L_{(2)}\), then \(L_{(3)}\) and
    so on. With the notation above, \(L_{(j)}\) can be written in block form as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L_{(j)} = \begin{pmatrix} L_{(j-1)} & \mathbf{0}\\ \bflambda_{(j)}^T
    & \ell_{j,j}. \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, once \(L_{(j-1)}\) is known, in order to compute \(L_{(j)}\) one only
    needs \(\bflambda_{(j)}\) and \(\ell_{j,j}\). We show next that they satisfy easily
    solvable systems of equations.
  prefs: []
  type: TYPE_NORMAL
- en: We first note that the \((1,1)\) entry of the matrix equation \(L L^T = B\)
    implies that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell_{1,1}^2 = b_{1,1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So we set
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{(1)} = \ell_{1,1} = \sqrt{b_{1,1}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For this step to be well-defined, it needs to be the case that \(b_{1,1} >
    0\). It is easy to see that it follows from the positive definiteness of \(B\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 < \langle \mathbf{e}_1, B \mathbf{e}_1\rangle = \mathbf{e}_1^T B_{\cdot,1}
    = b_{1,1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Proceeding by induction, assume \(L_{(j-1)}\) has been constructed. The first
    \(j-1\) elements of the \(j\)-th row of the matrix equation \(L L^T = B\) translate
    into
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{j,\cdot} (L^T)_{\cdot,1:j-1} = \bflambda_{(j)}^T L_{(j-1)}^T = \bfbeta_{(j)}^T,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \((L^T)_{\cdot,1:j-1}\) denotes the first \(j-1\) columns of \(L^T\).
    In the first equality above, we used the fact that \(L^T\) is upper triangular.
    Taking a transpose, the resulting linear system of equations
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{(j-1)} \bflambda_{(j)} = \bfbeta_{(j)} \]
  prefs: []
  type: TYPE_NORMAL
- en: can be solved by forward substitution (since \(\bfbeta_{(j)}\) is part of the
    input and \(L_{(j-1)}\) was previously computed). The fact that this system has
    a unique solution (more specifically, that the diagonal entries of \(L_{(j-1)}\)
    are strictly positive) is established in the proof of the *Cholesky Decomposition
    Theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: The \((j,j)\)-th entry of the matrix equation \(L L^T = B\) translates into
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{j,\cdot} (L^T)_{\cdot,j} = \sum_{k=1}^j \ell_{j,k}^2 = b_{j,j}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where again we used the fact that \(L^T\) is upper triangular. Since \(\ell_{j,1},
    \ldots, \ell_{j,j-1}\) are the elements of \(\bflambda_{(j)}\), they have already
    been determined. So we can set
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell_{j,j} = \sqrt{b_{j,j} - \sum_{k=1}^{j-1} \ell_{j,k}^2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The fact that we are taking the square root of a positive quantity is established
    in the proof of the *Cholesky Decomposition Theorem*. Finally, from \(L_{(j-1)}\),
    \(\bflambda_{(j)}\), and \(\ell_{j,j}\), we construct \(L_{(j)}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement the algorithm above. In our naive implementation,
    we assume that \(B\) is positive definite, and therefore that all steps are well-defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Here is a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: We can check that it produces the right factorization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof of Cholesky decomposition theorem** We give a proof of the *Cholesky
    Decomposition Theorem*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* Assuming by induction that the upper-left corner of the matrix
    \(B\) has a Cholesky decomposition, one finds equations for the remaining row
    that can be solved uniquely by the properties established in the previous subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* If \(n=1\), we have shown previously that \(b_{1,1} > 0\), and hence
    we can take \(L = [\ell_{1,1}]\) where \(\ell_{1,1} = \sqrt{b_{1,1}}\). Assuming
    the result holds for positive definite matrices in \(\mathbb{R}^{(n-1) \times
    (n-1)}\), we first re-write \(B = L L^T\) in block form'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} B_{11} & \bfbeta_{12}\\ \bfbeta_{12}^T & \beta_{22}
    \end{pmatrix} = \begin{pmatrix} \Lambda_{11} & \mathbf{0}\\ \bflambda_{12}^T &
    \lambda_{22} \end{pmatrix} \begin{pmatrix} \Lambda_{11}^T & \bflambda_{12}\\ \mathbf{0}^T
    & \lambda_{22} \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{11}, \Lambda_{11} \in \mathbb{R}^{n-1 \times n-1}\), \(\bfbeta_{12},
    \bflambda_{12} \in \mathbb{R}^{n-1}\) and \(\beta_{22}, \lambda_{22} \in \mathbb{R}\).
    By block matrix algebra, we get the system
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B_{11} = \Lambda_{11} \Lambda_{11}^T\\ \bfbeta_{12} = \Lambda_{11}
    \bflambda_{12}\\ \beta_{22} = \bflambda_{12}^T \bflambda_{12} + \lambda_{22}^2.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Principal Submatrices Lemma*, the principal submatrix \(B_{11}\) is
    positive definite. Hence, by induction, there is a unique lower-triangular matrix
    \(\Lambda_{11}\) with positive diagonal elements satisfying the first equation.
    We can then obtain \(\bfbeta_{12}\) from the second equation by forward substitution.
    And finally we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{22} = \sqrt{\beta_{22} - \bflambda_{12}^T \bflambda_{12}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We do have to check that the square root above exists. That is, we need to
    argue that the expression inside the square root is non-negative. In fact, for
    the claim to go through, we need it to be strictly positive. We notice that the
    expression inside the square root is in fact the Schur complement of the block
    \(B_{11}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \beta_{22} - \bflambda_{12}^T \bflambda_{12} &= \beta_{22}
    - (\Lambda_{11}^{-1} \bfbeta_{12})^T (\Lambda_{11}^{-1} \bfbeta_{12})\\ &= \beta_{22}
    - \bfbeta_{12}^T (\Lambda_{11}^{-1})^T \Lambda_{11}^{-1} \bfbeta_{12}\\ &= \beta_{22}
    - \bfbeta_{12}^T (\Lambda_{11} \Lambda_{11}^T)^{-1} \bfbeta_{12}\\ &= \beta_{22}
    - \bfbeta_{12}^T (B_{11})^{-1} \bfbeta_{12} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the equation \(\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\) on
    the first line, the identities \((Q W)^{-1} = W^{-1} Q^{-1}\) and \((Q^T)^{-1}
    = (Q^{-1})^T\) (see the exercise below) on the third line and the equation \(B_{11}
    = \Lambda_{11} \Lambda_{11}^T\) on the fourth line. By the *Schur Complement Lemma*,
    the Schur complement is positive definite. Because it is a scalar in this case,
    it is strictly positive (prove it!), which concludes the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Back to multivariate Gaussians** Returning to our motivation, we can generate
    samples from a \(N(\bmu, \bSigma)\) by first generating'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{Z} = (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d)) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(U_1, \ldots, U_d\) are independent uniform \([0,1]\) variables, then
    setting
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{X} = \bmu + L \mathbf{Z}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bSigma = L L^T\) is a Cholesky decomposition of \(\bSigma\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement this method.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: We generate some samples as an example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Using a Cholesky decomposition to solve the least squares problem** Another
    application of the Cholesky decomposition is to solving the least squares problem.
    In this section, we restrict ourselves to the case where \(A \in \mathbb{R}^{n\times
    m}\) has full column rank. By the *Least Squares and Positive Semidefiniteness
    Lemma*, we then have that \(A^T A\) is positive definite. By the *Cholesky Decomposition
    Theorem*, we can factorize this matrix as \(A^T A = L L^T\) where \(L\) is lower
    triangular with positive diagonal elements. The normal equations then reduce to'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L L^T \mathbf{x} = A^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This system can be solved in two steps. We first obtain the solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ L \mathbf{z} = A^T \mathbf{b} \]
  prefs: []
  type: TYPE_NORMAL
- en: by forward substitution. Then we obtain the solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ L^T \mathbf{x} = \mathbf{z} \]
  prefs: []
  type: TYPE_NORMAL
- en: by back-substitution. Note that \(L^T\) is indeed an upper triangular matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement this algorithm below. In our naive implementation,
    we assume that \(A\) has full column rank, and therefore that all steps are well-defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Other applications of the Cholesky decomposition are briefly described [here](https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications).
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.2.1\. Sentiment analysis[#](#sentiment-analysis "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As an application of the Naive Bayes model, we consider the task of sentiment
    analysis, which is a classification problem. We use a dataset available [here](https://www.kaggle.com/crowdflower/twitter-airline-sentiment).
    Quoting from there:'
  prefs: []
  type: TYPE_NORMAL
- en: A sentiment analysis job about the problems of each major U.S. airline. Twitter
    data was scraped from February of 2015 and contributors were asked to first classify
    positive, negative, and neutral tweets, followed by categorizing negative reasons
    (such as “late flight” or “rude service”).
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We first load a cleaned-up version of the data and look at its summary.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '|  | time | user | sentiment | text |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 2/24/15 11:35 | cairdin | neutral | @VirginAmerica What @dhepburn said.
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2/24/15 11:15 | jnardino | positive | @VirginAmerica plus you''ve added
    commercials t... |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2/24/15 11:15 | yvonnalynn | neutral | @VirginAmerica I didn''t today...
    Must mean I n... |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2/24/15 11:15 | jnardino | negative | @VirginAmerica it''s really aggressive
    to blast... |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2/24/15 11:14 | jnardino | negative | @VirginAmerica and it''s a really
    big bad thing... |'
  prefs: []
  type: TYPE_TB
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: We extract the text information in this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we convert our dataset into a matrix by creating a document-term matrix
    using [`sklearn.feature_extraction.text.CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform).
    Quoting [Wikipedia](https://en.wikipedia.org/wiki/Document-term_matrix):'
  prefs: []
  type: TYPE_NORMAL
- en: A document-term matrix or term-document matrix is a mathematical matrix that
    describes the frequency of terms that occur in a collection of documents. In a
    document-term matrix, rows correspond to documents in the collection and columns
    correspond to terms.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'By default, it first preprocesses the data. In particular, it lower-cases all
    words and removes punctuation. A more careful pre-procsseing would also include
    stemming, although we do not do this here. Regarding the latter, quoting [Wikipedia](https://en.wikipedia.org/wiki/Stemming):'
  prefs: []
  type: TYPE_NORMAL
- en: In linguistic morphology and information retrieval, stemming is the process
    of reducing inflected (or sometimes derived) words to their word stem, base or
    root form—generally a written word form. […] A computer program or subroutine
    that stems word may be called a stemming program, stemming algorithm, or stemmer.
    […] A stemmer for English operating on the stem cat should identify such strings
    as cats, catlike, and catty. A stemming algorithm might also reduce the words
    fishing, fished, and fisher to the stem fish. The stem need not be a word, for
    example the Porter algorithm reduces, argue, argued, argues, arguing, and argus
    to the stem argu.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: The list of all terms used can be accessed as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Because of our use of the multivariate Bernoulli naive Bayes model, it will
    be more convenient to work with a variant of the document-term matrix where each
    word is either present or absent. Note that, in the context of tweet data which
    are very short documents with likely little word repetition, there is probably
    not much difference.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: We also extract the labels (`neutral`, `postive`, `negative`) from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: We split the data into a training set and a test set using [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: We use the Naive Bayes method. We first construct the matrix \(N_{k,m}\) and
    the vector \(N_k\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: Above, the [`enumerate`](https://docs.python.org/3/library/functions.html#enumerate)
    function provides both the index and the value for each item in the `label_set`
    list during the loop. This allows the code to use the label’s value (`k`) and
    its numerical position (`i`) at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: We are ready to train on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: Next, we plot the vector \(p_{k,m}\) for each label \(k\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '![../../_images/e75f50e0059a06f686e844c95d499bf2dbc3ef78367a3fc95c0ed8f7df2202b1.png](../Images/dc490453944015ef54f86c78956d449e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compute a prediction on the test tweets. For example, for the 5th test
    tweet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: The following computes the overall accuracy over the test data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: To get a better understanding of the differences uncovered by Naive Bayes between
    the different labels, we identify words that are particularly common in one label,
    but not on the other. Recall that label `1` corresponds to `positive` while label
    `2` corresponds to `negative`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'One notices that many positive words do appear in this list: `awesome`, `best`,
    `great`, `love`, `thank`.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we notice: `bag`, `cancelled`, `delayed`, `hours`, `phone`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** The bag-of-words representation used in the sentiment analysis
    example is a simple but limited way to represent text data. More advanced representations
    such as word embeddings and transformer models can capture more semantic information.
    Ask your favorite AI chatbot to explain these representations and how they can
    be used for text classification tasks. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: '6.7.2.2\. Kalman filtering: missing data[#](#kalman-filtering-missing-data
    "Link to this heading")'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Kalman filtering, we can also allow for the possibility that some observations
    are missing. Imagine for instance losing GPS signal while going through a tunnel.
    The recursions above are still valid, with the only modification that the *Update*
    equations involving \(\bY_t\) are dropped at those times \(t\) where there is
    no observation. In Numpy, we can use [`NaN`](https://numpy.org/doc/stable/reference/constants.html#numpy.nan)
    to indicate the lack of observation. (Alternatively, one can use the [numpy.ma](https://numpy.org/doc/stable/reference/maskedarray.generic.html)
    module.)
  prefs: []
  type: TYPE_NORMAL
- en: We use a same sample path as above, but mask observations at times \(t=10,\ldots,20\).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: Here is the sample we are aiming to infer.
  prefs: []
  type: TYPE_NORMAL
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]</details> ![../../_images/f50483a562aeab113e7a2c066e9f1df40a3734ae2dcc59b104875762494a3a7e.png](../Images/23e83372ecc5a5cde94ad7f021c21a15.png)'
  prefs: []
  type: TYPE_NORMAL
- en: We modify the recursion accordingly, that is, skip the *Update* step when there
    is no observation to use for the update.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE137]</details> ![../../_images/2e92c840f3a0f15546d76a33647bbdb2a46015c60b362e0062398e2de1579630.png](../Images/6633cdad5605633dacdc68a6a28fde12.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 6.7.2.3\. Cholesky decomposition[#](#cholesky-decomposition "Link to this heading")
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we derive an important matrix factorization and apply it to
    generating multivariate Gaussians. We also revisit the least-squares problem.
    We begin with the motivation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generating multivariate Gaussians** Suppose we want to generate samples from
    a multivariate Gaussian \(\bX \sim N_d(\bmu, \bSigma)\) with given mean vector
    \(\bmu \in \mathbb{R}^d\) and positive definite covariance matrix \(\bSigma \in
    \mathbb{R}^{d \times d}\). Of course, in Numpy, we could use [`numpy.random.Generator.multivariate_normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html).
    But what is behind it? More precisely, suppose we have access to unlimited samples
    \(U_1, U_2, U_3, etc.\) from uniform random variables in \([0,1]\). How do we
    transform them to obtain samples from \(N_d(\bmu, \bSigma)\).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with the simplest case: \(d=1\), \(\mu = 0\), and \(\sigma^2 = 1\).
    That is, we first generate a univariate standard Normal. We have seen a recipe
    for doing this before, the inverse transform sampling method. Specifically, recall
    that the cumulative distribution function (CDF) of a random variable \(Z\) is
    defined as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ F_Z(z) = \mathbb{P}[Z \leq z], \qquad \forall z \in \mathbb{R}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Let \(\mathcal{Z}\) be the interval where \(F_Z(z) \in (0,1)\) and assume that
    \(F_X\) is strictly increasing on \(\mathcal{Z}\). Let \(U \sim \mathrm{U}[0,1]\).
    Then it can be shown that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbb{P}[F_X^{-1}(U) \leq z] = F_X(z). \]
  prefs: []
  type: TYPE_NORMAL
- en: So take \(F_Z = \Phi\), the CDF of the standard Normal. Then \(Z = \Phi^{-1}(U)\)
    is \(N(0,1)\).
  prefs: []
  type: TYPE_NORMAL
- en: How do we generate a \(N(\mu, \sigma^2)\) variable, for arbitrary \(\mu \in
    \mathbb{R}\) and \(\sigma^2 > 0\)? We use the fact that the linear transformation
    of Gaussian is still Gaussian. In particular, if \(Z \sim N(0,1)\), then
  prefs: []
  type: TYPE_NORMAL
- en: \[ X = \mu + \sigma Z \]
  prefs: []
  type: TYPE_NORMAL
- en: is \(N(\mu, \sigma^2)\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** In Python, \(\Phi^{-1}\) can be accessed using [`scipy.stats.norm.ppf`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html).
    We implement this next (with help from ChatGPT).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: We generate 1000 samples and plot the empirical distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]</details> ![../../_images/648741740d20b45ce65b2b2791d1603ea77bd902d8873c497c1a1d5f0cc41bb6.png](../Images/c4d1f8c3493a1c88b3b5727f55a25bef.png)'
  prefs: []
  type: TYPE_NORMAL
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**CHAT & LEARN** It turns out there is a neat trick to generate *two* independent
    samples from \(N(0,1)\) that does not rely on access to \(\Phi^{-1}\). It is called
    the Box-Muller transform. Ask your favorite AI chatbot about it. Modify our code
    above to implement it. \(\ddagger\)'
  prefs: []
  type: TYPE_NORMAL
- en: We move on to the multivariate case. We proceed similarly as before. First,
    how do we generate a \(d\)-dimensional Gaussian with mean vector \(\bmu = \mathbf{0}\)
    and identity covariance matrix \(\bSigma = I_{d \times d}\)? Easy – it has \(d\)
    independent components, each of which is standard Normal. So letting \(U_1, \ldots,
    U_d\) be independent uniform \([0,1]\) variables, then
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{Z} = (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d)) \]
  prefs: []
  type: TYPE_NORMAL
- en: is \(N(\mathbf{0}, I_{d \times d})\).
  prefs: []
  type: TYPE_NORMAL
- en: We now seek to generate a multivariate Gaussian with arbitrary mean vector \(\bmu\)
    and positive definite covariance matrix \(\bSigma\). Again, we use a linear transformation
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{X} = \mathbf{a} + A \mathbf{Z}. \]
  prefs: []
  type: TYPE_NORMAL
- en: What are the right choices for \(a \in \mathbb{R}^d\) and \(A \in \mathbb{R}^{d
    \times d}\)? We need to match the obtained and desired mean and covariance. We
    start with the mean. By linearity of expectation,
  prefs: []
  type: TYPE_NORMAL
- en: \[ \E[\mathbf{X}] = \E[\mathbf{a} + A \mathbf{Z}] = \mathbf{a} + A \,\E[\mathbf{Z}]
    = \mathbf{a}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Hence we pick \(\mathbf{a} := \bmu\).
  prefs: []
  type: TYPE_NORMAL
- en: As for the covariance, using the *Covariance of a Linear Transformation*, we
    get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \cov[\mathbf{X}] = A \,\cov[\mathbf{Z}] A^T = A A^T. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have a problem: what is a matrix \(A\) such that'
  prefs: []
  type: TYPE_NORMAL
- en: \[ A A^T = \bSigma? \]
  prefs: []
  type: TYPE_NORMAL
- en: In some sense, we are looking for a sort of “square root” of the covariance
    matrix. There are several ways of doing this. The Cholesky decomposition is one
    of them. We return to generating samples from \(N(\bmu, \bSigma)\) after introducing
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '**A matrix factorization** Our key linear-algebraic result of this section
    is the following. The matrix factorization in the next theorem is called a Cholesky
    decomposition. It has many [applications](https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications).'
  prefs: []
  type: TYPE_NORMAL
- en: '**THEOREM** **(Cholesky Decomposition)** Any positive definite matrix \(B \in
    \mathbb{R}^{n \times n}\) can be factorized uniquely as'
  prefs: []
  type: TYPE_NORMAL
- en: \[ B = L L^T \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(L \in \mathbb{R}^{n \times n}\) is a lower triangular matrix with positive
    entries on the diagonal. \(\sharp\)
  prefs: []
  type: TYPE_NORMAL
- en: 'The proof is provided below. It is based on deriving an algorithm for computing
    the Cholesky decomposition: we grow \(L\) starting from its top-left corner by
    successively computing its next row based on the previously constructed submatrix.
    Note that, because \(L\) is lower triangular, it suffices to compute its elements
    on and below the diagonal. We first give the algorithm, then establish that it
    is well-defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure:** Access pattern ([Source](https://en.wikipedia.org/wiki/File:Chol.gif))'
  prefs: []
  type: TYPE_NORMAL
- en: '![Access pattern](../Images/20de5b823040500b0d04511959484e97.png)'
  prefs: []
  type: TYPE_IMG
- en: \(\bowtie\)
  prefs: []
  type: TYPE_NORMAL
- en: '**EXAMPLE:** Before proceeeding with the general method, we give a small example
    to provide some intuition as to how it operates. We need a positive definite matrix.
    Consider the matrix'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} A = \begin{pmatrix} 1 & 2 & 1\\ 0 & -2 & 1\\ 0 & 0 & 1\\ 0 &
    0 & 0 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: It has full column rank (why?). Recall that, in that case, the \(B = A^T A\)
    is positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: That is, the matrix
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B = A^T A = \begin{pmatrix} 1 & 2 & 1\\ 2 & 8 & 0\\ 1 & 0 &
    3 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: is positive definite.
  prefs: []
  type: TYPE_NORMAL
- en: Let \(L = (\ell_{i,j})_{i,j=1}^3\) be lower triangular. We seek to solve \(L
    L^T = B\) for the nonzero entries of \(L\). Observe that
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \ell_{1,1} & 0 & 0\\ \ell_{2,1} & \ell_{2,2}
    & 0\\ \ell_{3,1} & \ell_{3,2} & \ell_{3,3} \end{pmatrix} \begin{pmatrix} \ell_{1,1}
    & \ell_{2,1} & \ell_{3,1}\\ 0 & \ell_{2,2} & \ell_{3,2}\\ 0 & 0 & \ell_{3,3} \end{pmatrix}
    = \begin{pmatrix} \ell_{1,1}^2 & \ell_{1,1}\ell_{2,1} & \ell_{1,1}\ell_{3,1}\\
    \ell_{1,1}\ell_{2,1} & \ell_{2,1}^2 + \ell_{2,2}^2 & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
    \ell_{1,1}\ell_{3,1} & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} & \ell_{3,1}^2
    + \ell_{3,2}^2 + \ell_{3,3} \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: The system
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} \ell_{1,1}^2 & \ell_{1,1}\ell_{2,1} & \ell_{1,1}\ell_{3,1}\\
    \ell_{1,1}\ell_{2,1} & \ell_{2,1}^2 + \ell_{2,2}^2 & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2}\\
    \ell_{1,1}\ell_{3,1} & \ell_{2,1}\ell_{3,1} + \ell_{2,2}\ell_{3,2} & \ell_{3,1}^2
    + \ell_{3,2}^2 + \ell_{3,3}^2 \end{pmatrix} = \begin{pmatrix} 1 & 2 & 1\\ 2 &
    8 & 0\\ 1 & 0 & 3 \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: turns out to be fairly simple to solve.
  prefs: []
  type: TYPE_NORMAL
- en: From the first entry, we get \(\ell_{1,1} = 1\) (where we took the positive
    solution to \(\ell_{1,1}^2 = 1\)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given that \(\ell_{1,1}\) is known, entry \(\ell_{2,1}\) is determined from
    \(\ell_{1,1}\ell_{2,1} =2\) in the first entry of the second row. That is, \(\ell_{2,1}
    =2\). Then the second entry of the second row gives \(\ell_{2,2}\) through \(\ell_{2,1}^2
    + \ell_{2,2}^2 = 8\). So \(\ell_{2,2} = 2\) (again we take the positive solution).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We move to the third row. The first entry gives \(\ell_{3,1} = 1\), the second
    entry gives \(\ell_{3,2} = -1\) and finally the third entry leads to \(\ell_{3,3}
    = 1\).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hence we have
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L = \begin{pmatrix} \ell_{1,1} & 0 & 0\\ \ell_{2,1} & \ell_{2,2}
    & 0\\ \ell_{3,1} & \ell_{3,2} & \ell_{3,3} \end{pmatrix} = \begin{pmatrix} 1 &
    0 & 0\\ 2 & 2 & 0\\ 1 & -1 & 1 \end{pmatrix}. \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: \(\lhd\)
  prefs: []
  type: TYPE_NORMAL
- en: To detail the computation of the Cholesky decomposition \(L L^T\) of \(B\),
    we will need some notation. Write \(B = (b_{i,j})_{i,j=1}^n\) and \(L = (\ell_{i,j})_{i,j=1}^n\).
    Let \(L_{(k)} = (\ell_{i,j})_{i,j=1}^k\) be the first \(k\) rows and columns of
    \(L\), let \(\bflambda_{(k)}^T = (\ell_{k,1},\ldots,\ell_{k,k-1})\) be the row
    vector corresponding to the first \(k-1\) entries of row \(k\) of \(L\), and let
    \(\bfbeta_{(k)}^T = (b_{k,1},\ldots,b_{k,k-1})\) be the row vector corresponding
    to the first \(k-1\) entries of row \(k\) of \(B\).
  prefs: []
  type: TYPE_NORMAL
- en: The strategy is to compute \(L_{(1)}\), then \(L_{(2)}\), then \(L_{(3)}\) and
    so on. With the notation above, \(L_{(j)}\) can be written in block form as
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} L_{(j)} = \begin{pmatrix} L_{(j-1)} & \mathbf{0}\\ \bflambda_{(j)}^T
    & \ell_{j,j}. \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: Hence, once \(L_{(j-1)}\) is known, in order to compute \(L_{(j)}\) one only
    needs \(\bflambda_{(j)}\) and \(\ell_{j,j}\). We show next that they satisfy easily
    solvable systems of equations.
  prefs: []
  type: TYPE_NORMAL
- en: We first note that the \((1,1)\) entry of the matrix equation \(L L^T = B\)
    implies that
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell_{1,1}^2 = b_{1,1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: So we set
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{(1)} = \ell_{1,1} = \sqrt{b_{1,1}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'For this step to be well-defined, it needs to be the case that \(b_{1,1} >
    0\). It is easy to see that it follows from the positive definiteness of \(B\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[ 0 < \langle \mathbf{e}_1, B \mathbf{e}_1\rangle = \mathbf{e}_1^T B_{\cdot,1}
    = b_{1,1}. \]
  prefs: []
  type: TYPE_NORMAL
- en: Proceeding by induction, assume \(L_{(j-1)}\) has been constructed. The first
    \(j-1\) elements of the \(j\)-th row of the matrix equation \(L L^T = B\) translate
    into
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{j,\cdot} (L^T)_{\cdot,1:j-1} = \bflambda_{(j)}^T L_{(j-1)}^T = \bfbeta_{(j)}^T,
    \]
  prefs: []
  type: TYPE_NORMAL
- en: where \((L^T)_{\cdot,1:j-1}\) denotes the first \(j-1\) columns of \(L^T\).
    In the first equality above, we used the fact that \(L^T\) is upper triangular.
    Taking a transpose, the resulting linear system of equations
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{(j-1)} \bflambda_{(j)} = \bfbeta_{(j)} \]
  prefs: []
  type: TYPE_NORMAL
- en: can be solved by forward substitution (since \(\bfbeta_{(j)}\) is part of the
    input and \(L_{(j-1)}\) was previously computed). The fact that this system has
    a unique solution (more specifically, that the diagonal entries of \(L_{(j-1)}\)
    are strictly positive) is established in the proof of the *Cholesky Decomposition
    Theorem*.
  prefs: []
  type: TYPE_NORMAL
- en: The \((j,j)\)-th entry of the matrix equation \(L L^T = B\) translates into
  prefs: []
  type: TYPE_NORMAL
- en: \[ L_{j,\cdot} (L^T)_{\cdot,j} = \sum_{k=1}^j \ell_{j,k}^2 = b_{j,j}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where again we used the fact that \(L^T\) is upper triangular. Since \(\ell_{j,1},
    \ldots, \ell_{j,j-1}\) are the elements of \(\bflambda_{(j)}\), they have already
    been determined. So we can set
  prefs: []
  type: TYPE_NORMAL
- en: \[ \ell_{j,j} = \sqrt{b_{j,j} - \sum_{k=1}^{j-1} \ell_{j,k}^2}. \]
  prefs: []
  type: TYPE_NORMAL
- en: The fact that we are taking the square root of a positive quantity is established
    in the proof of the *Cholesky Decomposition Theorem*. Finally, from \(L_{(j-1)}\),
    \(\bflambda_{(j)}\), and \(\ell_{j,j}\), we construct \(L_{(j)}\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement the algorithm above. In our naive implementation,
    we assume that \(B\) is positive definite, and therefore that all steps are well-defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: Here is a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: We can check that it produces the right factorization.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Proof of Cholesky decomposition theorem** We give a proof of the *Cholesky
    Decomposition Theorem*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof idea:* Assuming by induction that the upper-left corner of the matrix
    \(B\) has a Cholesky decomposition, one finds equations for the remaining row
    that can be solved uniquely by the properties established in the previous subsection.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Proof:* If \(n=1\), we have shown previously that \(b_{1,1} > 0\), and hence
    we can take \(L = [\ell_{1,1}]\) where \(\ell_{1,1} = \sqrt{b_{1,1}}\). Assuming
    the result holds for positive definite matrices in \(\mathbb{R}^{(n-1) \times
    (n-1)}\), we first re-write \(B = L L^T\) in block form'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} \begin{pmatrix} B_{11} & \bfbeta_{12}\\ \bfbeta_{12}^T & \beta_{22}
    \end{pmatrix} = \begin{pmatrix} \Lambda_{11} & \mathbf{0}\\ \bflambda_{12}^T &
    \lambda_{22} \end{pmatrix} \begin{pmatrix} \Lambda_{11}^T & \bflambda_{12}\\ \mathbf{0}^T
    & \lambda_{22} \end{pmatrix} \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: where \(B_{11}, \Lambda_{11} \in \mathbb{R}^{n-1 \times n-1}\), \(\bfbeta_{12},
    \bflambda_{12} \in \mathbb{R}^{n-1}\) and \(\beta_{22}, \lambda_{22} \in \mathbb{R}\).
    By block matrix algebra, we get the system
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{split} B_{11} = \Lambda_{11} \Lambda_{11}^T\\ \bfbeta_{12} = \Lambda_{11}
    \bflambda_{12}\\ \beta_{22} = \bflambda_{12}^T \bflambda_{12} + \lambda_{22}^2.
    \end{split}\]
  prefs: []
  type: TYPE_NORMAL
- en: By the *Principal Submatrices Lemma*, the principal submatrix \(B_{11}\) is
    positive definite. Hence, by induction, there is a unique lower-triangular matrix
    \(\Lambda_{11}\) with positive diagonal elements satisfying the first equation.
    We can then obtain \(\bfbeta_{12}\) from the second equation by forward substitution.
    And finally we get
  prefs: []
  type: TYPE_NORMAL
- en: \[ \lambda_{22} = \sqrt{\beta_{22} - \bflambda_{12}^T \bflambda_{12}}. \]
  prefs: []
  type: TYPE_NORMAL
- en: 'We do have to check that the square root above exists. That is, we need to
    argue that the expression inside the square root is non-negative. In fact, for
    the claim to go through, we need it to be strictly positive. We notice that the
    expression inside the square root is in fact the Schur complement of the block
    \(B_{11}\):'
  prefs: []
  type: TYPE_NORMAL
- en: \[\begin{align*} \beta_{22} - \bflambda_{12}^T \bflambda_{12} &= \beta_{22}
    - (\Lambda_{11}^{-1} \bfbeta_{12})^T (\Lambda_{11}^{-1} \bfbeta_{12})\\ &= \beta_{22}
    - \bfbeta_{12}^T (\Lambda_{11}^{-1})^T \Lambda_{11}^{-1} \bfbeta_{12}\\ &= \beta_{22}
    - \bfbeta_{12}^T (\Lambda_{11} \Lambda_{11}^T)^{-1} \bfbeta_{12}\\ &= \beta_{22}
    - \bfbeta_{12}^T (B_{11})^{-1} \bfbeta_{12} \end{align*}\]
  prefs: []
  type: TYPE_NORMAL
- en: where we used the equation \(\bfbeta_{12} = \Lambda_{11} \bflambda_{12}\) on
    the first line, the identities \((Q W)^{-1} = W^{-1} Q^{-1}\) and \((Q^T)^{-1}
    = (Q^{-1})^T\) (see the exercise below) on the third line and the equation \(B_{11}
    = \Lambda_{11} \Lambda_{11}^T\) on the fourth line. By the *Schur Complement Lemma*,
    the Schur complement is positive definite. Because it is a scalar in this case,
    it is strictly positive (prove it!), which concludes the proof. \(\square\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Back to multivariate Gaussians** Returning to our motivation, we can generate
    samples from a \(N(\bmu, \bSigma)\) by first generating'
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{Z} = (\Phi^{-1}(U_1),\ldots,\Phi^{-1}(U_d)) \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(U_1, \ldots, U_d\) are independent uniform \([0,1]\) variables, then
    setting
  prefs: []
  type: TYPE_NORMAL
- en: \[ \mathbf{X} = \bmu + L \mathbf{Z}, \]
  prefs: []
  type: TYPE_NORMAL
- en: where \(\bSigma = L L^T\) is a Cholesky decomposition of \(\bSigma\).
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement this method.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: We generate some samples as an example.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: <details class="hide above-input"><summary aria-label="Toggle hidden content">Show
    code cell source Hide code cell source</summary>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]</details>'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: '**Using a Cholesky decomposition to solve the least squares problem** Another
    application of the Cholesky decomposition is to solving the least squares problem.
    In this section, we restrict ourselves to the case where \(A \in \mathbb{R}^{n\times
    m}\) has full column rank. By the *Least Squares and Positive Semidefiniteness
    Lemma*, we then have that \(A^T A\) is positive definite. By the *Cholesky Decomposition
    Theorem*, we can factorize this matrix as \(A^T A = L L^T\) where \(L\) is lower
    triangular with positive diagonal elements. The normal equations then reduce to'
  prefs: []
  type: TYPE_NORMAL
- en: \[ L L^T \mathbf{x} = A^T \mathbf{b}. \]
  prefs: []
  type: TYPE_NORMAL
- en: This system can be solved in two steps. We first obtain the solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ L \mathbf{z} = A^T \mathbf{b} \]
  prefs: []
  type: TYPE_NORMAL
- en: by forward substitution. Then we obtain the solution to
  prefs: []
  type: TYPE_NORMAL
- en: \[ L^T \mathbf{x} = \mathbf{z} \]
  prefs: []
  type: TYPE_NORMAL
- en: by back-substitution. Note that \(L^T\) is indeed an upper triangular matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**NUMERICAL CORNER:** We implement this algorithm below. In our naive implementation,
    we assume that \(A\) has full column rank, and therefore that all steps are well-defined.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: \(\unlhd\)
  prefs: []
  type: TYPE_NORMAL
- en: Other applications of the Cholesky decomposition are briefly described [here](https://en.wikipedia.org/wiki/Cholesky_decomposition#Applications).
  prefs: []
  type: TYPE_NORMAL
