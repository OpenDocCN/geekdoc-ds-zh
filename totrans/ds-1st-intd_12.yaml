- en: 'Chapter 7 Regression I: K-nearest neighbors'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 回归I：K最近邻
- en: 原文：[https://datasciencebook.ca/regression1.html](https://datasciencebook.ca/regression1.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://datasciencebook.ca/regression1.html](https://datasciencebook.ca/regression1.html)
- en: 7.1 Overview
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 概述
- en: 'This chapter continues our foray into answering predictive questions. Here
    we will focus on predicting *numerical* variables and will use *regression* to
    perform this task. This is unlike the past two chapters, which focused on predicting
    categorical variables via classification. However, regression does have many similarities
    to classification: for example, just as in the case of classification, we will
    split our data into training, validation, and test sets, we will use `tidymodels`
    workflows, we will use a K-nearest neighbors (K-NN) approach to make predictions,
    and we will use cross-validation to choose K. Because of how similar these procedures
    are, make sure to read Chapters [5](classification1.html#classification1) and
    [6](classification2.html#classification2) before reading this one—we will move
    a little bit faster here with the concepts that have already been covered. This
    chapter will primarily focus on the case where there is a single predictor, but
    the end of the chapter shows how to perform regression with more than one predictor
    variable, i.e., *multivariable regression*. It is important to note that regression
    can also be used to answer inferential and causal questions, however that is beyond
    the scope of this book.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章继续我们探索回答预测问题的旅程。在这里，我们将专注于预测*数值*变量，并使用*回归*来完成这项任务。这与前两章不同，前两章专注于通过分类预测分类变量。然而，回归与分类有许多相似之处：例如，就像在分类的情况下，我们将我们的数据分为训练集、验证集和测试集，我们将使用`tidymodels`工作流程，我们将使用K最近邻（K-NN）方法进行预测，我们将使用交叉验证来选择K。由于这些程序如此相似，在阅读本章之前，请务必阅读第[5](classification1.html#classification1)章和第[6](classification2.html#classification2)章——我们将在这里更快地介绍已经覆盖的概念。本章将主要关注只有一个预测变量的情况，但本章的结尾展示了如何进行包含多个预测变量的回归，即*多元回归*。需要注意的是，回归也可以用来回答推断和因果问题，但这超出了本书的范围。
- en: 7.2 Chapter learning objectives
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 章节学习目标
- en: 'By the end of the chapter, readers will be able to do the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，读者将能够做到以下事项：
- en: Recognize situations where a regression analysis would be appropriate for making
    predictions.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别适合进行预测分析的情境。
- en: Explain the K-nearest neighbors (K-NN) regression algorithm and describe how
    it differs from K-NN classification.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释K最近邻（K-NN）回归算法，并描述它与K-NN分类的不同之处。
- en: Interpret the output of a K-NN regression.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释K-NN回归的输出。
- en: In a data set with two or more variables, perform K-nearest neighbors regression
    in R.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在包含两个或更多变量的数据集中，使用R执行K最近邻回归。
- en: Evaluate K-NN regression prediction quality in R using the root mean squared
    prediction error (RMSPE).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用均方根预测误差（RMSPE）在R中评估K-NN回归预测质量。
- en: Estimate the RMSPE in R using cross-validation or a test set.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证或测试集在R中估计RMSPE。
- en: Choose the number of neighbors in K-nearest neighbors regression by minimizing
    estimated cross-validation RMSPE.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过最小化估计的交叉验证均方根预测误差（RMSPE）来选择K最近邻回归中的邻居数量。
- en: Describe underfitting and overfitting, and relate it to the number of neighbors
    in K-nearest neighbors regression.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述欠拟合和过拟合，并将其与K最近邻回归中的邻居数量联系起来。
- en: Describe the advantages and disadvantages of K-nearest neighbors regression.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述K最近邻回归的优点和缺点。
- en: 7.3 The regression problem
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 回归问题
- en: Regression, like classification, is a predictive problem setting where we want
    to use past information to predict future observations. But in the case of regression,
    the goal is to predict *numerical* values instead of *categorical* values. The
    variable that you want to predict is often called the *response variable*. For
    example, we could try to use the number of hours a person spends on exercise each
    week to predict their race time in the annual Boston marathon. As another example,
    we could try to use the size of a house to predict its sale price. Both of these
    response variables—race time and sale price—are numerical, and so predicting them
    given past data is considered a regression problem.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 回归，就像分类一样，是一个预测性问题设置，我们希望利用过去的信息来预测未来的观测值。但在回归的情况下，目标是预测*数值*而不是*类别*值。你想要预测的变量通常被称为*响应变量*。例如，我们可以尝试使用一个人每周花在锻炼上的小时数来预测他们在年度波士顿马拉松的赛跑时间。另一个例子，我们可以尝试使用房屋的大小来预测其销售价格。这两个响应变量——赛跑时间和销售价格——都是数值，因此根据过去的数据预测它们被认为是回归问题。
- en: Just like in the classification setting, there are many possible methods that
    we can use to predict numerical response variables. In this chapter we will focus
    on the **K-nearest neighbors** algorithm ([Fix and Hodges 1951](#ref-knnfix);
    [Cover and Hart 1967](#ref-knncover)), and in the next chapter we will study **linear
    regression**. In your future studies, you might encounter regression trees, splines,
    and general local regression methods; see the additional resources section at
    the end of the next chapter for where to begin learning more about these other
    methods.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在分类设置中一样，有许多可能的方法我们可以用来预测数值响应变量。在本章中，我们将重点介绍**K最近邻**算法([Fix and Hodges 1951](#ref-knnfix)；[Cover
    and Hart 1967](#ref-knncover))，在下一章中我们将研究**线性回归**。在你的未来研究中，你可能会遇到回归树、样条和一般局部回归方法；下一章末尾的附加资源部分提供了开始学习这些其他方法的起点。
- en: Many of the concepts from classification map over to the setting of regression.
    For example, a regression model predicts a new observation’s response variable
    based on the response variables for similar observations in the data set of past
    observations. When building a regression model, we first split the data into training
    and test sets, in order to ensure that we assess the performance of our method
    on observations not seen during training. And finally, we can use cross-validation
    to evaluate different choices of model parameters (e.g., K in a K-nearest neighbors
    model). The major difference is that we are now predicting numerical variables
    instead of categorical variables.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 许多来自分类的概念可以映射到回归的设置中。例如，回归模型根据过去观测数据集中相似观测值的响应变量来预测新观测值的响应变量。在构建回归模型时，我们首先将数据分为训练集和测试集，以确保我们评估的方法在训练期间未看到的观测值上的性能。最后，我们可以使用交叉验证来评估不同模型参数选择（例如，K在K最近邻模型中）。主要区别在于我们现在预测的是数值变量而不是类别变量。
- en: '**Note:** You can usually tell whether a variable is numerical or categorical—and
    therefore whether you need to perform regression or classification—by taking the
    response variable for two observations X and Y from your data, and asking the
    question, “is response variable X *more* than response variable Y?” If the variable
    is categorical, the question will make no sense. (Is blue more than red? Is benign
    more than malignant?) If the variable is numerical, it will make sense. (Is 1.5
    hours more than 2.25 hours? Is $500,000 more than $400,000?) Be careful when applying
    this heuristic, though: sometimes categorical variables will be encoded as numbers
    in your data (e.g., “1” represents “benign”, and “0” represents “malignant”).
    In these cases you have to ask the question about the *meaning* of the labels
    (“benign” and “malignant”), not their values (“1” and “0”).'
  id: totrans-19
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**通常你可以通过从你的数据中取两个观测值X和Y的响应变量，并问自己，“响应变量X是否比响应变量Y*多*？”来判断一个变量是数值的还是类别的——因此是否需要执行回归或分类。如果变量是类别的，这个问题将没有意义。（蓝色是否比红色多？良性是否比恶性多？）如果变量是数值的，它将是有意义的。（1.5小时是否比2.25小时多？$500,000是否比$400,000多？）但是，在应用这个启发式方法时要小心：有时类别变量在你的数据中会被编码为数字（例如，“1”代表“良性”，而“0”代表“恶性”）。在这些情况下，你必须询问标签的*意义*（“良性”和“恶性”），而不是它们的值（“1”和“0”）。'
- en: 7.4 Exploring a data set
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 探索数据集
- en: 'In this chapter and the next, we will study a data set of [932 real estate
    transactions in Sacramento, California](https://support.spatialkey.com/spatialkey-sample-csv-data/)
    originally reported in the *Sacramento Bee* newspaper. We first need to formulate
    a precise question that we want to answer. In this example, our question is again
    predictive: Can we use the size of a house in the Sacramento, CA area to predict
    its sale price? A rigorous, quantitative answer to this question might help a
    realtor advise a client as to whether the price of a particular listing is fair,
    or perhaps how to set the price of a new listing. We begin the analysis by loading
    and examining the data, and setting the seed value.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和下一章中，我们将研究一个数据集，该数据集包含加利福尼亚州萨克拉门托的[932笔房地产交易](https://support.spatialkey.com/spatialkey-sample-csv-data/)，最初在《萨克拉门托蜜蜂报》上报道。我们首先需要制定一个精确的问题，这是我们想要回答的问题。在这个例子中，我们的问题仍然是预测性的：我们能否使用萨克拉门托，加利福尼亚地区房屋的大小来预测其销售价格？对这个问题的严格、定量的回答可能有助于房地产经纪人向客户建议某个特定列表的价格是否合理，或者如何设定新列表的价格。我们开始分析，通过加载数据、检查数据并设置种子值。
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The scientific question guides our initial exploration: the columns in the
    data that we are interested in are `sqft` (house size, in livable square feet)
    and `price` (house sale price, in US dollars (USD)). The first step is to visualize
    the data as a scatter plot where we place the predictor variable (house size)
    on the x-axis, and we place the response variable that we want to predict (sale
    price) on the y-axis.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 科学问题指导我们的初步探索：我们感兴趣的数据集中的列是`sqft`（房屋大小，以可居住平方英尺计）和`price`（房屋销售价格，以美元（USD）计）。第一步是将数据可视化为散点图，我们将预测变量（房屋大小）放在x轴上，我们将我们想要预测的响应变量（销售价格）放在y轴上。
- en: '**Note:** Given that the y-axis unit is dollars in Figure [7.1](regression1.html#fig:07-edaRegr),
    we format the axis labels to put dollar signs in front of the house prices, as
    well as commas to increase the readability of the larger numbers. We can do this
    in R by passing the `dollar_format` function (from the `scales` package) to the
    `labels` argument of the `scale_y_continuous` function.'
  id: totrans-25
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：**鉴于图[7.1](regression1.html#fig:07-edaRegr)中的y轴单位是美元，我们格式化轴标签，在房屋价格前放置美元符号，以及逗号以增加较大数字的可读性。我们可以在R中通过将`dollar_format`函数（来自`scales`包）传递给`scale_y_continuous`函数的`labels`参数来实现这一点。'
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Scatter plot of price (USD) versus house size (square feet).](../Images/5564a639ea3c5c4ccb6793abb85760c7.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![价格（美元）与房屋大小（平方英尺）的散点图。](../Images/5564a639ea3c5c4ccb6793abb85760c7.png)'
- en: 'Figure 7.1: Scatter plot of price (USD) versus house size (square feet).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：价格（美元）与房屋大小（平方英尺）的散点图。
- en: The plot is shown in Figure [7.1](regression1.html#fig:07-edaRegr). We can see
    that in Sacramento, CA, as the size of a house increases, so does its sale price.
    Thus, we can reason that we may be able to use the size of a not-yet-sold house
    (for which we don’t know the sale price) to predict its final sale price. Note
    that we do not suggest here that a larger house size *causes* a higher sale price;
    just that house price tends to increase with house size, and that we may be able
    to use the latter to predict the former.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示在图[7.1](regression1.html#fig:07-edaRegr)中。我们可以看到，在萨克拉门托，加利福尼亚，随着房屋大小的增加，其销售价格也增加。因此，我们可以推断，我们可能能够使用尚未售出的房屋的大小（我们不知道其销售价格）来预测其最终销售价格。请注意，我们在此处并不建议更大的房屋大小*导致*更高的销售价格；只是房屋价格往往随着房屋大小而增加，我们可能能够使用后者来预测前者。
- en: 7.5 K-nearest neighbors regression
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 K-最近邻回归
- en: Much like in the case of classification, we can use a K-nearest neighbors-based
    approach in regression to make predictions. Let’s take a small sample of the data
    in Figure [7.1](regression1.html#fig:07-edaRegr) and walk through how K-nearest
    neighbors (K-NN) works in a regression context before we dive in to creating our
    model and assessing how well it predicts house sale price. This subsample is taken
    to allow us to illustrate the mechanics of K-NN regression with a few data points;
    later in this chapter we will use all the data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类的情况类似，我们可以在回归中使用基于K-最近邻的方法进行预测。在我们深入创建模型并评估其预测房屋销售价格的效果之前，让我们先从图[7.1](regression1.html#fig:07-edaRegr)中的数据样本开始，了解K-最近邻（K-NN）在回归环境中的工作原理。这个子样本是为了让我们能够用几个数据点来说明K-NN回归的机制；在本章的后面部分，我们将使用所有数据。
- en: To take a small random sample of size 30, we’ll use the function `slice_sample`,
    and input the data frame to sample from and the number of rows to randomly select.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取一个大小为30的小随机样本，我们将使用`slice_sample`函数，并输入用于采样的数据框和要随机选择的行数。
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next let’s say we come across a 2,000 square-foot house in Sacramento we are
    interested in purchasing, with an advertised list price of $350,000\. Should we
    offer to pay the asking price for this house, or is it overpriced and we should
    offer less? Absent any other information, we can get a sense for a good answer
    to this question by using the data we have to predict the sale price given the
    sale prices we have already observed. But in Figure [7.2](regression1.html#fig:07-small-eda-regr),
    you can see that we have no observations of a house of size *exactly* 2,000 square
    feet. How can we predict the sale price?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，假设我们在萨克拉门托市发现了一座我们感兴趣购买的2,000平方英尺的房子，其广告标价为$350,000。我们应该出价购买这个房子的要价，还是它标价过高，我们应该出价更低？如果没有其他信息，我们可以通过使用我们已有的数据来预测基于已观察到的销售价格的销售价格，从而得到这个问题的良好答案。但在图[7.2](regression1.html#fig:07-small-eda-regr)中，你可以看到我们没有观察到大小为*正好*2,000平方英尺的房子。我们如何预测销售价格？
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Scatter plot of price (USD) versus house size (square feet) with vertical
    line indicating 2,000 square feet on x-axis.](../Images/b3dbc9516791e5812c6a44c1d3ead4c6.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![价格（美元）与房屋大小（平方英尺）的散点图，x轴上用垂直线表示2,000平方英尺。](../Images/b3dbc9516791e5812c6a44c1d3ead4c6.png)'
- en: 'Figure 7.2: Scatter plot of price (USD) versus house size (square feet) with
    vertical line indicating 2,000 square feet on x-axis.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：价格（美元）与房屋大小（平方英尺）的散点图，x轴上用垂直线表示2,000平方英尺。
- en: We will employ the same intuition from the classification chapter, and use the
    neighboring points to the new point of interest to suggest/predict what its sale
    price might be. For the example shown in Figure [7.2](regression1.html#fig:07-small-eda-regr),
    we find and label the 5 nearest neighbors to our observation of a house that is
    2,000 square feet.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用分类章节中的相同直觉，并使用新感兴趣点的邻近点来建议/预测其销售价格。对于图[7.2](regression1.html#fig:07-small-eda-regr)中显示的例子，我们找到并标记了与我们观察到的2,000平方英尺房屋的5个最近邻居。
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Scatter plot of price (USD) versus house size (square feet) with lines to
    5 nearest neighbors (highlighted in orange).](../Images/ec10104f2910ca434fa4b05fe2595347.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![价格（美元）与房屋大小（平方英尺）的散点图，到5个最近邻居的线条（用橙色突出显示）。](../Images/ec10104f2910ca434fa4b05fe2595347.png)'
- en: 'Figure 7.3: Scatter plot of price (USD) versus house size (square feet) with
    lines to 5 nearest neighbors (highlighted in orange).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：价格（美元）与房屋大小（平方英尺）的散点图，以及到5个最近邻居的线条（用橙色突出显示）。
- en: Figure [7.3](regression1.html#fig:07-knn3-example) illustrates the difference
    between the house sizes of the 5 nearest neighbors (in terms of house size) to
    our new 2,000 square-foot house of interest. Now that we have obtained these nearest
    neighbors, we can use their values to predict the sale price for the new home.
    Specifically, we can take the mean (or average) of these 5 values as our predicted
    value, as illustrated by the red point in Figure [7.4](regression1.html#fig:07-predictedViz-knn).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7.3](regression1.html#fig:07-knn3-example)说明了我们感兴趣的新的2,000平方英尺房屋的5个最近邻居（按房屋大小）之间的房屋大小差异。现在我们已经获得了这些最近邻居，我们可以使用他们的值来预测新房的销售价格。具体来说，我们可以取这5个值的平均值作为我们的预测值，如图[7.4](regression1.html#fig:07-predictedViz-knn)中的红色点所示。
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![Scatter plot of price (USD) versus house size (square feet) with predicted
    price for a 2,000 square-foot house based on 5 nearest neighbors represented as
    a red dot.](../Images/a0eb52d3a872ec70ba36bda6d3fa9733.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![价格（美元）与房屋大小（平方英尺）的散点图，基于5个最近邻居的预测价格，用红色点表示。](../Images/a0eb52d3a872ec70ba36bda6d3fa9733.png)'
- en: 'Figure 7.4: Scatter plot of price (USD) versus house size (square feet) with
    predicted price for a 2,000 square-foot house based on 5 nearest neighbors represented
    as a red dot.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：价格（美元）与房屋大小（平方英尺）的散点图，以及基于5个最近邻居的预测价格，用红色点表示。
- en: 'Our predicted price is $326,324 (shown as a red point in Figure [7.4](regression1.html#fig:07-predictedViz-knn)),
    which is much less than $350,000; perhaps we might want to offer less than the
    list price at which the house is advertised. But this is only the very beginning
    of the story. We still have all the same unanswered questions here with K-NN regression
    that we had with K-NN classification: which \(K\) do we choose, and is our model
    any good at making predictions? In the next few sections, we will address these
    questions in the context of K-NN regression.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测价格是 $326,324（在图 [7.4](regression1.html#fig:07-predictedViz-knn) 中以红色点表示），这比
    $350,000 少得多；也许我们可能想要以低于广告中列出的标价提供房屋。但这只是故事的开头。在这里，我们仍然有与 K-NN 分类相同的问题，即我们选择哪个
    \(K\)，我们的模型在做出预测方面是否表现良好？在接下来的几节中，我们将针对 K-NN 回归的上下文来解答这些问题。
- en: One strength of the K-NN regression algorithm that we would like to draw attention
    to at this point is its ability to work well with non-linear relationships (i.e.,
    if the relationship is not a straight line). This stems from the use of nearest
    neighbors to predict values. The algorithm really has very few assumptions about
    what the data must look like for it to work.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们想强调 K-NN 回归算法的一个优点，即它能够很好地处理非线性关系（即，如果关系不是一条直线）。这源于使用最近邻来预测值。该算法实际上对数据必须具有什么样子才能工作几乎没有假设。
- en: 7.6 Training, evaluating, and tuning the model
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 训练、评估和调整模型
- en: As usual, we must start by putting some test data away in a lock box that we
    will come back to only after we choose our final model. Let’s take care of that
    now. Note that for the remainder of the chapter we’ll be working with the entire
    Sacramento data set, as opposed to the smaller sample of 30 points that we used
    earlier in the chapter (Figure [7.2](regression1.html#fig:07-small-eda-regr)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们必须首先将一些测试数据放在一个保险箱中，我们将在选择最终模型后再回来查看。现在就让我们处理这个问题。请注意，在本章的剩余部分，我们将使用整个萨克拉门托数据集，而不是我们在本章早期使用的较小的30个点的样本（见图
    [7.2](regression1.html#fig:07-small-eda-regr)）。
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we’ll use cross-validation to choose \(K\). In K-NN classification, we
    used accuracy to see how well our predictions matched the true labels. We cannot
    use the same metric in the regression setting, since our predictions will almost
    never *exactly* match the true response variable values. Therefore in the context
    of K-NN regression we will use root mean square prediction error (RMSPE) instead.
    The mathematical formula for calculating RMSPE is:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用交叉验证来选择 \(K\)。在 K-NN 分类中，我们使用准确率来查看我们的预测与真实标签匹配得如何。在回归设置中，我们不能使用相同的指标，因为我们的预测几乎永远不会
    *完全* 匹配真实响应变量的值。因此，在 K-NN 回归的背景下，我们将使用均方根预测误差（RMSPE）代替。计算 RMSPE 的数学公式是：
- en: \[\text{RMSPE} = \sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2}\]
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: \[\text{RMSPE} = \sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2}\]
- en: 'where:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: \(n\) is the number of observations,
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(n\) 是观测数，
- en: \(y_i\) is the observed value for the \(i^\text{th}\) observation, and
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(y_i\) 是第 \(i^\text{th}\) 次观察的观测值，并且
- en: \(\hat{y}_i\) is the forecasted/predicted value for the \(i^\text{th}\) observation.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\hat{y}_i\) 是第 \(i^\text{th}\) 次观察的预测/预测值。
- en: In other words, we compute the *squared* difference between the predicted and
    true response value for each observation in our test (or validation) set, compute
    the average, and then finally take the square root. The reason we use the *squared*
    difference (and not just the difference) is that the differences can be positive
    or negative, i.e., we can overshoot or undershoot the true response value. Figure
    [7.5](regression1.html#fig:07-verticalerrors) illustrates both positive and negative
    differences between predicted and true response values. So if we want to measure
    error—a notion of distance between our predicted and true response values—we want
    to make sure that we are only adding up positive values, with larger positive
    values representing larger mistakes. If the predictions are very close to the
    true values, then RMSPE will be small. If, on the other-hand, the predictions
    are very different from the true values, then RMSPE will be quite large. When
    we use cross-validation, we will choose the \(K\) that gives us the smallest RMSPE.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们计算测试（或验证）集中每个观测值的预测值与真实响应值之间的*平方*差异，计算平均值，然后最后取平方根。我们使用*平方*差异（而不是差异）的原因是差异可以是正的或负的，即我们可以高估或低估真实响应值。图[7.5](regression1.html#fig:07-verticalerrors)展示了预测值与真实响应值之间的正负差异。因此，如果我们想衡量误差——即我们的预测值与真实响应值之间的距离——我们想要确保我们只累加正数，较大的正数代表较大的错误。如果预测值非常接近真实值，那么RMSPE将会很小。相反，如果预测值与真实值差异很大，那么RMSPE将会相当大。当我们使用交叉验证时，我们将选择给出最小RMSPE的\(K\)值。
- en: '![Scatter plot of price (USD) versus house size (square feet) with example
    predictions (blue line) and the error in those predictions compared with true
    response values (vertical lines).](../Images/a976e10edc2deeac1c87547e31f782dd.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![价格（美元）与房屋面积（平方英尺）的散点图，包括示例预测（蓝色线条）以及与真实响应值的误差比较（垂直线条）](../Images/a976e10edc2deeac1c87547e31f782dd.png)'
- en: 'Figure 7.5: Scatter plot of price (USD) versus house size (square feet) with
    example predictions (blue line) and the error in those predictions compared with
    true response values (vertical lines).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：价格（美元）与房屋面积（平方英尺）的散点图，包括示例预测（蓝色线条）以及与真实响应值的误差比较（垂直线条）。
- en: '**Note:** When using many code packages (`tidymodels` included), the evaluation
    output we will get to assess the prediction quality of our K-NN regression models
    is labeled “RMSE”, or “root mean squared error”. Why is this so, and why not RMSPE?
    In statistics, we try to be very precise with our language to indicate whether
    we are calculating the prediction error on the training data (*in-sample* prediction)
    versus on the testing data (*out-of-sample* prediction). When predicting and evaluating
    prediction quality on the training data, we say RMSE. By contrast, when predicting
    and evaluating prediction quality on the testing or validation data, we say RMSPE.
    The equation for calculating RMSE and RMSPE is exactly the same; all that changes
    is whether the \(y\)s are training or testing data. But many people just use RMSE
    for both, and rely on context to denote which data the root mean squared error
    is being calculated on.'
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**注意：** 当使用许多代码包（包括`tidymodels`）时，我们将得到的评估输出用于评估我们的K-NN回归模型的预测质量，被标记为“RMSE”，即“均方根误差”。为什么是这样，而不是RMSPE？在统计学中，我们试图用非常精确的语言来表示我们是在计算训练数据上的预测误差（*样本内*预测）还是在测试数据上的预测误差（*样本外*预测）。当在训练数据上预测和评估预测质量时，我们说RMSE。相比之下，当在测试或验证数据上预测和评估预测质量时，我们说RMSPE。计算RMSE和RMSPE的公式完全相同；唯一不同的是\(y\)是训练数据还是测试数据。但许多人只是使用RMSE，并依赖上下文来表示正在计算均方根误差的数据。'
- en: Now that we know how we can assess how well our model predicts a numerical value,
    let’s use R to perform cross-validation and to choose the optimal \(K\). First,
    we will create a recipe for preprocessing our data. Note that we include standardization
    in our preprocessing to build good habits, but since we only have one predictor,
    it is technically not necessary; there is no risk of comparing two predictors
    of different scales. Next we create a model specification for K-nearest neighbors
    regression. Note that we use `set_mode("regression")` now in the model specification
    to denote a regression problem, as opposed to the classification problems from
    the previous chapters. The use of `set_mode("regression")` essentially tells `tidymodels`
    that we need to use different metrics (RMSPE, not accuracy) for tuning and evaluation.
    Then we create a 5-fold cross-validation object, and put the recipe and model
    specification together in a workflow.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何评估我们的模型预测数值的好坏，让我们使用R来进行交叉验证并选择最优的\(K\)。首先，我们将创建一个预处理数据的配方。请注意，我们在预处理中包括标准化以养成良好的习惯，但由于我们只有一个预测因子，从技术上讲这不是必要的；没有比较不同尺度的两个预测因子的风险。接下来，我们为K近邻回归创建一个模型规范。请注意，我们现在在模型规范中使用`set_mode("regression")`来表示回归问题，而不是前几章中的分类问题。使用`set_mode("regression")`实际上告诉`tidymodels`我们需要使用不同的度量（RMSPE，而不是准确率）来进行调整和评估。然后我们创建一个5折交叉验证对象，并将配方和模型规范组合在一起形成一个工作流程。
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Next we run cross-validation for a grid of numbers of neighbors ranging from
    1 to 200. The following code tunes the model and returns the RMSPE for each number
    of neighbors. In the output of the `sacr_results` results data frame, we see that
    the `neighbors` variable contains the value of \(K\), the mean (`mean`) contains
    the value of the RMSPE estimated via cross-validation, and the standard error
    (`std_err`) contains a value corresponding to a measure of how uncertain we are
    in the mean value. A detailed treatment of this is beyond the scope of this chapter;
    but roughly, if your estimated mean RMSPE is $100,000 and standard error is $1,000,
    you can expect the *true* RMSPE to be somewhere roughly between $99,000 and $101,000
    (although it may fall outside this range). You may ignore the other columns in
    the metrics data frame, as they do not provide any additional insight.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对从1到200的邻居数量网格进行交叉验证。以下代码调整模型并返回每个邻居数量的均方根预测误差（RMSPE）。在`sacr_results`结果数据框的输出中，我们看到`neighbors`变量包含\(K\)的值，`mean`包含通过交叉验证估计的RMSPE的值，而`std_err`包含一个与我们对平均值不确定性的度量相对应的值。对此的详细讨论超出了本章的范围；但大致来说，如果你的估计均方根预测误差为$100,000，标准误差为$1,000，你可以预期真实的RMSPE大约在$99,000到$101,000之间（尽管它可能超出这个范围）。你可以忽略度量数据框中的其他列，因为它们不会提供任何额外的见解。
- en: '[PRE12]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Effect of the number of neighbors on the RMSPE.](../Images/4687f8d6f2ba43d514b407af5e02b5d4.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![邻居数量对RMSPE的影响。](../Images/4687f8d6f2ba43d514b407af5e02b5d4.png)'
- en: 'Figure 7.6: Effect of the number of neighbors on the RMSPE.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：邻居数量对RMSPE的影响。
- en: 'Figure [7.6](regression1.html#fig:07-choose-k-knn-plot) visualizes how the
    RMSPE varies with the number of neighbors \(K\). We take the *minimum* RMSPE to
    find the best setting for the number of neighbors:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7.6](regression1.html#fig:07-choose-k-knn-plot)展示了均方根预测误差（RMSPE）如何随着邻居数量\(K\)的变化而变化。我们取最小RMSPE来找到邻居数量的最佳设置：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The smallest RMSPE occurs when \(K =\) 52.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当\(K =\) 52时，出现最小的RMSPE。
- en: 7.7 Underfitting and overfitting
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.7欠拟合和过拟合
- en: Similar to the setting of classification, by setting the number of neighbors
    to be too small or too large, we cause the RMSPE to increase, as shown in Figure
    [7.6](regression1.html#fig:07-choose-k-knn-plot). What is happening here?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类设置类似，通过将邻居数量设置得太小或太大，我们导致RMSPE增加，如图[7.6](regression1.html#fig:07-choose-k-knn-plot)所示。这里发生了什么？
- en: 'Figure [7.7](regression1.html#fig:07-howK) visualizes the effect of different
    settings of \(K\) on the regression model. Each plot shows the predicted values
    for house sale price from our K-NN regression model on the training data for 6
    different values for \(K\): 1, 3, 25, 52, 250, and 680 (almost the entire training
    set). For each model, we predict prices for the range of possible home sizes we
    observed in the data set (here 500 to 5,000 square feet) and we plot the predicted
    prices as a blue line.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7.7](regression1.html#fig:07-howK)展示了不同的\(K\)设置对回归模型的影响。每个图表显示了我们的K-NN回归模型在6个不同\(K\)值（1、3、25、52、250和680，几乎整个训练集）的训练数据上的房价预测值。对于每个模型，我们预测数据集中观察到的可能住宅面积范围（此处为500至5,000平方英尺）的价格，并将预测价格以蓝色线条绘制。
- en: '![Predicted values for house price (represented as a blue line) from K-NN regression
    models for six different values for $K$.](../Images/11f818ccaa36f2890e1a966d55677567.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![从K-NN回归模型预测的房价值（以蓝色线条表示）对应于六个不同的\(K\)值。](../Images/11f818ccaa36f2890e1a966d55677567.png)'
- en: 'Figure 7.7: Predicted values for house price (represented as a blue line) from
    K-NN regression models for six different values for \(K\).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：从K-NN回归模型预测的房价值（以蓝色线条表示）对应于六个不同的\(K\)值。
- en: Figure [7.7](regression1.html#fig:07-howK) shows that when \(K\) = 1, the blue
    line runs perfectly through (almost) all of our training observations. This happens
    because our predicted values for a given region (typically) depend on just a single
    observation. In general, when \(K\) is too small, the line follows the training
    data quite closely, even if it does not match it perfectly. If we used a different
    training data set of house prices and sizes from the Sacramento real estate market,
    we would end up with completely different predictions. In other words, the model
    is *influenced too much* by the data. Because the model follows the training data
    so closely, it will not make accurate predictions on new observations which, generally,
    will not have the same fluctuations as the original training data. Recall from
    the classification chapters that this behavior—where the model is influenced too
    much by the noisy data—is called *overfitting*; we use this same term in the context
    of regression.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7.7](regression1.html#fig:07-howK)显示，当\(K\) = 1时，蓝色线条完美地穿过（几乎）所有我们的训练观察值。这是因为我们对于一个特定区域（通常）的预测值仅依赖于单个观察值。一般来说，当\(K\)值太小，线条会非常接近地跟随训练数据，即使它并不完全匹配。如果我们使用来自萨克拉门托房地产市场的不同训练数据集的房价和面积，我们最终会得到完全不同的预测。换句话说，模型过多地受到了数据的影响。因为模型非常接近地跟随训练数据，它将不会对新观察值做出准确的预测，而新观察值通常不会有与原始训练数据相同的波动。回想一下分类章节中的内容，这种模型过多地受到噪声数据影响的行称为*过拟合*；我们在回归的上下文中也使用这个相同的术语。
- en: What about the plots in Figure [7.7](regression1.html#fig:07-howK) where \(K\)
    is quite large, say, \(K\) = 250 or 680? In this case the blue line becomes extremely
    smooth, and actually becomes flat once \(K\) is equal to the number of datapoints
    in the training set. This happens because our predicted values for a given x value
    (here, home size), depend on many neighboring observations; in the case where
    \(K\) is equal to the size of the training set, the prediction is just the mean
    of the house prices (completely ignoring the house size). In contrast to the \(K=1\)
    example, the smooth, inflexible blue line does not follow the training observations
    very closely. In other words, the model is *not influenced enough* by the training
    data. Recall from the classification chapters that this behavior is called *underfitting*;
    we again use this same term in the context of regression.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，图[7.7](regression1.html#fig:07-howK)中\(K\)值相当大的图表又如何呢？例如，\(K\) = 250或680。在这种情况下，蓝色线条变得极其平滑，实际上当\(K\)等于训练数据点的数量时，它会变得平坦。这是因为我们对于一个特定x值（此处，住宅大小）的预测值依赖于许多邻近的观察值；在\(K\)等于训练集大小的情况下，预测值仅仅是房价的平均值（完全忽略了住宅大小）。与\(K=1\)的例子相比，平滑且不灵活的蓝色线条并不非常接近地跟随训练观察值。换句话说，模型没有足够地受到训练数据的影响。回想一下分类章节中的内容，这种行为称为*欠拟合*；我们在回归的上下文中再次使用这个相同的术语。
- en: 'Ideally, what we want is neither of the two situations discussed above. Instead,
    we would like a model that (1) follows the overall “trend” in the training data,
    so the model actually uses the training data to learn something useful, and (2)
    does not follow the noisy fluctuations, so that we can be confident that our model
    will transfer/generalize well to other new data. If we explore the other values
    for \(K\), in particular \(K\) = 52 (as suggested by cross-validation), we can
    see it achieves this goal: it follows the increasing trend of house price versus
    house size, but is not influenced too much by the idiosyncratic variations in
    price. All of this is similar to how the choice of \(K\) affects K-nearest neighbors
    classification, as discussed in the previous chapter.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们希望上述两种情况都不存在。相反，我们希望有一个模型（1）遵循训练数据中的整体“趋势”，这样模型实际上使用训练数据来学习一些有用的东西，并且（2）不遵循噪声波动，这样我们可以确信我们的模型将很好地转移到/泛化到其他新的数据。如果我们探索\(K\)的其他值，特别是\(K\)
    = 52（如交叉验证所建议的），我们可以看到它实现了这个目标：它遵循房价与房屋大小增加的趋势，但不太受价格中特殊变化的干扰。所有这些都类似于选择\(K\)如何影响前一章中讨论的K-最近邻分类。
- en: 7.8 Evaluating on the test set
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.8 在测试集上评估
- en: To assess how well our model might do at predicting on unseen data, we will
    assess its RMSPE on the test data. To do this, we will first re-train our K-NN
    regression model on the entire training data set, using \(K =\) 52 neighbors.
    Then we will use `predict` to make predictions on the test data, and use the `metrics`
    function again to compute the summary of regression quality. Because we specify
    that we are performing regression in `set_mode`, the `metrics` function knows
    to output a quality summary related to regression, and not, say, classification.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的模型在预测未见数据时的表现，我们将评估其在测试数据上的均方根误差（RMSPE）。为此，我们首先将在整个训练数据集上重新训练我们的K-NN回归模型，使用\(K
    =\) 52个邻居。然后我们将使用`predict`在测试数据上做出预测，并再次使用`metrics`函数来计算回归质量的摘要。因为我们指定在`set_mode`中执行回归，所以`metrics`函数知道要输出与回归相关的质量摘要，而不是，比如说，分类。
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Our final model’s test error as assessed by RMSPE is $90,529. Note that RMSPE
    is measured in the same units as the response variable. In other words, on new
    observations, we expect the error in our prediction to be *roughly* $90,529. From
    one perspective, this is good news: this is about the same as the cross-validation
    RMSPE estimate of our tuned model (which was $84,561), so we can say that the
    model appears to generalize well to new data that it has never seen before. However,
    much like in the case of K-NN classification, whether this value for RMSPE is
    *good*—i.e., whether an error of around $90,529 is acceptable—depends entirely
    on the application. In this application, this error is not prohibitively large,
    but it is not negligible either; $90,529 might represent a substantial fraction
    of a home buyer’s budget, and could make or break whether or not they could afford
    put an offer on a house.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终模型的测试误差，通过RMSPE评估，是$90,529。请注意，RMSPE是用响应变量的相同单位来衡量的。换句话说，在新观测值上，我们预计我们的预测误差大约是$90,529。从一个角度来看，这是一个好消息：这几乎与我们的调整模型的交叉验证RMSPE估计（$84,561）相同，因此我们可以认为该模型似乎很好地泛化到了它以前从未见过的新的数据。然而，正如K-NN分类的情况一样，RMSPE的这个值是否“好”——也就是说，大约$90,529的误差是否可以接受——完全取决于应用。在这个应用中，这个误差不是不可接受的，但也不是可以忽略的；$90,529可能代表一个家庭买家的预算中相当大的部分，并可能决定他们是否能够出价购买房屋。
- en: Finally, Figure [7.8](regression1.html#fig:07-predict-all) shows the predictions
    that our final model makes across the range of house sizes we might encounter
    in the Sacramento area. Note that instead of predicting the house price only for
    those house sizes that happen to appear in our data, we predict it for evenly
    spaced values between the minimum and maximum in the data set (roughly 500 to
    5000 square feet). We superimpose this prediction line on a scatter plot of the
    original housing price data, so that we can qualitatively assess if the model
    seems to fit the data well. You have already seen a few plots like this in this
    chapter, but here we also provide the code that generated it as a learning opportunity.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，图[7.8](regression1.html#fig:07-predict-all)显示了我们的最终模型在萨克拉门托地区可能遇到的房屋面积范围内的预测结果。请注意，我们不仅对那些在我们的数据中偶然出现的房屋面积预测房价，我们还对数据集中最小值和最大值之间的均匀间隔值（大约500到5000平方英尺）进行预测。我们将这个预测线叠加在原始房价数据的散点图上，这样我们就可以定性评估模型是否很好地拟合了数据。您在本章中已经看到了几个类似的图表，但在这里我们也提供了生成它的代码，作为学习的机会。
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Predicted values of house price (blue line) for the final K-NN regression
    model.](../Images/2fef79ce07be79264b6c0f10840e1387.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![最终K-NN回归模型的房价预测值（蓝色线）](../Images/2fef79ce07be79264b6c0f10840e1387.png)'
- en: 'Figure 7.8: Predicted values of house price (blue line) for the final K-NN
    regression model.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：最终K-NN回归模型的房价预测值（蓝色线）。
- en: 7.9 Multivariable K-NN regression
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.9 多变量K-NN回归
- en: As in K-NN classification, we can use multiple predictors in K-NN regression.
    In this setting, we have the same concerns regarding the scale of the predictors.
    Once again, predictions are made by identifying the \(K\) observations that are
    nearest to the new point we want to predict; any variables that are on a large
    scale will have a much larger effect than variables on a small scale. But since
    the `recipe` we built above scales and centers all predictor variables, this is
    handled for us.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与K-NN分类一样，我们可以在K-NN回归中使用多个预测变量。在这种情况下，我们对预测变量的尺度也有同样的担忧。再次强调，预测是通过识别最接近我们想要预测的新点的\(K\)个观测值来进行的；任何处于大尺度的变量将比小尺度的变量有更大的影响。但是，由于我们构建的`recipe`对所有的预测变量进行了缩放和中心化，所以这一点已经为我们处理好了。
- en: 'Note that we also have the same concern regarding the selection of predictors
    in K-NN regression as in K-NN classification: having more predictors is **not**
    always better, and the choice of which predictors to use has a potentially large
    influence on the quality of predictions. Fortunately, we can use the predictor
    selection algorithm from the classification chapter in K-NN regression as well.
    As the algorithm is the same, we will not cover it again in this chapter.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在K-NN回归中对预测变量选择的担忧与K-NN分类中相同：拥有更多的预测变量并不总是更好的，选择使用哪些预测变量可能会对预测质量产生重大影响。幸运的是，我们也可以在K-NN回归中使用分类章节中的预测变量选择算法。由于算法是相同的，我们在此章中不再重复介绍。
- en: We will now demonstrate a multivariable K-NN regression analysis of the Sacramento
    real estate data using `tidymodels`. This time we will use house size (measured
    in square feet) as well as number of bedrooms as our predictors, and continue
    to use house sale price as our response variable that we are trying to predict.
    It is always a good practice to do exploratory data analysis, such as visualizing
    the data, before we start modeling the data. Figure [7.9](regression1.html#fig:07-bedscatter)
    shows that the number of bedrooms might provide useful information to help predict
    the sale price of a house.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用`tidymodels`演示对萨克拉门托房地产数据的多元K-NN回归分析。这次我们将使用房屋面积（以平方英尺计）以及卧室数量作为我们的预测变量，并继续使用房屋销售价格作为我们试图预测的响应变量。在开始建模数据之前进行探索性数据分析，例如可视化数据，总是一个好的做法。图[7.9](regression1.html#fig:07-bedscatter)显示，卧室数量可能提供有用的信息，有助于预测房屋的销售价格。
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Scatter plot of the sale price of houses versus the number of bedrooms.](../Images/d85fcfca5d289159dc94ea38053d7a35.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![房价与卧室数量的散点图](../Images/d85fcfca5d289159dc94ea38053d7a35.png)'
- en: 'Figure 7.9: Scatter plot of the sale price of houses versus the number of bedrooms.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：房价与卧室数量的散点图。
- en: Figure [7.9](regression1.html#fig:07-bedscatter) shows that as the number of
    bedrooms increases, the house sale price tends to increase as well, but that the
    relationship is quite weak. Does adding the number of bedrooms to our model improve
    our ability to predict price? To answer that question, we will have to create
    a new K-NN regression model using house size and number of bedrooms, and then
    we can compare it to the model we previously came up with that only used house
    size. Let’s do that now!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7.9](regression1.html#fig:07-bedscatter)显示，随着卧室数量的增加，房屋的销售价格也倾向于增加，但这种关系相当弱。将卧室数量添加到我们的模型中能否提高我们预测价格的能力？为了回答这个问题，我们必须创建一个新的K-NN回归模型，使用房屋面积和卧室数量，然后我们可以将其与我们之前提出的仅使用房屋面积的那个模型进行比较。现在让我们来做这件事吧！
- en: First we’ll build a new model specification and recipe for the analysis. Note
    that we use the formula `price ~ sqft + beds` to denote that we have two predictors,
    and set `neighbors = tune()` to tell `tidymodels` to tune the number of neighbors
    for us.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将为分析构建一个新的模型规范和配方。请注意，我们使用公式 `price ~ sqft + beds` 来表示我们有两个预测因子，并将 `neighbors
    = tune()` 设置为告诉 `tidymodels` 为我们调整邻居的数量。
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we’ll use 5-fold cross-validation to choose the number of neighbors via
    the minimum RMSPE:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用5折交叉验证来通过最小均方根误差（RMSPE）选择邻居的数量：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here we see that the smallest estimated RMSPE from cross-validation occurs when
    \(K =\) 11. If we want to compare this multivariable K-NN regression model to
    the model with only a single predictor *as part of the model tuning process* (e.g.,
    if we are running forward selection as described in the chapter on evaluating
    and tuning classification models), then we must compare the RMSPE estimated using
    only the training data via cross-validation. Looking back, the estimated cross-validation
    RMSPE for the single-predictor model was $84,561. The estimated cross-validation
    RMSPE for the multivariable model is $81,839. Thus in this case, we did not improve
    the model by a large amount by adding this additional predictor.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到当 \(K =\) 11 时，交叉验证得到的估计RMSPE最小。如果我们想将这个多变量K-NN回归模型与只有单一预测因子作为模型调优过程的一部分进行比较（例如，如果我们正在运行第7章中描述的向前选择），那么我们必须比较仅使用交叉验证通过训练数据估计的RMSPE。回顾一下，单一预测因子模型的交叉验证估计RMSPE为$84,561。多变量模型的交叉验证估计RMSPE为$81,839。因此，在这种情况下，通过添加这个额外的预测因子，我们没有大幅提高模型。
- en: Regardless, let’s continue the analysis to see how we can make predictions with
    a multivariable K-NN regression model and evaluate its performance on test data.
    We first need to re-train the model on the entire training data set with \(K =\)
    11, and then use that model to make predictions on the test data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 不论如何，让我们继续分析，看看我们如何使用多变量K-NN回归模型进行预测，并评估其在测试数据上的性能。我们首先需要使用 \(K =\) 11 在整个训练数据集上重新训练模型，然后使用该模型在测试数据上进行预测。
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This time, when we performed K-NN regression on the same data set, but also
    included number of bedrooms as a predictor, we obtained a RMSPE test error of
    $90,862. Figure [7.10](regression1.html#fig:07-knn-mult-viz) visualizes the model’s
    predictions overlaid on top of the data. This time the predictions are a surface
    in 3D space, instead of a line in 2D space, as we have 2 predictors instead of
    1.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，当我们对相同的数据集执行K-NN回归时，我们还包含了卧室数量作为预测因子，我们得到了$90,862的RMSPE测试误差。图[7.10](regression1.html#fig:07-knn-mult-viz)可视化了模型在数据之上的预测。这次预测是一个3D空间中的表面，而不是2D空间中的线，因为我们有两个预测因子而不是一个。
- en: 'Figure 7.10: K-NN regression model’s predictions represented as a surface in
    3D space overlaid on top of the data using three predictors (price, house size,
    and the number of bedrooms). Note that in general we recommend against using 3D
    visualizations; here we use a 3D visualization only to illustrate what the surface
    of predictions looks like for learning purposes.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10：K-NN回归模型的预测以3D空间中的表面形式表示，并使用三个预测因子（价格、房屋面积和卧室数量）叠加在数据之上。请注意，通常我们不建议使用3D可视化；这里我们仅使用3D可视化来展示预测表面的样子，以供学习之用。
- en: We can see that the predictions in this case, where we have 2 predictors, form
    a surface instead of a line. Because the newly added predictor (number of bedrooms)
    is related to price (as price changes, so does number of bedrooms) and is not
    totally determined by house size (our other predictor), we get additional and
    useful information for making our predictions. For example, in this model we would
    predict that the cost of a house with a size of 2,500 square feet generally increases
    slightly as the number of bedrooms increases. Without having the additional predictor
    of number of bedrooms, we would predict the same price for these two houses.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在这种情况下，我们有两个预测因子，预测形成的是一个表面而不是一条线。因为新添加的预测因子（卧室数量）与价格（价格变化，卧室数量也变化）相关，并且不是完全由房屋大小（我们的另一个预测因子）决定的，所以我们得到了额外的有用信息，以便进行预测。例如，在这个模型中，我们会预测，对于面积为2,500平方英尺的房屋，随着卧室数量的增加，成本通常会略有增加。如果没有额外的卧室数量预测因子，我们会预测这两所房子的价格相同。
- en: 7.10 Strengths and limitations of K-NN regression
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.10 K-NN回归的优缺点
- en: 'As with K-NN classification (or any prediction algorithm for that matter),
    K-NN regression has both strengths and weaknesses. Some are listed here:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 就像K-NN分类（或任何预测算法）一样，K-NN回归既有优点也有缺点。以下是一些列出的：
- en: '**Strengths:** K-nearest neighbors regression'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点：** K近邻回归'
- en: is a simple, intuitive algorithm,
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是一个简单直观的算法，
- en: requires few assumptions about what the data must look like, and
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据必须呈现的样子要求很少，并且
- en: works well with non-linear relationships (i.e., if the relationship is not a
    straight line).
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在非线性关系（即，如果关系不是一条直线）中表现良好。
- en: '**Weaknesses:** K-nearest neighbors regression'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**弱点：** K近邻回归'
- en: becomes very slow as the training data gets larger,
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当训练数据量增大时，会变得非常慢，
- en: may not perform well with a large number of predictors, and
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能在大批预测因子的情况下表现不佳，并且
- en: may not predict well beyond the range of values input in your training data.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能无法很好地预测超出训练数据中输入值范围的情况。
- en: 7.11 Exercises
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.11 练习
- en: 'Practice exercises for the material covered in this chapter can be found in
    the accompanying [worksheets repository](https://worksheets.datasciencebook.ca)
    in the “Regression I: K-nearest neighbors” row. You can launch an interactive
    version of the worksheet in your browser by clicking the “launch binder” button.
    You can also preview a non-interactive version of the worksheet by clicking “view
    worksheet.” If you instead decide to download the worksheet and run it on your
    own machine, make sure to follow the instructions for computer setup found in
    Chapter [13](setup.html#setup). This will ensure that the automated feedback and
    guidance that the worksheets provide will function as intended.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所涵盖的练习材料可以在配套的[工作表存储库](https://worksheets.datasciencebook.ca)中找到，位于“回归I：K近邻”行。您可以通过点击“启动绑定器”按钮在浏览器中启动工作表的交互式版本。您还可以通过点击“查看工作表”预览非交互式版本的工作表。如果您决定下载工作表并在自己的机器上运行它，请确保遵循第[13](setup.html#setup)章中找到的计算机设置说明。这将确保工作表提供的自动反馈和指导按预期工作。
- en: References
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Cover, Thomas, and Peter Hart. 1967\. “Nearest Neighbor Pattern Classification.”
    *IEEE Transactions on Information Theory* 13 (1): 21–27.Fix, Evelyn, and Joseph
    Hodges. 1951\. “Discriminatory Analysis. Nonparametric Discrimination: Consistency
    Properties.” USAF School of Aviation Medicine, Randolph Field, Texas.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cover, Thomas, 和 Peter Hart. 1967. “Nearest Neighbor Pattern Classification.”
    *IEEE Transactions on Information Theory* 13 (1): 21–27. Fix, Evelyn, 和 Joseph
    Hodges. 1951. “Discriminatory Analysis. Nonparametric Discrimination: Consistency
    Properties.” USAF School of Aviation Medicine, Randolph Field, Texas.'
