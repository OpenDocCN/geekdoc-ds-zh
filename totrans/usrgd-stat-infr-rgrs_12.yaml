- en: 7  The statistics of least squares
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7  最小二乘法的统计特性
- en: 原文：[https://mattblackwell.github.io/gov2002-book/ols_properties.html](https://mattblackwell.github.io/gov2002-book/ols_properties.html)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://mattblackwell.github.io/gov2002-book/ols_properties.html](https://mattblackwell.github.io/gov2002-book/ols_properties.html)
- en: '[Regression](./linear_model.html)'
  id: totrans-2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[回归](./linear_model.html)'
- en: '[7  The statistics of least squares](./ols_properties.html)'
  id: totrans-3
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7  最小二乘法的统计特性](./ols_properties.html)'
- en: 'The last chapter showcased the least squares estimator and investigated many
    of its more mechanical properties, which are essential for the practical application
    of OLS. But we still need to understand its statistical properties, as we discussed
    in Part I of this book: unbiasedness, sampling variance, consistency, and asymptotic
    normality. As we saw then, these properties fall into finite-sample (unbiasedness,
    sampling variance) and asymptotic (consistency, asymptotic normality).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章展示了最小二乘估计量及其许多机械性质，这些性质对于OLS的实际应用至关重要。但我们仍需要了解其统计特性，正如本书第一部分所讨论的：无偏性、抽样方差、一致性和渐近正态性。正如我们当时所看到的，这些性质分为有限样本（无偏性和抽样方差）和渐近（一致性、渐近正态性）。
- en: In this chapter, we will focus on the asymptotic properties of OLS because those
    properties hold under the relatively mild conditions of the linear projection
    model introduced in [Section 5.2](linear_model.html#sec-linear-projection). We
    will see that OLS consistently estimates a coherent quantity of interest (the
    best linear predictor) regardless of whether the conditional expectation is linear.
    That is, for the asymptotic properties of the estimator, we will not need the
    commonly invoked linearity assumption. Later, when we investigate the finite-sample
    properties, we will show how linearity will help us establish unbiasedness and
    also how the normality of the errors can allow us to conduct exact, finite-sample
    inference. But these assumptions are very strong, so understanding what we can
    say about OLS without them is vital.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注OLS的渐近性质，因为这些性质在[第5.2节](linear_model.html#sec-linear-projection)中引入的线性投影模型的相对温和条件下成立。我们将看到，OLS始终一致地估计一个感兴趣的量（最佳线性预测器），无论条件期望是否线性。也就是说，对于估计量的渐近性质，我们不需要通常引用的线性假设。稍后，当我们研究有限样本性质时，我们将展示线性假设如何帮助我们建立无偏性，以及误差的正态性如何允许我们进行精确的有限样本推断。但这些假设非常强，因此理解在没有这些假设的情况下我们能说些什么关于OLS的至关重要。
- en: 7.1 Large-sample properties of OLS
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 OLS的大样本性质
- en: 'As we saw in [Chapter 3](asymptotics.html), we need two key ingredients to
    conduct statistical inference with the OLS estimator: (1) a consistent estimate
    of the variance of \(\bhat\) and (2) the approximate distribution of \(\bhat\)
    in large samples. Remember that, since \(\bhat\) is a vector, the variance of
    that estimator will actually be a variance-covariance matrix. To obtain the two
    key ingredients, we first establish the consistency of OLS and then use the central
    limit theorem to derive its asymptotic distribution, which includes its variance.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第3章](asymptotics.html)中看到的，进行OLS估计量的统计推断需要两个关键要素：(1) \(\bhat\) 方差的稳健估计和(2)
    大样本中 \(\bhat\) 的近似分布。记住，由于 \(\bhat\) 是一个向量，该估计量的方差实际上是一个方差-协方差矩阵。为了获得这两个关键要素，我们首先建立OLS的一致性，然后使用中心极限定理推导其渐近分布，其中包括其方差。
- en: We begin by setting out the assumptions needed for establishing the large-sample
    properties of OLS, which are the same as the assumptions needed to ensure that
    the best linear predictor, \(\bfbeta = \E[\X_{i}\X_{i}']^{-1}\E[\X_{i}Y_{i}]\),
    is well-defined and unique.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先阐述建立OLS（最小二乘法）大样本性质所需的假设，这些假设与确保最佳线性预测器 \(\bfbeta = \E[\X_{i}\X_{i}']^{-1}\E[\X_{i}Y_{i}]\)
    定义良好且唯一所需的假设相同。
- en: '*Linear projection assumptions* *The linear projection model makes the following
    assumptions:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性投影假设* 线性投影模型做出以下假设：'
- en: \(\{(Y_{i}, \X_{i})\}_{i=1}^n\) are iid random vectors
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(\{(Y_{i}, \X_{i})\}_{i=1}^n\) 是独立同分布的随机向量
- en: \(\E[Y^{2}_{i}] < \infty\) (finite outcome variance)
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(\E[Y^{2}_{i}] < \infty\) (有限的结果方差)
- en: \(\E[\Vert \X_{i}\Vert^{2}] < \infty\) (finite variances and covariances of
    covariates)
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(\E[\Vert \X_{i}\Vert^{2}] < \infty\) (协变量的有限方差和协方差)
- en: \(\E[\X_{i}\X_{i}']\) is positive definite (no linear dependence in the covariates)*  *Recall
    that these are mild conditions on the joint distribution of \((Y_{i}, \X_{i})\)
    and in particular, we are **not** assuming linearity of the CEF, \(\E[Y_{i} \mid
    \X_{i}]\), nor are we assuming any specific distribution for the data.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: \(\E[\X_{i}\X_{i}']\)是正定的（协变量中没有线性依赖性）*  *回想一下，这些是对\((Y_{i}, \X_{i})\)的联合分布的温和条件，特别是我们**不**假设CEF（\(\E[Y_{i}
    \mid \X_{i}]\)）的线性，也不假设数据具有任何特定的分布。
- en: We can helpfully decompose the OLS estimator into the actual BLP coefficient
    plus estimation error as \[ \bhat = \left( \frac{1}{n} \sum_{i=1}^n \X_i\X_i'
    \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n \X_iY_i \right) = \bfbeta + \underbrace{\left(
    \frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n
    \X_ie_i \right)}_{\text{estimation error}}. \]
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将OLS估计量有益地分解为实际BLP系数加上估计误差，即 \[ \bhat = \left( \frac{1}{n} \sum_{i=1}^n
    \X_i\X_i' \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n \X_iY_i \right) = \bfbeta
    + \underbrace{\left( \frac{1}{n} \sum_{i=1}^n \X_i\X_i' \right)^{-1} \left( \frac{1}{n}
    \sum_{i=1}^n \X_ie_i \right)}_{\text{估计误差}}. \]
- en: This decomposition will help us quickly establish the consistency of \(\bhat\).
    By the law of large numbers, we know that sample means will converge in probability
    to population expectations, so we have \[ \frac{1}{n} \sum_{i=1}^n \X_i\X_i' \inprob
    \E[\X_i\X_i'] \equiv \mb{Q}_{\X\X} \qquad \frac{1}{n} \sum_{i=1}^n \X_ie_i \inprob
    \E[\X_{i} e_{i}] = \mb{0}, \] which implies by the continuous mapping theorem
    (the inverse is a continuous function) that \[ \bhat \inprob \bfbeta + \mb{Q}_{\X\X}^{-1}\E[\X_ie_i]
    = \bfbeta, \] The linear projection assumptions ensure that the LLN applies to
    these sample means and that \(\E[\X_{i}\X_{i}']\) is invertible.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分解将帮助我们快速建立\(\bhat\)的一致性。根据大数定律，我们知道样本均值将以概率收敛到总体期望，因此我们有 \[ \frac{1}{n} \sum_{i=1}^n
    \X_i\X_i' \inprob \E[\X_i\X_i'] \equiv \mb{Q}_{\X\X} \qquad \frac{1}{n} \sum_{i=1}^n
    \X_ie_i \inprob \E[\X_{i} e_{i}] = \mb{0}, \] 这意味着根据连续映射定理（逆函数是连续函数），我们有 \[ \bhat
    \inprob \bfbeta + \mb{Q}_{\X\X}^{-1}\E[\X_ie_i] = \bfbeta, \] 线性投影假设确保LLN适用于这些样本均值，并且\(\E[\X_{i}\X_{i}']\)是可逆的。
- en: '**Theorem 7.1** Under the above linear projection assumptions, the OLS estimator
    is consistent for the best linear projection coefficients, \(\bhat \inprob \bfbeta\).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理7.1** 在上述线性投影假设下，OLS估计量对于最佳线性投影系数\(\bhat \inprob \bfbeta\)是一致的。'
- en: Thus, OLS should be close to the population linear regression in large samples
    under relatively mild conditions. Remember that this may not equal the conditional
    expectation if the CEF is nonlinear. What we can say is that OLS converges to
    the best *linear* approximation to the CEF. Of course, this also means that, if
    the CEF is linear, then OLS will consistently estimate the coefficients of the
    CEF.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在相对温和的条件下，OLS在大样本中应该接近总体线性回归。记住，如果CEF是非线性的，这可能与条件期望不相等。我们可以说的是，OLS收敛到CEF的最佳*线性*近似。当然，这也意味着，如果CEF是线性的，那么OLS将一致地估计CEF的系数。
- en: To emphasize, the only assumptions made about the dependent variable are that
    it (1) has finite variance and (2) is iid. Under this assumption, the outcome
    could be continuous, categorical, binary, or event count.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了强调，对因变量的唯一假设是它（1）具有有限的方差，并且（2）是独立同分布的。在这个假设下，结果可以是连续的、分类的、二元的或事件计数。
- en: Next, we would like to establish an asymptotic normality result for the OLS
    coefficients. We first review some key ideas about the Central Limit Theorem.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们希望为OLS系数建立渐近正态性结果。我们首先回顾一些关于中心极限定理的关键思想。
- en: '*CLT reminder* *Suppose that we have a function of the data iid random vectors
    \(\X_1, \ldots, \X_n\), \(g(\X_{i})\) where \(\E[g(\X_{i})] = 0\) and so \(\V[g(\X_{i})]
    = \E[g(\X_{i})g(\X_{i})'']\). Then if \(\E[\Vert g(\X_{i})\Vert^{2}] < \infty\),
    the CLT implies that \[ \sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} g(\X_{i}) - \E[g(\X_{i})]\right)
    = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} g(\X_{i}) \indist \N(0, \E[g(\X_{i})g(\X_{i}'')])
    \tag{7.1}\]*  *We now manipulate our decomposition to arrive at the *stabilized*
    version of the estimator, \[ \sqrt{n}\left( \bhat - \bfbeta\right) = \left( \frac{1}{n}
    \sum_{i=1}^n \X_i\X_i'' \right)^{-1} \left( \frac{1}{\sqrt{n}} \sum_{i=1}^n \X_ie_i
    \right). \] Recall that we stabilize an estimator to ensure it has a fixed variance
    as the sample size grows, allowing it to have a non-degenerate asymptotic distribution.
    The stabilization works by asymptotically centering it (that is, subtracting the
    value to which it converges) and multiplying by the square root of the sample
    size. We have already established that the first term on the right-hand side will
    converge in probability to \(\mb{Q}_{\X\X}^{-1}\). Notice that \(\E[\X_{i}e_{i}]
    = 0\), so we can apply [Equation 7.1](#eq-clt-mean-zero) to the second term. The
    covariance matrix of \(\X_ie_{i}\) is \[ \mb{\Omega} = \V[\X_{i}e_{i}] = \E[\X_{i}e_{i}(\X_{i}e_{i})'']
    = \E[e_{i}^{2}\X_{i}\X_{i}'']. \] The CLT will imply that \[ \frac{1}{\sqrt{n}}
    \sum_{i=1}^n \X_ie_i \indist \N(0, \mb{\Omega}). \] Combining these facts with
    Slutsky’s Theorem implies the following theorem.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '*CLT 提醒* *假设我们有一个关于独立同分布随机向量 \(\X_1, \ldots, \X_n\) 的函数 \(g(\X_{i})\)，其中 \(\E[g(\X_{i})]
    = 0\)，因此 \(\V[g(\X_{i})] = \E[g(\X_{i})g(\X_{i})'']\). 然后，如果 \(\E[\Vert g(\X_{i})\Vert^{2}]
    < \infty\)，CLT 意味着 \[ \sqrt{n}\left(\frac{1}{n} \sum_{i=1}^{n} g(\X_{i}) - \E[g(\X_{i})]\right)
    = \frac{1}{\sqrt{n}} \sum_{i=1}^{n} g(\X_{i}) \indist \N(0, \E[g(\X_{i})g(\X_{i}'')])
    \tag{7.1}\]* *我们现在对分解进行操作，得到估计量的*稳定*版本，\[ \sqrt{n}\left( \bhat - \bfbeta\right)
    = \left( \frac{1}{n} \sum_{i=1}^n \X_i\X_i'' \right)^{-1} \left( \frac{1}{\sqrt{n}}
    \sum_{i=1}^n \X_ie_i \right). \] 回想一下，我们稳定估计量是为了确保它在样本量增长时具有固定的方差，从而使其具有非退化的渐近分布。稳定是通过渐近地将其中心化（即减去其收敛到的值）并乘以样本大小的平方根来实现的。我们已经证明，右侧的第一个项将以概率收敛到
    \(\mb{Q}_{\X\X}^{-1}\)。注意，\(\E[\X_{i}e_{i}] = 0\)，因此我们可以将 [方程7.1](#eq-clt-mean-zero)
    应用于第二个项。向量 \(\X_ie_{i}\) 的协方差矩阵是 \[ \mb{\Omega} = \V[\X_{i}e_{i}] = \E[\X_{i}e_{i}(\X_{i}e_{i})'']
    = \E[e_{i}^{2}\X_{i}\X_{i}'']. \] CLT 将意味着 \[ \frac{1}{\sqrt{n}} \sum_{i=1}^n
    \X_ie_i \indist \N(0, \mb{\Omega}). \] 结合这些事实与Slutsky定理，可以得出以下定理。'
- en: '**Theorem 7.2** Suppose that the linear projection assumptions hold and, in
    addition, we have \(\E[Y_{i}^{4}] < \infty\) and \(\E[\lVert\X_{i}\rVert^{4}]
    < \infty\). Then the OLS estimator is asymptotically normal with \[ \sqrt{n}\left(
    \bhat - \bfbeta\right) \indist \N(0, \mb{V}_{\bfbeta}), \] where \[ \mb{V}_{\bfbeta}
    = \mb{Q}_{\X\X}^{-1}\mb{\Omega}\mb{Q}_{\X\X}^{-1} = \left( \E[\X_i\X_i''] \right)^{-1}\E[e_i^2\X_i\X_i'']\left(
    \E[\X_i\X_i''] \right)^{-1}. \]'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理7.2** 假设线性投影假设成立，并且此外我们还有 \(\E[Y_{i}^{4}] < \infty\) 和 \(\E[\lVert\X_{i}\rVert^{4}]
    < \infty\)。那么，OLS估计量在渐近上是正态分布的，有 \[ \sqrt{n}\left( \bhat - \bfbeta\right) \indist
    \N(0, \mb{V}_{\bfbeta}), \] 其中 \[ \mb{V}_{\bfbeta} = \mb{Q}_{\X\X}^{-1}\mb{\Omega}\mb{Q}_{\X\X}^{-1}
    = \left( \E[\X_i\X_i''] \right)^{-1}\E[e_i^2\X_i\X_i'']\left( \E[\X_i\X_i''] \right)^{-1}.
    \]'
- en: Thus, with a large enough sample size we can approximate the distribution of
    \(\bhat\) with a multivariate normal distribution with mean \(\bfbeta\) and covariance
    matrix \(\mb{V}_{\bfbeta}/n\). In particular, the square root of the \(j\)th diagonals
    of this matrix will be standard errors for \(\widehat{\beta}_j\). Knowing the
    shape of the OLS estimator’s multivariate distribution will allow us to conduct
    hypothesis tests and generate confidence intervals for both individual coefficients
    and groups of coefficients. But, first, we need an estimate of the covariance
    matrix.**  **## 7.2 Variance estimation for OLS
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在样本量足够大的情况下，我们可以用具有均值 \(\bfbeta\) 和协方差矩阵 \(\mb{V}_{\bfbeta}/n\) 的多元正态分布来近似
    \(\bhat\) 的分布。特别是，这个矩阵第 \(j\) 个对角线的平方根将是 \(\widehat{\beta}_j\) 的标准误差。了解OLS估计量的多元分布的形状将允许我们进行假设检验并为单个系数和系数组生成置信区间。但是，首先我们需要协方差矩阵的估计。**  **##
    7.2 OLS 的方差估计
- en: 'The asymptotic normality of OLS from the last section is of limited value without
    some way to estimate the covariance matrix, \[ \mb{V}_{\bfbeta} = \mb{Q}_{\X\X}^{-1}\mb{\Omega}\mb{Q}_{\X\X}^{-1}.
    \] Since each term here is a population mean, this is an ideal place in which
    to drop a plug-in estimator. For now, we will use the following estimators: \[
    \begin{aligned} \mb{Q}_{\X\X} &= \E[\X_{i}\X_{i}''] & \widehat{\mb{Q}}_{\X\X}
    &= \frac{1}{n} \sum_{i=1}^{n} \X_{i}\X_{i}'' = \frac{1}{n}\Xmat''\Xmat \\ \mb{\Omega}
    &= \E[e_i^2\X_i\X_i''] & \widehat{\mb{\Omega}} & = \frac{1}{n}\sum_{i=1}^n\widehat{e}_i^2\X_i\X_i''.
    \end{aligned} \] Under the assumptions of [Theorem 7.2](#thm-ols-asymptotic-normality),
    the LLN will imply that these are consistent for the quantities we need, \(\widehat{\mb{Q}}_{\X\X}
    \inprob \mb{Q}_{\X\X}\) and \(\widehat{\mb{\Omega}} \inprob \mb{\Omega}\). We
    can plug these into the variance formula to arrive at \[ \begin{aligned} \widehat{\mb{V}}_{\bfbeta}
    &= \widehat{\mb{Q}}_{\X\X}^{-1}\widehat{\mb{\Omega}}\widehat{\mb{Q}}_{\X\X}^{-1}
    \\ &= \left( \frac{1}{n} \Xmat''\Xmat \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n\widehat{e}_i^2\X_i\X_i''
    \right) \left( \frac{1}{n} \Xmat''\Xmat \right)^{-1}, \end{aligned} \] which by
    the continuous mapping theorem is consistent, \(\widehat{\mb{V}}_{\bfbeta} \inprob
    \mb{V}_{\bfbeta}\).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 上节中 OLS 的渐近正态性在没有某种方法来估计协方差矩阵的情况下价值有限，\[ \mb{V}_{\bfbeta} = \mb{Q}_{\X\X}^{-1}\mb{\Omega}\mb{Q}_{\X\X}^{-1}.
    \] 由于这里的每个项都是一个总体均值，这是一个放置插值估计器的理想位置。目前，我们将使用以下估计器：\[ \begin{aligned} \mb{Q}_{\X\X}
    &= \E[\X_{i}\X_{i}'] & \widehat{\mb{Q}}_{\X\X} &= \frac{1}{n} \sum_{i=1}^{n} \X_{i}\X_{i}'
    = \frac{1}{n}\Xmat'\Xmat \\ \mb{\Omega} &= \E[e_i^2\X_i\X_i'] & \widehat{\mb{\Omega}}
    & = \frac{1}{n}\sum_{i=1}^n\widehat{e}_i^2\X_i\X_i'. \end{aligned} \] 在 [定理 7.2](#thm-ols-asymptotic-normality)
    的假设下，大数定律将意味着这些估计量对于我们所需要的量是一致的，\(\widehat{\mb{Q}}_{\X\X} \inprob \mb{Q}_{\X\X}\)
    和 \(\widehat{\mb{\Omega}} \inprob \mb{\Omega}\)。我们可以将这些估计量代入方差公式，得到 \[ \begin{aligned}
    \widehat{\mb{V}}_{\bfbeta} &= \widehat{\mb{Q}}_{\X\X}^{-1}\widehat{\mb{\Omega}}\widehat{\mb{Q}}_{\X\X}^{-1}
    \\ &= \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n\widehat{e}_i^2\X_i\X_i'
    \right) \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1}, \end{aligned} \] 根据连续映射定理，这是一致的，\(\widehat{\mb{V}}_{\bfbeta}
    \inprob \mb{V}_{\bfbeta}\).
- en: 'This estimator is sometimes called the **robust variance estimator** or, more
    accurately, the **heteroskedasticity-consistent (HC) variance estimator**. Why
    is it robust? Consider the standard **homoskedasticity** assumption that most
    statistical software packages make when estimating OLS variances: the variance
    of the errors does not depend on the covariates, or \(\V[e_{i}^{2} \mid \X_{i}]
    = \V[e_{i}^{2}]\). This assumption is stronger than needed, and we can rely on
    a weaker assumption that the squared errors are uncorrelated with a specific function
    of the covariates: \[ \E[e_{i}^{2}\X_{i}\X_{i}''] = \E[e_{i}^{2}]\E[\X_{i}\X_{i}'']
    = \sigma^{2}\mb{Q}_{\X\X}, \] where \(\sigma^2\) is the variance of the residuals
    (since \(\E[e_{i}] = 0\)). Homoskedasticity simplifies the asymptotic variance
    of the stabilized estimator, \(\sqrt{n}(\bhat - \bfbeta)\), to \[ \mb{V}^{\texttt{lm}}_{\bfbeta}
    = \mb{Q}_{\X\X}^{-1}\sigma^{2}\mb{Q}_{\X\X}\mb{Q}_{\X\X}^{-1} = \sigma^2\mb{Q}_{\X\X}^{-1}.
    \] We already have an estimator for \(\mb{Q}_{\X\X}\), but we need one for \(\sigma^2\).
    We can easily use the SSR, \[ \widehat{\sigma}^{2} = \frac{1}{n-k-1} \sum_{i=1}^{n}
    \widehat{e}_{i}^{2}, \] where we use \(n-k-1\) in the denominator instead of \(n\)
    to correct for the residuals being slightly less variable than the actual errors
    (because OLS mechanically attempts to make the residuals small). For consistent
    variance estimation, \(n-k -1\) or \(n\) can be used, since either way \(\widehat{\sigma}^2
    \inprob \sigma^2\). Thus, under homoskedasticity, we have \[ \widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}}
    = \widehat{\sigma}^{2}\left(\frac{1}{n}\Xmat''\Xmat\right)^{{-1}} = n\widehat{\sigma}^{2}\left(\Xmat''\Xmat\right)^{{-1}},
    \] This is the standard variance estimator used by `lm()` in R and `reg` in Stata.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这个估计量有时被称为**稳健方差估计量**，或者更准确地说，是**异方差一致性（HC）方差估计量**。为什么它稳健？考虑大多数统计软件包在估计 OLS
    方差时所做的标准**同方差性**假设：误差的方差不依赖于协变量，或者说 \(\V[e_{i}^{2} \mid \X_{i}] = \V[e_{i}^{2}]\)。这个假设比所需的更强，我们可以依赖于一个更弱的假设，即平方误差与协变量的特定函数不相关：\[
    \E[e_{i}^{2}\X_{i}\X_{i}'] = \E[e_{i}^{2}]\E[\X_{i}\X_{i}'] = \sigma^{2}\mb{Q}_{\X\X},
    \] 其中 \(\sigma^2\) 是残差的方差（因为 \(\E[e_{i}] = 0\))。同方差性简化了稳定估计量 \(\sqrt{n}(\bhat
    - \bfbeta)\) 的渐近方差为 \[ \mb{V}^{\texttt{lm}}_{\bfbeta} = \mb{Q}_{\X\X}^{-1}\sigma^{2}\mb{Q}_{\X\X}\mb{Q}_{\X\X}^{-1}
    = \sigma^2\mb{Q}_{\X\X}^{-1}. \] 我们已经有了 \(\mb{Q}_{\X\X}\) 的估计量，但我们还需要一个 \(\sigma^2\)
    的估计量。我们可以很容易地使用 SSR，\[ \widehat{\sigma}^{2} = \frac{1}{n-k-1} \sum_{i=1}^{n} \widehat{e}_{i}^{2},
    \] 其中我们在分母中使用 \(n-k-1\) 而不是 \(n\) 来纠正残差略小于实际误差的变量性（因为 OLS 机械地试图使残差很小）。对于一致的方差估计，\(n-k
    -1\) 或 \(n\) 都可以使用，因为两种方式下 \(\widehat{\sigma}^2 \inprob \sigma^2\)。因此，在同方差性下，我们有
    \[ \widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}} = \widehat{\sigma}^{2}\left(\frac{1}{n}\Xmat'\Xmat\right)^{{-1}}
    = n\widehat{\sigma}^{2}\left(\Xmat'\Xmat\right)^{{-1}}, \] 这就是 R 中的 `lm()` 和 Stata
    中的 `reg` 所使用的标准方差估计量。
- en: How do these two estimators, \(\widehat{\mb{V}}_{\bfbeta}\) and \(\widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}}\),
    compare? Notice that the HC variance estimator and the homoskedasticity variance
    estimator will both be consistent when homoskedasticity holds. But as the “heteroskedasticity-consistent”
    label implies, only the HC variance estimator will be consistent when homoskedasticity
    fails to hold. So \(\widehat{\mb{V}}_{\bfbeta}\) has the advantage of being consistent
    regardless of the homoskedasticity assumption. This advantage comes at a cost,
    however. When homoskedasticity is correct, \(\widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}}\)
    incorporates that assumption into the estimator whereas the HC variance estimator
    has to estimate it. The HC estimator will therefore have higher variance (the
    variance estimator will be more variable!) when homoskedasticity actually does
    hold.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个估计量，\(\widehat{\mb{V}}_{\bfbeta}\) 和 \(\widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}}\)，如何比较？请注意，当同方差性成立时，HC
    方差估计量和同方差性方差估计量都将是一致的。但是，正如“异方差一致性”标签所暗示的，当同方差性不成立时，只有 HC 方差估计量将是一致的。因此，\(\widehat{\mb{V}}_{\bfbeta}\)
    具有无论同方差性假设是否成立都具有一致性的优势。然而，这种优势是有代价的。当同方差性正确时，\(\widehat{\mb{V}}_{\bfbeta}^{\texttt{lm}}\)
    将该假设纳入估计量中，而 HC 方差估计量则必须对其进行估计。因此，当实际上确实存在同方差性时，HC 估计量将具有更高的方差（方差估计量将更加多变）。
- en: Now that we have established the asymptotic normality of the OLS estimator and
    developed a consistent estimator of its variance, we can proceed with all of the
    statistical inference tools we discussed in Part I, including hypothesis tests
    and confidence intervals.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经建立了 OLS 估计量的渐近正态性和其方差的一致估计量，我们就可以继续使用我们在第一部分讨论的所有统计推断工具，包括假设检验和置信区间。
- en: We begin by defining the estimated **heteroskedasticity-consistent standard
    errors** as \[ \widehat{\se}(\widehat{\beta}_{j}) = \sqrt{\frac{[\widehat{\mb{V}}_{\bfbeta}]_{jj}}{n}},
    \] where \([\widehat{\mb{V}}_{\bfbeta}]_{jj}\) is the \(j\)th diagonal entry of
    the HC variance estimator. Note that we divide by \(\sqrt{n}\) here because \(\widehat{\mb{V}}_{\bfbeta}\)
    is a consistent estimator of the stabilized estimator \(\sqrt{n}(\bhat - \bfbeta)\)
    not the estimator itself.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义估计的**异方差一致标准误差**为 \[ \widehat{\se}(\widehat{\beta}_{j}) = \sqrt{\frac{[\widehat{\mb{V}}_{\bfbeta}]_{jj}}{n}},
    \] 其中 \([\widehat{\mb{V}}_{\bfbeta}]_{jj}\) 是 HC 方差估计器的第 \(j\) 个对角线元素。请注意，我们在这里除以
    \(\sqrt{n}\)，因为 \(\widehat{\mb{V}}_{\bfbeta}\) 是稳定估计量 \(\sqrt{n}(\bhat - \bfbeta)\)
    的一致估计量，而不是估计量本身。
- en: 'Hypothesis tests and confidence intervals for individual coefficients are almost
    precisely the same as with the most general case presented in Part I. For a two-sided
    test of \(H_0: \beta_j = b\) versus \(H_1: \beta_j \neq b\), we can build the
    t-statistic and conclude that, under the null, \[ \frac{\widehat{\beta}_j - b}{\widehat{\se}(\widehat{\beta}_{j})}
    \indist \N(0, 1). \] Statistical software will typically and helpfully provide
    the t-statistic for the null hypothesis of no (partial) linear relationship between
    \(X_{ij}\) and \(Y_i\), \[ t = \frac{\widehat{\beta}_{j}}{\widehat{\se}(\widehat{\beta}_{j})},
    \] which measures how large the estimated coefficient is in standard errors. With
    \(\alpha = 0.05\), asymptotic normality would imply that we reject this null when
    \(t > 1.96\). We can form asymptotically-valid confidence intervals with \[ \left[\widehat{\beta}_{j}
    - z_{\alpha/2}\;\widehat{\se}(\widehat{\beta}_{j}),\;\widehat{\beta}_{j} + z_{\alpha/2}\;\widehat{\se}(\widehat{\beta}_{j})\right].
    \] For reasons we will discuss below, standard software typically relies on the
    \(t\) distribution instead of the normal for hypothesis testing and confidence
    intervals. Still, this difference is of little consequence in large samples.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '对于单个系数的假设检验和置信区间几乎与第一部分中提出的最一般情况完全相同。对于对 \(H_0: \beta_j = b\) 与 \(H_1: \beta_j
    \neq b\) 的双边检验，我们可以构建 t 统计量并得出结论，在零假设下，\[ \frac{\widehat{\beta}_j - b}{\widehat{\se}(\widehat{\beta}_{j})}
    \indist \N(0, 1). \] 统计软件通常会提供对零假设（\(X_{ij}\) 与 \(Y_i\) 之间没有（部分）线性关系的零假设）的 t 统计量，\[
    t = \frac{\widehat{\beta}_{j}}{\widehat{\se}(\widehat{\beta}_{j})}, \] 这衡量了估计系数在标准误差中的大小。当
    \(\alpha = 0.05\) 时，渐近正态性意味着当 \(t > 1.96\) 时，我们将拒绝这个零假设。我们可以使用以下公式形成渐近有效的置信区间
    \[ \left[\widehat{\beta}_{j} - z_{\alpha/2}\;\widehat{\se}(\widehat{\beta}_{j}),\;\widehat{\beta}_{j}
    + z_{\alpha/2}\;\widehat{\se}(\widehat{\beta}_{j})\right]. \] 由于以下我们将讨论的原因，标准软件通常依赖于
    t 分布而不是正态分布来进行假设检验和置信区间。然而，在大样本中，这种差异影响甚微。'
- en: 7.3 Inference for multiple parameters
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 多参数的推断
- en: 'With multiple coefficients, we might have hypotheses that involve more than
    one coefficient. As an example, consider a regression with an interaction between
    two covariates, \[ Y_i = \beta_0 + X_i\beta_1 + Z_i\beta_2 + X_iZ_i\beta_3 + e_i.
    \] Suppose we wanted to test the hypothesis that \(X_i\) does not affect the best
    linear predictor for \(Y_i\). That would be \[ H_{0}: \beta_{1} = 0 \text{ and
    } \beta_{3} = 0\quad\text{vs}\quad H_{1}: \beta_{1} \neq 0 \text{ or } \beta_{3}
    \neq 0, \] where we usually write the null more compactly as \(H_0: \beta_1 =
    \beta_3 = 0\).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '在多个系数的情况下，我们可能会有涉及多个系数的假设。例如，考虑一个包含两个协变量交互作用的回归，\[ Y_i = \beta_0 + X_i\beta_1
    + Z_i\beta_2 + X_iZ_i\beta_3 + e_i. \] 假设我们想要检验 \(X_i\) 不影响 \(Y_i\) 的最佳线性预测器的假设。这将意味着
    \[ H_{0}: \beta_{1} = 0 \text{ and } \beta_{3} = 0\quad\text{vs}\quad H_{1}: \beta_{1}
    \neq 0 \text{ or } \beta_{3} \neq 0, \] 其中我们通常将零假设更紧凑地写成 \(H_0: \beta_1 = \beta_3
    = 0\)。'
- en: 'To test this null hypothesis, we need a test statistic that discriminates between
    the two hypotheses: it should be large when the alternative is true and small
    enough when the null is true. With a single coefficient, we usually test the null
    hypothesis of \(H_0: \beta_j = b_0\) with the \(t\)-statistic, \[ t = \frac{\widehat{\beta}_{j}
    - b_{0}}{\widehat{\se}(\widehat{\beta}_{j})}, \] and we usually take the absolute
    value, \(|t|\), as our measure of how extreme our estimate is given the null distribution.
    But notice that we could also use the square of the \(t\) statistic, which is
    \[ t^{2} = \frac{\left(\widehat{\beta}_{j} - b_{0}\right)^{2}}{\V[\widehat{\beta}_{j}]}
    = \frac{n\left(\widehat{\beta}_{j} - b_{0}\right)^{2}}{[\mb{V}_{\bfbeta}]_{[jj]}}.
    \tag{7.2}\]'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '为了检验这个零假设，我们需要一个检验统计量，它能在两个假设之间进行区分：当备择假设为真时，它应该很大；当零假设为真时，它应该足够小。对于单个系数，我们通常使用
    \(t\) 统计量来检验 \(H_0: \beta_j = b_0\) 的零假设，\[ t = \frac{\widehat{\beta}_{j} - b_{0}}{\widehat{\se}(\widehat{\beta}_{j})},
    \] 我们通常取绝对值 \(|t|\) 作为我们估计的极端程度的度量，给定零分布。但请注意，我们也可以使用 \(t\) 统计量的平方，它是 \[ t^{2}
    = \frac{\left(\widehat{\beta}_{j} - b_{0}\right)^{2}}{\V[\widehat{\beta}_{j}]}
    = \frac{n\left(\widehat{\beta}_{j} - b_{0}\right)^{2}}{[\mb{V}_{\bfbeta}]_{[jj]}}.
    \tag{7.2}\]'
- en: 'While \(|t|\) is the usual test statistic we use for two-sided tests, we could
    equivalently use \(t^2\) and arrive at the exact same conclusions (as long as
    we knew the distribution of \(t^2\) under the null hypothesis). It turns out that
    the \(t^2\) version of the test statistic will generalize more easily to comparing
    multiple coefficients. This version of the test statistic suggests another general
    way to differentiate the null from the alternative: by taking the squared distance
    between them and dividing by the variance of the estimate.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当 \(|t|\) 是我们用于双尾检验的常用检验统计量时，我们也可以等价地使用 \(t^2\) 并得出完全相同的结论（只要我们知道在零假设下 \(t^2\)
    的分布）。结果发现，检验统计量的 \(t^2\) 版本更容易推广到比较多个系数。这种检验统计量的版本提出了区分零假设和备择假设的另一种一般方法：通过计算它们之间的平方距离并除以估计量的方差。
- en: Can we generalize this idea to hypotheses about multiple parameters? Adding
    the sum of squared distances for each component of the null hypothesis is straightforward.
    For our interaction example, that would be \[ \widehat{\beta}_1^2 + \widehat{\beta}_3^2,
    \] Remember, however, that some of the estimated coefficients are noisier than
    others, so we should account for the uncertainty just like we did for the \(t\)-statistic.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否将这个想法推广到关于多个参数的假设？将零假设每个成分的平方距离之和加起来是直接的。对于我们的交互作用示例，这将是对 \(\widehat{\beta}_1^2
    + \widehat{\beta}_3^2\)，记住，然而，一些估计系数比其他系数更嘈杂，因此我们应该像对 \(t\) 统计量那样考虑不确定性。
- en: 'With multiple parameters and multiple coefficients, the variances will now
    require matrix algebra. We can write any hypothesis about linear functions of
    the coefficients as \(H_{0}: \mb{L}\bfbeta = \mb{c}\). For example, in the interaction
    case, we have \[ \mb{L} = \begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ \end{pmatrix}
    \qquad \mb{c} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \] Thus, \(\mb{L}\bfbeta
    = \mb{0}\) is equivalent to \(\beta_1 = 0\) and \(\beta_3 = 0\). Notice that with
    other \(\mb{L}\) matrices, we could represent more complicated hypotheses like
    \(2\beta_1 - \beta_2 = 34\), though we mostly stick to simpler functions. Let
    \(\widehat{\bs{\theta}} = \mb{L}\bhat\) be the OLS estimate of the function of
    the coefficients. By the delta method (discussed in [Section 3.9](asymptotics.html#sec-delta-method)),
    we have \[ \sqrt{n}\left(\mb{L}\bhat - \mb{L}\bfbeta\right) \indist \N(0, \mb{L}\mb{V}_{\bfbeta}\mb{L}'').
    \] We can now generalize the squared \(t\) statistic in [Equation 7.2](#eq-squared-t)
    by taking the distances \(\mb{L}\bhat - \mb{c}\) weighted by the variance-covariance
    matrix \(\mb{L}\mb{V}_{\bfbeta}\mb{L}''\), \[ W = n(\mb{L}\bhat - \mb{c})''(\mb{L}\mb{V}_{\bfbeta}\mb{L}'')^{-1}(\mb{L}\bhat
    - \mb{c}), \] which is called the **Wald test statistic**. This statistic generalizes
    the ideas of the t-statistic to multiple parameters. With the t-statistic, we
    recenter to have mean 0 and divide by the standard error to get a variance of
    1\. If we ignore the middle variance weighting, we have \((\mb{L}\bhat - \mb{c})''(\mb{L}\bhat
    - \mb{c})\) which is just the sum of the squared deviations of the estimates from
    the null. Including the \((\mb{L}\mb{V}_{\bfbeta}\mb{L}'')^{-1}\) weight has the
    effect of rescaling the distribution of \(\mb{L}\bhat - \mb{c}\) to make it rotationally
    symmetric around 0 (so the resulting dimensions are uncorrelated) with each dimension
    having an equal variance of 1\. In this way, the Wald statistic transforms the
    random vectors to be mean-centered and have variance 1 (just the t-statistic),
    but also to have the resulting random variables in the vector be uncorrelated.[¹](#fn1)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '在多个参数和多个系数的情况下，方差现在需要矩阵代数。我们可以将关于系数的线性函数的任何假设写成 \(H_{0}: \mb{L}\bfbeta = \mb{c}\)。例如，在交互作用的情况下，我们有
    \[ \mb{L} = \begin{pmatrix} 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ \end{pmatrix} \qquad
    \mb{c} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \] 因此，\(\mb{L}\bfbeta = \mb{0}\)
    等价于 \(\beta_1 = 0\) 和 \(\beta_3 = 0\)。请注意，使用其他 \(\mb{L}\) 矩阵，我们可以表示更复杂的假设，如 \(2\beta_1
    - \beta_2 = 34\)，尽管我们主要坚持更简单的函数。令 \(\widehat{\bs{\theta}} = \mb{L}\bhat\) 为系数函数的
    OLS 估计。通过 delta 方法（在第 3.9 节中讨论），我们有 \[ \sqrt{n}\left(\mb{L}\bhat - \mb{L}\bfbeta\right)
    \indist \N(0, \mb{L}\mb{V}_{\bfbeta}\mb{L}''). \] 我们现在可以通过取 \(\mb{L}\bhat - \mb{c}\)
    加权距离，权重为方差协方差矩阵 \(\mb{L}\mb{V}_{\bfbeta}\mb{L}''\) 来推广 [方程 7.2](#eq-squared-t)
    中的平方 \(t\) 统计量， \[ W = n(\mb{L}\bhat - \mb{c})''(\mb{L}\mb{V}_{\bfbeta}\mb{L}'')^{-1}(\mb{L}\bhat
    - \mb{c}), \] 这被称为 **Wald 统计量**。这个统计量将 t 统计量的思想推广到多个参数。使用 t 统计量，我们重新定位以使均值为 0，并通过标准误差进行除法以获得方差为
    1。如果我们忽略中间的方差加权，我们就有 \((\mb{L}\bhat - \mb{c})''(\mb{L}\bhat - \mb{c})\)，这仅仅是估计值与零假设的平方偏差之和。包括
    \((\mb{L}\mb{V}_{\bfbeta}\mb{L}'')^{-1}\) 权重的作用是将 \(\mb{L}\bhat - \mb{c}\) 的分布重新缩放，使其围绕
    0 旋转对称（因此结果维度是不相关的），每个维度具有相等的方差为 1。通过这种方式，Wald 统计量将随机向量转换为均值中心化和方差为 1（即 t 统计量），同时也使向量中的结果随机变量不相关。[¹](#fn1)'
- en: Why transform the data in this way? [Figure 7.1](#fig-wald) shows the contour
    plot of a hypothetical joint distribution of two coefficients from an OLS regression.
    We might want to know the distance between different points in the distribution
    and the mean, which in this case is \((1, 2)\). Without considering the joint
    distribution, the circle is obviously closer to the mean than the triangle. However,
    looking at the two points on the distribution, the circle is at a lower contour
    than the triangle, meaning it is more extreme than the triangle for this particular
    distribution. The Wald statistic, then, takes into consideration how much of a
    “climb” it is for \(\mb{L}\bhat\) to get to \(\mb{c}\) given the distribution
    of \(\mb{L}\bhat\).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要以这种方式转换数据？[图 7.1](#fig-wald) 显示了 OLS 回归中两个系数的假设联合分布的等高线图。我们可能想知道分布中不同点与均值之间的距离，在这种情况下是
    \((1, 2)\)。如果不考虑联合分布，圆显然比三角形更接近均值。然而，观察分布上的两个点，圆在比三角形更低的等高线上，这意味着对于这个特定的分布，圆比三角形更极端。因此，Wald
    统计量考虑了 \(\mb{L}\bhat\) 在给定 \(\mb{L}\bhat\) 分布的情况下到达 \(\mb{c}\) 的“爬升”程度。
- en: '![](../Images/1786b9fe10a772e336cbed68c1df9692.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/1786b9fe10a772e336cbed68c1df9692.png)'
- en: 'Figure 7.1: Hypothetical joint distribution of two slope coefficients. The
    circle is closer to the center of the distribution by the standard Euclidean distance,
    but the triangle is closer once you consider the joint distribution.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1：两个斜率系数的假设联合分布。圆圈通过标准欧几里得距离更接近分布中心，但一旦考虑联合分布，三角形则更接近。
- en: If \(\mb{L}\) only has one row, our Wald statistic is the same as the squared
    \(t\) statistic, \(W = t^2\). This fact will help us think about the asymptotic
    distribution of \(W\). Note that as \(n\to\infty\), we know that by the asymptotic
    normality of \(\bhat\), \[ t = \frac{\widehat{\beta}_{j} - \beta_{j}}{\widehat{\se}[\widehat{\beta}_{j}]}
    \indist \N(0,1) \] so \(t^2\) will converge in distribution to a \(\chi^2_1\)
    (since a \(\chi^2_1\) distribution is just one standard normal distribution squared).
    After recentering and rescaling by the covariance matrix, \(W\) converges to the
    sum of \(q\) squared independent normals, where \(q\) is the number of rows of
    \(\mb{L}\), or equivalently, the number of restrictions implied by the null hypothesis.
    Thus, under the null hypothesis of \(\mb{L}\bhat = \mb{c}\), we have \(W \indist
    \chi^2_{q}\).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 \(\mb{L}\) 只有一行，我们的 Wald 统计量与平方 \(t\) 统计量相同，\(W = t^2\)。这一事实将帮助我们思考 \(W\)
    的渐近分布。注意，当 \(n\to\infty\) 时，我们知道由于 \(\bhat\) 的渐近正态性，\[ t = \frac{\widehat{\beta}_{j}
    - \beta_{j}}{\widehat{\se}[\widehat{\beta}_{j}]} \indist \N(0,1) \] 因此 \(t^2\)
    将在分布上收敛到 \(\chi^2_1\)（因为 \(\chi^2_1\) 分布只是一个标准正态分布的平方）。在通过协方差矩阵重新中心和缩放后，\(W\)
    收敛到 \(q\) 个独立正态变量的平方和，其中 \(q\) 是 \(\mb{L}\) 的行数，或者等价地，零假设所隐含的限制数量。因此，在 \(\mb{L}\bhat
    = \mb{c}\) 的零假设下，我们有 \(W \indist \chi^2_{q}\)。
- en: We need to define the rejection region to use the Wald statistic in a hypothesis
    test. Because we are squaring each distance in \(W \geq 0\), larger values of
    \(W\) indicate more disagreement with the null in either direction. Thus, for
    an \(\alpha\)-level test of the joint null, we only need a one-sided rejection
    region of the form \(\P(W > w_{\alpha}) = \alpha\). Obtaining these values is
    straightforward (see the above callout tip). For \(q = 2\) and a \(\alpha = 0.05\),
    the critical value is roughly 6.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要定义拒绝域，以便在假设检验中使用 Wald 统计量。因为我们正在对 \(W \geq 0\) 中的每个距离进行平方，所以 \(W\) 的较大值表示与零假设在任一方向上的更大不一致。因此，对于
    \(\alpha\) 水平的联合零假设检验，我们只需要一个单侧拒绝域，形式为 \(\P(W > w_{\alpha}) = \alpha\)。获取这些值是直接的（参见上面的提示）。对于
    \(q = 2\) 和 \(\alpha = 0.05\)，临界值大约为 6。
- en: '*Chi-squared critical values* *We can obtain critical values for the \(\chi^2_q\)
    distribution using the `qchisq()` function in R. For example, if we wanted to
    obtain the critical value \(w\) such that \(\P(W > w_{\alpha}) = \alpha\) for
    our two-parameter interaction example, we could use:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*卡方临界值* 我们可以使用 R 中的 `qchisq()` 函数来获取 \(\chi^2_q\) 分布的临界值。例如，如果我们想获取临界值 \(w\)，使得
    \(\P(W > w_{\alpha}) = \alpha\)，对于我们的双参数交互示例，我们可以使用：'
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '*[PRE1]**  **The Wald statistic is not a common test provided by standard statistical
    software functions like `lm()` in R, though it is fairly straightforward to implement
    “by hand.” Alternatively, packages like [`{aod}`](https://cran.r-project.org/web/packages/aod/index.html)
    or [`{clubSandwich}`](http://jepusto.github.io/clubSandwich/) have implementations
    of the test. What is reported by most software implementations of OLS (like `lm()`
    in R) is the F-statistic, which is \[ F = \frac{W}{q}. \] This also typically
    uses the homoskedastic variance estimator \(\mb{V}^{\texttt{lm}}_{\bfbeta}\) in
    \(W\). The p-values reported for such tests use the \(F_{q,n-k-1}\) distribution
    because this is the exact distribution of the \(F\) statistic when the errors
    are (a) homoskedastic and (b) normally distributed. When these assumptions do
    not hold, the \(F\) distribution has no justification in statistical theory, but
    it is slightly more conservative than the \(\chi^2_q\) distribution, and the inferences
    from the \(F\) statistic will converge to those from the \(\chi^2_q\) distribution
    as \(n\to\infty\). So it might be justified as an *ad hoc* small-sample adjustment
    to the Wald test. For example, if we used the \(F_{q,n-k-1}\) with the interaction
    example where \(q=2\) and we have, say, a sample size of \(n = 100\), then in
    that case, the critical value for the F test with \(\alpha = 0.05\) is'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE1]**  **Wald统计量不是标准统计软件函数（如R中的`lm()`）提供的常见检验，尽管“手动”实现相当直接。或者，像`{aod}`（https://cran.r-project.org/web/packages/aod/index.html）或`{clubSandwich}`（http://jepusto.github.io/clubSandwich/）这样的包提供了该检验的实现。大多数OLS软件实现（如R中的`lm()`）报告的是F统计量，其公式为\[
    F = \frac{W}{q}. \] 这通常在W中使用同方差方差估计器\(\mb{V}^{\texttt{lm}}_{\bfbeta}\)。此类检验报告的p值使用\(F_{q,n-k-1}\)分布，因为这是当误差（a）同方差和（b）正态分布时\(F\)统计量的确切分布。当这些假设不成立时，在统计理论中，\(F\)分布没有依据，但它比\(\chi^2_q\)分布略为保守，并且随着\(n\to\infty\)，从\(F\)统计量得出的推断将收敛到从\(\chi^2_q\)分布得出的推断。因此，它可以作为一个*临时*的小样本调整Wald检验。例如，如果我们使用\(F_{q,n-k-1}\)与交互作用示例，其中\(q=2\)，并且样本大小为\(n
    = 100\)，那么在这种情况下，F检验的临界值（\(\alpha = 0.05\)）为'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*[PRE3]*  *This result implies a critical value of 6.182 on the scale of the
    Wald statistic (multiplying it by \(q = 2\)). Compared to the earlier critical
    value of 5.991 based on the \(\chi^2_2\) distribution, we can see that the inferences
    will be very similar even in moderately-sized datasets.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE3]*  *此结果意味着Wald统计量尺度上的临界值为6.182（乘以\(q = 2\)）。与基于\(\chi^2_2\)分布的早期临界值5.991相比，我们可以看到，即使在中等大小的数据集中，推断也将非常相似。'
- en: Finally, note that the F-statistic reported by `lm()` in R is the test of all
    the coefficients being equal to 0 jointly except for the intercept. In modern
    quantitative social sciences, this test is seldom substantively interesting.***  ***##
    7.4 Finite-sample properties with a linear CEF
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，R中的`lm()`函数报告的F统计量是对所有系数（除了截距）同时等于0的检验。在现代定量社会科学中，这种检验很少具有实质性意义。***  ***##
    7.4 线性CEF的有限样本性质
- en: All the above results have been large-sample properties, and we have not addressed
    finite-sample properties like the sampling variance or unbiasedness. Under the
    linear projection assumption above, OLS is generally biased without stronger assumptions.
    This section introduces the stronger assumption that will allow us to establish
    stronger properties for OLS. As usual, however, remember that these stronger assumptions
    can be wrong.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述结果都是大样本性质，我们尚未解决有限样本性质，如抽样方差或无偏性。在上述线性投影假设下，OLS通常在没有更强假设的情况下是有偏的。本节介绍了更强的假设，这将使我们能够为OLS建立更强的性质。然而，通常记住，这些更强的假设可能是错误的。
- en: '*Assumption: Linear Regression Model* *1.  The variables \((Y_{i}, \X_{i})\)
    satisfy the linear CEF assumption. \[ \begin{aligned} Y_{i} &= \X_{i}''\bfbeta
    + e_{i} \\ \E[e_{i}\mid \X_{i}] & = 0. \end{aligned} \]'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设：线性回归模型* *1.  变量\((Y_{i}, \X_{i})\)满足线性CEF假设。 \[ \begin{aligned} Y_{i} &=
    \X_{i}''\bfbeta + e_{i} \\ \E[e_{i}\mid \X_{i}] & = 0. \end{aligned} \]'
- en: 'The design matrix is invertible \(\E[\X_{i}\X_{i}''] > 0\) (positive definite).*  *We
    discussed the concept of a linear CEF extensively in [Chapter 5](linear_model.html).
    However, recall that the CEF might be linear mechanically if the model is **saturated**
    or when there are as many coefficients in the model as there are unique values
    of \(\X_i\). When a model is not saturated, the linear CEF assumption is just
    that: an assumption. What can this assumption do? It can aid in establishing some
    nice statistical properties in finite samples.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设计矩阵是可逆的 \(\E[\X_{i}\X_{i}'] > 0\)（正定）。*  *我们在第 5 章[线性模型](linear_model.html)中广泛讨论了线性
    CEF 的概念。然而，回想一下，如果模型是 **饱和的** 或者当模型中的系数与 \(\X_i\) 的唯一值一样多时，CEF 可能是线性的。当模型不是饱和的，线性
    CEF 假设仅仅是：一个假设。这个假设能做什么？它可以帮助在有限样本中建立一些良好的统计性质。
- en: Before proceeding, note that, when focusing on the finite sample inference for
    OLS, we focused on its properties **conditional on the observed covariates**,
    such as \(\E[\bhat \mid \Xmat]\) or \(\V[\bhat \mid \Xmat]\). The historical reason
    for this is that the researcher often chose these independent variables and so
    they were not random. Thus, sometimes \(\Xmat\) is treated as “fixed” in some
    older texts, which might even omit explicit conditioning statements.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，请注意，当我们关注 OLS 的有限样本推断时，我们关注的是其基于观察协变量的性质 **条件于观察到的协变量**，例如 \(\E[\bhat
    \mid \Xmat]\) 或 \(\V[\bhat \mid \Xmat]\)。这种历史原因在于研究者通常选择这些自变量，因此它们不是随机的。因此，有时在某些较老的文章中，\(\Xmat\)
    被视为“固定”的，甚至可能省略显式的条件语句。
- en: '**Theorem 7.3** Under the linear regression model assumption, OLS is unbiased
    for the population regression coefficients, \[ \E[\bhat \mid \Xmat] = \bfbeta,
    \] and its conditional sampling variance is \[ \mb{\V}_{\bhat} = \V[\bhat \mid
    \Xmat] = \left( \Xmat''\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2_i \X_i\X_i''
    \right) \left( \Xmat''\Xmat \right)^{-1}, \] where \(\sigma^2_{i} = \E[e_{i}^{2}
    \mid \Xmat]\).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 7.3** 在线性回归模型假设下，OLS 对总体回归系数是无偏的，\[ \E[\bhat \mid \Xmat] = \bfbeta, \]
    其条件抽样方差为 \[ \mb{\V}_{\bhat} = \V[\bhat \mid \Xmat] = \left( \Xmat''\Xmat \right)^{-1}\left(
    \sum_{i=1}^n \sigma^2_i \X_i\X_i'' \right) \left( \Xmat''\Xmat \right)^{-1}, \]
    其中 \(\sigma^2_{i} = \E[e_{i}^{2} \mid \Xmat]\)。'
- en: '*Proof*. To prove the conditional unbiasedness, recall that we can write the
    OLS estimator as \[ \bhat = \bfbeta + (\Xmat''\Xmat)^{-1}\Xmat''\mb{e}, \] and
    so taking (conditional) expectations, we have \[ \E[\bhat \mid \Xmat] = \bfbeta
    + \E[(\Xmat''\Xmat)^{-1}\Xmat''\mb{e} \mid \Xmat] = \bfbeta + (\Xmat''\Xmat)^{-1}\Xmat''\E[\mb{e}
    \mid \Xmat] = \bfbeta, \] because under the linear CEF assumption \(\E[\mb{e}\mid
    \Xmat] = 0\).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*。为了证明条件无偏性，回想一下我们可以将 OLS 估计量写成 \[ \bhat = \bfbeta + (\Xmat''\Xmat)^{-1}\Xmat''\mb{e},
    \] 因此取（条件）期望，我们有 \[ \E[\bhat \mid \Xmat] = \bfbeta + \E[(\Xmat''\Xmat)^{-1}\Xmat''\mb{e}
    \mid \Xmat] = \bfbeta + (\Xmat''\Xmat)^{-1}\Xmat''\E[\mb{e} \mid \Xmat] = \bfbeta,
    \] 因为在线性 CEF 假设下 \(\E[\mb{e}\mid \Xmat] = 0\)。'
- en: For the conditional sampling variance, we can use the same decomposition we
    have, \[ \V[\bhat \mid \Xmat] = \V[\bfbeta + (\Xmat'\Xmat)^{-1}\Xmat'\mb{e} \mid
    \Xmat] = (\Xmat'\Xmat)^{-1}\Xmat'\V[\mb{e} \mid \Xmat]\Xmat(\Xmat'\Xmat)^{-1}.
    \] Since \(\E[\mb{e}\mid \Xmat] = 0\), we know that \(\V[\mb{e}\mid \Xmat] = \E[\mb{ee}'
    \mid \Xmat]\), which is a matrix with diagonal entries \(\E[e_{i}^{2} \mid \Xmat]
    = \sigma^2_i\) and off-diagonal entries \(\E[e_{i}e_{j} \Xmat] = \E[e_{i}\mid
    \Xmat]\E[e_{j}\mid\Xmat] = 0\), where the first equality follows from the independence
    of the errors across units. Thus, \(\V[\mb{e} \mid \Xmat]\) is a diagonal matrix
    with \(\sigma^2_i\) along the diagonal, which means \[ \Xmat'\V[\mb{e} \mid \Xmat]\Xmat
    = \sum_{i=1}^n \sigma^2_i \X_i\X_i', \] establishing the conditional sampling
    variance.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于条件抽样方差，我们可以使用我们已有的分解，\[ \V[\bhat \mid \Xmat] = \V[\bfbeta + (\Xmat'\Xmat)^{-1}\Xmat'\mb{e}
    \mid \Xmat] = (\Xmat'\Xmat)^{-1}\Xmat'\V[\mb{e} \mid \Xmat]\Xmat(\Xmat'\Xmat)^{-1}.
    \] 由于 \(\E[\mb{e}\mid \Xmat] = 0\)，我们知道 \(\V[\mb{e}\mid \Xmat] = \E[\mb{ee}' \mid
    \Xmat]\)，这是一个对角线元素为 \(\E[e_{i}^{2} \mid \Xmat] = \sigma^2_i\) 和非对角线元素 \(\E[e_{i}e_{j}
    \Xmat] = \E[e_{i}\mid \Xmat]\E[e_{j}\mid\Xmat] = 0\) 的矩阵，其中第一个等式来自于误差在单位之间的独立性。因此，\(\V[\mb{e}
    \mid \Xmat]\) 是一个对角矩阵，其对角线元素为 \(\sigma^2_i\)，这意味着 \[ \Xmat'\V[\mb{e} \mid \Xmat]\Xmat
    = \sum_{i=1}^n \sigma^2_i \X_i\X_i', \] 建立了条件抽样方差。
- en: This means that, for any realization of the covariates, \(\Xmat\), OLS is unbiased
    for the true regression coefficients \(\bfbeta\). By the law of iterated expectation,
    we also know that it is unconditionally unbiased[²](#fn2) as well since \[ \E[\bhat]
    = \E[\E[\bhat \mid \Xmat]] = \bfbeta. \] The difference between these two statements
    usually isn’t incredibly meaningful.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，对于协变量的任何实现，\(\Xmat\)，OLS 对于真实的回归系数 \(\bfbeta\) 是无偏的。根据迭代期望定律，我们也知道它无条件地无偏[²](#fn2)，因为
    \[ \E[\bhat] = \E[\E[\bhat \mid \Xmat]] = \bfbeta. \] 这两个陈述之间的差异通常并不特别有意义。
- en: There are a lot of variances flying around, so reviewing them is helpful. Above,
    we derived the asymptotic variance of \(\mb{Z}_{n} = \sqrt{n}(\bhat - \bfbeta)\),
    \[ \mb{V}_{\bfbeta} = \left( \E[\X_i\X_i'] \right)^{-1}\E[e_i^2\X_i\X_i']\left(
    \E[\X_i\X_i'] \right)^{-1}, \] which implies that the approximate variance of
    \(\bhat\) will be \(\mb{V}_{\bfbeta} / n\) because \[ \bhat = \frac{Z_n}{\sqrt{n}}
    + \bfbeta \quad\implies\quad \bhat \overset{a}{\sim} \N(\bfbeta, n^{-1}\mb{V}_{\bfbeta}),
    \] where \(\overset{a}{\sim}\) means asymptotically distributed as. Under the
    linear CEF, the conditional sampling variance of \(\bhat\) has a similar form
    and will be similar to the
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方差在飞来飞去，所以回顾它们是有帮助的。上面，我们推导了 \(\mb{Z}_{n} = \sqrt{n}(\bhat - \bfbeta)\) 的渐近方差，\[
    \mb{V}_{\bfbeta} = \left( \E[\X_i\X_i'] \right)^{-1}\E[e_i^2\X_i\X_i']\left( \E[\X_i\X_i']
    \right)^{-1}, \] 这意味着 \(\bhat\) 的近似方差将是 \(\mb{V}_{\bfbeta} / n\)，因为 \[ \bhat =
    \frac{Z_n}{\sqrt{n}} + \bfbeta \quad\implies\quad \bhat \overset{a}{\sim} \N(\bfbeta,
    n^{-1}\mb{V}_{\bfbeta}), \] 其中 \(\overset{a}{\sim}\) 表示渐近分布为。在线性CEF下，\(\bhat\)
    的条件抽样方差具有类似的形式，并且将与
- en: \[ \mb{V}_{\bhat} = \left( \Xmat'\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2_i
    \X_i\X_i' \right) \left( \Xmat'\Xmat \right)^{-1} \approx \mb{V}_{\bfbeta} / n.
    \] In practice, these two derivations lead to basically the same variance estimator.
    Recall that the heteroskedastic-consistent variance estimator \[ \widehat{\mb{V}}_{\bfbeta}
    = \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n\widehat{e}_i^2\X_i\X_i'
    \right) \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1}, \] is a valid plug-in estimator
    for the asymptotic variance and \[ \widehat{\mb{V}}_{\bhat} = n^{-1}\widehat{\mb{V}}_{\bfbeta}.
    \] Thus, in practice, the asymptotic and finite-sample results under a linear
    CEF justify the same variance estimator.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \[ \mb{V}_{\bhat} = \left( \Xmat'\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2_i
    \X_i\X_i' \right) \left( \Xmat'\Xmat \right)^{-1} \approx \mb{V}_{\bfbeta} / n.
    \] 在实践中，这两个推导导致基本相同的方差估计量。回想一下，异方差一致方差估计量 \[ \widehat{\mb{V}}_{\bfbeta} = \left(
    \frac{1}{n} \Xmat'\Xmat \right)^{-1} \left( \frac{1}{n} \sum_{i=1}^n\widehat{e}_i^2\X_i\X_i'
    \right) \left( \frac{1}{n} \Xmat'\Xmat \right)^{-1}, \] 是渐近方差的合法插值估计量，并且 \[ \widehat{\mb{V}}_{\bhat}
    = n^{-1}\widehat{\mb{V}}_{\bfbeta}. \] 因此，在实践中，线性CEF下的渐近和有限样本结果证明了相同的方差估计量是合理的。
- en: 7.4.1 Linear CEF model under homoskedasticity
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 同方差性下的线性CEF模型
- en: If we are willing to assume that the standard errors are homoskedastic, we can
    derive even stronger results for OLS. Stronger assumptions typically lead to stronger
    conclusions, but, obviously, those conclusions may not be robust to assumption
    violations. But homoskedasticity of errors is such a historically important assumption
    that statistical software implementations of OLS like `lm()` in R assume it by
    default.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们愿意假设标准误差是同方差的，我们可以为OLS推导出更强的结果。更强的假设通常会导致更强的结论，但显然，这些结论可能不会对假设违反具有鲁棒性。但是，误差的同方差性是一个历史上非常重要的假设，因此像R中的`lm()`这样的统计软件实现默认假设了它。
- en: '*Assumption: Homoskedasticity with a linear CEF* *In addition to the linear
    CEF assumption, we further assume that \[ \E[e_i^2 \mid \X_i] = \E[e_i^2] = \sigma^2,
    \] or that variance of the errors does not depend on the covariates.*  ***Theorem
    7.4** Under a linear CEF model with homoskedastic errors, the conditional sampling
    variance is \[ \mb{V}^{\texttt{lm}}_{\bhat} = \V[\bhat \mid \Xmat] = \sigma^2
    \left( \Xmat''\Xmat \right)^{-1}, \] and the variance estimator \[ \widehat{\mb{V}}^{\texttt{lm}}_{\bhat}
    = \widehat{\sigma}^2 \left( \Xmat''\Xmat \right)^{-1} \quad\text{where,}\quad
    \widehat{\sigma}^2 = \frac{1}{n - k - 1} \sum_{i=1}^n \widehat{e}_i^2 \] is unbiased,
    \(\E[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat} \mid \Xmat] = \mb{V}^{\texttt{lm}}_{\bhat}\).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '*假设：具有线性 CEF 的同方差性* 除了线性 CEF 假设之外，我们进一步假设 \[ \E[e_i^2 \mid \X_i] = \E[e_i^2]
    = \sigma^2, \] 或者说误差的方差不依赖于协变量。***定理 7.4** 在具有同方差误差的线性 CEF 模型下，条件抽样方差是 \[ \mb{V}^{\texttt{lm}}_{\bhat}
    = \V[\bhat \mid \Xmat] = \sigma^2 \left( \Xmat''\Xmat \right)^{-1}, \] 并且方差估计量
    \[ \widehat{\mb{V}}^{\texttt{lm}}_{\bhat} = \widehat{\sigma}^2 \left( \Xmat''\Xmat
    \right)^{-1} \quad\text{其中，}\quad \widehat{\sigma}^2 = \frac{1}{n - k - 1} \sum_{i=1}^n
    \widehat{e}_i^2 \] 是无偏的，\(\E[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat} \mid \Xmat]
    = \mb{V}^{\texttt{lm}}_{\bhat}\).'
- en: '*Proof*. Under homoskedasticity \(\sigma^2_i = \sigma^2\) for all \(i\). Recall
    that \(\sum_{i=1}^n \X_i\X_i'' = \Xmat''\Xmat\). Thus, the conditional sampling
    variance from [Theorem 7.3](#thm-ols-unbiased), \[ \begin{aligned} \V[\bhat \mid
    \Xmat] &= \left( \Xmat''\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2 \X_i\X_i''
    \right) \left( \Xmat''\Xmat \right)^{-1} \\ &= \sigma^2\left( \Xmat''\Xmat \right)^{-1}\left(
    \sum_{i=1}^n \X_i\X_i'' \right) \left( \Xmat''\Xmat \right)^{-1} \\&= \sigma^2\left(
    \Xmat''\Xmat \right)^{-1}\left( \Xmat''\Xmat \right) \left( \Xmat''\Xmat \right)^{-1}
    \\&= \sigma^2\left( \Xmat''\Xmat \right)^{-1} = \mb{V}^{\texttt{lm}}_{\bhat}.
    \end{aligned} \]'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*. 在同方差性 \(\sigma^2_i = \sigma^2\) 对所有 \(i\) 成立的情况下。回想一下，\(\sum_{i=1}^n
    \X_i\X_i'' = \Xmat''\Xmat\). 因此，从 [定理 7.3](#thm-ols-unbiased) 的条件抽样方差，\[ \begin{aligned}
    \V[\bhat \mid \Xmat] &= \left( \Xmat''\Xmat \right)^{-1}\left( \sum_{i=1}^n \sigma^2
    \X_i\X_i'' \right) \left( \Xmat''\Xmat \right)^{-1} \\ &= \sigma^2\left( \Xmat''\Xmat
    \right)^{-1}\left( \sum_{i=1}^n \X_i\X_i'' \right) \left( \Xmat''\Xmat \right)^{-1}
    \\&= \sigma^2\left( \Xmat''\Xmat \right)^{-1}\left( \Xmat''\Xmat \right) \left(
    \Xmat''\Xmat \right)^{-1} \\&= \sigma^2\left( \Xmat''\Xmat \right)^{-1} = \mb{V}^{\texttt{lm}}_{\bhat}.
    \end{aligned} \]'
- en: For unbiasedness, we just need to show that \(\E[\widehat{\sigma}^{2} \mid \Xmat]
    = \sigma^2\). Recall that we defined \(\mb{M}_{\Xmat}\) as the residual-maker
    because \(\mb{M}_{\Xmat}\mb{Y} = \widehat{\mb{e}}\). We can use this to connect
    the residuals to the standard errors, \[ \mb{M}_{\Xmat}\mb{e} = \mb{M}_{\Xmat}\mb{Y}
    - \mb{M}_{\Xmat}\Xmat\bfbeta = \mb{M}_{\Xmat}\mb{Y} = \widehat{\mb{e}}, \] so
    \[ \V[\widehat{\mb{e}} \mid \Xmat] = \mb{M}_{\Xmat}\V[\mb{e} \mid \Xmat] = \mb{M}_{\Xmat}\sigma^2,
    \] where the first equality holds because \(\mb{M}_{\Xmat} = \mb{I}_{n} - \Xmat
    (\Xmat'\Xmat)^{-1} \Xmat'\) is constant conditional on \(\Xmat\). Notice that
    the diagonal entries of this matrix are the variances of particular residuals
    \(\widehat{e}_i\) and that the diagonal entries of the annihilator matrix are
    \(1 - h_{ii}\) (since the \(h_{ii}\) are the diagonal entries of \(\mb{P}_{\Xmat}\)).
    Thus, we have \[ \V[\widehat{e}_i \mid \Xmat] = \E[\widehat{e}_{i}^{2} \mid \Xmat]
    = (1 - h_{ii})\sigma^{2}. \] In the last chapter in [Section 6.9.1](least_squares.html#sec-leverage),
    we established that one property of these leverage values is \(\sum_{i=1}^n h_{ii}
    = k+ 1\), so \(\sum_{i=1}^n 1- h_{ii} = n - k - 1\) and we have \[ \begin{aligned}
    \E[\widehat{\sigma}^{2} \mid \Xmat] &= \frac{1}{n-k-1} \sum_{i=1}^{n} \E[\widehat{e}_{i}^{2}
    \mid \Xmat] \\ &= \frac{\sigma^{2}}{n-k-1} \sum_{i=1}^{n} 1 - h_{ii} \\ &= \sigma^{2}.
    \end{aligned} \] This establishes \(\E[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat}
    \mid \Xmat] = \mb{V}^{\texttt{lm}}_{\bhat}\).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了无偏性，我们只需证明 \(\E[\widehat{\sigma}^{2} \mid \Xmat] = \sigma^2\). 回想一下，我们定义 \(\mb{M}_{\Xmat}\)
    为残差生成器，因为 \(\mb{M}_{\Xmat}\mb{Y} = \widehat{\mb{e}}\). 我们可以利用这一点将残差与标准误差联系起来，\[
    \mb{M}_{\Xmat}\mb{e} = \mb{M}_{\Xmat}\mb{Y} - \mb{M}_{\Xmat}\Xmat\bfbeta = \mb{M}_{\Xmat}\mb{Y}
    = \widehat{\mb{e}}, \] 因此 \[ \V[\widehat{\mb{e}} \mid \Xmat] = \mb{M}_{\Xmat}\V[\mb{e}
    \mid \Xmat] = \mb{M}_{\Xmat}\sigma^2, \] 其中第一个等式成立是因为 \(\mb{M}_{\Xmat} = \mb{I}_{n}
    - \Xmat (\Xmat'\Xmat)^{-1} \Xmat'\) 在给定 \(\Xmat\) 的条件下是常数。注意，这个矩阵的对角线元素是特定残差 \(\widehat{e}_i\)
    的方差，而湮灭矩阵的对角线元素是 \(1 - h_{ii}\)（因为 \(h_{ii}\) 是 \(\mb{P}_{\Xmat}\) 的对角线元素）。因此，我们有
    \[ \V[\widehat{e}_i \mid \Xmat] = \E[\widehat{e}_{i}^{2} \mid \Xmat] = (1 - h_{ii})\sigma^{2}.
    \] 在上一章的 [第 6.9.1 节](least_squares.html#sec-leverage) 中，我们建立了这些杠杆值的一个性质是 \(\sum_{i=1}^n
    h_{ii} = k+ 1\)，所以 \(\sum_{i=1}^n 1- h_{ii} = n - k - 1\)，并且我们有 \[ \begin{aligned}
    \E[\widehat{\sigma}^{2} \mid \Xmat] &= \frac{1}{n-k-1} \sum_{i=1}^{n} \E[\widehat{e}_{i}^{2}
    \mid \Xmat] \\ &= \frac{\sigma^{2}}{n-k-1} \sum_{i=1}^{n} 1 - h_{ii} \\ &= \sigma^{2}.
    \end{aligned} \] 这就证明了 \(\E[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat} \mid \Xmat]
    = \mb{V}^{\texttt{lm}}_{\bhat}\).
- en: Thus, under the linear CEF model and homoskedasticity of the errors, we have
    an unbiased variance estimator that is a simple function of the sum of squared
    residuals and the design matrix. Most statistical software packages estimate standard
    errors using \(\widehat{\mb{V}}^{\texttt{lm}}_{\bhat}\).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在线性 CEF 模型和误差的同方差性假设下，我们有一个无偏方差估计量，它是残差平方和与设计矩阵的简单函数。大多数统计软件包使用 \(\widehat{\mb{V}}^{\texttt{lm}}_{\bhat}\)
    来估计标准误差。
- en: The final result we can derive for the linear CEF under the homoskedasticity
    assumption is an optimality result. That is, we might ask if there is another
    estimator for \(\bfbeta\) that would outperform OLS in the sense of having a lower
    sampling variance. Perhaps surprisingly, no linear estimator for \(\bfbeta\) has
    a lower conditional variance, meaning that OLS is the **best linear unbiased estimator**,
    often jovially shortened to BLUE. This result is famously known as the Gauss-Markov
    Theorem.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在同方差性假设下，对于线性 CEF 我们可以推导出的最终结果是一个最优性结果。也就是说，我们可能会问是否存在另一个 \(\bfbeta\) 的估计量，在降低抽样方差的意义上优于
    OLS。也许令人惊讶的是，没有线性估计量对于 \(\bfbeta\) 有更低的条件方差，这意味着 OLS 是 **最佳线性无偏估计量**，通常简称为 BLUE。这个结果广为人知，被称为高斯-马尔可夫定理。
- en: '**Theorem 7.5** Let \(\widetilde{\bfbeta} = \mb{AY}\) be a linear and unbiased
    estimator for \(\bfbeta\). Under the linear CEF model with homoskedastic errors,
    \[ \V[\widetilde{\bfbeta}\mid \Xmat] \geq \V[\bhat \mid \Xmat]. \]'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 7.5** 设 \(\widetilde{\bfbeta} = \mb{AY}\) 是 \(\bfbeta\) 的一个线性无偏估计量。在具有同方差误差的线性
    CEF 模型下，\[ \V[\widetilde{\bfbeta}\mid \Xmat] \geq \V[\bhat \mid \Xmat]. \]'
- en: '*Proof*. Note that if \(\widetilde{\bfbeta}\) is unbiased then \(\E[\widetilde{\bfbeta}
    \mid \Xmat] = \bfbeta\) and so \[ \bfbeta = \E[\mb{AY} \mid \Xmat] = \mb{A}\E[\mb{Y}
    \mid \Xmat] = \mb{A}\Xmat\bfbeta, \] which implies that \(\mb{A}\Xmat = \mb{I}_n\).
    Rewrite the competitor as \(\widetilde{\bfbeta} = \bhat + \mb{BY}\) where, \[
    \mb{B} = \mb{A} - \left(\Xmat''\Xmat\right)^{-1}\Xmat''. \] and note that \(\mb{A}\Xmat
    = \mb{I}_n\) implies that \(\mb{B}\Xmat = 0\). We now have \[ \begin{aligned}
    \widetilde{\bfbeta} &= \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\mb{Y}
    \\ &= \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\Xmat\bfbeta
    + \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\mb{e} \\ &= \bfbeta
    + \mb{B}\Xmat\bfbeta + \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\mb{e}
    \\ &= \bfbeta + \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\mb{e}
    \end{aligned} \] The variance of the competitor is, thus, \[ \begin{aligned} \V[\widetilde{\bfbeta}
    \mid \Xmat] &= \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\V[\mb{e}\mid
    \Xmat]\left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)'' \\ &= \sigma^{2}\left(
    \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\left( \Xmat\left(\Xmat''\Xmat\right)^{-1}
    + \mb{B}''\right) \\ &= \sigma^{2}\left(\left(\Xmat''\Xmat\right)^{-1}\Xmat''\Xmat\left(\Xmat''\Xmat\right)^{-1}
    + \left(\Xmat''\Xmat\right)^{-1}\Xmat''\mb{B}'' + \mb{B}\Xmat\left(\Xmat''\Xmat\right)^{-1}
    + \mb{BB}''\right)\\ &= \sigma^{2}\left(\left(\Xmat''\Xmat\right)^{-1} + \mb{BB}''\right)\\
    &\geq \sigma^{2}\left(\Xmat''\Xmat\right)^{-1} \\ &= \V[\bhat \mid \Xmat] \end{aligned}
    \] The first equality comes from the properties of covariance matrices, the second
    is due to the homoskedasticity assumption, and the fourth is due to \(\mb{B}\Xmat
    = 0\), which implies that \(\Xmat''\mb{B}'' = 0\) as well. The fifth inequality
    holds because matrix products of the form \(\mb{BB}''\) are positive definite
    if \(\mb{B}\) is of full rank (which we have assumed it is).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*证明*。注意，如果\(\widetilde{\bfbeta}\)是无偏的，那么\(\E[\widetilde{\bfbeta} \mid \Xmat]
    = \bfbeta\)，因此 \[ \bfbeta = \E[\mb{AY} \mid \Xmat] = \mb{A}\E[\mb{Y} \mid \Xmat]
    = \mb{A}\Xmat\bfbeta, \] 这意味着 \(\mb{A}\Xmat = \mb{I}_n\)。将竞争者重新写为 \(\widetilde{\bfbeta}
    = \bhat + \mb{BY}\)，其中， \[ \mb{B} = \mb{A} - \left(\Xmat''\Xmat\right)^{-1}\Xmat''.
    \] 并注意 \(\mb{A}\Xmat = \mb{I}_n\) 意味着 \(\mb{B}\Xmat = 0\)。我们现在有 \[ \begin{aligned}
    \widetilde{\bfbeta} &= \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\mb{Y}
    \\ &= \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\Xmat\bfbeta
    + \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\mb{e} \\ &= \bfbeta
    + \mb{B}\Xmat\bfbeta + \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\mb{e}
    \\ &= \bfbeta + \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\mb{e}
    \end{aligned} \] 因此，竞争者的方差为 \[ \begin{aligned} \V[\widetilde{\bfbeta} \mid \Xmat]
    &= \left( \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\V[\mb{e}\mid \Xmat]\left(
    \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)'' \\ &= \sigma^{2}\left(
    \left(\Xmat''\Xmat\right)^{-1}\Xmat'' + \mb{B}\right)\left( \Xmat\left(\Xmat''\Xmat\right)^{-1}
    + \mb{B}''\right) \\ &= \sigma^{2}\left(\left(\Xmat''\Xmat\right)^{-1}\Xmat''\Xmat\left(\Xmat''\Xmat\right)^{-1}
    + \left(\Xmat''\Xmat\right)^{-1}\Xmat''\mb{B}'' + \mb{B}\Xmat\left(\Xmat''\Xmat\right)^{-1}
    + \mb{BB}''\right)\\ &= \sigma^{2}\left(\left(\Xmat''\Xmat\right)^{-1} + \mb{BB}''\right)\\
    &\geq \sigma^{2}\left(\Xmat''\Xmat\right)^{-1} \\ &= \V[\bhat \mid \Xmat] \end{aligned}
    \] 第一个等式来自协方差矩阵的性质，第二个等式是由于同方差性假设，第四个等式是由于 \(\mb{B}\Xmat = 0\)，这同样意味着 \(\Xmat''\mb{B}''
    = 0\)。第五个不等式成立，因为如果 \(\mb{B}\) 是满秩的（我们假设它是），那么形式为 \(\mb{BB}''\) 的矩阵乘积是正定的。'
- en: In this proof, we saw that the variance of the competing estimator had variance
    \(\sigma^2\left(\left(\Xmat'\Xmat\right)^{-1} + \mb{BB}'\right)\) which we argued
    was “greater than 0” in the matrix sense, which is also called positive definite.
    What does this mean practically? Remember that any positive definite matrix must
    have strictly positive diagonal entries and that the diagonal entries of \(\V[\bhat
    \mid \Xmat]\) and \(V[\widetilde{\bfbeta}\mid \Xmat]\) are the variances of the
    individual parameters, \(\V[\widehat{\beta}_{j} \mid \Xmat]\) and \(\V[\widetilde{\beta}_{j}
    \mid \Xmat]\). Thus, the variances of the individual parameters will be larger
    for \(\widetilde{\bfbeta}\) than for \(\bhat\).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个证明中，我们看到了竞争估计量的方差为 \(\sigma^2\left(\left(\Xmat'\Xmat\right)^{-1} + \mb{BB}'\right)\)，我们论证了在矩阵意义上它是“大于0”的，这也被称为正定。这在实际中意味着什么？记住，任何正定矩阵都必须有严格正的对角线元素，而
    \(\V[\bhat \mid \Xmat]\) 和 \(V[\widetilde{\bfbeta}\mid \Xmat]\) 的对角线元素是单个参数的方差，即
    \(\V[\widehat{\beta}_{j} \mid \Xmat]\) 和 \(\V[\widetilde{\beta}_{j} \mid \Xmat]\)。因此，对于
    \(\widetilde{\bfbeta}\)，单个参数的方差将大于 \(\bhat\)。
- en: Many textbooks cite the Gauss-Markov theorem as a critical advantage of OLS
    over other methods, but recognizing its limitations is essential. It requires
    linearity and homoskedastic error assumptions, and these can be false in many
    applications.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 许多教科书将高斯-马尔可夫定理作为OLS方法相对于其他方法的重大优势，但认识到其局限性是至关重要的。它需要线性性和同方差误差假设，而这些假设在许多应用中可能是错误的。
- en: Finally, note that while we have shown this result for linear estimators, Hansen
    ([2022](references.html#ref-Hansen22)) proves a more general version of this result
    that applies to any unbiased estimator.**  **## 7.5 The normal linear model
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，虽然我们已经为线性估计量展示了这个结果，但汉森（[2022](references.html#ref-Hansen22)）证明了一个更通用的结果版本，该结果适用于任何无偏估计量。**  **##
    7.5 正态线性模型
- en: 'Finally, we add the strongest and thus least loved of the classical linear
    regression assumption: (conditional) normality of the errors. Historically the
    reason to use this assumption was that finite-sample inference hits a roadblock
    without some knowledge of the sampling distribution of \(\bhat\). Under the linear
    CEF model, we saw that \(\bhat\) is unbiased, and under homoskedasticity, we could
    produce an unbiased estimator of the conditional variance. But for hypothesis
    testing or for generating confidence intervals, we need to make probability statements
    about the estimator, and, for that, we need to know its exact distribution. When
    the sample size is large, we can rely on the CLT and know \(\bhat\) is approximately
    normal. But how do we proceed in small samples? Historically we would have assumed
    (conditional) normality of the errors, basically proceeding with some knowledge
    that we were wrong but hopefully not too wrong.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加了最强大但也是最不受欢迎的经典线性回归假设之一：（条件）误差的正态性。从历史上看，使用这个假设的原因是，在不知道 \(\bhat\) 的抽样分布的情况下，有限样本推断会遇到障碍。在线性
    CEF 模型下，我们看到了 \(\bhat\) 是无偏的，在同方差性下，我们可以产生条件方差的無偏估计量。但对于假设检验或生成置信区间，我们需要对估计量做出概率陈述，为此，我们需要知道其确切分布。当样本量很大时，我们可以依赖中心极限定理，知道
    \(\bhat\) 大约是正态分布的。但在小样本情况下我们该如何进行呢？从历史上看，我们会假设误差的（条件）正态性，基本上是带着一些我们知道我们可能是错误的，但希望不是太错误的知识进行。
- en: '*The normal linear regression model* *In addition to the linear CEF assumption,
    we assume that \[ e_i \mid \Xmat \sim \N(0, \sigma^2). \]*  *There are a couple
    of important points:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*正态线性回归模型* *除了线性 CEF 假设之外，我们还假设 \[ e_i \mid \Xmat \sim \N(0, \sigma^2). \]*  *有几个重要的观点：'
- en: The assumption here is not that \((Y_{i}, \X_{i})\) are jointly normal (though
    this would be sufficient for the assumption to hold), but rather that \(Y_i\)
    is normally distributed conditional on \(\X_i\).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这里的假设并不是说 \((Y_{i}, \X_{i})\) 是联合正态分布的（尽管这足以使假设成立），而是 \(Y_i\) 在给定 \(\X_i\) 的条件下是正态分布的。
- en: Notice that the normal regression model has the homoskedasticity assumption
    baked in.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意到标准回归模型内嵌了同方差性的假设。
- en: '**Theorem 7.6** Under the normal linear regression model, we have \[ \begin{aligned}
    \bhat \mid \Xmat &\sim \N\left(\bfbeta, \sigma^{2}\left(\Xmat''\Xmat\right)^{-1}\right)
    \\ \frac{\widehat{\beta}_{j} - \beta_{j}}{[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat}]_{jj}/\sqrt{n}}
    &\sim t_{n-k-1} \\ W/q &\sim F_{q, n-k-1}. \end{aligned} \]'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**定理 7.6** 在正态线性回归模型下，我们有 \[ \begin{aligned} \bhat \mid \Xmat &\sim \N\left(\bfbeta,
    \sigma^{2}\left(\Xmat''\Xmat\right)^{-1}\right) \\ \frac{\widehat{\beta}_{j} -
    \beta_{j}}{[\widehat{\mb{V}}^{\texttt{lm}}_{\bhat}]_{jj}/\sqrt{n}} &\sim t_{n-k-1}
    \\ W/q &\sim F_{q, n-k-1}. \end{aligned} \]'
- en: This theorem says that in the normal linear regression model, the coefficients
    follow a normal distribution, the t-statistics follow a \(t\)-distribution, and
    a transformation of the Wald statistic follows an \(F\) distribution. These are
    **exact** results and do not rely on large-sample approximations. Under the assumption
    of conditional normality of the errors, the results are as valid for \(n = 5\)
    as for \(n = 500,000\).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定理说明，在正态线性回归模型中，系数遵循正态分布，t 统计量遵循 t 分布，Wald 统计量的变换遵循 F 分布。这些都是 **精确** 的结果，不依赖于大样本近似。在误差条件正态性的假设下，这些结果对于
    \(n = 5\) 和 \(n = 500,000\) 都是有效的。
- en: 'Few people believe errors follow a normal distribution, so why even present
    these results? Unfortunately, most statistical software implementations of OLS
    implicitly assume this when calculating p-values for tests or constructing confidence
    intervals. In R, for example, the p-value associated with the \(t\)-statistic
    reported by `lm()` relies on the \(t_{n-k-1}\) distribution, and the critical
    values used to construct confidence intervals with `confint()` use that distribution
    as well. When normality does not hold, there is no principled reason to use the
    \(t\) or the \(F\) distributions in this way. But we might hold our nose and use
    this *ad hoc* procedure under two rationalizations:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有人相信误差遵循正态分布，那么为什么还要展示这些结果呢？不幸的是，大多数统计软件在计算测试的 p 值或构建置信区间时，对 OLS 的实现隐式地假设了这一点。例如，在
    R 中，`lm()` 报告的 \(t\) 统计量相关的 p 值依赖于 \(t_{n-k-1}\) 分布，而用于构建置信区间的临界值也使用该分布。当正态性不成立时，没有原则上的理由使用
    \(t\) 或 \(F\) 分布进行这种推断。但我们可以采取这种 *临时* 程序，并给出两个合理的解释：
- en: \(\bhat\) is asymptotically normal. This approximation might, however, be poor
    in smaller finite samples. The \(t\) distribution will make inference more conservative
    in these cases (wider confidence intervals, smaller test rejection regions), which
    might help offset its poor approximation of the normal distribution in small samples.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: \(\bhat\) 是渐近正态的。然而，这种近似在小样本中可能并不好。在这种情况下，\(t\) 分布将使推断更加保守（置信区间更宽，检验拒绝区域更小），这可能会帮助抵消其在小样本中对正态分布的较差近似。
- en: As \(n\to\infty\), the \(t_{n-k-1}\) will converge to a standard normal distribution,
    so the *ad hoc* adjustment will not matter much for medium to large samples.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 \(n\to\infty\) 时，\(t_{n-k-1}\) 将收敛到标准正态分布，因此这种 *临时* 调整对中等或大样本的影响不会很大。
- en: These arguments are not very convincing since whether the \(t\) approximation
    will be any better than the normal in finite samples is unclear. But it may be
    the best we can do while we go and find more data.*  *## 7.6 Summary
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些论点并不很有说服力，因为 \(t\) 近似在有限样本中是否比正态分布更好并不清楚。但当我们寻找更多数据时，这可能是我们能做的最好的事情。*  *##
    7.6 概述
- en: In this chapter, we discussed the large-sample properties of OLS, which are
    quite strong. Under mild conditions, OLS is consistent for the population linear
    regression coefficients and is asymptotically normal. The variance of the OLS
    estimator, and thus the variance estimator, depends on whether the projection
    errors are assumed to be unrelated to the covariates (**homoskedastic**) or possibly
    related (**heteroskedastic**). Confidence intervals and hypothesis tests for individual
    OLS coefficients are largely the same as discussed in Part I of this book, and
    we can obtain finite-sample properties of OLS such as conditional unbiasedness
    if we assume the conditional expectation function is linear. If we further assume
    the errors are normally distributed, we can derive confidence intervals and hypothesis
    tests that are valid for all sample sizes.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了 OLS 的大样本性质，这些性质相当强。在温和的条件下，OLS 对总体线性回归系数是一致的，并且是渐近正态的。OLS 估计量的方差，以及方差估计量，取决于投影误差是否假设与协变量无关（**同方差**）或可能相关（**异方差**）。对于单个
    OLS 系数的置信区间和假设检验与本书第一部分讨论的大致相同，如果我们假设条件期望函数是线性的，我们还可以获得 OLS 的有限样本性质，如条件无偏性。如果我们进一步假设误差是正态分布的，我们可以推导出对所有样本大小都有效的置信区间和假设检验。
- en: 'Hansen, Bruce E. 2022\. “A Modern Gauss–Markov Theorem.” *Econometrica* 90
    (3): 1283–94\. [https://doi.org/10.3982/ECTA19255](https://doi.org/10.3982/ECTA19255).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hansen, Bruce E. 2022\. “A Modern Gauss–Markov Theorem.” *Econometrica* 90
    (3): 1283–94\. [https://doi.org/10.3982/ECTA19255](https://doi.org/10.3982/ECTA19255).'
- en: '* * *'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: The form of the Wald statistic is that of a weighted inner product, \(\mb{x}'\mb{Ay}\),
    where \(\mb{A}\) is a symmetric positive-definite weighting matrix.[↩︎](#fnref1)
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Wald 统计量形式为加权内积，\(\mb{x}'\mb{Ay}\)，其中 \(\mb{A}\) 是对称正定加权矩阵。[↩︎](#fnref1)
- en: We are basically ignoring some edge cases when it comes to discrete covariates
    here. In particular, we assume that \(\Xmat'\Xmat\) is nonsingular with probability
    one. However, this assumption can fail if we have a binary covariate since there
    is some chance (however slight) that the entire column will be all ones or all
    zeros, which would lead to a singular matrix \(\Xmat'\Xmat\). Practically this
    is not a big deal, but it does mean that we have to ignore this issue theoretically
    or focus on conditional unbiasedness.[↩︎](#fnref2)********
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在处理离散协变量时，我们基本上忽略了某些边缘情况。特别是，我们假设 \(\Xmat'\Xmat\) 的概率为1是非奇异的。然而，如果我们有一个二元协变量，这个假设可能会失败，因为整个列全部为1或全部为0的可能性（尽管可能性很小），这将导致
    \(\Xmat'\Xmat\) 是奇异矩阵。实际上这并不是什么大问题，但这确实意味着我们理论上必须忽略这个问题，或者关注条件无偏性。[↩︎](#fnref2)********
