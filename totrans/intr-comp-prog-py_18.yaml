- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: STOCHASTIC PROGRAMS, PROBABILITY, AND DISTRIBUTIONS
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 随机程序、概率和分布
- en: There is something comforting about Newtonian mechanics. You push down on one
    end of a lever, and the other end goes up. You throw a ball up in the air; it
    travels a parabolic path, and eventually comes down. ![c17-fig-5001.jpg](../images/c17-fig-5001.jpg).
    In short, everything happens for a reason. The physical world is a completely
    predictable place—all future states of a physical system can be derived from knowledge
    about its current state.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 纽顿力学给人一种安慰。你在杠杆的一端施加压力，另一端就会上升。你把球抛向空中；它沿着抛物线轨迹移动，最终落下。简而言之，一切都是有原因的。物理世界是一个完全可预测的地方——所有物理系统的未来状态都可以从对其当前状态的知识中推导出来。
- en: For centuries, this was the prevailing scientific wisdom; then along came quantum
    mechanics and the Copenhagen Doctrine. The doctrine's proponents, led by Bohr
    and Heisenberg, argued that at its most fundamental level, the behavior of the
    physical world cannot be predicted. One can make probabilistic statements of the
    form “x is highly likely to occur,” but not statements of the form “x is certain
    to occur.” Other distinguished physicists, most notably Einstein and Schrödinger,
    vehemently disagreed.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 几个世纪以来，这一直是主流科学智慧；然后量子力学和哥本哈根公理出现了。以波尔和海森堡为首的公理支持者们争辩说，从最根本的层面来看，物理世界的行为是无法预测的。可以做出“x很可能发生”的概率性陈述，但不能做出“x必然发生”的陈述。其他著名物理学家，尤其是爱因斯坦和薛定谔，
    vehemently disagreed.
- en: This debate roiled the worlds of physics, philosophy, and even religion. The
    heart of the debate was the validity of **causal nondeterminism**, i.e., the belief
    that not every event is caused by previous events. Einstein and Schrödinger found
    this view philosophically unacceptable, as exemplified by Einstein's often-repeated
    comment, “God does not play dice.” What they could accept was **predictive nondeterminism**,
    i.e., the concept that our inability to make accurate measurements about the physical
    world makes it impossible to make precise predictions about future states. This
    distinction was nicely summed up by Einstein, who said, “The essentially statistical
    character of contemporary theory is solely to be ascribed to the fact that this
    theory operates with an incomplete description of physical systems.”
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这场辩论搅动了物理学、哲学甚至宗教的世界。辩论的核心是**因果非确定性**的有效性，即并非每个事件都是由之前的事件引起的信念。爱因斯坦和薛定谔发现这种观点在哲学上不可接受，正如爱因斯坦经常重复的那句话，“上帝不会掷骰子。”他们所能接受的是**预测非确定性**，即我们的测量能力有限使得无法对未来状态做出准确预测。这一区别被爱因斯坦很好地总结，他说：“当代理论的本质统计特征完全归因于这一理论以不完整的物理系统描述为基础。”
- en: The question of causal nondeterminism is still unsettled. However, whether the
    reason we cannot predict events is because they are truly unpredictable or because
    we simply don't have enough information to predict them is of no practical importance.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 因果非确定性的问题仍然没有解决。然而，我们无法预测事件的原因是因为它们真正不可预测，还是因为我们仅仅没有足够的信息去预测它们，这在实践中并无重要意义。
- en: While the Bohr/Einstein debate was about how to understand the lowest levels
    of the physical world, the same issues arise at the macroscopic level. Perhaps
    the outcomes of horse races, spins of roulette wheels, and stock market investments
    are causally deterministic. However, there is ample evidence that it is perilous
    to treat them as predictably deterministic.[^(108)](#c17-fn-0001)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当波尔/爱因斯坦的辩论涉及如何理解物理世界的最低层次时，相同的问题也出现在宏观层面。或许马赛的结果、轮盘赌的旋转以及股票市场的投资是因果确定的。然而，有充足的证据表明，将它们视为可预测的确定性是危险的。
- en: 17.1 Stochastic Programs
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.1 随机程序
- en: 'A program is **deterministic** if whenever it is run on the same input, it
    produces the same output. Notice that this is not the same as saying that the
    output is completely defined by the specification of the problem. Consider, for
    example, the specification of `square_root`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个程序是**确定性的**，那么每当它在相同输入上运行时，它产生相同的输出。注意，这与说输出完全由问题的规格定义并不相同。考虑，例如，`square_root`的规格：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This specification admits many possible return values for the function call
    `square_root(2, 0.001)`. However, the successive approximation algorithm that
    we looked at in Chapter 3 will always return the same value. The specification
    doesn't require that the implementation be deterministic, but it does allow deterministic
    implementations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这个规格为函数调用`square_root(2, 0.001)`允许了许多可能的返回值。然而，我们在第三章看到的连续逼近算法将始终返回相同的值。该规格并不要求实现是确定性的，但它确实允许确定性实现。
- en: Not all interesting specifications can be met by deterministic implementations.
    Consider, for example, implementing a program to play a dice game, say backgammon
    or craps. Somewhere in the program will be a function that simulates a fair roll
    of a single six-sided die.[^(109)](#c17-fn-0002) Suppose it had a specification
    something like
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 不是所有有趣的规格都可以通过确定性实现来满足。例如，考虑实现一个骰子游戏的程序，比如西洋双陆棋或掷骰子。程序中会有一个函数来模拟一个公平的六面骰子的掷骰子。[^(109)](#c17-fn-0002)假设它的规格是这样的。
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This would be problematic, since it allows the implementation to return the
    same number each time it is called, which would make for a pretty boring game.
    It would be better to specify that `roll_die` “`returns a randomly chosen int
    between 1 and 6`,” thus requiring a stochastic implementation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是个问题，因为它允许实现每次调用时返回相同的数字，这会使游戏变得相当无聊。更好的方法是指定`roll_die`“`返回一个在1和6之间随机选择的整数`”，这样就要求实现是随机的。
- en: Most programming languages, including Python, include simple ways to write stochastic
    programs, i.e., programs that exploit randomness. The tiny program in [Figure
    17-1](#c17-fig-0001) is a simulation model. Rather than asking some person to
    roll a die multiple times, we wrote a program to simulate that activity. The code
    uses one of several useful functions found in the imported Python standard library
    module `random`. (The code in this chapter assumes that the imports
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数编程语言，包括Python，都提供了简单的方法来编写随机程序，即利用随机性的程序。[图17-1](#c17-fig-0001)中的小程序是一个模拟模型。我们不是让某个人多次掷骰子，而是写了一个程序来模拟这个活动。代码使用了从导入的Python标准库模块`random`中找到的几个有用函数之一。
- en: '[PRE2]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: have all been executed.)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都已执行。
- en: The function `random.choice` takes a non-empty sequence as its argument and
    returns a randomly chosen member of that sequence. Almost all of the functions
    in `random` are built using the function `random.random`, which generates a random
    floating-point number between `0`.`0` and `1.0`.[^(110)](#c17-fn-0003)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`random.choice`以一个非空序列作为参数，并返回该序列中随机选择的成员。几乎所有`random`中的函数都是使用函数`random.random`构建的，该函数生成一个在`0.0`和`1.0`之间的随机浮点数。[^(110)](#c17-fn-0003)
- en: '![c17-fig-0001.jpg](../images/c17-fig-0001.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![c17-fig-0001.jpg](../images/c17-fig-0001.jpg)'
- en: '[Figure 17-1](#c17-fig-0001a) Roll die'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-1](#c17-fig-0001a) 掷骰子'
- en: Now, imagine running `roll_n(10)`. Would you be more surprised to see it print
    `1111111111` or `5442462412`? Or, to put it another way, which of these two sequences
    is more random? It's a trick question. Each of these sequences is equally likely,
    because the value of each roll is independent of the values of earlier rolls.
    In a stochastic process, two events are **independent** if the outcome of one
    event has no influence on the outcome of the other.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下运行`roll_n(10)`。你会对打印出`1111111111`还是`5442462412`感到更惊讶？换句话说，这两个序列哪个更随机？这是一个技巧性问题。这两个序列发生的可能性是一样的，因为每次掷骰子的值与之前的掷骰子值是独立的。在随机过程中的两个事件是**独立的**，如果一个事件的结果对另一个事件的结果没有影响。
- en: This is easier to see if we simplify the situation by thinking about a two-sided
    die (also known as a coin) with the values 0 and 1\. This allows us to think of
    the output of a call of `roll_n` as a binary number. When we use a binary die,
    there are `2`^n possible sequences that `roll_n` might return. Each of these sequences
    is equally likely; therefore each has a probability of occurring of `(1/2)`^n.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们通过考虑一个两面的骰子（也称为硬币），其值为0和1来简化情况，这样更容易理解。这样我们可以将`roll_n`的输出视为一个二进制数字。当我们使用二进制骰子时，`roll_n`可能返回的序列有`2`^n种可能性。这些序列的可能性是一样的，因此每个序列发生的概率为`(1/2)`^n。
- en: Let's go back to our six-sided die. How many different sequences are there of
    length `10`? `6`^(10). So, the probability of rolling 10 consecutive `1`'s is
    `1/6`^(10). Less than one out of 60 million. Pretty low, but no lower than the
    probability of any other sequence, e.g., `5442462412`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到六面骰子。长度为`10`的不同序列有多少种？`6`^(10)。所以，连续掷到10个`1`的概率是`1/6`^(10)。少于六千万分之一。相当低，但不低于任何其他序列的概率，例如`5442462412`。
- en: 17.2 Calculating Simple Probabilities
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.2 计算简单概率
- en: In general, when we talk about the **probability** of a result having some property
    (e.g., all `1`'s), we are asking what fraction of all possible results has that
    property. This is why probabilities range from 0 to 1\. Suppose we want to know
    the probability of getting any sequence other than all `1`'s when rolling the
    die. It is simply `1 – (1/6`^(10)`)`, because the probability of something happening
    and the probability of the same thing not happening must add up to `1`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，当我们谈论某个结果具有某种属性的**概率**（例如，都是`1`）时，我们是在询问所有可能结果中有多少比例具有该属性。这就是为什么概率的范围是从0到1。假设我们想知道掷骰子时得到的序列不是全部`1`的概率。它只是`1 – (1/6`^(10)`)`，因为某件事发生的概率和同一事件不发生的概率之和必须为`1`。
- en: 'Suppose we want to know the probability of rolling the die 10 times without
    getting a single `1`. One way to answer this question is to transform it into
    the question of how many of the `6^(10)` possible sequences don''t contain a `1`.
    This can be computed as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想知道在10次掷骰子中没有掷到一个`1`的概率。回答这个问题的一种方法是将其转化为`6^(10)`个可能序列中有多少个不包含`1`。这可以通过以下方式计算：
- en: 1\. The probability of not rolling a `1` on any single roll is `5/6`.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1\. 在任何一次掷骰子中，不掷到`1`的概率是`5/6`。
- en: 2\. The probability of not rolling a `1` on either the first or the second roll
    is `(5/6)`*`(5/6)`, or `(5/6)`².
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2\. 在第一次或第二次掷骰子时，不掷到`1`的概率是`(5/6)`*`(5/6)`，或`(5/6)`²。
- en: 3\. So, the probability of not rolling a `1` 10 times in a row is `(5/6)`^(10),
    slightly more than `0.16`.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3\. 所以，不连续掷到`1` 10次的概率是`(5/6)`^(10)，略高于`0.16`。
- en: Step 2 is an application of the **multiplicative law** for independent probabilities.
    Consider, for example, two independent events `A` and `B`. If `A` occurs one `1/3`
    of the time and `B` occurs `1/4` of the time, the probability that both `A` and
    `B` occur is `1/4 of 1/3`, i.e., `(1/3)/4` or `(1/3)*(1/4).`
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步是对独立概率的**乘法法则**的应用。例如，考虑两个独立事件`A`和`B`。如果`A`发生的概率为`1/3`，而`B`发生的概率为`1/4`，那么`A`和`B`同时发生的概率为`1/4
    of 1/3`，即`(1/3)/4`或`(1/3)*(1/4)`。
- en: What about the probability of rolling at least one `1`? It is simply 1 minus
    the probability of not rolling at least one `1`, i.e., `1` - `(5/6)`^(10). Notice
    that this cannot be correctly computed by saying that the probability of rolling
    a `1` on any roll is `1/6`, so the probability of rolling at least one `1` is
    `10*(1/6)`, i.e., `10/6`. This is obviously incorrect since a probability cannot
    be greater than `1`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 至于掷到至少一个`1`的概率呢？它就是1减去不掷到至少一个`1`的概率，即`1` - `(5/6)`^(10)。注意，不能简单地说任何一次掷骰子的`1`的概率是`1/6`，因此至少掷到一个`1`的概率是`10*(1/6)`，即`10/6`。这显然是不正确的，因为概率不能大于`1`。
- en: How about the probability of rolling exactly two `1's` in 10 rolls? This is
    equivalent to asking what fraction of the first `6^(10)` integers has exactly
    two `1's` in its base `6` representation. We could easily write a program to generate
    all of these sequences and count the number that contained exactly one `1`. Deriving
    the probability analytically is a bit tricky, and we defer it to Section 17.4.4.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在10次掷骰子中掷到恰好两个`1`的概率是多少呢？这相当于问在`6^(10)`个整数中，有多少个整数在其基数`6`表示中恰好有两个`1`。我们可以轻松编写程序生成所有这些序列，并计算包含恰好一个`1`的数量。用解析方法推导这个概率有点棘手，我们将其留到第17.4.4节讨论。
- en: 17.3 Inferential Statistics
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3 推论统计
- en: As you just saw, we can use a systematic process to derive the precise probability
    of some complex event based upon knowing the probability of one or more simpler
    events. For example, we can easily compute the probability of flipping a coin
    and getting 10 consecutive heads based on the assumption that flips are independent
    and we know the probability of each flip coming up heads. Suppose, however, that
    we don't actually know the probability of the relevant simpler event. Suppose,
    for example, that we don't know whether the coin is fair (i.e., a coin where heads
    and tails are equally likely).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你刚才看到的，我们可以使用系统化的过程基于已知的一个或多个简单事件的概率，推导出一些复杂事件的精确概率。例如，我们可以轻松计算投掷硬币得到10次连续正面的概率，前提是我们假设每次投掷是独立的，并且知道每次投掷出现正面的概率。然而，假设我们实际上并不知道相关的简单事件的概率。举例来说，假设我们不知道这枚硬币是否公平（即，正反面出现的概率相等）。
- en: All is not lost. If we have some data about the behavior of the coin, we can
    combine that data with our knowledge of probability to derive an estimate of the
    true probability. We can use **inferential statistics** to estimate the probability
    of a single flip coming up heads, and then conventional probability to compute
    the probability of a coin with that behavior coming up heads 10 times in a row.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一切尚未失去。如果我们对硬币的行为有一些数据，我们可以将这些数据与我们的概率知识结合，以推导出真实概率的估计。我们可以使用**推断统计**来估计一次投掷出现正面的概率，然后用传统概率来计算这种行为的硬币连续出现10次正面的概率。
- en: In brief (since this is not a book about statistics), the guiding principle
    of inferential statistics is that a random sample tends to exhibit the same properties
    as the population from which it is drawn.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之（因为这不是一本关于统计的书），推断统计的指导原则是，随机样本往往表现出与其来源总体相同的特性。
- en: Suppose Harvey Dent (also known as Two-Face) flipped a coin, and it came up
    heads. You would not infer from this that the next flip would also come up heads.
    Suppose he flipped it twice, and it came up heads both times. You might reason
    that the probability of this happening for a fair coin was `0.25`, so there was
    still no reason to assume the next flip would be heads. Suppose, however, `100`
    out of `100` flips came up heads. The number `(1/2)`^(100) (the probability of
    this event, assuming a fair coin) is pretty small, so you might feel safe in inferring
    that the coin has a head on both sides.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设哈维·登特（也称为“双面人”）投掷了一枚硬币，结果是正面。你不会由此推断下一次投掷也会是正面。假设他投掷了两次，结果都是正面。你可能会推测，对于一枚公平的硬币，这种情况发生的概率是`0.25`，因此仍然没有理由假设下一次投掷会是正面。然而，假设`100`次投掷中都有`100`次是正面。事件的概率`(1/2)^(100)`（假设硬币是公平的）非常小，因此你可能会觉得推断这枚硬币的两面都是正面是安全的。
- en: Your belief in whether the coin is fair is based on the intuition that the behavior
    of a single sample of `100` flips is similar to the behavior of the population
    of all samples of `100` flips. This belief seems appropriate when all `100` flips
    are heads. Suppose that `52` flips came up heads and `48` tails. Would you feel
    comfortable in predicting that the next `100` flips would have the same ratio
    of heads to tails? For that matter, how comfortable would you feel about even
    predicting that there would be more heads than tails in the next `100` flips?
    Take a few minutes to think about this, and then try the experiment. Or, if you
    don't happen to have a coin handy, simulate the flips using the code in [Figure
    17-2](#c17-fig-0002).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你对硬币是否公平的信念基于这样的直觉：单次`100`次投掷的行为与所有`100`次投掷样本的总体行为是相似的。当所有`100`次投掷都是正面时，这种信念似乎是合适的。假设有`52`次投掷是正面，`48`次是反面。你是否会对预测下一次`100`次投掷的正反面比例保持信心？更进一步，你会对预测下一次`100`次投掷中正面会多于反面感到有把握吗？花几分钟思考一下这个问题，然后尝试进行实验。或者，如果你没有现成的硬币，可以使用[图17-2](#c17-fig-0002)中的代码来模拟投掷。
- en: The function `flip` in [Figure 17-2](#c17-fig-0002) simulates flipping a fair
    coin `num_flips` times and returns the fraction of those flips that came up heads.
    For each flip, the call `random.choice(('H', ‘T'))` randomly returns either `'H'`
    or `'T'.`
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-2](#c17-fig-0002)中的函数`flip`模拟了一枚公平硬币投掷`num_flips`次，并返回其中出现正面的比例。对于每次投掷，调用`random.choice((''H'',
    ‘T''))`随机返回`''H''`或`''T''`。'
- en: '![c17-fig-0002.jpg](../images/c17-fig-0002.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![c17-fig-0002.jpg](../images/c17-fig-0002.jpg)'
- en: '[Figure 17-2](#c17-fig-0002a) Flipping a coin'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图17-2](#c17-fig-0002a) 投掷硬币'
- en: 'Try executing the function `flip_sim(10, 1)` a couple of times. Here''s what
    we saw the first two times we tried `print(''Mean ='', flip_sim(10, 1))`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试执行函数 `flip_sim(10, 1)` 几次。我们在前两次尝试 `print('Mean =', flip_sim(10, 1))` 时看到的结果如下：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It seems that it would be inappropriate to assume much (other than that the
    coin has both heads and tails) from any one trial of `10` flips. That''s why we
    typically structure our simulations to include multiple trials and compare the
    results. Let''s try `flip_sim(10, 100)` a couple of times:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从单次 `10` 次抛掷的试验中假设太多（除了硬币有正反两面）似乎不合适。这就是为什么我们通常将模拟结构化为包含多个试验并比较结果。让我们尝试 `flip_sim(10,
    100)` 几次：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Do you feel better about these results? When we tried `flip_sim(100, 100000)`,
    we got
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你对这些结果感觉更好了吗？当我们尝试 `flip_sim(100, 100000)` 时，我们得到了
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This looks really good (especially since we know that the answer should be `0.5`—but
    that's cheating). Now it seems we can safely conclude something about the next
    flip, i.e., that heads and tails are about equally likely. But why do we think
    that we can conclude that?
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来真的很好（尤其是我们知道答案应该是 `0.5`——但这算作弊）。现在似乎我们可以安全地得出关于下一次抛掷的结论，即正反面大致同样可能。但我们为什么认为可以得出这样的结论呢？
- en: What we are depending upon is the **law of large numbers** (also known as **Bernoulli's
    theorem**[^(111)](#c17-fn-0004)). This law states that in repeated independent
    tests (flips in this case) with the same actual probability `p` of a particular
    outcome in each test (e.g., an actual probability of `0.5` of getting a head for
    each flip), the chance that the fraction of times that outcome occurs differs
    from `p` converges to zero as the number of trials goes to infinity.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所依赖的是 **大数法则**（也称为 **伯努利定理**[^(111)](#c17-fn-0004)）。该法则声明，在相同实际概率 `p` 的重复独立测试中（在这种情况下为抛掷），该结果的出现频率与
    `p` 的差异概率随着试验次数趋向于无穷大而收敛为零。
- en: It is worth noting that the law of large numbers does not imply, as too many
    seem to think, that if deviations from expected behavior occur, these deviations
    are likely to be “evened out” by opposite deviations in the future. This misapplication
    of the law of large numbers is known as the **gambler's fallacy**.[^(112)](#c17-fn-0005)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，大数法则并不意味着，正如太多人所想，若出现与预期行为的偏差，这些偏差可能会被未来的相反偏差“抵消”。这种对大数法则的误用被称为 **赌徒谬误**。[^(112)](#c17-fn-0005)
- en: People often confuse the gambler's fallacy with regression to the mean. **Regression
    to the mean**[^(113)](#c17-fn-0006) states that following an extreme random event,
    the next random event is likely to be less extreme. If you were to flip a fair
    coin six times and get six heads, regression to the mean implies that the next
    sequence of six flips is likely to have closer to the expected value of three
    heads. It does not imply, as the gambler's fallacy suggests, that the next sequence
    of flips is likely to have fewer heads than tails.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 人们常常将赌徒谬误与回归均值混淆。**回归均值**[^(113)](#c17-fn-0006)表示，在经历极端随机事件后，下一个随机事件可能会更不极端。如果你抛一枚公正的硬币六次并得到六个正面，回归均值暗示下一个六次抛掷的结果更接近三个正面的期望值。它并不意味着，正如赌徒谬误所暗示的，下一次抛掷的正面数量可能少于反面数量。
- en: Success in most endeavors requires a combination of skill and luck. The skill
    component determines the mean and the luck component accounts for the variability.
    The randomness of luck leads to regression to the mean.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数事务的成功需要技能与运气的结合。技能成分决定了均值，而运气成分则解释了变异性。运气的随机性导致回归均值。
- en: The code in [Figure 17-3](#c17-fig-0003) produces a plot, [Figure 17-4](#c17-fig-0004),
    illustrating regression to the mean. The function `regress_to_mean` first generates
    `num_trials` trials of `num_flips` coin flips each. It then identifies all trials
    where the fraction of heads was either less than `1/3` or more than `2/3` and
    plots these extremal values as circles. Then, for each of these points, it plots
    the value of the subsequent trial as a triangle in the same column as the circle.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17-3](#c17-fig-0003) 中的代码生成了一个图，[图 17-4](#c17-fig-0004)，说明了回归均值。函数 `regress_to_mean`
    首先生成 `num_trials` 次试验，每次 `num_flips` 次抛硬币。然后，它识别出所有结果中正面比例低于 `1/3` 或高于 `2/3` 的试验，并将这些极端值绘制为圆圈。接着，对于每个这些点，它在与圆圈相同的列中绘制后续试验的值作为三角形。'
- en: The horizontal line at `0.5`, the expected mean, is created using the `axhline`
    function`.` The function `plt.xlim` controls the extent of the x-axis. The function
    call `plt.xlim(xmin, xmax)` sets the minimum and maximum values of the x-axis
    of the current figure. The function call `plt.xlim()` returns a tuple composed
    of the minimum and maximum values of the x-axis of the current figure. The function
    `plt.ylim` works the same way.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`0.5`处的水平线，即预期均值，是使用`axhline`函数创建的。`plt.xlim`函数控制x轴的范围。函数调用`plt.xlim(xmin,
    xmax)`设置当前图形x轴的最小值和最大值。函数调用`plt.xlim()`返回一个元组，包含当前图形x轴的最小值和最大值。`plt.ylim`函数的工作方式相同。'
- en: '![c17-fig-0003.jpg](../images/c17-fig-0003.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![c17-fig-0003.jpg](../images/c17-fig-0003.jpg)'
- en: '[Figure 17-3](#c17-fig-0003a) Regression to the mean'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 17-3](#c17-fig-0003a) 回归到均值'
- en: "``Notice that while the trial following an extreme result is typically followed\
    \ by a trial closer to the mean than the extreme result, that doesn't always occur—as\
    \ shown by the boxed pair.    **Finger exercise:** Andrea averages `5` strokes\
    \ a hole when she plays golf. One day, she took `40` strokes to complete the first\
    \ nine holes. Her partner conjectured that she would probably regress to the mean\
    \ and take `50` strokes to complete the next nine holes. Do you agree with her\
    \ partner?  ![c17-fig-0004.jpg](../images/c17-fig-0004.jpg)    [Figure 17-4](#c17-fig-0004a) Illustration\
    \ of regression to mean      [Figure 17-5](#c17-fig-0005) contains a function,\
    \ `flip_plot`, that produces two plots, [Figure 17-6](#c17-fig-0006), intended\
    \ to show the law of large numbers at work. The first plot shows how the absolute\
    \ value of the difference between the number of heads and number of tails changes\
    \ with the number of flips. The second plot compares the ratio of heads to tails\
    \ versus the number of flips. Since the numbers on the x-axis are large, we use\
    \ `plt.xticks(rotation = ‘vertical')` to rotate them.  ![c17-fig-0005.jpg](../images/c17-fig-0005.jpg)\
    \    [Figure 17-5](#c17-fig-0005a) Plotting the results of coin flips      The\
    \ line `random.seed(0)` near the bottom of the figure ensures that the pseudorandom\
    \ number generator used by `random.random` will generate the same sequence of\
    \ pseudorandom numbers each time this code is executed.[^(114)](#c17-fn-0007)\
    \ This is convenient for debugging. The function `random.seed` can be called with\
    \ any number. If it is called with no argument, the seed is chosen at random.\
    \  ![c17-fig-0006.jpg](../images/c17-fig-0006.jpg)    [Figure 17-6](#c17-fig-0006a) The\
    \ law of large numbers at work      The plot on the left seems to suggest that\
    \ the absolute difference between the number of heads and the number of tails\
    \ fluctuates in the beginning, crashes downwards, and then moves rapidly upwards.\
    \ However, we need to keep in mind that we have only two data points to the right\
    \ of `x = 300,000`. The fact that `plt.plot` connected these points with lines\
    \ may mislead us into seeing trends when all we have are isolated points. This\
    \ is not an uncommon phenomenon, so you should always ask how many points a plot\
    \ actually contains before jumping to any conclusion about what it means.    It's\
    \ hard to see much of anything in the plot on the right, which is mostly a flat\
    \ line. This too is deceptive. Even though there are 16 data points, most of them\
    \ are crowded into a small amount of real estate on the left side of the plot,\
    \ so that the detail is impossible to see. This occurs because the plotted points\
    \ have x values of `2`⁴`, 2`⁵`, 2`⁶`, …, 2`^(20), so the values on the x-axis\
    \ range from `16` to over a million, and unless instructed otherwise `plot` will\
    \ place these points based on their relative distance from the origin. This is\
    \ called **linear scaling**. Because most of the points have x values that are\
    \ small relative to `2`^(20), they will appear relatively close to the origin.\
    \    Fortunately, these visualization problems are easy to address in Python.\
    \ As we saw in Chapter 13 and earlier in this chapter, we can easily instruct\
    \ our program to plot unconnected points, e.g., by writing `plt.plot(xAxis, diffs,\
    \ 'ko')`.    Both plots in [Figure 17-7](#c17-fig-0007) use a logarithmic scale\
    \ on the x-axis. Since the x values generated by `flip_plot` are `2`^(minExp),\
    \ `2`^(minExp+1), …, `2`^(maxExp), using a logarithmic x-axis causes the points\
    \ to be evenly spaced along the x-axis—providing maximum separation between points.\
    \ The left plot in [Figure 17-7](#c17-fig-0007) uses a logarithmic scale on the\
    \ y-axis as well as on the x-axis. The y values on this plot range from nearly\
    \ `0` to around `550`. If the y-axis were linearly scaled, it would be difficult\
    \ to see the relatively small differences in y values on the left end of the plot.\
    \ On the other hand, on the plot on the right, the y values are fairly tightly\
    \ grouped, so we use a linear y-axis.  ![c17-fig-0007.jpg](../images/c17-fig-0007.jpg)\
    \    [Figure 17-7](#c17-fig-0007a) The law of large numbers at work      **Finger\
    \ exercise:** Modify the code in [Figure 17-5](#c17-fig-0005) so that it produces\
    \ plots like those shown in [Figure 17-7](#c17-fig-0007).    The plots in [Figure\
    \ 17-7](#c17-fig-0007) are easier to interpret than the earlier plots. The plot\
    \ on the right suggests strongly that the ratio of heads to tails converges to\
    \ `1.0` as the number of flips gets large. The meaning of the plot on the left\
    \ is less clear. It appears that the absolute difference grows with the number\
    \ of flips, but it is not completely convincing.    It is never possible to achieve\
    \ perfect accuracy through sampling without sampling the entire population. No\
    \ matter how many samples we examine, we can never be sure that the sample set\
    \ is typical until we examine every element of the population (and since we are\
    \ often dealing with infinite populations, e.g., all possible sequences of coin\
    \ flips, this is often impossible). Of course, this is not to say that an estimate\
    \ cannot be precisely correct. We might flip a coin twice, get one heads and one\
    \ tails, and conclude that the true probability of each is `0.5`. We would have\
    \ reached the right conclusion, but our reasoning would have been faulty.    How\
    \ many samples do we need to look at before we can have justified confidence in\
    \ our answer? This depends on the **variance** in the underlying distribution.\
    \ Roughly speaking, variance is a measure of how much spread there is in the possible\
    \ different outcomes. More formally, the variance of a collection of values, *X*,\
    \ is defined as  ![c17-fig-5002.jpg](../images/c17-fig-5002.jpg)      where |*X*|\
    \ is the size of the collection and *μ* (mu) its mean. Informally, the variance\
    \ describes what fraction of the values are close to the mean. If many values\
    \ are relatively close to the mean, the variance is relatively small. If many\
    \ values are relatively far from the mean, the variance is relatively large. If\
    \ all values are the same, the variance is zero.    The **standard deviation**\
    \ of a collection of values is the square root of the variance. While it contains\
    \ exactly the same information as the variance, the standard deviation is easier\
    \ to interpret because it is in the same units as the original data. For example,\
    \ it is easier to understand the statement “the mean height of a population is\
    \ `70` inches with a standard deviation of `4` inches,” than the sentence “the\
    \ mean of height of a population is `70` inches with a variance of `16` inches\
    \ squared.”    [Figure 17-8](#c17-fig-0009) contains implementations of variance\
    \ and standard deviation.[^(115)](#c17-fn-0008)  ![c17-fig-0008.jpg](../images/c17-fig-0008.jpg)\
    \    [Figure 17-8](#c17-fig-0009a) Variance and standard deviation      We can\
    \ use the notion of standard deviation to think about the relationship between\
    \ the number of samples we have looked at and how much confidence we should have\
    \ in the answer we have computed. [Figure 17-10](#c17-fig-0011) contains a modified\
    \ version of `flip_plot`. It uses the helper function `run_trial` to run multiple\
    \ trials of each number of coin flips, and then plots the means and standard deviations\
    \ for the `heads/tails` ratio. The helper function `make_plot`, [Figure 17-9](#c17-fig-0010),\
    \ contains the code used to produce the plots.  ![c17-fig-0009.jpg](../images/c17-fig-0009.jpg)\
    \    [Figure 17-9](#c17-fig-0010a) Helper function for coin-flipping simulation\
    \    ![c17-fig-0010.jpg](../images/c17-fig-0010.jpg)    [Figure 17-10](#c17-fig-0011a) Coin-flipping\
    \ simulation      Let's try `flip_plot1(4, 20, 20)`. It generates the plots in\
    \ [Figure 17-11](#c17-fig-0012).  ![c17-fig-0011.jpg](../images/c17-fig-0011.jpg)\
    \    [Figure 17-11](#c17-fig-0012a) Convergence of heads/tails ratios      This\
    \ is encouraging. The mean heads/tails ratio is converging towards `1` and the\
    \ log of the standard deviation is falling linearly with the log of the number\
    \ of flips per trial. By the time we get to about `10`⁶ coin flips per trial,\
    \ the standard deviation (about `10`^(-3)) is roughly three decimal orders of\
    \ magnitude smaller than the mean (about `1`), indicating that the variance across\
    \ the trials was small. We can, therefore, have considerable confidence that the\
    \ expected heads/tails ratio is quite close to `1.0`. As we flip more coins, not\
    \ only do we have a more precise answer, but more important, we also have reason\
    \ to be more confident that it is close to the right answer.    What about the\
    \ absolute difference between the number of heads and the number of tails? We\
    \ can take a look at that by adding to the end of `flip_plot1` the code in [Figure\
    \ 17-12](#c17-fig-0013). This produces the plots in [Figure 17-13](#c17-fig-0014).\
    \  ![c17-fig-0012.jpg](../images/c17-fig-0012.jpg)    [Figure 17-12](#c17-fig-0013a) Absolute\
    \ differences    ![c17-fig-0013.jpg](../images/c17-fig-0013.jpg)    [Figure 17-13](#c17-fig-0014a) Mean\
    \ and standard deviation of heads - tails      As expected, the absolute difference\
    \ between the numbers of heads and tails grows with the number of flips. Furthermore,\
    \ since we are averaging the results over 20 trials, the plot is considerably\
    \ smoother than when we plotted the results of a single trial in [Figure 17-7](#c17-fig-0007).\
    \ But what's up with the plot on the right of [Figure 17-13](#c17-fig-0014)? The\
    \ standard deviation is growing with the number of flips. Does this mean that\
    \ as the number of flips increases we should have less rather than more confidence\
    \ in the estimate of the expected value of the difference between heads and tails?\
    \    No, it does not. The standard deviation should always be viewed in the context\
    \ of the mean. If the mean were a billion and the standard deviation `100`, we\
    \ would view the dispersion of the data as small. But if the mean were `100` and\
    \ the standard deviation `100`, we would view the dispersion as large.    The\
    \ **coefficient of variation** is the standard deviation divided by the mean.\
    \ When comparing data sets with different means (as here), the coefficient of\
    \ variation is often more informative than the standard deviation. As you can\
    \ see from its implementation in [Figure 17-14](#c17-fig-0015), the coefficient\
    \ of variation is not defined when the mean is `0`.  ![c17-fig-0014.jpg](../images/c17-fig-0014.jpg)\
    \    [Figure 17-14](#c17-fig-0015a) Coefficient of variation      [Figure 17-15](#c17-fig-0016)\
    \ contains a function that plots coefficients of variation. In addition to the\
    \ plots produced by `flip_plot1`, it produces the plots in [Figure 17-16](#c17-fig-0017).\
    \  ![c17-fig-0015.jpg](../images/c17-fig-0015.jpg)    [Figure 17-15](#c17-fig-0016a) Final\
    \ version of `flip_plot`    ![c17-fig-0016.jpg](../images/c17-fig-0016.jpg)  \
    \  [Figure 17-16](#c17-fig-0017a) Coefficient of variation of heads/tails and\
    \ abs(heads – tails)      In this case we see that the plot of coefficient of\
    \ variation for the heads/tails ratio is not much different from the plot of the\
    \ standard deviation in [Figure 17-11](#c17-fig-0012). This is not surprising,\
    \ since the only difference between the two is the division by the mean, and since\
    \ the mean is close to `1`, that makes little difference.    The plot of the coefficient\
    \ of variation for the absolute difference between heads and tails is a different\
    \ story. While the standard deviation exhibited a clear trend in [Figure 17-13](#c17-fig-0014),\
    \ it would take a brave person to argue that the coefficient of variation is trending\
    \ in any direction. It seems to be fluctuating wildly. This suggests that dispersion\
    \ in the values of `abs(heads – tails)` is independent of the number of flips.\
    \ It's not growing, as the standard deviation might have misled us to believe,\
    \ but it's not shrinking either. Perhaps a trend would appear if we tried `1000`\
    \ trials instead of `20`. Let's see.    In [Figure 17-17](#c17-fig-0018), it looks\
    \ as if the coefficient of variation settles in somewhere in the neighborhood\
    \ of `0.73-0.76`. In general, distributions with a coefficient of variation of\
    \ less than `1` are considered low-variance.  ![c17-fig-0017.jpg](../images/c17-fig-0017.jpg)\
    \    [Figure 17-17](#c17-fig-0018a) A large number of trials      The main advantage\
    \ of the coefficient of variation over the standard deviation is that it allows\
    \ us to compare the dispersion of sets with different means. Consider, for example,\
    \ the distribution of weekly income in different regions of Australia, as depicted\
    \ in [Figure 17-18](#c17-fig-0019).  ![c17-fig-0018.jpg](../images/c17-fig-0018.jpg)\
    \    [Figure 17-18](#c17-fig-0019a) Income distribution in Australia      If we\
    \ use standard deviation as a measure of income inequality, it appears that there\
    \ is considerably less income inequality in Tasmania than in the ACT (Australian\
    \ Capital Territory). However, if we look at the coefficients of variation (about\
    \ 0.32 for ACT and 0.42 for Tasmania), we reach a rather different conclusion.\
    \    That isn't to say that the coefficient of variation is always more useful\
    \ than the standard deviation. If the mean is near `0`, small changes in the mean\
    \ lead to large (but not necessarily meaningful) changes in the coefficient of\
    \ variation, and when the mean is `0`, the coefficient of variation is undefined.\
    \ Also, as we shall see in Section 17.4.2, the standard deviation can be used\
    \ to construct a confidence interval, but the coefficient of variation cannot.``\
    \  [PRE6] print(scipy.integrate.quad(abs, 0, 5)[0]) [PRE7] \uFEFF`for x in range(-2,\
    \ 3):     print(gaussian(x, 0, 1)) prints` [PRE8] \uFEFF`\uFEFF0.05399096651318806\
    \ 0.24197072451914337 0.3989422804014327 0.24197072451914337 0.05399096651318806`\
    \ [PRE9] scipy.integrate.quad(gaussian, min_val, max_val, (mu, sigma))[0]) [PRE10]\
    \ \uFEFFFor mu = 2 and sigma = 7   Fraction within 1 std = 0.6827   Fraction within\
    \ 2 std = 0.9545   Fraction within 3 std = 0.9973 For mu = -9 and sigma = 5  \
    \ Fraction within 1 std = 0.6827   Fraction within 2 std = 0.9545   Fraction within\
    \ 3 std = 0.9973 For mu = 6 and sigma = 8   Fraction within 1 std = 0.6827   Fraction\
    \ within 2 std = 0.9545   Fraction within 3 std = 0.9973 [PRE11] `plt.errorbar(xVals,\
    \ means, yerr = 1.96*plt.array(sds))` [PRE12] def collision_prob(n, k):     prob\
    \ = 1.0     for i in range(1, k):         prob = prob * ((n - i)/n)     return\
    \ 1 - prob [PRE13] \uFEFFprint('Actual probability of a collision =', collision_prob(1000,\
    \ 50)) print('Est. probability of a collision =', find_prob(1000, 50, 10000))\
    \ print('Actual probability of a collision =', collision_prob(1000, 200)) print('Est.\
    \ probability of a collision =', find_prob(1000, 200, 10000)) [PRE14] \uFEFFActual\
    \ probability of a collision = 0.7122686568799875 Est. probability of a collision\
    \ = 0.7128 Actual probability of a collision = 0.9999999994781328 Est. probability\
    \ of a collision = 1.0 [PRE15]`"
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到，在极端结果后，通常会跟随一个更接近均值的试验，但这并不总是发生——如框中的一对所示。**手指练习：** 安德烈在打高尔夫时每洞平均`5`杆。一天，她完成前九洞用了`40`杆。她的搭档推测她可能会回归均值，接下来用`50`杆完成后九洞。你同意她的搭档吗？![c17-fig-0004.jpg](../images/c17-fig-0004.jpg)
    [图17-4](#c17-fig-0004a) 均值回归的插图 [图17-5](#c17-fig-0005)包含一个函数`flip_plot`，生成两个图，
    [图17-6](#c17-fig-0006)，旨在展示大数法则的应用。第一个图显示头和尾的数量差的绝对值如何随翻转次数变化。第二个图比较头与尾的比率和翻转次数。由于x轴上的数字较大，我们使用`plt.xticks(rotation
    = ‘vertical')`来旋转它们。![c17-fig-0005.jpg](../images/c17-fig-0005.jpg) [图17-5](#c17-fig-0005a) 掷硬币结果的绘图
    `random.seed(0)`在图底部确保`random.random`使用的伪随机数生成器每次执行时生成相同的伪随机数序列。这在调试时很方便。函数`random.seed`可以用任何数字调用。如果没有参数调用，则随机选择种子。![c17-fig-0006.jpg](../images/c17-fig-0006.jpg)
    [图17-6](#c17-fig-0006a) 大数法则的应用 左边的图似乎表明，头和尾的绝对差在开始时波动，随后迅速下跌，然后迅速上升。然而，我们需要记住，`x
    = 300,000`右侧只有两个数据点。`plt.plot`用线连接这些点可能会误导我们看到趋势，而实际上只有孤立点。这不是不常见的现象，所以在对图的含义做出任何结论之前，总是要询问图实际包含多少个点。
    在右侧的图中，很难看出任何内容，主要是平坦的线。这也是误导。尽管有16个数据点，但大多数都挤在图左侧的小区域中，因此细节难以看清。这是因为绘制的点的x值是`2`⁴`,
    2`⁵`, 2`⁶`, …, 2`^(20)`，因此x轴的值范围从`16`到超过一百万，除非另行指示，`plot`会根据这些点与原点的相对距离来放置它们。这被称为**线性缩放**。由于大多数点的x值相对于`2`^(20)较小，它们看起来相对接近原点。
    幸运的是，这些可视化问题在Python中很容易解决。正如我们在第13章和本章早些时候看到的，我们可以轻松指示程序绘制不连接的点，例如，通过编写`plt.plot(xAxis,
    diffs, 'ko')`。 [图17-7](#c17-fig-0007)中的两个图都在x轴上使用对数尺度。由于`flip_plot`生成的x值为`2`^(minExp),
    `2`^(minExp+1), …, `2`^(maxExp)，使用对数x轴使得点沿x轴均匀分布——提供点之间的最大分离。 [图17-7](#c17-fig-0007)左侧的图在y轴上也使用对数尺度。该图的y值范围从接近`0`到约`550`。如果y轴是线性缩放，则很难看到图左端y值的相对小差异。另一方面，右侧的图y值相对较紧凑，因此我们使用线性y轴。![c17-fig-0007.jpg](../images/c17-fig-0007.jpg)
    [图17-7](#c17-fig-0007a) 大数法则的应用 **手指练习：** 修改[图17-5](#c17-fig-0005)中的代码，使其生成如[图17-7](#c17-fig-0007)所示的图。
    [图17-7](#c17-fig-0007)中的图比早期图更容易解释。右侧的图强烈暗示，随着翻转次数增大，头与尾的比率收敛于`1.0`。左侧图的意义不太清晰。似乎绝对差随着翻转次数增长，但这并不完全令人信服。
    通过抽样实现完美的准确性永远是不可能的，除非抽样整个种群。无论我们检查多少样本，只有在检查种群的每个元素后，才能确定样本集是否典型（而且由于我们通常处理的是无限种群，例如所有可能的掷硬币序列，这往往是不可能的）。当然，这并不是说估计不能是精确正确的。我们可能掷硬币两次，得到一头一尾，得出每个结果的真实概率是`0.5`。我们会得出正确的结论，但我们的推理是错误的。
    我们需要查看多少个样本，才能对我们计算的答案有合理的信心？这取决于**方差**在潜在分布中的情况。大致而言，方差是可能不同结果的扩散程度的度量。更正式地，值集合*X*的方差定义为：![c17-fig-5002.jpg](../images/c17-fig-5002.jpg)
    其中|*X*|是集合的大小，*μ*（mu）是其均值。非正式地，方差描述有多少比例的值接近均值。如果许多值相对接近均值，方差相对较小。如果许多值相对远离均值，方差相对较大。如果所有值都相同，方差为零。
    一组值的**标准差**是方差的平方根。虽然它包含与方差完全相同的信息，但标准差更易于解释，因为它与原始数据的单位相同。例如，“一个种群的平均身高是`70`英寸，标准差是`4`英寸”的说法比“一个种群的平均身高是`70`英寸，方差是`16`平方英寸”的说法更易于理解。[图17-8](#c17-fig-0009)包含方差和标准差的实现。[^(115)](#c17-fn-0008)![c17-fig-0008.jpg](../images/c17-fig-0008.jpg)
    [图17-8](#c17-fig-0009a) 方差和标准差 我们可以利用标准差的概念来思考我们查看的样本数量与我们在计算的答案中应有的信心程度之间的关系。[图17-10](#c17-fig-0011)包含一个修改版的`flip_plot`。它使用辅助函数`run_trial`运行每个掷硬币次数的多个试验，然后绘制`头/尾`比率的均值和标准差。辅助函数`make_plot`，[图17-9](#c17-fig-0010)，包含生成图的代码。![c17-fig-0009.jpg](../images/c17-fig-0009.jpg)
    [图17-9](#c17-fig-0010a) 掷硬币模拟的辅助函数 ![c17-fig-0010.jpg](../images/c17-fig-0010.jpg)
    [图17-10](#c17-fig-0011a) 掷硬币模拟 让我们尝试`flip_plot1(4, 20, 20)`。它生成[图17-11](#c17-fig-0012)中的图。![c17-fig-0011.jpg](../images/c17-fig-0011.jpg)
    [图17-11](#c17-fig-0012a) 头尾比率的收敛 这令人鼓舞。平均头/尾比率正在收敛于`1`，标准差的对数与每次试验的翻转次数的对数线性下降。当我们进行大约`10`⁶次掷硬币时，标准差（约`10`^(-3)）大约比均值（约`1`）小三个数量级，因此我们可以相当有信心预期头/尾比率非常接近`1.0`。随着我们掷更多硬币，我们不仅得到更精确的答案，更重要的是，我们还有更多理由相信它
