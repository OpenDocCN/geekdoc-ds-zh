- en: '17'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: STOCHASTIC PROGRAMS, PROBABILITY, AND DISTRIBUTIONS
  prefs: []
  type: TYPE_NORMAL
- en: There is something comforting about Newtonian mechanics. You push down on one
    end of a lever, and the other end goes up. You throw a ball up in the air; it
    travels a parabolic path, and eventually comes down. ![c17-fig-5001.jpg](../images/c17-fig-5001.jpg).
    In short, everything happens for a reason. The physical world is a completely
    predictable place—all future states of a physical system can be derived from knowledge
    about its current state.
  prefs: []
  type: TYPE_NORMAL
- en: For centuries, this was the prevailing scientific wisdom; then along came quantum
    mechanics and the Copenhagen Doctrine. The doctrine's proponents, led by Bohr
    and Heisenberg, argued that at its most fundamental level, the behavior of the
    physical world cannot be predicted. One can make probabilistic statements of the
    form “x is highly likely to occur,” but not statements of the form “x is certain
    to occur.” Other distinguished physicists, most notably Einstein and Schrödinger,
    vehemently disagreed.
  prefs: []
  type: TYPE_NORMAL
- en: This debate roiled the worlds of physics, philosophy, and even religion. The
    heart of the debate was the validity of **causal nondeterminism**, i.e., the belief
    that not every event is caused by previous events. Einstein and Schrödinger found
    this view philosophically unacceptable, as exemplified by Einstein's often-repeated
    comment, “God does not play dice.” What they could accept was **predictive nondeterminism**,
    i.e., the concept that our inability to make accurate measurements about the physical
    world makes it impossible to make precise predictions about future states. This
    distinction was nicely summed up by Einstein, who said, “The essentially statistical
    character of contemporary theory is solely to be ascribed to the fact that this
    theory operates with an incomplete description of physical systems.”
  prefs: []
  type: TYPE_NORMAL
- en: The question of causal nondeterminism is still unsettled. However, whether the
    reason we cannot predict events is because they are truly unpredictable or because
    we simply don't have enough information to predict them is of no practical importance.
  prefs: []
  type: TYPE_NORMAL
- en: While the Bohr/Einstein debate was about how to understand the lowest levels
    of the physical world, the same issues arise at the macroscopic level. Perhaps
    the outcomes of horse races, spins of roulette wheels, and stock market investments
    are causally deterministic. However, there is ample evidence that it is perilous
    to treat them as predictably deterministic.[^(108)](#c17-fn-0001)
  prefs: []
  type: TYPE_NORMAL
- en: 17.1 Stochastic Programs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A program is **deterministic** if whenever it is run on the same input, it
    produces the same output. Notice that this is not the same as saying that the
    output is completely defined by the specification of the problem. Consider, for
    example, the specification of `square_root`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This specification admits many possible return values for the function call
    `square_root(2, 0.001)`. However, the successive approximation algorithm that
    we looked at in Chapter 3 will always return the same value. The specification
    doesn't require that the implementation be deterministic, but it does allow deterministic
    implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Not all interesting specifications can be met by deterministic implementations.
    Consider, for example, implementing a program to play a dice game, say backgammon
    or craps. Somewhere in the program will be a function that simulates a fair roll
    of a single six-sided die.[^(109)](#c17-fn-0002) Suppose it had a specification
    something like
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This would be problematic, since it allows the implementation to return the
    same number each time it is called, which would make for a pretty boring game.
    It would be better to specify that `roll_die` “`returns a randomly chosen int
    between 1 and 6`,” thus requiring a stochastic implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Most programming languages, including Python, include simple ways to write stochastic
    programs, i.e., programs that exploit randomness. The tiny program in [Figure
    17-1](#c17-fig-0001) is a simulation model. Rather than asking some person to
    roll a die multiple times, we wrote a program to simulate that activity. The code
    uses one of several useful functions found in the imported Python standard library
    module `random`. (The code in this chapter assumes that the imports
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: have all been executed.)
  prefs: []
  type: TYPE_NORMAL
- en: The function `random.choice` takes a non-empty sequence as its argument and
    returns a randomly chosen member of that sequence. Almost all of the functions
    in `random` are built using the function `random.random`, which generates a random
    floating-point number between `0`.`0` and `1.0`.[^(110)](#c17-fn-0003)
  prefs: []
  type: TYPE_NORMAL
- en: '![c17-fig-0001.jpg](../images/c17-fig-0001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 17-1](#c17-fig-0001a) Roll die'
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine running `roll_n(10)`. Would you be more surprised to see it print
    `1111111111` or `5442462412`? Or, to put it another way, which of these two sequences
    is more random? It's a trick question. Each of these sequences is equally likely,
    because the value of each roll is independent of the values of earlier rolls.
    In a stochastic process, two events are **independent** if the outcome of one
    event has no influence on the outcome of the other.
  prefs: []
  type: TYPE_NORMAL
- en: This is easier to see if we simplify the situation by thinking about a two-sided
    die (also known as a coin) with the values 0 and 1\. This allows us to think of
    the output of a call of `roll_n` as a binary number. When we use a binary die,
    there are `2`^n possible sequences that `roll_n` might return. Each of these sequences
    is equally likely; therefore each has a probability of occurring of `(1/2)`^n.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go back to our six-sided die. How many different sequences are there of
    length `10`? `6`^(10). So, the probability of rolling 10 consecutive `1`'s is
    `1/6`^(10). Less than one out of 60 million. Pretty low, but no lower than the
    probability of any other sequence, e.g., `5442462412`.
  prefs: []
  type: TYPE_NORMAL
- en: 17.2 Calculating Simple Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In general, when we talk about the **probability** of a result having some property
    (e.g., all `1`'s), we are asking what fraction of all possible results has that
    property. This is why probabilities range from 0 to 1\. Suppose we want to know
    the probability of getting any sequence other than all `1`'s when rolling the
    die. It is simply `1 – (1/6`^(10)`)`, because the probability of something happening
    and the probability of the same thing not happening must add up to `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to know the probability of rolling the die 10 times without
    getting a single `1`. One way to answer this question is to transform it into
    the question of how many of the `6^(10)` possible sequences don''t contain a `1`.
    This can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The probability of not rolling a `1` on any single roll is `5/6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 2\. The probability of not rolling a `1` on either the first or the second roll
    is `(5/6)`*`(5/6)`, or `(5/6)`².
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 3\. So, the probability of not rolling a `1` 10 times in a row is `(5/6)`^(10),
    slightly more than `0.16`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Step 2 is an application of the **multiplicative law** for independent probabilities.
    Consider, for example, two independent events `A` and `B`. If `A` occurs one `1/3`
    of the time and `B` occurs `1/4` of the time, the probability that both `A` and
    `B` occur is `1/4 of 1/3`, i.e., `(1/3)/4` or `(1/3)*(1/4).`
  prefs: []
  type: TYPE_NORMAL
- en: What about the probability of rolling at least one `1`? It is simply 1 minus
    the probability of not rolling at least one `1`, i.e., `1` - `(5/6)`^(10). Notice
    that this cannot be correctly computed by saying that the probability of rolling
    a `1` on any roll is `1/6`, so the probability of rolling at least one `1` is
    `10*(1/6)`, i.e., `10/6`. This is obviously incorrect since a probability cannot
    be greater than `1`.
  prefs: []
  type: TYPE_NORMAL
- en: How about the probability of rolling exactly two `1's` in 10 rolls? This is
    equivalent to asking what fraction of the first `6^(10)` integers has exactly
    two `1's` in its base `6` representation. We could easily write a program to generate
    all of these sequences and count the number that contained exactly one `1`. Deriving
    the probability analytically is a bit tricky, and we defer it to Section 17.4.4.
  prefs: []
  type: TYPE_NORMAL
- en: 17.3 Inferential Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you just saw, we can use a systematic process to derive the precise probability
    of some complex event based upon knowing the probability of one or more simpler
    events. For example, we can easily compute the probability of flipping a coin
    and getting 10 consecutive heads based on the assumption that flips are independent
    and we know the probability of each flip coming up heads. Suppose, however, that
    we don't actually know the probability of the relevant simpler event. Suppose,
    for example, that we don't know whether the coin is fair (i.e., a coin where heads
    and tails are equally likely).
  prefs: []
  type: TYPE_NORMAL
- en: All is not lost. If we have some data about the behavior of the coin, we can
    combine that data with our knowledge of probability to derive an estimate of the
    true probability. We can use **inferential statistics** to estimate the probability
    of a single flip coming up heads, and then conventional probability to compute
    the probability of a coin with that behavior coming up heads 10 times in a row.
  prefs: []
  type: TYPE_NORMAL
- en: In brief (since this is not a book about statistics), the guiding principle
    of inferential statistics is that a random sample tends to exhibit the same properties
    as the population from which it is drawn.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose Harvey Dent (also known as Two-Face) flipped a coin, and it came up
    heads. You would not infer from this that the next flip would also come up heads.
    Suppose he flipped it twice, and it came up heads both times. You might reason
    that the probability of this happening for a fair coin was `0.25`, so there was
    still no reason to assume the next flip would be heads. Suppose, however, `100`
    out of `100` flips came up heads. The number `(1/2)`^(100) (the probability of
    this event, assuming a fair coin) is pretty small, so you might feel safe in inferring
    that the coin has a head on both sides.
  prefs: []
  type: TYPE_NORMAL
- en: Your belief in whether the coin is fair is based on the intuition that the behavior
    of a single sample of `100` flips is similar to the behavior of the population
    of all samples of `100` flips. This belief seems appropriate when all `100` flips
    are heads. Suppose that `52` flips came up heads and `48` tails. Would you feel
    comfortable in predicting that the next `100` flips would have the same ratio
    of heads to tails? For that matter, how comfortable would you feel about even
    predicting that there would be more heads than tails in the next `100` flips?
    Take a few minutes to think about this, and then try the experiment. Or, if you
    don't happen to have a coin handy, simulate the flips using the code in [Figure
    17-2](#c17-fig-0002).
  prefs: []
  type: TYPE_NORMAL
- en: The function `flip` in [Figure 17-2](#c17-fig-0002) simulates flipping a fair
    coin `num_flips` times and returns the fraction of those flips that came up heads.
    For each flip, the call `random.choice(('H', ‘T'))` randomly returns either `'H'`
    or `'T'.`
  prefs: []
  type: TYPE_NORMAL
- en: '![c17-fig-0002.jpg](../images/c17-fig-0002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 17-2](#c17-fig-0002a) Flipping a coin'
  prefs: []
  type: TYPE_NORMAL
- en: 'Try executing the function `flip_sim(10, 1)` a couple of times. Here''s what
    we saw the first two times we tried `print(''Mean ='', flip_sim(10, 1))`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that it would be inappropriate to assume much (other than that the
    coin has both heads and tails) from any one trial of `10` flips. That''s why we
    typically structure our simulations to include multiple trials and compare the
    results. Let''s try `flip_sim(10, 100)` a couple of times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Do you feel better about these results? When we tried `flip_sim(100, 100000)`,
    we got
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This looks really good (especially since we know that the answer should be `0.5`—but
    that's cheating). Now it seems we can safely conclude something about the next
    flip, i.e., that heads and tails are about equally likely. But why do we think
    that we can conclude that?
  prefs: []
  type: TYPE_NORMAL
- en: What we are depending upon is the **law of large numbers** (also known as **Bernoulli's
    theorem**[^(111)](#c17-fn-0004)). This law states that in repeated independent
    tests (flips in this case) with the same actual probability `p` of a particular
    outcome in each test (e.g., an actual probability of `0.5` of getting a head for
    each flip), the chance that the fraction of times that outcome occurs differs
    from `p` converges to zero as the number of trials goes to infinity.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the law of large numbers does not imply, as too many
    seem to think, that if deviations from expected behavior occur, these deviations
    are likely to be “evened out” by opposite deviations in the future. This misapplication
    of the law of large numbers is known as the **gambler's fallacy**.[^(112)](#c17-fn-0005)
  prefs: []
  type: TYPE_NORMAL
- en: People often confuse the gambler's fallacy with regression to the mean. **Regression
    to the mean**[^(113)](#c17-fn-0006) states that following an extreme random event,
    the next random event is likely to be less extreme. If you were to flip a fair
    coin six times and get six heads, regression to the mean implies that the next
    sequence of six flips is likely to have closer to the expected value of three
    heads. It does not imply, as the gambler's fallacy suggests, that the next sequence
    of flips is likely to have fewer heads than tails.
  prefs: []
  type: TYPE_NORMAL
- en: Success in most endeavors requires a combination of skill and luck. The skill
    component determines the mean and the luck component accounts for the variability.
    The randomness of luck leads to regression to the mean.
  prefs: []
  type: TYPE_NORMAL
- en: The code in [Figure 17-3](#c17-fig-0003) produces a plot, [Figure 17-4](#c17-fig-0004),
    illustrating regression to the mean. The function `regress_to_mean` first generates
    `num_trials` trials of `num_flips` coin flips each. It then identifies all trials
    where the fraction of heads was either less than `1/3` or more than `2/3` and
    plots these extremal values as circles. Then, for each of these points, it plots
    the value of the subsequent trial as a triangle in the same column as the circle.
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal line at `0.5`, the expected mean, is created using the `axhline`
    function`.` The function `plt.xlim` controls the extent of the x-axis. The function
    call `plt.xlim(xmin, xmax)` sets the minimum and maximum values of the x-axis
    of the current figure. The function call `plt.xlim()` returns a tuple composed
    of the minimum and maximum values of the x-axis of the current figure. The function
    `plt.ylim` works the same way.
  prefs: []
  type: TYPE_NORMAL
- en: '![c17-fig-0003.jpg](../images/c17-fig-0003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 17-3](#c17-fig-0003a) Regression to the mean'
  prefs: []
  type: TYPE_NORMAL
- en: "``Notice that while the trial following an extreme result is typically followed\
    \ by a trial closer to the mean than the extreme result, that doesn't always occur—as\
    \ shown by the boxed pair.    **Finger exercise:** Andrea averages `5` strokes\
    \ a hole when she plays golf. One day, she took `40` strokes to complete the first\
    \ nine holes. Her partner conjectured that she would probably regress to the mean\
    \ and take `50` strokes to complete the next nine holes. Do you agree with her\
    \ partner?  ![c17-fig-0004.jpg](../images/c17-fig-0004.jpg)    [Figure 17-4](#c17-fig-0004a) Illustration\
    \ of regression to mean      [Figure 17-5](#c17-fig-0005) contains a function,\
    \ `flip_plot`, that produces two plots, [Figure 17-6](#c17-fig-0006), intended\
    \ to show the law of large numbers at work. The first plot shows how the absolute\
    \ value of the difference between the number of heads and number of tails changes\
    \ with the number of flips. The second plot compares the ratio of heads to tails\
    \ versus the number of flips. Since the numbers on the x-axis are large, we use\
    \ `plt.xticks(rotation = ‘vertical')` to rotate them.  ![c17-fig-0005.jpg](../images/c17-fig-0005.jpg)\
    \    [Figure 17-5](#c17-fig-0005a) Plotting the results of coin flips      The\
    \ line `random.seed(0)` near the bottom of the figure ensures that the pseudorandom\
    \ number generator used by `random.random` will generate the same sequence of\
    \ pseudorandom numbers each time this code is executed.[^(114)](#c17-fn-0007)\
    \ This is convenient for debugging. The function `random.seed` can be called with\
    \ any number. If it is called with no argument, the seed is chosen at random.\
    \  ![c17-fig-0006.jpg](../images/c17-fig-0006.jpg)    [Figure 17-6](#c17-fig-0006a) The\
    \ law of large numbers at work      The plot on the left seems to suggest that\
    \ the absolute difference between the number of heads and the number of tails\
    \ fluctuates in the beginning, crashes downwards, and then moves rapidly upwards.\
    \ However, we need to keep in mind that we have only two data points to the right\
    \ of `x = 300,000`. The fact that `plt.plot` connected these points with lines\
    \ may mislead us into seeing trends when all we have are isolated points. This\
    \ is not an uncommon phenomenon, so you should always ask how many points a plot\
    \ actually contains before jumping to any conclusion about what it means.    It's\
    \ hard to see much of anything in the plot on the right, which is mostly a flat\
    \ line. This too is deceptive. Even though there are 16 data points, most of them\
    \ are crowded into a small amount of real estate on the left side of the plot,\
    \ so that the detail is impossible to see. This occurs because the plotted points\
    \ have x values of `2`⁴`, 2`⁵`, 2`⁶`, …, 2`^(20), so the values on the x-axis\
    \ range from `16` to over a million, and unless instructed otherwise `plot` will\
    \ place these points based on their relative distance from the origin. This is\
    \ called **linear scaling**. Because most of the points have x values that are\
    \ small relative to `2`^(20), they will appear relatively close to the origin.\
    \    Fortunately, these visualization problems are easy to address in Python.\
    \ As we saw in Chapter 13 and earlier in this chapter, we can easily instruct\
    \ our program to plot unconnected points, e.g., by writing `plt.plot(xAxis, diffs,\
    \ 'ko')`.    Both plots in [Figure 17-7](#c17-fig-0007) use a logarithmic scale\
    \ on the x-axis. Since the x values generated by `flip_plot` are `2`^(minExp),\
    \ `2`^(minExp+1), …, `2`^(maxExp), using a logarithmic x-axis causes the points\
    \ to be evenly spaced along the x-axis—providing maximum separation between points.\
    \ The left plot in [Figure 17-7](#c17-fig-0007) uses a logarithmic scale on the\
    \ y-axis as well as on the x-axis. The y values on this plot range from nearly\
    \ `0` to around `550`. If the y-axis were linearly scaled, it would be difficult\
    \ to see the relatively small differences in y values on the left end of the plot.\
    \ On the other hand, on the plot on the right, the y values are fairly tightly\
    \ grouped, so we use a linear y-axis.  ![c17-fig-0007.jpg](../images/c17-fig-0007.jpg)\
    \    [Figure 17-7](#c17-fig-0007a) The law of large numbers at work      **Finger\
    \ exercise:** Modify the code in [Figure 17-5](#c17-fig-0005) so that it produces\
    \ plots like those shown in [Figure 17-7](#c17-fig-0007).    The plots in [Figure\
    \ 17-7](#c17-fig-0007) are easier to interpret than the earlier plots. The plot\
    \ on the right suggests strongly that the ratio of heads to tails converges to\
    \ `1.0` as the number of flips gets large. The meaning of the plot on the left\
    \ is less clear. It appears that the absolute difference grows with the number\
    \ of flips, but it is not completely convincing.    It is never possible to achieve\
    \ perfect accuracy through sampling without sampling the entire population. No\
    \ matter how many samples we examine, we can never be sure that the sample set\
    \ is typical until we examine every element of the population (and since we are\
    \ often dealing with infinite populations, e.g., all possible sequences of coin\
    \ flips, this is often impossible). Of course, this is not to say that an estimate\
    \ cannot be precisely correct. We might flip a coin twice, get one heads and one\
    \ tails, and conclude that the true probability of each is `0.5`. We would have\
    \ reached the right conclusion, but our reasoning would have been faulty.    How\
    \ many samples do we need to look at before we can have justified confidence in\
    \ our answer? This depends on the **variance** in the underlying distribution.\
    \ Roughly speaking, variance is a measure of how much spread there is in the possible\
    \ different outcomes. More formally, the variance of a collection of values, *X*,\
    \ is defined as  ![c17-fig-5002.jpg](../images/c17-fig-5002.jpg)      where |*X*|\
    \ is the size of the collection and *μ* (mu) its mean. Informally, the variance\
    \ describes what fraction of the values are close to the mean. If many values\
    \ are relatively close to the mean, the variance is relatively small. If many\
    \ values are relatively far from the mean, the variance is relatively large. If\
    \ all values are the same, the variance is zero.    The **standard deviation**\
    \ of a collection of values is the square root of the variance. While it contains\
    \ exactly the same information as the variance, the standard deviation is easier\
    \ to interpret because it is in the same units as the original data. For example,\
    \ it is easier to understand the statement “the mean height of a population is\
    \ `70` inches with a standard deviation of `4` inches,” than the sentence “the\
    \ mean of height of a population is `70` inches with a variance of `16` inches\
    \ squared.”    [Figure 17-8](#c17-fig-0009) contains implementations of variance\
    \ and standard deviation.[^(115)](#c17-fn-0008)  ![c17-fig-0008.jpg](../images/c17-fig-0008.jpg)\
    \    [Figure 17-8](#c17-fig-0009a) Variance and standard deviation      We can\
    \ use the notion of standard deviation to think about the relationship between\
    \ the number of samples we have looked at and how much confidence we should have\
    \ in the answer we have computed. [Figure 17-10](#c17-fig-0011) contains a modified\
    \ version of `flip_plot`. It uses the helper function `run_trial` to run multiple\
    \ trials of each number of coin flips, and then plots the means and standard deviations\
    \ for the `heads/tails` ratio. The helper function `make_plot`, [Figure 17-9](#c17-fig-0010),\
    \ contains the code used to produce the plots.  ![c17-fig-0009.jpg](../images/c17-fig-0009.jpg)\
    \    [Figure 17-9](#c17-fig-0010a) Helper function for coin-flipping simulation\
    \    ![c17-fig-0010.jpg](../images/c17-fig-0010.jpg)    [Figure 17-10](#c17-fig-0011a) Coin-flipping\
    \ simulation      Let's try `flip_plot1(4, 20, 20)`. It generates the plots in\
    \ [Figure 17-11](#c17-fig-0012).  ![c17-fig-0011.jpg](../images/c17-fig-0011.jpg)\
    \    [Figure 17-11](#c17-fig-0012a) Convergence of heads/tails ratios      This\
    \ is encouraging. The mean heads/tails ratio is converging towards `1` and the\
    \ log of the standard deviation is falling linearly with the log of the number\
    \ of flips per trial. By the time we get to about `10`⁶ coin flips per trial,\
    \ the standard deviation (about `10`^(-3)) is roughly three decimal orders of\
    \ magnitude smaller than the mean (about `1`), indicating that the variance across\
    \ the trials was small. We can, therefore, have considerable confidence that the\
    \ expected heads/tails ratio is quite close to `1.0`. As we flip more coins, not\
    \ only do we have a more precise answer, but more important, we also have reason\
    \ to be more confident that it is close to the right answer.    What about the\
    \ absolute difference between the number of heads and the number of tails? We\
    \ can take a look at that by adding to the end of `flip_plot1` the code in [Figure\
    \ 17-12](#c17-fig-0013). This produces the plots in [Figure 17-13](#c17-fig-0014).\
    \  ![c17-fig-0012.jpg](../images/c17-fig-0012.jpg)    [Figure 17-12](#c17-fig-0013a) Absolute\
    \ differences    ![c17-fig-0013.jpg](../images/c17-fig-0013.jpg)    [Figure 17-13](#c17-fig-0014a) Mean\
    \ and standard deviation of heads - tails      As expected, the absolute difference\
    \ between the numbers of heads and tails grows with the number of flips. Furthermore,\
    \ since we are averaging the results over 20 trials, the plot is considerably\
    \ smoother than when we plotted the results of a single trial in [Figure 17-7](#c17-fig-0007).\
    \ But what's up with the plot on the right of [Figure 17-13](#c17-fig-0014)? The\
    \ standard deviation is growing with the number of flips. Does this mean that\
    \ as the number of flips increases we should have less rather than more confidence\
    \ in the estimate of the expected value of the difference between heads and tails?\
    \    No, it does not. The standard deviation should always be viewed in the context\
    \ of the mean. If the mean were a billion and the standard deviation `100`, we\
    \ would view the dispersion of the data as small. But if the mean were `100` and\
    \ the standard deviation `100`, we would view the dispersion as large.    The\
    \ **coefficient of variation** is the standard deviation divided by the mean.\
    \ When comparing data sets with different means (as here), the coefficient of\
    \ variation is often more informative than the standard deviation. As you can\
    \ see from its implementation in [Figure 17-14](#c17-fig-0015), the coefficient\
    \ of variation is not defined when the mean is `0`.  ![c17-fig-0014.jpg](../images/c17-fig-0014.jpg)\
    \    [Figure 17-14](#c17-fig-0015a) Coefficient of variation      [Figure 17-15](#c17-fig-0016)\
    \ contains a function that plots coefficients of variation. In addition to the\
    \ plots produced by `flip_plot1`, it produces the plots in [Figure 17-16](#c17-fig-0017).\
    \  ![c17-fig-0015.jpg](../images/c17-fig-0015.jpg)    [Figure 17-15](#c17-fig-0016a) Final\
    \ version of `flip_plot`    ![c17-fig-0016.jpg](../images/c17-fig-0016.jpg)  \
    \  [Figure 17-16](#c17-fig-0017a) Coefficient of variation of heads/tails and\
    \ abs(heads – tails)      In this case we see that the plot of coefficient of\
    \ variation for the heads/tails ratio is not much different from the plot of the\
    \ standard deviation in [Figure 17-11](#c17-fig-0012). This is not surprising,\
    \ since the only difference between the two is the division by the mean, and since\
    \ the mean is close to `1`, that makes little difference.    The plot of the coefficient\
    \ of variation for the absolute difference between heads and tails is a different\
    \ story. While the standard deviation exhibited a clear trend in [Figure 17-13](#c17-fig-0014),\
    \ it would take a brave person to argue that the coefficient of variation is trending\
    \ in any direction. It seems to be fluctuating wildly. This suggests that dispersion\
    \ in the values of `abs(heads – tails)` is independent of the number of flips.\
    \ It's not growing, as the standard deviation might have misled us to believe,\
    \ but it's not shrinking either. Perhaps a trend would appear if we tried `1000`\
    \ trials instead of `20`. Let's see.    In [Figure 17-17](#c17-fig-0018), it looks\
    \ as if the coefficient of variation settles in somewhere in the neighborhood\
    \ of `0.73-0.76`. In general, distributions with a coefficient of variation of\
    \ less than `1` are considered low-variance.  ![c17-fig-0017.jpg](../images/c17-fig-0017.jpg)\
    \    [Figure 17-17](#c17-fig-0018a) A large number of trials      The main advantage\
    \ of the coefficient of variation over the standard deviation is that it allows\
    \ us to compare the dispersion of sets with different means. Consider, for example,\
    \ the distribution of weekly income in different regions of Australia, as depicted\
    \ in [Figure 17-18](#c17-fig-0019).  ![c17-fig-0018.jpg](../images/c17-fig-0018.jpg)\
    \    [Figure 17-18](#c17-fig-0019a) Income distribution in Australia      If we\
    \ use standard deviation as a measure of income inequality, it appears that there\
    \ is considerably less income inequality in Tasmania than in the ACT (Australian\
    \ Capital Territory). However, if we look at the coefficients of variation (about\
    \ 0.32 for ACT and 0.42 for Tasmania), we reach a rather different conclusion.\
    \    That isn't to say that the coefficient of variation is always more useful\
    \ than the standard deviation. If the mean is near `0`, small changes in the mean\
    \ lead to large (but not necessarily meaningful) changes in the coefficient of\
    \ variation, and when the mean is `0`, the coefficient of variation is undefined.\
    \ Also, as we shall see in Section 17.4.2, the standard deviation can be used\
    \ to construct a confidence interval, but the coefficient of variation cannot.``\
    \  [PRE6] print(scipy.integrate.quad(abs, 0, 5)[0]) [PRE7] \uFEFF`for x in range(-2,\
    \ 3):     print(gaussian(x, 0, 1)) prints` [PRE8] \uFEFF`\uFEFF0.05399096651318806\
    \ 0.24197072451914337 0.3989422804014327 0.24197072451914337 0.05399096651318806`\
    \ [PRE9] scipy.integrate.quad(gaussian, min_val, max_val, (mu, sigma))[0]) [PRE10]\
    \ \uFEFFFor mu = 2 and sigma = 7   Fraction within 1 std = 0.6827   Fraction within\
    \ 2 std = 0.9545   Fraction within 3 std = 0.9973 For mu = -9 and sigma = 5  \
    \ Fraction within 1 std = 0.6827   Fraction within 2 std = 0.9545   Fraction within\
    \ 3 std = 0.9973 For mu = 6 and sigma = 8   Fraction within 1 std = 0.6827   Fraction\
    \ within 2 std = 0.9545   Fraction within 3 std = 0.9973 [PRE11] `plt.errorbar(xVals,\
    \ means, yerr = 1.96*plt.array(sds))` [PRE12] def collision_prob(n, k):     prob\
    \ = 1.0     for i in range(1, k):         prob = prob * ((n - i)/n)     return\
    \ 1 - prob [PRE13] \uFEFFprint('Actual probability of a collision =', collision_prob(1000,\
    \ 50)) print('Est. probability of a collision =', find_prob(1000, 50, 10000))\
    \ print('Actual probability of a collision =', collision_prob(1000, 200)) print('Est.\
    \ probability of a collision =', find_prob(1000, 200, 10000)) [PRE14] \uFEFFActual\
    \ probability of a collision = 0.7122686568799875 Est. probability of a collision\
    \ = 0.7128 Actual probability of a collision = 0.9999999994781328 Est. probability\
    \ of a collision = 1.0 [PRE15]`"
  prefs: []
  type: TYPE_NORMAL
