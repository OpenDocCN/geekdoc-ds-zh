- en: Cache Associativity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缓存关联性
- en: 原文：[https://en.algorithmica.org/hpc/cpu-cache/associativity/](https://en.algorithmica.org/hpc/cpu-cache/associativity/)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://en.algorithmica.org/hpc/cpu-cache/associativity/](https://en.algorithmica.org/hpc/cpu-cache/associativity/)
- en: 'Consider a [strided incrementing loop](../cache-lines) over an array of size
    $N=2^{21}$ with a fixed step size of 256:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个大小为 $N=2^{21}$ 的数组上的[步进增量循环](../cache-lines)，步长为256：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'And then this one, with the step size of 257:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这个，步长为257：
- en: '[PRE1]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Which one will be faster to finish? There are several considerations that come
    to mind:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个会更快完成？有几个考虑因素浮现在脑海中：
- en: At first, you think that there shouldn’t be much difference, or maybe that the
    second loop is $\frac{257}{256}$ times faster or so because it does fewer iterations
    in total.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起初，你认为两者之间不会有太大差异，或者可能认为第二个循环比第一个快$\frac{257}{256}$倍左右，因为它总的迭代次数更少。
- en: Then you recall that 256 is a nice round number, which may have something to
    do with [SIMD](/hpc/simd) or the memory system, so maybe the first one is faster.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后你回想起256是一个很好的整数，这可能和[SIMD](/hpc/simd)或内存系统有关，所以也许第一个更快。
- en: 'But the right answer is very counterintuitive: the second loop is faster —
    and by a factor of 10.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但正确答案非常反直觉：第二个循环更快——并且快了10倍。
- en: 'This isn’t just a single bad step size. The performance degrades for all indices
    that are multiples of large powers of two:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是一个单一的坏步长。对于所有是大型2的幂次倍数的索引，性能都会下降：
- en: '![](../Images/fd3bd4ae9f3ac9c5c9dbe475ac5b327f.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/fd3bd4ae9f3ac9c5c9dbe475ac5b327f.png)'
- en: The array size is normalized so that the total number of iterations is constant
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 数组大小已标准化，以便总迭代次数保持不变
- en: There is no vectorization or anything, and the two loops produce the same assembly
    except for the step size. This effect is due only to the memory system, in particular
    to a feature called *cache associativity*, which is a peculiar artifact of how
    CPU caches are implemented in hardware.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 没有向量化或其他任何操作，这两个循环除了步长外，生成的汇编代码相同。这种效果仅归因于内存系统，特别是称为*缓存关联性*的特性，这是CPU缓存在硬件中实现的一个特殊产物。
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/associativity/#hardware-caches)Hardware
    Caches'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/cpu-cache/associativity/#hardware-caches)硬件缓存'
- en: When we were studying the memory system [theoretically](/hpc/external-memory),
    we discussed different ways one can [implement cache eviction policies](/hpc/external-memory/policies/)
    in software. One particular strategy we focused on was the *least recently used*
    (LRU) policy, which is simple and effective but still requires some non-trivial
    data manipulation.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们研究内存系统[理论](/hpc/external-memory)时，我们讨论了在软件中实现不同的缓存淘汰策略的方法。[实现缓存淘汰策略](/hpc/external-memory/policies/)。我们特别关注的一种策略是*最近最少使用*（LRU）策略，它简单有效，但仍需要一些非平凡的数据操作。
- en: 'In the context of hardware, such scheme is called *fully associative cache*:
    we have $M$ cells, each capable of holding a cache line corresponding to any of
    the $N$ total memory locations, and in case of contention, the one not accessed
    the longest gets kicked out and replaced with the new one.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在硬件的背景下，这种方案被称为*全关联缓存*：我们有$M$个单元格，每个单元格都能容纳一个与$N$个总内存位置中的任何一个相对应的缓存行，在发生冲突的情况下，最长时间未被访问的那个被踢出，并用新的一个替换。
- en: '![](../Images/e6a3d810be4f3b41f2bda49d6a0374f5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/e6a3d810be4f3b41f2bda49d6a0374f5.png)'
- en: Fully associative cache
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 全关联缓存
- en: The problem with fully associative cache is that implementing the “find the
    oldest cache line among millions” operation is pretty hard to do in software and
    just unfeasible in hardware. You can make a fully associative cache that has 16
    entries or so, but managing hundreds of cache lines already becomes either prohibitively
    expensive or so slow that it’s not worth it.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 全关联缓存的缺点是，在软件中实现“在数百万个缓存行中找到最旧的缓存行”的操作相当困难，在硬件中则完全不可行。你可以制作一个大约有16个条目的全关联缓存，但管理数百个缓存行已经变得要么过于昂贵，要么速度太慢，以至于不值得。
- en: 'We can resort to another, much simpler approach: just map each block of 64
    bytes in RAM to a single cache line which it can occupy. Say, if we have 4096
    blocks in memory and 64 cache lines for them, then each cache line at any time
    stores the contents of one of $\frac{4096}{64} = 64$ different blocks.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采取另一种更简单的方法：只需将RAM中的每个64字节块映射到一条单独的缓存行，它可以占用。比如说，如果我们有4096个内存块和64条缓存行，那么在任何时候，每条缓存行都存储着64个不同块中的内容，即
    $\frac{4096}{64} = 64$。
- en: '![](../Images/30793ef9a0d9d81ad7f434be1de5e2b2.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/30793ef9a0d9d81ad7f434be1de5e2b2.png)'
- en: Direct-mapped cache
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 直接映射缓存
- en: A direct-mapped cache is easy to implement doesn’t require storing any additional
    meta-information associated with a cache line except its tag (the actual memory
    location of a cached block). The disadvantage is that the entries can be kicked
    out too quickly — for example, when bouncing between two addresses that map to
    the same cache line — leading to lower overall cache utilization.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 直接映射缓存易于实现，不需要存储与缓存行相关联的任何其他元信息，除了标签（缓存的内存块的实际位置）。缺点是条目可能会被太快地踢出 — 例如，当在映射到同一缓存行的两个地址之间弹跳时
    — 导致整体缓存利用率降低。
- en: 'For that reason, we settle for something in-between direct-mapped and fully
    associative caches: the *set-associative cache*. It splits the address space into
    equal groups, which separately act as small fully-associative caches.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们选择了介于直接映射和完全相联缓存之间的方案：*组相联缓存*。它将地址空间分成相等的组，这些组分别作为小型完全相联缓存。
- en: '![](../Images/884707601cc8c5f89f43f871272d0e12.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/884707601cc8c5f89f43f871272d0e12.png)'
- en: Set-associative cache (2-way associative)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 组相联缓存（2 路相联）
- en: '*Associativity* is the size of these sets, or, in other words, how many different
    cache lines each data block can be mapped to. Higher associativity allows for
    more efficient utilization of cache but also increases the cost.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*相联性* 是这些组的大小，或者说，每个数据块可以映射到多少不同的缓存行。更高的相联性允许更有效地利用缓存，但也会增加成本。'
- en: For example, on [my CPU](https://en.wikichip.org/wiki/amd/ryzen_7/4700u), the
    L3 cache is 16-way set-associative, and there are 4MB available to a single core.
    This means that there are in total $\frac{2^{22}}{2^{6}} = 2^{16}$ cache lines,
    which are split into $\frac{2^{16}}{16} = 2^{12}$ groups, each acting as a fully
    associative cache of their own $(\frac{1}{2^{12}})$-th fraction of the RAM.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在我的[CPU](https://en.wikichip.org/wiki/amd/ryzen_7/4700u)上，L3 缓存是 16 路组相联的，每个核心有
    4MB 可用。这意味着总共有 $\frac{2^{22}}{2^{6}} = 2^{16}$ 个缓存行，这些行被分成 $\frac{2^{16}}{16}
    = 2^{12}$ 组，每组作为一个完全相联的缓存，占其自身 RAM 的 $(\frac{1}{2^{12}})$ 部分。
- en: Most other CPU caches are also set-associative, including the non-data ones
    such as the instruction cache and the TLB. The exceptions are small specialized
    caches that only house 64 or fewer entries — these are usually fully associative.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数其他 CPU 缓存也是组相联的，包括非数据缓存，如指令缓存和 TLB。例外的是只有 64 个或更少条目的小型专用缓存 — 这些通常是完全相联的。
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/associativity/#address-translation)Address
    Translation'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/cpu-cache/associativity/#address-translation)地址转换'
- en: 'There is only one ambiguity remaining: how exactly the cache line mapping is
    done.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的唯一歧义是缓存行映射的确切方式。
- en: 'If we implemented set-associative cache in software, we would compute some
    hash function of the memory block address and then use its value as the cache
    line index. In hardware, we can’t really do that because it is too slow: for example,
    for the L1 cache, the latency requirement is 4 or 5 cycles, and even [taking a
    modulo](/hpc/arithmetic/division) takes around 10-15 cycles, let alone something
    more sophisticated.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在软件中实现组相联缓存，我们将计算内存块地址的某些哈希函数，然后使用其值作为缓存行索引。在硬件中，我们实际上无法做到这一点，因为它太慢了：例如，对于
    L1 缓存，延迟要求是 4 或 5 个周期，即使是[取模](/hpc/arithmetic/division)也需要大约 10-15 个周期，更不用说更复杂的事情了。
- en: 'Instead, the hardware uses the lazy approach. It takes the memory address that
    needs to be accessed and splits it into three parts — from lower bits to higher:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，硬件采用懒惰的方法。它将需要访问的内存地址分成三部分 — 从低位到高位：
- en: '*offset* — the index of the word within a 64B cache line ($\log_2 64 = 6$ bits);'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*偏移量* — 64B 缓存行中字节的索引（$\log_2 64 = 6$ 位）；'
- en: '*index* — the index of the cache line set (the next $12$ bits as there are
    $2^{12}$ cache lines in the L3 cache);'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*索引* — 缓存行组的索引（由于 L3 缓存中有 $2^{12}$ 个缓存行，所以是下一个 $12$ 位）；'
- en: '*tag* — the rest of the memory address, which is used to tell the memory blocks
    stored in the cache lines apart.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*标签* — 剩余的内存地址，用于区分存储在缓存行中的内存块。'
- en: In other words, all memory addresses with the same “middle” part map to the
    same set.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，所有具有相同“中间”部分的内存地址都映射到同一个组。
- en: '![](../Images/1adadf7dfa9e8d2fd9ec5374823d419d.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/1adadf7dfa9e8d2fd9ec5374823d419d.png)'
- en: Address composition for a 64-entry 2-way set-associative cache
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 64 条条目的 2 路组相联缓存的地址组成
- en: This makes the cache system simpler and cheaper to implement but also susceptible
    to certain bad access patterns.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得缓存系统更简单、更便宜，但同时也容易受到某些不良访问模式的影响。
- en: '### [#](https://en.algorithmica.org/hpc/cpu-cache/associativity/#pathological-mappings)Pathological
    Mappings'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '### [#](https://en.algorithmica.org/hpc/cpu-cache/associativity/#pathological-mappings)病态映射'
- en: 'Now, where were we? Oh, yes: the reason why iteration with strides of 256 causes
    such a terrible slowdown.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们刚才说到哪里了？哦，是的：迭代步长为256导致如此严重的减速的原因。
- en: When we jump over 256 integers, the pointer always increments by $1024 = 2^{10}$,
    and the last 10 bits remain the same. Since the cache system uses the lower 6
    bits for the offset and the next 12 for the cache line index, we are essentially
    using just $2^{12 - (10 - 6)} = 2^8$ different sets in the L3 cache instead of
    $2^{12}$, which has the effect of shrinking our L3 cache by a factor of $2^4 =
    16$. The array stops fitting into the L3 cache ($N=2^{21}$) and spills into the
    order-of-magnitude slower RAM, which causes the performance to decrease.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们跳过256个整数时，指针总是增加$1024 = 2^{10}$，而最后10位保持不变。由于缓存系统使用低6位作为偏移量，接下来12位作为缓存行索引，我们实际上只是在L3缓存中使用$2^{12
    - (10 - 6)} = 2^8$个不同的集合，而不是$2^{12}$，这相当于将我们的L3缓存缩小了$2^4 = 16$倍。数组停止适合L3缓存（$N=2^{21}$）并溢出到速度慢一个数量级的RAM，这导致性能下降。
- en: 'Performance issues caused by cache associativity effects arise with remarkable
    frequency in algorithms because, for multiple reasons, programmers just love using
    powers of two when indexing arrays:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于多种原因，程序员在索引数组时特别喜欢使用2的幂次，因此由缓存关联性效应引起的性能问题在算法中出现的频率非常高：
- en: It is easier to calculate the address for multi-dimensional array accesses if
    the last dimension is a power of two, as it only requires a binary shift instead
    of a multiplication.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果多维数组的最后一个维度是2的幂次，计算多维数组访问的地址更容易，因为它只需要二进制移位而不是乘法。
- en: It is easier to calculate modulo a power of two, as it can be done with a single
    bitwise `and`.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算对2的幂次的取模运算更容易，因为它可以通过单次位与运算来完成。
- en: It is convenient and often even necessary to use power-of-two problem sizes
    in divide-and-conquer algorithms.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分而治之算法中使用2的幂次问题规模既方便，有时甚至必要。
- en: It is the smallest integer exponent, so using the sequence of increasing powers
    of two as problem sizes are a popular choice when benchmarking memory-bound algorithms.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是最小的整数指数，因此当基准测试内存绑定算法时，使用递增的2的幂次作为问题规模是一种流行的选择。
- en: Also, more natural powers of ten are by transitivity divisible by a slightly
    lower power of two.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，由于传递性，十的幂次也可以被稍低一点的2的幂次整除。
- en: This especially often applies to implicit data structures that use a fixed memory
    layout. For example, [binary searching](/hpc/data-structures/binary-search) over
    arrays of size $2^{20}$ takes about ~360ns per query while searching over arrays
    of size $(2^{20} + 123)$ takes ~300ns. When the array size is a multiple of a
    large power of two, then the indices of the “hottest” elements, the ones we likely
    request on the first dozen or so iterations, will also be divisible by some large
    powers of two and map to the same cache line — kicking each other out and causing
    a ~20% performance decrease.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这尤其适用于使用固定内存布局的隐式数据结构。例如，对大小为$2^{20}$的数组进行[二分查找](/hpc/data-structures/binary-search)大约需要每个查询约360ns，而搜索大小为$(2^{20}
    + 123)$的数组大约需要300ns。当数组大小是某个大2的幂次的倍数时，"最热"的元素（我们可能在第一次 dozen 或多轮迭代中请求的元素）的索引也将能被某些大2的幂次整除，并映射到相同的缓存行——相互踢出，导致约20%的性能下降。
- en: 'Luckily, such issues are more of an anomaly rather than serious problems. The
    solution is usually simple: avoid iterating in powers of two, make the last dimensions
    of multi-dimensional arrays a slightly different size or use any other method
    to insert “holes” in the memory layout, or create some seemingly random bijection
    between the array indices and the locations where the data is actually stored.
    [← Pointer Alternatives](https://en.algorithmica.org/hpc/cpu-cache/pointers/)[Memory
    Paging →](https://en.algorithmica.org/hpc/cpu-cache/paging/)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这类问题更多的是异常而不是严重问题。解决方案通常很简单：避免以2的幂次迭代，使多维数组的最后一个维度具有稍不同的尺寸，或使用任何其他方法在内存布局中插入“空洞”，或者创建一些看似随机的双射，将数组索引与数据实际存储的位置相对应。[←指针替代](https://en.algorithmica.org/hpc/cpu-cache/pointers/)[内存分页→](https://en.algorithmica.org/hpc/cpu-cache/paging/)
