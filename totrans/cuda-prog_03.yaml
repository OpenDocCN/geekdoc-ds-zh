- en: Chapter 3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章
- en: CUDA Hardware Overview
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CUDA硬件概述
- en: PC Architecture
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机架构
- en: Let’s start by looking at the typical Core 2 architecture we still find today
    in many PCs and how it impacts our usage of GPU accelerators ([Figure 3.1](#F0010)).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从看看我们今天仍在许多PC中发现的典型Core 2架构开始，看看它如何影响我们使用GPU加速器（[图3.1](#F0010)）。
- en: '![image](../images/F00003Xf03-01-9780124159334.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F00003Xf03-01-9780124159334.jpg)'
- en: Figure 3.1 Typical Core 2 series layout.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 典型的Core 2系列布局。
- en: Notice that all GPU devices are connected to the processor via the PCI-E bus.
    In this case we’ve assumed a PCI-E 2.0 specification bus, which is currently the
    fastest bus available, giving a transfer rate of 5 GB/s. PCI-E 3.0 has become
    available at the time of this writing and should significantly improve the bandwidth
    available.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，所有GPU设备都通过PCI-E总线与处理器连接。在这种情况下，我们假设使用的是PCI-E 2.0规范总线，这是当前最快的总线，提供5 GB/s的传输速率。在本文写作时，PCI-E
    3.0已经可用，预计将显著提高带宽。
- en: However, to get data from the processor, we need to go through the Northbridge
    device over the slow FSB (front-side bus). The FSB can run anything up to 1600
    MHz clock rate, although in many designs it is much slower. This is typically
    only one-third of the clock rate of a fast processor.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要从处理器获取数据，我们需要通过北桥设备通过较慢的FSB（前端总线）。FSB的时钟频率最高可以达到1600 MHz，尽管在许多设计中，它要慢得多。这通常只有快速处理器时钟频率的三分之一。
- en: Memory is also accessed through the Northbridge, and peripherals through the
    Northbridge and Southbridge chipset. The Northbridge deals with all the high-speed
    components like memory, CPU, PCI-E bus connections, etc. The Southbridge chip
    deals with the slower devices such as hard disks, USB, keyboard, network connections,
    etc. Of course, it’s quite possible to connect a hard-disk controller to the PCI-E
    connection, and in practice, this is the only true way of getting RAID high-speed
    data access on such a system.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 内存也是通过北桥访问的，外设则通过北桥和南桥芯片组连接。北桥处理所有高速组件，如内存、CPU、PCI-E总线连接等。南桥芯片处理较慢的设备，如硬盘、USB、键盘、网络连接等。当然，也可以将硬盘控制器连接到PCI-E连接中，实际上，这是在这种系统中获得RAID高速数据访问的唯一真正方式。
- en: PCI-E (Peripheral Communications Interconnect Express) is an interesting bus
    as, unlike its predecessor, PCI (Peripheral Component Interconnect), it’s based
    on guaranteed bandwidth. In the old PCI system each component could use the full
    bandwidth of the bus, but only one device at a time. Thus, the more cards you
    added, the less available bandwidth each card would receive. PCI-E solved this
    problem by the introduction of PCI-E lanes. These are high-speed serial links
    that can be combined together to form X1, X2, X4, X8, or X16 links. Most GPUs
    now use at least the PCI-E 2.0, X16 specification, as shown in [Figure 3.1](#F0010).
    With this setup, we have a 5 GB/s full-duplex bus, meaning we get the same upload
    and download speed, at the same time. Thus, we can transfer 5 GB/s to the card,
    while at the same time receiving 5 GB/s from the card. However, this does not
    mean we can transfer 10 GB/s *to* the card if we’re not receiving any data (i.e.,
    the bandwidth is not cumulative).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PCI-E（外设通讯互连快速总线）是一种有趣的总线，因为与其前身PCI（外设组件互连）不同，它基于保证的带宽。在旧的PCI系统中，每个组件可以使用总线的全部带宽，但每次只有一个设备可以使用。因此，添加的卡越多，每个卡能获得的带宽就越少。PCI-E通过引入PCI-E通道解决了这个问题。这些是高速串行连接，可以组合成X1、X2、X4、X8或X16连接。如今，大多数GPU至少使用PCI-E
    2.0 X16规范，如[图3.1](#F0010)所示。在这种设置下，我们有一个5 GB/s的全双工总线，意味着我们可以在同一时间同时获得相同的上传和下载速度。因此，我们可以向卡传输5
    GB/s，同时从卡接收5 GB/s。然而，这并不意味着如果我们没有接收任何数据时，我们可以向卡传输10 GB/s（即带宽不是累积的）。
- en: In a typical supercomputer environment, or even in a desktop application, we
    are dealing with a large dataset. A supercomputer may deal with petabytes of data.
    A desktop PC may be dealing with as little as a several GB high-definition video.
    In both cases, there is considerable data to fetch from the attached peripherals.
    A single 100 MB/s hard disk will load 6 GB of data in one minute. At this rate
    it takes over two and a half hours to read the entire contents of a standard 1
    TB disk.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的超级计算机环境中，甚至在桌面应用中，我们都在处理一个大型数据集。超级计算机可能处理数PB的数据。桌面PC可能处理少至几个GB的高清视频。在这两种情况下，都有大量的数据需要从附加外设中获取。一个单独的100
    MB/s硬盘将在一分钟内加载6 GB的数据。以这个速度，读取一个标准1 TB磁盘的全部内容需要两个半小时以上。
- en: If using MPI (Message Passing Interface), commonly used in clusters, the latency
    for this arrangement can be considerable if the Ethernet connections are attached
    to the Southbridge instead of the PCI-E bus. Consequently, dedicated high-speed
    interconnects like InfiniBand or 10 Gigabit Ethernet cards are often used on the
    PCI-E bus. This removes slots otherwise available for GPUs. Previously, as there
    was no direct GPU MPI interface, all communications in such a system are routed
    over the PCI-E bus to the CPU and back again. The GPU-Direct technology, available
    in the CUDA 4.0 SDK, solved this issue and it’s now possible for certain InfiniBand
    cards to talk directly to the GPU without having to go through the CPU first.
    This update to the SDK also allows direct GPU to GPU communication.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用MPI（消息传递接口），这种接口通常用于集群中，如果以太网连接连接到南桥而不是PCI-E总线，则这种安排的延迟可能会相当大。因此，通常使用如InfiniBand或10
    Gigabit以太网卡等专用高速互连，在PCI-E总线上。这会占用本可以用于GPU的插槽。以前，由于没有直接的GPU MPI接口，所有在这种系统中的通信都会通过PCI-E总线传输到CPU并再返回。CUDA
    4.0 SDK中提供的GPU-Direct技术解决了这个问题，现在某些InfiniBand卡可以直接与GPU通信，而无需先通过CPU。这一SDK更新还允许GPU之间的直接通信。
- en: We saw a number of major changes with the advent of the Nehalem architecture.
    The main change was to replace the Northbridge and the Southbridge chipset with
    the X58 chipset. The Nehalem architecture brought us QPI (Quick Path Interconnect),
    which was actually a huge advance over the FSB (Front Side Bus) approach and is
    similar to AMD’s HyperTransport. QPI is a high-speed interconnect that can be
    used to talk to other devices or CPUs. In a typical Nehalem system it will connect
    to the memory subsystem, and through an X58 chipset, the PCI-E subsystem ([Figure
    3.2](#F0015)). The QPI runs at either 4.8 GT/s or 6.4 GT/s in the Extreme/Xeon
    processor versions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Nehalem架构的问世，我们看到了一些重大变化。主要的变化是用X58芯片组取代了北桥和南桥芯片组。Nehalem架构带来了QPI（快速路径互连），这实际上是对FSB（前端总线）方法的一次重大改进，类似于AMD的HyperTransport。QPI是一种高速互连，可以用于与其他设备或CPU通信。在典型的Nehalem系统中，它将连接到内存子系统，并通过X58芯片组连接到PCI-E子系统（见[图3.2](#F0015)）。QPI在Extreme/Xeon处理器版本中以4.8
    GT/s或6.4 GT/s的速度运行。
- en: '![image](../images/F00003Xf03-02-9780124159334.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F00003Xf03-02-9780124159334.jpg)'
- en: Figure 3.2 Nehalem/X58 system.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 Nehalem/X58系统。
- en: With the X58 and 1366 processor socket, a total of 36 PCI-E lanes are available,
    which means up to two cards are supported at X16, or four cards at X8\. Prior
    to the introduction of the LGA2011 socket, this provided the best bandwidth solution
    for a GPU machine to date.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用X58和1366处理器插槽时，提供总共36条PCI-E通道，这意味着最多可以支持两个X16模式的卡，或者四个X8模式的卡。在LGA2011插槽推出之前，这是目前为止为GPU机器提供的最佳带宽解决方案。
- en: The X58 design is also available in a lesser P55 chipset where you get only
    16 lanes. This means one GPU card at X16, or two cards at X8.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: X58设计也可以在较低版本的P55芯片组中找到，但该版本仅提供16条通道。这意味着一个GPU卡在X16模式下，或者两个GPU卡在X8模式下。
- en: From the I7/X58 chipset design, Intel moved onto the Sandybridge design, shown
    in [Figure 3.3](#F0020). One of the most noticeable improvements was the support
    for the SATA-3 standard, which supports 600 MB/s transfer rates. This, combined
    with SSDs, allows for considerable input/output (I/O) performance with loading
    and saving data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从I7/X58芯片组设计开始，英特尔转向了Sandybridge设计，如[图3.3](#F0020)所示。最显著的改进之一是对SATA-3标准的支持，支持600
    MB/s的传输速率。结合固态硬盘（SSD），这大大提高了加载和保存数据时的输入/输出（I/O）性能。
- en: '![image](../images/F00003Xf03-03-9780124159334.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F00003Xf03-03-9780124159334.jpg)'
- en: Figure 3.3 Sandybridge design.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 Sandybridge设计。
- en: The other major advance with the Sandybridge design was the introduction of
    the AVX (Advanced Vector Extensions) instruction set, also supported by AMD processors.
    AVX allows for vector instructions that provide up to four double-precision (256
    bit/32 byte) wide vector operations. It’s a very interesting development and something
    that can be used to considerably speed up compute-bound applications on the CPU.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Sandybridge设计的另一个重大进展是引入了AVX（高级向量扩展）指令集，这也是AMD处理器所支持的。AVX支持向量指令，可以提供最多四个双精度（256位/32字节）宽向量操作。这是一个非常有趣的进展，可以显著加速CPU上的计算密集型应用。
- en: 'Notice, however, the big downside of socket 1155 Sandybridge design: It supports
    only 16 PCI-E lanes, limiting the PCI-E bandwidth to 16 GB/s theoretical, 10 GB/s
    actual bandwidth. Intel has gone down the route of integrating more and more into
    the CPU with their desktop processors. Only the socket 2011 Sandybridge-E, the
    server offering, has a reasonable number of PCI-E lanes (40).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，1155针Sandybridge设计的一个大缺点：它仅支持16条PCI-E通道，将PCI-E带宽限制为理论上的16 GB/s，实际带宽为10
    GB/s。英特尔在其桌面处理器中逐步将更多功能集成到CPU中。只有2011针Sandybridge-E（服务器版）才有足够数量的PCI-E通道（40条）。
- en: So how does AMD compare with the Intel designs? Unlike Intel, which has gradually
    moved away from large numbers of PCI-E lanes, in all but their server line, AMD
    have remained fairly constant. Their FX chipset, provides for either two X16 devices
    or four X8 PCI-E devices. The AMD3+ socket paired with the 990FX chipset makes
    for a good workhorse, as it provides SATA 6 GB/s ports paired with up to four
    X16 PCI-E slots (usually running at X8 speed).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，AMD与英特尔设计相比如何呢？与英特尔逐渐减少PCI-E通道数量不同，除其服务器系列外，AMD在所有产品中保持了相对稳定。其FX芯片组支持两个X16设备或四个X8
    PCI-E设备。AMD3+插槽配合990FX芯片组为工作站提供了良好的选择，因为它提供SATA 6 GB/s端口，并配有最多四个X16 PCI-E插槽（通常以X8速度运行）。
- en: One major difference between Intel and AMD is the price point for the number
    of cores. If you count only real processor cores and ignore logical (hyperthreaded)
    ones, for the same price point, you typically get more cores on the AMD device.
    However, the cores on the Intel device tend to perform better. Therefore, it depends
    a lot on the number of GPUs you need to support and the level of loading of the
    given cores.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 英特尔和AMD之间的一个主要区别是核心数量的定价。如果只计算真实的处理器核心而忽略逻辑（超线程）核心，那么在相同的价格点上，AMD设备通常会有更多的核心。然而，英特尔设备的核心性能通常更强。因此，最终取决于您需要支持的GPU数量和给定核心的负载水平。
- en: As with the Intel design, you see similar levels of bandwidth around the system,
    with the exception of bandwidth to main memory. Intel uses triple or quad channel
    memory on their top-end systems and dual-channel memory on the lower-end systems.
    AMD uses only dual-channel memory, leading to significantly less CPU host-memory
    bandwidth being available ([Figure 3.4](#F0025)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与英特尔设计类似，您会看到系统周围有相似的带宽水平，唯一例外的是主内存的带宽。英特尔在其高端系统上使用三通道或四通道内存，在低端系统上使用双通道内存。AMD仅使用双通道内存，这导致可用的CPU主机内存带宽显著较低（[图
    3.4](#F0025)）。
- en: '![image](../images/F00003Xf03-04-9780124159334.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F00003Xf03-04-9780124159334.jpg)'
- en: Figure 3.4 AMD.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 AMD。
- en: One significant advantage of the AMD chipsets over the Intel ones is the support
    for up to six SATA (Serial ATA) 6 GB/s ports. If you consider that the slowest
    component in any system usually limits the overall throughput, this is something
    that needs some consideration. However, SATA3 can very quickly overload the bandwidth
    of Southbridge when using multiple SSDs (solid state drives). A PCI-E bus solution
    may be a better one, but it obviously requires additional costs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: AMD芯片组相比英特尔芯片组的一个显著优势是支持最多六个SATA（串行ATA）6 GB/s端口。如果考虑到任何系统中最慢的组件通常会限制整体吞吐量，那么这一点需要特别关注。然而，在使用多个SSD（固态硬盘）时，SATA3很容易使南桥的带宽超载。PCI-E总线解决方案可能是一个更好的选择，但显然需要额外的成本。
- en: GPU Hardware
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU硬件
- en: GPU hardware is radically different than CPU hardware. [Figure 3.5](#F0030)
    shows how a multi-GPU system looks conceptually from the other side of the PCI-E
    bus.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: GPU硬件与CPU硬件截然不同。[图 3.5](#F0030)展示了从PCI-E总线另一端来看多GPU系统的概念。
- en: '![image](../images/F00003Xf03-05-9780124159334.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F00003Xf03-05-9780124159334.jpg)'
- en: Figure 3.5 Block diagram of a GPU (G80/GT200) card.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 GPU（G80/GT200）显卡的框图。
- en: 'Notice the GPU hardware consists of a number of key blocks:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，GPU硬件由多个关键模块组成：
- en: • Memory (global, constant, shared)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: • 内存（全局内存、常量内存、共享内存）
- en: • Streaming multiprocessors (SMs)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: • 流处理器（SMs）
- en: • Streaming processors (SPs)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: • 流处理器（SPs）
- en: The main thing to notice here is that a GPU is really an array of SMs, each
    of which has *N* cores (8 in G80 and GT200, 32–48 in Fermi, 8 plus in Kepler;
    see [Figure 3.6](#F0035)). This is the key aspect that allows scaling of the processor.
    A GPU device consists of one or more SMs. Add more SMs to the device and you make
    the GPU able to process more tasks at the same time, or the same task quicker,
    if you have enough parallelism in the task.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的主要一点是，GPU 实际上是由多个 SM 组成的阵列，每个 SM 都有 *N* 个核心（G80 和 GT200 为 8 个，Fermi 为
    32–48 个，Kepler 为 8 个以上；见 [图 3.6](#F0035)）。这是支持处理器扩展的关键因素。一个 GPU 设备由一个或多个 SM 组成。增加更多的
    SM 到设备中，可以使 GPU 同时处理更多的任务，或者在任务中有足够的并行性时，使任务处理更快。
- en: '![image](../images/F00003Xf03-06-9780124159334.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F00003Xf03-06-9780124159334.jpg)'
- en: Figure 3.6 Inside an SM.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 SM 内部结构。
- en: Like CPUs, if the programmer writes code that limits the processor usage to
    *N* cores, let’s say dual-core, when the CPU manufacturers bring out a quad-core
    device, the user sees no benefit. This is exactly what happened in the transition
    from dual- to quad-core CPUs, and lots of software then had to be rewritten to
    take advantage of the additional cores. NVIDIA hardware will increase in performance
    by growing a combination of the number of SMs and number of cores per SM. When
    designing software, be aware that the next generation may double the number of
    either.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 像 CPU 一样，如果程序员编写的代码将处理器的使用限制在 *N* 个核心上，比如说双核，当 CPU 制造商推出四核设备时，用户并不会看到任何性能提升。这正是从双核到四核
    CPU 转型时发生的情况，许多软件必须重新编写，以便利用更多的核心。NVIDIA 硬件的性能将通过增加 SM 的数量和每个 SM 的核心数来提升。在设计软件时，需要注意下一代设备可能会增加任一方面的数量。
- en: Now let’s take a closer look at the SMs themselves. There are number of key
    components making up each SM, however, not all are shown here for reasons of simplicity.
    The most significant part is that there are multiple SPs in each SM. There are
    8 SPs shown here; in Fermi this grows to 32–48 SPs and in Kepler to 192\. There
    is no reason to think the next hardware revision will not continue to increase
    the number of SPs/SMs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地看看 SM 本身。每个 SM 由多个关键组件构成，然而出于简化原因，图中并没有显示所有组件。最重要的部分是，每个 SM 中都有多个 SP（流处理器）。这里显示了
    8 个 SP；在 Fermi 中，这个数量增加到 32–48 个，在 Kepler 中增加到 192 个。没有理由认为下一个硬件版本不会继续增加每个 SM
    中的 SP 数量。
- en: Each SM has access to something called a register file, which is much like a
    chunk of memory that runs at the same speed as the SP units, so there is effectively
    zero wait time on this memory. The size of this memory varies from generation
    to generation. It is used for storing the registers in use within the threads
    running on an SP. There is also a shared memory block accessible only to the individual
    SM; this can be used as a program-managed cache. Unlike a CPU cache, there is
    no hardware evicting cache data behind your back—it’s entirely under programmer
    control.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 SM 都可以访问一个被称为寄存器文件的区域，这类似于一块与 SP 单元运行速度相同的内存，因此在这块内存上的等待时间几乎为零。这块内存的大小随着代际的变化而有所不同。它用于存储在
    SP 上运行的线程中使用的寄存器。此外，还有一个仅可由单个 SM 访问的共享内存块；它可以用作程序管理的缓存。与 CPU 缓存不同，程序员完全控制该内存区域，不会有硬件在你背后自动移除缓存数据。
- en: Each SM has a separate bus into the texture memory, constant memory, and global
    memory spaces. Texture memory is a special view onto the global memory, which
    is useful for data where there is interpolation, for example, with 2D or 3D lookup
    tables. It has a special feature of hardware-based interpolation. Constant memory
    is used for read-only data and is cached on all hardware revisions. Like texture
    memory, constant memory is simply a view into the main global memory.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 SM 都有一条单独的总线连接到纹理内存、常量内存和全局内存空间。纹理内存是对全局内存的特殊视图，对于需要插值的数据非常有用，例如 2D 或 3D
    查找表。它具有硬件级别的插值功能。常量内存用于只读数据，并在所有硬件版本中进行缓存。像纹理内存一样，常量内存仅仅是对主全局内存的一个视图。
- en: Global memory is supplied via GDDR (Graphic Double Data Rate) on the graphics
    card. This is a high-performance version of DDR (Double Data Rate) memory. Memory
    bus width can be up to 512 bits wide, giving a bandwidth of 5 to 10 times more
    than found on CPUs, up to 190 GB/s with the Fermi hardware.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 全局内存通过 GDDR（图形双倍数据速率）供给，这是一种高性能版本的 DDR（双倍数据速率）内存。内存总线宽度可达到 512 位，带宽比 CPU 上的内存大
    5 到 10 倍，Fermi 硬件的带宽最高可达 190 GB/s。
- en: Each SM also has two or more special-purpose units (SPUs), which perform special
    hardware instructions, such as the high-speed 24-bit sin/cosine/exponent operations.
    Double-precision units are also present on GT200 and Fermi hardware.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 每个SM（流处理单元）还配备了两个或更多的专用单元（SPUs），用于执行特殊的硬件指令，例如高速的24位正弦/余弦/指数运算。GT200和Fermi硬件上也配备了双精度单元。
- en: CPUs and GPUs
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU和GPU
- en: Now that you have some idea what the GPU hardware looks like, you might say
    that this is all very interesting, but what does it mean for us in terms of programming?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对GPU硬件有了一些了解，你可能会说这些都很有趣，但从编程的角度来看，这对我们意味着什么呢？
- en: Anyone who has ever worked on a large project will know it’s typically partitioned
    into sections and allocated to specific groups. There may be a specification group,
    a design group, a coding group, and a testing group. There are absolutely huge
    benefits to having people in each team who understand completely the job of the
    person before and after them in the chain of development.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 任何曾经参与过大型项目的人都知道，项目通常被分成几个部分并分配给特定小组。可能有规范小组、设计小组、编码小组和测试小组。每个小组中有了解前后流程的成员，能极大地提升项目的效率。
- en: Take, for example, testing. If the designer did not consider testing, he or
    she would not have included any means to test in software-specific hardware failures.
    If the test team could only test hardware failure by having the hardware fail,
    it would have to physically modify hardware to cause such failures. This is hard.
    It’s much easier for the software people to design a flag that inverts the hardware-based
    error flag in software, thus allowing the failure functionality to be tested easily.
    Working on the testing team you might see how hard it is to do it any other way,
    but with a blinkered view of your discipline, you might say that testing is not
    your role.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以测试为例。如果设计师没有考虑到测试，他们就不会在软件中包含任何测试硬件故障的手段。如果测试团队只能通过硬件故障来测试硬件问题，那么他们必须物理上修改硬件以引发故障。这是很困难的。软件人员设计一个标志，反转硬件故障标志，便可以轻松测试故障功能，这就容易得多。作为测试团队的一员，你可能会发现其他方式做起来有多困难，但如果你只关注自己的专业领域，你可能会认为测试不是你的职责。
- en: Some of the best engineers are those with a view of the processes before and
    after them. As software people, it’s always good to know how the hardware actually
    works. For serial code execution, it may be interesting to know how things work,
    but usually not essential. The vast majority of developers have never taken a
    computer architecture course or read a book on it, which is a great shame. It’s
    one of the main reasons we see such inefficient software written these days. I
    grew up learning BASIC at age 11, and was programming Z80 assembly language at
    14, but it was only during my university days that I really started to understand
    computer architecture to any great depth.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最优秀的工程师是那些能够看到自己工作前后流程的人。作为软件人员，了解硬件如何实际工作总是非常有益的。对于串行代码执行来说，了解事物如何运作可能很有趣，但通常并非必须。绝大多数开发人员从未学习过计算机架构课程或读过相关书籍，这是一个巨大的遗憾。这是我们今天看到如此低效的软件的主要原因之一。我从11岁开始学习BASIC，14岁时就开始编写Z80汇编语言，但直到大学时，我才真正开始深入理解计算机架构。
- en: Working in an embedded field gives you a very hands-on approach to hardware.
    There is no nice Windows operating system to set up the processor for you. Programming
    is a very low-level affair. With embedded applications, there are typically millions
    of boxes shipped. Sloppy code means poor use of the CPU and available memory,
    which could translate into needing a faster CPU or more memory. An additional
    50 cent cost on a million boxes is half a million dollars. This translates into
    a lot of design and programming hours, so clearly it’s more cost effective to
    write better code than buy additional hardware.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入式领域工作，你可以非常深入地接触到硬件。没有像Windows这样的操作系统来为你设置处理器。编程是一项非常底层的工作。对于嵌入式应用来说，通常会有数百万个产品发货。代码写得不规范意味着CPU和内存的利用效率低下，这可能意味着需要更快的CPU或更多的内存。每个产品上多出的50美分成本，乘以百万个产品，就是50万美元。这意味着大量的设计和编程工作，所以显然，写出更好的代码比购买额外的硬件更具成本效益。
- en: Parallel programming, even today, is very much tied to the hardware. If you
    just want to write code and don’t care about performance, parallel programming
    is actually quite easy. To really get performance out of the hardware, you need
    to understand how it works. Most people can drive a car safely and slowly in first
    gear, but if you are unaware that there are other gears, or do not have the knowledge
    to engage them, you will never get from point A to point B very quickly. Learning
    about the hardware is a little like learning to change gear in a car with a manual
    gearbox—a little tricky at first, but something that comes naturally after awhile.
    By the same analogy, you can also buy a car with an automatic gearbox, akin to
    using a library already coded by someone who understands the low-level mechanics
    of the hardware. However, doing this without understanding the basics of how it
    works will often lead to a suboptimal implementation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 即便是今天，并行编程仍然与硬件密切相关。如果你只想编写代码而不关心性能，那么并行编程实际上是相当简单的。要从硬件中真正获得性能，你需要理解它是如何工作的。大多数人可以在一档的情况下安全缓慢地驾驶汽车，但如果你不知道还有其他档位，或者没有能力切换档位，你就永远无法迅速从
    A 点到达 B 点。了解硬件有点像学习如何换手动挡汽车的档位——一开始可能有点棘手，但过一段时间后就会自然而然地掌握。同样的道理，你也可以购买一辆自动挡汽车，相当于使用已经有人编写的硬件低级机制的库。然而，如果没有理解其基本原理，这样做往往会导致次优的实现。
- en: Compute Levels
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算级别
- en: CUDA supports a number of compute levels. The original G80 series graphics cards
    shipped with the first version of CUDA. The compute capability is fixed into the
    hardware. To upgrade to a newer version users had to upgrade their hardware. Although
    this might sound like NVIDIA trying to force users to buy more cards, it in fact
    brings many benefits. When upgrading a compute level, you can often move from
    an older platform to a newer one, usually doubling the compute capacity of the
    card for a similar price to the original card. Given that NVIDIA typically brings
    out a new platform at least every couple of years, we have seen to date a huge
    increase in available compute power over the few years CUDA has been available.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: CUDA 支持多个计算级别。最初的 G80 系列显卡搭载了 CUDA 的第一个版本。计算能力被固定在硬件中。为了升级到更新的版本，用户必须升级硬件。虽然这可能听起来像是
    NVIDIA 强迫用户购买更多显卡，但实际上它带来了许多好处。升级计算级别时，通常可以从旧平台迁移到新平台，通常以与原始显卡相似的价格翻倍提升显卡的计算能力。考虑到
    NVIDIA 通常每隔几年就会推出一个新平台，到目前为止，我们已经看到了 CUDA 发布几年以来计算能力的巨大增长。
- en: A full list of the differences between each compute level can be found in the
    NVIDIA CUDA Programming Guide, Appendix G, which is shipped as part of the CUDA
    SDK. Therefore, we will only cover the major differences found at each compute
    level, that is, what you need to know as a developer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 每个计算级别之间的差异完整列表可以在 NVIDIA CUDA 编程指南的附录 G 中找到，该附录作为 CUDA SDK 的一部分随附。因此，我们将只讨论每个计算级别之间的主要区别，也就是作为开发者需要了解的内容。
- en: Compute 1.0
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算 1.0
- en: Compute level 1.0 is found on the older graphics cards, for example, the original
    8800 Ultras and many of the 8000 series cards as well as the Tesla C/D/S870s.
    The main features lacking in compute 1.0 cards are those for atomic operations.
    Atomic operations are those where we can guarantee a complete operation without
    any other thread interrupting. In effect, the hardware implements a barrier point
    at the entry of the atomic function and guarantees the *completion* of the operation
    (add, sub, min, max, logical and, or, xor, etc.) as one operation. Compute 1.0
    cards are effectively now obsolete, so this restriction, for all intents and purposes,
    can be ignored.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 计算能力 1.0 出现在较旧的显卡中，例如最初的 8800 Ultra 以及许多 8000 系列显卡和 Tesla C/D/S870s。计算 1.0 显卡缺少的主要功能是原子操作。原子操作是指我们能够保证一个完整操作不会被其他线程中断的操作。实际上，硬件在原子操作入口处实现了一个屏障点，并保证操作（加法、减法、最小值、最大值、逻辑与、或、异或等）作为一个完整的操作被执行。计算
    1.0 显卡现在实际上已经过时，因此这一限制从实际角度来看可以忽略不计。
- en: Compute 1.1
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算 1.1
- en: Compute level 1.1 is found in many of the later shipping 9000 series cards,
    such as the 9800 GTX, which were extremely popular. These are based on the G92
    hardware as opposed to the G80 hardware of compute 1.0 devices.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 计算能力 1.1 出现在许多后期发布的 9000 系列显卡中，如 9800 GTX，这些显卡曾非常受欢迎。这些显卡基于 G92 硬件，而不是计算 1.0
    设备的 G80 硬件。
- en: One major change brought in with compute 1.1 devices was support, on many but
    not all devices, for overlapped data transfer and kernel execution. The SDK call
    to `cudaGetDeviceProperties()` returns the `deviceOverlap` property, which defines
    if this functionality is available. This allows for a very nice and important
    optimization called double buffering, which works as shown in [Figure 3.7](#F0040).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 引入计算 1.1 设备的一个主要变化是，许多设备（但不是所有设备）支持重叠的数据传输和内核执行。SDK 调用 `cudaGetDeviceProperties()`
    返回 `deviceOverlap` 属性，该属性定义了此功能是否可用。这为一个非常好且重要的优化提供了支持，称为双缓冲，它的工作原理如 [图 3.7](#F0040)
    所示。
- en: '![image](../images/F00003Xf03-07-9780124159334.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F00003Xf03-07-9780124159334.jpg)'
- en: Figure 3.7 Double buffering with a single GPU.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 单 GPU 的双缓冲。
- en: 'To use this method we require double the memory space we’d normally use, which
    may well be an issue if your target market only had a 512 MB card. However, with
    Tesla cards, used mainly for scientific computing, you can have up to 6 GB of
    GPU memory, which makes such techniques very useful. Let’s look at what happens:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法，我们需要的内存空间是正常使用内存的两倍，如果目标市场的显卡只有 512 MB，这可能是一个问题。然而，使用主要用于科学计算的 Tesla
    卡，你可以拥有最多 6 GB 的 GPU 内存，这使得这些技术非常有用。让我们看看会发生什么：
- en: '**Cycle 0:** Having allocated two areas of memory in the GPU memory space,
    the CPU fills the first buffer.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**周期 0：** 在 GPU 内存空间中分配了两个内存区域后，CPU 填充第一个缓冲区。'
- en: '**Cycle 1:** The CPU then invokes a CUDA kernel (a GPU task) on the GPU, which
    returns immediately to the CPU (a nonblocking call). The CPU then fetches the
    next data packet, from a disk, the network, or wherever. Meanwhile, the GPU is
    processing away in the background on the data packet provided. When the CPU is
    ready, it starts filling the other buffer.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**周期 1：** 然后，CPU 在 GPU 上调用 CUDA 内核（GPU 任务），并立即返回到 CPU（非阻塞调用）。然后，CPU 获取下一个数据包，无论是来自磁盘、网络还是其他地方。与此同时，GPU
    正在后台处理提供的数据包。当 CPU 准备好时，它开始填充另一个缓冲区。'
- en: '**Cycle 2:** When the CPU is done filling the buffer, it invokes a kernel to
    process buffer 1\. It then checks if the kernel from cycle 1, which was processing
    buffer 0, has completed. If not, it waits until this kernel has finished and then
    fetches the data from buffer 0 and then loads the next data block into the same
    buffer. During this time the kernel kicked off at the start of the cycle is processing
    data on the GPU in buffer 1.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**周期 2：** 当 CPU 填充完缓冲区后，它调用一个内核来处理缓冲区 1。然后它检查周期 1 中处理缓冲区 0 的内核是否已完成。如果没有，它会等待该内核完成，然后从缓冲区
    0 中获取数据，并将下一个数据块加载到同一缓冲区中。在此期间，在周期开始时启动的内核正在 GPU 上处理缓冲区 1 中的数据。'
- en: '**Cycle *N*:** We then repeat cycle 2, alternating between which buffer we
    read and write to on the CPU with the buffer being processed on the GPU.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**周期 *N*：** 然后我们重复周期 2，交替在 CPU 上读取和写入哪个缓冲区，同时 GPU 上正在处理的缓冲区。'
- en: GPU-to-CPU and CPU-to-GPU transfers are made over the relatively slow (5 GB/s)
    PCI-E bus and this dual-buffering method largely hides this latency and keeps
    both the CPU and GPU busy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 到 CPU 和 CPU 到 GPU 的数据传输是通过相对较慢（5 GB/s）的 PCI-E 总线进行的，这种双缓冲方法在很大程度上隐藏了这种延迟，并保持
    CPU 和 GPU 都在忙碌。
- en: Compute 1.2
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 计算 1.2
- en: Compute 1.2 devices appeared with the low-end GT200 series hardware. These were
    the initial GTX260 and GTX280 cards. With the GT200 series hardware, NVIDIA approximately
    doubled the number of CUDA core processors on a single card, through doubling
    the number of multiprocessors present on the card. We’ll cover CUDA cores and
    multiprocessors later. In effect, this doubled the performance of the cards compared
    to the G80/G92 range before them.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 计算 1.2 设备出现在低端的 GT200 系列硬件中。这些是最初的 GTX260 和 GTX280 显卡。通过 GT200 系列硬件，NVIDIA 大约将单张卡上的
    CUDA 核心处理器数量翻倍，通过将卡上的多处理器数量翻倍。我们稍后将讨论 CUDA 核心和多处理器。实际上，这使得这些卡的性能相较于之前的 G80/G92
    系列翻倍。
- en: Along with doubling the number of multiprocessors, NVIDIA increased the number
    of concurrent warps a multiprocessor could execute from 24 to 32\. Warps are blocks
    of code that execute within a multiprocessor, and increasing the amount of available
    warps per multiprocessor gives us more scope to get better performance, which
    we’ll look at later.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将多处理器的数量翻倍外，NVIDIA 还将单个多处理器可以执行的并发 Warp 数量从 24 增加到 32。Warp 是在多处理器内执行的代码块，增加每个多处理器可用的
    Warp 数量为我们提供了更大的空间，以便获得更好的性能，我们稍后会探讨这一点。
- en: Issues with restrictions on coalesced access to the global memory and bank conflicts
    in the shared memory found in compute 1.0 and compute 1.1 devices were greatly
    reduced. This make the GT200 series hardware far easier to program and it greatly
    improved the performance of many previous, poorly written CUDA programs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在compute 1.0和compute 1.1设备中，关于全局内存的合并访问限制和共享内存中的银行冲突问题得到极大减少。这使得GT200系列硬件编程变得更加容易，并且大大提高了许多先前编写不当的CUDA程序的性能。
- en: Compute 1.3
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Compute 1.3
- en: The compute 1.3 devices were introduced with the move from GT200 to the GT200
    a/b revisions of the hardware. This followed shortly after the initial release
    of the GT200 series. Almost all higher-end cards from this era were compute 1.3
    compatible.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Compute 1.3设备是在GT200硬件的GT200 a/b版本发布时推出的。这一变化紧接着GT200系列的首次发布。几乎所有当时的高端卡都兼容compute
    1.3。
- en: The major change that occurs with compute 1.3 hardware is the introduction of
    support for limited double-precision calculations. GPUs are primarily aimed at
    graphics and here there is a huge need for fast single-precision calculations,
    but limited need for double-precision ones. Typically, you see an order of magnitude
    drop in performance using double-precision as opposed to single-precision floating-point
    operations, so time should be taken to see if there is any way single-precision
    arithmetic can be used to get the most out of this hardware. In many cases, a
    mixture of single and double-precision operations can be used, which is ideal
    since it exploits both the dedicated single-precision and double-precision hardware
    present.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在compute 1.3硬件中，主要的变化是引入了对有限双精度计算的支持。GPU主要面向图形处理，在这方面，对于快速的单精度计算有巨大的需求，但对于双精度计算的需求相对较小。通常，在进行双精度运算时，性能会比单精度运算下降一个数量级，因此应该考虑是否有办法使用单精度运算以最大限度地发挥硬件性能。在许多情况下，可以使用单精度和双精度运算的混合方式，这是理想的选择，因为它利用了硬件中专门的单精度和双精度硬件。
- en: Compute 2.0
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Compute 2.0
- en: Compute 2.0 devices saw the switch to Fermi hardware. The original guide for
    tuning applications for the Fermi architecture can be found on the NVIDIA website
    at [http://developer.nvidia.com/cuda/nvidia-gpu-computing-documentation](http://developer.nvidia.com/cuda/nvidia-gpu-computing-documentation).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Compute 2.0设备采用了Fermi硬件架构。调优Fermi架构应用程序的原始指南可以在NVIDIA官网找到，链接为：[http://developer.nvidia.com/cuda/nvidia-gpu-computing-documentation](http://developer.nvidia.com/cuda/nvidia-gpu-computing-documentation)。
- en: 'Some of the main changes in compute 2.x hardware are as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Compute 2.x硬件的一些主要变化如下：
- en: • Introduction of 16 K to 48 K of L1 cache memory on each SP.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个SP引入了16K到48K的L1缓存。
- en: • Introduction of a shared L2 cache for all SMs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: • 引入了所有SM共享的L2缓存。
- en: • Support in Tesla-based devices for ECC (Error Correcting Code)-based memory
    checking and error correction.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: • 在基于Tesla的设备中支持ECC（错误校正码）内存检查和错误修正。
- en: • Support in Tesla-based devices for dual-copy engines.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: • 在基于Tesla的设备中支持双副本引擎。
- en: • Extension in size of the shared memory from 16 K per SM up to 48 K per SM.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个SM的共享内存从16K扩展到48K。
- en: • For optimum coalescing of data, it must be 128-byte aligned.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: • 为了实现最佳的数据合并，数据必须是128字节对齐的。
- en: • The number of shared memory banks increased from 16 to 32.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: • 共享内存的银行数从16增加到32。
- en: Let’s look at the implications of some of these changes in detail. First, let’s
    pick up on the introduction of the L1 cache and what this means. An L1 (level
    one) cache is a cache present on a device and is the fastest cache type available.
    Compute 1.x hardware has no cache, except for the texture and constant memory
    caches. The introduction of a cache makes it much easier for many programmers
    to write programs that work well on GPU hardware. It also allows for applications
    that do not follow a known memory pattern at compile time. However, to exploit
    the cache, the application either needs to have a sequential memory pattern or
    have at least some data reuse.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些变化的影响。首先，我们来看看L1缓存的引入及其含义。L1（一级）缓存是设备上存在的缓存类型，是最快的缓存类型。Compute 1.x硬件没有缓存，除了纹理和常量内存缓存。引入缓存使得许多程序员编写能够在GPU硬件上良好运行的程序变得更加容易。它还允许应用程序在编译时无法预知内存访问模式的情况下运行。然而，要充分利用缓存，应用程序需要具有顺序的内存访问模式，或者至少需要一定的数据重用。
- en: The L2 cache is up to 768 K in size on Fermi and, importantly, is a unified
    cache, meaning it is shared and provides a consistent view for all the SMs. This
    allows for much faster interblock communication through global atomic operations.
    Compared to having to go out to the global memory on the GPU, using the shared
    cache is an order of magnitude faster.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在Fermi中，L2缓存的最大大小为768 K，重要的是，它是一个统一缓存，这意味着它是共享的，并为所有SM提供一致的视图。这允许通过全局原子操作进行更快的跨块通信。与需要访问GPU上的全局内存相比，使用共享缓存要快一个数量级。
- en: Support for ECC memory is a must for data centers. ECC memory provides for automatic
    error detection and correction. Electrical devices emit small amounts of radiation.
    When in close proximity to other devices, this radiation can change the contents
    of memory cells in the other device. Although the probability of this happening
    is tiny, as you increase the exposure of the equipment by densely packing it into
    data centers, the probability of something going wrong rises to an unacceptable
    level. ECC, therefore, detects and corrects single-bit upset conditions that you
    may find in large data centers. This reduces the amount of available RAM and negatively
    impacts memory bandwidth. Because this is a major drawback on graphics cards,
    ECC is only available on Tesla products.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ECC内存的支持是数据中心的必需项。ECC内存提供自动错误检测和更正功能。电子设备会发出少量辐射。当它们靠近其他设备时，这些辐射可能会改变其他设备内存单元的内容。尽管这种情况发生的概率很小，但随着设备密集地堆放在数据中心中，设备的暴露度增加，出现问题的概率也会升高到不可接受的水平。因此，ECC可以检测和更正你在大型数据中心中可能遇到的单比特错误。这样做会减少可用的RAM，并对内存带宽产生负面影响。由于这是显卡的一个主要缺点，ECC仅在Tesla产品中提供。
- en: Dual-copy engines allow you to extend the dual-buffer example we looked at earlier
    to use multiple streams. Streams are a concept we’ll look at in detail later,
    but basically, they allow for *N* independent kernels to be executed in a pipeline
    fashion as shown in [Figure 3.8](#F0045).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 双复制引擎使你可以将之前查看的双缓冲示例扩展到使用多个流。流是我们稍后会详细讨论的一个概念，但基本上，它们允许多个独立的内核按流水线方式执行，如[图3.8](#F0045)所示。
- en: '![image](../images/F00003Xf03-08-9780124159334.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![image](../images/F00003Xf03-08-9780124159334.jpg)'
- en: Figure 3.8 Stream pipelining.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 流管道化。
- en: Notice how the kernel sections run one after another in the figure. The copy
    operations are hidden by the execution of a kernel on another stream. The kernels
    and the copy engines execute concurrently, thus making the most use of the relevant
    units.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意图中内核部分是如何一个接一个运行的。复制操作被另一个流上内核的执行所隐藏。内核和复制引擎并行执行，从而最大化了相关单元的使用。
- en: Note that the dual-copy engines are physically available on almost all the top-end
    Fermi GPUs, such as the GTX480 or GTX580 device. However, only the Tesla cards
    make both engines visible to the CUDA driver.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，双复制引擎几乎在所有高端Fermi GPU上都可以使用，例如GTX480或GTX580设备。然而，只有Tesla卡才会使这两个引擎对CUDA驱动程序可见。
- en: Shared memory also changed drastically, in that it was transformed into a combined
    L1 cache. The L1 cache size is 64 K. However, to preserve backward compatibility,
    a minimum of 16 K must be allocated to the shared memory, meaning the L1 cache
    is really only 48 K in size. Using a switch, shared memory and L1 cache usage
    can be swapped, giving 48 K of shared memory and 16 K of L1 cache. Going from
    16 K of shared memory to 48 K of shared memory is a huge benefit for certain programs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 共享内存也发生了剧烈变化，它被转换为一个合并的L1缓存。L1缓存的大小是64 K。然而，为了保持向后兼容性，必须分配至少16 K给共享内存，这意味着L1缓存的实际大小只有48
    K。通过一个开关，共享内存和L1缓存的使用可以互换，从而提供48 K的共享内存和16 K的L1缓存。从16 K共享内存增加到48 K共享内存对于某些程序来说是一个巨大的好处。
- en: Alignment requirements for optimal use became more strict than in previous generations,
    due to the introduction of the L1 and L2 cache. Both use a cache line size of
    128 bytes. A cache line is the *minimum* amount of data the memory can fetch.
    Thus, if your program fetches subsequent elements of the data, this works really
    well. This is typically what most CUDA programs do, with groups of threads fetching
    adjacent memory addresses. The one requirement that comes out of this change is
    to have 128-byte alignment of the dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 由于引入了L1和L2缓存，最佳使用的对齐要求比以前的版本更为严格。两者的缓存行大小都是128字节。缓存行是内存可以提取的*最小*数据量。因此，如果你的程序提取数据的后续元素，这样的方式效果非常好。这通常是大多数CUDA程序的做法，线程组会提取相邻的内存地址。由此变化产生的唯一要求是数据集必须进行128字节对齐。
- en: However, if your program has a sparse and distributed memory pattern per thread,
    you need to disable this feature and switch to the 32-bit mode of cache operation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你的程序每个线程有稀疏且分布式的内存模式，你需要禁用此功能并切换到32位的缓存操作模式。
- en: Finally, one of the last major changes we’ll pick up on is the increase of shared
    memory banks from 16 to 32 bits. This is a major benefit over the previous generations.
    It allows each thread of the current warp (32 threads) to write to exactly one
    bank of 32 bits in the shared memory without causing a shared bank conflict.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们要提到的一个主要变化是共享内存银行从16位增加到32位。这是相较于上一代的一项重大改进。它允许当前warp（32个线程）中的每个线程写入共享内存中恰好一个32位的内存银行，而不会造成共享银行冲突。
- en: Compute 2.1
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Compute 2.1
- en: 'Compute 2.1 is seen on certain devices aimed specifically at the games market,
    such as the GTX460 and GTX560\. These devices change the architecture of the device
    as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Compute 2.1出现在一些专门面向游戏市场的设备上，比如GTX460和GTX560。这些设备的架构变更如下：
- en: • 48 CUDA cores per SM instead of the usual 32 per SM.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个SM有48个CUDA核心，而不是通常的每个SM有32个CUDA核心。
- en: • Eight single-precision, special-function units for transcendental per SM instead
    of the usual four.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: • 每个SM有八个单精度特殊功能单元用于处理超越函数，而不是通常的四个。
- en: • Dual-warp dispatcher instead of the usual single-warp dispatcher.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: • 双warp调度器，而不是通常的单warp调度器。
- en: The x60 series cards have always had a very high penetration into the midrange
    games market, so if your application is targeted at the consumer market, it is
    important to be aware of the implication of these changes.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: x60系列显卡在中端游戏市场一直有着非常高的渗透率，因此，如果你的应用面向消费市场，了解这些变化的影响是非常重要的。
- en: Noticeably different on the compute 2.1 hardware is the sacrifice of dual-precision
    hardware to increase the number of CUDA cores. For single-precision and integer
    calculation–dominated kernels, this is a good tradeoff. Most games make little
    use of double-precision floating-point data, but significant use of single-precision
    floating-point and integer math.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在compute 2.1硬件上，显著不同的是为了增加CUDA核心数而牺牲了双精度硬件。对于单精度和整数计算主导的内核，这是一个不错的折衷。大多数游戏几乎不使用双精度浮点数据，但会大量使用单精度浮点和整数计算。
- en: Warps, which we will cover in detail later, are groups of threads. On compute
    2.0 hardware, the single-warp dispatcher takes two clock cycles to dispatch instructions
    of an entire warp. On compute 2.1 hardware, instead of the usual two instruction
    dispatchers per two clock cycles, we now have four. In the hardware, there are
    three banks of 16 CUDA cores, 48 CUDA cores in total, instead of the usual two
    banks of 16 CUDA cores. If NVIDIA could have just squeezed in another set of 16
    CUDA cores, you’d have an ideal solution. Maybe we’ll see this in future hardware.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: warp是线程的组，我们将在后面详细讨论。在compute 2.0硬件上，单个warp调度器需要两个时钟周期来调度整个warp的指令。在compute
    2.1硬件上，我们现在有四个调度器，而不是通常的两个调度器（每两个时钟周期一个调度器）。在硬件中，有三组16个CUDA核心的银行，总共48个CUDA核心，而不是通常的两组16个CUDA核心。如果NVIDIA能够再加一个16个CUDA核心的组，那就会有一个理想的解决方案。也许我们在未来的硬件中会看到这一点。
- en: The compute 2.1 hardware is actually a superscalar approach, similar to what
    is found on CPUs from the original Pentium CPU onwards. To make use of all the
    cores, the hardware needs to identify instruction-level parallelism (ILP) within
    a *single* thread. This is a significant divergence from the universal thread-level
    parallelism (TLP) approach recommended in the past. For ILP to be present there
    need to be instructions that are independent of one another. One of the easiest
    ways to do this is via the special vector class covered later in the book.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Compute 2.1硬件实际上是一种超标量方法，类似于从原始Pentium CPU开始的CPU架构。为了充分利用所有核心，硬件需要在*单个*线程中识别指令级并行性（ILP）。这与过去推荐的线程级并行性（TLP）方法有显著差异。为了实现ILP，必须有相互独立的指令。一种最简单的方式是通过本书后面会介绍的特殊向量类来实现。
- en: Performance of compute 2.1 hardware varies. Some well-known applications like
    Folding at Home perform really well with the compute 2.1 hardware. Other applications
    such as video encoding packages, where it’s harder to extract ILP and memory bandwidth
    is a key factor, typically perform much worse.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Compute 2.1硬件的性能有所不同。一些著名的应用程序，如Folding at Home，在compute 2.1硬件上表现非常好。而其他应用程序，如视频编码软件，由于更难提取指令级并行性（ILP），并且内存带宽是一个关键因素，通常表现较差。
- en: The final details of Kepler and the new compute 3.0 platform were, at the time
    of writing, still largely unreleased. A discussion of the Kepler features already
    announced can be found in [Chapter 12](CHP012.html), under ‘Developing for Future
    GPUs’.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 凯普勒（Kepler）和新的计算 3.0 平台的最终细节，在本文写作时，仍然大部分未发布。关于已经公布的凯普勒特性，讨论内容可以在[第12章](CHP012.html)中找到，位于“面向未来GPU的开发”一节。
